<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 10 Aug 2024 22:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[DEF CON's response to the badge controversy (121 pts)]]></title>
            <link>https://old.reddit.com/r/Defcon/comments/1ep00ln/def_cons_response_to_the_badge_controversy/</link>
            <guid>41211519</guid>
            <pubDate>Sat, 10 Aug 2024 19:07:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/Defcon/comments/1ep00ln/def_cons_response_to_the_badge_controversy/">https://old.reddit.com/r/Defcon/comments/1ep00ln/def_cons_response_to_the_badge_controversy/</a>, See on <a href="https://news.ycombinator.com/item?id=41211519">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>DEF CON thrives on community collaboration and has operated for over 30 years successfully working with hundreds of vendors including the dozens that have helped with our badges over the years. For this year’s Raspberry Pi badges, DEF CON hired Entropic Engineering to do the hardware development and firmware. After going overbudget by more than 60%, several bad-faith charges, and with a product still in preproduction, DEF CON issued a stop work order. Any claims that DEF CON did not pay Entropic Engineering for its hardware or firmware development are false. Unfortunately, we heard that these issues with Entropic Engineering were not unique to DEF CON. We decided at that point to finish the badge on our own. We paid to send engineers to Vietnam to work onsite to finalize and test the badges in order to ensure they would be done on time for the conference. We never removed Entropic Engineering’s logo from our badge, it is still on the PCB. However, Entropic was not involved in the design and production of the case, and we removed their logo we had added as a courtesy.</p>

<p>We were happy to still include one of their contractors on the badge panel session. Unfortunately, shortly before the talk was set to take place DEF CON became aware that unauthorized code had been included in the firmware we had paid Entropic Engineering to produce, claiming credit for the whole badge and promoting their coin wallet to solicit money from DEF CON attendees above and beyond what we had negotiated. When asked about the unauthorized code, the engineer said it had been done as a “joke” two months ago and forgot to remove it, and we decided as an organization not to have him on stage while we kept the slides in the talk giving him credit for his work. We communicated the change in advance of the talk, and this individual decided to show up for the panel anyway. He refused to leave, demanding that our security team remove him. Wanting to ensure that the other people involved in creating the badge were able to deliver their presentation, we complied with his wishes and escorted him off the stage, where he was free to continue attending the conference.</p>

<p>Any issues of non-payment are between him and Entropic Engineering, DEF CON fulfilled its financial obligations.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["Jeff Bezos and Amazon tried to imprison my husband" (188 pts)]]></title>
            <link>https://twitter.com/Amy_K_Nelson/status/1822318185556648348</link>
            <guid>41211437</guid>
            <pubDate>Sat, 10 Aug 2024 18:53:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/Amy_K_Nelson/status/1822318185556648348">https://twitter.com/Amy_K_Nelson/status/1822318185556648348</a>, See on <a href="https://news.ycombinator.com/item?id=41211437">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Apple. Apple Please (142 pts)]]></title>
            <link>https://digipres.club/@misty/112927898002214724</link>
            <guid>41211235</guid>
            <pubDate>Sat, 10 Aug 2024 18:14:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://digipres.club/@misty/112927898002214724">https://digipres.club/@misty/112927898002214724</a>, See on <a href="https://news.ycombinator.com/item?id=41211235">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[OpenSnitch is a GNU/Linux interactive application firewall (222 pts)]]></title>
            <link>https://github.com/evilsocket/opensnitch</link>
            <guid>41209688</guid>
            <pubDate>Sat, 10 Aug 2024 14:15:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/evilsocket/opensnitch">https://github.com/evilsocket/opensnitch</a>, See on <a href="https://news.ycombinator.com/item?id=41209688">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><p>
  Join the project community on our server!
  </p><p>
  <a href="https://discord.gg/https://discord.gg/btZpkp45gQ" title="Join our community!" rel="nofollow">
    <img src="https://camo.githubusercontent.com/549e93886be3c89143d30b3a80f7b34e8fedee957710c2481953ddde669193c6/68747470733a2f2f646362616467652e6c696d65732e70696e6b2f6170692f7365727665722f68747470733a2f2f646973636f72642e67672f62745a706b7034356751" data-canonical-src="https://dcbadge.limes.pink/api/server/https://discord.gg/btZpkp45gQ">
  </a></p></div>
<hr>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/evilsocket/opensnitch/master/ui/opensnitch/res/icon.png"><img alt="opensnitch" src="https://raw.githubusercontent.com/evilsocket/opensnitch/master/ui/opensnitch/res/icon.png" height="160"></a>
  </p><p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/evilsocket/opensnitch/workflows/Build%20status/badge.svg"><img src="https://github.com/evilsocket/opensnitch/workflows/Build%20status/badge.svg"></a>
    <a href="https://github.com/evilsocket/opensnitch/releases/latest"><img alt="Release" src="https://camo.githubusercontent.com/219cfed611036fcac4f1c190954d3d0af9f7d489d2e5d69e10b2415a86f18de2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6576696c736f636b65742f6f70656e736e697463682e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/github/release/evilsocket/opensnitch.svg?style=flat-square"></a>
    <a href="https://github.com/evilsocket/opensnitch/blob/master/LICENSE.md"><img alt="Software License" src="https://camo.githubusercontent.com/d74ecd3c454461cffea50e16ee633e212ab258222b06e5fd630d34c5429c2fa5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c332d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/license-GPL3-brightgreen.svg?style=flat-square"></a>
    <a href="https://goreportcard.com/report/github.com/evilsocket/opensnitch/daemon" rel="nofollow"><img alt="Go Report Card" src="https://camo.githubusercontent.com/6d9060c6e28f36e61ee8e0f59335b109a23f9ce97e9554da72cd553a3efca3dd/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f6576696c736f636b65742f6f70656e736e697463682f6461656d6f6e3f7374796c653d666c61742d737175617265" data-canonical-src="https://goreportcard.com/badge/github.com/evilsocket/opensnitch/daemon?style=flat-square"></a>
    <a href="https://repology.org/project/opensnitch/versions" rel="nofollow"><img src="https://camo.githubusercontent.com/24dbb94e706fb18f6b34697db56522fcbe2f6172f058b05710822bd39a45a367/68747470733a2f2f7265706f6c6f67792e6f72672f62616467652f74696e792d7265706f732f6f70656e736e697463682e737667" alt="Packaging status" data-canonical-src="https://repology.org/badge/tiny-repos/opensnitch.svg"></a>
  </p>

<p dir="auto"><strong>OpenSnitch</strong> is a GNU/Linux application firewall.</p>
<p dir="auto">•• <a href="#key-features">Key Features</a> • <a href="#download">Download</a> • <a href="#installation">Installation</a> • <a href="#opensnitch-in-action">Usage examples</a> • <a href="#in-the-press">In the press</a> ••</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/2742953/85205382-6ba9cb00-b31b-11ea-8e9a-bd4b8b05a236.png"><img src="https://user-images.githubusercontent.com/2742953/85205382-6ba9cb00-b31b-11ea-8e9a-bd4b8b05a236.png" alt="OpenSnitch"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key features</h2><a id="user-content-key-features" aria-label="Permalink: Key features" href="#key-features"></a></p>
<ul dir="auto">
<li>Interactive outbound connections filtering.</li>
<li><a href="https://github.com/evilsocket/opensnitch/wiki/block-lists">Block ads, trackers or malware domains</a> system wide.</li>
<li>Ability to <a href="https://github.com/evilsocket/opensnitch/wiki/System-rules">configure system firewall</a> from the GUI (nftables).
<ul dir="auto">
<li>Configure input policy, allow inbound services, etc.</li>
</ul>
</li>
<li>Manage <a href="https://github.com/evilsocket/opensnitch/wiki/Nodes">multiple nodes</a> from a centralized GUI.</li>
<li><a href="https://github.com/evilsocket/opensnitch/wiki/SIEM-integration">SIEM integration</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download</h2><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<p dir="auto">Download deb/rpm packages for your system from <a href="https://github.com/evilsocket/opensnitch/releases">https://github.com/evilsocket/opensnitch/releases</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">deb</h4><a id="user-content-deb" aria-label="Permalink: deb" href="#deb"></a></p>
<blockquote>
<p dir="auto">$ sudo apt install ./opensnitch*.deb ./python3-opensnitch-ui*.deb</p>
</blockquote>
<p dir="auto"><h4 tabindex="-1" dir="auto">rpm</h4><a id="user-content-rpm" aria-label="Permalink: rpm" href="#rpm"></a></p>
<blockquote>
<p dir="auto">$ sudo yum localinstall opensnitch-1*.rpm; sudo yum localinstall opensnitch-ui*.rpm</p>
</blockquote>
<p dir="auto">Then run: <code>$ opensnitch-ui</code> or launch the GUI from the Applications menu.</p>
<p dir="auto">Please, refer to <a href="https://github.com/evilsocket/opensnitch/wiki/Installation">the documentation</a> for detailed information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">OpenSnitch in action</h2><a id="user-content-opensnitch-in-action" aria-label="Permalink: OpenSnitch in action" href="#opensnitch-in-action"></a></p>
<p dir="auto">Examples of OpenSnitch intercepting unexpected connections:</p>
<p dir="auto"><a href="https://github.com/evilsocket/opensnitch/discussions/categories/show-and-tell">https://github.com/evilsocket/opensnitch/discussions/categories/show-and-tell</a></p>
<p dir="auto">Have you seen a connection you didn't expect? <a href="https://github.com/evilsocket/opensnitch/discussions/new?category=show-and-tell">submit it!</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">In the press</h2><a id="user-content-in-the-press" aria-label="Permalink: In the press" href="#in-the-press"></a></p>
<ul dir="auto">
<li>2017 <a href="https://twitter.com/pentestmag/status/857321886807605248" rel="nofollow">PenTest Magazine</a></li>
<li>11/2019 <a href="https://itsfoss.com/opensnitch-firewall-linux/" rel="nofollow">It's Foss</a></li>
<li>03/2020 <a href="https://www.linux-magazine.com/Issues/2020/232/Firewalld-and-OpenSnitch" rel="nofollow">Linux Format #232</a></li>
<li>08/2020 <a href="https://linux-magazine.pl/archiwum/wydanie/387" rel="nofollow">Linux Magazine Polska #194</a></li>
<li>08/2021 <a href="https://github.com/evilsocket/opensnitch/discussions/631" data-hovercard-type="discussion" data-hovercard-url="/evilsocket/opensnitch/discussions/631/hovercard">Linux Format #280</a></li>
<li>02/2022 <a href="https://www.linux-community.de/magazine/linuxuser/2022/03/" rel="nofollow">Linux User</a></li>
<li>06/2022 <a href="https://www.linux-magazine.com/Issues/2022/259/OpenSnitch" rel="nofollow">Linux Magazine #259</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Donations</h2><a id="user-content-donations" aria-label="Permalink: Donations" href="#donations"></a></p>
<p dir="auto">If you find OpenSnitch useful and want to donate to the dedicated developers, you can do it from the <strong>Sponsor this project</strong> section on the right side of this repository.</p>
<p dir="auto">You can see here who are the current maintainers of OpenSnitch:
<a href="https://github.com/evilsocket/opensnitch/commits/master">https://github.com/evilsocket/opensnitch/commits/master</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<p dir="auto"><a href="https://github.com/evilsocket/opensnitch/graphs/contributors">See the list</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Translating</h2><a id="user-content-translating" aria-label="Permalink: Translating" href="#translating"></a></p>
<a href="https://hosted.weblate.org/engage/opensnitch/" rel="nofollow">
<img src="https://camo.githubusercontent.com/75c6d0a4c406c1a7802bb3b7283238c3df71084751e7aa00f9822d203876e903/68747470733a2f2f686f737465642e7765626c6174652e6f72672f776964676574732f6f70656e736e697463682f2d2f676c6f73736172792f6d756c74692d6175746f2e737667" alt="Translation status" data-canonical-src="https://hosted.weblate.org/widgets/opensnitch/-/glossary/multi-auto.svg">
</a>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deep Live Cam: Real-time face swapping and one-click video deepfake tool (184 pts)]]></title>
            <link>https://deeplive.cam</link>
            <guid>41209181</guid>
            <pubDate>Sat, 10 Aug 2024 13:05:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deeplive.cam">https://deeplive.cam</a>, See on <a href="https://news.ycombinator.com/item?id=41209181">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="hero"><h2>Deep Live Cam<br> <!-- -->The Next Leap in Real-Time Face Swapping and Video Deepfake Technology</h2><p>Deep Live Cam harnesses cutting-edge AI to push the boundaries of real-time face swapping and video deepfakes.<br> <!-- -->Achieve high-quality face replacement with just a single image.</p></section><div><div id="features"><p><h4>editions</h4><h2>Deep Live Cam Supports Multiple Execution Platforms</h2></p></div><div><h2>Deep Live Cam: Bringing Your Ideas to Life</h2><p>Deep Live Cam is a state-of-the-art AI tool that delivers astonishingly accurate real-time face swapping and video deepfakes. Here's what sets it apart:</p><div><div><p>Swap faces in real-time using a single image, with instant preview capabilities.</p></div><div><div><p><span>One-Click Video Deepfakes</span></p></div><p>Generate high-quality deepfake videos quickly and easily with simple operations.</p></div><div><p>Run on various platforms including CPU, NVIDIA CUDA, and Apple Silicon, adapting to different hardware setups.</p></div><div><p>Built-in checks prevent processing of inappropriate content, ensuring legal and ethical use.</p></div><div><p>Leverages optimized algorithms for significantly faster processing, especially on CUDA-enabled NVIDIA GPUs.</p></div><div><p>Benefit from an active community providing ongoing support and improvements, keeping the tool at the cutting edge.</p></div></div></div><div><h2>How Deep Live Cam Works</h2><p>Deep Live Cam employs advanced AI algorithms to achieve real-time face swapping and video deepfakes.</p></div><section><h2>What Users Are Saying About Deep Live Cam on X</h2><p>Explore real experiences and creations shared by developers and users on X. See how Deep Live Cam is inspiring creativity and solving practical problems across various fields, from stunning face-swap effects to innovative applications.</p></section><div id="faq"><div><h2>Frequently Asked Questions About Deep Live Cam</h2><p>Get answers to common questions about Deep Live Cam</p></div><div><div><h3>What is Deep Live Cam?</h3><p>Deep Live Cam is an open-source tool for real-time face swapping and one-click video deepfakes. It can replace faces in videos or images using a single photo, ideal for video production, animation, and various creative projects.</p><hr></div><div><h3>What are the main features of Deep Live Cam?</h3><p>Deep Live Cam's key features include: 1) Real-time face swapping; 2) One-click video deepfakes; 3) Multi-platform support; 4) Ethical use safeguards.</p><hr></div><div><h3>How do I use Deep Live Cam?</h3><p>To use Deep Live Cam: 1) Set up the required environment; 2) Clone the GitHub repository; 3) Download necessary models; 4) Install dependencies; 5) Run the program; 6) Select source image and target; 7) Start the face-swapping process.</p><hr></div><div><h3>Which platforms does Deep Live Cam support?</h3><p>Deep Live Cam supports various execution platforms, including CPU, NVIDIA CUDA, Apple Silicon (CoreML), DirectML (Windows), and OpenVINO (Intel). Users can choose the optimal platform based on their hardware configuration.</p><hr></div><div><h3>How does Deep Live Cam prevent misuse?</h3><p>Deep Live Cam incorporates built-in checks to prevent processing of inappropriate content (e.g., nudity, violence, sensitive material). The developers are committed to evolving the project within legal and ethical frameworks, implementing measures like watermarking outputs when necessary to prevent abuse.</p><hr></div><div><h3>Is Deep Live Cam free to use?</h3><p>Yes, Deep Live Cam is an open-source project and completely free to use. You can access the source code on GitHub and use it freely.</p><hr></div><div><h3>Can I use Deep Live Cam for commercial purposes?</h3><p>While Deep Live Cam is open-source, for commercial use, you should carefully review the project's license terms. Additionally, using deepfake technology may involve legal and ethical considerations. We recommend consulting with legal professionals before any commercial application.</p><hr></div><div><h3>What are the hardware requirements for Deep Live Cam?</h3><p>Deep Live Cam's performance varies with hardware configuration. Basic functionality runs on standard CPUs, but for optimal performance and results, we recommend using CUDA-enabled NVIDIA GPUs or devices with Apple Silicon chips.</p><hr></div><div><h3>Does Deep Live Cam support real-time video stream processing?</h3><p>Yes, Deep Live Cam supports real-time video stream processing. You can use a webcam for real-time face swapping, with the program providing live preview functionality.</p><hr></div><div><h3>How can I improve the face-swapping results in Deep Live Cam?</h3><p>To enhance face-swapping results, try: 1) Using high-quality, clear source images; 2) Choosing source and target images with similar angles and lighting; 3) Adjusting program parameters; 4) Running the program on more powerful hardware.</p><hr></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A wonderful coincidence or an expected connection: why π² ≈ g (331 pts)]]></title>
            <link>https://roitman.io/blog/91</link>
            <guid>41208988</guid>
            <pubDate>Sat, 10 Aug 2024 12:24:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roitman.io/blog/91">https://roitman.io/blog/91</a>, See on <a href="https://news.ycombinator.com/item?id=41208988">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Let’s take a brief trip back to our school years and recall some lessons in mathematics and physics. Do you remember what the number π equals? And what is π squared? That’s a strange question too. Of course, it’s 9.87. And do you remember the value of the acceleration due to gravity, g? Of course, that number was drilled into our memory so thoroughly that it’s impossible to forget: 9.81 m/s². Naturally, it can vary, but for solving basic school problems, we typically used this value.</p><p>August 9, 2024</p></div><p><span><h2><strong>Mysterious equality</strong></h2><p>And now, here’s the next question: how on earth is it that π² is approximately equal to g? You might say that such questions aren’t asked in polite society. First of all, they aren’t exactly equal. There’s already a difference in the second decimal place. Secondly, π is a dimensionless number, while g is a physical quantity with its own units.</p><p>And yet, no matter how you look at it, this can’t just be a simple coincidence.</p><h2><strong>Not as simple as it seems</strong></h2><p>Let's start by taking a close look at the right side. The value 9.81 is in m/s². But these are far from the only units of measurement. If you express this value in any other units, the magic immediately disappears. So, this is no coincidence—let's dig deeper into the meters and seconds.</p><p>What exactly is a "meter," and how could it be related to π? At first glance, not at all. According to Wikipedia, a "meter is the distance light travels in a vacuum during a time interval of 1/299,792,458 seconds." Great, now we have seconds involved—good! But there's still nothing about π.</p><p>Wait a minute, why exactly 1/299,792,458? Why not, for example, 1/300? Where did this number come from in the first place? It seems we need to delve into the history of the unit of length itself to understand this better.</p><h2>A standard for every honest merchant</h2><p>In the past, people didn't bother much with standards: they only cared about what was convenient for measurement. For example, why not measure length in human cubits? It might not be precise, but it was cheap, reliable, and practical. And the fact that everyone's cubits were of different lengths? Sometimes that was even useful. If you needed to buy more cloth, you'd call the tallest person in the village and have them measure the fabric with their cubits.</p><p>Later on, of course, people began thinking about standardization. They started creating various standards. But this turned out to be inconvenient and cumbersome: you couldn't always run to a single standard for measurement. So, copies of the standards began to appear. And then copies of the copies...</p><p>Serious people decided that such chaos was hindering serious business, so they set a goal: to come up with a definition of a unit of length that wouldn't depend on any arbitrary standards. It should only depend on natural constants, so that anyone with some basic tools could reproduce and measure it.</p><h2>Bright dreams of standardization and insidious gravity</h2><p>A "standard-free" definition for the meter was actually proposed back in the 17th century. The Dutch mechanic, physicist, mathematician, astronomer, and inventor Christiaan Huygens suggested using a simple pendulum for this purpose. You take a small object and suspend it on a string. The length of the string should be such that the pendulum completes a full oscillation (returns to its original position) in exactly two seconds. This length of string was called the "universal measure" or the "Catholic meter." This length differed from the modern meter by about half a centimeter.</p><p>The proposal was well-received and adopted. However, problems soon arose. First, Huygens was dealing with what he called a "mathematical pendulum." This is a "material point suspended on a weightless, inextensible string." A material point and a weightless string are hardly the simple tools that every merchant would have on hand.</p><p>Second, it was quickly discovered that the length of the pendulum's string varied in different parts of the Earth. Gravity cunningly decreased as one approached the equator and did not cooperate with humanity's bright dream of standardization.</p><h2>An astonishing equation</h2><p>But let’s return to our mysterious equation. To find the period of small oscillations of a mathematical pendulum as a function of the length of the suspension, the following formula is used:</p><figure><img src="https://nkjhvudpdnbuifryqtzj.supabase.co/storage/v1/object/public/pictures/public/e6232f8b-513a-46c4-a056-a0537b26f421"><figcaption></figcaption></figure><p>And here it is—our π! Let's substitute the parameters of Huygens’ pendulum into this formula. The length of the string l in Huygens' pendulum equals 1. The T - oscillation equals 2. Plugging these values into the formula, we get π²=g.</p><p>So, have we found the answer to our question? Well, not quite. We already saw that the equality is only approximate. It doesn’t feel right to equate 9.87 and 9.81 exactly. Does this mean that the meter has changed since then?</p><h2>With revolutionary greetings from France</h2><p>Yes, indeed, it did change! This occurred during the reform of the units of measurement initiated by the French Academy of Sciences in 1791. Intelligent people suggested maintaining the definition of the meter through the pendulum, but with the clarification that it should specifically be a French pendulum—at the latitude of 45° N (approximately between Bordeaux and Grenoble).</p><p>However, this did not sit well with the commission in charge of the reform. The problem was that the head of the commission, Jean-Charles de Borda, was a fervent supporter of transitioning to a new (revolutionary) system of angle measurement—using grads (a grad being one-hundredth of a right angle). Each grad was divided into 100 minutes, and each minute into 100 seconds. The method of the seconds pendulum did not fit into this neat concept.</p><h2>The true and final meter</h2><p>In the end, they successfully got rid of the seconds and defined the meter as one forty-millionth of the Paris meridian. Or, alternatively, as one ten-millionth of the distance from the North Pole to the equator along the surface of the Earth’s ellipsoid at the longitude of Paris. This measurement slightly differed from the "pendulum" meter. The commission, without false modesty, dubbed the resulting value as the "true and final meter."</p><p>The idea of a universal standard accessible to everyone waved goodbye and faded into the sunset. Need an accurate standard for the meter? No problem! All you have to do is measure the length of a meridian and divide it by a few million. By the way, the French actually did this—they physically measured a portion of the Paris meridian, the arc from Dunkirk to Barcelona. They laid out a chain of 115 triangles across France and part of Spain. Based on these measurements, they created a brass standard. Incidentally, they made a mistake—they didn't account for the Earth's polar flattening.</p><h2><strong>Conclusion</strong></h2><p>Let's return to our equation once again. Now we know where the inaccuracy comes from: π² and g differ by about 0.06. If it weren't for yet another attempt to reform and improve everything, we would now have a slightly different value for the meter and the elegant equation π² = g. Later, scientists did return to defining the meter through unchanging and reproducible natural constants, but the meter standard was no longer the same.</p></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ladybird browser to start using Swift language this fall (174 pts)]]></title>
            <link>https://twitter.com/awesomekling/status/1822236888188498031</link>
            <guid>41208836</guid>
            <pubDate>Sat, 10 Aug 2024 11:52:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/awesomekling/status/1822236888188498031">https://twitter.com/awesomekling/status/1822236888188498031</a>, See on <a href="https://news.ycombinator.com/item?id=41208836">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Susan Wojcicki has died (150 pts)]]></title>
            <link>https://www.facebook.com/share/p/qe2ZMcs9Bz4K1SPt/</link>
            <guid>41207446</guid>
            <pubDate>Sat, 10 Aug 2024 05:07:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.facebook.com/share/p/qe2ZMcs9Bz4K1SPt/">https://www.facebook.com/share/p/qe2ZMcs9Bz4K1SPt/</a>, See on <a href="https://news.ycombinator.com/item?id=41207446">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Susan Wojcicki has died (625 pts)]]></title>
            <link>https://twitter.com/sundarpichai/status/1822132667959386588</link>
            <guid>41207415</guid>
            <pubDate>Sat, 10 Aug 2024 04:58:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/sundarpichai/status/1822132667959386588">https://twitter.com/sundarpichai/status/1822132667959386588</a>, See on <a href="https://news.ycombinator.com/item?id=41207415">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Defcon stiffs badge HW vendor, drags FW author offstage during talk (455 pts)]]></title>
            <link>https://twitter.com/mightymogomra/status/1822119942281650278</link>
            <guid>41207221</guid>
            <pubDate>Sat, 10 Aug 2024 03:59:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/mightymogomra/status/1822119942281650278">https://twitter.com/mightymogomra/status/1822119942281650278</a>, See on <a href="https://news.ycombinator.com/item?id=41207221">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Caltech Develops First Noninvasive Method to Continually Measure Blood Pressure (171 pts)]]></title>
            <link>https://www.caltech.edu/about/news/caltech-team-develops-first-noninvasive-method-to-continually-measure-true-blood-pressure</link>
            <guid>41207182</guid>
            <pubDate>Sat, 10 Aug 2024 03:53:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.caltech.edu/about/news/caltech-team-develops-first-noninvasive-method-to-continually-measure-true-blood-pressure">https://www.caltech.edu/about/news/caltech-team-develops-first-noninvasive-method-to-continually-measure-true-blood-pressure</a>, See on <a href="https://news.ycombinator.com/item?id=41207182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-block-key="qqzwe">Solving a decades-old problem, a multidisciplinary team of Caltech researchers has figured out a method to noninvasively and continually measure blood pressure anywhere on the body with next to no disruption to the patient. A device based on the new technique holds the promise to enable better vital-sign monitoring at home, in hospitals, and possibly even in remote locations where resources are limited.</p><p data-block-key="dk5fr">The new patented technique, called resonance sonomanometry, uses sound waves to gently stimulate resonance in an artery and then uses ultrasound imaging to measure the artery's resonance frequency, arriving at a true measurement of blood pressure. In a small clinical study, the device, which gives patients a gentle buzzing sensation on the skin, produced results akin to those obtained using the standard-of-care blood pressure cuff.</p><p data-block-key="970vo">"We ended up with a device that is able to measure the absolute blood pressure—not only the systolic and diastolic numbers that we are used to getting from blood pressure cuffs—but the full waveform," says <a href="https://www.eas.caltech.edu/people/yaser">Yaser Abu-Mostafa</a> (PhD '83), professor of electrical engineering and computer science and one of the authors of a <a href="https://academic.oup.com/pnasnexus/article-lookup/doi/10.1093/pnasnexus/pgae252">new paper</a> describing the technique and device in the journal <i>PNAS Nexus</i>. "With this device you can measure blood pressure continuously and in different sites on the body, giving you much more information about the blood pressure of a person."</p><p data-block-key="2mnhc">"This team has been working for almost a decade, trying to build something that makes a difference, that is good enough to solve a real clinical problem," says Aditya Rajagopal (BS '08, PhD '14), visiting associate in electrical engineering at Caltech, research adjunct assistant professor of biomedical engineering at USC, and a co-author of the new paper. "Many groups, including tech giants like Apple and Google, have been working toward a solution like this, because it enables a spectrum of patient-monitoring possibilities from the hospital to the home. Our method broadens access to hospital-grade monitoring of blood pressure and cardiac health metrics."</p><p data-block-key="dj1o6"><b>Blood pressure 101</b></p><p data-block-key="7dki3">Blood pressure is simply the force of blood pushing on the walls of the body's blood vessels as it gets pumped around the body. High blood pressure, or hypertension, is related to risk of heart attack, stroke, chronic kidney disease, and other health problems. Low blood pressure, or hypotension, can also be a serious problem because it means the blood is not carrying enough oxygen to the organs. Taking regular measurements of blood pressure is considered one of the best ways to monitor overall health and to identify potential problems.</p><p data-block-key="62t65">Most of us have experienced the cuff-style measurement of blood pressure. A nurse, doctor, or machine inflates a cuff that fits around the upper arm until blood can no longer flow, and then slowly releases the air from the cuff while listening for the sound that blood makes as it once again begins to flow. The pressure in the cuff at that point corresponds to the blood pressure in the patient's arteries. But this technique has limitations: It can only be performed periodically, as it involves occluding a blood vessel, and can only collect data from the arm.</p><p data-block-key="a2ou5">Physicians would very much like to have continuous readings that provide full waveforms of a patient's blood pressure, and not only peripheral measurements from an arm but also central measurements from the chest and other parts of the body. To get the full information they need, intensive care physicians and surgeons sometimes resort to inserting a catheter directly into the artery of critical patients (a practice known as placing an arterial line, or "a-line"). This is invasive and can be risky, but, until now, it has been the only way to get a continuous readout of true blood pressure. In some cases, such as problems with heart valves, full blood-pressure waveforms can provide physicians with diagnostic information that they cannot get any other way.</p><p data-block-key="6gtjj">"There's a lot of information in that waveform that is really valuable," says Alaina Brinley Rajagopal, a visiting associate in electrical engineering at Caltech, an emergency medicine physician, and a co-author of the paper. And other blood pressure devices developed over the last decade or two require a calibration step that emergency physicians simply do not have time for, she says. "I need to be able to put something on a patient and have it work immediately."</p><p data-block-key="camvc">The new device fits the bill. The current prototype, built and tested by a spin-off company called Esperto Medical, is housed in a transducer case smaller than a deck of cards and is mounted on an armband, though the researchers say it could eventually fit within a package the size of a watch or adhesive patch. The team aims for the device to first be used in hospitals, where it would connect via wire to existing hospital monitors. It could mean that doctors would no longer have to weigh the risks of placing an a-line in order to get the continuous monitoring of real blood pressure for any patient.</p><p data-block-key="fqe92">Eventually, Brinley says their device could replace blood pressure cuffs as well. "Blood pressure cuffs only take one measurement as often as you run the cuff, so if you're asking patients to monitor their blood pressure at home, they have to know how to use the device, they have to put it on, and they have to be motivated to record the information, and I would say a majority of patients do not do that," says Brinley Rajagopal. "Having a device like ours, where it is just place and forget, you can wear it all day, and it can take however many measurements your provider wants, that would allow for better, precision dosing of medication."</p><p data-block-key="6c36e"><b>Developing a game changer</b></p><p data-block-key="eib58">Rajagopal recalls the long road it has been getting to this point with the blood pressure device. About a decade ago, Brinley Rajagopal returned from a global health trip particularly frustrated by the standard of care she could provide patients in remote locations. Talking with Rajagopal, the two wished they could invent something like a medical tricorder, a handheld device seen in <i>Star Trek</i> that helped the fictional doctors of the future scan patients, gather medical information, and diagnose. "That got us thinking about technologies we could adapt to get us closer to a goal like that," says Brinley Rajagopal. Those initial sci-fi–inspired discussions eventually led them down the path to try to develop a better blood pressure monitor.</p><p data-block-key="1cqst">But their first efforts did not pan out. After years of work on a possible solution using blood velocity to derive blood pressure, the team decided that they had reached a dead end. As with many other current blood pressure monitoring devices, that approach could only provide the <i>relative</i> blood pressure—the difference between the high and low measurements without the absolute number. It also required calibration.</p><p data-block-key="dlbkg"><b>Back to the drawing board</b></p><p data-block-key="8gfok">Rajagopal decided it was time to reevaluate and determine if they had any chance of solving this problem. "It was this moment of desperation that actually led to the key insight," says Rajagopal.</p><p data-block-key="1si80">Thinking back to his first-year physics course at Caltech, he began scribbling on a nearby wall. He remembered that his Physics1 textbook presented a canonical problem: You have a string under tension. How can you determine how taut the line is? If you tweeze the string, you can relate the velocity at which vibration waves travel back and forth on the string to the resonance frequency in the string, which could give you your answer. "I thought if I could stretch an artery in one direction and magically tweeze it and let it go, the ringing would give us the resonance frequency, which would get us to blood pressure," says Rajagopal. After six years of failures and returning to first principles, they finally had their guiding insight.</p><p data-block-key="dort6">And indeed, that is the underlying idea behind the new device: Like a guitar changing pitch as it is plucked while being tightened, the frequency at which an artery resonates when struck by sound waves changes depending on the pressure of the blood it contains.</p><p data-block-key="4803d">This resonance frequency can be measured with ultrasound, providing a measure of blood pressure. This measurement requires three parameters—a measurement of the artery's radius, the thickness of the artery's walls, and the tension or energy in the skin of the artery.</p><p data-block-key="8tptm">With the physics worked out, there were still a lot of other details to be resolved—identifying the sound waves that would make arteries resonate, understanding how to measure that resonance, and then determining how to efficiently map that back to blood pressure, and, significantly, how to build a working system.</p><p data-block-key="6pmif">"Building that system required some extraordinarily bespoke technologies," says Rajagopal. Caltech alumnus Raymond Jimenez (BS '13) was instrumental in building out that first system. "The art form, which involved a lot of other Caltech alumni, was to put the physics answer into a very simple, practical instrument."</p><p data-block-key="e1p0o">The resulting Esperto device is small, noninvasive, relatively inexpensive, and it has an automated method for locating the patient's blood vessel without needing to be physically repositioned. It also does not suffer from the problems that some blood pressure monitoring devices have, such as not being accurate for patients with low blood pressure or getting varying results depending on a patient's skin tone.</p><p data-block-key="ck0n8">It might not be a medical tricorder, but the team says the device solves the longstanding blood pressure monitoring problem. And Rajagopal says it is the product of a million small leaps. "Everything we've done is a product of the exact mistakes we've made over time," he says, "and all the work that others have done too."</p><p data-block-key="81uf3">"This work is emblematic of what makes Caltech so remarkable: solving a very hard problem by going back to first principles and understanding a physical phenomenon at the fundamental level," says Fred Farina, Caltech's Chief Innovation and Corporate Partnerships Officer. "This approach, combined with the tenacity and entrepreneurial drive of the team, is our homemade recipe for societal impact and improving people's lives."</p><p data-block-key="3vl0c">The paper describing the new technique is titled "Resonance sonomanometry for noninvasive, continuous monitoring of blood pressure." Additional authors on the paper include Raymond Jimenez (BS '13), Steven Dell, Austin C. Rutledge, Matt K. Fu (BS '13), and William P. Dempsey (PhD '12) of Esperto Medical, and Dominic Yurk (BS '17, PhD '23), a current member of Abu-Mostafa's group at Caltech. The work at Caltech was supported by Caltech trustee Charles Trimble (BS '63, MS '64), the Carver Mead Innovation Fund, and the Grubstake Fund.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a highly-available web service without a database (236 pts)]]></title>
            <link>https://blog.screenshotbot.io/2024/08/10/building-a-highly-available-web-service-without-a-database/</link>
            <guid>41206908</guid>
            <pubDate>Sat, 10 Aug 2024 02:37:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.screenshotbot.io/2024/08/10/building-a-highly-available-web-service-without-a-database/">https://blog.screenshotbot.io/2024/08/10/building-a-highly-available-web-service-without-a-database/</a>, See on <a href="https://news.ycombinator.com/item?id=41206908">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>If you’ve ever built a web service or a web app, you know the drill: pick a database, pick a web service framework (and in today’s day and age, pick a front-end framework, but let’s not get into that).</p>



<p>This has been the case for several decades now, and people don’t stop to question if this is still the best way to build a web app. Many things have changed in the last decade:</p>



<ul>
<li>Disk is a lot faster (NVMe)</li>



<li>Disk is a lot more robust (EBS/EFS etc.)</li>



<li>RAM is super cheap, for most startups you could probably fit all your data in RAM</li>



<li>You can rent a machine with hundreds of cores if your heart desires.</li>
</ul>



<p>This was not the case when I first worked at a Rails startup in 2010. But most importantly, there’s one new very important change that’s happened in the last decade:</p>



<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Raft_(algorithm)">Raft Consensus algorithm</a> was published in 2014 with many robust implementations easily available.</li>
</ul>



<p>In this blog post, we’re going to break down a new architecture for web development. We use it successfully for <a href="https://screenshotbot.io/">Screenshotbot</a>, and we hope you’ll use it too.</p>



<p>I’ll break this blog post into three parts: Explore, Expand and Extract, obviously referencing <a href="https://tidyfirst.substack.com/">Kent Beck</a>‘s 3X. Your needs are going to vary in each of these stages of your startup, and I’m going to demonstrate how you use the architecture in all three phases.</p>



<h2>Explore</h2>



<p>So you’re a new startup. You’re iterating on a product, you have no idea how people are going to use it, or even <em>if</em> they’re going to use it.</p>



<p>For most startups today, this would mean you’ll pick Rails or Django or Node or some such, backed with a MySQL or PostgreSQL or MongoDB or some such.</p>



<p>“<em>Keep it simple silly</em>,” you say, and this seems simple enough.</p>



<p>But is this as simple as possibly can be? Could we make it simpler? What if the web service and the database instance were exactly one and the same? I’m not talking about using something like SQLite where your data is still serialized, I’m saying what if all the memory in your RAM <em>is</em> your database.</p>



<p>Imagine all the wonderful things you could build if you never had to serialize data into SQL queries. First, you don’t need multiple front-end servers talking to a single DB, just get a bigger server with more RAM and more CPU if you need it. What about indices? Well, you can use in-memory indices, effectively just hash-tables to lookup objects. You don’t need clever indices like B-tree that are optimized for disk latency. (In fact, you can use some indices that were probably not possible with traditional databases. <a href="https://blog.screenshotbot.io/2023/10/29/scaling-screenshotbot/">One such index</a> using functional collections was critical to the scalability of Screenshotbot.)</p>



<p>You also won’t need special architectures to reduce round-trips to your database.  In particular, you won’t need any of that Async-IO business, because your threads are no longer IO bound. Retrieving data is just a matter of reading RAM. Suddenly debugging code has become a lot easier too.</p>



<p>You don’t need any services to run background jobs, because background jobs are just threads running in this large process.</p>



<p>You don’t need crazy concurrency protocols, because most of your concurrency requirements can be satisfied with simple in-memory mutexes and condition variables.</p>



<p>But then comes the important part: how do you recover when your process crashes? It turns out that answer is easy, periodically just take a snapshot of everything in RAM.</p>



<p>Hold on, what if you’ve made changes since the last snapshot? And this is the clever bit: you ensure that every time you change parts of RAM, we write a transaction to disk. So if you have a line like <code>foo.setBar(2)</code>, this will first write a transaction that says we’ve changed the <code>bar</code> field of <code>foo</code> to 2, and then actually set the field to 2. An operation like <code>new Foo()</code> writes a transaction to disk to say that a Foo object was created, and then returns the new object.</p>



<p>And so, if your process crashes and restarts, it first reloads the snapshot, and replays the transaction logs to fully recover the state. (Notice that index changes don’t need to be part of the transaction log. For instance if there’s an index on field <code>bar</code> from <code>Foo</code>, then <code>setBar</code> should just update the index, which will get updated whether it’s read from a snapshot, or from a transaction.)</p>



<p>Finally, this architecture enables some new kind of code that couldn’t be written before. Since all requests are being served by the same process, which <em>usually</em> doesn’t get killed, it means you can store closures in memory that can be used to serve pages. For example on Screenshotbot, if you ever see a “<a href="https://screenshotbot.io/n/" rel="nofollow">https://screenshotbot.io/n/</a><em>nnnnnnn</em>” URL, it’s actually a closure on the server, where <em>nnnnnnn</em> maps to an internal closure. But amazingly, this simple change means we don’t need to serialize objects across page transitions. The closure has references to the objects, so we don’t need to pass around object-ids across every single request. In Javascript, this might hypothetically look like:</p>



<pre><code>function renderMyObject(obj) {
   return &lt;html&gt;...
            &lt;a href=(() =&gt; obj.delete()) &gt;Delete&lt;/a&gt;
            ...
          &lt;/html&gt;
} </code></pre>



<p>What this all means is that you can iterate quickly. If you have to debug, there’s exactly one service that you need to debug. If you need to profile code, there’s exactly one service you need to profile (no more MySQL slow query logs). There’s exactly one service to monitor: if that one service goes down the site certainly goes down, but since there’s only one service and one server, the probability of failure is also much lower. If the server dies, AWS will automatically bring up a new server to replace it within a few minutes.</p>



<p>It’s also a lot easier to write test code, since you no longer have to mock out databases.</p>



<h2>Expand</h2>



<p>So you’re moving fast, iterating, and building out ideas, and slowly getting customers along the way.</p>



<p>Then one day, you get a high-profile customer. Bingo, you’re now in the <em>Expand</em> phase of your startup.</p>



<p>But there’s a catch: this high-profile customer requires 99.999% availability. </p>



<p>Surely, the architecture we just described cannot handle this. If the server goes down, we would need to wait several minutes for AWS to bring it back up. Once it’s back up, we might wait several minutes for our process to even restore the snapshot from disk. Even re-deploys are tricky: restarting the service can bring down the server for multiple minutes.</p>



<p>And this is where the Raft Consensus Protocol comes in to place. </p>



<p>Raft is a wonderful algorithm and protocol. It takes your <em>finite state machine</em> (your web server/database), and essentially replicates the transaction log. So now, we can take our very simple architecture and replicate it across three machines. If the leader goes down, then a new leader is elected within seconds and will continue to serve requests.</p>



<p>We’ve just made our simple little service into a full-fledged highly-available database, without fundamentally changing how developers write code.</p>



<p>With this mechanism, you can also do a rolling deploy without ever bringing the server down. (Although we rarely restart our server processes, more on that in a moment.) Because there’s just one service, it’s also easy to calculate your availability guarantees.</p>



<h2>Extract</h2>



<p>So your startup is doing well, and you have thousands of large customers.</p>



<p>To be honest, Screenshotbot is not at this stage, I’ll talk about where we are in a moment. But we’re preparing for this possibility, with monitoring in place for predicted bottlenecks. </p>



<p>The solution here is something large companies already do with their databases: sharding. You can break up your web services into shards, each shard being its own cluster. In particular, at Screenshotbot we already do this: each of our enterprise customers get their own dedicated cluster. (Fun story: Meta <a href="https://engineering.fb.com/2023/05/16/data-infrastructure/mysql-raft-meta/">switched to Raft</a> to handle replication for each of its MySQL clusters, so we’re essentially doing the same thing but without using a separate database.)</p>



<p>I don’t know what else to expect, since I’m more of a solve-today’s-problem kind of person. The main bottleneck I expect to see is scaling the commit-thread. The read threads parallelize beautifully. There’s one commit-thread that’s applying each transaction one at a time. It turns out the disk latency is irrelevant for this, since the Raft algorithm will just commit multiple transactions together to disk. My main concern is that the CPU cost for applying the transactions will exceed the single core performance. I highly doubt that I would ever see this, but it’s a possibility. At this point we could profile the cost of commits and improve it (for instance, move some of the work out of the transaction thread),  or we could just figure out sharding. I’ll probably write another blog post when that happens.</p>



<h2>Our Stack</h2>



<p>Now that I’ve described the idea to you, let me tell you about our stack, and why it turned out to be so suitable for this architecture.</p>



<p>We use Common Lisp. My initial implementation of Screenshotbot did use MySQL, but I quickly swapped it out for <a href="https://github.com/bknr-datastore/bknr-datastore">bknr.datastore</a> exactly because handling concurrency with MySQL was hard and Screenshotbot is a highly concurrent app. BKNR Datastore is a library that handles the architecture described in the <em>Explore</em> section, but built for Common Lisp. (There are similar libraries for other languages, but not a whole lot of them.)</p>



<p>Common Lisp is also heavily multi-threaded, and this is going to be crucial for this architecture since your web requests are being handled by threads in a single process. Ruby or Python would be disqualified by this requirement.</p>



<p>We also use the idea of closures that I mentioned earlier. But this means we can’t keep restarting the server frequently (if you restart the server, you lose the closures). So reloading code is just hot-reloading code in the running process. It turns out Common Lisp is excellent at this: so much so that a large part of the standard is all about handling reloading code. (For instance, if the class definition changes, how do you update objects of that class? <a href="http://clhs.lisp.se/Body/f_reinit.htm#reinitialize-instance">There’s a standard for it</a>.)</p>



<p>Occasionally, we do restart the servers. Currently, it looks like we only restart the servers about once every month or two months. When we need to do this, we just do a rolling restart with our Raft cluster. We use a cluster of 3 servers per installation, which allows for one server to go down. We don’t use Kubernetes, we don’t need it (at least, not yet).</p>



<p>For the Raft implementation, we wrote our own custom library built on top of bknr.datastore. We built and open-sourced <a href="https://github.com/tdrhq/bknr.cluster">bknr.cluster</a>, that under the hood uses the fantastic <a href="https://github.com/baidu/braft">Braft</a> library from Baidu. Braft is super solid, and I can highly recommend it. Braft also handles background snapshots, which means while we’re taking snapshots, the server can still continue serving requests.</p>



<p>To store image files, or blobs that shouldn’t be part of the datastore, we use EFS (a highly available NFS) that is shared between the three servers. EFS is easier to work with than S3, because we don’t have to handle error conditions. EFS also makes our code more testable, since we aren’t interacting with an external server, and just writing to disk.</p>



<p><strong>How well does this scale?</strong> We have a couple of big enterprise customers, but one especially well-known customer. Screenshotbot runs on their CI, so we get API requests 100s of times for every single commit and Pull Request. Despite this, we only need a 4-core 16GB machine to serve their requests. (And similar machines for the replicas, mostly running idle.) Even with this, the CPU usage maxes out at 20%, but even then most of that comes from image processing, so we have a lot of room to scale before we need to bump up the number of cores.</p>



<h2>Summary</h2>



<p>I think this architecture is excellent for new startups, and I’m hoping more companies will adopt it. Obviously, you’ll need to build out some of the tooling we’ve built out for the language of your choice. (Although, if you choose to use Common Lisp, it’s all here for you to use, and all open-source.)</p>



<p>We’re super grateful to the folk behind bknr.datastore, Braft and Raft, because without their work we wouldn’t be able to do any of this.</p>



<p>If you think this was useful or interesting, I would appreciate it if you could share it on social media. Please reach out to me at <a href="mailto:arnold@screenshotbot.io">arnold@screenshotbot.io</a> if you have any questions.</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rivian reduced electrical wiring by 1.6 miles and 44 pounds (158 pts)]]></title>
            <link>https://www.popsci.com/technology/rivian-zonal-electrical-architecture/</link>
            <guid>41206443</guid>
            <pubDate>Sat, 10 Aug 2024 00:12:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.popsci.com/technology/rivian-zonal-electrical-architecture/">https://www.popsci.com/technology/rivian-zonal-electrical-architecture/</a>, See on <a href="https://news.ycombinator.com/item?id=41206443">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-toc-container="">
			
<p>Unveiled just a couple of months ago, Rivian’s second-generation R1T pickup and R1S SUV will maintain their distinctive look, with playful headlamps and a sleek exterior shape. Underneath the surface is where the magic is taking place, specifically a wholly new electrical architecture the brand says is less costly and easier to service.</p>



<p><a rel="nofollow noreferrer" href="https://www.thedrive.com/news/the-rivian-r3s-retro-hatch-was-inspired-by-group-b-rally-legends" target="_blank">Rivian</a> senior vice president of electrical hardware Vidya Rajagopalan says the new electrical system offers more features, as well as an increase in sensing and computing capability. In the process of making the transition from Gen 1 to Gen 2 R1 vehicles, Rivian switched to a zonal architecture.</p>



<p>Ultimately, that results in a more sustainable option. The zonal approach reduces the total wiring length by a whopping 1.6 miles and enables each R1 vehicle to shed 44 pounds. Weight is a big deal for <a href="https://www.popsci.com/category/electric-vehicles/" target="_blank">EVs</a>, as it has a direct correlation to battery performance. Plus, the company claims a 20 percent savings in material costs and 15 percent reduction in its carbon footprint between Gen 1 and Gen 2.</p>



<p>All of it was developed in-house by Rivian’s hardware and software team, an impressive feat. Software complexity is a big deal, and Rivian is finding ways to simplify and streamline its golden goose as a software-defined automaker.</p>



<h2 id="h-zone-versus-domain-based-architecture"><strong>Zone versus domain-based architecture</strong></h2>



<p>In basketball, kids learn one-to-one defense early on. Each player is assigned to a competitor, and they stick to that person like glue for effective coverage. Zone defense requires each player to guard an area (zone) and any offensive player within those parameters. It’s a more elegant option, and one that requires more knowledge of the game.</p>



<p>Using a zonal approach, Rivian is showing off its technological prowess.</p>



<p>When Rivian engineers started building R1s, they designed a platform based on what’s called domain-based architecture, Rajagopalan explains. With this setup, each category of software is paired with a piece of hardware. Every time you open a door, raise the cabin temperature, slide your seats, turn on the lights, change the mode, and more, you could be connecting to different hardware. Those electrical control units, or ECUs, can multiply like Gremlins after dark.</p>



<figure><img decoding="async" width="2736" height="1324" loading="lazy" src="https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?strip=all&amp;quality=95" alt="x-ray-like image of SUV with ECU locations highlighted" srcset="https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png 2736w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1536&amp;h=743 1536w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=2048&amp;h=991 2048w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=930&amp;h=450 930w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=413&amp;h=200 413w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1364&amp;h=660 1364w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=827&amp;h=400 827w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1728&amp;h=836 1728w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1426&amp;h=690 1426w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=446&amp;h=216 446w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=835&amp;h=404 835w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1847&amp;h=894 1847w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1554&amp;h=752 1554w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1434&amp;h=694 1434w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=280&amp;h=135 280w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1440&amp;h=697 1440w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=289&amp;h=140 289w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=370&amp;h=179 370w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=308&amp;h=149 308w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=50&amp;h=24 50w" sizes="(max-width: 2736px) 100vw, 2736px"><figcaption>Each R1T and R1S is lighter in its second generation due to a vast reduction in electrical control units and wiring. <em>Image: Rivian</em> </figcaption></figure>



<p>“We had 17 ECUs [in Gen 1], each dedicated to a category,” Rajagopalan says. “Other manufacturers can have between 40-150 per vehicle, depending on how they work.”</p>



<p>Even though Rivian was using significantly fewer pieces of hardware than their competitors, they wanted to improve the system. More ECUs means more parts overall; consequently, that leads to increased opportunities for failure.</p>



<p>For the second generation of vehicles, four categories get their own ECUs: infotainment, autonomy, vehicle access, drive units, and its battery management system. Every other vehicle function is controlled by just three ECUs Rivian refers to as West, East, and South. The infotainment ECU alone is as powerful as a laptop and has the capabilities of a smartphone.</p>



<figure><img decoding="async" width="2573" height="1357" loading="lazy" src="https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?strip=all&amp;quality=95" alt="x-ray-like image of SUV with wiring locations highlighted" srcset="https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg 2573w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1536&amp;h=810 1536w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=2048&amp;h=1080 2048w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=853&amp;h=450 853w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=379&amp;h=200 379w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1251&amp;h=660 1251w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=758&amp;h=400 758w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1585&amp;h=836 1585w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1308&amp;h=690 1308w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=410&amp;h=216 410w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=766&amp;h=404 766w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1695&amp;h=894 1695w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1426&amp;h=752 1426w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1316&amp;h=694 1316w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=280&amp;h=148 280w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1440&amp;h=759 1440w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=289&amp;h=152 289w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=370&amp;h=195 370w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=308&amp;h=162 308w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=50&amp;h=26 50w" sizes="(max-width: 2573px) 100vw, 2573px"><figcaption>Switching from domain-based to zonal architecture allowed the company to reduce its complexity and improve scalability.&nbsp;<em>Image: Rivian</em> </figcaption></figure>



<p>However, the ECU that runs the vehicle’s autonomy platform is the most powerful computer in the R1S and R1T. The system includes an array of 11 internally developed cameras and five radars performing over 250 trillion operations per second, which<a rel="nofollow noreferrer" href="https://stories.rivian.com/meet-the-new-r1" target="_blank"> Rivian says is an industry-leading statistic</a>. As such, it connects to artificial intelligence, which helps identify and perceive the world in front of you to detect street signs, lanes, pedestrians, and more.</p>



<h2 id="h-reducing-parts-improves-scalability-and-reduces-cost"><strong>Reducing parts improves scalability and reduces cost</strong></h2>



<p>Fewer ECUs also results in less wiring, and another upshot of casting off ECUs is reduced weight.</p>



<p>“If your ECUs control function by function, you’ll have long wires running all over the car,” Rajagopalan says. “When you switch to a zone mindset, it’s more like a wheel-and-spoke function. Consolidation always wins; having fewer pieces reduces cost and makes manufacturing easier.”</p>



<p>It also makes room for scalability, something Rivian will need in order to grow.</p>



<p>Rajagopalan was working for Tesla as a senior director of engineering before joining Rivian at the end of 2020. Now she manages 400+ people at Rivian and has witnessed the evolution of the company. She says when Rivian first launched the first generation of its R1T and R1S, buyers loved the adventure-focused brand and the vehicles’ <a href="https://www.popsci.com/technology/rivian-r1t-r1s-handling/" target="_blank">off-road capabilities</a>. Since then, the automaker has proven it can ramp up its manufacturing capacity and make improvements.</p>



<p>With a new joint venture with Volkswagen in progress, Rivian will have the opportunity to build a common platform on a much larger scale. To start,<a rel="nofollow noreferrer" href="https://rivian.com/en-GB/newsroom/article/rivian-and-volkswagen-group-announce-plans-for-joint-venture" target="_blank"> VW will invest $1 billion in Rivian and invest another $4 billion over time</a>. In turn, VW will benefit from Rivian’s software-defined vehicle expertise and maverick approach.Meanwhile, we’ll be waiting for the <a href="https://www.popsci.com/technology/rivian-r3x-r2-2024/" target="_blank">smaller R2 and R3</a> to debut as Rivian ramps up.</p>
			
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grace Hopper, Nvidia's Halfway APU (117 pts)]]></title>
            <link>https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/</link>
            <guid>41206025</guid>
            <pubDate>Fri, 09 Aug 2024 22:52:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/">https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/</a>, See on <a href="https://news.ycombinator.com/item?id=41206025">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Nvidia and AMD are the biggest players in the high performance GPU space. But while Nvidia has a huge GPU market share advantage over AMD, the latter’s CPU prowess makes it a strong competitor. AMD can sell both a CPU and GPU as one unit, and that capability has gotten the company wins in consoles and supercomputers. Oak Ridge National Laboratory’s Frontier supercomputer is one such example. There, AMD MI250X GPUs interface with a custom EPYC server CPU via Infinity Fabric.</p>
<p>Of course, Nvidia is not blind to this situation. They too have a high speed in-house interconnect, called NVLink. Nvidia has also dabbled with bundling CPUs alongside their GPUs. The Nintendo Switch’s Tegra X1 is a prominent example. But Tegra used relatively small CPUs and GPUs to target low power mobile applications. Grace Hopper is an attempt to get Nvidia’s CPU efforts into high performance territory. Beyond providing server-level CPU core counts and memory bandwidth, Grace Hopper comes with Nvidia’s top-of-the-line H100 datacenter GPU.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30408"><img decoding="async" width="688" height="387" data-attachment-id="30408" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_press_image/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?fit=1077%2C606&amp;ssl=1" data-orig-size="1077,606" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_press_image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?fit=1077%2C606&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?fit=688%2C387&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?w=1077&amp;ssl=1 1077w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?resize=768%2C432&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>GH200 rendering from Nvidia, showing the CPU on the left and GPU on the right</figcaption></figure></div>
<p>I’ll be looking at GH200 as hosted on <a href="https://brokkr.hydrahost.com/">Hydra</a>. GH200 has several variants. The one I’m looking at has 480 GB of LPDDR5X memory on the CPU side, and 96 GB of HBM3 on the GPU side. I’ve already covered Neoverse V2 in Graviton 4, so I’ll focus on implementation differences rather than going over the core architecture again.</p>
<h2>System Architecture</h2>
<p>GH200 bundles a CPU and GPU together. The Grace CPU consists of 72 Neoverse V2 cores running at up to 3.44 GHz, supported by 114 MB of L3 cache. Cores and L3 cache sit on top of Nvidia’s Scalable Coherency Fabric (SCF). SCF is a mesh interconnect, with cores and L3 cache slices attached to mesh stops.</p>

<p>SCF’s responsibilities include ensuring cache coherency and proper memory ordering. From a core to core latency test, those requirements are satisfied with reasonably consistent latency across the mesh. Latency is generally comparable to Graviton 4’s, which uses Arm’s CMN-700 mesh interconnect.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30413"><img decoding="async" width="688" height="346" data-attachment-id="30413" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_c2c/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?fit=2945%2C1481&amp;ssl=1" data-orig-size="2945,1481" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_c2c" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?fit=2560%2C1287&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?fit=688%2C346&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=688%2C346&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?w=2945&amp;ssl=1 2945w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=768%2C386&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=1536%2C772&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=2048%2C1030&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=1200%2C603&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=1600%2C805&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=1320%2C664&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Core to core latency test on Grace’s CPU</figcaption></figure></div>
<p>DRAM access is handled by a 480-bit LPDDR5X-6400 setup, which provides 480 GB of capacity and 384 GB/s of theoretical bandwidth. Graviton 4 opts for a 768-bit DDR5-5200 setup for 500 GB/s of theoretical bandwidth and 768 GB of capacity. Nvidia may be betting on LPDDR5X providing lower power consumption, as DRAM power can count for a significant part of a server’s power budget.</p>
<p>GH200’s H100 GPU sits next to the Grace CPU. Even though both are sold as a single unit, it’s not an integrated GPU setup because the two chips have separate memory pools. Opting against an integrated GPU is a sensible decision because CPUs and GPUs have different memory subsystem requirements. CPUs are sensitive to memory latency and want a lot of DRAM capacity. GPUs require high memory bandwidth, but are less latency sensitive. A memory setup that excels in all of those areas will be very costly. GH200 avoids trying to square the circle, and its H100 GPU comes with 96 GB of dedicated HBM3 memory. That’s good for 4 TB/s of theoretical bandwidth, far more than what the LPDDR5X setup can provide.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30418"><img loading="lazy" decoding="async" width="688" height="386" data-attachment-id="30418" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/kbl_vega_m_hotchips/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?fit=954%2C535&amp;ssl=1" data-orig-size="954,535" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="kbl_vega_m_hotchips" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?fit=954%2C535&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?fit=688%2C386&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?resize=688%2C386&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?w=954&amp;ssl=1 954w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?resize=768%2C431&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>From Intel’s Hot Chips presentation</figcaption></figure></div>
<p>Conceptually, GH200’s design is similar to Intel’s Kaby Lake CPU with AMD’s Radeon RX Vega M graphics. That design also packages the CPU and GPU together as one unit. A 4 GB pool of HBM2 memory gives the GPU high memory bandwidth, while regular DDR4 memory gives the CPU high memory capacity and low latency. GH200 of course does this on a much larger scale on both the CPU and GPU side.</p>

<p>But an integrated GPU design has benefits too, mainly allowing for faster communication between the CPU and GPU. Games don’t require much bandwidth between the CPU and GPU, as long as there’s enough VRAM to handle the game in question. But compute applications are different, and can involve frequent data exchange between the CPU and GPU. Therefore, Nvidia connects the two dies with a high bandwidth proprietary interconnect called NVLink C2C. NVLink C2C offers 900 GB/s of cross-die bandwidth, or 450 GB/s in each direction. That’s an order of magnitude faster than a PCIe Gen 5 x16 link. </p>
<p>Besides higher bandwidth, NVLink C2C has hardware coherency support. The CPU can access HBM3 memory without explicitly copying it to LPDDR5X first, and the underlying hardware can ensure correct memory ordering without special barriers. Nvidia is confident enough in their NVLink C2C implementation that HBM3 memory is directly exposed to the CPU side as a NUMA node.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30500"><img loading="lazy" decoding="async" width="688" height="505" data-attachment-id="30500" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_numa_bw-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?fit=789%2C579&amp;ssl=1" data-orig-size="789,579" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_numa_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?fit=789%2C579&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?fit=688%2C505&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?resize=688%2C505&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?w=789&amp;ssl=1 789w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?resize=768%2C564&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Note that remote bandwidth for Grace Hopper is for CPU-owned memory allocated through standard Linux interfaces, which uses the CPU cores for all data transfer and does not make use of CUDA Copy Engines or other acceleration</figcaption></figure></div>
<p>Accessing the HBM3 memory pool across NVLink C2C provides comparable bandwidth to AMD’s current generation Zen 4 servers in a dual socket configuration. It’s a good performance, if a bit short compared to the theoretical figures. Bandwidth is still significantly higher than what AWS can achieve between two Graviton 4 chips, and shows the value of Nvidia’s proprietary interconnect. Bandwidth from Grace’s local LPDDR5X pool is also solid, and on par with AMD’s Bergamo with DDR5-4800.</p>
<p>Latency however is poor at nearly 800 ns, even when using 2 MB pages to minimize address translation penalties. That’s a difference of 592 ns compared to accessing directly attached LPDDR5X, which itself doesn’t offer particularly good latency.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30429"><img loading="lazy" decoding="async" width="688" height="520" data-attachment-id="30429" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_numa_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?fit=862%2C652&amp;ssl=1" data-orig-size="862,652" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_numa_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?fit=862%2C652&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?fit=688%2C520&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?resize=688%2C520&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?w=862&amp;ssl=1 862w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?resize=768%2C581&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Part of this is undoubtedly because HBM isn’t a technology designed to offer good latency characteristics. But testing from the H100 GPU shows about 300 ns of DRAM latency, suggesting HBM3 latency is only a minor factor. NVLink C2C therefore appears to have much higher latency than AMD’s Infinity Fabric, or whatever Graviton 4 is using. Intel’s QPI also offers better latency.</p>
<p>To make things worse, the system became unresponsive during that latency test run. The first signs of trouble appeared when vi, a simple text editor, took more than several seconds to load. Even weak systems like a Cortex A73 SBC usually load vi instantly. Then, the system stopped responding to all keystrokes over SSH. When I tried to establish another SSH session, it probably got past the TCP handshake stage because it didn’t time out. But the shell never loaded, and the system remained unusable. I eventually managed to recover it by initiating a reboot through the cloud provider, but that sort of behavior is non-ideal.</p>
<p>Since GH200 is a discrete GPU, it’s insightful to compare link latency against other discrete GPU setups. Here, I’m using Nemes’s Vulkan uplink latency test, which uses <code>vkMapMemory</code> to map a portion of GPU VRAM into the test program’s address space. Latency is then measured using pointer chasing accesses, just like above.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30434"><img loading="lazy" decoding="async" width="688" height="328" data-attachment-id="30434" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_vs_pcie_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?fit=802%2C382&amp;ssl=1" data-orig-size="802,382" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_vs_pcie_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?fit=802%2C382&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?fit=688%2C328&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?resize=688%2C328&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?w=802&amp;ssl=1 802w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?resize=768%2C366&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?resize=800%2C382&amp;ssl=1 800w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>This comparison is more reasonable, and NVLink C2C offers better latency than some discrete GPU configurations. It lands right between setups with AMD’s RX 5700 XT and HD 7950. Latency with that in mind is quite reasonable. However, CPU code will need to be careful about treating HBM3 memory simply as another NUMA node because of its high latency.</p>
<h2>Grace’s Neoverse V2 Implementation</h2>
<p>A CPU core architecture’s performance can vary depending on implementation. Zen 4 for example behaves very differently depending on whether it’s in a server, desktop, or mobile CPU. Neoverse V2’s situation is no different, and can vary even more because Arm wants to give implementers as much flexibility as possible.</p>
<figure><table><tbody><tr><td></td><td>Nvidia Grace</td><td>Amazon Graviton 4</td><td>Arm Neoverse V2 Emulation Environment</td></tr><tr><td>Clock Speed</td><td>3.44 GHz<br>3.1 GHz base<br>3.0 GHz all-core SIMD</td><td>2.7-2.8 GHz</td><td>3 GHz</td></tr><tr><td>Core Count</td><td>72</td><td>96</td><td>32</td></tr><tr><td>L2 Cache Capacity</td><td>1 MB</td><td>2 MB</td><td>2 MB</td></tr><tr><td>Interconnect</td><td>Nvidia SCF</td><td>Arm CMN-700</td><td>ARM CMN-700</td></tr><tr><td>L3 Cache</td><td>114 MB</td><td>36 MB</td><td>32 MB</td></tr><tr><td>Main Memory</td><td>480 GB LPDDR5X-6400, 480-bit bus</td><td>768 GB DDR5-5200, 768-bit bus</td><td>DDR5-5600, 128-bit bus</td></tr></tbody></table></figure>
<p>Grace targets parallel compute applications. To that end, Nvidia opted for a large shared L3 cache and higher clock speeds. Less parallel parts of a workload can benefit from a flexible boost policy, giving individual threads more performance when power and thermal conditions allow. A large L3 might handle better when threads from the same process share data.</p>
<p>Graviton 4 on the other hand has to server a lot of customers while maintaining consistent performance. Neoverse V2 cores on Graviton 4 get a larger L2, helping reduce noisy neighbor effects. Low clock speeds minimize workload-dependent thermal or power throttling, Finally, a higher core count lets Amazon fit more of their smallest instances on a single server. A latency test shows the memory hierarchy differences well.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30439"><img loading="lazy" decoding="async" width="688" height="343" data-attachment-id="30439" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_latency_cycles/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?fit=1054%2C525&amp;ssl=1" data-orig-size="1054,525" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_latency_cycles" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?fit=1054%2C525&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?fit=688%2C343&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?resize=688%2C343&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?w=1054&amp;ssl=1 1054w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?resize=768%2C383&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Latency in cycles is identical up to L2, because that’s part of the Neoverse V2 core design. Differences start to appear at L3, and do so in dramatic fashion. Large mesh interconnects tend to suffer high latency, and high capacity caches tend to come with a latency cost too. L3 load-to-use latency is north of 125 cycles on Grace. With such a high L2 miss cost, I would have preferred to see 2 MB of L2 cache. Graviton 4 and Intel’s Sapphire Rapids both use 2 MB of L2 cache to counter L3 latency. AMD’s Zen 4 does have a 1 MB L2, but has much lower L2 miss costs.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30442"><img loading="lazy" decoding="async" width="688" height="343" data-attachment-id="30442" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_latency_ns/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?fit=1054%2C525&amp;ssl=1" data-orig-size="1054,525" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_latency_ns" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?fit=1054%2C525&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?fit=688%2C343&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?resize=688%2C343&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?w=1054&amp;ssl=1 1054w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?resize=768%2C383&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Higher clock speeds do hand Grace an advantage over Graviton 4 when accesses hit L1 or L2. But L3 latency is still sky-high at over 38 ns. Even <a href="https://chipsandcheese.com/2023/03/12/a-peek-at-sapphire-rapids/">Intel’s Sapphire Rapids</a>, which also accesses a giant L3 over a giant mesh, does slightly better with 33 ns of L3 latency.</p>
<p>L3 cache misses head to Grace’s LPDDR5X controllers. Latency at that point is over 200 ns. Graviton 4’s DDR5 is better at 114.08 ns, putting it in the same ballpark as other server CPUs.</p>
<h3>Bandwidth</h3>
<p>Higher clocks mean higher bandwidth, so a Neoverse V2 core in Grace is comfortably ahead of its counterpart in Graviton 4. Cache bandwidth isn’t quite as high as AMD’s, which can be a disadvantage because Nvidia positions Grace as a CPU for highly parallel workloads. Such workloads are likely to be vectorized, and Zen 4 is very well optimized for those cases. Even when both are pulling data from large L3 caches, a Zen 4 core has more bandwidth on tap.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30446"><img loading="lazy" decoding="async" width="688" height="296" data-attachment-id="30446" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_st_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?fit=1258%2C541&amp;ssl=1" data-orig-size="1258,541" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_st_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?fit=1258%2C541&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?fit=688%2C296&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?resize=688%2C296&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?w=1258&amp;ssl=1 1258w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?resize=768%2C330&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?resize=1200%2C516&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>To Nvidia’s credit, a single Grace core can pull more bandwidth from L3 than a Graviton 4 core can. This test uses a prefetcher-friendly linear access pattern. I suspect Grace has a very aggressive prefetcher willing to queue up a ton of outstanding requests from a single core. Single core bandwidth is usually <a href="https://en.wikipedia.org/wiki/Little%27s_law">latency limited</a>, and a L2 prefetcher can create more in-flight requests even when the core’s out-of-order execution engine reaches its reordering limits. But even the prefetcher can only go so far, and cannot cope with LPDDR5X latency. DRAM bandwidth from a single core is only 21 GB/s compared to Graviton 4’s 28 GB/s.</p>
<p>When all cores are loaded, Grace can achieve a cool 10.7 TB/s of L1 bandwidth. L2 bandwidth is around 5 TB/s. Both figures are lower than Graviton 4’s, which makes up for lower clock speeds by having more cores. AMD’s Genoa-X has the best of both worlds, with high per-cycle cache bandwidth, higher clock speeds, and 96 cores.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30449"><img loading="lazy" decoding="async" width="688" height="312" data-attachment-id="30449" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_mt_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?fit=1346%2C611&amp;ssl=1" data-orig-size="1346,611" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_mt_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?fit=1346%2C611&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?fit=688%2C312&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?resize=688%2C312&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?w=1346&amp;ssl=1 1346w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?resize=768%2C349&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?resize=1200%2C545&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?resize=1320%2C599&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>L3 bandwidth is hard to see from this test because Grace and Graviton 4 have a lot of L2 capacity compared to L3. I usually split the test array across threads because testing with a shared array tends to overestimate DRAM bandwidth. Requests from different cores to the same cacheline may get combined at some level. But testing with a shared array does help to estimate Graviton 4 and Grace’s L3 bandwidth.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30451"><img loading="lazy" decoding="async" width="688" height="312" data-attachment-id="30451" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_shared_arr_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?fit=1346%2C611&amp;ssl=1" data-orig-size="1346,611" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_shared_arr_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?fit=1346%2C611&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?fit=688%2C312&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?resize=688%2C312&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?w=1346&amp;ssl=1 1346w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?resize=768%2C349&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?resize=1200%2C545&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?resize=1320%2C599&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Grace has over 2 TB/s of L3 bandwidth, putting it ahead of Graviton 4’s 750 GB/s. Nvidia wants Grace to serve bandwidth hungry parallel compute applications, and having that much L3 bandwidth on tap is a good thing. But AMD is still ahead. Genoa-X dodges the problem of servicing all cores from a unified cache. Instead, each octa-core cluster gets its own L3 instance. That keeps data closer to the cores, giving better L3 bandwidth scaling and lower latency. The downside is Genoa-X has more than 1 GB of last level cache, and a single core only allocates into 96 MB of it.</p>
<h3>Some Light Benchmarking</h3>
<p>In-depth benchmarking is best left to mainstream tech news sites with deeper budgets and full time employees. But I did dig briefly into Grace’s performance.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30455"><img loading="lazy" decoding="async" width="688" height="370" data-attachment-id="30455" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_libx264/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264.png?fit=704%2C379&amp;ssl=1" data-orig-size="704,379" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_libx264" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264.png?fit=704%2C379&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264.png?fit=688%2C370&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264.png?resize=688%2C370&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>libx264 uses plenty of vector instructions, and can demand a lot of bandwidth. It’s the kind of thing I’d expect Grace to do well at, especially with the test locked to matching core counts. But despite clocking higher, Grace’s Neoverse V2 cores fail to beat Graviton 4’s.</p>
<p>7-Zip is a file compression program that only uses scalar integer instructions. The situation is no better there, and I ran the test several times despite the clock running on cloud instance time.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30458"><img loading="lazy" decoding="async" width="688" height="377" data-attachment-id="30458" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_7z/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z.png?fit=702%2C385&amp;ssl=1" data-orig-size="702,385" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_7z" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z.png?fit=702%2C385&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z.png?fit=688%2C377&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z.png?resize=688%2C377&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Despite using the same command line parameters, 7-Zip wound up executing 2.58 trillion instructions to finish compressing the test file on GH200. On Graviton 4, the same work took a mere 1.86 trillion instructions. libx264’s instruction counts were similar on both Neoverse V2 implementations, at approximately 19.8 trillion instructions. That makes the 7-Zip situation a bit suspect, so I’ll focus on libx264.</p>
<blockquote>
<p>…counts cycles in which the core is unable to dispatch instructions from the front end to the back end due to a back end stall caused by a miss in the last level of cache within the core clock domain</p>
<p>Arm Neoverse V2 Technical Reference Manual</p>
</blockquote>
<p>Neoverse V2 has a STALL_BACKEND_MEM performance monitoring event. The description for this event is clear if a bit wordy. Let’s unpack it. L2 is the last level of cache that runs at core clock. Therefore, STALL_BACKEND_MEM only considers stalls caused by L3 and DRAM latency. Dispatching instructions from the frontend to the backend is what the renamer does, and we know the renamer is the narrowest part of Neoverse V2’s pipeline. Therefore, the event is counting L2 miss latency that the out-of-order engine couldn’t absorb. And, throughput lost from those events can’t be recovered by racing ahead later elsewhere in the pipeline.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30463"><img loading="lazy" decoding="async" width="686" height="370" data-attachment-id="30463" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_libx264_be_bound/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264_be_bound.png?fit=686%2C370&amp;ssl=1" data-orig-size="686,370" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_libx264_be_bound" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264_be_bound.png?fit=686%2C370&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264_be_bound.png?fit=686%2C370&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264_be_bound.png?resize=686%2C370&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>libx264 sees a massive increase in those stalls. Grace’s smaller L2 combined with worse L3 and DRAM latency isn’t a winning combination. Overall lost throughput measured at the rename stage only increased by a few percent. It’s a good demonstration of how Neoverse V2’s large backend can cope with extra latency. But it can’t cope hard enough, nullifying Grace’s clock speed advantage.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30466"><img loading="lazy" decoding="async" width="684" height="370" data-attachment-id="30466" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_7z_be_bound/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z_be_bound.png?fit=684%2C370&amp;ssl=1" data-orig-size="684,370" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_7z_be_bound" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z_be_bound.png?fit=684%2C370&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z_be_bound.png?fit=684%2C370&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z_be_bound.png?resize=684%2C370&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>The same counters in 7-Zip don’t show such a huge discrepancy, though Grace again suffers more from L2 miss latency. Grace’s poor showing in this workload is largely due to 7-Zip somehow executing more instructions to do the same work.</p>
<p>7-Zip and libx264 don’t benefit from Nvidia’s implementation choices, but that doesn’t mean Grace’s design is without merit. The large 114 MB L3 cache looks great for cache blocking techniques, and higher clocks can help speed up less parallel parts of a program. Some throughput bound programs may have prefetcher-friendly sections, which can be aided by Grace’s prefetcher. Specific workloads may do better on Grace than on Graviton 4, particularly if they receive optimizations to fit Grace’s memory subsystem. But that’s beyond the scope of this brief article.</p>
<h2>H100 On-Package GPU</h2>
<p>The GPU on GH200 is similar to the H100 SXM variant, since 132 Streaming Multiprocessors (SMs) are enabled out of 144 on the die. VRAM capacity is 96 GB compared to the 80 GB on separately sold H100 cards, indicating that all 12 HBM controllers are enabled. Each HBM controller has a 512-bit interface, so the GH200’s GPU has a 6144-bit memory bus. Even though GH200’s GPU is connected using a higher bandwidth NVLink C2C interface, it’s exposed to software as a regular PCIe device. </p>
<p><code>nvidia-smi</code> indicates GH200 has a 900W power limit. For comparison, H100’s SXM variant has a 700W power limit, while the H100 PCIe makes do with 350-400W. GH200 obviously has to share power between the CPU and GPU, but the GPU may have more room to breathe than its discrete counterparts when CPU load is low.</p>
<p>Compared to the PCIe version of the H100, GH200’s H100 runs at higher clocks, reducing cache latency. Otherwise, the H100 here looks a lot like other H100 variants. There’s large L1 cache backed by a medium capacity L2. H100 doesn’t have a gigantic last level cache like RDNA 2, CDNA 3, or Nvidia’s own Ada Lovelace client architecture. But it’s not a tiny cache either like on Ampere or other older GPUs.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30468"><img loading="lazy" decoding="async" width="688" height="321" data-attachment-id="30468" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_vk_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?fit=1201%2C561&amp;ssl=1" data-orig-size="1201,561" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_vk_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?fit=1201%2C561&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?fit=688%2C321&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?resize=688%2C321&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?w=1201&amp;ssl=1 1201w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?resize=768%2C359&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>VRAM latency sees a very substantial improvement, going down from 330 ns to under 300. It’s impossible to tell how much of this comes from higher clock speeds reducing time taken to traverse H100’s on-chip network, and how much comes from HBM3 offering better latency.</p>
<p>Bandwidth also goes up, thanks to more enabled SMs and higher clock speeds. Unfortunately, the test couldn’t get past 384 MB. That makes VRAM bandwidth difficult to determine. If things worked though, I assume GH200 would have higher GPU memory bandwidth than discrete H100 cards.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30472"><img loading="lazy" decoding="async" width="688" height="320" data-attachment-id="30472" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_vk_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?fit=1196%2C556&amp;ssl=1" data-orig-size="1196,556" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_vk_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?fit=1196%2C556&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?fit=688%2C320&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?resize=688%2C320&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?w=1196&amp;ssl=1 1196w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?resize=768%2C357&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Further tests would have been interesting. I wanted to test CPU to GPU bandwidth using the GPU’s copy engine. DMA engines can queue up memory accesses independently of CPU (or GPU) cores, and are generally more latency tolerant. Nemes does have a test that uses <code>vkCmdCopyBuffer</code> to test exactly that. Unfortunately, that test hung and never completed.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30489"><img loading="lazy" decoding="async" width="688" height="138" data-attachment-id="30489" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_pcie_errors/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?fit=1358%2C272&amp;ssl=1" data-orig-size="1358,272" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_pcie_errors" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?fit=1358%2C272&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?fit=688%2C138&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?resize=688%2C138&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?w=1358&amp;ssl=1 1358w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?resize=768%2C154&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?resize=1200%2C240&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?resize=1320%2C264&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Checking <code>dmesg</code> showed the kernel complaining about PCIe errors and graphics exceptions. I tried looking up some of those messages in Linux source code, but couldn’t find anything. They probably come from a closed source Nvidia kernel module. Overall, I had a frustrating experience exercising NVLink C2C. At least the Vulkan test didn’t hang the system, unlike running a plain memory latency test targeting the HBM3 memory pool. I also couldn’t use any OpenCL tests. <code>clinfo</code> could detect the GPU, but <code>clpeak</code> or any other application was unable to create an OpenCL context. I didn’t have the same frustrating experience with H100 PCIe cloud instances, where the GPU pretty much behaved as expected with Vulkan or OpenCL code. It’s a good reminder that designing and validating a custom platform like GH200 can be an incredibly difficult task.</p>
<h2>Final Words</h2>
<p>Nvidia’s GH200 and Grace CPU is an interesting Neoverse V2 implementation. With fewer cores and a higher power budget, Grace can clock higher than Graviton 4. But rather than providing better per-core performance as specifications might suggest, Grace is likely optimized for specific applications. General consumer workloads may not be the best fit, even well vectorized ones.</p>
<p>Previously I thought Arm’s Neoverse V2 had an advantage over Zen 4, because Arm can focus on a narrower range of power and performance targets. But after looking at Grace, I don’t think that captures the full picture. Rather, Arm faces a different set of challenges thanks to their business model. They don’t see chip designs through to completion like AMD and Intel. Those x86 vendors can design cores with a comparatively narrow set of platform characteristics in mind. Arm has to attract as many implementers as possible to get licensing revenue. Their engineers will have a harder time anticipating what the final platform looks like. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30486"><img loading="lazy" decoding="async" width="688" height="383" data-attachment-id="30486" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/arm_v2_hotchips_reference_platform/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?fit=1276%2C711&amp;ssl=1" data-orig-size="1276,711" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="arm_v2_hotchips_reference_platform" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?fit=1276%2C711&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?fit=688%2C383&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?resize=688%2C383&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?w=1276&amp;ssl=1 1276w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?resize=768%2C428&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?resize=1200%2C669&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Arm evaluated Neoverse V2 in an emulation environment that drastically differs from Grace and Graviton 4. Slide from Arm’s Hot Chips 2023 presentation</figcaption></figure></div>
<p>So, Neoverse V2 can find itself having to perform in an environment that doesn’t play nice with its core architecture. Nvidia’s selection of a 1 MB L2, high latency L3, and very high latency LPDDR5X present Neoverse V2 with a spicy challenge. As covered in the <a href="https://chipsandcheese.com/2024/07/22/arms-neoverse-v2-in-awss-graviton-4/">Graviton 4</a> article, Neoverse V2 has similar reordering capacity to Zen 4. I think Zen 4 would also trip over itself with 125 cycles of L3 latency and over 200 ns of memory latency. I don’t think it’s a coincidence that every Zen 4 implementation has a fast L3. Intel is another example, Golden Cove can see 11.8 ns of L3 latency in a Core i7-12700K, or 33.3 ns in a Xeon Platinum 8480+. Golden Cove has much higher reordering capacity, making it more latency tolerant. In a server environment, Golden Cove gets a 2 MB L2 cache as well.</p>
<p>GH200’s GPU implementation deserves comment too. It should be the most powerful H100 variant on the market, with a fully enabled memory bus and higher power limits. NVLink C2C should provide higher bandwidth CPU to GPU communication than conventional PCIe setups too.</p>
<p>But it’s not perfect. NVLink C2C’s theoretical 450 GB/s is difficult to utilize because of high latency. Link errors and system hangs are a concerning problem, and point to the difficulty of validating a custom interconnect. Exposing VRAM to software as a simple NUMA node is a good north star goal, because it makes VRAM access very easy and transparent from a software point of view. But with current technology, it might be a bridge too far.</p>

<p>Even though it’s not an iGPU, Grace Hopper might be Nvidia’s strongest shot at competing with AMD’s iGPU prowess. Nvidia has already scored a win with <a href="https://nvidianews.nvidia.com/news/aws-nvidia-strategic-collaboration-for-generative-ai">Amazon</a> and the <a href="https://www.bristol.ac.uk/news/2023/november/supercomputer-announcement.html">UK’s Isambard-AI supercomputer</a>. AMD’s MI300A is shaping up to be tough competition, with a win in Lawrence Livermore National Laboratory’s upcoming <a href="https://asc.llnl.gov/exascale/el-capitan">El Capitan</a> supercomputer. MI300A uses an integrated GPU setup, which speeds up CPU to GPU communication. However, it limits memory capacity to 128 GB, a compromise that Nvidia’s discrete GPU setup doesn’t need to make. It’s good to see Nvidia and AMD competing so fiercely in the CPU/GPU integration space, and it should be exciting to see how things play out.</p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">
<p><span>
<ul>
<li>
<div>
<p><img alt="clamchowder" src="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80"> </p>
</div>

</li>
</ul>
</span>
</p></div>





</div></div>]]></description>
        </item>
    </channel>
</rss>