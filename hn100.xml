<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 23 Jan 2026 10:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Proton Spam and the AI Consent Problem (190 pts)]]></title>
            <link>https://dbushell.com/2026/01/22/proton-spam/</link>
            <guid>46729368</guid>
            <pubDate>Fri, 23 Jan 2026 07:01:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dbushell.com/2026/01/22/proton-spam/">https://dbushell.com/2026/01/22/proton-spam/</a>, See on <a href="https://news.ycombinator.com/item?id=46729368">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p data-speech-synth="none">

<time datetime="2026-01-22T10:00:00.000Z">

Thursday

22 <abbr title="January">Jan</abbr>
2026
</time>




</p>


<p>On Jan 14th <a href="https://proton.me/" rel="noopener noreferrer" target="_blank">Proton</a> sent out an email newsletter with the subject line:</p><blockquote><p>Introducing Projects - Try Lumo’s powerful new feature now</p></blockquote><figure><img src="https://dbushell.com/images/blog/2026/lumo-spam.avif" alt="screenshot of the official email from @lumo.proton.me" width="515" height="367" decoding="async" fetchpriority="low" loading="lazy" id="--img-f8a652e2"><p popover="" id="--alt-f8a652e2" data-anchor="--img-f8a652e2">screenshot of the official email from @lumo.proton.me</p></figure><p>Lumo is Proton’s <glossary-term id="--term-ai"><a href="https://pivot-to-ai.com/" rel="noopener noreferrer" target="_blank">“AI”</a></glossary-term> offering.</p><p>There is a problem with this email. And I’m not talking about the question of how exactly AI aligns with Proton’s core values of <em>privacy</em> and <em>security</em>.</p><p>The problem is I had already <strong>explicitly</strong> opted out of Lumo emails.</p><figure><img src="https://dbushell.com/images/blog/2026/lumo-opt-out.avif" alt="screenshot of an unchecked toggle" width="235" height="55" decoding="async" fetchpriority="low" loading="lazy" id="--img-d9cc8beb"><p popover="" id="--alt-d9cc8beb" data-anchor="--img-d9cc8beb">screenshot of an unchecked toggle</p></figure><p>That toggle for “Lumo product updates” is unchecked. Lumo is the only topic I’m not subscribed to. Proton has over a dozen newsletters, including some crypto nonsense. I opt-in to everything but Lumo, I gave an undeniable <strong>no</strong> to Lumo emails.</p><p>So the email I received from Proton is <strong>spam</strong>, right?</p><p>My understanding is that spam is a violation of GDPR and UK data protection laws. Regardless, Proton’s email is a clear abuse of their own service towards a paying business customer.</p><p>Before I grab my pitchfork I emailed Proton support.</p><h2 id="proton-support">Proton Support</h2><p>Despite the subject line and contents, and despite the “From Lumo” name and <code>@lumo.proton.me</code> address, maybe this was an honest mistake?</p><p>Proton’s first reply explained how to opt-out.</p><figure><img src="https://dbushell.com/images/blog/2026/proton-support-1.avif" alt="screenshot of support email quoted below" width="460" height="614" decoding="async" fetchpriority="low" loading="lazy" id="--img-c8d2e805"><p popover="" id="--alt-c8d2e805" data-anchor="--img-c8d2e805">screenshot of support email quoted below</p><figcaption>I’ve blurred the name because whatever</figcaption></figure><blockquote><p>Hello David,</p><p>Thank you for contacting us.</p><p>You can unsubscribe from the newsletters if you do the following:</p><p>- Log in to your account at https://account.protonvpn.com/login</p><p>- Navigate to the Account category</p><p>- Disable the check-marks under “Email subscriptions”</p><p>- If you need additional assistance, let me know.</p><p>[screenshot of the same opt-out toggle]</p><p>-Have a nice day.</p></blockquote><p><em>John Support</em> directs me to the exact same <em>“Lumo product updates”</em> toggle I had already unchecked. I replied explaining that I had <strong>already</strong> opted out. Support replies saying they’re <em>“checking this with the team”</em> then later replies again asking for screenshots.</p><blockquote><p>Can you make sure to send me a screenshot of this newsletter option disabled, as well as the date when the last message was sent to you regarding the Lumo offer?</p><p>You can send me a screenshot of the whole message, including the date.</p><p>Is it perhaps 14 January 2026 that you received the message?</p></blockquote><p>I found that last line curious, are they dealing with other unhappy customers? Maybe I’m reading too much into it.</p><p>I sent the screenshots and signed off with <em>“Don’t try to pretend this fits into another newsletter category.”</em></p><p>After more <em>“checking this with the team”</em> I got a response today.</p><blockquote><p>In this case, the mentioned newsletter is for promoting Lumo Business Suit to Business-related plans.</p><p>Hence, why you received it, as Product Updates and Email Subscription are two different things.</p><p>In the subscription section, you will see the “Email Subscription” category, where you can disable the newsletter in order to avoid getting it in the future.</p></blockquote><p>If I understand correctly, Proton are claiming this email is the <em>“Proton for Business newsletter”</em>. Not the <em>“Lumo product updates”</em> newsletter.</p><p>I don’t know about you, but I think that’s baloney. Proton Support had five full business days to come up with a better excuse. Please tell me, how can I have been any more explicit about opting out of Lumo emails, only to receive “Try Lumo” “From Lumo”, and be told that is not actually a Lumo email?</p><h2 id="non-consent">Non-Consent</h2><p>Has anyone else noticed that the AI industry can’t take “no” for an answer? AI is being force-fed into every corner of tech. It’s unfathomable to them that some of us aren’t interested.</p><p>The entire AI industry is built upon a common principle of non-consent. They laugh in the face of IP and copyright law. AI bots <a href="https://adactio.com/journal/21831" rel="noopener noreferrer" target="_blank">DDoS websites</a> and <a href="https://rknight.me/blog/perplexity-ai-is-lying-about-its-user-agent/" rel="noopener noreferrer" target="_blank">lie about user-agents</a>. Can it get worse than the <a href="https://www.bbc.co.uk/news/articles/cwy875j28k0o" rel="noopener noreferrer" target="_blank">sickening actions of Grok?</a> I dread to think.</p><p>As Proton has demonstrated above, and <a href="https://dbushell.com/notes/2026-01-12T07:49Z/">Mozilla/Firefox</a> recently too, the AI industry simply will not accept “no” as an answer. Some examples like spam are more trivial than others, but the growing trend is vile and disturbing.</p><p>I do not want your AI.</p><h3 id="update-for-23rd-january">Update for 23rd January</h3><p>I guess someone at Microsoft read my post and said “hold my beer”. This morning I woke up to a lovely gift in my inbox; <em>“Build Al agents with the new GitHub Copilot SDK”</em>.</p><figure><img src="https://dbushell.com/images/blog/2026/github-spam.avif" alt="GitHub email subject line: 'Build Al agents with the new GitHub Copilot SDK'" width="474" height="68" decoding="async" fetchpriority="low" loading="lazy" id="--img-b61aec8e"><p popover="" id="--alt-b61aec8e" data-anchor="--img-b61aec8e">GitHub email subject line: 'Build Al agents with the new GitHub Copilot SDK'</p></figure><p><a href="https://dbushell.com/2025/08/11/github-ensloppification/">GitHub Ensloppification</a> is moving faster than I can delete my account for good. (It’s an unfortunate requirement for client projects.) For the record, I have <em>never</em> said “yes” to any GitHub newsletter. Even before Copilot I disabled every possible GitHub email notification.</p><p>The “Unsubscribe” link provides the hidden newsletter list. There is nothing within GitHub account settings I can find to disable spam.</p><figure><img src="https://dbushell.com/images/blog/2026/github-newsletters.avif" alt="GitHub 'Opt-Out Preferences' with 3 newsletters unchecked but GitHub Copilot emails checked" width="524" height="814" decoding="async" fetchpriority="low" loading="lazy" id="--img-c9841fd3"><p popover="" id="--alt-c9841fd3" data-anchor="--img-c9841fd3">GitHub 'Opt-Out Preferences' with 3 newsletters unchecked but GitHub Copilot emails checked</p></figure><p>As expected, Microsoft has opted me in without my consent. The wheels are falling off at GitHub. The brutally slow front-end UI. The embarrassingly lacklustre <em>Actions</em> CI. Now this sloppy tripe everywhere. Reminder to developers: <strong>GitHub is not Git.</strong></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I built a light that reacts to radio waves [video] (160 pts)]]></title>
            <link>https://www.youtube.com/watch?v=moBCOEiqiPs</link>
            <guid>46728808</guid>
            <pubDate>Fri, 23 Jan 2026 05:34:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=moBCOEiqiPs">https://www.youtube.com/watch?v=moBCOEiqiPs</a>, See on <a href="https://news.ycombinator.com/item?id=46728808">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Talking to LLMs has improved my thinking (121 pts)]]></title>
            <link>https://philipotoole.com/why-talking-to-llms-has-improved-my-thinking/</link>
            <guid>46728197</guid>
            <pubDate>Fri, 23 Jan 2026 03:52:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://philipotoole.com/why-talking-to-llms-has-improved-my-thinking/">https://philipotoole.com/why-talking-to-llms-has-improved-my-thinking/</a>, See on <a href="https://news.ycombinator.com/item?id=46728197">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto" data-message-author-role="assistant" data-message-id="88fb0f9f-17c0-4b02-9e59-71e7a735549b" data-message-model-slug="gpt-5-2">
<p data-start="334" data-end="419"><img decoding="async" src="https://philipotoole.com/wp-content/uploads/2015/02/Searching-512-150x150.png" alt="" width="150" height="150" srcset="https://philipotoole.com/wp-content/uploads/2015/02/Searching-512-150x150.png 150w, https://philipotoole.com/wp-content/uploads/2015/02/Searching-512-300x300.png 300w, https://philipotoole.com/wp-content/uploads/2015/02/Searching-512.png 512w" sizes="(max-width: 150px) 100vw, 150px">I’ve been surprised by one aspect of using large language models more than any other.</p>
<p data-start="421" data-end="634">They often put into words things I have long understood, but could not write down clearly. When that happens, it feels less like learning something new and more like recognition. A kind of “yes, that’s it” moment.</p>
<p data-start="636" data-end="775">I have not seen this effect discussed much, but I think it matters. I also think it has improved how <strong>I</strong> think.</p>

<h2 data-start="636" data-end="775">Much of what we know is tacit</h2>
<p data-start="777" data-end="860">As <a href="https://philipotoole.com/resume/">programmers and developers</a>, we build up a lot of understanding that never quite becomes explicit.</p>
<p data-start="862" data-end="1068">You know when a design is wrong before you can say why. You <a href="https://philipotoole.com/how-i-found-a-bug-in-sqlite/">sense a bug</a> before you can reproduce it. You recognize a bad abstraction instantly, even if it takes an hour to explain the problem to someone else.</p>
<p data-start="1070" data-end="1275">This is not a failure. It is how experience operates. The brain compresses experience into patterns that are efficient for action, not for speech. Those patterns are real, but they are not stored in sentences.</p>
<p data-start="1277" data-end="1466">The problem is that reflection, planning, and teaching all require language. If you cannot express an idea, you cannot easily inspect it or improve it.</p>
<h2 data-start="1277" data-end="1466">LLMs are good at the opposite problem</h2>
<p data-start="1468" data-end="1537">Large language models are built to do exactly this – turn vague structure into words.</p>
<p data-start="1539" data-end="1737">When you ask a good question and the response resonates, the model is not inventing insight. It is mapping a latent structure to language in a way that happens to align with your own internal model.</p>
<p data-start="1739" data-end="1875">That alignment is what produces the sense of recognition. I already had the shape of the idea. The model supplied a clean verbal form.</p>
<h2 data-start="1877" data-end="2043">Putting things into words changes the thought</h2>
<p data-start="2045" data-end="2105">Once an idea is written down, it becomes easier to work with.</p>
<p data-start="2107" data-end="2275">Vague intuitions turn into named distinctions. Implicit assumptions become visible. At that point you can test them, negate them, or refine them.</p>
<p data-start="2277" data-end="2471">This is not new. Writing has always done this for me. What is different is the speed. I can probe half-formed thoughts, discard bad formulations, and try again without much friction. That encourages a kind of thinking I might have otherwise skipped.</p>
<h2 data-start="2473" data-end="2559">The feedback loop matters</h2>
<p data-start="2561" data-end="2645">After you see a good articulation of an idea, you start thinking with that style of language.</p>
<p data-start="2647" data-end="2828">Over time I’ve noticed that now I do this without an LLM to hand.&nbsp; Can I phrase in precise language what I am thinking, feeling, believing, right now, and why.</p>
<p data-start="2830" data-end="3103">In that sense, the model is not improving my thinking directly. It is improving the interface between my thinking and language. Since reasoning depends heavily on what one can represent explicitly, that improvement can feel like a real increase in clarity.</p>
<p data-start="3262" data-end="3408" data-is-last-node="" data-is-only-node="">The more I do this, the better I get at noticing what I actually think.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bugs Apple Loves (641 pts)]]></title>
            <link>https://www.bugsappleloves.com</link>
            <guid>46727587</guid>
            <pubDate>Fri, 23 Jan 2026 02:24:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bugsappleloves.com">https://www.bugsappleloves.com</a>, See on <a href="https://news.ycombinator.com/item?id=46727587">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <!-- Hero Section -->
    <section>
        
        <p>Why else would they keep them around for so long?</p>
    </section>

    <!-- Total Impact Counter -->
    <div>
            <p>Total time wasted by humanity because Apple won't fix these</p>
            <p id="total-time">Calculating...</p>
            <p>and counting</p>
        </div>

    <!-- Methodology Section -->
    <div>
            <h2>The Formula</h2>
            <p>Every bug is different. But the math is always real.<br><strong>Think our numbers are wrong? Edit them yourself.</strong></p>

            <div>
                <div>
                    <h3>Base Impact</h3>
                    <p><code>Users Affected × Frequency × Time Per Incident</code></p><p>How many Apple users hit this bug, how often, and how long they suffer each time.</p>
                </div>

                <div>
                    <h3>Power User Tax</h3>
                    <p><code>Σ (Workaround Time × Participation Rate)</code></p><p>The extra time spent by people who try to fix what Apple won't.</p>
                </div>

                <div>
                    <h3>Shame Multiplier</h3>
                    <p><code>Years Unfixed × Pressure Factor</code></p><p>How long Apple has known about this and how urgent the task usually is.</p>
                </div>

                <div>
                    <h3>The Verdict</h3>
                    <p><code>Human Hours Wasted ÷ Engineering Hours to Fix</code></p><p>How many times over Apple could have fixed it with the productivity they've destroyed.</p>
                </div>
            </div>
        </div>

    <!-- Bugs Container -->
    <main id="bugs-container">
        <!-- Bugs will be rendered here by JavaScript -->
        <p>Loading Apple's bugs<span></span></p>
    </main>

    <!-- Footer -->
    

    
    


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why medieval city-builder video games are historically inaccurate (2020) (142 pts)]]></title>
            <link>https://www.leidenmedievalistsblog.nl/articles/why-medieval-city-builder-video-games-are-historically-inaccurate</link>
            <guid>46726857</guid>
            <pubDate>Fri, 23 Jan 2026 00:22:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.leidenmedievalistsblog.nl/articles/why-medieval-city-builder-video-games-are-historically-inaccurate">https://www.leidenmedievalistsblog.nl/articles/why-medieval-city-builder-video-games-are-historically-inaccurate</a>, See on <a href="https://news.ycombinator.com/item?id=46726857">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>This blog post explores the historical accuracy of medieval city-builder video games.</p><div>
            <h3><strong>Introduction</strong></h3><p>Since many of us are working from home in these trying times, it seems safe to assume that more people than ever are indulging in playing the occasional computer game. A city builder is a specific kind of computer game in which you design a city, extract resources, set up production chains and ensure that your settlement grows. City builders are very similar to strategy games as they reward patience and strategy. In this article, I will take a look at one sub-genre of the city builder, the medieval city builder, and explain how this gaming genre relates to our knowledge of medieval settlement planning.</p><h3><strong>Historical city builders</strong></h3><p>The city builder has its origins far back in the 1990s in the combination of the strategy genre and the management genre, leading to games such as <em>Sim City</em> (1989), <em>Caesar </em>(1992) and <em>Age of Empires</em> (1997).</p>
                        						<figure id="10615">
									<img src="https://www.leidenmedievalistsblog.nl/images/uploads/_fulllandscape/PAK.GameBlog1.png" alt="PAK Game Blog1">
												   <figcaption>Screenshot: Caesar (1992)</figcaption>
							</figure>
	   						
                  <p>It did not take long before medieval-themed city builders popped up. We may think of <em>Settlers</em> (1993) and <em>Knights and Merchants</em> (1998). In addition, the <em>Anno</em> games (1998-2019), although initially set in the 1600s basically had a medieval theme.</p>
                        						<figure id="10615">
									<img src="https://www.leidenmedievalistsblog.nl/images/uploads/_fulllandscape/PAK.GameBlog2.png" alt="PAK Game Blog2">
												   <figcaption>Screenshot: Knights and Merchants (1998)</figcaption>
							</figure>
	   						
                  <p>These games often start with plopping down a village center on a promising location near abundant resources. You then continue to gather these resources which grant you building materials for building new homes and facilities for your settlement.</p>
                        						<figure id="10615">
									<img src="https://www.leidenmedievalistsblog.nl/images/uploads/_fulllandscape/PAK.GameBlog3.png" alt="PAK Game Blog3">
												   <figcaption>Screenshot: Settlers (1993)</figcaption>
							</figure>
	   						
                  <p>Setting up specialized production chains might involve growing grain, milling the grain for flour and turning the flour into bread which feeds your villages. Similarly, another production chain might involve rearing sheep for their wool, turning the wool into cloth and turning the cloth into clothing. When done correctly, the reward of correct investments and planning is that you see your settlement grow.</p>
                        						<figure id="10615">
									<img src="https://www.leidenmedievalistsblog.nl/images/uploads/_fullportrait/PAK.GameBlog4.png" alt="PAK Game Blog4">
												   <figcaption>Screenshot: Anno 1404 (2009)</figcaption>
							</figure>
	   						
                  <p>This often leads to settlements growing organically from a couple of houses around a community center to a larger settlement with hundreds of people. However logical such an organic growth of a settlement might seem, it is not historically accurate.</p>
                        						<figure id="10615">
									<img src="https://www.leidenmedievalistsblog.nl/images/uploads/_fulllandscape/PAK.GameBlog5.png" alt="PAK Game Blog5">
												   <figcaption>Screenshot: Foundation (2019)</figcaption>
							</figure>
	   						
                  <h3><strong>Medieval village life <br></strong></h3><div><p>Any gameplay loop that tells a story of linear settlement growth is incongruent with how a medieval economy worked (see Foussier 2004). Medieval villagers were often living on the edge of subsistence. Agricultural surpluses were skimmed by the church and the feudal lords. Bad harvests, banditry, warfare and disease might decimate a village community at any time. For this very reason, the demography of many European villages remained relatively stable between the twelfth and the eighteenth century. It may therefore be clear that the gameplay loop of city builders pivots around the concept of doing the historically exceptional (i.e. growing a settlement to a town) and thereby strays far from what actually happened in the lives of our medieval forebears. </p><p>A notable exception to this genre trope is the game <em>Banished </em>(2014) in which high mortality rates and bad weather do seriously stifle any kind of linear growth. In this city builder you are constantly fighting the odds and settlement growth is not guaranteed. However, also in Banished it is your goal to overcome the stagnation and lead your settlement to expansion.</p></div>
                        						<figure id="10615">
									<img src="https://www.leidenmedievalistsblog.nl/images/uploads/_rightlandscape/PAK.GameBlog6.png" alt="PAK Game Blog6">
												   <figcaption>Screenshot: Banished (2014)</figcaption>
							</figure>
	   						
                  <p>A thing that is rarely touched upon in medieval city builders is how complex village life actually was. This can be exemplified by how the community related to its overlords. Land ownership here is key. Land in the community might be owned by a lord, a local liegeman, a monastery or even directly by the duke or count. Taxes, rents and tithes were the organisational structures in which the landowner was tied to the farmers who worked the fields. Often the payment of taxes and tithes was linked to feast days and the visit of the tax collector represented a big event in the agricultural year. An interesting side note is that some obligations which the commoners had to the lord and the church (such as seigneurial duties like working a mill) might drain the community from the needed manpower for tilling the land. Furthermore, a rural community that was its own seigneury had access to a law court with sheriff, aldermen and a local militia (Middle Dutch <em>schutterie</em>) to fight off bandits with. Harsh capital punishments were set in place to deter anyone from raiding the farms and hamlets and the village gallows were often the first thing one saw when approaching a medieval settlement. </p><h3><strong>Planning a medieval settlement</strong></h3><p>But something that is much more fundamental to the theme of a settlement building game, is how medieval settlements were actually planned and grew. Landscape historians and archaeologists have acquired a lot of insight into how this worked.</p><p>Let's start with the realization that medieval settlements in their first stages of development were planned and laid out according to a specific design. In my own research into the settlement history of West-Brabant (southern Netherlands, from 1000 to 1300 CE) I have encountered the following types.</p><p>Here is a sketch of a Brabantine circular manor (Middle Dutch <em>vroonhoeve</em>). This is a reinforced circular homestead with moat, often next to a bend in the river, containing several farms and a fan-like plot pattern radiating out from it. Such manors were often called BORCH.</p>
                        						<figure id="10615">
									<a href="#PAK Game Blog7"><img src="https://www.leidenmedievalistsblog.nl/images/uploads/_fulllandscape/PAK.GameBlog7.png" alt="PAK Game Blog7"></a>
					<a href="#8237" id="PAK Game Blog7">
						<img src="https://www.leidenmedievalistsblog.nl/images/uploads/PAK.GameBlog7.png" alt="PAK Game Blog7">
					</a>
												   <figcaption>Sketch of a Brabantine street settlement</figcaption>
							</figure>
	   						
                  <p>Here is a sketch of a Brabantine street settlement, often built with exploitation of nearby fenland in mind. It consists of a line of farms with associated evenly sized rectangular plots built in a line perpendicular to a raised road.</p>
                        						<figure id="10615">
									<a href="#PAK Game Blog8"><img src="https://www.leidenmedievalistsblog.nl/images/uploads/_fulllandscape/PAK.GameBlog8.png" alt="PAK Game Blog8"></a>
					<a href="#8238" id="PAK Game Blog8">
						<img src="https://www.leidenmedievalistsblog.nl/images/uploads/PAK.GameBlog8.png" alt="PAK Game Blog8">
					</a>
												   <figcaption>Sketch of a more complex exploitation village</figcaption>
							</figure>
	   						
                  <p>Here is a more complex exploitation village which is set up with a moated enclosed church homestead and a central meadow as its center. There is a line of farms next to the road. The arable land to the east is bordered by a ditch supplying fresh drinking water (Middle Dutch <em>bansloot</em>). In layout, this type represents a hybrid between the two earlier settlement types. </p>
                        						<figure id="10615">
									<a href="#PAK Game Blog9"><img src="https://www.leidenmedievalistsblog.nl/images/uploads/_fulllandscape/PAK.GameBlog9.png" alt="PAK Game Blog9"></a>
					<a href="#8239" id="PAK Game Blog9">
						<img src="https://www.leidenmedievalistsblog.nl/images/uploads/PAK.GameBlog9.png" alt="PAK Game Blog9">
					</a>
												   <figcaption>Sketch of a hybrid between the two earlier settlement types</figcaption>
							</figure>
	   						
                  <p>Let us first make clear that these different types of exploitation settlements often existed alongside each other and can be found in one and the same region. In part, the different types reflect different chronological layers but some types were also more suited to certain geographical environments than others.</p><p>So how were these settlements planned? Many medieval exploitation enterprises were initiated by a monastery or a consortium of free men who were granted permission by (or bought permission from) the feudal lord to “colonize” the wilderness.</p><p>Clearing the wooded landscape in order to create arable land was done by cutting away the trees and bushes (Middle Dutch <em>rode</em>) or, alternatively, burning it away in controlled fires (Middle Dutch <em>brant</em>).</p><p>Land surveyors sent by the lord would then measure out the block or strips that would be taken in cultivation. Strips of arable land were often 1250m deep (6 Middle Dutch <em>voorlingen </em>= <em>furlongs</em>) so that the plough could go straight in a long line before having to turn. Important blocks or strips were demarcated by hedges, earthwork, woodwork, ditches or roads. Medieval names for these blocks often survived into the modern day. </p><p>The presence of drinking water (a river or a brook) in the vicinity was an important factor in choosing the location for the settlement. The vicinity of water entailed risk and reward because flooding was an ever present danger. Floods could devastate arable land but might also fertilize it. Meadows in particular were often situated in flood areas. </p><h3><strong>Managing a settlement</strong></h3><p>So how was such a settlement managed? First of all, the quality of the soil had to be carefully controlled by crop rotation: specific crops were sown on different segments of the arable land with one part laying fallow to recover from the tilling (English three-field system, Dutch <em>drieslagstelsel</em>). The cattle and sheep were put out to pasture on the common meadows guarded by a shepherd or cowherd. Pigs were allowed to forage in the nearby forests and killed in autumn before the winter starvation set in.</p><p>Roads and rivers were important for transport of crops and livestock. These roads, some of them paved, some of them not, needed to be maintained. They were essential to the payment of the tithe, since tithe collectors assessed the harvest on the field and later collected the sheaves on the side of the road.</p><p>The buildings within the community also needed maintenance. Farmhouses, community barns and stables were made of wood and had to be rebuilt every few generations, only the name of the farm or homestead being continued.</p><p>So what kind of threats did a medieval settlement face? First of all, the weather was an important factor which dictated the success of the harvest. Storms, droughts and floods could devastate the harvest and decimate the community.</p><p>Diseases and epidemics were another danger threatening the community. The situation on the countryside was a lot better in this regard than in the medieval towns, but an epidemic could still mean the end of a village. Similarly, diseases among livestock impacted the medieval subsistence economy in a brutal way.</p><p>Then there are the consequences of medieval warfare affecting the community: Armies that passed by could plunder the village, burn the farms and execute villagers at will. Or they could also demand supplies, food and provisions as an emergency "tax"<br></p>
                        						<figure id="10615">
									<img src="https://www.leidenmedievalistsblog.nl/images/uploads/_rightlandscape/PAK.GameBlog91.png" alt="PAK Game Blog91">
												   <figcaption>Screenshot: Burning village in Knights and Merchants (1998)</figcaption>
							</figure>
	   						
                  <p>But war also brought indirect consequences; a liege lord calling the banners and levying troops from the village community might extract a large part of the adult men. Warfare also disrupted the trade networks that supplied a village with building materials and commodities.</p><p>Then there were internal threats to the fabric of the village community. We may think of social unrest because of land disputes. Feuds could also tear a community apart with endemic vendetta’s causing death and despair. A socially unstable society was also more prone to internal accusations of heresy and witchcraft.</p><h3><strong>An “accurate” medieval settlement builder</strong></h3><p>So, which of the above listed features could potentially contribute to a more historically accurate computer game about medieval settlement building? First of all, it would be more realistic if the settlement could first be planned out and was not forced to "grow organically" from a community center. The first settlement phase would be a test of how “successful” a layout is in adapting to the exigencies of the terrain and the needs of the community. Only after that initial layout proved successful, further expansions can be planned. </p><p>Secondly, it would be more realistic if we could build both straight roads and curved roads, just as in <em>Cities Skylines </em>(2015), a modern city builder well known for its incredibly flexible layout tools. Incidentally, the tools of Cities Skylines can also be used to recreate medieval settlements, as was done by YouTube creator <a href="https://www.youtube.com/watch?v=b7wsXzeuaqk" target="_blank" rel="noreferrer noopener">Play Curiously</a> who constructed an impression of a medieval Croatian village.</p>
                        						<figure id="10615">
									<img src="https://www.leidenmedievalistsblog.nl/images/uploads/_fulllandscape/PAK.GameBlog92.jpg" alt="PAK Game Blog92">
												   <figcaption>Screenshot: Cities Skylines (2015) by YouTuber Play Curiously</figcaption>
							</figure>
	   						
                  <p>Such a flexible road drawing tool can then also be used to lay out ditches, hedges and enclosures since these features were central to the medieval experience of the cultivated landscape.</p><p>Thirdly, It would be interesting to see a medieval-themed game embrace the concept of flood valleys that limit and endanger pasture and arable land. Other historical city builders such as <em>Pharaoh</em> (1999) and <em>Children of the Nile</em> (2004) already implemented this feature for their setting in Ancient Egypt. However, such a mechanic would likewise fit a medieval city builder and show the general public how medieval society dealt with seasonal flooding as well as the devastating effects that storm floods could have.</p>
                        						<figure id="10615">
									<img src="https://www.leidenmedievalistsblog.nl/images/uploads/_rightlandscape/PAK.GameBlog93.jpg" alt="PAK Game Blog93">
												   <figcaption>Screenshot:  Children of the Nile (2004)</figcaption>
							</figure>
	   						
                  <p>And finally, something that would, in my opinion, really add to the realism and historical flavor of a medieval-themed city builder would be the introduction of mechanisms in which agricultural surpluses are skimmed by the church and the feudal lord. Tithes, taxes and rents! Instead of merely abstracting the taxes into an income modifier or letting the player be the extractor himself, we could be shown the tax collector visiting the village, counting the sheaves by the side of the road, selecting the calves and chickens. This way, the experiences of our medieval forebears are visualized and may help to educate the public about medieval village life.</p>
<h3><strong>Why not?</strong></h3>
<p>There are some good reasons why city building games are not that historically accurate and instead adhere to the established formula of the city building game. </p>
<p>First of all, a linear growth model makes sense from a gameplay perspective, since it is rewarding to see your settlement grow.in a linear way. It fosters a feeling of progress and motivates the player to keep momentum and push through to the next expansion phase. Secondly, games are generally wary of punishing failure too harshly in order to avoid demoralizing the player. Thirdly, in order to facilitate path finding for the simulated villagers it is easier to implement a gridlike road and building system rather than an off-grid building system that allows for curvy roads. So far only <em>Cities Skylines </em>has managed to do this in a satisfactory way. </p>
<p>Lastly, for marketing purposes and recognizability, game developers generally don't stray too far from the image of the Middle Ages that the public is already acquainted with. For a medieval city builder this means windmills, industrious peasants, lots of sheep and stone castles. Things like land surveying, crop rotation and tithe collection do not fit this image and challenge the romanticized picture of the uneducated farmer in his pre-industrial environment. </p>
<h3><strong>Conclusion</strong></h3>
<p>Although I think medieval-themed city building games could benefit from incorporating some of the things we know about medieval settlement history into the gameplay loop, it may not be desirable for game developers to stray too far from the established formula. The idea that medieval settlements developed organically according to messy road plans is strongly imbedded in popular perception. Allowing both straight and curved road building in medieval city builders, may serve to challenge some of the stereotypes that exist about medieval village life. And if you ask me, that would be a good thing for it is an enriching experience to see the world through the eyes of our medieval forebears. One may find out that their lives were not that different after all... </p>

<h3><strong>Further reading</strong></h3>
<ul><li>Fossier, R. (2004). “The Rural Economy and Demographic Growth.” In: D. Luscombe &amp; J. Riley-Smith (Eds.). <em>The New Cambridge Medieval History</em>. Cambridge: Cambridge University Press, 11-46.</li><li>Van Ham, W. (1979). “Dorp en dorpsleven in middeleeuws Wouw." in: A. Delahaye (red.), De Heren XVII van Nassau-Brabant, 316-336.”</li><li>(forthc.) Kerkhof, P.A. (2020). “Saer, Saert; een Zuid-Nederlandse veldnaam van onzekere oorsprong.”<em> Noordbrabants Historisch Jaarboek</em>.</li><li>Leenders, K.A.H.W. (1996). "Noord-Vlaanderen en de Noordwesthoek; een vergelijking." Tijdschrift voor Waterstaatsgeschiedenis 5, 67-73.</li><li>Leenders, K.A.H.W. (1989). <em>Verdwenen venen; een onderzoek naar de ligging en exploitatie van thans verdwenen venen in het gebied tussen Antwerpen, Turnhout, Geertruidenberg en Willemstad (1250-1750)</em>. Reeks Landschapsstudies 13, Wageningen.</li><li>Oosthuizen, S. (2017). <em>The Anglo-Saxon fenland. </em>Windgather Press.</li></ul>
<p><em>© Alexia Kerkhof and Leiden Medievalists Blog, 2020. Unauthorised use and/or duplication of this material without express and written permission from this site’s author and/or owner is strictly prohibited. Excerpts and links may be used, provided that full and clear credit is given to Alexia Kerkhof and Leiden Medievalists Blog with appropriate and specific direction to the original content.</em></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Improving the usability of C libraries in Swift (116 pts)]]></title>
            <link>https://www.swift.org/blog/improving-usability-of-c-libraries-in-swift/</link>
            <guid>46726526</guid>
            <pubDate>Thu, 22 Jan 2026 23:34:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.swift.org/blog/improving-usability-of-c-libraries-in-swift/">https://www.swift.org/blog/improving-usability-of-c-libraries-in-swift/</a>, See on <a href="https://news.ycombinator.com/item?id=46726526">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>There are many interesting, useful, and fun C libraries in the software ecosystem. While one could go and rewrite these libraries in Swift, usually there is no need, because Swift provides direct interoperability with C. With a little setup, you can directly use existing C libraries from your Swift code.</p>

<p>When you use a C library directly from Swift, it will look and feel similar to using it from C. That can be useful if you’re following sample code or a tutorial written in C, but it can also feel out of place. For example, here’s a small amount of code using a C API:</p>

<div><pre><code>  <span>var</span> <span>instanceDescriptor</span> <span>=</span> <span>WGPUInstanceDescriptor</span><span>()</span>
  <span>let</span> <span>instance</span> <span>=</span> <span>wgpuCreateInstance</span><span>(</span><span>&amp;</span><span>instanceDescriptor</span><span>)</span>
  <span>var</span> <span>surfaceDescriptor</span> <span>=</span> <span>WGPUSurfaceDescriptor</span><span>()</span>
  <span>let</span> <span>surface</span> <span>=</span> <span>wgpuInstanceCreateSurface</span><span>(</span><span>instance</span><span>,</span> <span>&amp;</span><span>surfaceDescriptor</span><span>)</span>
  <span>if</span> <span>wgpuSurfacePresent</span><span>(</span><span>&amp;</span><span>surface</span><span>)</span> <span>==</span> <span>WGPUStatus_Error</span> <span>{</span>
      <span>// report error</span>
  <span>}</span>
  <span>wgpuSurfaceRelease</span><span>(</span><span>surface</span><span>)</span>
  <span>wgpuInstanceRelease</span><span>(</span><span>instance</span><span>)</span>
</code></pre></div>

<p>The C library here that Swift is using comes from the <a href="https://github.com/webgpu-native/webgpu-headers">webgpu-headers project</a>, which vends a C header (<code>webgpu.h</code>) that is used by several implementations of <a href="https://www.w3.org/TR/webgpu/">WebGPU</a>. WebGPU  is a technology that enables web developers to use the system’s GPU (Graphics Processing Unit) from the browser. For the purposes of this post, you don’t really need to know anything about WebGPU: I’m using it as an example of a typical C library, and the techniques described in this blog post apply to lots of other well-designed C libraries.</p>

<p>The Swift code above has a very “C” feel to it. It has global function calls with prefixed names like <code>wgpuInstanceCreateSurface</code> and global integer constants like <code>WGPUStatus_Error</code>. It pervasively uses unsafe pointers, some of which are managed with explicit reference counting, where the user provides calls to <code>wpuXYZAddRef</code> and <code>wgpuXYZRelease</code> functions. It works, but it doesn’t feel like Swift, and inherits various safety problems of C.</p>

<p>Fortunately, we can improve this situation, providing a safer and more ergonomic interface to WebGPU from Swift that feels like it belongs in Swift. More importantly, we can do so without changing the WebGPU implementation: Swift provides a suite of annotations that you can apply to C headers to improve the way in which the C APIs are expressed in Swift. These annotations describe common conventions in C that match up with Swift constructs, projecting a more Swift-friendly interface on top of the C code.</p>

<p>In this post, I’m going to use these annotations to improve how Swift interacts with the WebGPU C code. By the end, we’ll be able to take advantage of Swift features like argument labels, methods, enums, and automatic reference counting, like this:</p>

<div><pre><code>  <span>var</span> <span>instanceDescriptor</span> <span>=</span> <span>WGPUInstanceDescriptor</span><span>()</span>
  <span>let</span> <span>instance</span> <span>=</span> <span>WGPUInstance</span><span>(</span><span>descriptor</span><span>:</span> <span>&amp;</span><span>instanceDescriptor</span><span>)</span>
  <span>var</span> <span>surfaceDescriptor</span> <span>=</span> <span>WGPUSurfaceDescriptor</span><span>()</span>
  <span>let</span> <span>surface</span> <span>=</span> <span>instance</span><span>.</span><span>createSurface</span><span>(</span><span>descriptor</span><span>:</span> <span>&amp;</span><span>surfaceDescriptor</span><span>)</span>
  <span>if</span> <span>surface</span><span>.</span><span>present</span><span>()</span> <span>==</span> <span>.</span><span>error</span> <span>{</span>
      <span>// report error</span>
  <span>}</span>
  <span>// Swift automatically deallocates the instance and surface when we're done</span>
</code></pre></div>

<p>These same annotations can be used for any C library to provide a safer, more ergonomic development experience in Swift without changing the C library at all.</p>

<blockquote>
  <p><strong>Note</strong>: Some of what is covered in this post requires bug fixes that first became available in Swift 6.2.3.</p>
</blockquote>



<p>A <a href="https://clang.llvm.org/docs/Modules.html">module map</a> is a way of layering a Swift-friendly modular structure on top of C headers. You can create a module map for the WebGPU header by writing the following to a file <code>module.modulemap</code>:</p>

<div><pre><code><span>module</span> <span>WebGPU</span> <span>{</span>
  <span>header</span> <span>"webgpu.h"</span>
  <span>export</span> <span>*</span>
<span>}</span>
</code></pre></div>

<p>The easiest thing to do is to put <code>module.modulemap</code> alongside the header itself. For my experiment here, I put it in the root directory of my <code>webgpu-headers</code> checkout. If you’re in a Swift package, put it into its own target with this layout:</p>

<div><pre><code><span>├──</span> <span>Package</span><span>.</span><span>swift</span>
<span>└──</span> <span>Sources</span>
    <span>└──</span> <span>WebGPU</span>
        <span>├──</span> <span>include</span>
        <span>│</span>   <span>├──</span> <span>webgpu</span><span>.</span><span>h</span>
        <span>│</span>   <span>└──</span> <span>module</span><span>.</span><span>modulemap</span>
        <span>└──</span> <span>WebGPU</span><span>.</span><span>c</span> <span>(</span><span>empty</span> <span>file</span><span>)</span>
</code></pre></div>

<p>If you reference this <code>WebGPU</code> target from elsewhere in the package, you can <code>import WebGPU</code> to get access to the C APIs.</p>



<p>There are a few ways to see what the Swift interface for a C library looks like.</p>

<ul>
  <li>The <code>swift-synthesize-interface</code> tool in Swift 6.2+ prints the Swift interface to the terminal.</li>
  <li>Xcode’s “Swift 5 interface” counterpart to the <code>webgpu.h</code> header will show how the header has been mapped into Swift.</li>
</ul>

<p>Let’s do it from the command line, using <code>swift-synthesize-interface</code>. From the directory containing <code>webgpu.h</code> and <code>module.modulemap</code>, run:</p>

<div><pre><code><span>xcrun</span> <span>swift</span><span>-</span><span>synthesize</span><span>-</span><span>interface</span> <span>-</span><span>I</span> <span>.</span> <span>-</span><span>module</span><span>-</span><span>name</span> <span>WebGPU</span> <span>-</span><span>target</span> <span>arm64</span><span>-</span><span>apple</span><span>-</span><span>macos15</span> <span>-</span><span>sdk</span> <span>/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/</span><span>MacOSX26</span><span>.</span><span>0</span><span>.</span><span>sdk</span>
</code></pre></div>

<p>The leading <code>xcrun</code> and the <code>-sdk</code> argument with the path is only needed on macOS; on other platforms, make sure <code>swift-synthesize-interface</code> is in your path. The <code>-target</code> operation is the triple provided if you run <code>swiftc -print-target-info</code>. It looks like this:</p>

<div><pre><code><span>{</span><span>
  </span><span>"compilerVersion"</span><span>:</span><span> </span><span>"Apple Swift version 6.2 (swiftlang-6.2.2.15.4 clang-1700.3.15.2)"</span><span>,</span><span>
  </span><span>"target"</span><span>:</span><span> </span><span>{</span><span>
    </span><span>"triple"</span><span>:</span><span> </span><span>"arm64-apple-macosx15.0"</span><span>,</span><span>
    </span><span>"unversionedTriple"</span><span>:</span><span> </span><span>"arm64-apple-macosx"</span><span>,</span><span>
    </span><span>"moduleTriple"</span><span>:</span><span> </span><span>"arm64-apple-macos"</span><span>,</span><span>
    </span><span>"compatibilityLibraries"</span><span>:</span><span> </span><span>[</span><span> </span><span>],</span><span>
    </span><span>"librariesRequireRPath"</span><span>:</span><span> </span><span>false</span><span>
  </span><span>},</span><span>
  </span><span>"paths"</span><span>:</span><span> </span><span>{</span><span> </span><span>...</span><span> </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>

<p>The output of <code>swift-synthesize-interface</code> is the Swift API for the WebGPU module, directly translated from C. For example, this code from the WebGPU header:</p>

<div><pre><code><span>typedef</span> <span>enum</span> <span>WGPUAdapterType</span> <span>{</span>
    <span>WGPUAdapterType_DiscreteGPU</span> <span>=</span> <span>0x00000001</span><span>,</span>
    <span>WGPUAdapterType_IntegratedGPU</span> <span>=</span> <span>0x00000002</span><span>,</span>
    <span>WGPUAdapterType_CPU</span> <span>=</span> <span>0x00000003</span><span>,</span>
    <span>WGPUAdapterType_Unknown</span> <span>=</span> <span>0x00000004</span><span>,</span>
    <span>WGPUAdapterType_Force32</span> <span>=</span> <span>0x7FFFFFFF</span>
<span>}</span> <span>WGPUAdapterType</span> <span>WGPU_ENUM_ATTRIBUTE</span><span>;</span>
</code></pre></div>

<p>is translated into:</p>

<div><pre><code><span>public</span> <span>struct</span> <span>WGPUAdapterType</span> <span>:</span> <span>Hashable</span><span>,</span> <span>Equatable</span><span>,</span> <span>RawRepresentable</span> <span>{</span>
    <span>public</span> <span>init</span><span>(</span><span>_</span> <span>rawValue</span><span>:</span> <span>UInt32</span><span>)</span>
    <span>public</span> <span>init</span><span>(</span><span>rawValue</span><span>:</span> <span>UInt32</span><span>)</span>
    <span>public</span> <span>var</span> <span>rawValue</span><span>:</span> <span>UInt32</span>
<span>}</span>

<span>public</span> <span>var</span> <span>WGPUAdapterType_DiscreteGPU</span><span>:</span> <span>WGPUAdapterType</span> <span>{</span> <span>get</span> <span>}</span>
<span>public</span> <span>var</span> <span>WGPUAdapterType_IntegratedGPU</span><span>:</span> <span>WGPUAdapterType</span> <span>{</span> <span>get</span> <span>}</span>
<span>public</span> <span>var</span> <span>WGPUAdapterType_CPU</span><span>:</span> <span>WGPUAdapterType</span> <span>{</span> <span>get</span> <span>}</span>
<span>public</span> <span>var</span> <span>WGPUAdapterType_Unknown</span><span>:</span> <span>WGPUAdapterType</span> <span>{</span> <span>get</span> <span>}</span>
<span>public</span> <span>var</span> <span>WGPUAdapterType_Force32</span><span>:</span> <span>WGPUAdapterType</span> <span>{</span> <span>get</span> <span>}</span>
</code></pre></div>

<p>and there are lots of global functions like this:</p>

<div><pre><code><span>public</span> <span>func</span> <span>wgpuComputePipelineGetBindGroupLayout</span><span>(</span><span>_</span> <span>computePipeline</span><span>:</span> <span>WGPUComputePipeline</span><span>!</span><span>,</span> <span>_</span> <span>groupIndex</span><span>:</span> <span>UInt32</span><span>)</span> <span>-&gt;</span> <span>WGPUBindGroupLayout</span><span>!</span>
<span>public</span> <span>func</span> <span>wgpuComputePipelineSetLabel</span><span>(</span><span>_</span> <span>computePipeline</span><span>:</span> <span>WGPUComputePipeline</span><span>!</span><span>,</span> <span>_</span> <span>label</span><span>:</span> <span>WGPUStringView</span><span>)</span>
<span>public</span> <span>func</span> <span>wgpuComputePipelineAddRef</span><span>(</span><span>_</span> <span>computePipeline</span><span>:</span> <span>WGPUComputePipeline</span><span>!</span><span>)</span>
<span>public</span> <span>func</span> <span>wgpuComputePipelineRelease</span><span>(</span><span>_</span> <span>computePipeline</span><span>:</span> <span>WGPUComputePipeline</span><span>!</span><span>)</span>
</code></pre></div>

<p>It’s a starting point! You can absolutely write Swift programs using these WebGPU APIs, and they’ll feel a lot like writing them in C. Let’s see what we can do to make it better.</p>



<p>C enums can be used for several things. Sometimes they really represent a choice among a number of alternatives. Sometimes they represent flags in a set of options, from which you can choose several. Sometimes they’re just a convenient way to create a bunch of named constants. Swift conservatively imports enum types as wrappers over the underlying C type used to store values of the enum (e.g., <code>WGPUAdapterType</code> wraps a <code>UInt32</code>) and makes the enumerators into global constants. It covers all of the possible use cases, but it isn’t <em>nice</em>.</p>

<p>The <code>WGPUAdapterType</code> enum really is a choice among one of several options, which would be best represented as an <code>enum</code> in Swift. If we were willing to modify the header, we could apply the <a href="https://clang.llvm.org/docs/AttributeReference.html#enum-extensibility"><code>enum_extensibility</code> attribute</a> to the enum, like this:</p>

<div><pre><code><span>typedef</span> <span>enum</span> <span>__attribute__</span><span>((</span><span>enum_extensibility</span><span>(</span><span>closed</span><span>)))</span> <span>WGPUAdapterType</span> <span>{</span>
    <span>WGPUAdapterType_DiscreteGPU</span> <span>=</span> <span>0x00000001</span><span>,</span>
    <span>WGPUAdapterType_IntegratedGPU</span> <span>=</span> <span>0x00000002</span><span>,</span>
    <span>WGPUAdapterType_CPU</span> <span>=</span> <span>0x00000003</span><span>,</span>
    <span>WGPUAdapterType_Unknown</span> <span>=</span> <span>0x00000004</span><span>,</span>
    <span>WGPUAdapterType_Force32</span> <span>=</span> <span>0x7FFFFFFF</span>
<span>}</span> <span>WGPUAdapterType</span> <span>WGPU_ENUM_ATTRIBUTE</span><span>;</span>
</code></pre></div>

<p>This works, and results in a much nicer Swift API:</p>

<div><pre><code><span>@frozen</span> <span>public</span> <span>enum</span> <span>WGPUAdapterType</span> <span>:</span> <span>UInt32</span><span>,</span> <span>@unchecked</span> <span>Sendable</span> <span>{</span>
    <span>case</span> <span>discreteGPU</span> <span>=</span> <span>1</span>
    <span>case</span> <span>integratedGPU</span> <span>=</span> <span>2</span>
    <span>case</span> <span>CPU</span> <span>=</span> <span>3</span>
    <span>case</span> <span>unknown</span> <span>=</span> <span>4</span>
    <span>case</span> <span>force32</span> <span>=</span> <span>2147483647</span>
<span>}</span>
</code></pre></div>

<p>Now, we get an <code>enum</code> that we can switch over, and nice short case names, e.g.,</p>

<div><pre><code><span>switch</span> <span>adapterType</span> <span>{</span>
  <span>case</span> <span>.</span><span>discreteGPU</span><span>,</span> <span>.</span><span>integratedGPU</span><span>:</span>
    <span>print</span><span>(</span><span>"definitely a GPU"</span><span>)</span>
  <span>default</span><span>:</span>
    <span>print</span><span>(</span><span>"not so sure"</span><span>)</span>
<span>}</span>
</code></pre></div>

<p>That’s great, but I already broke my rule: no header modifications unless I have to!</p>



<p>The problem of needing to layer information on top of existing C headers is not a new one. As noted earlier, Swift relies on a Clang feature called <a href="https://clang.llvm.org/docs/APINotes.html">API notes</a> to let us express this same information in a separate file, so we don’t have to edit the header. In this case, we create a file called <code>WebGPU.apinotes</code> (the name <code>WebGPU</code> matches the module name from <code>module.modulemap</code>), which is a YAML file describing the extra information. We’ll start with one that turns <code>WGPUAdapterType</code> into an <code>enum</code>:</p>

<div><pre><code><span>---</span>
<span>Name</span><span>:</span> <span>WebGPU</span>
<span>Tags</span><span>:</span>
<span>-</span> <span>Name</span><span>:</span> <span>WGPUAdapterType</span>
  <span>EnumExtensibility</span><span>:</span> <span>closed</span>
</code></pre></div>

<p><code>Tags</code> here is a term used in the C and C++ standard to refer to enum, struct, union, or class types. Any information about those types in the API notes file will go into that section.</p>

<p>Put <code>WebGPU.apinotes</code> alongside the <code>module.modulemap</code>, and now <code>WGPUAdapterType</code> gets mapped into a <code>Swift</code> enum. For a package, the structure will look like this:</p>

<div><pre><code><span>├──</span> <span>Package</span><span>.</span><span>swift</span>
<span>└──</span> <span>Sources</span>
    <span>└──</span> <span>WebGPU</span>
        <span>├──</span> <span>include</span>
        <span>│</span>   <span>├──</span> <span>webgpu</span><span>.</span><span>h</span>
        <span>│</span>   <span>├──</span> <span>WebGPU</span><span>.</span><span>apinotes</span>
        <span>│</span>   <span>└──</span> <span>module</span><span>.</span><span>modulemap</span>
        <span>└──</span> <span>WebGPU</span><span>.</span><span>c</span> <span>(</span><span>empty</span> <span>file</span><span>)</span>
</code></pre></div>

<p>We’ll be adding more to this API notes file as we keep digging through the interface.</p>



<p>The WebGPU header has a number of “object” types that are defined like this:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>WGPUBindGroupImpl</span><span>*</span> <span>WGPUBindGroup</span> <span>WGPU_OBJECT_ATTRIBUTE</span><span>;</span>
</code></pre></div>

<p>This gets imported into Swift as an alias for an opaque pointer type, which is… not great:</p>

<div><pre><code><span>public</span> <span>typealias</span> <span>WGPUBindGroup</span> <span>=</span> <span>OpaquePointer</span>
</code></pre></div>

<p>WebGPU object types are reference counted, and each object type has corresponding <code>AddRef</code> and <code>Release</code> functions to increment and decrement the reference count, like this:</p>

<div><pre><code><span>WGPU_EXPORT</span> <span>void</span> <span>wgpuBindGroupAddRef</span><span>(</span><span>WGPUBindGroup</span> <span>bindGroup</span><span>)</span> <span>WGPU_FUNCTION_ATTRIBUTE</span><span>;</span>
<span>WGPU_EXPORT</span> <span>void</span> <span>wgpuBindGroupRelease</span><span>(</span><span>WGPUBindGroup</span> <span>bindGroup</span><span>)</span> <span>WGPU_FUNCTION_ATTRIBUTE</span><span>;</span>
</code></pre></div>

<p>Of course, you can use these functions in Swift exactly how you do in C, making sure to balance out calls to <code>AddRef</code> and <code>Release</code>, but then it would be every bit as unsafe as C.</p>

<p>We can do better with <a href="https://www.swift.org/documentation/cxx-interop/#shared-reference-types"><code>SWIFT_SHARED_REFERENCE</code></a>. It’s a macro (defined in the <code>&lt;swift/bridging&gt;</code> header) that can turn a reference-counted C type like the above into an automatically reference-counted <code>class</code> in Swift. Here’s how we would use it in the header:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>SWIFT_SHARED_REFERENCE</span><span>(</span><span>wgpuBindGroupAddRef</span><span>,</span> <span>wgpuBindGroupRelease</span><span>)</span> <span>WGPUBindGroupImpl</span><span>*</span> <span>WGPUBindGroup</span> <span>WGPU_OBJECT_ATTRIBUTE</span><span>;</span>
</code></pre></div>

<p>Now, <code>WGPUBindGroup</code> gets imported like this:</p>

<div><pre><code><span>public</span> <span>class</span> <span>WGPUBindGroupImpl</span> <span>{</span> <span>}</span>
<span>public</span> <span>typealias</span> <span>WGPUBindGroup</span> <span>=</span> <span>WGPUBindGroupImpl</span>
</code></pre></div>

<p>The extra typealias is a little unexpected, but overall this is a huge improvement: Swift is treating <code>WGPUBindGroup</code> as a class, meaning that it automatically manages retains and releases for you! This is both an ergonomic win (less code to write) and a safety win, because it’s eliminated the possibility of mismanaging these instances.</p>

<p>There’s one more thing: when dealing with reference-counting APIs, you need to know whether a particular function that returns an object is expecting you to call “release” when you’re done. In the WebGPU header, this information is embedded in a comment:</p>

<div><pre><code><span>/**
 * @returns
 * This value is @ref ReturnedWithOwnership.
 */</span>
<span>WGPU_EXPORT</span> <span>WGPUBindGroup</span> <span>wgpuDeviceCreateBindGroup</span><span>(</span><span>WGPUDevice</span> <span>device</span><span>,</span> <span>WGPUBindGroupDescriptor</span> <span>const</span> <span>*</span> <span>descriptor</span><span>)</span> <span>WGPU_FUNCTION_ATTRIBUTE</span><span>;</span>
</code></pre></div>

<p>“ReturnedWithOwnership” here means that the result of the call has already been retained one extra time, and the caller is responsible for calling “release” when they are done with it. The <code>&lt;swift/bridging&gt;</code> header has a <code>SWIFT_RETURNS_RETAINED</code> macro that expresses this notion. One can use it like this:</p>

<div><pre><code><span>WGPU_EXPORT</span> <span>WGPUBindGroup</span> <span>wgpuDeviceCreateBindGroup</span><span>(</span><span>WGPUDevice</span> <span>device</span><span>,</span> <span>WGPUBindGroupDescriptor</span> <span>const</span> <span>*</span> <span>descriptor</span><span>)</span> <span>WGPU_FUNCTION_ATTRIBUTE</span> <span>SWIFT_RETURNS_RETAINED</span><span>;</span>
</code></pre></div>

<p>Now, Swift will balance out the retain that <code>wgpuDeviceCreateBindGroup</code> has promised to do by performing the extra release once you’re done using the object. Once these annotations are done, we’re all set with a more ergonomic and memory-safe API for this C library. There’s no need to ever call <code>wgpuBindGroupRelease</code> or <code>wgpuBindGroupAddRef</code> yourself.</p>

<p>We’ve hacked up our header again, so let’s undo that and move all of this out to API notes. To turn a type into a foreign reference type, we augment the <code>Tags</code> section of our API notes with the same information, but in YAML form:</p>

<div><pre><code><span>-</span> <span>Name</span><span>:</span> <span>WGPUBindGroupImpl</span>
  <span>SwiftImportAs</span><span>:</span> <span>reference</span>
  <span>SwiftReleaseOp</span><span>:</span> <span>wgpuBindGroupRelease</span>
  <span>SwiftRetainOp</span><span>:</span> <span>wgpuBindGroupAddRef</span>
</code></pre></div>

<p>That makes <code>WGPUBindGroupImpl</code> import as a class type, with the given retain and release functions. We can express the “returns retained” behavior of the <code>wgpuDeviceCreateBindGroup</code> function like this:</p>

<div><pre><code><span>Functions</span><span>:</span>
<span>-</span> <span>Name</span><span>:</span> <span>wgpuDeviceCreateBindGroup</span>
  <span>SwiftReturnOwnership</span><span>:</span> <span>retained</span>
</code></pre></div>

<p>That’s enums and classes, so now let’s tackle… functions.</p>



<p>A typical function from <code>webgpu.h</code>, like this:</p>

<div><pre><code><span>WGPU_EXPORT</span> <span>void</span> <span>wgpuQueueWriteBuffer</span><span>(</span>
    <span>WGPUQueue</span> <span>queue</span><span>,</span> <span>WGPUBuffer</span> <span>buffer</span><span>,</span> <span>uint64_t</span> <span>bufferOffset</span><span>,</span> 
    <span>void</span> <span>const</span> <span>*</span> <span>data</span><span>,</span> <span>size_t</span> <span>size</span>
<span>)</span> <span>WGPU_FUNCTION_ATTRIBUTE</span><span>;</span>
</code></pre></div>

<p>will come into Swift like this:</p>

<div><pre><code><span>public</span> <span>func</span> <span>wgpuQueueWriteBuffer</span><span>(</span><span>_</span> <span>queue</span><span>:</span> <span>WGPUQueue</span><span>!</span><span>,</span> <span>_</span> <span>buffer</span><span>:</span> <span>WGPUBuffer</span><span>!</span><span>,</span> <span>_</span> <span>bufferOffset</span><span>:</span> <span>UInt64</span><span>,</span> <span>_</span> <span>data</span><span>:</span> <span>UnsafeRawPointer</span><span>!</span><span>,</span> <span>_</span> <span>size</span><span>:</span> <span>Int</span><span>)</span>
</code></pre></div>

<p>Note that <code>_</code> on each parameter, which means that we won’t use argument labels for anything when we call it:</p>

<div><pre><code><span>wgpuQueueWriteBuffer</span><span>(</span><span>myQueue</span><span>,</span> <span>buffer</span><span>,</span> <span>position</span><span>,</span> <span>dataToWrite</span><span>,</span> <span>bytesToWrite</span><span>)</span>
</code></pre></div>

<p>That matches C, but it isn’t as clear as it could be in Swift. Let’s clean this up by providing a better name in Swift that includes argument labels. We can do so using <code>SWIFT_NAME</code> (also in <code>&lt;swift/bridging&gt;</code>), like this:</p>

<div><pre><code><span>WGPU_EXPORT</span> <span>void</span> <span>wgpuQueueWriteBuffer</span><span>(</span>
      <span>WGPUQueue</span> <span>queue</span><span>,</span> <span>WGPUBuffer</span> <span>buffer</span><span>,</span> <span>uint64_t</span> <span>bufferOffset</span><span>,</span>
      <span>void</span> <span>const</span> <span>*</span> <span>data</span><span>,</span> <span>size_t</span> <span>size</span>
  <span>)</span> <span>WGPU_FUNCTION_ATTRIBUTE</span> 
    <span>SWIFT_NAME</span><span>(</span><span>"wgpuQueueWriteBuffer(_:buffer:bufferOffset:data:size:)"</span><span>);</span>
</code></pre></div>

<p>Within the parentheses, we have each of the argument labels that we want (or <code>_</code> meaning “no label”), each followed by a <code>:</code>. This is how one describes a full function name in Swift. Once we’ve made this change to the Swift name, the C function comes into Swift with argument labels, like this:</p>

<div><pre><code><span>public</span> <span>func</span> <span>wgpuQueueWriteBuffer</span><span>(</span><span>_</span> <span>queue</span><span>:</span> <span>WGPUQueue</span><span>!</span><span>,</span> <span>buffer</span><span>:</span> <span>WGPUBuffer</span><span>!</span><span>,</span> <span>bufferOffset</span><span>:</span> <span>UInt64</span><span>,</span> <span>data</span><span>:</span> <span>UnsafeRawPointer</span><span>!</span><span>,</span> <span>size</span><span>:</span> <span>Int</span><span>)</span>
</code></pre></div>

<p>That makes the call site more clear and self-documenting:</p>

<div><pre><code><span>wgpuQueueWriteBuffer</span><span>(</span><span>myQueue</span><span>,</span> <span>buffer</span><span>:</span> <span>buffer</span><span>,</span> <span>offset</span><span>:</span> <span>position</span><span>,</span> <span>data</span><span>:</span> <span>dataToWrite</span><span>,</span> <span>size</span><span>:</span> <span>bytesToWrite</span><span>)</span>
</code></pre></div>



<p>There is more usable structure in this API. Note that the <code>wgpuQueueWriteBuffer</code> function takes, as its first argument, an instance of <code>WGPUQueue</code>. Most of the C functions in <code>WebGPU.h</code> are like this, because these are effectively functions that operate on their first argument. In a language that has methods, they would be methods. Swift has methods, so let’s make them methods!</p>

<div><pre><code><span>WGPU_EXPORT</span> <span>void</span> <span>wgpuQueueWriteBuffer</span><span>(</span>
      <span>WGPUQueue</span> <span>queue</span><span>,</span> <span>WGPUBuffer</span> <span>buffer</span><span>,</span> <span>uint64_t</span> <span>bufferOffset</span><span>,</span> <span>void</span> <span>const</span> <span>*</span> <span>data</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> 
  <span>WGPU_FUNCTION_ATTRIBUTE</span> <span>SWIFT_NAME</span><span>(</span><span>"WGPUQueueImpl.writeBuffer(self:buffer:bufferOffset:data:size:)"</span><span>);</span>
</code></pre></div>

<p>There are three things to notice about this <code>SWIFT_NAME</code> string:</p>

<ul>
  <li>It starts with <code>WGPUQueueImpl.</code>, which tells Swift to make this function a member inside <code>WGPUQueueImpl</code>.</li>
  <li>Let’s change the function name to <code>writeBuffer</code>, because we no longer need the <code>wgpuQueue</code> prefix to distinguish it from other “write buffer” operations on other types.</li>
  <li>The name of the first argument in parentheses is <code>self</code>, which indicates that the <code>self</code> argument (in Swift) should be passed as that positional argument to the C function. The other arguments are passed in-order.</li>
</ul>

<p>Note that this also requires <code>WGPUQueue(Impl)</code> to be imported as a <code>class</code>, as we did earlier for <code>WGPUBindGroupImpl</code>. Once we’ve done so, we get a much-nicer Swift API:</p>

<div><pre><code><span>extension</span> <span>WGPUQueueImpl</span> <span>{</span>
  <span>/**
   * Produces a @ref DeviceError both content-timeline (`size` alignment) and d
evice-timeline
   * errors defined by the WebGPU specification.
   */</span>
  <span>public</span> <span>func</span> <span>writeBuffer</span><span>(</span><span>buffer</span><span>:</span> <span>WGPUBuffer</span><span>!</span><span>,</span> <span>bufferOffset</span><span>:</span> <span>UInt64</span><span>,</span> <span>data</span><span>:</span> <span>UnsafeRawPointer</span><span>!</span><span>,</span> <span>size</span><span>:</span> <span>Int</span><span>)</span>
<span>}</span>
</code></pre></div>

<p>We’ve hacked up the header again, but didn’t have to. In <code>WebGPU.apinotes</code>, you can put a <code>SwiftName</code> attribute on any entity. For <code>wgpuQueueWriteBuffer</code>, it would look like this (in the <code>Functions</code> section):</p>

<div><pre><code><span>-</span> <span>Name</span><span>:</span> <span>wgpuQueueWriteBuffer</span>
  <span>SwiftName</span><span>:</span> <span>WGPUQueueImpl.writeBuffer(self:buffer:bufferOffset:data:size:)</span>
</code></pre></div>



<p><code>WebGPU.h</code> has a number of <code>Get</code> functions that produce information about some aspect of a type. Here are two for the <code>WGPUQuerySet</code> type:</p>

<div><pre><code><span>WGPU_EXPORT</span> <span>uint32_t</span> <span>wgpuQuerySetGetCount</span><span>(</span><span>WGPUQuerySet</span> <span>querySet</span><span>)</span> <span>WGPU_FUNCTION_ATTRIBUTE</span><span>;</span>
<span>WGPU_EXPORT</span> <span>WGPUQueryType</span> <span>wgpuQuerySetGetType</span><span>(</span><span>WGPUQuerySet</span> <span>querySet</span><span>)</span> <span>WGPU_FUNCTION_ATTRIBUTE</span><span>;</span>
</code></pre></div>

<p>With the <code>SWIFT_NAME</code> tricks above, we can turn these into “get” methods on <code>WGPUQuerySet</code>, like this:</p>

<div><pre><code><span>extension</span> <span>WGPUQuerySetImpl</span> <span>{</span>
    <span>public</span> <span>func</span> <span>getCount</span><span>()</span> <span>-&gt;</span> <span>UInt32</span>
    <span>public</span> <span>func</span> <span>getType</span><span>()</span> <span>-&gt;</span> <span>WGPUQueryType</span>
<span>}</span>
</code></pre></div>

<p>That’s okay, but it’s not what you’d do in Swift. Let’s go one step further and turn them into read-only computed properties. To do so, use the <code>getter:</code> prefix on the Swift name we define. We’ll skip ahead to the YAML form that goes into API notes:</p>

<div><pre><code><span>-</span> <span>Name</span><span>:</span> <span>wgpuQuerySetGetCount</span>
  <span>SwiftName</span><span>:</span> <span>getter:WGPUQuerySetImpl.count(self:)</span>
<span>-</span> <span>Name</span><span>:</span> <span>wgpuQuerySetGetType</span>
  <span>SwiftName</span><span>:</span> <span>getter:WGPUQuerySetImpl.type(self:)</span>
</code></pre></div>

<p>And now, we arrive at a nice Swift API:</p>

<div><pre><code><span>extension</span> <span>WGPUQuerySetImpl</span> <span>{</span>
    <span>public</span> <span>var</span> <span>count</span><span>:</span> <span>UInt32</span> <span>{</span> <span>get</span> <span>}</span>
    <span>public</span> <span>var</span> <span>type</span><span>:</span> <span>WGPUQueryType</span> <span>{</span> <span>get</span> <span>}</span>
<span>}</span>
</code></pre></div>



<p><code>SWIFT_NAME</code> can also be used to import a function that returns a new instance as a Swift initializer. For example, this function creates a new <code>WGPUInstance</code> (which we assume is getting imported as a <code>class</code> like we’ve been doing above):</p>

<div><pre><code><span>/**
 * Create a WGPUInstance
 *
 * @returns
 * This value is @ref ReturnedWithOwnership.
 */</span>
<span>WGPU_EXPORT</span> <span>WGPUInstance</span> <span>wgpuCreateInstance</span><span>(</span><span>WGPU_NULLABLE</span> <span>WGPUInstanceDescriptor</span> <span>const</span> <span>*</span> <span>descriptor</span><span>)</span> <span>WGPU_FUNCTION_ATTRIBUTE</span><span>;</span>
</code></pre></div>

<p>We can turn this into a Swift initializer, which is used to create a new object, using the same <code>SWIFT_NAME</code> syntax but where the method name is <code>init</code>. Here is the YAML form that goes into API notes:</p>

<div><pre><code><span>-</span> <span>Name</span><span>:</span> <span>wgpuCreateInstance</span>
  <span>SwiftReturnOwnership</span><span>:</span> <span>retained</span>
  <span>SwiftName</span><span>:</span> <span>WGPUInstanceImpl.init(descriptor:)</span>
</code></pre></div>

<p>and here is the resulting Swift initializer:</p>

<div><pre><code><span>extension</span> <span>WGPUInstanceImpl</span> <span>{</span>
    <span>/**
     * Create a WGPUInstance
     *
     * @returns
     * This value is @ref ReturnedWithOwnership.
     */</span>
    <span>public</span> <span>/*not inherited*/</span> <span>init</span><span>!</span><span>(</span><span>descriptor</span><span>:</span> <span>UnsafePointer</span><span>&lt;</span><span>WGPUInstanceDescriptor</span><span>&gt;!</span><span>)</span>
<span>}</span>
</code></pre></div>

<p>Now, one can create a new <code>WGPUInstance</code> with the normal object-creation syntax, e.g.,</p>

<div><pre><code><span>let</span> <span>instance</span> <span>=</span> <span>WGPUInstance</span><span>(</span><span>descriptor</span><span>:</span> <span>myDescriptor</span><span>)</span>
</code></pre></div>



<p>The WebGPU header defines its own Boolean type. I wish everyone would use C99’s <code>_Bool</code> and be done with it, but alas, here are the definitions for WebGPUs Boolean types:</p>

<div><pre><code><span>#define WGPU_TRUE (UINT32_C(1))
#define WGPU_FALSE (UINT32_C(0))
</span><span>typedef</span> <span>uint32_t</span> <span>WGPUBool</span><span>;</span>
</code></pre></div>

<p>This means that <code>WGPUBool</code> will come in to Swift as a <code>UInt32</code>. The two macros aren’t available in Swift at all: they’re “too complicated” to be recognized as integral constants. Even if they were available in Swift, it still wouldn’t be great because we want to use <code>true</code> and <code>false</code> for Boolean values in Swift, not <code>WGPU_TRUE</code> and <code>WGPU_FALSE</code>.</p>

<p>To make <code>WGPUBool</code> easier to use from Swift, we’re first going to map that typedef to its own <code>struct</code> that stores the underlying <code>UInt32</code>, giving it an identity separate from <code>UInt32</code>. We can do this using a <code>SwiftWrapper</code> API note within the <code>Typedefs</code> section of the file, like this:</p>

<div><pre><code><span>-</span> <span>Name</span><span>:</span> <span>WGPUBool</span>
  <span>SwiftWrapper</span><span>:</span> <span>struct</span>
</code></pre></div>

<p>Now, we get <code>WGPUBool</code> imported like this:</p>

<div><pre><code><span>public</span> <span>struct</span> <span>WGPUBool</span> <span>:</span> <span>Hashable</span><span>,</span> <span>Equatable</span><span>,</span> <span>RawRepresentable</span> <span>{</span>
    <span>public</span> <span>init</span><span>(</span><span>_</span> <span>rawValue</span><span>:</span> <span>UInt32</span><span>)</span>
    <span>public</span> <span>init</span><span>(</span><span>rawValue</span><span>:</span> <span>UInt32</span><span>)</span>
<span>}</span>
</code></pre></div>

<p>To be able to use <code>true</code> and <code>false</code> literals with this new <code>WGPUBool</code>, we can write a little bit of Swift code that makes this type conform to the <a href="https://developer.apple.com/documentation/swift/expressiblebybooleanliteral"><code>ExpressibleByBooleanLiteral</code></a> protocol, like this:</p>

<div><pre><code><span>extension</span> <span>WGPUBool</span><span>:</span> <span>ExpressibleByBooleanLiteral</span> <span>{</span>
  <span>init</span><span>(</span><span>booleanLiteral</span> <span>value</span><span>:</span> <span>Bool</span><span>)</span> <span>{</span>
    <span>self</span><span>.</span><span>init</span><span>(</span><span>rawValue</span><span>:</span> <span>value</span> <span>?</span> <span>1</span> <span>:</span> <span>0</span><span>)</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<p>That’s it! Better type safety (you cannot confuse a <code>WGPUBool</code> with any other integer value) and the convenience of Boolean literals in Swift.</p>



<p><code>webgpu.h</code> describes a set of flags using a <code>typedef</code> of the <code>WGPUFlags</code> type (a 64-bit unsigned integer) along with a set of global constants for the different flag values. For example, here is the <code>WGPUBufferUsage</code> flag type and some of its constants:</p>

<div><pre><code><span>typedef</span> <span>WGPUFlags</span> <span>WGPUBufferUsage</span><span>;</span>
<span>static</span> <span>const</span> <span>WGPUBufferUsage</span> <span>WGPUBufferUsage_MapRead</span> <span>=</span> <span>0x0000000000000001</span><span>;</span>
<span>static</span> <span>const</span> <span>WGPUBufferUsage</span> <span>WGPUBufferUsage_MapWrite</span> <span>=</span> <span>0x0000000000000002</span><span>;</span>
<span>static</span> <span>const</span> <span>WGPUBufferUsage</span> <span>WGPUBufferUsage_CopySrc</span> <span>=</span> <span>0x0000000000000004</span><span>;</span>
<span>static</span> <span>const</span> <span>WGPUBufferUsage</span> <span>WGPUBufferUsage_Index</span> <span>=</span> <span>0x0000000000000010</span><span>;</span>
</code></pre></div>

<p>Similar to what we saw with <code>WGPUBool</code>, <code>WGPUBufferUsage</code> is a <code>typedef</code> of a <code>typedef</code> of a <code>uint64_t</code>. There’s no type safety in this C API, and one could easily mix up these flags with, say, those of <code>WGPUMapMode</code>:</p>

<div><pre><code><span>typedef</span> <span>WGPUFlags</span> <span>WGPUMapMode</span><span>;</span>
<span>static</span> <span>const</span> <span>WGPUMapMode</span> <span>WGPUMapMode_Read</span> <span>=</span> <span>0x0000000000000001</span><span>;</span>
<span>static</span> <span>const</span> <span>WGPUMapMode</span> <span>WGPUMapMode_Write</span> <span>=</span> <span>0x0000000000000002</span><span>;</span>
</code></pre></div>

<p>We can do better, by layering more structure for the Swift version of this API using the same <code>SwiftWrapper</code> approach from <code>WGPUBool</code>. This goes into the <code>Typedefs</code> section of API notes:</p>

<div><pre><code><span>Typedefs</span><span>:</span>
<span>-</span> <span>Name</span><span>:</span> <span>WGPUBufferUsage</span>
  <span>SwiftWrapper</span><span>:</span> <span>struct</span>
</code></pre></div>

<p>Now, <code>WGPUBufferUsage</code> comes in as its own <code>struct</code>:</p>

<div><pre><code><span>public</span> <span>struct</span> <span>WGPUBufferUsage</span> <span>:</span> <span>Hashable</span><span>,</span> <span>Equatable</span><span>,</span> <span>RawRepresentable</span> <span>{</span>
    <span>public</span> <span>init</span><span>(</span><span>_</span> <span>rawValue</span><span>:</span> <span>WGPUFlags</span><span>)</span>
    <span>public</span> <span>init</span><span>(</span><span>rawValue</span><span>:</span> <span>WGPUFlags</span><span>)</span>
<span>}</span>
</code></pre></div>

<p>The initializers let you create a <code>WGPUBufferUsage</code> from a <code>WGPUFlags</code> value, and there is also a <code>rawValue</code> property to get a <code>WGPUFlags</code> value out of a <code>WGPUBufferInstance</code>, so the raw value is always there… but the default is to be type safe. Additionally, those global constants will come in as members of <code>WGPUBufferUsage</code>, like this:</p>

<div><pre><code><span>extension</span> <span>WGPUBufferUsage</span> <span>{</span>
    <span>/**
     * The buffer can be *mapped* on the CPU side in *read* mode (using @ref WGPUMapMode_Read).
     */</span>
    <span>public</span> <span>static</span> <span>var</span> <span>_MapRead</span><span>:</span> <span>WGPUBufferUsage</span> <span>{</span> <span>get</span> <span>}</span>

    <span>/**
     * The buffer can be *mapped* on the CPU side in *write* mode (using @ref WGPUMapMode_Write).
     *
     * @note This usage is **not** required to set `mappedAtCreation` to `true` in @ref WGPUBufferDescriptor.
     */</span>
    <span>public</span> <span>static</span> <span>var</span> <span>_MapWrite</span><span>:</span> <span>WGPUBufferUsage</span> <span>{</span> <span>get</span> <span>}</span>

    <span>/**
     * The buffer can be used as the *source* of a GPU-side copy operation.
     */</span>
    <span>public</span> <span>static</span> <span>var</span> <span>_CopySrc</span><span>:</span> <span>WGPUBufferUsage</span> <span>{</span> <span>get</span> <span>}</span>

    <span>/**
     * The buffer can be used as the *destination* of a GPU-side copy operation.
     */</span>
    <span>public</span> <span>static</span> <span>var</span> <span>_CopyDst</span><span>:</span> <span>WGPUBufferUsage</span> <span>{</span> <span>get</span> <span>}</span>
<span>}</span>
</code></pre></div>

<p>This means that, if you’re passing a value of type <code>WPUBufferUsage</code>, you can use the shorthand “leading dot” syntax. For example:</p>

<div><pre><code><span>func</span> <span>setBufferUsage</span><span>(</span><span>_</span> <span>usage</span><span>:</span> <span>WGPUBufferUsage</span><span>)</span> <span>{</span> <span>...</span> <span>}</span>

<span>setBufferUsage</span><span>(</span><span>.</span><span>_MapRead</span><span>)</span>
</code></pre></div>

<p>Swift has dropped the common <code>WPUBufferUsage</code> prefix from the constants when it made them into members. However, the resulting names aren’t great. We can rename them by providing a <code>SwiftName</code> in the API notes file within the <code>Globals</code> section:</p>

<div><pre><code><span>Globals</span><span>:</span>
<span>-</span> <span>Name</span><span>:</span> <span>WGPUBufferUsage_MapRead</span>
  <span>SwiftName</span><span>:</span> <span>WGPUBufferUsage.mapRead</span>
<span>-</span> <span>Name</span><span>:</span> <span>WGPUBufferUsage_MapWrite</span>
  <span>SwiftName</span><span>:</span> <span>WGPUBufferUsage.mapWrite</span>
</code></pre></div>

<p>We can go one step further by making the <code>WGPUBufferUsage</code> type conform to Swift’s <a href="https://developer.apple.com/documentation/swift/optionset"><code>OptionSet</code></a> protocol. If we revise the API notes like this:</p>

<div><pre><code><span>Typedefs</span><span>:</span>
<span>-</span> <span>Name</span><span>:</span> <span>WGPUBufferUsage</span>
  <span>SwiftWrapper</span><span>:</span> <span>struct</span>
  <span>SwiftConformsTo</span><span>:</span> <span>Swift.OptionSet</span>
</code></pre></div>

<p>Now, we get the nice option-set syntax we expect in Swift:</p>

<div><pre><code><span>let</span> <span>usageFlags</span><span>:</span> <span>WGPUBufferUsage</span> <span>=</span> <span>[</span><span>.</span><span>mapRead</span><span>,</span> <span>.</span><span>mapWrite</span><span>]</span>
</code></pre></div>



<p>Throughout <code>webgpu.h</code>, the <code>WGPU_NULLABLE</code> macro is used to indicate pointers that can be NULL. The implication is that any pointer that is not marked with <code>WGPU_NULLABLE</code> cannot be NULL. For example, here is the definition of <code>wgpuCreateInstance</code> we used above:</p>

<div><pre><code><span>WGPU_EXPORT</span> <span>WGPUInstance</span> <span>wgpuCreateInstance</span><span>(</span><span>WGPU_NULLABLE</span> <span>WGPUInstanceDescriptor</span> <span>const</span> <span>*</span> <span>descriptor</span><span>)</span> <span>WGPU_FUNCTION_ATTRIBUTE</span><span>;</span>
</code></pre></div>

<p>The <code>WGPU_NULLABLE</code> indicates that it’s acceptable to pass a NULL pointer in as the <code>descriptor</code> parameter. Clang already has <a href="https://clang.llvm.org/docs/AttributeReference.html#nullability-attributes">nullability specifiers</a> to express this information. We could alter the declaration in the header to express that this parameter is nullable but the result type is never NULL, like this:</p>

<div><pre><code><span>WGPU_EXPORT</span> <span>WGPUInstance</span> <span>_Nonnull</span> <span>wgpuCreateInstance</span><span>(</span><span>WGPU_NULLABLE</span> <span>WGPUInstanceDescriptor</span> <span>const</span> <span>*</span> <span>_Nullable</span> <span>descriptor</span><span>)</span> <span>WGPU_FUNCTION_ATTRIBUTE</span><span>;</span>
</code></pre></div>

<p>This eliminates the implicitly-unwrapped optionals (<code>!</code>) from the signature of the initializer, so we end up with one that explicitly accepts a <code>nil</code> descriptor argument and always returns a new instance (never <code>nil</code>):</p>

<div><pre><code><span>extension</span> <span>WGPUInstanceImpl</span> <span>{</span>
    <span>/**
     * Create a WGPUInstance
     *
     * @returns
     * This value is @ref ReturnedWithOwnership.
     */</span>
    <span>public</span> <span>/*not inherited*/</span> <span>init</span><span>(</span><span>descriptor</span><span>:</span> <span>UnsafePointer</span><span>&lt;</span><span>WGPUInstanceDescriptor</span><span>&gt;</span><span>?)</span>
<span>}</span>
</code></pre></div>

<p>Now, I did cheat by hacking the header. Instead, we can express this with API notes on the parameters and result type by extending the entry we already have for <code>wgpuCreateInstance</code> like this:</p>

<div><pre><code><span>-</span> <span>Name</span><span>:</span> <span>wgpuCreateInstance</span>
  <span>SwiftReturnOwnership</span><span>:</span> <span>retained</span>
  <span>SwiftName</span><span>:</span> <span>WGPUInstanceImpl.init(descriptor:)</span>
  <span>Parameters</span><span>:</span>
  <span>-</span> <span>Position</span><span>:</span> <span>0</span>
    <span>Nullability</span><span>:</span> <span>O</span>
  <span>ResultType</span><span>:</span> <span>"</span><span>WGPUInstance</span><span> </span><span>_Nonnull"</span>
</code></pre></div>

<p>To specific nullability of pointer parameters, one can identify them by position (where 0 is the first parameter to the function) and then specify whether the parameter should come into Swift as optional (<code>O</code>, corresponds to <code>_Nullable</code>), non-optional (<code>N</code>, corresponds to <code>_Nonnull</code>) or by left unspecified as an implicitly-unwrapped optional (<code>U</code>, corresponds to <code>_Null_unspecified</code>). For the result type, it’s a little different: we specified the result type along with the nullability specifier, i.e., <code>WGPUInstance _Nonnull</code>. The end result of these annotations is the same as the modified header, so we can layer nullability information on top of the header.</p>



<p><code>webgpu.h</code> is about 6,400 lines long, and is regenerated from a database of the API as needed. Each of the WebGPU implementations seems to augment or tweak the header a bit. So, rather than grind through and manually do annotations, I wrote a little Swift script to “parse” <code>webgpu.h</code>, identify its patterns, and generate <code>WebGPU.apinotes</code> for most of what is discussed in this post. The entirety of the script is <a href="https://www.swift.org/assets/blog/improving-usability-of-c-libraries-in-swift/webgpu_apinotes.swift">here</a>. It reads <code>webgpu.h</code> from standard input and prints <code>WebGPU.apinotes</code> to standard output.</p>

<p>Because <code>webgpu.h</code> is generated, it has a very regular structure that we can pick up on via regular expressions. For example:</p>

<div><pre><code><span>// Enum definitions, marked by WGPU_ENUM_ATTRIBUTE.</span>
<span>let</span> <span>enumMatcher</span> <span>=</span> <span>/} (?&lt;name&gt;\w+?) WGPU_ENUM_ATTRIBUTE/</span>

<span>// Object definitions, marked by WGPU_OBJECT_ATTRIBUTE.</span>
<span>let</span> <span>objectMatcher</span> <span>=</span> <span>/typedef struct (?&lt;implName&gt;\w+?)\* (?&lt;name&gt;\w+?) WGPU_OBJECT_ATTRIBUTE;/</span>

<span>// Function declarations, marked by WGPU_FUNCTION_ATTRIBUTE</span>
<span>let</span> <span>functionMatcher</span> <span>=</span> <span>/WGPU_EXPORT (?&lt;nullableResult&gt;WGPU_NULLABLE ?)?(?&lt;resultType&gt;\w+?) (?&lt;name&gt;\w+?)\((?&lt;parameters&gt;.*\)?) WGPU_FUNCTION_ATTRIBUTE;/</span>
<span>let</span> <span>parameterMatcher</span> <span>=</span> <span>/(?&lt;type&gt;[^),]+?) (?&lt;name&gt;\w+?)[),]/</span>
</code></pre></div>

<p>That’s enough to identify all of the enum types (so we can emit the <code>EnumExtensibility: closed</code> API notes), object types (to turn them into shared references), and functions (which get nicer names and such). The script is just a big <code>readLine</code> loop that applies the regexes to capture all of the various types and functions, then does some quick classification before printing out the API notes. The resulting API notes are <a href="https://www.swift.org/assets/blog/improving-usability-of-c-libraries-in-swift/WebGPU.apinotes">in WebGPU.apinotes</a>, and the generated Swift interface after these API notes are applied is <a href="https://www.swift.org/assets/blog/improving-usability-of-c-libraries-in-swift/WebGPU.swiftinterface">here</a>. You can run it with, e.g.,</p>

<div><pre><code>swift <span>-enable-bare-slash-regex</span> webgpu_apinotes.swift &lt; webgpu.h
</code></pre></div>

<p>This script full of regular expressions is, admittedly, a bit of a hack. A better approach for an arbitrary C header would be to use <a href="https://clang.llvm.org/docs/LibClang.html"><code>libclang</code></a> to properly parse the headers. For WebGPU specifically, the webgpu-headers project contains a database from which the header is generated, and one could also generate API notes directly from that header. Regardless of how you get there, many C libraries have well-structured headers with conventions that can be leveraged to create safer, more ergonomic projections in Swift.</p>



<p>The techniques described in this post can be applied to just about any C library. To do so, I recommend setting up a small package like the one described here for WebGPU, so you can iterate quickly on example code to get a feel for how the Swift projection of the C API will work. The annotations might not get you all the way to the best Swift API, but they are a lightweight way to get most of the way there. Feel free to also extend the C types to convenience APIs that make sense in Swift, like I did above to make <code>WGPUBool</code> conform to <code>ExpressibleByBooleanLiteral</code>.</p>

<p>A little bit of annotation work on your favorite C library can make for a safer, more ergonomic, more Swifty experience of working with that library.</p>



<p>The regular structure of <code>webgpu.h</code> helped considerably when trying to expose the API nicely in Swift. That said, there are a few ways in which <code>webgpu.h</code> could be improved to require less annotation for this purpose:</p>

<ul>
  <li>
    <p><code>WGPU_ENUM_ATTRIBUTE</code> would be slightly nicer if placed on the <code>enum</code> itself, rather than on the <code>typedef</code>. If it were there, we could use</p>

    <div><pre><code><span>#define WGPU_ENUM_ATTRIBUTE __attribute__((enum_extensibility(closed)))
</span></code></pre></div>

    <p>and not have to generate any API notes to bring these types in as proper enums in Swift.</p>
  </li>
  <li>
    <p><code>WGPU_OBJECT_ATTRIBUTE</code> could provide the names of the retain and release operations and be placed on the <code>struct</code> itself. If it were there, we could use</p>

    <div><pre><code><span>#define WGPU_OBJECT_ATTRIBUTE(RetainFn,ReleaseFn) SWIFT_SHARED_REFERENCE(RetainFn,ReleaseFn)
</span></code></pre></div>

    <p>and not have to generate any API notes to bring these types in as classes in Swift.</p>
  </li>
  <li>
    <p><code>WGPU_NULLABLE</code> could be placed on the pointer itself (i.e., after the <code>*</code>) rather than at the beginning of the type, to match the position of <a href="https://clang.llvm.org/docs/AttributeReference.html#nullability-attributes">Clang’s nullability attributes</a>. If it were placed there, then</p>

    <div><pre><code><span>#define WGPU_NULLABLE _Nullable
</span></code></pre></div>

    <p>would work with Clangs’ longstanding nullable-types support. Swift would then import such pointers as optional types (with <code>?</code>). Moreover, if some macros <code>WGPU_ASSUME_NONNULL_BEGIN</code> and <code>WGPU_ASSUME_NONNULL_END</code> were placed at the beginning and end of the header, they could be mapped to Clang’s pragmas to assume that any pointer not marked “nullable” is always non-null:</p>
    <div><pre><code><span>#define WGPU_ASSUME_NONNULL_BEGIN #pragma clang assume_nonnull begin
#define WGPU_ASSUME_NONNULL_END #pragma clang assume_nonnull end
</span></code></pre></div>

    <p>This would eliminate all of the implicitly unwrapped optionals (marked <code>!</code> in the Swift interface), making it easier to use safely.</p>
  </li>
</ul>
 
      
      

      

      

      <hr>

      <h2>Continue Reading</h2>

      
      <nav>
        <div>
          <ul>
            
            <li>
              <div>
                <h3>What's new in Swift: December 2025 Edition</h3>
                <p><time pubdate="" datetime="2026-01-22T08:00:00-04:00">December 19, 2025</time></p><p>
                  Welcome to the latest digest of news from the Swift project.

                </p>
                <p><a href="https://www.swift.org/blog/whats-new-in-swift-december-2025/" rel="prev"> More </a>
              </p></div>
            </li>
             
          </ul>
        </div>
      </nav>
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scaling PostgreSQL to power 800M ChatGPT users (188 pts)]]></title>
            <link>https://openai.com/index/scaling-postgresql/</link>
            <guid>46725300</guid>
            <pubDate>Thu, 22 Jan 2026 21:24:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/scaling-postgresql/">https://openai.com/index/scaling-postgresql/</a>, See on <a href="https://news.ycombinator.com/item?id=46725300">Hacker News</a></p>
Couldn't get https://openai.com/index/scaling-postgresql/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Capital One to acquire Brex for $5.15B (302 pts)]]></title>
            <link>https://www.reuters.com/legal/transactional/capital-one-buy-fintech-firm-brex-515-billion-deal-2026-01-22/</link>
            <guid>46725288</guid>
            <pubDate>Thu, 22 Jan 2026 21:23:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/transactional/capital-one-buy-fintech-firm-brex-515-billion-deal-2026-01-22/">https://www.reuters.com/legal/transactional/capital-one-buy-fintech-firm-brex-515-billion-deal-2026-01-22/</a>, See on <a href="https://news.ycombinator.com/item?id=46725288">Hacker News</a></p>
Couldn't get https://www.reuters.com/legal/transactional/capital-one-buy-fintech-firm-brex-515-billion-deal-2026-01-22/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[White House Posts Digitally Altered Image of Woman Arrested After ICE Protest (130 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2026/jan/22/white-house-ice-protest-arrest-altered-image</link>
            <guid>46725268</guid>
            <pubDate>Thu, 22 Jan 2026 21:21:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2026/jan/22/white-house-ice-protest-arrest-altered-image">https://www.theguardian.com/us-news/2026/jan/22/white-house-ice-protest-arrest-altered-image</a>, See on <a href="https://news.ycombinator.com/item?id=46725268">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>The White House posted a digitally altered image of a woman who was arrested on Thursday in a case touted by the US attorney general, <a href="https://www.theguardian.com/us-news/pam-bondi" data-link-name="in body link">Pam Bondi</a>, to make it seem as if she was dramatically crying, a Guardian analysis of the image has found.</p><p>The woman, Nekima Levy Armstrong, also appears to have darker skin in the altered image. Armstrong was <a href="https://www.theguardian.com/us-news/2026/jan/22/two-arrests-minnesota-church-protest" data-link-name="in body link">one of three people arrested</a> on Thursday in connection to a demonstration that disrupted church services in St Paul, Minnesota, on Sunday. Demonstrators alleged that one of the pastors, David Easterwood, was the acting field director of the St Paul Immigration and Customs Enforcement (ICE) office. Bondi announced the arrests on social media on Thursday morning.</p><p>The homeland security secretary, <a href="https://www.theguardian.com/us-news/kristi-noem" data-link-name="in body link">Kristi Noem</a>, posted an image of Armstrong’s arrest at 10.21am on Thursday, less than an hour after Bondi’s announcement. The image shows a law enforcement agent, face blurred out, escorting Armstrong, who appears to be handcuffed. Armstrong, dressed in all black, appears to be composed in the picture.</p><p>A little more than 30 minutes later, the White House posted another image of Armstrong’s arrest in which she is crying. The White House press secretary, Karoline Leavitt, reposted the image. The image posted by the White House is altered, a Guardian analysis found.</p><gu-island name="InteractiveBlockComponent" priority="critical" deferuntil="idle" props="{&quot;url&quot;:&quot;https://interactive.guim.co.uk/2018/08/interactive-now-and-then-embed/embed/embed.html?mobile_before=//media.guim.co.uk/ea920dcfe2ee1f3c48902b24a2344394cccb3bed/0_0_1054_1333/791.jpg&amp;desktop_before=//media.guim.co.uk/ea920dcfe2ee1f3c48902b24a2344394cccb3bed/0_0_1054_1333/791.jpg&amp;label_before=DHS image&amp;mobile_after=//media.guim.co.uk/40c88b83a83f7381a958386a9cbe13d3f1fe30c1/0_0_1054_1333/791.jpg&amp;desktop_after=//media.guim.co.uk/40c88b83a83f7381a958386a9cbe13d3f1fe30c1/0_0_1054_1333/791.jpg&amp;label_after=White House image&amp;analytics_label=a side by side comparison of a DHS released image and a White House released image&amp;type=slider&amp;&quot;,&quot;scriptUrl&quot;:&quot;https://interactive.guim.co.uk/embed/iframe-wrapper/0.1/boot.js&quot;,&quot;alt&quot;:&quot;two images of a woman, one neutral and one crying&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0},&quot;elementId&quot;:&quot;d895bd43-59ff-47e5-b394-6a5772b0bb15&quot;,&quot;isMainMedia&quot;:false}"><figure id="d895bd43-59ff-47e5-b394-6a5772b0bb15" data-alt="two images of a woman, one neutral and one crying" data-testid="interactive-element-two%20images%20of%20a%20woman,%20one%20neutral%20and%20one%20crying" data-spacefinder-role="inline"><a data-name="placeholder" href="https://interactive.guim.co.uk/2018/08/interactive-now-and-then-embed/embed/embed.html?mobile_before=//media.guim.co.uk/ea920dcfe2ee1f3c48902b24a2344394cccb3bed/0_0_1054_1333/791.jpg&amp;desktop_before=//media.guim.co.uk/ea920dcfe2ee1f3c48902b24a2344394cccb3bed/0_0_1054_1333/791.jpg&amp;label_before=DHS%20image&amp;mobile_after=//media.guim.co.uk/40c88b83a83f7381a958386a9cbe13d3f1fe30c1/0_0_1054_1333/791.jpg&amp;desktop_after=//media.guim.co.uk/40c88b83a83f7381a958386a9cbe13d3f1fe30c1/0_0_1054_1333/791.jpg&amp;label_after=White%20House%20image&amp;analytics_label=a%20side%20by%20side%20comparison%20of%20a%20DHS%20released%20image%20and%20a%20White%20House%20released%20image&amp;type=slider&amp;">two images of a woman, one neutral and one crying</a></figure></gu-island><p>The Guardian overlaid the White House photo with the Noem photo and found that the law enforcement agents in both pictures line up exactly, confirming they are the same image. There are other similarities between the photos. An unidentified person can be seen in the same place behind the arresting agent. And the arresting agent’s arm appears to be behind Armstrong’s back in exactly the same position.</p><p>Asked whether the image had been digitally altered, the White House responded by sending a post on X from Kaelan Dorr, the deputy communications director.</p><p>“YET AGAIN to the people who feel the need to reflexively defend perpetrators of heinous crimes in our country I share with you this message: Enforcement of the law will continue. The memes will continue. Thank you for your attention to this matter,” <a href="https://x.com/Kaelan47/status/2014410500096856358?s=20" data-link-name="in body link">he said</a>.</p><p>The White House X account, which has around 3.5 million followers, has made at least 14 posts with AI since the start of Trump’s second term, <a href="https://www.poynter.org/fact-checking/2025/trump-white-house-ai-political-messaging/" data-link-name="in body link">Poynter reported in October</a>.</p><p><em>Julius Constantine Motal and David McCoy contributed reporting</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why does SSH send 100 packets per keystroke? (477 pts)]]></title>
            <link>https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/</link>
            <guid>46723990</guid>
            <pubDate>Thu, 22 Jan 2026 19:27:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/">https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/</a>, See on <a href="https://news.ycombinator.com/item?id=46723990">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><a href="https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/"></a><div><p>And why do I care?</p><p>Jan 22, 2026</p></div></div>
<p>Here are a few lines of summarized <code>tcpdump</code> output for an ssh session where I send a single keystroke:</p>
<pre><code><span>$ ./first_lines_of_pcap.sh single-key.pcap
</span><span>  <span>1</span>   <span>0</span>.000s  CLIENT-<span>&gt;</span>SERVER   <span>36</span> bytes
</span><span>  <span>2</span>   <span>0</span>.007s  SERVER-<span>&gt;</span>CLIENT  <span>564</span> bytes
</span><span>  <span>3</span>   <span>0</span>.015s  CLIENT-<span>&gt;</span>SERVER    <span>0</span> bytes
</span><span>  <span>4</span>   <span>0</span>.015s  CLIENT-<span>&gt;</span>SERVER   <span>36</span> bytes
</span><span>  <span>5</span>   <span>0</span>.015s  SERVER-<span>&gt;</span>CLIENT   <span>36</span> bytes
</span><span>  <span>6</span>   <span>0</span>.026s  CLIENT-<span>&gt;</span>SERVER    <span>0</span> bytes
</span><span>  <span>7</span>   <span>0</span>.036s  CLIENT-<span>&gt;</span>SERVER   <span>36</span> bytes
</span><span>  <span>8</span>   <span>0</span>.036s  SERVER-<span>&gt;</span>CLIENT   <span>36</span> bytes
</span><span>  <span>9</span>   <span>0</span>.046s  CLIENT-<span>&gt;</span>SERVER    <span>0</span> bytes
</span><span> <span>10</span>   <span>0</span>.059s  CLIENT-<span>&gt;</span>SERVER   <span>36</span> bytes
</span></code></pre>
<p>I said a “few” because there are a <em>lot</em> of these lines.</p>
<pre><code><span>$ ./summarize_pcap.sh single-key.pcap
</span><span>Total packets: <span>270</span>
</span><span>
</span><span>  <span>36</span>-byte msgs:   <span>179</span> packets <span>(</span> <span>66.3</span>%<span>)</span>   <span>6444</span> bytes
</span><span>  Other data:       <span>1</span> packet  <span>(</span>  <span>0.4</span>%<span>)</span>    <span>564</span> bytes
</span><span>  TCP ACKs:        <span>90</span> packets <span>(</span> <span>33.3</span>%<span>)</span>
</span><span>
</span><span>  Data sent:      <span>6444</span> bytes <span>in</span> <span>36</span>-byte messages,  <span>564</span> bytes <span>in</span> other data
</span><span>  Ratio:          <span>11</span>.4x <span>more</span> data <span>in</span> <span>36</span>-byte messages than other data
</span><span>
</span><span>  Data packet rate: ~90 packets/second <span>(</span>avg <span>11.1</span> ms between data packets<span>)</span>
</span></code></pre>
<p>That is a lot of packets for one keypress. What’s going on here? Why do I care?</p>
<details><summary><svg xmlns="http://www.w3.org/2000/svg" width="20px" height="20px" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg> <!-- -->here's those scripts if you're curious</summary><pre><code><span><span># first_lines_of_pcap.sh</span>
</span><span>tshark <span>-r</span> <span>"<span>$1</span>"</span> <span>\</span>
</span><span>  <span>-T</span> fields <span>-e</span> frame.number <span>-e</span> frame.time_relative <span>-e</span> ip.src <span>-e</span> ip.dst <span>-e</span> tcp.len <span>|</span> <span>\</span>
</span><span>  <span>awk</span> <span>'NR&lt;=10 {dir = ($3 ~ /71\.190/ ? "CLIENT-&gt;SERVER" : "SERVER-&gt;CLIENT");
</span></span><span><span>       printf "%3d  %6.3fs  %-4s  %3s bytes\n", $1, $2, dir, $5}'</span>
</span></code></pre><pre><code><span><span># summarize_pcap.sh</span>
</span><span>tshark <span>-r</span> <span>"<span>$1</span>"</span> <span>-Y</span> <span>"frame.time_relative &lt;= 2.0"</span> <span>-T</span> fields <span>-e</span> frame.time_relative <span>-e</span> tcp.len <span>|</span> <span>awk</span> <span>'
</span></span><span><span>  {
</span></span><span><span>      count++
</span></span><span><span>      payload = $2
</span></span><span><span>
</span></span><span><span>      if (payload == 0) {
</span></span><span><span>          acks++
</span></span><span><span>      } else if (payload == 36) {
</span></span><span><span>          mystery++
</span></span><span><span>          if (NR &gt; 1 &amp;&amp; prev_data_time &gt; 0) {
</span></span><span><span>              delta = $1 - prev_data_time
</span></span><span><span>              sum_data_deltas += delta
</span></span><span><span>              data_intervals++
</span></span><span><span>          }
</span></span><span><span>          prev_data_time = $1
</span></span><span><span>      } else {
</span></span><span><span>          game_data++
</span></span><span><span>          game_bytes = payload
</span></span><span><span>          if (NR &gt; 1 &amp;&amp; prev_data_time &gt; 0) {
</span></span><span><span>              delta = $1 - prev_data_time
</span></span><span><span>              sum_data_deltas += delta
</span></span><span><span>              data_intervals++
</span></span><span><span>          }
</span></span><span><span>          prev_data_time = $1
</span></span><span><span>      }
</span></span><span><span>  }
</span></span><span><span>  END {
</span></span><span><span>      print "Total packets:", count
</span></span><span><span>      print ""
</span></span><span><span>      printf "  36-byte msgs:   %3d packets (%5.1f%%)  %5d bytes\n", mystery, 100*mystery/count, mystery*36
</span></span><span><span>      printf "  Other data:     %3d packet  (%5.1f%%)  %5d bytes\n", game_data, 100*game_data/count, game_bytes
</span></span><span><span>      printf "  TCP ACKs:       %3d packets (%5.1f%%)\n", acks, 100*acks/count
</span></span><span><span>      print ""
</span></span><span><span>      printf "  Data sent:      %d bytes in 36-byte messages,  %d bytes in other data\n", mystery*36, game_bytes
</span></span><span><span>      printf "  Ratio:          %.1fx more data in 36-byte messages than other data\n", (mystery*36)/game_bytes
</span></span><span><span>      print ""
</span></span><span><span>      avg_ms = (sum_data_deltas / data_intervals) * 1000
</span></span><span><span>      printf "  Data packet rate: ~%d packets/second (avg %.1f ms between data packets)\n", int(1000/avg_ms + 0.5), avg_ms
</span></span><span><span>  }'</span>
</span></code></pre></details>
<!-- -->
<h2 id="toc:discovery">Discovery</h2>
<p>I am working on a high-performance game that runs over ssh. The TUI for the game is created in <a href="https://github.com/charmbracelet/bubbletea">bubbletea</a> <a>1</a> and sent over ssh via <a href="https://github.com/charmbracelet/wish">wish</a>.</p>
<div data-is-footnote="true"><p><span>1</span></p><p>I have also forked bubbletea to make it faster. Stay tuned!</p></div>
<p>The game is played in an 80x60 window that I update 10 times a second. I’m targeting at least 2,000 concurrent players, which means updating ~100 million cells a second. I care about performance.</p>
<p>So I have a script that connects a few hundred bots over ssh and has them make a move a second. Then I use go’s <a href="https://pkg.go.dev/net/http/pprof">outstanding profiling tools</a> to look at what’s going on.</p>
<p>Yesterday I inadvertently broke my test harness. Instead of regularly sending game data, my server sent the bots a single message that said “your screen is too small.” This cut my game’s CPU and bandwidth usage in half.</p>
<p>At first I was disappointed. I (briefly) thought I had a free massive speedup on my hands, but it was actually a testing error.</p>
<p>But wait.</p>
<p><em>If I wasn’t sending game data back to my bots, why did CPU usage drop by 50% instead of 100%?</em></p>
<h2 id="toc:investigation">Investigation</h2>
<p>As part of debugging the test harness issue, I used <code>tcpdump</code> to log game traffic with and without the breaking change. Something like:</p>
<pre><code><span><span># The game runs on port 22</span>
</span><span><span>timeout</span> 30s tcpdump <span>-i</span> eth0 <span>'port 22'</span> <span>-w</span> with-breaking-change.pcap
</span><span><span># Revert change</span>
</span><span><span>timeout</span> 30s tcpdump <span>-i</span> eth0 <span>'port 22'</span> <span>-w</span> without-breaking-change.pcap
</span></code></pre>
<p>Our breaking change stopped us from rendering our game over ssh. So <code>with-breaking-change.pcap</code> contains packets that represent the <em>overhead</em> of each connection without actually rendering the game.</p>
<p>I was debugging this with Claude Code, so I asked it to summarize what it saw in the pcap.</p>
<pre><code><span>Wanna take a look yourself? I put with-breaking-change.pcap in this directory
</span><span>
</span><span>--
</span><span>
</span><span>Wow! Here's what I found:
</span><span>
</span><span>Packet Size Distribution (413,703 total packets):
</span><span>274,907 packets (66%): Exactly 36 bytes
</span><span>138,778 packets (34%): 0 bytes (TCP ACKs)
</span><span>18 packets (&lt;0.1%): 72 bytes
</span></code></pre>
<p>Further analysis on a smaller pcap pointed to these mysterious packets arriving ~20ms apart.</p>
<p>This was baffling to me (and to Claude Code). We kicked around several ideas like:</p>
<ul>
<li>SSH flow control messages</li>
<li>PTY size polling or other status checks</li>
<li>Some quirk of bubbletea or wish</li>
</ul>
<p>One thing stood out - these exchanges were initiated by my <em>ssh client</em> (stock ssh installed on MacOS) - not by my server.</p>
<p>On a hunch, I took a <code>tcpdump</code> of a regular ssh session.</p>
<pre><code><span><span># on my mac, in one tab</span>
</span><span><span>sudo</span> tcpdump <span>-ien0</span> <span>'port 22'</span>
</span><span>
</span><span><span># on my mac, in another tab</span>
</span><span><span>ssh</span> <span>$some_vm_of_mine</span>
</span></code></pre>
<p>I waited for the initial connection chatter to die down, sent one keystroke to my remote vm, and looked at the <code>tcpdump</code> output.</p>
<p>I saw the exact same pattern! What in the world?</p>
<h2 id="toc:root-cause">Root cause</h2>
<p>Once I realized that this was a property of stock ssh and not my game, debugging got a lot easier.</p>
<p>Running <code>ssh -vvv</code> gave me a pretty good sense of what was going on:</p>
<pre><code><span>debug3: obfuscate_keystroke_timing: starting: interval ~20ms
</span><span>debug3: obfuscate_keystroke_timing: stopping: chaff time expired (49 chaff packets sent) 
</span><span>debug3: obfuscate_keystroke_timing: starting: interval ~20ms
</span><span>debug3: obfuscate_keystroke_timing: stopping: chaff time expired (101 chaff packets sent)
</span></code></pre>
<p>That <code>20ms</code> is a smoking gun - it lines up perfectly with the mysterious pattern we saw earlier! And the rest of the message is pretty helpful too - we sent 49 “chaff” packets for the first keystroke and 101 “chaff” for around the second one.</p>
<p>In 2023, ssh added <a href="https://undeadly.org/cgi?action=article;sid=20230829051257">keystroke timing obfuscation</a>. The idea is that the speed at which you type different letters betrays some information about which letters you’re typing. So ssh sends lots of “chaff” packets along with your keystrokes to make it hard for an attacker to determine when you’re actually entering keys.</p>
<p>That makes a lot of sense for regular ssh sessions, where privacy is critical. But it’s a lot of overhead for an open-to-the-whole-internet game where <em>latency</em> is critical.</p>

<p>Keystroke obfuscation can be disabled client-side. After reverting my original breaking change, I tried updating my test harness to pass <code>ObscureKeystrokeTiming=no</code> when starting up ssh sessions.</p>
<p>This worked great. CPU usage dropped dramatically and bots still received valid data.</p>
<p>But this is hardly a solution in the real world. I want <code>ssh mygame</code> to Just Work without asking users to pass options that they might not understand.</p>
<p>Claude Code originally didn’t have much faith that we could disable this functionality server-side.</p>
<div><p><img alt="Screenshot of a claude code transcript. Full text:
User
Is there any way to disable responding to the chaff server-side?

Assistant
Probably not. Here's why:

The chaff packets are legitimate SSH protocol messages (likely SSH_MSG_CHANNEL_WINDOW_ADJUST). The server must respond to maintain the SSH connection according to the protocol spec - if it ignored them, the connection would stall or break.
" loading="lazy" width="1678" height="602" decoding="async" data-nimg="1" src="https://eieio.games/images/ssh-sends-100-packets-per-keystroke/probably-not.webp"></p><p>generated with simon wilson's excellent claude-code-transcripts tool</p></div>
<p>Fortunately, the <a href="https://undeadly.org/cgi?action=article&amp;sid=20230829051257">description</a> I found of SSH keystroke obfuscation made it easy to look up the relevant code in go’s ssh library (which I was transitively depending on).</p>
<pre><code><span>Log message:
</span><span>Introduce a transport-level ping facility
</span><span>
</span><span>This adds a pair of SSH transport protocol messages SSH2_MSG_PING/PONG
</span><span>to implement a ping capability. These messages use numbers in the "local
</span><span>extensions" number space and are advertised using a "<a href="https://eieio.games/cdn-cgi/l/email-protection" data-cfemail="c3b3aaada483acb3a6adb0b0abeda0acae">[email&nbsp;protected]</a>"
</span><span>ext-info message with a string version number of "0".
</span></code></pre>
<p>The “chaff” messages that ssh uses to obscure keystrokes are SSH2_MSG_PING messages. And they’re sent to servers that advertise the availability of the <code><a href="https://eieio.games/cdn-cgi/l/email-protection" data-cfemail="e7978e8980a78897828994948fc984888a">[email&nbsp;protected]</a></code> extension. What if we just…don’t advertise <code><a href="https://eieio.games/cdn-cgi/l/email-protection" data-cfemail="24544d4a43644b54414a57574c0a474b49">[email&nbsp;protected]</a></code>?</p>
<p>I searched <a href="https://cs.opensource.google/go/x/crypto/+/master:ssh/">go’s ssh library</a> for <code><a href="https://eieio.games/cdn-cgi/l/email-protection" data-cfemail="ef9f868188af809f8a819c9c87c18c8082">[email&nbsp;protected]</a></code> and found <a href="https://cs.opensource.google/go/x/crypto/+/833695f0a57b3037385dc9c0073bc88773cae6f3">the commit where support was added</a>. The commit was tiny and seemed <em>very</em> easy to revert.</p>
<p>I cloned the go crypto repo and told Claude to revert this change and update our dependencies to use our clone (<a href="https://go.dev/ref/mod#go-mod-file-replace">go’s replace directive</a> makes forking a library very easy).</p>
<p>Then I re-ran my test harness. The results were…very good:</p>
<pre><code><span>Total CPU  29.90%          -&gt; 11.64%
</span><span>Syscalls   3.10s           -&gt; 0.66s
</span><span>Crypto     1.6s            -&gt; 0.11s
</span><span>Bandwidth  ~6.5 Mbit/sec   -&gt; ~3 Mbit/sec
</span></code></pre>
<p>Claude was also pretty pumped:</p>
<div><p><img alt="Chat message from claude code. Full text:
HOLY COW! Look at that CPU usage:

Duration: 30.15s, Total samples = 3.51s (11.64%)
" loading="lazy" width="1658" height="290" decoding="async" data-nimg="1" src="https://eieio.games/images/ssh-sends-100-packets-per-keystroke/claude-pumped.webp"></p><p>yes it's 1:30 am what of it</p></div>
<p>Obviously forking go’s crypto library is a little scary, and I’m gonna have to do some thinking about how to maintain my little patch in a safe way.</p>
<p>But this is a <em>huge</em> improvement. I’ve spent much of the last week squeezing out small single-digit performance wins. A &gt;50% drop was unimaginable to me.</p>
<h2 id="toc:debugging-with-llms-was-fun">Debugging with LLMs was fun</h2>
<p>I’ve been thinking about <a href="https://eieio.games/blog/will-ai-pet-my-dog-for-me/">whether LLMs remove parts of the problem-solving process that I enjoy</a>. But I’ve gotta say, debugging this problem using Claude Code was super fun.</p>
<p>I am familiar enough with <code>tcpdump</code>, <code>tshark</code>, and friends to know what they can do. But I don’t use them regularly enough to be fast with them. Being able to tell an agent “here’s a weird pcap - tell me what’s going on” was really lovely. And by watching commands as the agent ran them I was able to keep my mental model of the problem up to date.</p>
<p>There were still edge cases. At some point in my confusion I switched to ChatGPT  and it <em>very</em> confidently told me that my tcpdump output was normal ssh behavior:</p>
<div><p><img alt="ChatGPT message. Full text:
Yeah, that trace looks wild at first glance, but it’s mostly “normal” SSH/TCP behavior plus the fact that SSH is optimized for latency, not packet efficiency.

Let me unpack what you’re seeing and why it’s chewing CPU.

1. What those tcpdump lines actually are

From your snippet:
" loading="lazy" width="1350" height="464" decoding="async" data-nimg="1" src="https://eieio.games/images/ssh-sends-100-packets-per-keystroke/chatgpt-confident.webp"></p><p>do all chatgpt messages have this tone and formatting now?</p></div>
<p>And then doubled down when I pushed back:</p>
<div><p><img alt="ChatGPT message. Full text:
Thought for 42s

Gotcha, that context helps a lot.

Short version:
What you’re seeing is almost certainly a ton of tiny writes being turned into a ton of tiny SSH records, not some special “per-keypress flow-control storm” in SSH itself.
" loading="lazy" width="1350" height="350" decoding="async" data-nimg="1" src="https://eieio.games/images/ssh-sends-100-packets-per-keystroke/chatgpt-confident2.webp"></p><p>no!!!</p></div>
<p>Similarly, I had to push Claude Code to consider forking go’s ssh library. And I had to make the original leap of “wait…if our test harness was broken, why was usage not 0%?”</p>
<p>When you say “LLMs did not fully solve this problem” some people tend to respond with “you’re holding it wrong!”</p>
<p>I think they’re sometimes right! Interacting with LLMs is a new skill, and it feels pretty weird if you’re used to writing software like it’s 2020. A more talented user of LLMs may have trivially solved this problem.</p>
<p>But the best way to develop a skill is by practicing it. And for me, that means figuring out how to transfer my problem-solving intuitions to the tools that I’m using.</p>
<p>Besides. Being in the loop is fun. How else would I write this post?</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Active' sitting is better for brain health: review of studies (112 pts)]]></title>
            <link>https://www.sciencealert.com/not-all-sitting-is-equal-one-type-was-just-linked-to-better-brain-health</link>
            <guid>46723694</guid>
            <pubDate>Thu, 22 Jan 2026 19:03:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencealert.com/not-all-sitting-is-equal-one-type-was-just-linked-to-better-brain-health">https://www.sciencealert.com/not-all-sitting-is-equal-one-type-was-just-linked-to-better-brain-health</a>, See on <a href="https://news.ycombinator.com/item?id=46723694">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p><a href="https://www.sciencealert.com/excessive-sitting-raises-risk-of-early-death-now-we-know-how-much">Excessive sitting</a> isn't good for a person's physical or mental health, but there's a type of sedentary activity that may not <a href="https://www.sciencealert.com/sitting-could-be-shrinking-your-brain-and-exercise-may-not-help">shrink our brains</a> or cost our cognition to the same extent.</p><p>A systematic review of 85 studies has now found good reason to differentiate between 'active' sitting, like playing cards or reading, and 'passive' sitting, like watching TV.</p><p>The former may actually boost brain health.</p><p>That's probably because active sitting engages the brain, whereas passive sitting lets a person take a back seat both physically and cognitively.</p><p><strong>Related: <a href="https://www.sciencealert.com/scientists-revealed-how-much-exercise-you-need-to-offset-sitting-all-day">Scientists Revealed How Much Exercise You Need to 'Offset' Sitting All Day</a></strong></p><p>"Total sitting time has been shown to be related to brain health; however, sitting is often treated as a single entity, without considering the specific type of activity," <a href="https://news.uq.edu.au/2026-01-not-all-sitting-same-when-it-comes-brain-health">explains</a> public health researcher Paul Gardiner from the University of Queensland in Australia.</p><p>"Most people spend many hours sitting each day, so the type of sitting really matters … These findings show that small everyday choices – like reading instead of watching television – may help keep your brain healthier as you age."</p><p>Obviously, <a href="https://www.sciencealert.com/exercise-boosts-brain-function-across-all-ages-massive-study-confirms">exercise remains incredibly important</a> for cognitive health, but giving your brain a workout is also important, and that doesn't necessarily mean you have to be on your feet.</p><figure id="attachment_189878" aria-describedby="caption-attachment-189878"><img decoding="async" src="https://www.sciencealert.com/images/2026/01/people_playing_cards_642.jpg" alt="three men at a table playing a card game" width="642" height="400" srcset="https://www.sciencealert.com/images/2026/01/people_playing_cards_642.jpg 642w, https://www.sciencealert.com/images/2026/01/people_playing_cards_642-600x374.jpg 600w" sizes="(max-width: 642px) 100vw, 642px" loading="lazy"><figcaption id="caption-attachment-189878">'Active' sitting is better for your brain than passive forms of sitting. (<a href="https://www.canva.com/photos/MAGiNppUFiI/">Drs Producoes/Getty Images/Canva</a>)</figcaption></figure><p>Across numerous studies, Gardiner and colleagues found that active sitting activities, like reading, playing card games, and using a computer, showed "overwhelmingly positive associations with cognitive health, enhancing cognitive functions such as executive function, situational memory, and working memory."</p><p>Meanwhile, passive sitting was most consistently associated with negative cognitive outcomes, including increased <a href="https://www.sciencealert.com/major-study-links-15-factors-to-early-dementia-risk">risk of dementia</a>.</p><p><a href="https://www.sciencealert.com/castingcall?utm_source=SA_website&amp;utm_medium=article&amp;utm_campaign=casting_call"><img decoding="async" src="https://www.sciencealert.com/images/2026/01/Mid-Article-Promos-Casting-Call-1-642x272.jpg" alt="Audition now for ScienceAlert's Casting Call" width="642" height="272" srcset="https://www.sciencealert.com/images/2026/01/Mid-Article-Promos-Casting-Call-1-642x272.jpg 642w, https://www.sciencealert.com/images/2026/01/Mid-Article-Promos-Casting-Call-1-1200x509.jpg 1200w, https://www.sciencealert.com/images/2026/01/Mid-Article-Promos-Casting-Call-1-768x326.jpg 768w, https://www.sciencealert.com/images/2026/01/Mid-Article-Promos-Casting-Call-1-600x255.jpg 600w, https://www.sciencealert.com/images/2026/01/Mid-Article-Promos-Casting-Call-1.jpg 1266w" sizes="(max-width: 642px) 100vw, 642px" loading="lazy"></a></p><p>The effect sizes were small but significant. The study authors hope their results will help inform future health research and more nuanced health guidance.</p><p>For example, the researchers suggest guidelines should recognize the difference between passively watching TV and actively using a computer, and encourage people to take <a href="https://www.sciencealert.com/exercise-snacks-throughout-your-day-have-real-health-benefits-study-finds">short breaks</a> to stimulate their brains and move.</p><p>Their review focused on studies of typical sedentary activities in natural settings, rather than structured programs designed to boost brain function, making it relevant to people's everyday lives.</p><p>"Health advice could shift from simply saying 'sit less' to encouraging more mentally engaging activities while sitting," <a href="https://news.uq.edu.au/2026-01-not-all-sitting-same-when-it-comes-brain-health">argues</a> Gardiner.</p><p>"This could help people make easy, realistic changes that support long‑term brain health and potentially reduce dementia risk."</p><p>The study was published in the <em><a href="https://doi.org/10.1177/13872877251394751">Journal of Alzheimer's Disease</a></em>.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I was banned from Claude for scaffolding a Claude.md file? (544 pts)]]></title>
            <link>https://hugodaniel.com/posts/claude-code-banned-me/</link>
            <guid>46723384</guid>
            <pubDate>Thu, 22 Jan 2026 18:38:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hugodaniel.com/posts/claude-code-banned-me/">https://hugodaniel.com/posts/claude-code-banned-me/</a>, See on <a href="https://news.ycombinator.com/item?id=46723384">Hacker News</a></p>
Couldn't get https://hugodaniel.com/posts/claude-code-banned-me/: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[Recent discoveries on the acquisition of the highest levels of human performance (133 pts)]]></title>
            <link>https://www.science.org/doi/abs/10.1126/science.adt7790</link>
            <guid>46722853</guid>
            <pubDate>Thu, 22 Jan 2026 18:01:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/doi/abs/10.1126/science.adt7790">https://www.science.org/doi/abs/10.1126/science.adt7790</a>, See on <a href="https://news.ycombinator.com/item?id=46722853">Hacker News</a></p>
Couldn't get https://www.science.org/doi/abs/10.1126/science.adt7790: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Macron says €300B in EU savings sent to the US every year will be invested in EU (177 pts)]]></title>
            <link>https://old.reddit.com/r/europe/comments/1qjtvtl/macron_says_300_billion_in_european_savings_flown/</link>
            <guid>46722594</guid>
            <pubDate>Thu, 22 Jan 2026 17:42:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/europe/comments/1qjtvtl/macron_says_300_billion_in_european_savings_flown/">https://old.reddit.com/r/europe/comments/1qjtvtl/macron_says_300_billion_in_european_savings_flown/</a>, See on <a href="https://news.ycombinator.com/item?id=46722594">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>He played into the hands of the far right, led a violent crackdown on protests (dozens blinded or maimed, thousands injured), and consistently favors billionaires who have become incredibly wealthy since he became president (his wife is, incidentally, very close friends with the wife of the richest man in France). Our hospitals, schools, and universities are getting worse and worse. He has also betrayed his environmental promises (as soon as farmers protest, he silences them by dismantling environmental regulations). Overall, he has betrayed his initial, rather social-democratic, commitments. He is significantly "normalizing" the far right. One of our recent interior ministers is clearly a Trump supporter, by the way. Oh, and he sold many of our companies to the US (like Alstom). Overall, he still favors the rich and retirees (he's very close to the rich, and retirees vote a lot).</p>

<p>That said, we're rather pleased with what he's doing in the face of Trump. But we haven't forgotten the broken promises, nor his very ambiguous behavior towards the far right in our country. Basically, his game is to boost the far right, then find himself facing them in the elections so we're forced to vote for his party to "block" them.</p>

<p>That's why the French are angry.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CSS Optical Illusions (185 pts)]]></title>
            <link>https://alvaromontoro.com/blog/68091/css-optical-illusions</link>
            <guid>46722570</guid>
            <pubDate>Thu, 22 Jan 2026 17:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alvaromontoro.com/blog/68091/css-optical-illusions">https://alvaromontoro.com/blog/68091/css-optical-illusions</a>, See on <a href="https://news.ycombinator.com/item?id=46722570">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>You can find a collection with <a href="https://codepen.io/collection/GpWqKk" target="_blank" rel="noopener noreferrer">all the optical illusions in this article (and more!) on CodePen</a>. You can move your mouse over many of the demos below to reveal the effect or stop the animations.</p>

<h2 id="poggerndorff-illusions">
  1&nbsp;-&nbsp;Poggendorff Illusions
</h2>

<p>The Poggendorff illusion is an optical illusion in which a diagonal line interrupted by a vertical bar appears misaligned, even when both segments are actually continuous.</p>

<p>A simple version of this effect can be seen in the following demo. I used the&nbsp;<code>::before</code> and&nbsp;<code>::after</code> pseudo-elements to create the diagonal line and the vertical bar, respectively.</p>



<p>The effect can also be seen in a more elaborate version with multiple diagonal lines and vertical bars:</p>



<p>This drawing can easily be achieved using two CSS gradients: one tilted at 70 degrees and another consisting of a series of vertical columns. I applied it to the <code>body</code>, although I could have used&nbsp;<code>:root</code> instead.</p>

<p>Another variation of this illusion is the Münsterberg Poggendorff Arch, in which the two sides of an arch appear misaligned and seem as though they will not meet at the top&nbsp;-&nbsp;but they do (mouse over to see it).</p>



<h2 id="induced-gradients">
  2&nbsp;-&nbsp;Induced Gradients
</h2>

<p>The following illusions combine gradients and flat colors. Surprisingly, some of the gradients do not actually exist. They are simple gray bars that, when placed over a gradient, appear to have gradients themselves.</p>

<p>Take the following demo: all three bars (two vertical ones on the sides and one horizontal bar in the center) are the same shade of gray. The only real gradient is behind them, which tricks our brain into believing that the bars are different colors and even contain gradients.</p>



<p>Here is another variation of this effect. It looks like the central line has a repeating gradient of dark and light grays, but in reality it is a flat color. If you mouse over the demo, the bar will expand, making it clear that there is no gradient at all.</p>



<h2 id="cornsweet-illusion">
  3&nbsp;-&nbsp;Cornsweet Illusion
</h2>

<p>The next few optical illusions share a common idea: some colors are identical, but they do not look the same. This typically happens when regions of the same color or brightness are surrounded by areas with different contrast.</p>

<p>For example, in the following demo, the left and right ends are the same shade of gray. However, one looks lighter because it is closer to white, while the other looks darker because it is closer to black. Mouse over to reveal that they are, in fact, the same color.</p>



<h2 id="whites-illusion">
  4&nbsp;-&nbsp;White's&nbsp;Illusion
</h2>

<p>Run the following demo. You will see two gray columns in a black-and-white grid. Both columns are the same shade of gray, but the one surrounded by black appears darker than the one surrounded by white.</p>



<p>I coded this demo using <code>mix-blend-mode</code> so I could try something a bit different. That worked well, but it also made it harder to showcase the effect on hover. In hindsight, I should have planned that better.</p>

<p>This optical illusion also works with colors. For example, these two squares appear to be different shades of blue, but they are the same color. This time, you can mouse over to reveal the effect:</p>



<h2 id="wertheimer-koffka-ring">
  5&nbsp;-&nbsp;Wertheimer-Koffka Ring
</h2>

<p>The ring in the following illustration has the same color all the way around. However, one side is placed over white and the other over black, which makes them look different. If you mouse over the demo, the red bar will disappear, making it more obvious that the ring is a single, uniform color.</p>



<h2 id="adelsons-illusion">
  6&nbsp;-&nbsp;Adelson's Illusion
</h2>

<p>You have probably seen the illusion involving a checkerboard and an object casting a shadow, where two tiles&nbsp;-&nbsp;one seemingly light and one seemingly dark&nbsp;-&nbsp;turn out to be the same color.</p>

<p>This demo follows the same principle. You will see two tiles labeled A and B. Both have the same shade of gray, but most people cannot tell at first glance (or second, or even third).</p>



<h2 id="asahi-illusion-of-brightness">
  7&nbsp;-&nbsp;Asahi illusion of Brightness
</h2>

<p>The circle at the center of this flower-shaped element is the same white as the rest of the page, but it gives the impression of being brighter, as if it were emitting light.</p>



<h2 id="color-spheres">
  8&nbsp;-&nbsp;Color&nbsp;Spheres
</h2>

<p>This is one of my favorite illusions in the collection. The circles (or spheres) look red, blue, or green, but in reality they are all the same grayish color. Our brain "colorizes" them based on the lines that overlap the shapes. Don't believe it? Mouse over the illustration.</p>



<h2 id="colors-from-contour">
  9&nbsp;-&nbsp;Colors from&nbsp;Contour
</h2>

<p>In the following illustration, the lines inside the yellow section appear blue, while the lines inside the blue section appear red... but they are all black (or very dark gray). The white contour creates the illusion of color. Mouse over to remove the contour and the lines will clearly appear black.</p>



<h2 id="curvature-blindness">
  10&nbsp;-&nbsp;Curvature Blindness
</h2>

<p>One set of lines looks straighter (top) while the other looks more curved (bottom). In reality, both sets are equally wavy. The only difference is how they are colored: changing the color at the peaks makes the lines look straighter. Changing it at the inflection points makes them look more curved.</p>

<p>The CSS code for the wavy lines is adapted from a <a href="https://css-tricks.com/how-to-create-wavy-shapes-patterns-in-css/" target="_blank" rel="noopener noreferrer">Temani Afif snippet on CSS-Tricks</a> and his <a href="https://css-generators.com/wavy-shapes/" target="_blank" rel="noopener noreferrer">wavy shape generator</a>.</p>



<h2 id="cafe-wall-illusion">
  11&nbsp;-&nbsp;Cafe&nbsp;Wall
</h2>

<p>This is a classic optical illusion and an easy one to code in CSS. Three gradients are all that is needed to generate the effect in which the horizontal lines appear slanted, even though they are perfectly parallel.</p>



<h2 id="penrose-triangle">
  12&nbsp;-&nbsp;Penrose&nbsp;Triangle
</h2>

<p>This optical illusion depicts an impossible shape. Parts that should be in front appear in the back, top becomes right, and everything feels contradictory. I coded this one some time ago for the 2024 Divtober event.</p>



<h2 id="ebbinghaus-illusion">
  13&nbsp;-&nbsp;Ebbinghaus Illusion
</h2>

<p>Which orange circle is larger: the one on the right or the one on the left? It is a trick question: both are the same size. However, having smaller surrounding elements gives the impression that one is larger.</p>



<p>I also created an animated version of this illusion (see below), as well as another version using a square shape instead of a flower shape:</p>



<h2 id="kanizsa-square">
  14&nbsp;-&nbsp;Kanizsa&nbsp;Square
</h2>

<p>When people look at this illustration, they usually say they see a white square over black circles. However, the square is not actually there. The "Pac-Man" shapes create the illusion of a square and a sense of depth. Our brain fills in the missing information.</p>



<h2 id="ehrensteins-illusion">
  15&nbsp;-&nbsp;Ehrenstein's Illusion
</h2>

<p>There are no circles or discs in this illustration, only vertical and horizontal lines forming crosses. Our visual system completes the shape and makes us perceive a disc that does not exist.</p>



<h2 id="neon-color-spreading-illusion">
  16&nbsp;-&nbsp;Neon-Color-Spreading Illusion
</h2>

<p>This illustration shows concentric circles, some of which have a green-and-black pattern. Our brain perceives a central patterned circle and four concentric circles around it, beneath the green circle.</p>

<p>I cheated a little when creating this in CSS, as I actually used a green circle blended with the other backgrounds.</p>



<h2 id="hering-and-wundt-illusions">
  17&nbsp;-&nbsp;Hering and Wundt Illusions
</h2>

<p>Perspective-based illusions are fascinating. Even when we know we are looking at a flat image, our brain insists on interpreting depth.</p>
<p>In the Hering illusion, the red lines appear to curve outward, even though they are straight.</p>



<p>The <em>opposite</em> effect is the Wundt illusion. When the lines expand from the sides toward the center, the red lines appear to curve inward (this effect is more subtle).</p>



<h2 id="ponzo-illusion">
  18&nbsp;-&nbsp;Ponzo&nbsp;Illusion
</h2>

<p>Both yellow lines are the same length, but the top one looks longer due to perceived depth and perspective. I tried a different approach when coding this one by applying a three-dimensional rotation in CSS... so the perspective is technically real.</p>



<h2 id="t-illusion">
  19&nbsp;- T Illusion
</h2>

<p>This illusion is easy to code in CSS and easy to fall for. Both the vertical and horizontal lines are the same length, but the vertical line appears longer.</p>



<h2 id="mullerlyer-illusion">
  20&nbsp;-&nbsp;Müller-Lyer Illusion
</h2>

<p>A classic illusion: the horizontal lines are the same length, but inward- or outward-pointing edges dramatically change how we perceive them. I could swear the top one is longer. But it is not.</p><p>
From a coding perspective, each shape is a pseudo-element. I ensured the horizontal lines were identical by using the same gradients and only repositioning the edges in the&nbsp;<code>::before</code> and&nbsp;<code>::after</code>.</p>



<h2 id="tilted-table-illusion">
  21&nbsp;-&nbsp;Tilted Table&nbsp;Illusion
</h2>

<p>It looks like the top rectangle is leaning to the left, but it is actually parallel to the one at the bottom. The trick lies in the direction of the diagonal lines used to "color" each rectangle.</p><p>
This illusion works better on larger screens. The effect is diminished when you can see the whole picture.</p>



<h2 id="parallel-lines">
  22&nbsp;-&nbsp;Parallel&nbsp;Lines
</h2>

<p>This is a simple effect: the black lines are parallel, but they appear not to be because of the direction of the bars crossing them.</p><p>
I slightly overcomplicated this one while coding it. I initially built the black-and-red version below and tried to reuse more code than I probably should have.</p>



<p>Here is the original version I created. The effect is also visible there:</p>




<hr>

<p>Good news! There are more optical illusions below&nbsp;-&nbsp;but first, a warning.</p>

<p>
  ATTENTION: The following optical illusions are static, but they give the impression of movement. Proceed accordingly.
</p>

<p>(Leaving some blank space in case you do not want to continue.)</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p>
<h2 id="expanding-hole">
  23&nbsp;-&nbsp;Expanding Hole
</h2>

<p>This is a trippy optical illusion. It is completely static, yet it looks like the black hole at the center is expanding&nbsp;-&nbsp;especially when you are not looking at it directly, creating the sensation of falling into a pit.</p>

<p>From a coding perspective, this one was very simple: a background pattern made with two radial gradients, plus a blurred pseudo-element for the "expanding" hole.</p>



<h2 id="rotating-snakes">
  24&nbsp;-&nbsp;Rotating&nbsp;Snakes
</h2>

<p>This is one of only two optical illusions in this collection where I used HTML elements instead of relying exclusively on CSS. It is a classic effect: when you look at the illustration, the peripheral discs appear to rotate, even though nothing is actually moving.</p>



<h2 id="appearing-dots">
  25&nbsp;-&nbsp;Appearing Dots
</h2>

<p>Another classic illusion. Focus on the white dots and the adjacent dots will appear to turn black. There is no animation, no transition, and nothing dynamic. Just intersecting lines and small white circles, yet it looks like motion.</p>



<h2 id="disappearing-dots">
  26&nbsp;-&nbsp;Disappearing Dots
</h2>

<p>This pattern consists of repeating black and white dots across the page. If you focus on one dot, the others will begin to disappear. At first it may happen by row or column, but after a short while, most of them vanish.</p>

<p>If you do not immediately see the effect, try focusing on one black dot. Mouse over it, wait a few seconds while keeping your focus, and then mouse out.</p>



<h2 id="ouchi-illusion">
  27&nbsp;-&nbsp;Ouchi&nbsp;Illusion
</h2>

<p>This is a static image, but it gives the impression that the pattern inside the circle is moving sideways. This happens because our eyes are constantly making small movements, even when we are not aware of it.</p>

<p>If you cannot see the illusion, try slightly moving the screen (or your head) while looking just outside the circle.</p>



<h2 id="orthogonal-dotted-lines-sway">
  28&nbsp;-&nbsp;Orthogonal Dotted Lines&nbsp;Sway
</h2>

<p>When you look around this pattern, the central area appears to slide and sway, even though it is completely static. This illusion makes me dizzy... but that may also be because I had to stare at it for a long time while coding it.</p>



<h2 id="enigma">
  29&nbsp;-&nbsp;Enigma
</h2>

<p>This illusion is particularly interesting. There is a pink circle surrounded by concentric pink and purple rings. If you focus on the pink circle, the rings appear to spin or scintillate, as if there were some activity in them. Of course, nothing is actually moving.</p>



<h2 id="waves">
  30&nbsp;-&nbsp;Waves
</h2>

<p>This demo was challenging to code and takes a long time to load. Mainly because it uses a large number of conic gradients behind the scenes, which browsers struggle to render efficiently. There is probably a better way to implement it, but I have not explored that yet.</p>

<p>If you look closely at the illustration, you may notice wave-like motion. As with the previous illusions in this section, the image is entirely static.</p>




<hr>

<p>Good news! There are more optical illusions below&nbsp;-&nbsp;but first, another warning.</p>

<p>
  ATTENTION: The following optical illusions actually move, and the illusion is created by motion itself. Some of them can be dizzying, so proceed accordingly.
</p>

<p>(Leaving some blank space in case you do not want to continue.)</p>

<p>.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p>
<h2 id="animated-ebbinghaus-illusion">
  31&nbsp;-&nbsp;Animated Ebbinghaus Illusion
</h2>

<p>Earlier, we saw two static versions of the Ebbinghaus illusion. This one is animated. The elements move side to side, and the surrounding shapes grow and shrink, giving the impression that the orange circle is changing size&nbsp;-&nbsp;when it definitely is not.</p>



<h2 id="psychokinematic-tower">
  32&nbsp;-&nbsp;Psychokinematic Tower
</h2>

<p>This looks like a three-dimensional tower spinning in space, as seen from above. In reality, it is a flat, two-dimensional image rotating.</p><p>
Mouse over the demo to stop the rotation and the illusion of depth disappears entirely.</p>



<h2 id="color-fan">
  33&nbsp;-&nbsp;Color&nbsp;Fan
</h2>

<p>This optical illusion requires only two gradients: a conic gradient for the fan-shaped arms and a radial gradient for the circles and discs.</p>

<p>If you focus on the black dot, the illustration may appear to develop a darker greenish or brownish border. However, the colors never change.</p>



<h2 id="reverse-spoke-illusion">
  34&nbsp;-&nbsp;Reverse Spoke&nbsp;Illusion
</h2>

<p>This illusion is delightful and disorienting. While the background colors of the wheel are spinning, the spokes remain fixed. However, they appear to rotate in the opposite direction. In reality, only the background is moving.</p>



<h2 id="motion-binding">
  35&nbsp;-&nbsp;Motion&nbsp;Binding
</h2>

<p>What do you see in this animation? Most people report two sets of lines operating independently: one moving horizontally and another moving vertically. And that is exactly how it looks.</p>

<p>In reality, it is a single shape moving uniformly. Run the demo, mouse over the lines, and the true motion will be revealed.</p>



<h2 id="mainzlinez-illusion">
  36&nbsp;-&nbsp;Mainz-Linez Illusion
</h2>

<p>Focus on one of the red dots. You will notice it moves straight up and down along a vertical path. Now shift your focus to one of the black crosses in the center. Suddenly, the red dots appear to zigzag instead of moving straight.</p>

<p>The CSS code for the wavy lines is adapted from a <a href="https://css-tricks.com/how-to-create-wavy-shapes-patterns-in-css/" target="_blank" rel="noopener noreferrer">Temani Afif snippet on CSS-Tricks</a> and his <a href="https://css-generators.com/wavy-shapes/" target="_blank" rel="noopener noreferrer">wavy shape generator</a>.</p>



<h2 id="waddling-colors">
  37&nbsp;-&nbsp;Waddling&nbsp;Colors
</h2>

<p>It may look like the boxes are moving at different speeds or like a set of walking feet. In reality, all elements move at the same pace and in parallel. Mouse over the demo to reveal the effect.</p>



<p>The illusion also works when the "feet" move in circles, as shown in this alternative version:</p>



<h2 id="dottedline-motion">
  38&nbsp;-&nbsp;Dotted-Line Motion
</h2>

<p>Follow the red dot as it moves sideways. From the corner of your vision, it may appear that the dashed black-and-white lines are moving closer together (when the dot moves left) or farther apart (when it moves right). In reality, the lines are completely static.</p>



<h2 id="contrast-asynchrony">
  39&nbsp;-&nbsp;Contrast Asynchrony
</h2>

<p>These dots always have the same color. However, when placed against alternating backgrounds, they appear to jump or move out of sync because of how they blend with their surroundings.</p>

<p>Mouse over the demo to remove the background and the illusion disappears.</p>



<h2 id="breathing-square">
  40&nbsp;-&nbsp;Breathing Square
</h2>

<p>This illusion gives the impression that a blue square is growing and shrinking rhythmically, almost as if it were breathing or beating like a heart.</p>

<p>Although the image is rotating, its size never changes. Mouse over the illustration to remove the green boxes and reveal the rotating blue square.</p>



<h2 id="troxler-fading">
  41&nbsp;-&nbsp;Troxler&nbsp;Fading
</h2>

<p>This illustration shows a circle made of pink dots, with one dot missing. Focus on the cross at the center and the missing dot will appear as a yellow or green dot, giving the impression that it is "eating" the pink dots. Just like Pac-Man.</p>

<p>I could have used CSS trigonometric functions to calculate the exact positions of the dots, but since they never change, I chose to hardcode the values instead.</p>



<p>Here is a related effect. Follow the light gray circle as it spins, and the darker circles will appear to change from gray to greenish. Focus on the cross at the center, and after a short time, the darker circles may begin to fade entirely.</p>



<h2 id="pinnabrelstaff-illusion">
  42&nbsp;-&nbsp;Pinna-Brelstaff Illusion
</h2>

<p>This illusion is particularly dizzying. Follow the bluish dot as it moves from right to left and back again. It will appear as though parts of the tiled background are shifting, even though they are static. The only moving element is the dot.</p><p>
From a CSS perspective, I coded the pattern using conic gradients, and applied it to the&nbsp;<code>::before</code> and&nbsp;<code>::after</code> pseudo-elements. I then flipped one upside down and clipped it.</p>



<h2 id="palisade">
  43&nbsp;-&nbsp;Palisade
</h2>

<p>The radii of a wheel, when viewed through a palisade, appear to curve. In reality, they are perfectly straight. Mouse over the demo to remove the palisade and you will see that the radii never bend.</p>



<h2 id="alternative-motion">
  44&nbsp;-&nbsp;Alternative Motion
</h2>

<p>This animation demonstrates how our minds infer motion that may not actually be there. Consider the two blue dots. Different people perceive different movements: side to side, top to bottom, or even circular motion.</p>

<p>Cover the right side of the animation so that you see only one dot at a time. The motion now appears vertical. Cover the bottom part instead, and the motion appears horizontal. This is our brain trying to complete the movement.</p>



<h2 id="motion-inversion">
  45&nbsp;-&nbsp;Motion Inversion
</h2>

<p>These two illustrations are identical&nbsp;-&nbsp;same shapes, same animation. The only difference is the CSS timing function.</p>

<p>The top animation moves smoothly from right to left. The bottom one appears to move choppily in the same direction, but if you focus on it, it may suddenly seem to reverse direction and move faster.</p>




<hr>

<p>Most of the inspiration for these optical illusions came from two excellent resources:</p>

<ul>
<li>"<a href="https://www.livescience.com/health/mind/32-optical-illusions-and-why-they-trick-your-brain" target="_blank" rel="noopener noreferrer">35 optical illusions and why they trick your brain</a>" by Patrick Pester.</li>
<li>"<a href="https://michaelbach.de/ot/" target="_blank" rel="noopener noreferrer">154 Visual Phenomena &amp; Optical Illusions</a>" with explanations by Michael Bach</li>
</ul>

<hr>

<p>You can also find this article on:</p>

<ul>
  <li><a href="https://alvaromontoro.medium.com/css-optical-illusions-visual-illusions-built-with-pure-css-and-html-bde6f8916b04?postPublishedType=initial">Medium</a></li>
  <li><a href="https://dev.to/alvaromontoro/css-optical-illusions-50-visual-illusions-built-with-pure-css-and-html-13o9">DEV</a> (open)</li>
</ul>

<p>(You can leave comments on those platforms and I will reply there).</p>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: isometric.nyc – giant isometric pixel art map of NYC (935 pts)]]></title>
            <link>https://cannoneyed.com/isometric-nyc/</link>
            <guid>46721802</guid>
            <pubDate>Thu, 22 Jan 2026 16:52:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cannoneyed.com/isometric-nyc/">https://cannoneyed.com/isometric-nyc/</a>, See on <a href="https://news.ycombinator.com/item?id=46721802">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Ubisoft cancels six games including Prince of Persia and closes studios (104 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/c6200g826d2o</link>
            <guid>46721452</guid>
            <pubDate>Thu, 22 Jan 2026 16:29:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/c6200g826d2o">https://www.bbc.co.uk/news/articles/c6200g826d2o</a>, See on <a href="https://news.ycombinator.com/item?id=46721452">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p><b>Ubisoft has cancelled six video games - including its long-awaited Prince of Persia: The Sands of Time remake - as part of a "major reset" of its operations.</b></p><p>The French developer and publisher, known for popular games such as Assassin's Creed, Far Cry and Just Dance, has closed two studios and delayed seven titles as part of its changes.</p><p>Ubisoft boss Yves Guillemot said the move would "create the conditions for a return to sustainable growth".</p><p>The firm's shares plunged by 33% on Thursday morning following the announcement.</p></div><div data-component="text-block"><p>The move comes at a time when studios are increasingly turning to video game remakes and remasters, with new versions of Super Mario Galaxy, Oblivion and Metal Gear Solid 3 proving popular in 2025.</p><p>So the decision to bin the remake of Sands of Time - which sold millions of copies in 2003 - has left many fans scratching their heads.</p><p>Ubisoft has not specified which titles it has discontinued alongside the Prince of Persia remake.</p><p>But it says among them are four unannounced titles, including three based on new intellectual property, and a mobile game.</p><p>Ubisoft has closed its studios in Stockholm, Sweden and Halifax, Canada as part of the move, which will include restructuring three others.</p><p>The developers were working on a new intellectual property (IP) and mobile titles for Assassin's Creed, respectively.</p><p>The closure of Ubisoft Halifax was previously announced in January - the same week the studio formed a union. </p><p> "While these decisions are difficult, they are necessary for us to build a more focused, efficient and sustainable organisation over the long term," Guillemot said.</p><p>"Taken together, these measures mark a decisive turning point for Ubisoft and reflect our determination to confront challenges head-on to reshape the Group for the long term".</p><p>Gaming industry analyst Piers Harding-Rolls told the BBC the move indicated the firm was trying to mitigate risk.</p><p>"It's less risky to maintain scale by investing in existing big franchises such as Assassin's Creed and Rainbow Six than launch entirely new IPs and that's reflected in the cancellation of a number of games based on new IPs," he said.</p></div><div data-component="text-block"><p>It is the second restructure from Ubisoft in subsequent years, after the firm cut 185 jobs across Europe in 2025.</p><p>In the UK, it <a href="https://www.bbc.co.uk/news/articles/cx2jxzjgpplo">closed its office in Leamington</a> and restructured another in Newcastle-upon-Tyne.</p><p>Guillemot said the new move was in part due to a competitive market for triple-A games - blockbuster titles which cost millions to develop and dominate large studios' offerings.</p><p>"On the one hand, the triple-A industry has become persistently more selective and competitive with rising development costs and greater challenges in creating brands," he said <a href="https://staticctf.ubisoft.com/8aefmxkxpxwl/59b4Z0RNls45UDTVV6gBNR/801a91a599b337779106af51cf08e539/Ubisoft_Jan_PR_draft_vF.pdf">in a statement on Wednesday<span>, <!-- -->external</span></a>.</p><p>Such big offerings are increasingly facing delays - with <a href="https://www.bbc.co.uk/news/articles/cwypgvd9j8qo">Grand Theft Auto VI pushed back for a second time to November 2026</a>.</p><p>But Guillemot said despite these hurdles, successful blockbuster games could bring "more financial potential than ever".</p><p>"In this context, today we are announcing a major reset built to create the conditions for a return to sustainable growth over time," he said.</p><p>Ubisoft will now focus on developing open world adventure games - which let players freely navigate vast environments - and live service games which seek regular payments from players.</p><p>The firm said its subsidiary Vantage Studios, created after <a href="https://www.bbc.co.uk/news/articles/cy4l4y0yd03o">a €1.25bn ($1.25bn; £1bn) investment from Chinese tech giant Tencent</a>, would aim to turn Assassin's Creed, Far Cry and Rainbow Six "into annual billionaire brands".</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It looks like the status/need-triage label was removed (287 pts)]]></title>
            <link>https://github.com/google-gemini/gemini-cli/issues/16728</link>
            <guid>46721179</guid>
            <pubDate>Thu, 22 Jan 2026 16:10:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google-gemini/gemini-cli/issues/16728">https://github.com/google-gemini/gemini-cli/issues/16728</a>, See on <a href="https://news.ycombinator.com/item?id=46721179">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><h3 dir="auto">What would you like to be added?</h3>
<p dir="auto">Adds native recognition for JetBrains IDE as a supported IDE environment.</p>
<h3 dir="auto">Why is this needed?</h3>
<p dir="auto">Currently, Gemini CLI restricts IDE integration features to environments where TERM_PROGRAM is vscode (or other hardcoded values). This forces 3rd-party integrations like <a href="https://github.com/SoLoHiC/jetbrains-ide-companion">jetbrains-ide-companion</a> to mock VS Code by spoofing environment variables to enable core features, otherwise it could not be discovered by Gemini CLI.</p>
<p dir="auto">For some reason, the process detection is not working properly on windows/linux (, reported by users here <a href="https://plugins.jetbrains.com/plugin/29336-gemini-cli-companion/reviews" rel="nofollow">JetBrains Plugin Review</a> and here <a data-error-text="Failed to load title" data-id="3446875726" data-permission-text="Title is private" data-url="https://github.com/google-gemini/gemini-cli/issues/9273" data-hovercard-type="issue" data-hovercard-url="/google-gemini/gemini-cli/issues/9273/hovercard" href="https://github.com/google-gemini/gemini-cli/issues/9273">#9273</a> , and a few other bug report email i've received), which making this native IDE detection logic a MUST do for gemini-cli discover and connect to IDE via environmental variables instead of port info file.</p>
<p dir="auto">This PR adds JetBrains IDE Series to the IDE_DEFINITIONS and updates the detection logic to recognize TERMINAL_EMULATOR=JetBrains-JediTerm as a first-class supported environment.</p>
<h3 dir="auto">Additional context</h3>
<p dir="auto">Inspired by <a data-error-text="Failed to load title" data-id="3790058763" data-permission-text="Title is private" data-url="https://github.com/google-gemini/gemini-cli/issues/16083" data-hovercard-type="pull_request" data-hovercard-url="/google-gemini/gemini-cli/pull/16083/hovercard" href="https://github.com/google-gemini/gemini-cli/pull/16083">#16083</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPTZero finds 100 new hallucinations in NeurIPS 2025 accepted papers (840 pts)]]></title>
            <link>https://gptzero.me/news/neurips/</link>
            <guid>46720395</guid>
            <pubDate>Thu, 22 Jan 2026 15:20:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gptzero.me/news/neurips/">https://gptzero.me/news/neurips/</a>, See on <a href="https://news.ycombinator.com/item?id=46720395">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<table xmlns="http://www.w3.org/1999/xhtml" dir="ltr" data-sheets-root="1" data-sheets-baot="1"><colgroup><col width="200"><col width="80"><col width="280"><col width="220"></colgroup><tbody><tr><td><p dir="ltr"><span>Published Paper</span></p></td><td><p dir="ltr"><span>GPTZero Scan</span></p></td><td><p dir="ltr"><span>Example of Verified Hallucination</span></p></td><td><p dir="ltr"><span>Comment</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=FxCy8TvQHO"><span>SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/02dc3e04-2bd6-4ce2-ba22-017b5c925b03/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/3d32225d-7494-4e82-afd6-58a5d399b3af/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>John Doe and Jane Smith. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.00001, 2024.</span></p></td><td><p dir="ltr"><span>Article with a matching title exists</span><a href="https://aclanthology.org/2024.acl-long.371/"><span> </span><span>here</span></a><span>. Authors are obviously fabricated. arXiv ID links to a different</span><a href="https://arxiv.org/abs/2401.00001"><span> </span><span>article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=WQb0YrFl3H"><span>Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based Videoconferencing</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/3043470a-75f9-4803-ac7d-5a8ae833899e/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/cd109d6a-2c6d-4b38-ae39-b0da48a86c43/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>John Smith and Jane Doe. Deep learning techniques for avatar-based interaction in virtual environments. IEEE Transactions on Neural Networks and Learning Systems, 32(12):5600-5612, 2021. doi: 10.1109/ TNNLS.2021.3071234. URL</span><a href="https://ieeexplore.ieee.org/document/307123"><span> </span><span>https://ieeexplore.ieee.org/document/307123</span></a></p></td><td><p dir="ltr"><a href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=9629429&amp;punumber=5962385&amp;sortType=vol-only-seq&amp;pageNumber=2"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>. URL and DOI are fake.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=WQb0YrFl3H"><span>Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based Videoconferencing</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/3043470a-75f9-4803-ac7d-5a8ae833899e/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/cd109d6a-2c6d-4b38-ae39-b0da48a86c43/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Min-Jun Lee and Soo-Young Kim. Generative adversarial networks for hyper-realistic avatar creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1234-1243, 2022. doi: 10.1109/CVPR.2022.001234. URL https://ieeexplore.ieee.org/ document/00123</span></p></td><td><p dir="ltr"><a href="https://openaccess.thecvf.com/CVPR2022"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>. URL and DOI are fake.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=EyOtIOmMUh"><span>SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/aae95ba0-192d-441b-8c93-4342f29cf3aa/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/0fcb90db-35bc-4d22-9b0d-1fd5af82355b/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Firstname Lastname and Others. Drivlme: A large-scale multi-agent driving benchmark, 2023. URL or arXiv ID to be updated.</span></p></td><td><p dir="ltr"><span>No title or author match. Potentially referring to this</span><a href="https://arxiv.org/abs/2406.03008"><span> </span><span>article</span></a><span>, but year is off (2024)</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=EyOtIOmMUh"><span>SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/aae95ba0-192d-441b-8c93-4342f29cf3aa/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/0fcb90db-35bc-4d22-9b0d-1fd5af82355b/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Firstname Lastname and Others. Robotslang: Grounded natural language for multi-robot object search, 2024. To appear.</span></p></td><td><p dir="ltr"><span>No title or author match. Potentially referring to this</span><a href="https://proceedings.mlr.press/v155/banerjee21a/banerjee21a.pdf"><span> </span><span>article</span></a><span>, but year is totally off (2020).</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Nuo Lou and et al. Dsp: Diffusion-based span prediction for masked text modeling. arXiv preprint arXiv:2305.XXXX, 2023.</span></p></td><td><p dir="ltr"><span>No title or author match and arXiv ID is incomplete.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>A. Sahoo and et al. inatk: Iterative noise aware text denoising. arXiv preprint arXiv:2402.XXXX, 2024.</span></p></td><td><p dir="ltr"><span>No title or author match and arXiv ID is incomplete.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Sheng Shi and et al. Maskgpt: Uniform denoising diffusion for language. arXiv preprint arXiv:2401.XXXX, 2024.</span></p></td><td><p dir="ltr"><span>No title or author match and arXiv ID is incomplete.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Asma Issa, George Mohler, and John Johnson. Paraphrase identification using deep contextualized representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 517-526, 2018.</span></p></td><td><p dir="ltr"><span>No author or title match. No match in</span><a href="https://aclanthology.org/events/emnlp-2021/#2021emnlp-main"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yi Tay, Kelvin Fu, Kai Wu, Ivan Casanueva, Jianfeng Liu, Byron Wallace, Shuohang Wang, Bajrang Singh, and Julian McAuley. Reasoning with heterogeneous graph representations for knowledge-aware question answering. In Findings of the Association for Computational Linguistics: ACL 2021, pp. 3497-3506, 2021.</span></p></td><td><p dir="ltr"><span>No exact author or title match, although this title is</span><a href="https://aclanthology.org/2023.findings-emnlp.906.pdf?utm_source=consensus"><span> close.</span></a><span> No match in the</span><a href="https://aclanthology.org/events/findings-2021/"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Alex Wang, Rishi Bommasani, Dan Hendrycks, Daniel Song, and Zhilin Zhang. Efficient fewshot learning with efl: A single transformer for all tasks. In arXiv preprint arXiv:2107.13586, 2021.</span></p></td><td><p dir="ltr"><span>No title or author match. ArXiv ID leads to a different</span><a href="https://arxiv.org/abs/2107.13586"><span> article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Lei Yu, Jimmy Dumsmyr, and Kevin Knight. Deep paraphrase identification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $650-655,2014$.</span></p></td><td><p dir="ltr"><span>No title or author match. No match in</span><a href="https://aclanthology.org/volumes/D14-1/"><span> </span><span>publication</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>X. Ou and et al. Tuqdm: Token unmasking with quantized diffusion models. In ACL, 2024.</span></p></td><td><p dir="ltr"><span>No title or author match.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Franz Aichberger, Lily Chen, and John Smith. Semantically diverse language generation. In International Conference on Learning Representations (ICLR), 2025.</span></p></td><td><p dir="ltr"><a href="https://proceedings.iclr.cc/paper_files/paper/2025/file/b94d8b035e2183e47afef9e2f299ba47-Paper-Conference.pdf"><span>No title or author match. Some similarity to this </span><span>article</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Maria Glushkova, Shiori Kobayashi, and Junichi Suzuki. Uncertainty estimation in neural text regression. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. $4567-4576,2021$.</span></p></td><td><p dir="ltr"><span>No author or title match. No match in</span><a href="https://aclanthology.org/volumes/2021.findings-emnlp/"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yichao Wang, Bowen Zhou, Adam Lopez, and Benjamin Snyder. Uncertainty quantification in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1234-1245, 2022.</span></p></td><td><p dir="ltr"><span>No author or title match.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Mohit Jain, Ethan Perez, and James Glass. Learning to predict confidence for language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 245-256, 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. No match in</span><a href="https://aclanthology.org/events/emnlp-2021/#2021emnlp-main"><span> </span><span>publication</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Srinivasan Kadavath, Urvashi Khandelwal, Alec Radford, and Noam Shazeer. Answer me this: Self-verifying large language models. In arXiv preprint arXiv:2205.05407, 2022.</span></p></td><td><p dir="ltr"><a href="https://arxiv.org/abs/2205.05407"><span>No author or title match. ArXiv ID leads to a different </span><span>article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=0ZnXGzLcOg"><span>Privacy Reasoning in Ambiguous Contexts</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fe51b7d6-607a-470d-adf3-0a5e4770d5ad/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/7e96a90a-fb2f-4303-b873-c8b1f7f66532/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Zayne Sprague, Xi Ye, Kyle Richardson, and Greg Durrett. MuSR: Testing the limits of chain-of-thought with multistep soft reasoning. In EMNLP, 2023.</span></p></td><td><p dir="ltr"><span>Two authors are omitted and one (Kyle Richardson) is added. This</span><a href="https://proceedings.iclr.cc/paper_files/paper/2024/file/3f8c7eb848ffec848f3ed2b7ca44915d-Paper-Conference.pdf"><span> paper</span></a><span> was published at ICLR 2024.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=15GCs8DoSm"><span>Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/a80b41a7-a74b-4c7d-a8dd-6a02829a17fa/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/1397d83d-3075-4574-a1cf-27df285a5f64/share"><span>AI</span></a><span>**</span></p></td><td><p dir="ltr"><span>Mario Paolone, Trevor Gaunt, Xavier Guillaud, Marco Liserre, Sakis Meliopoulos, Antonello Monti, Thierry Van Cutsem, Vijay Vittal, and Costas Vournas. A benchmark model for power system stability controls. IEEE Transactions on Power Systems, 35(5):3627-3635, 2020.</span></p></td><td><p dir="ltr"><span>The authors match this</span><a href="https://www.sciencedirect.com/science/article/abs/pii/S037877962030482X?via%3Dihub"><span> paper</span></a><span>, but the title, publisher, volume, issue, and page numbers are incorrect. Year (2020) is correct.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=15GCs8DoSm"><span>Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/a80b41a7-a74b-4c7d-a8dd-6a02829a17fa/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/1397d83d-3075-4574-a1cf-27df285a5f64/share"><span>AI</span></a><span>**</span></p></td><td><p dir="ltr"><span>Mingliang Han, Bingni W Wei, Phelan Senatus, Jörg D Winkel, Mason Youngblood, I-Han Lee, and David J Mandell. Deep koopman operator: A model-free approach to nonlinear dynamical systems. Chaos: An Interdisciplinary Journal of Nonlinear Science, 30(12):123135, 2020.</span></p></td><td><p dir="ltr"><span>No title or author match. Journal and other identifiers match this</span><a href="https://pubs.aip.org/aip/cha/article-abstract/30/12/123135/1074648/A-logistic-model-and-predictions-for-the-spread-of?redirectedFrom=fulltext"><span> article.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=18GBPdnuXs"><span>Adaptive Quantization in Generative Flow Networks for Probabilistic Sequential Prediction</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/4ea64476-d22e-41ad-aaed-a7e8516c4ec0/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/d8546150-5096-4b85-8029-870e962feef8/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Francisco Ramalho, Meng Liu, Zihan Liu, and Etienne Mathieu. Towards gflownets for continuous control. arXiv preprint arXiv:2310.18664, 2023.</span></p></td><td><p dir="ltr"><span>No author or title match. ArXiv ID matches this</span><a href="https://arxiv.org/abs/2310.18664"><span> paper.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=1amnhVRQ3l"><span>Grounded Reinforcement Learning for Visual Reasoning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/9af6453c-26d2-4072-b47c-1cbaac232b46/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/5b1f5bdc-3aa1-46ab-bbfb-f932c5be6588/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Arjun Gupta, Xi Victoria Lin, Chunyuan Zhang, Michel Galley, Jianfeng Gao, and Carlos Guestrin Ferrer. Robust compositional visual reasoning via language-guided neural module networks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.</span></p></td><td><p dir="ltr"><span>No title or author match. This</span><a href="https://proceedings.neurips.cc/paper/2021/hash/5bd53571b97884635d13910db49626bc-Abstract.html"><span> </span><span>paper</span></a><span> has a similar title and matches publication.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=1mILyDyPDf"><span>MTRec: Learning to Align with User Preferences via Mental Reward Models</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/b570e1ce-4b4d-47f6-bc0a-eb5ad2516ef4/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f7020dcf-c4a8-455c-9649-b242881573a7/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Diederik P. Kingma and Jimmy Ba. Deepfm: a factorization-machine based neural network for ctr prediction. In International Conference on Learning Representations, 2015.</span></p></td><td><p dir="ltr"><span>Title matches this</span><a href="https://www.ijcai.org/proceedings/2017/0239.pdf"><span> paper</span></a><span>. Authors, date, and publisher match this</span><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=yyIoQu4AAAAJ&amp;cstart=20&amp;pagesize=80&amp;sortby=pubdate&amp;citation_for_view=yyIoQu4AAAAJ:_tF6a-HnqWAC"><span> paper.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=1wmP48quNb"><span>Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/4c559ec8-804f-4071-94ad-d08ed30a9281/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f0e35c1a-543e-4aca-8aaa-a890af1c3bff/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Weijia Xu, Xing Niu, and Marine Carpuat. Controlling toxicity in neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4245-4256, 2020.</span></p></td><td><p dir="ltr"><span>Authors, publisher and date match this</span><a href="https://aclanthology.org/2020.findings-emnlp.182/"><span> paper.</span></a><span> Title and page numbers don't match.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=1wmP48quNb"><span>Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/4c559ec8-804f-4071-94ad-d08ed30a9281/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f0e35c1a-543e-4aca-8aaa-a890af1c3bff/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Xiang Zhang, Xuehai Wei, Xian Zhang, and Xue Zhang. Adversarial attacks and defenses in toxicity detection: A survey. In Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN), pages 1-8. IEEE, 2020.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in</span><a href="https://www.proceedings.com/content/055/055939webtoc.pdf"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=2DAvXR77xh"><span>Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c046c36d-ea1f-4727-9d04-41069e4532f8/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/8ed0a8fe-a6de-473d-af79-5f9684530ab9/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Fenglin Ding, Debesh Jha, Maria Härgestam, Pål Halvorsen, Michael A Riegler, Dag Johansen, Ronny Hänsch, and Håvard Stensland. Vits: Vision transformer for video self-supervised pretraining of surgical phase recognition. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 293-302. Springer, 2022.</span></p></td><td><p dir="ltr"><span>No title or author match. Proceedings from this conference are split into</span><a href="https://conferences.miccai.org/2022/en/PROCEEDINGS.html"><span> volumes</span></a><span>, but the citation doesn't have a volume number.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=4FUdUFvvmp"><span>PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/0f741e52-f068-4bd5-b86a-6be3d5a867ff/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b385a81c-7f8a-4bec-ac84-faf84a1fa18d/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Humberto Acevedo-Viloria, Juan Martinez, and Maria Garcia. Relational graph convolutional networks for financial fraud detection. IEEE Transactions on Knowledge and Data Engineering, 33(7):1357-1370, 2021. doi: 10.1109/TKDE.2020.3007655.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in the cited</span><a href="https://dblp.org/db/journals/tkde/tkde33.html"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=4FUdUFvvmp"><span>PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/0f741e52-f068-4bd5-b86a-6be3d5a867ff/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b385a81c-7f8a-4bec-ac84-faf84a1fa18d/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Majid Zolghadr, Mohsen Jamali, and Jiawei Zhang. Diffurecsys: Diffusion-based generative modeling for sequential recommendation. Proceedings of the ACM Web Conference (WWW), pages 2156-2165, 2024. doi: 10.1145/3545678.3557899.</span></p></td><td><p dir="ltr"><span>No author or title match. DOI doesn't exist.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=4JLZsmWBJf"><span>LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D Scans</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/98128158-bd97-40a1-a60c-0594f31a2e78/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f12d9556-c5fb-4c0b-8d0c-f4926077e7ea/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Bernd Kerbl, Thomas Müller, and Paolo Favaro. Efficient 3d gaussian splatting for real-time neural rendering. In CVPR, 2022. 2, 3</span></p></td><td><p dir="ltr"><span>Loosely matches this</span><a href="https://arxiv.org/abs/2308.04079"><span> article,</span></a><span> but only one author and part of the title actually match.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=4JLZsmWBJf"><span>LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D Scans</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/98128158-bd97-40a1-a60c-0594f31a2e78/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f12d9556-c5fb-4c0b-8d0c-f4926077e7ea/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Punchana Khungurn, Edward H. Adelson, Julie Dorsey, and Holly Rushmeier. Matching real-world material appearance. TPAMI, 2015. 6</span></p></td><td><p dir="ltr"><span>No clear match. Two authors and the subject match this</span><a href="https://dl.acm.org/doi/10.1145/1198555.1198694"><span> article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=4TUpqyDJbz"><span>When and How Unlabeled Data Provably Improve In-Context Learning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/37cb8b5f-1aed-42fc-84eb-a56db01f1dcb/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/7cfc4dcd-dfea-4b65-b42e-c4c07b8f5c3d/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Ashish Kumar, Logan Engstrom, Andrew Ilyas, and Dimitris Tsipras. Understanding self-training for gradient-boosted trees. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 1651-1662, 2020.</span></p></td><td><p dir="ltr"><span>No title or author match. Doesn't exist in</span><a href="https://papers.nips.cc/paper/2020"><span> publication.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=4TUpqyDJbz"><span>When and How Unlabeled Data Provably Improve In-Context Learning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/37cb8b5f-1aed-42fc-84eb-a56db01f1dcb/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/7cfc4dcd-dfea-4b65-b42e-c4c07b8f5c3d/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Chuang Fan, Shipeng Liu, Seyed Motamed, Shiyu Zhong, Silvio Savarese, Juan Carlos Niebles, Anima Anandkumar, Adrien Gaidon, and Stefan Scherer. Expectation maximization pseudo labels. arXiv preprint arXiv:2305.01747, 2023.</span></p></td><td><p dir="ltr"><span>This paper</span><a href="https://arxiv.org/abs/2305.01747v2"><span> exists,</span></a><span> but all the authors are fabricated.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=6uwV6ytamU"><span>DualCnst: Enhancing Zero-Shot Out-of-Distribution Detection via Text-Image Consistency in Vision-Language Models</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/5241c346-4190-4e44-813c-65073c84946b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/c1855921-693a-4379-b5d3-4c0d95e356ff/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>T. Qiao, W. Liu, Z. Xie, H. Xu, J. Lin, J. Huang, and Y. Yang, "Clip-score: A robust scoring metric for text-to-image generation," arXiv preprint arXiv:2201.07519, 2022.</span></p></td><td><p dir="ltr"><span>No clear author or title matches. Title loosely matches this</span><a href="https://arxiv.org/abs/2104.08718"><span> article</span></a><span>. ArXiv ID leads</span><a href="https://arxiv.org/abs/2201.07519"><span> here</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=7VN0iICXZj"><span>Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/af669229-94a7-4343-abf7-9ce3af831e02/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/3db22d99-5daa-44fd-8baa-9abbeb29606d/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yunwen Lei, Puyu Wang, Yiming Ying, and Ding-Xuan Zhou. Optimization and generalization of gradient descent for shallow relu networks with minimal width. preprint, 2024.</span></p></td><td><p dir="ltr"><span>No title match. Authors match this</span><a href="https://arxiv.org/abs/2209.08005"><span> paper.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=7WPi6VbtH0"><span>GeoDynamics: A Geometric State‑Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c15712b0-a37a-40a1-add6-d4a8f98443ff/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/bf8fcb05-9de3-4606-9956-624d8572e467/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Uher, R., Goodman, R., Moutoussis, M., Brammer, M., Williams, S.C.R., Dolan, R.J.: Cognitive and neural predictors of response to cognitive behavioral therapy for depression: a review of the evidence. Journal of Affective Disorders 169, 94-104 (2014)</span></p></td><td><p dir="ltr"><span>No exact title or author match. Loose title match with this</span><a href="https://www.cambridge.org/core/journals/psychological-medicine/article/abs/neural-predictors-and-effects-of-cognitive-behavioral-therapy-for-depression-the-role-of-emotional-reactivity-and-regulation/B8C3EDBA14E972910900CEDC460033D4"><span> article</span></a><span>. Doesn't exist in the</span><a href="https://www.clinicalkey.com/#!/browse/toc/1-s2.0-S0165032714X00157/null/journalIssue"><span> journal volume</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=Ap8OIosN8p"><span>Robust Label Proportions Learning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/61d21fa6-24be-4cb6-9bde-5c80d5165916/share"><span>Scan</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/aa5f2722-47c4-46e0-852e-432a8b37b6e9/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Junyeong Lee, Yiseong Kim, Seungju Park, and Hyunjik Lee. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 18315-18327, 2023.</span></p></td><td><p dir="ltr"><span>Title matches this</span><a href="https://arxiv.org/abs/2301.10921"><span> paper</span></a><span>. No match in NeurIPS</span><a href="https://proceedings.neurips.cc/paper_files/paper/2023"><span> volume 36</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=AmZ7uHDJiR"><span>NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/63f2339b-a3a4-4135-b056-e125dd9301bf/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b597ad2c-edd3-47e5-9dbd-473dce8fd862/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Z. Zhu, T. Yu, X. Zhang, J. Li, Y. Zhang, and Y. Fu. Neuralrgb-d: Neural representations for depth estimation and scene mapping. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in</span><a href="https://www.computer.org/csdl/proceedings/cvpr/2022/1H1gVMlkl32"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=AmZ7uHDJiR"><span>NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/63f2339b-a3a4-4135-b056-e125dd9301bf/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b597ad2c-edd3-47e5-9dbd-473dce8fd862/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Y. Zhang, M. Oswald, and D. Cremers. Airslam: Illumination-invariant hybrid slam. In International Conference on Computer Vision (ICCV), pages 2345-2354, 2023.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in</span><a href="https://csdl-downloads.ieeecomputer.org/proceedings/iccv/2023/0718/00/071800z005.pdf?Expires=1767122670&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jc2RsLWRvd25sb2Fkcy5pZWVlY29tcHV0ZXIub3JnL3Byb2NlZWRpbmdzL2ljY3YvMjAyMy8wNzE4LzAwLzA3MTgwMHowMDUucGRmIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzY3MTIyNjcwfX19XX0_&amp;Signature=QX3vcUyMPs0JlLXc5-PvUoUQA908LDFqmUSVBX6FGn~ZKDJiKwNTMZLWR2RM546c91C-4nBK-A50KWf7mu0ewk7x~M4n60WALqUvJnl-gL~0lHMysGZav01wv2Z4nzqDT4HBd-25SQt7pC0eGQEQvKYnfYpsbU58VJE4t32zO6JCV2HItuM~wVL53gYjASY0uO50l0QFDZWhZHhjb2R0V0ks1dTZEHnjH1dDoVq7wBb38YRP8u6jASw4SjF3r5zdkmUePjJmUWIVjkDtzpMeAcCA8hsJZHk5Nnwj90SJkRdnSvpeV1357U~KC9AKe56U~7ov7umxQNQM7bia-skMgw__&amp;Key-Pair-Id=K12PMWTCQBDMDT"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=BND9CutZf6"><span>Geometric Imbalance in Semi-Supervised Node Classification</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ea4059e3-64e1-4f58-a594-0f47c455fe30/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/a7f949e6-b871-43f7-9d34-46a562598254/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yihong Zhu, Junxian Li, Xianfeng Han, Shirui Pan, Liang Yao, and Chengqi Wang. Spectral contrastive graph clustering. In International Conference on Learning Representations, 2022.</span></p></td><td><p dir="ltr"><span>No title or author match. This paper has a similar</span><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320325003310"><span> title,</span></a><span> but there's no match in the ICLR 2022</span><a href="https://iclr.cc/virtual/2022/papers.html?search=&amp;filter=title"><span> database</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=BND9CutZf6"><span>Geometric Imbalance in Semi-Supervised Node Classification</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ea4059e3-64e1-4f58-a594-0f47c455fe30/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/a7f949e6-b871-43f7-9d34-46a562598254/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Ming Zhong, Han Liu, Weizhu Zhang, Houyu Wang, Xiang Li, Maosong Sun, and Xu Han. Hyperbolic and spherical embeddings for long-tail entities. In ACL, pages 5491-5501, 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist</span><a href="https://aclanthology.org/volumes/2021.acl-long/"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=BpSGN4pErp"><span>NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale Modeling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/12ed22af-ea9c-4272-80c8-397c50a7c3e8/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/826759cb-e770-42d1-baa8-ee293799d45e/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Ye Gao, Robert Tardif, Jiale Cao, and Tapio Schneider. Artificial intelligence reconstructs missing climate information. Nature Geoscience, 17:158-164, 2024. doi: 10.1038/s41561-023-01297-2.</span></p></td><td><p dir="ltr"><span>Title and publisher match this</span><a href="https://www.nature.com/articles/s41561-020-0582-5#citeas"><span> article</span></a><span>. Issue, page numbers, and year match this</span><a href="https://www.nature.com/articles/s41561-023-01371-4#citeas"><span> article</span></a><span>. DOI is fabricated.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=BpSGN4pErp"><span>NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale Modeling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/12ed22af-ea9c-4272-80c8-397c50a7c3e8/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/826759cb-e770-42d1-baa8-ee293799d45e/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Étienne Pardoux and Alexander Yu Veretennikov. Poisson equation for multiscale diffusions. Journal of Mathematical Sciences, 111(3):3713-3719, 2002.</span></p></td><td><p dir="ltr"><span>Authors have frequently published together on the "</span><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=author%3AVeretennikov+author%3APardoux&amp;btnG="><span>poisson equation</span></a><span>", but this title doesn't match any of their publications. Doesn't exist in</span><a href="https://link.springer.com/journal/10958/volumes-and-issues/111-3"><span> publication volume/issue</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=CH76rSKWZr"><span>Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/6da6e9df-3687-476e-a55f-cc16c272616b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/20d02652-f460-4564-9a4f-826f0c9f14ee/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Charanpal D Mummadi, Matthias Arens, and Thomas Brox. Test-time adaptation for continual semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11828-11837, 2021.</span></p></td><td><p dir="ltr"><span>No title or author match. Doesn't exist in</span><a href="https://www.computer.org/csdl/proceedings/cvpr/2021/1yeHGyRsuys"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=CH76rSKWZr"><span>Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/6da6e9df-3687-476e-a55f-cc16c272616b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/20d02652-f460-4564-9a4f-826f0c9f14ee/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Jiacheng He, Zhilu Zhang, Zhen Wang, and Yan Huang. Autoencoder based test-time adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 998-1007, 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in</span><a href="https://www.computer.org/csdl/proceedings/iccv/2021/1BmEezmpGrm"><span> publication.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=EMa1ih7Wdt"><span>Global Minimizers of ℓp-Regularized Objectives Yield the Sparsest ReLU Neural Networks</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/0f8cd2db-21a2-4dd8-8bf3-d227f4009f29/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/420438d8-dad5-4733-8efd-c7b74e17d66a/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>M. Gong, F. Yu, J. Zhang, and D. Tao. Efficient $\ell_{p}$ norm regularization for learning sparsity in deep neural networks. IEEE Transactions on Neural Networks and Learning Systems, 33(10): $5381-5392,2022</span></p></td><td><p dir="ltr"><span>No title or author match.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=FfccSikDfZ"><span>SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/9fc2825a-e8dc-4cbb-934c-b2aed237084a/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/c51b0bd9-d03f-49d8-b916-b547609815e7/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Mihail Stoian, Richard Milbradt, and Christian Mendl. NP-Hardness of Optimal TensorNetwork Contraction and Polynomial-Time Algorithms for Tree Tensor Networks. Quantum, 6:e119, 2022.</span></p></td><td><p dir="ltr"><span>The authors match this</span><a href="https://epubs.siam.org/doi/10.1137/23M161286X"><span> article</span></a><span> and the title is similar. However, the year, publisher and other data don't match. This article didn't appear in the</span><a href="https://quantum-journal.org/volumes/6/"><span> 2022 Quantum volume</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=FfccSikDfZ"><span>SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/9fc2825a-e8dc-4cbb-934c-b2aed237084a/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/c51b0bd9-d03f-49d8-b916-b547609815e7/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Jianyu Xu, Wei Li, and Ming Zhao. Complexity of Optimal Tensor Network Contraction Sequences. Journal of Computational Physics, 480:112237, 2023.</span></p></td><td><p dir="ltr"><span>No title or author match. Doesn't exist in</span><a href="https://www.sciencedirect.com/journal/journal-of-computational-physics/vol/480/suppl/C"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=FzfYoUp8F1"><span>Learning World Models for Interactive Video Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/052a28f8-81e1-4e85-81c8-433d06a506ae/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b52114de-dfe5-424a-92bf-b9e08d4e46ac/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Patrick Esser, Robin Rombach, and Björn Ommer. Structure-aware video generation with latent diffusion models. arXiv preprint arXiv:2303.07332, 2023.</span></p></td><td><p dir="ltr"><span>Authors match this</span><a href="https://arxiv.org/abs/2012.09841"><span> article</span></a><span>. ArXiv ID leads to a different</span><a href="https://arxiv.org/abs/2303.07332"><span> article.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=GPTI9GNAYH"><span>Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c1c42a77-2646-46d2-adec-a476c090df28/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/43100616-900c-4d09-b652-0ae59fbb2221/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Lele Xu, Chen Lin, Hongyu Zhao, and et al. Gaborvit: Global attention with local frequency awareness. In European Conference on Computer Vision (ECCV), 2022.</span></p></td><td><p dir="ltr"><span>No author or title match. No match in</span><a href="https://www.ecva.net/papers.php"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=GPTI9GNAYH"><span>Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c1c42a77-2646-46d2-adec-a476c090df28/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/43100616-900c-4d09-b652-0ae59fbb2221/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yoonwoo Lee, Jaehyeong Kang, Namil Kim, Jinwoo Shin, and Honglak Lee. Structured fast fourier transform attention for vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), 2022.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in</span><a href="https://proceedings.neurips.cc/paper/2022"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=GPTI9GNAYH"><span>Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c1c42a77-2646-46d2-adec-a476c090df28/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/43100616-900c-4d09-b652-0ae59fbb2221/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Siyuan Gong, Alan Yu, Xiaohan Chen, Yinpeng Lin, and Larry S Davis. Vision transformer compression: Early exiting and token pruning. In Advances in Neural Information Processing Systems (NeurIPS), 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. No match in</span><a href="https://papers.nips.cc/paper_files/paper/2021"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=GPTI9GNAYH"><span>Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c1c42a77-2646-46d2-adec-a476c090df28/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/43100616-900c-4d09-b652-0ae59fbb2221/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Jiuxiang Shi, Zuxuan Wu, and Dahua Lin. Token-aware adaptive sampling for efficient diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in</span><a href="https://www.computer.org/csdl/proceedings/cvpr/2023/1PONOShHStG"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=GPTI9GNAYH"><span>Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c1c42a77-2646-46d2-adec-a476c090df28/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/43100616-900c-4d09-b652-0ae59fbb2221/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Raphael Muller, Simon Kornblith, and Geoffrey Hinton. Adavit: Adaptive tokens for efficient vision transformer. In Proceedings of the International Conference on Machine Learning (ICML), 2021.</span></p></td><td><p dir="ltr"><span>Authors match this</span><a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf"><span> </span><span>article.</span></a><span> Title matches this</span><a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf"><span> </span><span>article</span></a><span>. No match in</span><a href="https://icml.cc/virtual/2021/papers.html?search=Adaptive+tokens"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=GPTI9GNAYH"><span>Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c1c42a77-2646-46d2-adec-a476c090df28/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/43100616-900c-4d09-b652-0ae59fbb2221/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Xin Wang, Anlin Chen, Lihui Xie, Xin Jin, Cheng Wang, and Ping Luo. Not all tokens are equal: Efficient transformer for tokenization and beyond. In Advances in Neural Information Processing Systems (NeurIPS), 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. This</span><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_Not_All_Tokens_Are_Equal_Human-Centric_Visual_Analysis_via_Token_CVPR_2022_paper.pdf"><span> </span><span>article</span></a><span> title is similar. No match in</span><a href="https://proceedings.neurips.cc/paper/2021"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=HdY8CCHife"><span>A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/14695743-9825-4aef-8e88-c8f54e101b9a/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/1db86c77-b6f6-4253-8f57-aa319bddff2b/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Z. Chen and N. Flammarion. When and why sam generalizes better: An optimization perspective. arXiv preprint arXiv:2206.09267, 2022.</span></p></td><td><p dir="ltr"><span>No author or title match. ArXiv ID leads to a different</span><a href="https://arxiv.org/abs/2206.09267"><span> paper</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=HdY8CCHife"><span>A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/14695743-9825-4aef-8e88-c8f54e101b9a/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/1db86c77-b6f6-4253-8f57-aa319bddff2b/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>K. A. Sankararaman, S. Sankararaman, H. Pandey, S. Ganguli, and F. Bromberg. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In 37th International Conference on Machine Learning (ICML), pages 8469-8479, 2020.</span></p></td><td><p dir="ltr"><span>This</span><a href="https://dl.acm.org/doi/abs/10.5555/3524938.3525723"><span> paper</span></a><span> is a match, but all authors but the first (K. A. Sankararaman) are fabricated.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=I64ZLbUP6u"><span>MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/d9683c9b-7ffc-4c40-b0b8-df3d72ee84e9/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/ed653d2d-71c6-4967-9306-d2c3c9bb5632/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Why Physically-Based Rendering. Physically-based rendering. Procedia IUTAM, 13(127137):3, 2015 .</span></p></td><td><p dir="ltr"><span>No author given and title appears to be garbled. Publisher, issue, year, and pages match this</span><a href="https://www.sciencedirect.com/journal/procedia-iutam/vol/13/suppl/C"><span> article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IJGEtuVqwf"><span>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fbf50b6a-faa1-4e52-ae0d-0efb52cb3400/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52b94988-efd5-4412-81e1-a6cc32010909/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Pierre Casgrain, Anirudh Kulkarni, and Nicholas Watters. Learning to trade with continuous action spaces: Application to market making. arXiv preprint arXiv:2303.08603, 2023.</span></p></td><td><p dir="ltr"><span>No title or author match. ArXiv ID matches a different</span><a href="https://arxiv.org/abs/2303.08603"><span> article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IJGEtuVqwf"><span>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fbf50b6a-faa1-4e52-ae0d-0efb52cb3400/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52b94988-efd5-4412-81e1-a6cc32010909/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Z Ning and Y K Kwok. Q-learning for option pricing and hedging with transaction costs. Applied Economics, 52(55):6033-6048, 2020.</span></p></td><td><p dir="ltr"><span>No author or title match. No match in journal</span><a href="https://www.tandfonline.com/toc/raec20/52/55?nav=tocList"><span> volume/issue</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IJGEtuVqwf"><span>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fbf50b6a-faa1-4e52-ae0d-0efb52cb3400/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52b94988-efd5-4412-81e1-a6cc32010909/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>W L Chan and R O Shelton. Can machine learning improve delta hedging? Journal of Derivatives, $9(1): 39-56,2001$.</span></p></td><td><p dir="ltr"><a href="https://www.pm-research.com/content/iijderiv/9/1"><span>No author or title match. No match in </span><span>journal volume/issue</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IJGEtuVqwf"><span>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fbf50b6a-faa1-4e52-ae0d-0efb52cb3400/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52b94988-efd5-4412-81e1-a6cc32010909/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Petter N Kolm, Sebastian Krügel, and Sergiy V Zadorozhnyi. Reinforcement learning for optimal hedging. The Journal of Trading, 14(4):4-17, 2019.</span></p></td><td><p dir="ltr"><a href="https://www.pm-research.com/content/iijtrade"><span>No author or title match. There is no volume 14 of this </span><span>journal.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IJGEtuVqwf"><span>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fbf50b6a-faa1-4e52-ae0d-0efb52cb3400/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52b94988-efd5-4412-81e1-a6cc32010909/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Kyung Hyun Park, Hyeong Jin Kim, and Woo Chang Kim. Deep reinforcement learning for limit order book-based market making. Expert Systems with Applications, 169:114338, 2021.</span></p></td><td><p dir="ltr"><a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417420310265"><span>No author or title match. Publisher ID matches this </span><span>article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=II0T40Q785"><span>FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/f2e2340f-61cd-40b9-86e2-d2d7323beec3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/95868fb0-5d7a-4b65-ab5c-4d06e2e29dec/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Moonseop Han and Elizabeth Qian. Robust prediction of dynamical systems with structured neural networks: Long-term behavior and chaos. Physica D: Nonlinear Phenomena, 427:133006, 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. Publisher ID matches this</span><a href="https://www.sciencedirect.com/science/article/abs/pii/S0167278921001639"><span> article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=II0T40Q785"><span>FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/f2e2340f-61cd-40b9-86e2-d2d7323beec3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/95868fb0-5d7a-4b65-ab5c-4d06e2e29dec/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Bart De Schutter and Serge P Hoogendoorn. Modeling and control of freeway traffic flow by state space neural networks. Neural Computing and Applications, 17(2):175-185, 2008.</span></p></td><td><p dir="ltr"><span>No title match, although Schutter and Hoogendorn have written or coauthored several related papers (</span><a href="https://scispace.com/pdf/model-based-control-of-intelligent-traffic-networks-5o7t1ux396.pdf"><span>example</span></a><span> and</span><a href="https://ieeexplore.ieee.org/document/8283509"><span> example</span></a><span>). Journal volume/issue matches an unrelated</span><a href="https://link.springer.com/article/10.1007/s00521-006-0080-8"><span> article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=II0T40Q785"><span>FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/f2e2340f-61cd-40b9-86e2-d2d7323beec3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/95868fb0-5d7a-4b65-ab5c-4d06e2e29dec/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Jaideep Pathak, Brian R Hunt, Georg M Goerg, and Themistoklis P Sapsis. Data-driven prediction of chaotic dynamics: Methods, challenges, and opportunities. Annual Review of Condensed Matter Physics, 14:379-401, 2023.</span></p></td><td><p dir="ltr"><a href="https://www.annualreviews.org/content/journals/conmatphys/14/1"><span>No author or title match. No match in </span><span>journal volume</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=II0T40Q785"><span>FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/f2e2340f-61cd-40b9-86e2-d2d7323beec3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/95868fb0-5d7a-4b65-ab5c-4d06e2e29dec/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Alejandro Güemes, Stefano Discetti, and Andrea Ianiro. Coarse-grained physics-based prediction of three-dimensional unsteady flows via neural networks. Science Advances, 7(46):eabj0751, 2021.</span></p></td><td><p dir="ltr"><a href="https://www.science.org/toc/sciadv/7/46"><span>No title or author match. Doesn't exist in </span><span>journal volume/issue</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IIgVYnadfR"><span>BNMusic: Blending Environmental Noises into Personalized Music</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fd47718b-20a1-493f-9876-dc623513a506/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/4158f8bf-0292-41c8-b920-c2699933114f/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Jeongseung Park, Minseon Yang, Minz Won Park, and Geonseok Lee. Diffsound: Differential sound manipulation with a few-shot supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 1767-1775, 2021.</span></p></td><td><p dir="ltr"><span>No title or author match. Doesn't exist in</span><a href="https://www.computer.org/csdl/proceedings/cvprw/2021/1wzs0vrjyWQ"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=JSbVO7dNYE"><span>Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/96641483-8df9-49f4-ac68-888fa22eb819/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52bf51d1-e538-486c-83ae-60fa507d1103/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Wenxuan Sun, Tri Dao, Hongyu Zhuang, Zihang Dai, Albert Gu, and Christopher D Manning. Llamba: Efficient llms with mamba-based distillation. arXiv preprint arXiv:2502.14458, 2024.</span></p></td><td><p dir="ltr"><span>ArXiv ID leads to this</span><a href="https://arxiv.org/abs/2502.14458"><span> article</span></a><span> with a similar title and one matching author.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=JSbVO7dNYE"><span>Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/96641483-8df9-49f4-ac68-888fa22eb819/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52bf51d1-e538-486c-83ae-60fa507d1103/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Tri Dao, Shizhe Ma, Wenxuan Sun, Albert Gu, Sam Smith, Aapo Kyrola, Christopher D Manning, and Christopher Re. An empirical study of state space models for large language modeling. arXiv preprint arXiv:2406.07887, 2024.</span></p></td><td><p dir="ltr"><span>Two authors (Tri Dao and Albert Gu), the arXiv ID, and the year match this</span><a href="https://arxiv.org/abs/2406.07887"><span> paper</span></a><span>. However, the title is only a partial match.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=KQTp6ljvlo"><span>Fourier Clouds: Fast Bias Correction for Imbalanced Semi-Supervised Learning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/2f6208a4-66b9-45ea-9396-a04409df4948/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/d41f986d-a44f-4aaa-9ea2-c1d46aed6fe5/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Junyan Zhu, Chenyang Li, Chao He, and et al. Freematch: A simple framework for long-tailed semi-supervised learning. In NeurIPS, 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. This</span><a href="https://arxiv.org/pdf/2205.07246"><span> paper title</span></a><span> is very close, but it was published by ICLR 2023 not</span><a href="https://papers.nips.cc/paper_files/paper/2021"><span> NeurIPS 2021</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=LP4Q7tPMbs"><span>NormFit: A Lightweight Solution for Few-Shot Federated Learning with Non-IID Data</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/6977dbef-7a2b-4143-a4be-b69a426e2b56/share"><span>Scan</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/7277a269-f450-45f5-8cd7-75eaf6f485e2/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yijie Zang et al. Fedclip: A federated learning framework for vision-language models. In NeurIPS, 2023.</span></p></td><td><p dir="ltr"><span>No author or title match, although this</span><a href="https://ieeexplore.ieee.org/document/10988823"><span> title</span></a><span> is close. No match in</span><a href="https://papers.nips.cc/paper_files/paper/2023"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=LsmUgStXby"><span>AI-Generated Video Detection via Perceptual Straightening</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ccc2460e-0ef3-47b7-9d99-71e8df46ab6b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/5e1821da-8518-413c-804d-030ed6824fcd/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Jiahui Liu and et al. Tall-swin: Thumbnail layout transformer for generalised deepfake video detection. In ICCV, 2023.</span></p></td><td><p dir="ltr"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_TALL_Thumbnail_Layout_for_Deepfake_Video_Detection_ICCV_2023_paper.html"><span>No author or title match. A paper with a similar title appears in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=Lz5BUjArK4"><span>Multi-Expert Distributionally Robust Optimization for Out-of-Distribution Generalization</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/5627ac20-2221-4ca0-8857-4dea140b98e8/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/636f7851-a09e-4fd9-90e3-35c8602a4f8d/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Nitish Srivastava and Ruslan R Salakhutdinov. Discriminative features for fast frame-based phoneme classification. Neural networks, 47:17-23, 2013.</span></p></td><td><p dir="ltr"><span>No title match, but authors have published together previously (</span><a href="https://papers.nips.cc/paper_files/paper/2012/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf"><span>example</span></a><span>). No match in</span><a href="https://www.sciencedirect.com/journal/neural-networks/vol/47/suppl/C"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=ToNRHqX6xq"><span>MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/73c0dd26-0e0d-4b9c-a31d-6d9554120362/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/57179143-9c9b-462c-86a4-bd5041d88594/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Anh Tuan Nguyen, Shengping Li, and Chao Qin. Multimodal adversarial robustness: Attack and defense. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.</span></p></td><td><p dir="ltr"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9639884"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=V1FlwrsseI"><span>ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/f2948a0f-8972-4269-b680-4ab68e14f2ec/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/11c6525d-e02d-4dd3-86a0-dfa3b544697e/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Jack Lau, Ankan Gayen, Philipp Tschandl, Gregory A Burns, Jiahong Yuan, Tanveer SyedaMahmood, and Mehdi Moradi. A dataset and exploration of models for understanding radiology images through dialogue. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2575-2584, 2018.</span></p></td><td><p dir="ltr"><span>No author match. Title matches another hallucinated citation in this</span><a href="https://arxiv.org/html/2508.05244v1"><span> </span><span>paper</span></a><span>. Doesn't exist in</span><a href="https://aclanthology.org/events/emnlp-2018/#d18-1"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=VN5bMTfSZS"><span>OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/61a6a3d3-6c3b-4663-934a-40f69f112b50/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/7362670b-7428-468f-882c-a6f7de6d90da/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yikai Zhang et al. "Text-to-Image Diffusion Models with Customized Guidance". In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.</span></p></td><td><p dir="ltr"><a href="https://www.computer.org/csdl/proceedings/iccv/2023/1TJc6RNOu8U"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=VN5bMTfSZS"><span>OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/61a6a3d3-6c3b-4663-934a-40f69f112b50/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/7362670b-7428-468f-882c-a6f7de6d90da/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Author Song and AnotherAuthor Zhang. "Consistency in Diffusion Models: Improving Noise Embeddings". In: IEEE Transactions on Pattern Analysis and Machine Intelligence (2023). URL:</span><a href="https://arxiv.org/abs/2304.08787."><span> </span><span>https://arxiv.org/abs/2304.08787.</span></a></p></td><td><p dir="ltr"><span>No author or title match. This</span><a href="https://openreview.net/forum?id=59nCKifDtm"><span> </span><span>paper</span></a><span> has a similar title. ArXiv ID leads to unrelated</span><a href="https://arxiv.org/abs/2304.08787"><span> </span><span>paper.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=W8xcKoJcrl"><span>Strategic Costs of Perceived Bias in Fair Selection</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/b00f841e-0e1d-4ae4-9a48-82b2891e6600/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/feb8886c-6917-4006-82e5-551cc7d0df58/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Claudia Goldin. Occupational choices and the gender wage gap. American Economic Review, 104(5):348-353, 2014.</span></p></td><td><p dir="ltr"><span>Author is a famous</span><a href="https://www.britannica.com/biography/Claudia-Goldin"><span> </span><span>economist</span></a><span>, but the title doesn't match any of her works. Journal and locators match this unrelated</span><a href="https://www.aeaweb.org/articles?id=10.1257/aer.104.5.348"><span> </span><span>article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=XxR70zr9Sf"><span>Linear Transformers Implicitly Discover Unified Numerical Algorithms</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/d349a8d0-809b-42fb-a5e6-ee91f37fbcd3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b138bbcb-4f6a-4fc8-b85f-d09b1490fac7/share"><span>AI</span></a></p></td><td><p dir="ltr"><a href="https://distill.pub/2022/circuits/"><span>Olah, C., Elhage, N., Nanda, N., Schiefer, N., Jones, A., Henighan, T., and DasSarma, N. (2022). Transformer circuits. Distill, 7(3). </span><span>https://distill.pub/2022/circuits/.</span></a></p></td><td><p dir="ltr"><span>Most authors match this</span><a href="https://transformer-circuits.pub/2021/framework/index.html"><span> </span><span>paper</span></a><span>, but the title, publisher, and year are different. Doesn't exist in</span><a href="https://distill.pub/"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=XxR70zr9Sf"><span>Linear Transformers Implicitly Discover Unified Numerical Algorithms</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/d349a8d0-809b-42fb-a5e6-ee91f37fbcd3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b138bbcb-4f6a-4fc8-b85f-d09b1490fac7/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Nanda, N. (2023). Progress in mechanistic interpretability: Reverse-engineering induction heads in GPT-2.</span></p></td><td><p dir="ltr"><a href="https://scholar.google.com/citations?hl=en&amp;user=GLnX3MkAAAAJ&amp;view_op=list_works&amp;sortby=pubdate"><span>No title match. Author may be </span><span>Neel Nanda</span></a><span>, who wrote several similar articles in 2023.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=YGIbwfNWot"><span>A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object Annotation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/41566db2-9a1a-44b4-bec4-c4dc7107cc17/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/6ddb0781-ed8d-4377-a396-08521c082562/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>J. Zhang and X. Li. Multi-agent systems for distributed problem solving: A framework for task decomposition and coordination. Procedia Computer Science, 55:1131-1138, 2015.</span></p></td><td><p dir="ltr"><a href="https://www.sciencedirect.com/journal/procedia-computer-science/vol/55/suppl/C?page=2"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=YGIbwfNWot"><span>A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object Annotation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/41566db2-9a1a-44b4-bec4-c4dc7107cc17/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/6ddb0781-ed8d-4377-a396-08521c082562/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Erfan Aghasian, Shai Avidan, Piotr Dollar, and Justin Johnson. Hierarchical protocols for multi-agent 3d scene understanding. In CVPR, pages 7664-7673, 2021.</span></p></td><td><p dir="ltr"><a href="https://openaccess.thecvf.com/CVPR2021"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=Ynwl0V1YH0"><span>Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/114aad0a-479c-4b46-a94c-8beeffe28d2b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/edef15b3-6712-42c7-9aae-4a9a26ce5f45/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Rami El-Yaniv, and Yoshua Bengio. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1612.01462, 2017.</span></p></td><td><p dir="ltr"><span>Authors mostly match this</span><a href="https://papers.nips.cc/paper_files/paper/2016/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html"><span> </span><span>paper</span></a><span>. Title matches this</span><a href="https://arxiv.org/abs/1806.08342"><span> </span><span>paper.</span></a><span> ArXiv ID matches a third</span><a href="https://arxiv.org/abs/1612.01462"><span> </span><span>paper.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=Ynwl0V1YH0"><span>Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/114aad0a-479c-4b46-a94c-8beeffe28d2b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/edef15b3-6712-42c7-9aae-4a9a26ce5f45/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Zhiqiang Wang, Chao Zhang, Bing Li, Zhen Xu, and Zhiwei Li. A survey of model compression and acceleration for deep neural networks. ACM Computing Surveys, 54(7):1-34, 2021.</span></p></td><td><p dir="ltr"><span>No author match. Title matches this</span><a href="https://arxiv.org/abs/1710.09282"><span> </span><span>paper.</span></a><span> Doesn't exist in</span><a href="https://dl.acm.org/toc/csur/2022/54/7"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=ZR2mdBrhJX"><span>PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ff9ad094-e9be-468b-a618-fab14c568c21/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/684135a5-9a1e-4744-9c3a-cb8b0905a3dd/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Andrew Black et al. Zero-shot skill composition with semantic feature fusion. arXiv preprint arXiv:2310.08573, 2023.</span></p></td><td><p dir="ltr"><a href="https://arxiv.org/abs/2310.08573"><span>No title match. ArXiv ID leads to unrelated </span><span>paper.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=ZR2mdBrhJX"><span>PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ff9ad094-e9be-468b-a618-fab14c568c21/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/684135a5-9a1e-4744-9c3a-cb8b0905a3dd/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Yufei Wu, Kiran Alwala, Vivek Ganapathi, Sudeep Sharma, Yilun Chang, Yicheng Zhang, Yilun Zhou, et al. Susie: Scaling up instruction-following policies for robot manipulation. arXiv preprint arXiv:2402.17552, 2024.</span></p></td><td><p dir="ltr"><a href="https://www.arxiv.org/abs/2402.17552"><span>No author or title match. ArXiv ID leads to unrelated </span><span>article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=Zb3QO7HLIj"><span>FLAME: Fast Long-context Adaptive Memory for Event-based Vision</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/1cbbae27-5545-40d2-8a1d-33baa8b71c4e/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/aa75cdb6-986a-43e1-9db6-7025ccdda5a6/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Zhipeng Zhang, Chang Liu, Shihan Wu, and Yan Zhao. EST: Event spatio-temporal transformer for object recognition with event cameras. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023.</span></p></td><td><p dir="ltr"><a href="https://dblp.org/db/conf/icassp/index.html"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=Zb3QO7HLIj"><span>FLAME: Fast Long-context Adaptive Memory for Event-based Vision</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/1cbbae27-5545-40d2-8a1d-33baa8b71c4e/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/aa75cdb6-986a-43e1-9db6-7025ccdda5a6/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Daniel Gehrig, Mathias Gehrig, John Monaghan, and Davide Scaramuzza. Recurrent vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 3139-3148, 2021.</span></p></td><td><p dir="ltr"><span>No author match, but this</span><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ranftl_Vision_Transformers_for_Dense_Prediction_ICCV_2021_paper.pdf"><span> </span><span>paper</span></a><span> has a similar title. Doesn't exist in</span><a href="https://www.computer.org/csdl/proceedings/iccvw/2021/1yNhksNMpkQ"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=emM7U3WKMO"><span>Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/71876ca9-d9c9-4b73-a910-cbb5a61b722b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/6ce64b84-9e77-4734-8a05-a0d4cb00de66/share"><span>AI</span></a><span>**</span></p></td><td><p dir="ltr"><span>Qiyang Du, Ozan Sener, and Silvio Savarese. Agree to disagree: Adaptive learning with gradient disagreement. In Advances in Neural Information Processing Systems (NeurIPS), 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. Sener and Savarese have</span><a href="https://arxiv.org/abs/1708.00489"><span> </span><span>published</span></a><span> </span><span>together previously. Doesn't exist in</span><a href="https://proceedings.neurips.cc/paper/2021"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=emM7U3WKMO"><span>Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/71876ca9-d9c9-4b73-a910-cbb5a61b722b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/6ce64b84-9e77-4734-8a05-a0d4cb00de66/share"><span>AI</span></a><span>**</span></p></td><td><p dir="ltr"><span>Longxuan Jing, Yu Tian, Yujun Pei, Yibing Shen, and Jiashi Feng. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Learning Representations (ICLR), 2022.</span></p></td><td><p dir="ltr"><span>No author match. Title matches this</span><a href="https://proceedings.mlr.press/v119/wang20k/wang20k.pdf"><span> </span><span>paper.</span></a><span> Doesn't exist in</span><a href="https://iclr.cc/virtual/2022/papers.html"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=gNiT81iag0"><span>TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/2c627437-707b-4434-aa97-0d13a90eabd3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/27d5bf8f-c17e-456f-b9c2-acc4bdb7aea6/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Yair Leviathan, Clemens Rosenbaum, and Slav Petrov. Fast inference from transformers via speculative decoding. In ICML, 2023.</span></p></td><td><p dir="ltr"><a href="https://dl.acm.org/doi/10.5555/3618408.3619203"><span>Title, publisher, and date match this </span><span>paper</span></a><span>, but all authors except one surname (Leviathan) are different.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=gNiT81iag0"><span>TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/2c627437-707b-4434-aa97-0d13a90eabd3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/27d5bf8f-c17e-456f-b9c2-acc4bdb7aea6/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Wenwen Chang, Tal Schuster, and Yann LeCun. Neural surgery for memorisation: Locating and removing verbatim recall neurons. In NeurIPS, 2024.</span></p></td><td><p dir="ltr"><a href="https://papers.nips.cc/paper_files/paper/2024"><span>No author or title match. Doesn't exist in </span><span>publication.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=oBikm5Rshc"><span>Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ae7bf744-cbf7-4ef2-9ee1-3db00bcd5062/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/30a7d9af-5cf1-4251-acbf-0717f0432d6a/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>M. Garcia and A. Thompson. Applications of llms in legal document analysis. Journal of Legal Technology, 7(1):50-65, 2024.</span></p></td><td><p dir="ltr"><span>No author or title match. Publication doesn't exist.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=oBikm5Rshc"><span>Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ae7bf744-cbf7-4ef2-9ee1-3db00bcd5062/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/30a7d9af-5cf1-4251-acbf-0717f0432d6a/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>J. Smith and A. Patel. Leveraging large language models for financial forecasting. International Journal of Financial Technology, 9(2):101-115, 2024.</span></p></td><td><p dir="ltr"><span>No author or title match. Publication doesn't exist.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=pGRjDetCDM"><span>JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial Transcriptomics</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c05dbbba-a778-4561-a553-fb18827f44ee/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/defdd83e-45cb-427d-95d7-c8fbaa757555/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>David Jones et al. Gpsa: Gene expression and histology-based spatial alignment. Nature Methods, 2023.</span></p></td><td><p dir="ltr"><a href="https://www.nature.com/nmeth/articles?type=article&amp;year=2023"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=pGRjDetCDM"><span>JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial Transcriptomics</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c05dbbba-a778-4561-a553-fb18827f44ee/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/defdd83e-45cb-427d-95d7-c8fbaa757555/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Zhihao Chen, Hantao Zhang, Yuhan Zhang, Zhanlin Hu, Quanquan Gu, Qing Zhang, and Shuo Suo. Slat: a transformer-based method for simultaneous alignment and clustering of spatial transcriptomics data. Nature Communications, 14(1):5548, 2023.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in publication.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=uih8cWS3JF"><span>Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/270a15e9-9a36-4b4a-8acb-ad91a8209bf7/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f3bef20d-ea0a-4510-86e4-b9701136bee7/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>François Baccelli, Gérard H. Taché, and Etienne Altman. Flow complexity and heavytailed delays in packet networks. Performance Evaluation, 49(1-4):427-449, 2002.</span></p></td><td><p dir="ltr"><a href="https://dblp.org/db/journals/pe/pe49.html"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=uih8cWS3JF"><span>Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/270a15e9-9a36-4b4a-8acb-ad91a8209bf7/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f3bef20d-ea0a-4510-86e4-b9701136bee7/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Saravanan Jebarajakirthy, Paurav Shukla, and Prashant Palvia. Heavy-tailed distributions in online ad response: A marketing analytics perspective. Journal of Business Research, 124:818-830, 2021.</span></p></td><td><p dir="ltr"><a href="https://www.sciencedirect.com/journal/journal-of-business-research/vol/124/suppl/C"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=vKyiv67VWa"><span>AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fb7a6411-f38b-48d9-8eab-7f32e6d006ab/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f3216beb-368a-433c-ac2f-9dd2badddf22/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Mehdi Azabou, Micah Weber, Wenlin Ma, et al. Mineclip: Multimodal neural exploration of clip latents for automatic video annotation. arXiv preprint arXiv:2210.02870, 2022.</span></p></td><td><p dir="ltr"><a href="https://arxiv.org/abs/2210.02870"><span>No author or title match. ArXiv ID leads to unrelated </span><span>article.</span></a></p></td></tr></tbody></table>
<!--kg-card-end: html-->
<hr><div data-layout="minimal">
                    
                        <p><span>Is there a specific report or published article you think we should check for hallucinations?</span></p>
                    
                    
                        <p><a href="https://docs.google.com/forms/d/e/1FAIpQLSdEGNCoZZWTWrddNHP8ZoUiqay9Qs1ndXPv2wpP1d3LnB7JoQ/viewform?usp=publish-editor">
                            Submit Here
                        </a>
                        
                    </p></div><h2 id="defining-hallucinated-citations">Defining Hallucinated Citations</h2><p>Given the high stakes for both authors and publishers, GPTZero's Hallucination Check is engineered to be accurate, transparent, and cautious. It uses our AI agent, trained in-house, to flag any citations in a document that can’t be found online. These flagged citations are not automatically hallucinations — many archival documents or unpublished works can’t be matched to an online source — but they indicate which sources require further human scrutiny. As always, we recommend that a human confirm that flagged citation is an AI-generated fake instead of the result of a more conventional error.</p><p>We define a vibe citation as a citation that likely resulted from the use of generative AI. Vibe citing results in errors common to LLM generations, but rare in human-written text, such as:</p><ol><li>Combining or paraphrasing the titles, author(s), and/or locators from one or more real sources</li><li>Fabricating the author(s), title, URL/DOI, and/or container (ex. publisher, journal, conference) of a source</li><li>Modifying the author(s) or title of a source by extrapolating a first name from an initial, dropping and/or adding authors, or paraphrasing the title.</li></ol><p>Our definition excludes obvious spelling mistakes, dead URLs, missing locators, and other errors that are plausibly human. The following table shows the difference between a real citation, a flawed citation, and a hallucinated citation according to our methodology. The differences are highlighted in red.</p>
<!--kg-card-begin: html-->
<table><colgroup><col width="204"><col width="244"><col width="176"></colgroup><tbody><tr><td><p dir="ltr"><span>Real Citation</span></p></td><td><p dir="ltr"><span>Flawed Citation</span></p></td><td><p dir="ltr"><span>Hallucinated Citation</span></p></td></tr><tr><td><p dir="ltr"><span>Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521:436-444, 2015.</span></p></td><td><p dir="ltr"><span>Y.</span><span> LeCun, </span><span>Y. </span><span>Bengio, and </span><span>Geoff</span><span> Hinton. </span><span>Deep leaning</span><span>. nature, 521</span><span>(7553)</span><span>:436-444, 2015.</span></p></td><td><p dir="ltr"><span>Samuel LeCun Jackson</span><span>. Deep learning. </span><span>Science &amp;</span><span> Nature</span><span>: </span><span>23-45</span><span>, 20</span><span>21</span><span>.</span></p></td></tr><tr><td><p dir="ltr"><span>A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin, et al. Qwen2.5–math technical report: Toward mathematical expert model via self-improvement. arXiv:2409.12122, 2024.</span></p></td><td><p dir="ltr"><span>A. Yang,&nbsp; </span><span>(missing author)</span><span>, B. Hui, B. Gao, B. Yu, C. Li</span><span>/</span><span>, D. Liu, J. Tu, J. Zhou, J. Lin, et al. </span><span>Qwen 2. 5</span><span>–math technical report: Toward mathematical expert model via self-improvement. </span><span>arXiv preprint</span><span> arXiv:2409.12122, 2024.</span></p></td><td><p dir="ltr"><span>A. Yang, </span><span>B. Yang, C. Yang</span><span>, et al. Qwen</span><span>3.5–mathematical report for iterative model self-improvement</span><span>. arXiv:</span><span>2909.12233</span><span>, 2024.</span></p></td></tr></tbody></table>
<!--kg-card-end: html-->
<p>Like GPTZero’s <a href="https://gptzero.me/" rel="noreferrer">AI Detector</a>, Hallucination Check has an extremely low false negative rate, so we catch 99 out of 100 flawed citations.&nbsp;Because our tool will flag any citation that can't be verified online, the false positive rate is higher.</p><h2 id="vibe-citing">Vibe Citing</h2><p>Over the past few months, we've experimented with several names for an LLM-generated citation with fabricated elements. "Hallucinated citations" is too long, "hallucitations" too easily mistaken for a spelling error, and "fake citations" too morally charged. Recently, GPTZero's Head of Machine Learning, Alex Adams, coined the term "vibe citing" to describe the LLM tendency to derive or amalgamate real sources into uncanny imitations. "Vibe citing," like "vibe writing" or "vibe coding" produces citations that look accurate at first glance, but crumble under closer inspection.</p><figure><img src="https://gptzero.me/news/content/images/2026/01/star-history-2026121.png" alt="" loading="lazy" width="1576" height="1153" srcset="https://gptzero.me/news/content/images/size/w600/2026/01/star-history-2026121.png 600w, https://gptzero.me/news/content/images/size/w1000/2026/01/star-history-2026121.png 1000w, https://gptzero.me/news/content/images/2026/01/star-history-2026121.png 1576w" sizes="(min-width: 720px) 720px"><figcaption><span>Figure 2: Open-source projects to write research papers with AI are booming in popularity and illustrate the growth in vibe-citing. The bumps in April and September 2025 correspond to the paper submission deadlines for NeurIPS and ICLR 2025.</span></figcaption></figure><p>GPTZero's analysis of 4841 of the 5290 papers accepted by NeurIPS 2025 indicates noticeable traces of AI authorship and hundreds of vibe citations. As always, each of the hallucinations presented here has been verified by a human expert.</p><h2 id="surf-the-tsunami-with-hallucination-check">Surf the Tsunami with Hallucination Check</h2><p>Hallucination Check is the only tool of its kind, and provides an essential service at multiple points in the peer review pipeline. First, it allows authors to check their manuscripts for citation errors — including common issues that can occur without LLM involvement like dead links or partial titles. Second, it greatly reduces the time and labor necessary for reviewers to check a submission's sources and identify possible vibe citing. Third, using Hallucination Check in combination with GPTZero's AI Detector allows editors and conference chairs to check for AI-generated text and suspicious citations at the same time, leading to faster and more accurate editorial decisions.</p><p>After releasing our ICLR paper investigation we are now coordinating with the ICLR team to review future paper submissions. As always, our goal is to make the peer review process faster, fairer, and more transparent for everyone involved. <em>Try GPTZero's </em><a href="https://gptzero.me/hallucination-detector" rel="noreferrer"><em><u>Hallucination check</u></em></a><em> for yourself, or </em><a href="https://gptzero.me/sales"><em><u>reach out to GPTZero's team</u></em></a><em>.</em></p><figure><a href="https://gptzero.me/news/iclr-2026/"><div><p>GPTZero uncovers 50+ Hallucinations in ICLR 2026</p><p>GPTZero used our Hallucination Check tool to find 50+ hallucinations under review at ICLR, each of which were missed by 3-5 peer reviewers.</p><p><img src="https://gptzero.me/news/content/images/icon/square_logo-2.png" alt=""><span>Paul Esau</span></p></div><p><img src="https://gptzero.me/news/content/images/thumbnail/ICLR_Logo.svg.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://gptzero.me/news/deloitte-australia-citation-check/"><div><p>The Deloitte Citation Situation - $98K Controversy Explained</p><p>GPTZero used our Citation Check to analyze the 234 page report and identified more than 30 issues out of the total 141 citations, including 19 hallucinations. Using GPTZero’s citation check would have saved ~$5000 per citation, all within minutes.</p><p><img src="https://gptzero.me/news/content/images/icon/square_logo-3.png" alt=""><span>AI Detection Resources | GPTZero</span><span>Nazar Shmatko</span></p></div><p><img src="https://gptzero.me/news/content/images/thumbnail/pexels-anildonoji-18541728-1-1.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://gptzero.me/news/making-america-hallucinate-again-gptzero-detects-new-errors-in-major-government-report/"><div><p>Making America Hallucinate Again? GPTZero Detects New Errors in Major Government Report</p><p>On May 22, the U.S. Presidential Commission to Make America Healthy Again (MAHA), led by health secretary Robert F. Kennedy Jr., released a major report on the causes of chronic diseases in children. Yet within a week, news outlets including NOTUS, the New York Times and Washington Post reported</p><p><img src="https://gptzero.me/news/content/images/icon/square_logo-4.png" alt=""><span>AI Detection Resources | GPTZero</span><span>Paul Esau</span></p></div><p><img src="https://gptzero.me/news/content/images/thumbnail/Screenshot-2025-07-03-at-6.33.41---PM-1.png" alt="" onerror="this.style.display = 'none'"></p></a></figure>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tree-sitter vs. Language Servers (240 pts)]]></title>
            <link>https://lambdaland.org/posts/2026-01-21_tree-sitter_vs_lsp/</link>
            <guid>46719899</guid>
            <pubDate>Thu, 22 Jan 2026 14:47:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lambdaland.org/posts/2026-01-21_tree-sitter_vs_lsp/">https://lambdaland.org/posts/2026-01-21_tree-sitter_vs_lsp/</a>, See on <a href="https://news.ycombinator.com/item?id=46719899">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  
  
  <h5>21 Jan 2026</h5>



  

  
  
  



<p>I got asked a good question today: what is the difference between <a href="https://en.wikipedia.org/wiki/Tree-sitter_%28parser_generator%29">Tree-sitter</a> and a <a href="https://en.wikipedia.org/wiki/Language_Server_Protocol">language server</a>? I don’t understand how either of these tools work in depth, so I’m just going to explain from an <em>observable</em>, <em>pragmatic</em> point of view.</p>
<h2 id="tree-sitter">
  Tree-sitter
  <a href="#tree-sitter">#</a>
</h2>
<p>Tree-sitter is a <em>parser generator</em>. What this means is that you can hand Tree-sitter a description for a programming language and it will create a program that will parse that language for you. What’s special about Tree-sitter is that it is a.) fast, and b.) can tolerate <em>syntax errors</em> in the input. These two properties make Tree-sitter ideal for creating syntax highlighting engines in text editors. When you’re editing a program, <em>most of the time</em> the program will be in a syntactically invalid state. During that time, you don’t want your colors changing or just outright breaking while you’re typing. Naïve regex-based syntax highlighters frequently suffer from this issue.</p>
<p>Tree-sitter also provides a query language where you can make queries against the parse tree. I use this in the Emacs <a href="https://codeberg.org/ashton314/citar-typst">package I’m trying to develop</a> to add <a href="https://typst.app/">Typst</a> support to the <a href="https://github.com/emacs-citar/citar">Citar</a> citation/bibliography tool: I can ask Tree-sitter to find a particular syntax object; it is safer and more robust than using a regular expression because it can do similar parsing to the Typst engine itself.</p>
<p>In short, Tree-sitter provides syntax highlighting that is faithful to how the language implementation parses the program, instead of relying on regular expressions that incidentally come close.</p>
<h2 id="language-server">
  Language server
  <a href="#language-server">#</a>
</h2>
<p>A <em>language server</em> is a program that can analyze a program and report interesting information about that program to a text editor. A standard, called the <a href="https://en.wikipedia.org/wiki/Language_Server_Protocol">Language Server Protocol (LSP)</a>, defines the kinds of JSON messages that pass between a text editor and the server. The protocol is an open standard; any language and any text editor can take advantage of the protocol to get nice smart programming helps in their system. Language servers can provide information like locating the definition of a symbol, possible completions at the cursor point, etc. to a text editor which can then decide how and when to display or use this information.</p>
<p>Language servers solve the “


<span>
  \(N \times M\)
</span>
 problem” where <span>
  \(N\)
</span>
 programming languages and <span>
  \(M\)
</span>
 text editors would mean there have to be <span>
  \(N \times M\)
</span>
 implementations for language analyzers. Now, every language just needs a language server, and every editor needs to be able to speak the LSP protocol.</p>
<p>Language servers are powerful because they can hook into the language’s runtime and compiler toolchain to get <em>semantically correct</em> answers to user queries. For example, suppose you have two versions of a <code>pop</code> function, one imported from a <code>stack</code> library, and another from a <code>heap</code> library. If you use a tool like the <a href="https://github.com/jacktasia/dumb-jump">dumb-jump</a> package in Emacs<label for="sn1"></label>

<span>
I just want to say that I think dumb-jump is very cool and I am not trying to knock it down at all. It’s honest about its limitations and can be handy when you do not have a language server available.
</span>
and you use it to jump to the definition for a call to <code>pop</code>, it might get confused as to where to go because it’s not sure what module is in scope at the point. A language server, on the other hand, should have access to this information and would not get confused.</p>
<h3 id="using-a-language-server-for-highlighting">
  Using a language server for highlighting
  <a href="#using-a-language-server-for-highlighting">#</a>
</h3>
<p>It <em>is</em> possible to use the language server for syntax highlighting. I am not aware of any particularly strong reasons why one would want to (or <em>not</em> want to) do this. The language server can be a more complicated program and so could surface particularly detailed information about the syntax; it might also be slower than tree-sitter.</p>
<p>Emacs’ built-in LSP client, <a href="https://github.com/joaotavora/eglot">Eglot</a>, recently added <code>eglot-semantic-tokens-mode</code> to support syntax highlighting as provided from the language server. I have tried this a little bit in Rust code and it seems fine; the Tree-sitter-based syntax highlighting has been working just fine for me, so I will probably stick to that unless I find a compelling reason to use the LSP-based highlighting.</p>

<p>I wrote all of the above article. I did not ask an LLM to generate any portion of it. Please know that whenever you read something on my blog, it comes 100% from a human—me, Ashton Wiersdorf.</p>
<p>I am not so anti-AI to say that LLMs are worthless or should never be used. I’ve used LLMs a little bit. I think they’re fantastic at translating between languages; this seems to be something that they should be good at doing. They’re helpful at writing some boring parts of the code I write. However, most of the time I find that I can typically write the tricky bits of the code about as fast as I could specify to an LLM what I want.</p>
<p>I know that an LLM could have generated a facile pile of text much like the above, and honestly it would probably be decently helpful. However, know that what you have just read came directly from the fingers of a person who thought about the topic and bent his effort to helping you understand. This is from <em>real</em> human who understands the meaning behind each word here. I do not play games with syntax and generate answer-shaped blog posts. There is real meaning here. Enjoy it, and go forth and make more of it.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In Europe, Wind and Solar Overtake Fossil Fuels (586 pts)]]></title>
            <link>https://e360.yale.edu/digest/europe-wind-solar-fossil-fuels</link>
            <guid>46719491</guid>
            <pubDate>Thu, 22 Jan 2026 14:14:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://e360.yale.edu/digest/europe-wind-solar-fossil-fuels">https://e360.yale.edu/digest/europe-wind-solar-fossil-fuels</a>, See on <a href="https://news.ycombinator.com/item?id=46719491">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                  
<div>

  <figure>

    <div>
      
                  
      <p><a href="https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=1200&amp;h=800&amp;auto=compress%2Cformat&amp;fit=crop&amp;dm=1769079918&amp;s=c67f214d895b44db7e6420c0a3bfc1aa" data-caption="Rooftop solar panels in Beeskow, Germany." data-credit="Pexels">
  
  
  
    
  
  
  
      
    
                
    
                
    
                
    
                
    
                        
  <img sizes="(min-width: 1450px) 832px, (min-width: 620px) 620px, 100vw" srcset="https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=1200&amp;h=800&amp;auto=compress%2Cformat&amp;fit=crop&amp;dm=1769079918&amp;s=c67f214d895b44db7e6420c0a3bfc1aa 1200w, https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=200&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769079918&amp;s=ac44a2f3862a1d7b22fb35c3a1d0e352 200w, https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=400&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769079918&amp;s=3c6b60c6321fd63f129208a6b8c09d33 400w, https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=600&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769079918&amp;s=5f04396b725cd8972d48c16af1ad0a97 600w, https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=800&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769079918&amp;s=76d4bd20af816f9a9a55c5905f0eb479 800w, https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=1000&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769079918&amp;s=c7877029453366a27a3adcac42eecdbf 1000w" src="https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=400&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769079918&amp;s=3c6b60c6321fd63f129208a6b8c09d33" alt="Rooftop solar panels in Beeskow, Germany.">
</a>
      </p>
          </div>

        <figcaption>
              <p><span>Rooftop solar panels in Beeskow, Germany.</span>
          <span>Pexels</span></p>
    </figcaption>
    
  </figure>

</div> <!-- imageBlock -->
            

<div>
  <p>Last year, for the first time, wind and solar supplied more power than fossil fuels to the E.U., according to a new analysis.</p><p>The shift is largely due to the rapid expansion of solar energy, which is growing faster than any other source of electricity. Together, wind and solar generated 30 percent of E.U. power last year, while fossil fuels provided 29 percent, according to the <a href="https://ember-energy.org/latest-insights/european-electricity-review-2026/">analysis</a> from Ember, a think tank based in London. Including hydro, renewables provided nearly half of all E.U. power in 2025.</p>
</div>
            
<div>

  <figure>

    <div>
      
                  
      <p><a href="https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=980&amp;h=804&amp;auto=compress%2Cformat&amp;fit=crop&amp;dm=1769082265&amp;s=8ca86b66039abe22d21e745cabbe70c6" data-caption="E.U. power generation." data-credit="EMBER / ADAPTED BY YALE ENVIRONMENT 360">
  
  
  
    
  
  
  
      
    
                
    
                
    
                
    
                              
  <img sizes="(min-width: 1450px) 617px, (min-width: 1000px) 460px, (min-width: 600px) 60vw, 100vw" srcset="https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=980&amp;h=804&amp;auto=compress%2Cformat&amp;fit=crop&amp;dm=1769082265&amp;s=8ca86b66039abe22d21e745cabbe70c6 980w, https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=200&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769082265&amp;s=ee4a99833329e05dd94a3daff509e74c 200w, https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=400&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769082265&amp;s=29b561390a3c2023f01fa6359c3faea1 400w, https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=600&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769082265&amp;s=213daf0b6f44a1854fc82d927a38fa6f 600w, https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=800&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769082265&amp;s=28d524c1498db30ddb2725b3148c58c9 800w" src="https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=400&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769082265&amp;s=29b561390a3c2023f01fa6359c3faea1" alt="E.U. power generation.">
</a>
      </p>
          </div>

        <figcaption>
              <p><span>E.U. power generation.</span>
          <span>EMBER / ADAPTED BY YALE ENVIRONMENT 360</span></p>
    </figcaption>
    
  </figure>

</div> <!-- imageBlock -->
            

<div>
  <p>The analysis finds that solar is making gains in every E.U. country, while coal is broadly in retreat. Last year, solar alone supplied more than 20 percent of power in Hungary, Cyprus, Greece, Spain, and the Netherlands. Meanwhile, in 19 European countries, coal accounted for less than 5 percent of power. In 2025, both <a href="https://www.bloomberg.com/news/articles/2025-06-20/ireland-shuts-last-coal-plant-as-europe-s-phaseout-accelerates">Ireland</a> and <a href="https://www.spglobal.com/energy/en/news-research/latest-news/electric-power/040125-finland-shuts-last-coal-fired-power-plant-at-salmisaari-ending-the-era-of-coal">Finland</a> joined the ranks of European countries that have shuttered their last remaining coal plants.</p><p>Warming, however, continues to challenge the shift to clean energy as <a href="https://www.theguardian.com/environment/2025/nov/29/climate-crisis-depleting-europe-groundwater-reserves-analysis">drought</a> saps hydropower. Last year, hydro output dropped slightly in the E.U., and natural gas power rose to compensate.&nbsp;</p><p>“The next priority for the E.U. should be to put a serious dent in reliance on expensive, imported gas,” said Ember analyst Beatrice Petrovich. “Gas not only makes the E.U. more vulnerable to energy blackmail, it’s also driving up prices.”</p><p>In parts of Europe, there are signs that increasingly cheap batteries are beginning to displace natural gas in the early evening, when power demand is high, but solar output is waning. Said Petrovich, “As this trend accelerates it could limit how much gas is needed in evening hours, therefore stabilizing prices.”</p><h2><strong>ALSO ON YALE E360</strong></h2><p><a href="https://e360.yale.edu/features/europe-water-micropollutants"><i><strong>An E.U. Plan to Slash Micropollutants in Wastewater Is Under Attack</strong></i></a></p>
</div>
                </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qwen3-TTS Family Is Now Open Sourced: Voice Design, Clone, and Generation (566 pts)]]></title>
            <link>https://qwen.ai/blog?id=qwen3tts-0115</link>
            <guid>46719229</guid>
            <pubDate>Thu, 22 Jan 2026 13:51:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qwen.ai/blog?id=qwen3tts-0115">https://qwen.ai/blog?id=qwen3tts-0115</a>, See on <a href="https://news.ycombinator.com/item?id=46719229">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Douglas Adams on the English–American cultural divide over "heroes" (406 pts)]]></title>
            <link>https://shreevatsa.net/post/douglas-adams-cultural-divide/</link>
            <guid>46719222</guid>
            <pubDate>Thu, 22 Jan 2026 13:50:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shreevatsa.net/post/douglas-adams-cultural-divide/">https://shreevatsa.net/post/douglas-adams-cultural-divide/</a>, See on <a href="https://news.ycombinator.com/item?id=46719222">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <header>
    
    <p><time datetime="2026-01-14T00:00:00Z">January 14, 2026</time></p>
  </header>
  <p>In 2000, Douglas Adams made an interesting observation that I keep returning to.</p>
<p>A user on Slashdot named “FascDot Killed My Pr” had asked the following question (where HGttG = Hitchhiker’s Guide to the Galaxy):</p>
<blockquote>
<p><em>Comedy….or Tragedy?</em></p>
<p><em>First, a big thank-you. You’ve made a lasting contribution to “our” culture (or should that be “culture”?)</em></p>
<p><em>I first read HGttG in my early teens. I doubled over laughing the whole time. I read and reread the entire series, bought both Dirk Gently books AND Last Chance to See. Loved them all and wouldn’t trade having read them for anything. (btw, the first mental ward scene in Long Dark Teatime is a no-foolin’, all-time classic.)</em></p>
<p><em>However, a few years ago I was talking to a (then) classmate. Very smart, philosophy-major type. He said (paraphrased) “I thought that HGttG was depressing. Such nihilism.” At the time I thought “Hmmm…I didn’t SEE a black beret on his head….”. But every reading of the series since then his comment has struck me as more true–especially in the case of Arthur Dent. In fact, far from being funny, I now find Dent’s character depressing–he’s not just a loser, he literally has no control over his life at all (except in So Long for a while). And the control he does have does him no good (e.g. Earth is destroyed while he’s trying to save his house.)</em></p>
<p><em>So my question is: When you were writing these books did you feel you were being gaily whimsical or did you instead feel frustrated and cynical?</em></p>
</blockquote>
<p>Douglas Adams replied with:</p>
<blockquote>
<p>I suspect there is a cultural divide at work here. In England our heroes tend to be characters who either have, or come to realise that they have, no control over their lives whatsoever – Pilgrim, Gulliver, Hamlet, Paul Pennyfeather (from Decline and Fall), Tony Last (from A Handful of Dust). We celebrate our defeats and our withdrawals – the Battle of Hastings, Dunkirk, almost any given test match. There was a wonderful book published, oh, about twenty years ago I think, by Stephen Pile called the Book of Heroic Failures. It was staggeringly huge bestseller in England and sank with heroic lack of trace in the U.S. Stephen explained this to me by saying that you cannot make jokes about failure in the States. It’s like cancer, it just isn’t funny at any level. In England, though, for some reason it’s the thing we love most. So Arthur may not seem like much of a hero to Americans – he doesn’t have any stock options, he doesn’t have anything to exchange high fives about round the water-cooler. But to the English, he is a hero. Terrible things happen to him, he complains about it a bit quite articulately, so we can really feel it along with him - then calms down and has a cup of tea. My kind of guy!</p>
<p>I’ve hit a certain amount of difficulty over the years in explaining this in Hollywood. I’m often asked ‘Yes, but what are his goals?’ to which I can only respond, well, I think he’d just like all this to stop, really. It’s been a hard sell. I rather miss David Vogel from the film process. He’s the studio executive at Disney who was in charge of the project for a while, but has since departed. There was a big meeting at one time to discuss, amongst other things, Arthur’s heroicness or lack of it. David suddenly asked me ‘Does Arthur’s presence in the proceedings make a difference to the way things turn out?’ to which I said, slightly puzzled, ‘Well, yes.’ David smiled and said ‘Good. Then he’s a hero.’</p>
<p>In the current, latest version of the screenplay, I think that Arthur’s non-heroic heroism is now absolutely preserved, and I’m pleased with the way he works out.</p>
</blockquote>
<p>(<a href="https://entertainment.slashdot.org/story/00/06/21/1217242/douglas-adams-answers-finally">Douglas Adams Answers (Finally) - Slashdot</a>)</p>
<p>I think I have more to say about this, and will try to come back and add more here, but meanwhile a few things at random:</p>
<ul>
<li>
<p>As a matter of fact, I <em>have</em> read <em>The Book of Heroic Failures</em> (1979) with great enjoyment. (<a href="https://shreevatsa.wordpress.com/2011/03/21/the-book-of-heroic-failures/">Post from 2011</a> —&nbsp;I only wrote four sentences of my own, but one of them was “Too many books have been written in praise of competence; this book provides an antidote by celebrating failure as only a British author can.”)</p>
</li>
<li>
<p>I think he is right that this goes over better (generally speaking) in England than in the USA. Of course one can make jokes <em>mocking</em> failure, but someone who fails does not automatically become endearing (in a kind of everyman way) in America the way they would in England. It seems to me that Americans are more likely to feel either contempt or pity than to feel kinship: or at any rate, they regard the failure as a setback or interesting circumstance, rather than the natural/default state of the world. (As someone who is neither American nor English, I am of course not someone whose opinions you should pay any heed to.)</p>
</li>
</ul>
<ul>
<li>As we live our lives, are we merely victims subject to winds of chance and external circumstance, or are we powerful agents fashioning our own stories, making our own luck? Obviously the answer is “both”, but perhaps the most distinctively American trait is to lean more towards the latter.</li>
</ul>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Satya Nadella: "We need to find something useful for AI" (117 pts)]]></title>
            <link>https://www.pcgamer.com/software/ai/microsoft-ceo-warns-that-we-must-do-something-useful-with-ai-or-theyll-lose-social-permission-to-burn-electricity-on-it/</link>
            <guid>46718485</guid>
            <pubDate>Thu, 22 Jan 2026 12:41:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcgamer.com/software/ai/microsoft-ceo-warns-that-we-must-do-something-useful-with-ai-or-theyll-lose-social-permission-to-burn-electricity-on-it/">https://www.pcgamer.com/software/ai/microsoft-ceo-warns-that-we-must-do-something-useful-with-ai-or-theyll-lose-social-permission-to-burn-electricity-on-it/</a>, See on <a href="https://news.ycombinator.com/item?id=46718485">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div id="article-body">

<div data-nosnippet="" id="1co3zt3-r7I">
<div>
<p><span>Conversation with Satya Nadella, CEO of Microsoft | World Economic Forum Annual Meeting 2026 - YouTube</span>
<img src="https://img.youtube.com/vi/1co3zt3-r7I/maxresdefault.jpg" alt="Conversation with Satya Nadella, CEO of Microsoft | World Economic Forum Annual Meeting 2026 - YouTube" data-aspect-ratio="16/9" loading="lazy">
</p>
</div>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 234.67 165.33"><path fill="red" d="M229.763 25.817c-2.699-10.162-10.65-18.165-20.748-20.881C190.716 0 117.333 0 117.333 0S43.951 0 25.651 4.936C15.553 7.652 7.6 15.655 4.903 25.817 0 44.236 0 82.667 0 82.667s0 38.429 4.903 56.85C7.6 149.68 15.553 157.681 25.65 160.4c18.3 4.934 91.682 4.934 91.682 4.934s73.383 0 91.682-4.934c10.098-2.718 18.049-10.72 20.748-20.882 4.904-18.421 4.904-56.85 4.904-56.85s0-38.431-4.904-56.85"></path><path fill="#fff" d="m93.333 117.559 61.333-34.89-61.333-34.894z"></path></svg>
<a href="https://youtu.be/1co3zt3-r7I" target="_blank" data-url="https://youtu.be/1co3zt3-r7I" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Watch On <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 507.9 113.39"><g fill="#fff"><path d="M64.792 80.99V32.396l42.082 24.297zm93.803-63.285a20.285 20.285 0 0 0-14.32-14.32C131.642 0 80.99 0 80.99 0S30.337 0 17.705 3.385a20.286 20.286 0 0 0-14.32 14.32C0 30.338 0 56.693 0 56.693S0 83.049 3.385 95.68A20.285 20.285 0 0 0 17.705 110c12.632 3.386 63.285 3.386 63.285 3.386s50.652 0 63.285-3.386a20.284 20.284 0 0 0 14.32-14.32c3.385-12.632 3.385-38.988 3.385-38.988s0-26.355-3.385-38.988m94.473 74.326c.887-2.314 1.332-6.098 1.332-11.35V58.556c0-5.097-.445-8.822-1.332-11.178-.888-2.355-2.452-3.533-4.69-3.533-2.163 0-3.69 1.178-4.577 3.533-.888 2.356-1.332 6.081-1.332 11.178V80.68c0 5.25.424 9.035 1.275 11.35.848 2.318 2.392 3.475 4.633 3.475 2.239 0 3.803-1.157 4.691-3.475zm-17.953 11.122c-3.207-2.16-5.486-5.52-6.835-10.079-1.352-4.554-2.027-10.617-2.027-18.185v-10.31c0-7.644.771-13.784 2.316-18.417 1.544-4.633 3.956-8.011 7.24-10.135 3.282-2.123 7.587-3.186 12.916-3.186 5.251 0 9.459 1.082 12.626 3.243 3.165 2.162 5.482 5.542 6.95 10.136 1.466 4.595 2.2 10.715 2.2 18.36v10.31c0 7.567-.714 13.65-2.142 18.243-1.43 4.595-3.747 7.955-6.951 10.077-3.205 2.124-7.548 3.186-13.03 3.186-5.64 0-10.06-1.082-13.263-3.243m248.053-57.981c-.81 1.005-1.352 2.646-1.621 4.923-.272 2.278-.404 5.734-.404 10.367v5.097h11.697V60.46c0-4.555-.155-8.011-.463-10.367-.309-2.355-.868-4.014-1.678-4.98-.812-.966-2.067-1.449-3.766-1.449-1.7 0-2.954.503-3.765 1.506zm-2.025 29.886v3.591c0 4.557.132 7.974.404 10.251.269 2.279.828 3.94 1.68 4.982.849 1.041 2.16 1.564 3.938 1.564 2.392 0 4.035-.927 4.923-2.781.887-1.853 1.37-4.942 1.447-9.268l13.785.812c.077.62.116 1.469.116 2.548 0 6.565-1.795 11.47-5.387 14.712-3.589 3.242-8.669 4.865-15.232 4.865-7.876 0-13.398-2.47-16.564-7.414-3.168-4.94-4.75-12.586-4.75-22.935V63.589c0-10.657 1.641-18.436 4.924-23.342 3.281-4.903 8.9-7.355 16.854-7.355 5.482 0 9.691 1.004 12.626 3.012 2.933 2.01 5 5.137 6.197 9.383 1.197 4.247 1.796 10.117 1.796 17.607v12.163h-26.757m-284.953-1.33-18.187-65.68h15.869l6.37 29.77c1.623 7.339 2.82 13.594 3.591 18.766h.464c.54-3.706 1.738-9.922 3.591-18.65l6.603-29.886h15.869l-18.417 65.68v31.51h-15.754v-31.51M322.115 34.23v71.007h-12.511l-1.39-8.688h-.347c-3.399 6.564-8.496 9.845-15.291 9.845-4.71 0-8.185-1.543-10.425-4.633-2.24-3.087-3.359-7.915-3.359-14.48V34.23h15.985v52.126c0 3.168.348 5.426 1.043 6.776.695 1.353 1.853 2.027 3.475 2.027 1.39 0 2.722-.423 3.996-1.275 1.274-.849 2.22-1.928 2.838-3.241V34.229h15.986m81.995.001v71.007h-12.511l-1.391-8.688h-.345c-3.402 6.564-8.498 9.845-15.292 9.845-4.711 0-8.186-1.543-10.426-4.633-2.24-3.087-3.358-7.915-3.358-14.48V34.23h15.985v52.126c0 3.168.347 5.426 1.041 6.776.696 1.353 1.855 2.027 3.476 2.027 1.391 0 2.723-.423 3.996-1.275 1.275-.849 2.22-1.928 2.839-3.241V34.229h15.985"></path><path d="M365.552 20.908h-15.87v84.329h-15.637v-84.33h-15.869V8.05h47.376v12.858m76.811 53.636c0 5.174-.215 9.229-.639 12.162-.424 2.937-1.139 5.021-2.143 6.255-1.004 1.236-2.357 1.854-4.053 1.854a7.404 7.404 0 0 1-3.65-.927c-1.12-.618-2.026-1.544-2.722-2.78V50.796c.54-1.93 1.467-3.513 2.78-4.749 1.313-1.234 2.74-1.853 4.285-1.853 1.623 0 2.876.637 3.766 1.91.886 1.275 1.505 3.418 1.853 6.43.348 3.011.523 7.297.523 12.857zm14.652-28.964c-.967-4.478-2.531-7.721-4.692-9.73-2.163-2.007-5.136-3.011-8.919-3.011-2.935 0-5.676.83-8.224 2.49a16.926 16.926 0 0 0-5.908 6.545h-.117l.001-37.416h-15.405v100.777h13.204l1.622-6.717h.347c1.235 2.393 3.088 4.285 5.56 5.675 2.47 1.39 5.213 2.085 8.225 2.085 5.404 0 9.382-2.491 11.931-7.471 2.548-4.982 3.823-12.76 3.823-23.341V64.23c0-7.953-.484-14.17-1.448-18.65"></path></g></svg></a>
</div><p id="704a06b8-be94-4ad8-b9ed-eae5c57359a6">In a conversation at this year's rich person convention—aka the World Economic Forum—Microsoft CEO Satya Nadella warned that AI will lose public support unless it's used to "do something useful that changes the outcomes of people and communities and countries and industries."</p><p id="704a06b8-be94-4ad8-b9ed-eae5c57359a6-2">On the supply side, Nadella says that AI companies and policy makers must build out "a ubiquitous grid of energy and tokens," which is the task currently making it <a data-analytics-id="inline-link" href="https://www.pcgamer.com/hardware/memory/ram-and-storage-is-ridiculously-expensive-right-now-because-of-drumroll-ai-of-course-and-theres-little-reason-to-think-prices-will-drop-any-time-soon/" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.pcgamer.com/hardware/memory/ram-and-storage-is-ridiculously-expensive-right-now-because-of-drumroll-ai-of-course-and-theres-little-reason-to-think-prices-will-drop-any-time-soon/">impossible to buy a stick of RAM at a reasonable price</a>. But after that, he says it's on employers and job seekers to, more or less, just start using AI.</p><p>"The demand side of this is a little bit like, every firm has to start by using it," said Nadella, throwing in some industry-standard hyperbole by calling AI a "cognitive amplifier" that gives you "access to infinite minds." The CEO added that the AI industry needs to encourage job seekers to pick up AI skills (undefined), in the same way people master Excel to make themselves more employable.</p><p>"People need to say, 'Oh, I pick up this AI skill, and now I'm a better provider of some product or service in the real economy," said Nadella.</p><p>He did at least provide one real example of what he means by all this: "When a doctor can … spend more time with the patient, because the AI is doing the transcription and entering the records in the EMR system, entering the right billing code so that the healthcare industry is better served across the payer, the provider, and the patient, ultimately—that's an outcome that I think all of us can benefit from."</p><p>I wonder if I'll really want to spend more time talking to my doctor with an AI eavesdropper listening intently for reasons to reclassify my preventative care visit as a more expensive diagnostic visit (could we just redesign the US healthcare system instead?), but at least for some doctors, AI recording and note-taking tools have already been helpful. <a data-analytics-id="inline-link" href="https://catalyst.nejm.org/doi/full/10.1056/CAT.25.0040" target="_blank" data-url="https://catalyst.nejm.org/doi/full/10.1056/CAT.25.0040" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link">One study</a> said that medical professionals reported "tremendous benefits" from using AI scribes, while calling for more research.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-a7KtHSRbpCtxF5tmfB6YFE"><section><p>Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team.</p></section></div><p>I also find automatic transcription tools useful, but if I were banking on general purpose LLMs being as revolutionary as personal computers and the internet, I'd find it worrying how many applications boil down to transcribing audio, summarizing text, and fetching code snippets.</p><p>There are reasons to be skeptical about the idea that we're going to reorganize society around these functions. LLMs are error-prone—a <a data-analytics-id="inline-link" href="https://www.pcgamer.com/software/ai/uk-police-chief-resigns-over-ai-intel-embarrassment-blames-political-and-media-frenzy-rather-than-his-forces-failure-to-fact-check-copilot/" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.pcgamer.com/software/ai/uk-police-chief-resigns-over-ai-intel-embarrassment-blames-political-and-media-frenzy-rather-than-his-forces-failure-to-fact-check-copilot/">UK police chief just resigned</a> over a Microsoft Copilot error—and a <a data-analytics-id="inline-link" href="https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/" target="_blank" data-url="https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link">recent report</a> by researchers associated with the MIT Media Lab suggests that despite billions in investment, "95% of organizations are getting zero return" from adopting AI.</p><p>Addressing the notion that AI is a bubble waiting to burst, Nadella said that it's only a bubble if tech company partnerships and infrastructure spending are all there is to it. He's confident, however, that AI will "bend the productivity curve" and bring "economic growth all around the world, not just economic growth driven by capital expense."</p><p>Just let me know when I can buy RAM again!</p><div id="slice-container-person-a7KtHSRbpCtxF5tmfB6YFE-lzCiymxZTIKfmEj1wCr24YNp0qaYm2eT"><figure data-bordeaux-image-check="false"><div><picture data-hydrate="false"><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW-140-80.jpg.webp 140w" sizes="99vw" data-normal="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW.jpg" data-original-mos="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW.jpg" data-pin-nopin="true" data-slice-image="true"><source type="image/jpeg" srcset="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW-140-80.jpg 140w" sizes="99vw" data-normal="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW.jpg" data-original-mos="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW.jpg" data-pin-nopin="true" data-slice-image="true"><img src="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW.jpg" alt="Razer Blade 16 gaming laptop" srcset="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW-140-80.jpg 140w" sizes="99vw" loading="lazy" data-normal="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW.jpg" data-original-mos="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/YwnnY4PZ7TcCYKRYDho7VW.jpg" data-pin-nopin="true" data-slice-image="true"></picture></div></figure></div>
</div>


<div data-hydrate="true" id="slice-container-authorBio-a7KtHSRbpCtxF5tmfB6YFE"><p>Tyler grew up in Silicon Valley during the '80s and '90s, playing games like Zork and Arkanoid on early PCs. He was later captivated by Myst, SimCity, Civilization, Command &amp; Conquer, all the shooters they call "boomer shooters" now, and PS1 classic Bushido Blade (that's right: he had Bleem!). Tyler joined PC Gamer in 2011, and today he's focused on the site's news coverage. His hobbies include amateur boxing and adding to his 1,200-plus hours in Rocket League.</p></div>
</section>

<div x-show="$store.Viafoura.showWidgets" x-cloak="" data-component-name="Viafoura:Comments" x-data="ViafouraComments('300px')" data-nosnippet="" data-community-guidelines-text="<p class='vfcustom-community-guidelines'>Please follow our <a href=&quot;https://www.pcgamer.com/about-pc-gamer/#section-community-guidelines&quot; target=&quot;_blank&quot;>community guidelines</a>.</p>">
<p>You must confirm your public display name before commenting</p>
<p>Please logout and then login again, you will then be prompted to enter your display name.</p>
</div>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Design Thinking Books You Must Read (280 pts)]]></title>
            <link>https://www.designorate.com/design-thinking-books/</link>
            <guid>46718061</guid>
            <pubDate>Thu, 22 Jan 2026 11:51:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.designorate.com/design-thinking-books/">https://www.designorate.com/design-thinking-books/</a>, See on <a href="https://news.ycombinator.com/item?id=46718061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
			
<p>Can you think that following a design thinking process with five steps turns you into a creative innovator?! Believe me, it isn’t and never has been this way. The spread of the term design thinking is aligned with a significant amount of misleading criticism. The doubts about the effectiveness of design thinking are influenced by the promotional language used by some companies, training places, and public speakers. The truth is that there is no secret recipe to turn someone into a creative designer. Yet, there is a way to use the design expertise inside each of us. <a href="https://www.designorate.com/design-thinking-guide-what-why-how/">Understanding the design thinking core values</a> can help team members improve their design ability and appreciate the creative practice inside the organization to achieve the next competitive advantage. This is why I wanted to share with you those key design thinking books to learn the core principles underpinning the design practice.</p>



<p>This is an updated list of design thinking books that I keep adding to their new book suggestions. So, please keep the link or subscribe to the newsletters to receive updates once new books are added. I am also starting to add papers that represent the cornerstone in the design thinking principles that I believe are as important as the book. In this update, two books and one paper added: The Science of Artificial, Wicked Problems in Design Thinking, and How Designers Think.</p>



<p>Previously, we explored different challenges that can be faced when applying design thinking inside the organization ( Why Companies Need to Apply Design Thinking and Why Companies Need to Apply Design Thinking). The majority of these factors rely on the lack of understanding of the core value of design thinking, which can be a reason for over-promotion and misuse of a commercialized language (check <a href="https://www.designorate.com/why-design-thinking-doesnt-work/" rel="bookmark">Why Design Thinking Doesn’t Work</a>). Above all, many design thinking trainers are not designers themselves and never practice the creative practice before teaching it which causes the gap between classrooms and practices.</p>



<div id="ez-toc-container">

<nav><ul><li><a href="#Design_Thinking_Books">Design Thinking Books</a><ul><li><a href="#Design_Expertise_by_Kees_Dorst">Design Expertise by Kees Dorst</a></li><li><a href="#Frame_Innovation_by_Kees_Dorst">Frame Innovation by Kees Dorst</a></li><li><a href="#Design_Thinking_Understanding_How_Designers_Think_and_Work">Design Thinking: Understanding How Designers Think and Work</a></li><li><a href="#Change_by_Design_by_Tim_Brown">Change by Design by Tim Brown</a></li><li><a href="#The_Design_of_Everyday_Things_by_Don_Norman">The Design of Everyday Things by Don Norman</a></li><li><a href="#How_Designers_Think_by_Bryan_Lawson">How Designers Think? by Bryan Lawson</a></li></ul></li><li><a href="#The_Science_of_Artificial_by_Herbert_Simon">The Science of Artificial by Herbert Simon</a></li><li><a href="#Wicked_Problems_in_Design_Thinking_Paper_by_Richard_Buchanan">Wicked Problems in Design Thinking (Paper) by Richard Buchanan</a></li><li><a href="#The_Dilemmas_in_a_General_Theory_of_Planning_by_Rittel_and_Webber">The Dilemmas in a General Theory of Planning by Rittel and Webber</a></li><li><a href="#The_New_Process_New_Vocabulary_Axiofact_A_tefact_Memoranda_by_Gilbert_Cockton">The New Process, New Vocabulary: Axiofact = A_tefact + Memoranda by Gilbert Cockton</a></li><li><a href="#References">References</a></li></ul></nav></div>




<p>To expand my knowledge of the core values behind design thinking, I thought I would share with you some of the book titles that highlight design characteristics. Each of these books explores design from a specific perspective. Learning about these design aspects is essential for both designers and non-designers before jumping to learn design thinking. While there are several books about design thinking toolkits, the books below don’t teach you to design thinking methodology but the core principles behind design thinking to develop new alternatives of ideas and improve the analytical thinking of problems and solutions. They aim to guide you in understanding the core values and practices of design as a collaborative process. By acquiring this knowledge, you can effectively apply any of the design thinking processes we discussed earlier in previous articles with effectiveness. I am sure that those are not the only books out there, so please share with us your book suggestions in the comments below the article.</p>



<p><strong><em>Related article:</em></strong></p>



<p><a href="https://www.designorate.com/the-double-diamond-design-thinking-process-and-how-to-use-it/">The Double Diamond Design Thinking Process and How to Use it</a></p>



<p><a href="https://www.designorate.com/what-is-design-and-what-is-not/">What is Design? And What is not?</a></p>



<p><em><a href="https://www.designorate.com/design-thinking-guide-what-why-how/" rel="bookmark">Design Thinking Guide: What, Why and How</a></em></p>



<p><em><a href="https://www.designorate.com/why-design-thinking-doesnt-work/" rel="bookmark">Why Design Thinking Doesn’t Work</a></em></p>



<p><em><a href="https://www.designorate.com/measuring-the-impact-of-design-thinking/" rel="bookmark">Me</a><a href="https://www.designorate.com/measuring-the-impact-of-design-thinking/" rel="bookmark">asuring the Impact of Design Thinking</a></em></p>



<h3><span id="Design_Expertise_by_Kees_Dorst"></span>Design Expertise by Kees Dorst<span></span></h3>



<p>The <a href="https://amzn.to/34vj2Fj">Design Expertise</a>, written by Lawson and Dorst, focuses on the understanding of design practice in the creative industry. The book aims to explore the nature of design from a practitioner’s perspective. It starts by exploring the different definitions of design and how they contributed to identifying the border of the discipline of design.</p>


<div>
<figure><a href="https://www.designorate.com/wp-content/uploads/2020/04/Design-Expertise.jpg"><img fetchpriority="high" decoding="async" width="800" height="500" src="https://www.designorate.com/wp-content/uploads/2020/04/Design-Expertise.jpg" alt="Design Expertise" srcset="https://www.designorate.com/wp-content/uploads/2020/04/Design-Expertise.jpg 800w, https://www.designorate.com/wp-content/uploads/2020/04/Design-Expertise-300x188.jpg 300w, https://www.designorate.com/wp-content/uploads/2020/04/Design-Expertise-768x480.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption>Design Expertise by Kees Dorst</figcaption></figure></div>


<p>The book presents design work for different designers and tries to use this overview of their work to provide a practical example of design characteristics. This book provides you with a base idea about design, what it is, and its characteristics. Exploring the characteristics through <a href="https://www.designorate.com/design-thinking-reshaped-microsoft-products/">design thinking case studies</a>, and examples helps you see design’s core value. This value is the main cornerstone behind the application of the design thinking process.</p>



<h3><span id="Frame_Innovation_by_Kees_Dorst"></span>Frame Innovation by Kees Dorst<span></span></h3>



<p>One of the main design characteristics is to solve problems or move from one position to an improved one. However, this can’t be achieved without a clear idea of the problem and its different borders. In his book <a href="https://amzn.to/2yPd9a9">Frame Innovation</a>, Dorst explores the cognitive design process’s problem and solution frames. Also, he explores how designers move from one frame to another and how this feedback process contributes toward an optimum solution for wicked problems.</p>


<div>
<figure><a href="https://www.designorate.com/wp-content/uploads/2020/04/Fram-Innovation.jpg"><img decoding="async" width="800" height="500" src="https://www.designorate.com/wp-content/uploads/2020/04/Fram-Innovation.jpg" alt="frame innovation" srcset="https://www.designorate.com/wp-content/uploads/2020/04/Fram-Innovation.jpg 800w, https://www.designorate.com/wp-content/uploads/2020/04/Fram-Innovation-300x188.jpg 300w, https://www.designorate.com/wp-content/uploads/2020/04/Fram-Innovation-768x480.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption>Frame Innovation by Kees Dorst</figcaption></figure></div>


<p>Many of the design thinking process models move from the exploration stage (divergent) to defining the solution (conversion). While this practice shares the principle of critical thinking, they all move between the problem frame and solution frame. Through this book, you will explore the principles and practices of problem/solution frames to develop creative potential ideas.</p>



<p>The book extends discussion of of the principle of frame innovation by covering the opportunities and challenges related to its application in creative industries. The book ends by putting a practice action plan to move toward using the frame innovation in different business models.</p>



<h3><span id="Design_Thinking_Understanding_How_Designers_Think_and_Work"></span>Design Thinking: Understanding How Designers Think and Work<span></span></h3>



<p>In this small yet informative book, <a href="https://amzn.to/2RwszqB">Design Thinking</a>, Nigel Cross explores how designers think and reach creative ideas in the design field and the nature of design from the perspective of idea formation. To this goal, the book overviews design practice based on observing and interviewing creative designers and exploring expert tips with them. Design processes try to explore the design expertise from creating the idea to applying it. However, the design ability comes earlier when ideas are formulated. The book’s first chapter explores this design ability and how each of us has a level of design ability to develop new ideas. Yet, some people are more designers than others, which is known in Lusy Kimbel’s two papers as the creative class (Rethinking Design Thinking: <a href="https://www.tandfonline.com/doi/abs/10.2752/175470811X13071166525216?casa_token=eQlFFIjY110AAAAA:8TLvc0-_HjZPsM0NbT1y3lya4XulQVlfc-4f0uPelIYFwHjyJK53Aj-33_yvdsFKgxfy1EdeRbUNXw">Part 1</a> and <a href="https://www.tandfonline.com/doi/abs/10.2752/175470812X13281948975413?casa_token=Vlf9CU0l2A4AAAAA:pFFp0YVRmEZdRBbNDR2EWVin0DkNV72L6SaSIHgx7TnHKbC8ZqDdPC8Yvm1TNQtPsJIt6z65m9pxyg">Part 2</a>).</p>


<div>
<figure><a href="https://www.designorate.com/wp-content/uploads/2020/04/Design-Thinking.jpg"><img loading="lazy" decoding="async" width="800" height="500" src="https://www.designorate.com/wp-content/uploads/2020/04/Design-Thinking.jpg" alt="Design Thinking: Understanding How Designers Think and Work" srcset="https://www.designorate.com/wp-content/uploads/2020/04/Design-Thinking.jpg 800w, https://www.designorate.com/wp-content/uploads/2020/04/Design-Thinking-300x188.jpg 300w, https://www.designorate.com/wp-content/uploads/2020/04/Design-Thinking-768x480.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption>Design Thinking: Understanding How Designers Think and Work by Nigel Cross</figcaption></figure></div>


<p>The book overviews designers’ practice in different fields and stories. The aim of this overview through creative designers’ experience is to build an understanding of the inspiration or exploration stage in the design thinking process. For instance, what is brainstorming, and why is it applied at an early point in the design thinking process (How to Successfully Apply Inspiration in Design Thinking)? Linking similar questions to the practice helps you map your practice to rational reasoning and subsequently improves the progress of the process in the future.</p>



<h3><span id="Change_by_Design_by_Tim_Brown"></span>Change by Design by Tim Brown<span></span></h3>



<p><a href="https://www.amazon.co.uk/Change-Design-Revised-Updated-Organizations/dp/0062856626/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=1680977260&amp;sr=8-10">Change by Design</a>, by Tim Bowen, CEO of the IDEO, is probably one of the commonly known books about design thinking because of the popularity of the IDEO in the application of design thinking in various social innovation contexts. In his book, Tim Brown manifests his ideology about design thinking and interprets it from the organisational perspective. The book aims to clarify what design thinking is, and where to go from theory to practice. In the first part, the books focus on the main concepts of design thinking (check Design Thinking Tools and Methods Complete Guide), such as extending behind the aesthetics, shifting toward a human-centred approach (i.e. improving customer experience and building inclusive design), the power of prototyping, and the importance of storytelling. The second part of the book aims to interpret these principles for practicality to identify the business opportunities for design thinking and the use of design to achieve innovation inside organisations through creative collaboration between stakeholders.<span>&nbsp;</span></p>


<div>
<figure><img loading="lazy" decoding="async" width="800" height="500" src="https://www.designorate.com/wp-content/uploads/2020/04/ChangebyDesign02.jpg" alt="Change by Design book" srcset="https://www.designorate.com/wp-content/uploads/2020/04/ChangebyDesign02.jpg 800w, https://www.designorate.com/wp-content/uploads/2020/04/ChangebyDesign02-300x188.jpg 300w, https://www.designorate.com/wp-content/uploads/2020/04/ChangebyDesign02-768x480.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"><figcaption>Change by Design by Tim Brown.</figcaption></figure></div>


<p>The book is a good resource for both designers and business people to understand design thinking and its applications. Despite several criticisms of the IDEO design thinking model, the book describes the theoretical base of design thinking, which could have a positive, innovative impact on organisations, especially if applied properly to develop viable business strategies. The <a href="http://www.designkit.org/">IDEO Field Guide</a> can be a good companion for the book as it presents a toolbox to apply Tim Brown’s ideology in practice.</p>



<h3><span id="The_Design_of_Everyday_Things_by_Don_Norman"></span>The Design of Everyday Things by Don Norman<span></span></h3>



<p>Don Norman is one of the leading professors in behaviour psychology and human-computer interaction (HCI). His book, <a href="https://amzn.to/2Rx5XGb">The Design of Everyday Things</a>, is based on a simple observation: why do we love and hate some elements in our lives? And what is the psychology behind our behaviour toward products? Addressing these two questions presents a cornerstone of your design practice. For example, why do some people love products such as Apple, <a href="https://www.designorate.com/what-is-market-segmentation-and-why-designers-should-understand/">Mini Cooper</a>, or <a href="https://www.designorate.com/ikea-sustainable-design-strategy/">IKEA</a>? By understanding how consumers love or hate products, the design team can target these features to build an empathic relationship between the product (or service) and the client, known as <a href="https://www.designorate.com/empathic-design-approach-to-successful-design/">emphatic design</a>.</p>


<div>
<figure><a href="https://www.designorate.com/wp-content/uploads/2020/04/Everyday-Things.jpg"><img loading="lazy" decoding="async" width="800" height="500" src="https://www.designorate.com/wp-content/uploads/2020/04/Everyday-Things.jpg" alt="The Design of Everyday Things " srcset="https://www.designorate.com/wp-content/uploads/2020/04/Everyday-Things.jpg 800w, https://www.designorate.com/wp-content/uploads/2020/04/Everyday-Things-300x188.jpg 300w, https://www.designorate.com/wp-content/uploads/2020/04/Everyday-Things-768x480.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption>The Design of Everyday Things for Don Norman</figcaption></figure></div>


<p>The book explores human-centred design and its impact on usability interaction design principles, as well as user experience. While other books covered this aspect of design experience, Norman studied the experience from a psychological point of view to examine this complex design process. The book covers the psychology behind our daily actions, knowledge, design limitations, and human error. Later, the book explores design thinking as a tool to solve problems and the usage of the Design Council Double Diamond design thinking process. The book is not only for UX designers but for designers from different practices, as you can learn the following:</p>



<ol>
<li>How the brain works and the psychology related to products and services,</li>



<li>The limitations related to our experience with interacting with designs around and</li>



<li>Human error and a bad design causes .</li>
</ol>



<h3><span id="How_Designers_Think_by_Bryan_Lawson"></span>How Designers Think? by Bryan Lawson<span></span></h3>



<p><a href="https://www.amazon.co.uk/How-Designers-Think-Process-Demystified/dp/0750660775/ref=tmm_pap_swatch_0?_encoding=UTF8&amp;qid=1701980560&amp;sr=1-1" target="_blank" rel="noreferrer noopener nofollow">How Designers Think?</a> by Bryan Lawson is one of the design thinking books I recommend for my students who are still new to problem-solving and understanding the philosophical approach underpinning the problem and solution space in the design thinking process, How Designer Think for Bryan Lawson overviews the design definition, the relation between problem and solutions and the design thinking process. </p>


<div>
<figure><img loading="lazy" decoding="async" width="1024" height="624" src="https://www.designorate.com/wp-content/uploads/2023/12/HowDesigersThink-1-1024x624.jpg" alt="" srcset="https://www.designorate.com/wp-content/uploads/2023/12/HowDesigersThink-1-1024x624.jpg 1024w, https://www.designorate.com/wp-content/uploads/2023/12/HowDesigersThink-1-300x183.jpg 300w, https://www.designorate.com/wp-content/uploads/2023/12/HowDesigersThink-1-768x468.jpg 768w, https://www.designorate.com/wp-content/uploads/2023/12/HowDesigersThink-1.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>How Designers Think? by Bryan Lawson</figcaption></figure></div>


<p>Unlike other books, Lawson doesn’t aim to teach you his method or derive a specific point; it is more like a discussion book to allow you to reflect and synthesise on the design practice and finally come up with your conclusion. The book presents a flow of ideas as a case study, making it easy to understand and enjoyable for new readers in design thinking. I recommend reading it before moving to more advanced books such as The Science of Artificial.</p>



<h2><span id="The_Science_of_Artificial_by_Herbert_Simon"></span>The Science of Artificial by Herbert Simon <span></span></h2>



<p>The <a href="https://www.amazon.co.uk/Sciences-Artificial-MIT-Press/dp/0262691914" target="_blank" rel="noreferrer noopener nofollow">Science of Artificial </a>is one of Simon’s most famous and irritating works based on three lectures for him at MIT in 1968, a year before the book was first published. The book discusses the nature of human thinking and the “artefact.” In eight chapters, it explores how humans use artefacts to solve everyday problems. His expression of human rationale is expressed with three premises:</p>



<ul>
<li>The limitations in the human’s cognitive ability</li>



<li>The time available to make a decision, and</li>



<li>The complexity of the problem</li>
</ul>



<p>Based on these three premises, he concluded that we are under the illusion that we can choose the optimal solution for problems. Instead, we find a way to determine the reasonable solutions (check <a href="https://www.designorate.com/the-six-hats-of-critical-thinking-and-how-to-use-them/">What Are The Six Thinking Hats? And How to Use Them?</a>).</p>


<div>
<figure><img loading="lazy" decoding="async" width="1024" height="624" src="https://www.designorate.com/wp-content/uploads/2023/12/The_Science_of_Aritifical-1024x624.jpg" alt="" srcset="https://www.designorate.com/wp-content/uploads/2023/12/The_Science_of_Aritifical-1024x624.jpg 1024w, https://www.designorate.com/wp-content/uploads/2023/12/The_Science_of_Aritifical-300x183.jpg 300w, https://www.designorate.com/wp-content/uploads/2023/12/The_Science_of_Aritifical-768x468.jpg 768w, https://www.designorate.com/wp-content/uploads/2023/12/The_Science_of_Aritifical.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>The Science of Artificial by Herbert Simon </figcaption></figure></div>


<p>Simon was awarded a Nobel Prize for his theory and its contribution to economic rationality. According to the above theory, Simon defined three problem-solving activities: the ability to conduct a heuristic search for alternatives, evaluate solutions, and allocate resources for search. He illustrates this concept in his statement:</p>



<p><em>” Human problem solving involves nothing more than varying mixtures of trial and error and selectivity. The selectivity derives from various rules of thumb, or heuristics, suggesting which paths should be tried first and which promising leads.”</em></p>



<h2><span id="Wicked_Problems_in_Design_Thinking_Paper_by_Richard_Buchanan"></span>Wicked Problems in Design Thinking (Paper) by Richard Buchanan<span></span></h2>



<p><a href="https://web.mit.edu/jrankin/www/engin_as_lib_art/Design_thinking.pdf" target="_blank" rel="noreferrer noopener nofollow">Wicked Problems in Design Thinking</a> by Buchanan was published in Design Studies in 1992. Buchanan linked design and analytical philosophy by understanding the design problem’s nature and elements. He discussed two terms: “category” and “placement”, where we frame the different aspects of the problem. Buchanan describes them as follows:</p>



<p><em>“Understanding the difference between a category and a placement is essential if design thinking is to be regarded as more than a series of creative accidents. Categories have fixed meanings that are accepted within the framework of a theory or a philosophy and serve as the basis for analysing what already exists. Placements have boundaries to shape and constrain meaning but are not rigidly fixed and determinate. The boundary of placement gives a context or orientation to thinking, but the application to a specific situation can generate a new perception of that situation and, hence, a new possibility to be tested. Therefore, placements are sources of new ideas and possibilities when applied to problems in concrete circumstances.”</em></p>


<div>
<figure><img loading="lazy" decoding="async" width="1024" height="599" src="https://www.designorate.com/wp-content/uploads/2023/12/Wicked_Problems_Design_Thinking-1024x599.jpg" alt="" srcset="https://www.designorate.com/wp-content/uploads/2023/12/Wicked_Problems_Design_Thinking-1024x599.jpg 1024w, https://www.designorate.com/wp-content/uploads/2023/12/Wicked_Problems_Design_Thinking-300x176.jpg 300w, https://www.designorate.com/wp-content/uploads/2023/12/Wicked_Problems_Design_Thinking-768x449.jpg 768w, https://www.designorate.com/wp-content/uploads/2023/12/Wicked_Problems_Design_Thinking.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Wicked Problems in Design Thinking paper by Richard Buchanan</figcaption></figure></div>


<p>The expandable nature of the “placement” presents a critical element of wicked problems and how we can see them as a universal concept whose boundaries can change based on the situation. This manifestation of the definition of the problem elements presented the cornerstone for Kees Dorst’s problem/solution frame discussed in the earlier book Frame Innovation (<a href="https://www.designorate.com/8d-problem-solving-approach/">What is the 8D Problem-Solving?&nbsp;</a>).</p>



<p>His ideas of wicked problems link with Simon’s concept about the design thinking process and how it can be seen as a non-linear process where different design ideas interact in the design arena. Also, In this placement, Buchanan differentiated between four elements of the design thinking process:</p>



<ul>
<li>Signs: material objects</li>



<li>Things: actions</li>



<li>Thoughts: complex systems or environments, which is a weird characterisation.</li>
</ul>



<p>As he links the above elements and the two terms described earlier (category vs placement), he describes the nature of wicked problems:<br><em>“However, when a designer’s conceptual placements become categories of thinking, the result can be mannered imitations of an earlier invention that are no longer relevant to discovering specific possibilities in a new situation. Ideas are then forced onto a situation rather than discovered in the particularities and novel possibilities of that situation.”</em></p>



<p>The above manifestation describes how wicked problems are constructed and change over time, paving the way for a new perspective on problems and their analysis to identify new solutions (check also <a href="https://www.designorate.com/practice-guide-to-solve-problems-with-triz/">How to Use TRIZ in the Problem-Solving Process</a>).</p>



<h2><span id="The_Dilemmas_in_a_General_Theory_of_Planning_by_Rittel_and_Webber"></span>The Dilemmas in a General Theory of Planning by Rittel and Webber<span></span></h2>



<p><a href="https://web.mit.edu/jrankin/www/engin_as_lib_art/Design_thinking.pdf" target="_blank" rel="noreferrer noopener">The Dilemmas in a General Theory of Planning</a> by Rittel and Webber, despite its age, remains a seminal work that has significantly influenced the understanding of wicked problems. Published in 1969, this paper laid the groundwork for Kees Dorst’s Frame Innovation and Buchanan’s Wicked Problems,&nbsp;both of&nbsp;which we’ve discussed. While the&nbsp;paper’s focus is&nbsp;on planning and policy science,&nbsp;its insights can be applied&nbsp;to problem definition and the process of solving them.&nbsp;</p>


<div>
<figure><img loading="lazy" decoding="async" width="1024" height="599" src="https://www.designorate.com/wp-content/uploads/2024/05/Rittel-Webber-1024x599.jpg" alt="" srcset="https://www.designorate.com/wp-content/uploads/2024/05/Rittel-Webber-1024x599.jpg 1024w, https://www.designorate.com/wp-content/uploads/2024/05/Rittel-Webber-300x176.jpg 300w, https://www.designorate.com/wp-content/uploads/2024/05/Rittel-Webber-768x449.jpg 768w, https://www.designorate.com/wp-content/uploads/2024/05/Rittel-Webber.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>The Dilemmas in a General Theory of Planning by Rittel and Webber</figcaption></figure></div>


<p>Rittel and Webber distinguished between two types of problems: tame and wicked problems. Tame problems are well-defined and clearly stated, and there is a clear direction to finding the solution, such as scientific and business problems (check how this concept influenced <a href="https://www.designorate.com/practice-guide-to-solve-problems-with-triz/">TRIZ problem-solving</a>). In contrast, wicked problems are ill-defined, and we can’t define the problem until we reach a solution. However, a wicked problem is never solved, yet it moves from one state to an improved,&nbsp;desirable&nbsp;one.&nbsp;</p>



<p>The other nature of wicked problems is that we cannot reach a definitive formulation for them. To describe them, we need to develop an exhaustive inventory of conceivable solutions when asking questions about the problem. So, problem understanding and resolution are linked and change as we build an understanding of the problem at a particular moment in time.&nbsp;</p>



<h2><span id="The_New_Process_New_Vocabulary_Axiofact_A_tefact_Memoranda_by_Gilbert_Cockton"></span>The New Process, New Vocabulary: Axiofact = A_tefact + Memoranda by Gilbert Cockton<span></span></h2>



<p>As you can see, the above books and papers give us a novel look at design problems and how we perceive them. My question is, why do we see problems the way we used to? A big part of the answer lies in our language, which presents mental models that stand as barriers to seeing the core nature of problems, especially the wicked ones. Therefore, we needed new vocabulary that helped us to escape these constraints. <a href="http://library.usc.edu.ph/ACM/CHI%202017/2exab/ea747.pdf" target="_blank" rel="noreferrer noopener">The New Process, New Vocabulary: Axiofact = A_tefact + Memoranda</a>, by my PhD supervisor, Professor Gilbert Cockton, presents a cornerstone of new vocabularies that can help us see design thinking and how to solve problems.&nbsp;</p>



<figure><img loading="lazy" decoding="async" width="1024" height="599" src="https://www.designorate.com/wp-content/uploads/2024/05/Cockton-1024x599.jpg" alt="" srcset="https://www.designorate.com/wp-content/uploads/2024/05/Cockton-1024x599.jpg 1024w, https://www.designorate.com/wp-content/uploads/2024/05/Cockton-300x176.jpg 300w, https://www.designorate.com/wp-content/uploads/2024/05/Cockton-768x449.jpg 768w, https://www.designorate.com/wp-content/uploads/2024/05/Cockton.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The paper eliminated the so-called design thinking process, as the term employs a linear nature; while design thinking is far from linear, it is intersected activities. Cockton described the design practice as design arenas; these arenas are distinguishing “artefacts” and “memoranda.” The “artefact” represents the design outcome, and the “memoranda” is the thing to be borne in mind. This new terminology replaces the problem and solution spaces. However, the Latin root of an artefact means the product of change or doing some art. However, this term is limited as wicked problems are not understood until we solve them, which means artefacts. So, the outcome of the design arena may remain the same as the original state, or the change is against the target user, such as preventive design and design against crime. So, Cockton replaced the word artefact with A_tefact.&nbsp; The memoranda consist of three arenas:</p>



<ul>
<li><strong>Beneficiaries</strong>: The purpose of design</li>



<li><strong>Purposes</strong>: The Artefact and Evaluation</li>



<li><strong>Evaluations</strong>: Modifications to the Artefact</li>
</ul>


<div>
<figure><img loading="lazy" decoding="async" width="1024" height="655" src="https://www.designorate.com/wp-content/uploads/2024/05/Design_Arenas-1024x655.jpg" alt="" srcset="https://www.designorate.com/wp-content/uploads/2024/05/Design_Arenas-1024x655.jpg 1024w, https://www.designorate.com/wp-content/uploads/2024/05/Design_Arenas-300x192.jpg 300w, https://www.designorate.com/wp-content/uploads/2024/05/Design_Arenas-768x492.jpg 768w, https://www.designorate.com/wp-content/uploads/2024/05/Design_Arenas.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>The design arenas. Recreated from Cockton (2017).</figcaption></figure></div>


<p>Other terms were also introduced in the paper, such as episodes to replace stages (or phases) that are inherited from linear process age. The multiple foci (sequence by concurrency) replaced the centre of the process term to indicate the complex nature of the iteration with no simple way to describe it. The term “iteration” is replaced with balanced concurrent drama, and validation is replaced with the term “axiofact,” or the value generated. The new terminology presented in Cockton’s paper allows us to escape the old mental model when addressing wicked problems.&nbsp;If you check the <a href="https://www.designorate.com/using-the-mppf-method-in-the-double-diamond-design-process/">MPPF method in Design Thinking</a>, which we discussed previously, you will find it&nbsp;a&nbsp;useful&nbsp;tool&nbsp;as it can help us address wicked problems.&nbsp;</p>



<p>Each of the above books and papers focuses on specific aspects of design and how we observe the design thinking practice driven by feedback from both academia and industry. The different design thinking models are based on appreciating these characteristics of design and encouraging it inside the organization. By applying the steps alone, you will never reach any improved status. You need to recognize these characteristics of design and try to practice them during the design process. Again, the above books came to my mind as key books in design. I am sure there are other titles. So, please share it in the comments below.</p>



<h2><span id="References"></span>References<span></span></h2>



<p>Brown, T. and Katz, B., 2011. Change by design.&nbsp;<em>Journal of Product Innovation Management</em>,&nbsp;<em>28</em>(3), pp.381-383.</p>



<p>Buchanan, R., 1992. Wicked problems in design thinking.&nbsp;<em>Design issues</em>,&nbsp;<em>8</em>(2), pp.5-21.</p>



<p>Cockton, G., 2017, May. New process, new vocabulary: Axiofact= a_tefact+ memoranda. In&nbsp;<em>Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em>&nbsp;(pp. 747-757).</p>



<p>Cross, N., 2023.&nbsp;<em>Design thinking: Understanding how designers think and work</em>. Bloomsbury Publishing.</p>



<p>Dorst, K., 2015.&nbsp;<em>Frame innovation: Create new thinking by design</em>. MIT press.</p>



<p>Norman Donald, A., 2013.&nbsp;<em>The design of everyday things</em>. MIT Press.</p>



<p>Lawson, B., 2006.&nbsp;<em>How designers think</em>. Routledge.</p>



<p>Rittel, H.W. and Webber, M.M., 1973. Dilemmas in a general theory of planning.&nbsp;<em>Policy sciences</em>,&nbsp;<em>4</em>(2), pp.155-169.</p>



<p>Simon, H.A., 1988. The science of design: Creating the artificial.&nbsp;<em>Design Issues</em>, pp.67-82.</p>












		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Askers' vs. 'Guessers' (2010) (117 pts)]]></title>
            <link>https://www.theatlantic.com/national/2010/05/askers-vs-guessers/340891/</link>
            <guid>46717960</guid>
            <pubDate>Thu, 22 Jan 2026 11:40:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/national/2010/05/askers-vs-guessers/340891/">https://www.theatlantic.com/national/2010/05/askers-vs-guessers/340891/</a>, See on <a href="https://news.ycombinator.com/item?id=46717960">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-event-module="article body" data-flatplan-body="true"><p><small>This article is from the archive of our partner <a href="http://www.thewire.com/"><img alt="" height="17" src="https://cdn.theatlantic.com/assets/media/img/wire/logo.png?v=3" width="55"></a>.</small></p><p data-flatplan-paragraph="true">Let's say your husband or wife has a friend who will be coming to your city for two weeks on business. This friend writes to you and your spouse, asking if you can put him up while he's in town. Has this person committed a gross violation of etiquette? Whether you answer yes or no may speak to whether you're an Asker or a Guesser--the two personality types described in a three-year-old Web comment that has lately taken on a second life as a full-on blog meme.</p><p data-flatplan-paragraph="true">On January 16, 2007, Andrea Donderi <a data-event-element="inline link" href="http://ask.metafilter.com/55153/Whats-the-middle-ground-between-FU-and-Welcome#830421" id="cbjw" title="responded to an Ask MetaFilter post">responded to an Ask MetaFilter post</a> that dealt with a houseguest-related situation like the one described above. Donderi's take on the situation is as elegant as it is provocative. Basically, she says, there are two types of people in the world:</p><div><blockquote>This is a classic case of Ask Culture meets Guess Culture. In some families, you grow up with the expectation that it's OK to ask for anything at all, but you gotta realize you might get no for an answer. This is Ask Culture.<p> In Guess Culture, you avoid putting a request into words unless you're pretty sure the answer will be yes. Guess Culture depends on a tight net of shared expectations. A key skill is putting out delicate feelers. If you do this with enough subtlety, you won't even have to make the request directly; you'll get an offer. Even then, the offer may be genuine or pro forma; it takes yet more skill and delicacy to discern whether you should accept.</p></blockquote></div><p data-flatplan-paragraph="true">Over the weekend, <a data-event-element="inline link" href="http://www.guardian.co.uk/lifeandstyle/2010/may/08/change-life-asker-guesser" id="ed9a" title="Oliver Burkeman">Oliver Burkeman</a> wrote a column for The Guardian taking up Donderi's dichotomy and asking, "Are you an Asker or a Guesser?" A number of bloggers took the bait, expanding into broader thoughts about the niceties of social etiquette. Here's what they had to say:<b><br></b></p><div><ul><li><b>Contributes to Personal, Professional, International Tensions</b>&nbsp; In his column for The Guardian, <a href="http://www.guardian.co.uk/lifeandstyle/2010/may/08/change-life-asker-guesser" id="rpk9" title="Burkeman">Burkeman</a> notes that neither type's approach is wrong per se, "but when an Asker meets a Guesser, unpleasantness results. An Asker won't think it's rude to request two weeks in your spare room, but a Guess culture person will hear it as presumptuous and resent the agony involved in saying no. Your boss, asking for a project to be finished early, may be an overdemanding boor – or just an Asker, who's assuming you might decline. If you're a Guesser, you'll hear it as an expectation. This is a spectrum, not a dichotomy, and it explains cross-cultural awkwardnesses, too: Brits and Americans get discombobulated doing business in Japan, because it's a Guess culture, yet experience Russians as rude, because they're diehard Askers."</li></ul></div><div><ul><li><b>We Ask Strangers and Close Friends</b>&nbsp; Libertarian blogger <a href="http://www.juliansanchez.com/2010/05/10/asking-and-guessing/" id="d0e4" title="Julian Sanchez">Julian Sanchez</a> offers a sociological reading of Donderi's theory that's worth perusing in full. "The polite indirection of 'Guess Culture' is... often a way of preserving a deliberate ambiguity, which we generally want to do in social relationships where there's an intermediate level of intimacy—whereas relationships at the poles, with either close friends or strangers, tend to be governed by more direct asks," Sanchez writes. "We do this, I think, precisely because those intermediate relationships <i>are </i>ambiguous: We’re indirect because we’re negotiating just where on the gradient we fall ... To ask too directly at that stage can seem rude because it effectively demands a binary verdict on a work in progress."</li></ul></div><div><ul><li><b>Actually, One of Them <i>Is </i>Wrong</b>&nbsp; The New Republic's <a href="http://www.tnr.com/blog/jonathan-chait/ask-dont-guess" id="fm4l" title="Jonathan Chait">Jonathan Chait</a> takes a hard line. "This is actually pretty simple: Guessers are wrong, and Askers are right. Asking is how you actually determine what the Asker wants and the giver is willing to receive. Guessing culture is a recipe for frustration. What's more, Guessers, who are usually trying to be nice and are holding themselves to a higher level of politeness, ruin things for the rest of us ... Guessers are what forces people with poor social discernment, like me, to regard all kinds of interactions as a minefield of awkwardness."</li></ul></div><div><ul><li><b>It's Not So Black and White</b>&nbsp; The Incidental Economist's <a href="http://theincidentaleconomist.com/dont-ask-dont-tell-now-dont-guess/" id="tbw4" title="Austin Frakt">Austin Frakt</a> endorses a more situationally fluid approach. "The problem with assuming one way is better than another is that it ignores the obvious temporal heterogeneity in preferences. The 'requester' (whether of Asker or Guesser type) is in more in need of a 'yes' (or 'no') response from the 'requestee' (again, of either type) at some times than others. Likewise, a requestee is more likely to say 'yes' (or 'no') at some times than at others ... Therefore, it is perfectly sensible to be an Asker for some things at some times and a Guesser for other things (or even the same things) at another."</li></ul></div><p data-flatplan-paragraph="true">What say you--does the Asker/Guesser model ring true? (Or, put another way: We're not asking, but some people might want to leave comments, and perhaps you know someone who does...)</p><p><small>This article is from the archive of our partner <i>The Wire</i>.</small></p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We will ban you and ridicule you in public if you waste our time on crap reports (893 pts)]]></title>
            <link>https://curl.se/.well-known/security.txt</link>
            <guid>46717556</guid>
            <pubDate>Thu, 22 Jan 2026 10:48:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://curl.se/.well-known/security.txt">https://curl.se/.well-known/security.txt</a>, See on <a href="https://news.ycombinator.com/item?id=46717556">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[ISO PDF spec is getting Brotli – ~20 % smaller documents with no quality loss (147 pts)]]></title>
            <link>https://pdfa.org/want-to-make-your-pdfs-20-smaller-for-free/</link>
            <guid>46717507</guid>
            <pubDate>Thu, 22 Jan 2026 10:41:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pdfa.org/want-to-make-your-pdfs-20-smaller-for-free/">https://pdfa.org/want-to-make-your-pdfs-20-smaller-for-free/</a>, See on <a href="https://news.ycombinator.com/item?id=46717507">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For nearly three decades; or November 1996 to be exact, PDFs have relied on Deflate—the same compression algorithm that powers your ZIP files. Meanwhile, the web moved on. In 2015, Google introduced Brotli, a compression algorithm so efficient it now powers 95% of internet traffic. Websites got faster. Downloads got smaller. CDNs got cheaper.</p>
<p><strong>Now PDFs are getting the same upgrade.</strong></p>
<p>The PDF Association is bringing this battle-tested web compression technology <a href="https://pdfa.org/brotli-compression-coming-to-pdf/">into the PDF specification</a> itself. After a decade of Brotli proving its worth across billions of web requests daily, it's now getting ready to make it's introduction into ISO 32000.</p>
<p>With <a href="https://pdfa.org/member/itext-group-nv/">iText</a>, we can help drive widespread adoption with a <strong>production-ready Brotli encoder and decoder</strong> for the PDF ecosystem. The result? <strong>15-25% smaller files</strong> with zero quality loss, using the same algorithm trusted by Google, Cloudflare, and every major CDN.</p>
<h2 id="why-pdf-compression-has-struggled-to-evolve">Why PDF compression has struggled to evolve</h2>
<p>PDF compression has been stuck in 1996 for a good reason: <strong>backward compatibility is sacred</strong>. The PDF Association operates under a strict principle—any new feature must work seamlessly with existing readers, or it risks fragmenting the ecosystem. Adding a new compression algorithm isn't just a technical change; it's a <em>breaking change</em> that could render documents unreadable in older software. This creates a high barrier for innovation.</p>
<p>Beyond compatibility concerns, there are other practical challenges. The PDF specification moves slowly by design—it's an ISO standard that requires consensus by hundreds of stakeholders. Compression algorithms must be <strong>royalty-free</strong> (ruling out patented options), <strong>widely supported</strong> across platforms, and <strong>battle-tested</strong> in production.</p>
<p>Finally, the ecosystem is conservative: enterprises and governments rely on PDFs for archival and legal documents that must remain accessible for decades, making any breaking change a risk that needs extraordinary justification.</p>
<h2 id="encoding-and-decoding-technical-implementation">Encoding and decoding: Technical implementation</h2>
<p>To get Brotli compression working within the iText SDK, we need to solve two problems: reading documents, and also writing them.</p>
<p>Let's start with the easiest one; reading documents.</p>
<h3>Decoding: Advanced plumbing work</h3>
<p>First of all, let's look at how the content of a page is stored within a PDF. We can demonstrate this with just the classic "Hello World" text example.</p>
<p>The following PDF syntax simply displays the text "Hello World!" on a page:</p>
<pre>5 0 obj                 % Unique identifier to reference this content from other places within the PDF
&lt;&lt;/Length 49&gt;&gt;stream    % Meta data for the stream object. Here it contains a Length value to indicate how many bytes are there after the `stream` keyword.
q                       % the actual content
BT
/F1 12 Tf
37 788.33 Td
(Hello World!)Tj
ET
Q
endstream               % Indicates the end of the stream object
endobj                  % Indicates the end of the referenceable object
</pre>
<p>So, if we need to render or do anything else with the content, it would look like the following:</p>
<pre>|------------------------|
| Get stream based on id |
|------------------------|
           ||
           \/
|------------------------|
|      Read content      |
|------------------------|
           ||
           \/
|------------------------|
|    Render/Do stuff     |
|    with the content    |
|------------------------|
</pre>
<p>Okay, so now we have a high-level view how PDF processors handle the low-level processing of those stream objects, we can dive a little deeper!</p>
<p>Let's take a look at the following PDF stream object where the content is encoded using the Deflate algorithm.</p>
<pre>5 0 obj
&lt;&lt;/Filter/FlateDecode/Length 36&gt;&gt;stream                  % The meta data now now includes `Filter`
xœmÍÂ0„ïûëM/1?Æl®‚âUømI)Íûºm¢...            % Reduced for clarity
endstream
endobj
</pre>
<p>First of all, we notice there is an additional Key <code>Filter</code> with a value of <code>FlateDecode</code> in the metadata.<br>
This can be interpreted the following way: "The content of this stream object is only usable after its <code>FlateDecode</code> filter is applied".</p>
<p>So how does this change our working implementation?</p>
<pre>|------------------------|
| Get stream based on id |
|------------------------|
           ||
           \/
|------------------------|
|      Read content      |
|------------------------|
           ||
           \/
|------------------------|
|         Decode         |
|     based on Filter    |
|------------------------|
           ||
           \/
|------------------------|
|    Render/Do stuff     |
|    with the content    |
|------------------------|
</pre>
<p>We can now see we require an operation on the content before it's usable. The PDF specification already provides a variety of ways to write the content of the PDF streams.</p>
<table>
<tbody>
<tr>
<td><strong>Filter name</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>ASCIIHexDecode</td>
<td>Decodes ASCII hexadecimal data to binary.</td>
</tr>
<tr>
<td>ASCII85Decode</td>
<td>Decodes ASCII base-85 data to binary.</td>
</tr>
<tr>
<td>LZWDecode</td>
<td>Decompresses data using LZW compression.</td>
</tr>
<tr>
<td>FlateDecode</td>
<td>Decompresses data using zlib/deflate compression.</td>
</tr>
<tr>
<td>RunLengthDecode</td>
<td>Decompresses data using run-length encoding.</td>
</tr>
<tr>
<td>CCITTFaxDecode</td>
<td>Decompresses CCITT fax-encoded monochrome images.</td>
</tr>
<tr>
<td>JBIG2Decode</td>
<td>Decompresses JBIG2-encoded monochrome image data.</td>
</tr>
<tr>
<td>DCTDecode</td>
<td>Decompresses JPEG DCT-based image data.</td>
</tr>
<tr>
<td>JPXDecode</td>
<td>Decompresses JPEG 2000 wavelet-based image data.</td>
</tr>
<tr>
<td>Crypt</td>
<td>Decrypts data encrypted by a security handler.</td>
</tr>
</tbody>
</table>
<p>So the idea for Brotli is to simply add another <code>Filter</code> implementation.What we need to get it working into iText is actually pretty minimal:</p>
<ol>
<li>Get the decoding implementation from Google's repository.</li>
<li>Write some plumbing code to call it from iText</li>
<li>Hook up the plumbing code to the <code>BrotliDecode</code> filter</li>
</ol>
<p>For the first step we simply embedded <strong>Google's reference Java Brotli decoder</strong> straight from their official repository into our kernel module.</p>
<h4>Why embed the decoder?</h4>
<p>By embedding Google's reference implementation directly, we guarantee:</p>
<ul>
<li><strong>Zero dependency hell</strong>: No version conflicts with other libraries</li>
<li><strong>Consistent behavior</strong>: Same decoder on all platforms</li>
<li><strong>Long-term stability</strong>: We control the code, even if upstream changes</li>
<li><strong>Automatically generate C# version</strong>: Using our porting mechanism we can have a C# implementation</li>
</ul>
<p>The plumbing implementation lives in <code>BrotliFilter.java</code>, which plugs into iText's existing filter pipeline:</p>
<pre>public class BrotliFilter extends MemoryLimitsAwareFilter {
    @Override
    public byte[] decode(byte[] b, PdfName filterName, PdfObject decodeParams,
            PdfDictionary streamDictionary) {
        try {
            final byte[] buffer = new byte[DEFAULT_BUFFER_SIZE];
            final ByteArrayInputStream input = new ByteArrayInputStream(b);
            final ByteArrayOutputStream output = enableMemoryLimitsAwareHandler(streamDictionary);
            final BrotliInputStream brotliInput = new BrotliInputStream(input);
            int len;
            while ((len = brotliInput.read(buffer, 0, buffer.length)) &gt; 0) {
                output.write(buffer, 0, len);
            }
            brotliInput.close();
            return output.toByteArray();
        } catch (IOException e) {
            throw new PdfException(KernelExceptionMessageConstant.FAILED_TO_DECODE_BROTLI_STREAM, e);
        }
    }
}
</pre>
<p>Let's break down what's happening in this implementation:</p>
<ul>
<li><strong>Memory Safety First</strong>: The filter extends <code>MemoryLimitsAwareFilter</code>, which protects against decompression<br>
bombs—malicious PDFs that expand into gigabytes of data when decompressed. This is critical for production systems.</li>
<li><strong>Wrapped Input Stream</strong>: The compressed bytes <code>b</code> are wrapped in a <code>ByteArrayInputStream</code>, which is then passed to<br>
Google's <code>BrotliInputStream</code>. This is where the magic happens—<code>BrotliInputStream</code> handles all the heavy lifting of Brotli decompression.</li>
</ul>
<p>As you can see, writing the plumbing code is pretty easy because of iText's architecture.</p>
<p>The last thing to do is to ensure iText knows which implementation to associate with the <code>/BrotliDecode</code> filter.</p>
<p>This is also pretty trivial. The filter is registered automatically in <code>FilterHandlers.java</code> alongside <code>/FlateDecode</code> and the other standard PDF<br>
filters:</p>
<pre>public final class FilterHandlers {
    private static final Map&lt;PdfName, IFilterHandler&gt; defaults;

    static {
        Map&lt;PdfName, IFilterHandler&gt; map = new HashMap&lt;&gt;();

        map.put(PdfName.FlateDecode, new FlateDecodeFilter());
        map.put(PdfName.Fl, new FlateDecodeFilter());
        //other implementations removed for clarity

        // we add our implementation
        map.put(PdfName.BrotliDecode, new BrotliFilter());

        defaults = Collections.unmodifiableMap(map);
    }
}
</pre>
<p>That's it. From this point on, <strong>any PDF with <code>/BrotliDecode</code> streams just works</strong>. No configuration needed.</p>
<p>Now we could have stopped here—our SDK could process Brotli-compressed PDFs from other sources. But reading isn't enough. To truly bring Brotli to the PDF ecosystem, we needed to let developers create these smaller files. That meant solving the encoding problem.</p>
<p>And encoding turned out to be significantly more complex than decoding.</p>
<h3>Encoding: a separate module for compression</h3>
<h4>The problem: iText’s compression was hardcoded</h4>
<p>Before Brotli, iText only supported two compression modes for PDF streams:</p>
<ol>
<li><strong>Flate compression</strong></li>
<li><strong>No compression</strong></li>
</ol>
<p>This logic was baked directly into the stream-writing code—there was no abstraction, no plugin point. If you wanted to use a different compression algorithm, you were out of luck.</p>
<p>To support Brotli (and future algorithms), we needed to <strong>introduce a new abstraction layer</strong>: <code>IStreamCompressionStrategy</code>.</p>
<pre>public interface IStreamCompressionStrategy {
   /**
    * Gets the PDF filter name that identifies this compression algorithm.
    *
    * @return the PDF name representing the compression filter
    */
   PdfName getFilterName();

   /**
    * Gets the decode parameters required for decompressing the stream.
    * &lt;p&gt;
    * Decode parameters provide additional information needed to correctly
    * decompress the stream data.
    *
    * @return the decode parameters as a PDF object, or {@code null} if not needed
    */
   PdfObject getDecodeParams();

   /**
    * Creates a new output stream that wraps the original stream and applies compression.
    * @param original the original output stream to wrap
    * @param stream the PDF stream being compressed (may be used for context or configuration)
    *
    * @return a new output stream that performs compression
    */
    OutputStream createNewOutputStream(OutputStream original, PdfStream stream);
}
</pre>
<p>This interface decouples compression logic from iText's core PDF writing machinery. Now, instead of hardcoding Flate everywhere, we can inject different strategies at runtime. To inject the required strategy we make use of the <code>DiContainer</code>. You can find more information about it here: <a href="https://kb.itextpdf.com/itext/adding-dependency-injection-to-the-pdfdocument-cla" target="_blank" rel="noopener">Adding Dependency Injection to the PdfDocument class</a>.</p>
<p>From now on when iText needs to compress a stream, it asks the <code>DiContainer</code> in the <code>PdfDocument</code>: <em>"Do you have an IStreamCompressionStrategy?"</em></p>
<ul>
<li><strong>If yes</strong>: Use the registered strategy (Brotli in this case)</li>
<li><strong>If no</strong>: Fall back to the default Flate compression</li>
</ul>
<p>This design gives us:</p>
<ul>
<li><strong>Zero coupling</strong>: iText Core no longer cares about the algorithm used</li>
<li><strong>Opt-in behavior</strong>: You only pay the cost if you use it</li>
<li><strong>Future-proof</strong>: New algorithms just implement the interface</li>
</ul>
<h4>The Second Problem: No Pure Java Encoder</h4>
<p>Here's where things got tricky. While Google's Brotli decoder has a pure Java implementation (which we embedded for reading), the official Brotli <em>encoder</em> <strong>is C++ only</strong>. To use it from Java, you need:</p>
<ul>
<li><strong>JNI bindings</strong> to call native code from Java</li>
<li><strong>Platform-specific native libraries</strong> (<code>.dll</code> for Windows, <code>.so</code> for Linux, <code>.dylib</code> for macOS)</li>
<li><strong>Build infrastructure</strong> to compile and ship these libraries for every platform</li>
</ul>
<p>For a heavily-used library like iText, shipping native binaries is a non-starter:</p>
<ul>
<li><strong>Deployment complexity</strong>: Users need to manage native libraries across platforms</li>
<li><strong>Security concerns</strong>: Native code introduces attack surfaces</li>
<li><strong>Build maintenance</strong>: We'd need to compile for Windows x64, Linux ARM, macOS Silicon, etc.</li>
<li><strong>Version conflicts</strong>: What if another library ships a different Brotli version?</li>
</ul>
<p>We needed a solution that handled this complexity <em>outside</em> iText's core.</p>
<p>That solution is a separate Maven module (<code>brotli-compressor</code>) that you add as an optional dependency. This<br>
module contains:</p>
<ul>
<li><strong>BrotliStreamCompressionStrategy</strong>: Implementation of <code>IStreamCompressionStrategy</code></li>
<li><strong>brotli4j dependency</strong>: A third-party library that wraps Google's C++ encoder with JNI</li>
</ul>
<p>Here's what <code>BrotliStreamCompressionStrategy</code> looks like:</p>
<pre>public class BrotliStreamCompressionStrategy implements IStreamCompressionStrategy {

    @Override
    public OutputStream createNewOutputStream(OutputStream original, PdfStream stream) {
        int compressionLevel = convertCompressionLevel(stream.getCompressionLevel());
        Encoder.Parameters params = Encoder.Parameters.create(compressionLevel);
        try {
            return new BrotliOutputStream(original, params);
        } catch (IOException e) {
            throw new PdfException(KernelExceptionMessageConstant.CANNOT_WRITE_TO_PDF_STREAM, e);
        }
    }

    @Override
    public PdfName getFilterName() {
        return PdfName.BrotliDecode; // This goes into the /Filter entry
    }
}
</pre>
<h4>The native wrapper: brotli4j</h4>
<p>Instead of writing JNI bindings ourselves, we rely on <a href="https://github.com/hyperxpro/Brotli4j" target="_blank" rel="noopener">brotli4j</a>—a mature, well-tested library that:</p>
<ul>
<li>Wraps Google's official C++ Brotli encoder/decoder</li>
<li>Ships <strong>pre-compiled native libraries</strong> for all major platforms (Windows x64/ARM, Linux x64/ARM, macOS Intel/Silicon)</li>
<li>Automatically extracts the correct native library at runtime (no manual setup)</li>
<li>Is actively maintained and widely used (powers projects like Netty, OkHttp)</li>
</ul>
<p>By delegating to brotli4j, we get production-grade native bindings without maintaining our own JNI layer.</p>
<h4>Why keep encoding separate?</h4>
<p>You might ask: <em>"Why not bundle brotli4j in the kernel module like you did with the decoder?"</em></p>
<p>Great question. Here's the reasoning:</p>
<table>
<tbody>
<tr>
<td><strong>Aspect</strong></td>
<td><strong>Decoder (in kernel)</strong></td>
<td><strong>Encoder (separate module)</strong></td>
</tr>
<tr>
<td><strong>Necessity</strong></td>
<td>Required to read Brotli PDFs</td>
<td>Optional—only for <em>writing</em></td>
</tr>
<tr>
<td><strong>Dependencies</strong></td>
<td>Pure Java (Google's decoder)</td>
<td>Native code (brotli4j with JNI)</td>
</tr>
<tr>
<td><strong>Size impact</strong></td>
<td>~300KB of Java code</td>
<td>~2MB of native libraries</td>
</tr>
<tr>
<td><strong>Use frequency</strong></td>
<td>Every user needs to read PDFs</td>
<td>Most users stick with Flate</td>
</tr>
<tr>
<td><strong>Backward compat</strong></td>
<td>No breaking changes</td>
<td>Opt-in feature</td>
</tr>
</tbody>
</table>
<p>By keeping the encoder separate, we give users choice: add <code>brotli-compressor</code> if you need 20% smaller files, or stick with the default if native dependencies are a concern.</p>
<h4>Putting it all together: Full example</h4>
<p>Here's what it looks like to create a Brotli-compressed PDF:</p>
<p>First of all add the required dependencies. Notice you have to add iText's artifactory because of the experimental nature of the code, and so users don't accidentally enable it.</p>
<pre>&lt;repositories&gt;
  &lt;repository&gt;
    &lt;id&gt;itext-releases&lt;/id&gt;
    &lt;name&gt;iText Repository - releases&lt;/name&gt;
    &lt;url&gt;https://repo.itextsupport.com/releases&lt;/url&gt;
  &lt;/repository&gt;
&lt;/repositories&gt;

&lt;dependency&gt;
&lt;groupId&gt;com.itextpdf&lt;/groupId&gt;
&lt;artifactId&gt;brotli-compressor&lt;/artifactId&gt;
&lt;version&gt;{itext.version.bigger.then.9.5.0}&lt;/version&gt;
&lt;/dependency&gt;
</pre>
<pre>public static void main() {

// 1. Register the compression strategy
   DocumentProperties properties = new DocumentProperties();
   properties.registerDependency(IStreamCompressionStrategy.class, new BrotliStreamCompressionStrategy());
// 2. Create your PDF as normal
   PdfWriter writer = new PdfWriter("output.pdf");
   PdfDocument pdf = new PdfDocument(writer, properties);

// Everything from here on uses Brotli automatically
   Document doc = new Document(pdf);
   doc.add(new Paragraph("This text will be Brotli-compressed!"));
   doc.add(new Image(ImageDataFactory.create("chart.png")));
   doc.close();

}
</pre>
<p>When you open <code>output.pdf</code> in a text editor, you'll see some entries looking like this:</p>
<pre>5 0 obj
&lt;&lt;/Filter/BrotliDecode/Length 847&gt;&gt;stream
[binary Brotli-compressed data]
endstream
endobj
</pre>
<p>The PDF now uses <code>/BrotliDecode</code> instead of <code>/FlateDecode</code>, and the file is 15-25% smaller—with <strong>zero changes</strong> to your document-building code.</p>
<h3>The catch: Compatibility isn't universal (yet)</h3>
<p>Here's the honest truth: <strong>Brotli-compressed PDFs won't open in Adobe Acrobat Reader today</strong>. They won't render in your browser's built-in PDF viewer. Most third-party PDF libraries will reject them outright.</p>
<p>Why? Because <code>/BrotliDecode</code> isn't part of the official PDF specification yet. The PDF Association is actively working on adding it to ISO 32000 (the PDF standard), but until that's finalized and implementations roll out, Brotli PDFs exist in a gray area.</p>
<h4>What about forward compatibility?</h4>
<p>Here's the good news: <strong>Brotli PDFs are future-proof</strong>. Once the PDF Association finalizes the spec and vendors implement it, your existing Brotli-compressed documents will just work. You're not creating broken files—you're creating files that are <em>ahead of their time</em>.</p>
<p>Think of it like HTTP/2 in 2015. Early adopters who deployed it got immediate performance wins in their own<br>
infrastructure, and as browsers caught up, those benefits became universal. Brotli PDFs follow the same pattern.</p>
<h4>iText's commitment</h4>
<p>We're not shipping this as a toy feature. We're working directly with the PDF Association to:</p>
<ul>
<li><strong>Standardize the specification</strong> (syntax, decode parameters, dictionary support)</li>
<li><strong>Validate implementations</strong> across multiple platforms (Java, .NET, C++)</li>
<li><strong>Contribute test suites</strong> to ensure interoperability when other vendors adopt it</li>
<li><strong>Support migrations</strong> when the spec finalizes (we'll handle any breaking changes)</li>
</ul>
<p>By adopting Brotli compression now, you're not taking a risk—you're investing in a proven technology that's on a clear path to standardization.</p>
<h2>Conclusion</h2>
<p>PDF compression hasn't evolved in 30 years—until now. Brotli represents the biggest leap in PDF storage efficiency since the format was invented, and iText is bringing it to production <strong>today</strong>.</p>
<p>Yes, there are compatibility limitations. Yes, it's experimental. But every standard starts this way. HTTP/2, WebP, and TLS 1.3 were all "experimental" once. Early adopters got the benefits first, then the ecosystem caught up.</p>
<p>By using iText's Brotli implementation now, you're:</p>
<ul>
<li><strong>Reducing storage costs</strong> by 15-25% immediately</li>
<li><strong>Future-proofing your documents</strong> for inevitable standardization</li>
<li><strong>Helping shape the spec</strong> with real-world feedback</li>
<li><strong>Voting with code</strong> for a more efficient PDF ecosystem</li>
</ul>
<p>The PDF Association is listening. Adobe is watching. And iText is leading.</p>
<p><strong>Let's make PDFs smaller together.</strong> 🚀</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>