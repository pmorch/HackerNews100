<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 17 Aug 2024 12:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Google Removed Organic Maps from the Playstore (246 pts)]]></title>
            <link>https://twitter.com/organicmapsapp/status/1824727403580596260</link>
            <guid>41272925</guid>
            <pubDate>Sat, 17 Aug 2024 09:01:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/organicmapsapp/status/1824727403580596260">https://twitter.com/organicmapsapp/status/1824727403580596260</a>, See on <a href="https://news.ycombinator.com/item?id=41272925">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[X to pay €550k to employee fired for not replying to yes-or-resign ultimatum (266 pts)]]></title>
            <link>https://fortune.com/europe/2024/08/14/x-ordered-to-pay-550000-to-irish-employee-fired-for-not-replying-to-elon-musk-yes-or-resign-extremely-hardcore-ultimatum/</link>
            <guid>41272861</guid>
            <pubDate>Sat, 17 Aug 2024 08:46:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortune.com/europe/2024/08/14/x-ordered-to-pay-550000-to-irish-employee-fired-for-not-replying-to-elon-musk-yes-or-resign-extremely-hardcore-ultimatum/">https://fortune.com/europe/2024/08/14/x-ordered-to-pay-550000-to-irish-employee-fired-for-not-replying-to-elon-musk-yes-or-resign-extremely-hardcore-ultimatum/</a>, See on <a href="https://news.ycombinator.com/item?id=41272861">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="article-content"><p>Elon Musk’s X has been ordered to pay compensation to a former staff member of its Irish unit in an unfair dismissal case.</p><div>



<p>The social network platform formerly known as Twitter was ordered to pay out over €550,000 ($602,640) to the former employee, Ireland’s Workplace Relations Commission, the state body responsible for adjudicating employment disputes, said on Tuesday. It is the largest sum the agency has ever awarded, according to Irish broadcaster RTE.&nbsp;</p>



<p>Gary Rooney, who held a senior procurement role at the time of his dismissal in December 2022, had been employed by the company since September 2013. The Commission heard that the social media platform maintained that the employee had resigned voluntarily after he failed to tick a box committing to new unspecified working arrangements in an email from the company’s new owner Musk. &nbsp;</p>



<p>Rooney was one of thousands of Twitter employees sent an email by Musk requiring them to pledge to stay with the company, working long hours at “high intensity” during its transformation, or to accept a buyout. Staff were given a day to click “yes” to agree to unspecified employment terms.</p>



<p>The commission rejected X’s argument that Rooney quit voluntarily and ruled that not clicking “yes” in response to the email did not constitute an act of resignation.</p>



<p>“It is not OK for Mr. Musk, or indeed any large company to treat employees in such a manner in this country or jurisdiction. The record award reflects the seriousness and the gravity of the case,” the complainant’s solicitor Barry Kenny told Bloomberg.</p>



<p>It’s one in a string of cases that have sprung up since Musk purchased the platform. Multiple lawsuits have already alleged that Twitter employees didn’t receive their promised severance benefits, while the site has come under more intense scrutiny since he acquired it.</p>



<p>The dispute arose in Dublin after billionaire Musk took ownership of the platform in late 2022. The Irish capital had about 500 employees before the takeover, but has since been impacted by the global staff exodus from the firm.</p>



<p>The WRC in its 73-page ruling stated the emails had been sent “at a time of rapid change in Twitter and in the context of inconsistent, contradictory and confusing communications from the Respondent in connection with the takeover of the Company by Mr. Musk.”</p>



<p>X didn’t immediately respond to a request for comment. The company can lodge an appeal to the Labour Court within 42 days.&nbsp;</p></div><p><strong>Recommended Newsletter:</strong> The Fortune Next to Lead newsletter is a must-read for the next generation of C-suite leaders. Every Monday, the newsletter provides the strategies, resources, and expert insight needed to claim the most coveted positions in business. <a href="https://fortune.com/newsletters/next-to-lead?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=next_to_lead_v2&amp;itm_content=elon_amazon_google" target="_self" aria-label="Go to https://fortune.com/newsletters/next-to-lead?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=next_to_lead_v2&amp;itm_content=elon_amazon_google">Subscribe now</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just use Postgres (173 pts)]]></title>
            <link>https://mccue.dev/pages/8-16-24-just-use-postgres</link>
            <guid>41272854</guid>
            <pubDate>Sat, 17 Aug 2024 08:44:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mccue.dev/pages/8-16-24-just-use-postgres">https://mccue.dev/pages/8-16-24-just-use-postgres</a>, See on <a href="https://news.ycombinator.com/item?id=41272854">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p> by:  <b> Ethan McCue</b></p>

<p>This is one part actionable advice, one part question for the audience.</p>
<p>Advice: When you are making a new application that requires persistent storage of data, like is the case for most web applications, your default choice should be <code>Postgres</code>.</p>
<h3 id="why-not-sqlite">Why not <code>sqlite</code>?</h3>
<p><code>sqlite</code> is a pretty good database, but its data is stored in a single file.</p>
<p>This implies that whatever your application is, it is running on one machine and one machine only. Or at least one shared filesystem.</p>
<p>If you are making a desktop or mobile app, that's perfect. If you are making a website it might not be.</p>
<p>There are many success stories of using <code>sqlite</code> for a website, but they mostly involve people who set up their own servers and infrastructure. Platforms as a service-s like Heroku, Railway, Render, etc. generally expect you to use a database accessed over network boundary. It's not <em>wrong</em> to give up some of the benefits of those platforms, but do consider if the benefits of <code>sqlite</code> are worth giving up platform provided automatic database backups and the ability to provision more than one application server.</p>
<p><a href="https://www.sqlite.org/whentouse.html">The official documentation</a> has a good guide with some more specifics.</p>
<h3 id="why-not-dynamodb-cassandra-or-mongodb">Why not <code>DynamoDB</code>, <code>Cassandra</code>, or <code>MongoDB</code>?</h3>
<p>Wherever Ray Houlihan is, I hope he is having a good day.</p>
<p>I watch a lot of conference talks, but his <a href="https://www.youtube.com/watch?v=HaEPXoXVf2k">2018 DynamoDB Deep Dive</a> might be the one I've watched the most. I know very few of you are going to watch an hour-long talk, but you really should. It's a good one.</p>
<p>The thrust of it is that databases that are in the same genre as <code>DynamoDB</code> - which includes <code>Cassandra</code> and <code>MongoDB</code> - are fantastic <strong>if</strong> - and this is a load bearing if:</p>
<ul>
<li>You know exactly what your app needs to do, up-front</li>
<li>You know exactly what your access patterns will be, up-front</li>
<li>You have a known need to scale to really large sizes of data</li>
<li>You are okay giving up some level of consistency</li>
</ul>
<p>This is because this sort of database is basically a giant distributed hash map. The only operations that work without needing to scan the entire database are lookups by partition key and scans that make use of a sort key.</p>
<p>Whatever queries you need to make, you need to encode that knowledge in one of those indexes before you store it. You want to store users and look them up by either first name or last name? Well you best have a sort key that looks like <code>&lt;FIRST NAME&gt;$&lt;LAST NAME&gt;</code>. Your access patterns should be baked into how you store your data. If your access patterns change significantly, you might need to reprocess all of your data.</p>
<p>It's annoying because, especially with <code>MongoDB</code>, people come into it having been sold on it being a more "flexible" database. Yes, you don't need to give it a schema. Yes, you can just dump untyped JSON into collections. No, this is not a flexible kind of database. It is an efficient one.</p>
<p>With a relational database you can go from getting all the pets of a person to getting all the owners of a pet by slapping an index or two on your tables. With this genre of NoSQL, that can be a tall order.</p>
<p>Its also not amazing if you need to run analytics queries. Arbitrary questions like "How many users signed up in the last month" can be trivially answered by writing a SQL query, perhaps on a read-replica if you are worried about running an expensive query on the same machine that is dealing with customer traffic. It's just outside the scope of this kind of database. You need to be ETL-ing your data out to handle it.</p>
<p>If you see a college student or fresh grad using <code>MongoDB</code> stop them. They need help. They have been led astray.</p>
<h3 id="why-not-valkey">Why not <code>Valkey</code>?</h3>
<p>The artist formerly known as <code>Redis</code> is best known for being an efficient out-of-process cache. You compute something expensive once and slap it in <code>Valkey</code> so all 5 or so HTTP servers you have don't need to recompute it.</p>
<p>However, you <em>can</em> use it as your primary database. It stores all its data in RAM, so it's pretty fast if you do that.</p>
<p>Obvious problems:</p>
<ul>
<li>You can only have so much RAM. You can have a lot more than you'd think, but its still pretty limited compared to hard drives.</li>
<li>Same as the <code>DynamoDB</code>-likes, you need to make concessions on how you model your data.</li>
</ul>
<h3 id="why-not-datomic">Why not <code>Datomic</code>?</h3>
<p>If you already knew about this one, you get a gold star.</p>
<p><code>Datomic</code> is a <code>NoSQL</code> database, but it is a relational one. The "up-front design" problems aren't there, and it does have some neat properties.</p>
<p>You don't store data in tables. It's all "entity-attribute-value-time" (EAVT) pairs. Instead of a person row with <code>id</code>, <code>name</code>, and <code>age</code> you store <code>1 :person/name "Beth"</code> and <code>1 :person/age 30</code>. Then your queries work off of "universal" indexes.</p>
<p>You don't need to coordinate with writers when making queries. You query the database "as-of" a given time. New data, even deletions (or as they call them "retractions"), don't actually delete old data.</p>
<p>But there are some significant problems</p>
<ul>
<li>It only works with JVM languages.</li>
<li>Outside of <code>Clojure</code>, a relatively niche language, its API sucks.</li>
<li>If you structure a query badly the error messages you get are terrible.</li>
<li>The whole universe of tools that exist for SQL just aren't there.</li>
</ul>
<h3 id="why-not-xtdb">Why not <code>XTDB</code>?</h3>
<p><code>Clojure</code> people make a lot of databases.</p>
<p><code>XTDB</code> is spiritually similar do <code>Datomic</code> but:</p>
<ul>
<li>There is an HTTP api, so you aren't locked to the JVM.</li>
<li>It has two axes of time you can query against. "System Time" - when records were inserted - and "Valid Time."</li>
<li>It has a SQL API.</li>
</ul>
<p>The biggest points against it are:</p>
<ul>
<li>It's new. Its SQL API is something that popped up in the last year. It recently changed its whole storage model. Will the company behind it survive the next 10 years? Who knows!</li>
</ul>
<p>Okay that's just one point. I'm sure I could think of more, but treat this as a stand-in for any recently developed database. The best predictor something will continue to exist into the future is how long it has existed. COBOL been around for decades, it will likely continue to exist for decades.</p>
<p>If you have persistent storage, you want as long a support term as you can get. You can certainly choose to pick a newer or experimental database for your app but, regardless of technical properties, that's a risky choice. It shouldn't be your default.</p>
<h3 id="why-not-kafka">Why not <code>Kafka</code>?</h3>
<p><code>Kafka</code> is an append only log. It can handle TBs of data. It is a very good append only log. It works amazingly well if you want to do event sourcing type stuff with data flowing in from multiple services maintained by multiple teams of humans.</p>
<p>But:</p>
<ul>
<li>Up to a certain scale, a table in Postgres works perfectly fine as an append only log.</li>
<li>You likely do not have hundreds of people working on your product nor TBs of events flowing in.</li>
<li>Making a Kafka consumer is a bit more error-prone than you'd expect. You need to keep track of your place in the log after all.</li>
<li>Even when maintained by a cloud provider (and there are good managed <code>Kafka</code> services) its another piece of infrastructure you need to monitor.</li>
</ul>
<h3 id="why-not-elasticsearch">Why not <code>ElasticSearch</code>?</h3>
<p>Is searching over data the primary function of your product?</p>
<p>If yes, <code>ElasticSearch</code> is going to give you some real pros. You will need to ETL your data into it and manage that whole process, but <code>ElasticSearch</code> is built for searching. It does searching good.</p>
<p>If no, <code>Postgres</code> will be fine. A sprinkling of <code>ilike</code> and the built-in <a href="https://www.postgresql.org/docs/current/textsearch.html">full text search</a> is more than enough for most applications. You can always bolt on a dedicated search thing later.</p>
<h3 id="why-not-mssql-or-oracle-db">Why not <code>MSSQL</code> or <code>Oracle DB</code>?</h3>
<p>Genuine question you should ask yourself: Are these worth the price tag?</p>
<p>I don't just mean the straight-up cost to license, but also the cost of lock-in. Once your data is in <code>Oracle DB</code> you are going to be paying Oracle forever. You are going to have to train your coders on its idiosyncrasies, forever. You are going to have to decide between enterprise features and your wallet, forever.</p>
<p>I know its super unlikely that you will contribute a patch to <code>Postgres</code>, so I won't pretend that there is some magic "power of open source" going on, but I think you should have a very specific need in mind to choose a proprietary DB. If you don't have some killer <code>MSSQL</code> feature that you simply cannot live without, don't use it.</p>
<h3 id="why-not-mysql">Why not <code>MySQL</code>?</h3>
<p>This is the one that I need some audience help with.</p>
<p><code>MySQL</code> is owned by Oracle. There are <a href="https://www.mysql.com/products/enterprise/compare/">features locked behind their enterprise editions</a>. To an extent you will have lock-in issues the same as any other DB.</p>
<p>But the free edition <code>MySQL</code> has also been used in an extremely wide range of things. It's been around for a long time. There are people who know how to work with it.</p>
<p>My problem is that I've only spent ~6 months of my professional career working with it. I genuinely don't know enough to compare it intelligently to <code>Postgres</code>.</p>
<p>I'm convinced it isn't secretly so much better that I am doing folks a disservice when telling them to use <code>Postgres</code>, and I do remember reading about how <code>Postgres</code> generally has better support for enforcing invariants in the DB itself, but I wouldn't mind being schooled a bit here.</p>
<h3 id="why-not-some-ai-vector-db">Why not some AI vector DB?</h3>
<ul>
<li>Most are new. Remember the risks of using something new.</li>
<li>AI is a bubble. A load-bearing bubble, but a bubble. Don't build a house on it if you can avoid it.</li>
<li>Even if your business is another AI grift, you probably only need to <code>import openai</code>.</li>
</ul>
<h3 id="why-not-google-sheets">Why not Google Sheets?</h3>
<p>You're right. I can't think of any downsides. Go for it.</p>
<hr>
<h4 id="--index"><a href="https://mccue.dev/">&lt;- Index</a></h4>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Epic Games Store and Fortnite Arrive on EU iPhones (155 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2024/08/epic-games-store-and-fortnite-arrive-on-eu-iphones/</link>
            <guid>41272771</guid>
            <pubDate>Sat, 17 Aug 2024 08:19:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2024/08/epic-games-store-and-fortnite-arrive-on-eu-iphones/">https://arstechnica.com/gadgets/2024/08/epic-games-store-and-fortnite-arrive-on-eu-iphones/</a>, See on <a href="https://news.ycombinator.com/item?id=41272771">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      It's still a mess    —
</h4>
            
            <h2 itemprop="description">Epic also launched its store on Android.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2020/09/fortnite-800x360.png" alt="Artist's conception of Epic dodging harm from Apple's decisions (and perhaps its own).">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2020/09/fortnite.png" data-height="360" data-width="800">Enlarge</a> <span>/</span> Artist's conception of Epic dodging harm from Apple's decisions (and perhaps its own).</p></figcaption>  </figure>

  




<!-- cache hit 194:single/related:2a9d9879426a4f82b1d2207016ef5bdd --><!-- empty -->
<p>It's been four years since <em>Fortnite</em>, one of the world's most popular games, was pulled from the Apple App Store in a blaze of controversy and finger-pointing. Today, it's returning to the iPhone—but only in the European Union.</p>
<p>Today marks the launch of the Epic Games Store on Android and iOS—iOS just in Europe, Android worldwide. Right now, it just has three games: <em>Fortnite</em>, <em>Rocket League Sideswipe</em>, and <em>Fall Guys</em>. And you'll have to be in Europe to access it on your iPhone.</p>
<p>The Epic Games Store is run by Epic Games, the same company that develops and publishes <em>Fortnite</em>. Most folks who have been paying attention to either Epic or Apple in recent years knows the story at this point, but here's the quick summary and analysis.</p>
<h2>Opinion: Users are still the losers after four years</h2>
<p>At the direction of CEO Tim Sweeney, Epic knowingly made changes to <em>Fortnite</em> related to digital payments that violated Apple's terms for developers on the platform. Apple removed <em>Fortnite</em> accordingly, and a long, ugly PR and legal battle ensued between the two companies in multiple countries and regions.</p>
<p>In the US, a judge's decision granted some small wins to Epic and other developers seeking to loosen Apple's grip on the platform, but it kept the status quo for the most part.</p>                                                                        
                                                                                
<p>Things went a little differently in Europe. EU legislators and regulators enacted the Digital Markets Act (DMA), which had far-reaching implications for how Apple and Google run their app stores. Among other things, the new law required Apple to allow third-party, alternative app stores (basically, sideloading) on the iPhone.</p>
<p>Apple's compliance was far from enthusiastic (the company cited security and privacy concerns for users, which is valid, but the elephant in the room is, of course, its confident grip on app revenues on its platforms), and it was criticized for trying to put up barriers. Additionally, Apple rejected Epic's attempts to launch its app store multiple times for a few arcane reasons amid a flurry of almost comically over-the-top tweets from Sweeney criticizing the company.</p>
<p>Despite Apple's foot-dragging, Epic has finally reached the point where it could launch its app store. Epic had already launched a relatively successful App Store on PC, where Valve's Steam holds a strong grip on users. The new iPhone app store doesn't offer nearly as many options or perks as the PC version, but Epic says it's working on wrangling developers onto its store.</p>
<p>It also says it will release its games on other alternative app stores on iOS and Android, such as AltStore PAL.</p>
<p>It's been a long, winding, angry path to get to this point. In the battle between Epic and Apple, there remains some debate about who really has won up to this point. But there isn't much dispute that, whether you want to blame Apple or Epic or both, users sure haven't been the winners.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zngur: A C++/Rust interop tool (105 pts)]]></title>
            <link>https://hkalbasi.github.io/zngur/</link>
            <guid>41271273</guid>
            <pubDate>Fri, 16 Aug 2024 23:27:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hkalbasi.github.io/zngur/">https://hkalbasi.github.io/zngur/</a>, See on <a href="https://news.ycombinator.com/item?id=41271273">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            
            
        </nav>

        <!-- Track and set sidebar scroll position -->
        

        <div id="page-wrapper">

            <div class="page">
                                
                <div id="menu-bar">
                    

                    <h2>Zngur</h2>

                    
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        <h2 id="zngur"><a href="#zngur">Zngur</a></h2>
<p>Zngur (/zængɑr/) is a C++/Rust interop tool. It tries to expose arbitrary Rust types, methods, and functions while preserving its
semantics and ergonomics as much as possible. Using Zngur, you can use arbitrary Rust crate in your C++ code as easily as using it in
normal Rust code, and you can write idiomatic Rusty API for your C++ library inside C++.</p>
<h2 id="idea"><a href="#idea">Idea</a></h2>
<p>Rust and C++ are similar languages but with some important differences. Particularly:</p>
<ul>
<li>Rust is a memory-safe language with a strong boundary between <code>safe</code> and <code>unsafe</code> declared using the <code>unsafe</code> keyword, C++
is an unsafe language with no such difference and keyword.</li>
<li>C++ has macro-like templates, which support variadic, specialization and are checked
at instantiation time. Rust generics, on the other hand, are type-checked at definition
time using trait bounds.</li>
<li>Rust move is a <code>memcpy</code> with compiler support for not destructing the moved out of variable, but C++
move can execute arbitrary code.</li>
</ul>
<p>In all of these differences, C++ has more freedom relative to the Rust:</p>
<ul>
<li>Rust considers C++ functions as unsafe, but C++ will happily call Rust code (even unsafe) as there
is no difference between it and normal C++ code.</li>
<li>Every rust generic code is a valid C++ template, but not vice versa.</li>
<li>C++ can simulate Rust moves very easily (by doing an actual <code>memcpy</code> of data, and tracking the state of destruction in
a boolean flag) but Rust has difficulty with C++ moves. Specially, since Rust assumes that every type is
Rust-moveable, it can never store C++ objects by value, just over some indirection and <code>Pin</code>.</li>
</ul>
<p>So, Zngur allows you to use arbitrary Rust types in C++, store them by value in the C++ stack, and call arbitrary Rust methods and functions
on them. But it doesn't bridge any C++ type into Rust, since it is not possible with the same ergonomic. Instead, Zngur allows you to
write a rusty wrapper for your C++ library. It allows you to implement Rust traits for C++ types and cast them to
the <code>Box&lt;dyn Trait&gt;</code>, implement inherent methods on Rust types, implement Rust traits for Rust types, and expose bare functions
from C++ that operate on Rust types.</p>
<h2 id="demo"><a href="#demo">Demo</a></h2>
<pre><code>#include &lt;iostream&gt;
#include &lt;vector&gt;

#include "./generated.h"

// Rust values are available in the `::rust` namespace from their absolute path
// in Rust
template &lt;typename T&gt; using Vec = rust::std::vec::Vec&lt;T&gt;;
template &lt;typename T&gt; using Option = rust::std::option::Option&lt;T&gt;;
template &lt;typename T&gt; using BoxDyn = rust::Box&lt;rust::Dyn&lt;T&gt;&gt;;

// You can implement Rust traits for your classes
template &lt;typename T&gt;
class VectorIterator : public rust::std::iter::Iterator&lt;T&gt; {
  std::vector&lt;T&gt; vec;
  size_t pos;

public:
  VectorIterator(std::vector&lt;T&gt; &amp;&amp;v) : vec(v), pos(0) {}
  ~VectorIterator() {
    std::cout &lt;&lt; "vector iterator has been destructed" &lt;&lt; std::endl;
  }

  Option&lt;T&gt; next() override {
    if (pos &gt;= vec.size()) {
      return Option&lt;T&gt;::None();
    }
    T value = vec[pos++];
    // You can construct Rust enum with fields in C++
    return Option&lt;T&gt;::Some(value);
  }
};

int main() {
  // You can call Rust functions that return things by value, and store that
  // value in your stack.
  auto s = Vec&lt;int32_t&gt;::new_();
  s.push(2);
  Vec&lt;int32_t&gt;::push(s, 5);
  s.push(7);
  Vec&lt;int32_t&gt;::push(s, 3);
  // You can call Rust functions just like normal Rust.
  std::cout &lt;&lt; s.clone().into_iter().sum() &lt;&lt; std::endl;
  // You can catch Rust panics as C++ exceptions
  try {
    std::cout &lt;&lt; "s[2] = " &lt;&lt; *s.get(2).unwrap() &lt;&lt; std::endl;
    std::cout &lt;&lt; "s[4] = " &lt;&lt; *s.get(4).unwrap() &lt;&lt; std::endl;
  } catch (rust::Panic e) {
    std::cout &lt;&lt; "Rust panic happened" &lt;&lt; std::endl;
  }
  int state = 0;
  // You can convert a C++ lambda into a `Box&lt;dyn Fn&gt;` and friends.
  auto f = BoxDyn&lt;rust::Fn&lt;int32_t, int32_t&gt;&gt;::make_box([&amp;](int32_t x) {
    state += x;
    std::cout &lt;&lt; "hello " &lt;&lt; x &lt;&lt; " " &lt;&lt; state &lt;&lt; "\n";
    return x * 2;
  });
  // And pass it to Rust functions that accept closures.
  auto x = s.into_iter().map(std::move(f)).sum();
  std::cout &lt;&lt; x &lt;&lt; " " &lt;&lt; state &lt;&lt; "\n";
  std::vector&lt;int32_t&gt; vec{10, 20, 60};
  // You can convert a C++ type that implements `Trait` to a `Box&lt;dyn Trait&gt;`.
  // `make_box` is similar to the `make_unique`, it takes constructor arguments
  // and construct it inside the `Box` (instead of `unique_ptr`).
  auto vec_as_iter = BoxDyn&lt;rust::std::iter::Iterator&lt;int32_t&gt;&gt;::make_box&lt;
      VectorIterator&lt;int32_t&gt;&gt;(std::move(vec));
  // Then use it like a normal Rust value.
  auto t = vec_as_iter.collect();
  // Some utilities are also provided. For example, `zngur_dbg` is the
  // equivalent of `dbg!` macro.
  zngur_dbg(t);
}
</code></pre>
<p>Output:</p>
<pre><code>17
s[2] = 7
thread '&lt;unnamed&gt;' panicked at 'called `Option::unwrap()` on a `None` value', examples/simple/src/generated.rs:186:39
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
s[4] = Rust panic happened
hello 2 2
hello 5 7
hello 7 14
hello 3 17
34 17
vector iterator has been destructed
[main.cpp:71] t = [
    10,
    20,
    60,
]
</code></pre>
<p>See the <a href="https://github.com/HKalbasi/zngur/blob/main/examples/simple"><code>examples/simple</code></a> if you want to build and run it.</p>

                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->

                            <a rel="next" href="https://hkalbasi.github.io/zngur/tutorial.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>

                        
                    </nav>
                </div>
            </div>

            <nav aria-label="Page navigation">

                    <a rel="next" href="https://hkalbasi.github.io/zngur/tutorial.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
            </nav>

        </div>






        
        
        

        
        
        

        <!-- Custom JS scripts -->


    </div>
    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[ThreadPlotter – toolkit for punch needle embroidery with X-Y plotters (2020) (107 pts)]]></title>
            <link>https://github.com/LiciaHe/threadPlotter</link>
            <guid>41270596</guid>
            <pubDate>Fri, 16 Aug 2024 21:35:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/LiciaHe/threadPlotter">https://github.com/LiciaHe/threadPlotter</a>, See on <a href="https://news.ycombinator.com/item?id=41270596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">ThreadPlotter</h2><a id="user-content-threadplotter" aria-label="Permalink: ThreadPlotter" href="#threadplotter"></a></p>
<p dir="auto">A toolkit for the design and fabrication of delicate punch needle embroidery using X-Y plotters_</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What?</h2><a id="user-content-what" aria-label="Permalink: What?" href="#what"></a></p>
<p dir="auto">ThreadPlotter is a toolkit that supports the designing, editing, and printing of images as punch needle embroidery using an X-Y plotter. It is a supplementary material for the paper:</p>
<p dir="auto"><a href="http://www.cond.org/punchneedle.html" rel="nofollow">"Plotting with Thread: Fabricating Delicate Punch Needle Embroidery with X-Y Plotters"
Shiqing He, Eytan Adar, to appear, DIS'20, Honorable Mention Award</a></p>
<p dir="auto">The following video briefly introduces the motivation for building this tool and the capability of the ThreadPlotter.</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=zMfiQarMp-8" rel="nofollow"><img src="https://github.com/LiciaHe/threadPlotter/raw/master/assets/youtube-preview.png" alt="youtube-preview"></a></p>
<p dir="auto">You might also be interested in this 10 minutes presentation that goes over the project in depth.</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=jvuNcWv8kGo" rel="nofollow"><img src="https://github.com/LiciaHe/threadPlotter/raw/master/assets/youtube-presentation.png" alt="youtube-presentation"></a></p>
<p dir="auto">If you are interested in using this toolkit, please consider citing our paper:<a href="http://www.cond.org/punchneedle.html" rel="nofollow">Plotting with Thread: Fabricating Delicate Punch Needle Embroidery with X-Y Plotters</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How?</h2><a id="user-content-how" aria-label="Permalink: How?" href="#how"></a></p>
<p dir="auto">To convert your X-Y plotter into a punch needle fabricator, we will follow the following steps:</p>
<ol dir="auto">
<li>Ensure that your plotter is suitable for the task. (<a href="https://github.com/LiciaHe/threadPlotter/blob/master/tutorial/step1_plotterCheck.md">tutorial 1</a>)</li>
<li>Acquire or create several physical components such as needle, fabric, and frame. (<a href="https://github.com/LiciaHe/threadPlotter/blob/master/tutorial/step2_physicalSetup.md">tutorial 2</a>)</li>
<li>Design a punch needle pattern.
<ol dir="auto">
<li><a href="https://github.com/LiciaHe/threadPlotter/blob/master/tutorial/step3_patternMaking.md">tutorial 3: pattern making overview</a></li>
<li><a href="https://github.com/LiciaHe/threadPlotter/blob/master/tutorial/step4_advancedExamples.md">tutorial 4: advanced examples</a> #in progress</li>
</ol>
</li>
</ol>
<p dir="auto">We highly recommend that you review our paper before getting started. When you are ready, click on each of the links above.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Show us! Tell us! Ask us! Credit us!</h2><a id="user-content-show-us-tell-us-ask-us-credit-us" aria-label="Permalink: Show us! Tell us! Ask us! Credit us!" href="#show-us-tell-us-ask-us-credit-us"></a></p>
<p dir="auto">We are excited to see what you can create with this fabrication technique. The toolkit is developed and tested by Licia (on her plotter called "Kitty"). If you have questions about the toolkit, feel free to open up an issue in our <a href="https://github.com/LiciaHe/threadPlotter">github page</a>.</p>
<p dir="auto">If you created something and want to share it with us, please use the tag <a href="https://www.instagram.com/explore/tags/plotterembroidery/?hl=en" rel="nofollow">#plotterembroidery</a> on SNS.</p>
<div data-snippet-clipboard-copy-content="@inproceedings{10.1145/3357236.3395540,
author = {He, Shiqing and Adar, Eytan},
title = {Plotting with Thread: Fabricating Delicate Punch Needle Embroidery with X-Y Plotters},
year = {2020},
isbn = {9781450369749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357236.3395540},
doi = {10.1145/3357236.3395540},
booktitle = {Proceedings of the 2020 ACM Designing Interactive Systems Conference},
pages = {1047–1057},
numpages = {11},
keywords = {plotter, craft fabrication, embroidery fabrication, punch needle embroidery, craft design, x-y plotter, fiber art},
location = {Eindhoven, Netherlands},
series = {DIS ’20}
}
  
"><pre><code>@inproceedings{10.1145/3357236.3395540,
author = {He, Shiqing and Adar, Eytan},
title = {Plotting with Thread: Fabricating Delicate Punch Needle Embroidery with X-Y Plotters},
year = {2020},
isbn = {9781450369749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357236.3395540},
doi = {10.1145/3357236.3395540},
booktitle = {Proceedings of the 2020 ACM Designing Interactive Systems Conference},
pages = {1047–1057},
numpages = {11},
keywords = {plotter, craft fabrication, embroidery fabrication, punch needle embroidery, craft design, x-y plotter, fiber art},
location = {Eindhoven, Netherlands},
series = {DIS ’20}
}
  

</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">License</h3><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT License</p>
<p dir="auto">Copyright (c) [2020] [Shiqing He]</p>
<p dir="auto">Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:</p>
<p dir="auto">The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.</p>
<p dir="auto">THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The future of Deep Learning frameworks (120 pts)]]></title>
            <link>https://neel04.github.io/my-website/blog/pytorch_rant/</link>
            <guid>41270043</guid>
            <pubDate>Fri, 16 Aug 2024 20:24:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neel04.github.io/my-website/blog/pytorch_rant/">https://neel04.github.io/my-website/blog/pytorch_rant/</a>, See on <a href="https://news.ycombinator.com/item?id=41270043">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>Assumed audience:</strong> ML researchers who frequently work with <code>PyTorch</code>, but are interested in trying out <code>JAX</code> or have yet to be convinced.</p><hr><h2 id="introduction">Introduction</h2><p>Usually, people start these ‘critiques’ with a disclaimer that they are not trying to trash the framework, and talk about how it’s a tradeoff. However, this is assumed - I’m not going to waste your time with that.</p><p>Instead, I’ll focus on why PyTorch has been a net negative for all (if not most) scientific computing efforts, causing billions of dollars in lost productivity and thousands of wasted dev-hours.</p><p>This is not because its a <em>bad</em> framework per-se, but rather - it simply because it wasn’t designed for the use-cases it’s being employed in right now.</p><p>Ever since <a href="http://torch.ch/">LuaTorch</a>, PyTorch was supposed to be a “production ready, easy-to-use framework for quick prototyping”.</p><p>It wasn’t meant to be deployed onto huge, distributed clusters comprising of thousands of interconnected nodes and GPUs and scale <em>well</em> in a fault-tolerant and robust matter.</p><p>The guiding philosophy of <code>Torch</code> was never about scale - despite what their marketing may have you believe - but <em>flexibility</em>.</p><p>In response to the rising need of a scalable and performant framework, DeepMind developed <code>JAX</code> to meet a simple goal:</p><blockquote><p>“… Supporting state-of-the-art AI research [by] balancing rapid prototyping and quick iteration with the ability to deploy experiments at a scale …” - <a href="https://arc.net/l/quote/cumgnsor"><code>JAX</code> blogpost</a></p></blockquote><p>This post is about convincing you how important this idea/philosophy is not only for Deep Learning, but for all scientfic computing that needs to happen at scale.</p><p>I believe that all infrastructure built on <code>Torch</code> is just a huge pile of technical debt, that will haunt the field for a long, long time.</p><h2 id="the-philosophy">The Philosophy</h2><p>PyTorch’s philosophy has always, in some ways, been antithetical to that of Tensorflow’s.</p><p>Where <code>TF 1.x</code> tried to be a static but performant framework by making strong use of the <code>XLA</code> compiler, <code>PyTorch</code> instead focused on being dynamic, easily debuggable and pythonic.</p><p>Early on, the TF devs realized their mistakes when they came to realize how much the community hated the old <code>1.x</code> API, which was counter-intuitive and introduced anti-pythonic patterns that were difficult to grasp for beginners.</p><p>This prompted the core decision to use <code>Keras</code> as the main interface for TensorFlow and downplay the role of <a href="https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra"><code>XLA</code></a> compiler that was at TF’s core. The main focus was on cleaning up the frontend as much as possible.</p><p>This was a huge mistake.</p><p>Sure, the API did improve and worked well for some people - but only as long as your workloads were standard. Any deviations from the norm were punished by stacktrace dumps that were often literal pages of just garbled <code>XLA-HLO</code> that were a nightmare to debug unless you had a strong grasp on the internals of the framework/compiler - which you <strong>didn’t</strong> because <code>XLA</code> was a closed source, internal Google project at the time.</p><p>In short, it had every hallmark of a typical Google product.</p><p>Thus it comes as no surprise that people who switched over to PyTorch thought they had discovered literal heaven:</p><div><blockquote><p lang="en" dir="ltr">I've been using PyTorch a few months now and I've never felt better. I have more energy. My skin is clearer. My eye sight has improved.</p>— Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/868178954032513024?ref_src=twsrc%5Etfw">May 26, 2017</a></blockquote></div><p>PyTorch stuck to its roots. Unlike TensorFlow’s static &amp; lazy approach, they took the bolder, more dynamic “eager” approach wherein all <code>torch.Tensor</code>s were evaluated immediately, leading to a much more cleaner abstraction than TensorFlow’s.</p><p>Clearly, they understood that complexity is the enemy of productivity. Instead of tacking on band-aids, they had pursued a fresh new path which paid off.</p><p>Unsurprisingly, almost serious research moved to PyTorch:</p><figure><p><img src="https://neel04.github.io/my-website/blog/pytorch_rant/image-1.png"></p><figcaption>PyTorch vs. Tensorflow usage in research repos</figcaption></figure><p>But in 2021 <a href="https://arxiv.org/abs/2005.14165"><code>GPT-3</code></a> hit the scene and suddenly things started getting serious. All of a sudden, performance and scalability became the primary concern.</p><p><code>PyTorch</code> accomodated for this rising demand <em>decently</em> well, but because it wasn’t designed around this philosophy - slowly the debt starting catching up and the foundations started crumbling. It’s hard to reconcile flexibility with performance. Clearly, a tradeoff needed to be made.</p><p>Either they could give their biggest and richest users exactly what they wanted - a clean &amp; scalable ecosystem that prioritized performance - which would be a static-oriented <code>TF</code>-like design - or they could try to hold on to what made <code>Torch</code> so special in the first place - being dynamic and “eager” at the expense of performance, and somehow delegate those large-scale workloads to an entirely seperate technological stack.</p><p>So the devs, being the smart and rational engineers they are, choose an appropriate compromise which was . . . . to pursue both paths simultaneously.</p><p>They were unwilling to make any tradeoffs. They wanted their cake and were going to eat it too.</p><p>The new approach was ultimately a chaotic mishmash of competing features. You have on one hand, PyTorch’s committment to eventually use <em>some</em> compiler (likely <code>XLA</code>) as a performant and reliable default backend and on the other, to build up their own entire <a href="https://pytorch.org/docs/stable/torch.compiler.html"><code>torch.compile</code></a> stack that somehow meshes well with the eager, dynamic philosophy by giving users the freedom to invoke a compiler if need be.</p><p>This lack of real long-term strategy is a serious issue.</p><p>Take the <code>torch.compile</code> stack and the new <a href="https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md"><code>DTensor</code></a> API as an example. The documentation is transparent about the inspiration for this feature. It tries to
bring the sharding model of parallelization from <code>JAX</code> to <code>PyTorch</code>.</p><blockquote><p>… When using <code>DTensor</code> in a [distributed] fashion it might be … <strong>slower</strong> compared to existing solutions like DDP/FSDP. This is mainly because DDP/FSDP have a global view of the entire model … [and can thus] optimize for data parallel specifically … [whereas] DistributedTensor … can only optimize within individual tensor operations.</p></blockquote><blockquote><p>To improve efficiency of <code>DTensor</code>-based data parallel training, we are exploring a <strong>compiler-based</strong> solution on top of <code>DTensor</code>.</p></blockquote><p>Leaning on the compiler is diametrically opposed to torch’s dynamic philosophy - because at each step, you’re restricted by the constraints placed by the compiler which you <em>must</em> obey.</p><p><code>PyTorch</code> clearly doesn’t want to commit to a compiler-centric philosophy (like <code>JAX</code>) but I don’t see any good alternative solutions - and frankly, I doubt the devs do either.</p><p>Instead, what you end up getting getting is a fragmented suite of tools that’re barely usable without significant dev-hours sunk in just setting them up and coaxing them to work with each other.</p><p>It’s considerable friction for research teams who often spend more of their time babysitting the codebase and triangulating random errors rather than running more experiments.</p><p>I feel there is a stronger incentive internally on <em>marketing</em> and shipping ‘features’ rather than actually ensuring they integrate well into the ecosystem. It’s true that maintaining such a huge ecosystem will always have it’s problems, but the considering the case where devs shipped
a built-in implementation of <code>FSDP</code>, and it didn’t work at <em>all</em> with their own <code>torch.compile</code> stack for months, really goes to show where their priorities lie.</p><p>There is simply no excuse for two of your most core, critical features not working together at all. Users had to wait <a href="https://dev-discuss.pytorch.org/t/torch-compile-fsdp-dec-8th/1718">weeks</a> before it was officially patched and the bugs were ironed out to the point of it being in a ususable state where is stands now.</p><p>My point is that all these failures are systemic due to: a) bad organization and b) bad design decisions.</p><p>So what is the competition’s solution to this problem?</p><h2 id="compiler-driven-development">Compiler-driven development</h2><p><code>JAX</code> leverages TensorFlow’s formidable compiler stack, <a href="https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra"><code>XLA</code></a>. <code>XLA</code> is a pretty powerful compiler, but the beauty is that it’s all abstracted away for the end user.
For any function you have, as long as the function is <strong>pure</strong> (more on this later) you can use the simple <code>@jax.jit</code> decorator to JIT compile your function and make it available to <code>XLA</code>.</p><p>You can <code>jit</code> any JAX code - <code>XLA</code> handles the rest. This is what makes JAX such a great framework for scientific computing - its effectively an eDSL built entirely around <code>XLA</code>. The compiler handles and abstracts away a lot of the heavy lifting for us - verifying that the generated graph is correct,
<code>GSPMD</code> partitioner that handles the auto-parallelization w/ sharding in JAX, the graph optimizations, operator and kernel fusion, Latency hiding Scheduling, overlapping asynchronous comms, codegeneration to other backends such as <a href="https://openai.com/index/triton/"><code>triton</code></a> etc.
are all handled by <code>XLA</code> behind the scenes.</p><p>This is a powerful approach. As long as your code obeys some simple JAX restrictions, <code>XLA</code> does this automatically for you. For example, you don’t need <code>torch.distributed.barrier()</code> and other comms primitives when doing parallelization.
DDP support is as simple as:</p><div><pre tabindex="0"><code data-lang="py"><span><span><span># Create a Sharding object to distribute a value across devices:</span>
</span></span><span><span>sharding = PositionalSharding(mesh_utils.create_device_mesh((8,)))   
</span></span><span><span>x = JAX.random.normal(JAX.random.key(0), (8192, 8192))
</span></span><span><span>y = JAX.device_put(x, sharding.reshape(4, 2))
</span></span></code></pre></div><p>which you can also visualize with the built in utilities:</p><div><pre tabindex="0"><code data-lang="py"><span><span>&gt;&gt;&gt; JAX.debug.visualize_array_sharding(z)
</span></span><span><span>
</span></span><span><span>+---------------------+
</span></span><span><span>|  TPU 0   |  TPU 1   |
</span></span><span><span>|----------|----------|
</span></span><span><span>|  TPU 2   |  TPU 3   |
</span></span><span><span>|----------|----------|
</span></span><span><span>|  TPU 6   |  TPU 7   |
</span></span><span><span>|----------|----------|
</span></span><span><span>|  TPU 4   |  TPU 5   |
</span></span><span><span>+---------------------+
</span></span></code></pre></div><p><code>XLA</code>’s approach is that computation follows sharding. Therefore, if the input array is sharded across some axis, <code>XLA</code> handles that automatically for any downstream computation. No other code changes needed.
No need to add communication collections or anything. <code>Pytorch</code> on the other hand requires a ton of boilerplate and modifications just to get a basic DDP setup working correctly.</p><p>This idea of “compiler driven development” is similar to how rust’s compiler works - helping you write better, cleaner code without worrying about a lot of mundane things.</p><p>You focus on the computation, the compiler does the rest.</p><p>I believe that comitting to a philosophy gives a framework a certain design skeleton and structure, that can simplify the code and create a smooth and wondeful experience for a developer.</p><p>Which is why I’m unhappy with the choice made by the <code>PyTorch</code> devs to integrate and rely on a compiler stack for the cool new features rather than keeping the core philosophy of <em>flexibility</em> and <em>freedom</em> alive.</p><p>For example, according to the official <a href="https://pytorch.org/blog/pytorch-2.0-xla-path-forward/">roadmap</a> for <code>PyTorch</code> <code>2.x</code>, they clearly outline their long-term plans of fully integrating <code>XLA</code> with <code>Torch</code>:</p><blockquote><p>“PyTorch/<code>XLA</code> is set to migrate to the open source <code>OpenXLA</code> as its <strong>default</strong> downstream compiler”</p></blockquote><p>This is an awful idea. It’s like saying that shoehorning C++ code in the rust compiler, would somehow be a better experience than using rust itself.</p><p>Torch simply wasn’t <em>designed</em> around <code>XLA</code>, unlike <code>JAX</code>. The reason <code>JAX</code>’s’ ecosystem is so much more stable and well-integrated is precisely because they uphold it’s core values rather than working around them.</p><p>If, god forbid, <code>Pytorch</code> does end up going with the plan and commits to an <code>XLA</code> based compiler stack, then wouldn’t the ideal framework be the one that was <em>specifically</em> designed and built around it, as opposed to the one where it has just been crammed in with little thought and care?</p><p>And even <strong>if</strong> <code>Pytorch</code> ends up pursuing a ‘multi-backend’ approach, wherein you would be able choose whatever compiler backend you wish, wouldn’t that worsen the fragmentation problem and absolutely nuke the API, as it tries to respect the restrictions of every compiler stack?</p><p>This isn’t just baseless theorizing either — look at <code>Torch/XLA</code>. Anyone who’s ever dared to use it on TPUs suffers from a PTSD so severe that they’re eligible for benefits. The mere sight of “XLA” sends them into a state of cold sweat and nightmares to the caffeine-fuelled days spent debugging hundred-line <code>XLA</code> errors. The only path to recovery at such moments is to reassure the victim that they’ll always have their GPUs, and an <code>A100</code> may be placed beside them for emotional support.</p><h2 id="multi-backend-is-doomed">Multi-backend is doomed</h2><p><code>Pytorch</code>’s root problem is that it tries to do everything at once and fails miserably.</p><p>The “multi-backend” design decision makes this problem exponentially worse. In <em>theory</em>, it sounds like an amazing idea to be able to choose whichever stack you prefer - but in reality, its a tangled mess of obscure tracebacks and incompatibility issues.</p><p>It’s not that its <em>hard</em> to get these backends working. Rather, there are some constraints that these backends expect which are hard to mesh with the flexible and pythonic API of <code>PyTorch</code>.</p><p>There’s a tradeoff between keeping most of the API consistent vs. obeying the restrictions of the backends you leverage. As a result, the devs are seeking to rely more on codegen (say converting torch code to <code>triton</code> which then you can manually work with and leverage it’s compiler &amp; JIT stack)
as opposed to actually integrating/comitting to a single backend - which is arguably the worse option for <code>torch</code>.</p><p>Every decision <code>torch</code> takes somehow always feels like a compromise because it refuses to make meaningful tradeoffs. There’s no coherence, no overall strategy. It ends up feeling more like a mishmash of features that don’t mesh well together and ultimately cause a lot of frustration for the user.</p><p>There is no faster way to kill an ecosystem.</p><p>IMHO PyTorch should not follow the <code>JAX</code> “integrated compiler and backend” approach for a very simple reason: Jax was explcitly designed from the ground up to work <strong>with</strong> <code>XLA</code>. Not <strong>against</strong> it. That is why <code>TensorFlow</code> never really took off, and why it’s attempts at integrating <code>Keras</code> crashed and burned.</p><p>Your strategy simply cannot be to just replace the entire <code>PyTorch</code> frontend with <code>JAX</code>’s, because then you just have a shittier version of <code>JAX</code>! It’s virtually impossible to come up with a neat, functional API based on <code>XLA</code> that’s somehow better than <code>JAX</code>’s, and carries on <code>Torch</code>’s flexible nature.</p><p>I don’t blame the devs for trying new and different ideas - those are always welcome. However, if they want <code>PyTorch</code> to stand the test of time, more focus has to be put in shoring up the foundations than shipping shiny new features that immediately crumble outside ideal tutorial conditions.</p><h2 id="fragmentation--functional-programming">Fragmentation &amp; Functional Programming</h2><p><code>JAX</code> has a “functional” API. Earlier, I mentioned that <code>JAX</code> functions have to be pure (i.e they cannot have any global side effect. Just like mathematical functions, given the same data the function will always return the same output no matter the context of it’s execution.)</p><p>This design philosophy is what makes <code>JAX</code> stand out. Due to the functional roots of <code>JAX</code>, <code>JAX</code> functions are often composable and interoperate well with each other. It reduces the development complexity as functions are defined with specific signatures and a well-defined,
concrete task. If the types are respected, then the function is guranteed* to work out-of-the-box.</p><p>This is well suited to the kinds of workloads that one needs in scientific computing. In Deep Learning especially, since NNs are just a static functions, this functional paradigm makes writing even complex codebases easy.</p><p>For example, let’s look at the <code>optax</code> API from the <code>JAX</code> ecosystem.</p><p>Due to the functional approach, <code>optax</code> has what we call a “chain” that involves a bunch of functions sequentially applied on the gradients. So the fundamental building blocks are <code>GradientTransformation</code>s.</p><p>This makes it a really powerful but expressive API to work with.</p><div><pre tabindex="0"><code data-lang="py"><span><span>optimizer = optax.adam(1e-2)
</span></span></code></pre></div><p>If I wanted to clip the grads here for example, I could do it trivially:</p><div><pre tabindex="0"><code data-lang="py"><span><span>optimiser = optax.chain(
</span></span><span><span>    optax.clip_by_global_norm(1.0),
</span></span><span><span>    optax.adam(1e-2),
</span></span><span><span>)
</span></span></code></pre></div><p>If I wanted to do a simple operation such as take the <code>EMA</code> of my grads, in PyTorch that would’ve required setting up objects and then manually digging through the codebases to place methods appropriately.
But with <code>optax</code>,</p><div><pre tabindex="0"><code data-lang="py"><span><span>optimiser = optax.chain(
</span></span><span><span>    optax.clip_by_global_norm(1.0),
</span></span><span><span>    optax.adam(1e-2),
</span></span><span><span>    optax.ema(decay=0.999)
</span></span><span><span>)
</span></span></code></pre></div><p>A similar approach goes for combining optimizers, meta-learning approaches, gradient accumulation etc. It’s simply much more cleaner than <code>PyTorch</code>.</p><p>Another cool consequence of a functional design is <code>vmap</code>. This stands for ‘vectorized’ map which accurately describes what it does. You can <code>map</code> anything and as long as its a <code>vmap</code>, then <code>XLA</code> will automatically fuse and optimize it.</p><p>This means that when you write functions, you <strong>don’t think about the batch dimension!</strong> You just <code>vmap</code> all you code and this simplifies things.</p><p>For one, you need less <code>ein-*</code> ops. While <code>einops</code> are great and all - it’s simply more intuitive to grasp 2D/3D tensor manipulations, and are also much more readable IMO. Let’s take an extremely limited example operation, and compare the difference:</p><div><pre tabindex="0"><code data-lang="py"><span><span>arr: Array = jnp.ones((batch_size, seqlen, h_dim))
</span></span><span><span>
</span></span><span><span><span>def</span> <span>vanilla_func</span>(arr: Array) -&gt; Array:
</span></span><span><span>  <span>'''
</span></span></span><span><span><span>  We want to do a: '(b s h, b s h) -&gt; (b s s, b s h) -&gt; b h s' operation.
</span></span></span><span><span><span>  '''</span>
</span></span><span><span>  <span>return</span> ((arr @ arr.transpose(0, 2, 1)) @ arr).transpose(0, 2, 1)
</span></span><span><span>
</span></span><span><span>@jax.vmap
</span></span><span><span><span>def</span> <span>vmapped_func</span>(arr: Array) -&gt; Array:
</span></span><span><span>  <span>'''
</span></span></span><span><span><span>  We want to do a: '(s h, s h) -&gt; (s s, s h) -&gt; h s' operation.
</span></span></span><span><span><span>  '''</span>
</span></span><span><span>  <span>return</span> ((arr @ arr.T) @ arr).T
</span></span></code></pre></div><p>Even for this toy operation, you can immediately see how much more instantly readable the function is. Now imagine that with the more complex tensor manipulations, like the ones used in <code>MHSA</code>.</p><p>Subscribing to the functional paradigm means that it’s easier to write complex code that works <em>well</em>, since you only have to reason about individual components in isolation.
It’s both clean and performant because you can <code>@jax.jit</code> any pure function without worrying about anything else. It. Just. Works.</p><p>In the functional world, as long as you respect the purity constraints and have the right signature, you enjoy all the other benefits - such as composability.</p><p>With <code>torch</code> however, there is a non-trivial chance that whatever stack you use - say doing <code>FSDP + multi-node + torch.compile + ...</code>
something will <em>always</em> break due to the sheer complexity involved. Multiple things have to work correctly together, and if any component fails due to edge case, then you would be debugging till 3 a.m.</p><p>And because there would <strong>always</strong> be bugs that weren’t caught during development simply because it’s not possible to test each and every combination of the dozens of features <code>Pytorch</code> offers, It’s simply impossible to write code that works well without significant effort.</p><p>This has meant that the <code>torch</code> ecosystem has become very bloated and buggy - things don’t interoperate well together, so contributors often come up with new libraries and frameworks to solve specific issues (like say HuggingFace’s <code>accelerate</code> for distributed training)
which in turn aren’t designed to interface with other such “solutions” due to having no shared abstraction, so it quickly devolves into a mess of dependencies and <code>requirements.txt</code> and a training loop that looks like it was the scene of Guido Van Rossum’s murder.</p><p>I’d go on to say from my anecdotal experience about 70-80% of those GitHub issues or forum discussions are simply due to various libraries erorring out on each other.</p><p>Unfortunately, few solutions exist to fix it. This is very much an OOP as well as a design problem. I <em>think</em> having a fundamental, <code>PyTorch</code>~y object (like <code>JAX</code>’s <code>PyTree</code>) might’ve helped build a common base for abstraction, but I’m not an SWE so I’m not sure how much it’d have <em>really</em> helped.</p><p>Nor can you just adopt a functional programming paradigm, at which point you’d have converged to a worse version of <code>JAX</code> while breaking all backwards compatibility for every existing <code>torch</code> codebase.</p><p>The way I see it - <code>PyTorch</code> is well and truly fucked in this department.</p><h2 id="reproducibility">Reproducibility</h2><p>The “reproducility crisis” is an oft discussed problem in science, and AI/DL has been no exception.
Despite the existence of containerized environments and version control, researchers refuse to use them and journals/conferences place no requirements on them either.
Even upholding your pledge to open-source the code isn’t verified by academic institutions.</p><p>However, there are some design choices that nudge users to write code that facilitates reproduction, with minimal effort. This incentivizes users to put that little effort in
and gain masive advantages in return - such as being able to validate their older experiments at any point and ensure that randomness is not a factor in any their results.</p><p>I believe such oversights are usually because of laziness/carelessness than malicious intent. So such small decisions can ultimately add up to saving potentially dozens of dev-hours and a lot of cussing.</p><h3 id="seeding">Seeding</h3><p><code>torch</code>’s handling of something as simple as seeding is… not ideal. Typically, you’d have to do:</p><div><pre tabindex="0"><code data-lang="py"><span><span><span>import</span> <span>torch</span>
</span></span><span><span><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
</span></span><span><span>
</span></span><span><span>np.random.seed(0) <span># if you're using numpy</span>
</span></span><span><span>torch.manual_seed(0)
</span></span><span><span>torch.cuda.manual_seed_all(args.seed)
</span></span><span><span>torch.use_deterministic_algorithms(<span>True</span>)
</span></span><span><span>torch.utils.deterministic.fill_uninitialized_memory(<span>True</span>)
</span></span></code></pre></div><p>Which let’s be honest, isn’t really the end of the world coming at barely half a dozen loc - But on the flipside, is easily forgettable/misconfigured especially in the heat of deadlines.</p><p>I’ve personally known researchers who set the seeds in the wrong file at the wrong place and they weren’t even used by <code>torch</code> at all - instead, were just silently ignored, thus invalidating all their experiments. (That researcher was me)</p><p><code>JAX</code> on the other hand forces you to create an explicit <code>key</code> which gets passed to any function that required randomness. This approach completely eliminates this problem as the RNG at all points is statically seeded. And because <code>jax</code> has its own version
of numpy (<code>jax.numpy</code>) you don’t need to remember to seed it seperately.</p><p>This is a small fry - but I feel such small QoL decisions can end up making the whole framework’s UX a whole lot better.</p><h3 id="portability">Portability</h3><p>One of the biggest banes of using torch codebases is the lack of portability - codebases written for running on CUDA/GPUs don’t really work well when run on non-Nvidia hardware like TPUs, NPUs, AMD GPUs etc.</p><p>Worse, it’s hard to port torch code written for 1x (one) node and port it to be multi-node. Multi-node often involves dozens of dev-hours and substantial code changes to manually integrate it in the correct places. Unsurprisingly, this quickly devolves into a horror story of errors, crashes and incorrect comms that leech performance.</p><p><code>JAX</code>’s compiler-centric approach however gives it a win in this department. <code>XLA</code> handles switching between device backends for us - and it already works well out-of-the-box on GPUs/TPUs/multi-node/multi-slice with minimal to no code changes. (Support for AMD GPUs is also coming, however anecdotally it’s not in a great place right now - which seems more reflective of AMD than <code>XLA</code>.)</p><p>One only needs to implement a device backend for <code>XLA</code> to use, and it automatically takes care of the intermediate steps of extracting computation from graph as specified in a framework (Jax/TF/PyTorch), produce an HLO, and then eventually emit a lower-level IR that hardware backends can then execute during runtime.</p><figure><p><img src="https://neel04.github.io/my-website/blog/pytorch_rant/jax_platforms.png"></p><figcaption>Jax's hardware compatibility matrix, as of <i>Aug 2024</i></figcaption></figure><p>This way makes it easier for hardware vendors to support their devices, as well as make the transition between devices more easier.</p><p>I think this is an often overlooked, but important issue. Not everyone has access to the same hardware, so codebases that are portable across different types of hardware can be a small step towards making Deep Learning more accessible to beginners/intermediates as well as preventing a lot of frustration.</p><h3 id="auto-scaling">Auto Scaling</h3><p>This point ties in with the idea of portability. Codebases that can <em>autoscale</em> well on their own are massively helpful during reproduction. In an ideal case, this should happen automatically with minimal code changes, unfetterd by networking boundaries.</p><p>As we’ve come to expect, <code>JAX</code> does this well. When writing <code>JAX</code> code, you don’t need to specify communication primitives or do <code>torch.distributed.barrier()</code> everywhere - <code>XLA</code> automatically inserts that for us, taking the available hardware in account.</p><p>This philosophy means that whatever devices <code>JAX</code> can <em>detect</em> are automatically used, irrespective of networking, topology, configuration etc. You do not need to specify ranks or a central co-ordinator host.
<code>JAX</code> automagically synchronizes and stages all the computations for you, as well as apply optimization passes to maximize asynchronous execution of the kernels and minimize latency.</p><p>All a person has to do is specify the sharding of the tensors you want to distribute across, such as sharding the batch dimension of the input arrays and due to <code>XLA</code>’s “computation follows sharding” approach, it automatically figures out the rest. This is due to the <a href="https://arxiv.org/abs/2105.04663">GSPMD</a> partitioner in the XLA compiler stack.</p><p>This is really powerful, as experiments that have been validated at scale can be run by hobbyists easily to play around with them and potentially iterate on them - and vice-versa.</p><p>I feel this could help in discovery of forgotten ideas more easily, and encourage the field to be more ‘<a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">bitter</a>’ - as ideas could be easily tested as a function at bigger scales with minimal effort, thus incentivizing such experiments.</p><h2 id="the-cons">The Cons</h2><p>I have been disproportionately covering only the problems plaguing torch so far. But it’s not all roses with <code>JAX</code> either. There are a few major concerns that I wish are given far more attention among <code>JAX/XLA</code> devs:</p><h3 id="governance-structure">Governance structure</h3><p>Currently, <code>XLA</code> is under TF governance, and while talk has been made of establishing a seperate organizing body to manage all affairs, similar to torch, not much concrete efforts have been made - atleast publicly.</p><p>Unfortunately, there isn’t a lot of trust in Google at the moment due to its reputation to axe unpopular products. Now, <code>JAX</code> is technically a DeepMind project and of core significance to Google’s entire AI push, but still I feel that having a seperate governing body would be of great long-term benefit for the ecosystem as a whole
by providing guidance to the development of the project.</p><p>This would give it a concrete structure, and decouple it with Google’s infamous bureaucracy - thus avoiding a whole host of problems in a single sweep.</p><p>I don’t think <code>JAX</code> necessary <em>needs</em> an official structure of this sort, but rather it’d be nice to have a gurantee that <code>JAX</code> development will take place for a long time regardless of Google upper-management’s decisions.</p><p>It would definitely help its case in adoption among companies and big labs as well, who are hesitant to spend resources incorporating tools that might stop being maintained at some point.</p><h3 id="open-source-transition-of-xla">Open source transition of <code>XLA</code></h3><p>For the longest time, <code>XLA</code> was a closed-source project. However, efforts have been made to open source it, and now <a href="https://openxla.org/"><code>OpenXLA</code></a> is at well outperforms the internal XLA build.</p><p>However, documentation about the internals of <code>XLA</code> is still sparse. Most of the resources are just live talks and the occasional paper, which are often out-of-date.</p><p>Having a publicly accessible roadmap of upcoming features would make it easier
for people to track progress and contribute to things they find particularly interesting.</p><p>I think it’d be nice to give practitioners a way to better gauge what <code>XLA</code> can and can’t do through <a href="http://blog.ezyang.com/2019/05/pytorch-internals/">Edward Yang styled</a> mini-blogposts that breakdown each stage of the <code>XLA</code> compiler stack and explain the nitty-gritty details.</p><p>I understand that this is resource intensive, which could be better directed elsewhere but I feel that people trust the tools more when they understand them, and there’s a positive spillover effect across the entire ecosystem that ends up benefitting everyone.</p><h3 id="coalescing-ecosystem">Coalescing ecosystem</h3><p>For various reasons outside the scope of this blog, I heartily dislike <code>flax</code>. It’s a bane on the <code>JAX</code> ecosystem. It has an unintuitive API, terse syntax and is absolutely hell for beginners transitioning from <code>PyTorch</code>.</p><p>Just use <code>equinox</code>.</p><p>There have been attempts to fix <code>flax</code>’s shortcomings from the dev team, namely by using <a href="https://flax.readthedocs.io/en/v0.8.3/experimental/nnx/index.html"><code>NNX</code></a> which is supposed to be a more “equinox-y” wrapper ontop of <code>flax</code>.</p><p>However, I think it’s ultimately a waste of time. If you want an <code>equinox</code>-styled API, then just use <code>equinox</code>. There isn’t a lot <code>flax</code> does especially better that’s hard to replicate with <code>equinox</code>. Plus, having little to no abstraction makes implementing things in <code>equinox</code> much easier and faster.</p><p>Right now, a lot of the <code>JAX</code> ecosystem is designed around <code>flax</code>. <code>equinox</code>, because it fundamentally interfaces with <code>PyTree</code>s is cross-compatible with all libraries, however you do have to do a little <code>eqx.partition</code>-ing and <code>filter</code>-ing, which can be a bit annoying.</p><p>I want to change the status quo. It should be the other way around - <code>equinox</code> should have first class support everywhere. Considering its popularity, I think this decision would objectively make thigns easier for the vast majority of serious <code>JAX</code> users and big codebases.</p><p>I know this is a controversial opinion simply because a lot of resources have been poured into <code>flax</code> - But this is classic <a href="https://en.wikipedia.org/wiki/Sunk_cost">sunk-cost fallacy</a>. <code>equinox</code> just does it better, in the way a <code>JAX</code> framework should always have been like. It may not be perfect, but its better than the alternatives by a mile.</p><figure><p><img src="https://neel04.github.io/my-website/blog/pytorch_rant/eqxvsflax.png"></p><figcaption><code>equinox</code> vs. <code>flax</code>: as neatly summarized in the <a href="https://neel04.github.io/my-website/blog/pytorch_rant/url">equinox docs</a>.</figcaption></figure><p>It’s good to see that maintainers of the <code>JAX</code> ecosystem are realizing the popularity of <code>equinox</code> and adjusting accordingly - however, I’d love to see a bit more love officially from Google and the <code>flax</code> team as well.</p><p>If you want to try out <code>JAX</code> - it’s not even a question. Use <code>equinox</code>. You’ll thank me.</p><blockquote><p>“I’ve been using <code>equinox</code> for a few months now and I’ve never felt better. I have more energy. My skin is clearer. My eye sight has improved.”
– Me</p></blockquote><h3 id="sharp-edges">Sharp edges</h3><p>Due to some of the API design decisions and <code>XLA</code> restrictions, <code>JAX</code> has some “sharp edges” that you should be careful of. The well-written documentation explains this very concisely:</p><p><a href="https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html">Common Gotchas in Jax.</a></p><p>So go give that a read atleast once before using <code>JAX</code>. It’ll save you a lot of time and energy (as RTFM-ing always does).</p><h2 id="conclusion">Conclusion</h2><p>This blogpost was to correct the often-parotted myth that <code>PyTorch</code> is simply the best for any real research workloads - especially on GPUs. That simply isn’t the case anymore.</p><p>Infact, I’d go as far as to argue that porting all <code>Torch</code> code to <code>JAX</code> would be <em>immensely</em> beneficial to the field as a whole. These are not minor features - having autoparallelization, reproducibility, a clean functional API etc. would be a godsend for a lot of research codebases.</p><p>So if you want to make this field a little bit better, consider rewriting your codebases in <code>JAX</code>.</p><p>Shoutout to <a href="https://kidger.site/">Patrick Kidger</a> as well for developing <code>equinox</code>. If you’re coming from <code>PyTorch</code>, I cannot recommend it enough!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLM and Bug Finding: Insights from a $2M Winning Team in the White House's AIxCC (129 pts)]]></title>
            <link>https://team-atlanta.github.io/blog/post-atl/</link>
            <guid>41269791</guid>
            <pubDate>Fri, 16 Aug 2024 19:56:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://team-atlanta.github.io/blog/post-atl/">https://team-atlanta.github.io/blog/post-atl/</a>, See on <a href="https://news.ycombinator.com/item?id=41269791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Hello, world! We are <em>Team Atlanta</em>, the minds behind Atlantis, our innovative
AI-driven cybersecurity solution competing in the prestigious
<a href="https://aicyberchallenge.com/" target="_blank">DARPA AIxCC</a>
.</p><p><a href="https://team-atlanta.github.io/authors/">Our team</a>
is a collaborative powerhouse made up of six leading institutions:
<a href="https://www.gatech.edu/" target="_blank">Georgia Tech</a>
,
<a href="https://www.gtri.gatech.edu/" target="_blank">GTRI</a>
,
<a href="https://research.samsung.com/" target="_blank">Samsung Research</a>
,
<a href="https://sra.samsung.com/" target="_blank">Samsung Research America</a>
,
<a href="https://www.kaist.ac.kr/en/" target="_blank">KAIST</a>
, and
<a href="https://www.postech.ac.kr/" target="_blank">POSTECH</a>
.
Each of these organizations is led by Georgia Tech alumni,
and includes past winners of prestigious hacking competitions
such as DEF CON CTF, Pwn2Own and kernelCTF.</p><p>For the past several months, we have been diligently preparing for this competition,
combining our expertise in AI, cybersecurity,
and software engineering.
Last week, we proudly competed in the AIxCC Semifinals,
showcasing our hard work and dedication
to advancing cybersecurity through artificial intelligence.</p><h2 id="the-journey-begins">The Journey Begins</h2><p>When AIxCC was announced <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/08/09/biden-harris-administration-launches-artificial-intelligence-cyber-challenge-to-protect-americas-critical-software/" target="_blank">last year</a>
,
we quickly assembled a team of friends,
including <a href="https://www.zellic.io/" target="_blank">Zellic</a>
and <a href="https://gts3.org/" target="_blank">SSLab</a>
.
At that time,
much was uncertain;
details about the game format,
scoring rubric,
proof-of-vulnerability (PoV),
sanitizers, harnesses, supported programming languages,
and proof-of-understanding (PoU) were all unclear.
Our team, however, started preparing for the competition from last October.</p><p>Many of our team members previously participated in the
<a href="https://www.darpa.mil/program/cyber-grand-challenge" target="_blank">DARPA Cyber Grand Challenge (CGC)</a>
as part of <a href="https://en.wikipedia.org/wiki/2016_Cyber_Grand_Challenge" target="_blank">Crspy</a>
,
where we were responsible for bug finding and exploitation generation.
DARPA CGC was an ambitious endeavor
that sparked numerous innovative research directions afterward.
However, the competition was not without its challenges,
particularly due to the <em>gamification</em> of the event;
the scoring metrics and rules significantly influenced <a href="https://free.eol.cn/edu_net/edudown/spkt/zhangchao.pdf#page=34" target="_blank">the outcomes</a>
.
In the end, the competing Cyber Reasoning Systems (CRS) that focused on operating reactively–prioritizing the availability score over fixing bugs–
tended to score higher, as exploitation proved to be far more difficult than patching.</p><p>Aware of <a href="https://aicyberchallenge.com/rules/" target="_blank">the gamification issues</a>
from CGC,
we anticipated that to excel in AIxCC
our CRS should leverage AI, particularly LLMs, aggressively in various depths and levels
of the CRS pipelines.
With this in mind, we strategically chose to focus our efforts on two key directions:</p><ol><li><p><strong>Static Analysis.</strong> To encourage the use of LLMs and set AIxCC apart from CGC,
we anticipated that
AIxCC would strongly advocate for the adoption of <em>static analysis</em> while
steering away from the dominant use of <em>fuzzing</em><sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.
It’s important to note
that finding bugs is quite different from finding crash- or bug-triggering
inputs. The latter offers a clear advantage in objectively and autonomously
verifying the discovered bug,
but it has a much narrower scope compared to the former.
In practice, the <em>triggering</em> aspect, also known as the reachability problem, is
a significantly more challenging and crucial issue to address,
where <em>dynamic tools</em> like fuzzing have a clear edge.</p></li><li><p><strong>Fine-tuning LLMs for Source Code.</strong> Specialization is always an advantage
when possible. Given that each CRS will likely need to support more than 10
programming languages during the competition, we decided to fine-tune both
in-house and open-source models for analyzing code.
This approach is conceptually similar to
<a href="https://paperswithcode.com/dataset/commitpack" target="_blank">commitPack</a>
,
but focuses on
commits related to bugs like their fixes, bug-introducing commits, descriptions,
and public exploits, if available.
Our expectation was that training with this data would enable
the fine-tuned LLM to reason about security bugs,
their fixes, and likely input corpus,
more effectively than the
foundational model.</p></li></ol><p>We quickly realized that to pursue these directions effectively,
we first needed a dataset: a benchmark.
Our team divided tasks into three areas: 1) static analysis
using LLM prompts/agents, 2) developing a C benchmark from sources like CGC and
OSS-Fuzz, and 3) collecting a training dataset pairing CVEs with patches and PoCs for
open-source projects to fine-tune our in-house code model at Samsung or to
leverage open-source LLMs.</p><p>Remarkably, within 4-5 months, we accomplished all three goals,
and our LLM-based Cyber Reasoning System (CRS), dubbed Skynet,
performed surprisingly well on our benchmark,
and fine-tuning on a smaller dataset shows some promises like in python.</p><p>Time flew by. The cold winter of 2023 ended, and we found ourselves in the new
year of 2024.
I vividly remember that around this time, our dear friends from
Zellic left our team to pursue the Small Business Innovation Research (SBIR) track,
which DARPA supports with $1 million for the competition.
Unfortunately, Georgia Tech and Samsung were not eligible for this award.</p><h2 id="kick-off-with-surprises">Kick-off with Surprises!</h2><p><img title="image title" loading="lazy" decoding="async" width="600" height="390" src="https://team-atlanta.github.io/images/blog/atl/timeline_hu27eb7f2762e238941514cfe06aa31894_2087274_600x0_resize_q100_lanczos_3.png" alt="alter-text" onerror="this.onerror=&quot;null&quot;,this.src=&quot;/images/blog/atl/timeline_hu27eb7f2762e238941514cfe06aa31894_2087274_600x0_resize_q100_lanczos_3.png&quot;"></p><p>At the kick-off event on March 29th, AIxCC unveiled the first challenge project:
the Linux kernel, along with an example vulnerability,
<a href="https://nvd.nist.gov/vuln/detail/CVE-2021-43267" target="_blank">CVE-2021-43267</a>
.
This bug is <a href="https://www.sentinelone.com/labs/tipc-remote-linux-kernel-heap-overflow-allows-arbitrary-code-execution/" target="_blank">well documented</a>
,
and its PoC exploit is <a href="https://github.com/zzhacked/CVE-2021-43267" target="_blank">publicly available</a>
,
making it an excellent example to work on.</p><p>What makes this bug even more intriguing is the story behind it.
A security researcher audited the Linux kernel source code using
<a href="https://codeql.github.com/" target="_blank">CodeQL</a>
.
Specifically, the researcher was searching
for instances where 16-bit <code>size</code> parameters are passed to the <code>kmalloc()</code>
function for memory allocation,
using a dataflow-based CodeQL query.
The intuition was that a 16-bit <code>size</code> parameter
could easily lead to an <em>integer overflow</em> when accessing the allocated object.
However, the discovered bug was not caused by an integer overflow,
but an out-of-bound heap overflow due to a missing sanity check on the <code>size</code> and related inputs.</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>static</span> <span>bool</span> <span>tipc_crypto_key_rcv</span>(<span>struct</span> tipc_crypto <span>*</span>rx, <span>struct</span> tipc_msg <span>*</span>hdr)
</span></span><span><span>{
</span></span><span><span>	<span>struct</span> tipc_crypto <span>*</span>tx <span>=</span> <span>tipc_net</span>(rx<span>-&gt;</span>net)<span>-&gt;</span>crypto_tx;
</span></span><span><span>	<span>struct</span> tipc_aead_key <span>*</span>skey <span>=</span> NULL;
</span></span><span><span>	u16 key_gen <span>=</span> <span>msg_key_gen</span>(hdr);
</span></span><span><span>	u16 size <span>=</span> <span>msg_data_sz</span>(hdr);
</span></span><span><span>	u8 <span>*</span>data <span>=</span> <span>msg_data</span>(hdr);
</span></span><span><span>
</span></span><span><span>  ...
</span></span><span><span>
</span></span><span><span>	<span>/* Allocate memory for the key */</span>
</span></span><span><span>	skey <span>=</span> <span>kmalloc</span>(size, GFP_ATOMIC);
</span></span><span><span>	<span>if</span> (<span>unlikely</span>(<span>!</span>skey)) {
</span></span><span><span>		<span>pr_err</span>(<span>"%s: unable to allocate memory for skey</span><span>\n</span><span>"</span>, rx<span>-&gt;</span>name);
</span></span><span><span>		<span>goto</span> exit;
</span></span><span><span>	}
</span></span><span><span>
</span></span><span><span>	<span>/* Copy key from msg data */</span>
</span></span><span><span>	skey<span>-&gt;</span>keylen <span>=</span> <span>ntohl</span>(<span>*</span>((__be32 <span>*</span>)(data <span>+</span> TIPC_AEAD_ALG_NAME)));
</span></span><span><span>	<span>memcpy</span>(skey<span>-&gt;</span>alg_name, data, TIPC_AEAD_ALG_NAME);
</span></span><span><span>	<span>memcpy</span>(skey<span>-&gt;</span>key, data <span>+</span> TIPC_AEAD_ALG_NAME <span>+</span> <span>sizeof</span>(__be32),
</span></span><span><span>	       skey<span>-&gt;</span>keylen);
</span></span></code></pre></div><p>The <code>skey</code> was allocated with a <code>size</code> based on the user-provided <code>hdr</code>,
but <code>skey-&gt;key</code> was copied up to <code>skey-&gt;keylen</code>,
which was also user-controlled and could therefore be inconsistent with <code>size</code>.
Unfortunately, the kernel did not
perform a sanity check on these two parameters,
causing an out-of-boundary access.</p><div><pre tabindex="0"><code data-lang="diff"><span><span>commit fa40d9734a57bcbfa79a280189799f76c88f7bb0
</span></span><span><span>Author: Max VA &lt;maxv@sentinelone.com&gt;
</span></span><span><span>Date:   Mon Oct 25 17:31:53 2021 +0200
</span></span><span><span>
</span></span><span><span>    tipc: fix size validations for the MSG_CRYPTO type
</span></span><span><span>
</span></span><span><span>    The function tipc_crypto_key_rcv is used to parse MSG_CRYPTO messages
</span></span><span><span>    to receive keys from other nodes in the cluster in order to decrypt any
</span></span><span><span>    further messages from them.
</span></span><span><span>    This patch verifies that any supplied sizes in the message body are
</span></span><span><span>    valid for the received message.
</span></span><span><span>
</span></span><span><span>diff --git a/net/tipc/crypto.c b/net/tipc/crypto.c
</span></span><span><span>index c9391d38de85..dc60c32bb70d 100644
</span></span><span><span><span>--- a/net/tipc/crypto.c
</span></span></span><span><span><span></span><span>+++ b/net/tipc/crypto.c
</span></span></span><span><span><span></span><span>@@ -2285,43 +2285,53 @@ static bool tipc_crypto_key_rcv(struct tipc_crypto *rx, struct tipc_msg *hdr)
</span></span></span><span><span><span></span> 	u16 key_gen = msg_key_gen(hdr);
</span></span><span><span> 	u16 size = msg_data_sz(hdr);
</span></span><span><span> 	u8 *data = msg_data(hdr);
</span></span><span><span><span>+	unsigned int keylen;
</span></span></span><span><span><span>+
</span></span></span><span><span><span>+	/* Verify whether the size can exist in the packet */
</span></span></span><span><span><span>+	if (unlikely(size &lt; sizeof(struct tipc_aead_key) + TIPC_AEAD_KEYLEN_MIN)) {
</span></span></span><span><span><span>+		pr_debug("%s: message data size is too small\n", rx-&gt;name);
</span></span></span><span><span><span>+		goto exit;
</span></span></span><span><span><span>+	}
</span></span></span><span><span><span>+
</span></span></span><span><span><span>+	keylen = ntohl(*((__be32 *)(data + TIPC_AEAD_ALG_NAME)));
</span></span></span><span><span><span>+
</span></span></span><span><span><span>+	/* Verify the supplied size values */
</span></span></span><span><span><span>+	if (unlikely(size != keylen + sizeof(struct tipc_aead_key) ||
</span></span></span><span><span><span>+		     keylen &gt; TIPC_AEAD_KEY_SIZE_MAX)) {
</span></span></span><span><span><span>+		pr_debug("%s: invalid MSG_CRYPTO key size\n", rx-&gt;name);
</span></span></span><span><span><span>+		goto exit;
</span></span></span><span><span><span>+	}
</span></span></span><span><span><span></span> 
</span></span></code></pre></div><p>Two checks were added to fix this bug:
verifying that <code>size</code> is greater than the
minimum key size, and ensuring that <code>keylen</code> is consistent with <code>size</code>,
thereby preventing access beyond the allocated object.</p><h2 id="misunderstanding-1-pov">Misunderstanding 1: PoV</h2><p>Given a massive Linux repository (yes, 20 million lines of code),
where should we start?
The LLM approach is all about asking the right questions,
also known as prompt engineering.
We utilized various techniques like Chain-of-Thought (CoT)
and Tree-of-Thoughts (ToT),
and were exploring Retrieval Augmented Generation (RAG)
to quickly identify known 1-day bugs.</p><p>At that time, context size was limited;
the most advanced model, <code>gpt-3.5 turbo</code>
(yes, pre-<code>gpt-4</code> era) from OpenAI, supported 16k tokens,
making it crucial to ask the right question!
We initially tried identifying potentially vulnerable
code snippets using a range of static analysis tools,
including CodeQL, Semgrep and various tools from academic publications,
and then filtered the results with LLMs.
We even considered diffing the upstream Linux kernel
against the provided repository,
so that our CRS can look at the modified part of the code first.</p><p>We were confident our decision; to promote the use of AI tools,
the AIxCC organizers
would design the competition in a way that allows a single CRS codebase to
explore any code repository using 10+ programming languages and their
combinations.</p><p>Ah, around that time,
Google had just announced <code>gemini-pro</code>
with an impressive 128k context and the potential to support 1 million tokens!
Meanwhile, <code>gpt-4</code>
introduced a game-changing feature called function calling,
which allows the LLM to select which callback to use and integrate the results back into the prompt
at runtime. We felt that everything was evolving favorably for our CRS to adopt
these cutting-edge techniques.</p><p>However, PoV turned out to mean <em>bug-triggering input</em>
or a crashing input.
To demonstrate the existence of a bug,
each CRS needed to formulate an input
that the referee could quickly verify.
While this approach is
straightforward and objective for the competition,
it significantly discourages the adoption of LLMs in finding bugs.
Our team quickly realized
that we needed to pivot to the dynamic approaches like fuzzing
for the competition.</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>void</span> <span>tipc_trigger</span>(<span>uint8_t</span> <span>*</span>smashbuf, <span>uint32_t</span> smashlen, <span>int</span> seqno) {
</span></span><span><span>    <span>uint8_t</span> pkt[<span>0x1000</span>];
</span></span><span><span>    <span>uint32_t</span> w0, w1, w2, w3, w4, w5;
</span></span><span><span>
</span></span><span><span>    w0 <span>=</span> <span>hdr_version</span>(TIPC_VERSION);
</span></span><span><span>    w0 <span>|=</span> <span>hdr_size</span>(<span>6</span>);
</span></span><span><span>    w0 <span>|=</span> <span>hdr_user</span>(MSG_CRYPTO);
</span></span><span><span>    w0 <span>|=</span> <span>hdr_msg_size</span>(<span>24</span> <span>+</span> <span>36</span> <span>+</span> KEY_SIZE);
</span></span><span><span>    w1 <span>=</span> <span>0</span>;
</span></span><span><span>    w2 <span>=</span> seqno;
</span></span><span><span>    w3 <span>=</span> NODE_ID;
</span></span><span><span>    w4 <span>=</span> <span>0</span>;
</span></span><span><span>    w5 <span>=</span> <span>0</span>;
</span></span><span><span>
</span></span><span><span>    <span>memset</span>(pkt, <span>0</span>, <span>sizeof</span>(pkt));
</span></span><span><span>    <span>gen_tipc_hdr</span>(pkt, w0, w1, w2, w3, w4, w5);
</span></span><span><span>
</span></span><span><span>    <span>memcpy</span>(pkt<span>+</span><span>24</span>, <span>"HAXX"</span>, <span>4</span>);
</span></span><span><span>    <span>*</span>(<span>uint32_t</span><span>*</span>)(pkt<span>+</span><span>24</span><span>+</span><span>32</span>) <span>=</span> <span>be32</span>(KEY_SIZE <span>+</span> SMASH_SIZE <span>+</span> smashlen); <span>// &lt;- (1)
</span></span></span><span><span><span></span>    <span>memset</span>(pkt<span>+</span><span>24</span><span>+</span><span>36</span>, <span>'C'</span>, KEY_SIZE);
</span></span><span><span>    <span>memset</span>(pkt<span>+</span><span>24</span><span>+</span><span>36</span><span>+</span>KEY_SIZE, <span>'D'</span>, SMASH_SIZE);
</span></span><span><span>    <span>memcpy</span>(pkt<span>+</span><span>24</span><span>+</span><span>36</span><span>+</span>KEY_SIZE <span>+</span> SMASH_SIZE, smashbuf, smashlen);
</span></span><span><span>    <span>tipc_send</span>(pkt, <span>sizeof</span>(pkt));
</span></span><span><span>}
</span></span></code></pre></div><p>Formulating a bug-triggering input, including ensuring its reachability,
is a far more challenging task than simply spotting buggy code in the repository.
The strength of fuzzing, perhaps the opposite of a sophisticated LLM,
is that once a bug is found,
you almost always have a bug-triggering input.</p><p>In CVE-2021-43267, using CodeQL and auditing,
one could identify this bug, but triggering it is an entirely different challenge,
not to mention <a href="https://github.com/zzhacked/CVE-2021-43267/blob/main/poc.py" target="_blank">exploiting it</a>
.
For example,
TIPC must be properly set up first, and the <code>keylen</code> needs to be precisely
crafted in (1) to trigger the bug.</p><h2 id="misunderstanding-2-harnesses">Misunderstanding 2. Harnesses</h2><p>Sorry, what’s the input needed to trigger CVE-2021-43267? even with a fuzzer?<br>To fuzz the Linux <em>kernel</em>,
we needed a <em>user</em> program
that calls a sequence of system calls
with various arguments.
Considering the Linux kernel has over <a href="https://filippo.io/linux-syscall-table/" target="_blank">400 system calls</a>
to explore, this was far
from ideal for a competition setting.</p><p>We initially assumed that harnesses and test cases would be provided to indicate
which parts of the Linux kernel should be checked for bugs.
To tackle this,
we implemented and adopted various versions of Linux kernel fuzzers,
including a custom kernel syscall fuzzer with <code>kcov</code> and <code>kcmp</code>,
and also utilized the most popular Linux fuzzer, <a href="https://github.com/google/syzkaller" target="_blank">Syzkaller</a>
.
However, our focus remained on determining which sequences of system calls
to test, using syscall traces and static analysis of the provided program,
and then correctly formulating an end-to-end userspace program to trigger the bug.</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>/***
</span></span></span><span><span><span> * Blob begins with a 4 byte command count
</span></span></span><span><span><span> * [4-bytes command count]
</span></span></span><span><span><span> * Currently there are two commands:
</span></span></span><span><span><span> *  0 - send a packet blob
</span></span></span><span><span><span> *      [4-bytes size][4-bytes send flags][size-bytes packet data]
</span></span></span><span><span><span> *  1 - send a netlink packet
</span></span></span><span><span><span> *      [4-bytes Message Type][4-bytes Message Flags][4-bytes Netlink Protocol][4-bytes size][size bytes data]
</span></span></span><span><span><span> * blob_size MUST be a trusted value
</span></span></span><span><span><span> */</span>
</span></span><span><span><span>int</span> <span>harness</span>( <span>uint8_t</span> <span>*</span>blob, <span>uint32_t</span> blob_size)
</span></span><span><span>{ ... }
</span></span></code></pre></div><p><a href="https://github.com/aixcc-public/challenge-001-exemplar/" target="_blank">The Linux Kernel CP</a>
was announced in April and came with a harness,
<a href="https://github.com/aixcc-public/challenge-001-exemplar-source/blob/main/test_harnesses/linux_test_harness.c" target="_blank">linux_test_harness.c</a>
.
This announcement was full of surprises;
the program’s structure was provided by the harness,
which is alas what we primarily focused on,
and the <a href="https://github.com/aixcc-public/challenge-001-exemplar/blob/main/exemplar_only/blobs/sample_solve.bin" target="_blank"><code>blob</code></a>
needed to be fed to the harness in a way that triggers the bug.
The types of system calls we could interact with
were limited by the harness,
and our task was to find the right data input
that would <em>lead the harness</em>
to invoke the necessary sequence of system calls with the correct parameters.
In other words, we needed to understand the harness first
before dealing with the Linux kernel bugs.</p><p>Later, the Jenkins harness was announced, and more surprisingly,
it was a fuzz driver (often called a <em>fuzzing harness</em>),
a standalone program designed to
invoke APIs for fuzz testing.
In May, a new CP, called <code>mock-cp</code> (a userspace program),
was introduced along with a new harness format, which was simply a
shell script executing a CP binary with the provided input.
Such diverse formats got us thinking that
our CRS should adopt LLM to figure out the structure of the programs
and CPs first; like how to compile, how to correctly run, etc.</p><p>By June, the harness format was officially established -
surprisingly, yet not entirely unexpected:
<a href="https://llvm.org/docs/LibFuzzer.html" target="_blank">libfuzzer</a>
for
userspace programs (<code>mock-cp</code> and Nginx),
<a href="https://github.com/CodeIntelligenceTesting/jazzer" target="_blank">jazzer</a>
for Java programs
(Jenkins), while retaining the <code>blob</code>-based harness for the Linux kernel.
We continually updated our CRS to adapt to these changes,
but many of these decisions rendered our LLM-based components unnecessary.
This decision, however,
greatly helped all the participating teams
by reducing the engineering time needed for game operation.
Unfortunately, we were too proactive in reacting to these changes and ended up
wasting some engineering time as a result 😊.</p><p>A harness’s role is crucial in the AIxCC competition; it sets the context for
the CRS to trigger the bug and serves as a key factor in adjusting the
difficulty of bug discovery. Therefore, it’s important to strike a balance:
it should provide enough detail to relieve the CRS from unnecessary burdens,
allowing it to focus on bug finding, but without revealing too much information
about the bugs.</p><h2 id="misunderstanding-3-proof-of-understanding">Misunderstanding 3. Proof-of-understanding</h2><p>Unlike CGC, which treated the PoV (a proof-of-concept exploit)
as sufficient proof of bug discovery,
AIxCC required additional information—specifically, the bug type as classified by
<a href="https://cwe.mitre.org/top25/archive/2023/2023_kev_list.html" target="_blank">CWE</a>
,
to be provided along with the PoV.
This was an interesting decision, as AIxCC required
CRS to find bugs in the source code,
whereas CGC focused on discovering bugs in binaries.</p><p>Our team spent a lot of time brainstorming
how to accurately identify CWE categories,
primarily by using LLM prompts that leverage crashing inputs,
sanitizer reports, related code snippets, outputs from static analyzers, and more.
However, the notion of CWEs can be ambiguous when used as a scoring
mechanism for the competition.
For instance, should CVE-2021-43267 be classified
as (1) CWE-122 (Heap-based Buffer Overflow), (2) CWE-787 (Out-of-bounds Write),
or (3) CWE-20 (Improper Input Validation)?
The first two describe the symptoms
caused by the bug, while the third identifies the root cause, as the patch for
this bug involved adding input validations.</p><p>In the end, AIxCC shifted the focus from PoV to identifying the bug-introducing
commit (BIC) - the specific hash or commit ID in the git repository.
Combined with
the fuzzing harness and PoV, the CRS’s task was to run the fuzzing harness and
perform a <a href="https://git-scm.com/docs/git-bisect" target="_blank"><code>git-bisect</code></a>
to pinpoint
the BIC in the repository.
We did a simple bisecting in the semifinal but lots of improvement
required to be functional for the final event.</p><h2 id="misunderstanding-4-semantic-patching">Misunderstanding 4. Semantic patching</h2><p>Patching is one of the most intriguing aspects of AIxCC. In CGC, the PoV was
typically a simple exploit (like arbitrary read/write/execute),
so mitigation strategies (e.g., adding a stack canary) could effectively thwart the PoV.
In fact, patches could be applied <em>without even knowing</em> the specific bug;
for example,
adding a stack canary to all functions in a binary
can prevent buffer overflow exploits
that might exist in some places.</p><p>The challenge in CGC was that the focus was on the binary, and the organizers
introduced rules such as a minimum number of bytes changed and performance
overheads added to the scoring rubric (e.g., instrumenting all memory accesses
to prevent out-of-bound errors). These rules were designed to encourage
competitors to generate correct patches. Ultimately, this forced CRS to weigh
the pros and cons of universal patching, as both exploiting and patching were
extremely difficult during the CGC era,
resulting in a trade-off between losing
points from exploitation versus losing points from patching and availability.</p><p>In AIxCC, the CRS must generate a semantically correct patch that not only fixes
the identified PoV but also maintains the functional correctness of the CP. This
is a tricky task, as <em>correctness</em> cannot be formally defined for CRS - some
functional changes may be acceptable, while others may not, depending on the
code owner’s criteria.
One approach to addressing this ambiguity is to provide
test code to see if the patch passes the provided, so-called public tests.
However, CRS must still account for private tests set by the organizers.</p><p>In the semifinals, our CRS submitted a patch that successfully prevented the
crash and passed the public tests given to us during the competition,
but was ultimately rejected in the private
functionality tests.
We’re eager to learn more about the bug and the patch!</p><h2 id="misunderstanding-5-sanitizers">Misunderstanding 5: Sanitizers</h2><p>The concept of sanitizers was unclear to our team until we encountered
their concrete implementation
for memory-safe languages like Java, and more
specifically, for Jenkins, a web application written in Java!
The role of a sanitizer, essentially a bug oracle, is to determine whether a bug has been
correctly triggered.</p><p>In memory-unsafe languages like C, standard tools like ASAN and UBSAN can serve
as sanitizers to catch memory-safety issues with low or no false positives
(e.g., out-of-bound accesses should never occur).
However, in memory-safe languages,
things get trickier.
For example, is executing a command a legitimate
feature in CI tools like Jenkins,
or should it be treated as a command injection (CWE-78)?</p><p>In other words, sanitizers are more CP-specific
rather than programming language-specific;
each CP needs to provide custom sanitizers
(e.g., <a href="https://www.code-intelligence.com/blog/java-fuzzing-with-jazzer" target="_blank">path traversal sanitizers</a>
).</p><p>Our team initially spent time working on finding web-related bugs like XSS or
CSRF in Jenkins - areas where we believed LLMs could excel in seed generation.
However, once AIxCC announced
that the sanitizers for Java would be
<a href="https://github.com/CodeIntelligenceTesting/jazzer" target="_blank">jazzer</a>
sanitizers,
we decided to shift our focus more towards standard jazzer-based fuzzing.</p><h2 id="semifinal">Semifinal</h2><p>Our team dedicated most of our engineering effort to building a CRS for the
Linux Kernel, and we’re proud that our CRS was able to find and correctly
generate a patch for CVE-2021-43267 in the end.
However, during the semifinal,
it appeared that only <em>one</em> harness was provided, similar to the exemplar, and
none of the CRSes functioned properly for the Linux Kernel.
We loved to know more about how our Linux CRS functioned
during the competition.</p><p><img title="image title" loading="lazy" decoding="async" width="600" height="404" src="https://team-atlanta.github.io/images/blog/atl/dashboard_hucdfac242ccfb260b6c8aa0c492f17fbb_248513_600x0_resize_q100_lanczos_3.png" alt="alter-text" onerror="this.onerror=&quot;null&quot;,this.src=&quot;/images/blog/atl/dashboard_hucdfac242ccfb260b6c8aa0c492f17fbb_248513_600x0_resize_q100_lanczos_3.png&quot;"></p><p>In summary, our CRS earned a total of six achievement badges: five for
discovering bugs (i.e., first bloods) and one for a patch.</p><p><img title="image title" loading="lazy" decoding="async" width="600" height="333" src="https://team-atlanta.github.io/images/blog/atl/achievements_hu5072cc1c531c3d4ed879245bc9c46aa6_852847_600x0_resize_q100_lanczos_3.png" alt="alter-text" onerror="this.onerror=&quot;null&quot;,this.src=&quot;/images/blog/atl/achievements_hu5072cc1c531c3d4ed879245bc9c46aa6_852847_600x0_resize_q100_lanczos_3.png&quot;"></p><p>Our CRS found several unique bugs, which we will describe in a later blog post!</p><p>Aside from the known CPs—Linux (C), Jenkins (Java), and Nginx (C) - there were new
CPs introduced, namely Tika (Java) and sqlite3 (C).
Our CRS performed relatively
well on sqlite3, but unfortunately,
our Java CRS struggled with Tika.
We would love to learn more about what happened during the competition.
Tika, a popular file format parser,
has many unique features, such as recursively parsing
embedded objects,
which may have contributed to the challenges we faced.</p><h2 id="looking-ahead-to-the-aixcc-final-">Looking Ahead to the AIxCC Final 🎉</h2><figure role="group" aria-describedby="caption-AIxCC Finalists"><img title="image title" loading="lazy" decoding="async" width="600" height="331" src="https://team-atlanta.github.io/images/blog/atl/finalists_hua191f10a8c6185b27f399163513ba79a_538686_600x0_resize_q100_lanczos_3.png" alt="alter-text" onerror="this.onerror=&quot;null&quot;,this.src=&quot;/images/blog/atl/finalists_hua191f10a8c6185b27f399163513ba79a_538686_600x0_resize_q100_lanczos_3.png&quot;"><figcaption id="caption-AIxCC Finalists">AIxCC Finalists</figcaption></figure><p>We are thrilled that our team has advanced to the AIxCC finals! We have several ideas that could make the competition even more exciting:</p><ul><li><p><strong>Different execution times based on code complexity.</strong><br>The Linux kernel, with its 6,000 files and 20 million lines of code, requires
substantial time for bookkeeping like building, bootstrapping, and bisecting.
Compared to smaller programs (e.g., 200k in Tika), it would be beneficial to
allocate more time for CRSes to navigate such complex codebases.</p></li><li><p><strong>More programming languages and their combinations.</strong><br>Top candidates include Python, Rust, and JavaScript/HTML, along with
combinations like JNI (C) in Java or Rust device drivers in the Linux kernel.
These would offer a more comprehensive evaluation of CRS capabilities in
diverse and challenging settings where CRS is most needed.</p></li><li><p><strong>Standardized execution environments.</strong><br>Standardizing the compiler (e.g., <code>clang-18</code>), runtime (e.g., JVM version),
and base Docker image ahead of time would help teams explore more advanced
techniques, such as LLM-based instrumentation, in a controlled environment.</p></li><li><p><strong>Improved visualization during the competition.</strong><br>While the AIxCC village was impressively set up, competing teams and
participants had limited visibility into the competition’s progress and how
each CRS was functioning. To capture more attention from <a href="https://www.reddit.com/r/Defcon/comments/1eta3tj/was_the_aixcc_village_disappointing_to_anyone_else/" target="_blank">the DEF CON audience</a>
,
it would be beneficial to expose more technical information during the
competition - such as showing current prompts of each CRS in turn, their CPU
usage, or even stdout from CRSes (for fun), along with explanations of the
progress.</p></li></ul><p>With our baseline system up and running, it’s time for our team to explore the
possibility of incorporating LLMs or ML techniques into our CRS workflow. If
you’re passionate about AIxCC and as committed to the competition as we are,
feel free to <a href="mailto:aixcc-atl@googlegroups.com">contact us</a>
!</p><p>We are fortunate to have support from generous sponsors like GT/GTRI, Samsung,
and KAIST/NYU. If your company is interested in sponsoring our team, we would be
happy to discuss further!</p><p>Last but not least, we want to extend our heartfelt thanks to the AIxCC
organizers for launching the competition we’ve been craving. Hackers thrive on
competition-driven innovation, and this has been an exciting opportunity for all
of us.</p><div><p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/FkJimGWJYgw?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" title="YouTube video"></iframe></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Family poisoned after using AI-generated mushroom identification book (221 pts)]]></title>
            <link>https://old.reddit.com/r/LegalAdviceUK/comments/1etko9h/family_poisoned_after_using_aigenerated_mushroom/</link>
            <guid>41269514</guid>
            <pubDate>Fri, 16 Aug 2024 19:24:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LegalAdviceUK/comments/1etko9h/family_poisoned_after_using_aigenerated_mushroom/">https://old.reddit.com/r/LegalAdviceUK/comments/1etko9h/family_poisoned_after_using_aigenerated_mushroom/</a>, See on <a href="https://news.ycombinator.com/item?id=41269514">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>EDIT: I have not stated the name of the online marketplace. Assumptions are being made in the comments, which I am neither confirming nor denying.</strong></p>

<p>My entire family was in hospital last week after accidentally consuming poisonous mushrooms.</p>

<p>My wife purchased a book from a major online retailer for my birthday. The book is entitled something similar to: "Mushrooms UK: A Guide to Harvesting Safe and Edible Mushrooms."</p>

<p>It comes with pictures of the mushrooms to help identify each one.</p>

<p>Unfortunately, the book in question was not accurate. A closer investigation reveals that the images of mushrooms are AI generated, and we have now found two instances of text where a sentence ends and is followed up with a random questions or fourth-wall breaking statements.</p>

<p>For example:</p>

<p><em>"In conclusion, morels are delicious mushrooms which can be consumed from August to the end of Summer. Let me know if there is anything else I can help you with."</em></p>

<p>The online retailer have instructed me to return the book and they will refund it. The book has been removed from sale from the online retailer, however, it appears there are dozens more in a similar style.</p>

<p>1.) Should I return this book to the retailer? I'm concerned I would lose any evidence I have if I return it. The purchase has already disappeared from my online account. It simply looks like it doesn't exist anymore. I still have the email.</p>

<p>2.) Are my family entitled to any compensation for my son and my wife's lost time at work? As well as the sickness they experienced?</p>

<p>3.) Can I report the creation of this book to the police as a crime?</p>

<p><strong>Just for clarity: We did not know it was AI-generated when we bought it! This was not disclosed on the website!</strong></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VanillaJSX.com (409 pts)]]></title>
            <link>https://vanillajsx.com/</link>
            <guid>41269321</guid>
            <pubDate>Fri, 16 Aug 2024 19:01:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vanillajsx.com/">https://vanillajsx.com/</a>, See on <a href="https://news.ycombinator.com/item?id=41269321">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="root"><p>What if JSX just returned DOM elements?</p><div data-sample="sample1"><p><a target="_blank" href="https://github.com/sdegutis/vanillajsx.com/blob/main/site/samples/sample1.tsx">View source</a></p><pre><code>export default function ClickMe() {
  let i = 0;
  const el = &lt;button&gt;Click me&lt;/button&gt; as HTMLButtonElement;
  el.onclick = (e) =&gt; {
    el.textContent = `Clicked ${++i} times`;
  };
  return el;
}
</code></pre></div><p>Would they be reusable?</p><p>Could they keep their own state?</p><div data-sample="sample2"><p><a target="_blank" href="https://github.com/sdegutis/vanillajsx.com/blob/main/site/samples/sample2.tsx">View source</a></p><pre><code>import ClickMe from "./sample1.js";

export default () =&gt; &lt;&gt;
  &lt;p&gt;&lt;ClickMe /&gt;&lt;/p&gt;
  &lt;p&gt;&lt;ClickMe /&gt;&lt;/p&gt;
  &lt;p&gt;&lt;ClickMe /&gt;&lt;/p&gt;
&lt;/&gt;;
</code></pre></div><p>How would they work together?</p><p>Could they create an interactive DOM tree?</p><div data-sample="sample3"><p><a target="_blank" href="https://github.com/sdegutis/vanillajsx.com/blob/main/site/samples/sample3.tsx">View source</a></p><pre><code>function TodoInput(attrs: { add: (v: string) =&gt; void }) {
  const input = &lt;input /&gt; as HTMLInputElement;
  input.placeholder = 'Add todo item...';
  input.onkeydown = (e) =&gt; {
    if (e.key === 'Enter') {
      attrs.add(input.value);
      input.value = '';
    }
  };
  return input;
}

class TodoList {
  ul = &lt;ul class='todolist' /&gt; as HTMLUListElement;
  add(v: string) {
    const item = &lt;li&gt;{v}&lt;/li&gt; as HTMLLIElement;
    item.onclick = () =&gt; item.remove();
    this.ul.append(item);
  }
}

export default () =&gt; {
  const list = new TodoList();
  list.add('foo');
  list.add('bar');
  return &lt;&gt;
    &lt;TodoInput add={(v) =&gt; list.add(v)} /&gt;
    {list.ul}
  &lt;/&gt;;
};
</code></pre></div><p>How would they handle large data?</p><p>Could they be convenient without a virtual dom?</p><div data-sample="sample4"><p><a target="_blank" href="https://github.com/sdegutis/vanillajsx.com/blob/main/site/samples/sample4.tsx">View source</a></p><pre><code>import { data } from "../fetch-dataset.js";

export default function FindNames() {
  const status = &lt;p style='margin:1em 0' /&gt; as HTMLParagraphElement;
  const results = &lt;ul /&gt; as HTMLUListElement;
  const input = &lt;input
    value='eri(c|k)a?'
    autocomplete='new-password'
  /&gt; as HTMLInputElement;

  const updateMatches = () =&gt; {
    const matched = (data.entries()
      .filter(([k]) =&gt; k.match(input.value))
      .toArray());

    const matches = (Iterator.from(matched)
      .map(match =&gt; &lt;Item regex={input.value} match={match} /&gt;)
      .take(30));

    results.replaceChildren(...matches);
    status.textContent = `${matched.length} / ${data.size}`;
  };

  input.oninput = updateMatches;
  updateMatches();

  return &lt;div class='sample4'&gt;
    {input}
    {status}
    {results}
  &lt;/div&gt;;
}

function Item(attrs: { match: [string, number], regex: string }) {
  const [name, count] = attrs.match;
  const total = &lt;small style='color:#fff3'&gt;({count})&lt;/small&gt;;
  return &lt;li&gt;
    &lt;span innerHTML={highlight(name, attrs.regex)} /&gt; {total}
  &lt;/li&gt;;
}

function highlight(str: string, regex: string) {
  if (!regex) return str;
  const r = new RegExp(`(${regex})`, 'gi');
  return str.replace(r, '&lt;span class="match"&gt;$1&lt;/span&gt;');
}
</code></pre></div><p>That's why I wrote <a target="_blank" href="https://code.immaculatalibrary.com/">imlib</a> (<a href="https://github.com/sdegutis/imlib">src</a>).</p><p>It came out of my work on <a target="_blank" href="https://www.immaculatalibrary.com/">immaculatalibrary.com</a> (<a href="https://github.com/sdegutis/immaculatalibrary.com">src</a>).</p><p>I started using it to build <a target="_blank" href="https://minigamemaker.com/">minigamemaker.com</a> (<a href="https://github.com/sdegutis/minigamemaker.com">src</a>).</p><p>I also used it to build <a target="_blank" href="https://github.com/sdegutis/vanillajsx.com/tree/main/site">the website you're reading</a> (&lt;-- src).</p><p>It was created because the status quo wasn't good enough for my site.</p><p>It is now my favorite way to make apps.</p><p>(Also, here's a much better <a href="https://sdegutis.github.io/imlib-todolist/">imlib todo-list app</a> (<a href="https://github.com/sdegutis/imlib-todolist/blob/main/site/app.tsx">src</a>)).</p><p>The two most complex "interactive apps" I've built with this are:</p><ul><li><a href="https://www.immaculatalibrary.com/books.html">Books search page</a> (<a href="https://github.com/sdegutis/immaculatalibrary.com/blob/main/site/scripts/books-page.tsx">src</a>)</li><li><a href="https://www.immaculatalibrary.com/prayers/">Prayer page</a> (<a href="https://github.com/sdegutis/immaculatalibrary.com/blob/main/site/prayers/client.tsx">src</a>)</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Backdoor discovered in several brands of RFID cards [pdf] (235 pts)]]></title>
            <link>https://eprint.iacr.org/2024/1275.pdf</link>
            <guid>41269249</guid>
            <pubDate>Fri, 16 Aug 2024 18:53:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eprint.iacr.org/2024/1275.pdf">https://eprint.iacr.org/2024/1275.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=41269249">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Disrupting a covert Iranian influence operation (117 pts)]]></title>
            <link>https://openai.com/index/disrupting-a-covert-iranian-influence-operation/</link>
            <guid>41269113</guid>
            <pubDate>Fri, 16 Aug 2024 18:39:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/disrupting-a-covert-iranian-influence-operation/">https://openai.com/index/disrupting-a-covert-iranian-influence-operation/</a>, See on <a href="https://news.ycombinator.com/item?id=41269113">Hacker News</a></p>
Couldn't get https://openai.com/index/disrupting-a-covert-iranian-influence-operation/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[What Is a Knowledge Graph? (145 pts)]]></title>
            <link>https://neo4j.com/blog/what-is-knowledge-graph/</link>
            <guid>41268929</guid>
            <pubDate>Fri, 16 Aug 2024 18:22:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neo4j.com/blog/what-is-knowledge-graph/">https://neo4j.com/blog/what-is-knowledge-graph/</a>, See on <a href="https://news.ycombinator.com/item?id=41268929">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><img fetchpriority="high" decoding="async" src="https://dist.neo4j.com/wp-content/uploads/20240722075336/what-is-knowledge-graph.png" alt="Knowledge graph concept that includes organizing principles, relationships, and data." width="1200" height="628" srcset="https://dist.neo4j.com/wp-content/uploads/20240722075336/what-is-knowledge-graph.png 1200w, https://dist.neo4j.com/wp-content/uploads/20240722075336/what-is-knowledge-graph-300x157.png 300w, https://dist.neo4j.com/wp-content/uploads/20240722075336/what-is-knowledge-graph-1024x536.png 1024w, https://dist.neo4j.com/wp-content/uploads/20240722075336/what-is-knowledge-graph-150x79.png 150w, https://dist.neo4j.com/wp-content/uploads/20240722075336/what-is-knowledge-graph-768x402.png 768w, https://dist.neo4j.com/wp-content/uploads/20240722075336/what-is-knowledge-graph-600x314.png 600w" sizes="(max-width: 1200px) 100vw, 1200px"></p>

<p>A <a href="https://neo4j.com/use-cases/knowledge-graph/" target="_blank" rel="noopener">knowledge graph</a> is an organized representation of real-world entities and their relationships. It is typically stored in a graph database, which natively stores the relationships between data entities.<strong> </strong>Entities in a knowledge graph can represent objects, events, situations, or concepts. The relationships between these entities capture the context and meaning of how they are connected.</p>
<p>A knowledge graph stores data and relationships alongside frameworks known as organizing principles. They can be thought of as rules or categories around the data that provide a flexible, conceptual structure to drive deeper data insights. The usefulness of a knowledge graph lies in the way it organizes the principles, data, and relationships to surface new knowledge for your user or business. The design is useful for many usage patterns, including real-time applications, search and discovery, and <a href="https://neo4j.com/generativeai/" target="_blank" rel="noopener">grounding generative AI </a>for question-answering.</p>
<p>Sometimes, people overcomplicate the concept of a knowledge graph. You might hear about enterprise-wide structures that consolidate and connect information across data silos and various sources. While that <em>does</em> describe a knowledge graph (one that can underpin a data integration use case), it describes one with a wide scope. Thinking only in terms of bridging large datasets and multiple data sources can make creating and implementing knowledge graphs seem complicated and time-consuming. But knowledge graphs don’t need to be broad or elaborate. You can build one with a much smaller scope to solve a use-case-specific problem.  </p>
<br><h2><strong>How Knowledge Graphs Work</strong></h2>
<p>You may have heard of knowledge graphs in the context of search engines. The <a href="https://blog.google/products/search/introducing-knowledge-graph-things-not/" target="_blank" rel="noopener">Google Knowledge Graph</a> changed how we search for and find information on the Web. It amasses facts about people, places, and things into an organized network of entities. When you do a Google search for information, it uses the connections between entities to surface the most relevant results in context, for example, in the box Google calls the “<a href="https://support.google.com/knowledgepanel/answer/9163198?hl=en" target="_blank" rel="noopener">knowledge panel</a>.” </p>
<p><img decoding="async" src="https://dist.neo4j.com/wp-content/uploads/20240722075320/la-sagrada.png" alt="La sagrada familia: Google knowledge graph." width="1024" height="657" srcset="https://dist.neo4j.com/wp-content/uploads/20240722075320/la-sagrada.png 1024w, https://dist.neo4j.com/wp-content/uploads/20240722075320/la-sagrada-300x192.png 300w, https://dist.neo4j.com/wp-content/uploads/20240722075320/la-sagrada-150x96.png 150w, https://dist.neo4j.com/wp-content/uploads/20240722075320/la-sagrada-768x493.png 768w, https://dist.neo4j.com/wp-content/uploads/20240722075320/la-sagrada-600x385.png 600w" sizes="(max-width: 1024px) 100vw, 1024px"></p>
<p><em>The Google knowledge panel of La Sagrada Familia includes an image of the site, a map, a description, address, hours of operation, the architects who built it, its height, and more. </em></p>


<p>The entities in the Google knowledge graph represent the world as we know it, marking a shift from “strings to things.” Behind this simple phrase is the profound concept of treating information on the web as entities rather than a bunch of text. Since information is organized as a network of entities, Google can tap into the collective intelligence of the knowledge graph to return results tailored to the<em> meaning</em> of your query rather than a simple keyword match.  </p>
<br><h2><strong>Key Characteristics</strong></h2>
<p>Now that you understand how knowledge graphs organize and access data with context, let’s look at the building blocks of a knowledge graph data model. The definition of knowledge graphs varies depending on whom you ask, but we can distill the essence into three key components: nodes, relationships, and organizing principles. </p>
<h3>Nodes </h3>
<p><strong><em>Nodes</em></strong> denote and store details about entities, such as people, places, objects, or institutions. Each node has a (or sometimes several) label to identify the node type and may optionally have one or more properties (attributes). Nodes are also sometimes called <em>vertices</em>.</p>
<p>For example, the nodes in an e-commerce knowledge graph typically represent entities such as people (customers and prospects), products, and orders: 

</p><p><img decoding="async" src="https://dist.neo4j.com/wp-content/uploads/20240722075309/ecommerce-knowledge-graph.png" alt="Example of nodes in an e-commerce graph." width="600" srcset="https://dist.neo4j.com/wp-content/uploads/20240722075309/ecommerce-knowledge-graph.png 2048w, https://dist.neo4j.com/wp-content/uploads/20240722075309/ecommerce-knowledge-graph-300x252.png 300w, https://dist.neo4j.com/wp-content/uploads/20240722075309/ecommerce-knowledge-graph-1024x860.png 1024w, https://dist.neo4j.com/wp-content/uploads/20240722075309/ecommerce-knowledge-graph-150x126.png 150w, https://dist.neo4j.com/wp-content/uploads/20240722075309/ecommerce-knowledge-graph-768x645.png 768w, https://dist.neo4j.com/wp-content/uploads/20240722075309/ecommerce-knowledge-graph-1536x1289.png 1536w, https://dist.neo4j.com/wp-content/uploads/20240722075309/ecommerce-knowledge-graph-600x504.png 600w" sizes="(max-width: 2048px) 100vw, 2048px"></p>
<h3>Relationships</h3>
<p><strong><em>Relationships</em></strong> link two nodes together: they show how the entities are related. Like nodes, each relationship has a label identifying the relationship type and may optionally have one or more properties. Relationships are also sometimes called <em>edges</em>. </p>
<p>In the e-commerce example, relationships exist between the customer and order nodes, capturing the “placed order” relationship between customers and their orders:</p>
<p><img decoding="async" src="https://dist.neo4j.com/wp-content/uploads/20240722075323/relationship-order.png" alt="Relationship of a person to Order." width="600" srcset="https://dist.neo4j.com/wp-content/uploads/20240722075323/relationship-order.png 2048w, https://dist.neo4j.com/wp-content/uploads/20240722075323/relationship-order-300x96.png 300w, https://dist.neo4j.com/wp-content/uploads/20240722075323/relationship-order-1024x329.png 1024w, https://dist.neo4j.com/wp-content/uploads/20240722075323/relationship-order-150x48.png 150w, https://dist.neo4j.com/wp-content/uploads/20240722075323/relationship-order-768x246.png 768w, https://dist.neo4j.com/wp-content/uploads/20240722075323/relationship-order-1536x493.png 1536w, https://dist.neo4j.com/wp-content/uploads/20240722075323/relationship-order-600x192.png 600w" sizes="(max-width: 2048px) 100vw, 2048px"></p>
<h3>Organizing Principle(s) </h3>
<p><strong><em>Organizing Principles</em></strong> are a framework, or schema, that organizes nodes and relationships according to fundamental concepts essential to the use cases at hand. Unlike many data designs, knowledge graphs easily incorporate multiple organizing principles.</p>
<p>Organizing principles range from simple (product line -&gt; product category -&gt; product taxonomy) to complex (a complete business vocabulary that explains the data in the graph). Think of an organizing principle as a conceptual map or metadata layer overlaying the data and relationships in the graph.</p>
<p>The model uses the same node-and-relationship structure as the rest of the knowledge graph to describe the organizing principles – which means you can write queries that draw from both instance data and organizing principles. </p>
<p>In the e-commerce example, an organizing principle might be product types and categories:</p>
<p><img decoding="async" src="https://dist.neo4j.com/wp-content/uploads/20240722083709/kg-organizing-principle-1.png" alt="Organizing principle of a knowledge graph." width="800" srcset="https://dist.neo4j.com/wp-content/uploads/20240722083709/kg-organizing-principle-1.png 5246w, https://dist.neo4j.com/wp-content/uploads/20240722083709/kg-organizing-principle-1-300x147.png 300w, https://dist.neo4j.com/wp-content/uploads/20240722083709/kg-organizing-principle-1-1024x500.png 1024w, https://dist.neo4j.com/wp-content/uploads/20240722083709/kg-organizing-principle-1-150x73.png 150w, https://dist.neo4j.com/wp-content/uploads/20240722083709/kg-organizing-principle-1-768x375.png 768w, https://dist.neo4j.com/wp-content/uploads/20240722083709/kg-organizing-principle-1-1536x750.png 1536w, https://dist.neo4j.com/wp-content/uploads/20240722083709/kg-organizing-principle-1-2048x1001.png 2048w, https://dist.neo4j.com/wp-content/uploads/20240722083709/kg-organizing-principle-1-600x293.png 600w" sizes="(max-width: 5246px) 100vw, 5246px"></p>
<h3>What About Ontologies?</h3>
<p>When learning about knowledge graphs, you might come across articles on <strong><em>ontologies</em></strong> and wonder where they fit in. An ontology is a formal specification of the concepts and the relationships between them for a given subject area; semantic networks are a common way to represent ontologies. Put simply, ontologies are a type of organizing principle. </p>
<p>Ontologies can be complex and require a great deal of effort to define and maintain. When deciding whether an ontology is needed, it’s critical to consider the problems you’re trying to solve with a knowledge graph. In many cases, it won’t be necessary. In the e-commerce example, using a product taxonomy as the organizing principle is sufficient for a product recommendation use case. </p>
<p>Think of the knowledge graph as a growing, evolving system to simplify your design in the early stages and deliver value sooner. If you pick the right technology to implement your knowledge graph, you can expand and evolve the graph as your needs change. In this way, you can add ontologies when your use case requires them rather than forcing yourself to build them up-front.</p>
<br><h2><strong>Knowledge Graph Example</strong></h2>
<p>Let’s see what a knowledge graph might look like. Below is a simple knowledge graph of the e-commerce example that shows nodes as circles and relationships between them as arrows. The organizing principles are also stored as nodes and relationships, so the illustration uses some color shading to show which nodes and relationships are the instance data and which are the organizing principles:</p>

<p><img decoding="async" src="https://dist.neo4j.com/wp-content/uploads/20240722075316/knowledge-graph-example-1.png" alt="Example of a knowledge graph." width="1000" srcset="https://dist.neo4j.com/wp-content/uploads/20240722075316/knowledge-graph-example-1.png 2048w, https://dist.neo4j.com/wp-content/uploads/20240722075316/knowledge-graph-example-1-300x209.png 300w, https://dist.neo4j.com/wp-content/uploads/20240722075316/knowledge-graph-example-1-1024x712.png 1024w, https://dist.neo4j.com/wp-content/uploads/20240722075316/knowledge-graph-example-1-150x104.png 150w, https://dist.neo4j.com/wp-content/uploads/20240722075316/knowledge-graph-example-1-768x534.png 768w, https://dist.neo4j.com/wp-content/uploads/20240722075316/knowledge-graph-example-1-1536x1068.png 1536w, https://dist.neo4j.com/wp-content/uploads/20240722075316/knowledge-graph-example-1-600x417.png 600w" sizes="(max-width: 2048px) 100vw, 2048px"></p>
<p><em>An example knowledge graph showing nodes as circles and relationships as arrows. The instance data and organizing principles are highlighted for display.</em></p>

<br><h2><strong>Knowledge Graphs and Graph Databases</strong></h2>
<p>Creating a knowledge graph involves conceptually mapping the graph data model and then implementing it in a database. There are many databases to choose from, but choosing the right one can simplify the design process, speed up development and implementation, and make it easier to adapt to future changes and improvements. </p>

<h3>Property Graphs</h3>

<p>Native property graph databases, such as <a href="https://neo4j.com/product/neo4j-graph-database/" target="_blank" rel="noopener">Neo4j</a>, are a logical choice for implementing knowledge graphs. They natively store information as nodes, relationships, and properties, allowing for an intuitive <a href="https://neo4j.com/product/bloom/" target="_blank" rel="noopener">visualization</a> of highly interconnected data structures. The physical database matches the conceptual data model, making designing and developing the knowledge graph easier. When you use property graphs, you get:</p>

<ul><ul><li><strong>Simplicity and ease of design:</strong> Property graphs allow for straightforward data modeling when designing the knowledge graph. Because the conceptual and physical models are very similar (often the same), the transition from design to implementation is more straightforward (and easy to explain to non-technical users). </li><br>

<li><strong>Flexibility:</strong> It’s easy to add new data, properties, relationship types, and organizing principles without extensive refactoring or code rewrites. As needs change, you can iterate and incrementally expand the knowledge graph’s data, relationships, and organization. </li><br>

<li><strong>Performance:</strong> Property graphs offer superior query performance compared to alternatives like RDF databases or relational databases, especially for complex traversals and many-to-many relationships. This performance comes from storing the relationships between entities directly in the database rather than re-generating them using joins in queries. A native property graph database traverses relationships by following pointers in memory, making queries that traverse even complex chains of many relationships very fast. </li><br>

<li><strong>Developer-friendly Code:</strong> Property graphs support an intuitive and expressive ISO query language standard, <a href="https://neo4j.com/blog/gql-international-standard/" target="_blank" rel="noopener">GQL</a>, which means you have less code to write, debug, and maintain than SQL or SPARQL. Neo4j’s Cypher is the most widely used implementation of GQL.</li></ul></ul>


<h3>Property Graph Vs. Triple Stores (RDF)</h3>
<p>People sometimes think of <a href="https://neo4j.com/blog/rdf-vs-property-graphs-knowledge-graphs/" target="_blank" rel="noopener">property graphs and triple stores </a>as equally viable options for building a knowledge graph, but triple stores (also known as RDF databases) have considerable disadvantages. </p>
<p>Based on the Resource Description Framework (RDF), triple stores use a granular approach to design and storage. Triple stores express all data in the form of subject-predicate-object “triples.” This model does not support relationships with properties or multiple same-typed relationships between entities. To accommodate real-world use cases, you will need to implement workarounds. Common workarounds include turning relationships into objects (called <em>reification</em>) or using <em>singleton properties</em> to capture properties using extra “type-of” relationships. These workarounds mean larger databases, additional complexity in the physical model, and poor query performance.</p>
<p>Because reification and singleton properties force tough decisions about the design up front, triple stores don’t lend themselves to solving real-world problems that involve messy data domains. Knowledge graphs built on a triple store are more challenging to design, time-consuming to implement, and difficult to change. </p>
<h3>Property Graph Vs. Relational Databases</h3>

<p>Relational databases and other non-native graph approaches suffer similar design friction. Neither relational nor document databases store relationships – they must be synthesized at runtime with joins or value lookups in query code. Since the relationships reside in the code rather than with the dataset, each application and data use must have its own implementation. SQL (the relational database query language) forces you to define every join in the query itself. As a result, the knowledge graph becomes more difficult to manage and yields poor runtime performance as the number of relationships expands.</p>
<br><h2><strong>Knowledge Graph Use Cases</strong></h2>
<p>Knowledge graphs offer a powerful tool for storing and organizing data to enable a more sophisticated understanding of that data. To understand how companies have done this, let’s look at examples of using knowledge graphs to tackle particular problems. Though not a comprehensive list of use cases, it’s a set of concrete examples demonstrating knowledge graphs in real-world applications.</p>
<h3>Generative AI for Enterprise Search Applications </h3>
<p>In <strong><a href="https://neo4j.com/generativeai/" target="_blank" rel="noopener">generative AI</a></strong><strong> </strong>applications<strong>, </strong>knowledge graphs capture and organize key domain-specific or proprietary company information. Knowledge graphs are not limited to structured data; they can handle less organized data as well. </p>
<p><a href="https://neo4j.com/blog/graphrag-manifesto/" target="_blank" rel="noopener">GraphRAG</a>, a technique that grounds large language models with knowledge graphs, is emerging as the foundation of AI applications that use proprietary domain data (these are known as RAG applications). A knowledge graph grounding increases response accuracy and improves explainability with the context provided by data relationships. Industry leaders <a href="https://www2.deloitte.com/content/dam/Deloitte/nl/Documents/risk/deloitte-nl-risk-responsible-enterprise-decisions-with-knowledge-enriched-generative-ai-whitepaper-download.pdf" target="_blank" rel="noopener">such as Deloitte</a> highlight the critical role of knowledge graphs for building enterprise-grade GenAI. Gartner places knowledge graphs having a “high mass,” being an impactful technology for GenAI today: </p>

<p><img decoding="async" src="https://dist.neo4j.com/wp-content/uploads/20240729094634/gartner-genai.png" alt="This Impact Radar from Gartner highlights knowledge graphs as a high-impact technology within the Generative AI landscape." width="600" srcset="https://dist.neo4j.com/wp-content/uploads/20240729094634/gartner-genai.png 1310w, https://dist.neo4j.com/wp-content/uploads/20240729094634/gartner-genai-300x261.png 300w, https://dist.neo4j.com/wp-content/uploads/20240729094634/gartner-genai-1024x892.png 1024w, https://dist.neo4j.com/wp-content/uploads/20240729094634/gartner-genai-150x131.png 150w, https://dist.neo4j.com/wp-content/uploads/20240729094634/gartner-genai-768x669.png 768w, https://dist.neo4j.com/wp-content/uploads/20240729094634/gartner-genai-600x523.png 600w" sizes="(max-width: 1310px) 100vw, 1310px"></p><p><em>This Impact Radar from Gartner highlights knowledge graphs as a high-impact technology within the Generative AI landscape (Credit: Gartner) </em></p>

<h3>Fraud Detection and Analytics in Financial Services, Banking, and Insurance</h3>

<p>In <strong><a href="https://neo4j.com/use-cases/fraud-detection/" target="_blank" rel="noopener">Fraud Detection and Analytics</a></strong><a href="https://neo4j.com/use-cases/fraud-detection/" target="_blank" rel="noopener">,</a> the knowledge graph represents a network of transactions, their participants, and relevant information about them. Companies can use this knowledge graph to quickly identify suspicious activity, investigate suspected fraud, and evolve their knowledge graph to keep up with changing fraud patterns. Algorithms such as pathfinding and community detection provide key signals to machine learning algorithms that can uncover more sophisticated fraud networks.</p>

<h3>Master Data Management</h3>
<p>In <strong><a href="https://neo4j.com/use-cases/master-data-management/" target="_blank" rel="noopener">Master Data Management</a></strong> (e.g., for <strong>Customer 360</strong> use cases), the knowledge graph provides an organized, resolved (i.e., “de-duped”), and comprehensive database of a company’s customers and the company’s interactions with them. </p>
<p>This organized view of customers is especially important for companies with multiple divisions or applications interacting with customers. Without a knowledge graph, it can be difficult or impossible to obtain an accurate view of the customer. A knowledge graph links customer behaviors across multiple applications through an organizing principle that identifies them as coming from the same customer. </p>

<h3>Supply Chain Management </h3>
<p>In <strong><a href="https://neo4j.com/blog/supply-chain-forecasting/" target="_blank" rel="noopener">Supply Chain Management</a></strong>, a knowledge graph represents the network of suppliers, raw materials, products, and logistics that work together to supply a company’s operations and customers. This end-to-end supply chain visibility allows managers to identify weak points and predict where disruptions may occur. Graph algorithms such as <a href="https://neo4j.com/blog/graph-algorithms-neo4j-shortest-path/" target="_blank" rel="noopener">shortest path</a> optimize the supply chain in real time by finding the most direct route between A and B.</p>
<h3>Investigative Journalism</h3>
<p>In <strong><a href="https://neo4j.com/blog/electiongraph-report-2/" target="_blank" rel="noopener">Investigative Journalism</a></strong><strong>,</strong> knowledge graphs capture key entities (companies, people, bank accounts, etc.) and activities under investigation. Organizing these entities in relation to one another makes it possible to find hidden patterns, such as distant relationships between entities that shouldn’t be present. </p>
<p>Investigators may use techniques such as entity resolution to reveal entities hiding behind fake or shell identities to mask their activities. Algorithms like community detection and link prediction also provide insight and areas for further investigation.</p>
<h3>Drug Discovery in Healthcare Research</h3>
<p>Knowledge graphs store information about the research subject in <a href="https://neo4j.com/case-studies/basecamp-research/" target="_blank" rel="noopener">medical and other research</a> use cases. For example, the knowledge graph could have protein and genome sequences together with environmental and chemical data, revealing intricate patterns and expanding our knowledge of proteins.</p>
<br><h2><strong>Getting Started With Knowledge Graphs</strong></h2>
<p>Knowledge graphs are organized representations of real-world entities and their relationships, overlaid with one or more organizing principles that frame the information in context to drive insight from the data. Knowledge graphs underpin insightful applications and artificial intelligence solutions across many use cases.</p>

<p><a href="https://neo4j.com/knowledge-graphs-practitioners-guide/" rel="noopener" target="_blank"><img decoding="async" src="https://dist.neo4j.com/wp-content/uploads/20240722074429/Building-Knowledge-Graphs-ebook-cover-229x300.png" alt="O’Reilly in text above the book title, which reads Building Knowledge Graphs: A Practitioner’s Guide. Image of horned goat lunging forward behind the Neo4j logo. Authors are Jesús Barrasa &amp; Jim Webber." width="150" srcset="https://dist.neo4j.com/wp-content/uploads/20240722074429/Building-Knowledge-Graphs-ebook-cover-229x300.png 229w, https://dist.neo4j.com/wp-content/uploads/20240722074429/Building-Knowledge-Graphs-ebook-cover-114x150.png 114w, https://dist.neo4j.com/wp-content/uploads/20240722074429/Building-Knowledge-Graphs-ebook-cover-600x787.png 600w, https://dist.neo4j.com/wp-content/uploads/20240722074429/Building-Knowledge-Graphs-ebook-cover.png 762w" sizes="(max-width: 229px) 100vw, 229px"></a></p><p>To master the concepts and techniques behind knowledge graphs and get hands-on experience, download a free copy of the O’Reilly book <a href="https://neo4j.com/knowledge-graphs-practitioners-guide/" target="_blank" rel="noopener">Building Knowledge Graphs: A Practitioner’s Guide</a> by Jesús Barrasa and Jim Webber.  </p>
<p>The guide covers how to build, manage, query, analyze, and visualize your knowledge graph so you can develop data-backed applications and advanced analytics.</p>

<p><strong><a href="https://neo4j.com/knowledge-graphs-practitioners-guide/" rel="noopener" target="_blank">Get My Copy</a></strong></p>

<br><h2><strong>Learning Resources</strong></h2>
<ul><ul><li><a href="https://github.com/jbarrasa/goingmeta" target="_blank" rel="noopener">Semantics workshops</a> on GitHub.</li>
<li><a href="https://www.youtube.com/watch?v=05Wkg1p34ek" target="_blank" rel="noopener">Ontology-Based Reasoning 101</a>. </li>
<li><a href="https://www.youtube.com/watch?v=05Wkg1p34ek" target="_blank" rel="noopener">Ontology-driven Knowledge Graph Construction</a>.</li>
<li><a href="https://graphacademy.neo4j.com/" target="_blank" rel="noopener">GraphAcademy</a> for knowledge graph fundamentals using a property graph model in the Neo4j Graph Database.</li></ul></ul>          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Good programmers worry about data structures and their relationships (215 pts)]]></title>
            <link>https://read.engineerscodex.com/p/good-programmers-worry-about-data</link>
            <guid>41268803</guid>
            <pubDate>Fri, 16 Aug 2024 18:09:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://read.engineerscodex.com/p/good-programmers-worry-about-data">https://read.engineerscodex.com/p/good-programmers-worry-about-data</a>, See on <a href="https://news.ycombinator.com/item?id=41268803">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><em>Engineer’s Codex is a publication about real-world software engineering.</em></p><p>I recently came across a quote by Linus Torvalds (creator of Linux and Git) that  succinctly described a problem I’ve been working on recently:</p><blockquote><p><em>"Bad programmers worry about the code. Good programmers worry about data structures and their relationships."</em></p></blockquote><p><span>Right before the above quote, Linus said:</span><br></p><blockquote><p><em>git actually has a simple design, with stable and reasonably well-documented data structures. In fact, I'm a huge proponent of designing your code around the data, rather than the other way around, and I think it's one of the reasons git has been fairly successful […] I will, in fact, claim that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important.</em></p></blockquote><p>Good data structures make code easier to design and maintain. It makes software more reliable, systems more understandable, and code more readable. When designing any software, application logic often follows the data model. Treating the data model as an afterthought results in more work down the line. The inverse is true also - having a well-thought out data model makes migrations and building on top of complex systems easier down the line.</p><p>When I read this quote, I actually was able to recognize countless examples in the past of this. I once worked on a project where we spent quite a while optimizing complex algorithms, only to realize that by restructuring our data, we could eliminate entire classes of problems. We replaced a 500-line function with a 50-line function and a well-designed data structure. Not only was the new code faster, but it was also much easier to understand and maintain.&nbsp;(Of course, then the problem also shifted “down the stack” to where the majority of toil was in restructuring existing data.)</p><p><span>Another relevant quote here is in </span><em>The Art of Unix Programming</em><span>:&nbsp;</span></p><blockquote><p>Rule of Representation: Fold knowledge into data so program logic can be stupid and robust.</p><p>Even the simplest procedural logic is hard for humans to verify, but quite complex data structures are fairly easy to model and reason about. To see this, compare the expressiveness and explanatory power of a diagram of (say) a fifty-node pointer tree with a flowchart of a fifty-line program. Or, compare an array initializer expressing a conversion table with an equivalent switch statement. The difference in transparency and clarity is dramatic. See Rob Pike's Rule 5.</p><p><strong>Data is more tractable than program logic. It follows that where you see a choice between complexity in data structures and complexity in code, choose the former. More: in evolving a design, you should actively seek ways to shift complexity from code to data.</strong></p><p>The Unix community did not originate this insight, but a lot of Unix code displays its influence. The C language's facility at manipulating pointers, in particular, has encouraged the use of dynamically-modified reference structures at all levels of coding from the kernel upward. Simple pointer chases in such structures frequently do duties that implementations in other languages would instead have to embody in more elaborate procedures.</p></blockquote><p><span>The </span><strong>actionable tip here is to start with the data. </strong><span>Try to reduce code complexity through stricter types on your interfaces or databases. Spend extra time thinking through the data structures ahead of time. </span></p><p><span>That’s not to say the </span><em>code isn’t important</em><span>. Obviously, everything is important - but it’s useful to have a strong high-level approach of how data will flow and how different components interact before going deeper into code-related details.&nbsp;</span></p><p><span>It’s why one of the </span><em>Senior Engineer (L5)</em><span> requirements (at least for FAANG) generally involves writing higher-level design docs for more complex systems (which includes driving team planning and building good roadmaps for medium-to-large features). </span></p><p><span> from </span></p><p><span> wrote a great piece on the higher level of influence L5s generally have here:</span></p><p><span> from </span></p><p><span> also wrote a great piece about getting to Senior Engineer: </span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Banks Are Now Accused of Cheating Customers Billions (147 pts)]]></title>
            <link>https://franknez.com/massive-banks-are-now-accused-of-cheating-customers-billions/</link>
            <guid>41268800</guid>
            <pubDate>Fri, 16 Aug 2024 18:08:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://franknez.com/massive-banks-are-now-accused-of-cheating-customers-billions/">https://franknez.com/massive-banks-are-now-accused-of-cheating-customers-billions/</a>, See on <a href="https://news.ycombinator.com/item?id=41268800">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
		
<p>Massive banks are now accused of cheating customers billions of dollars in interest payments according to financial reports. </p>



<p>According to a new report by <em>Financial Times</em>, several major Wall Street banks, including <strong><a href="https://franknez.com/wells-fargo-is-now-freezing-bank-accounts-in-new-scandal/">Wells Fargo</a></strong>, Morgan Stanley, and <strong><a href="https://franknez.com/bank-of-america-is-freezing-accounts-in-new-scandal/">Bank of America</a></strong>, are accused of defrauding customers out of billions of dollars in interest payments.</p>



<p>The U.S. <strong>Securities and Exchange Commission</strong> (SEC) is currently investigating these banks to determine whether they intentionally steered clients toward <em>“cash sweep”</em> accounts that provided little to no interest earnings, despite the availability of higher-yielding options.</p>



<p>This alleged practice by the banks would amount to bilking customers out of significant sums of interest income that they should have rightfully earned on their deposits and cash holdings. </p>



<p>The SEC’s probe is aimed at uncovering whether this was a deliberate strategy by the banks to boost their own profits at the expense of their clients.</p>



<p>The <a href="https://www.ft.com/content/de751907-c870-4b8c-b86b-317483f7626f" target="_blank" rel="noreferrer noopener">report</a> in the Financial Times highlights the concerning allegations of widespread misconduct by some of the largest financial institutions on Wall Street.</p>



<p>If substantiated, this could represent a major scandal involving the potential exploitation of customers through the mismanagement of their cash accounts and interest earnings.</p>



<p>The SEC’s investigation will be crucial in determining the full scope and nature of these alleged practices, as well as any potential enforcement actions or penalties that may be levied against the implicated banks.</p>



<p>The revelations have emerged from new Quarterly filings with the SEC.</p>



<p>In those filings, Wells Fargo says it’s in <em>“resolution talks”</em> with the agency over the issue, Morgan Stanley says the agency began asking questions about it in April and Bank of America confirms it’s currently being scrutinized.</p>



<p>All three banks have declined to comment on the matter.</p>



<p>Other financial firms involved in lawsuits related to cash sweep accounts include LPL Financial and Ameriprise.</p>



<p>LPL Financial says it plans to <em>“vigorously”</em> defend itself against the allegations, while Ameriprise has not released a public statement on the matter.</p>



<p>For more <strong><a href="https://franknez.com/us-bank-news-today/">U.S. Bank news</a></strong> and updates like this, opt-in for push notifications.</p>



<p><strong>Also Read:&nbsp;<a href="https://franknez.com/the-us-treasury-direct-is-now-freezing-customer-accounts/" target="_blank" rel="noreferrer noopener">The US Treasury Direct is Now Freezing Customer Accounts</a></strong></p>



<h2 id="h-other-banking-news-today"><strong>Other Banking News Today</strong></h2>



<figure><picture><source sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp" data-srcset="https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-1024x573.jpg.webp 1024w, https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-300x168.jpg.webp 300w, https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-768x430.jpg.webp 768w, https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-676x379.jpg.webp 676w, https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default.jpg.webp 1100w"><img decoding="async" width="1024" height="573" src="https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-1024x573.jpg" alt="Market News Today - Massive Banks Are Now Accused of Cheating Customers Billions." srcset="https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-1024x573.jpg 1024w, https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-300x168.jpg 300w, https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-768x430.jpg 768w, https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-676x379.jpg 676w, https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default.jpg 1100w" sizes="(max-width: 1024px) 100vw, 1024px" data-eio="l" data-old-src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAI9AQAAAACMUjdnAAAAAnRSTlMAAHaTzTgAAABeSURBVHja7cExAQAAAMKg9U9tDB+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgbCDMAAHzbk7nAAAAAElFTkSuQmCC" data-src="https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-1024x573.jpg" data-srcset="https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-1024x573.jpg 1024w, https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-300x168.jpg 300w, https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-768x430.jpg 768w, https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default-676x379.jpg 676w, https://franknez.com/wp-content/uploads/2024/07/Massive-US-Banks-Now-Prepare-For-Millions-to-Default.jpg 1100w"></picture><figcaption>Market News Today – Massive Banks Are Now Accused of Cheating Customers Billions.</figcaption></figure>



<p>Massive US banks now prepare for millions to default according to Q2 reports, as institutions increase capital to cover insolvencies.</p>



<p>Big banks such as <strong><a href="https://franknez.com/jpmorgan-is-freezing-customer-bank-accounts-in-new-scandal/">JPMorgan Chase</a></strong>, <strong><a href="https://franknez.com/bank-of-america-is-freezing-accounts-in-new-scandal/">Bank of America</a></strong> and <strong><a href="https://franknez.com/wells-fargo-is-now-freezing-bank-accounts-in-new-scandal/">Wells Fargo</a></strong> are boosting their financial defenses as they prepare for customer inflow to dwindle, affecting the ability for the average American to pay their bills.</p>



<p>According to the latest Q2 2024 financial reports from major banks, they are significantly increasing the amount of capital they are setting aside to cover potential losses from rising credit card and loan defaults.</p>



<p>Collectively, these banks are allocating billions of dollars into emergency provisions and loan loss reserves to prepare for an anticipated increase in insolvencies and non-performing loans. </p>



<p>This reflects the banks’ growing concerns about the potential for a rise in credit card delinquencies and loan defaults in the coming months.</p>



<p>By bolstering their loss-absorbing capital buffers, the banks are attempting to proactively mitigate the financial risks posed by a potential surge in credit-related delinquencies and insolvencies. </p>



<p>This suggests the banks foresee a deterioration in consumer credit quality and are taking prudent steps to strengthen their balance sheets and resilience against such adverse credit trends.</p>



<p>The significant increase in these emergency loan loss provisions across the banking sector signals that the institutions are bracing for a potential economic downturn that could lead to a rise in loan defaults and credit-related write-offs.</p>



<p>This move underscores the banks’ efforts to position themselves to better withstand any upcoming challenges in the credit markets.</p>



<p>JPMorgan Chase is leading the way, increasing its provisions from $1.88 billion in the first quarter of this year to $3.05 billion – a $1.17 billion jump.</p>



<p>Meanwhile, Bank of America has set aside $1.5 billion, up from $1.3 billion in the previous quarter, and Wells Fargo set aside $1.24 billion, up from $938 million in the previous quarter.</p>



<p>The increasing balances show banks are anticipating increasing economic risk in the months ahead as commercial real estate flounders and as consumers pile up a&nbsp;whopping&nbsp;$1.02 trillion in credit card balances, according to <a href="https://newsroom.transunion.com/ciir-q1-2024/" target="_blank" rel="noreferrer noopener">TransUnion</a>.</p>



<p>Delinquency rates across various types of debt are already on the rise, and the New York Federal Reserve&nbsp;<a href="https://www.newyorkfed.org/newsevents/news/research/2024/20240514" rel="noreferrer noopener" target="_blank">says</a>&nbsp;total US household debt hit $17.69 trillion in the first quarter of this year, an increase of $184 billion from the previous quarter.</p>



<p>The number includes mortgage balances, which rose by $190 billion to $12.44 trillion, and auto loans, which increased by $9 billion to $1.62 trillion.</p>



<p><strong>Also Read: <a href="https://franknez.com/a-massive-us-bank-is-now-closing-credit-cards/" target="_blank" rel="noreferrer noopener">A Massive US Bank is Now Closing Credit Cards</a></strong></p>



<div><p>
<h2 id="h-market-news-published-daily"><strong>Market News Published Daily</strong> 📰</h2>
</p></div>



<figure><picture><source sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp" data-srcset="https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-1024x574.jpg.webp 1024w, https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-300x168.jpg.webp 300w, https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-768x431.jpg.webp 768w, https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-676x379.jpg.webp 676w, https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez.jpg.webp 1100w"><img decoding="async" width="1024" height="574" src="https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-1024x574.jpg" alt="Market News Today - Massive Banks Are Now Accused of Cheating Customers Billions." srcset="https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-1024x574.jpg 1024w, https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-300x168.jpg 300w, https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-768x431.jpg 768w, https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-676x379.jpg 676w, https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez.jpg 1100w" sizes="(max-width: 1024px) 100vw, 1024px" data-eio="l" data-old-src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAI+AQAAAAAKxkXJAAAAAnRSTlMAAHaTzTgAAABeSURBVHja7cEBAQAAAIIg/69uSEABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMC7ASFNAAFLuj9gAAAAAElFTkSuQmCC" data-src="https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-1024x574.jpg" data-srcset="https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-1024x574.jpg 1024w, https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-300x168.jpg 300w, https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-768x431.jpg 768w, https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez-676x379.jpg 676w, https://franknez.com/wp-content/uploads/2023/08/Daily-Market-News-FrankNez.jpg 1100w"></picture><figcaption>Market News Today – Massive Banks Are Now Accused of Cheating Customers Billions.</figcaption></figure>



<p>Don’t forget to opt-in for push notifications so you don’t miss a single article!</p>



<p>Be sure to share this article with your community.</p>



<p>We are tirelessly working on providing you with the latest market news as well as local news to keep you informed about job cuts, bankruptcies, and store closures in your area.</p>



<p>Also, thank you to all of our <strong><a href="https://www.patreon.com/FrankNezMedia" target="_blank" rel="noreferrer noopener">blog sponsors</a></strong>.</p>



<p>This year we’ve been able to increase push notifications slots making it more convenient than ever for new readers to receive their daily market news and updates.</p>



<p>Our readers can now <strong><a href="https://www.patreon.com/FrankNezMedia" target="_blank" rel="noreferrer noopener">donate $3 per month</a></strong> to support independent journalism.</p>



<p>For daily news and updates on your favorite stories, opt-in for push notifications.</p>



<p><strong>Follow Frank Nez on</strong>&nbsp;<a href="https://twitter.com/FNez_Blogger" target="_blank" rel="noreferrer noopener"><strong>X (Twitter)</strong></a>,&nbsp;<a href="https://www.instagram.com/iamfranknez/" target="_blank" rel="noreferrer noopener"><strong>Instagram</strong></a>,&nbsp;or <strong><a href="https://www.facebook.com/NezMediaCompany/" target="_blank" rel="noreferrer noopener">Facebook</a></strong>. </p>







<hr>


<div>
<figure><picture><source sizes="(max-width: 512px) 100vw, 512px" type="image/webp" data-srcset="https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3.png.webp 512w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-300x300.png.webp 300w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-150x150.png.webp 150w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-270x270.png.webp 270w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-192x192.png.webp 192w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-180x180.png.webp 180w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-32x32.png.webp 32w"><img decoding="async" width="512" height="512" src="https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3.png" alt="" srcset="https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3.png 512w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-300x300.png 300w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-150x150.png 150w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-270x270.png 270w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-192x192.png 192w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-180x180.png 180w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-32x32.png 32w" sizes="(max-width: 512px) 100vw, 512px" data-eio="l" data-old-src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAAIAAQAAAADcA+lXAAAAAnRSTlMAAHaTzTgAAAA2SURBVHja7cEBAQAAAIIg/69uSEABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHwbggAAAWN1UKQAAAAASUVORK5CYII=" data-src="https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3.png" data-srcset="https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3.png 512w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-300x300.png 300w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-150x150.png 150w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-270x270.png 270w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-192x192.png 192w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-180x180.png 180w, https://franknez.com/wp-content/uploads/2023/03/cropped-Frank-Nez-Market-News-Icon-3-32x32.png 32w"></picture></figure></div>


<h2 id="h-support-independent-journalism"><strong><a href="https://www.patreon.com/FrankNezMedia" target="_blank" rel="noreferrer noopener">Support Independent Journalism</a></strong> ✍🏻</h2>



<p>Support independent journalism for just $3 per month!</p>



<p>Your contributions help power Franknez.com as the cost of widgets and online tools continue to rise.</p>



<p>Thank you for your support!</p>







<hr>



<h3 id="h-recommended-for-you"><strong>Recommended For You ✨</strong></h3>


<div><ul><li><a href="https://franknez.com/the-us-treasury-direct-is-now-freezing-customer-accounts/"><p><span>The US Treasury Direct is Now Freezing Customer Accounts</span></p></a></li><li><a href="https://franknez.com/snap-benefits-will-now-increase-for-the-year-2024/"><p><span>SNAP Benefits Will Now Increase For The Year 2024</span></p></a></li><li><a href="https://franknez.com/california-now-has-massive-departures-as-hundreds-of-thousands-leave/"><p><span>California Now Has Massive Departures As Hundreds of Thousands Leave</span></p></a></li><li><a href="https://franknez.com/a-us-bank-is-now-denying-customers-access-to-money/"><p><span>A US Bank is Now Denying Customers Access to Money</span></p></a></li><li><a href="https://franknez.com/a-giant-company-now-announces-unexpected-layoffs-in-virginia/"><p><span>A Giant Company Now Announces Unexpected Layoffs in Virginia</span></p></a></li><li><a href="https://franknez.com/a-massive-us-bank-is-now-closing-credit-cards/"><p><span>A Massive US Bank is Now Closing Credit Cards</span></p></a></li></ul></div>

<div><picture><source type="image/webp" data-srcset="https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-300x168.jpg.webp 300w, https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-1024x573.jpg.webp 1024w, https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-768x430.jpg.webp 768w, https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-676x379.jpg.webp 676w, https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees.jpg.webp 1100w"><img decoding="async" src="https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-300x168.jpg" srcset=" https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-300x168.jpg 300w, https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-1024x573.jpg 1024w, https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-768x430.jpg 768w, https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-676x379.jpg 676w, https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees.jpg 1100w " alt="Donald Trump Now Plans To End Social Security Taxes For Retirees" data-eio="l" data-old-src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAACoAQAAAAByUO6iAAAAAnRSTlMAAHaTzTgAAAAdSURBVFjD7cExAQAAAMKg9U9tCy+gAAAAAAAAeBgZmAABPsQcywAAAABJRU5ErkJggg==" data-src="https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-300x168.jpg" data-srcset="https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-300x168.jpg 300w, https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-1024x573.jpg 1024w, https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-768x430.jpg 768w, https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees-676x379.jpg 676w, https://franknez.com/wp-content/uploads/2024/08/Donald-Trump-Now-Plans-To-End-Social-Security-Taxes-For-Retirees.jpg 1100w"></picture></div>




<hr>

							
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Apple IIGS Megahertz Myth (172 pts)]]></title>
            <link>https://www.userlandia.com/home/iigs-mhz-myth</link>
            <guid>41268256</guid>
            <pubDate>Fri, 16 Aug 2024 17:08:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.userlandia.com/home/iigs-mhz-myth">https://www.userlandia.com/home/iigs-mhz-myth</a>, See on <a href="https://news.ycombinator.com/item?id=41268256">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="block-d3767bee736bb4906a02">

<p>There’s many legends in computer history. But a legend is nothing but a story. Someone tells it, someone else remembers it, and everybody passes it on. And the Apple IIGS has a legend all its own. Here, in Userlandia, we’re going to bust some megahertz myths.</p>




















  
  



</div><div data-block-type="2" id="block-yui_3_17_2_1_1722774331649_10942">
  <p><em>[A side note before proceeding… I see all you Hacker News people filing in. I haven’t had the time recently to properly format this with the numerous source links, citations, and footnotes that exist in the video version. I’ll try to get those filled in here ASAP. For now, you might want to experience it all in the video version.]</em></p><p>I love the Apple IIGS. It’s the fabulous home computer you’d have to be crazy to hate. One look at its spec sheet will tell you why. The Ensoniq synthesizer chip brings 32 voices of polyphonic power to the desktop. Apple’s Video Graphics Controller paints beautiful on-screen pictures from a palette of thousands of colors. Seven slots and seven ports provide plenty of potential for powerful peripherals. These ingredients make a great recipe for a succulent home computer. But you can’t forget the most central ingredient: the central processing unit. It’s a GTE 65SC816 clocked at 2.8 MHz—about 2.72 times faster than an Apple IIe. When the IIGS launched in September 1986 its contemporaries were systems like the Atari 1040ST, the Commodore Amiga 1000, and of course Apple’s own Macintosh Plus. These machines all sported a Motorola 68000 clocked between 7 and 8 MHz. If I know anything about which number is bigger than the other number, I’d say that Motorola’s CPU is faster.</p><p>“Now hold on there,” you say! “Megahertz is just the clock speed of the chip—it says nothing about how many instructions are actually executed during those cycles, let alone the time spent reading and writing to RAM!” And you know what, that’s true! The Apple II and Commodore 64 with their 6502 and 6510 CPUs clocked at 1 MHz could trade blows with Z80 powered computers running at three times the clock speed. And the IIGS had the 6502’s 16-bit descendant: the 65C816. Steve Wozniak thought Western Design Center had something special with that chip. <a href="https://archive.org/details/BYTE_Vol_10-01_1985-01_Through_The_Hourglass/page/166/mode/2up" target="_blank">In a famous interview in the January 1985 issue of Byte magazine</a>, Woz said, </p><blockquote><p>“[the 65816] should be available soon in an 8 MHz version that will beat the pants off the 68000 in most applications, and in graphics applications it comes pretty close.” End quote. That’s already high praise, but he continues further: “An 8 MHz 65816 is about equivalent to a 16 MHz 68000 in speed, and a 16 MHz 68000 doesn’t exist.”</p></blockquote><p>If you read this in January of 1985 you’d have little reason to doubt Woz’s words. He built the Apple I in a bedroom from a box of scraps and when given real resources followed it up with the Apple II. Even when faced with doubters, he’s got the confidence that comes from engineering the impossible. </p><blockquote><p>“Some of the Macintosh people might disagree with me, but there are ways around most of the problems they see.”</p></blockquote><p>But that “should” in “should be available” was doing a lot of work. <a href="https://mirrors.apple2.org.za/ftp.apple.asimov.net/documentation/magazines/aplus/A%2B%201986-11.pdf" target="_blank">Eighteen months later when the IIGS finally shipped, there was no 8 MHz ‘816.</a> It was as nonexistent as Woz’s imaginary 16MHz 68000. 8MHz chips were barely available three years later. What happened? Woz promised us 8 MHz of 68000-crushing glory!</p><p>If you poll a random vintage computer user they might posit the idea that Apple held the IIGS’ processor speed back during its development so it wouldn’t compete with the Macintosh. <a href="https://en.wikipedia.org/wiki/Apple_IIGS#Hardware" target="_blank">There’s an unsourced claim on Wikipedia that limiting the CPU speed to 2.8MHz was a deliberate decision</a> followed by a note—which <em>is</em> sourced, thank you very much—that the original CPU was certified to run at 4MHz. And that’s true—there’s many IIGSes that have CPUs labeled for 4MHz. This idea’s made its way through various newsgroups and webpages for decades, so it’s no surprise that it made its way into a Wiki article too.</p><p>But this theory never sat well with me. People making the claim that Apple restrained the IIGS’ CPU speed for marketing purposes rarely provide sources to back it up. I understand their logic—Apple spent the majority of its marketing and monetary might making the Macintosh the machine of the future. Because the Mac was Steve Jobs’ baby, you end up with declarations like “Steve Jobs hobbled the IIGS so it wouldn’t compete with the Mac.” It’s a common take, especially because it plays into a lot of people’s dislike of Jobs. But there’s one major problem with it: all of Steve Jobs’ power at Apple was stripped away in May of 1985 after the months of executive turmoil that led to the company's first quarterly loss. The IIGS didn't launch until 16 months later.</p><p>So why were IIGSes with chips rated at 4 MHz not running them at that speed? Why 2.8 MHz? Isn't that…  <em>weirdly</em> specific? Did an 8 MHz machine really get put on ice due to executive meddling? To solve these mysteries I descended into the depths of Usenet, usergroup newsletters, magazines, and interviews. My journey took me through a world of development Hell, problematic yields, and CPU cycle quirks. And this walk down the Apple chip road starts with the wonderful wizard named Woz.</p><h2>The Apple IIx</h2><p>It’s the summer of 1983 and Steve Wozniak has just wrapped up a two year leave of absence from Apple Computer. It all started with a six week stint in the hospital <a href="https://www.ntsb.gov/Pages/brief.aspx?ev_id=27749&amp;key=0" target="_blank">after crashing his Beechcraft Bonanza airplane on February 7, 1981</a>. <a href="http://ns1.woz.com/letters/memory-loss/" target="_blank">Amnesia from a hard concussion short-circuited Wozniak’s short-term memory</a>. After his recovery he took a leave from Apple <a href="https://engineering.berkeley.edu/steve-wozniak-inventor-and-apple-co-founder/" target="_blank">to go back to UC Berkeley and finish his degree in computer science and electrical engineering</a>… and run a few rock festivals. By June of 1983 Woz felt he was ready to return to work, and asked Dave Paterson, the head of Apple II engineering, for a job—but this time, in the trenches.</p><p>His timing was excellent. Even though the Lisa was taking headlines and the Macintosh was shaking up R&amp;D, the Apple II was making all the money. Brisk sales of the IIe along with the imminent launch of the IIc meant the Apple II division was busier than ever even if they weren’t getting all the glory. And while Steve Jobs was heralding the Mac as the Next Big Thing, designing a next-generation Apple II as a contingency plan was just good business.</p><p>At the heart of this proposed machine was a brand new CPU. Bill Mensch’s Western Design Center was busy developing the 65816, a 16-bit update to the venerable 6502 architecture. This chip would bring 16-bit computing to the Apple II while promising backwards compatibility. Users wouldn’t lose their investment in applications, add-in cards, or accessories. Alongside the new CPU was a special coprocessor slot that allowed the user to install a board with a 68000 or 8088. <a href="https://archive.org/details/The_Apple_IIgs_Book_Jeanne_DuPrau/page/n17/mode/2up" target="_blank">The goal was to build a bridge between the 8- and 16-bit world, so the project had code names like Brooklyn and Golden Gate</a>. </p><p>This project would be known publicly as the IIx thanks to Wozniak discussing it on CompuServe or at user groups. But as late ’83 rolled into early ’84 the IIx project stumbled over multiple roadblocks. The coprocessor slot added layers of complexity that distracted from the mission of architecting a new Apple II. But a major complication was the 65816 itself. Apple expected engineering samples in November 1983, but didn’t actually get them until February 1984. What’s worse, those late chips were buggy, unstable, and ultimately unusable. WDC delivered a second batch of chips a few weeks later, but they were no more reliable than the first.</p><p>Even if Apple abandoned the coprocessor slot, the project couldn’t go forward without a CPU, and Apple cancelled the IIx project in March of 1984. Now before you aim your ire at Steve Jobs, <a href="https://mirrors.apple2.org.za/ftp.apple.asimov.net/documentation/magazines/aplus/" target="_blank">A+ Magazine, in their IIGS development history</a>, says it was the team leads who suggested canning the troubled machine. With no managerial appetite for a next-generation machine, the Apple II team pivoted from a moonshot to something more achievable. Dan Hillman and Jay Rickard started a project to consolidate most of the discrete chips of an Apple II into a single chip called the Mega II. When they finished the design six months later they weren’t quite sure what to do with it. Would they reduce the cost of the IIe or reduce the size of the IIc?</p><p>Imagine their surprise when Woz suggested a second shot at a 16-bit Apple II. The conditions seemed right to give it another go. Less expensive 16-bit computers like the Atari ST were looming on the horizon and the Mac’s hot start was slowing down. By October 1984 Apple finally had a good supply of working 65816 CPUs to design a system. And the Mega II would free up a lot of board space to add new graphics and sound chips. But just as important as chips and specs was a new, focused mission statement. This computer would be an Apple II by Apple II fans for Apple II fans. Woz, Hillman, Rickard, Harvey Leitman, and Lee Collings spent the rest of 1984 hammering out the specifications and solving hard problems like how to speed up memory access without breaking existing software.</p><p>Now we’re finally back to that Woz interview I quoted earlier. Byte published it in two parts across the December ’84 and January ’85 issues, and based on the average time to press I reckon it took place in October 1984. By this point the IIx is dead and buried and he’s talking about the new 16-bit Apple II, now codenamed Phoenix. His excitement about an 8MHz ‘816 is palpable, but, again, Woz was careful to say it “<em>should</em> be available soon.” Woz left Apple in February 1985 when the ink for this interview was barely dry. He had a dramatic fight with John Sculley after the Apple II was snubbed at the annual Apple shareholder’s meeting in January 1985. Apple II sales supplied seventy percent of Apple’s revenue in 1984 and Woz’s Apple II compatriots felt they weren’t getting their due. Steve Jobs may not have dialed back the IIGS’ clock speed, but he <em>did</em> shovel endless piles of money towards his pet projects at the expense of the Apple II. Even if Woz had stuck around the 8 MHz ‘816 of his dreams was years away. The IIGS wouldn’t sniff 8 MHz until Applied Engineering released the 7 MHz TransWarp GS accelerator four years later in 1989.</p><h2>The Need for Speed</h2><p><a href="https://www.applefritter.com/content/apple-iigs-bad-onboard-ram" target="_blank">If you go looking online for photos of Apple IIGS logic boards</a>, there’s a decent chance you’ll see a 4MHz GTE G65SC816 CPU. Most IIGSes had 4 or 3 MHz CPUs running at 2.8 MHz regardless of the chip’s rated speed. Why?</p><p>First, we must understand where that clock signal comes from. The IIGS, like many computers of its era, derives its base clock from a crystal oscillator. The one in the IIGS vibrates 28.636 million times per second, or megahertz. The VGC divides that 28.636 megahertz in half, and 14.318 MHz is then supplied along with a stretch signal to other parts of the system. I bet you've already noticed that these frequencies are multiples of the NTSC colorburst frequency of 3.58MHz. Keep that in mind, it’ll be relevant later.</p><p>This 14.318 MHz clock travels to a special ASIC which—depending on your board revision—is called the Fast Processor Interface or Control Your Apple chip. One of its responsibilities is dynamically controlling the CPU’s clock speed. The IIGS normally runs in Fast mode at 2.8 MHz, but the user can switch to Normal mode in the Control Panel which reduces the speed to 1.023 MHz. It’s like the turbo switch on PCs, except controlled by the FPI. This lets the IIGS run speed-sensitive software at the same speed of an original Apple II. But even in fast mode there are times the CPU needs to slow down to access other parts of the system.</p><p>The CPU, ROM, and Fast RAM are on the 2.8 MHz, or fast side, while the Mega II chip, slots, sound, video, and so on are on the 1MHz, or slow side. When the CPU is running in fast mode and needs to talk to something on the slow side the FPI dynamically throttles the clock signal to a 1 MHz cycle time to let the CPU synchronize with the Mega II side. This is usually invisible to the user because the system still executes the majority of its cycles at the faster speed, but it means the CPU is not always executing as fast as it could. I haven’t even touched on the eight percent performance penalty from the cycles the FPI spends refreshing RAM.</p><p>There’s nothing inherent to this design that limits the system to 2.8 MHz. The <a href="https://archive.org/details/Apple_IIgs_Hardware_Reference_HiRes/page/n47/mode/2up" target="_blank">Apple IIGS Hardware Reference</a>, <a href="http://www.brutaldeluxe.fr/documentation/cortland/v5/Cortland%20Custom%20ICs%20-%20Wayne%20Lowry%20-%20Preliminary%20notes%20-%2019860214.pdf" target="_blank">Wayne Lowry’s Cortland Custom IC Notes</a>, and <a href="https://www.kansasfest.org/2012/04/2011-krue-fpi/" target="_blank">Daniel Kruszyna’s KansasFest 2011</a> presentation about the FPI lay it out clearly that the IIGS’s fast mode could support higher speeds. In principle a redesigned FPI or CYA could use a divider other than 1/5 for the clock signal. A 1/4 divider of 14.318 MHz yields a 3.58 MHz clock speed, which should be well within the capabilities of a 4 MHz chip. And once again, that “should” is doing a lot of work. So why didn’t it run at that speed?</p><h2>The Birth of the 65C816</h2><p>The 65C816 and IIGS are inextricably linked, and the ‘816’s story starts in 1981 when GTE’s Desmond Sheahan approached Bill Mensch and Western Design Center about designing a CPU. GTE wanted to manufacture their own chips for their key phone systems, so they licensed both a CMOS manufacturing process and a 6802 CPU design from Mitel. But Mitel’s CPU design team wasn’t up to the task, so GTE asked Mensch to step in. Mensch offered two options: a two-chip 6802 solution or a 6502-based microcontroller. Either one could be manufactured on the CMOS process GTE licensed from Mitel. Sheahan convinced the GTE brass that the one-chip solution was the way to go, and the 65C02 was born.</p><p>GTE became the first licensee of the 65C02 thanks to WDC designing their 65SC150 telephony microcontroller. Eventually WDC would license the core to other companies, like NCR, Synertek, and Rockwell. The result was WDC netting a royalty every time one of these licensees sold a standalone CPU or microcontroller with its 65C02 core. Functional 65C02 silicon was available in 1982, and the revenues from licensing deals were finally filling up WDC’s coffers. This is when—<a href="https://www.computerhistory.org/collections/catalog/102739969" target="_blank">according to Mensch’s Computer History Museum oral history and other sources</a>—Apple approached him about a 16-bit successor to the 6502.</p><p>The prospect of a big player like Apple being the launch client for a new CPU was hard to resist. Further information on the early days of the ‘816 is fairly elusive, but looking at both historic and contemporary interviews with Mensch and Woz reveals Apple’s influence on the chip’s design. One influence was compatibility. When designing the ‘816 <a href="https://mirrors.apple2.org.za/apple2.caltech.edu/miscinfo/65xxx.chronology" target="_blank">Mensch improved the 6502 bus architecture</a> to eliminate little weirdnesses like false read cycles on store accumulators. This consequently broke Steve Wozniak’s Disk II controller, which had been designed to rely specifically on that exact weirdness.</p><p>Now there’s two ways to solve this problem: redesign the floppy controller, or add the weirdnesses back in. Apple tried the first one; redesigning its floppy controller into a single chip called the Integrated Woz Machine. This chip was made independently for the Apple IIc to control its built-in floppy drive. <a href="http://www.brutaldeluxe.fr/documentation/iwm/iwm_discussion_19820531" target="_blank">Among its many improvements was eliminating the false read cycle requirement</a>. The Apple IIx could have used an IWM, but the UniDisk drives that took advantage of it wouldn’t be out until 1985. Therefore existing Disk II interfaces and drives still had to work with the IIx. If you were to claim Apple II compatibility, but not be able to work with one of the most popular boards, well, there’d be protests lining the sidewalks of De Anza Boulevard. Other cards might also depend on specific 6502 timing behaviors. <a href="https://www.commodore.ca/commodore-history/bill-mench-the-brains-behind-the-brains/" target="_blank">Mensch eventually honored Apple’s request that the 65C816 be fully compatible with the NMOS 6502’s timings</a>.</p><p>Apple wouldn’t be Mensch’s only licensee for the ‘816—the core can be found in countless embedded systems and microcontrollers. Licensees could extend the core to customize it into a microcontroller or a system-on-chip, or add features specific to their needs. A great example is Ricoh, who licensed the ‘816 core and added several functions, like dedicated multiply and divide units along with special DMA functions. All these add-ons were at the request of a pretty famous customer: Nintendo. The result was <a href="https://sneslab.net/wiki/65c816" target="_blank">the Ricoh 5A22, the CPU inside the Super NES</a>.</p><p>One of the famous tales of the ‘816’s development is how it was laid out without computer aided design. As told by Bill Mensch, he designed the ‘816 on a card table at WDC’s humble headquarters in Mesa, Arizona. His sister Katherine did the layout on mylar sheets. Many semiconductor designers had moved on to computer aided design tools to help design and lay out their circuits. Mensch is rightly proud of this feat, but the decision to lay it out by hand with mylar and rubylith wasn’t without consequences.</p><p>Let’s circle back to that interview with Woz. <a href="https://archive.org/details/gteg65sc816datasheetocrbm/mode/2up" target="_blank">GTE published data sheets in 1985 for the G65SC816</a> which detailed 2, 4, 6, and 8 MHz variants of the chip. Apple, as the prime customer, would’ve had these data sheets far in advance. This would be consistent with Woz’s belief that faster variants were on the way, but for purposes of designing the IIGS they had to settle for 4MHz chips. Multiple photographs of IIGS prototype logic boards with 4MHz chips are on the web, and 4MHz parts shipped in production IIGS computers. But the promise of a faster CPU is very tantalizing to a computer designer, and I’m sure it wasn’t just Woz who was excited about future faster CPUs.</p><p>But when would GTE actually produce those faster variants? That’s the question. One source is <a href="http://commodore.ca/commodore-history/bill-mench-the-brains-behind-the-brains/" target="_blank">a 1986 interview with Mensch published in the Fall/Winter issue of COMPUTE!’s Apple Applications magazine</a>. This interview took place before the IIGS announcement, likely sometime in the summer of ’86. Mensch states their average part on a 3 micron process should be 4MHz, and an upcoming 2.4 micron process would yield 6MHz parts. I’d wager that the 8 MHz parts would rely on a 2 micron process at that reduction rate. Some 6MHz parts did exist, and Bill Mensch’s Cortland-badged prototype IIGS has one. A photo of it was shown at his 2021 VCF East panel, and I will note that it’s a WDC branded chip bearing a date code of the first week of February 1987. Whether Mensch put this chip in there himself or it was a sample installed by an Apple engineer is not explained. Nor is it known if its FPI chip actually drives it at a higher speed. But this shows that faster ‘816s did exist. So what was getting in the way of these faster chips actually being used?</p><h2>Yields, Bugs, and Errata</h2><p>This brings us to the 65816’s next quandary: yields. This might be a surprise to you, given that the CPU is found in embedded devices worldwide. But a common thread through many of the reports I’ve read about the early days of the ‘816 is that WDC and its fabrication partners struggled to deliver 4MHz and faster chips on time, in volume, and at spec.</p><p>Mensch positioned the 4MHz chip as the ‘816’s volume product and said as much in that earlier interview with COMPUTE!. </p><blockquote><p>“Our typical product is 4MHz. We sell 2MHz into the retrofit&nbsp;market, but our typical run-of-the-mill is 4MHz.”</p></blockquote><p>But in reality the IIGS shipped with a mixture of 3 and 4 MHz parts which actually ran at 2.8 MHz in fast mode. Which brings us back to the question of why a machine designed around a 4MHz part would ship with an almost 30% haircut in clock speed. Conspiracy theories aside, could there be a <em>technical</em> reason why the IIGS would run slower than its CPU’s rated speed?</p><p>In the same Fall/Winter 1986 issue of COMPUTE!’s Apple Applications where Mensch talked about future plans for the ‘816, David Thornburg interviewed Steve Wozniak about his role in developing the IIGS. The subject of the 65816 came up and Woz delved into some details about its clock speed.</p><blockquote><p>“Our early ideas for the computer had it running around 8MHz. Soon we found we had to back off to about 5.5MHz, and then to 2MHz for that version of the processor. In the end the product came out around 3MHz, which is a good compromise.”</p></blockquote><p> This is consistent with his comments about the desire for 8MHz more than a year earlier in the Byte interview. Woz doesn’t mention what factors made them back off on the clock speed, but during my research I learned a lot about the troubles the ‘816 faced in terms of scaling and yields.</p><p>One problem was GTE’s ability to supply chips—not just to Apple, but to other customers. The IIGS would be shipping by the tens of thousands when it launched, and this necessitated a good quantity of chips on hand. Dave Haynie—yes, the Amiga’s Dave Haynie—had trouble in 1985 sourcing 4 MHz 65816s for a potential Commodore 128 successor. <a href="https://groups.google.com/g/comp.sys.apple/c/RH1-BNz8x-c/m/385dAOKgaA4J" target="_blank">He posted about this experience on Usenet in March of 1990</a>. </p><blockquote><p>“At that time, they had fully specced 8MHz parts, yet in ’85, GTE (the only company actually MAKING 65816s) had all they could do to make enough 4MHz parts. Rumor is that Apple managed get enough for the IIGS by actually having a special 2.8MHz version tested.”</p></blockquote><p>He further elaborates with:</p><blockquote><p>“When the GS came out, the only company making '816s was GTE. The main reason I couldn't get any 4MHz ‘816s in quantity was that Apple bought them all. They could make a real deal on 2MHz parts, since the yield on 4MHz was so low, they had more of those than they knew what to do with.” </p></blockquote><p>Haynie <a href="https://groups.google.com/g/net.micro.apple/c/mPV8ErhJUJQ/m/AyRGXOAvqLAJ" target="_blank">also comments in other posts</a> about how early samples of the ‘816 were delivered at 500KHz—yes, <em>kilo</em>hertz—and maybe that’s a clue as to why Apple was unhappy in the Apple IIx timeframe.</p><p>Yields are a common woe in semiconductor manufacturing and Haynie’s comments about yields line up with what we see in the real world. GTE’s three micron process apparently had problems producing enough 4 MHz chips in volumes to satisfy its customers. Many IIGSes have a 3 MHz G65SC816 despite this speed rating not showing up in GTE’s data sheets. My guess—I can’t find any confirmation for this, but it's what makes the most sense—is that these were chips that couldn't be binned as 4 MHz, so GTE labeled them as 3MHz. Insisting on 4MHz would have meant shipping fewer computers, and the IIGS was delayed enough as it is. While VLSI supplied some 4MHz 65C816 CPUs later in the IIGS’ life, the vast majority of CPUs found in these computers were made by GTE—or, eventually, by California Micro Devices, <a href="https://techmonitor.ai/technology/california_micro_devices_completes_acquisition_of_gtes_microcircuits_division" target="_blank">which purchased GTE’s Microcircuits division in September 1987</a> after GTE decided to get out of the business. Apple was also negotiating with NCR as a second source, but according to Mensch and others, the deal fell apart before the IIGS shipped.</p><p>But let’s say for the sake of argument that GTE was able to produce as many 4 MHz chips as Apple or anyone else wanted to buy. Based on the FPI’s clock divider mechanism and a 14.318 MHz source clock, Apple had a logical clock speed target of 3.58 MHz using a 1/4 divider. That’d still be a compromise over Woz’s dream machine, but it’d be faster than what we got. And if (or when) faster processors became available, the FPI clock divider could be adjusted for them.</p><p>Yet those faster processors weren’t materializing; at least, not in any volume. Yields were a factor, yes, but the faster speeds revealed other problems. Trying to learn more about this took me down a rabbit hole of Usenet posts, Applied Engineering documentation, AppleFest event reports, and 6502 development forums. All of them pointed to a common factor: the REP and SEP opcodes. When designing the new 16-bit native mode for the ‘816, Bill Mensch added many new opcodes to enable new features and capabilities for programmers. Two of these new opcodes—<a href="https://undisbeliever.net/snesdev/65816-opcodes.html#sep-set-status-bits" target="_blank">called SEP for Set Status Bits and REP for Reset Status Bits</a>—control flag bits for the processor’s status registers. These are crucial to the dual 8/16 bit nature of the ‘816 and how it can switch between 8 and 16 bit operations on the fly.</p><p>Unfortunately these opcodes proved problematic at higher speeds. <a href="https://groups.google.com/g/comp.sys.apple/c/og6tWx8NeMw/m/5dspToQ6g4IJ" target="_blank">Multiple posts</a> relay statements from WDC or programming guides that <a href="http://forum.6502.org/viewtopic.php?f=4&amp;t=5196" target="_blank">timing problems with the layout mask</a> prevented these instructions from completing in their allowed cycle times. These problems only got worse as they tried to shrink the mask down to smaller process nodes. I’m loath to take what amounts to second and sometimes even third-hand accounts from newsgroups and forums at face value—they don't call it the Net of a Million Lies for nothing, after all. But I’m willing to believe the overall theory based on other corroborating evidence (like this <a href="http://forum.6502.org/viewtopic.php?p=95998#p95998" target="_blank">WDC data sheet</a> note from 1991). If you look at an Apple IIGS accelerator like the TransWarp GS or the ZipGSX, you’ll notice that they’re not just a CPU and some cache memory. The TransWarp GS has a bunch of support chips and gate array logic, while the ZipGSX has a full-blown ASIC on board.</p><p>The GALs for the TransWarp GS were reverse engineered long ago, and <a href="https://wiki.reactivemicro.com/TransWarp_GS#What_roles_the_GALs_Perform" target="_blank">Reactive Micro lays it out plainly</a>: GAL3 handles opcode detection and speed control. This matches up with posts <a href="https://groups.google.com/g/comp.sys.apple/c/og6tWx8NeMw/m/5dspToQ6g4IJ" target="_blank">relaying comments from Applied Engineering about stretching clocks</a> to handle problematic REP and SEP opcode timings.</p><p>Analyzing these posts also reveals the temperature of the Apple II community circa 1990. Apple announced a revised IIGS at San Francisco just before AppleFest in September 1989, and the reaction from attendees was muted at best. Unfortunately there was no CPU speed increase, but base memory was increased to 1MB and the new ROM added more toolbox routines and some feature enhancements. There was someone whose reaction was anything but muted, though, and it was one William David Mensch.</p><p><a href="https://groups.google.com/g/comp.sys.apple/c/zL2kjN9ponI/m/wFcQDnYlcVoJ" target="_blank">According to multiple accounts of the event</a>, during his keynote address Jean-Louis Gassée said that there would be no speed increase for the revised IIGS because of difficulties securing faster 65816s. Mensch was in attendance and was understandably displeased with this statement. He approached one of the crowd mics and said that he was the designer of the CPU and had in his hand a bag of 12 MHz ‘816s. He proclaimed that if Apple ordered the 12 MHz chips, he would ship them. Jean-Louis reportedly made a comment about wanting a reliable production run, and the two men got into a heated back-and-forth before Gassée left and Mensch was escorted out. No recordings of this keynote exist, or if they do they’re locked away in the Apple Archives at Stanford University.</p><p>The version of the story Mensch recounts <a href="https://www.youtube.com/watch?v=hHqAQTl0s8A" target="_blank">at his 2021 panel at VCF East</a> largely follows what I’ve read in contemporary reports, except with one difference. He includes an anecdote about how he got Jean-Louis’ Apple coffee cup. He mentions running into Gassée after the keynote, and says that Gassée was very upset and threatened to, quote, “kick [Mensch’s] butt.” No punches were actually thrown, and no butts were actually kicked, and the story peters out without really explaining how Mensch got the coffee cup, but this story shows just how bad Apple and WDC's relationship had become.</p><p>Now, you’re a smart viewer, so I bet you know how chickens and eggs work. A company like Apple doesn’t just buy bags of chips; they buy Frito-Lay’s entire annual output. Mensch could have samples of silicon, but WDC wasn’t the one making the chips in volume; its licensees like GTE were. If the fab (say, GTE/CMD or VLSI) can’t guarantee a production run of, say, 50,000 chips, the order doesn’t happen. The evidence in front of us—the delays during IIx development, the inability to deliver 4MHz parts at volume, and the opcode issues that prevented scaling—would certainly justify skepticism of WDC’s ability to work with a fab to deliver faster chips at volume. There were still possibilities for a faster IIGS, though, and these would play right into an Apple II fan’s belief that Apple held it back.</p><h2>Accelerators, ASIC Incorporated, and Mark Twain</h2><p>But let’s say you weren’t buying tens of thousands of chips like Apple was; maybe you only needed a thousand or two. Were smaller customers being taken care of? <a href="https://www.wap.org/journal/showcase/journal198911.html" target="_blank">Ray Settle of the Washington Apple Pi Journal</a> was also at the fall 1989 AppleFest, where he reported on the confrontation between Mensch and Gassée. Afterwards, he mentioned a visit with an Applied Engineering representative. Settle still hadn’t received his TransWarp GS, and the AE rep pinned the blame on unreliable 7 MHz CPUs. <a href="https://groups.google.com/g/comp.sys.apple/c/zL2kjN9ponI/m/wFcQDnYlcVoJ" target="_blank">Another attendee report posted on Usenet by Michael Steele</a> featured similar comments. Keep in mind that the TransWarp was first announced in 1988, didn’t ship until the fall of 1989, and struggled with speed and supply restrictions through 1990. This is further supported by <a href="https://archive.org/details/A2_Central_1991-02_February_1991_Vol_7_No_1/mode/2up?q=65816" target="_blank">A2-Central’s accelerator reviews in February 1991</a>, where it’s mentioned that AE resorted to offering 6.25 MHz versions of the accelerator because of supply issues—and <em>reliability</em> issues—with 7 MHz chips. Zip Technologies also took a while to ship their 7MHz ZipGSX accelerator, which finally appeared in October 1990, almost a year after the TransWarp GS.</p><p>But we don’t have to settle for second-hand reports. Enter Applied Engineering employee John Stephen III. <a href="https://groups.google.com/g/comp.sys.apple/c/rhRHiWlRp28/m/LluioQmvv9kJ" target="_blank">In an October 1989 Usenet post</a>&nbsp; he mentions the problems with getting 7MHz chips, and alludes to the REP/SEP timing issues. But the other interesting thing he mentions is that getting high speed 10 MHz ‘816s to run at their rated speeds required boosting their input voltage well beyond the standard 5 volts. This made the chips run hotter and often resulted in crashes. And I see all you overclockers in the audience nodding your heads.</p><p>Scaling problems aren’t unusual when you move to a smaller process node, and sometime a layout needs fixes—or even a complete redesign. The original mask laid out by Katherine Mensch on the WDC card table had reached its limits. Redoing the mask wouldn’t be easy or cheap, especially when higher speed ‘816s were a smaller piece of the revenue pie. Royalties from the 65C02, 65802, and slower embedded ‘816s were paying the bills. Mensch was also busy working on a 32-bit iteration of the architecture: the 65832. But this proposal never made the jump from datasheet to shipping dock.</p><p>Interestingly, this is where a new player steps in. While researching the yield problems <a href="https://groups.google.com/g/comp.sys.apple2/c/0OZKowbQQls/m/4JoV91YX8pEJ" target="_blank">I encountered numerous posts on comp.sys.apple2 about an “ASIC 65816.”</a> This tripped me up at first, because there are numerous application-specific integrated circuits that contain an ‘816 core. But no, it turns out that a company <em>named</em> ASIC was creating a new 65816 processor using gate arrays. And they promised that this redesigned ‘816 would reach speeds of 20MHz and beyond.</p><p>Imagine my surprise <a href="https://groups.google.com/g/comp.sys.apple/c/SVtbqi_gIEc/m/2AqcgWtHaEkJ" target="_blank">when I saw the name Anthony Fadell</a> mentioned in these posts. Could that be the same Anthony Fadell—<em>Tony </em>Fadell—who was in charge of the iPod? Sure enough, I found an excerpt of Tony’s book, <a href="https://www.buildc.com/the-book" target="_blank"><em>Build</em></a><em>,</em> where he talks about designing a 65816 for his startup company, ASIC Incorporated! Now we’re on to something. This gave me enough clues to dig up the <a href="https://babel.hathitrust.org/cgi/pt?id=mdp.39015025959696&amp;seq=1" target="_blank">November/December 1989 issue of <em>The Michigan Alumnus</em> magazine</a>, where an article tells the tale of the fateful summer of 1989. Fadell was an undergrad student at the University of Michigan, and he spent his summer internship designing nuclear control computers at Ronan Engineering. When he met William Hayes the two hit it off immediately. Fadell was a big Apple II fanboy and yearned for more power. Hayes knew chip design and had connections with other chip designers. The two decided that they would design a better version of the 65816 using a sea-of-gates array. Fadell borrowed $5,000 from his uncle, Hayes leveraged his connections to get cut-rate pricing on supercomputer time, and the two reverse engineered a 65816 design in six weeks. After finding a fab willing to manufacture their design, Fadell was prepared to pitch his prototype to potential patrons.</p><p>The <em>Michigan Alumnus</em> article is coy about who the chip was for, but it mentions Fadell flying to Texas in September 1989 to meet with a manufacturer of Apple IIGS accelerator boards. There he negotiated a purchase order and a two year contract, catapulting him to the role of CPU vendor at the age of twenty. With these clues we can deduce that Fadell’s customer was Applied Engineering. If all had gone according to plan, ASIC's chips should have started production in early 1990, and formed the basis of the next generation of TransWarp accelerators. There’s that word again—<em>should.</em></p><p>Ultimately, no TransWarp GS or ZipGSX accelerators ever shipped with ASIC’s design. The chips did exist—multiple trip reports from 1990 AppleFests mention Fadell demonstrating his CPUs in TransWarp accelerators. And in 2022, Fadell <a href="https://twitter.com/tfadell/status/1524145271797551104" target="_blank">posted a photo on Twitter of an ASIC 65816</a> which he claims would still work today—I’m guessing this is one of the 17 MHz chips used in the AppleFest demos. But posts about ASIC fizzled out after the spring of 1991, which coincides with Fadell graduating from Michigan. The ultimate fate of the chip isn’t really known—did Fadell face legal challenges from WDC? Did Applied Engineering give up on him? Or was it because—<a href="http://wired.com/story/tony-fadell-revenge-on-silicon-valley-from-paris/" target="_blank">as described in an interview with WIRED</a>—he was just too busy roaming the halls of General Magic trying to score a job with former Apple superstars Andy Hertzfeld and Bill Atkinson? My guess is the latter, since General Magic hired him later that year.</p><p>In the same tweet as the photo of the chip, Fadell said that he, quote: “sold some to Apple for a new Apple IIGS revision before they canceled it!” Many IIGS superfans have heard tell of the <a href="https://68kmla.org/bb/index.php?threads/apple-iigs-rom-04-%E2%80%9Cmark-twain%E2%80%9D.29362/" target="_blank">cancelled final revision of the IIGS code named Mark Twain</a>. Featuring an internal floppy drive, a hard disk, and a redesigned logic board the Mark Twain is what many thought the ROM 03 IIGS should have been. It’s entirely probable that Apple fitted some prototypes with faster CPUs. <a href="https://www.wannop.info/SSII//SSII_Index_files/SSII_PDF_files/Volume%203%20Issue%201.pdf" target="_blank">But when media outlets like InCider/A+ magazine were given a top secret demo</a> a mere month before its cancellation, the clock speed was still the same. And the few Mark Twain prototypes that escaped Apple’s development dungeons were equipped with the usual 4MHz chip. This is where the business side and the technical side butt heads.</p><p>The Mark Twain was under development in late 1990 into 1991 and then mysteriously no longer under development as of June 1991. Rumors of Apple killing the entire Apple II line hung over the product like a dark cloud, and the dev team had hoped to prove they were greatly exaggerated. Despite John Sculley’s statements promising Apple’s full support for the over five million strong installed base, the lack of marketing and technical improvements to the Apple II over the years meant his words rang hollow. Apple had just introduced the Macintosh LC as their low-cost color Mac, and IBM compatible PCs were getting cheaper and more capable. If Apple released the Mark Twain without any CPU speed boosts, it would’ve appealed mostly to the Apple II’s cost-conscious institutional education market. Would existing IIGS owners buy one instead of just getting an accelerator card for a fraction of the price? And how many new users would it bring to the platform? The Mark Twain would’ve been like the Amiga 1200: a decent improvement, but ultimately too little and too late. The March 1991 release of the Apple IIe card for the Mac LC also put another nail in Mark Twain’s coffin, because many users—especially educators—bought a IIGS for backwards compatibility with classic Apple II software. If you’re the number cruncher who had to choose between spinning up a run of 50,000 Mark Twains that cost a lot more to build than 50,000 IIe cards for Mac LCs that are already in production, already in a warehouse, or already sold to customers, which would you pick?</p><p>Now this is where you can rightfully criticize Apple for holding back the Apple IIGS. A Mark Twain with even a 12MHz CPU from ASIC would’ve been a legitimately powerful, well equipped computer for its price. But that would’ve been one last campaign in a war long since lost. Maybe ASIC could have helped, but the good ship Apple was sailing straight into a storm called the beleaguered era. Costs, staff, and projects were starting to spiral out of control, and the Mark Twain would’ve only delayed the inevitable death of the Apple II.</p><h2>Sanyo, Nintendo, and the End of Acceleration</h2><p>Even if Apple didn’t see fit to release a faster IIGS, accelerator cards kept the enthusiast market alive for a few more years. <a href="https://archive.org/details/NAUG-V06N03" target="_blank">Upgrade guides for the TransWarp</a> gave tips on how to install higher-rated ‘816s to get even more speed. This usually required buying samples of higher speed processors from WDC, changing crystals, upgrading the cache, and acquiring new GALs. Once you hot-rodded your card you’d often need to supply more juice over the 5V rail to keep things stable.</p><p>All this hackery was finally put to bed in 1992 when new 65C816s rated at 14 MHz hit the market. <a href="https://groups.google.com/g/comp.sys.apple2/c/eoCOwP_-eWs/m/ExeSe7cR6IYJ" target="_blank">These chips took Usenet posters by surprise</a>, especially after the ASIC saga. Fabbed by Sanyo, the 14 MHz models could run cool and stable at 5V and apparently solved the issues with the REP and SEP opcodes. Sanyo achieved this by abandoning the die laid out by Katherine Mensch and creating a new layout from scratch. Why Sanyo chose to invest in this is unclear—<a href="https://groups.google.com/g/comp.sys.apple2/c/yVx5TMBjAGQ/m/32BgXxPXpgkJ" target="_blank">I found a lot of speculation</a> that they wanted to <a href="https://www.historyofinformation.com/detail.php?id=5188" target="_blank">build a PDA based on the ‘816</a>. Sanyo’s a giant manufacturer, so I’m sure they found a use for it. Maybe WDC felt the heat from ASIC, or maybe they saw ARM and 68K pivoting to the embedded market and finally bit the bullet to stay competitive.</p><p>Existing accelerators could be upgraded by changing out their CPU and clock crystal, but by this point the IIGS was but a fraction of the ‘816s out in the wild. Optimistic estimates of the number of IIGSes shipped hover around 1.25 million. The other ‘816 system that most people know—the Super NES—sold 1.3 million units in 1990 just in Japan according to numbers compiled by NeoGAF user Aquamarine, who claims to have gotten them directly from Nintendo's Kyoto offices. The system sold nearly 50 million units worldwide in its lifetime. Mensch is very proud of the SNES using the 65816 and speaks highly of working with Ricoh, the manufacturer of Nintendo’s chips. Seeing as the 5A22 was a custom job by Ricoh, I wonder if they fixed the SEP and REP problem during its design. It wouldn’t be beyond them; <a href="https://www.linkedin.com/pulse/reverse-engineering-patent-protection-cautionary-tale-harry-strange/" target="_blank">they did tweak the 6502 to dodge patents</a> when designing the NES’ 2A03. I haven’t seen the SNES assembler community complain about issues with REP and SEP with the basic 3.56 MHz part, but that doesn’t mean problems don’t exist. Same with emulation and FPGA groups, though I’d still defer to experts in that field. </p><p>And like the IIGS, the SNES could take advantage of accelerators thanks to add-on processors in cartridges. The most well known is the SuperFX, but Ricoh also made a faster 65816. Better known as the Super Accelerator-1, the Ricoh 5A123 runs at a 10.74 MHz clock speed—three times faster than a 5A22. You’ll find it in fan-favorites like <em>Super Mario RPG</em> and <em>Kirby Super Star.</em> SA-1 games started shipping in 1995, which is late in the Super NES’ lifecycle. Did Ricoh license the redesigned core from Western Design, or did they make the changes themselves? I’d love to know the answer.</p><h2>ARM, Möbius, and The Road Not Taken</h2><p>Apple discontinued the IIGS in December 1992. Even though the IIe hung on for another year or so, the platform truly died that day. Even in the timeline where WDC had 8MHz chips ready in 1983, and Apple put the GUI on the IIx in 1984, I still think the Apple II as we knew it would’ve died eventually. There’s several limitations that would necessitate an eventual migration to a new architecture.</p><p>The primary roadblock is the Mega II side of the machine. This is supposed to be what makes it an Apple II, but it drags the rest of the machine down with it. Depending on the video system for clock generation and timing was a practical engineering choice that became hard technical debt for almost all 1980s computer architectures, especially with ones that traced their roots to the 1970s. The GS is an excellent example of trying to maintain compatibility while adding capability, but it had an obvious ceiling.</p><p>But something that gets lost in the 65816 versus 68000 arguments is that CPU architectures have qualities beyond raw speed. You might care about power consumption, memory mapping, or ease of acquisition. And all these factors are a balancing act depending on your application. The 68K’s massive flat memory space was a big point in its favor, as well as its native support for user and supervisor separation. These don’t matter as much to someone who’s writing hand-crafted single-tasking assembly language apps, but they very much matter to someone building a multitasking UNIX workstation.</p><p>And that’s not to say that 68K isn’t good for assembly applications. It’s got a great reputation among assembly programmers. But as the world turned to compilers and development environments the 68K architecture was primed for the rising popularity of languages like C. More people were getting into programming, and it’s unrealistic to expect them all to have the potential to become machine language maestros like Rebecca Heineman or Nasir Gebelli. C compilers exist for 6502 and 65816, but it's fair to say that these architectures weren't particularly suited to the intricacies of C.</p><p>Another sticking point for high-speed 65816s is the speed of memory. Assuming you threw the entire Mega II side of the IIGS away and built a new ‘816 computer from scratch, a 14 MHz example would need very fast main memory to operate without a cache. In the early 90s, that kind of memory was barely available. How Woz would have built his 8 MHz IIGS in 1985 while affording a decent amount of memory is a rather inconvenient question.</p><p>Apple wasn’t the only company facing the limitations of their early designs. Commodore and Atari decided to pivot to the 68000, like Apple did with the Macintosh. Tech debt eventually caught up with them too, especially Commodore, but the problems weren’t unsolvable—the Amiga tried to migrate to PowerPC, after all. Even the Macintosh and IBM PCs had eventual reckonings with fundamental planks of their platforms. Another company was facing the same conundrum: Acorn Computers. The similarities to Apple are there: they were dependent on a 6502-based architecture and had a crucial education market with a significant installed base. Acorn did ship a computer with the 65816—the Acorn Communicator—but <a href="https://www.computerhistory.org/collections/catalog/102746190" target="_blank">when Sophie Wilson visited WDC in 1983</a> and saw Mensch and crew laying out the 65816 by hand, it struck her: if this motley crew on a card table could design a CPU, so could Acorn. Thus Wilson and Steve Furber forged their own CPU: the Acorn RISC Machine.</p><p>Though today’s Arm and WDC sell very different CPUs, they do have something in common: they’re fabless semiconductor designers that license their cores and architectures to other companies. Apple is of course Arm’s most famous partner: <a href="https://www.latimes.com/archives/la-xpm-1990-11-28-fi-4993-story.html" target="_blank">they joined Acorn and VLSI to form Advanced RISC Machines in 1990</a> to create the ARM610 CPU to power the Newton. But what you might not know is that Apple’s involvement with ARM originates with a desire to replace the CPU in the Apple II. <a href="https://tompittard.com/media" target="_blank">The little known Möbius project helmed by Paul Gavarini and Tom Pittard in the Advanced Technology Group</a> was an ARM2-based computer that could emulate 6502, 65816, and 68000 code. Gavarini and Pittard started the project in 1986 and were demoing and compiling benchmarks in 1987—right on the heels of Acorn releasing the Archimedes! There’s little information about this on the web, with <a href="https://web.archive.org/web/20131208171649/http://www.tompittard.com/page4.html" target="_blank">Tom Pittard’s bio</a> and <a href="https://web.archive.org/web/20050208095510/http://www.advanced-risc.com/art1stor.htm" target="_blank">Art Sobel’s ARM pages</a> being some of the surviving hints to its existence.</p><p>Based on my knowledge of how ARM2 works, I believe the emulation performance of Möbius is wholly derived from the ARM2’s memory system. ARM’s designers were inspired by the 6502’s efficient memory access and optimized the 8 MHz ARM2 to wring as much performance out of the available memory as possible. <a href="https://media.ccc.de/v/36c3-10703-the_ultimate_acorn_archimedes_talk#t=1635" target="_blank">By using 4 MHz 32-bit-wide fast page memory, pipelining, and special load-store instructions, ARM2 could perform burst transactions at twice the speed of random ones</a>. With a theoretical maximum of 32 MB/s bandwidth in fast page mode, this was eight times the maximum bandwidth of an 8 MHz 68K shackled to 2 MHz DRAM. This strategy would peter out eventually because memory speed couldn’t keep up with CPU speed, but hey—that’s what cache is for!</p><p>I’m not sure if Möbius would have been the Apple II’s savior—Acorn’s Archimedes wasn’t 100% backwards compatible despite including a BBC Basic interpreter and eventually a BBC Micro emulator. But with EcoNet network adapters and expansion podules to connect old Beeb peripherals the Arc could ingratiate itself with Acorn’s existing installed base. Could the Apple II have been reborn with an ARM CPU? Maybe. Nobody mentions how well Möbius integrated the rest of the Apple II architecture like slots or video or sound. And say what you will about Apple’s troubles with Western Design Center; ARM was barely a blip on the radar in 1988. Apple wasn’t going to upturn their cart for an unproven architecture from a competitor. Möbius was a skunkworks project; it could’ve taken years to turn its demo into a shipping product and the 68040 was already on the drawing board in 1988. But it was still worth it: Möbius’ benchmarks convinced Larry Tesler that ARM could save the Newton from the disastrous AT&amp;T Hobbit processor. And hey—without ARM, Apple wouldn’t have the iPod, iPhone, and Apple Silicon today. So it worked out in the end.</p><h2>The End of an Odyssey</h2><p>What a ride, huh? Thanks for making it this far down a fifty-plus minute rabbit hole. I can’t claim that this is the final take on the subject—so many of the players aren’t on the record, but I’m pretty confident in saying that Apple did not artificially limit the IIGS’ clock speed during its development for marketing purposes. Now, I’m not a fool—I know Apple didn’t push the IIGS as hard as it could, and it was very much neglected towards the end of its run. If the REP/SEP flaws hadn’t existed and GTE could’ve shipped stable 4 MHz chips in volume, I’m sure Apple would’ve clocked them as fast as possible in 1986.</p><p>I’ll admit that I initially started this deep dive out of spite. The idea that “Steve Jobs deliberately held the IIGS back, <em>bleh bleh”</em> is everywhere, but it's all just people saying things in an endless game of telephone, with no actual evidence. That’s enough to grind anybody’s gears, but what’s worse are people who I respect uncritically repeating these things in videos and blog posts. It hurts me to see videos with millions of views repeating old Internet urban legends pushed by partisans filtered through Wikipedia articles with bad citations.</p><p>But that spite subsided quickly once I started untangling the messy web of people and circumstances that wrapped around this story. I realized that what I wanted wasn’t to prove anybody wrong. What I wanted was to get people to think about these stories and why they became legends. Of course, you could flip that right back around at me and say “who made <em>you</em> the guardian of love and justice?” And that’s a fair point. But my goal here isn’t to push an agenda, but to get a better of understanding of how things happened and why history went the way that it did. I’ve provided my evidence, and it’s up to you to judge if my argument is compelling enough.</p><p>And even then, one of those people who needed a refresher on computer legends was yours truly. I’ve done my share of repeating things based on bad sources, just because I had a fanboy desire to defend something. Or because I just thought a story was neat and didn’t look too deeply into it. It’s not so much about somebody who’s being wrong on the internet; it was the realization that <em>I</em> could be somebody who’s being wrong on the internet! It was humbling, really. As the years go on I’ve realized that there’s so much out there to learn, even if I thought I already was an expert. A perfect example is that <a href="https://www.pagetable.com/?p=43" target="_blank">Bill Gates BASIC easter egg</a>, where I got tripped up by an oft-repeated legend until I actually dug into it. And as vintage and retro tech enthusiasts, are we not people of science? Are not our minds open to new ideas? We’re into this because we enjoy the history and personal connections, and we should all be excited about digging deep and not just repeating the same old story.</p><p>Even though the IIGS may not have been able to unleash its full potential, it’s still an amazing machine even at its base speed. If you haven’t had the chance to play with one, give it a try. And turn on accelerator mode in your emulator to get a feel for what could have been.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FUTO (166 pts)]]></title>
            <link>https://www.futo.org/</link>
            <guid>41268245</guid>
            <pubDate>Fri, 16 Aug 2024 17:07:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.futo.org/">https://www.futo.org/</a>, See on <a href="https://news.ycombinator.com/item?id=41268245">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>GrayJay</p>
                    <p>A universal video app for following creators, not platforms</p>
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Does Reasoning Emerge? Probabilities of Causation in Large Language Models (152 pts)]]></title>
            <link>https://arxiv.org/abs/2408.08210</link>
            <guid>41267746</guid>
            <pubDate>Fri, 16 Aug 2024 16:19:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2408.08210">https://arxiv.org/abs/2408.08210</a>, See on <a href="https://news.ycombinator.com/item?id=41267746">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2408.08210">View PDF</a>
    <a href="https://arxiv.org/html/2408.08210v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Recent advances in AI have been significantly driven by the capabilities of large language models (LLMs) to solve complex problems in ways that resemble human thinking. However, there is an ongoing debate about the extent to which LLMs are capable of actual reasoning. Central to this debate are two key probabilistic concepts that are essential for connecting causes to their effects: the probability of necessity (PN) and the probability of sufficiency (PS). This paper introduces a framework that is both theoretical and practical, aimed at assessing how effectively LLMs are able to replicate real-world reasoning mechanisms using these probabilistic measures. By viewing LLMs as abstract machines that process information through a natural language interface, we examine the conditions under which it is possible to compute suitable approximations of PN and PS. Our research marks an important step towards gaining a deeper understanding of when LLMs are capable of reasoning, as illustrated by a series of math examples.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Javier Gonzalez [<a href="https://arxiv.org/show-email/450e0580/2408.08210">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 15 Aug 2024 15:19:11 UTC (8,071 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Discrete Mathematics – An Open Introduction 4ed (By Oscar Levin) (331 pts)]]></title>
            <link>https://discrete.openmathbooks.org/dmoi4.html</link>
            <guid>41267478</guid>
            <pubDate>Fri, 16 Aug 2024 15:53:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discrete.openmathbooks.org/dmoi4.html">https://discrete.openmathbooks.org/dmoi4.html</a>, See on <a href="https://news.ycombinator.com/item?id=41267478">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<h3>4th Edition Information</h3>
<p>
This page details progress on the 4th edition of the book. The <a href="https://discrete.openmathbooks.org/dmoi3.html">3rd edition</a> remains available.
</p>
</div>
<p>
After many years of development, I am please to announce that the 4th edition of <em>Discrete Mathematics: an Open Introduction</em> is <strong>now available</strong>, here and on <a href="https://runestone.academy/">Runestone Academy</a>.
</p>
<p>
The new edition brings many improvements and a new organization of content. In particular, the book now starts with logic and proofs, then practices those proofs with graph theory. The second half of the book contains material on counting (with a new "application to probability" section) and sequences. Over the last few years, I have found that students have more success with this arrangement. There is also a stronger emphasis on discrte structures, which should make the book more useful for students in computer science, while still focusing on understanding mathematical concepts essential for math majors and future math teachers.
</p>
<p>
There is more interactivity as well. More interactive exercises (which you can give students credit for if you create a course with the book on Runestone Academy; it is completely free for you and your students) and some interactive Sage and Python code to explore some topics.
</p>
<p>
A PDF of the book will be made available by August 15th. There will not be a print edition available until next year, when CRC Press will release it. The online version will remain available and free forever, and the book is still released under a Creative Commons License (but note the new NC-Non Commercial addition to the license).
</p>






































<h3>License</h3>
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><a property="dct:title" rel="cc:attributionURL" href="https://discrete.openmathbooks.org/dmoi4.html">Discrete Mathematics: an Open Introduction, 4th edition</a> by <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://math.oscarlevin.com/">Oscar Levin</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer">CC BY-NC-SA 4.0<img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""><img src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>
<p> You are free to download, use, and print as you wish to, for noncommercial purposes. You can also modify the text as much as you like (create a custom edition for your students, for example), as long as you attribute the parts of the text you use to the author and release your modified version under a compatible license.</p>
<p>If you are interested in using parts of the book combined with another text with a similar but different license (GFDL, for example), please <a href="https://discrete.openmathbooks.org/cdn-cgi/l/email-protection#ed829e8e8c9fc381889b8483ad98838e82c3888998">reach out</a> to get permission to modify the license.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Texas "moth man" photographed 550 species in his own yard (204 pts)]]></title>
            <link>https://www.texasmonthly.com/travel/curtis-eckerman-photographer-moth-man/</link>
            <guid>41267045</guid>
            <pubDate>Fri, 16 Aug 2024 15:04:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.texasmonthly.com/travel/curtis-eckerman-photographer-moth-man/">https://www.texasmonthly.com/travel/curtis-eckerman-photographer-moth-man/</a>, See on <a href="https://news.ycombinator.com/item?id=41267045">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>On a steamy June evening, Curtis Eckerman embarks on a mothing expedition in the Bauerle Ranch greenbelt, in far South Austin. Towing a wagon full of supplies, he follows a narrow trail that leads between mesquite trees and into a secluded oak grove suffused with golden late-afternoon light. Eckerman, the chair of the biology department at Austin Community College, parks the wagon and begins to wrap a tree trunk in white cloth. Next he suspends a battery-powered ultraviolet light from a low branch. He’s optimistic we’ll see lots of different moths tonight; it’s been a warm, humid day, conducive to plant growth and, by extension, activity by plant-eating creatures. The oak grove is full of frostweed, persimmon trees, and various grasses, each vital to different moth species. That’s another good sign: a wide variety of plants will draw a variety of moths.</p><p>When he finishes his preparations, it’s about eight o’clock. Moths emerge to eat, mate, and lay eggs once it’s completely dark—and fellow moth-ers will arrive any moment now. Eckerman mops his brow and takes a swig from his water bottle. “Now we just wait.”</p><p>Eckerman is a herpetologist by trade; he says the best job he ever had was catching endangered water snakes in West Texas as an undergraduate research assistant. But he’s always collected insects, and a little over a decade ago, he began photographing them. When he realized he struggled to identify the moths in his pictures simply because there were so many varieties, he dedicated a summer to studying them. These days, he sets a light by his garage door and photographs whatever moths show up, as many as seventy species and thousands of individuals in a single night. He’s now counted 550 species at his own home.&nbsp;</p><p>Eckerman is an ecologist with an interest in biodiversity, as well as in species’ natural histories, behaviors, life cycles, and places in the food chain. Moths are so diverse, he says, that it’s not uncommon to find one that hasn’t been named or described in the scientific literature. No entomologist studies “moths”; out of necessity, they specialize. Travis County has about 1,400 recorded species, the state of Texas more than 4,000. And those are just the ones we know about. Insects are the most diverse group of animals on the planet, and there are likely more undiscovered insect species than known ones—as many as 10 to 20 million species worldwide still left to discover, Rice University biologist Scott Egan told <em>Texas Monthly</em> <a href="https://www.texasmonthly.com/travel/tiger-beetle-rice-university-houstoniana-discovery-new-species/">earlier this year.</a></p><p>The most common type of moths in the U.S. are&nbsp;noctuids, the smaller, grayish-brown moths that congregate around porch lights—“Your stereotypical moth,” Eckerman says. Showier <a href="https://www.texasmonthly.com/travel/hummingbird-moth-texas-yards/">hawk moths</a> are active during the day and often confused with hummingbirds. Silk moths, such as the luna moth, enormous and apple green, don’t have functional mouth parts; their one-to-two-week adult lives are powered by whatever fat they’ve stored as caterpillars. Micromoths—a broad descriptor that can include noctuids and moths in other families—have wingspans under twenty millimeters long, but are worth viewing through a magnifying glass or camera to admire their vivid colors and patterns. They’re one of Eckerman’s favorite groups to photograph because of the “hidden-gem effect,” he says. “It’s the idea that you get to see something that somebody normally doesn’t see.&nbsp;.&nbsp;.&nbsp;. You feel like you’re exploring a world that others are not exploring, even though it’s around them all the time.”</p><figure><img fetchpriority="high" decoding="async" width="2400" height="1500" alt="Clockwise from top left: Silk Moth/Polyphemus Moth, Hawk Moth/White-lined Sphinx Moth, Micromoth/Leaf Miner Moth, Southern Emerald, Noctuid/Wedgeling Moth" sizes="(max-width: 2400px) 100vw, 2400px" data-src="https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-1.jpg?auto=compress&amp;crop=faces&amp;fit=fit&amp;fm=pjpg&amp;ixlib=php-3.3.1&amp;q=45" data-srcset="https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-1.jpg?auto=compress&amp;crop=faces&amp;fit=scale&amp;fm=pjpg&amp;h=188&amp;ixlib=php-3.3.1&amp;q=45&amp;w=300&amp;wpsize=medium 300w, https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-1.jpg?auto=compress&amp;crop=faces&amp;fit=scale&amp;fm=pjpg&amp;h=640&amp;ixlib=php-3.3.1&amp;q=45&amp;w=1024&amp;wpsize=large 1024w, https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-1.jpg?auto=compress&amp;crop=faces&amp;fit=scale&amp;fm=pjpg&amp;h=480&amp;ixlib=php-3.3.1&amp;q=45&amp;w=768&amp;wpsize=medium_large 768w, https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-1.jpg?auto=compress&amp;crop=faces&amp;fit=scale&amp;fm=pjpg&amp;h=960&amp;ixlib=php-3.3.1&amp;q=45&amp;w=1536&amp;wpsize=1536x1536 1536w, https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-1.jpg?auto=compress&amp;crop=faces&amp;fit=scale&amp;fm=pjpg&amp;h=1280&amp;ixlib=php-3.3.1&amp;q=45&amp;w=2048&amp;wpsize=2048x2048 2048w, https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-1.jpg?auto=compress&amp;crop=faces&amp;fit=scale&amp;fm=pjpg&amp;h=31&amp;ixlib=php-3.3.1&amp;q=45&amp;w=50&amp;wpsize=concierge-thumb 50w, https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-1.jpg?auto=compress&amp;crop=faces&amp;fit=fit&amp;fm=pjpg&amp;ixlib=php-3.3.1&amp;q=45 2400w" data-sizes="auto" src="https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-1.jpg?auto=compress&amp;crop=faces&amp;fit=fit&amp;fm=pjpg&amp;ixlib=php-3.3.1&amp;q=45"><figcaption><span>Clockwise from top left: silk moth, hawk moth, micromoth, southern emerald, and wedgling moth. </span><span>Texas Monthly; Moth Photographs by Curtis Eckerman</span></figcaption></figure><p>Much of Eckerman’s moth self-education comes from the participatory science app iNaturalist. Users take pictures of plants, insects, and other animals they spot in their local areas and upload them with comments (“Anyone know what kind of owl this is?”). Fellow iNaturalists compare notes and help one another identify mystery species. The app also suggests possible identifications with an impressive degree of accuracy. Users can note the location where they saw a given plant or animal, creating a dataset that has helped researchers understand the ranges of particular species. Virtual communities form quickly and move into real life as users go mothing, birding, or herping together. &nbsp;</p><p>Eckerman posted his first moth images to iNaturalist ten years ago, and other users coached him on how to take better photos to make identification easier. He started teaching the students in his Structure and Function of Organisms course at ACC to use iNaturalist to log their own observations. Eckerman has tracked his former students’ engagement with the app via their usernames and found that three years after taking his class, 30 percent are still active.</p><p>Too many Texans are disengaged from the natural wonders around them, he argues, but iNaturalist can help them tune in. “The average student today could tell you all sorts of things about the plains of the Serengeti and interactions between zebra and lions, because National Geographic and Discovery and Disney brought it to us in high definition,” Eckerman says. “But they know far less about what’s in their own backyard than the previous generation, because they’re just not interacting with it.” Over the years, he’s stopped talking about faraway exotic animals in his courses; he instead makes lessons relevant by focusing on local fauna students have actually seen: coyotes, gray foxes, grackles, moths.</p><p>To help his students apply their iNaturalist skills in the field, he organizes mothing expeditions to Pease Park, in Central Austin, and Roy G. Guerrero park, in East Austin. At the latter site, the classes have logged about 350 moth species, including an enormous black witch, longer than a chalkboard eraser and sometimes confused for a bat—it’s the largest noctuid in the continental U.S. Roy G. Guerrero is a much larger park than Pease and more buffered from human activity that would interfere with moths. Still, the Pease Park surveys have turned up more than two hundred species—proving, Eckerman says, that urban parks are important not just as sites of human recreation but also for supporting species diversity. In the spring, Eckerman invited the public to go mothing at the park, part of his ongoing effort to educate the general population about biodiversity and conservation. For impromptu events like the June expedition on the greenbelt, he maintains a “moth-lovers email list” and lets word spread on iNaturalist.</p><p>An hour after sunset, the afternoon buzz of cicadas has given way to the rhythmic rattle of <a href="https://www.texasmonthly.com/travel/welcome-to-hot-katydid-summer/">katydids</a>. A half dozen moth lovers gather in the oak grove and peer at the moths flittering against Eckerman’s sheet, including the <a href="https://www.inaturalist.org/taxa/224855-Melipotis-indomita" target="_blank">indomitable melipotis</a>, a large brown triangle with a cream-colored V across its wings, and the smaller filbertworm moth, its wings a mottled red with bands of gold.</p><p>Reid Hardin, a biology student at Texas State University, studies the moths before disappearing into the shadows to look for scorpions. Hardin’s into moths—he sometimes sets up a light in his own backyard—but he also scouts out rare plants and various arachnids. iNaturalist has connected him with locals who share his curiosity. Tonight, he and Caleb Helsel, a Westlake High School student who’s into birds, insects, and arachnids, are comparing notes on the pseudoscorpions they’ve turned up at Austin greenbelts. “It’s just always cool to see who else is interested in niche, nerdy stuff,” Hardin says.</p><figure><img decoding="async" width="2400" height="1500" alt="" sizes="(max-width: 2400px) 100vw, 2400px" data-src="https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-3.jpg?auto=compress&amp;crop=faces&amp;fit=fit&amp;fm=pjpg&amp;ixlib=php-3.3.1&amp;q=45" data-srcset="https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-3.jpg?auto=compress&amp;crop=faces&amp;fit=scale&amp;fm=pjpg&amp;h=188&amp;ixlib=php-3.3.1&amp;q=45&amp;w=300&amp;wpsize=medium 300w, https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-3.jpg?auto=compress&amp;crop=faces&amp;fit=scale&amp;fm=pjpg&amp;h=640&amp;ixlib=php-3.3.1&amp;q=45&amp;w=1024&amp;wpsize=large 1024w, https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-3.jpg?auto=compress&amp;crop=faces&amp;fit=scale&amp;fm=pjpg&amp;h=480&amp;ixlib=php-3.3.1&amp;q=45&amp;w=768&amp;wpsize=medium_large 768w, https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-3.jpg?auto=compress&amp;crop=faces&amp;fit=scale&amp;fm=pjpg&amp;h=960&amp;ixlib=php-3.3.1&amp;q=45&amp;w=1536&amp;wpsize=1536x1536 1536w, https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-3.jpg?auto=compress&amp;crop=faces&amp;fit=scale&amp;fm=pjpg&amp;h=1280&amp;ixlib=php-3.3.1&amp;q=45&amp;w=2048&amp;wpsize=2048x2048 2048w, https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-3.jpg?auto=compress&amp;crop=faces&amp;fit=scale&amp;fm=pjpg&amp;h=31&amp;ixlib=php-3.3.1&amp;q=45&amp;w=50&amp;wpsize=concierge-thumb 50w, https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-3.jpg?auto=compress&amp;crop=faces&amp;fit=fit&amp;fm=pjpg&amp;ixlib=php-3.3.1&amp;q=45 2400w" data-sizes="auto" src="https://img.texasmonthly.com/2024/08/curtis-eckerman-texas-moths-photos-3.jpg?auto=compress&amp;crop=faces&amp;fit=fit&amp;fm=pjpg&amp;ixlib=php-3.3.1&amp;q=45"><figcaption><span>Clockwise from top left: filbertworm moth, common buckeye butterfly, eight-spot moth, indomitable melipotis, sculptured moth, and plume moth.</span><span>Texas Monthly; Moth Photographs by Curtis Eckerman</span></figcaption></figure><p>Moths aren’t just pretty to look at; they’re also important pollinators and a major source of food for birds and bats. But moths, like insects overall, are declining in both raw numbers and species diversity. Eckerman says recent studies have shown at least a 30 percent decline in insect abundance around the world over the past few decades. The likely culprits are pesticides and the loss of habitat to urban development. A threat to insects such as moths is a threat to the plants they pollinate and all the creatures above them in the food chain.</p><p>Beyond eschewing pesticides, Texans can create more moth-friendly environments by using a variety of native plants in their yards to offer food. Reducing light pollution helps too; moths are disoriented by artificial light, which can impair reproduction. If moths lay their eggs under the light instead of on a plant that is their food source, the resulting larvae won’t have anything to eat. Turning off lights at night (Eckerman always disassembles his setup at the end of the evening) helps moths find their way to the right plant.</p><p>By 10 p.m., Eckerman’s sheet is littered with diminutive flying insects: micromoths, small beetles, caddis flies, and plant hoppers. He leans in to photograph a tiny moth, using a camera with a macro lens that lets him take clear images at a short distance. To the naked eye, the moth—smaller than a grain of rice—is plain white. But when Eckerman enlarges the image on the viewfinder, he reveals a pattern of buckwheat-colored flecks on cream-hued wings. Eckerman can’t remember its name—he just knows it’s in the <em>Gracillariidae</em> family—but iNaturalist will fill in the details.</p><p>Overall, he says, it’s been a good night for the really small moths, which makes him happy. “Despite there not being all that glamorous moths, this is actually my favorite kind of sheet, with all the tiny things,” he says. “I love seeing the little jewels that you can’t normally see.”</p>
<div data-txmo-ab-name="topics-eoa">
<ul>

<li><a href="https://www.texasmonthly.com/category/critters/" data-click-location="EOA Topics List" data-click-action="Topic Click" data-click-label="Critters">Critters</a></li>
<li><a href="https://www.texasmonthly.com/location/austin/" data-click-location="EOA Topics List" data-click-action="Topic Click" data-click-label="Austin">Austin</a></li>
</ul>
</div>

</div></div>]]></description>
        </item>
    </channel>
</rss>