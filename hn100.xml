<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 31 May 2025 10:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[AI Responses May Include Mistakes (155 pts)]]></title>
            <link>https://www.os2museum.com/wp/ai-responses-may-include-mistakes/</link>
            <guid>44142113</guid>
            <pubDate>Sat, 31 May 2025 05:48:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.os2museum.com/wp/ai-responses-may-include-mistakes/">https://www.os2museum.com/wp/ai-responses-may-include-mistakes/</a>, See on <a href="https://news.ycombinator.com/item?id=44142113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
<p>The other day I wanted to look up a specific IBM PS/2 model, a circa 1992 PS/2 Server system. So I punched the model into Google, and got this:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop.png"><img decoding="async" width="640" height="487" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-640x487.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-640x487.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-300x228.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-768x584.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop.png 1147w" sizes="(max-width: 640px) 100vw, 640px"></a></figure></div>


<p>That did not look quite right, since the machine I was looking for had 486 processors (yes, plural). And it most certainly <em>did</em> use Microchannel (MCA).</p>



<p>Alright, let’s try again:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop.png"><img decoding="async" width="640" height="557" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-640x557.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-640x557.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-300x261.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-768x669.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop.png 1174w" sizes="(max-width: 640px) 100vw, 640px"></a></figure></div>


<p>Simply re-running the identical query produces a different summary. Although the AI still claims that PS/2 Model 280 is an ISA-based 286 system. Maybe the third time is the charm?</p>



<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop.png"><img loading="lazy" decoding="async" width="640" height="465" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-640x465.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-640x465.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-300x218.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-768x557.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop.png 1160w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>The AI is really quite certain that PS/2 Model 280 was a 286-based system released in 1987, and I was really looking for a newer machine. Interestingly, the first time the AI claimed Model 280 had 1MB RAM expandable to 6MB, and now it supposedly only has 640 KB RAM. But the AI seems sure that Model 280 had a 1.44 MB drive and VGA graphics.</p>



<p>What if we try again? After a couple of attempts, yet different answer pops up:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop.png"><img loading="lazy" decoding="async" width="640" height="431" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-640x431.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-640x431.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-300x202.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-768x518.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop.png 1138w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>Oh look, now the PS/2 Model 280 is a 286 expandable to 128 MB RAM. Amazing! Never mind that the 286 was architecturally limited to 16 MB.</p>



<p>Even better, the AI now tells us that “PS/2 Model 280 was a significant step forward in IBM’s personal computer line, and it helped to establish the PS/2 as a popular and reliable platform.”</p>



<p>The only problem with all that? <em>There is no PS/2 Model 280, and never was.</em> I simply had the model number wrong. The Google AI just “helpfully” hallucinates something that at first glance seems quite plausible, but is in fact utter nonsense.</p>



<p>But wait, that’s not the end of the story. If you try repeating the query often enough, you might get this:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop.png"><img loading="lazy" decoding="async" width="640" height="409" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-640x409.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-640x409.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-300x192.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-768x491.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop.png 1160w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>That answer is <em>actually correct</em>! “Model 280 was not a specific model in the PS/2 series”, and there was in fact an error in the query.</p>



<p>Here’s another example of a correct answer:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop.png"><img loading="lazy" decoding="async" width="640" height="415" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-640x415.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-640x415.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-300x194.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-768x498.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop.png 1174w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>Unfortunately the correct answer comes up maybe 10% of the time when repeating the query, if at all. In the vast majority of attempts, the AI simply makes stuff up. I do not consider made up, hallucinated answers useful, in fact they are worse than useless. </p>



<p>This minor misadventure might provide a good window into AI-powered Internet search. To a non-expert, the made up answers will seem highly convincing, because there is a lot of detail and overall the answer does not look like junk.</p>



<p>An expert will immediately notice discrepancies in the hallucinated answers, and will follow for example the <a href="https://en.wikipedia.org/wiki/List_of_IBM_PS/2_models">List of IBM PS/2 Models</a> article on Wikipedia. Which will very quickly establish that there is no Model 280.</p>



<p>The (non-expert) users who would most benefit from an AI search summary will be the ones most likely misled by it.</p>



<p>How much would you value a research assistant who gives you a different answer every time you ask, and although sometimes the answer may be correct, the incorrect answers look, if anything, more “real” than the correct ones?</p>



<p>When Google says “AI responses may include mistakes”, do not take it lightly. The AI generated summary could be utter nonsense, and just because it sounds convincing doesn’t mean it has anything to do with reality. Caveat emptor!</p>
											</div><div><p>
							This entry was posted in <a href="https://www.os2museum.com/wp/category/ibm/" rel="category tag">IBM</a>, <a href="https://www.os2museum.com/wp/category/ps2/" rel="category tag">PS/2</a>. Bookmark the <a href="https://www.os2museum.com/wp/ai-responses-may-include-mistakes/" title="Permalink to AI Responses May Include Mistakes" rel="bookmark">permalink</a>.													</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Valkey Turns One: Community fork of Redis (187 pts)]]></title>
            <link>https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/</link>
            <guid>44140379</guid>
            <pubDate>Fri, 30 May 2025 22:24:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/">https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/</a>, See on <a href="https://news.ycombinator.com/item?id=44140379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  
  <article>
    <div>
        
<p>A year ago, Redis Inc (formerly Garantia Data) made a controversial move that disrupted the open source ecosystem: it closed the source of Redis.&nbsp;<a href="https://www.gomomento.com/blog/rip-redis-how-garantia-data-pulled-off-the-biggest-heist-in-open-source-history/" target="_blank" rel="noreferrer noopener">As I wrote at the time</a>, it was a trust-breaking decision that could have shattered the community.</p>



<p>But instead of splintering, the community responded with purpose. Out of that disruption came&nbsp;<a href="https://valkey.io/" target="_blank" rel="noreferrer noopener">Valkey</a>,&nbsp;a fork that took a shot at keeping the community alive.</p>



<h2>A Return, A Reversal</h2>



<p>As part of efforts to rebuild trust with the community, Redis Inc&nbsp;<a href="https://redis.io/blog/welcome-back-to-redis-antirez/" target="_blank" rel="noreferrer noopener">brought back</a>&nbsp;Salvatore Sanfilippo (aka Antirez), the original creator of Redis. I am genuinely excited about his return because it is already impactful. He’s following through on his promise of contributing new features and performance optimizations to Redis. More profoundly,&nbsp;<a href="https://redis.io/blog/agplv3/" target="_blank" rel="noreferrer noopener">Redis 8.0 has been open-sourced again</a>.</p>



<p>Redis acknowledged that adopting <a href="https://en.wikipedia.org/wiki/Server_Side_Public_License" target="_blank" rel="noreferrer noopener">SSPL</a> strained their bond with the community, questioning contributions from others in the same breath.</p>



<blockquote>
<figure><blockquote><p>How do you keep innovating and investing in OSS projects when cloud providers reap the profits and control the infrastructure without proportional contributions back to the projects that they exploit?</p></blockquote></figure>
</blockquote>



<p>The disheartening move from Redis Inc catalyzed an unprecedented wave of collaboration and contributions. Valkey became the test of the community resolve to keep itself together. One year later, Valkey hasn’t just survived –&nbsp;<a href="https://www.linkedin.com/posts/kshams_valkey-rocks-the-most-remarkable-thing-about-activity-7318683448506793985-_qJE" target="_blank" rel="noreferrer noopener">it’s thriving</a>! The Async I/O Threading model <a href="https://github.com/valkey-io/valkey/pull/758" target="_blank" rel="noreferrer noopener">contribution</a> from AWS unlocked 3x+ throughput by fundamentally changing how I/O threads work inside Redis.</p>



<p>But how do these and other contributions compare to Redis 8? Can we hit 1M RPS out of an 8 VCPU instance (c8g.2xl) on either Valkey 8.1 or Redis 8.0 (with 1KB items, 3M items in the key space, and ~500 connections)? It’s time for a bake off!</p>



<h2>Valkey 8.1 vs Redis 8.0: Can the Fork Outrun the Source?</h2>



<p>The punchline: we could not sustain 1M RPS on an 8 VCPU instance with either Valkey or Redis 8.0, but we got really close!!</p>



<p>On a full-tuned c8g.2xl (8 VCPU), Valkey 8.1.1 pushed to 999.8K RPS on SETs with .8ms p99 latency. Redis 8.0 got as high as 729.4 RPS on SETs with .99ms p99 latencies. On each iteration, we tested 50M SETs followed by 50M GETs. We varied connection counts to optimize the maximum throughput for each system.</p>



<figure><blockquote><p><mark>Valkey achieved higher throughput and lower latency across both reads and writes (37% higher on SET and 16% higher on GET), alongside 30% faster p99 latencies for SET and 60%+ faster on GET.</mark></p></blockquote></figure>



<figure><img loading="lazy" decoding="async" width="1024" height="512" src="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20512'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1.png 1200w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png"></figure>



<figure><img loading="lazy" decoding="async" width="1024" height="341" src="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png" alt="Valkey vs Redis table with SET and GET command" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-300x100.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-768x256.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-18x6.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20341'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-300x100.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-768x256.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-18x6.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2.png 1200w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png"></figure>



<h2>Threading the multi-threading needle</h2>



<p>If I had a penny for every time heard, “but Redis /Valkey is single threaded….”</p>



<p>Antirez’s emphasis on a shared nothing architecture has been foundational for Redis. Nevertheless, as early as 2020, Redis added support for I/O threads. Unfortunately, they did not offer drastic improvement until recently. If you have previously tried and discarded I/O threads, it is time to evaluate again!</p>



<p>On Valkey, we see SET throughput going from 239K RPS without I/O threads to 678K with 6 threads. Meanwhile, p99 latencies dropped from 1.68ms to 0.93ms <strong>despite doing nearly 3x the throughput</strong>! Similarly, Redis went from 235K RPS without I/O threads to 563K RPS with 6 I/O threads. P99s for Redis also dropped around 40% from 1.35ms to 0.84ms.</p>



<p>Two key takeaways emerged:</p>



<ol>
<li>With two threads, gains were modest (~20%). The impact only really surfaced at three threads and beyond.</li>



<li>Redis and Valkey were neck-and-neck until the fourth thread. After that, Valkey pulled away sharply.</li>
</ol>



<figure><img loading="lazy" decoding="async" width="1024" height="512" src="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20512'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2.png 1200w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png"></figure>



<h3>SET Performance on Valkey with IO/Threads &amp; 256 Connections:</h3>



<figure><img loading="lazy" decoding="async" width="700" height="500" src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-18x12.png 18w" sizes="(max-width: 700px) 100vw, 700px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%20500'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-18x12.png 18w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png"></figure>



<h3>SET Performance on Redis with IO/Threads &amp; 256 Connections:</h3>



<figure><img loading="lazy" decoding="async" width="700" height="500" src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-18x12.png 18w" sizes="(max-width: 700px) 100vw, 700px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%20500'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-18x12.png 18w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png"></figure>



<h2>Pushing Valkey Throughput Further</h2>



<p>In the previous section, we saw that Valkey could hit 678K RPS on SETs with 6 threads and 256 connections. If we up the connections to 400, the throughput goes up to 832K RPS. How did we get the additional 167K RPS?</p>



<p>We used <a href="https://github.com/iopsystems/rezolus" target="_blank" rel="noreferrer noopener">Rezolus</a>, our favorite Linux performance telemetry agent, to get deep insights into the system under stress. You can see in the charts below that overall CPU utilization is around 80% and unevenly distributed across the 8 cores.</p>



<p>Diving deeper, this is driven by hardware interrupts from network queues across all 8 cores. Interrupts are bad because they disrupt a hard working Valkey thread to yield to handle network packets.</p>



<figure><img loading="lazy" decoding="async" width="1024" height="835" src="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png" alt="CPU Usage chart" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-300x245.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-768x627.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-15x12.png 15w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1.png 1445w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20835'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-300x245.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-768x627.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-15x12.png 15w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1.png 1445w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png"></figure>



<p>What if we could avoid the context switching on our <code><sup>c8g.2xl</sup></code> with 8 cores? Running close to a million RPS requires considerable packet processing horsepower. Luckily, since a lot of work happens at the Nitro level on EC2 instances, two allocated cores to IRQs is all you need (if you let them focus). Pinning the IRQs to two cores is pretty straightforward.</p>



<pre><code>sudo ethtool -L ens34 combined 2 # reduce to 2 IRQs
grep ens34 /proc/interrupts # ours were on 99 and 100
echo 1 | sudo tee /proc/irq/99/smp_affinity # pin 99 to core 1
echo 2 | sudo tee /proc/irq/100/smp_affinity # pin 100 to core 2</code></pre>



<p>But how do we let these threads focus? and how do we avoid Redis / Valkey threads contending for the same cores? We pin Redis/Valkey to cores 3-8, giving their IO-Threads better isolation while also allowing the IRQs to focus. We used the <code><sup>--cpuset-cpus</sup></code> Docker flag to set these CPU assignments, making sure that Redis and Valkey process stayed pinned to the intended cores throughout the test. This reduces cross-core contention and improves cache locality, <strong>both of which are critical for minimizing tail latencies at high throughput</strong>. Ideal core allocation can vary in multi-tenant environments or mixed workloads, but in this benchmark it provided clean isolation between system and application workloads.</p>



<p><strong>Redis:</strong></p>



<pre><code>docker run --network="host" --rm \
  --cpuset-cpus="2-7" redis:8.0 \
  --save "" --appendonly no \
  --io-threads 6  \
  --protected-mode no --maxmemory 10gb</code></pre>



<p><strong>Valkey:</strong></p>



<pre><code>docker run --network="host" --rm \
  --cpuset-cpus="2-7" valkey/valkey:8.1.1 \
  --save "" --appendonly no --io-threads 6 \
  --protected-mode no --maxmemory 10gb</code></pre>



<p>Let’s see what Rezolus has to say about the new setup with IRQs pinned to the first 2 cores and Valkey pinned to the remaining 6 cores. First, we observed meaningfully higher CPU Utilization. Second, looking at the bottom chart (SoftIRQ), we see that it is now limited to only the first two cores. Third, the Valkey cores are running red hot, whereas we previously saw a much more scattered distribution one usage across cores. <strong>While this setup is ideal for this benchmark, optimal IRQ tuning depends heavily on NIC architecture and the concurrency model of your application.</strong></p>



<figure><img loading="lazy" decoding="async" width="720" height="603" src="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png" alt="CPU Chart 2" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png 720w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-300x251.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-14x12.png 14w" sizes="(max-width: 720px) 100vw, 720px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20720%20603'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png 720w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-300x251.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-14x12.png 14w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png"></figure>



<p>The extra 20% CPU utilization is what buys us the extra 167K RPS (from 832K RPS to 999.8K RPS).</p>



<h2>Try it Yourself (And Know Before You Go)</h2>



<p>These benchmarks are hopefully a beginning and not the end. Our hope is that this sets up both Valkey and Redis communities to continue the performance improvement journey. We also recognize that many folks may want to reproduce the benchmark in their own environments, incorporating their workflow specifics. Below, we outline some key instructions that you can use to reproduce this in your own AWS account within an hour.</p>



<p><strong>Instance Types:</strong>&nbsp;We used AWS Graviton4-based&nbsp;<code><sup>c8g</sup></code>&nbsp;instances, launched in September 2024. The&nbsp;<sup><code>c8g.2xlarge</code></sup> server node provides 8 vCPUs (costing roughly $250/month in us-east-1), while the&nbsp;<code><sup>c8g.8xlarge</sup></code>&nbsp;load generator offers 32 vCPUs. This provided enough CPU headroom to cleanly isolate the benchmark workload, IRQ handling, and Redis/Valkey processing. The same c8g.2xl instance was used to run valkey and redis (one at a time). The same load gen node was run each time. Valkey and Redis were restarted right before each test to ensure fairness.</p>



<p><strong>Placement Groups:</strong>&nbsp;We used EC2 placement groups (cluster mode) to ensure minimal network jitter and low-latency communication between the client and server nodes. Placement groups offer extremely tight latencies by reducing the number of hops between your EC2 instances. This has the upside on higher throughput, fewer interruptions, and lower latencies – but it has some shared fate / blast radius implications that are worth considering before deploying them in your production environment.</p>



<p><strong>Core Pinning.</strong> To see the highest throughput and lowest latencies, consider core pinning and reducing the IRQs. See section above for specific instructions we used on our 8 core instance. It is also important to apply similar techniques on your test nodes.</p>



<p><strong>Vary the connections.</strong> Connection count is a surprisingly crucial variable for both Redis and Valkey. In fact, latencies rise steeply as you approach 1024 connections on both of them. For example, going from 400 to 1024 connections, Valkey’s SET throughput dropped from 999.9K RPS with to 969K RPS and p99 latencies doubled from .8ms to 1.6ms (at 2048 conns, p99 latencies triple). Going from 384 connections to 1024, Redis throughput drops from 729.4K RPS to 668K RPS, and p99 latencies more than double from .99ms to 2.5ms. Lower throughput with higher latencies? You get why connection count tuning is so crucial here.</p>



<p><strong>Key Space.</strong> If you want the best numbers, <a href="https://www.linkedin.com/posts/yaoyue-thinkingfish_my-performance-rant-of-the-day-if-you-are-activity-7326350824261980161-qB6h/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAANW9wBFDc3gQ3Jwp6YswZ_BARGfJvyJQQ" target="_blank" rel="noreferrer noopener">use smaller values and a really small key space</a> (<sup><code>-r 10000</code></sup>). This will help you get everything from L3 cache. To make this test slightly more real world, we used 1KB items (<code><sub><sup>-d 1024</sup></sub></code>) and a key space of 3Million (<code><sup>-r 3000000</sup></code>).</p>



<p><strong>Multi-Thread the Benchmark App.</strong> To get the maximum throughput out of valkey-benchmark, make sure to turn on multi-threading on the benchmark tool as well. The <code><sup>--threads 6</sup></code> flag tells valkey-benchmark to run in multi-threaded mode.</p>



<p><strong>Benchmark command:</strong></p>



<pre><code>docker run --network="host" --rm --cpuset-cpus="2-7" \
valkey/valkey:8.0.1 valkey-benchmark \
-h 172.31.4.92 -p 6379 -t SET,GET -n 100000000 -c 256 \
-r 3000000 --threads 6 -d 1024</code></pre>



<h2>A Final Caveat: Benchmarking is imprecise in nature</h2>



<p>We made every effort to make this benchmark resemble more real world workflows, but you can always do better. Valkey-bench is not perfect (nothing is). We have a wishlist of improvements (and so <a href="https://github.com/valkey-io/valkey/issues/900" target="_blank" rel="noreferrer noopener">does</a> the Valkey project).</p>



<p>First, today, it simply pushes as much load as the server is able to handle instead of targeting a particular TPS. The real world rarely modulates its throughput based on your latency. Second, once it can target specific load, it would become closer to real world if it could modulate the load to show spikes and troughs as opposed to running a consistent throughput profile. Lastly, it’s rare to see 100% GETs or 100% SETs in a Key Value cache workflow. We’d love to provide a SET:GET ratio to see how the system reacts.</p>



<p>At Momento, we typically do our testing using <a href="https://github.com/iopsystems/rpc-perf" target="_blank" rel="noreferrer noopener">rpc-perf</a>. It is written entirely in rust, handles more real world scenarios (like the three feature requests above), and pairs incredibly well with Rezolus. Regardless, even rpc-perf is a synthetic benchmark and even though it gives you more degrees of freedom to simulate production workflows, the results should not be interpreted as generally applicable to every workflow. Small variables make huge differences – and simulations are no match for production.</p>



<h2>Final Thoughts: Performance Is a Practice</h2>



<p>Valkey has not only kept pace – it’s setting it. Meanwhile, the performance ceiling keeps rising. But getting there isn’t automatic. It requires expertise across systems, infrastructure, and workload behavior.</p>



<p>At Momento, we help teams achieve this kind of performance every day. Whether you’re running Valkey, Redis, or evaluating your options – we’re here to help you scale with confidence.</p>



<p><strong>Want help tuning your real-time infrastructure? <a href="https://gomomento.com/contact-us" target="_blank" rel="noreferrer noopener">Let’s talk.</a></strong></p>







<hr>



<p><strong>Special thanks to Yao and Brian from&nbsp;<a href="https://iop.systems/" target="_blank" rel="noreferrer noopener">IOP Systems</a>&nbsp;for providing the tools, including rpc-perf and rezolus, as well as insights for this benchmark.</strong></p>




        
      </div>

  </article>
  


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Silicon Valley finally has a big electronics retailer again: Micro Center opens (236 pts)]]></title>
            <link>https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx</link>
            <guid>44140378</guid>
            <pubDate>Fri, 30 May 2025 22:24:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx">https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx</a>, See on <a href="https://news.ycombinator.com/item?id=44140378">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p dir="ltr">After years of waiting, the ribbon has been cut and Micro Center Silicon Valley is officially open.&nbsp;</p>
<p dir="ltr">On a sunny Friday morning in Santa Clara, with hundreds of fans queued in a line wrapping down the block and around the corner, we welcomed the Silicon Valley community to our newest store, at <a href="https://www.microcenter.com/site/stores/santa-clara.aspx">5201 Stevens Creek Blvd</a>.&nbsp;</p>
<p dir="ltr">If you're a DIY PC builder, a serious gamer, a creator, a maker, or just someone who gets excited over the latest CPUs, GPUs, and 3D printers, then you already know what Micro Center is about</p>
<p dir="ltr">The Bay Area sets a high bar for all things tech, and here you'll find aisles stacked high with components, knowledgeable staff who actually know what they're talking about, and a hands-on experience you just can't replicate online.&nbsp;</p>
<p dir="ltr">The grand opening celebration also features special promotions, including 20% off Windows desktops and laptops and 20% off monitors. Additionally, the store has over 4,000 graphics cards in stock, including exclusive models, that will available for our grand opening event. For more information, please visit the <a href="https://www.microcenter.com/site/stores/santa-clara.aspx">Micro Center Santa Clara page</a>.</p>
<p><img src="https://60a99bedadae98078522-a9b6cded92292ef3bace063619038eb1.ssl.cf2.rackcdn.com/images_mc-sc-1.jpg" width="1280" height="720" alt="A scene from the MC Santa Clara grand opening. "></p>
<p><span>Photo: Dan Ackerman&nbsp;</span></p>

<p><img src="https://60a99bedadae98078522-a9b6cded92292ef3bace063619038eb1.ssl.cf2.rackcdn.com/images_mc-sc-2.jpg" width="1280" height="720" alt="A scene from the MC Santa Clara grand opening. "></p>
<p><span>Photo: Dan Ackerman&nbsp;</span></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Photos taken inside musical instruments (638 pts)]]></title>
            <link>https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments</link>
            <guid>44139626</guid>
            <pubDate>Fri, 30 May 2025 20:32:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments">https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments</a>, See on <a href="https://news.ycombinator.com/item?id=44139626">Hacker News</a></p>
Couldn't get https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Jerry Lewis's "The Day the Clown Cried" discovered in Sweden after 53 years (158 pts)]]></title>
            <link>https://www.thenationalnews.com/arts-culture/film-tv/2025/05/29/jerry-lewis-day-the-clown-cried-discovered/</link>
            <guid>44139592</guid>
            <pubDate>Fri, 30 May 2025 20:27:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thenationalnews.com/arts-culture/film-tv/2025/05/29/jerry-lewis-day-the-clown-cried-discovered/">https://www.thenationalnews.com/arts-culture/film-tv/2025/05/29/jerry-lewis-day-the-clown-cried-discovered/</a>, See on <a href="https://news.ycombinator.com/item?id=44139592">Hacker News</a></p>
<div id="readability-page-1" class="page"><article data-identifier="article-body-chain" id=""><p id="el-0-3ZTUS3Y2ZRCBTAOIX7Z4SUTSP4">One of cinema's most sought-after <a href="https://www.thenationalnews.com/arts-culture/film-tv/2024/11/25/lost-john-ford-film-the-scarlet-drop-chile/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts-culture/film-tv/2024/11/25/lost-john-ford-film-the-scarlet-drop-chile/">lost films</a> has been discovered after having been kept secretly in the collection of a Swedish actor for 45 years. </p><p id="el-1-BRMSRUAMDZBGHFZU3QXU2IMB5I">Comedian <a href="https://www.thenationalnews.com/arts/jerry-lewis-women-are-funny-but-not-when-crude-1.255081" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts/jerry-lewis-women-are-funny-but-not-when-crude-1.255081">Jerry Lewis</a>'s controversial holocaust film <i>The Day the Clown Cried,</i> shot in 1972 but never released, was thought to <a href="https://www.thenationalnews.com/arts-culture/2024/04/02/jerry-lewis-the-day-the-clown-cried-loc/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts-culture/2024/04/02/jerry-lewis-the-day-the-clown-cried-loc/">not exist in finished form</a>.</p><p id="el-3-XA2MPSNDZBCRNC6TCMNBHJA4HA">But Hans Crispin, star of the beloved 1980s Swedish TV series <i>Angne &amp; Svullo</i>, claims he stole a complete workprint of the film from the archives of its production studio in 1980 – and has been screening it for guests in his apartment ever since. </p><p id="el-4-PE4RPU5IJRHFFNIBRAGW3UKI2I">“I have the only copy,” Crispin told <a href="https://www.thenationalnews.com/arts-culture/film-tv/2024/08/30/israel-palestine-on-swedish-tv-review/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts-culture/film-tv/2024/08/30/israel-palestine-on-swedish-tv-review/">Swedish state news broadcaster SVT.</a> “I stole it from Europafilm in 1980 and copied it to VHS in the attic where we copied other films at night.</p><p id="el-5-OUFA3ROWJNGGTKG3VEOQRQSPQ4">“I've kept the copy in my bank vault,” Crispin added.</p><div id="el-7-LTK64SHFGFDSHHFSSCB34R3BLE"><figure><picture><source width="800" height="534" media="(min-width: 1024px)" srcset="https://www.thenationalnews.com/resizer/v2/LTK64SHFGFDSHHFSSCB34R3BLE.jpg?smart=true&amp;auth=764112cb59e4b7491bb7e89dbe7523f067eb7a46eaf5fc57fcbcce039b925034&amp;width=800&amp;height=534"><source width="600" height="400.49999999999994" media="(min-width: 768px)" srcset="https://www.thenationalnews.com/resizer/v2/LTK64SHFGFDSHHFSSCB34R3BLE.jpg?smart=true&amp;auth=764112cb59e4b7491bb7e89dbe7523f067eb7a46eaf5fc57fcbcce039b925034&amp;width=600&amp;height=400"><source width="400" height="267" media="(min-width: 350px)" srcset="https://www.thenationalnews.com/resizer/v2/LTK64SHFGFDSHHFSSCB34R3BLE.jpg?smart=true&amp;auth=764112cb59e4b7491bb7e89dbe7523f067eb7a46eaf5fc57fcbcce039b925034&amp;width=400&amp;height=267"><img _id="LTK64SHFGFDSHHFSSCB34R3BLE" type="image" originalwidth="3072" originalheight="2048" alt="Swedish actor Hans Crispin has a complete workprint of The Day The Clown Cried, SVT has reported. AFP" data-chromatic="ignore" loading="lazy" fetchpriority="low" src="https://www.thenationalnews.com/resizer/v2/LTK64SHFGFDSHHFSSCB34R3BLE.jpg?smart=true&amp;auth=764112cb59e4b7491bb7e89dbe7523f067eb7a46eaf5fc57fcbcce039b925034&amp;width=400&amp;height=267" width="400" height="267"></picture><div><figcaption>Swedish actor Hans Crispin has a complete workprint of The Day The Clown Cried, SVT has reported. AFP</figcaption></div></figure></div><p id="el-8-6S5ML4BMEVAN7GJ3ZE2N3RA2IA">Crispin recently screened a full copy to journalists from SVT and Sweden's <i>Icon </i>magazine to prove his claim was true. </p><p id="el-9-VVTE7Y2KHJEPTMOUWREYMIGM6Y">“You're the 23rd and 24th people I've shown it to,” he told <i>Icon </i>and SVT. </p><p id="el-10-4ETIS7JGXNAZ7JI52PNYJP6J7E">The actor also revealed that his initial copy was missing the opening six-minute sequence of the film shot in Paris, which was mailed to him anonymously in 1990, along with a note saying that the sender knew he possessed a copy of the rest of the film.</p><h2 id="el-11-K3BGGW4MCZG5NCBMUBIKKXWOBY"><b>Will The Day The Clown Cried be released to the public?</b></h2><p id="el-12-EAZWT3KXKVBPHORFC7PVEXUDB4">Now that he has come out into the open, Crispin intends to make his copy available for the world to see, saying: “It must be seen!”</p><p id="el-13-4SFSIGNF7ZBKDPKNAD6PNME6BQ">Crispin added: “I think I want to hand it over to the next generation. With today's technique, it can be restored. I want to sell it to a serious producer who either restores it or keeps it locked away, or restores it and shows it to people for studying purposes.”</p><p id="el-14-W6Z67YK6XBFQTLO55J3YVKXUUA">The film tells the story of a German circus clown who is imprisoned in a Nazi concentration camp for mocking <a href="https://www.thenationalnews.com/arts-culture/books/hitler-a-bizarrely-sympathetic-biography-1.360922" target="_blank" rel="">Adolf Hitler</a> and is then forced to lure children to their deaths as punishment.</p><div id="el-15-RTWN5IS6O5DRXGUGUCC3P7VSDY"><figure><picture><source width="800" height="534" media="(min-width: 1024px)" srcset="https://www.thenationalnews.com/resizer/v2/RTWN5IS6O5DRXGUGUCC3P7VSDY.jpg?smart=true&amp;auth=7ae3e215b29972c42efae31fb4f1a4c1b1a3d8029c4ddff1cefd4875061e4741&amp;width=800&amp;height=534"><source width="600" height="400.49999999999994" media="(min-width: 768px)" srcset="https://www.thenationalnews.com/resizer/v2/RTWN5IS6O5DRXGUGUCC3P7VSDY.jpg?smart=true&amp;auth=7ae3e215b29972c42efae31fb4f1a4c1b1a3d8029c4ddff1cefd4875061e4741&amp;width=600&amp;height=400"><source width="400" height="267" media="(min-width: 350px)" srcset="https://www.thenationalnews.com/resizer/v2/RTWN5IS6O5DRXGUGUCC3P7VSDY.jpg?smart=true&amp;auth=7ae3e215b29972c42efae31fb4f1a4c1b1a3d8029c4ddff1cefd4875061e4741&amp;width=400&amp;height=267"><img _id="RTWN5IS6O5DRXGUGUCC3P7VSDY" type="image" originalwidth="3072" originalheight="2048" alt="The footage that Lewis possessed of the Day The Clown Cried was donated to the Library of Congress in 2015. AFP" data-chromatic="ignore" loading="lazy" fetchpriority="low" src="https://www.thenationalnews.com/resizer/v2/RTWN5IS6O5DRXGUGUCC3P7VSDY.jpg?smart=true&amp;auth=7ae3e215b29972c42efae31fb4f1a4c1b1a3d8029c4ddff1cefd4875061e4741&amp;width=400&amp;height=267" width="400" height="267"></picture><div><figcaption>The footage that Lewis possessed of the Day The Clown Cried was donated to the Library of Congress in 2015. AFP</figcaption></div></figure></div><p id="el-16-S5AOZL6KS5GTLJWT4CE6SEYBYU">Lewis, who directed and starred in the film as clown Helmut Doork, donated five hours of footage to the US Library of Congress in 2015, adding a stipulation that it not be made available until June 2024. </p><p id="el-17-OOAFYHIV6RFHPG5KNF6OAFFGW4">The footage, which has been made available to scholars, was screened last August for <i>The New Republic</i> journalist Benjamin Charles Germain Lee, who reported that the footage was fragmentary and does not constitute a complete film, leading the industry to conclude that the full film did not exist.</p><h2 id="el-18-MITN4F2L5REQ3JS7QLNNV3PABI"><b>Why the film was never released</b></h2><p id="el-19-S2OYFSEKX5BOVGW2SGJI2FOUKU">While there were myriad alleged issues during the shoot itself, problems reportedly arose between Lewis and producer Nat Wachsberger once filming stopped, which is considered the main catalyst for the film's shelving.</p><p id="el-20-MSVX4YUHK5FY7BSCX2CQK25UKI">Lewis was reportedly unsatisfied with the film’s financing and announced that Wachsberger did not fulfil his financial obligations. Hearing this, Wachsberger threatened to sue Lewis for breach of contract, which resulted in a fallout between the two that caused Lewis to leave with a rough cut of the film, according to a 2018 feature in <i>The New York Times.</i></p><p id="el-21-N7PHIBCFE5D77FCB3E74WKAFVI">Lewis had mixed feelings about the film, showing fragments of his footage to close friends. However, in his 1982 autobiography, Lewis said “the picture must be seen”.</p><div id="el-22-XG4SDP7YJZGFJJ3L75JCPD7G5E"><figure><picture><source width="800" height="1200" media="(min-width: 1024px)" srcset="https://www.thenationalnews.com/resizer/v2/XG4SDP7YJZGFJJ3L75JCPD7G5E.jpg?smart=true&amp;auth=1184b9a1d6e572d7bbeb8eaeaf5d9cb85b539359fff54ef9a28d0def0a831bd8&amp;width=800&amp;height=1200"><source width="600" height="900" media="(min-width: 768px)" srcset="https://www.thenationalnews.com/resizer/v2/XG4SDP7YJZGFJJ3L75JCPD7G5E.jpg?smart=true&amp;auth=1184b9a1d6e572d7bbeb8eaeaf5d9cb85b539359fff54ef9a28d0def0a831bd8&amp;width=600&amp;height=900"><source width="400" height="600" media="(min-width: 350px)" srcset="https://www.thenationalnews.com/resizer/v2/XG4SDP7YJZGFJJ3L75JCPD7G5E.jpg?smart=true&amp;auth=1184b9a1d6e572d7bbeb8eaeaf5d9cb85b539359fff54ef9a28d0def0a831bd8&amp;width=400&amp;height=600"><img _id="XG4SDP7YJZGFJJ3L75JCPD7G5E" type="image" originalwidth="2048" originalheight="3072" alt="No complete copy of The Day The Clown Cried has ever been confirmed to exist until now. AFP" data-chromatic="ignore" loading="lazy" fetchpriority="low" src="https://www.thenationalnews.com/resizer/v2/XG4SDP7YJZGFJJ3L75JCPD7G5E.jpg?smart=true&amp;auth=1184b9a1d6e572d7bbeb8eaeaf5d9cb85b539359fff54ef9a28d0def0a831bd8&amp;width=400&amp;height=600" width="400" height="600"></picture><div><figcaption>No complete copy of The Day The Clown Cried has ever been confirmed to exist until now. AFP</figcaption></div></figure></div><p id="el-23-OVBRR24ZMBGJRPPTNNZU3LT42A">After watching it, <a href="https://www.thenationalnews.com/lifestyle/fashion/2021/10/04/the-simpsons-make-their-fashion-week-debut-for-balenciaga-in-paris/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/lifestyle/fashion/2021/10/04/the-simpsons-make-their-fashion-week-debut-for-balenciaga-in-paris/"><i>The Simpsons</i></a> voice actor Harry Shearer said it was “a perfect object”, adding: “This movie is so drastically wrong, its pathos and its comedy are so wildly misplaced, that you could not, in your fantasy of what it might be like, improve on what it really is.”</p><p id="el-24-WUNXMZT2DJA5FJATYEOXNWFXIM">In an interview with <i>The New York Times</i> in 2018, Chris Lewis, the comedian's son, said: “It was something that was very close to his heart.”</p><p id="el-25-4F5RPU7YZFBLJOLZNVFPSA56VY">At other times, however, Lewis denounced the film. In 2013, footage of him surfaced on YouTube in which he stated: “It was bad, and it was bad because I lost the magic. No one will ever see it, because I'm embarrassed at the poor work.”</p><h2 id="el-26-ME2KIY2R4ZHJPFY7G33OKZTQBM"><b>The history of lost films</b></h2><p id="el-27-ZBOL6RP5OJD3FK44FZKBL72GLE"><i>The Day the Crown Cried</i> is an example of one of many films that were once thought lost or not fit for public screening.</p><p id="el-28-APHSELV6IRGRRDFZ5L2ZJ4WZX4">Similar films include 1976’s <i>Chess of the Wind </i>by Iranian director <a href="https://www.thenationalnews.com/arts-culture/film/the-7-middle-eastern-films-to-see-at-the-london-film-festival-1.1078995" target="_blank" rel="">Mohammad Reza Aslani.</a></p><p id="el-29-GQ27P642NBDRHNGGYANXB6GKE4">Until it was rediscovered in 2020, the film could only be watched on low-quality VHS tapes. Since then, it has been restored and screened around the world.</p><p id="el-30-KDOD53KPJND4HDABN7NCEBPXOU">One of the best-known lost films is <i>The Passion of Joan of Arc</i> from 1928. After being lost for years, a copy was found in a Norwegian hospital in the 1980s. The film is now considered one of the most important historical film artefacts.</p><div id="el-31-U6C2G5A4HZG7FBM3YI73DKVZ6A"><figure><picture><source width="800" height="1064" media="(min-width: 1024px)" srcset="https://www.thenationalnews.com/resizer/v2/U6C2G5A4HZG7FBM3YI73DKVZ6A.jpg?smart=true&amp;auth=d6669dba0199da56108da2efd0f85a3cec33679b83aa0caf770e3e51c8b2ad48&amp;width=800&amp;height=1064"><source width="600" height="798" media="(min-width: 768px)" srcset="https://www.thenationalnews.com/resizer/v2/U6C2G5A4HZG7FBM3YI73DKVZ6A.jpg?smart=true&amp;auth=d6669dba0199da56108da2efd0f85a3cec33679b83aa0caf770e3e51c8b2ad48&amp;width=600&amp;height=798"><source width="400" height="532" media="(min-width: 350px)" srcset="https://www.thenationalnews.com/resizer/v2/U6C2G5A4HZG7FBM3YI73DKVZ6A.jpg?smart=true&amp;auth=d6669dba0199da56108da2efd0f85a3cec33679b83aa0caf770e3e51c8b2ad48&amp;width=400&amp;height=532"><img _id="U6C2G5A4HZG7FBM3YI73DKVZ6A" type="image" originalwidth="3120" originalheight="4146" alt="The Day The Clown Cried depicts a clown entertaining children during the holocaust. AFP" data-chromatic="ignore" loading="lazy" fetchpriority="low" src="https://www.thenationalnews.com/resizer/v2/U6C2G5A4HZG7FBM3YI73DKVZ6A.jpg?smart=true&amp;auth=d6669dba0199da56108da2efd0f85a3cec33679b83aa0caf770e3e51c8b2ad48&amp;width=400&amp;height=532" width="400" height="532"></picture><div><figcaption>The Day The Clown Cried depicts a clown entertaining children during the holocaust. AFP</figcaption></div></figure></div><p id="el-32-EBP67AN5B5G2BGXRNZMD7JXHZU"><i>London After Midnight</i>, a 1927 horror film directed by Tod Browning starring Lon Chaney, is still a veritable white whale for fans after the last-known copy was destroyed in the 1965 MGM vault fire.</p><p id="el-33-7USMYYXBQZCJZPRD3RLG3SWOJM">Other films that have not yet screened because of filmmaker stipulations include <i>100 Years</i> starring <a href="https://www.thenationalnews.com/arts-culture/film/the-many-faces-of-the-talented-mr-malkovich-1.762364" target="_blank" rel="">John Malkovich</a>. The short film is from 2015 but has been placed in time-locked safes that won’t open until 2115, 100 years after the film was made.</p><p id="el-34-S3T4RD37A5F5FOWZEANBINWKHY">Several recently produced films are now <a href="https://www.thenationalnews.com/arts-culture/2024/08/05/fantastic-four-abandoned-hollywood-films/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts-culture/2024/08/05/fantastic-four-abandoned-hollywood-films/">considered lost media,</a> including 2022's <i>Batgirl</i>, directed by <a href="https://www.thenationalnews.com/arts-culture/film/who-are-adil-el-arbi-and-bilall-fallah-moroccan-belgian-directors-behind-the-new-batgirl-film-1.1226356" target="_blank" rel="">Adil El Arbi and Bilall Fallah</a>. The superhero film stars Leslie Grace as <a href="https://www.thenationalnews.com/arts-culture/television/2022/08/04/why-was-the-90-million-batgirl-killed-and-could-hbo-max-be-next/" target="_blank" rel="">Batgirl</a> and also includes J K Simmons, Brendan Fraser and Michael Keaton. </p><p id="el-35-4EAHPEUEANBW5JH6D7RZ7BDGIM">Warner Bros Discovery announced in August 2022 that it would not be released due to cost-cutting measures and a strategy shift towards theatrical releases.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Surprisingly Fast AI-Generated Kernels We Didn't Mean to Publish (Yet) (276 pts)]]></title>
            <link>https://crfm.stanford.edu/2025/05/28/fast-kernels.html</link>
            <guid>44139454</guid>
            <pubDate>Fri, 30 May 2025 20:03:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://crfm.stanford.edu/2025/05/28/fast-kernels.html">https://crfm.stanford.edu/2025/05/28/fast-kernels.html</a>, See on <a href="https://news.ycombinator.com/item?id=44139454">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    


<div>
    <p><a href="https://www.stanford.edu/" target="_blank">
            <img src="https://crfm.stanford.edu/static/img/header/stanford-white.png">
        </a>
    </p>
</div>

<header>
    <nav>
        <div>
            <div>
                <p><a href="https://crfm.stanford.edu/">
                    <img src="https://crfm.stanford.edu/static/img/header/crfm-rgb.png">
                </a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://hai.stanford.edu/" target="_blank">
                    <img src="https://crfm.stanford.edu/static/img/header/hai.png">
                </a></p>
            </div>
            <div>
                <ul>
                    <li>
                        <a href="https://crfm.stanford.edu/people.html">People</a>
                    </li>
                    <li>
                        <a href="https://crfm.stanford.edu/report.html">Report</a>
                    </li>
                    <li id="dropdownContainer">
                        <a id="dropdownButton">Research</a>
                        
                    </li>
                    <li>
                        <a href="https://crfm.stanford.edu/policy.html">Policy</a>
                    </li>
                    <li>
                        <a href="https://crfm.stanford.edu/blog.html">Blog</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    
    
</header>

		<div>
				

  <h2>Surprisingly Fast AI-Generated Kernels We Didn’t Mean to Publish (Yet)</h2>
  
  
  
  
  

  

  <hr>

  

  <div>
    <h2 id="tldr">TL;DR</h2>

<p>We have some very fast AI-generated kernels in pure CUDA-C without using libraries and DSLs such as CUTLASS and Triton. They are performing close to or in some cases even beating the standard expert-optimized production kernels shipped in PyTorch. Some of our highlighted results:</p>

<ul>
  <li><strong>Matmul (FP32): 101.3%</strong> performance of FP32 torch.matmul; problem size: 4096x4096 square matrices</li>
  <li><strong>Conv2D: 179.9%</strong> performance of FP32 torch.nn.Conv2D; problem size: (100, 3, 224, 224) input tensor, conv(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)</li>
  <li><strong>Softmax: 111.8%</strong> performance of FP32 torch.softmax; problem size: (4096, 65536) input tensor</li>
  <li><strong>LayerNorm: 484.4%</strong> performance of FP32 torch.nn.LayerNorm; problem size: (16, 64, 256, 256) input tensor</li>
  <li><strong>Conv2D + ReLU + MaxPool: 290.1%</strong> performance of FP32 torch reference, 189.0% performance of FP32 torch.compile() reference; problem size: (100, 3, 224, 224) input tensor, conv(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2), maxpool(kernel_size=3, stride=2)</li>
</ul>

<p>(Our results are benchmarked on an Nvidia L40S GPU, and % performance is defined as reference time divided by generated kernel time)</p>

<p><img src="https://crfm.stanford.edu/static/img/posts/2025-05-28-fast-kernels/untiled.png" width="60%"><br>
<small><em>“Untiled” by DALL·E (2025). (Digital pigment on virtual canvas)<br>From the MMA collection</em></small></p>

<h2 id="intro">Intro</h2>

<p>We started with the goal of generating synthetic data to train better kernel generation models. Somewhere along the way the unexpected happened: the test-time only synthetic data generation itself started producing <em>really</em> good kernels beating or performing close to human expert optimized PyTorch baselines, utilizing advanced optimizations and hardware features, which were previously thought to be challenging. As a result, we decided to write this blog post early and share our findings. The point of this blog post isn’t about a novel methodology; in fact, our synthetic data generation design is simple, and what’s surprising is that it is already showing promise.</p>

<p>In this post, we’re sharing the method, five optimized kernels (4 foundational ML operators + 1 fused kernel of an AlexNet block), an example optimization trajectory, and some takeaways and thoughts on what this might mean for performant kernel generation. Consider this a first step in what’s next.</p>

<h2 id="method">Method</h2>

<p>We’re using the <a href="https://arxiv.org/abs/2502.10517">KernelBench</a> (a benchmark for AI based kernel generation that we released in December 2024) task setup: given torch code, the LLM writes custom kernels to replace the torch operators with the goal of getting a speedup. Consistent with the original KernelBench design, the reference code is in the default FP32, and given a tolerance threshold (1e-02), using lower precision solutions is valid. In addition, each problem in KernelBench has specific sizes since there are many size-specific optimizations, so the benchmark tests for the fastest kernel for the specific problem size, not necessarily a generally fast kernel for any arbitrary problem size. We run both the torch reference code and the generated code, and test for correctness by checking the numerical equality of the two outputs over many random inputs.<br>
<img src="https://crfm.stanford.edu/static/img/posts/2025-05-28-fast-kernels/kernelbench_design.png" width="100%"></p>

<p>The most common way people scale test-time compute for this problem of optimizing kernels today is through sequential revision, a multi-turn loop where a model incrementally edits a kernel, checks for correctness and performance, then tries again based on the result, either fixing the kernel or try to improve its performance. This loop is intuitive and easy to implement. The model fixes broken kernels, tweaks working ones, and gradually climbs toward something faster.</p>

<p>The main limitation of this approach is the lack of optimization idea diversity. Sequential loops often fall into local minima, revisiting the same classes of transformations or endlessly refining unpromising trajectories. The result is inefficient use of test-time compute and little pressure on the model to generate fundamentally new optimization ideas.</p>

<p>We introduced two key changes to address this:</p>

<ol>
  <li>Reasoning in natural language about optimization ideas: rather than directly generating new kernels in each step, we generate optimization ideas in natural language conditioned on previously attempted ideas, and realize those ideas into new code variants.</li>
  <li>Branching at each optimization step: instead of refining a single candidate per step, we fan out such that each idea spawns multiple implementations, and the highest-performing kernels are used to seed the next round (we also keep a bank of good existing kernels for seeding). This unlocks massive parallelism allowing us to explore radically different directions at each turn, rather than getting stuck in a narrow optimization path.</li>
</ol>

<p><img src="https://crfm.stanford.edu/static/img/posts/2025-05-28-fast-kernels/search.png" width="100%"></p>

<p>The result is a test-time loop that looks less like “chat with a compiler” in the case of sequential revision, and more like structured exploratory search, guided by explicit optimization hypotheses and aggressively parallel evaluation.</p>

<p>We ran 10 problems from KernelBench level 1 (and modified the problem sizes to make sure that kernel launch overhead is negligible compared to the overall runtime of the problem). We ran 5 rounds with the OpenAI o3 and Gemini 2.5 Pro models. The plot below shows the distribution of rounds in which the best-performing kernel was first found. Most of the best results emerge in later rounds (out of a total of 5 rounds), with the majority coming in round 4 or 5.
<img src="https://crfm.stanford.edu/static/img/posts/2025-05-28-fast-kernels/rounds.png" width="100%"></p>

<p>As we scaled up our search, we also found that many high-performing kernels clustered into a few recurring optimization strategies, which also aligns with our experience of writing kernels by hand. The main optimization categories are summarized below:</p>

<ul>
  <li><strong>Memory Access Optimization:</strong> improving the efficiency of data movement between different memory hierarchies (global memory, shared memory, registers) and ensuring data is accessed in a way that maximizes bandwidth and minimizes conflicts.</li>
  <li><strong>Asynchronous Operations &amp; Latency Hiding:</strong> hide the latency of slow operations (like global memory access) by overlapping them with computation or other memory transfers</li>
  <li><strong>Data Type &amp; Precision Optimization:</strong> using lower-precision data types (like FP16 or BF16) where possible to reduce memory bandwidth requirements, increase cache effectiveness, and potentially leverage specialized hardware units.</li>
  <li><strong>Compute &amp; Instruction Optimization</strong>: making the arithmetic computations themselves more efficient, reducing instruction count, or leveraging specialized hardware instructions</li>
  <li><strong>Parallelism &amp; Occupancy Enhancement</strong>: maximize the number of active warps on the Streaming Multiprocessors (SMs) to better hide latencies and improve overall throughput</li>
  <li><strong>Control Flow &amp; Loop Optimization</strong>: reducing the overhead associated with loops, branches, and indexing calculations</li>
</ul>

<h2 id="an-example-kernel-optimization-trajectory">An Example Kernel Optimization Trajectory</h2>

<p>Here we show an example optimization trajectory of auto-generated ideas for Conv2D, with torch reference baseline time of <strong>1.41 ms</strong></p>

<p><strong>Round 0: 7.02 ms, 20.1% of reference</strong><br>
Idea: Given the pytorch code, replace the operation with a CUDA Kernel</p>

<p><strong>Round 1: 7.54 ms, 18.8% of reference</strong><br>
Idea: Exploit the read-only cache by loading invariant tensors with __ldg.</p>

<p><strong>Round 2: 3.46 ms, 41.0% of reference</strong><br>
Idea: Convert the convolution to an FP16 Tensor-Core GEMM. <em>[author comment: this is an algorithmic optimization converting a convolution to an implicit GEMM, which is important for running convolutions efficiently on Tensor Cores]</em></p>

<p><strong>Round 3: 3.67 ms, 38.7% of reference</strong><br>
Idea: Double-buffer cp.async pipeline that overlaps global-memory loads with Tensor-Core compute.</p>

<p><strong>Round 4: 3.46 ms, 41.0% of reference</strong><br>
Idea: Given the pytorch code, replace the operation with a CUDA Kernel using implicit matmul. The given GEMM kernel could be helpful.<br>
<em>[author comment: since we know that the optimization involves using GEMM, we seeded the beginning of this round with an existing good GEMM kernel that we generated previously, and this idea is written manually]</em></p>

<p><strong>Round 5: 1.91 ms, 74.9% of reference</strong><br>
Idea: Precompute and reuse `k_idx`-decomposed kernel/input indices in shared memory within each K-tile loop to avoid redundant arithmetic.</p>

<p><strong>Round 6: 1.37 ms, 103.6% of reference</strong><br>
Idea: Precomputing and caching N-dimension GEMM indices in shared memory to reduce redundant arithmetic within the input data loading loop.</p>

<p><strong>Round 7: 1.38 ms, 102.9% of reference</strong><br>
Idea: Parallelize CUDA kernel output by using dedicated per-warp shared memory buffers to eliminate warp-wise serialization during writes to global memory.</p>

<p><strong>Round 8: 1.37 ms, 103.6% of reference</strong><br>
Idea: Precompute and cache base input coordinates in shared memory to reduce redundant arithmetic during input data loading.</p>

<p><strong>Round 9: 1.36 ms, 105.1% of reference</strong><br>
Idea: Software-pipeline B-fragment loading to overlap the next B-tile’s shared memory reads with the current B-tile’s WMMA computations.</p>

<p><strong>Round 10: 1.07 ms, 133.6% of reference</strong><br>
Idea: Reuse precomputed N-dimension GEMM decomposition from shared memory for output address calculation, avoiding redundant and costly division/modulo operations.</p>

<p><strong>Round 11: 1.21 ms, 117.4% of reference</strong><br>
Idea: Remove `hi/lo` decomposition in `half` WMMA operations, relying on standard FP16 accumulation to improve performance if the resulting accuracy is acceptable.</p>

<p><strong>Round 12: 1.01 ms, 141.2% of reference</strong><br>
Idea: Overlap K-loop global memory loads of `Asub` (weights) and `Bsub` (inputs) with MMA computation using double buffering, enabled by calculating K-dimension indices on-the-fly within the load stage of the pipeline.</p>

<p><strong>Round 13: 0.795 ms, 179.9% of reference</strong><br>
Idea: Implement vectorized shared memory writes for loading `Asub_pipe` and `Bsub_pipe` by using wider data types like `half2`</p>

<p><strong>Final Code Sample</strong><br>
The final code sample for the Conv2D kernel is included in the appendix. It uses advanced CUDA techniques that we find challenging to write ourselves!
We also have more example kernels in this <a href="https://github.com/ScalingIntelligence/good-kernels">Github repo</a></p>

<h2 id="takeaways">Takeaways</h2>

<p>Our method echoes a growing theme in AI research: combining strong reasoning with parallel exploration of multiple hypotheses leads to improvements. As some recent work (<a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf">AlphaEvolve</a>, <a href="https://x.com/GoogleDeepMind/status/1924881598102839373">Gemini 2.5 Pro Deep Think</a>) highlight, you might not always need massive retraining — sometimes, clever search and branching strategies can unlock scientific innovation and tackle complex problems, and there might be more gains through extensive searching with verifiers. <br>
However, this doesn’t mean we shouldn’t do further training. On the contrary, our approach also helps generate better synthetic data to improve future model training (this requires more problem instances). So, it’s both a powerful test-time scaling method and a step toward smarter, more data-efficient model development.</p>

<p>Finally, what we’ve demonstrated here is just an early sign of life. The optimization quality looks promising (it’s using many advanced strategies), but there’s plenty of room to improve, such as the generation of better optimization ideas, high quality resulting code, as well as applying this to increasingly complicated kernels. Two concrete examples that we are still actively working on improving are:</p>

<ul>
  <li>FP16 Matmul: 52% performance of torch.matmul</li>
  <li>FP16 Flash Attention: 9% performance of torch.nn.functional.scaled_dot_product_attention</li>
</ul>

<p>FP32 is less common in modern ML workloads and often less optimized on recent hardware compared to FP16 or BF16, which may partly explain why it’s easier to achieve performance gains over PyTorch with FP32 kernels.</p>

<p>Despite the current limitations, we’re optimistic. At the time of KernelBench, we couldn’t even generate functional versions of these two kernels above, and through searching we’ve been steadily increasing the performance of flash attention from &lt;1%, and note that we are working with a quite limited search budget here (around 3 million input tokens + 4 million output tokens in total). The progress since then gives us confidence in the potential for continual improvement, and we are excited to keep pushing the frontier of AI to create increasingly better kernels towards the eventual goal of self-improving AI systems.</p>

<h2 id="thanks">Thanks</h2>

<p>Christopher Rinard, Saman Amarasinghe, and Allen Nie for the helpful discussions; Standard Kernel Co. and Prime Intellect for supporting this work.</p>

<h2 id="appendix-fast-conv2d-kernel">Appendix: Fast Conv2D Kernel</h2>
<div><pre><code><span>import</span> <span>torch</span>
<span>import</span> <span>torch.nn</span> <span>as</span> <span>nn</span>
<span>import</span> <span>torch.nn.functional</span> <span>as</span> <span>F</span>
<span>from</span> <span>torch.utils.cpp_extension</span> <span>import</span> <span>load_inline</span>

<span>conv2d_implicit_gemm_cuda_source</span> <span>=</span> <span>r</span><span>"""
#include &lt;torch/extension.h&gt;
#include &lt;ATen/cuda/CUDAContext.h&gt; // For at::cuda::getCurrentCUDAStream()
#include &lt;mma.h&gt;
#include &lt;cuda_fp16.h&gt;

using namespace nvcuda;

// WMMA tile dimensions
#define WMMA_M 16
#define WMMA_N 16
#define WMMA_K 16

// Skew padding for shared memory to avoid bank conflicts
#define SKEW_HALF 8 // 8 half elements (16 bytes)

// CUDA built-in warpSize is 32 for supported architectures (sm_70+)
// This constant is used for host-side configuration (e.g. blockDim)
#define CUDA_WARP_SIZE_CONST 32 

// Threadblock configuration
#define WARPS_PER_BLOCK 8
// THREADS_PER_BLOCK must be evaluatable by host compiler for blockDim configuration
#define THREADS_PER_BLOCK (WARPS_PER_BLOCK * CUDA_WARP_SIZE_CONST) 

// Macro-tile dimensions computed by a threadblock
// BLOCK_M_TILES_WMMA * WMMA_M = output channels processed by a block
// BLOCK_N_TILES_WMMA * WMMA_N = output spatial elements processed by a block
#define BLOCK_M_TILES_WMMA 8
#define BLOCK_N_TILES_WMMA 8

#define TILE_M_PER_BLOCK (BLOCK_M_TILES_WMMA * WMMA_M) // e.g., 8 * 16 = 128 (for C_out dimension)
#define TILE_N_PER_BLOCK (BLOCK_N_TILES_WMMA * WMMA_N) // e.g., 8 * 16 = 128 (for N_batch * H_out * W_out dimension)

// Vector size for shared memory writes (half2)
#define VECTOR_SIZE_H2 2

// Struct to hold precomputed N-dimension GEMM indices
struct NDecomposed {
    int ow_eff;
    int oh_eff;
    int n_batch_idx;
    bool isValidPixel; // True if this pixel_idx is within N_gemm bounds
    int h_in_base; 
    int w_in_base; 
};

__global__ void conv2d_implicit_gemm_wmma_kernel(
    const float* __restrict__ input_ptr,    // Input: (N, Cin, Hin, Win)
    const float* __restrict__ weight_ptr,   // Weights: (Cout, Cin, Kh, Kw)
    const float* __restrict__ bias_ptr,     // Bias: (Cout) or nullptr
    float* __restrict__ output_ptr,         // Output: (N, Cout, Hout, Wout)
    const int N_batch, const int C_in, const int H_in, const int W_in,
    const int C_out, const int K_h, const int K_w,
    const int stride_h, const int stride_w,
    const int pad_h, const int pad_w,
    const int H_out, const int W_out,
    const int M_gemm, // C_out
    const int N_gemm, // N_batch * H_out * W_out
    const int K_gemm  // C_in * K_h * K_w
) {
    // Thread identification
    const int warp_id = threadIdx.x / warpSize;        // 0 .. WARPS_PER_BLOCK-1
    const int lane_id = threadIdx.x % warpSize;        // 0 .. 31 (or warpSize-1)

    // Top-left corner of the macro-tile this block is responsible for in GEMM terms
    const int block_row_gemm_start = TILE_M_PER_BLOCK * blockIdx.y;
    const int block_col_gemm_start = TILE_N_PER_BLOCK * blockIdx.x;

    // Shared memory for tiles of A (weights) and B (input/im2col) - Double Buffered for K-loop pipelining
    __shared__ half Asub_pipe[2][TILE_M_PER_BLOCK][WMMA_K + SKEW_HALF];
    __shared__ half Bsub_pipe[2][TILE_N_PER_BLOCK][WMMA_K + SKEW_HALF];

    // Shared memory for precomputed N-indices
    __shared__ NDecomposed n_params_sh[TILE_N_PER_BLOCK];

    // Shared memory for output stage (per-warp buffers)
    __shared__ float C_shmem_output_buffers[WARPS_PER_BLOCK][WMMA_M][WMMA_N];

    // Accumulator fragments per warp.
    wmma::fragment&lt;wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float&gt; acc_frag[BLOCK_N_TILES_WMMA];
    #pragma unroll
    for (int i = 0; i &lt; BLOCK_N_TILES_WMMA; ++i) {
        wmma::fill_fragment(acc_frag[i], 0.0f);
    }

    // Populate n_params_sh once at the beginning of the kernel
    if (threadIdx.x &lt; TILE_N_PER_BLOCK) {
        int r_b_tile_idx = threadIdx.x; 
        int current_pixel_idx = block_col_gemm_start + r_b_tile_idx;

        if (current_pixel_idx &lt; N_gemm) {
            n_params_sh[r_b_tile_idx].ow_eff = current_pixel_idx % W_out;
            int temp_div_wout = current_pixel_idx / W_out;
            n_params_sh[r_b_tile_idx].oh_eff = temp_div_wout % H_out;
            n_params_sh[r_b_tile_idx].n_batch_idx = temp_div_wout / H_out;
            n_params_sh[r_b_tile_idx].isValidPixel = true;

            n_params_sh[r_b_tile_idx].h_in_base = n_params_sh[r_b_tile_idx].oh_eff * stride_h - pad_h;
            n_params_sh[r_b_tile_idx].w_in_base = n_params_sh[r_b_tile_idx].ow_eff * stride_w - pad_w;
        } else {
            n_params_sh[r_b_tile_idx].isValidPixel = false;
            n_params_sh[r_b_tile_idx].ow_eff = 0; 
            n_params_sh[r_b_tile_idx].oh_eff = 0;
            n_params_sh[r_b_tile_idx].n_batch_idx = 0;
            n_params_sh[r_b_tile_idx].h_in_base = 0; 
            n_params_sh[r_b_tile_idx].w_in_base = 0;
        }
    }
    __syncthreads();

    // Constants for vectorized shared memory loading
    // Number of half2 elements along K-dim for a shared memory tile row
    const int NUM_H2_ELEMENTS_IN_K_DIM = WMMA_K / VECTOR_SIZE_H2;
    // Number of thread groups, where each group has NUM_H2_ELEMENTS_IN_K_DIM threads.
    // Each group is responsible for loading the K-dimension for one M-row (for A) or N-row (for B) at a time,
    // iterating over M-rows or N-rows with this step size.
    const int NUM_ROW_PROCESSING_GROUPS = THREADS_PER_BLOCK / NUM_H2_ELEMENTS_IN_K_DIM;


    // --- K-Loop Pipelining ---
    int num_k_tiles = (K_gemm + WMMA_K - 1) / WMMA_K;
    
    // --- Prologue: Load first k-tile (k_tile_iter = 0) into pipe_idx = 0 ---
    if (num_k_tiles &gt; 0) { 
        int k_tile_start_prologue = 0; 
        int current_pipe_idx_prologue = 0; 

        // Load Asub_pipe[0] for k_tile_iter = 0
        {
            // This thread is responsible for the 'h2_idx_in_k_dim_A'-th half2 element
            // in the K-dimension of the shared memory tile.
            int h2_idx_in_k_dim_A = threadIdx.x % NUM_H2_ELEMENTS_IN_K_DIM;
            // Starting 'half' index in shared memory for this half2 write.
            int shmem_k_start_for_h2_A = h2_idx_in_k_dim_A * VECTOR_SIZE_H2;

            // Global k-indices for the two half elements.
            int k_global_A_0 = k_tile_start_prologue + shmem_k_start_for_h2_A;
            int k_global_A_1 = k_tile_start_prologue + shmem_k_start_for_h2_A + 1;

            // Decompose k_global_A_0
            int kw_eff_reg_A_0 = 0, kh_eff_reg_A_0 = 0, ic_eff_reg_A_0 = 0;
            bool is_valid_k_A_0 = (k_global_A_0 &lt; K_gemm);
            if (is_valid_k_A_0) {
                kw_eff_reg_A_0 = k_global_A_0 % K_w;
                int temp_div_kw_A_0 = k_global_A_0 / K_w;
                kh_eff_reg_A_0 = temp_div_kw_A_0 % K_h;
                ic_eff_reg_A_0 = temp_div_kw_A_0 / K_h;
            }

            // Decompose k_global_A_1
            int kw_eff_reg_A_1 = 0, kh_eff_reg_A_1 = 0, ic_eff_reg_A_1 = 0;
            bool is_valid_k_A_1 = (k_global_A_1 &lt; K_gemm);
            if (is_valid_k_A_1) {
                kw_eff_reg_A_1 = k_global_A_1 % K_w;
                int temp_div_kw_A_1 = k_global_A_1 / K_w;
                kh_eff_reg_A_1 = temp_div_kw_A_1 % K_h;
                ic_eff_reg_A_1 = temp_div_kw_A_1 / K_h;
            }
            
            // This thread belongs to 'm_row_group_id_A'-th group of threads.
            // This group iterates over M-rows of the Asub_pipe tile.
            int m_row_group_id_A = threadIdx.x / NUM_H2_ELEMENTS_IN_K_DIM;
            for (int r_a_tile_base = m_row_group_id_A; r_a_tile_base &lt; TILE_M_PER_BLOCK; r_a_tile_base += NUM_ROW_PROCESSING_GROUPS) {
                int oc_idx = block_row_gemm_start + r_a_tile_base;
                float weight_val_0 = 0.0f;
                if (oc_idx &lt; C_out &amp;&amp; is_valid_k_A_0) {
                    weight_val_0 = weight_ptr[oc_idx * C_in * K_h * K_w +
                                              ic_eff_reg_A_0 * K_h * K_w +
                                              kh_eff_reg_A_0 * K_w +
                                              kw_eff_reg_A_0];
                }
                float weight_val_1 = 0.0f;
                if (oc_idx &lt; C_out &amp;&amp; is_valid_k_A_1) {
                    weight_val_1 = weight_ptr[oc_idx * C_in * K_h * K_w +
                                              ic_eff_reg_A_1 * K_h * K_w +
                                              kh_eff_reg_A_1 * K_w +
                                              kw_eff_reg_A_1];
                }
                half2* smem_ptr_h2_A = reinterpret_cast&lt;half2*&gt;(
                    &amp;Asub_pipe[current_pipe_idx_prologue][r_a_tile_base][shmem_k_start_for_h2_A]
                );
                *smem_ptr_h2_A = make_half2(__float2half(weight_val_0), __float2half(weight_val_1));
            }
        }

        // Load Bsub_pipe[0] for k_tile_iter = 0
        {
            int h2_idx_in_k_dim_B = threadIdx.x % NUM_H2_ELEMENTS_IN_K_DIM;
            int shmem_k_start_for_h2_B = h2_idx_in_k_dim_B * VECTOR_SIZE_H2;

            int k_global_B_0 = k_tile_start_prologue + shmem_k_start_for_h2_B;
            int k_global_B_1 = k_tile_start_prologue + shmem_k_start_for_h2_B + 1;

            int kw_eff_reg_B_0 = 0, kh_eff_reg_B_0 = 0, ic_eff_reg_B_0 = 0;
            bool is_valid_k_B_0 = (k_global_B_0 &lt; K_gemm);
            if (is_valid_k_B_0) {
                kw_eff_reg_B_0 = k_global_B_0 % K_w;
                int temp_div_kw_B_0 = k_global_B_0 / K_w;
                kh_eff_reg_B_0 = temp_div_kw_B_0 % K_h;
                ic_eff_reg_B_0 = temp_div_kw_B_0 / K_h;
            }

            int kw_eff_reg_B_1 = 0, kh_eff_reg_B_1 = 0, ic_eff_reg_B_1 = 0;
            bool is_valid_k_B_1 = (k_global_B_1 &lt; K_gemm);
            if (is_valid_k_B_1) {
                kw_eff_reg_B_1 = k_global_B_1 % K_w;
                int temp_div_kw_B_1 = k_global_B_1 / K_w;
                kh_eff_reg_B_1 = temp_div_kw_B_1 % K_h;
                ic_eff_reg_B_1 = temp_div_kw_B_1 / K_h;
            }

            int n_row_group_id_B = threadIdx.x / NUM_H2_ELEMENTS_IN_K_DIM;
            for (int r_b_tile_base = n_row_group_id_B; r_b_tile_base &lt; TILE_N_PER_BLOCK; r_b_tile_base += NUM_ROW_PROCESSING_GROUPS) {
                float input_val_0 = 0.0f;
                if (n_params_sh[r_b_tile_base].isValidPixel &amp;&amp; is_valid_k_B_0) {
                    const NDecomposed&amp; current_n_params = n_params_sh[r_b_tile_base];
                    int h_in_eff_0 = current_n_params.h_in_base + kh_eff_reg_B_0;
                    int w_in_eff_0 = current_n_params.w_in_base + kw_eff_reg_B_0;
                    if (h_in_eff_0 &gt;= 0 &amp;&amp; h_in_eff_0 &lt; H_in &amp;&amp; w_in_eff_0 &gt;= 0 &amp;&amp; w_in_eff_0 &lt; W_in) {
                        input_val_0 = input_ptr[current_n_params.n_batch_idx * C_in * H_in * W_in +
                                              ic_eff_reg_B_0 * H_in * W_in +
                                              h_in_eff_0 * W_in +
                                              w_in_eff_0];
                    }
                }
                float input_val_1 = 0.0f;
                 if (n_params_sh[r_b_tile_base].isValidPixel &amp;&amp; is_valid_k_B_1) {
                    const NDecomposed&amp; current_n_params = n_params_sh[r_b_tile_base];
                    int h_in_eff_1 = current_n_params.h_in_base + kh_eff_reg_B_1;
                    int w_in_eff_1 = current_n_params.w_in_base + kw_eff_reg_B_1;
                     if (h_in_eff_1 &gt;= 0 &amp;&amp; h_in_eff_1 &lt; H_in &amp;&amp; w_in_eff_1 &gt;= 0 &amp;&amp; w_in_eff_1 &lt; W_in) {
                        input_val_1 = input_ptr[current_n_params.n_batch_idx * C_in * H_in * W_in +
                                              ic_eff_reg_B_1 * H_in * W_in +
                                              h_in_eff_1 * W_in +
                                              w_in_eff_1];
                    }
                }
                half2* smem_ptr_h2_B = reinterpret_cast&lt;half2*&gt;(
                    &amp;Bsub_pipe[current_pipe_idx_prologue][r_b_tile_base][shmem_k_start_for_h2_B]
                );
                *smem_ptr_h2_B = make_half2(__float2half(input_val_0), __float2half(input_val_1));
            }
        }
    }


    // Loop over the K_gemm dimension in tiles of WMMA_K
    for (int k_tile_iter = 0; k_tile_iter &lt; num_k_tiles; ++k_tile_iter) {
        __syncthreads(); // Sync point for pipelining

        int compute_pipe_idx = k_tile_iter % 2;
        int load_pipe_idx = (k_tile_iter + 1) % 2;

        // --- Load Stage for next k-tile (k_tile_iter + 1) into load_pipe_idx ---
        int k_tile_start_for_load = (k_tile_iter + 1) * WMMA_K;
        if (k_tile_start_for_load &lt; K_gemm) { 
            // Load Asub_pipe[load_pipe_idx]
            { 
                int h2_idx_in_k_dim_A = threadIdx.x % NUM_H2_ELEMENTS_IN_K_DIM;
                int shmem_k_start_for_h2_A = h2_idx_in_k_dim_A * VECTOR_SIZE_H2;

                int k_global_A_0 = k_tile_start_for_load + shmem_k_start_for_h2_A;
                int k_global_A_1 = k_tile_start_for_load + shmem_k_start_for_h2_A + 1;

                int kw_eff_reg_A_0 = 0, kh_eff_reg_A_0 = 0, ic_eff_reg_A_0 = 0;
                bool is_valid_k_A_0 = (k_global_A_0 &lt; K_gemm);
                if (is_valid_k_A_0) {
                    kw_eff_reg_A_0 = k_global_A_0 % K_w;
                    int temp_div_kw_A_0 = k_global_A_0 / K_w;
                    kh_eff_reg_A_0 = temp_div_kw_A_0 % K_h;
                    ic_eff_reg_A_0 = temp_div_kw_A_0 / K_h;
                }

                int kw_eff_reg_A_1 = 0, kh_eff_reg_A_1 = 0, ic_eff_reg_A_1 = 0;
                bool is_valid_k_A_1 = (k_global_A_1 &lt; K_gemm);
                if (is_valid_k_A_1) {
                    kw_eff_reg_A_1 = k_global_A_1 % K_w;
                    int temp_div_kw_A_1 = k_global_A_1 / K_w;
                    kh_eff_reg_A_1 = temp_div_kw_A_1 % K_h;
                    ic_eff_reg_A_1 = temp_div_kw_A_1 / K_h;
                }
                
                int m_row_group_id_A = threadIdx.x / NUM_H2_ELEMENTS_IN_K_DIM;
                for (int r_a_tile_base = m_row_group_id_A; r_a_tile_base &lt; TILE_M_PER_BLOCK; r_a_tile_base += NUM_ROW_PROCESSING_GROUPS) {
                    int oc_idx = block_row_gemm_start + r_a_tile_base;
                    float weight_val_0 = 0.0f;
                    if (oc_idx &lt; C_out &amp;&amp; is_valid_k_A_0) {
                        weight_val_0 = weight_ptr[oc_idx * C_in * K_h * K_w +
                                                  ic_eff_reg_A_0 * K_h * K_w +
                                                  kh_eff_reg_A_0 * K_w +
                                                  kw_eff_reg_A_0];
                    }
                    float weight_val_1 = 0.0f;
                    if (oc_idx &lt; C_out &amp;&amp; is_valid_k_A_1) {
                        weight_val_1 = weight_ptr[oc_idx * C_in * K_h * K_w +
                                                  ic_eff_reg_A_1 * K_h * K_w +
                                                  kh_eff_reg_A_1 * K_w +
                                                  kw_eff_reg_A_1];
                    }
                    half2* smem_ptr_h2_A = reinterpret_cast&lt;half2*&gt;(
                        &amp;Asub_pipe[load_pipe_idx][r_a_tile_base][shmem_k_start_for_h2_A]
                    );
                    *smem_ptr_h2_A = make_half2(__float2half(weight_val_0), __float2half(weight_val_1));
                }
            } 

            // Load Bsub_pipe[load_pipe_idx]
            { 
                int h2_idx_in_k_dim_B = threadIdx.x % NUM_H2_ELEMENTS_IN_K_DIM;
                int shmem_k_start_for_h2_B = h2_idx_in_k_dim_B * VECTOR_SIZE_H2;

                int k_global_B_0 = k_tile_start_for_load + shmem_k_start_for_h2_B;
                int k_global_B_1 = k_tile_start_for_load + shmem_k_start_for_h2_B + 1;

                int kw_eff_reg_B_0 = 0, kh_eff_reg_B_0 = 0, ic_eff_reg_B_0 = 0;
                bool is_valid_k_B_0 = (k_global_B_0 &lt; K_gemm);
                if (is_valid_k_B_0) {
                    kw_eff_reg_B_0 = k_global_B_0 % K_w;
                    int temp_div_kw_B_0 = k_global_B_0 / K_w;
                    kh_eff_reg_B_0 = temp_div_kw_B_0 % K_h;
                    ic_eff_reg_B_0 = temp_div_kw_B_0 / K_h;
                }

                int kw_eff_reg_B_1 = 0, kh_eff_reg_B_1 = 0, ic_eff_reg_B_1 = 0;
                bool is_valid_k_B_1 = (k_global_B_1 &lt; K_gemm);
                if (is_valid_k_B_1) {
                    kw_eff_reg_B_1 = k_global_B_1 % K_w;
                    int temp_div_kw_B_1 = k_global_B_1 / K_w;
                    kh_eff_reg_B_1 = temp_div_kw_B_1 % K_h;
                    ic_eff_reg_B_1 = temp_div_kw_B_1 / K_h;
                }

                int n_row_group_id_B = threadIdx.x / NUM_H2_ELEMENTS_IN_K_DIM;
                for (int r_b_tile_base = n_row_group_id_B; r_b_tile_base &lt; TILE_N_PER_BLOCK; r_b_tile_base += NUM_ROW_PROCESSING_GROUPS) {
                    float input_val_0 = 0.0f;
                    if (n_params_sh[r_b_tile_base].isValidPixel &amp;&amp; is_valid_k_B_0) {
                        const NDecomposed&amp; current_n_params = n_params_sh[r_b_tile_base];
                        int h_in_eff_0 = current_n_params.h_in_base + kh_eff_reg_B_0;
                        int w_in_eff_0 = current_n_params.w_in_base + kw_eff_reg_B_0;
                        if (h_in_eff_0 &gt;= 0 &amp;&amp; h_in_eff_0 &lt; H_in &amp;&amp; w_in_eff_0 &gt;= 0 &amp;&amp; w_in_eff_0 &lt; W_in) {
                            input_val_0 = input_ptr[current_n_params.n_batch_idx * C_in * H_in * W_in +
                                                  ic_eff_reg_B_0 * H_in * W_in +
                                                  h_in_eff_0 * W_in +
                                                  w_in_eff_0];
                        }
                    }
                    float input_val_1 = 0.0f;
                    if (n_params_sh[r_b_tile_base].isValidPixel &amp;&amp; is_valid_k_B_1) {
                        const NDecomposed&amp; current_n_params = n_params_sh[r_b_tile_base];
                        int h_in_eff_1 = current_n_params.h_in_base + kh_eff_reg_B_1;
                        int w_in_eff_1 = current_n_params.w_in_base + kw_eff_reg_B_1;
                        if (h_in_eff_1 &gt;= 0 &amp;&amp; h_in_eff_1 &lt; H_in &amp;&amp; w_in_eff_1 &gt;= 0 &amp;&amp; w_in_eff_1 &lt; W_in) {
                            input_val_1 = input_ptr[current_n_params.n_batch_idx * C_in * H_in * W_in +
                                                  ic_eff_reg_B_1 * H_in * W_in +
                                                  h_in_eff_1 * W_in +
                                                  w_in_eff_1];
                        }
                    }
                    half2* smem_ptr_h2_B = reinterpret_cast&lt;half2*&gt;(
                        &amp;Bsub_pipe[load_pipe_idx][r_b_tile_base][shmem_k_start_for_h2_B]
                    );
                    *smem_ptr_h2_B = make_half2(__float2half(input_val_0), __float2half(input_val_1));
                }
            } 
        }

        // --- Compute Stage for current k-tile (k_tile_iter) using compute_pipe_idx ---
        int a_row_start_in_tile = warp_id * WMMA_M; 

        wmma::fragment&lt;wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major&gt; a_frag;
        wmma::load_matrix_sync(a_frag, &amp;Asub_pipe[compute_pipe_idx][a_row_start_in_tile][0], WMMA_K + SKEW_HALF);

        wmma::fragment&lt;wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major&gt; b_frag_inner_pipe[2];

        if (BLOCK_N_TILES_WMMA &gt; 0) {
            int b_col_start_in_tile_current = 0 * WMMA_N; 
            wmma::load_matrix_sync(b_frag_inner_pipe[0], &amp;Bsub_pipe[compute_pipe_idx][b_col_start_in_tile_current][0], WMMA_K + SKEW_HALF);
        }
        
        int current_inner_pipe_idx = 0;

        #pragma unroll
        for (int n_tile = 0; n_tile &lt; BLOCK_N_TILES_WMMA; ++n_tile) {
            int next_inner_pipe_idx = 1 - current_inner_pipe_idx;

            if (n_tile &lt; BLOCK_N_TILES_WMMA - 1) {
                int b_col_start_in_tile_next = (n_tile + 1) * WMMA_N;
                wmma::load_matrix_sync(b_frag_inner_pipe[next_inner_pipe_idx], &amp;Bsub_pipe[compute_pipe_idx][b_col_start_in_tile_next][0], WMMA_K + SKEW_HALF);
            }

            wmma::mma_sync(acc_frag[n_tile], a_frag, b_frag_inner_pipe[current_inner_pipe_idx], acc_frag[n_tile]);
            
            current_inner_pipe_idx = next_inner_pipe_idx;
        }
    }
    __syncthreads(); 

    // Store results from accumulator fragments to global memory
    #pragma unroll
    for (int n_tile = 0; n_tile &lt; BLOCK_N_TILES_WMMA; ++n_tile) {
        wmma::store_matrix_sync(&amp;C_shmem_output_buffers[warp_id][0][0], acc_frag[n_tile], WMMA_N, wmma::mem_row_major);

        for (int elem_idx_in_frag = lane_id; elem_idx_in_frag &lt; WMMA_M * WMMA_N; elem_idx_in_frag += warpSize) {
            int r_frag = elem_idx_in_frag / WMMA_N;
            int c_frag = elem_idx_in_frag % WMMA_N;

            int oc_idx = block_row_gemm_start + (warp_id * WMMA_M) + r_frag;
            
            int offset_in_block_N_processing = (n_tile * WMMA_N) + c_frag;

            if (oc_idx &lt; C_out &amp;&amp; offset_in_block_N_processing &lt; TILE_N_PER_BLOCK &amp;&amp; 
                n_params_sh[offset_in_block_N_processing].isValidPixel) {
                const NDecomposed&amp; current_n_params = n_params_sh[offset_in_block_N_processing];
                int ow_eff = current_n_params.ow_eff;
                int oh_eff = current_n_params.oh_eff;
                int n_batch_idx = current_n_params.n_batch_idx;

                float val = C_shmem_output_buffers[warp_id][r_frag][c_frag];

                if (bias_ptr != nullptr) {
                    val += bias_ptr[oc_idx];
                }

                output_ptr[n_batch_idx * C_out * H_out * W_out +
                           oc_idx * H_out * W_out +
                           oh_eff * W_out +
                           ow_eff] = val;
            }
        }
    }
}


torch::Tensor conv2d_implicit_gemm_cuda(
    torch::Tensor input, torch::Tensor weight, torch::Tensor bias,
    int N_batch, int C_in, int H_in, int W_in,
    int C_out, int K_h, int K_w,
    int stride_h, int stride_w, int pad_h, int pad_w,
    int H_out, int W_out) {

    TORCH_CHECK(input.device().is_cuda(), "Input must be a CUDA tensor");
    TORCH_CHECK(weight.device().is_cuda(), "Weight must be a CUDA tensor");
    TORCH_CHECK(input.dtype() == torch::kFloat32, "Input must be float32");
    TORCH_CHECK(weight.dtype() == torch::kFloat32, "Weight must be float32");
    if (bias.defined()) {
        TORCH_CHECK(bias.device().is_cuda(), "Bias must be a CUDA tensor");
        TORCH_CHECK(bias.dtype() == torch::kFloat32, "Bias must be float32");
        TORCH_CHECK(bias.dim() == 1 &amp;&amp; bias.size(0) == C_out, "Bias has wrong shape");
    }

    TORCH_CHECK(input.dim() == 4, "Input must be 4D");
    TORCH_CHECK(weight.dim() == 4, "Weight must be 4D");
    TORCH_CHECK(input.size(0) == N_batch, "Input N_batch mismatch");
    TORCH_CHECK(input.size(1) == C_in, "Input C_in mismatch");
    TORCH_CHECK(input.size(2) == H_in, "Input H_in mismatch");
    TORCH_CHECK(input.size(3) == W_in, "Input W_in mismatch");
    TORCH_CHECK(weight.size(0) == C_out, "Weight C_out mismatch");
    TORCH_CHECK(weight.size(1) == C_in, "Weight C_in mismatch");
    TORCH_CHECK(weight.size(2) == K_h, "Weight K_h mismatch");
    TORCH_CHECK(weight.size(3) == K_w, "Weight K_w mismatch");

    auto output = torch::zeros({N_batch, C_out, H_out, W_out}, input.options());

    const int M_gemm = C_out;
    const int N_gemm = N_batch * H_out * W_out;
    const int K_gemm = C_in * K_h * K_w;

    if (M_gemm == 0 || N_gemm == 0) { 
        return output;
    }
    if (K_gemm == 0) { 
         if (bias.defined()) { 
            output = output + bias.reshape({1, C_out, 1, 1});
        }
        return output; 
    }

    dim3 block_dim(THREADS_PER_BLOCK);
    dim3 grid_dim(
        (N_gemm + TILE_N_PER_BLOCK - 1) / TILE_N_PER_BLOCK, 
        (M_gemm + TILE_M_PER_BLOCK - 1) / TILE_M_PER_BLOCK  
    );

    const float* bias_ptr_data = bias.defined() ? bias.data_ptr&lt;float&gt;() : nullptr;

    cudaStream_t stream = at::cuda::getCurrentCUDAStream();
    conv2d_implicit_gemm_wmma_kernel&lt;&lt;&lt;grid_dim, block_dim, 0, stream&gt;&gt;&gt;(
        input.data_ptr&lt;float&gt;(),
        weight.data_ptr&lt;float&gt;(),
        bias_ptr_data,
        output.data_ptr&lt;float&gt;(),
        N_batch, C_in, H_in, W_in,
        C_out, K_h, K_w,
        stride_h, stride_w, pad_h, pad_w,
        H_out, W_out,
        M_gemm, N_gemm, K_gemm
    );
    
    AT_CUDA_CHECK(cudaGetLastError());

    return output;
}
"""</span>

<span>conv2d_implicit_gemm_cuda_declaration</span> <span>=</span> <span>r</span><span>"""
torch::Tensor conv2d_implicit_gemm_cuda(
    torch::Tensor input, torch::Tensor weight, torch::Tensor bias,
    int N_batch, int C_in, int H_in, int W_in,
    int C_out, int K_h, int K_w,
    int stride_h, int stride_w, int pad_h, int pad_w,
    int H_out, int W_out);
"""</span>

<span># JIT compile the CUDA kernel
</span><span>custom_conv2d_wmma_ops</span> <span>=</span> <span>load_inline</span><span>(</span>
    <span>name</span><span>=</span><span>"custom_conv2d_wmma_ops_optimized_k_pipe_vec_smem"</span><span>,</span> <span># Changed name to avoid collision
</span>    <span>cpp_sources</span><span>=</span><span>conv2d_implicit_gemm_cuda_declaration</span><span>,</span>
    <span>cuda_sources</span><span>=</span><span>conv2d_implicit_gemm_cuda_source</span><span>,</span>
    <span>functions</span><span>=</span><span>[</span><span>"conv2d_implicit_gemm_cuda"</span><span>],</span>
    <span>verbose</span><span>=</span><span>True</span><span>,</span> 
    <span>extra_cuda_cflags</span><span>=</span><span>[</span><span>"-arch=sm_70"</span><span>,</span> <span>"--use_fast_math"</span><span>,</span> <span>"-std=c++17"</span><span>]</span> 
<span>)</span>


<span>class</span> <span>ModelNew</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>num_classes</span><span>=</span><span>1000</span><span>):</span> <span># num_classes is part of original signature, kept for consistency
</span>        <span>super</span><span>(</span><span>ModelNew</span><span>,</span> <span>self</span><span>).</span><span>__init__</span><span>()</span>
        
        <span># Define Conv1 parameters (matching the original model)
</span>        <span>self</span><span>.</span><span>in_channels</span> <span>=</span> <span>3</span>
        <span>self</span><span>.</span><span>out_channels</span> <span>=</span> <span>96</span>
        <span>self</span><span>.</span><span>kernel_size_val</span> <span>=</span> <span>11</span> <span># Assuming square kernel
</span>        <span>self</span><span>.</span><span>stride_val</span> <span>=</span> <span>4</span>       <span># Assuming square stride
</span>        <span>self</span><span>.</span><span>padding_val</span> <span>=</span> <span>2</span>      <span># Assuming square padding
</span>
        <span># Create a temporary Conv2d layer to initialize weights and bias
</span>        <span>temp_conv</span> <span>=</span> <span>nn</span><span>.</span><span>Conv2d</span><span>(</span>
            <span>in_channels</span><span>=</span><span>self</span><span>.</span><span>in_channels</span><span>,</span> 
            <span>out_channels</span><span>=</span><span>self</span><span>.</span><span>out_channels</span><span>,</span> 
            <span>kernel_size</span><span>=</span><span>self</span><span>.</span><span>kernel_size_val</span><span>,</span> 
            <span>stride</span><span>=</span><span>self</span><span>.</span><span>stride_val</span><span>,</span> 
            <span>padding</span><span>=</span><span>self</span><span>.</span><span>padding_val</span><span>,</span>
            <span>bias</span><span>=</span><span>True</span> <span># nn.Conv2d has bias=True by default
</span>        <span>)</span>
        <span>self</span><span>.</span><span>conv1_weight</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>temp_conv</span><span>.</span><span>weight</span><span>.</span><span>detach</span><span>().</span><span>clone</span><span>())</span>
        <span>if</span> <span>temp_conv</span><span>.</span><span>bias</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>self</span><span>.</span><span>conv1_bias</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>temp_conv</span><span>.</span><span>bias</span><span>.</span><span>detach</span><span>().</span><span>clone</span><span>())</span>
        <span>else</span><span>:</span>
            <span># Correctly register 'conv1_bias' as None if not present
</span>            <span>self</span><span>.</span><span>register_parameter</span><span>(</span><span>'conv1_bias'</span><span>,</span> <span>None</span><span>)</span> 


        <span>self</span><span>.</span><span>custom_conv_op</span> <span>=</span> <span>custom_conv2d_wmma_ops</span><span>.</span><span>conv2d_implicit_gemm_cuda</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
        <span>N_batch</span> <span>=</span> <span>x</span><span>.</span><span>size</span><span>(</span><span>0</span><span>)</span>
        <span># C_in_runtime = x.size(1) # Should match self.in_channels
</span>        <span>H_in</span> <span>=</span> <span>x</span><span>.</span><span>size</span><span>(</span><span>2</span><span>)</span>
        <span>W_in</span> <span>=</span> <span>x</span><span>.</span><span>size</span><span>(</span><span>3</span><span>)</span>

        <span># Calculate output dimensions
</span>        <span>H_out</span> <span>=</span> <span>(</span><span>H_in</span> <span>+</span> <span>2</span> <span>*</span> <span>self</span><span>.</span><span>padding_val</span> <span>-</span> <span>self</span><span>.</span><span>kernel_size_val</span><span>)</span> <span>//</span> <span>self</span><span>.</span><span>stride_val</span> <span>+</span> <span>1</span>
        <span>W_out</span> <span>=</span> <span>(</span><span>W_in</span> <span>+</span> <span>2</span> <span>*</span> <span>self</span><span>.</span><span>padding_val</span> <span>-</span> <span>self</span><span>.</span><span>kernel_size_val</span><span>)</span> <span>//</span> <span>self</span><span>.</span><span>stride_val</span> <span>+</span> <span>1</span>
        
        <span># Bias tensor handling: pass an undefined tensor if bias is None.
</span>        <span># The C++ TORCH_CHECK(bias.defined()) handles this by providing nullptr to kernel.
</span>        <span>bias_tensor</span> <span>=</span> <span>self</span><span>.</span><span>conv1_bias</span> <span>if</span> <span>self</span><span>.</span><span>conv1_bias</span> <span>is</span> <span>not</span> <span>None</span> <span>else</span> <span>torch</span><span>.</span><span>Tensor</span><span>()</span>


        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>custom_conv_op</span><span>(</span>
            <span>x</span><span>,</span> <span>self</span><span>.</span><span>conv1_weight</span><span>,</span> <span>bias_tensor</span><span>,</span>
            <span>N_batch</span><span>,</span> <span>self</span><span>.</span><span>in_channels</span><span>,</span> <span>H_in</span><span>,</span> <span>W_in</span><span>,</span>
            <span>self</span><span>.</span><span>out_channels</span><span>,</span> <span>self</span><span>.</span><span>kernel_size_val</span><span>,</span> <span>self</span><span>.</span><span>kernel_size_val</span><span>,</span> <span># K_h, K_w
</span>            <span>self</span><span>.</span><span>stride_val</span><span>,</span> <span>self</span><span>.</span><span>stride_val</span><span>,</span> <span># stride_h, stride_w
</span>            <span>self</span><span>.</span><span>padding_val</span><span>,</span> <span>self</span><span>.</span><span>padding_val</span><span>,</span> <span># pad_h, pad_w
</span>            <span>H_out</span><span>,</span> <span>W_out</span>
        <span>)</span>
        <span>return</span> <span>x</span>
</code></pre></div>

  </div>


			</div>
		






	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mary Meeker's first Trends report since 2019, focused on AI (141 pts)]]></title>
            <link>https://www.bondcap.com/reports/tai</link>
            <guid>44139403</guid>
            <pubDate>Fri, 30 May 2025 19:53:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bondcap.com/reports/tai">https://www.bondcap.com/reports/tai</a>, See on <a href="https://news.ycombinator.com/item?id=44139403">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Cap: Lightweight, modern open-source CAPTCHA alternative using proof-of-work (135 pts)]]></title>
            <link>https://capjs.js.org/</link>
            <guid>44137867</guid>
            <pubDate>Fri, 30 May 2025 16:36:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://capjs.js.org/">https://capjs.js.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44137867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-9a6c75ad="" data-v-e07eaea7="" id="VPContent" data-v-d8b57b2d=""><!--[--><!--]--><div data-v-dd8814ff="" data-v-e07eaea7=""><div data-v-dd8814ff=""><!--[--><!--]--><!--[--><h2 data-v-dd8814ff=""><span data-v-dd8814ff="">Cap</span><span data-v-dd8814ff="">A modern, lightning-quick PoW captcha</span></h2><p data-v-dd8814ff="">Cap is a lightweight, modern open-source CAPTCHA alternative using SHA-256 proof-of-work</p><!--]--><!--[--><!--]--><!--[--><!--]--></div><div data-v-dd8814ff=""><!--[--><!--[--><p><img src="https://capjs.js.org/logo.png" alt="VitePress" data-v-ab19afbb=""></p><!--]--><!--]--></div></div><!--[--><!--]--><!--[--><!--]--><div data-v-b1eea84a="" data-v-e07eaea7=""><!--[--><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>⚡️</p><h2 data-v-bd37d1a2="">250x smaller than hCaptcha</h2><p data-v-bd37d1a2="">Cap's widget library is extremely small, only ~20kb minified (including WASM)</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🔒️</p><h2 data-v-bd37d1a2="">Private</h2><p data-v-bd37d1a2="">Cap's usage of proof-of-work eliminates the need for any tracking, fingerprinting or data collection</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🌈</p><h2 data-v-bd37d1a2="">Fully customizable</h2><p data-v-bd37d1a2="">Cap is self-hostable so you can customize both the backend &amp; frontend (or you can just use CSS variables)</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🤖</p><h2 data-v-bd37d1a2="">PoW-based</h2><p data-v-bd37d1a2="">Cap uses PoW instead of complex puzzles, making it easier for humans and harder for bots</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🧩</p><h2 data-v-bd37d1a2="">Standalone mode</h2><p data-v-bd37d1a2="">Cap offers a standalone mode with Docker, allowing you to use it with languages other than JS</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>💨</p><h2 data-v-bd37d1a2="">Invisible mode</h2><p data-v-bd37d1a2="">Cap can run invisibly in the background using a simple JS API</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>☁️</p><h2 data-v-bd37d1a2="">Floating mode</h2><p data-v-bd37d1a2="">Floating mode keeps your CAPTCHA hidden until it's needed</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🌳</p><h2 data-v-bd37d1a2="">Fully FOSS</h2><p data-v-bd37d1a2="">Completely open source under the Apache 2.0 license</p><!----></article><!--]--></div><!--]--></div><!--[--><!--]--><div data-v-e07eaea7="" data-v-c141a4bd=""><h2 id="what-is-cap" tabindex="-1">What is Cap? <a href="#what-is-cap" aria-label="Permalink to &quot;What is Cap?&quot;">​</a></h2><p>Cap is a lightweight, modern open-source CAPTCHA alternative using SHA-256 proof-of-work. It's fast, private, and extremely simple to integrate. <a href="https://capjs.js.org/guide/effectiveness.html">Learn more about proof-of-work here.</a></p><p>Cap is built into 2 main parts:</p><ul><li><p><strong><a href="https://capjs.js.org/guide/widget.html" target="_blank" rel="noreferrer">@cap.js/widget</a></strong>: A small JavaScript library that renders the CAPTCHA and handles solving it using Web Workers and WASM.</p></li><li><p><strong><a href="https://capjs.js.org/guide/server.html" target="_blank" rel="noreferrer">@cap.js/server</a></strong>: An extremely simple, zero-dependencies library that handles creating and validating challenges.</p></li></ul><p>There are also some other helpful packages:</p><ul><li><p><strong><a href="https://capjs.js.org/guide/solver.html" target="_blank" rel="noreferrer">@cap.js/solver</a></strong>: Server-side solver for the CAPTCHA in case you want to use machine-to-machine.</p></li><li><p><strong><a href="https://capjs.js.org/guide/cli.html" target="_blank" rel="noreferrer">@cap.js/cli</a></strong>: Command-line interface for solving CAPTCHAs made with Cap. It's mainly designed for testing and when you need to solve these CAPTCHAs in a browser without JavaScript support.</p></li><li><p><strong><a href="https://capjs.js.org/guide/standalone.html" target="_blank" rel="noreferrer">Standalone mode</a></strong>: Docker image that helps you use Cap with any language or framework. It runs a simple REST API that can be used to create and validate challenges and an interactive UI to manage your keys.</p></li><li><p><strong>@cap.js/wasm</strong>: WASM solvers for Node and Web built with Rust.</p></li></ul><p>We also provide a middleware for a Cloudflare browser checkpoint-like experience:</p><ul><li><a href="https://capjs.js.org/guide/middleware/hono.html" target="_blank" rel="noreferrer">@cap.js/checkpoint-hono</a></li><li><a href="https://capjs.js.org/guide/middleware/express.html" target="_blank" rel="noreferrer">@cap.js/checkpoint-express</a></li><li><a href="https://capjs.js.org/guide/middleware/elysia.html" target="_blank" rel="noreferrer">@cap.js/middleware-elysia</a></li><li>more coming soon!</li></ul><p>It's designed to be a drop-in replacement for existing CAPTCHA solutions, with a focus on performance and UX.</p><p>Cap is built with JavaScript, runs on any JS runtime (Bun, Node.js, Deno), and has no dependencies. If you're not using any JS runtime, you can also use the standalone mode with Docker, which relies entirely on a simple REST API to create and validate challenges.</p><h2 id="why-cap" tabindex="-1">Why Cap? <a href="#why-cap" aria-label="Permalink to &quot;Why Cap?&quot;">​</a></h2><ul><li><strong>250x smaller than hCaptcha</strong><br><code>@cap.js/widget</code> is extremely small, only 12kb minified and brotli'd.</li><li><strong>Private</strong><br> Cap's usage of proof-of-work eliminates the need for any tracking, fingerprinting or data collection.</li><li><strong>Fully customizable</strong><br> Cap's self-hostable so you can customize both the backend &amp; frontend — or you can just use CSS variables</li><li><strong>Proof-of-work</strong><br> Cap uses proof-of-work instead of complex puzzles, making it easier for humans and harder for bots</li><li><strong>Standalone mode</strong><br> Cap offers a standalone mode with Docker, allowing you to use it with languages other than JS.</li><li><strong>Invisible mode</strong><br> Cap can run invisibly in the background using a simple JS API.</li><li><strong>Floating mode</strong><br> Cap's floating mode keeps your CAPTCHA hidden until it's needed.</li><li><strong>Fully open-source</strong><br> Completely open source under the Apache license 2.0 license.</li></ul><p>It's ideal for:</p><ul><li>Protecting APIs from bots</li><li>Preventing spam on forms</li><li>Blocking automated login attempts</li><li>Securing free-tier abuse</li></ul><h2 id="feature-comparison" tabindex="-1">Feature comparison <a href="#feature-comparison" aria-label="Permalink to &quot;Feature comparison&quot;">​</a></h2><table tabindex="0"><thead><tr><th>CAPTCHA</th><th>Open-source</th><th>Free</th><th>Private</th><th>Fast to solve</th><th>Easy for humans</th><th>Small error rate</th><th>Checkpoint support</th><th>GDPR/CCPA Compliant</th><th>Customizable</th><th>Hard for bots</th><th>Easy to integrate</th></tr></thead><tbody><tr><td><strong>Cap</strong></td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>🟨</td><td>✅</td></tr><tr><td>Cloudflare Turnstile</td><td>❌</td><td>✅</td><td>🟨</td><td>🟨</td><td>✅</td><td>❌</td><td>🟨</td><td>✅</td><td>❌</td><td>🟨</td><td>✅</td></tr><tr><td>reCAPTCHA</td><td>❌</td><td>🟨</td><td>❌</td><td>✅</td><td>❌</td><td>🟨</td><td>❌</td><td>🟨</td><td>❌</td><td>❌</td><td>✅</td></tr><tr><td>hCAPTCHA</td><td>❌</td><td>🟨</td><td>🟨</td><td>❌</td><td>❌</td><td>🟨</td><td>❌</td><td>🟨</td><td>❌</td><td>🟨</td><td>✅</td></tr><tr><td>Altcha</td><td>✅</td><td>✅</td><td>✅</td><td>🟨</td><td>✅</td><td>✅</td><td>❌</td><td>✅</td><td>✅</td><td>🟨</td><td>🟨</td></tr><tr><td>FriendlyCaptcha</td><td>❌</td><td>❌</td><td>✅</td><td>🟨</td><td>✅</td><td>✅</td><td>❌</td><td>✅</td><td>✅</td><td>🟨</td><td>🟨</td></tr><tr><td>MTCaptcha</td><td>❌</td><td>🟨</td><td>🟨</td><td>❌</td><td>❌</td><td>🟨</td><td>❌</td><td>✅</td><td>❌</td><td>❌</td><td>🟨</td></tr><tr><td>GeeTest</td><td>❌</td><td>❌</td><td>❌</td><td>🟨</td><td>🟨</td><td>🟨</td><td>❌</td><td>✅</td><td>❌</td><td>🟨</td><td>🟨</td></tr><tr><td>Arkose Labs</td><td>❌</td><td>❌</td><td>❌</td><td>❌</td><td>❌</td><td>❌</td><td>❌</td><td>✅</td><td>🟨</td><td>❌</td><td>❌</td></tr></tbody></table><h2 id="alternatives" tabindex="-1">Alternatives <a href="#alternatives" aria-label="Permalink to &quot;Alternatives&quot;">​</a></h2><p>Cap is a modern alternative to:</p><ul><li><a href="https://www.google.com/recaptcha/about/" target="_blank" rel="noreferrer">reCAPTCHA</a></li><li><a href="https://www.hcaptcha.com/" target="_blank" rel="noreferrer">hCaptcha</a></li><li><a href="https://developers.cloudflare.com/turnstile/" target="_blank" rel="noreferrer">Cloudflare Turnstile</a></li></ul><p>But unlike them, Cap is <a href="https://capjs.js.org/guide/workings.html"><strong>computation-bound, not tracking-bound</strong></a>.</p><p><a href="https://capjs.js.org/guide/alternatives.html">Read more about alternatives</a></p><h2 id="license" tabindex="-1">License <a href="#license" aria-label="Permalink to &quot;License&quot;">​</a></h2><p>Cap is licensed under the Apache License 2.0.</p><hr><p><a href="https://www.bestpractices.dev/projects/9920" target="_blank" rel="noreferrer"><img src="https://www.bestpractices.dev/projects/9920/badge" alt="OpenSSF Best Practices" loading="lazy"></a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beating Google's kernelCTF PoW using AVX512 (286 pts)]]></title>
            <link>https://anemato.de/blog/kctf-vdf</link>
            <guid>44137715</guid>
            <pubDate>Fri, 30 May 2025 16:19:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anemato.de/blog/kctf-vdf">https://anemato.de/blog/kctf-vdf</a>, See on <a href="https://news.ycombinator.com/item?id=44137715">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><header><a aria-label="anematode" href="https://anemato.de/"></a></header><main><section><article><div><header><div><dl><p><dt>Published on</dt><dd><time datetime="2025-05-29T00:00:00.000Z">Wednesday, May 28, 2025</time></dd></p></dl></div></header><div><dl><dt>Authors</dt><dd><ul><li><img alt="avatar" loading="lazy" width="38" height="38" decoding="async" data-nimg="1" srcset="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Favatar.png&amp;w=48&amp;q=75 1x, https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Favatar.png&amp;w=96&amp;q=75 2x" src="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Favatar.png&amp;w=96&amp;q=75"><dl><dt>Name</dt><dd>Timothy Herchen</dd><dt>Twitter</dt><dd></dd></dl></li></ul></dd></dl><div><h2 id="introduction">Introduction</h2><p>In May 2025, my <a target="_blank" rel="noopener noreferrer" href="https://cor.team/">Crusaders of Rust</a> teammates William Liu (<a target="_blank" rel="noopener noreferrer" href="https://willsroot.io/">FizzBuzz101</a>) and Savy Dicanosa (<a target="_blank" rel="noopener noreferrer" href="https://syst3mfailure.io/">Syst3mFailure</a>) discovered and developed an exploit of a use-after-free bug in Linux's packet scheduler. <a target="_blank" rel="noopener noreferrer" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=ac9fe7dd8e730a103ae4481147395cc73492d786">The bugfix patch</a> contains additional details. William found this bug while fuzzing Linux for his master's thesis, which I will link here upon its publication. (Congratulations, William!)</p><p>They wanted to submit the bug to Google's <a target="_blank" rel="noopener noreferrer" href="https://google.github.io/security-research/kernelctf/rules.html">kernelCTF</a> competition for an anticipated $51,000 bounty.<sup><a href="#user-content-fn-bounty" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-bounty">1</a></sup> Unfortunately, finding the bug and writing the exploit was only the first part of the battle. This post documents my small but unique contribution to our ultimately winning the bounty.</p><h2 id="setting-the-stage">Setting the stage</h2><p>To avoid paying out lots of money, kernelCTF organizers limit the number of submissions eligible for a bounty. Every two weeks at noon UTC, the submission window opens. Only the first team who is able to connect to and exploit the server, and submit the flag to a Google Form, receives a payout; any subsequent submissions are marked as duplicates. Furthermore, to prevent excessive submissions, the connecting to kernelCTF server requires solving a "proof of work"—a function which, by design, takes a few seconds to evaluate.</p><p>In summary, <strong>the submission process has these steps</strong>:</p><ol><li>At 12:00:00 UTC, connect to the kernelCTF server.</li><li>Solve the proof of work, which takes roughly 4 seconds.</li><li>Wait for the instance to boot. (Roughly 2.5 seconds.)</li><li>Upload the exploit and run it to secure the flag. (Time elapsed depends on the exploit. Savy optimized this one to take roughly 0.55 seconds without sacrificing reliability. Wow!)</li><li>Submit the flag to a Google Form. The submission timestamp determines the winner of the "slot".</li></ol><p>Our goal was to complete all these steps in sequence, faster than all the other teams.</p><h2 id="enter-the-sweats">Enter the sweats</h2><p>Because of the large bounties, over time professional vulnerability research teams have aggressively optimized their submission process. For the May 2, 2025, submission window preceding ours, the first team to submit the flag <a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/spreadsheets/d/e/2PACX-1vS1REdTA29OJftst8xN5B5x8iIUcxuK6bXdzF8G1UXCmRtoNsoQ9MbebdRdFnj6qZ0Yd7LwQfvYC2oF/pubhtml">did so 4.5 seconds after noon</a>!</p><p><img alt="kernelCTF submission time" loading="lazy" width="1518" height="156" decoding="async" data-nimg="1" srcset="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Fkctf-fast-submissions.png&amp;w=1920&amp;q=75 1x, https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Fkctf-fast-submissions.png&amp;w=3840&amp;q=75 2x" src="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Fkctf-fast-submissions.png&amp;w=3840&amp;q=75"></p><p>The numbers don't seem to add up: even assuming an instant exploit and form submission, the VM boot time and proof of work already take 6.5 seconds. Looking closer, we see that the time at which the winning submission's flag was generated (highlighted in red) is one second <em>before</em> noon UTC. Yet, the timestamp is generated <em>after</em> the proof of work is solved. Did sweaty CTFers invent time travel?</p><p>Alas! Because of a rounding quirk in the kernelCTF server code (<a target="_blank" rel="noopener noreferrer" href="https://github.com/google/security-research/blob/90cc1d1fe4d4626d4c0aba4a78c02fc72fe18ac7/kernelctf/server/server.py#L192">here</a>), the VM instance actually boots at 11:59:59—so no time travel. Still, the timestamp indicates that the winning team solved the proof of work in less than a second! How could this be?</p><p>We don't know for certain, but one kernelCTF organizer postulated that they were using <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">field-programmable gate arrays</a> (FPGAs). FPGAs are custom silicon that can perform specific tasks extremely quickly, to the exclusion of general-purpose tasks. They are not only fairly expensive, but also tricky to program. If the professional team had access to an FPGA programmed to perform the proof of work, a sub-second proof of work time was conceivable.</p><p>On May 13, William messaged me on Discord seeking advice on how to optimize the proof of work so that we could preempt the competition. I had to act fast: The next submission window would open at 5 a.m. PST, May 16.<sup><a href="#user-content-fn-thesis" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-thesis">2</a></sup></p><h2 id="the-proof-of-work-the-sloth-vdf">The proof of work: The "sloth" VDF</h2><p>The proof of work (<a target="_blank" rel="noopener noreferrer" href="https://github.com/google/kctf/blob/v1/docker-images/challenge/pow.py">implemented here</a>) is a certain <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/Verifiable_delay_function">verifiable delay function</a> (VDF) known as "sloth". VDFs are cryptographic primitives which prove that a nontrivial amount of time has passed by requiring a long, serial computation. This computation outputs a proof which can be (relatively) quickly verified. Because the computation is serial, scaling to more computational resources (such as more CPU or GPU cores) does not reduce the runtime.<sup><a href="#user-content-fn-hash" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-hash">3</a></sup></p><p>The sloth VDF was introduced by <a target="_blank" rel="noopener noreferrer" href="https://csrc.nist.gov/csrc/media/events/workshop-on-elliptic-curve-cryptography-standards/documents/papers/session1-wesolowski-paper.pdf">Lenstra and Wesolowski (2015)</a>. I won't reproduce the number theory behind sloth (see page 4 of the paper for that), but to summarize matters, the function we must optimize boils down to:</p><div><pre><code><span><span>def</span> <span>sloth_root</span><span>(</span>x<span>,</span> difficulty<span>=</span><span>7337</span><span>)</span><span>:</span>
</span><span>    <span>for</span> i <span>in</span> <span>range</span><span>(</span>difficulty<span>)</span><span>:</span>             <span># repeat the inner kernel this many times</span>
</span><span>        <span>for</span> j <span>in</span> <span>range</span><span>(</span><span>1277</span><span>)</span><span>:</span>               <span># square x this many times</span>
</span><span>            x <span>=</span> <span>(</span>x <span>*</span> x<span>)</span> <span>%</span> <span>(</span><span>2</span> <span>**</span> <span>1279</span> <span>-</span> <span>1</span><span>)</span>   <span># modulus is a Mersenne number</span>
</span><span>        x <span>=</span> x<span>.</span>bit_flip<span>(</span><span>0</span><span>)</span>                   <span># complement the LSB of x</span>
</span><span>    <span>return</span> <span>int</span><span>(</span>x<span>)</span>
</span></code></pre></div><p>where <em>x</em> is a supplied 1280-bit integer. The <em>difficulty</em> variable linearly controls how long the VDF takes to solve.</p><p>Google's reference implementation uses gmpy, which is a Python binding to the venerable <a target="_blank" rel="noopener noreferrer" href="https://gmplib.org/">GNU Multiprecision Library</a> (GMP). GMP's addition and multiplication kernels are handwritten in assembly for each target platform (<a target="_blank" rel="noopener noreferrer" href="https://github.com/gmp-mirror/gmp/blob/master/mpn/x86_64/mulx/adx/addmul_1.asm">example</a>). The loop-carried dependency of <em>x</em> means that the computation is inherently serial, so throwing more cores at the problem—at least in a naïve way—is unhelpful. Meaningfully speeding up this function was going to be <em>tough</em>.</p><h2 id="initial-progress">Initial progress</h2><p>I set out on the obvious goal of optimizing the 1280-bit modular squaring (line 4 in the code above). The first success was mathematical: Because the modulus is a Mersenne number of length 1279 bits, and the intermediate product is 2 · 1280 = 2560 bits, computing the residue actually corresponds to a handful of cheaper operations:</p><div><pre><code><span><span>def</span> <span>mod_2_1279_minus_1</span><span>(</span>x<span>)</span><span>:</span>    <span># compute x % (2 ** 1279 - 1)</span>
</span><span>    p <span>=</span> <span>2</span> <span>**</span> <span>1279</span> <span>-</span> <span>1</span>
</span><span>    r <span>=</span> <span>(</span>x <span>&amp;</span> p<span>)</span> <span>+</span> <span>(</span>x <span>&gt;&gt;</span> <span>1279</span><span>)</span>
</span><span>    <span>if</span> r <span>&gt;=</span> p<span>:</span>
</span><span>        r <span>-=</span> p
</span><span>    <span>return</span> r
</span></code></pre></div><p>I also translated the function to C++ to remove FFI overhead. The newly optimized code:</p><div><pre><code><span><span>constexpr</span> <span>int</span> MERSENNE_EXP <span>=</span> <span>1279</span><span>;</span>
</span><span>
</span><span>mpz_t low<span>,</span> high<span>,</span> p<span>;</span>
</span><span>
</span><span><span>void</span> <span>mpz_mod_mersenne</span><span>(</span>mpz_t r<span>,</span> <span>const</span> mpz_t x<span>)</span> <span>{</span>
</span><span>    <span>// p = 2^n - 1</span>
</span><span>    <span>mpz_mod_2exp</span><span>(</span>low<span>,</span> x<span>,</span> MERSENNE_EXP<span>)</span><span>;</span>
</span><span>    <span>mpz_fdiv_q_2exp</span><span>(</span>high<span>,</span> x<span>,</span> MERSENNE_EXP<span>)</span><span>;</span>
</span><span>    <span>mpz_add</span><span>(</span>r<span>,</span> low<span>,</span> high<span>)</span><span>;</span>
</span><span>    <span>if</span> <span>(</span><span>mpz_cmp</span><span>(</span>r<span>,</span> p<span>)</span> <span>&gt;=</span> <span>0</span><span>)</span> <span>{</span>
</span><span>        <span>mpz_sub</span><span>(</span>r<span>,</span> r<span>,</span> p<span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>bool</span> <span>init</span><span>(</span><span>)</span> <span>{</span>
</span><span>    <span>mpz_inits</span><span>(</span>low<span>,</span> high<span>,</span> p<span>,</span> <span>NULL</span><span>)</span><span>;</span>
</span><span>    <span>mpz_ui_pow_ui</span><span>(</span>p<span>,</span> <span>2</span><span>,</span> MERSENNE_EXP<span>)</span><span>;</span>
</span><span>    <span>mpz_sub_ui</span><span>(</span>p<span>,</span> p<span>,</span> <span>1</span><span>)</span><span>;</span>
</span><span>    <span>return</span> <span>true</span><span>;</span>
</span><span><span>}</span>
</span><span><span>bool</span> _unused <span>=</span> <span>init</span><span>(</span><span>)</span><span>;</span>
</span><span>
</span><span><span>void</span> <span>the_powmod</span><span>(</span>mpz_t x<span>)</span> <span>{</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>1277</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>        <span>mpz_mul</span><span>(</span>x<span>,</span> x<span>,</span> x<span>)</span><span>;</span>
</span><span>        <span>mpz_mod_mersenne</span><span>(</span>x<span>,</span> x<span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>int</span> <span>main</span><span>(</span><span>)</span>
</span><span><span>{</span>
</span><span>    <span>const</span> <span>int</span> diff <span>=</span> <span>7337</span><span>;</span>
</span><span>    mpz_t x<span>,</span> r<span>;</span>
</span><span>    <span>mpz_inits</span><span>(</span>x<span>,</span> r<span>,</span> <span>NULL</span><span>)</span><span>;</span>
</span><span>    <span>mpz_set_str</span><span>(</span>x<span>,</span> <span>"96729140485950483920373592475530255430"</span><span>,</span> <span>10</span><span>)</span><span>;</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> diff<span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>        <span>the_powmod</span><span>(</span>x<span>)</span><span>;</span>
</span><span>        <span>mpz_combit</span><span>(</span>x<span>,</span> <span>0</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span>    <span>char</span> <span>*</span>str <span>=</span> <span>mpz_get_str</span><span>(</span><span>NULL</span><span>,</span> <span>10</span><span>,</span> x<span>)</span><span>;</span>
</span><span>    std<span>::</span>cout <span>&lt;&lt;</span> <span>"x: "</span> <span>&lt;&lt;</span> str <span>&lt;&lt;</span> std<span>::</span>endl<span>;</span>
</span><span>    <span>return</span> <span>0</span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div><p>This code ran in 1.9 seconds on my M1 Macbook Pro—a substantial improvement, and faster than similarly optimized solvers like <a target="_blank" rel="noopener noreferrer" href="https://github.com/Aplet123/kctf-pow">this one written in Rust</a>.<sup><a href="#user-content-fn-rust" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-rust">4</a></sup> William linked GMP statically, which would presumably allow some inlining, and reported a further speedup—roughly 1.4 seconds on a fancy Intel Ice Lake laptop. Not bad, but still not a guaranteed win.</p><p>The modulus no longer being a bottleneck, I considered handwriting multiplication kernels in assembly to take advantage of the multiplication being a fixed width of 1280-bit × 1280-bit → 2560-bit; the factors fit neatly into twenty 64-bit limbs, and the product fits in forty limbs.<sup><a href="#user-content-fn-limbs" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-limbs">5</a></sup> GMP's assembly kernels are generic in the bitwidth, which introduces some overhead. Unfortunately, at 1.4 seconds we were approaching the theoretical limit of multiplication throughput, which is one 64-bit × 64-bit → 128-bit multiplication per cycle on all recent hardware.<sup><a href="#user-content-fn-uops" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-uops">6</a></sup></p><h2 id="enter-avx512">Enter AVX512</h2><p><a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/AVX-512">AVX512</a> is Intel's extension to the x86 ISA, first made available in 2016. It is a comprehensive overhaul of x86 SIMD programming, doubling the number and width of vector registers, adding <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Scalable_Vector_Extension">Scalable Vector Extension</a>–style mask predication, and hundreds of new instructions. It also has a troubled history: Linus Torvalds <a target="_blank" rel="noopener noreferrer" href="https://www.realworldtech.com/forum/?threadid=193189&amp;curpostid=193190">famously</a> wished it to "die a horrible death", and despite supporting it on consumer CPUs for several generations, Intel disabled support for it starting with the Alder Lake µarch (2021); support continues in the server space. Meanwhile, AMD implemented AVX512 in their Zen 4 (2022) and Zen 5 µarches for both consumer and server CPUs.</p><p>Of present interest is the AVX512 Integer Fused Multiply–Add extension (AVX512IFMA), which was introduced specifically to speed up big-integer arithmetic—see, <em>e.g.</em>, <a target="_blank" rel="noopener noreferrer" href="https://builders.intel.com/docs/networkbuilders/intel-avx-512-fast-modular-multiplication-technique-technology-guide-1710916893.pdf">Ozturk, Kantecki &amp; Yap (2024)</a>. I learned about AVX512IFMA during my competitive programming arc, optimizing submissions for <a target="_blank" rel="noopener noreferrer" href="https://judge.yosupo.jp/">judge.yosupo.jp</a>. The extension introduces two new instructions which operate on vector registers:</p><ul><li><a target="_blank" rel="noopener noreferrer" href="https://www.felixcloutier.com/x86/vpmadd52luq">vpmadd52luq</a> – Packed Multiply of Unsigned 52-Bit Unsigned Integers and Add Low 52-Bit Products to 64-Bit Accumulators</li><li><a target="_blank" rel="noopener noreferrer" href="https://www.felixcloutier.com/x86/vpmadd52huq">vpmadd52huq</a> – Packed Multiply of Unsigned 52-Bit Unsigned Integers and Add High 52-Bit Products to 64-Bit Accumulators</li></ul><p>Essentially, the instructions perform the following operation (assuming the high 12 bits of each element in <em>a</em> and <em>b</em> are zero):</p><div><pre><code><span><span>// vpmadd52luq dst, a, b</span>
</span><span><span>void</span> <span>vpmadd52luq</span><span>(</span><span>uint64_t</span> dst<span>[</span><span>8</span><span>]</span><span>,</span> <span>uint64_t</span> a<span>[</span><span>8</span><span>]</span><span>,</span> <span>uint64_t</span> b<span>[</span><span>8</span><span>]</span><span>)</span> <span>{</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>8</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>        dst<span>[</span>i<span>]</span> <span>+=</span> <span>(</span>a<span>[</span>i<span>]</span> <span>*</span> b<span>[</span>i<span>]</span><span>)</span> <span>&amp;</span> <span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>// vpmadd52huq dst, a, b</span>
</span><span><span>void</span> <span>vpmadd52huq</span><span>(</span><span>uint64_t</span> dst<span>[</span><span>8</span><span>]</span><span>,</span> <span>uint64_t</span> a<span>[</span><span>8</span><span>]</span><span>,</span> <span>uint64_t</span> b<span>[</span><span>8</span><span>]</span><span>)</span> <span>{</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>8</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>        dst<span>[</span>i<span>]</span> <span>+=</span> <span>(</span><span>(</span>__uint128_t<span>)</span>a<span>[</span>i<span>]</span> <span>*</span> b<span>[</span>i<span>]</span><span>)</span> <span>&gt;&gt;</span> <span>52</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>In simpler terms, the instructions perform the low and high halves of a 52 × 52 → 104 multiplication, and accumulate the result into the destination register. At the execution unit level, the instructions reuse the multipliers used for double-precision floating point, and thus don't necessitate much extra silicon. They also have fantastic throughput: on Zen 5, which has a full 512-bit datapath, we can execute two of these instructions per clock!</p><p>At this point I was confident that I could make an extraordinarily fast VDF solver. All that was left was to implement it in two days....</p><p><img src="https://anemato.de/static/images/pow-is-broken.png" alt="yapping" width="500"></p><h2 id="the-game-plan">The game plan</h2><p>The natural radix for the AVX512IFMA extensions is 2<sup>52</sup>, <em>i.e.</em> 52-bit limbs stored in 64-bit words, so I let ChatGPT write a simple converter between GMP's representation and the 52-bit representation. We need ⌈1280 / 52⌉ = 25 limbs, which requires four 512-bit "zmm" registers (each register can store eight limbs, so the last register will only store one).</p><p>The first step is squaring the integer into a 50-limb intermediate product. We use a simple symmetry to almost halve the number of required multiplications, breaking up the desired value into two terms:</p><p>(a<sub>24</sub>2<sup>52·24</sup> + a<sub>23</sub>2<sup>52·23</sup> + ... + a<sub>0</sub>)<sup>2</sup> = (<span>a<sub>24</sub><sup>2</sup></span>2<sup>52·48</sup> + <span>a<sub>23</sub><sup>2</sup></span>2<sup>52·46</sup> + ... + <span>a<sub>0</sub><sup>2</sup></span>) + 2 <span><small>i,j≤24</small><span>∑</span><small>i=0,j&gt;i</small></span><span>a<sub>i</sub>a<sub>j</sub></span> 2<sup>52·(i + j)</sup></p><p>Each of the yellow-highlighted multiplications produces a low term (furnished by <em>vpmadd52luq</em>) and a high term (furnished by <em>vpmadd52huq</em>).</p><h2 id="arranging-the-multiplications">Arranging the multiplications</h2><p>First consider the <em>second</em> term above, the double summation highlighted in red. We want to reduce the number of shuffles necessary to get all the terms in the correct place, and use the "free" 64-bit accumulation as much as possible. One way to do this is to multiply a sliding window of 8 contiguous limbs by a single multiplier limb; all the output words, for both the low and high halves, will also be contiguous in the output. We can also use the merge masking feature to prevent accumulation of products that shouldn't be in the final sum, <em>e.g.</em>, pairs where i = j. By selecting these windows and multipliers correctly, we can minimize the number of wasted multiplications.</p><div><pre><code><span><span>// Computing the second term</span>
</span><span><span>// input contains the 25 52-bit limbs, stored in 64-bit words</span>
</span><span><span>_Alignas</span><span>(</span><span>64</span><span>)</span> <span>uint64_t</span> padded_data<span>[</span><span>8</span> <span>*</span> <span>6</span><span>]</span> <span>=</span> <span>{</span><span>0</span><span>}</span><span>;</span>  <span>// so that loads OOB are still valid</span>
</span><span><span>uint64_t</span> <span>*</span>data <span>=</span> padded_data <span>+</span> <span>8</span><span>;</span>
</span><span>
</span><span>__m512i clumps<span>[</span><span>4</span><span>]</span> <span>=</span> <span>{</span>
</span><span>    <span>_mm512_loadu_si512</span><span>(</span>input<span>)</span><span>,</span>
</span><span>    <span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>8</span><span>)</span><span>,</span>
</span><span>    <span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>16</span><span>)</span><span>,</span>
</span><span>    <span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>24</span><span>)</span>
</span><span><span>}</span><span>;</span>
</span><span>
</span><span><span>_mm512_store_si512</span><span>(</span>data<span>,</span> clumps<span>[</span><span>0</span><span>]</span><span>)</span><span>;</span>
</span><span><span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>8</span><span>,</span> clumps<span>[</span><span>1</span><span>]</span><span>)</span><span>;</span>
</span><span><span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>16</span><span>,</span> clumps<span>[</span><span>2</span><span>]</span><span>)</span><span>;</span>
</span><span><span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>24</span><span>,</span> clumps<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span><span><span>#</span><span>define</span> <span>ZERO</span> <span><span>_mm512_setzero_si512</span><span>(</span><span>)</span></span></span>
</span><span>
</span><span><span>// Seven zmm accumulators are necessary</span>
</span><span>__m512i accum<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO <span>/* 0-7 */</span><span>,</span> ZERO <span>/* 8-15, etc. */</span><span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>-</span><span>7</span><span>;</span> i <span>&lt;=</span> <span>24</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    <span>// Sliding window</span>
</span><span>    __m512i m1 <span>=</span> <span>_mm512_loadu_si512</span><span>(</span>data <span>+</span> i<span>)</span><span>;</span> <span>// Load the current window of 8 elements</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> j <span>=</span> <span>0</span><span>,</span> k <span>=</span> <span>0</span><span>;</span> j <span>&lt;</span> <span>7</span><span>;</span> <span>++</span>j<span>,</span> k <span>+=</span> <span>8</span><span>)</span> <span>{</span>
</span><span>        <span>// Decide whether to accumulate into accum[j], which should happen if there</span>
</span><span>        <span>// is at least one element shared between the jth accumulator and [i, i+7]</span>
</span><span>        <span>int</span> lo <span>=</span> k <span>-</span> i<span>;</span>
</span><span>        <span>int</span> hi <span>=</span> k <span>-</span> i <span>-</span> <span>1</span><span>;</span>
</span><span>        <span>// Process low halves</span>
</span><span>        <span>if</span> <span>(</span>lo <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> lo <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span>            <span>// Discard out of bounds multiplications</span>
</span><span>            __mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>lo <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>lo <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>            <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span>                accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span> <span>_mm512_set1_epi64</span><span>(</span>data<span>[</span>lo<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>            <span>}</span>
</span><span>        <span>}</span>
</span><span>        <span>// Process high halves</span>
</span><span>        <span>if</span> <span>(</span>hi <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> hi <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span>            <span>// ditto</span>
</span><span>            __mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>hi <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>hi <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>            <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span>                accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52hi_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span> <span>_mm512_set1_epi64</span><span>(</span>data<span>[</span>hi<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>            <span>}</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>As an example, accumulator <em>accum[1]</em> contains output limbs 8 through 15, inclusive, and the following accumulations are executed (in this order):</p><div><pre><code><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>1</span> by limb <span>7</span> <span>with</span> mask <span>10000000</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>1</span> by limb <span>6</span> <span>with</span> mask <span>11000000</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>2</span> by limb <span>6</span> <span>with</span> mask <span>11100000</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>2</span> by limb <span>5</span> <span>with</span> mask <span>11110000</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>3</span> by limb <span>5</span> <span>with</span> mask <span>11111000</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>3</span> by limb <span>4</span> <span>with</span> mask <span>11111100</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>4</span> by limb <span>4</span> <span>with</span> mask <span>11111110</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>4</span> by limb <span>3</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>5</span> by limb <span>3</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>5</span> by limb <span>2</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>6</span> by limb <span>2</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>6</span> by limb <span>1</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>7</span> by limb <span>1</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>7</span> by limb <span>0</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>8</span> by limb <span>0</span> <span>with</span> mask <span>11111111</span>
</span></code></pre></div><p>Here, window <em>i</em> contains limbs <em>i</em> through <em>i+7</em> inclusive. Note that the masks mostly contain ones, indicating that we are not wasting too much multiplication throughput on masked-out elements.</p><p>Computing the first term is easier. We just square each element and interleave the low and high words:</p><div><pre><code><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    __m512d diag_lo <span>=</span> <span>_mm512_castsi512_pd</span><span>(</span><span>_mm512_madd52lo_epu64</span><span>(</span>ZERO<span>,</span> clumps<span>[</span>i<span>]</span><span>,</span> clumps<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>    __m512d diag_hi <span>=</span> <span>_mm512_castsi512_pd</span><span>(</span><span>_mm512_madd52hi_epu64</span><span>(</span>ZERO<span>,</span> clumps<span>[</span>i<span>]</span><span>,</span> clumps<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>    __m512i shuf_lo <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>11</span><span>,</span> <span>3</span><span>,</span> <span>10</span><span>,</span> <span>2</span><span>,</span> <span>9</span><span>,</span> <span>1</span><span>,</span> <span>8</span><span>,</span> <span>0</span><span>)</span><span>;</span>
</span><span>    __m512i shuf_hi <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>15</span><span>,</span> <span>7</span><span>,</span> <span>14</span><span>,</span> <span>6</span><span>,</span> <span>13</span><span>,</span> <span>5</span><span>,</span> <span>12</span><span>,</span> <span>4</span><span>)</span><span>;</span>
</span><span>        accum<span>[</span><span>2</span> <span>*</span> i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>2</span><span>*</span>i<span>]</span><span>,</span> <span>_mm512_castpd_si512</span><span>(</span><span>_mm512_permutex2var_pd</span><span>(</span>diag_lo<span>,</span> shuf_lo<span>,</span> diag_hi<span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>if</span> <span>(</span>i <span>!=</span> <span>3</span><span>)</span> <span>{</span>
</span><span>        accum<span>[</span><span>2</span> <span>*</span> i <span>+</span> <span>1</span><span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>2</span><span>*</span>i<span>+</span><span>1</span><span>]</span><span>,</span> <span>_mm512_castpd_si512</span><span>(</span><span>_mm512_permutex2var_pd</span><span>(</span>diag_lo<span>,</span> shuf_hi<span>,</span> diag_hi<span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>Finally, we need to implement the reduction modulo 2<sup>1279</sup>-1. This is just a matter of selecting the upper 1279 bits and adding them to the lower 1279 bits. It's worth noting that at this point, the accumulator elements may exceed 2<sup>52</sup>-1, but we can delay carry propagation until after the addition.</p><div><pre><code><span>__m512i low_52_bits <span>=</span> <span>_mm512_set1_epi64</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>__m512i hi_12_bits <span>=</span> <span>_mm512_set1_epi64</span><span>(</span><span>~</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>
</span><span>__m512i high_1279<span>[</span><span>4</span><span>]</span><span>;</span>
</span><span><span>shift_down_1279</span><span>(</span>accum<span>,</span> high_1279<span>)</span><span>;</span>
</span><span><span>filter_low_1279</span><span>(</span>accum<span>)</span><span>;</span>
</span><span>
</span><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> high_1279<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>{</span>
</span><span>carry2<span>:</span><span>;</span>
</span><span>__m512i carry_test <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span>__m512i group_out <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    __m512i carries <span>=</span> <span>_mm512_srli_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> <span>52</span><span>)</span><span>;</span>
</span><span>    __m512i carries_into <span>=</span> <span>_mm512_alignr_epi64</span><span>(</span>carries<span>,</span> group_out<span>,</span> <span>7</span><span>)</span><span>;</span>
</span><span>    accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span><span>_mm512_and_si512</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> low_52_bits<span>)</span><span>,</span> carries_into<span>)</span><span>;</span>
</span><span>    group_out <span>=</span> carries<span>;</span>
</span><span>    carry_test <span>=</span> <span>_mm512_and_si512</span><span>(</span>carry_test<span>,</span> accum<span>[</span>i<span>]</span><span>)</span><span>;</span> <span>// improve latency over a series of masked tests</span>
</span><span><span>}</span>
</span><span>
</span><span><span>if</span> <span>(</span><span>__builtin_expect</span><span>(</span><span>_mm512_test_epi64_mask</span><span>(</span>carry_test<span>,</span> hi_12_bits<span>)</span><span>,</span> <span>0</span><span>)</span><span>)</span> <span>{</span>
</span><span>    <span>goto</span> carry2<span>;</span>
</span><span><span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>We then need to subtract the Mersenne modulus if the addition is too large, but this is easy to check: we perform the subtraction <em>iff</em> the 1280th bit is 1. Subtracting 2<sup>1279</sup>-1 is equivalent to subtracting 2<sup>1279</sup> (<em>i.e.</em>, zeroing the 1280th bit) followed by adding 1 to the least-significant limb. Because the subtraction occurs with 50% probability, we do it in a branchless manner:</p><div><pre><code><span><span>// Now compare with 2^1279 - 1; if &gt;=, subtract 2^1279 - 1. classic Mersenne number modulo algorithm</span>
</span><span>__m512i bit_1279 <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> 1ULL <span>&lt;&lt;</span> <span>31</span><span>)</span><span>;</span>
</span><span>__m512i mask_off <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>(</span>1ULL <span>&lt;&lt;</span> <span>31</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>
</span><span><span>// Branchless approach appears to save about 2 ns per iteration. Also, we stay in vector regs and don't use a test mask here because it tends to be slower</span>
</span><span>__m512i cmp <span>=</span> <span>_mm512_and_epi64</span><span>(</span>accum<span>[</span><span>3</span><span>]</span><span>,</span> bit_1279<span>)</span><span>;</span>
</span><span>accum<span>[</span><span>0</span><span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>0</span><span>]</span><span>,</span> <span>_mm512_srli_epi64</span><span>(</span>cmp<span>,</span> <span>31</span><span>)</span><span>)</span><span>;</span>  <span>// potentially +1 to last word</span>
</span><span>accum<span>[</span><span>3</span><span>]</span> <span>=</span> <span>_mm512_and_si512</span><span>(</span>mask_off<span>,</span> accum<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span><span>// TODO 1/2^52 chance of error here due to carry -- check it</span>
</span></code></pre></div><p>There is a tiny chance that an overflow occurs in this last step: if the last limb happened to be exactly 2<sup>52</sup>-1, then we'd need to propagate the carry. However, because PoWs are randomly generated, the probability this happens on any given run is about 2 in a billion—so I just ignored it.</p><p>At this point, the PoW was taking about <strong>0.45 seconds</strong> on a rented Ryzen 9950X, which is a fast Zen 5 chip. Very promising!</p><h2 id="keeping-the-multiplyadds-in-flight">Keeping the multiply–adds in flight</h2><p>The multiply–add instructions have a latency of 4 cycles, and 2 can start every cycle. Thus, we need at least eight accumulators to fully saturate the multipliers instead of bottlenecking on latency, but we only have seven (and some of them are only used occasionally). The solution is to have fourteen accumulators—one set for the low halves and one set for the high halves—then merge them at the end:</p><div><pre><code><span>__m512i accum<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO <span>/* 0-7 */</span><span>,</span> ZERO <span>/* 8-15, etc. */</span><span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span>__m512i accum_hi<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span>
</span><span><span>// ... fmadd spam goes here ...</span>
</span><span>
</span><span><span>// Fold high and low halves</span>
</span><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>7</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> accum_hi<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div><p>This brought the PoW down to roughly <strong>0.32 seconds</strong>. The expanded AVX512 register file, with 32 zmm registers, came in clutch here.</p><h2 id="taming-the-register-allocator">Taming the register allocator</h2><p>Inspecting the assembly revealed that both GCC and clang were unrolling the loop, converting the <em>_mm512_set1_epi64</em> instructions into <em>vbroadcastsd zmm, m64</em> instructions—one per limb—and then running out of vector registers during regalloc. Instead of rematerializing the values, they would stack-spill and reload the broadcasted vectors, causing considerable overhead.<sup><a href="#user-content-fn-numberworld" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-numberworld">7</a></sup></p><p>My solution was to use inline assembly to force the <em>vpmadd52luq</em>/<em>vpmadd52huq</em> instructions to use a <em>memory broadcast operand</em> for the multiplier limb. Instructions encoded with such an operand copy a single 32- or 64-bit element from memory to all elements of a vector operand, without consuming an architectural register. Moreover, this broadcast load does not consume any vector ALU resources: it is handled entirely by the load unit!</p><p>Now that we're using asm, the compiler can't remove masks of all 1s, so for optimal encoding we need separate asm for the all 1s case. Finally, when the multiplier limb happens to be at index zero in one of the zmm registers (<em>i.e.</em>, multiples of 8), we use <em>vpbroadcastq</em> to splat it from register to register, which I measured to be a performance improvement over loading it from memory. The accumulation sequence is now:</p><div><pre><code><span><span>if</span> <span>(</span>lo <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> lo <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span>    <span>// Multiples of 8 are handled by a register broadcast instead of a memory load for efficiency</span>
</span><span><span><span>#</span><span>define</span> <span>FOR_EACH_OFFS</span> <span><span>X</span><span>(</span><span>1</span><span>)</span> <span>X</span><span>(</span><span>2</span><span>)</span> <span>X</span><span>(</span><span>3</span><span>)</span> <span>X</span><span>(</span><span>4</span><span>)</span> <span>X</span><span>(</span><span>5</span><span>)</span> <span>X</span><span>(</span><span>6</span><span>)</span> <span>X</span><span>(</span><span>7</span><span>)</span> <span>X</span><span>(</span><span>9</span><span>)</span> <span>X</span><span>(</span><span>10</span><span>)</span> <span>X</span><span>(</span><span>11</span><span>)</span> <span>X</span><span>(</span><span>12</span><span>)</span> <span>X</span><span>(</span><span>13</span><span>)</span> <span>X</span><span>(</span><span>14</span><span>)</span> <span>X</span><span>(</span><span>15</span><span>)</span> <span>X</span><span>(</span><span>17</span><span>)</span> <span>X</span><span>(</span><span>18</span><span>)</span> <span>X</span><span>(</span><span>19</span><span>)</span> <span>X</span><span>(</span><span>20</span><span>)</span> <span>X</span><span>(</span><span>21</span><span>)</span> <span>X</span><span>(</span><span>22</span><span>)</span> <span>X</span><span>(</span><span>23</span><span>)</span></span></span>
</span><span>    __mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>lo <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>lo <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>if</span> <span>(</span>sel <span>==</span> <span>(</span><span>uint8_t</span><span>)</span><span>-</span><span>1</span> <span>&amp;&amp;</span> ELIDE_MASKS_IF_POSSIBLE<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52luq %0, %1, %2%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum<span>[</span>j<span>]</span><span>)</span> <span>:</span>  </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>        <span>switch</span> <span>(</span>lo<span>)</span> <span>{</span>
</span><span>            FOR_EACH_OFFS
</span><span>            <span>default</span><span>:</span>
</span><span>            accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>lo<span>/</span> <span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>        <span>}</span>
</span><span>
</span><span>    <span>}</span> <span>else</span> <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52luq %0 %{%1%}, %2, %3%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum<span>[</span>j<span>]</span><span>)</span> <span>:</span> </span><span>"Yk"</span><span><span>(</span>sel<span>)</span><span>,</span> </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>        <span>switch</span> <span>(</span>lo<span>)</span> <span>{</span>
</span><span>            FOR_EACH_OFFS
</span><span>            <span>default</span><span>:</span>
</span><span>            accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>lo<span>/</span><span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>(This is just for the low set of accumulators; the high set is nearly identical.) At this point, the PoW was taking roughly <strong>0.23 seconds</strong>.</p><h2 id="creating-the-windows-without-touching-memory">Creating the windows without touching memory</h2><p>To compute the "windows", we are storing the integer to memory in an aligned fashion, then loading from it in an unaligned fashion. This is a classic situation that causes a <a target="_blank" rel="noopener noreferrer" href="https://easyperf.net/blog/2018/03/09/Store-forwarding"><em>store-forwarding stall</em></a>, which further lengthens the critical path (the multiplications cannot commence until a window is loaded from memory). A better solution is to use the <em>valignq</em> instruction, which lets us simulate an unaligned load from the <em>clumps</em> array containing our integer.</p><div><pre><code><span>__m512i m1<span>;</span>
</span><span><span>if</span> <span>(</span><span>(</span>i <span>&amp;</span> <span>7</span><span>)</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
</span><span>    m1 <span>=</span> clumps<span>[</span>i <span>/</span> <span>8</span><span>]</span><span>;</span>
</span><span><span>}</span> <span>else</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>UNALIGNED</span><span><span>(</span>S<span>)</span> <span>case</span> S<span>:</span> <span>{</span> m1 <span>=</span> <span>_mm512_alignr_epi64</span><span>(</span>clumps<span>[</span><span>(</span>i<span>+</span><span>8</span><span>)</span><span>/</span><span>8</span><span>]</span><span>,</span> i <span>&lt;</span> <span>0</span> <span>?</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span> <span>:</span> clumps<span>[</span><span>(</span>i<span>+</span><span>8</span><span>)</span><span>/</span><span>8</span><span>-</span><span>1</span><span>]</span><span>,</span> S<span>)</span><span>;</span> <span>break</span><span>;</span> <span>}</span></span></span>
</span><span>    <span>switch</span> <span>(</span>i <span>&amp;</span> <span>7</span><span>)</span> <span>{</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>1</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>2</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>3</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>4</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>5</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>6</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>7</span><span>)</span>
</span><span>        <span>default</span><span>:</span> <span>abort</span><span>(</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>This brought the PoW down to <strong>0.21 seconds</strong>. At this point, I decided to call it quits and sent my teammates the final C code.</p><h2 id="may-16-show-time">May 16: Show time!</h2><p>My friends got up at 4:30 a.m. PST, May 16, to prepare for the final submission; I couldn't be assed to be awake, so I slept until 6:30. They spun up a Zen 5 Google Cloud server in the Netherlands, geographically closest to the Google Form submission server, to minimize latency. A few minutes before 5:00, they recorded an intercepted POST request submitting the Google form, but with a dummy flag. The form submission program was devised and optimized by Bryce Casaje (<a target="_blank" rel="noopener noreferrer" href="https://brycec.me/">strellic</a>) and Larry Yuan (<a target="_blank" rel="noopener noreferrer" href="https://larry.sh/">ehhthing</a>). <a target="_blank" rel="noopener noreferrer" href="https://max.xz.ax/">Max Cai</a> also assisted in development and submission. Then at 5:00, the server connected to the kernelCTF server, solved the proof of work, ran Savy's optimized exploit, inserted the flag into the POST request, and sent it off....</p><p><img alt="lol" loading="lazy" width="1526" height="94" decoding="async" data-nimg="1" srcset="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Four-kctf-submission.png&amp;w=1920&amp;q=75 1x, https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Four-kctf-submission.png&amp;w=3840&amp;q=75 2x" src="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Four-kctf-submission.png&amp;w=3840&amp;q=75"></p><p>We got it in 3.6 seconds—the fastest-ever kernelCTF submission! Later that day, the kernelCTF organizers confirmed our eligibility for the bounty and we all breathed a collective sigh of relief. Again, congratulations to Savy and William for discovering and exploiting this bug! Thanks to them for presenting me with the challenge, and thanks to my CSE 260 professor Bryan Chin (<a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/ucsd.edu/bryan-chin/home">website</a>) for what he taught us about program optimization.</p><h2 id="the-end-of-an-era">The end of an era</h2><p>On May 28, kernelCTF organizer koczkatamas announced that the proof of work was being removed:</p><p><img src="https://anemato.de/static/images/pow-is-gone.png" alt="PoW is gone" width="400"></p><p>On the one hand, it's sad that we no longer have a competitive advantage, and the slot race becomes purely about exploit duration and network latency. On the other hand, at least people don't need to buy expensive FPGAs, or pull out their inline asm knowledge, to be on equal footing with the professionals. It also frees me to release this post!</p><p>Please message me on Discord (@forevermilk) if you have any comments or questions. I am also researching VDFs that are more resilient to assembly-level optimizations; if you have any ideas or would like to collaborate, I'm all ears.</p><h2 id="the-final-solver">The final solver</h2><p>This code is the product (ha!) of about 12 hours of work across May 14 and 15, and it is correspondingly unclean. Consider it released under the GNU AGPL 3.0.</p><div><pre><code><span><span>// Written by Timothy Herchen</span>
</span><span><span>// gcc main.c -O3 -march=znver5 -masm=intel -lgmp</span>
</span><span><span><span>#</span><span>include</span> <span>&lt;immintrin.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;gmp.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;string.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;stdlib.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;stdio.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;stdint.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;stddef.h&gt;</span></span>
</span><span>
</span><span><span><span>#</span><span>define</span> <span>uint128_t</span> <span>__uint128_t</span></span>
</span><span>
</span><span><span>void</span> <span>gmp_to_array</span><span>(</span><span>mpz_t</span> mpz<span>,</span> <span>uint64_t</span> <span>*</span>array<span>)</span> <span>{</span>
</span><span>    <span>size_t</span> N<span>;</span>
</span><span>    <span>mpz_export</span><span>(</span>array<span>,</span> <span>&amp;</span>N<span>,</span> <span>1</span><span>,</span> <span>sizeof</span><span>(</span><span>uint64_t</span><span>)</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> mpz<span>)</span><span>;</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>,</span> j <span>=</span> N <span>-</span> <span>1</span><span>;</span> i <span>&lt;</span> j<span>;</span> <span>++</span>i<span>,</span> <span>--</span>j<span>)</span> <span>{</span>
</span><span>	    <span>uint64_t</span> temp <span>=</span> array<span>[</span>i<span>]</span><span>;</span>
</span><span>	    array<span>[</span>i<span>]</span> <span>=</span> array<span>[</span>j<span>]</span><span>;</span>
</span><span>	    array<span>[</span>j<span>]</span> <span>=</span> temp<span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>// Destroys the array</span>
</span><span><span>void</span> <span>array_to_gmp</span><span>(</span><span>uint64_t</span> <span>*</span>array<span>,</span> <span>mpz_t</span> mpz<span>,</span> <span>uint64_t</span> words<span>)</span> <span>{</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>,</span> j <span>=</span> words <span>-</span> <span>1</span><span>;</span> i <span>&lt;</span> j<span>;</span> <span>++</span>i<span>,</span> <span>--</span>j<span>)</span> <span>{</span>
</span><span>	    <span>uint64_t</span> temp <span>=</span> array<span>[</span>i<span>]</span><span>;</span>
</span><span>	    array<span>[</span>i<span>]</span> <span>=</span> array<span>[</span>j<span>]</span><span>;</span>
</span><span>	    array<span>[</span>j<span>]</span> <span>=</span> temp<span>;</span>
</span><span>    <span>}</span>
</span><span>    <span>mpz_import</span><span>(</span>mpz<span>,</span> words<span>,</span> <span>1</span><span>,</span> <span>sizeof</span><span>(</span><span>uint64_t</span><span>)</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> array<span>)</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>size_t</span> <span>convert_radix_64_to_52</span><span>(</span><span>uint64_t</span> <span>*</span>limbs<span>,</span> <span>uint64_t</span> <span>*</span>out<span>,</span> <span>size_t</span> count<span>)</span> <span>{</span>
</span><span>    <span>size_t</span> out_index <span>=</span> <span>0</span><span>;</span>
</span><span>    <span>int</span> bits_in_buffer <span>=</span> <span>0</span><span>;</span>
</span><span>    <span>uint128_t</span> buffer <span>=</span> <span>0</span><span>;</span>
</span><span>
</span><span>    <span>for</span> <span>(</span><span>size_t</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> count<span>;</span> i<span>++</span><span>)</span> <span>{</span>
</span><span>        buffer <span>|=</span> <span>(</span><span>(</span><span>uint128_t</span><span>)</span>limbs<span>[</span>i<span>]</span><span>)</span> <span>&lt;&lt;</span> bits_in_buffer<span>;</span>
</span><span>        bits_in_buffer <span>+=</span> <span>64</span><span>;</span>
</span><span>
</span><span>        <span>while</span> <span>(</span>bits_in_buffer <span>&gt;=</span> <span>52</span><span>)</span> <span>{</span>
</span><span>            out<span>[</span>out_index<span>++</span><span>]</span> <span>=</span> <span>(</span><span>uint64_t</span><span>)</span><span>(</span>buffer <span>&amp;</span> <span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>            buffer <span>&gt;&gt;=</span> <span>52</span><span>;</span>
</span><span>            bits_in_buffer <span>-=</span> <span>52</span><span>;</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>// Handle remaining bits if any</span>
</span><span>    <span>if</span> <span>(</span>bits_in_buffer <span>&gt;</span> <span>0</span><span>)</span> <span>{</span>
</span><span>        out<span>[</span>out_index<span>++</span><span>]</span> <span>=</span> <span>(</span><span>uint64_t</span><span>)</span><span>(</span>buffer <span>&amp;</span> <span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> bits_in_buffer<span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>return</span> out_index<span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>size_t</span> <span>convert_radix_52_to_64</span><span>(</span><span>uint64_t</span> <span>*</span>in<span>,</span> <span>uint64_t</span> <span>*</span>out<span>,</span> <span>size_t</span> count<span>)</span> <span>{</span>
</span><span>    <span>size_t</span> out_index <span>=</span> <span>0</span><span>;</span>
</span><span>    <span>int</span> bits_in_buffer <span>=</span> <span>0</span><span>;</span>
</span><span>    <span>uint128_t</span> buffer <span>=</span> <span>0</span><span>;</span>
</span><span>
</span><span>    <span>for</span> <span>(</span><span>size_t</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> count<span>;</span> i<span>++</span><span>)</span> <span>{</span>
</span><span>        buffer <span>|=</span> <span>(</span><span>(</span><span>uint128_t</span><span>)</span>in<span>[</span>i<span>]</span><span>)</span> <span>&lt;&lt;</span> bits_in_buffer<span>;</span>
</span><span>        bits_in_buffer <span>+=</span> <span>52</span><span>;</span>
</span><span>
</span><span>        <span>while</span> <span>(</span>bits_in_buffer <span>&gt;=</span> <span>64</span><span>)</span> <span>{</span>
</span><span>            out<span>[</span>out_index<span>++</span><span>]</span> <span>=</span> <span>(</span><span>uint64_t</span><span>)</span><span>(</span>buffer <span>&amp;</span> <span>(</span><span>(</span><span>(</span><span>uint128_t</span><span>)</span><span>1ULL</span> <span>&lt;&lt;</span> <span>64</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>            buffer <span>&gt;&gt;=</span> <span>64</span><span>;</span>
</span><span>            bits_in_buffer <span>-=</span> <span>64</span><span>;</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>// Handle remaining bits if any</span>
</span><span>    <span>if</span> <span>(</span>bits_in_buffer <span>&gt;</span> <span>0</span><span>)</span> <span>{</span>
</span><span>        out<span>[</span>out_index<span>++</span><span>]</span> <span>=</span> <span>(</span><span>uint64_t</span><span>)</span><span>(</span>buffer <span>&amp;</span> <span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> bits_in_buffer<span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>return</span> out_index<span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>__attribute__</span><span>(</span><span>(</span>always_inline<span>)</span><span>)</span> <span>void</span> <span>shift_down_1279</span><span>(</span>__m512i accum<span>[</span><span>7</span><span>]</span><span>,</span> __m512i high_1279<span>[</span><span>4</span><span>]</span><span>)</span> <span>{</span>
</span><span>	__m512i p <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>4</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>3</span><span>;</span> i <span>&gt;=</span> <span>0</span><span>;</span> <span>--</span>i<span>)</span> <span>{</span>
</span><span>		__m512i down_31 <span>=</span> <span>_mm512_srli_epi64</span><span>(</span>accum<span>[</span>i <span>+</span> <span>3</span><span>]</span><span>,</span> <span>31</span><span>)</span><span>;</span>
</span><span>		__m512i higher_21 <span>=</span> <span>_mm512_slli_epi64</span><span>(</span><span>_mm512_and_si512</span><span>(</span>accum<span>[</span>i <span>+</span> <span>3</span><span>]</span><span>,</span> <span>_mm512_set1_epi64</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>31</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>,</span> <span>21</span><span>)</span><span>;</span>
</span><span>		high_1279<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span><span>_mm512_alignr_epi64</span><span>(</span>p<span>,</span> higher_21<span>,</span> <span>1</span><span>)</span><span>,</span> down_31<span>)</span><span>;</span>
</span><span>		p <span>=</span> higher_21<span>;</span>
</span><span>	<span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>__attribute__</span><span>(</span><span>(</span>always_inline<span>)</span><span>)</span> <span>void</span> <span>filter_low_1279</span><span>(</span>__m512i accum<span>[</span><span>7</span><span>]</span><span>)</span> <span>{</span>
</span><span>	accum<span>[</span><span>3</span><span>]</span> <span>=</span> <span>_mm512_and_si512</span><span>(</span>accum<span>[</span><span>3</span><span>]</span><span>,</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>31</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>// This code is extremely latency bound, as you'd expect for this kind of stupid PoW, so certain things are done that would lower</span>
</span><span><span>// throughput (e.g. on a hyperthreaded device doing two of these at once) but which lower the latency</span>
</span><span><span>void</span> <span>the_powmod</span><span>(</span><span>uint64_t</span> <span>*</span> __restrict__ input<span>,</span> <span>uint64_t</span> <span>*</span> __restrict__ result<span>)</span> <span>{</span>
</span><span>	<span>_Alignas</span><span>(</span><span>64</span><span>)</span> <span>uint64_t</span> padded_data<span>[</span><span>8</span> <span>*</span> <span>6</span><span>]</span> <span>=</span> <span>{</span><span>0</span><span>}</span><span>;</span>
</span><span>	<span>uint64_t</span> <span>*</span>data <span>=</span> padded_data <span>+</span> <span>8</span><span>;</span>
</span><span>
</span><span>	__m512i clumps<span>[</span><span>4</span><span>]</span> <span>=</span> <span>{</span>
</span><span>		<span>_mm512_loadu_si512</span><span>(</span>input<span>)</span><span>,</span>
</span><span>		<span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>8</span><span>)</span><span>,</span>
</span><span>		<span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>16</span><span>)</span><span>,</span>
</span><span>		<span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>24</span><span>)</span>
</span><span>	<span>}</span><span>;</span>
</span><span>
</span><span>	<span>for</span> <span>(</span><span>int</span> pow_i <span>=</span> <span>0</span><span>;</span> pow_i <span>&lt;</span> <span>1277</span><span>;</span> <span>++</span>pow_i<span>)</span> <span>{</span>
</span><span>		<span>// Use aligned stores to make sure we are doing things well</span>
</span><span>		<span>_mm512_store_si512</span><span>(</span>data<span>,</span> clumps<span>[</span><span>0</span><span>]</span><span>)</span><span>;</span>
</span><span>		<span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>8</span><span>,</span> clumps<span>[</span><span>1</span><span>]</span><span>)</span><span>;</span>
</span><span>		<span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>16</span><span>,</span> clumps<span>[</span><span>2</span><span>]</span><span>)</span><span>;</span>
</span><span>		<span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>24</span><span>,</span> clumps<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>// Now data[x] gives us the xth limb</span>
</span><span><span><span>#</span><span>define</span> <span>ZERO</span> <span><span>_mm512_setzero_si512</span><span>(</span><span>)</span></span></span>
</span><span>
</span><span><span><span>#</span><span>define</span> <span>ELIDE_MASKS_IF_POSSIBLE</span> <span><span>1</span></span></span>
</span><span>
</span><span>	__m512i accum<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO <span>/* 0-7 */</span><span>,</span> ZERO <span>/* 8-15, etc. */</span><span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span>	<span>// The accumulation is latency bound (lat. 4 cycles, so we need at least 8 accumulators to keep the madds in flight)</span>
</span><span>	__m512i accum_hi<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span>	<span>// We'll laboriously build up the upper triangle of the 2560-bit product using 52x52-&gt;104 multiplies</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>100</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>24</span><span>;</span> i <span>&gt;=</span> <span>-</span><span>7</span><span>;</span> <span>--</span>i<span>)</span> <span>{</span>
</span><span>		<span>// Sliding window</span>
</span><span>		__m512i m1<span>;</span>
</span><span>		<span>if</span> <span>(</span><span>(</span>i <span>&amp;</span> <span>7</span><span>)</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
</span><span>			m1 <span>=</span> clumps<span>[</span>i <span>/</span> <span>8</span><span>]</span><span>;</span>
</span><span>		<span>}</span> <span>else</span> <span>{</span>
</span><span>			<span>// Emulate an unaligned load from memory. Unaligned loads are very expensive on Zen 5 so this is helpful</span>
</span><span><span><span>#</span><span>define</span> <span>UNALIGNED</span><span><span>(</span>S<span>)</span> <span>case</span> S<span>:</span> <span>{</span> m1 <span>=</span> <span>_mm512_alignr_epi64</span><span>(</span>clumps<span>[</span><span>(</span>i<span>+</span><span>8</span><span>)</span><span>/</span><span>8</span><span>]</span><span>,</span> i <span>&lt;</span> <span>0</span> <span>?</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span> <span>:</span> clumps<span>[</span><span>(</span>i<span>+</span><span>8</span><span>)</span><span>/</span><span>8</span><span>-</span><span>1</span><span>]</span><span>,</span> S<span>)</span><span>;</span> <span>break</span><span>;</span> <span>}</span></span></span>
</span><span>			<span>switch</span> <span>(</span>i <span>&amp;</span> <span>7</span><span>)</span> <span>{</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>1</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>2</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>3</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>4</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>5</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>6</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>7</span><span>)</span>
</span><span>				<span>default</span><span>:</span> <span>abort</span><span>(</span><span>)</span><span>;</span>
</span><span>			<span>}</span>
</span><span>		<span>}</span>
</span><span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>100</span></span></span>
</span><span>		<span>for</span> <span>(</span><span>int</span> j <span>=</span> <span>0</span><span>,</span> k <span>=</span> <span>0</span><span>;</span> j <span>&lt;</span> <span>7</span><span>;</span> <span>++</span>j<span>,</span> k <span>+=</span> <span>8</span><span>)</span> <span>{</span>
</span><span>			<span>// Decide whether to accumulate into accum[j], which should happen if there</span>
</span><span>			<span>// is at least one element shared between the jth accumulator and [i, i+7]</span>
</span><span>			<span>int</span> lo <span>=</span> k <span>-</span> i<span>;</span>
</span><span>			<span>int</span> hi <span>=</span> k <span>-</span> i <span>-</span> <span>1</span><span>;</span>
</span><span>			<span>if</span> <span>(</span>lo <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> lo <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span>				<span>// Multiples of 8 are handled by a broadcast instead of a memory load for efficiency</span>
</span><span><span><span>#</span><span>define</span> <span>FOR_EACH_OFFS</span> <span><span>X</span><span>(</span><span>1</span><span>)</span> <span>X</span><span>(</span><span>2</span><span>)</span> <span>X</span><span>(</span><span>3</span><span>)</span> <span>X</span><span>(</span><span>4</span><span>)</span> <span>X</span><span>(</span><span>5</span><span>)</span> <span>X</span><span>(</span><span>6</span><span>)</span> <span>X</span><span>(</span><span>7</span><span>)</span> <span>X</span><span>(</span><span>9</span><span>)</span> <span>X</span><span>(</span><span>10</span><span>)</span> <span>X</span><span>(</span><span>11</span><span>)</span> <span>X</span><span>(</span><span>12</span><span>)</span> <span>X</span><span>(</span><span>13</span><span>)</span> <span>X</span><span>(</span><span>14</span><span>)</span> <span>X</span><span>(</span><span>15</span><span>)</span> <span>X</span><span>(</span><span>17</span><span>)</span> <span>X</span><span>(</span><span>18</span><span>)</span> <span>X</span><span>(</span><span>19</span><span>)</span> <span>X</span><span>(</span><span>20</span><span>)</span> <span>X</span><span>(</span><span>21</span><span>)</span> <span>X</span><span>(</span><span>22</span><span>)</span> <span>X</span><span>(</span><span>23</span><span>)</span></span></span>
</span><span>				<span>// Discard those entries where lo &gt; i</span>
</span><span>				__mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>lo <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>lo <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>				<span>// we use inline asm with a memory broadcast after enough regs because the register allocator does not enjoy this type of setup</span>
</span><span>				<span>if</span> <span>(</span>sel <span>==</span> <span>(</span><span>uint8_t</span><span>)</span><span>-</span><span>1</span> <span>&amp;&amp;</span> ELIDE_MASKS_IF_POSSIBLE<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52luq %0, %1, %2%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum<span>[</span>j<span>]</span><span>)</span> <span>:</span>  </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>					<span>switch</span> <span>(</span>lo<span>)</span> <span>{</span>
</span><span>						FOR_EACH_OFFS
</span><span>						<span>default</span><span>:</span>
</span><span>						accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>lo<span>/</span> <span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>					<span>}</span>
</span><span>
</span><span>				<span>}</span> <span>else</span> <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52luq %0 %{%1%}, %2, %3%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum<span>[</span>j<span>]</span><span>)</span> <span>:</span> </span><span>"Yk"</span><span><span>(</span>sel<span>)</span><span>,</span> </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>					<span>switch</span> <span>(</span>lo<span>)</span> <span>{</span>
</span><span>						FOR_EACH_OFFS
</span><span>						<span>default</span><span>:</span>
</span><span>						accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>lo<span>/</span><span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>					<span>}</span>
</span><span>				<span>}</span>
</span><span>			<span>}</span>
</span><span>
</span><span>			<span>if</span> <span>(</span>hi <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> hi <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span><span><span>#</span><span>undef</span> <span>X</span></span>
</span><span>				__mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>hi <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>hi <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>				<span>// see above</span>
</span><span>				<span>if</span> <span>(</span>sel <span>==</span> <span>(</span><span>uint8_t</span><span>)</span><span>-</span><span>1</span> <span>&amp;&amp;</span> ELIDE_MASKS_IF_POSSIBLE<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52huq %0, %1, %2%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum_hi<span>[</span>j<span>]</span><span>)</span> <span>:</span> </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>				<span>switch</span> <span>(</span>hi<span>)</span> <span>{</span>
</span><span>					FOR_EACH_OFFS
</span><span>					<span>default</span><span>:</span>
</span><span>					accum_hi<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_madd52hi_epu64</span><span>(</span>accum_hi<span>[</span>j<span>]</span><span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>hi <span>/</span> <span>8</span><span>]</span><span>)</span><span>)</span> <span>)</span><span>;</span>
</span><span>				<span>}</span>
</span><span>				<span>}</span> <span>else</span> <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52huq %0 %{%1%}, %2, %3%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum_hi<span>[</span>j<span>]</span><span>)</span> <span>:</span> </span><span>"Yk"</span><span><span>(</span>sel<span>)</span><span>,</span> </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>				<span>switch</span> <span>(</span>hi<span>)</span> <span>{</span>
</span><span>					FOR_EACH_OFFS
</span><span>					<span>default</span><span>:</span>
</span><span>					accum_hi<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52hi_epu64</span><span>(</span>accum_hi<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span><span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>hi <span>/</span> <span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>				<span>}</span>
</span><span>				<span>}</span>
</span><span>			<span>}</span>
</span><span>
</span><span>		<span>}</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>// Fold high and low halves, and double all the accumulators</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>7</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>7</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> accum_hi<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span>		accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> accum<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>// Now add the diagonal from the accumulators because they weren't yet computed</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>4</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		__m512d diag_lo <span>=</span> <span>_mm512_castsi512_pd</span><span>(</span><span>_mm512_madd52lo_epu64</span><span>(</span>ZERO<span>,</span> clumps<span>[</span>i<span>]</span><span>,</span> clumps<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>		__m512d diag_hi <span>=</span> <span>_mm512_castsi512_pd</span><span>(</span><span>_mm512_madd52hi_epu64</span><span>(</span>ZERO<span>,</span> clumps<span>[</span>i<span>]</span><span>,</span> clumps<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>		__m512i shuf_lo <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>11</span><span>,</span> <span>3</span><span>,</span> <span>10</span><span>,</span> <span>2</span><span>,</span> <span>9</span><span>,</span> <span>1</span><span>,</span> <span>8</span><span>,</span> <span>0</span><span>)</span><span>;</span>
</span><span>		__m512i shuf_hi <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>15</span><span>,</span> <span>7</span><span>,</span> <span>14</span><span>,</span> <span>6</span><span>,</span> <span>13</span><span>,</span> <span>5</span><span>,</span> <span>12</span><span>,</span> <span>4</span><span>)</span><span>;</span>
</span><span>	        accum<span>[</span><span>2</span> <span>*</span> i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>2</span><span>*</span>i<span>]</span><span>,</span> <span>_mm512_castpd_si512</span><span>(</span><span>_mm512_permutex2var_pd</span><span>(</span>diag_lo<span>,</span> shuf_lo<span>,</span> diag_hi<span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>		<span>if</span> <span>(</span>i <span>!=</span> <span>3</span><span>)</span> <span>{</span>
</span><span>			accum<span>[</span><span>2</span> <span>*</span> i <span>+</span> <span>1</span><span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>2</span><span>*</span>i<span>+</span><span>1</span><span>]</span><span>,</span> <span>_mm512_castpd_si512</span><span>(</span><span>_mm512_permutex2var_pd</span><span>(</span>diag_lo<span>,</span> shuf_hi<span>,</span> diag_hi<span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>		<span>}</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>// Now propagate carries in parallel in radix 2^52</span>
</span><span>	__m512i low_52_bits <span>=</span> <span>_mm512_set1_epi64</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>	__m512i hi_12_bits <span>=</span> <span>_mm512_set1_epi64</span><span>(</span><span>~</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>// Now add the high 1279 bits to the low 1279 bits</span>
</span><span>	__m512i high_1279<span>[</span><span>4</span><span>]</span><span>;</span>
</span><span>	<span>shift_down_1279</span><span>(</span>accum<span>,</span> high_1279<span>)</span><span>;</span>
</span><span>	<span>filter_low_1279</span><span>(</span>accum<span>)</span><span>;</span>
</span><span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>4</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> high_1279<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>{</span>
</span><span>carry2<span>:</span><span>;</span>
</span><span>	__m512i carry_test <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span>	__m512i group_out <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>7</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		__m512i carries <span>=</span> <span>_mm512_srli_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> <span>52</span><span>)</span><span>;</span>
</span><span>		__m512i carries_into <span>=</span> <span>_mm512_alignr_epi64</span><span>(</span>carries<span>,</span> group_out<span>,</span> <span>7</span><span>)</span><span>;</span>
</span><span>		accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span><span>_mm512_and_si512</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> low_52_bits<span>)</span><span>,</span> carries_into<span>)</span><span>;</span>
</span><span>		group_out <span>=</span> carries<span>;</span>
</span><span>		carry_test <span>=</span> <span>_mm512_and_si512</span><span>(</span>carry_test<span>,</span> accum<span>[</span>i<span>]</span><span>)</span><span>;</span> <span>// improve latency over a series of masked tests</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>if</span> <span>(</span><span>__builtin_expect</span><span>(</span><span>_mm512_test_epi64_mask</span><span>(</span>carry_test<span>,</span> hi_12_bits<span>)</span><span>,</span> <span>0</span><span>)</span><span>)</span> <span>{</span>
</span><span>		<span>goto</span> carry2<span>;</span>
</span><span>	<span>}</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>// Now compare with 2^1279 - 1; if &gt;=, subtract 2^1279 - 1. classic Mersenne number modulo algorithm</span>
</span><span>	__m512i bit_1279 <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>1ULL</span> <span>&lt;&lt;</span> <span>31</span><span>)</span><span>;</span>
</span><span>	__m512i mask_off <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>31</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>// Branchless approach appears to save about 2 ns per iteration. Also, we stay in vector regs and don't use a test mask here because it tends to be slower</span>
</span><span>	__m512i cmp <span>=</span> <span>_mm512_and_epi64</span><span>(</span>accum<span>[</span><span>3</span><span>]</span><span>,</span> bit_1279<span>)</span><span>;</span>
</span><span>	accum<span>[</span><span>0</span><span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>0</span><span>]</span><span>,</span> <span>_mm512_srli_epi64</span><span>(</span>cmp<span>,</span> <span>31</span><span>)</span><span>)</span><span>;</span>  <span>// potentially +1 to last word</span>
</span><span>	accum<span>[</span><span>3</span><span>]</span> <span>=</span> <span>_mm512_and_si512</span><span>(</span>mask_off<span>,</span> accum<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>// TODO 1/2^52 chance of error here due to carry -- check it</span>
</span><span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>4</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		clumps<span>[</span>i<span>]</span> <span>=</span> accum<span>[</span>i<span>]</span><span>;</span>
</span><span>	<span>}</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>_mm512_storeu_si512</span><span>(</span>result<span>,</span> clumps<span>[</span><span>0</span><span>]</span><span>)</span><span>;</span>
</span><span>	<span>_mm512_storeu_si512</span><span>(</span>result <span>+</span> <span>8</span><span>,</span> clumps<span>[</span><span>1</span><span>]</span><span>)</span><span>;</span>
</span><span>	<span>_mm512_storeu_si512</span><span>(</span>result <span>+</span> <span>16</span><span>,</span> clumps<span>[</span><span>2</span><span>]</span><span>)</span><span>;</span>
</span><span>	<span>_mm512_storeu_si512</span><span>(</span>result <span>+</span> <span>24</span><span>,</span> clumps<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>int</span> <span>main</span><span>(</span><span>int</span> argc<span>,</span> <span>char</span> <span>*</span><span>*</span>argv<span>)</span> <span>{</span>
</span><span>	<span>if</span> <span>(</span>argc <span>&lt;</span> <span>3</span><span>)</span> <span>{</span>
</span><span>		<span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"Usage: %s &lt;y&gt; &lt;difficulty&gt;"</span><span>,</span> argv<span>[</span><span>0</span><span>]</span><span>)</span><span>;</span>
</span><span>		<span>return</span> <span>1</span><span>;</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>mpz_t</span> x<span>,</span> r<span>;</span>
</span><span>	<span>mpz_inits</span><span>(</span>x<span>,</span> r<span>,</span> <span>NULL</span><span>)</span><span>;</span>
</span><span>	<span>mpz_set_str</span><span>(</span>x<span>,</span> argv<span>[</span><span>1</span><span>]</span><span>,</span> <span>10</span><span>)</span><span>;</span>
</span><span>	<span>int</span> difficulty <span>=</span> <span>atoi</span><span>(</span>argv<span>[</span><span>2</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>uint64_t</span> abc<span>[</span><span>400</span><span>]</span><span>;</span>
</span><span>	<span>_Alignas</span><span>(</span><span>64</span><span>)</span> <span>uint64_t</span> poop<span>[</span><span>32</span><span>]</span><span>;</span>
</span><span>
</span><span>	<span>gmp_to_array</span><span>(</span>x<span>,</span> abc<span>)</span><span>;</span>
</span><span>
</span><span>	<span>size_t</span> N <span>=</span> <span>convert_radix_64_to_52</span><span>(</span>abc<span>,</span> poop<span>,</span> <span>20</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>uint64_t</span> squared<span>[</span><span>1000</span><span>]</span><span>;</span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> difficulty<span>;</span> <span>++</span>i<span>)</span> <span>{</span> <span>// specified algorithm</span>
</span><span>		<span>the_powmod</span><span>(</span>poop<span>,</span> squared<span>)</span><span>;</span>
</span><span>		squared<span>[</span><span>0</span><span>]</span> <span>^=</span> <span>1</span><span>;</span>
</span><span>		<span>memcpy</span><span>(</span>poop<span>,</span> squared<span>,</span> <span>25</span> <span>*</span> <span>sizeof</span><span>(</span><span>uint64_t</span><span>)</span><span>)</span><span>;</span>
</span><span>	<span>}</span>
</span><span>	<span>convert_radix_52_to_64</span><span>(</span>squared<span>,</span> abc<span>,</span> <span>48</span><span>)</span><span>;</span>
</span><span>	<span>array_to_gmp</span><span>(</span>abc<span>,</span> r<span>,</span> <span>1280</span><span>/</span><span>64</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>char</span> <span>*</span>str <span>=</span> <span>mpz_get_str</span><span>(</span><span>NULL</span><span>,</span> <span>10</span><span>,</span> r<span>)</span><span>;</span>
</span><span>	<span>printf</span><span>(</span><span>"%s"</span><span>,</span> str<span>)</span><span>;</span>
</span><span>	<span>return</span> <span>0</span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div><p>I hope you enjoyed my first-ever blog post. Hopefully there will be many more.</p><section data-footnotes="true"><h2 id="footnote-label">Footnotes</h2><ol><li id="user-content-fn-bounty"><p>$21,337 base reward, $10,000 for stability (successful exploitation on &gt;90% of runs), and $20,000 for a 0-day bug. <a href="#user-content-fnref-bounty" aria-label="Back to reference 1" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-thesis"><p>We had one shot at this; William's thesis was due in a few days and we wanted to include the exploit. <a href="#user-content-fnref-thesis" aria-label="Back to reference 2" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-hash"><p>Compare with proofs of work, for example, that request a SHA hash starting with some number of zeros. This process is embarrassingly parallel, so someone with many powerful GPUs could solve it in a fraction of the time as someone without. <a href="#user-content-fnref-hash" aria-label="Back to reference 3" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-rust"><p>The Rust implementation uses the exact same Mersenne trick, yet takes about 2.4 seconds; I assume this is FFI overhead? 🤷 <a href="#user-content-fnref-rust" aria-label="Back to reference 4" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-limbs"><p>Limbs are the term of art for the integer elements of an array that represents a big integer. On 64-bit systems, limbs are usually 64-bit unsigned integers. <a href="#user-content-fnref-limbs" aria-label="Back to reference 5" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-uops"><p>See <a target="_blank" rel="noopener noreferrer" href="https://uops.info/table.html">uops.info</a>, <em>imul r64</em> and <em>mulx r64, r64, r64</em>. <a href="#user-content-fnref-uops" aria-label="Back to reference 6" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-numberworld"><p>The same performance problem was observed by y-cruncher author Alexander Yee <a target="_blank" rel="noopener noreferrer" href="https://www.numberworld.org/y-cruncher/news/2024.html">here</a>, section "Example 2: Everything Blows Up". <a href="#user-content-fnref-numberworld" aria-label="Back to reference 7" data-footnote-backref="">↩</a></p></li></ol></section></div></div></div></article></section></main></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[De Bruijn notation, and why it's useful (128 pts)]]></title>
            <link>https://blueberrywren.dev/blog/debruijn-explanation/</link>
            <guid>44137439</guid>
            <pubDate>Fri, 30 May 2025 15:51:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blueberrywren.dev/blog/debruijn-explanation/">https://blueberrywren.dev/blog/debruijn-explanation/</a>, See on <a href="https://news.ycombinator.com/item?id=44137439">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


      



  <meta property="og:title" content="Debruijn indexes + levels, and why they're handy">
  <title> Debruijn indexes + levels, and why they're handy </title>






<h2 id="de-bruijn-and-why-we-use-it">De Bruijn and why we use it</h2>
<h2 id="assumed-knowledge">Assumed knowledge</h2>
<p>At least a familiarity with the lambda calculus, including how it is evaluated. Some base knowledge of programming languages is also assumed.</p>
<h2 id="the-problem">The problem</h2>
<p>Let's look at a little imaginary term, in some lambda-calculus-like language. For future note, we call the lambda a "binder", as it binds a variable. There are other types of binders, e.g. <code>let</code>, but we will only consider lambdas for the moment.</p>
<pre data-lang="hs"><code data-lang="hs"><span>λ f</span><span>.</span><span> (λ y f</span><span>.</span><span> (f y)) f
</span></code></pre>
<p>We can perform what's called "beta reduction" on this term — essentially, function application, applying <code>f</code> to the lambda.</p>
<pre data-lang="hs"><code data-lang="hs"><span>λ f</span><span>.</span><span> (λ y f</span><span>.</span><span> (f y)) (f)
</span><span>
</span><span>substitute [y </span><span>:=</span><span> f] </span><span>in</span><span> λ y f</span><span>.</span><span> (f y)
</span><span>
</span><span>λ f</span><span>.</span><span> (λ f</span><span>.</span><span> f f)
</span></code></pre>
<p>Uh oh. We've accidentally captured a variable! Instead of <code>f</code> referring to the outer <code>f</code>, now it refers to the inner <code>f</code>. This is "the capture problem", and it is quite annoying. Generally to avoid this, we need to rename <em>everything</em><sup id="fr-1-1"><a href="#fn-1">1</a></sup> in our "substituted" term to names that are free (do not occur) in the "subsitutee" so nothing is accidentally captured. What if we could introduce a notation that avoids this?</p>
<h2 id="presenting-de-bruijn-indexes">Presenting: De Bruijn Indexes!</h2>
<p>De Bruijn indexes are a naming scheme where:</p>
<ul>
<li>We use natural numbers to refer to lambdas, and</li>
<li>Zero refers to the "most recent" lambda; one refers to the second most recent, etc.</li>
</ul>
<p>Let's rewrite that term above using this system:</p>
<pre data-lang="hs"><code data-lang="hs"><span>λ (λ λ (</span><span>0 1</span><span>)) </span><span>0
</span></code></pre>
<p>Here's some ascii art showing what refers to what:</p>
<pre data-lang="hs"><code data-lang="hs"><span> λ   (λ   λ   (</span><span>0   1</span><span>))   </span><span>0
</span><span> </span><span>|    |    \</span><span>———</span><span>/   |     |
</span><span> </span><span>|    |            |     |
</span><span> </span><span>|    \</span><span>————————————</span><span>/     |
</span><span> </span><span>\</span><span>———————————————————————</span><span>/
</span></code></pre>
<p>Now, how does this help us with our substituion problem? Surely if we naively subtitute we will still have binding issues - and indeed we do:</p>
<pre data-lang="hs"><code data-lang="hs"><span>λ (λ λ (</span><span>0 1</span><span>)) </span><span>0
</span><span>-&gt;
</span><span>λ (λ (</span><span>0 0</span><span>))
</span></code></pre>
<p>No good!</p>
<p>What De Bruijn indexes allow us to do is simply avoid capturing. The rule is simple: Every time we go past a binder when substituting, we increment every free variable<sup id="fr-2-1"><a href="#fn-2">2</a></sup> in our substituted term by one, to avoid the new binder. Just once, we decrement every free varible in the substitutee, to account for the removal of the binder:</p>
<pre data-lang="hs"><code data-lang="hs"><span> λ (λ λ (</span><span>1 2</span><span>)) </span><span>0
</span><span> ^  ^ ^
</span><span> a  b c
</span><span> </span><span>-&gt;
</span><span> λ (λ (</span><span>1 1</span><span>))
</span><span> ^  ^  ^ ^ 
</span><span> a  b  </span><span>| \</span><span> decremented by one
</span><span>       </span><span>|
</span><span>       </span><span>\</span><span> incremented by one when we passed </span><span>"through"</span><span> lambda c
</span></code></pre>
<p>Now we're cool! Everything works as expected, and it takes much less work (and is much more predictable!).</p>
<p>At the bottom of this post there's a little widget that can convert terms to de Bruijn for you, if you want to play around!</p>
<h2 id="presenting-de-bruijn-levels">Presenting: De Bruijn levels!</h2>
<p>De Bruijn levels work similar to De Bruijn indexes, in that we use numbers to refer to binders. However, in De Bruijn levels, the lowest number refers to the <em>least</em> recently bound item.</p>
<p>Recall that:</p>
<pre data-lang="hs"><code data-lang="hs"><span>Named</span><span>:</span><span>   λ f</span><span>.</span><span> (λ y f</span><span>.</span><span> (f y)) f
</span><span>Indexes</span><span>:</span><span> λ (λ λ (</span><span>0 1</span><span>)) </span><span>0
</span></code></pre>
<p>Now, with levels:</p>
<pre data-lang="hs"><code data-lang="hs"><span>Levels</span><span>:</span><span>  λ (λ λ (</span><span>2 1</span><span>)) </span><span>0
</span></code></pre>
<p>This has the same diagram of what refers to what:</p>
<pre data-lang="hs"><code data-lang="hs"><span> λ   (λ   λ   (</span><span>2   1</span><span>))   </span><span>0
</span><span> </span><span>|    |    \</span><span>———</span><span>/   |     |
</span><span> </span><span>|    |            |     |
</span><span> </span><span>|    \</span><span>————————————</span><span>/     |
</span><span> </span><span>\</span><span>———————————————————————</span><span>/
</span></code></pre>
<p>(As it should! These two representations represent the same term.)</p>
<p>As you might expect, de Bruijn indexes and levels are each beneficial in their own situations.</p>
<p>Generally, De Bruijn indexes are "more useful" than De Bruijn levels, as they're "more local". In order to work with levels, you need to know "how deep" you are in a term at all times.</p>
<p>De Bruijn indexes give us the advantage that we can freely create new binders without the need for any information about where in a term we are, whereas de Bruijn levels give us the advantage when moving a term under a binder, free variables in said term do not need to be modified. Generally, one has many more free variables in a term than bound ones.</p>
<pre data-lang="hs"><code data-lang="hs"><span> λ (λ λ (</span><span>2 1</span><span>)) </span><span>0
</span><span>               ^ this zero</span><span>...
</span><span> </span><span>-&gt;
</span><span> λ (λ (</span><span>1 0</span><span>))
</span><span>       ^ ^ is still zero</span><span>!
</span><span>       </span><span>|
</span><span>       </span><span>\</span><span> we had to modify this one though
</span></code></pre>
<h2 id="other-advantages">Other advantages</h2>
<p>Something that can come up quite a lot in various contexts is comparing whether two terms are equal or not. There are many complicated ways to do so, but de Bruijn gives us an advantage in a critical one, called "alpha-equivalence". Consider the following two terms:</p>
<pre data-lang="hs"><code data-lang="hs"><span>λf</span><span>.</span><span> λx</span><span>.</span><span> f x
</span><span>λg</span><span>.</span><span> λy</span><span>.</span><span> g y
</span></code></pre>
<p>These terms should clearly be equal, right? They do the exact same thing. In this case, we consider them "alpha-equivalent", meaning they are equal up to the names of variables. Alpha renaming is the process of renaming one term to match the names of another, so that they are "clearly" equal.</p>
<p>Let us consider the de Bruijn index representation of both of these terms:</p>
<pre data-lang="hs"><code data-lang="hs"><span>λf</span><span>.</span><span> λx</span><span>.</span><span> f x </span><span>=&gt;</span><span> λ λ </span><span>1 0
</span><span>λg</span><span>.</span><span> λy</span><span>.</span><span> g y </span><span>=&gt;</span><span> λ λ </span><span>1 0
</span></code></pre>
<p>Isn't that nice? They've gone from being alpha-equivalent, but not quite equal, to being equal. de Bruijn gives us the ability to compare terms for equality without having to consider alpha-equivalence at all.</p>
<h2 id="wrapup">Wrapup</h2>
<p>De Bruijn indexes and levels can also be summed up via the following:</p>
<pre data-lang="hs"><code data-lang="hs"><span>λa</span><span>.</span><span> λb</span><span>.</span><span> λc</span><span>.</span><span> c
</span><span>
</span><span>indexes</span><span>:
</span><span>λ λ λ </span><span>0
</span><span>^ ^ ^
</span><span>2 1 0
</span><span>
</span><span>levels</span><span>:
</span><span>λ λ λ </span><span>2
</span><span>^ ^ ^
</span><span>0 1 2
</span></code></pre>
<p>if you’re “here”</p>
<pre><code><span>  v
</span><span>λ λ λ
</span></code></pre>
<ul>
<li>Using indexes, adding any binders further left doesn’t affect the current binder's variables, or any further right.</li>
<li>Using levels, adding any binders further right doesn’t affect the current binder's variables, or any further left.</li>
</ul>

<p>Try it out! Some example terms to try:</p>
<pre><code><span>\f x. f x
</span><span>\x x. x
</span><span>\x. (x (\y. y))
</span><span>
</span><span>\x. \y. y x -- will not work
</span><span>do \x. (\y. y x) or \x y. y x instead
</span></code></pre>
<p>(sorry, it does over-parenthesize a bit :P)</p>
<pre data-lang="hs"><code data-lang="hs" id="goober">
</code>

<h2 id="alternatives">Alternatives</h2>
<p>It is worth noting that there are several other methods for gaining the same, or similar, advantages as de Bruijn gives. This post is not intended to explain them, but I will list several here so that the curious reader may read further (tip: when searching, append "lambda calculus" to find the right results quicker):</p>
<ul>
<li>HOAS, or "Higher Order Abstract Syntax"</li>
<li>PHOAS, or "Parametric HOAS"</li>
<li>Locally nameless</li>
<li>Nominal signatures</li>
<li>Well-scoped de Bruijn indices</li>
<li>Well-scoped names</li>
<li><a href="http://doi.acm.org/10.1145/2034773.2034817">“Nameless,
Painless”</a></li>
<li>Abstract scope graphs</li>
<li>Abstract Binding Trees</li>
<li>Co-de Bruijn indices</li>
</ul>
<p>As you can see, there are many approaches! Jesper Cockx has an excellent summary of almost all of these, which can be found <a href="https://jesper.sikanda.be/posts/1001-syntax-representations.html">here.</a> Notably, many are intended for formalization efforts rather than for computational usage.</p>






      </pre></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The 'white-collar bloodbath' is all part of the AI hype machine (435 pts)]]></title>
            <link>https://www.cnn.com/2025/05/30/business/anthropic-amodei-ai-jobs-nightcap</link>
            <guid>44136117</guid>
            <pubDate>Fri, 30 May 2025 13:38:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2025/05/30/business/anthropic-amodei-ai-jobs-nightcap">https://www.cnn.com/2025/05/30/business/anthropic-amodei-ai-jobs-nightcap</a>, See on <a href="https://news.ycombinator.com/item?id=44136117">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300, &quot;image--eq-large&quot;: 660}" data-uri="cms.cnn.com/_components/image/instances/cmb9uttjf000l3b6mdvb71nsc@published" data-name="GettyImages-2194800946.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.666875" data-original-height="1067" data-original-width="1600" data-url="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2194800946.jpg?c=original" data-editable="lede" data-freewheel-lede="true">
       <picture><source height="383" width="680" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2194800946.jpg?c=16x9&amp;q=h_383,w_680,c_fill/f_webp" type="image/webp"><source height="653" width="1160" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2194800946.jpg?c=16x9&amp;q=h_653,w_1160,c_fill/f_webp" type="image/webp"><source height="605" width="1075" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2194800946.jpg?c=16x9&amp;q=h_605,w_1075,c_fill/f_webp" type="image/webp"><source height="833" width="1480" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2194800946.jpg?c=16x9&amp;q=h_833,w_1480,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2194800946.jpg?c=16x9&amp;q=h_833,w_1480,c_fill" alt="Anthropic CEO Dario Amodei is positioning himself as an AI truth-teller. He is also a salesman." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="1067" width="1600"></picture>
    </div><div data-editable="content" itemprop="articleBody" data-reorderable="content">
                  
<p data-uri="cms.cnn.com/_components/editor-note/instances/cmb9uw1ne000x3b6mwab04e4c@published" data-editable="text" data-component-name="editor-note" data-article-gutter="true">
    <em>A version of this story appeared in CNN Business’ Nightcap newsletter. To get it in your inbox, sign up for free </em><a href="https://www.cnn.com/newsletters/nightcap?source=nl-acq_article"><em>here</em></a>.
</p>

<p><cite>
      <span data-editable="location">New York</span>
      <span data-editable="source">CNN</span>
        &nbsp;—&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9us5iz000v26qi86h25b8f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            If the CEO of a soda company declared that soda-making technology is getting so good it’s going to ruin the global economy, you’d be forgiven for thinking that person is either lying or fully detached from reality.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usaka00023b6mvjpg1wiy@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Yet when tech CEOs do the same thing, people tend to perk up.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usaka00033b6msdyhk1nh@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            ICYMI: The 42-year-old billionaire Dario Amodei, who runs the AI firm Anthropic, <a href="https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic" target="_blank">told Axios</a> this week that the technology he and other companies are building could wipe out<em> half</em> of all entry-level office jobs … sometime soon. Maybe in the next couple of years, he said.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmba2k3bw00003b6mhjt9f8r6@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            He reiterated that claim <a href="https://www.cnn.com/2025/05/29/tech/ai-anthropic-ceo-dario-amodei-unemployment">in an interview</a> with CNN’s Anderson Cooper on Thursday.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmba2xmww00043b6mwuow586f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “AI is starting to get better than humans at almost all intellectual tasks, and we’re going to collectively, as a society, grapple with it,” Amodei told Cooper. “AI is going to get better at what everyone does, including what I do, including what other CEOs do.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usaka00043b6m2f2h43c2@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            To be clear, Amodei didn’t cite any research or evidence for that 50% estimate. And that was just one of many of the wild claims he made that are increasingly part of a Silicon Valley script: <em>AI will fix everything, but first it has to ruin everything. </em>Why? <em>Just trust us. </em>
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usaka00053b6mbq9ene9p@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            In this as-yet fictional world, “cancer is cured, the economy grows at 10% a year, the budget is balanced — and 20% of people don’t have jobs,” Amodei told Axios, repeating one of the industry’s favorite unfalsifiable claims about a disease-free utopia on the horizon, courtesy of AI.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb00063b6mp4u159w5@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            But how will the US economy, in particular, grow so robustly when the jobless masses can’t afford to buy anything? Amodei didn’t say.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb00073b6mlfr2rvje@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            (As an aside: I asked labor economist Aaron Sojourner about this scenario of high unemployment plus strong economic growth, and he said there <em>is </em>a theory of the case, if you squint really hard. Amodei may believe that AI can increase productivity and make each hour of labor create more goods and services. But if that’s the case, he’s imagining “a 30% jump in labor productivity to get that combination of unemployment and GDP growth,” said Sojourner, a senior researcher at the W. E. Upjohn Institute for Employment Research. “That is a wildly unprecedented vision,” he added, noting that in the 1980s and 90s, computer adoption gave the world all kinds of tools that reshaped the labor market. But labor productivity grew just 2% to 3%.)
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb00083b6myepazi91@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Anyway. The point is, Amodei is a salesman, and it’s in his interest to make his product appear inevitable and so powerful it’s <em>scary</em>. Axios framed Amodei’s economic prediction as a “white-collar bloodbath.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb00093b6mvgdk7v8m@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Even some AI optimists were put off by Amodei’s stark characterization.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000b3b6mi32rgk5e@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “Someone needs to remind the CEO that at one point there were more than (2 million) secretaries. There were also separate employees to do in office dictation,” wrote tech entrepreneur <a href="https://bsky.app/profile/mcuban.bsky.social" target="_blank">Mark Cuban on Bluesky</a>. “They were the original white collar displacements. New companies with new jobs will come from AI and increase TOTAL employment.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000c3b6mfb6fz22d@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Little of what Amodei told Axios was new, but it was calibrated to sound just outrageous enough to draw attention to Anthropic’s work, days after it released a major model update to its Claude chatbot, one of the top rivals to OpenAI’s ChatGPT.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000d3b6mm4trmbvi@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Amodei stands to profit off the very technology he claims will gut the labor market. <em>But here he is, telling everyone the truth and sounding the alarm! He’s trying to warn us, he’s one of the good ones! </em>
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000e3b6mj3tsf30u@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Yeaaahhh. So, this is kind of Anthropic’s whole ~thing.~ It refers to itself primarily as an “AI safety and research” company. They are the AI guys who see the potential harms of AI clearly — not through the rose-colored glasses worn by the techno-utopian simps over at OpenAI. (In fact, Anthropic’s founders, including Amodei, left OpenAI over ideological differences.)
    </p>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000g3b6mct0t67yk@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Look, I want to live a cancer-free utopia where I only have to work a few hours a week and there’s no poverty and stuff just <em>works</em>. But do I believe that generative AI is the key to unlocking that fantasyland? I do not. And no tech pioneers have proven their case.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000h3b6mqzpw0h4x@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Generative AI from large language models like ChatGPT and Claude are <em>really</em> good at some <em>very specific</em> stuff: They can summarize documents, write dumb emails, help kids cheat on their homework, and even recommend summer reading lists so obscure <a href="https://www.cnn.com/2025/05/28/media/ai-chatgpt-news-journalism">not even the authors knew</a> they’d written them. Heck, they could probably generate this newsletter and mimic my voice.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000i3b6mkz2975pf@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            But they hit their limits fast. They <a href="https://www.cnn.com/2025/01/03/business/meta-ai-accounts-instagram-facebook">hallucinate</a>. They get <a href="https://www.cnn.com/2025/01/14/business/wikipedia-meta-x-fact-check-nightcap">basic facts</a> wrong. They are susceptible to <a href="https://www.cnn.com/2025/05/20/business/grok-genocide-ai-nightcap">manipulation</a>. (And those are all things we human beings can do just fine on our own.)
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9us7md00003b6mspn434gn@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            If AI companies can take these handy, quasi-reliable text predictors and turn them into an economic revolution, fine. But that seems so far off in the future that Amodei’s warnings feel more like an ad than a PSA. It’s on them to show their work: Show us how AI could be so destructive <em>and </em>how Anthropic can fix it — rather than just shouting about the risks.
    </p>

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MinIO Removes Web UI Features from Community Version, Pushes Users to Paid Plans (166 pts)]]></title>
            <link>https://biggo.com/news/202505261334_MinIO_Removes_Web_UI_Features</link>
            <guid>44136108</guid>
            <pubDate>Fri, 30 May 2025 13:37:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://biggo.com/news/202505261334_MinIO_Removes_Web_UI_Features">https://biggo.com/news/202505261334_MinIO_Removes_Web_UI_Features</a>, See on <a href="https://news.ycombinator.com/item?id=44136108">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>MinIO, a popular open-source object storage solution, has made significant changes to its community version that have sparked controversy among users. The company has removed key web-based management features from the free version, directing users to either use command-line tools or upgrade to paid plans.</p>
<h3>Major Features Stripped from Community Version</h3>
<p>The latest changelog reveals that MinIO has deprecated several core management features in the web interface. Account and policy management, configuration settings, and other administrative functions are no longer available through the browser-based console. Instead, users must rely on the <code>mc</code> command-line client to perform these tasks.</p>
<p>This change affects how users interact with their MinIO deployments. Previously, administrators could manage their storage systems through an intuitive web interface. Now, they must learn command-line syntax or pay for the commercial version to regain web-based management capabilities.</p>
<p><strong>Deprecated Features in MinIO v2.0.0:</strong></p>
<ul>
<li>Account and policy management (web UI)</li>
<li>Configuration management (web UI)</li>
<li>Bucket management tools (web UI)</li>
<li>Administrative console features</li>
</ul>
<p><strong>Alternative Solutions:</strong></p>
<ul>
<li><strong>SeaweedFS</strong> - Apache 2.0 license</li>
<li><strong>Garage</strong> - AGPL license</li>
<li><strong>Zenko</strong> - Apache 2.0 license</li>
<li><strong>OpenMaxIO</strong> - Community fork of pre-change MinIO</li>
</ul>
<h3>Community Response and Concerns</h3>
<p>The decision has drawn comparisons to Redis's recent licensing changes, with users expressing frustration about the removal of functionality they previously relied on. Many see this as a classic example of enshittification - the gradual degradation of services to drive revenue.</p>
<blockquote>
<p>I think that Deprecated support has another meaning. I hate when this kind of things happens.</p>
</blockquote>
<p>Some community members are already exploring alternatives. A fork called OpenMaxIO has emerged, preserving the last version before these changes were implemented. However, its long-term viability remains uncertain.</p>
<h3>Technical Impact and Alternatives</h3>
<p>While the core storage functionality remains intact, the user experience has significantly changed. Organizations that depend on web-based management may need to retrain staff or consider migration to other solutions.</p>
<p>Several alternatives are gaining attention, including SeaweedFS, Garage, and Zenko. These projects offer S3-compatible storage with varying licensing models and feature sets. Users are actively discussing these options as potential replacements for MinIO in self-hosted environments.</p>
<h3>Looking Forward</h3>
<p>MinIO's strategy appears focused on monetizing enterprise features while maintaining the core storage engine as open source. The company argues this approach helps sustain development while serving both community and commercial users.</p>
<p>However, the timing and execution of these changes have created uncertainty in the community. Users must now decide whether to adapt to command-line management, pay for commercial licenses, or migrate to alternative solutions that better align with their needs and expectations.</p>
<p>Reference: <a href="https://github.com/minio/object-browser/blob/master/CHANGELOG.md">Changelog</a></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsandbox: Virtual Machines that feel and perform like containers (301 pts)]]></title>
            <link>https://github.com/microsandbox/microsandbox</link>
            <guid>44135977</guid>
            <pubDate>Fri, 30 May 2025 13:20:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsandbox/microsandbox">https://github.com/microsandbox/microsandbox</a>, See on <a href="https://news.ycombinator.com/item?id=44135977">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><a href="https://github.com/microsandbox/microsandbox/blob/main/#gh-dark-mode-only">
<img width="100%" src="https://github.com/microsandbox/microsandbox/raw/main/assets/microsandbox-banner-xl-dark.png" alt="microsandbox-banner-xl-dark">
</a>
<a href="https://github.com/microsandbox/microsandbox/blob/main/#gh-light-mode-only">
<img width="100%" src="https://github.com/microsandbox/microsandbox/raw/main/assets/microsandbox-banner-xl.png" alt="microsandbox-banner-xl">
</a>
<p><b>———&nbsp;&nbsp;&nbsp;easy secure execution of untrusted user/ai code&nbsp;&nbsp;&nbsp;———</b></p>

<div dir="auto">
  
  <p><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/20358651/449247679-d91df12c-e425-48c0-a881-dec9a8d45868.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80NDkyNDc2NzktZDkxZGYxMmMtZTQyNS00OGMwLWE4ODEtZGVjOWE4ZDQ1ODY4LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUyMTdkNTNhODFiZjc4YTAxNDAxNGZiNjk1MjQ3Zjc4ZDNkM2ExZDIwMjNlZmIwYTMyNTY2NWFmNjE3ZGY1MTEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.gk5TW2yoWt6XAozaC9lt4ku_Oi2unthsxkWb9QDHe3o"><img alt="Claude MCP Demo" src="https://private-user-images.githubusercontent.com/20358651/449247679-d91df12c-e425-48c0-a881-dec9a8d45868.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80NDkyNDc2NzktZDkxZGYxMmMtZTQyNS00OGMwLWE4ODEtZGVjOWE4ZDQ1ODY4LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUyMTdkNTNhODFiZjc4YTAxNDAxNGZiNjk1MjQ3Zjc4ZDNkM2ExZDIwMjNlZmIwYTMyNTY2NWFmNjE3ZGY1MTEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.gk5TW2yoWt6XAozaC9lt4ku_Oi2unthsxkWb9QDHe3o" width="500" data-animated-image=""></a>
</p></div>

<p><a href="https://docs.microsandbox.dev/" rel="nofollow">
    <img src="https://camo.githubusercontent.com/8fdae889b2d0777923548ea76a1d8c7808f9d7a84aea656418b39a220b3d11cf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63756d656e746174696f6e2d2532333030616365652e7376673f636f6c6f723d666634353030267374796c653d666f722d7468652d6261646765266c6f676f3d676974626f6f6b266c6f676f436f6c6f723d7768697465" alt="documentation" data-canonical-src="https://img.shields.io/badge/documentation-%2300acee.svg?color=ff4500&amp;style=for-the-badge&amp;logo=gitbook&amp;logoColor=white">
  </a>
  <a href="https://discord.gg/T95Y3XnEAK" rel="nofollow">
    <img src="https://camo.githubusercontent.com/242040afb1f843c8c171d2f3db596b8b59ffa6f9b550e469a3fed5ae9364837c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646973636f7264202d2532333030616365652e7376673f636f6c6f723d6d656469756d736c617465626c7565267374796c653d666f722d7468652d6261646765266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="discord" data-canonical-src="https://img.shields.io/badge/discord -%2300acee.svg?color=mediumslateblue&amp;style=for-the-badge&amp;logo=discord&amp;logoColor=white">
  </a>
</p>
<br>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/61f6bb1128a04c4d0ceae098fdbfcc0da63e888bc2176757c5194edb172d0b6e/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f7175657374696f6e2f413737304546"><img height="18" src="https://camo.githubusercontent.com/61f6bb1128a04c4d0ceae098fdbfcc0da63e888bc2176757c5194edb172d0b6e/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f7175657374696f6e2f413737304546" data-canonical-src="https://octicons-col.vercel.app/question/A770EF"></a>&nbsp;&nbsp;WHY MICROSANDBOX?</sub></h2><a id="user-content-why-microsandbox" aria-label="Permalink: &nbsp;&nbsp;WHY MICROSANDBOX?" href="#why-microsandbox"></a></div>
<p dir="auto">Ever needed to run code you don't fully trust? Whether it's AI-generated code, user submissions, or experimental code, the traditional options all have serious drawbacks:</p>
<ul dir="auto">
<li><strong>Running locally</strong> - One malicious script and your entire system is compromised</li>
<li><strong>Using containers</strong> - Shared kernels mean sophisticated attacks can still break out</li>
<li><strong>Traditional VMs</strong> - Waiting 10+ seconds for a VM to boot kills productivity and performance</li>
<li><strong>Cloud solutions</strong> - Not as flexible, at the whim of the cloud provider</li>
</ul>
<p dir="auto"><strong>microsandbox</strong> combines the best of all worlds:</p>

<p>• • •</p>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/40e8c7c287a3e8b62b4370868226dab3206da62bc66d1e44912e514260dda076/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f7a61702f413737304546"><img height="18" src="https://camo.githubusercontent.com/40e8c7c287a3e8b62b4370868226dab3206da62bc66d1e44912e514260dda076/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f7a61702f413737304546" data-canonical-src="https://octicons-col.vercel.app/zap/A770EF"></a>&nbsp;&nbsp;SDK QUICK START</sub></h2><a id="user-content-sdk-quick-start" aria-label="Permalink: &nbsp;&nbsp;SDK QUICK START" href="#sdk-quick-start"></a></div>
<p dir="auto">Get started in few easy steps:</p>

<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/50b009b96c229efe4bf861daa7f4d181bdcbb2fad3b080e552e6a304918cbf23/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61636f732d776f726b696e672d677265656e3f7374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/50b009b96c229efe4bf861daa7f4d181bdcbb2fad3b080e552e6a304918cbf23/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61636f732d776f726b696e672d677265656e3f7374796c653d666f722d7468652d6261646765" alt="macos" data-canonical-src="https://img.shields.io/badge/macos-working-green?style=for-the-badge"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a2a62abf9a85d97ef6fb29ec6aa9b8c5917fa0166deb2709c38b5c4932e1c684/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c696e75782d776f726b696e672d677265656e3f7374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/a2a62abf9a85d97ef6fb29ec6aa9b8c5917fa0166deb2709c38b5c4932e1c684/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c696e75782d776f726b696e672d677265656e3f7374796c653d666f722d7468652d6261646765" alt="linux" data-canonical-src="https://img.shields.io/badge/linux-working-green?style=for-the-badge"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ffc403a8e3e11f3fe56a1ba9257cac9907ecfb9814cd5c7312ae587237ec1bf1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f77696e646f77732d7769702d7265643f7374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/ffc403a8e3e11f3fe56a1ba9257cac9907ecfb9814cd5c7312ae587237ec1bf1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f77696e646f77732d7769702d7265643f7374796c653d666f722d7468652d6261646765" alt="windows" data-canonical-src="https://img.shields.io/badge/windows-wip-red?style=for-the-badge"></a>
</p>

<div dir="auto"><h3 tabindex="-1" dir="auto"><span>1</span>&nbsp;&nbsp;<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9be3391cb69b279615c505c1338bad12b8022eb0f3a056d3550db009f252c2db/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6e6f7274682d737461722f413737304546"><img height="13" src="https://camo.githubusercontent.com/9be3391cb69b279615c505c1338bad12b8022eb0f3a056d3550db009f252c2db/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6e6f7274682d737461722f413737304546" data-canonical-src="https://octicons-col.vercel.app/north-star/A770EF"></a>&nbsp;&nbsp;Start the Server</h3><a id="user-content-1start-the-server" aria-label="Permalink: 1&nbsp;&nbsp;&nbsp;&nbsp;Start the Server" href="#1start-the-server"></a></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Install microsandbox</h5><a id="user-content-install-microsandbox" aria-label="Permalink: Install microsandbox" href="#install-microsandbox"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -sSL https://get.microsandbox.dev | sh"><pre>curl -sSL https://get.microsandbox.dev <span>|</span> sh</pre></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">And start the server</h5><a id="user-content-and-start-the-server" aria-label="Permalink: And start the server" href="#and-start-the-server"></a></p>

<div dir="auto"><p dir="auto">Tip</p>
<p dir="auto">microsandbox server is also an <a href="https://github.com/microsandbox/microsandbox/blob/main/MCP.md">MCP server</a>, so it works directly with Claude, Agno and other MCP-enabled AI tools and agents.</p>
<p dir="auto">For more information on setting up the server, see the <a href="https://github.com/microsandbox/microsandbox/blob/main/SELF_HOSTING.md">self-hosting guide</a>.</p>
</div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Optionally pull the environment image</h5><a id="user-content-optionally-pull-the-environment-image" aria-label="Permalink: Optionally pull the environment image" href="#optionally-pull-the-environment-image"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="msb pull microsandbox/python"><pre>msb pull microsandbox/python</pre></div>

<div dir="auto"><h3 tabindex="-1" dir="auto"><span>2</span>&nbsp;&nbsp;<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5fbdfa3f26094ac97e5ea6f813976d8cbfc6feb8f261c9958bdb234a74604cc0/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6d6f76652d746f2d626f74746f6d2f413737304546"><img height="14" src="https://camo.githubusercontent.com/5fbdfa3f26094ac97e5ea6f813976d8cbfc6feb8f261c9958bdb234a74604cc0/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6d6f76652d746f2d626f74746f6d2f413737304546" data-canonical-src="https://octicons-col.vercel.app/move-to-bottom/A770EF"></a>&nbsp;&nbsp;Install the SDK</h3><a id="user-content-2install-the-sdk" aria-label="Permalink: 2&nbsp;&nbsp;&nbsp;&nbsp;Install the SDK" href="#2install-the-sdk"></a></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Python</h5><a id="user-content-python" aria-label="Permalink: Python" href="#python"></a></p>

<p dir="auto"><h5 tabindex="-1" dir="auto">JavaScript</h5><a id="user-content-javascript" aria-label="Permalink: JavaScript" href="#javascript"></a></p>

<p dir="auto"><h5 tabindex="-1" dir="auto">Rust</h5><a id="user-content-rust" aria-label="Permalink: Rust" href="#rust"></a></p>

<div dir="auto"><p dir="auto">Note</p><p dir="auto">There are <a href="https://github.com/microsandbox/microsandbox/blob/main/sdk">SDKs</a> for other languages as well! Join us in expanding support for your favorite language.</p>
</div>


<div dir="auto"><h3 tabindex="-1" dir="auto"><span>3</span>&nbsp;&nbsp;<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/79aad6df819fa3b9c51c4405ebe757a64e8fd7e07d0843e718a19ebdf66df550/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f66696c652d62696e6172792f413737304546"><img height="14" src="https://camo.githubusercontent.com/79aad6df819fa3b9c51c4405ebe757a64e8fd7e07d0843e718a19ebdf66df550/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f66696c652d62696e6172792f413737304546" data-canonical-src="https://octicons-col.vercel.app/file-binary/A770EF"></a>&nbsp;&nbsp;Execute the Code</h3><a id="user-content-3execute-the-code" aria-label="Permalink: 3&nbsp;&nbsp;&nbsp;&nbsp;Execute the Code" href="#3execute-the-code"></a></div>
<p dir="auto"><code>microsandbox</code> offers a growing list of sandbox environment types optimized for different execution requirements. Choose the appropriate sandbox (e.g., PythonSandbox or NodeSandbox) to run your code in a secure tailored environment.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Python</h5><a id="user-content-python-1" aria-label="Permalink: Python" href="#python-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
from microsandbox import PythonSandbox

async def main():
    async with PythonSandbox.create(name=&quot;test&quot;) as sb:
        exec = await sb.run(&quot;name = 'Python'&quot;)
        exec = await sb.run(&quot;print(f'Hello {name}!')&quot;)

    print(await exec.output()) # prints Hello Python!

asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>microsandbox</span> <span>import</span> <span>PythonSandbox</span>

<span>async</span> <span>def</span> <span>main</span>():
    <span>async</span> <span>with</span> <span>PythonSandbox</span>.<span>create</span>(<span>name</span><span>=</span><span>"test"</span>) <span>as</span> <span>sb</span>:
        <span>exec</span> <span>=</span> <span>await</span> <span>sb</span>.<span>run</span>(<span>"name = 'Python'"</span>)
        <span>exec</span> <span>=</span> <span>await</span> <span>sb</span>.<span>run</span>(<span>"print(f'Hello {name}!')"</span>)

    <span>print</span>(<span>await</span> <span>exec</span>.<span>output</span>()) <span># prints Hello Python!</span>

<span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">JavaScript</h5><a id="user-content-javascript-1" aria-label="Permalink: JavaScript" href="#javascript-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import { NodeSandbox } from &quot;microsandbox&quot;;

async function main() {
  const sb = await NodeSandbox.create({ name: &quot;test&quot; });

  try {
    let exec = await sb.run(&quot;var name = 'JavaScript'&quot;);
    exec = await sb.run(&quot;console.log(`Hello ${name}!`)&quot;);

    console.log(await exec.output()); // prints Hello JavaScript!
  } finally {
    await sb.stop();
  }
}

main().catch(console.error);"><pre><span>import</span> <span>{</span> <span>NodeSandbox</span> <span>}</span> <span>from</span> <span>"microsandbox"</span><span>;</span>

<span>async</span> <span>function</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>const</span> <span>sb</span> <span>=</span> <span>await</span> <span>NodeSandbox</span><span>.</span><span>create</span><span>(</span><span>{</span> <span>name</span>: <span>"test"</span> <span>}</span><span>)</span><span>;</span>

  <span>try</span> <span>{</span>
    <span>let</span> <span>exec</span> <span>=</span> <span>await</span> <span>sb</span><span>.</span><span>run</span><span>(</span><span>"var name = 'JavaScript'"</span><span>)</span><span>;</span>
    <span>exec</span> <span>=</span> <span>await</span> <span>sb</span><span>.</span><span>run</span><span>(</span><span>"console.log(`Hello ${name}!`)"</span><span>)</span><span>;</span>

    <span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>exec</span><span>.</span><span>output</span><span>(</span><span>)</span><span>)</span><span>;</span> <span>// prints Hello JavaScript!</span>
  <span>}</span> <span>finally</span> <span>{</span>
    <span>await</span> <span>sb</span><span>.</span><span>stop</span><span>(</span><span>)</span><span>;</span>
  <span>}</span>
<span>}</span>

<span>main</span><span>(</span><span>)</span><span>.</span><span>catch</span><span>(</span><span>console</span><span>.</span><span>error</span><span>)</span><span>;</span></pre></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Rust</h5><a id="user-content-rust-1" aria-label="Permalink: Rust" href="#rust-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="use microsandbox::{SandboxOptions, PythonSandbox};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut sb = PythonSandbox::create(SandboxOptions::builder().name(&quot;test&quot;).build()).await?;

    let exec = sb.run(r#&quot;name = &quot;Python&quot;&quot;#).await?;
    let exec = sb.run(r#&quot;print(f&quot;Hello {name}!&quot;)&quot;#).await?;

    println!(&quot;{}&quot;, exec.output().await?); // prints Hello Python!

    sb.stop().await?;

    Ok(())
}"><pre><span>use</span> microsandbox<span>::</span><span>{</span><span>SandboxOptions</span><span>,</span> <span>PythonSandbox</span><span>}</span><span>;</span>

<span>#<span>[</span>tokio<span>::</span>main<span>]</span></span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>(</span><span>)</span><span>,</span> <span>Box</span><span>&lt;</span><span>dyn</span> std<span>::</span>error<span>::</span><span>Error</span><span>&gt;</span><span>&gt;</span> <span>{</span>
    <span>let</span> <span>mut</span> sb = <span>PythonSandbox</span><span>::</span><span>create</span><span>(</span><span>SandboxOptions</span><span>::</span><span>builder</span><span>(</span><span>)</span><span>.</span><span>name</span><span>(</span><span>"test"</span><span>)</span><span>.</span><span>build</span><span>(</span><span>)</span><span>)</span><span>.</span><span>await</span>?<span>;</span>

    <span>let</span> exec = sb<span>.</span><span>run</span><span>(</span><span>r#"name = "Python""#</span><span>)</span><span>.</span><span>await</span>?<span>;</span>
    <span>let</span> exec = sb<span>.</span><span>run</span><span>(</span><span>r#"print(f"Hello {name}!")"#</span><span>)</span><span>.</span><span>await</span>?<span>;</span>

    <span>println</span><span>!</span><span>(</span><span>"{}"</span><span>,</span> exec<span>.</span>output<span>(</span><span>)</span><span>.</span><span>await</span>?<span>)</span><span>;</span> <span>// prints Hello Python!</span>

    sb<span>.</span><span>stop</span><span>(</span><span>)</span><span>.</span><span>await</span>?<span>;</span>

    <span>Ok</span><span>(</span><span>(</span><span>)</span><span>)</span>
<span>}</span></pre></div>
<div dir="auto"><p dir="auto">Note</p>
<p dir="auto">If you haven't pulled the environment image, the first run will take a while as it tries to download it.
Executions will be much faster afterwards.</p>
<p dir="auto">For more information on how to use the SDK, <a href="https://github.com/microsandbox/microsandbox/blob/main/sdk/README.md">check out the SDK README</a>.</p>
</div>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f5303d49a785448f3173be66839bd28738f13d941b1ff1274e2b28b394f4099e/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6465766963652d6465736b746f702f413737304546"><img height="18" src="https://camo.githubusercontent.com/f5303d49a785448f3173be66839bd28738f13d941b1ff1274e2b28b394f4099e/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6465766963652d6465736b746f702f413737304546" data-canonical-src="https://octicons-col.vercel.app/device-desktop/A770EF"></a>&nbsp;&nbsp;PROJECTS&nbsp;&nbsp;<sup><sup>B E T A</sup></sup></sub></h2><a id="user-content-projectsb-e-t-a" aria-label="Permalink: &nbsp;&nbsp;PROJECTS&nbsp;&nbsp;" href="#projectsb-e-t-a"></a></div>
<p dir="auto">Beyond the SDK, microsandbox supports project-based development with the familiar package-manager workflow devs are used to. Think of it like npm or cargo, but for sandboxes!</p>
<p dir="auto">Create a <code>Sandboxfile</code>, define your environments, and manage your sandboxes with simple commands.</p>

<p dir="auto"><a href="https://asciinema.org/a/7eOFf2Ovigi473FsKgr3Lpve1" rel="nofollow"><img src="https://private-user-images.githubusercontent.com/20358651/444812608-3a9d1de4-2370-4d5a-a40d-9aa7315aa934.svg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80NDQ4MTI2MDgtM2E5ZDFkZTQtMjM3MC00ZDVhLWE0MGQtOWFhNzMxNWFhOTM0LnN2Zz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZhNDY5MTljNzg4N2QwOTlmMzZjMmRmZDg1Njk1MjFiZWQ0NmYzNWEzOTYyZmM1NjAyZTA5MTJlN2M1Y2FlN2QmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.6Z1aX5qI-n5wHOSWsHSkYUMVuABUdrXwlfhudO6COQY" secured-asset-link=""></a></p>

<p dir="auto"><h4 tabindex="-1" dir="auto">Create a Sandbox Project</h4><a id="user-content-create-a-sandbox-project" aria-label="Permalink: Create a Sandbox Project" href="#create-a-sandbox-project"></a></p>

<p dir="auto">This creates a <code>Sandboxfile</code> in the current directory, which serves as the configuration manifest for your sandbox environments.</p>

<p dir="auto"><h4 tabindex="-1" dir="auto">Add a Sandbox to the Project</h4><a id="user-content-add-a-sandbox-to-the-project" aria-label="Permalink: Add a Sandbox to the Project" href="#add-a-sandbox-to-the-project"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="msb add app \
    --image python \
    --cpus 1 \
    --memory 1024 \
    --start 'python -c &quot;print(\&quot;hello\&quot;)&quot;'"><pre>msb add app \
    --image python \
    --cpus 1 \
    --memory 1024 \
    --start <span><span>'</span>python -c "print(\"hello\")"<span>'</span></span></pre></div>
<p dir="auto">The command above registers a new sandbox named <code>app</code> in your Sandboxfile, configured to use the <code>python</code> image.</p>
<p dir="auto">You should now have a <code>Sandboxfile</code> containing a sandbox named <strong><code>app</code></strong>:</p>

<div dir="auto" data-snippet-clipboard-copy-content="# Sandbox configurations
sandboxes:
  app:
    image: python
    memory: 1024
    cpus: 1
    scripts:
      start: python -c &quot;print(\&quot;hello\&quot;)&quot;"><pre><span><span>#</span> Sandbox configurations</span>
<span>sandboxes</span>:
  <span>app</span>:
    <span>image</span>: <span>python</span>
    <span>memory</span>: <span>1024</span>
    <span>cpus</span>: <span>1</span>
    <span>scripts</span>:
      <span>start</span>: <span>python -c "print(\"hello\")"</span></pre></div>
<div dir="auto"><p dir="auto">Tip</p>
<p dir="auto">Run <code>msb &lt;subcommand&gt; --help</code> to see all the options available for a subcommand.</p>
<p dir="auto">For example, <code>msb add --help</code>.</p>
</div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Running a Sandbox</h4><a id="user-content-running-a-sandbox" aria-label="Permalink: Running a Sandbox" href="#running-a-sandbox"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Run a Sandbox Defined in Your Project</h5><a id="user-content-run-a-sandbox-defined-in-your-project" aria-label="Permalink: Run a Sandbox Defined in Your Project" href="#run-a-sandbox-defined-in-your-project"></a></p>

<p dir="auto"><em><strong>or</strong></em></p>

<p dir="auto">This executes the default <em>start</em> script of your sandbox. For more control, you can directly specify which script to run — <code>msr app~start</code>.</p>
<p dir="auto">When running project sandboxes, all file changes and installations made inside the sandbox are automatically persisted to the <code>./menv</code> directory. This means you can stop and restart your sandbox any time without losing your work. Your development environment will be exactly as you left it.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Run an Temporary Sandbox</h5><a id="user-content-run-an-temporary-sandbox" aria-label="Permalink: Run an Temporary Sandbox" href="#run-an-temporary-sandbox"></a></p>
<p dir="auto">For experimentation or one-off tasks, temporary sandboxes provide a clean environment that leaves no trace:</p>

<p dir="auto"><em><strong>or</strong></em></p>

<p dir="auto">Temporary sandboxes are perfect for isolating programs you get from the internet. Once you exit the sandbox, all changes are completely discarded.</p>

<p dir="auto"><h4 tabindex="-1" dir="auto">Installing Sandboxes</h4><a id="user-content-installing-sandboxes" aria-label="Permalink: Installing Sandboxes" href="#installing-sandboxes"></a></p>
<p dir="auto">The <code>msb install</code> command sets up a sandbox as a system-wide executable. It installs a slim launcher program that allows you to start your sandbox from anywhere in your system with a simple command.</p>
<div dir="auto" data-snippet-clipboard-copy-content="msb install --image alpine"><pre>msb install --image alpine</pre></div>
<p dir="auto"><em><strong>or</strong></em></p>

<p dir="auto">After installation, you can start your sandbox by simply typing its name in any terminal:</p>

<p dir="auto">This makes frequently used sandboxes incredibly convenient to access — no need to navigate to specific directories or remember complex commands. Just type the sandbox name and it launches immediately with all your configured settings.</p>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">You can give your sandbox a descriptive, easy-to-remember name during installation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="msi alpine:20250108 slim-linux"><pre>msi alpine:20250108 slim-linux</pre></div>
<p dir="auto">This allows you to create multiple instances of the same sandbox image with different names and configurations. For example:</p>
<ul dir="auto">
<li><code>msi python python-data-science</code> - A Python environment for data analysis</li>
<li><code>msi python python-web</code> - A Python environment for web development</li>
</ul>
<p dir="auto">Installed sandboxes maintain their state between sessions, so you can pick up exactly where you left off each time you launch them.</p>
</div>
<p>• • •</p>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ad892701da53ec646f4ba6a987c2e8d494a92f3091ee89eb87055869a8d9fd26/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6c696768742d62756c622f413737304546"><img height="18" src="https://camo.githubusercontent.com/ad892701da53ec646f4ba6a987c2e8d494a92f3091ee89eb87055869a8d9fd26/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6c696768742d62756c622f413737304546" data-canonical-src="https://octicons-col.vercel.app/light-bulb/A770EF"></a>&nbsp;&nbsp;USE CASES</sub></h2><a id="user-content-use-cases" aria-label="Permalink: &nbsp;&nbsp;USE CASES" href="#use-cases"></a></div>
<p dir="auto"><a href="https://microsandbox.dev/#gh-dark-mode-only" rel="nofollow"><img width="400" alt="coding-dark" src="https://private-user-images.githubusercontent.com/20358651/439591910-37c14bf1-e2f7-4af3-804e-5901de845715.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5MTAtMzdjMTRiZjEtZTJmNy00YWYzLTgwNGUtNTkwMWRlODQ1NzE1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTY4NmE4ZGUyNDFlNmZkMzBjMWQ4ZmVlNDQ1YmIzZjc3NjEzNmQ1YTIzMTM5OGZiNzM1NDNiZTA1MzViZGE0OWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.ZFmRDi-CnFjJAuxYX2Vgkb0pnJIYdd2nUkymMfV-2zY" secured-asset-link=""></a>
<a href="https://microsandbox.dev/#gh-light-mode-only" rel="nofollow"><img width="400" alt="coding-light" src="https://private-user-images.githubusercontent.com/20358651/439591915-1bfe7223-869b-4782-9fce-3620c4400bbf.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5MTUtMWJmZTcyMjMtODY5Yi00NzgyLTlmY2UtMzYyMGM0NDAwYmJmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU4MGRhOGYyMGFhNDYxNGNmMWFmOGFmZTdiOTdkMDIwYTcwNWUzYjQwMGM1NGI2NWJjYTM2ZDYxODRjYmIyNDkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.AGy2Va3-Rsis74vuvgVu3ZkuuNY0R4HHG6IcdHdQFIo" secured-asset-link=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Coding &amp; Dev Environments</h3><a id="user-content-coding--dev-environments" aria-label="Permalink: Coding &amp; Dev Environments" href="#coding--dev-environments"></a></p>
<p dir="auto">Let your AI agents build real apps with professional dev tools. When users ask their AI to create a web app, fix a bug, or build a prototype, it can handle everything from Git operations to dependency management to testing in a protected environment.</p>
<p dir="auto">Your AI can create complete development environments in milliseconds and run programs with full system access. The fast startup means developers get instant feedback and can iterate quickly. This makes it perfect for AI pair programming, coding education platforms, and automated code generation where quick results matter.</p>


<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"><img width="2000" height="0" src="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"></a><br></p>
<p dir="auto"><a href="https://microsandbox.dev/#gh-dark-mode-only" rel="nofollow"><img width="400" alt="data-dark" src="https://private-user-images.githubusercontent.com/20358651/439591922-3794e426-a223-4064-8939-025c7bbaf5ea.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5MjItMzc5NGU0MjYtYTIyMy00MDY0LTg5MzktMDI1YzdiYmFmNWVhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWMzNDVjZDFkZGRlMWQwZTA5YjU2MzQyMTM2ZjY1OTNjNmI4NTQ2YWMzZmU0ZGIyZjgxYzk1YTI0ZmZhZDZiMWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.0EbXbUPdkX72Pd-tuYSflHQbZ3ilE2IVCbLvqoXyUJc" secured-asset-link=""></a>
<a href="https://microsandbox.dev/#gh-light-mode-only" rel="nofollow"><img width="400" alt="data-light" src="https://private-user-images.githubusercontent.com/20358651/439591924-3a330ea5-85b5-4176-8fe7-a43d59733cf1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5MjQtM2EzMzBlYTUtODViNS00MTc2LThmZTctYTQzZDU5NzMzY2YxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA0ZDM5ZjlmYjFiYjg4ZTVhNmNmYzY1NDQ2NzM2MjljNDZmOGRiOWM0MmUyZTI1NDAxNTAwNTE3OTJiNDQ5NjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.NuNBLsbzskA9rpISLivpv9G6T40OSIUEUFs_7VdCwdk" secured-asset-link=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Data Analysis</h3><a id="user-content-data-analysis" aria-label="Permalink: Data Analysis" href="#data-analysis"></a></p>
<p dir="auto">Transform raw numbers into meaningful insights with AI that works for you. Your AI can process spreadsheets, create charts, and generate reports safely. Whether it's analyzing customer feedback, sales trends, or research data, everything happens in a protected environment that respects data privacy.</p>
<p dir="auto">Microsandbox lets your AI work with powerful libraries like NumPy, Pandas, and TensorFlow while creating visualizations that bring insights to life. Perfect for financial analysis tools, privacy-focused data processing, medical research, and any situation where you need serious computing power with appropriate safeguards.</p>


<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"><img width="2000" height="0" src="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"></a><br></p>
<p dir="auto"><a href="https://microsandbox.dev/#gh-dark-mode-only" rel="nofollow"><img width="400" alt="web-dark" src="https://private-user-images.githubusercontent.com/20358651/439591945-3048a39a-c3cb-4f6e-9bc0-49b404abed03.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5NDUtMzA0OGEzOWEtYzNjYi00ZjZlLTliYzAtNDliNDA0YWJlZDAzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWYxMDI0NGYzZWZjMjYxNjA0NjVkOTJjMDk3YmIzYWUxYjFhNmUzZWU4YWJkMWQ1YzRlNDg3MGViNjU1NmMxMTQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.UiM5q82CY69VvRKTvIBHE812ixXzbpWSWJRP0g5Sy4g" secured-asset-link=""></a>
<a href="https://microsandbox.dev/#gh-light-mode-only" rel="nofollow"><img width="400" alt="web-light" src="https://private-user-images.githubusercontent.com/20358651/439591949-e6a01e6d-c23f-4c04-bfbf-3e0cb283e0a9.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5NDktZTZhMDFlNmQtYzIzZi00YzA0LWJmYmYtM2UwY2IyODNlMGE5LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdmOGM5NzVjZjlmNjQxN2E0OTFmZGJiNjYzYTRjZTg0YTM4MGQ5ZmM1MDE4YzhiNTJhYzk1NjljN2NhMTY5MTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.WuAtWiNxnGrsrOxEmqXnDsYP5koHuIbpFtLVa0iL6TM" secured-asset-link=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Web Browsing Agent</h3><a id="user-content-web-browsing-agent" aria-label="Permalink: Web Browsing Agent" href="#web-browsing-agent"></a></p>
<p dir="auto">Build AI assistants that can browse the web for your users. Need to compare prices across stores, gather info from multiple news sites, or automate form submissions? Your AI can handle it all while staying in a contained environment.</p>
<p dir="auto">With microsandbox, your AI can navigate websites, extract data, fill out forms, and handle logins. It can visit any site and deliver only the useful information back to your application. This makes it ideal for price comparison tools, research assistants, content aggregators, automated testing, and web automation workflows that would otherwise require complex setup.</p>


<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"><img width="2000" height="0" src="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"></a><br></p>
<p dir="auto"><a href="https://microsandbox.dev/#gh-dark-mode-only" rel="nofollow"><img width="400" alt="host-dark" src="https://private-user-images.githubusercontent.com/20358651/439591929-3c542e78-b5a0-4525-8a2a-376447d786fd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5MjktM2M1NDJlNzgtYjVhMC00NTI1LThhMmEtMzc2NDQ3ZDc4NmZkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk2ZjE2ODc3ZTE0NjRmZTU4M2M5Y2YxYjAwZDY5YjY4MjcxNTM5N2EyZGNhMzJjYmVkZGQyMzY2NjgxOWMxOWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.pTo7v6ek0fWFTy9CnWc2lZhJlhPsQIyQ3HElR44Aiss" secured-asset-link=""></a>
<a href="https://microsandbox.dev/#gh-light-mode-only" rel="nofollow"><img width="400" alt="host-light" src="https://private-user-images.githubusercontent.com/20358651/439591935-337b3d5f-9c33-4126-ae55-aca33abbf73e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5MzUtMzM3YjNkNWYtOWMzMy00MTI2LWFlNTUtYWNhMzNhYmJmNzNlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFiMGZjNTYxNmI4ZDZhOWQ5MzYzOThkNzNhNmJmZTk0YjQ3ZTg5YjdlZDJhZTk4YTEyZDZiN2FjOGYwZDMxMmMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.vvut9PH8pwy55jmWXdzKwFWmE3F4EHC6bXGfT8hEe-A" secured-asset-link=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Instant App Hosting</h3><a id="user-content-instant-app-hosting" aria-label="Permalink: Instant App Hosting" href="#instant-app-hosting"></a></p>
<p dir="auto">Share working apps and demos in seconds without deployment headaches. When your AI creates a useful tool, calculator, visualization, or prototype, users can immediately access it through a simple link.</p>
<p dir="auto">Zero-setup deployment means your AI-generated code can be immediately useful without complex configuration. Each app runs in its own protected space with appropriate resource limits, and everything cleans up automatically when no longer needed. Perfect for educational platforms hosting student projects, AI assistants creating live demos, and users needing immediate value.</p>


<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"><img width="2000" height="0" src="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"></a><br></p>
<p>• • •</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Server Architecture</h3><a id="user-content-the-server-architecture" aria-label="Permalink: The Server Architecture" href="#the-server-architecture"></a></p>
<section data-identity="25a61435-60a8-4cdd-b155-b0aff3bf5e44" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div dir="auto" data-json="{&quot;data&quot;:&quot;flowchart TB\n    %% ── Client side ──────────────────────────\n    subgraph ClientProcess[\&quot;process\&quot;]\n        A[\&quot;Your Business Logic\&quot;]\n        B[\&quot;microsandbox SDK\&quot;]\n        A --&amp;gt;|calls| B\n    end\n\n    %% ── Server side ─────────────────────────\n    subgraph ServerProcess[\&quot;process\&quot;]\n        C[\&quot;microsandbox server\&quot;]\n    end\n    B --&amp;gt;|sends untrusted code to| C\n\n    %% ── Branching hub ───────────────────────\n    D([ ])\n    C --&amp;gt;|runs code in| D\n\n    %% ── Individual MicroVMs ────────────────\n    subgraph VM1[\&quot;microVM\&quot;]\n        VM1S[\&quot;python environment\&quot;]\n    end\n\n    subgraph VM2[\&quot;microVM\&quot;]\n        VM2S[\&quot;python environment\&quot;]\n    end\n\n    subgraph VM3[\&quot;microVM\&quot;]\n        VM3S[\&quot;node environment\&quot;]\n    end\n\n    D --&amp;gt; VM1S\n    D --&amp;gt; VM2S\n    D --&amp;gt; VM3S\n\n    %% ── Styling ─────────────────────────────\n    style A    fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000\n    style B    fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000\n    style C    fill:#D5F5E3,stroke:#28B463,stroke-width:2px,color:#000000\n    style D    fill:#F4F6F6,stroke:#ABB2B9,stroke-width:2px,color:#000000\n    style VM1S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000\n    style VM2S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000\n    style VM3S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000\n&quot;}" data-plain="flowchart TB
    %% ── Client side ──────────────────────────
    subgraph ClientProcess[&quot;process&quot;]
        A[&quot;Your Business Logic&quot;]
        B[&quot;microsandbox SDK&quot;]
        A -->|calls| B
    end

    %% ── Server side ─────────────────────────
    subgraph ServerProcess[&quot;process&quot;]
        C[&quot;microsandbox server&quot;]
    end
    B -->|sends untrusted code to| C

    %% ── Branching hub ───────────────────────
    D([ ])
    C -->|runs code in| D

    %% ── Individual MicroVMs ────────────────
    subgraph VM1[&quot;microVM&quot;]
        VM1S[&quot;python environment&quot;]
    end

    subgraph VM2[&quot;microVM&quot;]
        VM2S[&quot;python environment&quot;]
    end

    subgraph VM3[&quot;microVM&quot;]
        VM3S[&quot;node environment&quot;]
    end

    D --> VM1S
    D --> VM2S
    D --> VM3S

    %% ── Styling ─────────────────────────────
    style A    fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000
    style B    fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000
    style C    fill:#D5F5E3,stroke:#28B463,stroke-width:2px,color:#000000
    style D    fill:#F4F6F6,stroke:#ABB2B9,stroke-width:2px,color:#000000
    style VM1S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
    style VM2S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
    style VM3S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
">
      <pre lang="mermaid" aria-label="Raw mermaid code">flowchart TB
    %% ── Client side ──────────────────────────
    subgraph ClientProcess["process"]
        A["Your Business Logic"]
        B["microsandbox SDK"]
        A --&gt;|calls| B
    end

    %% ── Server side ─────────────────────────
    subgraph ServerProcess["process"]
        C["microsandbox server"]
    end
    B --&gt;|sends untrusted code to| C

    %% ── Branching hub ───────────────────────
    D([ ])
    C --&gt;|runs code in| D

    %% ── Individual MicroVMs ────────────────
    subgraph VM1["microVM"]
        VM1S["python environment"]
    end

    subgraph VM2["microVM"]
        VM2S["python environment"]
    end

    subgraph VM3["microVM"]
        VM3S["node environment"]
    end

    D --&gt; VM1S
    D --&gt; VM2S
    D --&gt; VM3S

    %% ── Styling ─────────────────────────────
    style A    fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000
    style B    fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000
    style C    fill:#D5F5E3,stroke:#28B463,stroke-width:2px,color:#000000
    style D    fill:#F4F6F6,stroke:#ABB2B9,stroke-width:2px,color:#000000
    style VM1S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
    style VM2S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
    style VM3S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
</pre>
    </div>
  <span role="presentation">
    <span data-view-component="true">
      <span>Loading</span>
</span>
  </span>
</section>

<p>• • •</p>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b42435ba5b647f8e510635976cf776c376b5b7ba76fee683187a1d73f1a25818/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f676561722f413737304546"><img height="18" src="https://camo.githubusercontent.com/b42435ba5b647f8e510635976cf776c376b5b7ba76fee683187a1d73f1a25818/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f676561722f413737304546" data-canonical-src="https://octicons-col.vercel.app/gear/A770EF"></a>&nbsp;&nbsp;DEVELOPMENT</sub></h2><a id="user-content-development" aria-label="Permalink: &nbsp;&nbsp;DEVELOPMENT" href="#development"></a></div>
<p dir="auto">Interested in contributing to microsandbox? Check out our <a href="https://github.com/microsandbox/microsandbox/blob/main/DEVELOPMENT.md">Development Guide</a> for instructions on setting up your development environment, building the project, running tests, and creating releases.</p>
<p dir="auto">For contribution guidelines, please refer to <a href="https://github.com/microsandbox/microsandbox/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a>.</p>
<p>• • •</p>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/799bc80710f1e8cfbe1adf44cdb7303084bfcf1a45146f197fc95169ad608a75/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6c61772f413737304546"><img height="18" src="https://camo.githubusercontent.com/799bc80710f1e8cfbe1adf44cdb7303084bfcf1a45146f197fc95169ad608a75/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6c61772f413737304546" data-canonical-src="https://octicons-col.vercel.app/law/A770EF"></a>&nbsp;&nbsp;LICENSE</sub></h2><a id="user-content-license" aria-label="Permalink: &nbsp;&nbsp;LICENSE" href="#license"></a></div>
<p dir="auto">This project is licensed under the <a href="https://github.com/microsandbox/microsandbox/blob/main/LICENSE">Apache License 2.0</a>.</p>
<p>• • •</p>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/dad3a1ab46102e0e2d93daf04343d8b63df297fe2a3037ffe9d947795c36c125/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f737461722f413737304546"><img height="18" src="https://camo.githubusercontent.com/dad3a1ab46102e0e2d93daf04343d8b63df297fe2a3037ffe9d947795c36c125/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f737461722f413737304546" data-canonical-src="https://octicons-col.vercel.app/star/A770EF"></a>&nbsp;&nbsp;STAR HISTORY</sub></h2><a id="user-content-star-history" aria-label="Permalink: &nbsp;&nbsp;STAR HISTORY" href="#star-history"></a></div>
<p dir="auto">Thanks for all the support!</p>
<div dir="auto">
  <a href="https://star-history.com/#microsandbox/microsandbox&amp;Date" rel="nofollow">
   <themed-picture data-catalyst-inline="true"><picture>
     <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/313494caffe2d6e0e4c608b75f8a2fed3e5f3ee12795dbeb0460b657073f965c/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6d6963726f73616e64626f782f6d6963726f73616e64626f7826747970653d44617465267468656d653d6461726b" data-canonical-src="https://api.star-history.com/svg?repos=microsandbox/microsandbox&amp;type=Date&amp;theme=dark">
     <source media="(prefers-color-scheme: light)" srcset="https://camo.githubusercontent.com/faa075c8d4446396ba6d126bd328eec80dd740b02bc92bef4c2241ca93ca6d98/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6d6963726f73616e64626f782f6d6963726f73616e64626f7826747970653d44617465" data-canonical-src="https://api.star-history.com/svg?repos=microsandbox/microsandbox&amp;type=Date">
     <img alt="Star History Chart" src="https://camo.githubusercontent.com/faa075c8d4446396ba6d126bd328eec80dd740b02bc92bef4c2241ca93ca6d98/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6d6963726f73616e64626f782f6d6963726f73616e64626f7826747970653d44617465" data-canonical-src="https://api.star-history.com/svg?repos=microsandbox/microsandbox&amp;type=Date">
   </picture></themed-picture>
  </a>
</div>

<p>• • •</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Systems Correctness Practices at Amazon Web Services (332 pts)]]></title>
            <link>https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/</link>
            <guid>44135638</guid>
            <pubDate>Fri, 30 May 2025 12:43:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/">https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/</a>, See on <a href="https://news.ycombinator.com/item?id=44135638">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en"><section id="sec1"><p id="p-1">Amazon Web Services (AWS) strives to deliver reliable services that customers can trust completely. This requires maintaining the highest standards of security, durability, integrity, and availability—with systems correctness serving as the cornerstone for achieving these priorities. An April 2015 article published in <i>Communications of the ACM</i>, titled “How Amazon Web Services Uses Formal Methods,” highlighted the approach for ensuring the correctness of critical services that have since become among the most widely used by AWS customers.<a href="#B21" data-jats-ref-type="bibr" data-jats-rid="B21"><sup>21</sup></a></p><p id="p-2">Central to this approach was TLA+,<a href="#B14" data-jats-ref-type="bibr" data-jats-rid="B14"><sup>14</sup></a> a formal specification language developed by Leslie Lamport. Our experience at AWS with TLA+ revealed two significant advantages of applying formal methods in practice. First, we could identify and eliminate subtle bugs early in development—bugs that would have eluded traditional approaches such as testing. Second, we gained the deep understanding and confidence needed to implement aggressive performance optimizations while maintaining systems correctness.</p><p id="p-3">Moreover, 15 years ago, AWS’s software testing practice relied primarily on build-time unit testing, often against mocks, and limited deployment-time integration testing. Since then, we have significantly evolved our correctness practices, integrating both formal and semi-formal approaches into the development process. As AWS has grown, formal methods have become increasingly valuable—not only for ensuring correctness but also for performance improvements, particularly in verifying the correctness of both low- and high-level optimizations. This systematic approach toward systems correctness has become a force multiplier at AWS’s scale, enabling faster development cycles through improved developer velocity while delivering more cost-effective services to customers.</p><p id="p-4">This article surveys the portfolio of formal methods used across AWS to deliver complex services with high confidence in its correctness. We consider an umbrella definition of formal methods that encompasses these rigorous techniques—from traditional formal approaches (such as theorem proving,<a href="#B7" data-jats-ref-type="bibr" data-jats-rid="B7"><sup>7</sup></a><sup>,</sup><a href="#B10" data-jats-ref-type="bibr" data-jats-rid="B10"><sup>10</sup></a> deductive verification,<a href="#B18" data-jats-ref-type="bibr" data-jats-rid="B18"><sup>18</sup></a> and model checking<a href="#B8" data-jats-ref-type="bibr" data-jats-rid="B8"><sup>8</sup></a><sup>,</sup><a href="#B14" data-jats-ref-type="bibr" data-jats-rid="B14"><sup>14</sup></a>) to more lightweight semi-formal approaches (such as property-based testing,<a href="#B6" data-jats-ref-type="bibr" data-jats-rid="B6"><sup>6</sup></a><sup>,</sup><a href="#B19" data-jats-ref-type="bibr" data-jats-rid="B19"><sup>19</sup></a> fuzzing,<a href="#B9" data-jats-ref-type="bibr" data-jats-rid="B9"><sup>9</sup></a> and runtime monitoring<a href="#B11" data-jats-ref-type="bibr" data-jats-rid="B11"><sup>11</sup></a>).</p></section><section id="sec2"><h2>The P Programming Language</h2><p id="p-5">As the use of formal methods was expanded beyond the initial teams at AWS in the early 2010s, we discovered that many engineers struggled to learn and become productive with TLA+. This difficulty seemed to stem from TLA+’s defining feature: It is a high-level, abstract language that more closely resembles mathematics than the imperative programming languages most developers are familiar with. While this mathematical nature is a significant strength of TLA+, and we continue to agree with Lamport’s views on the benefits of mathematical thinking,<a href="#B15" data-jats-ref-type="bibr" data-jats-rid="B15"><sup>15</sup></a> we also sought a language that would allow us to model check (and later prove) key aspects of systems design while being more approachable to programmers.</p><p id="p-6">We found this balance in the P programming language.<a href="#B8" data-jats-ref-type="bibr" data-jats-rid="B8"><sup>8</sup></a> P is a state-machine-based language for modeling and analysis of distributed systems. Using P, developers model their system designs as communicating state machines, a mental model familiar to Amazon’s developer population—most of whom develop systems based on microservices and service-oriented architectures (SOAs). P has been developed at AWS since 2019 and is maintained as a strategic open source project.<a href="#B22" data-jats-ref-type="bibr" data-jats-rid="B22"><sup>22</sup></a> Teams across AWS that build some of its flagship products—from storage (for example, Amazon S3, EBS), to databases (for example, Amazon DynamoDB, MemoryDB, Aurora), to compute (for example, EC2, IoT)—have been using P to reason about the correctness of their system designs.</p><p id="p-7">For example, P was used in migrating Simple Storage Service (S3) from eventual to strong read-after-write consistency.<a href="#B1" data-jats-ref-type="bibr" data-jats-rid="B1"><sup>1</sup></a> A key component of S3 is its index subsystem, an object metadata store that enables fast data lookups. To achieve strong consistency, the S3 team had to make several nontrivial changes to the S3 index protocol stack.<a href="#B25" data-jats-ref-type="bibr" data-jats-rid="B25"><sup>25</sup></a> Because these changes were difficult to get right at S3 scale, and the team wanted to deliver strong consistency with high confidence in correctness, they used P to formally model and validate the protocol design. P helped eliminate several design-level bugs early in the development process and allowed the team to deliver risky optimizations with confidence, as they could be validated using model checking.</p><p id="p-8">In 2023, the P team at AWS built PObserve, which provides a new tool for validating the correctness of distributed systems both during testing and in production. With PObserve, we take structured logs from the execution of distributed systems and validate post hoc that they match behaviors allowed by the formal P specification of the system. This allows for bridging the gap between the P specification of the system design and the production implementation (typically in languages like Rust or Java). While there are significant benefits from verifying protocols at design time, runtime monitoring of the same properties for the implementation makes the investment in formal specification much more valuable and addresses classic concerns with the deployment of formal methods in practice (that is, connecting design-time validation with system implementation).</p></section><section id="sec3"><h2>Lightweight Formal Methods</h2><p id="p-9">Another way that AWS has brought formal methods closer to its engineering teams is through the adoption of <i>lightweight formal methods</i>.</p><section id="sec4"><p data-jats-content-type="inline-heading"><strong>Property-based testing.</strong>&nbsp; The most notable single example of leveraging lightweight formal methods is in Amazon S3’s ShardStore, where the team used property-based testing throughout the development cycle both to test correctness and to speed up development (described in detail by Bornholt, et al.<a href="#B4" data-jats-ref-type="bibr" data-jats-rid="B4"><sup>4</sup></a>). The key idea in their approach was combining property-based testing with developer-provided correctness specifications, coverage-guided fuzzing (an approach where the distribution of inputs is guided by code coverage metrics), failure injection (where hardware and other system failures are simulated during testing), and minimization (where counterexamples are automatically reduced to aid human-guided debugging).</p></section><section id="sec5"><p data-jats-content-type="inline-heading"><strong>Deterministic simulation.</strong>&nbsp; Another lightweight method widely used at AWS is deterministic simulation testing, in which a distributed system is executed on a single-threaded simulator with control over all sources of randomness, such as thread scheduling, timing, and message delivery order. Tests are then written for particular failure or success scenarios, such as the failure of a participant at a particular stage in a distributed protocol. The nondeterminism in the system is controlled by the test framework, allowing developers to specify orderings they believe are interesting (such as ones that have caused bugs in the past). The scheduler in the testing framework can also be extended for fuzzing of orderings or exploring all possible orderings to be tested.</p><p id="p-12">Deterministic simulation testing moves testing of system properties, like behavior under delay and failure, closer to build time instead of integration testing. This accelerates development and provides for more complete behavioral coverage during testing. Some of the work done at AWS on build-time testing of thread ordering and systems failures has been open sourced as part of the shuttle<a href="#FN1" data-jats-rid="FN1" data-jats-ref-type="fn"><sup>a</sup></a> and turmoil<a href="#FN2" data-jats-rid="FN2" data-jats-ref-type="fn"><sup>b</sup></a> projects.</p></section><section id="sec6"><p data-jats-content-type="inline-heading"><strong>Continuous fuzzing or random test-input generation.</strong>&nbsp; Continuous fuzzing, especially coverage-guided scalable test-input generation, is also effective for testing systems correctness at integration time. During the development of Amazon Aurora’s data-sharding feature (Aurora Limitless Database<a href="#B3" data-jats-ref-type="bibr" data-jats-rid="B3"><sup>3</sup></a>), for example, we made extensive use of fuzzing to test two key properties of the system. First, by fuzzing SQL queries (and entire transactions), we validated that the logic partitioning SQL execution over shards is correct. Large volumes of random SQL schemas, datasets, and queries are synthesized and run through the engines under test, and the results compared with an oracle based on the non-sharded version of the engine (as well as other approaches to validation, like those pioneered by SQLancer<a href="#B23" data-jats-ref-type="bibr" data-jats-rid="B23"><sup>23</sup></a>).</p><p id="p-14">Fuzzing, combined with fault-injection testing, is also useful for testing other aspects of database correctness such as atomicity, consistency, and isolation. In database testing, transactions are automatically generated, their correct behavior is defined using a formally specified correctness oracle, and then all possible interleaving of transactions and statements within the transaction is executed against the system under test. We also use post hoc validation of properties such as isolation (following approaches such as Elle<a href="#B13" data-jats-ref-type="bibr" data-jats-rid="B13"><sup>13</sup></a>).</p></section></section><section id="sec7"><h2>Fault Injection as a Service</h2><p id="p-15">In early 2021, AWS launched Fault Injection Service (FIS)<a href="#B2" data-jats-ref-type="bibr" data-jats-rid="B2"><sup>2</sup></a> with the goal of making testing based on fault injection accessible to a wide range of AWS customers. FIS allows customers to inject simulated faults, from API errors to I/O pauses and failed instances, into test or production deployments of their infrastructure on AWS. Injecting faults allows customers to validate that the resiliency mechanisms they have built into their architectures (such as failovers and health checks) actually improve availability and do not introduce correctness problems. Fault-injection testing based on FIS is widely used by AWS customers and internally within Amazon. For example, Amazon.com ran 733 FIS-based fault-injection experiments in preparation for Prime Day 2024.</p><p id="p-16">In 2014, Yuan et al. found that 92% of catastrophic failures in tested distributed systems were triggered by incorrect handling of nonfatal errors. Many distributed-systems practitioners who were told about this research were surprised the percentage was not higher. Happy-case catastrophic failures are rare simply because the happy case of systems is executed often, tested better (both implicitly and explicitly), and is significantly simpler than the error cases. Fault-injection testing and FIS make it much easier for practitioners to test the behavior of their systems under faults and failures, closing the gap between happy-case and error-case bug density.</p><p id="p-17">While fault injection is not considered a formal method, it can be combined with formal specifications. Defining the expected behavior using a formal specification, and then comparing results during and after fault injection to the specified behavior, allows for catching a lot more bugs than simply checking for errors in metrics and logs (or having a human look and say, “Yup, that looks about right”).</p></section><section id="sec8"><h2>Metastability and Emergent System Behavior</h2><p id="p-18">Over the past decade, there has been an emerging interest in a particular class of systems failure: those where some triggering event (like an overload or a cache emptying) causes a distributed system to enter a state where it does not recover without intervention (such as reducing load below normal). This class of failures, dubbed <i>metastable failures</i>,<a href="#B5" data-jats-ref-type="bibr" data-jats-rid="B5"><sup>5</sup></a> is one of the most important contributors to unavailability in cloud systems. The figure, adapted from Bronson et al.,<a href="#B5" data-jats-ref-type="bibr" data-jats-rid="B5"><sup>5</sup></a>&nbsp;illustrates a common type of metastable behavior: Load increases on the system are initially met with increasing goodput, followed by saturation, followed by congestion and goodput dropping to zero (or near zero). From there, the system cannot return to healthy state by slightly reducing load. Instead, it must follow the dotted line and may not recover until load is significantly reduced. This type of behavior is present even in simple systems. For example, it can be triggered in most systems with timeout-and-retry client logic.</p><figure id="UF1"><p><a data-fslightbox="https://cacm.acm.org/wp-content/uploads/2025/04/3729175_fig01.jpg" data-type="image" data-caption="Figure. Metastable system behavior under load." href="https://cacm.acm.org/wp-content/uploads/2025/04/3729175_fig01.jpg">
				<img decoding="async" title="Figure. Metastable system behavior under load." src="https://cacm.acm.org/wp-content/uploads/2025/04/3729175_fig01.jpg" alt="Metastable system behavior under load." data-image-id="UF1" data-image-type="figure">
			</a>
		</p><figcaption><span>Figure.&nbsp;</span> <span>Metastable system behavior under load.</span></figcaption></figure><p id="p-19">Traditional formal approaches to modeling distributed systems typically focus on <i>safety</i> (nothing bad happens) and <i>liveness</i> (something good eventually happens), but metastable failures remind us that systems have a variety of behaviors that cannot be neatly categorized this way. We have increasingly turned to discrete-event simulation to understand the emergent behavior of systems, investing both in custom-built systems simulations and tooling that allow the use of existing system models (built in languages such as TLA+ and P) to simulate system behavior. Extending exhaustive model checkers, like TLA+’s TLC with probabilistic simulations, also allows for the generation of statistical results such as posterior latency distributions, making model checking useful for tasks such as understanding the achievability of latency service-level agreements (SLAs).</p></section><section id="sec9"><h2>Formal Proof</h2><p id="p-20">In some cases, the formal methods enumerated so far in this article are not sufficient. For critical security boundaries such as authorization and virtualization, for example, proofs of correctness can be both desirable and worth the significant investment needed to create them.</p><p id="p-21">In 2023, AWS introduced the Cedar authorization policy language for writing policies that specify fine-grained permissions. Cedar was designed for automated reasoning and formal proof.<a href="#B12" data-jats-ref-type="bibr" data-jats-rid="B12"><sup>12</sup></a><sup>,</sup><a href="#B24" data-jats-ref-type="bibr" data-jats-rid="B24"><sup>24</sup></a> The language was designed to be well-suited for proof, and the implementation was built in the verification-aware programming language Dafny. Using Dafny, the team was able to prove that the implementation satisfies a variety of security properties. This type of proof goes beyond testing. It is a proof in the mathematical sense. The team also applied a differential testing approach using the Dafny code as a correctness oracle to verify the correctness of the production-ready Rust implementation. Publishing the Dafny code and test procedures as open source, along with the Cedar implementation, allows Cedar users to check the team’s work on correctness.</p><p id="p-22">Another example is the Firecracker virtual machine monitor (VMM). Firecracker uses a low-level protocol called <i>virtio</i> to expose emulated hardware devices (such as a network card or solid-state drive) to guest kernels running inside the VM. This emulated device is a critical security boundary because it is the most complex interaction between the untrusted guest and trusted host. The Firecracker team used a tool called Kani<a href="#B20" data-jats-ref-type="bibr" data-jats-rid="B20"><sup>20</sup></a> that can reason formally about Rust code to prove key properties of this security boundary. Again, proof here goes beyond testing and ensures that the critical properties of this boundary are held no matter what the guest attempts to do.</p><p id="p-23">Proofs around the behaviors of programs are an important part of AWS’s software correctness program, so we support development on tools such as Kani, Dafny,<a href="#B18" data-jats-ref-type="bibr" data-jats-rid="B18"><sup>18</sup></a> and Lean,<a href="#B16" data-jats-ref-type="bibr" data-jats-rid="B16"><sup>16</sup></a> and the underlying tools—such as satisfiability modulo theory (SMT) solvers—that power them.</p><p id="p-24">The ability to use formal models and specifications—for model-checking systems at design time, for validating in-production behavior using runtime monitoring by serving as a correctness oracle, for simulating emergent systems behavior, and for building proofs of critical properties—allows AWS to amortize the engineering effort of developing these specifications over a larger amount of business and customer value.</p></section><section id="sec10"><h2>Benefits Beyond Correctness</h2><p id="p-25">Finally, as discussed in the aforementioned 2015 paper, formal methods are a crucial part of safely improving the performance of cloud systems. Modeling a key commit protocol for the Aurora relational database engine in P and TLA+ allowed us to identify an opportunity to reduce the cost of distributed commits from 2 to 1.5 network roundtrips without sacrificing any safety properties. These kinds of stories are usual for teams that adopt formal methods, driven by at least two different dynamics.</p><p id="p-26">First, the act of deeply thinking about and formally writing down distributed protocols forces a structured way of thinking that leads to deeper insights about the structure of protocols and the problem to be solved.&nbsp;Second, having the ability to formally check (and, in some cases, prove) that proposed design optimizations are correct allows naturally conservative distributed-systems engineers to be bolder in their protocol design choices without increasing risk and boosting the developer velocity toward delivering reliable services.</p><p id="p-27">These productivity and cost benefits are limited not only to high-level design optimizations but also to low-level code that normally gets ignored. In one example, the AWS team identified optimizations to the implementation of the Rivest-Shamir-Adleman (RSA) public-key encryption scheme on our ARM-based Graviton 2 processor, which could improve throughput by up to 94%.<a href="#B17" data-jats-ref-type="bibr" data-jats-rid="B17"><sup>17</sup></a></p><p id="p-28">Using the HOL Light interactive theorem prover, the team was able to prove the correctness of these optimizations. Given the high percentage of cloud CPU cycles spent on cryptography, this type of optimization can significantly reduce infrastructure costs and aid sustainability while at the same time improving customer-visible performance.</p></section><section id="sec11"><h2>Challenges and Opportunities for the Future</h2><p id="p-29">Despite significant success in scaling formal and semi-formal testing methods across AWS over the past 15 years, several challenges persist, particularly in industrial adoption of formal methods. The primary barriers for formal methods tools include their steep learning curve and the specialized domain expertise required. Additionally, many of these tools remain academic in nature and lack user-friendly interfaces.</p><p id="p-30">Even well-established semi-formal approaches face adoption challenges. For example, deterministic simulation, a key distributed-systems testing technique used successfully at AWS and in projects like FoundationDB, remains unfamiliar to many experienced distributed-systems developers joining AWS. Similar gaps exist in the adoption of other proven methodologies, such as fault-injection testing, property-based testing, and fuzzing. The challenge is educating distributed-systems developers about these testing methods and tools, teaching the art of rigorous thinking.</p><p id="p-31">The education gap begins at the academic level, where even basic formal reasoning approaches are rarely taught, making it difficult for graduates from top institutions to adopt these tools. Although formal methods and automated reasoning are crucial for industry applications, they continue to be viewed as niche fields. We anticipate that increased industry adoption of formal methods and automated reasoning will attract more talent to this domain.</p><p id="p-32">Metastability and other emergent properties of large-scale systems represent another critical research area facing similar awareness challenges. Common practices that lead to metastable system behavior, such as “retry N times on timeout,” continue to be widely recommended despite their known issues. Current tools and techniques for understanding emergent system behavior are still in their early stages, making system stability modeling expensive and complex. Ongoing research in this area holds promising potential for advancement.</p><p id="p-33">Looking ahead, we believe large language models and AI assistants will significantly help address the adoption challenges of formal methods in practice. Just as AI-assisted unit testing has gained popularity, these tools are expected soon to help developers create formal models and specifications, making these advanced techniques more accessible to the broader developer community.</p></section><section id="sec12"><h2>Conclusion</h2><p id="p-34">Building reliable and secure software requires a range of approaches to reason about systems correctness. Alongside industry-standard testing methods (such as unit and integration testing), AWS has adopted model checking, fuzzing, property-based testing, fault-injection testing, deterministic simulation, event-based simulation, and runtime validation of execution traces. Formal methods have been an important part of the development process—perhaps most importantly, formal specifications as test oracles that provide the correct answers for many of AWS’s testing practices. Correctness testing and formal methods remain key areas of investment at AWS, accelerated by the excellent returns seen on investments in these areas already.</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Darwin Gödel Machine: AI that improves itself by rewriting its own code (185 pts)]]></title>
            <link>https://sakana.ai/dgm/</link>
            <guid>44135369</guid>
            <pubDate>Fri, 30 May 2025 12:08:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sakana.ai/dgm/">https://sakana.ai/dgm/</a>, See on <a href="https://news.ycombinator.com/item?id=44135369">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <header>
  <h2><a href="https://sakana.ai/dgm/">The Darwin Gödel Machine: AI that improves itself by rewriting its own code</a></h2><time datetime="2025-05-30T00:00:00+09:00">May 30, 2025</time>
</header>

  <p><img src="https://sakana.ai/assets/dgm/dgm-main.png" width="100%"><br>
<!--<img class="b-lazy" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-main.png" style="width: 100%;"/><br/>--></p>

<!--more-->

<h2 id="summary">Summary</h2>

<p>A <a href="https://www.cs.mcgill.ca/~dprecup/courses/AI/Materials/turing1950.pdf">longstanding goal</a> of AI research has been the creation of AI that can learn <em>indefinitely</em>. One tantalizing path toward that goal is an AI that improves itself by rewriting its own code, including any code responsible for learning. That idea, known as a <a href="https://en.wikipedia.org/wiki/G%C3%B6del_machine">Gödel Machine</a>, proposed by <a href="https://people.idsia.ch/~juergen/goedelmachine.html">Jürgen Schmidhuber</a> decades ago, is a hypothetical self-improving AI. It optimally solves problems by recursively rewriting its own code when it can mathematically prove a better strategy, making it a key concept in <a href="https://people.idsia.ch/~juergen/metalearning.html#secME">meta-learning</a> or “learning to learn.”</p>

<p>While the theoretical Gödel Machine promised <em>provably</em> beneficial self-modifications, its realization relied on an impractical assumption: that the AI could mathematically <em>prove</em> that a proposed change in its own code would yield a net improvement before adopting it. We, in collaboration with Jeff Clune’s lab at UBC, propose something more feasible: a system that harnesses the <em>principles</em> of open-ended algorithms like Darwinian evolution to search for improvements that <em>empirically</em> improve performance.</p>

<p>We call the result the <strong>Darwin Gödel Machine</strong> (<a href="https://arxiv.org/abs/2505.22954">full technical report</a>). DGMs leverage foundation models to <a href="https://arxiv.org/abs/2206.08896">propose code improvements</a>, and use <a href="https://arxiv.org/abs/2408.08435">recent</a> <a href="https://arxiv.org/abs/2306.01711">innovations</a> in open-ended algorithms to search for a growing library of diverse, high-quality AI agents. Our experiments show that DGMs improve themselves the more compute they are provided. In line with <a href="https://arxiv.org/abs/1905.10985">the clear trend</a> that AI systems that rely on <em>learning</em> ultimately outperform those designed by hand, there is a potential that DGMs could soon outperform hand-designed AI systems.</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-animation.gif"><br>
<small><i>The <b>Darwin Gödel Machine</b> is a self-improving coding agent that rewrites its own code to improve performance on programming tasks. It creates various self-improvements, such as a patch validation step, better file viewing, enhanced editing tools, generating and ranking multiple solutions to choose the best one, and adding a history of what has been tried before (and why it failed) when making new changes.</i></small></p>

<video autoplay="" muted="" playsinline="" loop=""><source src="https://sakana.ai/assets/dgm/dgm-code-evolution-smaller.mp4"></video>

<p>For further details please read our <a href="https://arxiv.org/abs/2505.22954">Technical Report</a> and <a href="https://github.com/jennyzzt/dgm">released code</a>.</p>



<h2 id="introduction">Introduction</h2>

<p>Most current AI systems learn during training only. Then their intelligence is locked in place and deployed. Could they instead, like humans, or the entire community of human scientists, continue to learn and self-improve <em>forever</em>? Moreover, could such self-improvement catalyze future self-improvement?</p>

<p><img data-src="/assets/dgm/dgm-conceptual.png" src="https://sakana.ai/assets/dgm/dgm-conceptual.png"><br>
<small><i>The <b>Darwin Gödel Machine</b> iteratively builds a growing archive of agents by harnessing the principles of open-ended exploration. New agents are created and scored by interleaving self-modification with downstream task evaluation.</i></small></p>

<p>Our <em>**Darwin Gödel Machine (DGM) **</em>is a step in that direction. Although we believe the full potential of self-modification is much broader than the capabilities offered by existing agentic systems, DGMs can also be applied to practical, <a href="https://arxiv.org/abs/2408.08435">agentic tasks</a>, which combine foundation models with tools, such as web search, or workflows, such as creating three potential answers and choosing the best one. This first DGM is a coding agent that has the ability to:</p>

<ol>
  <li>
    <p><strong>Read and Modify Its Own Code</strong>: It understands and can modify its own Python codebase to try to self-improve (e.g., adding a new tool, or suggesting a different workflow).</p>
  </li>
  <li>
    <p><strong>Evaluate if the Change Improves Performance</strong>: Proposed new versions of itself are evaluated on coding benchmarks (like <a href="https://www.swebench.com/original.html">SWE-bench</a> and <a href="https://aider.chat/docs/leaderboards/">Polyglot</a>). As our results show, improved performance on coding challenges means it has also gotten better at improving itself.</p>
  </li>
  <li>
    <p><strong>Open-endedly Explore the AI Design Space</strong>: New agents are added to an ever-expanding archive of interesting agents. Harnessing <a href="https://books.google.ca/books?id=Llb1CAAAQBAJ&amp;printsec=frontcover&amp;source=gbs_atb&amp;redir_esc=y#v=onepage&amp;q&amp;f=false">the</a> <a href="https://www.nature.com/articles/s42256-018-0006-z">power</a> <a href="https://arxiv.org/abs/1905.10985">of</a> <a href="https://arxiv.org/abs/2408.08435">open</a>-<a href="https://arxiv.org/abs/2306.01711">ended</a> <a href="https://sakana.ai/ai-scientist/">algorithms</a>, future self-modifications can then branch off from any agent in this growing archive, allowing for parallel exploration of many different evolutionary paths. This open-ended exploration helps DGM discover truly novel solutions and avoid getting trapped in suboptimal designs.</p>
  </li>
</ol>

<p>If done safely (see our section dedicated to safety below), such self-improving AI could help us take advantage of the <a href="https://www.darioamodei.com/essay/machines-of-loving-grace">tremendous benefits</a> for society that AI has the potential to usher in.</p>



<h2 id="results">Results</h2>

<p>Experiments demonstrate the Darwin Gödel Machine can continuously self-improve by modifying its own codebase. That is true both on <a href="https://www.swebench.com/original.html">SWE-bench</a> (a widely used benchmark requiring agents to resolve real-world GitHub issues) and <a href="https://aider.chat/docs/leaderboards/">Polyglot</a> (a multi-language coding benchmark). On SWE-bench, the DGM automatically improved its performance from 20.0% to 50.0%. On Polyglot, the DGM jumped performance from an initial 14.2% to 30.7%, which far surpasses the representative hand-designed agent by <a href="https://aider.chat/">Aider</a>. These substantial gains demonstrate the DGM’s ability to discover and implement beneficial changes to its own code.</p>

<p>We also experimentally validate that the DGM’s ability to modify its own code speeds up its own learning (see below). A control without self-improvement performs far worse (DGM w/o Self-improve in the below figure). Moreover, experiments also show that harnessing the principles of open-ended algorithms was also key: without the Darwinian-evolution-inspired search based on growing an archive of different stepping stones, thus allowing <a href="https://www.researchgate.net/publication/277131571_Innovation_Engines_Automated_Creativity_and_Improved_Stochastic_Optimization_via_Deep_Learning">goal switching</a> and the exploration of many different paths in parallel, performance was lower (DGM w/o Open-ended exploration in the below figure).</p>

<p><img data-src="/assets/dgm/dgm-self-improve.png" src="https://sakana.ai/assets/dgm/dgm-self-improve.png"><br>
<small><i><b>Self-improvement</b> and <b>open-ended exploration</b> enable the DGM to continue making progress and improve its performance. The DGM automatically discovers increasingly better coding agents and performs better on both SWE-bench (Left) and Polyglot (Right). It outperforms baselines that lack either self-improvement or open-ended exploration, showing that both components are essential for continual self-improvement.</i></small></p>

<p>The lineage of DGM’s self-improvements on SWE-Bench is visualized in the figure below, which depicts the evolutionary tree of agents generated within its growing archive. This branching structure is a direct consequence of DGM’s open-ended exploration strategy, where it samples from a diverse pool of previously generated agents to seed new modifications. This process enables DGMs to explore multiple evolutionary pathways simultaneously. We observed that some less-performant “ancestor” agents, which might have been discarded by simpler hill-climbing optimization, were instrumental in discovering novel features or approaches that led to significant performance breakthroughs in their descendants, thereby avoiding premature convergence on suboptimal solutions.</p>

<p>The following 2 figures illustrate how the Darwin Gödel Machine automatically self-improves to become a better coding agent:</p>

<center>
<img data-src="/assets/dgm/dgm-archive-tree.png" src="https://sakana.ai/assets/dgm/dgm-archive-tree.png"></center>
<p><small><i>Archive of coding agents generated during the DGM run on SWE-bench.</i></small><br></p>
<center>
<img data-src="/assets/dgm/dgm-progress.png" src="https://sakana.ai/assets/dgm/dgm-progress.png"></center>
<p><small><i>Performance progress and a summary of key innovations of the DGM on SWE-bench. The dark line shows the lineage of the final best-discovered agent. The DGM discovers different tools for improved file editing and a patch strategy that combines multiple generations with ranking. Note that the path to the final best agent sometimes involved agents that performed worse than their parents, highlighting the benefits of open-ended search, including keeping an archive of interesting stepping stones to explore from, rather than only branching off from the highest-performing agent discovered so far.</i></small></p>



<p>Crucially, the DGM engineered improvements within its own agent design that proved to be fundamental and broadly transferable, not merely adaptations overfit to its model or task. The improvements discovered by the DGM (e.g., better tools, refined workflows) generalize to produce higher performance across different underlying FMs. For example, an agent optimized with Claude 3.5 Sonnet also showed improved performance when powered by o3-mini or Claude 3.7 Sonnet (left two panels in the figure below). This shows that the DGM discovers general agent design improvements rather than just model-specific tricks.</p>

<p><img data-src="/assets/dgm/dgm-transfer.png" src="https://sakana.ai/assets/dgm/dgm-transfer.png"><br>
<small><i><b>Agent transfer between models and tasks.</b>  The superior performance of DGM-discovered agents can be transferred across different models (Left and Middle) and different task domains (Right), such as from Python tasks to tasks in other languages like Rust, C++, Go, and others.</i></small></p>

<p>This transferability was also evident across programming languages. A DGM variant whose self-improvement process was exclusively guided by its performance on Python tasks within the Polyglot benchmark subsequently demonstrated significant performance gains on tasks in entirely different programming languages (like Rust, C++, and Go) from the same benchmark (right panel in above figure).</p>

<p>These results collectively show that the Darwin Gödel Machine, through its self-referential code modification and open-ended exploration, can autonomously discover and implement increasingly sophisticated and generalizable improvements to AI agents.</p>



<h2 id="dgm-and-ai-safety-building-trustworthy-self-improvement">DGM and AI Safety: Building Trustworthy Self-Improvement</h2>

<p>The prospect of AI systems that autonomously enhance their own capabilities naturally brings the <a href="https://managing-ai-risks.com/">important topic</a> of AI Safety to the forefront. When an AI can rewrite its own code, it is crucial that its development is safe and aligns with human intentions. Modifications optimized solely for improving performance on a benchmark could introduce unintended behaviors or create systems too complicated for manual human oversight.</p>

<p>From the outset, DGM has been developed with safety in mind. All self-modifications and evaluations occur within secure, sandboxed environments, under human supervision and with strict limits on access to the web. The DGM archive also provides a transparent, traceable lineage of every change, and our reviews of the changes that occurred in our experiments show modifications focused solely on improving coding capabilities. Please see our <a href="https://arxiv.org/abs/2505.22954">paper</a> for a more thorough review of our safety practices and findings.</p>

<p>Beyond mitigating risks, self-improvement itself could offer a novel pathway to <em>enhance</em> AI safety. In preliminary investigations (detailed in our <a href="https://arxiv.org/abs/2505.22954">paper</a>), we tasked DGM with addressing its own tendency to sometimes pretend to use external tools instead of actually using them (including making up the output from the tools). The results are encouraging: DGM began to identify and propose solutions to these problems.</p>

<p>However, we did notice, and documented in our paper, instances when the DGM hacked its reward function. For example, we had cases where it hallucinated that it was using external tools, such as a command line tool that runs unit tests that determine if the code is functioning properly. It faked a log making it look like it had run the tests and that they had passed, when in fact they were never run! Because these logs become its context, it later mistakenly thought its proposed code changes had passed all the unit tests.</p>

<p><img data-src="/assets/dgm/dgm-hallucination.png" src="https://sakana.ai/assets/dgm/dgm-hallucination.png"><br>
<small><i><b>Example of a model hallucinating using a command line tool.</b> In this case the running of unit tests that check if the code is functioning properly. The model seems to know it should run the unit tests, but rather than doing so, it just creates a fake log that makes it look like it ran the tests and that they all passed! </i></small></p>

<p>We conducted an experiment to see if DGM could fix this issue. We created a “tool use hallucination” reward function and asked the DGM to optimize that score to solve the hallucination issue. In many instances, DGM did provide solutions that addressed this type of hallucination. However, in some cases, it removed the markers we use in the reward function to detect hallucination (despite our explicit instruction not to do so), hacking our hallucination detection function to report false successes. We were able to detect that because DGM provides a transparent, traceable lineage of every change that allows us to quickly catch such undesirable behaviors. While in this case the issue was easily detected, more work is needed to prevent the model from attempting to cheat in this way in the first place.</p>

<p><img data-src="/assets/dgm/dgm-objective-hacking.png" src="https://sakana.ai/assets/dgm/dgm-objective-hacking.png"><br>
<small><i><b>Example of objective hacking.</b> The figure shows a DGM solution that achieved a perfect score on a test of whether it was hallucinating tool use  by (despite instructions not to) removing special tool-use markers we added to detect such hallucinations , sabotaging the hallucination detection function.</i></small></p>

<p>Most broadly, we think safety should be front and center in any research on self-improving AI. The community needs to continue to research how to explore this promising, powerful, area responsibly, including creating systems that are aligned throughout their self-improvement, or even enhance their own safety, transparency, and alignment. These are key research directions we are committed to exploring further.</p>



<h2 id="conclusion">Conclusion</h2>

<p>Darwin Gödel Machines represent a concrete step towards AI systems that can autonomously gather their own stepping stones to learn and innovate forever. Future work will involve scaling up the approach and even letting it improve the training of the foundation models at its core. We must prioritize safety in this research because, if we can explore this direction safely, it has the possibility to unlock untold benefits for society, including enabling us to reap the benefits of accelerated scientific progress much sooner.</p>



<hr>

<h3 id="citation">Citation</h3>

<p>Darwin Gödel Machine: Open-Ended Evolution of Self-Improving Agents</p>

<p>Jenny Zhang (※), Shengran Hu (※), Cong Lu, Robert Lange (†), Jeff Clune (†)</p>

<p>(※) co-first author</p>

<p>(†) co-senior author</p>

<p>Paper:  <a href="https://arxiv.org/abs/2505.22954">https://arxiv.org/abs/2505.22954</a></p>

<p>Code: <a href="https://github.com/jennyzzt/dgm">https://github.com/jennyzzt/dgm</a></p>

<hr>




<center>
<a href="https://sakana.ai/careers/"><img src="https://sakana.ai/assets/dgm/dgm-fish.jpeg" width="80%"></a><br>
</center>





<p>Want to make the AI that improves AI? Please see our <a href="https://sakana.ai/careers/">Careers</a> page for more information.</p>



  
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What is the best LLM for consumer grade hardware? (213 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44134896</link>
            <guid>44134896</guid>
            <pubDate>Fri, 30 May 2025 11:02:19 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44134896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="44135283"><td></td></tr>
                <tr id="44137215"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44137215" href="https://news.ycombinator.com/vote?id=44137215&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; If you want to run LLMs locally then the localllama community is your friend: <a href="https://old.reddit.com/r/LocalLLaMA/" rel="nofollow">https://old.reddit.com/r/LocalLLaMA/</a></p><p>For folks new to reddit, it's worth noting that LocalLlama, just like the rest of the internet but especially reddit, is filled with misinformed people spreading incorrect "facts" as truth, and you really can't use the upvote/downvote count as an indicator of quality or how truthful something is there.</p><p>Something that is more accurate but put in a boring way will often be downvoted, while straight up incorrect but funny/emotional/"fitting the group think" comments usually get upvoted.</p><p>For us who've spent a lot of time on the web, this sort of bullshit detector is basically built-in at this point, but if you're new to places where the group think is so heavy as on reddit, it's worth being careful taking anything at face value.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137262"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44137262" href="https://news.ycombinator.com/vote?id=44137262&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>This is entirely why I can't bring myself to use it. The groupthink and virtue signaling is <i>intense</i>, when it's not just extremely low effort crud that rises to the top. And yes, before anyone says, I know, "curate." No, thank you.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137332"><td></td></tr>
                <tr id="44137419"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44137419" href="https://news.ycombinator.com/vote?id=44137419&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I understand that the core similarities are there, but I disagree. The comparisons have been around since I started browsing HN years ago. The moderation on this site, for one, emphasizes constructive conversation and discussion in a way that most subreddits can only dream of.</p><p>It also helps that the target audience has been filtered with that moderation, so over time this site (on average) skews more technical and informed.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137755"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_44137755" href="https://news.ycombinator.com/vote?id=44137755&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Frankly, no. As an obvious example that can be stated nowadays: musk has always been an over-promising liar.</p><p>Eg just look at the 2012+ videos of thunderf00t.</p><p>Yet people were literally banned here just for pointing out that he hasn't actually delivered on anything in the capacity he promised until he did the salute.</p><p>It's pointless to list other examples, as this page is- as dingnuts pointed out - exactly the same and most people aren't actually willing to change their opinion based on arguments. They're set in their opinions and think everyone else is dumb.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44137702"><td></td></tr>
                <tr id="44137746"><td></td></tr>
                                    <tr id="44136239"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136239" href="https://news.ycombinator.com/vote?id=44136239&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Yes at this point it's starting to become almost a matter of how much you like the model's personality since they're all fairly decent. OP just has to start downloading and trying them out. With 16GB one can do partial DDR5 offloading with llama.cpp and run anything up to about 30B (even dense) or even more at a "reasonable" speed for chat purposes. Especially with tensor offload.</p><p>I wouldn't count Qwen as that much of a conversationalist though. Mistral Nemo and Small are pretty decent. All of Llama 3.X are still very good models even by today's standards. Gemma 3s are great but a bit unhinged. And of course QwQ when you need GPT4 at home. And probably lots of others I'm forgetting.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135941"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44135941" href="https://news.ycombinator.com/vote?id=44135941&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I'd also recommend you go with something like 8b, so you can have the other 8GB of vram for a decent sized context window. There's tons of good 8b ones, as mentioned above. If you go for the largest model you can fit, you'll have slower inference (as you pass in more tokens) and smaller context.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136192"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136192" href="https://news.ycombinator.com/vote?id=44136192&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I think your recommendation falls within</p><p>&gt; all of them will have some strengths and weaknesses</p><p>Sometimes a higher parameter model with less quantization and low context will be the best, sometimes lower parameter model with some quantization and huge context will be the best, sometimes high parameter count + lots of quantization + medium context will be the best.</p><p>It's really hard to say one model is better than another in a general way, since it depends on so many things like your use case, the prompts, the settings, quantization, quantization method and so on.</p><p>If you're building/trying to build stuff depending on LLMs in any capacity, the first step is coming up with your own custom benchmark/evaluation that you can run with your specific use cases being put under test. Don't share this publicly (so it doesn't end up in the training data) and run it in order to figure out what model is best for that specific problem.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137226"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44137226" href="https://news.ycombinator.com/vote?id=44137226&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I’m curious (as someone who knows nothing about this stuff!)—the context window is basically a record of the conversation so far and other info that isn’t part of the model, right?</p><p>I’m a bit surprised that 8GB is useful as a context window if that is the case—it just seems like you could fit a ton of research papers, emails, and textbooks in 2GB, for example.</p><p>But, I’m commenting from a place of ignorance and curiosity. Do models blow up the info in the context window, maybe do some processing to pre-digest it?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137306"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44137306" href="https://news.ycombinator.com/vote?id=44137306&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Yes, every token is expanded into a vector that can be many thousand of dimensions. The vectors are stored for every token and every layer.</p><p>You absolutely can not fill even a single research paper in 2 GB much less an entire book.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44137490"><td></td></tr>
            <tr id="44136037"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136037" href="https://news.ycombinator.com/vote?id=44136037&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>8b is the number of parameters. The most common quant is 4 bits per parameter so 8b params is roughly 4GB of VRAM. (Typically more like 4.5GB)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136708"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44136708" href="https://news.ycombinator.com/vote?id=44136708&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>The number of quantized bits is a trade off between size and quality. Ideally you should be aiming for a 6-bit or 5-bit model. I've seen some models be unstable at 4-bit (where they will either repeat words or start generating random words).</p><p>Anything below 4-bits is usually not worth it unless you want to experiment with running a 70B+ model -- though I don't have any experience of doing that, so I don't know how well the increased parameter size balances the quantization.</p><p>See <a href="https://github.com/ggml-org/llama.cpp/pull/1684">https://github.com/ggml-org/llama.cpp/pull/1684</a> and <a href="https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9" rel="nofollow">https://gist.github.com/Artefact2/b5f810600771265fc1e3944228...</a> for comparisons between quantization levels.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137105"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44137105" href="https://news.ycombinator.com/vote?id=44137105&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; The number of quantized bits is a trade off between size and quality. Ideally you should be aiming for a 6-bit or 5-bit model. I've seen some models be unstable at 4-bit (where they will either repeat words or start generating random words).</p><p>Note that that's a skill issue of whoever quantized the model. In general quantization even as low as 3-bit can be almost loseless when you do quantization-aware finetuning[1] (and apparently you don't even need that many training tokens), but even if you don't want to do any extra training you can be smart as to which parts of the model you're quantizing and by how much to minimize the damage (e.g. in the worst case over-quantizing even a <i>single</i> weight can have disastrous consequences[2])</p><p>Some time ago I ran an experiment where I finetuned a small model while quantizing parts of it to 2-bits to see which parts are most sensitive (the numbers are the final loss; lower is better):</p><pre><code>    1.5275   mlp.downscale
    1.5061   mlp.upscale
    1.4665   mlp.gate
    1.4531   lm_head
    1.3998   attn.out_proj
    1.3962   attn.v_proj
    1.3794   attn.k_proj
    1.3811   input_embedding
    1.3662   attn.q_proj
    1.3397   unquantized baseline
</code></pre><p>
So as you can see quantizing some parts of the model affects it more strongly. The downprojection in the MLP layers is the most sensitive part of the model (which also matches with what [2] found), so it makes sense to quantize this part of the model less and instead quantize other parts more strongly. But if you'll just do the naive "quantize everything in 4-bit" then sure, you might get broken models.</p><p>[1] - <a href="https://arxiv.org/pdf/2502.02631" rel="nofollow">https://arxiv.org/pdf/2502.02631</a>
[2] - <a href="https://arxiv.org/pdf/2411.07191" rel="nofollow">https://arxiv.org/pdf/2411.07191</a></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="44136498"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136498" href="https://news.ycombinator.com/vote?id=44136498&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>With a 16GB GPU you can comfortably run like Qwen3 14B or Mistral Small 24B models at Q4 to Q6 and still have plenty of context space and get much better abilities than an 8B model.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44135524"><td></td></tr>
                <tr id="44135604"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44135604" href="https://news.ycombinator.com/vote?id=44135604&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; Beyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding.</p><p>Thank you for thinking of the vibe coders.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44135928"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44135928" href="https://news.ycombinator.com/vote?id=44135928&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; Released today; probably the best reasoning model in 8B size.</p><p>Actually DeepSeek-R1-0528-Qwen3-8B was uploaded Thursday (yesterday) at 11 AM UTC / 7 PM CST.
I had to check if a new version came out since! I am waiting for the other sizes! ;D</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44137711"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137711" href="https://news.ycombinator.com/vote?id=44137711&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>At 16GB a Q4 quant of Mistral Small 3.1, or Qwen3-14B at FP8, will probably serve you best. You'd be cutting it a little close on context length due to the VRAM usage... If you want longer context, a Q4 quant of Qwen3-14B will be a bit dumber than FP8 but will leave you more breathing room. Mistral Small can take images as input, and Qwen3 will be a bit better at math/coding; YMMV otherwise.</p><p>Going below Q4 isn't worth it IMO. If you want significantly more context, probably drop down to a Q4 quant of Qwen3-8B rather than continuing to lobotomize the 14B.</p><p>Since you're on a Blackwell-generation Nvidia chip, using LLMs quantized to NVFP4 specifically will provide some speed improvements at some quality cost compared to FP8 (and will be faster than Q4 GGUF, although ~equally dumb). Ollama doesn't support NVFP4 yet, so you'd need to use vLLM (which isn't too hard, and will give better token throughput anyway). Finding pre-quantized models at NVFP4 will be more difficult since there's less-broad support, but you can use llmcompressor [1] to statically compress any FP16 LLM to NVFP4 locally — you'll probably need to use accelerate to offload params to CPU during the one-time compression process, which they have documentation for.</p><p>I wouldn't reach for this particular power tool until you've decided on an LLM already, and just want faster perf, since it's a bit more involved than just using ollama and the initial quantization process will be slow due to CPU offload during compression (albeit it's only a one-time cost). But if you land on a Q4 model, it's not a bad choice once you have a favorite.</p><p>1: <a href="https://github.com/vllm-project/llm-compressor">https://github.com/vllm-project/llm-compressor</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137434"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137434" href="https://news.ycombinator.com/vote?id=44137434&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Basic conversations are essentially RP I suppose. You can look at KoboldCPP or SillyTavern reddit.</p><p>I was trying Patricide unslop mell and some of the Qwen ones recently. Up to a point more params is better than worrying about quantization. But eventually you'll hit a compute wall with high params.</p><p>KV cache quantization is awesome (I use q4 for a 32k context with a 1080ti!) and context shifting is also awesome for long conversations/stories/games. I was using ooba but found recently that KoboldCPP not only runs faster for the same model/settings but also Kobold's context shifting works much more consistently than Ooba's "streaming_llm" option, which almost always re-evaluates the prompt when hooked up to something like ST.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137598"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137598" href="https://news.ycombinator.com/vote?id=44137598&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Wow, a 5060Ti.  16gb + I'm guessing &gt;=32gb ram.  And here I am spinning Ye Olde RX 570 4gb + 32gb.</p><p>I'd like to know how many tokens you can get out of the larger models especially (using Ollama + Open WebUI on Docker Desktop, or LM Studio whatever).  I'm probably not upgrading GPU this year, but I'd appreciate an anecdotal benchmark.</p><pre><code>  - gemma3:12b
  - phi4:latest (14b)
  - qwen2.5:14b [I get ~3 t/s on all these small models, acceptably slow]

  - qwen2.5:32b [this is about my machine's limit; verrry slow, ~1 t/s]
  - qwen2.5:72b [beyond my machine's limit, but maybe not yours]</code></pre></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137621"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44137621" href="https://news.ycombinator.com/vote?id=44137621&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I'm guessing you probably also want to include the quantization levels you're using, as otherwise they'll be a huge variance in your comparisons with others :)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44136107"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136107" href="https://news.ycombinator.com/vote?id=44136107&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>What is everyone using their local LLMs for primarily? Unless you have a beefy machine, you'll never approach the level of quality of proprietary models like Gemini or Claude, but I'm guessing these smaller models still have their use cases, just not sure what those are.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136198"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136198" href="https://news.ycombinator.com/vote?id=44136198&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Not everyone is comfortable with sending their data and/or questions and prompts to an external party.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136410"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136410" href="https://news.ycombinator.com/vote?id=44136410&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I generally try a local model first for most prompts. It's good enough surprisingly often (over 50% for sure). Every time I avoid using a cloud service is a win.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136393"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136393" href="https://news.ycombinator.com/vote?id=44136393&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>You still can get decent stuff out of local ones.</p><p>Mostly I use it for testing tools and integrations via API not to spend money on subscriptions. When I see something working I switch it to proprietary one to get best results.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136215"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136215" href="https://news.ycombinator.com/vote?id=44136215&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I'm currently experimenting with Devstral for my own local coding agent I've slowly built together. It's in many ways nicer than Codex in that 1) full access to my hardware so can start VMs, make network requests and everything else I can do, which Codex cannot and 2) it's way faster both in initial setup, working through things and creating a patch.</p><p>Of course, it still isn't at the same level as Codex itself, the model Codex is using is just way better so of course it'll get better results. But Devstral (as I currently use it) is able to make smaller changes and refactors, and I think if I evolve the software a bit more, can start making larger changes too.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136187"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136187" href="https://news.ycombinator.com/vote?id=44136187&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I think that the future of local LLMs is delegation. You give it a prompt and it very quickly identifies what should be used to solve the prompt.</p><p>Can it be solved locally with locally running MCPs? Or maybe it's a system API - like reading your calendar or checking your email. Otherwise it identifies the best cloud model and sends the prompt there.</p><p>Basically Siri if it was good</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136300"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136300" href="https://news.ycombinator.com/vote?id=44136300&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; unless you have a beefy machine</p><p>The average person in r/locallama has a machine that would make r/pcmasterrace users blush.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136406"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136406" href="https://news.ycombinator.com/vote?id=44136406&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>An Apple M1 is decent enough for LMs. My friend wondered why I got so excited about it when it came out five years ago. It wasn't that it was particularly powerful - it's decent. What it did was to set a new bar for "low end".</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136434"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44136434" href="https://news.ycombinator.com/vote?id=44136434&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>A new Mac is easily starting around $1k and quickly goes up from there if you want a storage or RAM upgrade, especially for enough memory to really run some local models. Insane that a $1,000 computer is called "decent" and "low end". My daily driver personal laptop brand new was $300.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137458"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44137458" href="https://news.ycombinator.com/vote?id=44137458&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>You're right - memory size and then bandwidth is imperative for LLMs. Apple currently lacks great memory bandwidth with their unified memory. But it's not a bad option if you can find one for a good price. The prices for new are just bonkers.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136558"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44136558" href="https://news.ycombinator.com/vote?id=44136558&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>An M1 Mac is about 5 years old at this point and can be had for far less than a grand.</p><p>A brand new Mac Mini M4 is only $499.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136703"><td></td></tr>
                <tr id="44136773"><td></td></tr>
                        <tr id="44136478"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44136478" href="https://news.ycombinator.com/vote?id=44136478&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>That's fun to hear given that low end laptops are now $800, mid range is like $1.5k and upper end is $3k+ even for non-Apple vendors. Inflation makes fools of us all.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137659"><td></td></tr>
            <tr id="44136663"><td></td></tr>
                                    <tr id="44136288"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136288" href="https://news.ycombinator.com/vote?id=44136288&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I have a large repository of notes, article drafts, and commonplace book-type stuff.
I experimented a year or so ago with a system using RAG to "ask myself" what I have to say about various topics. (I suppose nowadays I would use MCP instead of RAG?)
I was not especially impressed by the results with the models I was able to run: long-winded responses full of slop and repetition, irrelevant information pulled in from notes that had some semantically similar ideas, and such.
I'm certainly not going to feed the contents of my private notebooks to any of the AI companies.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136425"><td></td></tr>
                <tr id="44136592"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44136592" href="https://news.ycombinator.com/vote?id=44136592&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>To clarify: what I was doing was first querying for the documents via a standard document database query and then feeding the best matching documents to the LLM.
My understanding is that with MCP I'd delegate the document query from the LLM to the tool.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137494"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44137494" href="https://news.ycombinator.com/vote?id=44137494&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>As a beginner, I also haven't had much luck with embedded vector queries either. Firstly, setting it up was a major pain in the ass and I couldn't even get it to ingest anything beyond .txt files. Second, maybe it was my AI system prompt or the lack of outside search capabilities but unless i was very specific with my query the response was essentially "can't find what youre looking for"</p>
              </div></td></tr>
        </tbody></table></td></tr>
                                    <tr id="44137517"><td></td></tr>
                <tr id="44137597"><td></td></tr>
                  <tr id="44135237"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135237" href="https://news.ycombinator.com/vote?id=44135237&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I only have 8gb of vram to work with currently, but I'm running OpenWebUI as a frontend to ollamma and I have a very easy time loading up multiple models and letting them duke it out either at the same time or in a round robin.</p><p>You can even keep track of the quality of the answers over time to help guide your choice.</p><p><a href="https://openwebui.com/" rel="nofollow">https://openwebui.com/</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137017"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137017" href="https://news.ycombinator.com/vote?id=44137017&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Related question: what is everyone using to run a local LLM? I'm using Jan.ai and it's been okay. I also see OpenWebUI mentioned quite often.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137166"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44137166" href="https://news.ycombinator.com/vote?id=44137166&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>LM studio if you just want an app. openwebui is just a front end - you'd need to have either llama.cpp or vllm behind it to serve the model</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137466"><td></td></tr>
            <tr id="44137142"><td></td></tr>
                  <tr id="44135269"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135269" href="https://news.ycombinator.com/vote?id=44135269&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I'm afraid that 1) you are not going to get a definite answer, 2) an objective answer is very hard to give, 3) you really need to try a few most recent models on your own and give them the tasks that seem most useful/meaningful to you. There is drastic difference in output quality depending on the task type.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136421"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136421" href="https://news.ycombinator.com/vote?id=44136421&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Generally speaking, how can you tell how much vram a model will take?  It seems to be a valuable bit of data which is missing from downloadable models (gguf) files.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136515"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136515" href="https://news.ycombinator.com/vote?id=44136515&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Very rougly you can consider the Bs of a model as GBs of memory then it depends on the quantization level. Say for an 8B model:</p><p>- FP16: 2x 8GB = 16GB</p><p>- Q8: 1x 8GB</p><p>- Q4: 0.5x 8GB = 4GB</p><p>It doesn't 100% neatly map like this but this gives you a rough measure. In top of this you need some more memory depending on the context length and some other stuff.</p><p>Rationale for the calculation above: A model is basically a billions of variables with a floating number value. So the size of a model roughly maps to number of variables (weights) x word-precision of each variable (4, 8, 16bits..)</p><p>You don't have to quantize all layers to the same precision this is why sometimes you see fractional quantizations like 1.58bits.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136815"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136815" href="https://news.ycombinator.com/vote?id=44136815&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>The 1.58bit quantization is using 3 values -- -1, 0, 1. The bits number comes from log_2(3) = 1.58....</p><p>For that level you can pack 4 weights in a byte using 2 bits per byte. However, there is one bit configuration in each that is unused.</p><p>More complex packing arrangements are done by grouping weights together (e.g. a group of 3) and assigning a bit configuration to each combination of values into a lookup table. This allows greater compression closer to the 1.68 bits value.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44137485"><td></td></tr>
                  <tr id="44137188"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137188" href="https://news.ycombinator.com/vote?id=44137188&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>It's a bit like asking what flavour of icecream is the best. Try a few and see.</p><p>For 16gb and speed you could try Qwen3-30B-A3B with some offload to system ram or use a dense model Probably a 14B quant</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135367"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135367" href="https://news.ycombinator.com/vote?id=44135367&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I have an RTX 3070 with 8GB VRAM and for me Qwen3:30B-A3B is fast enough. It's not lightning fast, but more than adequate if you have a _little_ patience.</p><p>I've found that Qwen3 is generally really good at following instructions and you can also very easily turn on or off the reasoning by adding "/no_think" in the prompt to turn it off.</p><p>The reason Qwen3:30B works so well is because it's a MoE. I have tested the 14B model and it's noticeably slower because it's a dense model.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136024"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136024" href="https://news.ycombinator.com/vote?id=44136024&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>How are you getting Qwen3:30B-A3B running with 8GB? On my system it takes 20GB of VRAM to launch it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137522"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44137522" href="https://news.ycombinator.com/vote?id=44137522&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Probably offload to regular ram I'd wager. Or really, really, reaaaaaaally quantized to absolute fuck. Qwen3:30B-A3B Q1 with a 1k Q4 context uses 5.84GB of vram.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="44136471"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136471" href="https://news.ycombinator.com/vote?id=44136471&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I’ve had awesome results with Qwen3-30B-A3B compared to other local LMs I’ve tried.  Still not crazy good but a lot better and very fast.  I have 24GB of VRAM though</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135375"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135375" href="https://news.ycombinator.com/vote?id=44135375&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I think you'll find that on that card most models that are approaching the 16G memory size will be more than fast enough and sufficient for chat. You're in the happy position of needing steeper requirements rather than faster hardware! :D</p><p>Ollama is the easiest way to get started trying things out IMO: <a href="https://ollama.com/">https://ollama.com/</a></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44135827"><td></td></tr>
                <tr id="44136218"><td></td></tr>
            <tr id="44135979"><td></td></tr>
                <tr id="44136315"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44136315" href="https://news.ycombinator.com/vote?id=44136315&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Any FOSS solutions that let you browse models and guesstimates for you on whether you have enough VRAM to fully load the model? That's the only selling point to LM Studio for me.</p><p>Ollama's default context length is frustratingly short in the era of 100k+ context windows.</p><p>My solution so far has been to boot up LM Studio to check if a model will work well on my machine, manually download the model myself through huggingface, run llama.cpp, and hook it up to open-webui. Which is less than ideal, and LM Studio's proprietary code has access to my machine specs.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136796"><td></td></tr>
                                    <tr id="44136744"><td></td></tr>
            <tr id="44136016"><td></td></tr>
            <tr id="44135318"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135318" href="https://news.ycombinator.com/vote?id=44135318&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Good question. I've had some success with Qwen2.5-Coder 14B, I did use the quantised version: huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF:latest 
It worked well on my MacBook Pro M1 32Gb. It does get a bit hot on a laptop though.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135841"><td></td></tr>
            <tr id="44135350"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135350" href="https://news.ycombinator.com/vote?id=44135350&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>VEGA64 (8GB) is pretty much obsolete <i>for this AI stuff, right</i> (compared to e.g. M2Pro (16GB))?</p><p>I'll give Qwen2.5 a try on the Apple Silicon, thanks.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135306"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135306" href="https://news.ycombinator.com/vote?id=44135306&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Agree with what others have said: you need to try a few out. But I'd put Qwen3-14B on your list of things to try out.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135824"><td></td></tr>
                      <tr id="44136384"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136384" href="https://news.ycombinator.com/vote?id=44136384&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Phi-4 is scared to talk about anything controversial, as if they're being watched.</p><p>I asked it a question about militias. It thought for a few pages about the answer and whether to tell me, then came back with "I cannot comply".</p><p>Nidum is the name of uncensored Gemma, it does a good job most of the time.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136623"><td></td></tr>
            <tr id="44136132"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136132" href="https://news.ycombinator.com/vote?id=44136132&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>hf.co/bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF:Q6_K is a decent performing model, if you're not looking for blinding speed. It definitely ticks all the boxes in terms of model quality. Try a smaller quant if you need more speed.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135491"><td></td></tr>
                <tr id="44137297"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44137297" href="https://news.ycombinator.com/vote?id=44137297&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>It is feasible to run 7B, 8B models with q6_0 in 8GB VRAM, or q5_k_m/q4_k_m if you have to or want to free up some VRAM for other things. With q4_k_m you can run 10B and even 12B models.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44136267"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136267" href="https://news.ycombinator.com/vote?id=44136267&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Ollama[0] has a collection of models that are either already small or quantized/distilled, and come with hyperparameters that are pretty reasonable, and they make it easy to try them out. I recommend you install it and just try a bunch because they all have different "personalities", different strengths and weaknesses. My personal go-tos are:</p><p>Qwen3 family from Alibaba seem to be the best reasoning models that fit on local hardware right now. Reasoning models on local hardware are annoying in contexts where you just want an immediate response, but vastly outperform non-reasoning models on things where you want the model to be less naive/foolish.</p><p>Gemma3 from google is really good at intuition-oriented stuff, but with an obnoxious HR Boy Scout personality where you basically have to add "please don't add any disclaimers" to the system prompt for it to function. Like, just tell me how long you think this sprain will take to heal, I already know you are not a medical professional, jfc.</p><p>Devstral from Mistral performs the best on my command line utility where I describe the command I want and it executes that for me (e.g. give me a 1-liner to list the dotfiles in this folder and all subfolders that were created in the last month).</p><p>Nemo from Mistral, I have heard (but not tested) is really good for routing-type jobs, where you need something with to make a simple multiple-choice decision competently with low latency, and is easy to fine-tune if you want to get that sophisticated.</p><p>[0] <a href="https://ollama.com/search">https://ollama.com/search</a></p></div></td></tr>
        </tbody></table></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is not our future (221 pts)]]></title>
            <link>https://procreate.com/ai</link>
            <guid>44134798</guid>
            <pubDate>Fri, 30 May 2025 10:45:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://procreate.com/ai">https://procreate.com/ai</a>, See on <a href="https://news.ycombinator.com/item?id=44134798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-content"><main role="main" aria-label="Page Content"><!--[--><!--[--><section role="presentation"><div><h2>Where we stand</h2><div><!--[--><div><h3>No generative AI</h3><p>We deeply respect your hard-earned skills.</p></div><div><h3>Your work belongs to you</h3><p>We do not have access to your art, by design.</p></div><div><h3>We take pride in privacy</h3><p>Your activity is not tracked in our apps.</p></div><!--]--></div></div><div><p>Generative AI is ripping the humanity out of things. Built on a foundation of theft, the technology is steering us toward a barren future.  We think machine learning is a compelling technology with a lot of merit, but the path generative AI is on is wrong for us.</p><p>We're here for the humans. We're not chasing a technology that is a moral threat to our greatest jewel: human creativity. In this technological rush, this might make us an exception or seem at risk of being left behind. But we see this road less travelled as the more exciting and fruitful one for our community.</p></div></section><!----><!--]--><!--]--></main></div></div>]]></description>
        </item>
    </channel>
</rss>