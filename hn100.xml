<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 19 Aug 2025 12:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Prime Number Grid (152 pts)]]></title>
            <link>https://susam.net/primegrid.html</link>
            <guid>44949162</guid>
            <pubDate>Tue, 19 Aug 2025 07:33:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://susam.net/primegrid.html">https://susam.net/primegrid.html</a>, See on <a href="https://news.ycombinator.com/item?id=44949162">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <p><label for="start">Start</label>
      
    </p>
    <p><label for="rows">Rows</label>
      
    </p>
    <p><label for="cols">Cols</label>
      
    </p>
    
    
    
    
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenMower ‚Äì An open source lawn mower (381 pts)]]></title>
            <link>https://github.com/ClemensElflein/OpenMower</link>
            <guid>44946996</guid>
            <pubDate>Tue, 19 Aug 2025 00:35:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ClemensElflein/OpenMower">https://github.com/ClemensElflein/OpenMower</a>, See on <a href="https://news.ycombinator.com/item?id=44946996">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">I am available for hire</h2><a id="user-content-i-am-available-for-hire" aria-label="Permalink: I am available for hire" href="#i-am-available-for-hire"></a></p>
<p dir="auto">Hello! With a background in software engineering, embedded programming, hardware design, and robotics, I'm on the lookout for new challenges.
If you're in search of someone with my skills, let's team up and create something amazing! <a href="https://x-tech.online/" rel="nofollow">https://x-tech.online/</a></p>
<br>
<hr>

<p dir="auto"><h2 tabindex="-1" dir="auto">OpenMower - The DIY Smart Mowing Robot for Everyone</h2><a id="user-content-openmower---the-diy-smart-mowing-robot-for-everyone" aria-label="Permalink: OpenMower - The DIY Smart Mowing Robot for Everyone" href="#openmower---the-diy-smart-mowing-robot-for-everyone"></a></p>
    <p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ClemensElflein/OpenMower/blob/main/img/open_mower_header.jpg"><img src="https://github.com/ClemensElflein/OpenMower/raw/main/img/open_mower_header.jpg"></a></p>
  


<p dir="auto">
  <a href="#license"><img src="https://camo.githubusercontent.com/f61dcd7e9460d79b9e8e19683c964e21cc2455ff9d8860cc5ca30f35457be635/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d434325323042592d2d4e432d2d5341253230342e302d6c69676874677265792e737667" data-canonical-src="https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg"></a></p>
  
  <p dir="auto"><b>Join the Discord server for OpenMower discussion:</b>
  </p><p dir="auto"><a href="https://discord.gg/jE7QNaSxW7" rel="nofollow"><img src="https://camo.githubusercontent.com/3ed6b7ce339222fba86763bbfdbf5c999c91abd21b044916d00ac6a2c4469171/68747470733a2f2f62616467656e2e6e65742f62616467652f69636f6e2f646973636f72643f69636f6e3d646973636f7264266c6162656c" data-canonical-src="https://badgen.net/badge/icon/discord?icon=discord&amp;label"></a></p>

<div dir="auto"><p dir="auto">Warning</p>
<p dir="auto"><b>DISCLAIMER:</b></p>
<p dir="auto"><strong>IF YOU ARE NOT 100% SURE WHAT YOU ARE DOING, PLEASE DON'T TRY THIS AT HOME! ASK IN <a href="https://discord.gg/jE7QNaSxW7" rel="nofollow">DISCORD</a>, IF YOU HAVE ANY QUESTIONS!</strong></p>
</div>
<div dir="auto"><p dir="auto">Important</p>
<p dir="auto"><b>This project is active!</b></p>
<p dir="auto">This is the hardware repository, so it might seem that the project is inactive, since hardware is pretty stable by now.
Most of the development work is done on the ROS code here: <a href="https://github.com/ClemensElflein/open_mower_ros">https://github.com/ClemensElflein/open_mower_ros</a></p>
</div>

<p dir="auto"><h2 tabindex="-1" dir="auto">About the Project</h2><a id="user-content-about-the-project" aria-label="Permalink: About the Project" href="#about-the-project"></a></p>
<p dir="auto">If you want to see a quick overview, you can check out this video:</p>
<p dir="auto">
  <a href="https://www.youtube.com/watch?v=BSF04i3zNGw" rel="nofollow"><img src="https://user-images.githubusercontent.com/2864655/161540069-f4263fa7-a47b-49d2-a7bc-d1cdc3a47704.jpg"></a>
</p>
<p dir="auto">Let's be honest: The current generation of robotic lawn mowers sucks. Basically all of these bots drive in a random direction until they hit the border of the lawn, rotate for a randomized duration and repeat. <strong>I think we can do better!</strong></p>
<p dir="auto">Therefore, we have disassembled the cheapest off-the-shelf robotic mower  we could find (YardForce Classic 500) and were surprised that the hardware itself is actually quite decent:</p>
<ul dir="auto">
<li>Geared sensored brushless motors for the wheels</li>
<li>A sensored brushless motor for the mower motor itself</li>
<li>The whole construction seems robust, waterproof and all in all thought through</li>
<li>All components are connected using standard connectors, therefore upgrading the hardware is easily possible.</li>
</ul>
<p dir="auto">The bottom line is: The bot itself is surprisingly high quality and doesn't need to be changed at all. <strong>We just need some better software in there</strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Goals</h2><a id="user-content-project-goals" aria-label="Permalink: Project Goals" href="#project-goals"></a></p>
<p dir="auto">Here is a quick overview of this project's goals:</p>
<p dir="auto">‚úîÔ∏è <strong>Autonomous Lawn Mowing:</strong> Obviously, the device should be able to mow the lawn automatically.</p>
<p dir="auto">‚úîÔ∏è <strong>Good Safety:</strong> The device must be safe, e.g. emergency stop if lifted or crashed.</p>
<p dir="auto">‚úîÔ∏è <strong>No Perimeter Wire Needed:</strong> We want to be flexible and support multiple mowing areas.</p>
<p dir="auto">‚úîÔ∏è <strong>Low Cost:</strong> It should be cheaper than a mid range off-the-shelf product</p>
<p dir="auto">‚úîÔ∏è <strong>Open:</strong> I want to share knowledge and enable others to build an OpenMower as well.</p>
<p dir="auto">‚úîÔ∏è <strong>Nice to Look At:</strong> You should not be ashamed to have an OpenMower mowing your lawn.</p>
<p dir="auto">‚úîÔ∏è <strong>Avoid Obstacles:</strong> The mower should detect obstacles and avoid them during mowing.</p>
<p dir="auto">‚úîÔ∏è <strong>Rain Detection:</strong> The device should be able to detect bad weather conditions and pause mowing until they improve.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Open Mower App</h2><a id="user-content-open-mower-app" aria-label="Permalink: Open Mower App" href="#open-mower-app"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ClemensElflein/OpenMower/blob/main/img/open_mower_app_1.jpg"><img src="https://github.com/ClemensElflein/OpenMower/raw/main/img/open_mower_app_1.jpg" alt="Open Mower App 1"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ClemensElflein/OpenMower/blob/main/img/open_mower_app_2.jpg"><img src="https://github.com/ClemensElflein/OpenMower/raw/main/img/open_mower_app_2.jpg" alt="Open Mower App 2"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Current State</h2><a id="user-content-current-state" aria-label="Permalink: Current State" href="#current-state"></a></p>
<p dir="auto">The basic mowing function finally works! As you can see in the video, map teaching and mowing work as expected. It even returns to the docking station automatically as soon as the battery gets low and continues once it's recharged.</p>
<p dir="auto">At this point I can recommend that brave tech savvy users can build one for themselves! Since it's quite an expensive and complex project, please don't be shy and ask if you have any questions. I'm glad to help üôÇ</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware</h3><a id="user-content-hardware" aria-label="Permalink: Hardware" href="#hardware"></a></p>
<p dir="auto">By now we have a stable revision of the mainboard as well as two motor controllers to go with it. The <a href="https://github.com/clemensElflein/xesc">xESC mini</a> and the <a href="https://github.com/clemensElflein/xesc2040">xESC 2040</a>. I'm currently using the xESC mini for my builds and it works very well. The problem with this controller is, its parts are currently hard to source. That's why we created the xESC 2040 based on the RP2040 chip. This is the low-cost variant and its support is currently experimental.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Hardware To-Do:</h4><a id="user-content-hardware-to-do" aria-label="Permalink: Hardware To-Do:" href="#hardware-to-do"></a></p>
<ul>
<li> Low Level Firmware Implementation
<ul>
<li> Voltage / Current Sense</li>
<li> Emergency Stop Button tracking</li>
<li> IMU Communication</li>
<li> Rain Sensor</li>
<li> Charging State</li>
<li> Sound Module</li>
<li> UI Board Communication</li>
<li> Discharge current for more accurate battery charge estimation</li>
</ul>
</li>
<li> ROS Hardware Interface</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Software</h3><a id="user-content-software" aria-label="Permalink: Software" href="#software"></a></p>
<p dir="auto">The basic software is basically done; Our prototype works as intended (but is not able to avoid obstacles yet).</p>
<p dir="auto">The software for the robot can be found in a separate repository: <a href="https://github.com/ClemensElflein/open_mower_ros">https://github.com/ClemensElflein/open_mower_ros</a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Software To-Do:</h4><a id="user-content-software-to-do" aria-label="Permalink: Software To-Do:" href="#software-to-do"></a></p>
<ul>
<li> Mowing State Machine (Docking / Mowing, ...)</li>
<li> Path Planning</li>
<li> Obstacle Avoidance</li>
<li> App / Visualization</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">If you want to read how to get started building a robot for yourself, check the <a href="https://openmower.de/" rel="nofollow">OpenMower Website</a>. There you can find information on which parts to buy, how to install the software and so on. If you find anything missing, please join the Discord server and ask there. Also there's the <a href="https://wiki.openmower.de/" rel="nofollow">OpenMower Wiki</a> which is written by the community. It has some additional guides and information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How You Can Help</h2><a id="user-content-how-you-can-help" aria-label="Permalink: How You Can Help" href="#how-you-can-help"></a></p>
<p dir="auto">You can help by starting an OpenMower build of your own. This helps to validate the concept and helps to create useful documentation for new users.</p>
<p dir="auto">Additionally, you can help by starring üåü and watching üëÄ this repository, since it will help with visibility. You can also subscribe to my <a href="https://youtube.com/c/ClemensElflein" rel="nofollow">YouTube channel</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compatible Robotic Mowers</h2><a id="user-content-compatible-robotic-mowers" aria-label="Permalink: Compatible Robotic Mowers" href="#compatible-robotic-mowers"></a></p>
<p dir="auto">While disassembling the bot, I wondered about its mainboard: Instead of "YardForce" it read "GForce". After checking the internet for "GForce" robots, I found that that very similar looking robotic mowers are sold under the Herkules brand. Naturally I tried to dig deeper and actually found evidence that the mainboard is manufactured by some chinese company (SUMEC Hardware).</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ClemensElflein/OpenMower/blob/main/img/mainboard.jpg"><img src="https://github.com/ClemensElflein/OpenMower/raw/main/img/mainboard.jpg" alt="GForce Robot Mower Mainboard"></a></p>
<p dir="auto">It is therefore quite safe to assume that many robot mowers are basically the same device in a different case. This would be a huge win for the community, since this would mean that by making one of those robots smarter, we could upgrade lots of robots.</p>
<p dir="auto">Therefore it might be a good idea to start a list of compatible devices. So if you have a cheap robotic lawn mower, you can check, if it was already disassembled in the list below. If it's not there, it would be nice of you to check, if it contains the same mainboard as ours and add your robot to the list with some some pictures / model numbers.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">List of Compatible Mowers</h3><a id="user-content-list-of-compatible-mowers" aria-label="Permalink: List of Compatible Mowers" href="#list-of-compatible-mowers"></a></p>
<p dir="auto">By now, some guys have disassembled their mowers and it doesn't look as good as I initially hoped. The GForce boards are basically just used by YardForce and some rebranded versions for the EU market. My exact hardware was only found in the mower I'm using (YardForce Classic 500) and in recently manufactured SA650 ECOs. The SA650 has a different chassis and we don't have a way of mounting the GPS antenna yet. Therefore at the moment, the only compatible mower is mine (the YardForce Classic 500).</p>
<p dir="auto">If you want to have a look at the disassembled mowers, check the Google Docs <a href="https://docs.google.com/spreadsheets/d/1BX0-KEs5v-VED8-RA4BLE-wRdXHtlmcKy4n9K5vJVAA" rel="nofollow">here</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">More Infos</h2><a id="user-content-more-infos" aria-label="Permalink: More Infos" href="#more-infos"></a></p>
<p dir="auto">This page only contains the basic overview of the project. To follow my current development state, check out my <a href="https://x-tech.online/" rel="nofollow">Blog</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Patents, Local Laws, Liability</h2><a id="user-content-patents-local-laws-liability" aria-label="Permalink: Patents, Local Laws, Liability" href="#patents-local-laws-liability"></a></p>
<p dir="auto">Before building a robot based on the designs published here, please make sure that you are allowed to do so in your specific regions.
There may be patents and / or laws prohibiting you of doing so.</p>
<p dir="auto">The code/schematics/PCB files are distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</p>
<p dir="auto">This basically means: I'm just documenting a project of mine here for free and I don't have the time and resources to check that devices built using this information will be safe to use, legal to use or even work as intended. You will need technical know-how to use this project and I'm not liable for any damages your devices do to anyone or anything.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow"><img alt="Creative Commons License" src="https://camo.githubusercontent.com/62be294f71c9a1885f9cd8f54aa8b8bd42d432fd14b5393a8b25bcd1f34daa42/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d6e632d73612f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a><br>This work is licensed under a <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</p>
<p dir="auto">Feel free to use the design in your private/educational projects, but don't try to sell the design or products based on it without getting my consent first. The idea here is to share knowledge, not to enable others to simply sell my work. Thank you for understanding.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ted Chiang: The Secret Third Thing (157 pts)]]></title>
            <link>https://linch.substack.com/p/ted-chiang-review</link>
            <guid>44946774</guid>
            <pubDate>Tue, 19 Aug 2025 00:05:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linch.substack.com/p/ted-chiang-review">https://linch.substack.com/p/ted-chiang-review</a>, See on <a href="https://news.ycombinator.com/item?id=44946774">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>I really like Ted Chiang‚Äôs writing.</p><p>I think he's probably the best science fiction short story writer alive, and possibly the best short story writer, period.</p><p><span>I've read every one of his stories at least twice, and The Merchant and the Alchemist's Gate more like seven times. I‚Äôve noticed many of his readers, including some of his most positive reviewers</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-171116224" href="https://linch.substack.com/p/ted-chiang-review#footnote-1-171116224" target="_self" rel="">1</a></span><span>, miss one key point or another of his works, and thus don't fully appreciate his genius.</span></p><p>This review covers what he does extremely well, especially unique elements that other science fiction writers have not done as well, or at all.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!H6ea!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae13be8-25e5-4a7d-9f54-4c246cac09ab_1600x1600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!H6ea!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae13be8-25e5-4a7d-9f54-4c246cac09ab_1600x1600.png 424w, https://substackcdn.com/image/fetch/$s_!H6ea!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae13be8-25e5-4a7d-9f54-4c246cac09ab_1600x1600.png 848w, https://substackcdn.com/image/fetch/$s_!H6ea!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae13be8-25e5-4a7d-9f54-4c246cac09ab_1600x1600.png 1272w, https://substackcdn.com/image/fetch/$s_!H6ea!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae13be8-25e5-4a7d-9f54-4c246cac09ab_1600x1600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!H6ea!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae13be8-25e5-4a7d-9f54-4c246cac09ab_1600x1600.png" width="477" height="477" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3ae13be8-25e5-4a7d-9f54-4c246cac09ab_1600x1600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:477,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!H6ea!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae13be8-25e5-4a7d-9f54-4c246cac09ab_1600x1600.png 424w, https://substackcdn.com/image/fetch/$s_!H6ea!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae13be8-25e5-4a7d-9f54-4c246cac09ab_1600x1600.png 848w, https://substackcdn.com/image/fetch/$s_!H6ea!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae13be8-25e5-4a7d-9f54-4c246cac09ab_1600x1600.png 1272w, https://substackcdn.com/image/fetch/$s_!H6ea!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae13be8-25e5-4a7d-9f54-4c246cac09ab_1600x1600.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Science fiction critics often divide the genre into:</p><ul><li><p><span>"hard" science fiction: aka </span><em>engineering</em><span> fiction, stories built on scientifically accurate extrapolations of real physics and technology (think Arthur C. Clarke)</span></p></li><li><p><span>"soft" science fiction: aka science </span><em>fantasy</em><span>, which uses scientific trappings as window dressing for character-driven or sociological stories (think Star Wars).</span></p></li></ul><p><span>Ted Chiang has written stories plausibly categorized as either, but more excitingly, many of his stories are </span><em>neither</em><span>. He often writes what I think of as true </span><em>science</em><span> fiction, where the </span><em>principles of science themselves</em><span> are meaningfully different from our world, but still internally consistent.</span></p><p><span>In </span><em>Omphalos</em><span>, Young Earth Creationism is empirically true</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-171116224" href="https://linch.substack.com/p/ted-chiang-review#footnote-2-171116224" target="_self" rel="">2</a></span><span>. Astronomers can only see light from stars 6,000 light-years away. Fossilized trees have centers with no rings. The first God-created humans lack belly buttons. The scientists in that story keep discovering multiple independent lines of evidence that converge on creationism: because in that universe, they're simply correct.</span></p><p><span>In </span><em>Seventy-Two Letters</em><span>, technology springs from Jewish Kabbalah. Golems and divine names drive industrial progress in a steampunk world</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-171116224" href="https://linch.substack.com/p/ted-chiang-review#footnote-3-171116224" target="_self" rel="">3</a></span><span>.</span></p><p><span>Excitingly, he does this not just with natural sciences but social sciences as well. In </span><em>Story of Your Life</em><span>, strong </span><a href="http://sapir-whorf/" rel="">Sapir-Whorf</a><span> (the idea that language significantly constrains thought) isn't a largely discredited linguistic hypothesis, but the key to navigating First Contact with alien minds that experience past and future as equally present.</span></p><p>This comes up in his other stories as well:</p><ul><li><p><span>In </span><em>Exhalation,</em><span> thermodynamics appear to work differently</span></p></li><li><p><span>In </span><em>Division By Zero</em><span>, mathematics itself is broken from within.</span></p></li><li><p><span>In </span><em>Hell Is the Absence of God</em><span>, divine intervention is empirically observable and follows consistent rules</span></p></li></ul><p><span>Many of his readers, even in their otherwise rave reviews, miss this. Multiple reviewers complain about how the science in his stories are ‚Äúunrealistic‚Äù (e.g. strong Sapir-Whorf is ‚Äúdiscredited‚Äù). They expected </span><em>hard</em><span> science fiction; Chiang was doing something different. Chiang created different universes with internally self-consistent scientific laws, using science fiction and alternative science as a vehicle for exploring philosophical progress and human relationships.</span></p><p><span>Science fiction writers used to </span><em>like</em><span> technology. For some reason, this has become increasingly uncommon, even pass√©. Doubly so for Western writers, and quadruply so for Western, literary, ‚Äúhumanist‚Äù writers.</span></p><p><span>Now it‚Äôs hip and trendy to think of every new technology as the </span><a href="https://x.com/AlexBlechman/status/1457842724128833538?lang=en" rel="">Torment Nexus</a><span>. Most science fiction today feels like </span><a href="https://en.wikipedia.org/wiki/Black_Mirror" rel="">Black Mirror,</a><span> which ran 7 seasons with exactly one happy ending.</span></p><p><span>Chiang bucks this trend. </span><a href="https://www.newyorker.com/magazine/2019/05/13/science-fiction-doesnt-have-to-be-dystopian" rel="">Joyce Carol Oates</a><span>:</span></p><blockquote><p>It is both a surprise and a relief to encounter fiction that [...] ask[s] anew philosophical questions that have been posed repeatedly through millennia to no avail. Chiang‚Äôs materialist universe is a secular place, in which God, if there is one, belongs to the phenomenal realm of scientific investigation and usually has no particular interest in humankind. But it is also a place in which the natural inquisitiveness of our species leads us to ever more astonishing truths, and an alliance with technological advances is likely to enhance us, not diminish us. Human curiosity, for Chiang, is a nearly divine engine of progress.</p></blockquote><p><span>In the hands of a lesser (or perhaps just more pessimistic) writer, many of the technologies and ideas Chiang explores will have an accursed quality to them, a monkey‚Äôs paw that curls into delivering a future much worse than a more innocent, pastoral past. Chiang resists those cliches. In </span><em>The Truth of Fact, The Truth of Feeling</em><span>, memory augmentation technology allows the narrator to understand his own self-deceptions, and work towards becoming a better person and reconciling with loved ones and even himself. In </span><em>Liking What You See: A Documentary</em><span>, a technology that gives users acquired face-blindness allows the main characters to meditate on the nature of human beauty and the shallowness inherent in privileging the beautiful.</span></p><p>Even in situations where the story is overall tragic, like when the characters are faced with existential crisis (in the individual sense), or existential catastrophe (in the world-ending sense), technology isn't the villain but the vehicle for understanding unbearable truths (whether about the world or about ourselves).</p><p><span>Chiang consistently shows us the potential of technology to help us become </span><em>more</em><span> human, and have a deeper appreciation for the world and our place in it.</span></p><p><em>‚ÄúCompatibilism is a philosophical stance that reconciles free will with determinism. It argues that free will, understood as the ability to act according to one's desires, is compatible with the idea that all events, including human actions, are causally determined by prior events. Essentially, compatibilists believe that even if our choices are predetermined, we can still be considered free and morally responsible if those choices are a result of our own internal states, like desires and intentions.‚Äù</em></p><p><span>Does that make sense to you? I‚Äôm not sure it does to me. In practice, compatibilism says something like ‚Äú</span><em>free will in the normal, pretheoretic sense of the term, doesn‚Äôt exist. Your choices still meaningfully matter nonetheless. You can‚Äôt meaningfully get out of the bind philosophically. What you can do, however, is make peace with it.‚Äù</em></p><p>Chiang makes this realization visceral. From The Merchant and the Alchemist's Gate:</p><blockquote><p>‚ÄúMy journey to the past had changed nothing, but what I had learned had changed everything...Nothing erases the past. There is repentance, there is atonement, and there is forgiveness. That is all, but that is enough.‚Äù</p></blockquote><p><span>‚ÄúThat is all, but that is enough‚Äù on the surface sounds like a </span><a href="https://philosophytalk.org/blog/deepities-and-bullshit/" rel="">deepity</a><span>, but I genuinely find it more moving than anything else I‚Äôve read on free will, determinism, or compatibilism.</span></p><p>When Chiang uses time travel as a motif, the stories differ from typical time travel stories. Because causation is a closed loop, knowing the future does not give you special access to it, and Chiang‚Äôs characters tend to not fight the future (successfully or otherwise).</p><p><span>In Story of Your Life [SPOILERS], the narrator learns an atemporal alien language and begins experiencing past and future as equally real.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-171116224" href="https://linch.substack.com/p/ted-chiang-review#footnote-4-171116224" target="_self" rel="">4</a></span><span> It takes her some time to make peace with it, but eventually she fully accepts the truth of determinism. She understands that life is full of tragedy, including that her daughter will die young, but life is full of beauty too. With both regret and awe, she sets forth on the path that she was destined to take</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-171116224" href="https://linch.substack.com/p/ted-chiang-review#footnote-5-171116224" target="_self" rel="">5</a></span><span>.</span></p><p>This is compatibilism from the inside. In both stories, the characters discover they cannot change what will happen, but this knowledge transforms how they experience what must happen: with forgiveness, acceptance, and even joy.</p><p><span>As a friend of mine puts it, ‚Äúhe treats philosophical ideas as lived experiences.‚ÄùThe mathematician in </span><em>Division by Zero</em><span> doesn't just intellectually understand that mathematics is broken; she experiences it as a personal catastrophe, on par with (and concurrent with) her marriage's collapse. In </span><em>Lifecycle of Software Objects</em><span>, the ‚Äúwe are the parents of our mind-children‚Äù metaphor for building sentient AI systems becomes quite literal.</span></p><p>I've reread all his stories not just because I love his writing, but because they often mean something different the second time through. Like those optical illusions where seeing the duck in the rabbit changes the image forever, his endings don't surprise so much as illuminate.</p><p>In a sense, this narrative technique is compatibilism as literary form: the ending was always determined, but discovering it still matters. Knowing where you're headed changes how you experience the journey, not whether you take it.</p><p>Notably, he achieves this without cheap tricks or twist endings (however foreshadowed), but something deeper. The endings feel inevitable rather than clever. This technique is so difficult that it's surprising he's managed it repeatedly, using different approaches each time.</p><p>Chiang combines these unique factors that he does exceptionally well with strengths that he shares with other top literary science fiction writers: simple yet beautiful prose, diverse settings, a rigorous understanding of science, philosophy, and human psychology, and appealing, interesting, and diverse characters.</p><p>He is probably the best science fiction short story writer alive, and possibly the best short story writer in general.</p><p><span>Is Chiang perfect? Of course not. I won't belabor obvious points like his nonfictional views on current-generation LLMs being surprisingly shallow</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-171116224" href="https://linch.substack.com/p/ted-chiang-review#footnote-6-171116224" target="_self" rel="">6</a></span><span>, or his lack of output being tragic for a generational talent</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-171116224" href="https://linch.substack.com/p/ted-chiang-review#footnote-7-171116224" target="_self" rel="">7</a></span><span>.</span></p><p>Instead, I'll focus on two points: weaknesses in portraying how entire societies interface with technology, and his lack of home runs outside of compatibilism.</p><p>I‚Äôm an intellectual dilettante. I‚Äôm widely read in many fields, and an expert in none. But to the extent I have any real expertise, it‚Äôs probably the intersection of the social sciences and technology. And Chiang is surprisingly weak here.</p><p>Chiang‚Äôs scientific imagination is strongest at the very macro level (What would the objective physical evidence look like if God created Earth 6,000 years ago?) and the very micro level: This is one person, a named character. How would she and her husband interface with technology? How would technology radically change her life and her self-perception? Chiang‚Äôs much weaker at the middle level, where we consider how societies and civilizations collectively face novel technologies.</p><p><span>In </span><em>Anxiety is the Dizziness of Freedom</em><span>, people invented a new Prism(‚Ñ¢) that allows them to talk to copies in nearby universes. Chiang treats this technology as an interesting tool to explore his favorite questions of free will, determinism, and what does it mean to be a person, etc.</span></p><p>I think he doesn‚Äôt understand the power of this singularity-level technology he just introduced.</p><p>The ability to send bits across parallel universes is just insane in terms of economic and experimentation value. For example, pharmaceutical companies can do $100 billion trials for all sorts of novel drugs, and trade the results of this information with their clones in other universes. Massive R&amp;D projects in general can be done in parallel across different universes, as long as the results can be compressed in enough bits to be sent over these prisms.</p><p>More than that, you can now have not just individual or small-group level studies but societal-wide or even multiverse-wide ones! Wondering whether raising interest rates can cause recessions? Just conduct RCTs across multiverses (and if they can‚Äôt coordinate on RCTs, even the observational data would be insanely useful)!</p><p>While the main characters work through existential crises and their mundane, human worries, the rest of society should be accelerating at breakneck speed toward doom, utopia, or something else equally fascinating. Chiang doesn't explore this, even as backdrop.</p><p>While his fictional portrayals of compatibilism are probably the world's best, and he's covered different angles repeatedly, his other philosophical explorations don't achieve the same resonance. Having something both philosophically deep and emotionally resonant is genuinely hard. So I don‚Äôt want to be overly critical. But perhaps the lack of attempts on new ideas is not unrelated to his extreme perfectionism and sparse output?</p><p><span>I cannot recommend Chiang more highly. If you could only read one science fiction book this year, I recommend reading </span><em>Stories of Your Life</em><span>. If you could only read 2 science fiction books this year, I recommend reading both </span><em>Stories of Your Life</em><span> and </span><em>Exhalation</em><span>.</span></p><figure data-drag-handle="true" data-component-name="ImageGallery"><div><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!l_KA!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a03822c-fe6c-4013-876c-46f0b77ca3b5_778x1200.jpeg 424w, https://substackcdn.com/image/fetch/$s_!l_KA!,w_720,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a03822c-fe6c-4013-876c-46f0b77ca3b5_778x1200.jpeg 720w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!l_KA!,w_720,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a03822c-fe6c-4013-876c-46f0b77ca3b5_778x1200.jpeg" sizes="100vw" alt="" srcset="https://substackcdn.com/image/fetch/$s_!l_KA!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a03822c-fe6c-4013-876c-46f0b77ca3b5_778x1200.jpeg 424w, https://substackcdn.com/image/fetch/$s_!l_KA!,w_720,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a03822c-fe6c-4013-876c-46f0b77ca3b5_778x1200.jpeg 720w" width="720"></picture><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!g7N-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35666649-5888-4d16-8a61-65d72db60197_973x1500.jpeg 424w, https://substackcdn.com/image/fetch/$s_!g7N-!,w_720,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35666649-5888-4d16-8a61-65d72db60197_973x1500.jpeg 720w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!g7N-!,w_720,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35666649-5888-4d16-8a61-65d72db60197_973x1500.jpeg" sizes="100vw" alt="" srcset="https://substackcdn.com/image/fetch/$s_!g7N-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35666649-5888-4d16-8a61-65d72db60197_973x1500.jpeg 424w, https://substackcdn.com/image/fetch/$s_!g7N-!,w_720,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35666649-5888-4d16-8a61-65d72db60197_973x1500.jpeg 720w" width="720"></picture></div><figcaption>https://www.amazon.com/Stories-Your-Life-Others-Chiang/dp/1101972122  https://www.amazon.com/Exhalation-Ted-Chiang/dp/1101972084/ </figcaption></div></figure><p><a href="https://www.amazon.com/Stories-Your-Life-Others-Chiang/dp/1101972122" rel="">Story Of Your Life</a><span>//</span><a href="https://www.amazon.com/Exhalation-Ted-Chiang/dp/1101972084/" rel="">Exhalation</a></p><p><span>And if you are planning to read 5 science fiction books this year, congrats on making the time for it! In that case I recommend you read </span><em>Story of Your Life</em><span> twice and </span><em>Exhalation</em><span> three times.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://linch.substack.com/p/ted-chiang-review?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://linch.substack.com/p/ted-chiang-review?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><ol><li><p><strong>Berkeley/SF Bay Area readers:</strong><span> We're hosting an in-person screening of </span><em>Arrival</em><span> (based on "Story of Your Life") on August 25th! Still have spots. Please comment or DM for an invite.</span></p></li><li><p><strong>Coming soon:</strong><span> I'm planning to add paid subscriptions for unfinished drafts, research notes, and so people can show general support. Nothing important will be paywalled. Thoughts welcome!</span></p></li><li><p><strong>Thank you:</strong><span> We've hit 200+ subscribers in 1.5 months! I'd love your feedback on topics you'd like to see or how I can improve.</span></p></li></ol></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Croatian freediver held breath for 29 minutes (244 pts)]]></title>
            <link>https://divernet.com/scuba-news/freediving/how-croatian-freediver-held-breath-for-29-minutes/</link>
            <guid>44946762</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://divernet.com/scuba-news/freediving/how-croatian-freediver-held-breath-for-29-minutes/">https://divernet.com/scuba-news/freediving/how-croatian-freediver-held-breath-for-29-minutes/</a>, See on <a href="https://news.ycombinator.com/item?id=44946762">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="15d3585" data-element_type="widget" data-widget_type="theme-post-content.default">
					
<p>The world record for longest underwater breath-hold using oxygen is often viewed as much as a conjuring trick as an athletic feat ‚Äì and at one time the title was indeed held by US magician David Blaine. But now a remarkable new mark has been set by a Croatian already recognised for his more conventional competitive freediving achievements.</p>





<p>Vitomir Mariƒçiƒá has set a new Guinness World Record (GWR) of 29min 3sec for ‚Äúthe longest breath held voluntarily under water using oxygen‚Äù ‚Äì surpassing the previous record by more than four minutes.</p>



<p>Also read: <strong><a href="https://divernet.com/scuba-news/freediving/champagne-master-dies-freediving/">Champagne master dies freediving</a></strong></p>



<p>His near-half-hour feat took place on 14 June in a 3m-deep pool at the Bristol Hotel in Opatija, Croatia in front of five official judges and some 100 spectators.</p>



<figure><img fetchpriority="high" decoding="async" width="1024" height="683" src="https://divernet.com/wp-content/uploads/2025/06/Vitomir-2-1024x683.jpg" data-orig-src="https://divernet.com/wp-content/uploads/2025/06/Vitomir-2-1024x683.jpg" alt="Croatian freediver Vitomir Mariƒçiƒá" title="How Croatian freediver held breath for 29 minutes 1" srcset="https://divernet.com/wp-content/uploads/2025/06/Vitomir-2-300x200.jpg 300w, https://divernet.com/wp-content/uploads/2025/06/Vitomir-2-768x512.jpg 768w, https://divernet.com/wp-content/uploads/2025/06/Vitomir-2-1024x683.jpg 1024w, https://divernet.com/wp-content/uploads/2025/06/Vitomir-2-1320x880.jpg 1320w, https://divernet.com/wp-content/uploads/2025/06/Vitomir-2-1536x1024.jpg 1536w, https://divernet.com/wp-content/uploads/2025/06/Vitomir-2.jpg 1920w" data-srcset="https://divernet.com/wp-content/uploads/2025/06/Vitomir-2-300x200.jpg 300w, https://divernet.com/wp-content/uploads/2025/06/Vitomir-2-768x512.jpg 768w, https://divernet.com/wp-content/uploads/2025/06/Vitomir-2-1024x683.jpg 1024w, https://divernet.com/wp-content/uploads/2025/06/Vitomir-2-1320x880.jpg 1320w, https://divernet.com/wp-content/uploads/2025/06/Vitomir-2-1536x1024.jpg 1536w, https://divernet.com/wp-content/uploads/2025/06/Vitomir-2.jpg 1920w" data-sizes="auto" data-orig-sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Freediver Vitomir Mariƒçiƒá </figcaption></figure>



<p>Mariƒçiƒá prepared by pre-breathing pure oxygen for an unspecified length of time before his immersion, in line with <a href="https://www.guinnessworldrecords.com/" target="_blank" rel="nofollow noopener">GWR</a> guidelines. In past attempts, up to 30 minutes has been allowed for this preparatory phase. He then lay on his back at the bottom of the pool, hands behind his head.</p>



<p>Also read: <strong><a href="https://divernet.com/scuba-news/freediving/59yr-old-claims-mens-breath-hold-walk-record/">59yr-old claims men‚Äôs breath-hold walk record</a></strong></p>



<p>‚ÄúAfter the 20-minute mark, everything became easier, at least mentally,‚Äù he said after surfacing, but explained that the experience had ‚Äúgot worse and worse physically, especially for my diaphragm, because of the contractions. But mentally I knew I wasn‚Äôt going to give up.‚Äù He credited his achievement to the support of his team, family and friends.</p>



<h2 id="record-history">Record history</h2>



<p>The record had previously been held by fellow-Croatian Budimir ≈†obat who, in 2021 at the age of 56, held his breath for 24min, 37sec, breaking the existing record by 34sec.</p>



<p>Back in 2008, magician and endurance artist David Blaine had set the GWR record at 17min 4sec during a live broadcast on <em>The Oprah Winfrey Show</em>.&nbsp;</p>



<p>For comparison, the official AIDA world record for static apnea (underwater breath-hold on air) is 11min 35sec, set by Frenchman St√©phane Mifsud in 2013. The GWR static apnea record, which has its own verification protocols, was set by Serbian Branko Petroviƒá at 11min 54sec the following year. </p>



<p>Mariƒçiƒá‚Äôs AIDA static apnea best is 10min 8sec. He also set the Guinness World Record for <a href="https://divernet.com/scuba-news/freediver-goes-for-an-epic-walk/" data-type="link" data-id="https://divernet.com/scuba-news/freediver-goes-for-an-epic-walk/">longest underwater walk</a> on one breath at 107m in 2021.</p>



<p>Oxygen pre-breathing is understood to be a technique used by actors on some underwater film-shoots to allow them to stay immersed for longer. Denitrogenation, the process of replacing the nitrogen in the lungs with oxygen, can increase the amount of usable oxygen from some 450ml to almost 3 litres. </p>



<p>Reducing carbon dioxide build-up delays the urge to breathe and extends the ‚Äúsafe apnea time‚Äù before oxygen levels drop to dangerous levels. </p>



<p>Highly controlled diaphragmatic breathing is required in the breathe-up, and the body has to be deeply relaxed to keep the heart-rate low, requiring exceptional body awareness, breathing technique and mental control.</p>



<p><strong>Also on Divernet: <a href="https://divernet.com/scuba-news/freediver-goes-for-an-epic-walk/">Freediver goes for an epic walk</a>, <a href="https://divernet.com/scuba-news/freediving/freediver-klovar-breaks-trubridges-17-year-no-fins-reign/">Freediver Klovar breaks Trubridge‚Äôs 17-year no-fins reign</a>, <a href="https://divernet.com/scuba-news/kate-winslet-breath-holding-for-britain/">Kate Winslet: breath-holding for Britain?</a>, <a href="https://divernet.com/scuba-news/freediving/ex-scuba-instructor-died-in-pool-breath-hold/">Ex scuba instructor died in pool breath-hold</a></strong></p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What could have been (134 pts)]]></title>
            <link>https://coppolaemilio.com/entries/what-could-have-been/</link>
            <guid>44945966</guid>
            <pubDate>Mon, 18 Aug 2025 22:29:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://coppolaemilio.com/entries/what-could-have-been/">https://coppolaemilio.com/entries/what-could-have-been/</a>, See on <a href="https://news.ycombinator.com/item?id=44945966">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p>No matter what your opinion on AI is, you probably have one already. Positive or negative, AI discourse is hard to avoid. 
The most dedicated AI evangelists will tell you that: <em>‚ÄúAGI is around the corner. Your job? obsolete. You don‚Äôt think so? Well, see how much things improved from last year? imagine how it will be like in 5 years!‚Äù</em> You‚Äôve read everything that could be said about it already at least 10 times.</p>

<p>I‚Äôve seen people trying to debunk all those claims but there‚Äôs something that I rarely see.</p>

<p>What could have been if instead of spending so much energy and resources on developing ‚ÄúAI features‚Äù we focused on making our existing technology better?</p>

<p>AI gets added into every software possible nowadays. Features almost no one wants, and no one needs, that only make the existing software worse. I‚Äôm sure you know what I mean, we have all been victims of it in some way or another.</p>

<p>(I would insert here one of those funny screenshots of Google telling you to eat rocks but you‚Äôve seen it already)</p>

<p>It pains me to think about all the money being funneled into the AI bubble. Money that could have funded so many real solutions to real problems. There is a lot of software to be made, improved, and released, but the tech industry refuses to see this.</p>

<p>Tech executives are robbing every investor blind. Promising the biggest breakthrough in technology while our current technology rots to the core. The operative systems we run, the browsers we use, our critical infrastructure gets consistently neglected to chase a promised wonderland of automation that never arrives.</p>

<p>There isn‚Äôt a single day where I don‚Äôt have to deal with software that‚Äôs broken but no one cares to fix. And I know what AI enthusiast will say: don‚Äôt worry, AI will fix them for us!</p>

<p>On Bluesky I saw <a href="https://bsky.app/profile/rystorm.com/post/3lwoqjqu6t22u">this post by Robin-Yann Storm</a>:</p>

<blockquote>
  <p>Gamescom‚Äôs app added an AI feature this year and it did not go well. Folks were overwhelmed with automatically generated meeting requests that they did not want. It generated a lot of stuff, but not value.</p>
</blockquote>

<p>The post contains an image of an email from Gamescom that reads:</p>

<blockquote>
  <p>Hello Robin-Yann,
We tested a new feature today - the Al meeting generator. The aim was to suggest suitable business contacts based on your profiles and make it easier for you to plan your trade fair contacts.
However, your honest feedback shows us that this feature does not provide the desired added value. We have therefore decided to completely remove the automatically generated meetings from your profiles.
Thank you for your openness. Your feedback is a central component of our further development - together we will make the platform better.
We apologize for any inconvenience caused.
Your gamescom team</p>
</blockquote>

<p>I‚Äôve been to many conferences since <a href="https://coppolaemilio.com/entries/my-first-gdc/">I started working at the Godot Foundation</a>, and they all have one thing in common: horribly broken meeting/networking apps. If you‚Äôve been to a conference you probably already suffered them. The direct messages work half of the time leaving messages undelivered. Registered people missing when you search for them, but still visible if you open a direct link to their profile. Scheduling features that fail to process any kind of rescheduling. The list goes on.</p>

<p>People end up ditching them to network via other apps: linkedin, twitter, email, bluesky. But I still believe that the idea of a networking app for a conference is still good and should be pursued. So I ask: Why is adding AI the priority here? What could have been if the investment went into making these apps better?</p>

<p>I‚Äôm not naive. What motivates people to include AI everywhere is the promise of profit. What motivates most AI startups or initiatives is just that. A promise.</p>

<p>But, don‚Äôt you think that by making a good product you will achieve that profit? Your product doesn‚Äôt need AI to get more users, more money, more features. It just needs to be better.</p>

<p>Unfortunately, people making decisions (if there are any) only chase ghosts and short term profits. They don‚Äôt think that they are crippling their companies and dooming their long term profitability. The enshittification is a disease that will keep spreading trying to suck every ounce of life from every product until they are too weak to continue.</p>

<p>I could now finish the post by breaking down budgets of organizations such as Blender, Godot, or Ladybird and comparing them with those that the tech gigants are spending on chasing their AI dreams while delivering absolutely nothing of value. But it would be too depressing. Last time I did this, with just one company‚Äôs budget you could fund more than 100 years of real open-source software development. Solving real needs we have now. Improving software we use every day. Making critical infrastructure that our life depends on.</p>

<p>Even if we manage to snap out of the AI bubble, we are never going to get these years back. I can only be left to wonder what could have been.</p>

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lab-grown salmon hits the menu (147 pts)]]></title>
            <link>https://www.smithsonianmag.com/smart-news/lab-grown-salmon-hits-the-menu-at-an-oregon-restaurant-as-the-fda-greenlights-the-cell-cultured-product-180986769/</link>
            <guid>44945959</guid>
            <pubDate>Mon, 18 Aug 2025 22:29:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/smart-news/lab-grown-salmon-hits-the-menu-at-an-oregon-restaurant-as-the-fda-greenlights-the-cell-cultured-product-180986769/">https://www.smithsonianmag.com/smart-news/lab-grown-salmon-hits-the-menu-at-an-oregon-restaurant-as-the-fda-greenlights-the-cell-cultured-product-180986769/</a>, See on <a href="https://news.ycombinator.com/item?id=44945959">Hacker News</a></p>
Couldn't get https://www.smithsonianmag.com/smart-news/lab-grown-salmon-hits-the-menu-at-an-oregon-restaurant-as-the-fda-greenlights-the-cell-cultured-product-180986769/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Shamelessness as a strategy (2019) (193 pts)]]></title>
            <link>https://nadia.xyz/shameless</link>
            <guid>44945943</guid>
            <pubDate>Mon, 18 Aug 2025 22:27:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nadia.xyz/shameless">https://nadia.xyz/shameless</a>, See on <a href="https://news.ycombinator.com/item?id=44945943">Hacker News</a></p>
<div id="readability-page-1" class="page"><div ?="">
        <p>I‚Äôve enjoyed playing a game called Avalon recently. I won‚Äôt go too far into the rules, but it‚Äôs a <a href="https://www.bestplay.co/ultimate-guide-hidden-role-board-games/">hidden role game</a> in the vein of Secret Hitler or Werewolf, where one team is ‚Äúgood‚Äù, trying to uncover who among them is ‚Äúevil‚Äù, before the evil team wins.</p>

<p>One of the characters you can play is Merlin. Merlin knows who the evil players are, but can‚Äôt reveal what he knows, because the evil team can kill Merlin and win the game. So Merlin relies on another character, Percival, to be his decoy.</p>

<p>Percival‚Äôs only power is that he knows who Merlin is. So he secretly watches Merlin‚Äôs actions throughout the game and amplifies those signals to the rest of the group. The typical strategy is for Percival to attract attention away from Merlin and towards himself.</p>

<p>But another, riskier strategy is for <em>Merlin</em> to play as though <em>he</em> is Percival. In this case, Merlin displays what he knows so shamelessly that he throws everyone off. The evil team, believing that no Merlin would be stupid enough to put himself out there like that, figures he must be Percival, and writes him off.</p>

<p>The Merlin-as-Percival strategy is bold, because it blatantly defies our expectations about how the game is won. To pull it off, Merlin must create confusion around his actions. He needs the other players to feel unsure about whether he‚Äôs being incredibly stupid or incredibly smart.</p>

<p>Increasingly, I think the ‚Äúshameless‚Äù approach is becoming a dominant strategy today. It was first popularized in modern canon by Paris Hilton, who played the ‚Äúdumb blonde heiress‚Äù stereotype so smoothly that everyone assumed she really was as stupid as she seemed.</p>

<p>Paris didn‚Äôt play by the ‚Äúobvious‚Äù rules for famous people. She was widely derided by both media and her peers as at best, a train wreck, and at worst, a self-serving aggrandizer. And yet, people couldn‚Äôt stop talking about her. A decade later, Paris is now remembered as the mastermind behind the playbook that‚Äôs made the Kardashians, Jenners, and other celebrity socialites so successful.</p>

<p>It‚Äôs important to note that people were dismissive of Paris because validating her playbook would mean admitting that <em>they</em> were playing an inferior game. Everyone else had invested years into optimizing for the most legible version of the rules. They‚Äôd look silly if they were to admit she had found a better way of doing things.</p>

<p>Without getting into tiresome politics, the ‚Äúshameless‚Äù strategy also defined the 2016 U.S. presidential elections. It was shouted down by people in both established political parties, because they were used to playing by the ‚Äúobvious‚Äù rules, but I suspect that in a decade‚Äôs time, we‚Äôll look back on that election and realize that it marked the beginning of an entirely new style of politics.</p>

<p>Ditto to, perhaps, the leadership styles of Mark Zuckerberg, who‚Äôs followed the ‚Äúobvious‚Äù playbook for years, versus Jack Dorsey, who employs tactics that seem so obviously stupid (tweeting about fasting and meditation!) that we‚Äôre quick to write them off. And yet, I‚Äôd guess that Zuckerberg‚Äôs strategy makes him increasingly unlikeable and untrustworthy, in the same way that any major politician sticking to a pre-2016 playbook today is almost certainly not going to win.</p>

<p>The shameless strategy feels counterintuitive, because our first instinct is to want to punish that sort of behavior. And historically, those sanctions have been effective. Punishing outlandish behavior is an important aspect of cooperative governance: it preserves social order by ensuring that we all play by the same rules.</p>

<p>Today, it seems like punishing shamelessness only <em>increases</em> social rewards to the transgressor. What‚Äôs changed?</p>

<p>One explanation might be that it‚Äôs an expected effect of the blurring of social boundaries today. In the past, if the size of your community was finitely bounded (like a village, or an aristocratic social class), people didn‚Äôt enter or exit these communities as frequently. Under these conditions, sanctions are probably still effective, because members of the community want to be liked and accepted.</p>

<p>But the borders to online communities are much more fluid - perhaps even nonexistent. Under open borders, sanctions will backfire, because they just serve as a signaling boost for the transgressor, attracting outsiders who resonate with that person‚Äôs message. What‚Äôs meant to be punishment instead becomes a flare shot straight into the night sky.</p>

<p>The ‚Äúestablishment‚Äù mistakenly assumes that a shameless person wants the approval of their community, when it turns out that, much like any cult or counterculture, that person‚Äôs goal was to attract a following, regardless of who the members are. The disgust of one‚Äôs peers doesn‚Äôt matter anymore, because that disgust forms the basis for an entirely new community.</p>

<p>A common critique of shameless people is questioning their intelligence. But one of the most bizarre aspects here is it doesn‚Äôt actually matter how aware that person is of what they‚Äôre doing. The concept of a ‚Äúgenius mastermind‚Äù is itself outdated, because it assumes that someone needs to be in control. The shameless person is simply a host for a set of ideas, which, like any virus, will continue to propagate as long as there are willing hosts to receive it.</p>

<p>I‚Äôm not really sure what the long-term implications of shamelessness will be. I also don‚Äôt think that everybody has to employ this strategy to win (at least, I hope not!). But what I do know is when I see my peers rolling their eyes at someone or deriding them for being ‚Äúshameless‚Äù, there‚Äôs a good chance that, instead of writing them off, we should examine their actions a bit more closely.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Newsmax agrees to pay $67M in defamation case over bogus 2020 election claims (178 pts)]]></title>
            <link>https://apnews.com/article/dominion-voting-newsmax-defamation-trump-2020-3b2366dfdae3a8432afe822bf14fe1ef</link>
            <guid>44945925</guid>
            <pubDate>Mon, 18 Aug 2025 22:23:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/dominion-voting-newsmax-defamation-trump-2020-3b2366dfdae3a8432afe822bf14fe1ef">https://apnews.com/article/dominion-voting-newsmax-defamation-trump-2020-3b2366dfdae3a8432afe822bf14fe1ef</a>, See on <a href="https://news.ycombinator.com/item?id=44945925">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>DENVER (AP) ‚Äî The conservative network Newsmax will pay $67 million to settle a lawsuit accusing it of defaming a voting equipment company by spreading lies about President <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/hub/donald-trump">Donald Trump‚Äôs</a></span> 2020 election loss, according to documents filed Monday.</p><p>The settlement comes after Fox News Channel <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/fox-news-dominion-lawsuit-trial-trump-2020-0ac71f75acfacc52ea80b3e747fb0afe">paid $787.5 million</a></span> to settle a similar lawsuit in 2023 and Newsmax paid what court papers describe as $40 million to <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/smartmatic-newsmax-lawsuit-2020-election-96d35dc10009b68cbb548ef7bea10284">settle a libel lawsuit</a></span> from a different voting machine manufacturer, Smartmatic, which also was <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/election-2020-joe-biden-donald-trump-technology-electronic-voting-cd68ad2022611a36154ff3f243fcd1d8">a target</a></span> of pro-Trump conspiracy theories on the network.</p><p>Delaware Superior Court Judge Eric Davis had ruled earlier that Newsmax did indeed defame Denver-based Dominion Voting Systems by airing false information about the company and its equipment. But Davis left it to a jury to eventually decide whether that was done with malice, and, if so, how much Dominion deserved from Newsmax in damages. Newsmax and Dominion reached the settlement before the trial could take place.</p>
    
<p>The settlement was disclosed by Newsmax in a new filing with the U.S. Securities and Exchange Commission. It said the deal was reached Friday. </p>



<p>‚ÄúNewsmax believed it was critically important for the American people to hear both sides of the election disputes that arose in 2020,‚Äù the company said in a statement. ‚ÄúWe stand by our coverage as fair, balanced, and conducted within professional standards of journalism.‚Äù</p>
    
    
    
<p>A spokesperson for Dominion said the company was pleased to have settled the lawsuit.</p><p>The disclosure of the settlement came as Trump, who <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/joe-biden-wins-white-house-ap-fd58df73aa677acb74fce2a69adb71f9">lost his 2020 reelection bid</a></span> to Democrat Joe Biden, vowed in a social media post Monday to eliminate mail-in ballots and voting machines such as those supplied by Dominion and other companies. It was unclear how the Republican president could achieve that.</p><p>The same judge also handled the Dominion-Fox News case and made a similar ruling that the network repeated <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/fox-news-dominion-lawsuit-trial-explainer-trump-fbd401a951905879d837a8860b3bec5e">numerous lies</a></span> by Trump‚Äôs allies about his 2020 loss despite <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/united-states-government-news-media-donald-trump-fraud-b52914ec21a97dec8b5d878a908d566f">internal communications</a></span> showing Fox officials knew the claims <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/politics-fraud-donald-trump-24d6322f99281fdfb46c272e3ac6bacf">were bogus</a></span>. At the time, Davis found it was ‚ÄúCRYSTAL clear‚Äù that none of the allegations was true.</p>
    
<p>Internal correspondence from Newsmax officials likewise shows they knew the claims were baseless.</p><p>‚ÄúHow long are we going to play along with election fraud?‚Äù Newsmax host Bob Sellers said two days after the 2020 election was <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/election-claims-biden-won-explained-bd53b14ce871412b462cb3fe2c563f18">called for Biden</a></span>, according to internal documents revealed as part of the case. </p><p>Newsmax took pride that it was not calling the election for Biden and, the internal documents show, saw a business opportunity in catering to viewers who believed Trump won. Private communications that surfaced as part of Dominion‚Äôs earlier defamation case against Fox News <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/politics-television-donald-trump-business-1a4337a89c8abd952a814c60fa269b3c">also revealed</a></span> how the network‚Äôs business interests intersected with decisions it made related to coverage of <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/capitol-riot-trump-election-lies-explainer-816a43ed964e6d35f03b0930e6e56c82">Trump‚Äôs 2020 election claims</a></span>.</p><p>At Newsmax, employees repeatedly warned against false allegations from pro-Trump guests such as attorney <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/donald-trump-joe-biden-michigan-detroit-election-2020-4fd2ba9b84e9d9a6bcddd51872ba3f97">Sidney Powell</a></span>, according to documents in the lawsuit. In one text, even Newsmax owner Chris Ruddy, a Trump ally, said he found it ‚Äúscary‚Äù that Trump was meeting with Powell.</p>
    
<p>Dominion was at the heart of many of the wild claims aired by guests on Newsmax and elsewhere, who promoted a conspiracy theory involving deceased Venezuelan president Hugo Chavez to rig the machines for Biden. The network <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/fox-newsmax-smartmatic-dominion-lawsuits-826071eb5b52ec8aea6b0028da78c61c">retracted some of the more bombastic allegations</a></span> in December 2020.</p><p>Though Trump has insisted <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-2020-election-lies-debunked-4fc26546b07962fdbf9d66e739fbb50d">his fraud claims</a></span> are real, there‚Äôs no evidence they were, and the lawsuits in the Fox and Newsmax cases show how some of the president‚Äôs biggest supporters knew they were false at the time. Trump‚Äôs then-attorney general, William Barr, said there was <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/barr-no-widespread-election-fraud-b1f1488796c9a98c4b1a9061a6c7f49d">no evidence</a></span> of widespread fraud.</p><p>Trump and his backers lost <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/donald-trump-losing-election-lawsuits-36d113484ac0946fa5f0614deb7de15e">dozens of lawsuits</a></span> alleging fraud, some before Trump-appointed judges. Numerous <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/election-2020-joe-biden-donald-trump-georgia-elections-4eeea3b24f10de886bcdeab6c26b680a">recounts</a></span>, <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/elections-government-and-politics-nevada-ed4d5296d9fd7fd9afd83a3fe845c205">reviews</a></span> and <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/joe-biden-wisconsin-presidential-elections-state-elections-madison-9a2f172dd8074668ded26bd5b0b41fbb">audits</a></span> of the election results, including <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/donald-trump-joe-biden-election-2020-elections-government-and-politics-4b6643aa699480dc63cbce8555aac946">some run by Republicans</a></span>, turned up no signs of significant wrongdoing or error and affirmed Biden‚Äôs win.</p><p>After returning to office, Trump <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/capitol-jan-6-pardons-trump-justice-department-8ce8b2a8f8cb602d5eaf85ac7b969606">pardoned</a></span> those who tried to halt the transfer of power during the <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/congress-confirm-joe-biden-78104aea082995bbd7412a6e6cd13818">Jan. 6, 2021, attack</a></span> on the U.S. Capitol and <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-retaliation-miles-taylor-chris-krebs-efb1416926df9d1086fa21349a18f90b">directed</a></span> his Department of Justice to investigate Chris Krebs, a former Trump cybersecurity appointee who had <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/top-officials-elections-most-secure-66f9361084ccbc461e3bbf42861057a5">vouched for the security and accuracy</a></span> of the 2020 election.</p>
    
<p>As an initial trial date approached in the Dominion case earlier this year, Trump issued an <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://www.whitehouse.gov/presidential-actions/2025/04/addressing-risks-from-susman-godfrey/" target="_blank" rel="noopener">executive order</a></span> attacking the law firm that litigated it and the Fox case, Susman Godfrey. The order, part of a series <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-perkins-coie-law-firm-executive-order-206052ec8157380fb2e23010a6f88815">targeting law firms</a></span> Trump has tussled with, cited Susman Godfrey‚Äôs work on elections and said the government would not do business with any of its clients or permit any of its staff in federal buildings.</p><p>A federal judge put that action on hold, saying the framers would view it as <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-law-firm-susman-godfrey-eada6cc436533ea3b483568c8287600e">‚Äúa shocking abuse of power.</a></span> ‚Äù</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Phrack 72 (142 pts)]]></title>
            <link>https://phrack.org/issues/72/1</link>
            <guid>44945660</guid>
            <pubDate>Mon, 18 Aug 2025 21:43:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phrack.org/issues/72/1">https://phrack.org/issues/72/1</a>, See on <a href="https://news.ycombinator.com/item?id=44945660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<table>
   <tbody>
      <tr><td><a href="https://phrack.org/issues/72/1_md.html#article">Introduction</a></td><td>Phrack Staff</td></tr>

<tr><td><a href="https://phrack.org/issues/72/2.html#article">Phrack Prophile on Gera</a></td><td>Phrack Staff</td></tr>

<tr><td><a href="https://phrack.org/issues/72/3_md.html#article">Linenoise</a></td><td>Phrack Staff</td></tr>

<tr><td><a href="https://phrack.org/issues/72/4_md.html#article">Loopback</a></td><td>Phrack Staff</td></tr>

<tr><td><a href="https://phrack.org/issues/72/5_md.html#article">The Art of PHP - My CTF Journey and Untold Stories!</a></td><td>Orange Tsai</td></tr>

<tr><td><a href="https://phrack.org/issues/72/6_md.html#article">Guarding the PHP Temple</a></td><td>mr_me</td></tr>

<tr><td><a href="https://phrack.org/issues/72/7_md.html#article">APT Down - The North Korea Files</a></td><td>Saber, cyb0rg</td></tr>

<tr><td><a href="https://phrack.org/issues/72/8_md.html#article">A learning approach on exploiting CVE-2020-9273</a></td><td>dukpt</td></tr>

<tr><td><a href="https://phrack.org/issues/72/9_md.html#article">Mapping IOKit Methods Exposed to User Space on macOS</a></td><td>Karol Mazurek</td></tr>

<tr><td><a href="https://phrack.org/issues/72/10_md.html#article">Popping an alert from a sandboxed WebAssembly module</a></td><td>th0mas.nl</td></tr>

<tr><td><a href="https://phrack.org/issues/72/11_md.html#article">Desync the Planet - Rsync RCE</a></td><td>Simon, Pedro, Jasiel</td></tr>

<tr><td><a href="https://phrack.org/issues/72/12_md.html#article">Quantom ROP</a></td><td>Yoav Shifman, Yahav Rahom</td></tr>

<tr><td><a href="https://phrack.org/issues/72/13_md.html#article">Revisiting Similarities of Android Apps</a></td><td>Jakob Bleier, Martina Lindorfer</td></tr>

<tr><td><a href="https://phrack.org/issues/72/14_md.html#article">Money for Nothing, Chips for Free</a></td><td>Peter Honeyman</td></tr>

<tr><td><a href="https://phrack.org/issues/72/15_md.html#article">E0 - Selective Symbolic Instrumentation</a></td><td>Jex Amro</td></tr>

<tr><td><a href="https://phrack.org/issues/72/16_md.html#article">Roadside to Everyone</a></td><td>Jon Gaines</td></tr>

<tr><td><a href="https://phrack.org/issues/72/17_md.html#article">A CPU Backdoor</a></td><td>uty</td></tr>

<tr><td><a href="https://phrack.org/issues/72/18_md.html#article">The Feed Is Ours</a></td><td>tgr</td></tr>

<tr><td><a href="https://phrack.org/issues/72/19.html#article">The Hacker's Renaissance - A Manifesto Reborn</a></td><td>TMZ</td></tr>

   </tbody>
</table>

<p><strong>Title</strong> : Introduction</p>
<p><strong>Author</strong> : Phrack Staff</p>
<pre>                              ==Phrack Inc.==

                Volume 0x10, Issue 0x48, Phile #0x01 of 0x12

|=-----------------------------------------------------------------------=|
|=-------------------------=[ Introduction ]=----------------------------=|
|=-----------------------------------------------------------------------=|
|=----------------------=[    Phrack Staff    ]=-------------------------=|
|=-----------------------=[ <a href="https://phrack.org/cdn-cgi/l/email-protection" data-cfemail="d9aaadb8bfbf99a9b1abb8bab2f7b6abbe">[email&nbsp;protected]</a> ]=--------------------------=|
|=-----------------------------------------------------------------------=|
|=----------------------=[  August  19, 2025  ]=-------------------------=|
|=-----------------------------------------------------------------------=|

    _______ ____ ____    _______     _______________  _____   _____
 ._\\____  \\   |    |__\\__    \  _\\__   /\    __//_\    | /    /
 :    |/   &gt;&gt;   :    :    :/    /./    /    |    |   /.    !/    /
 |    :    /         |    /     \|    __    |    |    |    /     \
 |    ____/|    |    |    \      \     |    |__  :    |    \      \
 |    |/// |____|    |_____\     |\___ |    ://\      !_____\      \
 |    :    /////:____|/////\\____|////\\___/.   \\____://///\\_____/
 |___/ e-zine   /////:     //////|     ////      /////       //////
 ////                .           :                x0^67^aMi5H^iMP!


--[ Hacker Evolution 


For 40 years, Phrack has published papers that have reflected and shaped 
hacker culture. The knowledge shared in Phrack has laid the foundation for
many fields of study, providing insight, a shared language, resources 
and tools, as well as context and history. Phrack is written by hackers, 
for hackers, and offers a glimpse into the world just beyond what most 
people see.


Phrack is both a technical journal and a cultural document. Like all zines, 
it represents a snapshot of the scene at the time. We share not just our 
discoveries, but the stories of how we came to know things and the context 
in which we existed. We share our triumphs, failures, and lessons learned. 
By fostering a culture of communal idea sharing, we learn how to solve 
problems creatively, and make the most of our current situation. 


Over the past 40 years, hacking has evolved, splintered, and mutated into 
a variety of forms. Phrack has documented many of the key innovations in 
hacking since its first issue: From showcasing ways to manipulate the phone 
system and other large computers, to pioneering vulnerability scanning, to 
generalizing security concepts such as buffer overflows, ROP, and heap 
exploitation, to bringing it all together within complex ecosystems that 
seemed like just a fantasy in years past. Each generation builds off the 
previous and offers us new perspectives, remixing with older ideas and 
demonstrating how they can be reapplied to new situations. When Phrack was 
first published in the mid-80s, our relationship with technology was quite 
different. Many of our challenges involved simply getting and staying 
online. Today we face entirely new challenges based on what is, what was, 
and what will be.


The hacker ethos remains the same - be curious about your world, make do 
with what you have, and show how things can be better.


What was done before us, and the knowledge shared, provides the base for 
us to build off of and evolve from. As hackers, we pass on our best 
characteristics by teaching others. We document how and why we did things
based on what was available at the time. We expand on previous generations'
work and piece together our own understanding, informed by our own personal 
experience. The reward is the beauty of what we discover and create, and
the joy of sharing and inspiring others. Over time, all of our most beloved 
and reliable techniques and tools become common knowledge, and new 
permutations pop up. As circumstances change, so do our needs. What's old
becomes new again, new ideas recontextualize the old. The cycle continues.


Knowledge is the hacker DNA. Our instincts and curiosity are complimented 
by the stories of how things were accomplished before. Like hackers and 
humans before us, we adapt to our environment, and figure out how to meet
our needs and achieve our goals. As technology becomes more optimized and 
abstracted, it can be easy to lose track of the fundamentals. Just because 
tech has evolved doesn't mean the foundation has changed. We still use AT 
commands to control our modems. We still activate the A20 line to access
memory beyond 1MB on x86 CPUs. In-band signaling is still a pathway into 
the toughest systems. Weird machines still manifest throughout it all, 
waiting to be discovered by a hacker like you.


Everything is in a state of flux, and the only constant is change. Yet, if 
we position ourselves correctly, our actions can influence the future. 
Things mutate. There are happy accidents. The world is chaos, and out of 
chaos emerges hackers.


Hacking has no choice but to evolve, and hacker zines like Phrack evolve
with it. We look back at our foundation for inspiration, while we also look
forward towards uncharted territory, unafraid of going beyond. We venture 
into the deepest darkest rabbit holes that few dare to tread, where we see 
the light that leads us to the most amazing treasures. We address the needs 
of our communities, find ways to grow together, and encourage each other
to keep exploring. We maintain projects like Phrack because it gives a 
platform for the unadulterated voice of the hacker. 


Humans are hackers. We were put here to figure things out. Hacking is an
innate skill to be tapped into and developed. The hacker spirit guides us 
through situations once thought hopeless. Hacking is a way to answer your 
own burning questions, a way to discover your own potential, and a way to 
create a world you want to live in.


There is a hacker born every day. It's our duty to share things that can
inform them of the past and present, and give them hope for a better 
tomorrow. After all, we're all alike.


--[ Welcome to Phrack #72 

This edition is not just a milestone but a testament to the relentless
curiosity, stubborn brilliance, and uncompromising spirit of a global 
community that refuses to be silenced. A tribute to the old school and 
the new blood. To the legends who paved the way, and to those just 
starting to carve their path. It‚Äôs held together with tape, sweat, late 
nights, fried brains, and that twitchy love for the broken and beautiful 
mess of systems.

Huge thanks to every author who contributed their knowledge, tools, 
exploits, vision, and war stories. Your work keeps the scene alive 
and sharp.

To the reviewers and editors who read between the lines and asked the 
hard questions. You pushed for clarity without dulling the edge.

To the artists who dropped visuals, raw pixel filth, and clean design. 
You gave this issue texture. You made it feel.

To the layout crew who made this beast look like something worth 
printing in blood. Your work is proof that style and substance can 
coexist.

To everyone who tested drafts, pointed out typos, suggested better 
payloads, or tighter phrasing. You know who you are. We see you.

To the donors who pitched in to fund the printing of our anniversary 
edition. You helped keep this thing afloat, independent, and untamed.

And to the scene. The real one. You‚Äôre the reason we‚Äôre still doing 
this. This is yours.

A scream in a world that wants silence. A spark under a mountain of 
dead protocol.

Phrack lives because you do, so welcome to the noise and enjoy.

‚Äî Phrack Staff
ISSN 1068-1035


In this edition:

- 16 bangers from some of the sharpest minds in the scene

- A Prophile on the legendary Gera

- A puzzle to melt your brain (and flex your ego)

- A full-on CTF ‚Äî win it and claim your Phrack coin!

- Visuals that hit like a payload: stunning GFX, ASCII and glitch art, 
  pure eye candy


--[ Greetz

Phrack 72 would not have been possible without the incredible hacker scene 
coming together to help make it happen. Thank you to all the authors, 
artists, donors, editors, and logistics experts who made this historic 
issue and international release a reality.

Special thanks to our artists: ackmage, amnesia, aNACHRONiST, araknet, 
bubok arsonian, digitalis, fyodor, ic3qu33n, jinn, mar, mavenmob, netspooky, 
p0rtL, s01den, x0, ytcracker. 

Massive thanks to the Paged Out crew for their help making an incredible 
print layout.

This zine would not have been possible without the following people:

ackmage         -- minted the only coin that'll be worth anything in 2026
bsdaemon        -- still carrying hackers forward
chompie         -- omg will u sign my driver?
clockwerk       -- l0ve
dark tangent    -- uber-supporter since dayZero
diaul           -- w0rd
horizon         -- ADM 4ever
Julio           -- The rise of the  alligatorzzzz
messede         -- &lt;b&gt;awesome&lt;/b&gt;
netspooky       -- ultimate cyber skull
pinguino        -- all your news are belong to her
retr0id         -- switch 2, retr0id 1, day 0
richinseattle   -- keeper of the lore
sblip           -- whaddup blip
skyper          -- tripleg√§nger
TMZ             -- just a chill guy
Phrack Staff    -- for holding it down
Phrack's Tariff -- we got PCBs at home


--[ Phrack policy

phrack:~# head -77 /usr/include/std-disclaimer.h
/*
 *  All information in Phrack Magazine is, to the best of the ability of
 *  the editors and contributors, truthful and accurate.  When possible,
 *  all facts are checked, all code is compiled.  However, we are not
 *  omniscient (hell, we don't even get paid).  It is entirely possible
 *  something contained within this publication is incorrect in some way.
 *  If this is the case, please drop us some email so that we can correct
 *  it in a future issue.
 *
 *
 *  Also, keep in mind that Phrack Magazine accepts no responsibility for
 *  the entirely stupid (or illegal) things people may do with the
 *  information contained herein.  Phrack is a compendium of knowledge,
 *  wisdom, wit, and sass.  We neither advocate, condone nor participate
 *  in any sort of illicit behavior.  But we will sit back and watch.
 *
 *
 *  Lastly, it bears mentioning that the opinions that may be expressed in
 *  the articles of Phrack Magazine are intellectual property of their
 *  authors.
 *  These opinions do not necessarily represent those of the Phrack Staff.
 */


--[ Phrack 73 Call For Papers 

Why should you write for Phrack?

- You have a project you've been working on that pushes the limits in some 
  way, doing things that haven't been publicly shared before. If you do 
  cutting edge security research, you should write for Phrack.

- You are interested in aspects of security and technology that other people
  don't seem to care about or understand. If you feel like you need to shed
  light on a certain topic for all to see, you should write for Phrack.

- You keep seeing the same problems over and over and wish someone would 
  just write a straightforward guide for everyone. If you are the person 
  who can write that for now and future generations, you should write for 
  Phrack.

- You deserve a place to share your finest work without fear. If you don't
  want something you poured your heart and soul into turned into another 
  metric for shareholders and potential investors or trapped forever inside
  a corporate VPN, you should write for Phrack.

Worst Case Scenario: You just wrote a cool paper you can share on your own.

Best Case Scenario: Your paper gets published in Phrack. :)


::. What do we want? .::

Articles about hacking, exploit development, vulnerability research, and 
other fine topics.

More Specifically:  

- Exploitation: Demonstrate the latest techniques in breaking mitigations 
  and proving vulnerabilities still have bite.

- Persistence: Whether its userland, kernel, or below ring0, share a trick 
  in the dark art of stealth persistence.

- Fuzzing and Code Analysis: Automated bug hunting finds the low hanging 
  fruit, how can we make it more lethal?

- Binary Obfuscation: We have decompilers but custom VMs and powerful 
  transformations create impenetrable code.

- Data Obfuscation: Time to exfil, how do you move a terabyte of data 
  without looking like a fool?

- Anti-Forensics: Leave no trace, hide in the mist, or maybe even deploy 
  countermeasures on the parsers!

- Web Applications: Discuss confusion, bypass, and injection attacks on 
  the multi-headed hydra of modern applications.

- Cloud Security: Complexity and lack of visibility is the soft underbelly
  of cloud compute, discuss.

- Data Storage Weaknesses: If data theft is the end goal, let's talk about 
  all the leaks in storage infra and APIs. 

- Exploit Mitigation: Finding every bug is futile, how do we design more 
  secure systems to block these attacks?

- Malware Analysis: The world is covered in a diaspora of malware, flay 
  open the code and inspect the innards.

- Malware Defense: We wade in a pool of parasites attacking from every 
  angle, how do we block their vicious effects?

- Exotic Reverse Engineering: What are the corrupt hidden hands of power 
  in this electronic substrate?

- Scene History: Let us learn the often isolated tribal history that hasn't
  been written about before.

- Tales from the Crypt: Want to tell a personal story of a righteous hack?
  Maybe we'll believe you.


::. When Do We Want It By? .::

June 2026


::. When will Phrack 73 be released? .::

Summer 2026

                   ----( Contact )----

    &lt;  Editors           : staff[at]phrack{dot}org         &gt;
    &gt;  Submissions       : submissions[at]phrack{dot}org   &lt;
    &lt;  /dev/urandom      : loopback[at]phrack{dot}org      &gt;
    &gt;  Arts &amp; Leisure    : arts[at]phrack{dot}org          &lt;
    &lt;  Twitter/X         : phrack                          &gt;
    &gt;  Mastodon          : phrack[at]haunted{dot}computer  &lt;
    &lt;  BlueSky           : phrack{dot}org                  &gt;


    The rules are simple:

    + 7-bit ASCII wrapped to 76 columns OR Markdown UTF-8
    + ANTISPAM in the subject line or face the Spam God and 
      walk backwards into hell
</pre>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Obsidian Bases (565 pts)]]></title>
            <link>https://help.obsidian.md/bases</link>
            <guid>44945532</guid>
            <pubDate>Mon, 18 Aug 2025 21:28:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://help.obsidian.md/bases">https://help.obsidian.md/bases</a>, See on <a href="https://news.ycombinator.com/item?id=44945532">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Fractional jobs ‚Äì part-time roles for engineers (236 pts)]]></title>
            <link>https://www.fractionaljobs.io</link>
            <guid>44945379</guid>
            <pubDate>Mon, 18 Aug 2025 21:10:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fractionaljobs.io">https://www.fractionaljobs.io</a>, See on <a href="https://news.ycombinator.com/item?id=44945379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2>HEY COMPANIES,</h2><div><p>Welcome to the place to hire&nbsp;</p><p>. We‚Äôre the largest network. We send you the top candidates. You can hire them directly.</p></div><div><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/686fee8d37b5301b91b82a2d_Group%20277%20(1).png" loading="lazy" alt=""></p></div></div><div><div><h2>Open Jobs</h2></div><div><div><div><p>Thank you! Your submission has been received!</p></div><div><p>Oops! Something went wrong while submitting the form.</p></div></div><div id="w-node-_9e108133-0d83-1535-4f7b-7f859e28816b-075f0119"><h3>Hiring?</h3><div><p>Fractional talent is ready today</p><p>Mid-level - executive talent only</p><p>Convert to full-time when ready</p><p>Ditch the W2 salaries, payroll tax, etc.</p></div></div></div></div><div id="jobs"><div><h2 id="live-no">x</h2><h2>Live Jobs</h2></div><div><div id="filter-container"><h3>Filter Jobs</h3><div><div><p>Thank you! Your submission has been received!</p></div><div><p>Oops! Something went wrong while submitting the form.</p></div></div></div><div id="live-jobs"><div fs-cmsfilter-element="list" role="list"><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Steelbay Equity Partners</h3><h3>&nbsp;-&nbsp;</h3><h3>Founding Marketer</h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote</p><div><p>Marketing</p><p>Syndicated</p><p>August 18, 2025</p><p>founding-marketer-at-steelbay-equity-partners</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Wedwallet</h3><h3>&nbsp;-&nbsp;</h3><h3>Legal Advisor</h3></p></div><div><p>10 - 20 hrs </p><p>&nbsp;|&nbsp;</p><p>$300 ‚Äì $500 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote</p><div><p>Legal</p><p>Syndicated</p><p>August 18, 2025</p><p>legal-advisor-at-wedwallet</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Everlywell</h3><h3>&nbsp;-&nbsp;</h3><h3>Product Manager</h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote</p><div><p>Product</p><p>Syndicated</p><p>August 18, 2025</p><p>product-manager-at-everlywell</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>PeakHealth</h3><h3>&nbsp;-&nbsp;</h3><h3>Marketing Lead </h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote </p><div><p>Marketing</p><p>Syndicated</p><p>August 18, 2025</p><p>marketing-lead-at-peakhealth</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>DUNE Suncare</h3><h3>&nbsp;-&nbsp;</h3><h3>Head of Marketing</h3></p></div><div><p>15 - 25 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote</p><div><p>Marketing</p><p>Syndicated</p><p>August 18, 2025</p><p>head-of-marketing-at-dune-suncare</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>IV DRIPS</h3><h3>&nbsp;-&nbsp;</h3><h3>Lead Developer </h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote</p><div><p>Engineering</p><p>Syndicated</p><p>August 18, 2025</p><p>lead-developer-at-iv-drips</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Burwood</h3><h3>&nbsp;-&nbsp;</h3><h3>Executive Director</h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>On-site (Chicago)</p><div><p>Other</p><p>Syndicated</p><p>August 18, 2025</p><p>executive-director-at-burwood</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>An AI Tutoring Startup</h3><h3>&nbsp;-&nbsp;</h3><h3>Chief Financial Officer</h3></p></div><div><p>10 hrs</p><p>&nbsp;|&nbsp;</p><p>¬£100 - ¬£125 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (UK only)</p><div><p>Finance</p><p>Syndicated</p><p>August 18, 2025</p><p>chief-financial-officer-at-an-ai-tutoring-startup</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>A European Insurtech Startup</h3><h3>&nbsp;-&nbsp;</h3><h3>Senior AI Engineer</h3></p></div><div><p>20 - 40 hrs</p><p>&nbsp;|&nbsp;</p><p>‚Ç¨85 - ‚Ç¨100 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (CET +/- 6hrs)</p><div><p>Engineering</p><p>Syndicated</p><p>August 15, 2025</p><p>senior-ai-engineer-at-a-european-insurtech-startup</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>A Wellness App for New Parents</h3><h3>&nbsp;-&nbsp;</h3><h3>Director of Marketing</h3></p></div><div><p>10 hrs</p><p>&nbsp;|&nbsp;</p><p>$6.5K - $7.5K / month</p><p>&nbsp;|&nbsp;</p><p>Remote (USA / Canada only)</p><div><p>Marketing</p><p>Syndicated</p><p>August 14, 2025</p><p>director-of-marketing-at-a-wellness-app-for-new-parents</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>An AI Model Training Startup</h3><h3>&nbsp;-&nbsp;</h3><h3>Senior Product Manager</h3></p></div><div><p>40 hrs</p><p>&nbsp;|&nbsp;</p><p>$20K - $25K / month</p><p>&nbsp;|&nbsp;</p><p>Remote (USA only)</p><div><p>Product</p><p>Syndicated</p><p>August 13, 2025</p><p>senior-product-manager-at-an-ai-model-training-startup-2</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>A Consumer Social Startup</h3><h3>&nbsp;-&nbsp;</h3><h3>Senior Full-Stack Engineer</h3></p></div><div><p>20 - 40 hrs</p><p>&nbsp;|&nbsp;</p><p>$125 - $150 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (EST +/- 5 hrs)</p><div><p>Engineering</p><p>Syndicated</p><p>August 12, 2025</p><p>senior-full-stack-engineer-at-a-consumer-social-startup</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Credible</h3><h3>&nbsp;-&nbsp;</h3><h3>Controller </h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote (USA only)</p><div><p>Finance</p><p>Syndicated</p><p>August 11, 2025</p><p>controller-at-credible</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>A79.ai</h3><h3>&nbsp;-&nbsp;</h3><h3> Vice President, AI Sales </h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote</p><div><p>Sales</p><p>Syndicated</p><p>August 11, 2025</p><p>vice-president-ai-sales-at-a79-ai</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Good Trouble</h3><h3>&nbsp;-&nbsp;</h3><h3>Chief Financial Officer </h3></p></div><div><p>8 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote</p><div><p>Finance</p><p>Syndicated</p><p>August 11, 2025</p><p>chief-financial-officer-at-good-trouble</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>HeartStamp</h3><h3>&nbsp;-&nbsp;</h3><h3>General Counsel</h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>$110K ‚Äì $160K yearly equiv.</p><p>&nbsp;|&nbsp;</p><p>Remote (USA only)</p><div><p>Legal</p><p>Syndicated</p><p>August 11, 2025</p><p>counsel-generative-ai-ip-at-heartstamp</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>A Creator-focused AI Startup</h3><h3>&nbsp;-&nbsp;</h3><h3>AI Engineer</h3></p></div><div><p>10 - 15 hrs</p><p>&nbsp;|&nbsp;</p><p>$100 - $125 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (USA, Canada, or Europe only)</p><div><p>Engineering</p><p>Syndicated</p><p>August 11, 2025</p><p>ai-engineer-at-a-creator-focused-ai-startup</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Project Own</h3><h3>&nbsp;-&nbsp;</h3><h3>Head of Product </h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>$125-$175 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (USA only)</p><div><p>Product</p><p>Syndicated</p><p>August 11, 2025</p><p>head-of-product-at-project-own</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Bayesian Health</h3><h3>&nbsp;-&nbsp;</h3><h3>Senior Product Designer </h3></p></div><div><p>10 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote</p><div><p>Design</p><p>Syndicated</p><p>August 11, 2025</p><p>senior-product-designer-at-bayesian-health</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>An Auto Parts eCommerce Brand</h3><h3>&nbsp;-&nbsp;</h3><h3>Tech Lead</h3></p></div><div><p>10 hrs</p><p>&nbsp;|&nbsp;</p><p>$100 - $125 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (USA time zones only)</p><div><p>Engineering</p><p>Syndicated</p><p>August 11, 2025</p><p>tech-lead-at-an-auto-parts-ecommerce-brand</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>A Social App for Activity Buddies</h3><h3>&nbsp;-&nbsp;</h3><h3>Chief Marketing Officer</h3></p></div><div><p>5 hrs</p><p>&nbsp;|&nbsp;</p><p>$150 - $200 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (USA only)</p><div><p>Marketing</p><p>Syndicated</p><p>August 8, 2025</p><p>chief-marketing-officer-at-a-social-app-for-activity-buddies</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>A Consumer Healthtech Marketplace</h3><h3>&nbsp;-&nbsp;</h3><h3>Director of Operations</h3></p></div><div><p>20 - 40 hrs</p><p>&nbsp;|&nbsp;</p><p>$125 - $150 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (SF, LA, MIA, NYC only)</p><div><p>Operations</p><p>Syndicated</p><p>August 8, 2025</p><p>director-of-operations-at-a-consumer-healthtech-marketplace</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>A Consumer Healthtech Marketplace</h3><h3>&nbsp;-&nbsp;</h3><h3>Growth Marketing Lead</h3></p></div><div><p>20 - 40 hrs</p><p>&nbsp;|&nbsp;</p><p>$125 - $150 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (SF, LA, MIA, NYC only)</p><div><p>Marketing</p><p>Syndicated</p><p>August 8, 2025</p><p>growth-marketing-lead-at-a-consumer-healthtech-marketplace</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>A Consumer Healthtech Marketplace</h3><h3>&nbsp;-&nbsp;</h3><h3>Chief Technology Officer</h3></p></div><div><p>20 - 40 hrs</p><p>&nbsp;|&nbsp;</p><p>$175 - $200 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (SF, LA, MIA, NYC only)</p><div><p>Engineering</p><p>Syndicated</p><p>August 8, 2025</p><p>chief-technology-officer-at-a-consumer-healthtech-marketplace</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Nicklpass</h3><h3>&nbsp;-&nbsp;</h3><h3>Founding UX Designer </h3></p></div><div><p>5 - 10 hrs</p><p>&nbsp;|&nbsp;</p><p>$75k ‚Äì $135k yearly equiv.</p><p>&nbsp;|&nbsp;</p><p>Remote (Worldwide)</p><div><p>Design</p><p>Syndicated</p><p>August 4, 2025</p><p>founding-ux-designer-at-nicklpass</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Morreale</h3><h3>&nbsp;-&nbsp;</h3><h3>Chief Financial Officer</h3></p></div><div><p>16 - 24 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Hybrid (Chicago)</p><div><p>Finance</p><p>Syndicated</p><p>August 4, 2025</p><p>chief-financial-officer-at-morreale</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Neuranics</h3><h3>&nbsp;-&nbsp;</h3><h3>Chief Financial Officer </h3></p></div><div><p>8 - 16 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Onsite (Glasgow, UK)</p><div><p>Finance</p><p>Syndicated</p><p>August 4, 2025</p><p>chief-financial-officer-at-neuranics</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Gitcoin</h3><h3>&nbsp;-&nbsp;</h3><h3>Operations Leader</h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>$4K - $6.5K / mo.</p><p>&nbsp;|&nbsp;</p><p>Remote (Worldwide)</p><div><p>Operations</p><p>Syndicated</p><p>August 4, 2025</p><p>operations-leader-at-gitcoin</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Novapulse</h3><h3>&nbsp;-&nbsp;</h3><h3>Chief Technology Officer</h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote </p><div><p>Engineering</p><p>Syndicated</p><p>August 4, 2025</p><p>chief-technology-officer-at-novapulse</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Rec</h3><h3>&nbsp;-&nbsp;</h3><h3>Field Marketing Lead</h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Hybrid (SF)</p><div><p>Marketing</p><p>Syndicated</p><p>August 4, 2025</p><p>field-marketing-lead-at-rec</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Digital United</h3><h3>&nbsp;-&nbsp;</h3><h3>Financial Consultant</h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote</p><div><p>Finance</p><p>Syndicated</p><p>August 4, 2025</p><p>financial-consultant-at-digital-united</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Phoenix3</h3><h3>&nbsp;-&nbsp;</h3><h3>General Counsel </h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Onsite (Waltham, MA)</p><div><p>Legal</p><p>Syndicated</p><p>August 4, 2025</p><p>general-counsel-at-phoenix3</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Sincere</h3><h3>&nbsp;-&nbsp;</h3><h3>PR Manager </h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Hybrid (Boston)</p><div><p>Marketing</p><p>Syndicated</p><p>August 4, 2025</p><p>pr-manager-at-sincere</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>White Snake Projects</h3><h3>&nbsp;-&nbsp;</h3><h3>Major Gift Officer </h3></p></div><div><p>12 hrs</p><p>&nbsp;|&nbsp;</p><p>$3000 / mo.</p><p>&nbsp;|&nbsp;</p><p>Onsite (Boston)</p><div><p>Other</p><p>Syndicated</p><p>August 4, 2025</p><p>major-gift-officer-at-white-snake-projects</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>A Home Maintenance Concierge Startup</h3><h3>&nbsp;-&nbsp;</h3><h3>Director of Marketing</h3></p></div><div><p>10 - 15 hrs</p><p>&nbsp;|&nbsp;</p><p>$150 - $175 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (USA / Canada only)</p><div><p>Marketing</p><p>Syndicated</p><p>August 1, 2025</p><p>director-of-marketing-at-a-home-maintenance-concierge-startup</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Totally Flawless</h3><h3>&nbsp;-&nbsp;</h3><h3>Lead Mobile Engineer</h3></p></div><div><p>15 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>$80 - $100 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (USA only)</p><div><p>Engineering</p><p>Syndicated</p><p>July 28, 2025</p><p>lead-mobile-engineer-at-totally-flawless</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Marshall Medical Group</h3><h3>&nbsp;-&nbsp;</h3><h3>Chief Marketing Officer</h3></p></div><div><p>20 hrs</p><p>&nbsp;|&nbsp;</p><p>$4K - $6K / mo</p><p>&nbsp;|&nbsp;</p><p>Hybrid (Lexington, KY)</p><div><p>Marketing</p><p>Syndicated</p><p>July 28, 2025</p><p>chief-marketing-officer-at-marshall-medical-group</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Flashii </h3><h3>&nbsp;-&nbsp;</h3><h3>VP of Sales </h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote</p><div><p>Sales</p><p>Syndicated</p><p>July 28, 2025</p><p>vice-president-of-sales-at-flashii--fg829</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Nexture Bio</h3><h3>&nbsp;-&nbsp;</h3><h3>Sales Lead </h3></p></div><div><p>8 - 15 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Hybrid (Sacramento)</p><div><p>Sales</p><p>Syndicated</p><p>July 28, 2025</p><p>sales-lead-at-nexture-bio</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>A NYC Hospitality Group</h3><h3>&nbsp;-&nbsp;</h3><h3>Head of People Operations</h3></p></div><div><p>5 - 10 hrs</p><p>&nbsp;|&nbsp;</p><p>$125 - $175 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (USA only)</p><div><p>People</p><p>Syndicated</p><p>July 28, 2025</p><p>head-of-people-operations-at-a-nyc-hospitality-group</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>SE10 PR</h3><h3>&nbsp;-&nbsp;</h3><h3>New Business Development </h3></p></div><div><p>10 - 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote</p><div><p>Sales</p><p>Syndicated</p><p>July 28, 2025</p><p>new-business-development-at-se10-pr</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>Mod Ventures LLC</h3><h3>&nbsp;-&nbsp;</h3><h3>Chief Operations &amp; Financial Officer </h3></p></div><div><p> 10 ‚Äì 20 hrs</p><p>&nbsp;|&nbsp;</p><p>Unknown</p><p>&nbsp;|&nbsp;</p><p>Remote</p><div><p>Operations</p><p>Syndicated</p><p>July 28, 2025</p><p>chief-operations-financial-officer-at-mod-ventures-llc</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>An HR-tech Analytics Platform</h3><h3>&nbsp;-&nbsp;</h3><h3>Staff Frontend Engineer</h3></p></div><div><p>20 - 40 hrs</p><p>&nbsp;|&nbsp;</p><p>$120 - $180 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (USA / Canada only)</p><div><p>Engineering</p><p>Syndicated</p><p>May 27, 2025</p><p>staff-frontend-engineer-at-an-hr-tech-analytics-platform</p></div></div></div></div><div role="listitem"><div tabindex="0" role="button" aria-roledescription="open-modal-trigger" aria-controls="fs-modal-1-popup" aria-haspopup="dialog" aria-expanded="false" data-w-id="9e108133-0d83-1535-4f7b-7f859e2881ab"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/661a6911701485a8909acb22_star.svg" loading="lazy" alt=""></p></div><div><div><p><h3>A Boutique Fractional CFO Firm</h3><h3>&nbsp;-&nbsp;</h3><h3>FP&amp;A Lead</h3></p></div><div><p>10 - 15 hrs</p><p>&nbsp;|&nbsp;</p><p>$80 - $120 / hr</p><p>&nbsp;|&nbsp;</p><p>Remote (Worldwide)</p><div><p>Finance</p><p>Syndicated</p><p>April 23, 2025</p><p>fp-a-lead-at-a-boutique-fractional-cfo-firm</p></div></div></div></div></div><div><div id="w-node-_9e108133-0d83-1535-4f7b-7f859e2881d8-075f0119"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/65b262a0934f65c19a253d80_email%201%20(2)%20(1).webp" loading="lazy" alt=""></p></div><div id="source-fractional-section"><div><p><h3>No <span id="job-placeholder">jobs</span>... yet. Get emailed when the next one goes live.</h3></p><p>This is a spam-free zone.</p></div><div><div id="success-message-2"><div><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/65ad278adba71fccff520329_Frame%2063.svg" loading="lazy" alt=""></p><p>You‚Äôre in! Check your inbox to confirm.</p></div><div><p>We also post job alerts on</p></div></div><div id="error-message-2"><p>Hhmm, try again. That didn‚Äôt work.</p></div></div></div></div><div><div id="w-node-_9e108133-0d83-1535-4f7b-7f859e288228-075f0119"><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/65b262a0934f65c19a253d80_email%201%20(2)%20(1).webp" loading="lazy" alt=""></p></div><div id="source-fractional-section"><div><p><h3>Fractional Job Alerts&nbsp; </h3><h3>In Your Inbox</h3><h3> Weekly.</h3></p><p>This is a spam-free zone.</p></div><div><div id="success-message-2"><div><p><img src="https://cdn.prod.website-files.com/6578c49d9871fac26dd7700e/65ad278adba71fccff520329_Frame%2063.svg" loading="lazy" alt=""></p><p>You‚Äôre in! Check your inbox to confirm.</p></div><div><p>We also post job alerts on</p></div></div><div id="error-message-2"><p>Hhmm, try again. That didn‚Äôt work.</p></div></div></div></div></div></div></div><div><div><h2>Playbooks</h2></div><div id="w-node-_9e108133-0d83-1535-4f7b-7f859e2882c3-075f0119"><h3>Want to Read More?</h3><div><p>Everything you need to go from zero to fractional operator, quickly.</p></div></div></div><div><div><h2>And More...</h2></div><div><div id="w-node-e444a3f1-5c67-c955-450c-0b68c7dfb507-c7dfb4fd"><h3>The Toolkit</h3><div><p>The tools and communities we recommend to help you build a successful fractional practice.</p></div></div><div id="w-node-e444a3f1-5c67-c955-450c-0b68c7dfb512-c7dfb4fd"><h3>The Blog</h3><div><p>Our latest thoughts and news related to the fractional world. Featuring select community members, too!</p></div></div></div></div><div id="contact"><p><h2>Contact Us</h2></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A minimal tensor processing unit (TPU), inspired by Google's TPU (215 pts)]]></title>
            <link>https://github.com/tiny-tpu-v2/tiny-tpu</link>
            <guid>44945008</guid>
            <pubDate>Mon, 18 Aug 2025 20:34:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tiny-tpu-v2/tiny-tpu">https://github.com/tiny-tpu-v2/tiny-tpu</a>, See on <a href="https://news.ycombinator.com/item?id=44945008">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">tiny-tpu</h2><a id="user-content-tiny-tpu" aria-label="Permalink: tiny-tpu" href="#tiny-tpu"></a></p>
<p dir="auto">A minimal tensor processing unit (TPU), reinvented from Google's TPU V2 and V1.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description tinytpu.mp4">tinytpu.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/110429254/479205816-b5d6aefe-4250-4c6d-866e-65d519e4de74.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTU1NzgxMDIsIm5iZiI6MTc1NTU3NzgwMiwicGF0aCI6Ii8xMTA0MjkyNTQvNDc5MjA1ODE2LWI1ZDZhZWZlLTQyNTAtNGM2ZC04NjZlLTY1ZDUxOWU0ZGU3NC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwODE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDgxOVQwNDMwMDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xOTgxMjEzMmMzYjgwNzI1NzhkNzYyMjM2NjFhMmU5Njg4NDIzOTkzMzBmNTZiYzI3M2VlZDRhOWI0NWJiNzBlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.G-NWijGwV-znD7kaoLTWLXXtUFX6CElnqDWxOM3gfsk" data-canonical-src="https://private-user-images.githubusercontent.com/110429254/479205816-b5d6aefe-4250-4c6d-866e-65d519e4de74.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTU1NzgxMDIsIm5iZiI6MTc1NTU3NzgwMiwicGF0aCI6Ii8xMTA0MjkyNTQvNDc5MjA1ODE2LWI1ZDZhZWZlLTQyNTAtNGM2ZC04NjZlLTY1ZDUxOWU0ZGU3NC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwODE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDgxOVQwNDMwMDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xOTgxMjEzMmMzYjgwNzI1NzhkNzYyMjM2NjFhMmU5Njg4NDIzOTkzMzBmNTZiYzI3M2VlZDRhOWI0NWJiNzBlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.G-NWijGwV-znD7kaoLTWLXXtUFX6CElnqDWxOM3gfsk" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#motivation">Motivation</a></li>
<li><a href="#architecture">Architecture</a>
<ul dir="auto">
<li><a href="#processing-element-pe">Processing Element (PE)</a></li>
<li><a href="#systolic-array">Systolic Array</a></li>
<li><a href="#vector-processing-unit-vpu">Vector Processing Unit (VPU)</a></li>
<li><a href="#unified-buffer-ub">Unified Buffer (UB)</a></li>
<li><a href="#control-unit">Control Unit</a></li>
</ul>
</li>
<li><a href="#instruction-set">Instruction Set</a></li>
<li><a href="#example-instruction-sequence">Example Instruction Sequence</a></li>
<li><a href="#future-steps">Future Steps</a></li>
<li><a href="#setup">Setup</a>
<ul dir="auto">
<li><a href="#macos-specific">MacOS specific</a></li>
<li><a href="#ubuntu-specific">Ubuntu specific</a></li>
</ul>
</li>
<li><a href="#adding-a-new-module-to-the-tiny-tpu">Adding a new module to the tiny-tpu</a>
<ul dir="auto">
<li><a href="#1-create-the-module-file">1. Create the module file</a></li>
<li><a href="#2-create-the-dump-file">2. Create the dump file</a></li>
<li><a href="#3-create-the-test-file">3. Create the test file</a></li>
<li><a href="#4-update-the-makefile">4. Update the Makefile</a></li>
<li><a href="#5-view-waveforms">5. View waveforms</a></li>
</ul>
</li>
<li><a href="#running-commands-from-makefile">Running commands from Makefile</a></li>
<li><a href="#fixed-point-viewing-in-gtkwave">Fixed point viewing in gtkwave</a></li>
<li><a href="#what-is-a-gtkw-file">What is a gtkw file?</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tiny-tpu-v2/tiny-tpu/blob/main/images/tpu.png"><img src="https://github.com/tiny-tpu-v2/tiny-tpu/raw/main/images/tpu.png" alt="TPU Architecture"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Processing Element (PE)</h3><a id="user-content-processing-element-pe" aria-label="Permalink: Processing Element (PE)" href="#processing-element-pe"></a></p>
<ul dir="auto">
<li><strong>Function</strong>: Performs a multiply-accumulate operation every clock cycle</li>
<li><strong>Data Flow</strong>:
<ul dir="auto">
<li>Incoming data is multiplied by a stored weight and added to an incoming partial sum to produce an output sum</li>
<li>Incoming data also passes through to the next element for propagation across the array</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Systolic Array</h3><a id="user-content-systolic-array" aria-label="Permalink: Systolic Array" href="#systolic-array"></a></p>
<ul dir="auto">
<li><strong>Architecture</strong>: A grid of processing elements, starting from 2x2</li>
<li><strong>Data Movement</strong>:
<ul dir="auto">
<li>Input values flow horizontally across the array</li>
<li>Partial sums flow vertically down the array</li>
<li>Weights remain fixed within each processing element during computation</li>
</ul>
</li>
<li><strong>Input Preprocessing</strong>:
<ul dir="auto">
<li>Input matrices are rotated 90 degrees (implemented in hardware)</li>
<li>Inputs are staggered for correct computation in the systolic array</li>
<li>Weight matrices are transposed and staggered to align with mathematical formulas</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Vector Processing Unit (VPU)</h3><a id="user-content-vector-processing-unit-vpu" aria-label="Permalink: Vector Processing Unit (VPU)" href="#vector-processing-unit-vpu"></a></p>
<ul dir="auto">
<li>Performs element-wise operations after the systolic array</li>
<li><strong>Control</strong>: Module selection depends on the computation stage</li>
<li><strong>Modules (pipelined)</strong>:
<ol dir="auto">
<li>Bias addition</li>
<li>Leaky ReLU activation function</li>
<li>MSE loss</li>
<li>Leaky ReLU derivative</li>
</ol>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Unified Buffer (UB)</h3><a id="user-content-unified-buffer-ub" aria-label="Permalink: Unified Buffer (UB)" href="#unified-buffer-ub"></a></p>
<ul dir="auto">
<li>Dual-port memory for storing intermediate values</li>
<li><strong>Stored Data</strong>:
<ul dir="auto">
<li>Input matrices</li>
<li>Weight matrices</li>
<li>Bias vectors</li>
<li>Post-activation values for backpropagation</li>
<li>Activation leak factors</li>
<li>Inverse batch size constant for MSE backpropagation</li>
</ul>
</li>
<li><strong>Interface</strong>:
<ul dir="auto">
<li>Two read and two write ports per data type</li>
<li>Data is accessed by specifying a start address and count</li>
<li>Reads can occur continuously in the background until the requested count is reached</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Control Unit</h3><a id="user-content-control-unit" aria-label="Permalink: Control Unit" href="#control-unit"></a></p>
<ul dir="auto">
<li><strong>Instruction width</strong>: 94 bits</li>
<li>See <a href="#instruction-set">Instruction Set</a> section below for more information.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Instruction Set</h2><a id="user-content-instruction-set" aria-label="Permalink: Instruction Set" href="#instruction-set"></a></p>
<p dir="auto">Our ISA is 94 bits wide. The full image is available in the <code>images/</code> folder.</p>
<p dir="auto">Our ISA defines all necessary signals for transferring data and interacting with our TPU. The implementation of the control unit (which reads instructions) can be found at <code>src/control_unit.sv</code>.</p>
<p dir="auto">The <code>instruction</code> bus is <strong>94 bits wide</strong> (<code>[93:0]</code>) and is divided into fields that directly control subsystems.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bits [0‚Äì4]: 1-bit Control Signals</h3><a id="user-content-bits-04-1-bit-control-signals" aria-label="Permalink: Bits [0‚Äì4]: 1-bit Control Signals" href="#bits-04-1-bit-control-signals"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Bit</th>
<th>Signal</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td><code>sys_switch_in</code></td>
<td>System mode switch (general-purpose "on/off" CU)</td>
<td><code>1 = system active</code>, <code>0 = idle</code></td>
</tr>
<tr>
<td>1</td>
<td><code>ub_rd_start_in</code></td>
<td>Start UB (Unified Buffer) read transaction</td>
<td><code>1 = trigger read</code>, <code>0 = no read</code></td>
</tr>
<tr>
<td>2</td>
<td><code>ub_rd_transpose</code></td>
<td>UB read transpose mode</td>
<td><code>1 = transpose</code>, <code>0 = normal</code></td>
</tr>
<tr>
<td>3</td>
<td><code>ub_wr_host_valid_in_1</code></td>
<td>Host write channel 1 valid flag</td>
<td><code>1 = write valid</code>, <code>0 = not valid</code></td>
</tr>
<tr>
<td>4</td>
<td><code>ub_wr_host_valid_in_2</code></td>
<td>Host write channel 2 valid flag</td>
<td><code>1 = write valid</code>, <code>0 = not valid</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bits [6:5]: UB Read Column Size (2-bit)</h3><a id="user-content-bits-65-ub-read-column-size-2-bit" aria-label="Permalink: Bits [6:5]: UB Read Column Size (2-bit)" href="#bits-65-ub-read-column-size-2-bit"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Field</th>
<th>Signal</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>[6:5]</td>
<td><code>ub_rd_col_size</code></td>
<td>Number of columns to read</td>
<td><code>00=0</code>, <code>01=1</code>, <code>10=2</code>, <code>11=3</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bits [14:7]: UB Read Row Size (8-bit)</h3><a id="user-content-bits-147-ub-read-row-size-8-bit" aria-label="Permalink: Bits [14:7]: UB Read Row Size (8-bit)" href="#bits-147-ub-read-row-size-8-bit"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Field</th>
<th>Signal</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>[14:7]</td>
<td><code>ub_rd_row_size</code></td>
<td>Number of rows to read (0‚Äì255)</td>
<td><code>0x08 = read 8 rows</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bits [22:15]: UB Read Address (8-bit)</h3><a id="user-content-bits-2215-ub-read-address-8-bit" aria-label="Permalink: Bits [22:15]: UB Read Address (8-bit)" href="#bits-2215-ub-read-address-8-bit"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Field</th>
<th>Signal</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>[22:15]</td>
<td><code>ub_rd_addr_in</code></td>
<td>UB read address (0‚Äì255)</td>
<td><code>0x10 = read bank 16</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bits [25:23]: UB Pointer Select (3-bit)</h3><a id="user-content-bits-2523-ub-pointer-select-3-bit" aria-label="Permalink: Bits [25:23]: UB Pointer Select (3-bit)" href="#bits-2523-ub-pointer-select-3-bit"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Field</th>
<th>Signal</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>[25:23]</td>
<td><code>ub_ptr_sel</code></td>
<td>Selects UB pointer</td>
<td><code>3‚Äôb001 = route read ptr to bias module in VPU</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bits [41:26]: UB Write Host Data In 1 (16-bit, Fixed-Point)</h3><a id="user-content-bits-4126-ub-write-host-data-in-1-16-bit-fixed-point" aria-label="Permalink: Bits [41:26]: UB Write Host Data In 1 (16-bit, Fixed-Point)" href="#bits-4126-ub-write-host-data-in-1-16-bit-fixed-point"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Field</th>
<th>Signal</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>[41:26]</td>
<td><code>ub_wr_host_data_in_1</code></td>
<td>First host write word</td>
<td><code>0xABCD</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bits [57:42]: UB Write Host Data In 2 (16-bit, Fixed-Point)</h3><a id="user-content-bits-5742-ub-write-host-data-in-2-16-bit-fixed-point" aria-label="Permalink: Bits [57:42]: UB Write Host Data In 2 (16-bit, Fixed-Point)" href="#bits-5742-ub-write-host-data-in-2-16-bit-fixed-point"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Field</th>
<th>Signal</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>[57:42]</td>
<td><code>ub_wr_host_data_in_2</code></td>
<td>Second host write word</td>
<td><code>0x1234</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bits [61:58]: VPU Data Pathway (4-bit)</h3><a id="user-content-bits-6158-vpu-data-pathway-4-bit" aria-label="Permalink: Bits [61:58]: VPU Data Pathway (4-bit)" href="#bits-6158-vpu-data-pathway-4-bit"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Field</th>
<th>Signal</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>[61:58]</td>
<td><code>vpu_data_pathway</code></td>
<td>Routing of data in VPU</td>
<td><code>0001=bias + relu routing</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bits [77:62]: Inverse Batch Size √ó 2 (16-bit, Fixed-Point)</h3><a id="user-content-bits-7762-inverse-batch-size--2-16-bit-fixed-point" aria-label="Permalink: Bits [77:62]: Inverse Batch Size √ó 2 (16-bit, Fixed-Point)" href="#bits-7762-inverse-batch-size--2-16-bit-fixed-point"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Field</th>
<th>Signal</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>[77:62]</td>
<td><code>inv_batch_size_times_two_in</code></td>
<td>Precomputed scaling factor (2/batch)</td>
<td><code>0x0010 = (2/32)</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bits [93:78]: VPU Leak Factor (16-bit, Fixed-Point)</h3><a id="user-content-bits-9378-vpu-leak-factor-16-bit-fixed-point" aria-label="Permalink: Bits [93:78]: VPU Leak Factor (16-bit, Fixed-Point)" href="#bits-9378-vpu-leak-factor-16-bit-fixed-point"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Field</th>
<th>Signal</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>[93:78]</td>
<td><code>vpu_leak_factor_in</code></td>
<td>Leak factor for activation (e.g., Leaky ReLU)</td>
<td><code>0x00A0 = 0.625</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example Instruction Sequence</h2><a id="user-content-example-instruction-sequence" aria-label="Permalink: Example Instruction Sequence" href="#example-instruction-sequence"></a></p>
<p dir="auto">Instructions are directly loaded into an instruction buffer on the chip from a testbench file.</p>
<ul dir="auto">
<li>See <code>tests/tpu.v</code> for our forward and backward pass instruction sequence</li>
<li>See the <a href="#setup">Setup</a> section on how to run this testbench</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Future Steps</h2><a id="user-content-future-steps" aria-label="Permalink: Future Steps" href="#future-steps"></a></p>
<ol dir="auto">
<li>Compiler for this instruction set</li>
<li>Scaling TPU to larger dimensions (256√ó256 or 512√ó512)</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto">We are open source and appreciate any contributions! Here is our workflow and steps to set up our development environment:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">MacOS Specific</h3><a id="user-content-macos-specific" aria-label="Permalink: MacOS Specific" href="#macos-specific"></a></p>
<ol dir="auto">
<li>Create a virtual environment and run:

</li>
<li>Install iverilog using Homebrew:

</li>
<li>Build gtkwave <strong>FROM SOURCE</strong> (important: other installation methods currently do not work)</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Ubuntu/Linux Specific</h3><a id="user-content-ubuntulinux-specific" aria-label="Permalink: Ubuntu/Linux Specific" href="#ubuntulinux-specific"></a></p>
<ol dir="auto">
<li>Create a virtual environment and run:

</li>
<li>Install gtkwave:

</li>
<li>Install iverilog:
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install iverilog"><pre>sudo apt install iverilog</pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Adding Modules</h2><a id="user-content-adding-modules" aria-label="Permalink: Adding Modules" href="#adding-modules"></a></p>
<p dir="auto">Follow these steps to add a new module to the project:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">1. Create the Module File</h3><a id="user-content-1-create-the-module-file" aria-label="Permalink: 1. Create the Module File" href="#1-create-the-module-file"></a></p>
<p dir="auto">Add your new module file <code>&lt;MODULE_NAME&gt;.sv</code> in the <code>src/</code> directory.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">2. Create the Dump File</h3><a id="user-content-2-create-the-dump-file" aria-label="Permalink: 2. Create the Dump File" href="#2-create-the-dump-file"></a></p>
<p dir="auto">Create <code>dump_&lt;MODULE_NAME&gt;.sv</code> in the <code>test/</code> directory with the following code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="module dump();
initial begin
  $dumpfile(&quot;waveforms/<MODULE_NAME>.vcd&quot;);
  $dumpvars(0, <MODULE_NAME>); 
end
endmodule"><pre><span>module</span> <span>dump</span>();
<span>initial</span> <span>begin</span>
  <span>$dumpfile</span>(<span><span>"</span>waveforms/&lt;MODULE_NAME&gt;.vcd<span>"</span></span>);
  <span>$dumpvars</span>(<span>0</span>, <span>&lt;</span><span>MODULE_NAME</span><span>&gt;</span>); 
<span>end</span>
<span>endmodule</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">3. Creating Tests</h3><a id="user-content-3-creating-tests" aria-label="Permalink: 3. Creating Tests" href="#3-creating-tests"></a></p>
<p dir="auto">Create <code>test_&lt;MODULE_NAME&gt;.py</code> in the <code>test/</code> directory.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">4. Makefile Updates</h3><a id="user-content-4-makefile-updates" aria-label="Permalink: 4. Makefile Updates" href="#4-makefile-updates"></a></p>
<p dir="auto">Add your module to the <code>SOURCES</code> variable and create a test target:</p>
<div dir="auto" data-snippet-clipboard-copy-content="test_<MODULE_NAME>: $(SIM_BUILD_DIR)
	$(IVERILOG) -o $(SIM_VVP) -s <MODULE_NAME> -s dump -g2012 $(SOURCES) test/dump_<MODULE_NAME>.sv
	PYTHONOPTIMIZE=$(NOASSERT) MODULE=test_<MODULE_NAME> $(VVP) -M $(COCOTB_LIBS) -m libcocotbvpi_icarus $(SIM_VVP)
	! grep failure results.xml
	mv <MODULE_NAME>.vcd waveforms/ 2>/dev/null || true"><pre><span>test_&lt;MODULE_NAME&gt;</span>: <span>$(<span>SIM_BUILD_DIR</span>)</span>
	<span>$(<span>IVERILOG</span>)</span> -o <span>$(<span>SIM_VVP</span>)</span> -s <span>&lt;</span>MODULE_NAME<span>&gt;</span> -s dump -g2012 <span>$(<span>SOURCES</span>)</span> test/dump_<span>&lt;</span>MODULE_NAME<span>&gt;</span>.sv
	PYTHONOPTIMIZE=<span>$(<span>NOASSERT</span>)</span> MODULE=test_<span>&lt;</span>MODULE_NAME<span>&gt;</span> <span>$(<span>VVP</span>)</span> -M <span>$(<span>COCOTB_LIBS</span>)</span> -m libcocotbvpi_icarus <span>$(<span>SIM_VVP</span>)</span>
	<span>!</span> grep failure results.xml
	mv <span>&lt;</span>MODULE_NAME<span>&gt;</span>.vcd waveforms/ <span>2&gt;</span>/dev/null <span>||</span> <span>true</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">5. View Waveforms</h3><a id="user-content-5-view-waveforms" aria-label="Permalink: 5. View Waveforms" href="#5-view-waveforms"></a></p>
<p dir="auto">Run the following command to view the generated waveforms:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gtkwave waveforms/<MODULE_NAME>.vcd"><pre>gtkwave waveforms/<span>&lt;</span>MODULE_NAME<span>&gt;</span>.vcd</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Makefile Commands</h2><a id="user-content-makefile-commands" aria-label="Permalink: Makefile Commands" href="#makefile-commands"></a></p>
<p dir="auto">Run tests:</p>

<p dir="auto">View waveforms:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gtkwave waveforms/<MODULE_NAME>.vcd"><pre>gtkwave waveforms/<span>&lt;</span>MODULE_NAME<span>&gt;</span>.vcd</pre></div>
<p dir="auto">Or use the shorthand:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">GTKWwave Setup</h2><a id="user-content-gtkwwave-setup" aria-label="Permalink: GTKWwave Setup" href="#gtkwwave-setup"></a></p>
<ol dir="auto">
<li>Right-click all signals</li>
<li>Navigate to: <strong>Data Format</strong> ‚Üí <strong>Fixed Point Shift</strong> ‚Üí <strong>Specify</strong></li>
<li>Enter <code>8</code> and click <strong>OK</strong></li>
<li>Set: <strong>Data Format</strong> ‚Üí <strong>Signed Decimal</strong></li>
<li>Enable: <strong>Data Format</strong> ‚Üí <strong>Fixed Point Shift</strong> ‚Üí <strong>ON</strong></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is a .gtkw File?</h2><a id="user-content-what-is-a-gtkw-file" aria-label="Permalink: What is a .gtkw File?" href="#what-is-a-gtkw-file"></a></p>
<p dir="auto">A <code>.gtkw</code> file stores the signal configuration for <code>make show_&lt;MODULE_NAME&gt;</code>. You only need to save it once after running:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gtkwave waveforms/<MODULE_NAME>.vcd"><pre>gtkwave waveforms/<span>&lt;</span>MODULE_NAME<span>&gt;</span>.vcd</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Motivation</h2><a id="user-content-motivation" aria-label="Permalink: Motivation" href="#motivation"></a></p>
<p dir="auto">The details of TPU architecture are closed source, as is most of chip design. We want this resource to be the ultimate guide to breaking into building chip accelerators for all levels of technical expertise ‚Äî even if you just learned high school math and only know y = mx + b.</p>
<p dir="auto">Before this project, none of us had professional experience in hardware architecture/design. We started this ambitious project as a dedicated group wanting to break into hardware design. We've collectively gained significant design experience from this project.</p>
<p dir="auto">We hope that the inventive nature of the article at <a href="https://tinytpu.com/" rel="nofollow">tinytpu.com</a>, this README, and the code in this repository will help you walk through our steps and learn how to approach problems with an inventive mindset.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GenAI FOMO has spurred businesses to light nearly $40B on fire (223 pts)]]></title>
            <link>https://www.theregister.com/2025/08/18/generative_ai_zero_return_95_percent/</link>
            <guid>44944620</guid>
            <pubDate>Mon, 18 Aug 2025 19:54:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/08/18/generative_ai_zero_return_95_percent/">https://www.theregister.com/2025/08/18/generative_ai_zero_return_95_percent/</a>, See on <a href="https://news.ycombinator.com/item?id=44944620">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>US companies have invested between $35 and $40 billion in Generative AI initiatives and, so far, have almost nothing to show for it.</p>
<p>According to <a href="https://docs.google.com/forms/d/e/1FAIpQLSc8rU8OpQWU44gYDeZyINUZjBFwu--1uTbxixK_PRSVrfaH8Q/viewform" rel="nofollow">a report</a> [PDF] from MIT's NANDA (Networked Agents and Decentralized AI) initiative, 95 percent of enterprise organizations have gotten zero return from their AI efforts.</p>
<p>Only 5 percent of organizations have successfully integrated AI tools into production at scale.</p>

    

<p>The report is based on 52 structured interviews with enterprise leaders and on analysis of more than 300 public AI initiatives and announcements, and a survey of 153 business professionals.</p>

        


        

<p>The report authors ‚Äì Aditya Challapally, Chris Pease, Ramesh Raskar, and Pradyumna Chari ‚Äì attribute this GenAI Divide not to insufficient infrastructure, learning, or talent, but to the inability of AI systems to retain data, to adapt, and to learn over time.</p>
<blockquote>

<p>The GenAI Divide is starkest in deployment rates, only 5 percent of custom enterprise AI tools reach production</p>
</blockquote>
<p>"The GenAI Divide is starkest in deployment rates, only 5 percent of custom enterprise AI tools reach production," the report says. "Chatbots succeed because they're easy to try and flexible, but fail in critical workflows due to lack of memory and customization."</p>
<p>As an unidentified CIO put it in an interview with the authors, "We've seen dozens of demos this year. Maybe one or two are genuinely useful. The rest are wrappers or science projects."</p>
<p>The authors' findings echo <a href="https://www.theregister.com/2025/07/09/csuite_sours_on_ai/">other recent research</a> showing a decline in confidence about AI initiatives among corporate leaders.</p>

        

<p>The NANDA report does say that a small percentage of companies have found GenAI helpful and that the technology is having a material impact on two out of nine industrial sectors ‚Äì Technology and Media &amp; Telecom.&nbsp;</p>
<p>For the remaining sectors ‚Äì&nbsp;Professional Services, Healthcare &amp; Pharma, Consumer &amp; Retail, Financial Services, Advanced Industries, and Energy &amp; Materials ‚Äì Generative AI has been inconsequential.</p>
<p>An unidentified COO at a mid-market manufacturing firm is quoted as saying, "The hype on LinkedIn says everything has changed, but in our operations, nothing fundamental has shifted. We're processing some contracts faster, but that's all that has changed."</p>

        

<p>One thing that is changing is the employment landscape, at least in affected industries. In the Technology and Media sectors, the report notes, "[more than] 80 percent of executives anticipate reduced hiring volumes within 24 months."</p>
<p>According to the authors, the GenAI-driven workforce reductions have been occurring in non-core business activities that often get outsourced, such as customer support operations, administrative processing, and standardized development tasks.&nbsp;</p>
<p>"These roles exhibited vulnerability prior to AI implementation due to their outsourced status and process standardization," the report says, suggesting that, in the affected sectors, between five and 20 percent of support and admin processing has been impacted.&nbsp;</p>
<ul>

<li><a href="https://www.theregister.com/2025/08/18/aws_updated_kiro_pricing/">AWS pricing for Kiro dev tool dubbed 'a wallet-wrecking tragedy'</a></li>

<li><a href="https://www.theregister.com/2025/08/18/ai_form_fillers/">UK drafts AI to help Joe Public decipher its own baffling bureaucracy</a></li>

<li><a href="https://www.theregister.com/2025/08/18/opinion_column_gen_ai/">Generative AI isn't just a matter of life and death. It's far more important than that</a></li>

<li><a href="https://www.theregister.com/2025/08/17/nabiha_syed_remakes_mozilla_foundation/">Nabiha Syed remakes Mozilla Foundation in the era of Trump and AI</a></li>
</ul>
<p><em>The Register</em> has been told that Oracle's <a href="https://www.theregister.com/2025/08/15/oracle_cuts_300_in_california/">recent layoffs</a> reflect efforts to balance AI capital expenditures, <a href="https://www.theregister.com/2025/07/31/amazon_earnings_q2_2025/">an albatross around the necks of US tech giants</a>. At IBM, staffers have argued that <a href="https://www.theregister.com/2024/09/24/ibm_layoffs_ai_talent/">AI has been used as an excuse to offshore jobs</a>.</p>
<p>Whatever the stated rationale and actual motive for job cuts may be, Generative AI is having an impact on the Tech and Media &amp; Telecom sectors, where it has seen the broadest adoption.</p>
<p>While about 50 percent of AI budgets get allocated to marketing and sales, the report authors suggest that corporate investment instead should flow toward activities generating meaningful business results. This includes lead qualification and customer retention on the front end and, in the elimination of business process outsourcing, ad agency spending, and financial service risk checking on the back end.&nbsp;</p>
<p>Looking at the way Generative AI has been successful for certain companies, the report argues that generic tools like OpenAI's ChatGPT do better than bespoke enterprise tools, even when those enterprise tools use the same AI models under the hood.</p>
<p>The stated reason is that workers tend to be more familiar with ChatGPT's interface and thus use it more ‚Äì a consequence of employee-driven shadow IT. The report cites an interview with a corporate lawyer who described her mid-size firm's dissatisfaction with a specialized contract analysis tool that cost $50,000.</p>
<p>"Our purchased AI tool provided rigid summaries with limited customization options," the attorney told the researchers. "With ChatGPT, I can guide the conversation and iterate until I get exactly what I need. The fundamental quality difference is noticeable, ChatGPT consistently produces better outputs, even though our vendor claims to use the same underlying technology."</p>
<p>Companies that bridge the GenAI divide approach AI procurement as business process outsourcing customers rather than as software-as-a-service clients, the authors argue.</p>
<p>"They demand deep customization, drive adoption from the front lines, and hold vendors accountable to business metrics," the report concludes. "The most successful buyers understand that crossing the divide requires partnership, not just purchase." ¬Æ</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built a toy TPU that can do inference and training on the XOR problem (114 pts)]]></title>
            <link>https://www.tinytpu.com</link>
            <guid>44944592</guid>
            <pubDate>Mon, 18 Aug 2025 19:52:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tinytpu.com">https://www.tinytpu.com</a>, See on <a href="https://news.ycombinator.com/item?id=44944592">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Nobody really understands how TPUs work‚Ä¶and neither do we! So we wanted to make this because we wanted to take a shot and try to guess how it works‚Äìfrom the perspective of complete novices!</p><p>We wanted to do something very challenging to prove to ourselves that we can do anything we put our mind to. The reasoning for why we chose to build a TPU specifically is fairly simple:</p><p>None of us have real professional experience in hardware design, which, in a way, made the TPU even more appealing since we weren't able to estimate exactly how difficult it would be. As we worked on the initial stages of this project, we established a strict design philosophy: ALWAYS TRY THE HACKY WAY. This meant trying out the "dumb" ideas that came to our mind first BEFORE consulting external sources. This philosophy helped us make sure we weren't reverse engineering the TPU, but rather <b>re-inventing it</b>, which helped us derive many of the key mechanisms used in the TPU ourselves.</p><p>We also wanted to treat this project as an exercise to code without relying on AI to write for us, since we felt that our initial instinct recently has been to reach for these AI tools whenever we faced a slight struggle. We wanted to cultivate a certain<!-- --> <b>style of thinking</b> that we could take forward with us and use in any future endeavours to think through difficult problems.<sup><a href="#fn1" id="fn1-ref">[1]</a></sup></p><div><p>A TPU is an application specific chip (ASIC) designed by Google to make inferencing (using) and training ML models faster and more efficient. Whereas a GPU can be used to render frames AND run ML workloads, a TPU can only perform math operations, allowing it to be better at what it's designed for. Naturally, trying to master a single task is much easier and will yield better results than trying to master multiple tasks and the TPU strongly employs this philosophy.</p><div><div><p><i>Quick primer on hardware design:</i></p><p>In hardware, the unit of time we're dealing with is called a clock cycle. This is an arbitrary period of time that we can set, as developers, to meet our requirements. Generally, a single clock cycle can range from 1 picosecond (ps) to 1 nanosecond (ns) and any operations we run will be executed BETWEEN clock cycles.</p></div><figure><p><img alt="Clock cycle diagram" loading="lazy" width="679" height="269" decoding="async" data-nimg="1" src="https://www.tinytpu.com/clock-cycle.svg"></p><figcaption>Clock cycle timing diagram showing how operations are synchronized in hardware</figcaption></figure><p>The language we use to describe hardware is called Verilog. It's a hardware description language that allows us to describe the behaviour of a given hardware module (similar to functions in software), but instead of executing as a program, it synthesizes into boolean logic gates (AND, OR, NOT, etc.) that can be combined to build the digital logic for any chip we want. Here's a simple example of an addition in Verilog:</p><br><pre><code>module add <span>(</span>
    <span>input</span> wire <span>clk</span>,
    <span>// reset signal to reset the module</span>
    <span>input</span> wire <span>rst</span>,

    <span>// registers to hold the <span>input</span> and <span>output</span> values</span>
    <span>input</span> reg a,
    <span>input</span> reg b,
    <span>output</span> reg c
  <span>)</span>;
    
    <span>always</span> @<span>(</span><span>posedge</span> <span>clk</span><span>)</span> <span>begin</span> 

    <span>// everything in this block will be executed every clock cycle</span>
    
      <span>if</span> <span>(</span><span>rst</span><span>)</span> <span>begin</span>
      <span>// reset the <span>output</span> to <span>0</span> when the reset signal is high</span>
        c <span>&lt;=</span> <span>0</span>; 
      <span>end</span> <span>else</span> <span>begin</span>
        <span>// add the two inputs and store the result in the <span>output</span></span>
        c <span>&lt;=</span> a <span>+</span> b; 
      <span>end</span>
    <span>end</span>

endmodule
</code></pre><p>In the example above, the value of the signal b at the next clock cycle is set to the current value of the signal a. You'll find that in most cases, signals (variables) are updated in sequential clock cycles, as opposed to immediate updates like you would find in software design.</p></div><p>Specifically, the TPU is very efficient at performing matrix multiplications, which make up 80-90% of the compute operations in transformers (up to 95% in very large models) and 70-80% in CNNs. Each matrix multiplication represents the calculation for a single layer in an MLP, and in deep learning, we have many of these layers, making TPUs increasingly efficient for larger models.</p></div><div><p>When we started this project, all we knew was that the equation y = mx + b is the foundational building block for neural networks. However, we needed to fully UNDERSTAND the math behind neural networks to build other modules in our TPU. So before we started writing any code, each of us worked out the math of a simple 2 -&gt; 2 -&gt; 1 multi-layer perceptron (MLP).</p><figure><p><img alt="XOR MLP Neural Network Architecture showing 2 input nodes, 2 hidden layer nodes, and 1 output node with weight connections" loading="lazy" width="679" height="269" decoding="async" data-nimg="1" src="https://www.tinytpu.com/xor-mlp.svg"></p><figcaption>Architecture of our 2‚Üí2‚Üí1 multi-layer perceptron for solving the XOR problem</figcaption></figure><br><h3>Why XOR?</h3><p>The reason we chose this specific network is because we were targeting inference and training for the XOR problem (the "hello world" of neural networks). The XOR problem is one of the simplest problems a neural network can solve. All other gates (AND, OR, etc) can predict the outputs from its inputs using just one linear line (one neuron) to separate which inputs correspond to a 0 and which ones correspond to a 1. But to classify all XOR, an MLP is needed, since it requires curved decision boundaries, which can't be achieved with ONLY linear equations. For a geometric and first-principles treatment, the free book<!-- --> <a href="https://udlbook.github.io/udlbook/" target="_blank" rel="noopener noreferrer">Understanding Deep Learning</a> <!-- -->is excellent.</p><figure><p><img alt="OR and XOR decision boundaries" loading="lazy" width="679" height="269" decoding="async" data-nimg="1" src="https://www.tinytpu.com/or-xor.svg"></p><figcaption>OR and XOR decision boundaries</figcaption></figure><h3>Batching and dimensions</h3><p>Now, say we want to do continuous inference (i.e. self driving car making multiple predictions a second). That would imply that we're sending multiple pieces of data at once. Since data is inherently multidimensional and has many features, we would have matrices with very large dimensions. However, the XOR problem simplifies the dimensions for us, as there are only two features (0 or 1) and 4 possible pieces of input data (four possible binary combinations of 0 and 1). This gives us a 4x2 matrix, where 4 is the number of rows (batch size) and 2 is the number of columns (feature size).</p><div><p>The XOR input matrix and target outputs:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">X</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>4</mn><mo>√ó</mo><mn>2</mn></mrow></msup><mo separator="true">,</mo><mspace width="1em"></mspace><mi mathvariant="bold">y</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>4</mn><mo>√ó</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">
                  \mathbf{X} =
                  \begin{bmatrix}
                  0 &amp; 0 \\[0.3em]
                  0 &amp; 1 \\[0.3em]
                  1 &amp; 0 \\[0.3em]
                  1 &amp; 1
                  \end{bmatrix} \in \mathbb{R}^{4 \times 2}, \quad
                  \mathbf{y} = \begin{bmatrix} 0 \\[0.3em] 1 \\[0.3em] 1 \\[0.3em] 0 \end{bmatrix} \in \mathbb{R}^{4 \times 1}
                </annotation></semantics></math></span></span></span></p></div><p>Each row represents one of the four possible XOR inputs, and the output vector shows the expected XOR results</p></div><p>Another simplification we're making for our systolic array example here is that we'll use a 2x2 instead of the 256x256 array used in the TPUv1. However, the math is still faithful so nothing is actually dumbed down, rather scaled down instead.</p><p>The first step in the equation is multiplying m with x, which, in matrix form, would be<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><msup><mi mathvariant="bold">W</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{X}\mathbf{W}^T</annotation></semantics></math></span></span></span>.</p><div><p>More formally:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">Z</mi><mo>=</mo><mi mathvariant="bold">X</mi><msup><mi mathvariant="bold">W</mi><mi>T</mi></msup><mo>+</mo><mi mathvariant="bold">b</mi></mrow><annotation encoding="application/x-tex">\mathbf{Z} = \mathbf{X}\mathbf{W}^T + \mathbf{b}</annotation></semantics></math></span></span></span></p></div><p>where<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>√ó</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{X} \in \mathbb{R}^{n \times d}</annotation></semantics></math></span></span></span> <!-- -->is our input matrix,<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">W</mi><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>m</mi><mo>√ó</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W} \in \mathbb{R}^{m \times d}</annotation></semantics></math></span></span></span> <!-- -->is our weight matrix, and<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">b</mi><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>1</mn><mo>√ó</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{b} \in \mathbb{R}^{1 \times m}</annotation></semantics></math></span></span></span> <!-- -->is our bias vector</p></div><p>How can we perform matrix multiplication in hardware? Well, we can use a unit called the systolic array!</p><h3>Systolic array and PEs</h3><p>The heart of a TPU is a unit called the systolic array.<sup><a href="#fn2" id="fn2-ref">[2]</a></sup> <!-- -->It consists of individual building blocks called Processing Elements (PE) which are connected together in a grid-like structure. Each PE performs a multiply-accumulate operation, meaning it multiplies an incoming input X with a stationary weight W<sup><a href="#fn3" id="fn3-ref">[3]</a></sup> <!-- -->and adds it to an incoming accumulated sum, all in the same clock cycle.</p><figure><p><img alt="PE diagram" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/PE.svg"></p><figcaption>Processing Element (PE) architecture showing multiply-accumulate operation (without load weight and start flags)</figcaption></figure><div><pre><code><span>always_ff</span> @<span>(</span><span>posedge</span> <span>clk</span> or <span>posedge</span> <span>rst</span><span>)</span> <span>begin</span>
        <span>if</span> <span>(</span><span>rst</span><span>)</span> <span>begin</span>
            <span>input_out</span> <span>&lt;=</span> <span>0</span>;
            <span>psum_out</span> <span>&lt;=</span> <span>0</span>;
            <span>weight_reg</span> <span>&lt;=</span> <span>0</span>;
        <span>end</span> <span>else</span> <span>if</span> <span>(</span><span>load_weight</span><span>)</span> <span>begin</span>
            <span>weight_reg</span> <span>&lt;=</span> <span>weight</span>;
        <span>end</span> <span>else</span> <span>if</span> <span>(</span><span>start</span><span>)</span> <span>begin</span>
            <span>input_out</span> <span>&lt;=</span> <span>input_in</span>;
            <span>// the main multiply-accumulate operation</span>
            <span>psum_out</span> <span>&lt;=</span> <span>(</span><span>input_in</span> <span>*</span> <span>weight_reg</span><span>)</span> <span>+</span> <span>psum_in</span>;
        <span>end</span>
    <span>end</span></code></pre></div><h3>Systolic matrix multiplication</h3><p>When these PEs are connected together, they can be used to perform matrix multiplication systolically, meaning multiple elements of the output matrix can be calculated every clock cycle. The inputs enter the systolic array from the left and move to the neighbouring PE to the right, every clock cycle. The accumulated sums start with the multiplication output from the first row of PEs, move downwards, and get added to the products of each successive PE, until they up at the last row of PEs where they become an element of the output matrix.</p><figure><p><img alt="Systolic array diagram" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/sys-array-standalone.svg"></p><figcaption>Systolic array architecture showing how PEs are connected to perform matrix multiplication</figcaption></figure><p>Because of this single unit (and the fact that matrix multiplications dominate the computations performed in models), TPUs can very easily inference and train any model.</p><h3>Worked example</h3><p>Now let's walk through the example of our XOR problem:</p><p>Our systolic array takes two inputs: the input matrix and the weight matrix. For our XOR network, we initialize with the following weights and biases:</p><div><p>Layer 1 parameters:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mn>1</mn></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2985</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.5792</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0913</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.4234</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>2</mn><mo>√ó</mo><mn>2</mn></mrow></msup><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi mathvariant="bold">b</mi><mn>1</mn></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.4939</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.189</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>1</mn><mo>√ó</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}_1 = \begin{bmatrix} \phantom{-}0.2985 &amp; -0.5792 \\[0.3em] \phantom{-}0.0913 &amp; \phantom{-}0.4234 \end{bmatrix} \in \mathbb{R}^{2 \times 2}, \quad \mathbf{b}_1 = \begin{bmatrix} -0.4939 &amp; \phantom{-}0.189\phantom{0} \end{bmatrix} \in \mathbb{R}^{1 \times 2}</annotation></semantics></math></span></span></span></p></div><p>Layer 2 parameters:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mn>2</mn></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.5266</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2958</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>1</mn><mo>√ó</mo><mn>2</mn></mrow></msup><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi mathvariant="bold">b</mi><mn>2</mn></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.6358</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>1</mn><mo>√ó</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}_2 = \begin{bmatrix} \phantom{-}0.5266 &amp; \phantom{-}0.2958 \end{bmatrix} \in \mathbb{R}^{1 \times 2}, \quad \mathbf{b}_2 = \begin{bmatrix} \phantom{-}0.6358 \end{bmatrix} \in \mathbb{R}^{1 \times 1}</annotation></semantics></math></span></span></span></p></div></div><h3>Input and weight scheduling</h3><p>To input our input batch within the systolic array, we need to:</p><ul><li>Rotate our X matrix by 90 degrees</li><br><figure><p><img alt="Rotate X matrix by 90 degrees" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/rotate.svg"></p><figcaption>Matrix rotation by 90 degrees to prepare for systolic array input</figcaption></figure><br><li>STAGGER the inputs (delay each row by 1 clock cycle)<sup><a href="#fn4" id="fn4-ref">[4]</a></sup></li><figure><p><img alt="Stagger input matrix" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/stagger-x.svg"></p><figcaption>Input matrix staggering pattern for systolic array processing</figcaption></figure><br></ul><p>To input our weight matrix: we need to:</p><ul><li>Stagger the weight matrix (similar to the inputs)</li><figure><p><img alt="Stagger weight matrix" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/stagger-w.svg"></p><figcaption>Weight matrix staggering pattern for systolic array processing</figcaption></figure><li>Transpose it!</li><figure><p><img alt="Matrix transposition" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/transpose.svg"></p><figcaption>Weight matrix transposition for correct mathematical alignment</figcaption></figure></ul><p>Note that the rotating and staggering don't have any mathematical significance ‚Äî they are simply required to make the systolic array work. The transpoing too is just for mathematical bookkeeping ‚Äì it's required to make the matrix math work because of how we set up our weight pointers within the neural network drawing.</p><h3>Staggering and FIFOs</h3><p>To perform the staggering, we designed near-identical accumulators for the weights and inputs that would sit above and to the left of the systolic array, respectively.</p><p>Since the activations are fed into the systolic array one-by-one, we thought a first-in-first-out queue (FIFO) would be the optimal data storage option. There was a slight difference between a traditional FIFO and the accumulators we built, however. Our accumulators had 2 input ports ‚Äî one for writing weights manually to the FIFO and one for writing the previous layer's outputs from the activation modules BACK into the input FIFOs (the previous layer's outputs are inputs for the current layer).</p><p>We also needed to load the weights in a similar fashion for every layer, so we replicated the logic for the weight FIFOs, without the second port.</p><div><h3>Systolic array matrix multiplication</h3><p><span>clk <!-- -->0</span></p></div><h3>Bias and activation</h3><p>The next step in the equation is adding the bias. To do this in hardware, we need to create a bias module under each column of the systolic array. We can see that as the sums move out of the last row within the systolic array, we can immediately stream them into our bias modules to compute our pre-activations.<b> We will denote these values with the variable Z.</b></p><div><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">Z</mi><mtext>biased</mtext></msub><mo>=</mo><mi mathvariant="bold">Z</mi><mo>+</mo><mi mathvariant="bold">b</mi></mrow><annotation encoding="application/x-tex">\mathbf{Z}_{\text{biased}} = \mathbf{Z} + \mathbf{b}</annotation></semantics></math></span></span></span></p></div><p>The bias vector <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">b</mi></mrow><annotation encoding="application/x-tex">\mathbf{b}</annotation></semantics></math></span></span></span> is broadcast across all rows of the matrix ‚Äî meaning it's added to each row of <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Z</mi></mrow><annotation encoding="application/x-tex">\mathbf{Z}</annotation></semantics></math></span></span></span></p></div><p>Now our equation is starting to look a lot like what we've learned in high school ‚Äìbut just in multidimensional form, where each column that streams out of the systolic array represents its own feature!</p><p>Next we have to apply the activation, for which we chose Leaky ReLU.<sup><a href="#fn5" id="fn5-ref">[5]</a></sup> <!-- -->This is also an element-wise operation, similar to the bias, meaning we need an activation module under every bias module (and by proxy under every column of the systolic array) and we can stream the outputs of our bias modules into the activation modules immediately.<b>We will denote these post-activation values with H</b>.</p><div><p>The Leaky ReLU function applies element-wise:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext>LeakyReLU</mtext><mi>Œ±</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>z</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if&nbsp;</mtext><mi>z</mi><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>Œ±</mi><mo>‚ãÖ</mo><mi>z</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if&nbsp;</mtext><mi>z</mi><mo>‚â§</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\text{LeakyReLU}_\alpha(z) = \begin{cases} z &amp; \text{if } z &gt; 0 \\[0.3em] \alpha \cdot z &amp; \text{if } z \leq 0 \end{cases}</annotation></semantics></math></span></span></span></p></div><p>where <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.5</annotation></semantics></math></span></span></span> is our leak factor. For matrices, this applies to each element independently.</p></div><div><p>For our XOR example, let's see how Layer 1 processes the data. First, the systolic array computes<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><msubsup><mi mathvariant="bold">W</mi><mn>1</mn><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathbf{X}\mathbf{W}_1^T</annotation></semantics></math></span></span></span>:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">Z</mi><mn>1</mn></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>0</mn><mphantom><mi mathvariant="normal">.</mi></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>0</mn><mphantom><mi mathvariant="normal">.</mi></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>0</mn><mphantom><mi mathvariant="normal">.</mi></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>1</mn><mphantom><mi mathvariant="normal">.</mi></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>1</mn><mphantom><mi mathvariant="normal">.</mi></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>0</mn><mphantom><mi mathvariant="normal">.</mi></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>1</mn><mphantom><mi mathvariant="normal">.</mi></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>1</mn><mphantom><mi mathvariant="normal">.</mi></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2985</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0913</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.5792</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.4234</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0000</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0000</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.5792</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.4234</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2985</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0913</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.2807</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.5147</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Z}_1 = \begin{bmatrix} \phantom{.}0\phantom{.} &amp; \phantom{.}0\phantom{.} \\[0.2em] \phantom{.}0\phantom{.} &amp; \phantom{.}1\phantom{.} \\[0.2em] \phantom{.}1\phantom{.} &amp; \phantom{.}0\phantom{.} \\[0.2em] \phantom{.}1\phantom{.} &amp; \phantom{.}1\phantom{.} \end{bmatrix} \begin{bmatrix} \phantom{-}0.2985 &amp; \phantom{-}0.0913 \\[0.2em] -0.5792 &amp; \phantom{-}0.4234 \end{bmatrix} = \begin{bmatrix} \phantom{-}0.0000 &amp; \phantom{-}0.0000 \\[0.2em] -0.5792 &amp; \phantom{-}0.4234 \\[0.2em] \phantom{-}0.2985 &amp; \phantom{-}0.0913 \\[0.2em] -0.2807 &amp; \phantom{-}0.5147 \end{bmatrix}</annotation></semantics></math></span></span></span></p></div><p>Then bias is added:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">Z</mi><mn>1</mn></msub><mo>=</mo><msub><mi mathvariant="bold">Z</mi><mn>1</mn></msub><mo>+</mo><msub><mi mathvariant="bold">b</mi><mn>1</mn></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.4939</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.1890</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>1.0731</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.6124</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.1954</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2803</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.7746</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.7037</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Z}_1 = \mathbf{Z}_1 + \mathbf{b}_1 = \begin{bmatrix} -0.4939 &amp; \phantom{-}0.1890 \\[0.2em] -1.0731 &amp; \phantom{-}0.6124 \\[0.2em] -0.1954 &amp; \phantom{-}0.2803 \\[0.2em] -0.7746 &amp; \phantom{-}0.7037 \end{bmatrix}</annotation></semantics></math></span></span></span></p></div><p>Finally, LeakyReLU is applied element-wise:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">H</mi><mn>1</mn></msub><mo>=</mo><msub><mtext>LeakyReLU</mtext><mn>0.5</mn></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">Z</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.2470</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.1890</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.5366</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.6124</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0977</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2803</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.3873</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.7037</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{H}_1 = \text{LeakyReLU}_{0.5}(\mathbf{Z}_1) = \begin{bmatrix} -0.2470 &amp; \phantom{-}0.1890 \\[0.2em] -0.5366 &amp; \phantom{-}0.6124 \\[0.2em] -0.0977 &amp; \phantom{-}0.2803 \\[0.2em] -0.3873 &amp; \phantom{-}0.7037 \end{bmatrix}</annotation></semantics></math></span></span></span></p></div><p>Negative values are multiplied by 0.5, positive values pass through unchanged.</p></div><div><h3>Systolic array with bias and leaky ReLU</h3><p><span>clk <!-- -->0</span></p></div><h3>Pipelining</h3><p>Now you might be asking ‚Äì why don't we merge the bias term and the activation term in one clock cycle? Well, this is because of something called pipelining! Pipelining allows multiple operations to be executed simultaneously across different stages of the TPU ‚Äîinstead of waiting for one complete operation to finish before starting the next, you break the work into stages that can overlap. Think of it like an assembly line: while one worker (activation module) processes a part, the previous worker (bias module) is already working on the next part. This keeps all of the modules busy rather than having them sit idle waiting for the previous stage to complete. It also affects the speed at which we can run our TPU ‚Äî if we have one module that tries to squeeze many operations in a single cycle, our clock speed will be bottlenecked by that module, as the other modules can only run as fast as that single module. Therefore, it's efficient and best practice to split up operations into individual clock cycles as much as possible.</p><figure><p><img alt="Pipeline diagram" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/pipelining.svg"></p><figcaption>Pipelining stages showing how operations overlap across clock cycles</figcaption></figure><p>Another mechanism we used to run our chip as efficiently as possible, was a propagating "start" signal, which we called a travelling chip enable (denoted by the purple dot). Because everything in our design was staggered, we realized that we could very elegantly assert a start signal for a single clock cycle at the first accumulator and have it propagate to neighbouring modules exactly when they needed to be turned on.</p><p>This would extend into the systolic array and eventually the bias and activation modules, where neighbouring PEs and modules, moving from the top left to the bottom right, were turned on in consecutive clock cycles. This ensured that every module was only performing computations when it was required to and wasn't wasting power in the background.</p><h3>Double buffering</h3><p>Now, we know that starting a new layer means we must compute the same <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><msup><mi mathvariant="bold">W</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{X}\mathbf{W}^T</annotation></semantics></math></span></span></span> using a new weight matrix. How can we do this if our systolic array is weight-stationary? How can we change the weights?</p><p>While thinking about this problem, we came across the idea of double buffering, which originates from video games. The reason why double buffering exists is to prevent something called screen tearing on your monitor. Ultimately, pixels take time to load and we'd like to "hide away" that time somehow. And if you paid attention, this is the exact same problem we're currently facing with the systolic array. Fortunately, video game designers have already come up with a solution for this problem. By adding a second "shadow" buffer, which holds the weights of the next layer while the current layer is being computed on, we can load in new weights during computation, cutting the total clock cycle count in half.</p><p>To make this work, we also needed to add some signals to move the data. First, we needed a signal to indicate when to switch the weights in the shadow buffer and the active buffer. We called this signal the "switch" signal (denoted by the blue dot) and it copied the values in the shadow buffer to the active buffer. It propagated from the top left of the systolic array to the bottom right (the same path as the travelling chip enable, but only within the systolic array). We then needed one more signal to indicate when we wanted to move the weights down by one row and we called this the "accept" flag (denoted by the green dot) because each row is ACCEPTING a new set of weights. This would move the new weights into the top row of the systolic array, as well as each row of weights down into the next row of the systolic array. These two control flags worked in tandem to make our double buffering mechanism work.</p><p>If you haven't already noticed, this allows the systolic array to do something powerful‚Ä¶continuous inference!!! We can continuously stream in new weights and inputs and compute forward pass for as many layers as we want. This touches into a core design philosophy of the systolic array: we want to maximize PE usage. We always want to keep the systolic array fed!</p><div><p>For Layer 2, the outputs from Layer 1 (<span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">H</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{H}_1</annotation></semantics></math></span></span></span>) now become our inputs:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">z</mi><mn>2</mn></msub><mo>=</mo><msub><mi mathvariant="bold">H</mi><mn>1</mn></msub><msubsup><mi mathvariant="bold">W</mi><mn>2</mn><mi>T</mi></msubsup><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.2470</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.1890</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.5366</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.6124</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0977</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2803</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.3873</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.7037</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.5266</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2958</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0741</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.1014</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0315</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0042</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}_2 = \mathbf{H}_1\mathbf{W}_2^T = \begin{bmatrix} -0.2470 &amp; \phantom{-}0.1890 \\[0.2em] -0.5366 &amp; \phantom{-}0.6124 \\[0.2em] -0.0977 &amp; \phantom{-}0.2803 \\[0.2em] -0.3873 &amp; \phantom{-}0.7037 \end{bmatrix} \begin{bmatrix} \phantom{-}0.5266 \\[0.2em] \phantom{-}0.2958 \end{bmatrix} = \begin{bmatrix} -0.0741 \\[0.2em] -0.1014 \\[0.2em] \phantom{-}0.0315 \\[0.2em] \phantom{-}0.0042 \end{bmatrix}</annotation></semantics></math></span></span></span></p></div><p>Adding bias and applying activation:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">z</mi><mn>2</mn></msub><mo>=</mo><msub><mi mathvariant="bold">z</mi><mn>2</mn></msub><mo>+</mo><msub><mi mathvariant="bold">b</mi><mn>2</mn></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.5617</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.5344</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.6673</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.6400</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}_2 = \mathbf{z}_2 + \mathbf{b}_2 = \begin{bmatrix} \phantom{-}0.5617 \\[0.2em] \phantom{-}0.5344 \\[0.2em] \phantom{-}0.6673 \\[0.2em] \phantom{-}0.6400 \end{bmatrix}</annotation></semantics></math></span></span></span></p></div><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi mathvariant="bold">y</mi><mo>^</mo></mover><mo>=</mo><msub><mi mathvariant="bold">h</mi><mn>2</mn></msub><mo>=</mo><msub><mtext>LeakyReLU</mtext><mn>0.5</mn></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">z</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.5617</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.5344</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.6673</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.6400</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\mathbf{y}} = \mathbf{h}_2 = \text{LeakyReLU}_{0.5}(\mathbf{z}_2) = \begin{bmatrix} \phantom{-}0.5617 \\[0.2em] \phantom{-}0.5344 \\[0.2em] \phantom{-}0.6673 \\[0.2em] \phantom{-}0.6400 \end{bmatrix}</annotation></semantics></math></span></span></span></p></div><p>All values are positive, so they pass through unchanged. These are our final predictions for the XOR problem!</p><div><h3>Forward pass walkthrough (with double buffering)</h3><p><span>clk <!-- -->0</span></p></div></div><h3>Control unit and ISA</h3><p>Our final step for inference was making a control unit to use a custom instruction set (ISA) to assert all of our control flags and load data through a data bus. Including the data bus, our ISA was 24 bits long and it made our testbench more elegant as we could pass a single string of bits every clock cycle, rather than individually setting multiple flags.</p><p>We then put everything together and got inference completely working! This was a big milestone for us and we were very proud about what we had accomplished.</p><h2>Backpropagation and training</h2><div><p>Ok we've solved inference ‚Äî but what about training? Well here's the beauty: We can use the same architecture we use for inference for training! Why? Because training is just matrix multiplications with a few extra steps.</p><p>Here's where things get really exciting. Let's say we just ran inference on the XOR problem and got a prediction that looks something like [0.8, 0.3, 0.1, 0.9] when we actually wanted [1, 0, 0, 1]. Our model is performing poorly! We need to make it better. This is where training comes in. We're going to use something called a loss function to tell our model exactly how poorly it's doing. For simplicity, we chose Mean Squared Error (MSE) ‚Äî think of it like measuring the "distance" between what we predicted and what we actually wanted, just like how you might measure how far off target your basketball shot was.<!-- --> <b>Let's denote the loss with L.</b></p><div><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>‚àí</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\mathcal{L} = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2</annotation></semantics></math></span></span></span></p></div><p>where <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span></span></span> is the target output,<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\hat{y}_i</annotation></semantics></math></span></span></span> is our prediction, and<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span></span> is the number of samples</p></div><div><p>For our XOR example, with predictions<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi mathvariant="bold">y</mi><mo>^</mo></mover><mo>=</mo><mo stretchy="false">[</mo><mn>0.5617</mn><mo separator="true">,</mo><mn>0.5344</mn><mo separator="true">,</mo><mn>0.6673</mn><mo separator="true">,</mo><mn>0.6400</mn><msup><mo stretchy="false">]</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\hat{\mathbf{y}} = [0.5617, 0.5344, 0.6673, 0.6400]^T</annotation></semantics></math></span></span></span> <!-- -->and targets <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">y</mi><mo>=</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mn>0</mn><msup><mo stretchy="false">]</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{y} = [0, 1, 1, 0]^T</annotation></semantics></math></span></span></span>:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><mo stretchy="false">[</mo><mo stretchy="false">(</mo><mn>0</mn><mo>‚àí</mo><mn>0.5617</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>‚àí</mo><mn>0.5344</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>‚àí</mo><mn>0.6673</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mn>0</mn><mo>‚àí</mo><mn>0.6400</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\mathcal{L} = \frac{1}{4}[(0 - 0.5617)^2 + (1 - 0.5344)^2 + (1 - 0.6673)^2 + (0 - 0.6400)^2]</annotation></semantics></math></span></span></span></p></div><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><mo stretchy="false">[</mo><mn>0.3155</mn><mo>+</mo><mn>0.2168</mn><mo>+</mo><mn>0.1107</mn><mo>+</mo><mn>0.4096</mn><mo stretchy="false">]</mo><mo>=</mo><mn>0.2631</mn></mrow><annotation encoding="application/x-tex">\mathcal{L} = \frac{1}{4}[0.3155 + 0.2168 + 0.1107 + 0.4096] = 0.2631</annotation></semantics></math></span></span></span></p></div><p>This loss value tells us how far off our predictions are from the true XOR outputs.</p></div><p>So right after we finish computing our final layer's activations (let's call them<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">H</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{H}_2</annotation></semantics></math></span></span></span>), we immediately stream them into a loss module to calculate just how bad our predictions are. These loss modules sit right below our activation modules, and we only use them when we've reached our final layer. But here's the key insight: you don't actually need to calculate the loss value itself to train. You just need its derivative. Why? Because that derivative tells us which direction to adjust our weights to make the loss smaller. It's like having a compass that points toward "better performance."</p><h3>The magic of the chain rule</h3><p>This is where calculus enters the picture. To make our model better, we need to figure out how changing each weight affects our loss. The chain rule lets us break this massive calculation into smaller, manageable pieces.</p><div><p>The chain rule for gradients:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">W</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">Z</mi></mrow></mfrac><mo>‚ãÖ</mo><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">Z</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">W</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{W}} = \frac{\partial \mathcal{L}}{\partial \mathbf{Z}} \cdot \frac{\partial \mathbf{Z}}{\partial \mathbf{W}}</annotation></semantics></math></span></span></span></p></div><p>This allows us to compute gradients layer by layer, propagating them backwards through the network</p></div><p><img alt="Long chain diagram" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/longchain.svg"></p><p>Let's trace through what happens step by step.</p><ol><li>Calculate<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">H</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{H}_2}</annotation></semantics></math></span></span></span> <!-- -->- how much the loss changes with respect to our final activations.</li><br><li>Compute<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">H</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{H}_2}{\partial \mathbf{Z}_2}</annotation></semantics></math></span></span></span> <!-- -->by taking the derivative of the activation (leaky ReLU in our case).</li><br><li>Compute<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">W</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{Z}_2}{\partial \mathbf{W}_2}</annotation></semantics></math></span></span></span>,<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">H</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{Z}_2}{\partial \mathbf{H}_2}</annotation></semantics></math></span></span></span></li></ol></div><p>Since all elements of <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">z</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{z}_2</annotation></semantics></math></span></span></span> are positive, the LeakyReLU gradient is 1:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">b</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{Z}_2}{\partial \mathbf{b}_2}</annotation></semantics></math></span></span></span></p></div><h3>The beautiful symmetry of forward and backward pass</h3><p>After drawing out the entire computational graph, we discovered something remarkable: the longest chain in backpropagation closely resembles forward pass! In forward pass, we multiply activation matrices with transposed weight matrices. In backward pass, we multiply gradient matrices with weight matrices (untransposed). It's like looking in a mirror!</p><div><p>Propagating gradients to the hidden layer:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">H</mi><mn>1</mn></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">z</mi><mn>2</mn></msub></mrow></mfrac><msub><mi mathvariant="bold">W</mi><mn>2</mn></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2808</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.2328</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.1664</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.3200</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.5266</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2958</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.1479</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0831</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.1226</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0689</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0876</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0492</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.1685</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0947</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{H}_1} = \frac{\partial \mathcal{L}}{\partial \mathbf{z}_2} \mathbf{W}_2 = \begin{bmatrix} \phantom{-}0.2808\phantom{0} \\[0.5em] -0.2328\phantom{0} \\[0.5em] -0.1664\phantom{0} \\[0.5em] \phantom{-}0.3200\phantom{0} \end{bmatrix} \begin{bmatrix} \phantom{-}0.5266\phantom{0} &amp; \phantom{-}0.2958\phantom{0} \end{bmatrix} = \begin{bmatrix} \phantom{-}0.1479\phantom{0} &amp; \phantom{-}0.0831\phantom{0} \\[0.5em] -0.1226\phantom{0} &amp; -0.0689\phantom{0} \\[0.5em] -0.0876\phantom{0} &amp; -0.0492\phantom{0} \\[0.5em] \phantom{-}0.1685\phantom{0} &amp; \phantom{-}0.0947\phantom{0} \end{bmatrix}</annotation></semantics></math></span></span></span></p></div><p>And through the first layer's activation:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>1</mn></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">H</mi><mn>1</mn></msub></mrow></mfrac><mo>‚äô</mo><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mtext>LeakyReLU</mtext><mn>0.5</mn></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">Z</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>1</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{Z}_1} = \frac{\partial \mathcal{L}}{\partial \mathbf{H}_1} \odot \frac{\partial \text{LeakyReLU}_{0.5}(\mathbf{Z}_1)}{\partial \mathbf{Z}_1}</annotation></semantics></math></span></span></span></p></div><p>With mixed positive and negative values in<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">Z</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{Z}_1</annotation></semantics></math></span></span></span>, the gradient is:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>1</mn></msub></mrow></mfrac><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.1479</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0831</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.1226</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0689</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0876</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0492</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.1685</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0947</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>‚äô</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>0.5</mn><mphantom><mn>.0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>1</mn><mphantom><mi mathvariant="normal">.</mi></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>0.5</mn><mphantom><mn>.0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>1</mn><mphantom><mi mathvariant="normal">.</mi></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>0.5</mn><mphantom><mn>.0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>1</mn><mphantom><mi mathvariant="normal">.</mi></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>0.5</mn><mphantom><mn>.0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mi mathvariant="normal">.</mi></mphantom><mn>1</mn><mphantom><mi mathvariant="normal">.</mi></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0739</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0831</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0613</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0689</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0438</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0492</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0843</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0947</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{Z}_1} = \begin{bmatrix} \phantom{-}0.1479\phantom{0} &amp; \phantom{-}0.0831\phantom{0} \\[0.5em] -0.1226\phantom{0} &amp; -0.0689\phantom{0} \\[0.5em] -0.0876\phantom{0} &amp; -0.0492\phantom{0} \\[0.5em] \phantom{-}0.1685\phantom{0} &amp; \phantom{-}0.0947\phantom{0} \end{bmatrix} \odot \begin{bmatrix} \phantom{.}0.5\phantom{.0} &amp; \phantom{.}1\phantom{.} \\[0.5em] \phantom{.}0.5\phantom{.0} &amp; \phantom{.}1\phantom{.} \\[0.5em] \phantom{.}0.5\phantom{.0} &amp; \phantom{.}1\phantom{.} \\[0.5em] \phantom{.}0.5\phantom{.0} &amp; \phantom{.}1\phantom{.} \end{bmatrix} = \begin{bmatrix} \phantom{-}0.0739\phantom{0} &amp; \phantom{-}0.0831\phantom{0} \\[0.5em] -0.0613\phantom{0} &amp; -0.0689\phantom{0} \\[0.5em] -0.0438\phantom{0} &amp; -0.0492\phantom{0} \\[0.5em] \phantom{-}0.0843\phantom{0} &amp; \phantom{-}0.0947\phantom{0} \end{bmatrix}</annotation></semantics></math></span></span></span></p></div></div><p>Once we have all of these individual derivatives, we can multiply them together to find any derivative with respect of the loss (i.e.<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">H</mi><mn>2</mn></msub></mrow></mfrac><mo>‚ãÖ</mo><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">H</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub></mrow></mfrac><mo>‚ãÖ</mo><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">W</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{H}_2} \cdot \frac{\partial \mathbf{H}_2}{\partial \mathbf{Z}_2} \cdot \frac{\partial \mathbf{Z}_2}{\partial \mathbf{W}_2}</annotation></semantics></math></span></span></span> <!-- -->gives us<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">W</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{W}_2}</annotation></semantics></math></span></span></span>).</p><p>After that, we have to compute the activation derivative<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">H</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{H}_2}{\partial \mathbf{Z}_2}</annotation></semantics></math></span></span></span>, for which the formula is<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mtext>LeakyReLU</mtext><mi>Œ±</mi></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub></mrow></mfrac><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if&nbsp;</mtext><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>Œ±</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if&nbsp;</mtext><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub><mo>‚â§</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial \text{LeakyReLU}_{\alpha}(\mathbf{Z}_2)}{\partial \mathbf{Z}_2} = \begin{cases} 1 &amp; \text{if } \mathbf{Z}_2 &gt; 0 \\[0.3em] \alpha &amp; \text{if } \mathbf{Z}_2 \leq 0 \end{cases}</annotation></semantics></math></span></span></span>. This is also an element-wise computation, meaning we can structure it exactly like the loss module (and bias and activation modules), but it will perform a different calculation. One important note about this module, however, is that it requires the activations we computed during forward pass.</p><p>Now you might be wondering ‚Äî how do we actually compute derivatives in hardware? Let's look at Leaky ReLU as an example, since it's beautifully simple but demonstrates the key principles. Remember that Leaky ReLU applies different operations based on whether the input is positive or negative. The derivative follows the same pattern: it outputs 1 for positive inputs and a small constant (we used 0.01) for negative inputs.</p><div><p>The Leaky ReLU gradient:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mtext>LeakyReLU</mtext><mi>Œ±</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><mi>z</mi></mrow></mfrac><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if&nbsp;</mtext><mi>z</mi><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>Œ±</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if&nbsp;</mtext><mi>z</mi><mo>‚â§</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial \text{LeakyReLU}_\alpha(z)}{\partial z} = \begin{cases} 1 &amp; \text{if } z &gt; 0 \\[0.3em] \alpha &amp; \text{if } z \leq 0 \end{cases}</annotation></semantics></math></span></span></span></p></div></div><pre><code><span>always</span> @<span>(</span><span>posedge</span> <span>clk</span><span>)</span> <span>begin</span>
    <span>if</span> <span>(</span><span>rst</span><span>)</span> <span>begin</span>
        <span>output</span> <span>&lt;=</span> <span>0</span>;
    <span>end</span> <span>else</span> <span>begin</span>
        <span>output</span> <span>&lt;=</span> <span>(</span><span>input</span> <span>&gt;</span> <span>0</span><span>)</span> <span>?</span> <span>input</span> <span>:</span> <span>0</span>.01 <span>*</span> <span>input</span>;
    <span>end</span>
<span>end</span>
</code></pre><figure><p><img alt="Leaky ReLU derivative" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/leaky-relu-derivative.svg"></p><figcaption>Leaky ReLU derivative implementation in hardware showing the conditional logic</figcaption></figure><p>What's beautiful about this is that it's just a simple comparison ‚Äì no complex arithmetic needed. The hardware can compute this derivative in a single clock cycle, keeping our pipeline flowing smoothly. This same principle applies to other activation functions: their derivatives often simplify to basic operations that hardware can execute very efficiently. This insight led us to compute the long chain first ‚Äî getting all our<span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mi>n</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{Z}_n}</annotation></semantics></math></span></span></span> <!-- -->gradients just like we computed activations in forward pass. We could cache these gradients and reuse them, following the same efficient pattern we'd already mastered.</p><div><p>You'll notice a really cool pattern emerging: all these modules that sit underneath the systolic array process column vectors that stream out one by one. This gave us the idea to unify them into something we called a <b>vector processing unit (VPU)</b> ‚Äì because that's exactly what they're doing, processing vectors element-wise!<sup><a href="#fn6" id="fn6-ref">[6]</a></sup></p><p>Not only is this more elegant to work with, it's also useful when we scale our TPU beyond a 2x2 systolic array, as we'll have N number of these modules (N being the size of the systolic array), each of which we would have to interface with individually. Unifying these modules under a parent module makes our design more scalable and elegant!</p></div><figure><p><img alt="Vector processing unit" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/vpu.svg"></p><figcaption>Vector Processing Unit (VPU) architecture showing unified element-wise operations</figcaption></figure><p>Additionally, by incorporating control signals for each module, which we call the VPU pathway bits, we can selectively enable or skip specific operations. This makes the VPU flexible enough to support both inference and training. For instance, during the forward pass, we want to apply biases and activations but skip computing loss or activation derivatives. When transitioning to the backward pass, all modules are engaged, but within the backward chain we only need to compute the activation derivative. Due to pipelining, all values that flow through the VPU pass through each of the four modules, and any unused modules simply act as registers, forwarding their inputs to outputs without performing computation.</p><p>The next few derivatives are interesting because we can actually use matrix multiplication (and the systolic array!) to compute the derivatives with the help of these three identities:</p><ol><li>If we have<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Z</mi><mo>=</mo><mi mathvariant="bold">X</mi><msup><mi mathvariant="bold">W</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{Z} = \mathbf{X}\mathbf{W}^T</annotation></semantics></math></span></span></span> and take its derivative with respect to the weights, we get<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">Z</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">W</mi></mrow></mfrac><mo>=</mo><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{Z}}{\partial \mathbf{W}} = \mathbf{X}</annotation></semantics></math></span></span></span></li><li>If we have<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Z</mi><mo>=</mo><mi mathvariant="bold">X</mi><msup><mi mathvariant="bold">W</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{Z} = \mathbf{X}\mathbf{W}^T</annotation></semantics></math></span></span></span> and take its derivative with respect to the inputs<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math></span></span></span>, we get<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">Z</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">X</mi></mrow></mfrac><mo>=</mo><msup><mi mathvariant="bold">W</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{Z}}{\partial \mathbf{X}} = \mathbf{W}^T</annotation></semantics></math></span></span></span> <!-- -->(just the weight matrix transposed)</li><li>For the bias term, the derivative is simply 1.</li></ol><p>This means that we can multiply the previous<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">H</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">Z</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{H}}{\partial \mathbf{Z}}</annotation></semantics></math></span></span></span> <!-- -->with <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math></span></span></span>,<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">W</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}^T</annotation></semantics></math></span></span></span>, and 1 to get<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">H</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">W</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{H}}{\partial \mathbf{W}}</annotation></semantics></math></span></span></span>,<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">H</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">X</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{H}}{\partial \mathbf{X}}</annotation></semantics></math></span></span></span>, and<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">H</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">b</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{H}}{\partial \mathbf{b}}</annotation></semantics></math></span></span></span>, respectively, and we can multiply all of these by<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">H</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{H}}</annotation></semantics></math></span></span></span> <!-- -->to get the gradients of the loss with respect to all of our second layer parameters. And because all of the gradients are actually gradient matrices, we can use the systolic array!</p><p>Now something to note about the activation derivative<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">H</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{H}_2}{\partial \mathbf{Z}_2}</annotation></semantics></math></span></span></span> <!-- -->and the weight derivative<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">Z</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="bold">W</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{Z}}{\partial \mathbf{W}}</annotation></semantics></math></span></span></span> <!-- -->is that they both require the post-activations (H) we calculate during forward pass. This means we need to store the outputs of every layer in some form of memory to be able to perform training. Here's where we created a new scratchpad memory module<sup><a href="#fn7" id="fn7-ref">[7]</a></sup> <!-- -->which we called the unified buffer (UB).<sup><a href="#fn8" id="fn8-ref">[8]</a></sup> <!-- -->This lets us store our H values immediately after we compute them during forward pass.</p><p>We realized that we can also get rid of the input and weight accumulators, as well as manually loading the bias and leak factors into their respective modules, by using the UB to store them. This is also better practice, rather than loading in new data every clock cycle with the instruction set. Since we want to access two values (2 inputs or 2 weights for each row/col of the systolic array) at the same time, we added TWO read and write ports. We did this for each data primitive (inputs, weights, bias, leak factor, post activations) to minimize data contention since we have many different types of data.</p><p>To read values, we supply a starting address and the number of values, we supply a starting address and the number of locations we want the UB to read and it will read 2 values every clock cycle. Writing is a similar mechanism, where we specify which values we want to write to each of the two input ports. The beauty in the read mechanism is that it runs in the background once we supply a starting address until the number of locations given are read, meaning we only need to provide an instruction for this every few clock cycles.</p><figure><p><img alt="Unified Buffer diagram" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/ub-diagram.svg"></p><figcaption>Unified Buffer (UB) architecture showing dual-port read mechanism</figcaption></figure><figure><p><img alt="Unified Buffer waveform" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/ub-waveform.svg"></p><figcaption>Unified Buffer timing waveform showing read operation</figcaption></figure><p>At the end of the day, not having these mechanisms wouldn't break the TPU ‚Äî but they allow us to always keep the systolic array fed, which is a core design principle we couldn't compromise.</p><p>While we were working on this, we realized we could make one last small optimization for the activation derivative module ‚Äî since we only use the <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">H</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{H}_2</annotation></semantics></math></span></span></span> values once (for computing<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">H</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathbf{H}_2}{\partial \mathbf{Z}_2}</annotation></semantics></math></span></span></span>), we created a tiny cache within the VPU instead of storing them in the UB. The rest of the <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">H</mi></mrow><annotation encoding="application/x-tex">\mathbf{H}</annotation></semantics></math></span></span></span> values will be stored in the UB because they're needed to compute multiple derivatives.</p><figure><p><img alt="H-cache diagram" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/h-cache.svg"></p><figcaption>H-cache optimization for storing temporary activation values</figcaption></figure><p>This is what the new TPU architecture, modified to perform training, looks like:</p><figure><p><img alt="Complete TPU architecture" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/tpu.svg"></p><figcaption>Complete TPU architecture showing all components for both inference and training</figcaption></figure><p>Now we can do backpropagation!</p><p>Going back to the computational graph, we discovered something remarkable: the longest chain in backpropagation closely resembles forward pass! In forward pass, we multiply activation matrices with transposed weight matrices. In backward pass, we multiply gradient matrices with weight matrices (untransposed). It's like looking in a mirror!</p><figure><p><img alt="Forward pass diagram" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/forward-pass.svg"></p><figcaption>Forward pass computation flow showing matrix operations</figcaption></figure><p>This insight led us to compute the long chain of the computational graph first (highlighted in yellow) ‚Äì getting all our<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mi>n</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{Z}_n}</annotation></semantics></math></span></span></span> <!-- -->gradients just like we computed activations in forward pass. We could cache these gradients and reuse them, following the same efficient pattern we'd already mastered.</p><p>We create a loop where we:</p><ol><li>Fetch a bridge node (<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mi>n</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{Z}_n}</annotation></semantics></math></span></span></span>) from our unified buffer</li><li>Fetch the corresponding <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">H</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{H}_n</annotation></semantics></math></span></span></span> <!-- -->matrix, also from unified buffer</li><li>Stream these through our systolic array to compute the weight gradients</li></ol><div><h3>Backward pass through second hidden layer</h3><p><span>clk <!-- -->0</span></p></div><p>And here's where something really magical happens: we can stream these weight gradients directly into a gradient descent module while we're still computing them! This module takes the current weights stored in memory and updates them using the gradients.</p><div><p>The gradient descent update rule:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold-italic">Œ∏</mi><mtext>new</mtext></msub><mo>=</mo><msub><mi mathvariant="bold-italic">Œ∏</mi><mtext>old</mtext></msub><mo>‚àí</mo><mi>Œ±</mi><msub><mi mathvariant="normal">‚àá</mi><mi mathvariant="bold-italic">Œ∏</mi></msub><mi mathvariant="script">L</mi></mrow><annotation encoding="application/x-tex">\bm{\theta}_{\text{new}} = \bm{\theta}_{\text{old}} - \alpha \nabla_{\bm{\theta}} \mathcal{L}</annotation></semantics></math></span></span></span></p></div><p>where <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span></span></span> is the learning rate and<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">Œ∏</mi></mrow><annotation encoding="application/x-tex">\bm{\theta}</annotation></semantics></math></span></span></span> represents any parameter (weights or biases)</p></div><div><p>Computing weight gradients for our XOR network:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">W</mi><mn>2</mn></msub></mrow></mfrac><mo>=</mo><msup><mrow><mo fence="true">(</mo><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">z</mi><mn>2</mn></msub></mrow></mfrac><mo fence="true">)</mo></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">H</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{W}_2} = \left(\frac{\partial \mathcal{L}}{\partial \mathbf{z}_2}\right)^T \mathbf{H}_1</annotation></semantics></math></span></span></span></p></div><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2808</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.2328</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.1664</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.3200</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.2470</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.1890</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.5366</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.6124</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0977</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2803</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.3873</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.7037</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0521</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0891</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">= \begin{bmatrix} \phantom{-}0.2808\phantom{0} &amp; -0.2328\phantom{0} &amp; -0.1664\phantom{0} &amp; \phantom{-}0.3200\phantom{0} \end{bmatrix} \begin{bmatrix} -0.2470\phantom{0} &amp; \phantom{-}0.1890\phantom{0} \\[0.5em] -0.5366\phantom{0} &amp; \phantom{-}0.6124\phantom{0} \\[0.5em] -0.0977\phantom{0} &amp; \phantom{-}0.2803\phantom{0} \\[0.5em] -0.3873\phantom{0} &amp; \phantom{-}0.7037\phantom{0} \end{bmatrix} = \begin{bmatrix} -0.0521\phantom{0} &amp; \phantom{-}0.0891\phantom{0} \end{bmatrix}</annotation></semantics></math></span></span></span></p></div><p>Similarly for Layer 1:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">W</mi><mn>1</mn></msub></mrow></mfrac><mo>=</mo><msup><mrow><mo fence="true">(</mo><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mn>1</mn></msub></mrow></mfrac><mo fence="true">)</mo></mrow><mi>T</mi></msup><mi mathvariant="bold">X</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0531</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0920</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0138</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0404</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{W}_1} = \left(\frac{\partial \mathcal{L}}{\partial \mathbf{Z}_1}\right)^T \mathbf{X} = \begin{bmatrix} \phantom{-}0.0531\phantom{0} &amp; \phantom{-}0.0920\phantom{0} \\[0.5em] \phantom{-}0.0138\phantom{0} &amp; -0.0404\phantom{0} \end{bmatrix}</annotation></semantics></math></span></span></span></p></div><p>Bias gradients (sum over samples):</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">b</mi><mn>2</mn></msub></mrow></mfrac><mo>=</mo><mn>0.2017</mn><mo separator="true">,</mo><mspace width="1em"></mspace><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">b</mi><mn>1</mn></msub></mrow></mfrac><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0531</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0138</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{b}_2} = 0.2017, \quad \frac{\partial \mathcal{L}}{\partial \mathbf{b}_1} = \begin{bmatrix} \phantom{-}0.0531\phantom{0} \\[0.5em] \phantom{-}0.0138\phantom{0} \end{bmatrix}</annotation></semantics></math></span></span></span></p></div><p>Applying gradient descent with learning rate<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mo>=</mo><mn>0.75</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.75</annotation></semantics></math></span></span></span>:</p><div><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi mathvariant="bold">W</mi><mn>2</mn><mtext>new</mtext></msubsup><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.5266</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2958</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>‚àí</mo><mn>0.75</mn><mo>‚ãÖ</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>0.0521</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.0891</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.5657</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mphantom><mo>‚àí</mo></mphantom><mn>0.2290</mn><mphantom><mn>0</mn></mphantom></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{W}_2^{\text{new}} = \begin{bmatrix} \phantom{-}0.5266\phantom{0} &amp; \phantom{-}0.2958\phantom{0} \end{bmatrix} - 0.75 \cdot \begin{bmatrix} -0.0521\phantom{0} &amp; \phantom{-}0.0891\phantom{0} \end{bmatrix} = \begin{bmatrix} \phantom{-}0.5657\phantom{0} &amp; \phantom{-}0.2290\phantom{0} \end{bmatrix}</annotation></semantics></math></span></span></span></p></div></div><p>No waiting around ‚Äî everything flows like water through our pipeline.</p><p>You might be wondering: "We've used our matrix multiplication identities for the long chain and weight gradients ‚Äî how do we calculate bias gradients?" Well, we've actually already done most of the work! Since we're processing batches of data, we can simply sum (the technical term is "reduce") the<!-- --> <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi mathvariant="bold">Z</mi><mi>n</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial \mathbf{Z}_n}</annotation></semantics></math></span></span></span> <!-- -->gradients across the batch dimension. The beauty is that we can do this reduction right when we're computing the long chain ‚Äî no extra work required!</p><p>With all these new changes and control flags, our instruction is significantly longer ‚Äî 94 bits in fact! But we can confirm that every single one of these bits is needed and we ensured that we couldn't make the instruction set any smaller without compromising the speed and efficiency of the TPU.</p><figure><div><p><img alt="Instruction Set Architecture diagram" loading="lazy" decoding="async" data-nimg="fill" src="https://www.tinytpu.com/isa.svg"></p></div><figcaption>94-bit Instruction Set Architecture (ISA) layout showing control flags and data fields</figcaption></figure><h3>Putting it all together</h3><p>By continuing this same process iteratively ‚Äì forward pass, backward pass, weight updates ‚Äì we can train our network until it performs exactly how we want. The same systolic array that powered our inference now powers our training, with just a few additional modules to handle the gradient computations.</p><p>What started as a simple idea about matrix multiplication has grown into a complete training system. Every component works together in harmony: data flows through pipelines, modules operate in parallel, and our systolic array stays fed with useful work.</p><figure><p><img alt="Final waveform simulation results" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://www.tinytpu.com/_next/image?url=%2Ffinal-waveform.png&amp;w=640&amp;q=75 640w, https://www.tinytpu.com/_next/image?url=%2Ffinal-waveform.png&amp;w=750&amp;q=75 750w, https://www.tinytpu.com/_next/image?url=%2Ffinal-waveform.png&amp;w=828&amp;q=75 828w, https://www.tinytpu.com/_next/image?url=%2Ffinal-waveform.png&amp;w=1080&amp;q=75 1080w, https://www.tinytpu.com/_next/image?url=%2Ffinal-waveform.png&amp;w=1200&amp;q=75 1200w, https://www.tinytpu.com/_next/image?url=%2Ffinal-waveform.png&amp;w=1920&amp;q=75 1920w, https://www.tinytpu.com/_next/image?url=%2Ffinal-waveform.png&amp;w=2048&amp;q=75 2048w, https://www.tinytpu.com/_next/image?url=%2Ffinal-waveform.png&amp;w=3840&amp;q=75 3840w" src="https://www.tinytpu.com/_next/image?url=%2Ffinal-waveform.png&amp;w=3840&amp;q=75"></p><figcaption>Final waveform simulation in GTKWave showing the weight and bias updates in memory after one epoch!</figcaption></figure></div><p id="fn2">[2] Fun fact: the name of the systolic array is actually inspired by the human heart ‚Äî just as systolic blood pressure is created by coordinated heart contractions that push blood through the cardiovascular system in waves, a systolic array processes data through coordinated computational "beats" that push information through the processing elements in waves.<!-- --> <a href="#fn2-ref">‚Ü© back</a></p><p id="fn3">[3] This is a weight-stationary systolic array, which means the weights for each layer are stationary within their respective PEs and don't move around. However, there is a non-weight-stationary systolic array where the weights move along with the inputs, which has its own advantages and disadvantages.<!-- --> <a href="#fn3-ref">‚Ü© back</a></p><p id="fn4">[4] Many illustrations online that depict staggering are actually flat out wrong because they pad consecutive rows with zeros, insetad of delaying them by a clock cycle. While this still gets the correct output, it wastes memory because we would have to store additional zeros that we don't use.<!-- --> <a href="#fn4-ref">‚Ü© back</a></p><p id="fn5">[5] We chose Leaky ReLU over ReLU because we found that since we have a very small network, the model wasn't training properly when we used ReLU ‚Äî it needed more non-linearity.<!-- --> <a href="#fn5-ref">‚Ü© back</a></p><p id="fn7">[7] A scratchpad memory is a large bank of registers (each of which can store individual values) that lets us access any register we want. A FIFO for example is NOT a scratchpad memory since you can only access the first element in the queue.<!-- --> <a href="#fn7-ref">‚Ü© back</a></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[T-Mobile claimed selling location data without consent is legal‚Äìjudges disagree (334 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/08/t-mobile-claimed-selling-location-data-without-consent-is-legal-judges-disagree/</link>
            <guid>44944291</guid>
            <pubDate>Mon, 18 Aug 2025 19:25:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/08/t-mobile-claimed-selling-location-data-without-consent-is-legal-judges-disagree/">https://arstechnica.com/tech-policy/2025/08/t-mobile-claimed-selling-location-data-without-consent-is-legal-judges-disagree/</a>, See on <a href="https://news.ycombinator.com/item?id=44944291">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2112551">
  
  <header>
  <div>
    <div>
      

      

      <p>
        T-Mobile can't overturn $92 million fine; AT&amp;T and Verizon verdicts still to come.
      </p>

      
    </div>

    <div>
    
    <p><span>
          Credit:

          
          Aurich Lawson | Getty Images

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>A federal appeals court rejected T-Mobile's attempt to overturn $92 million in fines for selling customer location information to third-party firms.</p>
<p>The Federal Communications Commission last year <a href="https://arstechnica.com/tech-policy/2024/04/fcc-fines-big-three-carriers-196m-for-selling-users-real-time-location-data/">fined T-Mobile, AT&amp;T, and Verizon</a>, saying the carriers illegally shared access to customers' location information without consent and did not take reasonable measures to protect that sensitive data against unauthorized disclosure. The fines relate to sharing of real-time location data that was <a href="https://arstechnica.com/tech-policy/2018/06/verizon-and-att-will-stop-selling-your-phones-location-to-data-brokers/">revealed in 2018</a>, but it took years for the FCC to finalize the penalties.</p>
<p>The three carriers appealed the rulings in three different courts, and the first major decision was handed down Friday. A three-judge panel at the US Court of Appeals for the District of Columbia Circuit <a href="https://media.cadc.uscourts.gov/opinions/docs/2025/08/24-1224-2130255.pdf">ruled unanimously</a> against T-Mobile and its subsidiary Sprint.</p>
<p>"Every cell phone is a tracking device," the ruling begins. "To receive service, a cell phone must periodically connect with the nearest tower in a wireless carrier's network. Each time it does, it sends the carrier a record of the phone's location and, by extension, the location of the customer who owns it. Over time, this information becomes an exhaustive history of a customer's whereabouts and '<a href="https://www.supremecourt.gov/opinions/17pdf/16-402_h315.pdf">provides an intimate window into [that] person's life</a>.'"</p>
<p>Until 2019, T-Mobile and Sprint sold customer location information (CLI) to location information aggregators LocationSmart and Zumigo. The carriers did not verify whether buyers obtained customer consent, the ruling said. "Several bad actors abused Sprint and T-Mobile's programs to illicitly access CLI without the customers' knowledge, let alone consent. And even after Sprint and T-Mobile became aware of those abuses, they continued to sell CLI for some time without adopting new safeguards," judges wrote.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<h2>Carriers claimed selling data didn‚Äôt violate law</h2>
<p>Instead of denying the allegations, the carriers argued that the FCC overstepped its authority. But the appeals court panel decided that the FCC acted properly:</p>
<blockquote><p>Sprint and T-Mobile (collectively, "the Carriers") now petition for our review. Neither denies what happened. Instead, they argue that the undisputed facts do not amount to a violation of the law. The Carriers also argue that the Commission misinterpreted the Communications Act, miscalculated the penalties, and violated the Seventh Amendment by not affording them a jury trial. Because the Carriers' arguments lack merit, we deny the petitions for review.</p></blockquote>
<p>The FCC fines included $80.1 million for T-Mobile and $12.2 million for Sprint. T-Mobile, which bought Sprint in 2020, reported service revenue of $17.4 billion and net income of $3.2 billion in the <a href="https://s29.q4cdn.com/310188824/files/doc_financials/2025/q2/Q2-2025-Earnings-Release-vFinal.pdf">most recent quarter</a>.</p>
<p>Although the FCC first <a href="https://arstechnica.com/tech-policy/2020/02/fcc-issues-wrist-slap-fines-to-carriers-that-sold-your-phone-location-data/">proposed the fines</a> in 2020, under Republican Chairman Ajit Pai, the 2024 vote to finalize the penalties was 3-2, with dissents from Republicans Brendan Carr and Nathan Simington. Carr is now chairman of the FCC.</p>
<p>T-Mobile told Ars today that it is "currently reviewing the court's action" but did not provide further comment. The carrier could seek an <em>en banc</em> review in front of all the appeals court's justices, or ask the Supreme Court to review the case. Meanwhile, AT&amp;T is challenging its fine in the 5th Circuit appeals court while Verizon is challenging in the 2nd Circuit.</p>
<p>AT&amp;T and Verizon were fined $57.3 million and $46.9 million, respectively. The FCC last year said the major carriers disclosed customer location information "without customer consent or other legal authorization to a Missouri Sheriff through a 'location-finding service' operated by Securus, a provider of communications services to correctional facilities, to track the location of numerous individuals."</p>

          
                  </div>
                    
        
          
    
    <div>
          
          

<h2>Carriers gave up right to jury trial, court rules</h2>
<p>AT&amp;T and Verizon <a href="https://arstechnica.com/tech-policy/2024/11/verizon-att-tell-courts-fcc-cant-punish-us-for-selling-user-location-data/">made similar arguments</a> about their right to a jury trial and cited the Supreme Court's June 2024 <a href="https://www.supremecourt.gov/opinions/23pdf/22-859_1924.pdf">ruling</a> in <em>Securities and Exchange Commission v. Jarkesy</em>. That ruling held that "when the SEC seeks civil penalties against a defendant for securities fraud, the Seventh Amendment entitles the defendant to a jury trial."</p>
<p>In the ruling against T-Mobile, the DC Circuit panel held that the carriers gave up any potential right to a jury trial when they "chose to pay their fines and to seek direct review in this court... The Carriers may not now complain that they were denied a right they voluntarily surrendered."</p>
<p>The carriers could have obtained a jury trial if they simply failed to pay the fines and waited to be served with a complaint, the ruling said. "Even if the Seventh Amendment applies, it was not violated because the Carriers had the opportunity to put their case before a jury," judges wrote.</p>
<p>The carriers <a href="https://storage.courtlistener.com/recap/gov.uscourts.cadc.41085/gov.uscourts.cadc.41085.01208711724.0.pdf">argued</a> that they didn't really have a right to a jury trial because the FCC orders "are final agency actions with real-world effects; indeed, the FCC acknowledges that it may use its untested factual findings in license-renewal decisions and penalty calculations."</p>
<p>The carriers argued that in some jurisdictions where the government could bring a collection action, "the Companies would not have the right to raise factual and legal challenges to the Orders. The possibility of a government-initiated collection action therefore does not satisfy the Seventh Amendment and Article III."</p>
<p>The appeals court panel responded that "this court has not adopted the rule that troubles" the carriers. If "the government brought an enforcement action in a jurisdiction with the unfavorable rule, the Carriers could have raised as-applied challenges in those proceedings. But we cannot 'invalidate legislation on the basis of... hypothetical... situations not before' us," judges wrote.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<h2>Carriers quibbled over definition of sensitive data</h2>
<p>The carriers also argued that the device-location information, which is "passively generated when a mobile device pings cell towers to support both voice and data services," does not qualify as Customer Proprietary Network Information (CPNI) under the law. The carriers said the law "covers information relating to the 'location... of use' of a telecommunications service," and claimed that only call location information fits that description.</p>
<p>Judges faulted T-Mobile and Sprint for relying on "strained interpretations" of the statute. "We begin with the text. The Communications Act refers to the 'location... of a telecommunications service, not the location of a voice call... Recall that cell phones connect periodically to cell towers, and that is what enables the devices to send and receive calls at any moment," the ruling said.</p>
<p>In the judges' view, "a customer 'uses' a telecommunications service whenever his or her device connects to the carrier's network for the purpose of being able to send and receive calls. And the Carriers' reading therefore does not narrow 'location... of use' to times when the customer is actively on a voice call."</p>
<p>Judges also weren't persuaded by the argument that the fines were too large. "The Carriers note that the Commission previously had imposed such large fines only in cases involving fraud or intentional efforts to mislead consumers, and they are guilty of neither form of misconduct," the ruling said. "The Commission reasonably explained, however, that the Carriers' conduct was 'egregious': Even after the Securus breach exposed Sprint and T-Mobile's safeguards as inadequate, both carriers continued to sell access to CLI under a broken system."</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/jon-brodkin/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2016/05/j.brodkin-11_2.jpg" alt="Photo of Jon Brodkin"></a></p>
  </div>

  <div>
    

    <p>
      Jon is a Senior IT Reporter for Ars Technica. He covers the telecom industry, Federal Communications Commission rulemakings, broadband consumer affairs, court cases, and government regulation of the tech industry.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/tech-policy/2025/08/t-mobile-claimed-selling-location-data-without-consent-is-legal-judges-disagree/#comments" title="35 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    35 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/gadgets/2025/08/ars-technica-system-guide-back-to-pc-building-for-back-to-school/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/IMG_2568-768x432.jpeg" alt="Listing image for first story in Most Read: Ars Technica System Guide: Five sample PC builds, from $500 to $5,000" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: We started building an AI dev tool but it turned into a Sims-style game (128 pts)]]></title>
            <link>https://www.youtube.com/watch?v=sRPnX_f2V_c</link>
            <guid>44943986</guid>
            <pubDate>Mon, 18 Aug 2025 18:51:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=sRPnX_f2V_c">https://www.youtube.com/watch?v=sRPnX_f2V_c</a>, See on <a href="https://news.ycombinator.com/item?id=44943986">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[How much do electric car batteries degrade? (119 pts)]]></title>
            <link>https://www.sustainabilitybynumbers.com/p/electric-car-battery-degradation</link>
            <guid>44943420</guid>
            <pubDate>Mon, 18 Aug 2025 17:53:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sustainabilitybynumbers.com/p/electric-car-battery-degradation">https://www.sustainabilitybynumbers.com/p/electric-car-battery-degradation</a>, See on <a href="https://news.ycombinator.com/item?id=44943420">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>It‚Äôs always the battery in my mobile phone that gives up on me first. After just a few years, it can barely make it through the day without getting another charge.</p><p>Most electric cars have the same types of batteries ‚Äî usually lithium-ion ‚Äî so the assumption is that they degrade just as quickly. This is a fairly common fear for people considering a new EV: ‚ÄúWon‚Äôt the battery need to be replaced after a few years?‚Äù. And I think it‚Äôs even more prominent in the second-hand market: ‚ÄúOh, I‚Äôd never buy a second-hand battery!‚Äù.</p><p>But the types and structures of electric car and mobile phone batteries are not the same. Car batteries are designed to last far longer.</p><p><span>A few months ago, I </span><a href="https://www.sustainabilitybynumbers.com/p/used-electric-car-costs" rel="">wrote about</a><span> the fact that the electric versions of many cars were now cheaper than their petrol equivalents in the second-hand market. Most of the responses to that article suggested that battery degradation is the reason why. But looking at the data, I‚Äôd say that it‚Äôs more likely to be the </span><em>perception</em><span> of battery degradation that pushes the value down, not the actual degradation in reality. Pessimism about battery longevity is giving us all cheaper second-hand EVs, which is a nice perk for now, but not great if we want to see a widespread shift from petrol to electric. This is not the </span><em>only</em><span> reason I think used EVs are now undercutting the price of petrol cars; the fact that so many newer (and better) models are coming on the market means that many models ‚Äî even just a few years old ‚Äî lose some of their comparative value. Again, I think this is a pretty good thing (at least if you‚Äôre a prospective buyer).</span></p><p>Anyway, let‚Äôs take a look at battery degradation: why it happens, how much EV batteries degrade, and how to reduce it.</p><p>Two types of degradation happen in an electric car battery:</p><p><span>First, </span><strong>calendar aging</strong><span>, which is when the battery loses capacity over time, even when the car isn‚Äôt being used. So if you were to have an electric car, and not touch it for a year, the battery would still experience small amounts of degradation.</span></p><p>Why does this happen? Lithium-ion batteries have a thin layer called the SEI (Solid Electrolyte Interphase) that forms on the anode surface. This slowly grows thicker over time, and as it thickens, it uses lithium and reduces usable capacity. Calendar aging tends to be small ‚Äî typically around 1% to 2% per year ‚Äî but can be higher in very hot climates.</p><p><span>Second, we have </span><strong>cyclical aging</strong><span>, which is the degradation that happens when batteries charge and discharge. Every time a battery recharges or discharges, lithium ions move in and out of the electrodes. The mechanical stress of this process gradually creates structural changes in the electrodes, which reduce their capacity. This charge cycling can also grow the SEI layer (which I mentioned above), reducing usable capacity.</span></p><p><span>Before we quantify </span><em>how big</em><span> this effect is, it‚Äôs interesting to look at how these processes work over the life of a battery. In the chart below, you can see battery retention measured across a large cohort of Teslas up to 200,000 miles (that‚Äôs already telling us something about how big the effect is).</span></p><p>But what‚Äôs interesting is that degradation tends to happen quickest in the first 20,000 miles or so. This is because initial lithium salts react with other materials and start building that SEI layer we discussed earlier. After this initial drop, degradation is fairly slow and linear.</p><p>Of course, this fact might be one of the explanations why even fairly low-mileage electric cars quickly lose a lot of value once they‚Äôve been driven. As soon as you get on the road, you‚Äôre entering the steepest part of the decline.</p><p>What‚Äôs missing, though, is the context that the overall drop in capacity is still small ‚Äî probably around 3% to 5% within 25,000 miles ‚Äî and degradation won‚Äôt continue at this rate. So if you buy a second-hand electric car that‚Äôs done 20,000 miles, it‚Äôs not going to degrade at the same pace that it was.</p><p>We‚Äôve now had enough electric cars on the road - and for long enough - to have a good idea of how the battery holds up over time.</p><p>Here we‚Äôll focus on a metric used to capture the battery‚Äôs ‚ÄúState of Health‚Äù (SoH). It‚Äôs what percentage of a battery‚Äôs initial capacity is still usable after a given number of miles or years.</p><p>Let‚Äôs start with the results of the huge Tesla cohort that we looked at above. In its 2023 Impact Report, Tesla reported that after 200,000 miles of use, the batteries in a Model 3 and Model Y had lost just 15% of their capacity, on average. For the Model S and X, it was just 12%.</p><p>That‚Äôs not bad, given that most cars are scrapped somewhere in the 150,000 to 200,000 miles range. At that point, a Tesla will have more than 80% of its initial capacity, and in some cases, even more. So people will probably give up their car, well, well before the battery gets close to becoming a burden.</p><p>What about other car models?</p><p><span>The very early Nissan LEAFs ‚Äî one of the first electric cars to break through ‚Äî did have real degradation problems, especially in hotter climates. They used a </span><em>passive</em><span> thermal management system ‚Äî in other words, there was no </span><em>active</em><span> cooling of the battery ‚Äî which led to faster degradation. Many of these batteries would need to be replaced.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-171243707" href="https://www.sustainabilitybynumbers.com/p/electric-car-battery-degradation#footnote-1-171243707" target="_self" rel="">1</a></span></p><p>But the early Nissan LEAFs were a vital lesson. Most manufacturers do not experience the same issues today. Manufacturers such as Tesla, GM, Kia and Volkswagen using liquid cooling systems to prevent this. </p><p><span>A large study of 7,000 cars </span><a href="https://www.p3-group.com/en/p3-updates/battery-aging-in-practice/" rel="">by AVILOO</a><span> ‚Äî some of which had done as much as 300,000 kilometres (almost 200,000 miles) ‚Äî found that the majority still had more than 80% of battery capacity, even at these high-mileage levels.</span></p><p><span>In </span><a href="https://www.recurrentauto.com/research/lessons-in-electric-car-battery-health" rel="">another study</a><span> across 15,000 cars ‚Äî which had collectively clocked up 250 million miles ‚Äî just 1.5% had needed a battery replacement </span><em>for any reason</em><span>, so the share that needed one due to degradation was probably even lower.</span></p><p>I would expect that many cars with far more than 200,000 miles would still have a fairly healthy battery left. But not many cars get to this driving distance, and I‚Äôd be a bit cautious about survivorship bias if we had a very small sample size. This is also something to be aware of, even when talking about 200,000-mile vehicles, although here the sample sizes are not that small.</p><p>When looking at degradation rates, I‚Äôd recommend looking at real-world data rather than some of the earlier models.</p><p>A common model ‚Äî the P3 SoH ‚Äî tends to overestimate degradation rates and is, therefore, too pessimistic about how long EV batteries last. It‚Äôs based on battery cell data generated from laboratory tests (without a battery management system), but these tend not to be a great match for real driving conditions.</p><p>In the chart, you can see the P3 SoH predicted line in red, and the Aviloo trend line - based on real car data - in blue. Some cars do degrade as quickly as the P3 model would suggest after 200,000+ miles, but these tend to be the poorer-performing outliers, rather than the typical experience.</p><p><span>A </span><a href="https://www.nature.com/articles/s41560-024-01675-8" rel="">study published</a><span> in </span><em>Nature Energy</em><span> also found that under ‚Äúreal‚Äù driving conditions, batteries lasted around 38% longer compared to laboratory tests.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-171243707" href="https://www.sustainabilitybynumbers.com/p/electric-car-battery-degradation#footnote-2-171243707" target="_self" rel="">2</a></span></p><p>If you want to understand car batteries, we now have more than enough data from actual drivers and experiences on the road. Just look at that.</p><p><span>The final reason to have confidence in the performance of batteries over time is that manufacturers clearly have confidence. Most now offer battery warranties: if your battery degrades more than this within a given mileage or timescale, then they‚Äôll repair or replace it for you.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-171243707" href="https://www.sustainabilitybynumbers.com/p/electric-car-battery-degradation#footnote-3-171243707" target="_self" rel="">3</a></span></p><p><span>Most manufacturers offer a warranty somewhere in the range of 8 to 10 years, and 100,000 miles. That usually means that if your battery is below 70% health within </span><em>either</em><span> 8 years or 100,000 miles, they‚Äôll replace it for you.</span></p><p>Some are going even further. Some Mercedes models offer over 150,000 miles and 10 years. The Lexus UX300e offers over 600,000 miles. They clearly have a huge amount of confidence that by the end of your car‚Äôs life (which is going to be at far less than 600,000 miles), the battery will still have well over 70% of capacity.</p><p><span>The point is </span><em>not</em><span> that battery degradation is not an issue at all. Knowing that your car‚Äôs capacity could drop by 10% to 20% over its lifetime is important and useful to know. I said the same </span><a href="https://www.sustainabilitybynumbers.com/p/electric-cars-cold" rel="">in my article</a><span> on how a car‚Äôs range changes in cold temperatures: they do lose a bit and I think it‚Äôs important for buyers and drivers to know that up-front. The point is that for most drivers, it‚Äôs not a dealbreaker. Most are going to manage fine, even with this drop.</span></p><p>But it‚Äôs also important to know what things we can do to protect the battery, and slow this degradation as much as possible. Some of these things will not be new to most of you. But here‚Äôs a relatively uncontroversial list of things that are recommended:</p><ul><li><p><strong>Avoid extremely high or low temperatures</strong><span>. This tends to increase both cyclical and calendar aging. Try to keep it out of direct heat if you can. If you can find a model with a heat pump, this is useful to reduce degradation from cold charging.</span></p></li><li><p><strong>Avoid extreme ‚Äústate-of-charge‚Äù</strong><span>. Leaving the battery sitting with more than 80% or less than 10% of charge can accelerate calendar aging.</span></p></li><li><p><strong>Don‚Äôt fast charge all the time</strong><span>. Fast charging can increase degradation rates, so only use it when necessary. There are examples of EVs in taxi fleets ‚Äî which relied heavily on fast-charging ‚Äî where batteries needed to be replaced.</span></p></li></ul><p>Finally, it‚Äôs worth noting that battery designs and chemistries are getting better every day. From here on out, this is as bad as things are going to get. The longevity of the batteries that went into cars a decade ago ‚Äî and are now reaching 200,000 miles or the end of their lives ‚Äî is worse than that of the ones going into cars today.</p><p>As I‚Äôve previously said about the emissions associated with an electric car, things are only going to get better.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Left to Right Programming (355 pts)]]></title>
            <link>https://graic.net/p/left-to-right-programming</link>
            <guid>44942936</guid>
            <pubDate>Mon, 18 Aug 2025 17:08:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://graic.net/p/left-to-right-programming">https://graic.net/p/left-to-right-programming</a>, See on <a href="https://news.ycombinator.com/item?id=44942936">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><!--[--><sub>2025-08-17</sub>  <h3>Programs Should Be Valid as They Are Typed</h3> <hr><!--]--> <!--[!--><!--]--> <!----><p>I don‚Äôt like Python‚Äôs list comprehensions:</p> <pre><!----><code>text <span>=</span> <span>"apple banana cherry\ndog emu fox"</span>
words_on_lines <span>=</span> <span>[</span>line<span>.</span>split<span>(</span><span>)</span> <span>for</span> line <span>in</span> text<span>.</span>splitlines<span>(</span><span>)</span><span>]</span></code><!----></pre> <p>Don‚Äôt get me wrong, declarative programming is good. However, this syntax has poor ergonomics. Your editor can‚Äôt help you out as you write it. To see what I mean, lets walk through typing this code.</p> <pre><!----><code>words_on_lines <span>=</span> <span>[</span>l</code><!----></pre> <p>Ideally, your editor would be to autocomplete <code>line</code> here. Your editor can‚Äôt do this because <code>line</code> hasn‚Äôt been declared yet.</p> <pre><!----><code>words_on_lines <span>=</span> <span>[</span>line<span>.</span>sp</code><!----></pre> <p>Here, our editor knows we want to access some property of <code>line</code>, but since it doesn‚Äôt know the type of <code>line</code>, it can‚Äôt make any useful suggestions. Should our editor flag <code>line</code> as a non-existent variable? For all it knows, we might have meant to refer to some existing <code>lime</code> variable.</p> <pre><!----><code>words_on_lines <span>=</span> <span>[</span>line<span>.</span>split<span>(</span><span>)</span> <span>for</span> line <span>in</span></code><!----></pre> <p>Okay, now we know that <code>line</code> is the variable we‚Äôre iterating over. Is <code>split()</code> a method that exists for <code>line</code>? Who knows!</p> <pre><!----><code>words_on_lines <span>=</span> <span>[</span>line<span>.</span>split<span>(</span><span>)</span> <span>for</span> line <span>in</span> text<span>.</span>splitlines<span>(</span><span>)</span><span>]</span></code><!----></pre> <p>Ah! now we know the type of <code>line</code> and can validate the call to <code>split()</code>.
Notice that since <code>text</code> had already been declared, our editor is able to autocomplete <code>splitlines()</code>.</p> <p>This sucked! If we didn‚Äôt know what the <code>split()</code> function was called and wanted some help from our editor, we‚Äôd have to write</p> <pre><!----><code>words_on_lines <span>=</span> <span>[</span>_ <span>for</span> line <span>in</span> text<span>.</span>splitlines<span>(</span><span>)</span><span>]</span></code><!----></pre> <p>and go back to the <code>_</code> to get autocomplete on <code>line.sp</code></p> <!----> <hr> <!----> <!----> <p>You deserve better than this.</p> <p>To see what I mean, lets look at a Rust example that does it <!--#s1--><span> <label for="s1-footnote">better. <span>‚Ä†</span><span id="slot">[<!---->The most elegant solution here is Haskell‚Äôs <code>map words $ lines text</code> but that breaks all the principles I‚Äôm arguing for.<!---->]</span></label></span><!----></p> <pre><!----><code><span>let</span> text <span>=</span> <span>"apple banana cherry\ndog emu fox"</span><span>;</span>
<span>let</span> words_on_lines <span>=</span> text<span>.</span><span>lines</span><span>(</span><span>)</span><span>.</span><span>map</span><span>(</span><span><span>|</span>line<span>|</span></span> line<span>.</span><span>split_whitespace</span><span>(</span><span>)</span><span>)</span><span>;</span></code><!----></pre> <p>If you aren‚Äôt familiar with Rust syntax, <code>|argument| result</code> is an anonymous function equivilent to <code>function myfunction(argument) { return result; }</code></p> <p>Here, your program is constructed left to right. The first time you type <code>line</code> is the declaration of the variable. as soon as you type <code>line.</code> your editor is able to give you suggestions of <!--#s2--><span> <label for="s2-footnote">possible methods. <span>‚Ä†</span><span id="slot">[<!---->In fact, I didn‚Äôt know that Rust had a <code>split_whitespace</code> function until it popped up as I was typing this example.<!---->]</span></label></span><!----></p> <p>This is much more pleasent. Since the program is always in a somehwat valid state as you type it, your editor is able to guide you towards the <a href="https://blog.codinghorror.com/falling-into-the-pit-of-success/" rel="nofollow">Pit of Success</a>.</p> <!----> <hr> <!----> <!----> <p>There‚Äôs a principle in design called <a href="https://en.wikipedia.org/wiki/Progressive_disclosure" rel="nofollow">progressive disclosure</a>. The user should only be exposed to as much complexity as is neccessary to complete a task.
Additionally, complexity should naturally surface itself as it is relevant to the user.
You shouldn‚Äôt have to choose a font family and size before you start typing into Word, and options to change text wrapping around images should appear when you add an image.</p> <p>In C, you can‚Äôt have methods on structs. This means that any function that could be <code>myStruct.function(args)</code> has to be <code>function(myStruct, args)</code>.</p> <p>Suppose you have a <code>FILE *file</code> and you want to get it‚Äôs contents.
Ideally, you‚Äôd be able to type <code>file.</code> and see a list of every function that is primarily concerned with files.
From there you could pick <code>read</code> and get on with your day.</p> <p>Instead, you must know that functions releated to <code>FILE *</code> tend to start with <code>f</code>, and when you type <code>f</code> the best your editor can do is show you all functions ever written that start with an <code>f</code>.
From there you can eventually find <code>fread</code>, but you have no confidence that it was the best choice. Maybe there was a more efficient <code>read_lines</code> function that does exactly what you want, but you‚Äôll never discover it by accident.</p> <p>In a more ideal language, you‚Äôd see that a <code>close</code> method exists while you‚Äôre typing <code>file.read</code>. This gives you a hint that you need to close your file when you‚Äôre done with it. You naturally came accross this information right as it became relevant to you. In C, you have to know ahead of time that <code>fclose</code> is a function that you‚Äôll need to call once you‚Äôre done with the file.</p> <!----> <hr> <!----> <!----> <p>C is not the only language that has this problem. Python has plenty of examples too. Consider the following Python and JavaScript snippets:</p> <pre><!----><code><span># Python</span>
text <span>=</span> <span>"lorem ipsum dolor sit amet"</span>
word_lengths <span>=</span> <span>map</span><span>(</span><span>len</span><span>,</span> text<span>.</span>split<span>(</span><span>)</span><span>)</span></code><!----></pre> <pre><!----><code><span>// JavaScript</span>
text <span>=</span> <span>"lorem ipsum dolor sit amet"</span>
wordLengths <span>=</span> text<span>.</span><span>split</span><span>(</span><span>" "</span><span>)</span><span>.</span><span>map</span><span>(</span><span>word</span> <span>=&gt;</span> word<span>.</span>length<span>)</span></code><!----></pre> <p>While Python gets some points for using a <!--#s3--><span> <label for="s3-footnote">first-class function <span>‚Ä†</span><span id="slot">[<!---->Haskell, of course, solos with <code>map len $ words text</code><!---->]</span></label></span><!---->, the functions are not discoverable. Is string length <code>len</code>, <code>length</code>, <code>size</code>, <code>count</code>, <code>num</code>, or <!--#s4--><span> <label for="s4-footnote"># <span>‚Ä†</span><span id="slot">[<!---->It is in Lua! I‚Äôve seen all of these names used at some point<!---->]</span></label></span><!---->? Is there even a global function for length? You won‚Äôt know until you try all of them.</p> <p>In the JavaScript version, you see length as soon as you type <code>word.l</code>. There is less guesswork for what the function is named. The same is true for the <code>map</code>. When you type <code>.map</code>, you know that this function is going to work with the data you have. You aren‚Äôt going to get some weird error because the <code>map</code> function actually expected some other type, or because your language actually calls this function <!--#s5--><span> <label for="s5-footnote">select <span>‚Ä†</span><span id="slot">[<!---->As it is in C# LINQ<!---->]</span></label></span><!---->.</p> <!----> <hr> <!----> <!----> <p>While the Python code in the previous example is still readable, it gets worse as the complexity of the logic increases. Consider the following code that was part of <a href="https://github.com/Graicc/advent-of-code-2024/blob/0d7bf0f4f05489f0b5a09255fde47370084066e3/day_2/aoc2.py#L9" rel="nofollow">my 2024 Advent of Code solutions</a>.</p> <pre><!----><code><span>len</span><span>(</span><span>list</span><span>(</span><span>filter</span><span>(</span><span>lambda</span> line<span>:</span> <span>all</span><span>(</span><span>[</span><span>abs</span><span>(</span>x<span>)</span> <span>&gt;=</span> <span>1</span> <span>and</span> <span>abs</span><span>(</span>x<span>)</span> <span>&lt;=</span> <span>3</span> <span>for</span> x <span>in</span> line<span>]</span><span>)</span> <span>and</span> <span>(</span><span>all</span><span>(</span><span>[</span>x <span>&gt;</span> <span>0</span> <span>for</span> x <span>in</span> line<span>]</span><span>)</span> <span>or</span> <span>all</span><span>(</span><span>[</span>x <span>&lt;</span> <span>0</span> <span>for</span> x <span>in</span> line<span>]</span><span>)</span><span>)</span><span>,</span> diffs<span>)</span><span>)</span><span>)</span></code><!----></pre> <p>Yikes. You have to jump back and forth between the start and end of the line to figure out what‚Äôs going on. ‚ÄúOkay so we have the length of a list of some filter which takes this lambda‚Ä¶ is it both of these conditions or just one? Wait which parenthesis does this go with‚Ä¶‚Äù</p> <p>In JavaScript:</p> <pre><!----><code>diffs<span>.</span><span>filter</span><span>(</span><span>line</span> <span>=&gt;</span> 
    line<span>.</span><span>every</span><span>(</span><span>x</span> <span>=&gt;</span> Math<span>.</span><span>abs</span><span>(</span>x<span>)</span> <span>&gt;=</span> <span>1</span> <span>&amp;&amp;</span> Math<span>.</span><span>abs</span><span>(</span>x<span>)</span> <span>&lt;=</span> <span>3</span><span>)</span> <span>&amp;&amp;</span>
    <span>(</span>line<span>.</span><span>every</span><span>(</span><span>x</span> <span>=&gt;</span> x <span>&gt;</span> <span>0</span><span>)</span> <span>||</span> line<span>.</span><span>every</span><span>(</span><span>x</span> <span>=&gt;</span> x <span>&lt;</span> <span>0</span><span>)</span><span>)</span>
<span>)</span><span>.</span>length<span>;</span></code><!----></pre> <p>Ah, okay. We have some list of <code>diffs</code>, that we filter down based on two conditons, and then we return the number that pass. The logic of the program can be read from left to right!</p> <!----> <hr> <!----> <!----> <p>All of these examples illustrate a common principle:</p> <h2><center>Programs should be valid as they are typed.</center></h2> <p>When you‚Äôve typed <code>text</code>, the program is valid.
When you‚Äôve typed <code>text.split(" ")</code>, the program is valid.
When you‚Äôve typed <code>text.split(" ").map(word =&gt; word.length)</code>, the program is valid.
Since the program is valid as you build it up, your editor is able to help you out. If you had a REPL, you could even see the result as you type your program out.</p> <p>Make good APIs!</p><!----></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Whispering ‚Äì Open-source, local-first dictation you can trust (436 pts)]]></title>
            <link>https://github.com/epicenter-so/epicenter/tree/main/apps/whispering</link>
            <guid>44942731</guid>
            <pubDate>Mon, 18 Aug 2025 16:52:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/epicenter-so/epicenter/tree/main/apps/whispering">https://github.com/epicenter-so/epicenter/tree/main/apps/whispering</a>, See on <a href="https://news.ycombinator.com/item?id=44942731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>


                <li>
      

      <div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
          <p>
            GitHub Copilot
          </p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_spark&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_spark_link_product_navbar&quot;}" href="https://github.com/features/spark">
      
      <div>
          <p>
            GitHub Spark
              <span>
                New
              </span>
          </p><p>
        Build and deploy intelligent apps
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_product_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
          <p>
            GitHub Models
              <span>
                New
              </span>
          </p><p>
        Manage and compare prompts
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
          <p>
            GitHub Advanced Security
          </p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
          <p>
            Actions
          </p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    
                </ul>
              </div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
          <p>
            Codespaces
          </p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
          <p>
            Issues
          </p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
          <p>
            Code Review
          </p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
          <p>
            Discussions
          </p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
          <p>
            Code Search
          </p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
          

      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      

      <div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
          <p>
            GitHub Sponsors
          </p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
          <p>
            The ReadME Project
          </p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      

      <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
          <p>
            Enterprise platform
          </p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:epicenter-so/epicenter" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="4luAhuAdn-jJh95P57GWAOb_XLXg0xHodbnDFC953UjFmhDbdBwUMGTTlwUQXmyYZuE4DDsrkLx-Qd9nI9BdTQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="epicenter-so/epicenter" data-current-org="epicenter-so" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Ffiles%2Fdisambiguate&amp;source=header-repo&amp;source_repo=epicenter-so%2Fepicenter" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/epicenter-so/epicenter/tree/main/apps/whispering&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="f56bbfb881377e09d87bc947754ac94749d4b634b2d83bbe06146636fcaf18ed" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/files/disambiguate;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-52dcc01b-ceec-4b40-ba1b-bb1fdf6b9f02" for="icon-button-d749c8c4-8f0b-4993-a32c-298260a616f4" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.c82a4db79200850fb016.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My Retro TVs (139 pts)]]></title>
            <link>https://www.myretrotvs.com/</link>
            <guid>44942602</guid>
            <pubDate>Mon, 18 Aug 2025 16:40:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.myretrotvs.com/">https://www.myretrotvs.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44942602">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Anna's Archive: An Update from the Team (929 pts)]]></title>
            <link>https://annas-archive.org/blog/an-update-from-the-team.html</link>
            <guid>44942501</guid>
            <pubDate>Mon, 18 Aug 2025 16:31:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://annas-archive.org/blog/an-update-from-the-team.html">https://annas-archive.org/blog/an-update-from-the-team.html</a>, See on <a href="https://news.ycombinator.com/item?id=44942501">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  <p>
    annas-archive.li/blog, 2025-08-17
  </p>

  

  <p>We are still alive and kicking. In recent weeks we‚Äôve seen increased attacks on our mission. We are taking steps to harden our infrastructure and operational security. The work of securing humanity‚Äôs legacy is worth fighting for.</p>

  <p>Since we started in 2022, we have liberated tens of millions of books, scientific articles, magazines, newspapers, and more. These are now forever protected from destruction by natural disasters, wars, budget cuts, and other catastrophes, thanks to everyone who helps with torrenting.</p>

  <p>Anna‚Äôs Archive itself has organized some of the largest scrapes: we acquired tens of millions of files from IA Controlled Digital Lending, HathiTrust, DuXiu, and many more.</p>

  <p>We have also scraped and published the largest book metadata collections in history: WorldCat, Google Books, and others. With this we‚Äôll be able to identify which books are still missing from our collections, and prioritize saving the rarest ones.</p>

  <p>Much thanks to all of our volunteers for making these projects happen.</p>

  <p>We‚Äôve forged some incredible partnerships. We‚Äôve partnered with two LibGen forks, STC/Nexus, Z-Library. We‚Äôve secured tens of millions additional files through these partnerships. And they are helping the mission by mirroring our files.</p>

  <p>Unfortunately we have seen the disappearance of one of the LibGen forks. We don‚Äôt have further information about what happened there, but are saddened by this development.</p>

  <p>There is a new entrant: WeLib. They appear to have mirrored most of our collection, and use a fork of our codebase. We have copied some of their user interface improvements, and are grateful for that push. Sadly, we are not seeing them share any new collections, nor share their codebase improvements. Since they haven‚Äôt shown commitment to contributing back to the ecosystem, we advise extreme caution. <em>We recommend not using them.</em></p>

  <p>In the meantime, we have some exciting projects in the works. We have hundreds of terabytes in new collections sitting on our servers, waiting to be processed. If you‚Äôre at all interested in helping out, feel free to check out our Volunteering and Donate pages. We run all of this on a minimal budget, so any help is greatly appreciated.</p>

  <p>Keep fighting.</p>

  <p>- Anna and the team (<a href="https://www.reddit.com/r/Annas_Archive/" rel="noopener noreferrer nofollow" target="_blank">Reddit</a>)</p>
     </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Who Invented Backpropagation? (186 pts)]]></title>
            <link>https://people.idsia.ch/~juergen/who-invented-backpropagation.html</link>
            <guid>44941963</guid>
            <pubDate>Mon, 18 Aug 2025 15:50:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://people.idsia.ch/~juergen/who-invented-backpropagation.html">https://people.idsia.ch/~juergen/who-invented-backpropagation.html</a>, See on <a href="https://news.ycombinator.com/item?id=44941963">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="754">
<tbody><tr>
<td>
<a href="https://people.idsia.ch/~juergen/deep-learning-history.html"><img src="https://people.idsia.ch/~juergen/backprop754x466seppo.png" alt="Who Invented Backpropagation?"></a>



<p>
<table><colgroup><col><col></colgroup><tbody><tr><td><a href="https://people.idsia.ch/~juergen/">J√ºrgen Schmidhuber</a> (2014, updated <a href="https://people.idsia.ch/~juergen/who-invented-backpropagation-1970.html">2020</a>, <a href="https://people.idsia.ch/~juergen/who-invented-backpropagation-2022.html">2022</a>, 2025)
<br>Pronounce: <span color="#2266aa">You_again Shmidhoobuh</span>
<br>See also this <a href="https://www.linkedin.com/feed/update/urn:li:activity:7354090939369283585/">LinkedIn post (2025)</a>
</td> <td>
<a href="https://people.idsia.ch/~juergen/blog.html">AI Blog</a>
<br><a href="https://twitter.com/SchmidhuberAI">@SchmidhuberAI</a> 
</td></tr></tbody></table>



</p>

<span size="4">






<p>Efficient backpropagation (BP) is central to the ongoing    
<a href="http://www.idsia.ch/~juergen/deeplearning.html">Neural Network (NN) ReNNaissance and "Deep Learning."</a> 
Who invented it? 

</p><p> BP's modern version (also called the reverse mode of automatic differentiation) was first published in 1970 
by Finnish master student <b>Seppo Linnainmaa</b> <a href="#BP1">[BP1]</a> <a href="#R7">[R7]</a>. <b>In 2020, we celebrated BP's half-century anniversary!</b>
A precursor of BP was published by Henry J. Kelley in 1960 
<a href="#BPA">[BPA]</a>‚Äîin 2020, we celebrated its 60-year anniversary.

</p><p>In the 2020s, it was still easy to find misleading accounts of BP's history <a href="#HIN">[HIN]</a><a href="#T22">[T22]</a><a href="#DLP">[DLP]</a><a href="#NOB">[NOB]</a>. I had a look at the original papers from the 1960s and 70s, and talked to BP pioneers. Here is a summary based on my  award-winning
<a href="https://people.idsia.ch/~juergen/deep-learning-overview.html">2014 survey</a> <a href="#DL1">[DL1]</a>
which includes most of the references mentioned below.

</p><p>The minimisation of errors through gradient descent (Cauchy 1847 <a href="#GD'">[GD']</a>, Hadamard, 1908 <a href="#GD''">[GD'']</a>) in the parameter space of complex, nonlinear, differentiable, multi-stage, NN-related systems has been discussed at least since the early 1960s,
e.g., Kelley (1960) <a href="#BPA">[BPA]</a>; Bryson (1961) <a href="#BPB">[BPB]</a>; Pontryagin et al. (1961); Dreyfus (1962) <a href="#BPC">[BPC]</a>; Wilkinson (1965); Tsypkin (1966) <a href="#GDa">[GDa-b]</a>; Amari (1967-68) <a href="#GD2">[GD2,GD2a]</a>; Bryson and Ho (1969); initially within the framework of Euler-LaGrange equations in the Calculus of Variations, e.g., Euler (1744). 

</p><p>Steepest descent in the weight space of such systems can be performed (Kelley, 1960 <a href="#BPA">[BPA]</a>; Bryson, 1961 <a href="#BPB">[BPB]</a>) by iterating the chain rule (<a href="https://people.idsia.ch/~juergen/leibniz-father-computer-science-375.html">Leibniz</a>, 1676 <a href="#LEI07">[LEI07-10]</a><a href="#DLH">[DLH]</a>; L'Hopital, 1696) in Dynamic Programming style (DP, e.g., Bellman, 1957 <a href="#BEL53">[BEL53]</a>). A simplified derivation (Dreyfus, 1962 <a href="#BPC">[BPC]</a>) of this backpropagation method uses only the <a href="https://people.idsia.ch/~juergen/leibniz-father-computer-science-375.html">Leibniz</a> chain rule  <a href="#LEI07">[LEI07]</a>.

</p><p>The systems of the 1960s were already efficient in the DP sense. However, they backpropagated derivative information through standard Jacobian matrix calculations from one "layer" to the previous one, without explicitly addressing either direct links across several layers or potential additional efficiency gains due to network sparsity. 

</p><p>Explicit, efficient error backpropagation (BP) in arbitrary, discrete, possibly sparsely connected, NN-like networks was first described in a 1970 master's thesis (Linnainmaa, 1970, 1976) <a href="#BP1">[BP1]</a><a href="#R7">[R7]</a>, albeit without reference to NNs. This kind of BP is also known as the <em>reverse mode of automatic differentiation</em> (e.g., Griewank, 2012 <a href="#BP5">[BP5]</a>), where the costs of forward activation spreading essentially equal the costs of backward derivative calculation. See early BP FORTRAN code (Linnainmaa, 1970) <a href="#BP1">[BP1]</a> and closely related but slightly later work by Ostrovskii et al. (1971) <a href="#BP1a">[BP1a]</a> (apparently the <em>first journal publication</em> on backpropagation). As of 2020, all modern software packages for NNs (such as Google's Tensorflow) are based on Linnainmaa's method of 1970.

</p><p>BP was soon explicitly used to minimize cost functions by adapting control parameters (weights) (Dreyfus, 1973). This was followed by some preliminary, NN-specific discussion (Werbos, 1974, section 5.5.1) and a computer program for automatically deriving and implementing BP in differentiable systems (Speelpenning, 1980).
The first NN-specific application of efficient BP as above was apparently described by  Werbos in 1982 <a href="#BP2">[BP2]</a> (but not yet in his 1974 thesis, as is sometimes claimed). 


</p><p> However, already in 1967, Amari suggested to train deep multilayer perceptrons (MLPs) with many layers in non-incremental end-to-end fashion from scratch by stochastic gradient descent (SGD) <a href="#GD1">[GD1]</a>, a method proposed in 1951 <a href="#STO51">[STO51-52]</a>.
Amari's implementation <a href="#GD2">[GD2,GD2a]</a> (with his student Saito) learned <em>internal representations</em> in a five layer MLP with two modifiable layers <a href="#DLH">[DLH]</a><a href="#NOB">[NOB]</a>, which was trained to classify
non-linearily separable pattern classes. Back then compute was billions of times more expensive than today. 

</p><p>Compare the first deep learning MLPs called GMDH networks (Ivakhnenko and Lapa, since 1965) whose layers are incrementally grown and trained by regression analysis <a href="#DEEP1">[DEEP1-2]</a><a href="#R8">[R8]</a><a href="#DLH">[DLH]</a><a href="#NOB">[NOB]</a>. These were  actually the first deep NNs that learned to create hierarchical, distributed, <em>internal representations</em> of incoming data. 

</p><p> Additional work on backpropagation was published  later (e.g., Parker, 1985; LeCun, 1985). 
By 1985, compute was about 1,000 times cheaper than in 1970 <a href="#BP1">[BP1]</a>, and 
the first desktop computers 
became accessible in wealthier academic labs. 
An experimental analysis of the known method <a href="#BP1">[BP1-2]</a> by Rumelhart et al. then demonstrated that backpropagation can yield useful internal representations in hidden layers of NNs <a href="#RUM">[RUM]</a>. At least for supervised learning, this tends to be more efficient than Amari's above-mentioned deep learning through the more general SGD method (1967), which learned useful internal representations in NNs about 2 decades earlier <a href="#GD1">[GD1-2a]</a>.


</p><p> Some ask: <em> "Isn't backpropagation just the chain rule of <a href="https://people.idsia.ch/~juergen/leibniz-father-computer-science-375.html">Leibniz</a> (1676) <a href="#LEI07">[LEI07-10]</a> &amp; L'Hopital (1696)?"</em>  No, it is the efficient way of applying the chain rule to big networks with differentiable nodes‚Äîsee <a href="https://people.idsia.ch/~juergen/scientific-integrity-turing-award-deep-learning.html#XII">Sec. XII</a> of <a href="#T22">[T22]</a><a href="#DLH">[DLH]</a>). (There are also many inefficient ways of  doing this.) It was not published until 1970 <a href="#BP1">[BP1]</a>.

</p><p> It took 4 decades until the backpropagation method of 1970 <a href="#BP1">[BP1-2]</a> got widely accepted as a training method for deep NNs. Before 2010, many thought that the training of NNs with many layers requires <a href="https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%2019">unsupervised pre-training</a>, a methodology introduced 
<a href="https://people.idsia.ch/~juergen/very-deep-learning-1991.html">by myself in 1991</a> <a href="#UN">[UN][UN0-3]</a>, and later championed by others (2006) <a href="#UN4">[UN4]</a>. In fact, it was claimed <a href="#VID1">[VID1]</a><a href="#DLP">[DLP]</a>  that "nobody in their right mind would ever suggest" to apply plain backpropagation to deep NNs. However, in 2010, our team with my outstanding Romanian 
postdoc Dan Ciresan 
<a href="https://people.idsia.ch/~juergen/2010-end-to-end-deep-learning-breakthrough.html">
showed that deep FNNs  
can be trained by plain backpropagation and do not at all require unsupervised
pre-training for important applications</a> <a href="#MLP1">[MLP1-3]</a><a href="#MOST">[MOST]</a>.

<a name="ack"></a>
</p><h2><hr>Acknowledgments <hr></h2>


<p><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a>Thanks to several backpropagation pioneers and expert reviewers for useful comments. Since science is about self-correction, let me know under <em>juergen@idsia.ch</em> if you can spot any remaining error. The contents of this article may be used for educational and non-commercial purposes, including articles for Wikipedia and similar sites. This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. 
</p><h2><hr>References (more in <a href="#DL1">[DL1]</a><a href="#DLH">[DLH]</a>)<hr></h2>


<p><a name="BEL53"></a>
[BEL53] R. Bellman. An introduction to the theory of dynamic programming. RAND Corp. Report, 1953


</p><p><a name="BP1"></a>
[BP1] S. Linnainmaa. The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master's Thesis (in Finnish), Univ. Helsinki, 1970.
<em>See chapters 6-7 and FORTRAN code on pages 58-60.</em>
<a href="http://www.idsia.ch/~juergen/linnainmaa1970thesis.pdf">PDF</a>.
See also  BIT 16, 146-160, 1976.
<a href="http://link.springer.com/article/10.1007%2FBF01931367">Link.</a>
<em>The first publication on "modern" backpropagation, also known as the reverse mode of automatic differentiation.</em>

</p><p><a name="BP1a"></a>
[BP1a] G. M. Ostrovskii, Y. M. Volin, W. W. Borisov (1971). Ueber die Berechnung von Ableitungen.
Wiss. Z. Tech. Hochschule fuer Chemie, 13:382‚Äì384.

</p><p><a name="BP2"></a>
[BP2] P. J. Werbos. Applications of advances in nonlinear sensitivity analysis. In R. Drenick, F. Kozin, (eds): System Modeling and Optimization: Proc. IFIP, 
Springer, 1982. 
<a href="http://werbos.com/Neural/SensitivityIFIPSeptember1981.pdf">PDF</a>.
<em>First application of backpropagation<sup><small><small><a href="#BP1">[BP1]</a></small></small></sup> to NNs (concretizing thoughts in his 1974 thesis).</em>

</p><p>
<a name="BP4"></a>
[BP4] J. Schmidhuber (<a href="https://people.idsia.ch/~juergen/blog.html">AI Blog</a>, 2014-). 
<a href="https://people.idsia.ch/~juergen/who-invented-backpropagation.html">Who invented backpropagation</a>? 



</p><p>
<a name="BP5"></a>
[BP5] 
A. Griewank (2012). Who invented the reverse mode of differentiation?
Documenta Mathematica, Extra Volume ISMP (2012): 389-400.

</p><p>
<a name="BP6"></a>
[BP6]
S. I. Amari (1977).
Neural Theory of Association and Concept Formation. 
Biological Cybernetics, vol. 26, p. 175-185, 1977.   
<em>See Section 3.1 on using gradient descent for learning in multilayer networks.</em>

</p><p><a name="BPA"></a>
[BPA]
H. J. Kelley.  Gradient Theory of Optimal Flight Paths. ARS Journal, Vol. 30, No. 10, pp. 947-954, 1960. 
<em>Precursor of modern <a href="https://people.idsia.ch/~juergen/who-invented-backpropagation.html">backpropagation</a>.<sup><small><small><a href="#BP1">[BP1-5]</a></small></small></sup></em>

</p><p><a name="BPB"></a>
[BPB]
A. E. Bryson. A gradient method for optimizing multi-stage allocation processes. Proc. Harvard Univ. Symposium on digital computers and their applications, 1961.

</p><p><a name="BPC"></a>
[BPC]
S. E. Dreyfus. The numerical solution of variational problems. Journal of Mathematical Analysis and Applications, 5(1): 30-45, 1962.




</p><p><a name="DEC"></a>
[DEC] J. Schmidhuber (<a href="https://people.idsia.ch/~juergen/blog.html">AI Blog</a>, 02/20/2020, updated 2025). <a href="https://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html">The 2010s: Our Decade of Deep Learning / Outlook on the 2020s</a>.  <em>The recent decade's most important developments and industrial applications based on our AI, with an outlook on the 2020s, also addressing privacy and data markets.</em>




</p><p><a name="DEEP1"></a>
[DEEP1]
Ivakhnenko, A. G. and Lapa, V. G. (1965). Cybernetic Predicting Devices. CCM Information Corporation. <em>First working Deep Learners with many layers, learning internal representations.</em>

</p><p><a name="DEEP1a"></a>
[DEEP1a]
Ivakhnenko, Alexey Grigorevich. The group method of data of handling; a rival of the method of stochastic approximation. Soviet Automatic Control 13 (1968): 43-55.

</p><p><a name="DEEP2"></a>
[DEEP2]
Ivakhnenko, A. G. (1971). Polynomial theory of complex systems. IEEE Transactions on Systems, Man and Cybernetics, (4):364-378.

</p><p>
<a name="DL1"></a>
[DL1] J. Schmidhuber, 2015. 
Deep learning in neural networks: An overview. Neural Networks, 61, 85-117. 
<a href="https://people.idsia.ch/~juergen/deep-learning-overview.html">More</a>.
<em>Got the first Best Paper Award ever issued by the journal Neural Networks, founded in 1988.</em>


</p><p><a name="DL2"></a>
[DL2] J. Schmidhuber, 2015. 
<a href="http://www.scholarpedia.org/article/Deep_Learning">Deep Learning</a>.
Scholarpedia, 10(11):32832.

</p><p><a name="DL3"></a>
[DL3] Y. LeCun, Y. Bengio, G. Hinton (2015). Deep Learning. Nature 521, 436-444.
<a href="https://www.nature.com/articles/nature14539">HTML</a>. 
<em>A "survey" of deep learning that does not mention the pioneering works of deep learning <a href="#T22">[T22]</a><a href="#DLP">[DLP]</a><a href="#NOB">[NOB]</a>.</em>

</p><p><a name="DL3a"></a>
[DL3a] Y. Bengio, Y. LeCun, G. Hinton (2021). Turing Lecture: Deep Learning for AI. Communications of the ACM, July 2021. <a href="https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltext">HTML</a>.
<a href="https://people.idsia.ch/~juergen/DLforAIjuly2021.html">Local copy</a> (HTML only).
<em>A "survey" of deep learning that does not mention the pioneering works of deep learning <a href="#DLP">[DLP]</a><a href="#NOB">[NOB]</a>.</em>


</p><p><a name="DLC"></a>
[DLC] J. Schmidhuber (<a href="https://people.idsia.ch/~juergen/blog.html">AI Blog</a>, June 2015).  
<a href="https://people.idsia.ch/~juergen/deep-learning-conspiracy.html">Critique of Paper</a> by self-proclaimed "Deep Learning Conspiracy" (Nature 521 p 436). 
<em>The inventor of an important method should get credit for inventing it. She may not always be the one who popularizes it. Then the popularizer should get credit for popularizing it (but not for inventing it). More: <a href="#T22">[T22]</a><a href="#DLP">[DLP]</a><a href="#NOB">[NOB]</a>.</em> 


</p><p><a name="DLH"></a>
[DLH]
J. Schmidhuber (<a href="https://people.idsia.ch/~juergen/blog.html">AI Blog</a>, 2022). 
<a href="https://people.idsia.ch/~juergen/deep-learning-history.html">Annotated History of Modern AI and Deep Learning</a>. Technical Report IDSIA-22-22, IDSIA, Lugano, Switzerland, 2022. 
Preprint <a href="https://arxiv.org/abs/2212.11279">arXiv:2212.11279</a>.
<a href="https://x.com/SchmidhuberAI/status/1606333832956973060">Tweet of 2022</a>.


</p><p><a name="DLP"></a>
[DLP] 
J. Schmidhuber (<a href="https://people.idsia.ch/~juergen/blog.html">AI Blog</a>, 2023). 
<a href="https://people.idsia.ch/~juergen/ai-priority-disputes.html"> How 3 Turing awardees republished key methods and ideas whose creators they failed to credit.</a> Technical Report IDSIA-23-23, Swiss AI Lab IDSIA, 14 Dec 2023.
<a href="https://x.com/SchmidhuberAI/status/1735313711240253567">Tweet of 2023</a>.


</p><p>
<a name="GD'"></a>
[GD']
C. Lemarechal. Cauchy and the Gradient Method. Doc Math Extra, pp. 251-254, 2012.

</p><p>
<a name="GD''"></a>
[GD'']
J. Hadamard. Memoire sur le probleme d'analyse relatif a Vequilibre des plaques elastiques encastrees. Memoires presentes par divers savants estrangers √† l'Academie des Sciences de l'Institut de France, 33, 1908.


</p><p>
<a name="GDa"></a>
[GDa]
 Y. Z. Tsypkin (1966). Adaptation, training and self-organization automatic control systems, 
Avtomatika I Telemekhanika, 27, 23-61. 
<em>On gradient descent-based on-line learning for non-linear systems.</em>

</p><p>
<a name="GDb"></a>
[GDb]
Y. Z. Tsypkin (1971). Adaptation and Learning in Automatic Systems, Academic Press, 1971.
<em>On gradient descent-based on-line learning for non-linear systems.</em>

</p><p>
<a name="GD1"></a>
[GD1]
S. I. Amari (1967).
A theory of adaptive pattern classifier, IEEE Trans, EC-16, 279-307 (Japanese version published in 1965).
<a href="https://people.idsia.ch/~juergen/amari1967.pdf">PDF.</a>
<em>Probably the first paper on using stochastic gradient descent<sup><small><small><a href="#STO51">[STO51-52]</a></small></small></sup> for learning in multilayer neural networks
(without specifying the specific gradient descent method now known as reverse mode of automatic differentiation or backpropagation<sup><small><small><a href="#BP1">[BP1]</a></small></small></sup>).</em>

</p><p>
<a name="GD2"></a>
[GD2]
S. I. Amari (1968).
Information Theory‚ÄîGeometric Theory of Information, Kyoritsu Publ., 1968 (in Japanese).
<a href="https://people.idsia.ch/~juergen/amari1968p94-135ocr.pdf">OCR-based PDF scan of pages 94-135</a> (see pages 119-120).
<em>Contains computer simulation results for a five layer network (with 2 modifiable layers) which learns internal representations to classify
non-linearily separable pattern classes.</em>

</p><p>
<a name="GD2a"></a>
[GD2a]
H. Saito (1967). Master's  thesis, Graduate School of Engineering, Kyushu University, Japan. 
<em>Implementation of Amari's 1967 stochastic gradient descent method for multilayer perceptrons.<sup><small><small><a href="#GD1">[GD1]</a></small></small></sup> (S. Amari, personal communication, 2021.)</em>

</p><p>
<a name="GD3"></a>
[GD3]
S. I. Amari (1977).
Neural Theory of Association and Concept Formation. 
Biological Cybernetics, vol. 26, p. 175-185, 1977.   
<em>See Section 3.1 on using gradient descent for learning in multilayer networks.</em>

</p><p><a name="HIN"></a>
[HIN] J. Schmidhuber (<a href="https://people.idsia.ch/~juergen/blog.html">AI Blog</a>, 2020). <a href="https://people.idsia.ch/~juergen/critique-honda-prize-hinton.html">Critique of Honda Prize for Dr. Hinton</a>.   <em>Science must not allow corporate PR to distort the academic record. See also <a href="#T22">[T22]</a><a href="#DLP">[DLP]</a><a href="#NOB">[NOB]</a>.</em>



</p><p>
<a name="LEI07"></a>
[LEI07]
J. M. Child (translator), G. W. Leibniz (Author). The Early Mathematical Manuscripts of Leibniz. Merchant Books, 2007. <em>See p. 126: the chain rule appeared in a 1676 memoir by Leibniz.</em>



</p><p>
<a name="LEI10"></a>
[LEI10]
O. H. Rodriguez, J. M. Lopez Fernandez (2010). A semiotic reflection on the didactics of the Chain rule. The Mathematics Enthusiast: Vol. 7 : No. 2 , Article 10. DOI: https://doi.org/10.54870/1551-3440.1191. 


</p><p><a name="MIR"></a>
[MIR] J. Schmidhuber (Oct 2019, updated 2021, 2022, 2025). <a href="https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html">Deep Learning: Our Miraculous Year 1990-1991.</a> Preprint 
<a href="https://arxiv.org/abs/2005.05744">arXiv:2005.05744</a>. <em>The Deep Learning Artificial Neural Networks (NNs)
of our team have
 revolutionised 
<a href="https://people.idsia.ch/~juergen/deep-learning-history.html">Machine Learning &amp; AI</a>.
Many of the basic ideas behind this revolution were published within the 12 months of our <em>"Annus Mirabilis"</em> 1990-1991 at our lab in TU Munich.
Back then, few people were interested, but a quarter century later, NNs based on our <em>"Miraculous Year"</em>
<a href="https://people.idsia.ch/~juergen/impact-on-most-valuable-companies.html">were on over 3 billion devices,
and used many billions of times per day,
consuming a significant fraction of the world's compute</a>.
In particular, in 1990-91, we laid foundations of Generative AI, publishing principles of  (1) 
<a href="https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html#sec1">Generative Adversarial Networks</a> for <a href="https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html">Artificial Curiosity and Creativity</a> (now used for deepfakes), (2) <a href="https://people.idsia.ch/~juergen/fast-weight-programmer-1991-transformer.html">Transformers</a> (the T in ChatGPT‚Äîsee the <a href="https://people.idsia.ch/~juergen/1991-unnormalized-linear-transformer.html">1991 Unnormalized Linear Transformer</a>),  (3) <a href="https://people.idsia.ch/~juergen/very-deep-learning-1991.html">Pre-training</a> for deep NNs (see the P in ChatGPT), (4)  <a href="https://people.idsia.ch/~juergen/very-deep-learning-1991.html">NN distillation</a> (key for <a href="https://x.com/SchmidhuberAI/status/1885357355938046382">DeepSeek</a>), and (5) recurrent <a href="https://people.idsia.ch/~juergen/world-models-planning-curiosity-fki-1990.html">World Models</a> for
<a href="https://people.idsia.ch/~juergen/deep-learning-history.html#rl">Reinforcement Learning and Planning</a> in partially observable environments. The year 1991 also marks the emergence of the defining features of (6)
<a href="https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%204">LSTM</a>, the most cited AI paper of the 20th century (based on constant error flow through residual NN connections), and (7) ResNet, the most cited AI paper of the 21st century, based on our LSTM-inspired <a href="https://people.idsia.ch/~juergen/highway-networks.html">Highway Net</a> that was 10 times deeper than previous record-breaking NNs.
</em>


</p><p>
<a name="MLP1"></a>
[MLP1] D. C. Ciresan, U. Meier, L. M. Gambardella, J. Schmidhuber. Deep Big Simple Neural Nets For Handwritten Digit Recognition. Neural Computation 22(12): 3207-3220, 2010. <a href="http://arxiv.org/abs/1003.0358">ArXiv Preprint.</a>
<em>Showed that plain backprop for deep standard NNs is sufficient to break benchmark records, without any unsupervised pre-training.</em>

</p><p>
<a name="MLP2"></a>
[MLP2] J. Schmidhuber
(<a href="https://people.idsia.ch/~juergen/blog.html">AI Blog</a>, Sep 2020). <a href="https://people.idsia.ch/~juergen/2010-breakthrough-supervised-deep-learning.html">10-year anniversary of supervised deep learning breakthrough (2010). No unsupervised pre-training</a>. <em>By 2010, when compute was 100 times more expensive than today, both the feedforward NNs<sup><small><small><a href="#MLP1">[MLP1]</a></small></small></sup> and the earlier recurrent NNs of Schmidhuber's team were able to beat all competing algorithms on important problems of that time.</em>

</p><p>
<a name="MLP3"></a>
[MLP3] J. Schmidhuber
(<a href="https://people.idsia.ch/~juergen/blog.html">AI Blog</a>, 2025). <a href="https://people.idsia.ch/~juergen/2010-end-to-end-deep-learning-breakthrough.html">2010: Breakthrough of end-to-end deep learning (no layer-by-layer training, no unsupervised pre-training). The rest is history.</a>
<em>By 2010, when compute was 1000 times more expensive than in 2025, both our feedforward NNs<sup><small><small><a href="#MLP1">[MLP1]</a></small></small></sup> and our earlier recurrent NNs were able to beat all competing algorithms on important problems of that time. 
This deep learning revolution quickly spread from Europe to North America and Asia.</em>


</p><p><a name="MOST"></a>
[MOST]
J.&nbsp; Schmidhuber (<a href="https://people.idsia.ch/~juergen/blog.html">AI Blog</a>, 2021, updated 2025). <a href="https://people.idsia.ch/~juergen/most-cited-neural-nets.html">The most cited neural networks all build on work done in my labs</a>: <em> 1. <a href="https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%204">Long Short-Term Memory</a> (LSTM), the most cited AI of the 20th century. 2. ResNet (open-gated <a href="https://people.idsia.ch/~juergen/highway-networks.html">Highway Net</a>), the most cited AI of the 21st century. 3. AlexNet &amp; VGG Net (the similar but earlier <a href="https://people.idsia.ch/~juergen/DanNet-triggers-deep-CNN-revolution-2011.html">DanNet</a> of 2011 <a href="https://people.idsia.ch/~juergen/computer-vision-contests-won-by-gpu-cnns.html">won 4 image recognition challenges</a> before them). 4. GAN (an instance of <a href="https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html#sec1">Adversarial Artificial Curiosity</a> of 1990). 5. Transformer variants‚Äîsee the <a href="https://people.idsia.ch/~juergen/1991-unnormalized-linear-transformer.html">1991 unnormalised linear Transformer</a> (ULTRA). Foundations of Generative AI were published in 1991: the principles of  <a href="https://people.idsia.ch/~juergen/deep-learning-history.html#gan">GANs</a> (now used for deepfakes), <a href="https://people.idsia.ch/~juergen/fast-weight-programmer-1991-transformer.html">Transformers</a> (the T in ChatGPT), <a href="https://people.idsia.ch/~juergen/very-deep-learning-1991.html">Pre-training</a> for deep NNs (the P in ChatGPT), <a href="https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%202">NN distillation</a>, and the famous DeepSeek‚Äîsee the <a href="https://x.com/SchmidhuberAI/status/1885357355938046382">tweet</a>.</em>


</p><p><a name="NOB"></a>
[NOB] J. Schmidhuber.
A Nobel Prize for Plagiarism. 
<a href="https://people.idsia.ch/~juergen/physics-nobel-2024-plagiarism.html">Technical Report IDSIA-24-24 (7 Dec 2024).</a>
<em>Sadly, the Nobel Prize in Physics 2024 for Hopfield &amp; Hinton is a Nobel Prize for plagiarism. They republished methodologies for artificial neural networks developed in Ukraine and Japan by Ivakhnenko and Amari in the 1960s &amp; 1970s, as well as other techniques, without citing the original papers. Even in later surveys, they didn't credit the original inventors (thus turning what may have been unintentional plagiarism into a deliberate form). None of the important algorithms for modern Artificial Intelligence were created by Hopfield &amp; Hinton. 
See also popular 
<a href="https://x.com/SchmidhuberAI/status/1844022724328394780">tweet1</a>,
<a href="https://x.com/SchmidhuberAI/status/1865310820856393929">tweet2</a>, and
<a href="https://lnkd.in/eS92dg86">LinkedIn post</a>.</em>



</p><p><a name="SV20"></a>
[SV20] S. Vazire (2020). A toast to the error detectors. Let 2020 be the year in which we value those who ensure that science is self-correcting. Nature, vol 577, p 9, 2/2/2020.

</p><p><a name="T20"></a>
[T20] J. Schmidhuber (June 2020). <a href="http://people.idsia.ch/~juergen/critique-turing-award-bengio-hinton-lecun.html">Critique of 2018 Turing Award</a>. 

</p><p><a name="T22"></a>
[T22] J. Schmidhuber (<a href="https://people.idsia.ch/~juergen/blog.html">AI Blog</a>, 2022). 
<a href="https://people.idsia.ch/~juergen/scientific-integrity-turing-award-deep-learning.html">Scientific Integrity and the History of Deep Learning: The 2021 Turing Lecture, and the 2018 Turing Award</a>. Technical Report IDSIA-77-21 (v3), IDSIA, Lugano, Switzerland, 22 June 2022. 




</p><p><a name="R7"></a>
[R7] Reddit/ML, 2019. <a href="https://www.reddit.com/r/MachineLearning/comments/e5vzun/d_jurgen_schmidhuber_on_seppo_linnainmaa_inventor/">J. Schmidhuber on Seppo Linnainmaa, inventor of backpropagation in 1970.</a>

</p><p><a name="R8"></a>
[R8] Reddit/ML, 2019. <a href="https://www.reddit.com/r/MachineLearning/comments/ed7asg/d_jurgen_schmidhuber_on_alexey_ivakhnenko/">J. Schmidhuber on Alexey Ivakhnenko, godfather of deep learning 1965.</a>

</p><p><a name="RUM"></a>
[RUM] DE Rumelhart, GE Hinton, RJ Williams (1985). Learning Internal Representations by Error Propagation. TR No. ICS-8506, California Univ San Diego La Jolla Inst for Cognitive Science. Later version published as:
Learning representations by back-propagating errors. Nature, 323, p. 533-536 (1986).
<em>This experimental analysis of <a href="https://people.idsia.ch/~juergen/who-invented-backpropagation.html">backpropagation</a> did not cite the origin of the method,<sup><small><small><a href="#BP1">[BP1-5]</a></small></small></sup> also known as the reverse mode of automatic differentiation.
The paper also failed to cite 
the first working algorithms for deep learning of internal representations (Ivakhnenko &amp; Lapa, 1965)<sup><small><small><a href="#DEEP1">[DEEP1-2]</a><a href="#HIN">[HIN]</a><a href="#DLH">[DLH]</a></small></small></sup> as well as
Amari's work (1967-68)<sup><small><small><a href="#GD1">[GD1-2]</a></small></small></sup> on learning internal representations in deep nets through stochastic gradient descent.
Even later surveys by the authors<sup><small><small><a href="#DL3">[DL3,3a]</a></small></small></sup> failed to cite the prior art.<sup><small><small><a href="#T22">[T22]</a><a href="#DLP">[DLP]</a><a href="#NOB">[NOB]</a></small></small></sup>
</em>

</p><p>
<a name="S80"></a>
[S80]
B. Speelpenning (1980). Compiling Fast Partial Derivatives of Functions Given by Algorithms. PhD
thesis, Department of Computer Science, University of Illinois, Urbana-Champaign.


</p><p><a name="STO51"></a>
[STO51]
H. Robbins, S. Monro (1951). A Stochastic Approximation Method. The Annals of Mathematical Statistics. 22(3):400, 1951.

</p><p><a name="STO52"></a>
[STO52]
J. Kiefer, J. Wolfowitz  (1952). Stochastic Estimation of the Maximum of a Regression Function. 
The Annals of Mathematical Statistics. 23(3):462, 1952.


</p><p><a name="UN"></a>
[UN]
J. Schmidhuber (<a href="https://people.idsia.ch/~juergen/blog.html">AI Blog</a>, 2021). <a href="https://people.idsia.ch/~juergen/very-deep-learning-1991.html">30-year anniversary. 1991: First very deep learning with unsupervised pre-training. First neural network distillation</a>. <em>Unsupervised hierarchical predictive coding (with self-supervised target generation) finds compact internal representations of sequential data to facilitate downstream deep learning. The hierarchy can be distilled into a single deep neural network (suggesting a simple model of conscious and subconscious information processing). 1993: solving problems of depth &gt;1000.</em>

</p><p>
<a name="UN0"></a>
[UN0]
J.&nbsp; Schmidhuber.
  Neural sequence chunkers.
  Technical Report FKI-148-91, Institut f√ºr Informatik, Technische
  Universit√§t M√ºnchen, April 1991.
<a href="https://people.idsia.ch/~juergen/FKI-148-91ocr.pdf">PDF.</a>
 <a href="https://people.idsia.ch/~juergen/very-deep-learning-1991.html">More.</a>



</p><p><a name="UN1"></a>
[UN1] J. Schmidhuber. Learning complex, extended sequences using the principle of history compression. Neural Computation, 4(2):234-242, 1992. Based on TR FKI-148-91, TUM, 1991.<sup><small><small><a href="#UN0">[UN0]</a></small></small></sup> <a href="https://sferics.idsia.ch/pub/juergen/chunker.pdf">PDF</a>. 
 <a href="https://people.idsia.ch/~juergen/very-deep-learning-1991.html">More.</a>

</p><p><a name="UN2"></a>
[UN2] J.  Schmidhuber.  Habilitation thesis, TUM, 1993. <a href="https://sferics.idsia.ch/pub/juergen/habilitation.pdf">PDF</a>.  
<em>An ancient experiment on "Very Deep Learning" with credit assignment across 1200 time steps or virtual layers and unsupervised / self-supervised pre-training for a stack of recurrent NN  
<a href="http://www.idsia.ch/~juergen/habilitation/node114.html">can be found here</a> (depth &gt; 1000).</em>

</p><p><a name="UN3"></a>
[UN3]
J.&nbsp; Schmidhuber, M.&nbsp;C. Mozer, and D.&nbsp;Prelinger.
<a href="https://sferics.idsia.ch/pub/juergen/aachen.ps.gz">
  Continuous history compression.
</a>
  In H.&nbsp;H√ºning, S.&nbsp;Neuhauser, M.&nbsp;Raus, and W.&nbsp;Ritschel, editors,
  <em>Proc. of Intl. Workshop on Neural Networks, RWTH Aachen</em>, pages 87-95.
  Augustinus, 1993.


</p><p><a name="UN4"></a>
[UN4]  G. E. Hinton, R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, Vol. 313. no. 5786, pp. 504‚Äî507, 2006.  <a href="http://www.cs.toronto.edu/~hinton/science.pdf">PDF</a>. 
<em>
This work describes unsupervised layer-wise pre-training of stacks of <em>feedforward</em> NNs (FNNs) 
called <em>Deep Belief Networks</em> (DBNs).
However, this work neither cited the original layer-wise training of deep NNs by Ivakhnenko &amp; Lapa (1965)<sup><small><small><a href="#DEEP1">[DEEP1-2]</a></small></small></sup> nor
the 1991 unsupervised pre-training of stacks of more general <em>recurrent</em> NNs (RNNs)<sup><small><small><a href="#UN">[UN0-3]</a></small></small></sup> 
which  introduced
<a href="https://people.idsia.ch/~juergen/very-deep-learning-1991.html">the first NNs shown to solve very deep problems</a>.
The 2006 justification of the authors was essentially the one Schmidhuber used for the 1991 RNN stack: 
each higher level tries to reduce the description length 
(or negative log probability) of the data representation in the level below.<sup><small><small><a href="#HIN">[HIN]</a><a href="#T22">[T22]</a><a href="#MIR">[MIR]</a></small></small></sup>
This can greatly facilitate very deep downstream learning.<sup><small><small><a href="#UN">[UN0-3]</a></small></small></sup> </em>


</p></span></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Road That Killed Legend Jenkins Was Working as Designed (118 pts)]]></title>
            <link>https://www.strongtowns.org/journal/2025/8/18/the-road-that-killed-legend-jenkins-was-working-exactly-as-designed</link>
            <guid>44941766</guid>
            <pubDate>Mon, 18 Aug 2025 15:33:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.strongtowns.org/journal/2025/8/18/the-road-that-killed-legend-jenkins-was-working-exactly-as-designed">https://www.strongtowns.org/journal/2025/8/18/the-road-that-killed-legend-jenkins-was-working-exactly-as-designed</a>, See on <a href="https://news.ycombinator.com/item?id=44941766">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-sqsp-text-block-content="" data-block-type="2" data-sqsp-block="text" id="block-edb6ef0d7c4d2de9458a">
  <p><strong>On May 27, 2025, two brothers in Gastonia, North Carolina, asked their dad if they could walk to the neighborhood Food Lion and Subway.</strong> It was less than ten minutes from their apartment.</p><p>Their dad hesitated. He and his wife are protective parents. But he agreed, on the condition that the boys stay on the phone with him the whole way. They made it to the store and back toward home without incident, until they reached West Hudson Boulevard.</p><p>Legend Jenkins, seven years old, stepped off the median into the road and was struck by an SUV. His father was still on the phone when it happened.</p><p>In the days that followed, the family‚Äôs grief deepened into something almost unimaginable. Two days after their son‚Äôs death, <a href="https://www.nytimes.com/2025/08/06/opinion/children-traffic-death-parents.html" target="_blank">the district attorney charged both parents with involuntary manslaughter</a>, set bail at $1.5 million each, and placed their surviving six children in the care of relatives.</p><p>I‚Äôve seen pedestrian advocates respond to this case with outrage: ‚ÄúKids should be able to walk to the store. Parents shouldn‚Äôt be criminalized for it.‚Äù I understand that impulse. I agree with the sentiment. But when I look at the location where Legend died, I can honestly tell you that nobody ‚Äî child or adult ‚Äî should be walking there.</p><p><a href="https://www.google.com/maps/@35.2349131,-81.2038714,17z?entry=ttu&amp;g_ep=EgoyMDI1MDgwNi4wIKXMDSoASAFQAw%3D%3D" target="_blank">West Hudson Boulevard</a> is a high-speed arterial road with narrow sidewalks, a tiny median, and no truly safe crossings. Even a healthy, alert adult is taking their life in their hands by walking to that store. For a child, it‚Äôs playing the worst kind of roulette.</p><p>If this were a neighborhood where people regularly fired guns in the air, we would warn parents to keep their kids inside. A stray bullet may not be intentional, but it‚Äôs a <a href="https://www.strongtowns.org/journal/2015/6/8/bullets">predictable outcome of such an environment</a>. On West Hudson Boulevard, the stray bullets are motor vehicles, and the result is the same: occasional, random, but entirely foreseeable deaths.</p><p>This wasn‚Äôt an ‚Äúaccident‚Äù in the sense of something random or unexpected. It was the statistically inevitable outcome of building a place where human life outside of a car has no real value in the design. Humans are, at best, a tiny afterthought. At worst, an annoyance.</p><p>The truth is, <a href="https://www.strongtowns.org/journal/2017/11/1/gross-negligence">everyone who participates in building these places is complicit</a>. Everyone. The planners who approved the land use. The engineers who designed the road geometry. The developers who built apartments near retail without safe connections. The retailers who designed parking-lot entrances instead of pedestrian routes. The public officials who sign off on all of it.</p><p>None of them intended for a child to die here. But if you build an environment that makes random deaths inevitable, the deaths will happen. There is a clear cause. Yet, by the composition and consensus of all involved, none of them can ever be held responsible. Ultimately, if everyone involved is responsible, nobody is responsible.</p><p>But society demands that someone be held accountable. So, when that inevitability came to pass, the system‚Äôs first response was to narrow the frame <a href="https://www.strongtowns.org/journal/2025/6/26/when-parents-are-charged-but-the-stroad-is-the-culprit">until blame could be pinned on someone</a>. In this case, that was the parents. That same instinct often points the finger at the driver.&nbsp;</p><p>Yes, the driver is the one who struck Legend. Yes, the parents chose to let him walk. But that doesn‚Äôt explain why this road exists in a form that makes a tragedy like this certain to happen again and again and again.</p><p>I‚Äôve spent quite a bit of time looking at this corridor. It is familiar in all the wrong ways. My diagnosis is that this is not a fixable situation, not in any meaningful sense. You can‚Äôt slap in a crosswalk, a flashing beacon, or a strip of sidewalk and call it safe. The entire nature of the road ‚Äî its speed, its function, its relationship to surrounding land uses ‚Äî is <a href="https://www.strongtowns.org/journal/2025/7/30/annapolis-needs-safe-street-design-not-orange-flags">incompatible with the safe movement of people</a>. That‚Äôs unsafe for those both inside and especially outside a motor vehicle.</p><p>And yet, we build more places just like this every day. Everyone knows better, but we do it anyway. When we do, we make an unspoken agreement: some number of people will die here every year. Some number of people will be sacrificed for the sake of a built environment that few professionals really believe is worthy of their energy, expertise, or even their attention.</p><p>We don‚Äôt say that part out loud because, if we did, it would force us to confront the morality of our choices. These choices are deeply immoral.</p><p>So we do the next best thing for our consciences: we blame the victims. We prosecute the parents, <a href="https://www.strongtowns.org/journal/2022/3/7/the-reckless-driver-narrative-is-reckless-stop-spreading-it">demonize the driver</a>, or scold the pedestrian for ‚Äúnot being careful.‚Äù And in doing so, we avoid indicting the real culprit: the American development culture that produced this environment.</p><p>If we actually wanted to prevent the next Legend Jenkins, we would stop replicating places like West Hudson Boulevard. We would start building neighborhoods where a grocery store, a sandwich shop, and an apartment complex can exist within a short, safe walk of each other. And when tragedies happen, we would put the <em>environment</em> on trial, not the people who got caught in it.</p><p>There is no simple way to fix this street, but there is a way to start fixing the culture that builds places like this. The <a href="https://www.strongtowns.org/crashstudio">Strong Towns Crash Analysis Studio</a> approach looks at every contributing factor in a crash ‚Äî from sight lines to land use ‚Äî and makes recommendations to address those shortcomings. It‚Äôs not about finding someone to punish, but about having a public conversation that says, ‚ÄúThis place killed someone. Here‚Äôs why, and here‚Äôs how we stop it from happening again.‚Äù</p><p>Legend‚Äôs death was not a fluke. It was the expected outcome of a system working exactly as designed. Until we confront that reality ‚Äî and change it ‚Äî we‚Äôre just waiting for the next name to add to the list of unnecessary tragedies.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Counter-Strike: A billion-dollar game built in a dorm room (264 pts)]]></title>
            <link>https://www.nytimes.com/2025/08/18/arts/counter-strike-half-life-minh-le.html</link>
            <guid>44941369</guid>
            <pubDate>Mon, 18 Aug 2025 14:59:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/08/18/arts/counter-strike-half-life-minh-le.html">https://www.nytimes.com/2025/08/18/arts/counter-strike-half-life-minh-le.html</a>, See on <a href="https://news.ycombinator.com/item?id=44941369">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/08/18/arts/counter-strike-half-life-minh-le.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[95% of AI Pilots Failing (158 pts)]]></title>
            <link>https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/</link>
            <guid>44941118</guid>
            <pubDate>Mon, 18 Aug 2025 14:36:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/">https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/</a>, See on <a href="https://news.ycombinator.com/item?id=44941118">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Good morning. Companies are betting on AI‚Äîyet nearly all enterprise pilots are stuck at the starting line.</p><div>



<div><p><em>The GenAI Divide: State of AI in Business 2025</em>, <a href="https://nanda.media.mit.edu/ai_report_2025.pdf" target="_blank" rel="noopener" aria-label="Go to https://nanda.media.mit.edu/ai_report_2025.pdf">a new report</a> published by MIT‚Äôs <a href="https://nanda.media.mit.edu/" target="_blank" rel="noopener" aria-label="Go to https://nanda.media.mit.edu">NANDA</a> initiative, reveals that while generative AI holds promise for enterprises, most initiatives to drive rapid revenue growth are falling flat.</p><p>Despite the rush to integrate powerful new models, about 5% of AI pilot programs achieve rapid revenue acceleration; the vast majority stall, delivering little to no measurable impact on P&amp;L. The research‚Äîbased on 150 interviews with leaders, a survey of 350 employees, and an analysis of 300 public AI deployments‚Äîpaints a clear divide between success stories and stalled projects.</p><p>To unpack these findings, I spoke with Aditya Challapally, the lead author of the report, and a research contributor to project NANDA at MIT.</p><p>‚ÄúSome large companies‚Äô pilots and younger startups are really excelling with generative AI,‚Äù Challapally said. Startups led by 19- or 20-year-olds, for example, ‚Äúhave seen revenues jump from zero to $20 million in a year,‚Äù he said. ‚ÄúIt‚Äôs because they pick one pain point, execute well, and partner smartly with companies who use their tools,‚Äù he added.</p><p>But for 95% of companies in the dataset, generative AI implementation is falling short. The core issue? Not the quality of the AI models, but the ‚Äúlearning gap‚Äù for both tools and organizations. While executives often blame regulation or model performance, MIT‚Äôs research points to flawed enterprise integration. Generic tools like ChatGPT excel for individuals because of their flexibility, but they stall in enterprise use since they don‚Äôt learn from or adapt to workflows, Challapally explained.</p><p>The data also reveals a misalignment in resource allocation. More than half of generative AI budgets are devoted to sales and marketing tools, yet MIT found the biggest ROI in back-office automation‚Äîeliminating business process outsourcing, cutting external agency costs, and streamlining operations.



</p></div><h3><strong>What‚Äôs behind successful AI deployments?</strong></h3>



<div><p>How companies adopt AI is crucial. Purchasing AI tools from specialized vendors and building partnerships succeed about 67% of the time, while internal builds succeed only one-third as often.</p><p>This finding is particularly relevant in financial services and other highly regulated sectors, where many firms are building their own proprietary generative AI systems in 2025. Yet, <a href="https://nanda.media.mit.edu/ai_report_2025.pdf" target="_blank" rel="noopener" aria-label="Go to https://nanda.media.mit.edu/ai_report_2025.pdf">MIT‚Äôs research</a> suggests companies see far more failures when going solo.</p><p>Companies surveyed were often hesitant to share failure rates, Challapally noted. ‚ÄúAlmost everywhere we went, enterprises were trying to build their own tool,‚Äù he said, but the data showed purchased solutions delivered more reliable results.</p><p>Other key factors for success include empowering line managers‚Äînot just central AI labs‚Äîto drive adoption, and selecting tools that can integrate deeply and adapt over time.</p><p>Workforce disruption is already underway, especially in customer support and administrative roles. Rather than mass layoffs, companies are increasingly not backfilling positions as they become vacant. Most changes are concentrated in jobs previously outsourced due to their perceived low value.</p><p>The report also highlights the widespread use of ‚Äúshadow AI‚Äù‚Äîunsanctioned tools like ChatGPT‚Äîand the ongoing challenge of measuring AI‚Äôs impact on productivity and profit.</p><p>Looking ahead, the most advanced organizations are already experimenting with agentic AI systems that can learn, remember, and act independently within set boundaries‚Äîoffering a glimpse at how the next phase of enterprise AI might unfold.



</p></div><p><strong>Sheryl</strong>&nbsp;<strong>Estrada</strong><br><a href="mailto:sheryl.estrada@fortune.com" target="_blank" rel="noreferrer noopener" aria-label="Go to mailto:sheryl.estrada@fortune.com">sheryl.estrada@fortune.com</a></p><h3>Leaderboard</h3><p><b>Michael A. Discenza</b><span> was appointed VP and CFO of </span><a href="https://www.prnewswire.com/news-releases/timken-names-michael-a-discenza-chief-financial-officer-302529485.html" target="_blank" rel="noopener" aria-label="Go to https://www.prnewswire.com/news-releases/timken-names-michael-a-discenza-chief-financial-officer-302529485.html"><span>The Timken Company</span></a><span> (NYSE: TKR), effective immediately. Discenza has 25 years of experience at Timken in roles of increasing responsibility, including the last 10 as VP of finance, and group controller.</span></p><p><b>John Cole</b><span> was appointed CFO of </span><a href="https://www.globenewswire.com/news-release/2025/08/14/3133524/0/en/ELB-Learning-Appoints-John-Cole-as-Chief-Financial-Officer-to-Support-Organization-s-Strategic-Growth.html" target="_blank" rel="noopener" aria-label="Go to https://www.globenewswire.com/news-release/2025/08/14/3133524/0/en/ELB-Learning-Appoints-John-Cole-as-Chief-Financial-Officer-to-Support-Organization-s-Strategic-Growth.html"><span>ELB Learning</span></a><span>, a provider of immersive learning solutions. He brings more than 25 years of experience leading finance and operations for Fortune 100 and 500 companies, according to ELB. Cole aims to strengthen the financial infrastructure to support the company‚Äôs next phase of growth.</span></p><h3>Big Deal</h3><div><p>Modern manufacturing relies heavily on connected devices and industrial control systems, which are prime targets for cyberattacks. For protection, manufacturers are increasingly turning to AI to help manage these risks, according to the&nbsp;<a href="https://www.rockwellautomation.com/en-us/capabilities/digital-transformation/state-of-smart-manufacturing.html" target="_blank" rel="noopener" aria-label="Go to https://www.rockwellautomation.com/en-us/capabilities/digital-transformation/state-of-smart-manufacturing.html"><em>State of Smart Manufacturing Report</em></a>&nbsp;by Rockwell Automation, Inc.
</p><p>The report‚Äôs findings are based on a survey of more than 1,500 manufacturing leaders across 17 major manufacturing countries. Cybersecurity now ranks among the top external risks, second only to inflation and economic growth. One-third of respondents hold responsibilities spanning both information technology (IT) and operational technology (OT) cybersecurity.
</p><p>Nearly half (48%) of cybersecurity professionals identified securing converged architectures as key to positive outcomes over the next five years, compared to just 37% of all respondents.
</p><p>However, a shortage of skilled talent, training challenges, and rising labor costs remain major hurdles. As manufacturers recruit the next generation, cybersecurity and analytical skills are becoming hiring priorities‚Äîreinforcing the need to align technical innovation with human development, according to the report.
</p></div><h3>Going deeper</h3><p>In a new <em>Fortune</em> <a href="https://fortune.com/2025/08/15/black-female-leadership-future-ceos-erased-dei/" target="_self" aria-label="Go to https://fortune.com/2025/08/15/black-female-leadership-future-ceos-erased-dei/">opinion piece</a>,&nbsp;"Future CEOs, erased: the economic cost of losing Black women in the workforce,"&nbsp;Katica Roy, the CEO and founder of the Denver-based Pipeline, a SaaS company, explains&nbsp;the implications of&nbsp;almost <a href="https://www.goodmorningamerica.com/video/124115661" target="_blank" rel="noopener" aria-label="Go to https://www.goodmorningamerica.com/video/124115661">300,000 Black women exited the labor force</a> so far this year‚Äîthinning a pipeline that was already too narrow.</p><div><p>"This isn‚Äôt a seasonal fluctuation or statistical footnote. It‚Äôs a strategic failure with long-term consequences," Roy writes. "Black women have long been a cornerstone of America‚Äôs economic engine‚Äîdriving participation, powering key industries, and anchoring family incomes. Now, that foundation is fracturing. And the fallout is more than short-term‚Äîit‚Äôs a direct threat to corporate succession planning, innovation, and growth. The U.S. economy has always depended on Black women‚Äôs labor. In fact, no group of women in America has historically had <a href="https://www.bls.gov/opub/reports/race-and-ethnicity/2023/" target="_blank" rel="noopener" aria-label="Go to https://www.bls.gov/opub/reports/race-and-ethnicity/2023/">higher labor force participation</a> than Black women."</p></div><h3>Overheard</h3><p><strong>‚ÄúEvery single Monday was called 'AI Monday.' You couldn‚Äôt have customer calls, you couldn‚Äôt work on budgets, you had to only work on AI projects.‚Äù</strong></p><p>‚ÄîEric Vaughan, CEO of enterprise software company IgniteTech, told <a href="https://fortune.com/2025/08/17/ceo-laid-off-80-percent-workforce-ai-sabotage/" target="_self" aria-label="Go to https://fortune.com/2025/08/17/ceo-laid-off-80-percent-workforce-ai-sabotage/"><em>Fortune</em> in an interview</a> that he established a mandate: on Mondays, staff could only work on AI. In early 2023, convinced generative AI was an ‚Äúexistential‚Äù transformation, Vaughan saw that his team was not fully on board. His ultimate response? He replaced nearly 80% of the staff within a year, according to headcount figures reviewed by <em>Fortune</em>.
</p></div><p>This is the web version of CFO Daily, a newsletter on the trends and individuals shaping corporate finance. <a href="https://www.fortune.com/newsletters/cfodaily?&amp;itm_source=fortune&amp;itm_medium=nl_article_tout&amp;itm_campaign=cfo_daily" target="_self" aria-label="Go to https://www.fortune.com/newsletters/cfodaily?&amp;itm_source=fortune&amp;itm_medium=nl_article_tout&amp;itm_campaign=cfo_daily">Sign up for free</a>.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is predominantly replacing outsourced, offshore workers (152 pts)]]></title>
            <link>https://www.axios.com/2025/08/18/ai-jobs-layoffs</link>
            <guid>44940944</guid>
            <pubDate>Mon, 18 Aug 2025 14:20:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/2025/08/18/ai-jobs-layoffs">https://www.axios.com/2025/08/18/ai-jobs-layoffs</a>, See on <a href="https://news.ycombinator.com/item?id=44940944">Hacker News</a></p>
Couldn't get https://www.axios.com/2025/08/18/ai-jobs-layoffs: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Class-action suit claims Otter AI records private work conversations (139 pts)]]></title>
            <link>https://www.npr.org/2025/08/15/g-s1-83087/otter-ai-transcription-class-action-lawsuit</link>
            <guid>44940554</guid>
            <pubDate>Mon, 18 Aug 2025 13:47:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/2025/08/15/g-s1-83087/otter-ai-transcription-class-action-lawsuit">https://www.npr.org/2025/08/15/g-s1-83087/otter-ai-transcription-class-action-lawsuit</a>, See on <a href="https://news.ycombinator.com/item?id=44940554">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="resg-s1-83088">
            <div data-crop-type="">
        <picture>
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/400/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/900/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/1200/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/1600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/1800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg" sizes="(min-width: 1025px) 650px, calc(100vw - 30px)" type="image/webp">
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/400/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/900/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/1200/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/1600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/1800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg" sizes="(min-width: 1025px) 650px, calc(100vw - 30px)" type="image/jpeg">
            <img src="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/1100/quality/50/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/1200x676+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F5b%2F94%2F9964049746dd8edc7fd29b2fe608%2Fotter-pic.jpg" alt="Otter.ai is a Mountain View, Calif.-based tech company that uses artificial intelligence to generate speech-to-text transcriptions. It has become a popular tool for transcribing virtual office meetings." fetchpriority="high">
        </picture>
</div>
<div>
    <div>
        <p>
                Otter.ai is a Mountain View, Calif.-based tech company that uses artificial intelligence to generate speech-to-text transcriptions. It has become a popular tool for transcribing virtual office meetings.
                <b aria-label="Image credit">
                    
                    Source: Otter
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Source: Otter
        
    </span>
</p></div>
   </div>
   <p>A federal lawsuit seeking class-action status accuses Otter.ai of "deceptively and surreptitiously" recording private conversations that the tech company uses to train its popular transcription service without permission from the people using it.</p>   <p>The company's AI-powered transcription service called Otter Notebook, which can do real-time transcriptions of Zoom, Google Meet and Microsoft Teams meetings, by default does not ask meeting attendees for permission to record and fails to alert participants that recordings are shared with Otter to improve its artificial intelligence systems, according to <a href="https://www.documentcloud.org/documents/26052769-otter-complaint/" target="_blank"><u>the suit</u></a> filed on Friday.</p>   <p>The plaintiff in the suit is a man named Justin Brewer of San Jacinto, Calif., who alleges his privacy was "severely invaded" upon realizing Otter was secretly recording a confidential conversation.</p>   
   <p>The suit, filed in the U.S. District Court for the Northern District of California, claims Otter's covert recording violates state and federal privacy and wiretap laws. It seeks to represent others in California who have had chats unknowingly shared with Otter, which the lawyers contend Otter does to "derive financial gain."</p>   
   
<!-- END ID="RESG-S1-83087-100" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Neither Brewer's legal team nor a spokesperson for Otter returned requests for comment.</p>   <p>Otter's <a href="https://otter.ai/privacy-policy" target="_blank">privacy policy</a> does not hide the AI training. It says it receives "explicit permission" from users to train its systems on meeting transcripts when users check a box granting Otter and third parties to ingest the private conversations "for training and product improvement purposes," but the lawsuit maintains many are still being duped.</p>   <p>In recent months, new privacy questions have dogged Otter as it has become increasingly deployed in workplaces around the world.</p>   <p>Some 25 million people now use its AI transcription tools, which have recorded and processed more than 1 billion meetings since the company was founded in 2016, the company <a href="https://otter.ai/blog/otter-ai-breaks-100m-arr-barrier-and-transforms-business-meetings-launching-industry-first-ai-meeting-agent-suite" target="_blank"><u>says</u></a>.</p>   <p>Users have shared horror stories on platforms such as X and Reddit about Otter's automated recording tools backfiring.</p>   <p>Last year, an AI researcher and engineer <a href="https://x.com/alexbilz/status/1839393095236104598" target="_blank"><u>said</u></a> Otter had recorded a Zoom meeting with investors, then shared with him a transcription of the chat including "intimate, confidential details" about a business discussed after he had left the meeting. Those portions of the conversation ended up killing a deal, <em>The Washington Post</em> <a href="https://www.washingtonpost.com/business/2024/10/02/ai-assistant-transcription-work-secrets-meetings/" target="_blank"><u>reported</u></a>.</p>   
   
<!-- END ID="RESG-S1-83087-101" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Politico's China correspondent has <a href="https://www.politico.com/news/2022/02/16/my-journey-down-the-rabbit-hole-of-every-journalists-favorite-app-00009216" target="_blank"><u>written about</u></a> interviewing a Uyghur human rights activist using Otter and realizing that the company shares user data with third parties, raising fears over the possibility that the Chinese government could attempt to access raw transcriptions of conversations with dissidents. Otter has said it does not share any data with foreign governments or law enforcement agencies.</p>   <p>On Reddit, users <a href="https://www.reddit.com/r/projectmanagement/comments/1j0cfei/do_not_join_otterai_unless_you_want_your_whole/" target="_blank"><u>have complained</u></a> about Otter joining meetings automatically when the service is linked to workplace calendars and recording chats without consent.</p>   
   <p>It's a phenomenon also highlighted by the lawsuit. If someone has an Otter account and joins a virtual meeting, the software will typically ask the meeting's host for permission to record, but it does not by default ask all the other participants.</p>   <p>"In fact, if the meeting host is an Otter accountholder who has integrated their relevant Google Meet, Zoom, or Microsoft Teams accounts with Otter, an Otter Notetaker may join the meeting without obtaining the affirmative consent from any meeting participant, including the host," the lawsuit alleges. "What Otter has done is use its Otter Notetaker meeting assistant to record, transcribe, and utilize the contents of conversations without the Class members' informed consent."</p>   <p>Otter <a href="https://otter.ai/privacy-policy" target="_blank"><u>claims</u></a> that before the audio of meetings is fed into its machine learning systems to help improve an AI speech recognition feature, it is "de-identified," a method by which data can be anonymized.</p>   <p>Yet the suit filed on Friday raises concerns about Otter's ability to do this effectively, saying the company provides no public explanation of its "de-identifying" process.</p>   <p>"Upon information and belief, Otter's deidentification process does not remove confidential information or guarantee speaker anonymity," the lawsuit argues.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FFmpeg Assembly Language Lessons (311 pts)]]></title>
            <link>https://github.com/FFmpeg/asm-lessons</link>
            <guid>44940485</guid>
            <pubDate>Mon, 18 Aug 2025 13:39:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/FFmpeg/asm-lessons">https://github.com/FFmpeg/asm-lessons</a>, See on <a href="https://news.ycombinator.com/item?id=44940485">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">Welcome to the FFmpeg School of Assembly Language. You have taken the first step on the most interesting, challenging, and rewarding journey in programming. These lessons will give you a grounding in the way assembly language is written in FFmpeg and open your eyes to what's actually going on in your computer.</p>
<p dir="auto"><strong>Required Knowledge</strong></p>
<ul dir="auto">
<li>Knowledge of C, in particular pointers. If you don't know C, work through <a href="https://en.wikipedia.org/wiki/The_C_Programming_Language" rel="nofollow">The C Programming Language</a> book</li>
<li>High School Mathematics (scalar vs vector, addition, multiplication etc)</li>
</ul>
<p dir="auto"><strong>Lessons</strong></p>
<p dir="auto">In this Git repository there are lessons and assignments (not uploaded yet) that correspond with each lessons. By the end of the lessons you'll be able to contribute to FFmpeg.</p>
<p dir="auto">A discord server is available to answer questions:
<a href="https://discord.com/invite/Ks5MhUhqfB" rel="nofollow">https://discord.com/invite/Ks5MhUhqfB</a></p>
<p dir="auto"><strong>Translations</strong></p>
<ul dir="auto">
<li><a href="https://github.com/FFmpeg/asm-lessons/blob/main/README.fr.md">Fran√ßais</a></li>
<li><a href="https://github.com/FFmpeg/asm-lessons/blob/main/README.es.md">Spanish</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Texas law gives grid operator power to disconnect data centers during crisis (136 pts)]]></title>
            <link>https://www.utilitydive.com/news/texas-law-gives-grid-operator-power-to-disconnect-data-centers-during-crisi/751587/</link>
            <guid>44940416</guid>
            <pubDate>Mon, 18 Aug 2025 13:34:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.utilitydive.com/news/texas-law-gives-grid-operator-power-to-disconnect-data-centers-during-crisi/751587/">https://www.utilitydive.com/news/texas-law-gives-grid-operator-power-to-disconnect-data-centers-during-crisi/751587/</a>, See on <a href="https://news.ycombinator.com/item?id=44940416">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        
                        

<h3>Dive Brief:</h3>
<ul>
<li><span><span><span><span><span><span>Data centers and other large, non-critical power consumers connected to the Electric Reliability Council of Texas transmission grid must accept curtailment during firm load shed events under a </span></span></span></span></span></span><a href="https://capitol.texas.gov/BillLookup/History.aspx?LegSess=89R&amp;Bill=SB6"><span><span><span><span><span><span><span><span>landmark law</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span> Republican Gov. Greg Abbott signed Friday.</span></span></span></span></span></span></li>
<li><a href="https://capitol.texas.gov/tlodocs/89R/billtext/pdf/SB00006F.pdf#navpanes=0"><span><span><span><span><span><span><span><span>Senate Bill 6</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span> pairs mandatory curtailment with a voluntary demand response procurement program under which loads of 75 MW or more could ramp down or switch to backup generation at utilities‚Äô request. It also includes new interconnection disclosure and cost-sharing rules, mandatory interconnection study fees and protocols for colocating large loads with existing generators.</span></span></span></span></span></span></li>
<li><span><span><span><span><span><span>S.B. 6 ‚Äúwill provide regulatory certainty for independent power producers, such as Vistra Corp. and NRG Energy, and data centers seeking colocation arrangements,‚Äù Capstone energy analysts Monica Chen and Jack Painter said in a </span></span></span></span></span></span><a href="https://go.capstonedc.com/l/970603/2025-06-23/5czwf/970603/1750693635J1TfthPe/20250623_Quick_Take_Texas_Governor_Signs_Senate_Bill_6.pdf"><span><span><span><span><span><span><span><span>Monday note</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span>.</span></span></span></span></span></span></li>
</ul>



<h3>Dive Insight:</h3>
<p><span><span><span><span><span><span>Utilities, energy system analysts and ERCOT expect exponential growth of data centers and other large loads in Texas over the next several years. ERCOT forecasts </span></span></span></span></span></span><a href="https://gridstrategiesllc.com/wp-content/uploads/National-Load-Growth-Report-2024.pdf"><span><span><span><span><span><span><span><span>138 GW of large loads</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span> on its grid by 2030, up from 87 GW this year.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>Even if only a fraction of proposed data centers get built, the boom could threaten grid reliability during the spring and fall months, when many thermal generators go down for planned maintenance, Aurora Energy Research </span></span></span></span></span></span><a href="https://www.utilitydive.com/news/shoulder-season-reliability-a-growing-concern-in-ercot-other-isos-aurora/750939/"><span><span><span><span><span><span><span><span>said earlier this month</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span>. Reliability is already a concern in some parts of ERCOT ‚Äî including the San Antonio area, where </span></span></span></span></span></span><a href="https://www.utilitydive.com/news/centerpoint-mobile-generators-san-antonio-ercot/751163/"><span><span><span><span><span><span><span><span>ERCOT is deploying more than 400 MW of mobile generation units</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span> and inked a costly ‚Äúreliability must run‚Äù agreement with an aging 400-MW gas plant.&nbsp;</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>Aurora models suggest data centers will be the largest single source of load flexibility available to mitigate Texas‚Äô reliability risk. By 2030, up to 50% of the expected 35 GW of ERCOT‚Äôs data center capacity could provide some degree of emergency reliability support, Aurora said.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>S.B. 6 authorizes the Public Utility Commission of Texas to develop two demand management programs ‚Äî one mandatory and one voluntary ‚Äî to ensure Texas data centers and other non-critical large loads help rather than hinder reliability.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>The law‚Äôs intent is ‚Äúto make sure [large loads] pose as little reliability risk to the system as possible and [are] not drinking the milkshake of all other Texas power customers,‚Äù NRG Vice President of Regulatory Affairs Travis Kavulla said in an interview.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>S.B. 6 could avoid a future scenario like Winter Storm Uri, the dayslong freeze in February 2021 that saw millions of residential customers cut off from the grid as nearby industrial loads hummed along, Kavulla added.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>The mandatory demand management program applies to loads of 75 MW or greater that interconnect to ERCOT from January onwards. It allows utilities to disconnect eligible loads during firm load shed events and mandates the installation of shutoff equipment as a condition of grid interconnection.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>The voluntary program is a competitively procured reliability service active during specific times of the year, subject to a minimum 24-hour notice period and off-limits to any large-load customer that ‚Äúcurtails in response to the wholesale price of electricity ‚Ä¶ or that otherwise participates in a different reliability or ancillary service,‚Äù the law says.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>The advance warning period is key for this sort of voluntary program, especially one counting on participation from hyperscale data centers with sensitive IT equipment worth billions, Kavulla said.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>‚ÄúThis should not be the kind of demand response where you‚Äôre calling it with no notice and curtailing the customer straight off,‚Äù he said.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>The mandatory program will surely alleviate stress on the ERCOT grid during extreme weather events but the jury is still out on customer uptake for the voluntary program, Kavulla said. Some data center operators have sounded open to voluntarily curtailing their loads or switching to onsite backup generation when needed, while others have been more resistant, he noted.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>Kavulla credited Texas legislators for ‚Äúcalling the question,‚Äù however.&nbsp;</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>‚ÄúThey have decided to create a market and test [customers‚Äô] willingness to participate,‚Äù he said. ‚ÄúNothing gets people thinking like offering them money.‚Äù</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>Kavulla and Texas Blockchain Council President Lee Bratcher cheered other S.B. 6 provisions, like a $100,000 minimum initial interconnection fee for large load customers and a requirement that such customers disclose to utilities any potentially duplicative interconnection requests elsewhere in Texas.&nbsp;</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>Both provisions could mitigate the ‚Äúphantom loads‚Äù gumming up utility and grid operator forecasts in Texas and elsewhere, Bratcher said in an email.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>‚ÄúThe Texas Blockchain Council and our member companies are glad to see that Senate Bill 6 tackles the phantom load challenge associated with the interconnection queue [and gives ERCOT] a more accurate picture of future load growth,‚Äù Bratcher said.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>Some experts say </span></span></span></span></span></span><a href="https://www.utilitydive.com/news/a-fraction-of-proposed-data-centers-will-get-built-utilities-are-wising-up/748214/"><span><span><span><span><span><span><span><span>80% to 90% of proposed data centers</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span> in the U.S. interconnection queue will never get built, in part because they duplicate requests made in other utility territories.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>The next step for ERCOT and its continental counterpart, the North American Electric Reliability Corporation, is to ‚Äúdevelop a non-firm load category for modeling purposes [that] would greatly increase the efficient utilization of transmission infrastructure and properly signal load behavior expectations to the transmission/distribution service providers,‚Äù Bratcher said.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>And while Texas‚Äôs intrastate electricity market makes it something of a special case, some core S.B. 6 provisions are transferable to other states in the restructured Eastern markets, Kavulla said.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>For example, states in the PJM Interconnection ‚Äúcould certainly precondition or accelerate interconnection of large loads on the basis of their willingness to participate in demand response,‚Äù he said.</span></span></span></span></span></span></p>



                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vibe coding tips and tricks (180 pts)]]></title>
            <link>https://github.com/awslabs/mcp/blob/main/VIBE_CODING_TIPS_TRICKS.md</link>
            <guid>44940089</guid>
            <pubDate>Mon, 18 Aug 2025 12:57:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/awslabs/mcp/blob/main/VIBE_CODING_TIPS_TRICKS.md">https://github.com/awslabs/mcp/blob/main/VIBE_CODING_TIPS_TRICKS.md</a>, See on <a href="https://news.ycombinator.com/item?id=44940089">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>


                <li>
      

      <div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
          <p>
            GitHub Copilot
          </p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_spark&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_spark_link_product_navbar&quot;}" href="https://github.com/features/spark">
      
      <div>
          <p>
            GitHub Spark
              <span>
                New
              </span>
          </p><p>
        Build and deploy intelligent apps
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_product_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
          <p>
            GitHub Models
              <span>
                New
              </span>
          </p><p>
        Manage and compare prompts
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
          <p>
            GitHub Advanced Security
          </p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
          <p>
            Actions
          </p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    
                </ul>
              </div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
          <p>
            Codespaces
          </p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
          <p>
            Issues
          </p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
          <p>
            Code Review
          </p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
          <p>
            Discussions
          </p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
          <p>
            Code Search
          </p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
          

      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      

      <div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
          <p>
            GitHub Sponsors
          </p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
          <p>
            The ReadME Project
          </p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      

      <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
          <p>
            Enterprise platform
          </p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:awslabs/mcp" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="50zcFtmi9uBNOq_-FtfDJlXmWyfDbVAWFHrK82uIMaaCsxvjegVSiIRmYCpqK-hKN-ELF8vmnPB64h2khGAd1w" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="awslabs/mcp" data-current-org="awslabs" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=awslabs%2Fmcp" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/awslabs/mcp/blob/main/VIBE_CODING_TIPS_TRICKS.md&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="c8441e4edb2a7beb3cf80554ca34bd39a15cb86ef15e36c7d0ee321375358431" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-6ab139c7-b91d-4851-9bc3-50be92788dd3" for="icon-button-74d955e1-81a2-43ea-9c77-bcdf014eac13" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.c869ee9cf4c55200cc46.module.css">
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.6311bb0c3463e440edd1.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
    </channel>
</rss>