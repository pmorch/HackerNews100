<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 28 Jan 2025 19:30:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[New speculative attacks on Apple CPUs (198 pts)]]></title>
            <link>https://predictors.fail/</link>
            <guid>42856023</guid>
            <pubDate>Tue, 28 Jan 2025 18:31:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://predictors.fail/">https://predictors.fail/</a>, See on <a href="https://news.ycombinator.com/item?id=42856023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<section id="demos">
					<h4>Demos</h4>

					<h5>Leaking Proton Mail's Inbox Data</h5>
					<p>We train the M3 CPU's LVP via sandboxed JavaScript code running inside WebKit (Safari's browsing engine). When the mouse cursor is over our demo webpage, our proof-of-concept opens Proton Mail's inbox in a new window, but uses the same process to render the inbox. This brings the inbox content into the address space, making it accessible with a sandbox escape. Finally, we use the LVP to craft an arbitrary read primitive to anywhere in this address space, recovering the sender and subject lines shown on the inbox page.</p>
					

					<h5>Reading The Great Gatsby Using Load Address Prediction</h5>
					<p>We demonstrate an LAP proof-of-concept on the Apple M2 CPU that recovers a secret string. The string holds the first paragraph of The Great Gatsby, but is never architecturally accessed. At the LAP's incorrectly guessed memory address, we place a pointer to the characters of the string. Subsequently, we train and activate the LAP.</p>
					

					<h5>Reading Harry Potter Using Load Value Prediction</h5>
					<p>On the Apple M3 CPU, we demonstrate an LVP proof-of-concept that recovers the first paragraph of Harry Potter and the Sorcerer's Stone, which is also never architecturally read by the CPU core. We cause the LVP to predict and access an incorrect array index. There, we place the pointer to the string's characters, which the CPU then dereferences.
					</p>
					
				</section>

				<section id="people">
					<h4>The People Behind
						<span>SLAP and FLOP</span>
					</h4>
					<div>
								<ul>
									<li><a href="https://jas0n.kim/">Jason Kim </a><span><a href="https://www.gatech.edu/">Georgia Institute of
												Technology</a></span></li>
									<li><a href="https://sites.cc.gatech.edu/grads/j/jchuang9/">Jalen Chuang</a> <span><a href="https://www.gatech.edu/">Georgia
												Institute of
												Technology</a></span></li>
									<li><a href="https://faculty.cc.gatech.edu/~genkin/">Daniel Genkin</a> <span><a href="https://www.gatech.edu/">Georgia Institute of
												Technology</a></span>
									</li>
									<li><a href="https://yuval.yarom.org/">Yuval Yarom</a> <span><a href="https://www.ruhr-uni-bochum.de/">Ruhr University
												Bochum</a></span></li>
								</ul>
							</div>
				</section>

				<section id="qa">
					<h4>Frequently Asked <span>Questions</span></h4>

					<h5>SLAP and FLOP Basics</h5>
					<div id="accordion" aria-labelledby="basics-question-1">
									<p>The affected Apple devices are the following:</p>
									<ul>
										<li>All Mac laptops from 2022-present (MacBook Air, MacBook Pro)</li>
										<li>All Mac desktops from 2023-present (Mac Mini, iMac, Mac Studio, Mac Pro) </li>
										<li>All iPad Pro, Air, and Mini models from September 2021-present (Pro 6th and 7th gen., Air 6th gen., Mini 6th gen.)</li>
										<li>All iPhones from September 2021-present (All 13, 14, 15, and 16 models, SE 3rd gen.)</li>
									</ul>
								</div>
					<div id="accordion" aria-labelledby="basics-question-2">
								<p>There are hardware and software measures to ensure that two open webpages are isolated from each other, 
									   preventing one of them form (maliciously) reading the other's contents. SLAP and FLOP break these protections, 
									   allowing attacker pages to read sensitive login-protected data from target webpages. In our work, we show
										that this data ranges from location history to credit card information.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-3">
								<p> While FLOP has an actionable mitigation, implementing it requires patches from software vendors and cannot be done by users. 
									Apple has communicated to us that they plan to address these issues in an
										upcoming security update, hence it is important to enable automatic updates and
										ensure that your devices are running the latest operating system and applications.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-4">
								<p>We have not yet observed load address prediction or load value prediction 
									   in other processor vendors' products, such as Intel, AMD, Qualcomm, or Ampere.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-5">
								<p> We do not know, as we have not tested other browsers such as Firefox. </p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-6">
								<p>Since SLAP and FLOP are microarchitecture-based attacks, they do not leave any traces in
										the system's log files. While cached copies of previously visited websites may
										be present in the web browser, it is difficult to automatically detect malicious
										code patterns that exploit hardware vulnerabilities.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-7">
								<p>So far, we do not have any evidence that either SLAP or FLOP has been used in the
										wild.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-8">
								<p>We disclosed SLAP to Apple on May 24, 2024, and FLOP on September 3, 2024.</p>
							</div>

					<h5>Technical Questions</h5>
					<div id="accordion" aria-labelledby="tech-question-1">
									<p>
										Most computer bugs arise from mistakes in programming, such as missing bounds
										checks 	or use-after-frees. However, a side-channel attack exploits the implementation
										of a computer's hardware to attack it, even if the software it runs is a secure algorithm.
										Systems can leak sensitive data through sound, electromagnetic radiation, or
										thermal throttling, just for a few examples.
									</p>
									<p>
										Many side channels, including ones we use for SLAP and FLOP, comes from the
										CPU's microarchitecture. Whenever an attacker and target run on the physical CPU, they share the CPU's
										internal resources such as cores, caches, and internal buffers.
										Sharing resources leads to contention, and contention can be measured indirectly
										through several variables like timing or power consumption.
										These measurements leave fingerprints on the target's behavior on the CPU.
										Accordingly, an attacker can abuse this to make inferences about the target's
										secrets even if they are isolated at the process level or the hypervisor level.
									</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-2">
									<p>
										Virtually all modern CPUs use a performance optimization where they predict the
										control flow the CPU should take (such as branches and returns), should the
										outcome not be readily available. Once a prediction is made, the CPU will execute instructions along
										the prediction, a process called speculative execution. If the CPU realizes it had
										mispredicted, it must revert all changes in the state it performed after the
										prediction. Nearly all desktop and mobile CPUs exhibit this behavior, regardless of
										manufacturer (such as Apple, AMD, or Intel).
									</p>
									<p>
										<a href="https://spectreattack.com/">Spectre</a> is a hardware vulnerability in
										virtually all modern CPUs that occurs when speculative execution backfires.
										While the CPU should ideally revert all changes in state, speculative execution leaves
										traces in the CPU's microarchitectural state and especially the cache. A Spectre
										attack coerces the CPU into speculatively executing the wrong flow of
										instructions. If this wrong flow has instructions depending on sensitive data, their value can
										be inferred through a side channel even after the CPU realizes the mistake and
										reverts its changes. An adversary can abuse this behavior to read data that they cannot
										normally access through program semantics. Because speculative execution is an
										important part of CPU performance that is infeasible to simply remove as a
										countermeasure, Spectre continues to be dangerous to software even years after
										its discovery.
									</p>
									<p>
										In SLAP and FLOP, we demonstrate that recent Apple CPUs go beyond this, not only
										predicting the control flow the CPU should take, but also the data flow the CPU
										should operate on if data are not readily available from the memory subsystem.
										Unlike Spectre, mispredictions on data flow do not directly result in the CPU
										speculatively executing the wrong instructions. Instead, they result in the CPU
										executing arbitrary instructions on the wrong data. However, we show this can be combined with
										indirection techniques to execute wrong instructions.
									</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-3">
									<p>To orchestrate SLAP, we begin by reverse engineering Apple's implementation of Load Address Prediction (LAP). 
									We discover that if we train the LAP on striding memory addresses, the LAP will access address the next sequence 
									in the striding pattern and compute using the data in that address, even if the program never actually accesses it.
									Here, we note that this is different from hardware prefetching. While prefetchers may bring the data inside the predicted addresses, 
									they do not speculatively execute downstream instructions based on the prediction. </p>
									<p>Next, we find an attack surface in Safari. Previously, <a href="https://ileakage.com/">iLeakage</a> demonstrated a corner case 
									in Safari's isolation scheme where an adversary's webpage can coerce an arbitrary target webpage to be handled by the same process. 
									We find that when this occurs, the two webpages also share internal memory allocation regions for data, such as strings. 
									In turn, this allows the adversary to jump the LAP to the target webpage's string and trick the CPU into operating on it, eventually leaking the string's content over a covert channel.</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-4">
									<p>Similarly to SLAP, we reverse engineer the Load Value Prediction (LVP) mechanism in Apple CPUs. 
									We found that if the LVP sees the same data value being repeatedly returned from the memory subsystem for the same load instruction, 
									the LVP will attempt to guess the load's outcome the next time that load instruction executes, even if the memory accessed by the load now contains a completely different value! 
									Therefore, using the LVP, we can trick the CPU into computing on incorrect data values.</p>
									<p>We first demonstrate the dangers stemming from LVP in Safari, whose JavaScript engine first vets the type information of JavaScript data structures before determining the 
									appropriate computations to run on them. If we train the LVP on the load instruction that retrieves this type information, we can cause code that is only supposed to run for 
									one data structure on another data structure, causing speculative type confusion, and obtaining a read primitive to arbitrary 64-bit addresses. </p>
									
									<p> Next, we move to Chrome, where internal table data structures for calling WebAssembly functions also vet the signature of each function before calling them with arguments. 
									Here, we show that the LVP allows us to run a function with the wrong arguments (e.g., pointer instead of integer), again resulting in a type confusion based primitive for reading arbitrary memory addresses.</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-5">
									<p>SLAP exploits a phenomenon in Safari where strings that belong to different webpages can be allocated within a close distance to each other, and thus discloses cross-origin strings that are allocated in proximity to the adversary's own strings. On the other hand, FLOP is a speculative type confusion attack that causes the CPU to bypass integrity checks on data structures, resulting in memory read primitives from arbitrary addresses in Safari and Chrome.</p>
									<p>
									Furthermore, the underlying CPU microarchitecture that SLAP and FLOP exploit are also different. SLAP uses the Load Address Predictor (LAP), while FLOP uses the Load Value Predictor (LVP).
									As suggested by their names, the LAP predicts addresses while the LVP predicts values. 
									Consider the following statement: "The CPU accesses memory at address 0xdeadbeef, 
									which contains the value 0x1234." The next time the CPU performs a memory access, the LAP predicts the next address, i.e., what 0xdeadbeef will change to. Meanwhile, the LVP predicts the next value returned from memory, that is, what 0x1234 will change to. 
									Going deeper in detail, we observe their internal structures are also different. For instance, the LAP requires a longer training sequence than the LVP to activate reliably, but only the LAP can observe strides and generate predictions accordingly.</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-6">
								<p>
										JavaScript and WebAssembly are two programming languages that make up the
										backbone of interactive webpages, such as online games and video streaming
										services. JavaScript can update the content of the website directly, while
										WebAssembly is used for high-performance web applications. Ultimately,
										WebAssembly interfaces with JavaScript to deliver dynamic content to users.
										Since both are sandboxed in a browser environment, side-channel attacks are
										notably more difficult to implement in these languages. However, the impact is
										drastically greater, as browsers execute both types of code automatically and do
										not require the user to download the malicious program.
									</p>
							</div>
					<div id="accordion" aria-labelledby="tech-question-7">
									<p>For leaking secrets, both SLAP and FLOP are confined to the address space they are trained in. 
									As pointed out by <a href="https://ileakage.com/">iLeakage</a>, Safari lacks Site Isolation, a measure used to enforce that two different webpages not from the same domain can never be handled by the same process.
									Thus, in Safari it is possible for an adversary's webpage to be handled by the same process (and thus address space) with an arbitrary webpage, increasing the attack surface including LAP- and LVP-based exploits.
									 </p>
									
									<p>On the other hand, although Chrome is equipped with Site Isolation, we demonstrate that it is not a perfect mitigation. We show the real-world existence of corner cases, where two subdomains of the same site can be merged into one process, again leading to LAP- and LVP-based attacks.</p>
								</div>

					<h5>Miscellaneous</h5>
					<div id="accordion" aria-labelledby="misc-question-1">
									<p>Yes, with rights waived via <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC0</a>. You can
										right-click each logo, which should list an option to save the image as a file.
									</p></div>
					
					

				</section>
				<section id="acknowledgments">
					<h4>Acknowledgments</h4>
					<div><p>
							This research was supported by
							the Air Force Office of Scientific Research (AFOSR) under award number FA9550-24-1-0079;
							the Alfred P Sloan Research Fellowship;
							an ARC Discovery Project number DP210102670;
							the Defense Advanced Research Projects Agency (DARPA) under contract numbers
							W912CG-23-C-0022,
							the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's
							Excellence Strategy - EXC 2092 CASA - 390781972;
							and gifts from Qualcomm, Cisco (SLAP), and Zama (FLOP).
							</p><p>
							The views and conclusions contained in this document are those of the authors and should not
							be interpreted as representing the official policies, either expressed or implied, of the
							U.S. Government.
						</p></div>
				</section>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Berkeley Researchers Replicate DeepSeek R1's Core Tech for Just $30: A Small Mod (156 pts)]]></title>
            <link>https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek</link>
            <guid>42855283</guid>
            <pubDate>Tue, 28 Jan 2025 17:36:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek">https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek</a>, See on <a href="https://news.ycombinator.com/item?id=42855283">Hacker News</a></p>
Couldn't get https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[How has DeepSeek improved the Transformer architecture? (121 pts)]]></title>
            <link>https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture</link>
            <guid>42855170</guid>
            <pubDate>Tue, 28 Jan 2025 17:29:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture">https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture</a>, See on <a href="https://news.ycombinator.com/item?id=42855170">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>DeepSeek has recently released DeepSeek v3, which is currently state-of-the-art in benchmark performance among open-weight models, alongside <a href="https://arxiv.org/abs/2412.19437">a technical report</a> describing in some detail the training of the model. Impressively, they’ve achieved this SOTA performance by only using 2.8 million H800 hours of training hardware time—equivalent to about 4e24 FLOP if we assume 40% MFU. This is about ten times less training compute than the similarly performing Llama 3.1 405B.</p>

<p>In this issue, I’ll cover some of the important architectural improvements that DeepSeek highlight in their report and why we should expect them to result in better performance compared to a vanilla Transformer. The full technical report contains plenty of non-architectural details as well, and I strongly recommend reading it if you want to get a better idea of the engineering problems that have to be solved when orchestrating a moderate-sized training run.</p>

<figure>
  <img src="https://epoch.ai/assets/images/gradient-updates/2025/how-has-deepseek-improved-the-transformer-architecture/figure-1.png">
<figcaption>
    <p>Figure 1: The DeepSeek v3 architecture with its two most important improvements: DeepSeekMoE and multi-head latent attention (MLA). Multi-token prediction is not shown. From the DeepSeek v3 technical report.</p>
  </figcaption>
</figure>



<h2 id="multi-head-latent-attention-mla">Multi-head latent attention (MLA)</h2>

<p>Multi-head latent attention (abbreviated as MLA) is the most important architectural innovation in DeepSeek’s models for long-context inference. This technique was first introduced in DeepSeek v2 and is a superior way to reduce the size of the KV cache compared to traditional methods such as grouped-query and multi-query attention.</p>

<p>I’ll start with a brief explanation of what the KV cache is all about. If you’re familiar with this, you can skip directly to the next subsection.</p>

<h3 id="what-is-the-kv-cache-and-why-does-it-matter">What is the KV cache and why does it matter?</h3>

<p>When a Transformer is used to generate tokens sequentially during inference, it needs to see the context of all of the past tokens when deciding which token to output next. The naive way to do this is to simply do a forward pass including all past tokens every time we want to generate a new token, but this is inefficient because those past tokens have already been processed before. We would just be recomputing results we’ve already obtained previously and discarded.</p>

<p>To avoid this recomputation, it’s efficient to cache the relevant internal state of the Transformer for all past tokens and then retrieve the results from this cache when we need them for future tokens. Because the only way past tokens have an influence on future tokens is through their key and value vectors in the attention mechanism, it suffices to cache these vectors. This is where the name key-value cache, or KV cache for short, comes from.</p>

<p>This works well when context lengths are short, but can start to become expensive when they become long. This is because cache reads are not free: we need to save all those vectors in GPU high-bandwidth memory (HBM) and then load them into the tensor cores when we need to involve them in a computation. If each token needs to know all of its past context, this means for each token we generate we must read the entire past KV cache from HBM.</p>

<p>In a vanilla Transformer using a standard multi-head attention mechanism, the number of KV cache parameters per past token can be expressed as:</p>

<p>2 * attention head dimension * number of attention heads * number of Transformer blocks</p>

<p>For instance, GPT-3 had 96 attention heads with 128 dimensions each and 96 blocks, so for each token we’d need a KV cache of 2.36M parameters, or 4.7 MB at a precision of 2 bytes per KV cache parameter.</p>

<p>GPT-3 didn’t support long context windows, but if for the moment we assume it did, then each additional token generated at a 100K context length would require 470 GB of memory reads, or around 140 ms of H100 time given the H100’s HBM bandwidth of 3.3 TB/s. The price per million tokens generated at $2 per hour per H100 would then be $80, around 5 times more expensive than Claude 3.5 Sonnet’s price to the customer (which is likely significantly above its cost to Anthropic itself). This naive cost can be brought down e.g. by speculative sampling, but it gives a decent ballpark estimate.</p>

<p>This rough calculation shows why it’s crucial to find ways to reduce the size of the KV cache when we’re working with context lengths of 100K or above. The most popular way in open-source models so far has been grouped-query attention. In this architectural setting, we assign multiple query heads to each pair of key and value heads, effectively grouping the query heads together - hence the name of the method. This cuts down the size of the KV cache by a factor equal to the group size we’ve chosen. In models such as Llama 3.3 70B and Mistral Large 2, grouped-query attention reduces the KV cache size by around an order of magnitude.</p>

<h3 id="beating-grouped-query-attention">Beating grouped-query attention</h3>

<p>The fundamental problem with methods such as grouped-query attention or KV cache quantization is that they involve compromising on model quality in order to reduce the size of the KV cache. Instead of this, DeepSeek has found a way to reduce the KV cache size <em>without</em> compromising on quality, at least in their internal experiments.</p>

<p>They accomplish this by turning the computation of key and value vectors from the residual stream into a two-step process. In a vanilla Transformer, key and value vectors are computed by directly multiplying the residual stream vector by a matrix of the shape</p>

<p>(number of heads · head dimension) x (model dimension)</p>

<p>DeepSeek’s method essentially forces this matrix to be low rank: they pick a latent dimension and express it as the product of two matrices, one with dimensions latent times model and another with dimensions (number of heads · head dimension) times latent. Then, during inference, we only cache the latent vectors and not the full keys and values. We can then shrink the size of the KV cache by making the latent dimension smaller.</p>

<figure>
  <img src="https://epoch.ai/assets/images/gradient-updates/2025/how-has-deepseek-improved-the-transformer-architecture/figure-2.png">
<figcaption>
    <p>Figure 2: An illustration of multi-head latent attention from the DeepSeek v2 technical report.</p>
  </figcaption>
</figure>

<p>Naively, this shouldn’t fix our problem, because we would have to recompute the actual keys and values every time we need to generate a new token. After all, we need the full vectors for attention to work, not their latents. Multi-head latent attention is based on the clever observation that this is actually not true, because we can merge the matrix multiplications that would compute the upscaled key and value vectors from their latents with the query and post-attention projections, respectively.</p>

<p>The reason low-rank compression is so effective is because there’s plenty of information overlap between what different attention heads need to know about. If we used low-rank compression on the key and value vectors of individual heads instead of all keys and values of all heads stacked together, the method would simply be equivalent to using a smaller head dimension to begin with and we would get no gain. Exploiting the fact that different heads need access to the same information is essential for the mechanism of multi-head latent attention.</p>

<p>Methods such as grouped-query attention exploit the possibility of the same overlap, but they do so ineffectively by forcing attention heads that are grouped together to all respond similarly to queries. In other words, information sharing becomes coupled to having identical behavior in some restricted sense, a clearly undesirable property. Low-rank compression, on the other hand, allows the same <em>information</em> to be used in <em>very different ways</em> by different heads. In theory, this could even have beneficial regularizing effects on training, and DeepSeek reports finding such effects in their technical reports.</p>

<p>I see this as one of those innovations that look obvious in retrospect but that require a good understanding of what attention heads are actually doing to come up with. Once you see the approach, it’s immediately obvious that it cannot be any worse than grouped-query attention and it’s also likely to be significantly better. However, coming up with the idea of trying this is another matter.</p>

<h2 id="mixture-of-experts-innovations">Mixture-of-experts innovations</h2>

<p>One of the most popular improvements to the vanilla Transformer was the introduction of mixture-of-experts (MoE) models. These models divide the feedforward blocks of a Transformer into multiple distinct experts and add a routing mechanism which sends each token to a small number of these experts in a context-dependent manner. This means the model can have more parameters than it activates for each specific token, in a sense decoupling how much the model knows from the arithmetic cost of processing individual tokens. Probably the most influential model that is currently known to be an MoE is the original GPT-4.</p>

<p>Expert routing algorithms work as follows: once we exit the attention block of any layer, we have a residual stream vector that is the output. Each expert has a corresponding expert vector of the same dimension, and we decide which experts will become activated by looking at which ones have the highest inner products with the current residual stream.</p>

<p>The problem with this is that it introduces a rather ill-behaved discontinuous function with a discrete image at the heart of the model, in sharp contrast to vanilla Transformers which implement continuous input-output relations. This causes gradient descent optimization methods to behave poorly in MoE training, often resulting in “routing collapse”, where the model gets stuck always activating the same few experts for every token instead of spreading its knowledge and computation around all of the available experts.</p>

<p>To get an intuition for routing collapse, consider attempting to train a model such as GPT-4 with 16 experts in total and 2 experts active per token. Now, suppose that for random initialization reasons two of these experts just happen to be the best performing ones at the start. Gradient descent will then reinforce the tendency to pick these experts. This will mean these experts will get almost all of the gradient signals during updates and become better while other experts lag behind, and so the other experts will continue not being picked, producing a positive feedback loop that results in other experts never getting chosen or trained.</p>

<p>The fundamental issue is that gradient descent just heads in the direction that’s locally best. This usually works fine in the very high dimensional optimization problems encountered in neural network training. However, when our neural network is so discontinuous in its behavior, even the high dimensionality of the problem space may not save us from failure.</p>

<p>It is nontrivial to address these training difficulties. DeepSeek v3 does so by combining several different innovations, each of which I will discuss in turn.</p>

<h3 id="auxiliary-loss-free-load-balancing">Auxiliary-loss-free load balancing</h3>

<p>A popular method for avoiding routing collapse is to force “balanced routing”, i.e. the property that each expert is activated roughly an equal number of times over a sufficiently large batch, by adding to the training loss a term measuring how imbalanced the expert routing was in a particular batch. This term is called an “auxiliary loss” and it makes intuitive sense that introducing it pushes the model towards balanced routing. However, the DeepSeek v3 technical report notes that such an auxiliary loss hurts model performance even if it ensures balanced routing.</p>

<p>Their alternative is to add expert-specific bias terms to the routing mechanism which get added to the expert affinities. These bias terms are not updated through gradient descent but are instead adjusted throughout training to ensure load balance: if a particular expert is not getting as many hits as we think it should, then we can slightly bump up its bias term by a fixed small amount every gradient step until it does. The technical report notes this achieves better performance than relying on an auxiliary loss while still ensuring appropriate load balance.</p>

<h3 id="shared-experts">Shared experts</h3>

<p>A serious problem with the above method of addressing routing collapse is that it assumes, without any justification, that an optimally trained MoE would have balanced routing. However, this is a dubious assumption.</p>

<p>To see why, consider that any large language model likely has a small amount of information that it uses a lot, while it has a lot of information that it uses rather infrequently. For instance, almost any English request made to an LLM requires the model to know how to speak English, but almost no request made to an LLM would require it to know who the King of France was in the year 1510. So it’s quite plausible the optimal MoE should have a few experts which are accessed a lot and store “common information”, while having others which are accessed sparsely and store “specialized information”.</p>

<p>If we force balanced routing, we lose the ability to implement such a routing setup and have to redundantly duplicate information across different experts. However, if we don’t force balanced routing, we face the risk of routing collapse. To escape this dilemma, DeepSeek separates experts into two types: <em>shared experts</em> and <em>routed experts</em>. Shared experts are always routed to no matter what: they are excluded from both expert affinity calculations and any possible routing imbalance loss term. We concern ourselves with ensuring balanced routing only for routed experts.</p>

<p>The key observation here is that “routing collapse” is an extreme situation where the likelihood of each individual expert being chosen is either 1 or 0. Naive load balancing addresses this by trying to push the distribution to be uniform, i.e. every expert should have the same chance of being selected. However, if our sole concern is to avoid routing collapse then there’s no reason for us to target specifically a uniform distribution. DeepSeek v3 instead targets a distribution where each expert is either selected for sure (probability 1) or selected with some fixed probability p &gt; 0 for each token.</p>

<p>I think it’s likely even this distribution is not optimal and a better choice of distribution will yield better MoE models, but it’s already a significant improvement over just forcing a uniform distribution.</p>

<h2 id="multi-token-prediction">Multi-token prediction</h2>

<p>The final change that DeepSeek v3 makes to the vanilla Transformer is the ability to predict multiple tokens out for each forward pass of the model. This allows them to use a multi-token prediction objective during training instead of strict next-token prediction, and they demonstrate a performance improvement from this change in ablation experiments.</p>

<p>The basic idea is the following: we first do an ordinary forward pass for next-token prediction. As we would in a vanilla Transformer, we use the final residual stream vector to generate next token probabilities through unembedding and softmax. However, unlike in a vanilla Transformer, we <em>also</em> feed this vector into a subsequent Transformer block, and we use the output of that block to make predictions about the second next token. We can iterate this as much as we like, though DeepSeek v3 only predicts two tokens out during training.</p>

<figure>
  <img src="https://epoch.ai/assets/images/gradient-updates/2025/how-has-deepseek-improved-the-transformer-architecture/figure-3.png">
<figcaption>
    <p>Figure 3: An illustration of DeepSeek v3’s multi-token prediction setup taken from its technical report.</p>
  </figcaption>
</figure>

<p>They incorporate these predictions about further out tokens into the training objective by adding an additional cross-entropy term to the training loss with a weight that can be tuned up or down as a hyperparameter. This not only gives them an additional target to get signal from during training but also allows the model to be used to <a href="https://arxiv.org/abs/2211.17192">speculatively decode</a> itself. We can generate a few tokens in each forward pass and then show them to the model to decide from which point we need to reject the proposed continuation.</p>

<p>DeepSeek v3 only uses multi-token prediction up to the second next token, and the acceptance rate the technical report quotes for second token prediction is between 85% and 90%. This is quite impressive and should allow nearly double the inference speed (in units of tokens per second per user) at a fixed price per token if we use the aforementioned speculative decoding setup. It doesn’t look worse than the acceptance probabilities one would get when decoding Llama 3 405B with Llama 3 70B, and might even be better.</p>

<p>I’m curious what they would have obtained had they predicted further out than the second next token. If e.g. each subsequent token gives us a 15% relative reduction in acceptance, it might be possible to squeeze out some more gain from this speculative decoding setup by predicting a few more tokens out.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I see many of the improvements made by DeepSeek as “obvious in retrospect”: they are the kind of innovations that, had someone asked me in advance about them, I would have said were good ideas. However, as I’ve said earlier, this doesn’t mean it’s easy to come up with the ideas in the first place.</p>

<p>I’ve heard many people express the sentiment that the DeepSeek team has “good taste” in research. Based just on these architectural improvements I think that assessment is right. None of these improvements seem like they were found as a result of some brute-force search through possible ideas. Instead, they look like they were carefully devised by researchers who understood how a Transformer works and how its various architectural deficiencies can be addressed.</p>

<p>If I had to guess where similar improvements are likely to be found next, probably prioritization of compute would be a good bet. Right now, a Transformer spends the same amount of compute per token regardless of which token it’s processing or predicting. This seems intuitively inefficient: the model should think more if it’s making a harder prediction and less if it’s making an easier one. To some extent this can be incorporated into an inference setup through variable test-time compute scaling, but I think there should also be a way to incorporate it into the architecture of the base models directly.</p>


          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sam Altman said startups with $10M were 'hopeless' competing with OpenAI (125 pts)]]></title>
            <link>https://www.tomshardware.com/tech-industry/artificial-intelligence/sam-altman-said-startups-with-only-usd10-million-were-totally-hopeless-competing-with-openai-deepseeks-disruption-says-otherwise</link>
            <guid>42854525</guid>
            <pubDate>Tue, 28 Jan 2025 16:48:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/sam-altman-said-startups-with-only-usd10-million-were-totally-hopeless-competing-with-openai-deepseeks-disruption-says-otherwise">https://www.tomshardware.com/tech-industry/artificial-intelligence/sam-altman-said-startups-with-only-usd10-million-were-totally-hopeless-competing-with-openai-deepseeks-disruption-says-otherwise</a>, See on <a href="https://news.ycombinator.com/item?id=42854525">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1200-80.jpg" alt="Sam Altman at a Q&amp;amp;A in June 2023" srcset="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ.jpg" data-pin-nopin="true" fetchpriority="high" crossorigin="anonymous">
</picture>
</div>
<figcaption>
<span>(Image credit: ET Now video)</span>
</figcaption>
</div>

<div id="article-body">
<p>Sam Altman's comments on the prospects of startups hoping to break through in the AI business may have come back to bite him. Several posts on X (and probably other platforms) ridicule the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer">OpenAI</a> boss and co-founder's dismissal of potential competition emanating from the startup scene, particularly those with only limited financial resources in the range of $10 million. Altman's comments were made during a Q&amp;A session after a 'Conversations' presentation to India VCs, recorded in June 2023. The comments seem way off the mark in early 2025, with DeepSeek now on the scene claiming its groundbreaking model only cost $5.6 million to train.</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">This is pretty hilarious in retrospect.In India in 2023, Altman was asked how if a small, smart team with a budget of $10 million could build something substantial within AI.His reply: "It’s totally hopeless to compete with us on training foundation models" https://t.co/pdYIhV2x1m<a href="https://twitter.com/cantworkitout/status/1884113684978622538" data-url="https://twitter.com/cantworkitout/status/1884113684978622538" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">January 28, 2025</a></p></blockquote></div><p>Entrepreneur Arnaud Bertrand reckons that Altman's response in the above clip is "pretty hilarious in retrospect." In other words, Bertrand thinks Altman's dismissal of the Indian VC's question about startups challenging the likes of OpenAI showed a startling lack of foresight.</p><p>The video begins with the VC stating that there is a very vibrant startup ecosystem in India. He goes on to muse whether Altman might see a gap in the AI business, one which an Indian startup could fill. More specifically, the VC asks whether a trio of super-smart engineers from India "with say, not $100M, but $10M – could build something truly substantial?"</p><p>Altman's response was quite dismissive of the VC's well-mannered query. "Look, the way this works is we're going to tell you it's totally hopeless to compete with us on training foundation models. You shouldn't try, and it's your job to try anyway, and I believe both of those things," was Altman's disjointed stream-of-consciousness style reply, followed by audience titters. "I think it is pretty hopeless," he added, possibly wishing to soften his initial response.</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">deepseek's r1 is an impressive model, particularly around what they're able to deliver for the price.we will obviously deliver much better models and also it's legit invigorating to have a new competitor! we will pull up some releases.<a href="https://twitter.com/cantworkitout/status/1884066337103962416" data-url="https://twitter.com/cantworkitout/status/1884066337103962416" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">January 28, 2025</a></p></blockquote></div><p>To Altman's credit, earlier today he posted a thread on X praising the catalyst behind his recent social media ridiculing. The launch of China's DeepSeek has caused significant AI business and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidia-loses-usd589-billion-in-market-cap-broad-stock-plunge-triggered-by-deepseek-ai-release" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidia-loses-usd589-billion-in-market-cap-broad-stock-plunge-triggered-by-deepseek-ai-release">tech industry tremors</a>, and Altman has now publicly admitted it is "an impressive model, particularly around what they're able to deliver for the price."</p><p>Nevertheless, like the funding-hungry CEO he is, Altman quickly turned the thread around to OpenAI promising jam tomorrow, with the execution of the firm's roadmap, amazing next-gen AI models, and "bringing you all AGI and beyond."</p><p>The amount of money DeepSeek truly spent on training its model, which it claims is $5.6 million, is contested. However, despite those contentions, it is clear that the company pulled off training a frontier model with disruptively low costs, shocking the US titans of AI.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-HyBY2hdXNd75Zxm2t4xDaJ"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div>
</div>



<!-- Drop in a standard article here maybe? -->


<div id="slice-container-authorBio-HyBY2hdXNd75Zxm2t4xDaJ"><p>Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.</p></div>
</section>





<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bitwarden introduces mandatory 2FA for new devices (157 pts)]]></title>
            <link>https://bitwarden.com/help/new-device-verification/</link>
            <guid>42853696</guid>
            <pubDate>Tue, 28 Jan 2025 15:51:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bitwarden.com/help/new-device-verification/">https://bitwarden.com/help/new-device-verification/</a>, See on <a href="https://news.ycombinator.com/item?id=42853696">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>To keep your account safe and secure, in February 2025, Bitwarden will require additional verification <strong>for users who do not use two-step login</strong>. After entering your Bitwarden master password, you will be prompted to enter a one-time verification code sent to your account email to complete the login process <strong>when logging in from a device you have not logged in to previously</strong>. For example, if you are logging in to a mobile app or a browser extension that you have used before, you will not receive this prompt.</p><p>Most users will not experience this prompt unless they are frequently logging into new devices. This verification is only needed for new devices or after clearing browser cookies.</p><p>If you regularly access your email, retrieving the verification code should be straightforward. If you prefer not to rely on your Bitwarden account email for verification, you can set up two-step login through an Authenticator app, a hardware key, or two-step login via a different email.</p><p>Users affected by this change will see the following in-product communication and should have received an email informing them of the change:</p><figure><img alt="New device verification announcement" id="3194724a-920f-5941-8c03-e54e6e214cb2" height="532" width="468" src="https://res.cloudinary.com/bw-com/image/upload/f_auto/v1/ctf/7rncvj1f8mw7/2tYcj1sDClqQ23ANh7nrIq/b88330b0a660fa47baaf59c68e0b9178/541c2ef1-b086-4489-a21f-a21fa6c1a905.png?_a=DAJAUVWIZAA0"><figcaption><cite>New device verification announcement</cite></figcaption></figure><p id="faqs"><a href="#faqs" title="#faqs"><svg version="1.1" viewBox="0 0 32 32"><path d="M30.939 9.669l-7.372-0.014 1.332-8.422c0.022-0.138 0.016-0.279-0.016-0.415s-0.092-0.264-0.174-0.377c-0.082-0.113-0.186-0.209-0.305-0.282s-0.251-0.122-0.389-0.144-0.279-0.016-0.415 0.016c-0.136 0.033-0.264 0.092-0.377 0.174-0.229 0.166-0.382 0.415-0.426 0.694l-1.388 8.75-8.728-0.018 1.316-8.31c0.028-0.141 0.027-0.286-0.002-0.426s-0.087-0.274-0.169-0.391c-0.082-0.118-0.187-0.218-0.309-0.294s-0.257-0.127-0.399-0.15c-0.142-0.022-0.287-0.016-0.426 0.019s-0.27 0.097-0.384 0.184c-0.114 0.087-0.21 0.195-0.282 0.32s-0.117 0.262-0.134 0.405l-1.366 8.64-7.776-0.016c-0.283 0-0.554 0.112-0.754 0.312s-0.312 0.471-0.312 0.754c0 0.283 0.112 0.554 0.312 0.754s0.471 0.312 0.754 0.312l7.442 0.014-1.4 8.876-7.718-0.016c-0.283 0-0.554 0.112-0.754 0.312s-0.312 0.471-0.312 0.754c0 0.283 0.112 0.554 0.312 0.754s0.471 0.312 0.754 0.312l7.384 0.016-1.266 8c-0.044 0.279 0.024 0.564 0.19 0.793s0.416 0.382 0.694 0.427c0.052 0.009 0.104 0.013 0.156 0.014 0.254 0 0.499-0.091 0.692-0.256s0.32-0.394 0.358-0.644l1.32-8.33 8.73 0.018-1.248 7.89c-0.044 0.279 0.024 0.564 0.19 0.793s0.415 0.382 0.694 0.427c0.056 0.009 0.112 0.013 0.168 0.014 0.253-0.001 0.498-0.092 0.691-0.257s0.32-0.393 0.359-0.643l1.3-8.218 7.762 0.016c0.283 0 0.554-0.112 0.754-0.312s0.312-0.471 0.312-0.754c0-0.283-0.112-0.554-0.312-0.754s-0.471-0.312-0.754-0.312l-7.428-0.016 1.4-8.876 7.704 0.016c0.283 0 0.554-0.112 0.754-0.312s0.312-0.471 0.312-0.754-0.112-0.554-0.312-0.754c-0.2-0.2-0.471-0.312-0.754-0.312h0.014zM19.671 20.657l-8.732-0.018 1.4-8.876 8.73 0.018-1.398 8.876z"></path></svg></a><h2>FAQs</h2></p><h4>When will this happen?</h4><p>This change will go into effect starting February 2025.</p><h4>Why is Bitwarden implementing this?</h4><p>Bitwarden is implementing this change to enhance security for users who don't have two-step login activated. If someone gains access to your password, they still won't be able to log into your account without secondary verification (the code sent to your email). This extra layer helps protect your data from hackers who often target weak or exposed passwords to gain unauthorized access.</p><h4>When will I get prompted for this verification?</h4><p>You will only get prompted for this verification when logging in from new devices. If you’re logging into a device that you’ve used before, you will not be prompted.&nbsp; </p><h4>What is considered a new device?&nbsp;</h4><p>A new device is any device that hasn't been previously used to log into your Bitwarden account. This could include a new phone, tablet, computer, or browser extension that you’ve never logged in from before. When you log in from a new device, you'll be asked to verify your identity via a one-time code sent to your email.&nbsp;</p><p>Other scenarios that will initiate a new device will be:</p><ul><li><p>If you uninstall and reinstall the mobile or desktop app</p></li><li><p>Clearing browser cookies&nbsp;<br></p></li></ul><h4>My email credentials are saved in Bitwarden. Will I be locked out of Bitwarden?</h4><p>Email verification codes will only be required on new devices for users that do not have two-step login enabled. You will not see this prompt on previously logged in devices and you will log in as normal with your account email and your master password.&nbsp;</p><p>If you are logging into a new device, your Bitwarden account email will receive a one-time verification code. If you have access to your email, i.e. a persistent logged in email on your mobile phone, then you will be able to grab the one-time verification code to log in. Once logged in to the new device, you will not be prompted again for the verification code.&nbsp;</p><p>If you regularly log into your email using credentials saved in Bitwarden or do not want to rely on your email for verification, you should set up two-step login that will be independent from the Bitwarden account email. This includes an authenticator app, security key, or email-based two-step login with a different email. Having any 2FA method active will opt the user out of the email-based new device verification. Users with 2FA active should also save their Bitwarden <a href="https://bitwarden.com/help/two-step-recovery-code/">recovery code</a> in a safe place.</p><h4>Who is excluded from this account email-based new device verification?</h4><p>The following categories of logins are excluded:</p><ul><li><p>Users who have two-step login set up are excluded.</p></li><li><p>Users who log in with SSO, a passkey, or with an API key are excluded.</p></li><li><p>Self-hosted users are excluded.</p></li><li><p>Users who log in from a device where they have previously logged in are excluded.</p></li></ul><h4>My organization uses SSO, do my users have to complete new device verification?</h4><p>No. Users logging in with SSO will be exempt and not asked to verify the login on a new device. However, if a user, without two-step login enabled, logs in with a username and password without going through SSO, they will be asked to verify the new device.</p><h4>I do not want to share my real email with Bitwarden, how can I set up my account?</h4><p>Users who want to remain anonymous have several options available:</p><ul><li><p>Use a two-step login option that doesn’t require an email, including an authenticator app, security key, or email-based two-step login with a different email.</p></li><li><p>Use an email alias forwarding service.</p></li><li><p>Self-host Bitwarden.</p></li></ul><p>Bitwarden encourages users to have an active email, as Bitwarden sends important security alerts like failed login attempts.</p><div id="help-page-suggestions"><p><span><h3>Suggest changes to this page</h3><p>How can we improve this page for you? <br>For technical, billing, and product questions, please <a href="https://bitwarden.com/contact/">contact support</a></p></span></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Almost one in 10 people use the same four-digit PIN (138 pts)]]></title>
            <link>https://www.abc.net.au/news/2025-01-28/almost-one-in-ten-people-use-the-same-four-digit-pin/103946842</link>
            <guid>42853617</guid>
            <pubDate>Tue, 28 Jan 2025 15:45:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.abc.net.au/news/2025-01-28/almost-one-in-ten-people-use-the-same-four-digit-pin/103946842">https://www.abc.net.au/news/2025-01-28/almost-one-in-ten-people-use-the-same-four-digit-pin/103946842</a>, See on <a href="https://news.ycombinator.com/item?id=42853617">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="Decoy" data-key="article"><p>Find out if you're one of them.</p><p>The last line of security for much of your digital life probably isn't as secure as you think.</p><p>Whether it's to unlock your smartphone, access your online banking or get cash out of the ATM, a four-digit PIN is often there to keep your secrets and your money safe.</p><p>It’s an important little code, but not all choices are equally secure.</p><p>That's why we analysed 29 million of them from <a href="https://haveibeenpwned.com/" data-component="Link" target="_blank" rel="noopener noreferrer">Have I Been Pwned?</a> – an Australian-run site that helps people all over the world find out if they've been affected by data breaches.</p><p>The most commonly used PINs turned out to be staggeringly popular, meaning they're particularly easy to guess when phones and bank cards fall into the wrong hands.</p><p>This grid of green squares might remind you of an old Space Invaders game, but it's actually something like a mind-reader.</p><figure role="group" data-print="inline-media" aria-labelledby="104257640" data-component="Figure" data-uri="coremedia://image/104257640"><div><p><img alt="A grid of all 10,000 PIN codes, with two bright lines across it showing where the most popular ones are" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/29b2240ce9d896959a78aa2e10cbdfd8?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257640" data-component="Figure__figcaption"> <!-- -->How popular each of the 10,000 possible PIN codes are.</figcaption></figure><p>It's going to let us peer inside and find out why humans choose some PINs more than others.</p><p>Every square in the grid represents a four-digit code.</p><p>We've highlighted <strong>4560</strong> as an example to get you across how it works.</p><p>The grid is arranged by splitting the digits of each code into pairs.</p><p>The first two digits (<strong>45</strong>) are taken from the vertical axis, and the last two digits (<strong>60</strong>) are from the horizontal one.</p><p>The brighter the square, the more popular the code – which means the blocks of bright squares are the ones we need to avoid when choosing a PIN.</p><p>So, which number is the clear favourite? Chances are you've used it at some stage.</p><figure role="group" data-print="inline-media" aria-labelledby="104257642" data-component="Figure" data-uri="coremedia://image/104257642"><div><p><img alt="1234 is highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/87cdc6eee4aef6f923fe3374c144a23f?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257642" data-component="Figure__figcaption"> <!-- -->1234 is easily the most popular four-digit PIN.</figcaption></figure><p><strong>1234</strong> is the most popular choice by a huge margin, accounting for nearly one in 10 of the millions of PINs we looked at.</p><p>And then there's the diagonal line running from the bottom-left corner to the top-right one.</p><p>It stands out, and that's because it's made up of PINs that use repeated digits...</p><p>... like <strong>0000</strong>, which is the second most popular code.</p><figure role="group" data-print="inline-media" aria-labelledby="104257644" data-component="Figure" data-uri="coremedia://image/104257644"><div><p><img alt="1111 is highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/1cf7163c5868ca943d8b42247f185296?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257644" data-component="Figure__figcaption"> <!-- -->1111 sits on the diagonal line from bottom-left to top-right.</figcaption></figure><p>And right behind it is <strong>1111</strong>.</p><p><strong>1212</strong> and <strong>4444</strong> are in the top ten as well.</p><figure role="group" data-print="inline-media" aria-labelledby="104257646" data-component="Figure" data-uri="coremedia://image/104257646"><div><p><img alt="1212 and 4444 are highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/363ac8ca9bb7865ddb26fc788dea7ef4?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257646" data-component="Figure__figcaption"> <!-- -->1212 and 4444 are also popular choices with repeated digits.</figcaption></figure><p>There's also a (broken) horizontal line, split between <strong>19</strong> and <strong>20</strong> for the first two digits.</p><p>What does that remind you of?</p><figure role="group" data-print="inline-media" aria-labelledby="104257648" data-component="Figure" data-uri="coremedia://image/104257648"><div><p><img alt="2004 and 1986 is highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/dbaef0e2a139e7aec8bdb5ca03a15dab?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257648" data-component="Figure__figcaption"> <!-- -->1986 is the most popular year to use as a PIN code.</figcaption></figure><p>They are the birth years of people who are alive today.</p><p><strong>1986</strong> is the most popular of these, while <strong>2004</strong> is also in the top 20.</p><p>There's also a block-ish area around the bottom left that needs some explaining.</p><figure role="group" data-print="inline-media" aria-labelledby="104257650" data-component="Figure" data-uri="coremedia://image/104257650"><div><p><img alt="2512 is highlighted on the grid of all possible four-digit codes. It sits inside a block from 0101 to 3112" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/78449ac5a9d503082ab3763a8b9b4c82?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257650" data-component="Figure__figcaption"> <!-- -->Christmas day (2512) is a popular choice.</figcaption></figure><p>These are all the combinations that could represent dates like <strong>2512</strong>.</p><p><strong>2902</strong> is not as popular as its neighbours, but that's probably because it only comes around once every four years.</p><figure role="group" data-print="inline-media" aria-labelledby="104257652" data-component="Figure" data-uri="coremedia://image/104257652"><div><p><img alt="0229 is highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/109c46995b22d40228b6e203074066fc?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257652" data-component="Figure__figcaption"> <!-- -->The US style of formatting dates is also popular.</figcaption></figure><p>If you live in the United States, you'd be using <strong>0229</strong> instead.</p><p>This explains the slightly less prominent, yet almost perfectly symmetrical grid overlapping the other one.</p><p>But what about the other popular codes that don't fall on any of these special lines or grids?</p><p>The reason for choosing <strong>4321</strong> is no real mystery. It's just <strong>1234</strong> in reverse.</p><p>Some people have tried to be clever; they've mixed things up by choosing <strong>1342</strong>.</p><p>So many of them, in fact, that it's the 4th most popular code of all.</p><p><strong>2580</strong> might seem like a strange one to be in the top 40…</p><p>…until you realise it draws a line directly down the keypad on a phone.</p><p>It makes sense why some four-digit codes are chosen again and again, but this phenomenon brings with it a serious security risk.</p><p>Even though there are 10,000 possible combinations, when humans get involved that equation changes dramatically.</p><p>If someone wants to unlock a stolen phone – or retrieve money from an ATM – and only have five guesses, this data suggests they still have a one-in-eight chance of guessing correctly.</p><p>And, while it's harder to visualise, there is a similar weakness to be found in regular passwords too.</p><p><strong>1234</strong> was as high as fourth on a list of common passwords compiled by NordPass VPN.</p><p>Even when people have the entire keyboard to choose from, the only choices that were more popular were <strong>123456</strong>, "admin" and "password".</p><p>All in all, it paints a worrying picture of the last line of defence for our digital lives.</p><p>Earlier this year, journalists attending a briefing at the UK's National Cyber Security Centre (NCSC) were given a security code to access the building's facilities.</p><p>The code they were given was <strong>1234</strong>.</p><p>The NCSC later clarified this was <a href="https://www.theregister.com/2024/05/10/ncsc_entry_code/" data-component="Link">only a temporary code used for the briefing</a>.</p><p>And there's a lesson in that: if you're one of the millions of people using an ill-advised PIN, perhaps yours should be temporary too.</p><p>Remember, it's never too late to change yours to something more secure.</p><h2 data-component="Heading">The top 50 codes to avoid</h2><p><em>These are the 50 most popular codes in the full Have I Been Pwned? dataset, in order of popularity.</em></p><div data-component="ContentOverflow"><table data-component="Table"><thead><tr><th>Ranking</th><th>Code</th><th>Popularity</th></tr></thead><tbody><tr><td>1</td><td><strong>1234</strong></td><td>9.0%</td></tr><tr><td>2</td><td><strong>1111</strong></td><td>1.6%</td></tr><tr><td>3</td><td><strong>0000</strong></td><td>1.1%</td></tr><tr><td>4</td><td><strong>1342</strong></td><td>0.6%</td></tr><tr><td>5</td><td><strong>1212</strong></td><td>0.4%</td></tr><tr><td>6</td><td><strong>2222</strong></td><td>0.3%</td></tr><tr><td>7</td><td><strong>4444</strong></td><td>0.3%</td></tr><tr><td>8</td><td><strong>1122</strong></td><td>0.3%</td></tr><tr><td>9</td><td><strong>1986</strong></td><td>0.3%</td></tr><tr><td>10</td><td><strong>2020</strong></td><td>0.3%</td></tr><tr><td>11</td><td><strong>7777</strong></td><td>0.3%</td></tr><tr><td>12</td><td><strong>5555</strong></td><td>0.3%</td></tr><tr><td>13</td><td><strong>1989</strong></td><td>0.3%</td></tr><tr><td>14</td><td><strong>9999</strong></td><td>0.2%</td></tr><tr><td>15</td><td><strong>6969</strong></td><td>0.2%</td></tr><tr><td>16</td><td><strong>2004</strong></td><td>0.2%</td></tr><tr><td>17</td><td><strong>1010</strong></td><td>0.2%</td></tr><tr><td>18</td><td><strong>4321</strong></td><td>0.2%</td></tr><tr><td>19</td><td><strong>6666</strong></td><td>0.2%</td></tr><tr><td>20</td><td><strong>1984</strong></td><td>0.2%</td></tr><tr><td>21</td><td><strong>1987</strong></td><td>0.2%</td></tr><tr><td>22</td><td><strong>1985</strong></td><td>0.2%</td></tr><tr><td>23</td><td><strong>8888</strong></td><td>0.2%</td></tr><tr><td>24</td><td><strong>2000</strong></td><td>0.2%</td></tr><tr><td>25</td><td><strong>1980</strong></td><td>0.2%</td></tr><tr><td>26</td><td><strong>1988</strong></td><td>0.2%</td></tr><tr><td>27</td><td><strong>1982</strong></td><td>0.2%</td></tr><tr><td>28</td><td><strong>2580</strong></td><td>0.2%</td></tr><tr><td>29</td><td><strong>1313</strong></td><td>0.2%</td></tr><tr><td>30</td><td><strong>1990</strong></td><td>0.2%</td></tr><tr><td>31</td><td><strong>1991</strong></td><td>0.2%</td></tr><tr><td>32</td><td><strong>1983</strong></td><td>0.2%</td></tr><tr><td>33</td><td><strong>1978</strong></td><td>0.2%</td></tr><tr><td>34</td><td><strong>1979</strong></td><td>0.2%</td></tr><tr><td>35</td><td><strong>1995</strong></td><td>0.2%</td></tr><tr><td>36</td><td><strong>1994</strong></td><td>0.2%</td></tr><tr><td>37</td><td><strong>1977</strong></td><td>0.2%</td></tr><tr><td>38</td><td><strong>1981</strong></td><td>0.2%</td></tr><tr><td>39</td><td><strong>3333</strong></td><td>0.2%</td></tr><tr><td>40</td><td><strong>1992</strong></td><td>0.2%</td></tr><tr><td>41</td><td><strong>1975</strong></td><td>0.2%</td></tr><tr><td>42</td><td><strong>2005</strong></td><td>0.2%</td></tr><tr><td>43</td><td><strong>1993</strong></td><td>0.2%</td></tr><tr><td>44</td><td><strong>1976</strong></td><td>0.2%</td></tr><tr><td>45</td><td><strong>1996</strong></td><td>0.2%</td></tr><tr><td>46</td><td><strong>2002</strong></td><td>0.2%</td></tr><tr><td>47</td><td><strong>1973</strong></td><td>0.2%</td></tr><tr><td>48</td><td><strong>2468</strong></td><td>0.2%</td></tr><tr><td>49</td><td><strong>1998</strong></td><td>0.1%</td></tr><tr><td>50</td><td><strong>1974</strong></td><td>0.1%</td></tr></tbody></table></div><ul data-component="List" role="list"><li data-component="ListItem"><span></span><strong>1234</strong></li><li data-component="ListItem"><span></span><strong>1111</strong></li><li data-component="ListItem"><span></span><strong>0000</strong></li><li data-component="ListItem"><span></span><strong>1342</strong></li><li data-component="ListItem"><span></span><strong>1212</strong></li><li data-component="ListItem"><span></span><strong>2222</strong></li><li data-component="ListItem"><span></span><strong>4444</strong></li><li data-component="ListItem"><span></span><strong>1122</strong></li><li data-component="ListItem"><span></span><strong>1986</strong></li><li data-component="ListItem"><span></span><strong>2020</strong></li><li data-component="ListItem"><span></span><strong>7777</strong></li><li data-component="ListItem"><span></span><strong>5555</strong></li><li data-component="ListItem"><span></span><strong>1989</strong></li><li data-component="ListItem"><span></span><strong>9999</strong></li><li data-component="ListItem"><span></span><strong>6969</strong></li><li data-component="ListItem"><span></span><strong>2004</strong></li><li data-component="ListItem"><span></span><strong>1010</strong></li><li data-component="ListItem"><span></span><strong>4321</strong></li><li data-component="ListItem"><span></span><strong>6666</strong></li><li data-component="ListItem"><span></span><strong>1984</strong></li><li data-component="ListItem"><span></span><strong>1987</strong></li><li data-component="ListItem"><span></span><strong>1985</strong></li><li data-component="ListItem"><span></span><strong>8888</strong></li><li data-component="ListItem"><span></span><strong>2000</strong></li><li data-component="ListItem"><span></span><strong>1980</strong></li><li data-component="ListItem"><span></span><strong>1988</strong></li><li data-component="ListItem"><span></span><strong>1982</strong></li><li data-component="ListItem"><span></span><strong>2580</strong></li><li data-component="ListItem"><span></span><strong>1313</strong></li><li data-component="ListItem"><span></span><strong>1990</strong></li><li data-component="ListItem"><span></span><strong>1991</strong></li><li data-component="ListItem"><span></span><strong>1983</strong></li><li data-component="ListItem"><span></span><strong>1978</strong></li><li data-component="ListItem"><span></span><strong>1979</strong></li><li data-component="ListItem"><span></span><strong>1995</strong></li><li data-component="ListItem"><span></span><strong>1994</strong></li><li data-component="ListItem"><span></span><strong>1977</strong></li><li data-component="ListItem"><span></span><strong>1981</strong></li><li data-component="ListItem"><span></span><strong>3333</strong></li><li data-component="ListItem"><span></span><strong>1992</strong></li><li data-component="ListItem"><span></span><strong>1975</strong></li><li data-component="ListItem"><span></span><strong>2005</strong></li><li data-component="ListItem"><span></span><strong>1993</strong></li><li data-component="ListItem"><span></span><strong>1976</strong></li><li data-component="ListItem"><span></span><strong>1996</strong></li><li data-component="ListItem"><span></span><strong>2002</strong></li><li data-component="ListItem"><span></span><strong>1973</strong></li><li data-component="ListItem"><span></span><strong>2468</strong></li><li data-component="ListItem"><span></span><strong>1998</strong></li><li data-component="ListItem"><span></span><strong>1974</strong></li></ul><h2 data-component="Heading">About this story</h2><ul data-component="List" role="list"><li data-component="ListItem"><span></span>This visualisation was inspired by similar work from <a href="http://datagenetics.com/blog/september32012/index.html" data-component="Link">Nick Berry</a> in 2013</li><li data-component="ListItem"><span></span>The popularity of each PIN code was retrieved from <a href="https://haveibeenpwned.com/Passwords" data-component="Link">Have I Been Pwned's pwned passwords API</a>, which includes passwords leaked from a variety of sources and likely contains duplicate data. While it isn't a perfect data set, it aligns with likely usage patterns, even if it's just because people repeat their PIN codes on their computers.</li></ul><h2 data-component="Heading">Credits</h2><ul data-component="List" role="list"><li data-component="ListItem"><span></span><strong>Reporting and development:</strong> <a href="https://www.abc.net.au/news/julian-fell/13905936" data-component="ContentLink" data-uri="coremedia://person/13905936">Julian Fell</a></li><li data-component="ListItem"><span></span><strong>Design:</strong> <a href="https://www.abc.net.au/news/teresa-tan/9250964" data-component="ContentLink" data-uri="coremedia://person/9250964">Teresa Tan</a></li><li data-component="ListItem"><span></span><strong>Editing:</strong> <a href="https://www.abc.net.au/news/cristen-tilley/4942860" data-component="ContentLink" data-uri="coremedia://person/4942860">Cristen Tilley</a></li></ul></div><p><span data-component="Text">Posted<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2025-01-27T18:53:33.000Z">22 hours ago</time><time data-component="Text">Mon 27 Jan 2025 at 6:53pm</time>, <span data-component="Text">updated<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2025-01-28T01:28:13.000Z">16 hours ago</time><time data-component="Text">Tue 28 Jan 2025 at 1:28am</time></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Maxima in the browser using Embedded Common Lisp on WASM (116 pts)]]></title>
            <link>https://maxima-on-wasm.pages.dev/</link>
            <guid>42853528</guid>
            <pubDate>Tue, 28 Jan 2025 15:37:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://maxima-on-wasm.pages.dev/">https://maxima-on-wasm.pages.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=42853528">Hacker News</a></p>
Couldn't get https://maxima-on-wasm.pages.dev/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[IAC confirms existence of a Super-earth in the habitable zone of a Sun-like Star (133 pts)]]></title>
            <link>https://www.iac.es/en/outreach/news/iac-confirms-existence-super-earth-habitable-zone-sun-star</link>
            <guid>42853174</guid>
            <pubDate>Tue, 28 Jan 2025 15:09:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iac.es/en/outreach/news/iac-confirms-existence-super-earth-habitable-zone-sun-star">https://www.iac.es/en/outreach/news/iac-confirms-existence-super-earth-habitable-zone-sun-star</a>, See on <a href="https://news.ycombinator.com/item?id=42853174">Hacker News</a></p>
Couldn't get https://www.iac.es/en/outreach/news/iac-confirms-existence-super-earth-habitable-zone-sun-star: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek: X2 Speed for WASM with SIMD (533 pts)]]></title>
            <link>https://simonwillison.net/2025/Jan/27/llamacpp-pr/</link>
            <guid>42852866</guid>
            <pubDate>Tue, 28 Jan 2025 14:44:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Jan/27/llamacpp-pr/">https://simonwillison.net/2025/Jan/27/llamacpp-pr/</a>, See on <a href="https://news.ycombinator.com/item?id=42852866">Hacker News</a></p>
Couldn't get https://simonwillison.net/2025/Jan/27/llamacpp-pr/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Janus Pro 1B running 100% locally in-browser on WebGPU (168 pts)]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/</link>
            <guid>42852400</guid>
            <pubDate>Tue, 28 Jan 2025 14:04:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/">https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/</a>, See on <a href="https://news.ycombinator.com/item?id=42852400">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Osaka bans smoking on all of its streets, vaping included (103 pts)]]></title>
            <link>https://soranews24.com/2025/01/28/osaka-bans-smoking-on-all-of-its-streets-violators-will-be-fined-by-patrols-vaping-included/</link>
            <guid>42852073</guid>
            <pubDate>Tue, 28 Jan 2025 13:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://soranews24.com/2025/01/28/osaka-bans-smoking-on-all-of-its-streets-violators-will-be-fined-by-patrols-vaping-included/">https://soranews24.com/2025/01/28/osaka-bans-smoking-on-all-of-its-streets-violators-will-be-fined-by-patrols-vaping-included/</a>, See on <a href="https://news.ycombinator.com/item?id=42852073">Hacker News</a></p>
Couldn't get https://soranews24.com/2025/01/28/osaka-bans-smoking-on-all-of-its-streets-violators-will-be-fined-by-patrols-vaping-included/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[US pauses all Federal aid and grants (135 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c77rdy6gzy5o</link>
            <guid>42851248</guid>
            <pubDate>Tue, 28 Jan 2025 11:34:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c77rdy6gzy5o">https://www.bbc.com/news/articles/c77rdy6gzy5o</a>, See on <a href="https://news.ycombinator.com/item?id=42851248">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="byline-new" data-component="byline-block"><p><span>James FitzGerald &amp; Ana Faguy</span></p><p><span>BBC News<!-- --></span></p></div><div data-component="text-block"><p>US President Donald Trump has paused grants, loans and other federal assistance, according to a leaked government memo verified by the BBC's US partner, CBS News.<!-- --></p><p>In the memo, the acting head of the Office of Management and Budget (OMB) calls on government agencies to ensure spending is consistent with Trump's priorities.<!-- --></p><p>The full impact of the pause is not yet clear, although the memo specifies that Medicare and Social Security benefits are not affected. It comes days after the US halted nearly all foreign aid.<!-- --></p><p>The move has been criticised by members of the rival Democratic Party who warn of "devastating consequences" on programmes that people rely on. <!-- --></p></div><div data-component="text-block"><p>Diane Yentel of the National Council of Nonprofits said the order could stop cancer research, food assistance and suicide hotlines.<!-- --></p><p>Given the spending that is now on hold was apportioned by Congress, it is likely this will face legal challenges about the scope of presidential power.<!-- --></p><p>The memo, signed by acting OMB chief Matthew Vaeth, calls on government agencies to temporarily pause their financial assistance programmes, so they can review spending that could be impacted by the various orders Trump has signed .<!-- --></p><p>It says this encompasses "financial assistance for foreign aid, nongovernmental organizations, DEI, woke gender ideology, and the green new deal".<!-- --></p></div><div data-component="text-block"><p>A deadline of 17:00 EST (22:00 GMT) has been set. Each agency is told to pause the issuing of new awards as well as the disbursement of funds under existing awards. <!-- --></p><p>The memo further demands that all agencies report which programmes have been paused by 10 February. <!-- --></p><p>The White House has not yet commented officially on the leaked document.<!-- --></p><p>Democrats in Washington DC were quick to sound an alarm of concern about the plan. <!-- --></p><p>The top Democratic appropriators in Congress - Washington Senator Patty Murray and Connecticut Congresswoman Rosa DeLauro - sent a letter to the White House Monday evening expressing their "extreme alarm" with the memo. <!-- --></p><p>"The scope of what you are ordering is breathtaking, unprecedented, and will have devastating consequences across the country," the congresswomen wrote. "We write today to urge you in the strongest possible terms to uphold the law and the Constitution and ensure all federal resources are delivered in accordance with the law."<!-- --></p><p>The Democratic minority leader of the US Senate, Chuck Schumer, was also critical of the pause: "Congress approved these investments and they are not optional; they are the law." <!-- --></p><p>He added: "It will mean missed payrolls and rent payments and everything in between: chaos for everything from universities to non-profit charities."<!-- --></p><p>The move follows last week's news that the Department of State had issued a halt to nearly all existing foreign assistance and paused new aid, according to an internal memo sent to officials and US embassies abroad. <!-- --></p><p>It appeared to affect everything from development assistance to military aid, making exceptions only for emergency food aid and for military funding for Israel and Egypt.<!-- --></p><p>Trump had earlier issued an executive order for a 90-day pause in foreign development assistance pending a review of efficiencies and consistency with his foreign policy. <!-- --></p><p>The US is the world's biggest international aid donor, having spent $68bn (£66bn) in 2023 according to government figures. The State Department notice appears to affect everything from development assistance to military aid.<!-- --></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cleveland police used AI to justify a search warrant. It derailed a murder case (131 pts)]]></title>
            <link>https://www.cleveland.com/news/2025/01/cleveland-police-used-ai-to-justify-a-search-warrant-it-has-derailed-a-murder-case.html</link>
            <guid>42851124</guid>
            <pubDate>Tue, 28 Jan 2025 11:17:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cleveland.com/news/2025/01/cleveland-police-used-ai-to-justify-a-search-warrant-it-has-derailed-a-murder-case.html">https://www.cleveland.com/news/2025/01/cleveland-police-used-ai-to-justify-a-search-warrant-it-has-derailed-a-murder-case.html</a>, See on <a href="https://news.ycombinator.com/item?id=42851124">Hacker News</a></p>
Couldn't get https://www.cleveland.com/news/2025/01/cleveland-police-used-ai-to-justify-a-search-warrant-it-has-derailed-a-murder-case.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[US Civil servants are being asked who they voted for in 2024 election (134 pts)]]></title>
            <link>https://www.independent.co.uk/news/world/americas/us-politics/trump-civil-service-loyalty-transition-b2678674.html</link>
            <guid>42850644</guid>
            <pubDate>Tue, 28 Jan 2025 09:58:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.independent.co.uk/news/world/americas/us-politics/trump-civil-service-loyalty-transition-b2678674.html">https://www.independent.co.uk/news/world/americas/us-politics/trump-civil-service-loyalty-transition-b2678674.html</a>, See on <a href="https://news.ycombinator.com/item?id=42850644">Hacker News</a></p>
Couldn't get https://www.independent.co.uk/news/world/americas/us-politics/trump-civil-service-loyalty-transition-b2678674.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek-R1 with Dynamic 1.58-bit Quantization (606 pts)]]></title>
            <link>https://unsloth.ai/blog/deepseekr1-dynamic</link>
            <guid>42850222</guid>
            <pubDate>Tue, 28 Jan 2025 08:52:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unsloth.ai/blog/deepseekr1-dynamic">https://unsloth.ai/blog/deepseekr1-dynamic</a>, See on <a href="https://news.ycombinator.com/item?id=42850222">Hacker News</a></p>
Couldn't get https://unsloth.ai/blog/deepseekr1-dynamic: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Reinforcement Learning – A Reference (105 pts)]]></title>
            <link>https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference</link>
            <guid>42850111</guid>
            <pubDate>Tue, 28 Jan 2025 08:31:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference">https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference</a>, See on <a href="https://news.ycombinator.com/item?id=42850111">Hacker News</a></p>
Couldn't get https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[FTC Takes Action Against GoDaddy for Alleged Lax Data Security (152 pts)]]></title>
            <link>https://www.ftc.gov/news-events/news/press-releases/2025/01/ftc-takes-action-against-godaddy-alleged-lax-data-security-its-website-hosting-services</link>
            <guid>42849632</guid>
            <pubDate>Tue, 28 Jan 2025 07:02:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ftc.gov/news-events/news/press-releases/2025/01/ftc-takes-action-against-godaddy-alleged-lax-data-security-its-website-hosting-services">https://www.ftc.gov/news-events/news/press-releases/2025/01/ftc-takes-action-against-godaddy-alleged-lax-data-security-its-website-hosting-services</a>, See on <a href="https://news.ycombinator.com/item?id=42849632">Hacker News</a></p>
Couldn't get https://www.ftc.gov/news-events/news/press-releases/2025/01/ftc-takes-action-against-godaddy-alleged-lax-data-security-its-website-hosting-services: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Open-R1: an open reproduction of DeepSeek-R1 (376 pts)]]></title>
            <link>https://huggingface.co/blog/open-r1</link>
            <guid>42849536</guid>
            <pubDate>Tue, 28 Jan 2025 06:40:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huggingface.co/blog/open-r1">https://huggingface.co/blog/open-r1</a>, See on <a href="https://news.ycombinator.com/item?id=42849536">Hacker News</a></p>
Couldn't get https://huggingface.co/blog/open-r1: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Machine Learning in Production (CMU Course) (430 pts)]]></title>
            <link>https://mlip-cmu.github.io/s2025/</link>
            <guid>42847834</guid>
            <pubDate>Tue, 28 Jan 2025 01:18:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mlip-cmu.github.io/s2025/">https://mlip-cmu.github.io/s2025/</a>, See on <a href="https://news.ycombinator.com/item?id=42847834">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="container">

<h2>Machine Learning in Production (17-445/17-645/17-745) / AI Engineering (11-695)</h2>
<h3>Spring 2025</h3>
<p><em>CMU course that covers how to build, deploy, assure, and maintain software products with machine-learned models. Includes the entire lifecycle from a prototype ML model to an entire system deployed in production. Covers also <strong>responsible AI</strong> (including safety, security, fairness, explainability) and <strong>MLOps</strong>. For earlier offerings see websites for&nbsp;<a href="https://ckaestne.github.io/seai/F2019">Fall 2019</a>,&nbsp;<a href="https://ckaestne.github.io/seai/S2020">Summer 2020</a>, <a href="https://ckaestne.github.io/seai/F2020/">Fall 2020</a>, <a href="https://ckaestne.github.io/seai/S2021/">Spring 2021</a>&nbsp;&nbsp;<a href="https://ckaestne.github.io/seai/S2022/">Spring 2022</a>, <a href="https://ckaestne.github.io/seai/F2022/">Fall 2022</a>,&nbsp;<a href="https://github.com/mlip-cmu/s2023">Spring 2023</a>, <a href="https://github.com/mlip-cmu/s2024">Spring 2024</a>, and <a href="https://github.com/mlip-cmu/f2024">Fall 2024</a>. This Spring 2025 offering is designed for students with some data science experience (e.g., has taken a machine learning course, has used sklearn) and basic programming skills (e.g., basic Python programming with libraries, can navigate a Unix shell), but will not expect a software engineering background (i.e., experience with testing, requirements, architecture, process, or teams is not required). Going forward we expect to offer this course at least every spring semester and possibly some fall semesters (not summer semesters).</em></p>
<p><strong>Waitlist: we often cannot accommodate all interested students in Spring semesters, though we expect there to be waitlist movement.  We encourage students who are able to move to alternative labs with space, to do so.  For students enrolled in 17-XXX numbers, contact Jenni Cooper (<a href="mailto:cooperj@andrew.cmu.edu">cooperj@andrew.cmu.edu</a>) for assistance; for students enrolled in 11-XXX numbers, contact Amber Vivis (<a href="mailto:albrown@andrew.cmu.edu">albrown@andrew.cmu.edu</a>) and Karen Kirk (<a href="mailto:karensuk@andrew.cmu.edu">karensuk@andrew.cmu.edu</a>).  Note that the instructors cannot help with waitlist/registration movement, please contact the course admins instead!</strong>  </p>
<hr>
<p>For researchers, educators, or others interested in this topic, we share all course material, including slides and assignments, under a creative commons license on GitHub (<a href="https://github.com/mlip-cmu">https://github.com/mlip-cmu</a>) and have also published a <a href="https://mlip-cmu.github.io/book/">textbook</a> with chapters corresponding to almost every lecture. A while ago we also wrote an article describing the rationale and the initial design of this course: <a href="https://arxiv.org/abs/2001.06691">Teaching Software Engineering for AI-Enabled Systems</a>. Video recordings of the Summer 2020 offering are online on the <a href="https://ckaestne.github.io/seai/S2020/#course-content">course page</a>, though they are a bit outdated by now. We would be happy to see this course or a similar version taught at other universities. See also an <a href="https://github.com/ckaestne/seaibib">annotated bibliography</a> on research in this field.</p>
<h2>Course Description</h2>
<p>This is a course for those who want to build <strong>software products</strong> with <strong>machine learning</strong>, not just models and demos. We assume that you can train a model or build prompts to make predictions, but what does it take to turn the model into a product and actually deploy it, have confidence in its quality, and successfully operate and maintain it at scale? </p>
<p>The course is designed to establish a working relationship between <strong>software engineers</strong> and <strong>data scientists</strong>: both contribute to building ML-enabled systems but have different expertise and focuses. To work together they need a mutual understanding of their roles, tasks, concerns, and goals and build a working relationship. This course is aimed at <strong>software engineers</strong> who want to build robust and responsible products meeting the specific challenges of working with ML components and at <strong>data scientists</strong> who want to understand the requirements of the model for production use and want to facilitate getting a prototype model into production; it facilitates communication and collaboration between both roles. The course is a good fit for student looking at a career as an <strong>ML engineer</strong>. <em>The course focuses on all the steps needed to turn a model into a production system in a responsible and reliable manner.</em></p>
<p><img src="https://mlip-cmu.github.io/s2025/overview.svg" alt="Course overview"></p>
<p>It covers topics such as:</p>
<ul>
<li><strong>How to design for wrong predictions the model may make?</strong> How to assure <em>safety</em> and <em>security</em> despite possible mistakes? How to design the <em>user interface</em> and the entire system to operate in the real world?</li>
<li><strong>How to reliably deploy and update models in production?</strong> How can we <em>test</em> the entire machine learning pipeline? How can <em>MLOps</em> tools help to automate and scale the deployment process? How can we <em>experiment in production</em> (A/B testing, canary releases)? How do we detect <em>data quality</em> issues, <em>concept drift</em>, and <em>feedback loops</em> in production?</li>
<li><strong>How to scale production ML systems?</strong> How do we design a system to process huge amounts of training data, telemetry data, and user requests? Should we use stream processing, batch processing, lambda architecture, or data lakes?</li>
<li><strong>How to test and debug production ML systems?</strong> How can we <em>evaluate</em> the quality of a model’s predictions in production? How can we <em>test</em> the entire ML-enabled system, not just the model? What lessons can we learn from <em>software testing</em>, <em>automated test case generation</em>, <em>simulation</em>, and <em>continuous integration</em> for testing for production machine learning?</li>
<li><strong>Which qualities matter beyond a model’s prediction accuracy?</strong> How can we identify and measure important quality requirements, including <em>learning and inference latency, operating cost, scalability, explainablity, fairness, privacy, robustness</em>, and <em>safety</em>? Does the application need to be able to <em>operate offline</em> and how often do we need to update the models? How do we identify what’s important in a ML-enabled product in a production setting for a business? How do we resolve <em>conflicts</em> and <em>tradeoffs</em>?</li>
<li><strong>How to work effectively in interdisciplinary teams?</strong> How can we bring data scientists, software engineers, UI designers, managers, domain experts, big data specialists, operators, legal council, and other roles together and develop a <em>shared understanding</em> and <em>team culture</em>?</li>
</ul>
<p><strong>Examples and case studies</strong> of ML-driven products we discuss include automated audio transcription; distributed detection of missing children on webcams and instant translation in augmented reality; cancer detection, fall detection, COVID diagnosis, and other smart medical and health services; automated slide layout in Powerpoint; semi-automated college admissions; inventory management; smart playlists and movie recommendations; ad fraud detection; delivery robots and smart driving features; and many others.</p>
<p>An extended group project focuses on building, deploying, evaluating, and maintaining a robust and scalable <em>movie recommendation service</em> under somewhat realistic “production” conditions with 1 million users.</p>
<h3>Learning Outcomes</h3>
<p>After taking this course, among others, students should be able to</p>
<ul>
<li>analyze tradeoffs for designing production systems with ML-components, analyzing various qualities beyond accuracy such as operation cost, latency, updateability, and explainability</li>
<li>plan for mistakes in ML components and implement production-quality systems that are robust to those mistakes</li>
<li>design fault-tolerant and scalable data infrastructure for learning models, serving models, versioning, and experimentation</li>
<li>ensure quality of the entire machine learning pipeline with test automation and other quality assurance techniques, including automated checks for data quality, data drift, feedback loops, and model quality</li>
<li>build systems that can be tested and monitored in production and build robust deployment pipelines</li>
<li>consider system-level requirements such as safety, security, privacy, fairness, and usability when building complex ML-enabled products</li>
<li>communicate effectively in interdisciplinary teams</li>
</ul>
<p>In addition, students will gain familiarity with production-quality infrastructure tools, including stream processing with Apache Kafka, test automation with Jenkins, monitoring with Prometheus and Grafana, and deployment with Docker and various MLOps tools.</p>
<h2>Logistics and People</h2>
<p>17-445/17-645/17-745, 12 Units</p>
<p>The course is the same under all course numbers, except for the PhD-level 17-745 number, which replaces two homework assignments with a mandatory <a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/research_project.md">research project</a>.</p>
<p>Open to all undergraduate and graduate students meeting the prerequisites.</p>
<h3>Spring 2025</h3>
<p>Lectures Monday/Wednesday 2:00-3:20pm, in person, PH 100</p>
<p>Labs Friday 9:30-10:50am in PH 226C (A) and SH 236 (B) and 11-12:20pm in PH A22 (C) and PH 226A (D) and 2-3:20 in PH 226C (E) and TEP 1308 (F). There is also a remote only lab (G), Friday 11:00-12:20 pm. </p>
<p>Instructors: <a href="https://www.cs.cmu.edu/~clegoues">Claire Le Goues</a> and <a href="https://austinhenley.com/">Austin Henley</a></p>
<p>TAs: </p>
<h3>Coordination</h3>
<p>We are happy to answer questions by email and over Slack, meet in person, and will jump on a quick Zoom call if you ask us. We also always arrive 5 to 10 min early to class and stay longer for discussions and questions. If you have questions about assignments and logistics, we prefer that you ask them publicly on Slack.</p>
<h2>Course content</h2>
<p>The general course content has been fairly stable over the last few years, though specific topics and tools are constantly updated with new research and tooling. Our list of learning goals under <a href="https://github.com/mlip-cmu/s2025/blob/main/learning_goals.md">Learning Goals</a> describes what we aim to cover. Below is a table of a preliminary schedule. This is subject to change and will be updated as the semester progresses, especially to help focus on requested topics or support learning.</p>
<p>[Schedule]</p>
<table>
<thead>
<tr>
<th>Date</th>
<th>Topic</th>
<th><a href="https://mlip-cmu.github.io/book/">Book Chapter</a></th>
<th>Reading</th>
<th>Assignment due</th>
</tr>
</thead>
<tbody><tr>
<td>Mon, Jan 13</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/01_introduction/intro.html">Introduction and Motivation </a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/01_introduction/intro.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/01_introduction/intro.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/01/">1</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Jan 15</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/02_systems/systems.html">From Models to AI-Enabled Systems</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/02_systems/systems.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/02_systems/systems.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/02/">2</a>,<a href="https://mlip-cmu.github.io/book/04/">4</a>,<a href="https://mlip-cmu.github.io/book/05/">5</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 4, 5, 7, 8</td>
<td></td>
</tr>
<tr>
<td>Fri, Jan 17</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab01.md">Calling, securing, and creating APIs: Flask</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Jan 20</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> MLK Day, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Jan 22</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/03_requirements/requirements.html">Gathering Requirements </a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/03_requirements/requirements.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/03_requirements/requirements.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/06/">6</a></td>
<td><a href="https://scholar.google.com/scholar?cluster=1090758480873197042">The World and the Machine</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Jan 24</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab02.md">Stream processing: Apache Kafka</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Jan 27</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/04_mistakes/mistakes.html">Planning for Mistakes</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/04_mistakes/mistakes.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/04_mistakes/mistakes.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/07/">7</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 6, 8, 24</td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I1_mlproduct.md">I1: ML Product</a></td>
</tr>
<tr>
<td>Wed, Jan 29</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/05_modelaccuracy/modelquality1.html">Model Quality</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/05_modelaccuracy/modelquality1.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/05_modelaccuracy/modelquality1.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/15/">15</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 19</td>
<td></td>
</tr>
<tr>
<td>Fri, Jan 31</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab03.md">Collaboration with git</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 3</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/06_teamwork/teams.html">Fostering Interdisciplinary (Student) Teams</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/06_teamwork/teams.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/06_teamwork/teams.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td><a href="https://third-bit.com/2018/05/11/meetings/">Meetings</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I2_requirements.md">I2: Requirements</a></td>
</tr>
<tr>
<td>Wed, Feb 5</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/07_modeltesting/modelquality2.html">Behavioral Model Testing</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/07_modeltesting/modelquality2.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/07_modeltesting/modelquality2.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/15/">15</a></td>
<td><a href="https://aclanthology.org/2020.acl-main.442.pdf">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 7</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab04.md">Model testing</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 10</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/08_architecture/tradeoffs.html">Toward Architecture and Design </a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/08_architecture/tradeoffs.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/08_architecture/tradeoffs.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/08/">8</a>,<a href="https://mlip-cmu.github.io/book/09/">9</a>,<a href="https://mlip-cmu.github.io/book/11/">11</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems, Ch. 18</a> and <a href="https://hackernoon.com/choosing-the-right-machine-learning-algorithm-68126944ce1f">Choosing the Right Machine Learning Algorithm</a></td>
<td></td>
</tr>
<tr>
<td>Wed, Feb 12</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/09_deploying_a_model/deployment.html">Deploying a Model</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/09_deploying_a_model/deployment.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/09_deploying_a_model/deployment.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/10/">10</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 13 and <a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/1feg4j8/alma991019735160604436">Machine Learning Design Patterns</a>, Pat. 16</td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 14</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab05.md">Containers: Docker</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 17</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/10_qainproduction/qainproduction.html">Testing and Experimenting in Production</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/10_qainproduction/qainproduction.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/10_qainproduction/qainproduction.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/19/">19</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Chs. 14 and 15</td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M1: Modeling and First Deployment</a></td>
</tr>
<tr>
<td>Wed, Feb 19</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/11_dataquality/dataquality.html">Data Quality</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/11_dataquality/dataquality.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/11_dataquality/dataquality.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/16/">16</a></td>
<td><a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445518">Data Cascades in High-Stakes AI</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 21</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab06.md">Continuous Integration: Jenkins</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 24</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/12_pipelinequality/pipelinequality.html">Automating and Testing ML Pipelines</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/12_pipelinequality/pipelinequality.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/12_pipelinequality/pipelinequality.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/11/">11</a>,<a href="https://mlip-cmu.github.io/book/18/">18</a>,<a href="https://mlip-cmu.github.io/book/19/">19</a></td>
<td><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46555.pdf">The ML Test Score</a></td>
<td></td>
</tr>
<tr>
<td>Wed, Feb 26</td>
<td><img src="https://img.shields.io/badge/-midterm-blue.svg" alt="Midterm"> Midterm 1</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 28</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> No lab (happy spring break)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 3</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Spring break, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Mar 5</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Spring break, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 7</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Spring break, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 10</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/13_dataatscale/dataatscale.html">Scaling the System</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/13_dataatscale/dataatscale.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/13_dataatscale/dataatscale.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/12/">12</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019577936304436">Big Data: Principles and best practices of scalable realtime data systems</a>, Ch. 1</td>
<td></td>
</tr>
<tr>
<td>Wed, Mar 12</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/14_operations/operations.html">Planning for Operations</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/14_operations/operations.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/14_operations/operations.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/13/">13</a></td>
<td><a href="https://arxiv.org/abs/2209.09125">Operationalizing machine learning: An interview study</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 14</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab07.md">Monitoring: Prometheus, Grafana</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 17</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/15_provenance/provenance.html">Versioning, Provenance, and Reproducability</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/15_provenance/provenance.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/15_provenance/provenance.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/24/">24</a></td>
<td><a href="http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">Hidden Technical Debt in Machine Learning Systems</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M2: Infrastructure Quality</a></td>
</tr>
<tr>
<td>Wed, Mar 19</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/16_process/process.html">Process &amp; Technical Debt</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/16_process/process.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/16_process/process.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/20/">20</a></td>
<td><a href="https://arxiv.org/pdf/2110.10234.pdf">Collaboration Challenges in Building ML-Enabled Systems</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 21</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab08.md">Pipeline automation: MLFlow</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 24</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/17_intro_ethics_fairness/intro-ethics-fairness.html">Intro to Ethics + Fairness</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/17_intro_ethics_fairness/intro-ethics-fairness.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/17_intro_ethics_fairness/intro-ethics-fairness.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/23/">23</a>,<a href="https://mlip-cmu.github.io/book/26/">26</a></td>
<td><a href="https://datasociety.net/wp-content/uploads/2018/04/Data_Society_Algorithmic_Accountability_Primer_FINAL-4.pdf">Algorithmic Accountability: A Primer</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I3_mlops_tools.md">I3: MLOps Tools</a></td>
</tr>
<tr>
<td>Wed, Mar 26</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/18_fairness_measures/model_fairness.html">Measuring Fairness</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/18_fairness_measures/model_fairness.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/18_fairness_measures/model_fairness.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/26/">26</a></td>
<td><a href="https://dl.acm.org/doi/pdf/10.1145/3178876.3186138">Human Perceptions of Fairness in Algorithmic Decision Making</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 28</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab09.md">Container orchestration: Kubernetis</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 31</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/19_system_fairness/system_fairness.html">Building Fairer Systems</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/19_system_fairness/system_fairness.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/19_system_fairness/system_fairness.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/26/">26</a></td>
<td><a href="http://users.umiacs.umd.edu/~hal/docs/daume19fairness.pdf">Improving Fairness in Machine Learning Systems</a></td>
<td></td>
</tr>
<tr>
<td>Wed, Apr 2</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/20_explainability/explainability.html">AVAILABLE</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/20_explainability/explainability.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/20_explainability/explainability.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/25/">25</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 4</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Carnival, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Apr 7</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/21_transparency/transparency.html">Explainability</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/21_transparency/transparency.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/21_transparency/transparency.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/29/">29</a></td>
<td><a href="https://dataskeptic.com/blog/episodes/2020/black-boxes-are-not-required">Interpretability Podcast</a> or <a href="https://arxiv.org/abs/1811.10154">equivalent artice</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M3: Monitoring and CD</a></td>
</tr>
<tr>
<td>Wed, Apr 9</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/22_security/security.html">Transparency &amp; Accountability</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/22_security/security.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/22_security/security.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/28/">28</a></td>
<td><a href="https://pair.withgoogle.com/chapter/explainability-trust/">Google chapter on Explainability and Trust</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 11</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab10.md">Model Explainability Tools</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Apr 14</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/23_safety/safety.html">Security and Privacy</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/23_safety/safety.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/23_safety/safety.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/27/">27</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 25, and <a href="https://canvas.cmu.edu/courses/45008/files/12156923/download?wrap=1">The Top 10 Risks of Machine Learning Security</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I4_explainability.md">I4: Explainability</a></td>
</tr>
<tr>
<td>Wed, Apr 16</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/24_summary/all.html">Safety</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/24_summary/all.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/24_summary/all.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td><a href="http://ceur-ws.org/Vol-2560/paper40.pdf">Practical Solutions for Machine Learning Safety in Autonomous Vehicles</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 18</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab11.md">LLM Jailbreaking</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Apr 21</td>
<td>Explainability Discussion / Summary / Review</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Apr 23</td>
<td><img src="https://img.shields.io/badge/-midterm-blue.svg" alt="Midterm"> Midterm 2</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 25</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> No lab</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M4: Fairness, Security and Feedback Loops</a></td>
</tr>
<tr>
<td>TBD</td>
<td>Final Project Presentations (5:30-8:30pm in GHC 4401)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">Final report</a></td>
</tr>
</tbody></table>
<h2>Course Syllabus and Policies</h2>
<p>The course uses Canvas and Gradescope for homework submission, grading, discussion, questions, announcements, and supplementary documents; slides will be posted here; Slack is used for communication around homework and projects; Github is used to coordinate group work. All public course material (assignments, slides, syllabus) can be found in the course’s <a href="https://github.com/mlip-cmu/s2025">GitHub repository</a>; announcements and all <em>private</em> material (e.g., grades, passwords) will be shared through Canvas.</p>
<p><strong>Prerequisites:</strong> The course does not have formal prerequisites, but we describe background knowledge that will help you be successful in the course. In a nutshell, we expect basic exposure to machine learning and basic programming skills, but do not require software engineering experience. </p>
<p><em>Machine learning (some experience recommended):</em> We suggest that you have basic familiarity with the process of extracting features, building and evaluating models, and a basic understanding of how and when different kinds of learning techniques work. Familiarity with Python and Jupyter notebooks is helpful. Courses such as 10-301, 10-315, and 05-434 will prepare you well, but project experience or self-learning from books or online courses will likely be sufficient for our purposes. For example, if you have no prior experience, we recommend the book <a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019665684604436">Hands-On Machine Learning</a> to get practical experience in building and evaluating models prior to taking this course. We have set up a <em><a href="https://forms.gle/JcS61Uao7wHSFQen8">prerequisite knowledge check</a></em> as a Google Form, where we ask 10 questions on machine learning, which help you assess your background – this is set up as an anonymous and ungraded quiz, where you can compare your knowledge against what we believe is useful for you to be successful in this course (click on <em>“view score”</em> after submitting your answer). After submitting your answers, the system will give specific pointers to readings and exercises that may help you fill gaps in background knowledge. </p>
<p><em>Programming (basic proficiency required):</em> The course has a substantial programming component, especially in the first assignment and the team project, so basic programming skills will be needed. If you take the course without programming experience, you will significantly struggle and it may cause conflicts within the group project. We expect that you meet the following criteria: (1) basic fluency in a programming language like Python, (2) ability to install and learn libraries in that language, (3) ability to ssh into a Unix machine and perform basic command line operations, and (4) ability to install and learn new tools like Docker. We do not prescribe a programming language, but almost all student teams decide to work primarily in Python. We will provide some introductions and examples for essential tools like Git, Docker, Grafana, and Jenkins in labs, but we expect that you will be able to pick up new tools and libraries on your own. For example, we expect that you will be able, on your own, to learn basic use of a library like <a href="https://flask.palletsprojects.com/en/2.1.x/">Flask</a> to write a web service. Throughout the semester, expect to read lots of documentation and tutorials to learn various libraries and tools on your own. If you are worried whether your technical background is sufficient, we recommend that you look at (or even try) <a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I1_mlproduct.md">homework I1</a> before the semester.</p>
<p><em>Software engineering (no experience required):</em> Many students will have some software engineering experience beyond basic programming skills from software engineering courses, from internships, or from working in industry, for example experience with requirements engineering, software design, software testing, distributed systems, continuous deployment, or managing teams. No such experience is expected as a prerequisite; we will cover these topics in the course.</p>
<p>Email the instructors if you would like to further talk to us about prerequisites.</p>
<p><strong>In-person teaching and lecture recordings:</strong> The course will be taught in person.  We consider in-class participation an important part of the learning experience. We <em>do</em> make <em>best effort</em> lecture recordings, which will be available in Canvas.  We do <em>not</em> provide a synchronous remote option, and we do not record labs.  You are welcome to use recordings to make up missed lectures and review material. However, absent extenuating circumstances (see below), viewing the recording will not make up for missed in-class activities.  </p>
<p>We regularly use Slack for in-class activities. Please make sure that you have access to Slack on a laptop, tablet, or mobile phone during class.</p>
<p>If you cannot attend class due to a medical issue, family emergency, interview, or other unforeseeable reason, please contact us about possible accommodations. We try to be as flexible as we can, but will handle these cases individually.</p>
<p><strong>Exams:</strong> The course has two midterms and a final project presentation, but no final exam. We typically use the registrar-assigned final exam timeslot (to be announced about halfway through the semester <a href="https://www.cmu.edu/hub/docs/final-exams.pdf">here</a>) for the final project presentation. The midterms are during the normal class period as per schedule. The second midterm is not comprehensive, and only covers material after the first midterm. Examples of past midterms can be found in the <a href="https://github.com/mlip-cmu/s2025/tree/main/exams">course repository</a>.</p>
<p><strong>Grading:</strong> Evaluation will be based on the following distribution: 35% individual assignments, 30% group project, 15% midterms, 5% participation, 10% labs, 5% reading quizzes. No final exam.</p>
<p>We strive to provide clear specifications and clear point breakdowns for all homework to set clear expectations and take the guessing out of homework. We often give you choices to self-direct your learning, deciding what to work on and how to address a problem (e.g., we never prescribe a programming language and often give choices to answer a subset of possible questions). Clear specifications and point breakdowns allow you to intentionally decide to skip parts of assignments with clear upfront consequences. All parts will be graded pass/fail, no partial credit. For opportunities to redo work, see <em>resubmissions</em> below. For grading participation and quizzes see below. Some assignments have a small amount of bonus points. </p>
<p>Since we give flexibility to resubmit assignments, we set grade boundaries fairly high. We expect the following grade boundaries:</p>
<table>
<thead>
<tr>
<th>Grade</th>
<th>Cutoff</th>
</tr>
</thead>
<tbody><tr>
<td>A+</td>
<td>&gt;99%</td>
</tr>
<tr>
<td>A</td>
<td>&gt;96%</td>
</tr>
<tr>
<td>A-</td>
<td>&gt;94%</td>
</tr>
<tr>
<td>B+</td>
<td>&gt;91%</td>
</tr>
<tr>
<td>B</td>
<td>&gt;86%</td>
</tr>
<tr>
<td>B-</td>
<td>&gt;82%</td>
</tr>
<tr>
<td>C</td>
<td>&gt;75%</td>
</tr>
<tr>
<td>D</td>
<td>&gt;60%</td>
</tr>
</tbody></table>
<p><strong>Participation:</strong> Design and engineering content requires active engagement with the material and discussions of judgment decisions on specific scenarios and cases. We strongly believe in in-class discussions and in-class exercises and want all students to participate, e.g., answering or asking questions in class, sharing own experiences, presenting results, or participating in in-class votes and surveys. We will give many opportunities for participation in every lecture and lab. We note student engagement with in-class activities to include as a component in grading.  We will provide feedback at mid-semester so that you can check in on how you’re doing. Again, please talk to us if you need accommodations.</p>
<p>We assign participation grades as follows:</p>
<ul>
<li>100%: Participates actively at least once in most lectures (4 lectures waived, no questions asked)</li>
<li>90%: Participates actively at least once in two thirds of the lectures</li>
<li>75%: Participates actively at least once in over half of the lectures</li>
<li>50%: Participates actively at least once in one quarter of the lectures</li>
<li>20%: Participates actively at least once in at least 3 lectures.</li>
<li>0%: Participation in less than 3 lectures.</li>
</ul>
<p><strong>Labs:</strong> Labs typically introduce tools and have a task with one or more clear deliverables. Lab assignments are designed to take about 1h of work and can be completed before or during the lab session. Each deliverable is graded pass/fail at any time during that week's lab session by showing your work to the TA. Typically showing your work involves showing source code, demoing executions, and (verbally) answering a few questions. The TA may ask a few questions about your implementation to probe that you understand your work.</p>
<p>We intend labs to be very low stakes – this is your first practical engagement with the material and mistakes are a normal part of the learning process. Deliverables are graded pass/fail on whether they meet the stated expectations for the deliverables. If your solution does not meet the expectations you can continue working on it during the lab session until it does. Outside of explicit accommodations (e.g., medical issues) or using tokens (see below), we do not accept lab solutions after the end of the lab session.</p>
<p>We encourage collaboration on labs: You can work together with other students both before the lab session and during the lab session. While we do not recommend it, you may look at other students’ solutions and reference solutions and even copy them. However, you will have to present and explain your solution to the TA on your own.</p>
<p><strong>Textbook, reading assignments, and reading quizzes:</strong> We will be using Goeff Hulten's <em>"Building Intelligent Systems: A Guide to Machine Learning Engineering"</em> (ISBN: 1484234316) throughout much of the course. The library provides an <a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">electronic copy</a>. In addition, we will provide various additional readings, including blog posts and academic papers, throughout the semester.</p>
<p>We also wrote our own textbook "<a href="https://mlip-cmu.github.io/book/">Machine Learning in Production</a>" that aligns closely with the lecture content. The book will be published by MIT Press and is additionally available under a creative commons license online. We will not assign chapters from our own textbook, but we always point to the corresponding chapter for each lecture, which we suggest as supplementary reading.</p>
<p>We will assign readings for most classes and post a corresponding quiz on Canvas that is due before class. Each quiz contains an open-ended question that relates to the reading. Reading quizzes are intended to be low-stakes assessments and are graded pass/fail for a good-faith effort to engage with the question. </p>
<p><strong>Teamwork:</strong> Teamwork is an essential part of this course. The course contains a multi-milestone group project to be done in teams of 3-5 students. Teams will be assigned by the instructor. A TA will serve as a mentor for each team. We will help teams throughout the semester and cover some specific content on teamwork as part of the course. Peer rating will be performed for team assignments with regard to <em>team citizenship</em> (i.e., being active and cooperative members), following a procedure adapted from <a href="https://www.cs.tufts.edu/~nr/cs257/archive/teaching/barbara-oakley/JSCL-collaboration.pdf">this article</a>, which we will further explain in an early lecture. Use <a href="https://mlip-cmu.github.io/s2025/assignments/peergrading.html">this form</a> to preview the expected adjustments for peer ratings. The team's mentor will also debrief with the team after every milestone and discuss possible strategies to improve teamwork. </p>
<p><strong>Late work policy and resubmissions:</strong> We understand that students will always have competing deadlines, unusual events, interviews for job searches, and other activities that compete with coursework. We therefore build flexibility and a safety net directly into the rubric. If you need additional accommodations, please contact us.</p>
<p>In addition, we expect that the past/fail grading scheme without partial credit, may lead to harsh point deductions for missing small parts of the requirements, so we provide a mechanism to resubmit work with a short reflection to regain lost points.</p>
<p>Every student receives <em>8 individual tokens</em> that they can spend throughout the semester in the following ways:</p>
<ul>
<li>For each token, a student can submit a homework assignment 1 day late (with 2 tokens a student can submit two homeworks one day late each or a single homework up to two days late).</li>
<li>For <em>three</em> tokens, a student can improve or redo an individual homework assignment and resubmit together with a short reflection. The earlier submission is discarded and the regraded assignment counts toward the final grade. Resubmissions can be made at any time in the semester up to the final project presentation (see schedule). – Note that this technically allows a student to blow the original deadline (no submission necessary, receiving 0 points initially) and then resubmit the homework arbitrarily late for three tokens.</li>
<li>For one token, a student can submit a reading quiz late (any time before the final presentation) or resubmit a graded reading quiz.</li>
<li>For one token, a student can complete a lab late or redo a lab (any time before the final presentation) by showing the work to a TA during office hours.</li>
<li>Remaining individual tokens at the end of the semester are counted as one participation day each.</li>
</ul>
<p>If a student runs out of tokens, late individual assignments receive a penalty of 15% per started day. Late team formation survey and teamwork peer assessment surveys do not receive any points.</p>
<p>Every team independently receives <em>8 team tokens</em> that they can spend for extensions of any milestone deadline (1 token per day per milestone, except final presentation deadline) or to resubmit any milestone with a reflection (3 tokens each, resubmitted any time before the final presentation). If a team runs out of tokens, late submissions in group assignments receive a penalty of 15% per started day.</p>
<p>Individual tokens and team tokens are entirely separate; it is not possible to use individual tokens for teamwork or vice versa. The team should make collective decisions about how to use team tokens.</p>
<p>In general, late submissions and resubmissions can be done at any point in the semester before the final presentations. Late submissions that are 1-3 days late can be made directly to Gradescope; for everything else see instructions and forms on Canvas.</p>
<p>Exceptions to this policy will be made at the discretion of the instructor in important circumstances, almost always involving a family or medical emergency and an email from your advisor — you can ask your academic advisor or the Dean of Student Affairs requesting the exception on your behalf. Where issues affect teamwork, please communicate proactively with your team.</p>
<p><strong>Communication:</strong> We make important announcements on Slack; we recommend to enable Slack notifications. We answer email and monitor Slack, which may all be used for clarifying homework assignments and other interactions. We strongly recommend to ask questions publicly on Slack if others might have similar questions. Email or slack us if you would like to make an appointment.</p>
<p><strong>Auditing:</strong> Due to the high demand for this course, we do <em>not</em> allow auditing. If you like to self-study, all course materials are online. We welcome interested students and visitors to sit in for lectures as long as the room capacity allows it. </p>
<p><strong>Time management:</strong> This is a 12-unit course, and it is our intention to manage it so that you spend close to 12 hours a week on the course, on average. In general, 3 hours/week will be spent in class, about 1 hour for the labs, 1-2 hours on readings and reading quizzes, and 6-7 hours on assignments. Notice that much homework is done in groups, so please account for the overhead and decreased time flexibility that comes with groupwork. Please give the course staff feedback if the time the course is taking for you differs significantly from our intention.</p>
<p><strong>Writing:</strong> Describing tradeoffs among decisions and communication with stakeholders from other backgrounds are key aspects of this class. Many homework assignments have a component that requires discussing issues in written form or reflecting about experiences. To practice writing skills, the Global Communications Center (GCC) offers one-on-one help for students, along with workshops. The instructors are also happy to provide additional guidance if requested.</p>
<p><strong>Use of content generation AI tools and external sources:</strong> Given the nature of this course, we are open to using AI tools for completing work. We place no restrictions on the use of content generation tools, such as ChatGPT, Bard, Co-Pilot, or Stable Diffusion. You may also reuse code from external sources, such as StackOverflow or tutorials. In any case, you will be solely responsible for the correctness of the solution. Note that content generation tools often create plausible-looking but incorrect answers, which will not receive credit. You are also responsible for complying with any applicable licenses. If you use content generation tools, we encourage you to share your experience with the course staff or the entire class.</p>
<p><strong>Academic honesty and collaboration:</strong> The usual policies apply, especially the <a href="https://www.cmu.edu/policies/student-and-student-life/academic-integrity.html">University Policy on Academic Integrity</a>. Many parts of the work will be done in groups. We expect that group members collaborate with one another, but that groups work independently from other groups, not exchanging results with other groups. Within groups, we expect that you are honest about your contribution to the group's work. This implies not taking credit for others' work and not covering for team members that have not contributed to the team. This also applies to in-class discussions, where indicating working with others who did not participate in the discussion is considered an academic honesty violation. Otherwise, our expectations regarding academic honestly and collaboration for group and pair work are the same as for individual work, substituting elevated to the level of "group."</p>
<p>Beyond that, the key guiding principle of academic honesty in this course is: <em>"You may not copy any part of a solution to a problem that was written by another student (in this or prior iterations of the class), or was developed together with another student, or was delegated to another person. You may not look at another student's solution, even if you have completed your own, nor may you knowingly give your solution to another student or leave your solution where another student can see it.</em>" Note that this implies that you cannot publicly post your solutions on GitHub (e.g., as part of a portfolio during job applications). While the use of AI content generation tools is okay (see above) using the work from other students is not. Discussing challenges and solution strategies with others at a high level is okay, sharing code or text is not.</p>
<p>You may collaborate with other students on labs, but not on reading quizzes, homeworks, and exams.</p>
<p>We also expect and respect honesty when communicating with the course staff.</p>
<p>Any violation of this policy is cheating. The minimum penalty for cheating will be a zero grade for the whole assignment. Cheating incidents will also be reported through University channels, with possible additional disciplinary action (see the University Policy on Academic Integrity). There is no statute of limitations for violations of the collaboration policy; penalties may be assessed (and referred to the university disciplinary board) after you have completed the course, and some requirements of the collaboration policy (such as restrictions on you posting your solutions) extend beyond your completion of the course.</p>
<p>If you have any question about how this policy applies in a particular situation, ask the instructors for clarification.</p>
<p><strong>Research in this Course:</strong> We are conducting academic research in this course. This research will involve analyzing student work of assignment. You will not be asked to do anything above and beyond the normal learning activities and assignments that are part of this course. You are free not to participate in this research, and your participation will have no influence on your grade for this course or your academic career at CMU. If you do not wish to participate, please send an email to Nadia Nahar (<a href="mailto:nadian@andrew.cmu.edu">nadian@andrew.cmu.edu</a>). Participants will not receive any compensation or extra credit. The data collected as part of this research will not include student grades. All analyses of data from participants’ coursework will be conducted after the course is over and final grades are submitted -- instructors will not know who chooses not to participate before final grades are submitted. All data will be analyzed in de-identified form and presented in the aggregate, without any personal identifiers. If you have questions pertaining to your rights as a research participant, or to report concerns to this study, please contact Nadia Nahar (<a href="mailto:nadian@andrew.cmu.edu">nadian@andrew.cmu.edu</a>) or the Office of Research Integrity and Compliance at Carnegie Mellon University (<a href="mailto:irb-review@andrew.cmu.edu">irb-review@andrew.cmu.edu</a>; phone: 412-268-4721).</p>
<p><strong>Accommodations for students with disabilities:</strong> If you have a disability with an accommodations letter from the Disability Resources office, we encourage you to discuss your accommodations and needs with us as early in the semester as possible. We will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, we encourage you to contact them at <a href="mailto:access@andrew.cmu.edu">access@andrew.cmu.edu</a>.</p>
<p><strong>Respect for diversity:</strong> It is our intent that students from all diverse backgrounds and perspectives be well served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that students bring to this class be viewed as a resource, strength and benefit. It is my intent to present materials and activities that are respectful of diversity: gender, sexuality, disability, age, socioeconomic status, ethnicity, race, and culture. Especially in lectures on fairness we will also cover diversity discussions, typically through a lens of the contemporary discourse in the US. Your suggestions are encouraged and appreciated. Please let us know ways to improve the effectiveness of the course for you personally or for other students or student groups. </p>
<p><strong>A note on self care.</strong> Please take care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress. All of us benefit from support during times of struggle. You are not alone. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is often helpful.
If you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 and visit their website at <a href="http://www.cmu.edu/counseling/">http://www.cmu.edu/counseling/</a>. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help.</p>

        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why OpenAI's $157B valuation misreads AI's future (Oct 2024) (135 pts)]]></title>
            <link>https://foundationcapital.com/why-openais-157b-valuation-misreads-ais-future/</link>
            <guid>42847825</guid>
            <pubDate>Tue, 28 Jan 2025 01:17:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://foundationcapital.com/why-openais-157b-valuation-misreads-ais-future/">https://foundationcapital.com/why-openais-157b-valuation-misreads-ais-future/</a>, See on <a href="https://news.ycombinator.com/item?id=42847825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<figure><img fetchpriority="high" decoding="async" width="1024" height="576" src="https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-1024x576.jpg" alt="" srcset="https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-1024x576.jpg 1024w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-300x169.jpg 300w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-768x432.jpg 768w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-1536x864.jpg 1536w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-375x211.jpg 375w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-323x182.jpg 323w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1.jpg 1921w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>







<p><em>I break down the logic behind the company’s towering price tag and why I think the most valuable AI companies have yet to be built.</em></p>



<p>When <a target="_blank" rel="noreferrer noopener" href="https://finance.yahoo.com/news/openai-closed-funding-round-raising-161842157.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAKFtOVtzprQmt1Og2sCCvZcWat-nslChIXPcvptCstCn0VSkVmkJhLNkMG81WlL0UJ5usiDd_A3A1TlLylFLfmyMmR3XTInJnRHlMkYh5_xXipGOGrWDZK_V5B63fSmIlcw4wbPuxnewKZc8DCEjwjVFHwA-Pj7dQWC7d39RGOKF">OpenAI raised $6.6B</a> earlier this month—the second-largest private funding round in history, topped only by its own $10B raise from Microsoft last January—it wasn’t just setting records. It was making an argument about where AI will create value and who stands to capture it.</p>



<p>Understanding this argument, both its logic and its limitations, sheds light on where I believe the most promising opportunities in AI are likely to emerge.</p>



<h2><strong>The bull case</strong></h2>



<p>OpenAI’s growth has been nothing short of meteoric. Monthly revenue reached <a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html">$300M</a> in August 2023, a 1,700% increase from January. 10M users pay $20/month for ChatGPT, and the company projects $11.6B in revenue next year.&nbsp;</p>



<p>This growth, which outpaces even the early days of Google and Facebook, underpins the bull case by <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=ePfNAKopT20">investors like Brad Gerstner</a>. In his view, calling OpenAI a “model company” is like calling Google a “search algorithm company”—it fundamentally misunderstands the scale of the opportunity. ChatGPT isn’t just another tech product; it’s a fundamental advance in how humans interact with computers, “a hundred times better than the card catalog known as 10 blue links that Google built 25 years ago.”&nbsp;</p>



<p>Google did indeed transcend its origins in search, but only by first mastering its core business. Today, OpenAI is attempting to be a research lab, a developer platform, an enterprise solution, and a consumer product company all at once. Its investors are betting that AI is so transformative that the usual rules of focus and specialization don’t apply. The first company to achieve AGI will win everything, so the optimal strategy is to pursue every advantage simultaneously.</p>



<h2><strong>The bear case</strong></h2>



<h3><strong>💸 Bad economics</strong></h3>



<p>This narrative collides with a stubborn reality: the economics of AI don’t work like traditional software. OpenAI is currently valued at <a target="_blank" rel="noreferrer noopener" href="https://www.linkedin.com/posts/jonmcneill1_157-billion-thats-the-latest-valuation-activity-7252026319608131585-Z-Us?utm_source=share&amp;utm_medium=member_desktop">13.5x forward revenue</a>—similar to what Facebook commanded at its IPO. But while Facebook’s costs decreased as it scaled, OpenAI’s costs are growing in lockstep with its revenue, and sometimes faster.</p>



<p>In traditional software, increasing scale leads to improving economics. A typical software company might spend heavily on development upfront, but each additional user costs almost nothing to serve. Fixed costs are spread across a growing revenue base, creating the enviable margins that make today’s tech giants among the most profitable businesses in history.&nbsp;</p>



<p>Generative AI plays by different rules. Each query to a model costs money in compute resources, while each new model requires massive investments in training. OpenAI expects to lose $5B this year on $3.7B in revenue. Their projected losses through 2028 <a target="_blank" rel="noreferrer noopener" href="https://www.theinformation.com/articles/openai-projections-imply-losses-tripling-to-14-billion-in-2026?rc=e43qsi">amount to $44B</a>, excluding stock compensation. Computing costs alone will reach $6B this year.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.newcomer.co/p/the-bear-case-for-openai-at-157-billion">Google’s 2004 IPO</a> followed years of profitable operation, with $106M in profit on $962M in revenue. Facebook went public after achieving $1B in profit on $3.7B in revenue. Both companies demonstrated that growth improved their profit margins. OpenAI, by contrast, plans to 100x revenue to $100B by 2029 while piling up progressively larger losses. This requires maintaining <a target="_blank" rel="noreferrer noopener" href="https://www.marketwatch.com/story/openai-making-money-from-the-ai-revolution-is-far-from-certain-32249d5a">93% annual growth</a> for five years—a rate achieved by only a handful of companies in history, all of which saw their economics improve with scale.</p>



<p>The infrastructure needs to sustain AI’s progress are staggering. Microsoft, OpenAI’s primary partner, plans to spend $80-110B on AI infrastructure next year alone. According to semiconductor analyst <a target="_blank" rel="noreferrer noopener" href="https://www.dwarkeshpatel.com/p/dylan-jon?open=false#%C2%A7are-we-financing-an-ai-bubble">Dylan Patel</a>, Microsoft is building computing clusters with 100,000 GPUs and aims to construct a single facility consuming one gigawatt of power by 2026. By 2028, their computing requirements could reach multiple gigawatts—equivalent to an <a target="_blank" rel="noreferrer noopener" href="https://e360.yale.edu/features/artificial-intelligence-climate-energy-emissions">entire country’s</a> electricity demand.&nbsp;</p>



<p>To put these numbers in context: At the height of the dot-com bubble, the entire internet economy generated <a target="_blank" rel="noreferrer noopener" href="https://www.marketwatch.com/story/openai-making-money-from-the-ai-revolution-is-far-from-certain-32249d5a">$1.5T in revenue</a> (adjusted to 2024 dollars). Today, generative AI companies produce less than $10B in revenue while planning infrastructure investments that <a target="_blank" rel="noreferrer noopener" href="https://www.goldmansachs.com/insights/articles/will-the-1-trillion-of-generative-ai-investment-pay-off">could exceed $1T</a>.</p>



<h3><strong>🚫 No technical moat</strong></h3>



<p>OpenAI’s challenges extend beyond economics. Its leadership team has seen <a target="_blank" rel="noreferrer noopener" href="https://www.theinformation.com/articles/behind-openais-staff-churn-turf-wars-burnout-compensation-demands?rc=e43qsi">near total turnover</a> over the past year. Eight of its eleven co-founders, including CTO Mira Murati and chief scientist Ilya Sutskever (who are launching competing ventures), have left. CEO Sam Altman has responded by recruiting experienced executives, like Sarah Friar as CFO and Kevin Weil as head of product. But when nearly all the talent that built your breakthrough technology goes elsewhere, it’s worth asking why.</p>



<p>What’s more, OpenAI’s massive infrastructure investments might not be building a moat—they might just be the cost of staying in the race. As board chair <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=vRhPc0zt2IE">Bret Taylor</a> admits, we’re watching “the fastest technology commoditization cycle we’ve ever seen.” GPT-4’s pricing per token has <a target="_blank" rel="noreferrer noopener" href="https://www.sequoiacap.com/article/generative-ais-act-o1/">plummeted 98%</a> since last year’s dev day. The gap between their SOTA models and open-source alternatives is narrowing with a speed that should make any investor nervous.</p>



<h3><strong>🌍 Distribution and openness matter</strong></h3>



<p>This brings me to what might be the most important dynamics in AI today: distribution and openness. In tech, the winners aren’t always those with the most advanced technology—they’re often those who build the most compelling ecosystems.</p>



<p>In a <a target="_blank" rel="noreferrer noopener" href="https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/">recent memo</a>, Zuckerberg draws a parallel to the early days of high-performance computing, when major tech companies poured resources into proprietary Unix systems. At the time, few imagined that open-source alternatives could win. Yet Linux ultimately prevailed—not because it was better from the start, but because it allowed developers to modify the code freely, run it more securely and affordably, and build a broader ecosystem that enabled more capabilities than any closed system.</p>



<p>Meta is betting AI will follow a similar path. While Llama 2 could only match older, closed models, Llama 3 has reached parity with the frontier. Their <a target="_blank" rel="noreferrer noopener" href="https://ai.meta.com/blog/meta-llama-3-1/">Llama 3.1 405B</a> model can reportedly run at roughly half the cost of GPT-4, while giving enterprises something potentially more valuable than raw performance: complete control over their data and freedom from vendor lock-in.&nbsp;</p>



<p>Meanwhile, Meta’s consumer distribution is unmatched. LLaMA 3 currently powers AI features across Facebook, Instagram, and WhatsApp, reaching 1.1B users in 14 countries—and they’re only a third through their rollout. While ChatGPT has reached <a target="_blank" rel="noreferrer noopener" href="https://www.pewresearch.org/short-reads/2024/03/26/americans-use-of-chatgpt-is-ticking-up-but-few-trust-its-election-information/">23% of adults in the U.S.</a>, Meta is deploying its AI to billions of users globally. LLaMA 3 might not match GPT-4 in every research benchmark, but it doesn’t need to. Meta has optimized for what most users want: quick, reliable responses on mobile devices, especially in emerging markets where simpler queries dominate.</p>



<p>This creates brutal economics for OpenAI. While they reportedly plan to <a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html">raise ChatGPT’s subscription price</a> to $44/month over the next five years, Meta can give away their AI for free. As <a target="_blank" rel="noreferrer noopener" href="https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/">Zuckerberg notes</a>: “Selling access to AI models isn’t our business model. Openly releasing Llama doesn’t undercut our revenue or ability to invest in research like it does for closed providers.”</p>



<p>Other factors in Meta’s favor: it’s unencumbered by the legacy issues slowing down competitors like Google, whose search-based advertising business faces an existential threat from generative AI. It also has a founder-CEO willing to prioritize long-term market dominance over short-term profits.</p>



<h3><strong>⚓ The Microsoft dilemma</strong></h3>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2024/10/17/technology/microsoft-openai-partnership-deal.html">OpenAI’s relationship with Microsoft</a> introduces another layer of complexity. What started as a lifeline—providing essential capital and computing resources—now risks becoming a constraint. Microsoft receives 75% of OpenAI’s profits until its $13B investment is recouped, followed by 49% until it hits $92B.</p>



<p>Their partnership shows growing signs of strain. Microsoft’s $650M acquisition of Inflection AI’s team looks less like opportunistic talent acquisition and more like a hedge against overreliance on OpenAI. Meanwhile, OpenAI’s $10B computing contract with Oracle (while structured through Microsoft to maintain exclusivity agreements) suggests a push for independence from Microsoft’s infrastructure.</p>



<p>Adding to the tensions is <a target="_blank" rel="noreferrer noopener" href="https://www.wsj.com/tech/ai/the-14-billion-question-dividing-openai-and-microsoft-71cf7d37">OpenAI’s pending change</a> from a nonprofit to a for-profit entity—one of the most controversial corporate restructurings in recent history. Both companies have engaged investment banks to negotiate Microsoft’s future equity position. OpenAI needs to complete this conversion within two years or risk investors from the latest round demanding their money back. Any major changes will also need approval from the FTC, which has been less than friendly of late to big tech.</p>



<h2><strong>Where value in AI will accrue</strong></h2>



<p>So where will the most promising opportunities in AI for investors and startups lie? Analyzing the AI ecosystem layer by layer reveals where the most durable value is likely to emerge.</p>



<h3><strong>🏗️ Physical and cloud infrastructure</strong></h3>



<p>At the bottom of the AI stack is hardware: vast arrays of GPUs, specialized processors, networking equipment, and massive data centers. Hyperscalers are investing billions here—not for immediate returns, but for strategic control. Each is pursuing vertical integration, developing proprietary silicon to reduce reliance on suppliers like NVIDIA, which is also <a target="_blank" rel="noreferrer noopener" href="https://venturebeat.com/ai/nvidia-just-dropped-a-bombshell-its-new-ai-model-is-open-massive-and-ready-to-rival-gpt-4/">moving up the stack</a> to compete in models. Several AI chip startups are also competing at this layer.</p>



<h3><strong>🧬 Foundation models</strong></h3>



<p>This layer seems exciting until you look at the economics. Building and improving foundation models requires massive ongoing investment, yet their value erodes faster than perhaps any other technology to date. Hyperscalers can absorb these costs by using models to drive demand for their cloud services, but independent startups face a steeper climb. The future of general-purpose models increasingly resembles a race to the bottom.&nbsp;</p>



<p>Still, there may be a handful of opportunities for model startups to carve out niches, whether by leveraging proprietary data or developing new architectures like state-space models. We have a few stealth investments in this category.</p>



<h3><strong>🛠️ Software infrastructure and developer tools</strong></h3>



<p>The next layer up includes companies that help developers build AI-powered software. Startups like Anyscale, Arize, Skyflow, and Turing are players here.</p>



<p>The cloud era saw <a target="_blank" rel="noreferrer noopener" href="https://www.sequoiacap.com/article/generative-ais-act-o1/">about 15 companies</a> scale to $1B+ revenue at this layer. I expect a similar pattern to play out in AI, with a few dozen startups building valuable franchises to serve AI developers. Our current investments align with this view, and we’re particularly excited about tooling to develop agentic systems.&nbsp;</p>



<p>At the same time, this category is challenging due to the rapid pace of innovation, competition from open source, and bundling by both hyperscalers and proprietary model providers. As one example, vector databases like Pinecone were highly sought after just a year ago, but their appeal has since waned.</p>



<h3><strong>🤖 AI Applications</strong></h3>



<p>The top of the stack is where I see the most promise. AI is not just adding features to existing software; it’s transforming entire service industries into software products. This shift expands the addressable market from the $350B software sector to the multi-trillion dollar services industry.</p>



<p>While OpenAI has focused on building general-purpose models, a new wave of specialized startups is addressing specific industry needs with precision. The early dismissal of these companies as “GPT wrappers”—basic interfaces layered over foundation models—now feels outdated. We’re seeing the rise of <a target="_blank" rel="noreferrer noopener" href="https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/">compound AI systems</a> that combine multiple AI models with retrieval mechanisms, external tools, and diverse data sources, orchestrated by advanced control logic.</p>



<p>History suggests this layer will produce the most winners. The cloud era created <a target="_blank" rel="noreferrer noopener" href="https://www.sequoiacap.com/article/generative-ais-act-o1/">over 20 application companies with $1B+ revenue</a>. In AI, we believe this number could exceed 100. These new companies will redefine how industries operate and potentially replace legacy systems of record like Salesforce and SAP.</p>



<h2>Where I land</h2>



<p>New technologies, no matter how revolutionary, don’t automatically translate into sustainable businesses. OpenAI’s $157B valuation suggests we might be forgetting this lesson.</p>



<p>This isn’t to diminish what OpenAI has achieved. They’ve shown us that AI can do things many thought impossible just a few years ago. They’ve forced enterprises to rethink how they operate and changed how humans interact with computers. But starting a revolution isn’t the same as profiting from it. Today’s headline-grabbing AI companies are creating tremendous value, but that doesn’t guarantee they’ll be the ones to capture it in the long run.</p>



<div><p>I’d argue that the most valuable companies of the AI era don’t exist yet. They’ll be the startups that harness AI’s potential to solve specific, costly problems across our economy—from engineering and finance to healthcare, logistics, legal, marketing, sales, and more.</p><p><em>For more, I also publish to <a href="https://ashugarg.substack.com/">Substack</a>, along with more frequent updates on <a href="https://www.linkedin.com/in/ashugargvc">LinkedIn</a> and <a href="https://x.com/ashugarg?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">X</a> where I share perspectives on enterprise software, AI, and the technical founder’s journey.</em></p></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I trusted an LLM, now I'm on day 4 of an afternoon project (262 pts)]]></title>
            <link>https://nemo.foo/blog/day-4-of-an-afternoon-project</link>
            <guid>42845933</guid>
            <pubDate>Mon, 27 Jan 2025 21:37:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nemo.foo/blog/day-4-of-an-afternoon-project">https://nemo.foo/blog/day-4-of-an-afternoon-project</a>, See on <a href="https://news.ycombinator.com/item?id=42845933">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>TLDR - AI isn’t a co-pilot; it’s a junior dev faking competence. Trust it at your own risk.</strong></p>
<p>I’m 4 days into an afternoon project. I was so sure I’d crush this one. I had a good plan and the stoke was high. Let me introduce <em>Deskthang</em>. It’s a thang for your desk. When I work, I want to put my phone in the other room, and only get the important notifications (thangs) in a different way. If my deployment pipeline fails, I want a globe on my desk to turn red and show me a gitlab logo. I do not want to check my phone or email or anywhere a distraction might find me.</p>
<blockquote>
<p>Quick backstory: I work full time++ doing boring enterprise software dev and rarely get to flex my engineering skills. While my title says engineer, I’d disagree.</p>
</blockquote>
<h2>The Problem I’m Trying to Solve</h2>
<p>I always try to align multiple interests for a side project. I wanted to pull my electronics hardware box out of storage, I wanted to solve the notifications and focus issue for myself, and I wanted to see how scared I should be about AI taking my job. As they say, I was trying to get a few birds stoned at once.</p>
<p><img src="https://nemo.foo/blog-content/deskthang/two_birds_stoned.gif" alt=""></p>
<h3>1. I miss working with hardware.</h3>
<p>During COVID lock-downs I landed an R&amp;D contract for a IoT Prototype. That R&amp;D job was the most fulfilling work of my career. I worked with a small, scrappy team with some of my best friends. I was 3D printing models, soldering components, writing embedded C, and field-testing with mechanical engineers… Real engineers. We worked hard, often late into the night, and the collaboration felt more like playing StarCraft with the boiz than a 9-to-5. I’ve missed that deeply ever since. Recently, I’ve been inspired recently by <a href="https://x.com/_MaxBlade">@_MaxBlade and DeskHub</a> and wanted to brush the dust off my electronics skills.</p>
<h3>2. I hate the UX of MFA (Multi Factor Authentication).</h3>
<p>I use GitLab heavily for CI/CD with my personal Kubernetes projects. Knowing the status of my pipelines is crucial… broken builds could disrupt all 7 of my users! Logging into GitLab feels like getting stabbed in the spleen. Every time I log in (multiple times a day), I face captchas, authenticator apps, or waiting for email codes, followed by yet another captcha. I’ve tried pipeline notifications through Slack, Discord, and Telegram, but those apps are like productivity black holes. I don’t want my phone near me while working, or to open chat apps that derail my focus. Removing these distractions keeps me locked in.</p>
<h3>3. I want to see how good these AI tools are.</h3>
<p>I want to figure out if AI is going to take my job. I’m skeptical it can replace what I do, but I like testing my assumptions. Sometimes AI surprises me; other times, it’s just a rabbit hole of wasted hours when I avoid doing real thinking.</p>
<p>Recently, I used Claude Sonnet 3.5 to brute-force hundreds of React compile errors while upgrading a project from React 15 to 18. I threw <code>package.json</code> updates, deleted <code>node_modules</code>, and burned through a small fortune in AI tokens. To my surprise, we had a passing build by the end of the day. Work has been encouraging us to adopt an AI-first workflow and giving us unlimited tokens. It’s a wild experiment.</p>
<p>This happened on a Friday. I wiped the sweat off my brow after a hard day’s prompting, and headed home early to start on my side project…</p>
<h2>The Plan</h2>
<p>Unlike me, my wife likes to leave the house and do things. I’ve spent a few years turning my garage into my favorite place to be. My wife and I have a deal where 1 day a month, She takes the kiddo and I am absolved of all responsibilities. I get a full day to lock in and build projects. From her perspective, I order doordash and turn into a degen who is unfit to father. From my perspective, I get to enjoy my favorite place and just tinker or play games or do whatever. These are the days I get to play mad scientist and feel most like myself. I look forward to it every month. My plan was to learn zig, brush off my hardware skills, build this project, write a blog post and make a video about it. Totally achievable.</p>
<p>I wanted to wire up a Raspberry Pi Pico, a small 240x240 LCD display and some RGB LEDs. I was going to learn Zig and use it to send image data over USB to the pico which will put an image on the screen and change the LED color. I would set up webhooks from GitLab to call an API in my Kube cluster and setup my host Zig app to poll that same API for changes and send updates to the Pico. I really wanted to transmit the data over USB because I’ve never done that before. I’ve already used Bluetooth, LTE and Wifi and just wanted to do something new.</p>
<p>The wiring is simple. Common patterns I was familiar with like <a href="https://en.wikipedia.org/wiki/Serial_Peripheral_Interface">SPI (Serial Peripheral Interface)</a> for the display + some RGB leds. The <a href="https://itsfoss.com/what-is-tty-in-linux/?utm_source=chatgpt.com">TTY (TeleTYpewriter)</a> serial data port on Linux <code>/dev/ttyACM0</code> for USB communication with the Pico felt familiar because of how I had setup debug logging in the past. It looked like I had enough example repos collected that I could stitch a solution together. I did a little research each day and felt a little more sure each time. I’ve been using ChatGPT and Claude more and more to do initial research. I was at an AI hype peak and was bold enough to trust it…</p>
<p><img src="https://nemo.foo/blog-content/deskthang/excalidraw.png" alt=""></p>
<p>Since I do full stack web stuff on the daily, the api, webhooks and postgres are out of scope for the degen day. I was scoping the day’s work to Zig -&gt; Pico image transfer.</p>
<h3>1. Setup Pico</h3>
<ul>
<li>Organize the workspace</li>
<li>Find a micro usb cable that supports data and not just charging… really why are they all power only?!</li>
<li>Wire up a breadboard with Pico &amp; display and LED</li>
<li>Setup the C SDK for the raspi pico and a repo <a href="https://gitlab.com/nemofoo/deskthang">gitlab</a>, <a href="https://github.com/nemofoo/deskthang">gihub-mirror</a></li>
<li>Push a build and see logs on <code>cat /dev/ttyACM0</code></li>
</ul>
<p><img src="https://nemo.foo/blog-content/deskthang/first_assembly.jpg" alt=""></p>
<h3>2. Setup Pico Display</h3>
<ul>
<li>Put something on the screen during the boot loop.</li>
</ul>
<p><img src="https://nemo.foo/blog-content/deskthang/first_test_pattern.jpg" alt=""></p>
<p><a href="https://youtube.com/shorts/H6b64PJI40o">youtube link (12 sec)</a></p>
<h3>3. Setup Host Zig Project</h3>
<ul>
<li>Setup a host directory in repo</li>
<li>Init zig project</li>
<li>Send a message and see something on the screen</li>
</ul>
<p><a href="https://youtu.be/Y0wkzbwGWJc">youtube link (9 sec)</a></p>
<h3>4. Image Transfer</h3>
<ul>
<li>Yeet the raw rgb image data over USB</li>
<li>It’s bidirectional safe right</li>
<li>USB CDC is bidirectional safe
<ul>
<li>TTY interface is built on top of USB CDC
<ul>
<li>TTY is bidirectional safe because CDC is (no it’s not… thanks gpt)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://nemo.foo/blog-content/deskthang/gptlies.png" alt=""></p>
<blockquote>
<p>In the above image you can see the outright lie that broke me… USB CDC has separate TX (Transmit) and RX (Receive) buffers so it’s bidirectional safe. The same is not true for TTY which is bidirectional but less safe with a single buffer for TX and RX data.</p>
</blockquote>
<h2>Timeline of Actuality (AI Woes)</h2>
<p>After a dozen duds, I found a data capable usb micro cable and everything went smoothly until the image transfer. I used Claude, Cline, and ChatGPT to AI-max my way to a buggy but working implementation. I sent commands from my terminal with Zig over USB to the Pico which read them and changed the screen. This only took a few hours and I was excited that the AI assisted dream was real. It’s not complex but I think it was faster than I could have done alone. I have no experience with zig besides hearing ThePrimeagen yap about it. I haven’t even read the docs.</p>
<p>Multiple times, I found myself stopping Cline from starting completely new implementations of already solved issues. I didn’t catch everything though. When Cline blew through my API limits, I added Claude to my harem and ran both in parallel when possible.</p>
<blockquote>
<p>As I look through the code now, I realize that I’m lousy at multitasking and was gaslighting myself.</p>
</blockquote>
<h3>The Image Transfer Disaster</h3>
<p>This is where my hubris came into play. In my mind, I pictured sending all the image data in one go, like an S3 upload. I imagined clean, raw data streaming over <code>/dev/ttyACM0</code>. It wasn’t clean. It wasn’t raw. It was chaos.</p>
<p>I expected to see:</p>
<pre><code>pico - heartbeat
zig - start image transfer
zig - [240x240 COLOR PIXELS]
zig - end image transfer
pico - heartbeat
</code></pre>
<p>What I actually saw looked like this, but worse:</p>
<pre><code>pico - heartbeat
zig - start ima%
pico - heage transfert
pico - heartbeat
zig - [240x240 COL
pico - heartbea
OR PIXELtS]
pico - heartbeat
zig - end image transfer
pico - heartbeat
</code></pre>
<p><a href="https://youtu.be/jnmqlsdD6oU">It’s just like that interrupting cow knock knock joke.</a> Completely unfunny and day ruining.</p>
<p><strong>Key Problems:</strong></p>
<p><strong>1. Buffer Conflicts:</strong> <code>/dev/ttyACM0</code> was the battlefield. The same buffer was used for both logging and image transfer. If a log slipped in during the data stream… well good luck figuring out what the hell just happened.</p>
<p><strong>2. Noise:</strong> Some weird corruption was happening. Maybe I wasn’t clearing buffers properly. Maybe the gods of USB communication just hate me.</p>
<p>The bottom line? Neither the Pico nor my laptop could trust the data. Each system needed to learn to yield, and I needed to build the round-a-bout to force them to be polite and wait their turn.</p>
<h3>Packets, Protocols &amp; State Machines, Oh my…</h3>
<p>I needed to get serious. So, naturally, I let Claude write some docs:</p>
<ul>
<li>Detailed a packet shape.</li>
<li>Documented a checksum verification plan.</li>
<li>Described data format for transfer.</li>
<li>Denoted how to chunk and rebuild the image.</li>
<li>Depicted the state machine transitions.</li>
<li>Demonstrated command system.</li>
<li>Designed a logging system that doesn’t break incoming commands.</li>
</ul>
<p>After delving down dem docs, I let AI run with the actual implementations. At this point, my “degen hat” came off, and I resumed my dad duties while letting Cursor and Cline play StarCraft with my codebase. This is the dream use case for AI, right? Just let it rip and come back to a perfectly functioning system. Let’s see just how close we get to the sun.</p>
<p>Reality Check: AI tools are like interns who know how to Google really fast but don’t understand context. Cursor started changing core implementations for unrelated edits. Cline would randomly rewrite half the system without asking. By the time I noticed, my codebase looked like the aftermath of a spaghetti fight at a junior developer convention. Most of the codebase was actually unreachable.</p>
<h2>What did I learn?</h2>
<p>Like Icarus, my codebase is irrecoverable. A tangled heap of wing fragments and melted wax, dripping with half-baked ideas and unsupervised AI chaos. My grand vision of outsourcing grunt work to AI had sent me soaring, but the sun of reality burned away any hope of landing gracefully. Here’s what I’m taking away from this flaming descent.</p>
<h3>1. AI is a tool, not a co-pilot</h3>
<p>AI is great for generating ideas or drafting code, but it doesn’t understand. It’s like giving a junior developer a chainsaw instead of a scalpel—it might finish the job, but you’ll spend twice as long cleaning up the mess. I learned that I need to stay firmly in the driver’s seat when tackling new tech.</p>
<h3>2. Friction forces focus</h3>
<p>Having AI directly in my editor felt like playing with infinite cheat codes. It was too easy to let it run wild and harder to maintain control. Moving forward, I’m introducing deliberate friction. I will be using AI only in web interfaces or as a brainstorming tool. If I have to paste its suggestions into my code manually, I’ll be more mindful of the process and less likely to reach for it.</p>
<h3>3. Mistakes teach better than shortcuts</h3>
<p>When I make mistakes, I learn. Debugging my own failures has always been one of the best ways to understand a new language or concept. Relying on AI to “fix” things for me short-circuited that learning process. As a result, I’m left with no deeper understanding of Zig than when I started.</p>
<h3>4. Patience beats hubris</h3>
<p>Building something new, with unfamiliar tools takes time. The idea that I could fully implement my vision in a single “degen day” was overly optimistic, bordering on foolish. Sometimes, you have to respect the complexity of what you’re trying to achieve.</p>
<h2>Moving Forward</h2>
<p>Deskthang has grown from a casual afternoon project into a saga of overconfidence, AI misadventures, and lessons learned the hard way. For now, I’m shelving the AI driven shortcuts and committing to a rewrite on my next no-responsibilities day.</p>
<p>I’ve picked up a pen and started writing docs by hand like it’s the stone age. I plan to work through some Advent of Code problems in Zig to actually learn the language before taking another crack at this project.</p>
<p>Want to see if Deskthang ever works, or just enjoy the chaos as I fail forward? Subscribe below for a monthly email drop of my latest misadventures and skill issues. Together, we’ll learn how to coexist with AI, relearn the lessons I forget, and hopefully build something worthwhile in the process.</p>
<p>LFG 🚀</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia sheds almost $600B in market cap, biggest one-day loss in US history (242 pts)]]></title>
            <link>https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html</link>
            <guid>42845681</guid>
            <pubDate>Mon, 27 Jan 2025 21:13:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html">https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html</a>, See on <a href="https://news.ycombinator.com/item?id=42845681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="SpecialReportArticle-ArticleBody-6" data-module="ArticleBody" data-test="articleBody-2" data-analytics="SpecialReportArticle-articleBody-6-2"><div id="ArticleBody-InlineImage-108082997" data-test="InlineImage"><p>Nvidia CEO Jensen Huang holds a Blackwell GeForce RTX 50 Series GPU (L) and a RTX 5000 laptop as he delivers a keynote address at the Consumer Electronics Show (CES) in Las Vegas, Nevada on January 6, 2025.&nbsp;</p><p>Patrick T. Fallon | Afp | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/NVDA/">Nvidia</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> lost close to $600 billion in market cap on Monday, the biggest drop for any company on a single day in U.S. history.</p><p>The chipmaker's stock price plummeted 17% to close at $118.58. It was Nvidia's worst day on the market since March 16, 2020, which was early in the Covid pandemic. After <a href="https://www.cnbc.com/2025/01/21/nvidia-passes-apple-again-to-become-worlds-most-valuable-company-.html">Nvidia surpassed</a> <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-3"><a href="https://www.cnbc.com/quotes/AAPL/">Apple</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> last week to become the most valuable publicly traded company, the stock's drop Monday led a 3.1% slide in the tech-heavy Nasdaq.</p><p>The sell-off was <a href="https://www.cnbc.com/2025/01/27/nvidia-falls-10percent-in-premarket-trading-as-chinas-deepseek-triggers-global-tech-sell-off.html">sparked</a> by concerns that Chinese <a href="https://www.cnbc.com/ai-artificial-intelligence/">artificial intelligence</a> lab DeepSeek is presenting increased competition in the global AI battle. In late December, <a href="https://www.cnbc.com/2025/01/24/how-chinas-new-ai-model-deepseek-is-threatening-us-dominance.html">DeepSeek unveiled</a> a free, open-source large language model that <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf" target="_blank">it&nbsp;said</a>&nbsp;took only two months and less than $6 million to build, using reduced-capability chips from&nbsp;<a href="https://www.cnbc.com/quotes/NVDA/">Nvidia</a>&nbsp;called H800s.&nbsp;</p><p>Nvidia's graphics processing units, or GPUs, dominate the market for AI data center chips in the U.S., with tech giants such as <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-9"><a href="https://www.cnbc.com/quotes/GOOGL/">Alphabet</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-10"><a href="https://www.cnbc.com/quotes/META/">Meta</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-11"><a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> spending billions of dollars on the processors to train and run their AI models. </p><p>Analysts at Cantor wrote in a report Monday that the release of DeepSeek's latest technology has caused "great angst as to the impact for compute demand, and therefore, fears of peak spending on GPUs."</p></div><div id="SpecialReportArticle-RelatedContent-1"><h2>Read more DeepSeek coverage</h2><div><ul><li><a href="https://www.cnbc.com/2025/01/27/chinas-deepseek-ai-tops-chatgpt-app-store-what-you-should-know.html">China's DeepSeek AI dethrones ChatGPT on App Store</a></li><li><a href="https://www.cnbc.com/2025/01/27/deepseek-hit-with-large-scale-cyberattack-says-its-limiting-registrations.html">DeepSeek hit with large-scale cyberattack, says it's limiting registrations</a></li><li><a href="https://www.cnbc.com/2025/01/24/how-chinas-new-ai-model-deepseek-is-threatening-us-dominance.html">How China's new AI model DeepSeek is threatening U.S. dominance</a></li><li><a href="https://www.cnbc.com/2025/01/27/nvidia-falls-10percent-in-premarket-trading-as-chinas-deepseek-triggers-global-tech-sell-off.html">Nvidia hits new low for session on threat from DeepSeek AI model</a></li><li><a href="https://www.cnbc.com/2025/01/27/how-the-buzz-around-chinese-ai-model-deepseek-sparked-a-massive-nasdaq-sell-off.html">Buzz around Chinese AI model DeepSeek sparks massive Nasdaq sell-off</a></li><li><a href="https://www.cnbc.com/2025/01/27/the-key-chart-levels-to-watch-on-nvidia-tech-stocks-on-deepseek-fear.html">Pro: The key chart levels to watch on Nvidia and other tech stocks amid DeepSeek rout</a></li></ul></div></div><div><p>The analysts said they "think this view is farthest from the truth" and that advancements in AI will most likely lead to "the AI industry wanting more compute, not less." They recommend buying Nvidia shares.</p><p>But after Nvidia's huge run-up — the stock soared 239% in <a href="https://www.cnbc.com/2023/05/30/nvidia-on-track-to-hit-1-trillion-market-cap-when-market-opens.html">2023</a> and 171% in <a href="https://www.cnbc.com/2024/12/25/ai-crypto-top-tech-stocks-applovin-microstrategy-palantir-nvidia.html">2024</a> — the market is on edge about any possible pullback in spending. <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-14"><a href="https://www.cnbc.com/quotes/AVGO/">Broadcom</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, the other big U.S. chipmaker to see giant valuation gains from AI, fell 17% on Monday, pulling its market cap down by $200 billion.</p><p>Data center companies reliant on Nvidia's GPUs for their hardware sales saw big sell-offs as well. <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-15"><a href="https://www.cnbc.com/quotes/DELL/">Dell</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-16"><a href="https://www.cnbc.com/quotes/HPE/">Hewlett Packard Enterprise</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-17"><a href="https://www.cnbc.com/quotes/SMCI/">Super Micro Computer</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> dropped at least 5.8%. <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-18"><a href="https://www.cnbc.com/quotes/ORCL/">Oracle</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, a part of President<a href="https://www.cnbc.com/donald-trump/"> Donald Trump's</a> latest AI initiative, fell 14%.</p><p>For Nvidia, the loss was more than <a href="https://www.cnbc.com/2024/09/04/asian-chip-stocks-fall-after-nvidia-sell-off-on-wall-street-overnight.html">double the $279 billion drop</a> the company saw in September, which was the biggest one-day market value loss in history at the time, unseating Meta's <a href="https://www.cnbc.com/2022/02/03/facebooks-232billion-drop-in-value-sets-all-time-record.html">$232 billion loss</a> in 2022. Before that, the steepest drop was $182 billion by Apple in 2020.</p><p>Nvidia's decline is more than double the market cap of <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-22"><a href="https://www.cnbc.com/quotes/KO/">Coca-Cola</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-23"><a href="https://www.cnbc.com/quotes/CVX/">Chevron</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and exceeds the market value of both Oracle and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-24"><a href="https://www.cnbc.com/quotes/NFLX/">Netflix</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>.</p><p>CEO Jensen Huang's net worth also took a massive hit, declining roughly $21 billion, according to <a href="https://www.forbes.com/real-time-billionaires/#458f33ab3d78" target="_blank">Forbes' real-time billionaires list</a>. The move demoted Huang to 17th on the richest-person list.</p><p>The sudden excitement around DeepSeek over the weekend pushed its app <a href="https://www.cnbc.com/2025/01/27/chinas-deepseek-ai-tops-chatgpt-app-store-what-you-should-know.html">past OpenAI's ChatGPT</a> as the most-downloaded free app in the U.S. on Apple's app store. The model's development comes despite a slew of recent curbs on U.S. chip exports to China.</p><p>Venture capitalist David Sacks, who was <a href="https://www.cnbc.com/2024/12/05/trump-david-sacks-billionaire-ai-crypto.html">tapped</a> by Trump to be the White House's AI and crypto czar, <a href="https://x.com/DavidSacks/status/1883935713877782884" target="_blank">wrote on X</a> that DeepSeek's model "shows that the AI race will be very competitive" and that Trump was right to rescind President <a href="https://www.cnbc.com/video/2019/04/25/joe-biden-enters-2020-presidential-race.html">Joe Biden</a>'s executive order last week on AI safety.</p><p>"I'm confident in the U.S. but we can't be complacent," Sacks wrote.</p><p>Nvidia is now the third most-valuable public company, behind Apple and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-30"><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>.</p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2025/01/27/pro-watch-cnbcas-full-interview-with-bernsteins-stacy-rasgon-trivariateas-adam-parker-and-payne-capitalas-courtney-garcia.html">CNBC's full interview with Bernstein's Stacy Rasgon</a></p></div><div id="Placeholder-ArticleBody-Video-108093000" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000364746" aria-labelledby="Placeholder-ArticleBody-Video-108093000"><p><img src="https://image.cnbcfm.com/api/v1/image/108093001-17380094731738009466-38180086461-1080pnbcnews.jpg?v=1738009472&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Watch CNBC’s full interview with Bernstein's Stacy Rasgon, Trivariate’s Adam Parker and Payne Capital’s Courtney Garcia"></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Illustrated DeepSeek-R1 (460 pts)]]></title>
            <link>https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1</link>
            <guid>42845488</guid>
            <pubDate>Mon, 27 Jan 2025 20:51:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1">https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1</a>, See on <a href="https://news.ycombinator.com/item?id=42845488">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em><span>[Draft post, updates to come, please let me know if you have any suggestions or feedback here or on </span><a href="https://bsky.app/profile/jayalammar.bsky.social" rel="">Bluesky</a><span> or </span><a href="https://x.com/JayAlammar" rel="">X/Twitter</a><span>]</span></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png" width="1130" height="408" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/98138856-a4de-45e3-ad08-1434378127c2_1130x408.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:408,&quot;width&quot;:1130,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>DeepSeek-R1 is the latest resounding beat in the steady drumroll of AI progress. For the ML R&amp;D community, it is a major release for reasons including: </p><ol><li><p>It is an open weights model with smaller, distilled versions and </p></li><li><p>It shares and reflects upon a training method to reproduce a reasoning model like OpenAI O1. </p></li></ol><p>In this post, we’ll see how it was built.</p><p>Contents:</p><ul><li><p>Recap: How LLMs are trained</p></li><li><p>DeepSeek-R1 Training Recipe</p></li><li><p>1- Long chains of reasoning SFT Data</p></li><li><p>2- An interim high-quality reasoning LLM (but worse at non-reasoning tasks).</p></li><li><p>3- Creating reasoning models with large-scale reinforcement learning (RL) </p><ul><li><p>3.1- Large-Scale Reasoning-Oriented Reinforcement Learning (R1-Zero)</p></li><li><p>3.2- Creating SFT reasoning data with the interim reasoning model</p></li><li><p>3.3- General RL training phase </p></li></ul></li><li><p>Architecture</p></li></ul><p><span>Most of the foundational knowledge you need to understand how such a model works is available in our book, </span><a href="https://github.com/handsOnLLM/Hands-On-Large-Language-Models" rel="">Hands-On Large Language Models</a><span>.</span></p><p>Just like most existing LLMs, DeepSeek-R1 generates one token at a time, except it excels at solving math and reasoning problems because it is able to spend more time processing a problem through the process of generating thinking tokens that explain its chain of thought.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif" width="613" height="152" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5280089e-8989-45d7-8194-93396b25557d_613x152.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:152,&quot;width&quot;:613,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3441758,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The following figure, from Chapter 12 of our book shows the general recipe of creating a high-quality LLM over three steps:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png" width="1456" height="434" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:434,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>1) The language modeling step where we train the model to predict the next word using a massive amount of web data. This step results in a base model.</p><p>2) a supervised fine-tuning step that makes the model more useful in following instructions and answering questions. This step results in an instruction tuned model or a supervised fine -tuning / SFT model.</p><p>3) and finally a preference tuning step which further polishes its behaviors and aligns to human preferences, resulting in the final preference-tuned LLM which you interact with on playgrounds and apps.</p><p><span>DeepSeek-R1 follows this general recipe. The details of that first step come from a </span><a href="https://arxiv.org/pdf/2412.19437v1" rel="">previous paper for the DeepSeek-V3 model</a><span>. R1 uses the </span><em>base</em><span> model (not the final DeepSeek-v3 model) from that previous paper, and still goes through an SFT and preference tuning steps, but the details of how it does them are what's different.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png" width="854" height="234" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:234,&quot;width&quot;:854,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:30102,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>There are three special things to highlight in the R1 creation process.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png" width="854" height="434" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/26136780-897d-4f64-b1e5-45936b6078dd_854x434.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:434,&quot;width&quot;:854,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:43757,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This is a large number of long chain-of-thought reasoning examples (600,000 of them). These are very hard to come by and very expensive to label with humans at this scale. Which is why the process to create them is the second special thing to highlight</p><p><span>This data is created by a precursor to R1, an unnamed sibling which specializes in reasoning. This sibling is inspired by a third model called </span><em>R1-Zero </em><span>(that we’ll discuss shortly). It is significant not because it’s a great LLM to use, but because creating it required so little labeled data alongside large-scale reinforcement learning resulting in a model that excels at solving reasoning problems. </span></p><p>The outputs of this unnamed specialist reasoning model can then be used to train a more general model that can also do other, non-reasoning tasks, to the level users expect from an LLM.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png" width="924" height="427" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:427,&quot;width&quot;:924,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:50317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This happens in two steps:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png" width="1456" height="629" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:629,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:176092,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Here, RL is used to create the interim reasoning model. The model is then used to  generate the SFT reasoning examples. But what makes creating this model possible is an earlier experiment creating an earlier model called </span><em>DeepSeek-R1-Zero</em><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png" width="1456" height="483" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:483,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:103072,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>R1-Zero is special because it is able to excel at reasoning tasks without having a labeled SFT training set. Its training goes directly from a pre-trained base model through a RL training process (no SFT step). It does this so well that it’s competitive with o1.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png" width="1456" height="383" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:383,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:106577,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This is significant because data has always been the fuel for ML model capability. How can this model depart from that history? This points to two things:</p><p>1- Modern base models have crossed a certain threshold of quality and capability (this base model was trained on 14.8 trillion high-quality tokens).</p><p>2- Reasoning problems, in contrast to general chat or writing requests, can be automatically verified or labeled. Let’s show this with an example. This can be a prompt/question that is a part of this RL training step:</p><blockquote><p>Write python code that takes a list of numbers, returns them in a sorted order, but also adds 42 at the start.</p></blockquote><p>A question like this lends itself to many ways of automatic verification. Say we present this this to the model being trained, and it generates a completion:</p><ul><li><p>A software linter can check if the completion is proper python code or not</p></li><li><p>We can execute the python code to see if it even runs</p></li><li><p>Other modern coding LLMs can create unit tests to verify the desired behavior (without being reasoning experts themselves). </p></li><li><p>We can go even one step further and measure execution time and make the training process prefer more performant solutions over other solutions — even if they’re correct python programs that solve the issue.</p></li></ul><p>We can present a question like this to the model in a training step, and generate multiple possible solutions.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png" width="798" height="444" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:444,&quot;width&quot;:798,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:60456,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>We can automatically check (with no human intervention) and see that the first completion is not even code. The second one is indeed python code but does not solve the problem. The third is a possible solution, but fails the unit tests, and the forth is a correct solution.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png" width="972" height="517" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f9645a0-b1fb-4753-942c-583504297c25_972x517.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:517,&quot;width&quot;:972,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:74268,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>These are all signals that can be directly use to improve the model. This is of course done over many examples (in mini-batches) and over successive training steps.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png" width="955" height="543" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:543,&quot;width&quot;:955,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:92262,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>These reward signals and model updates are how the model continues improving on tasks over the RL training process as seen in Figure 2 from the paper.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png" width="1456" height="833" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:833,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:211203,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Corresponding with the improvement of this capability is the length of the generated response, where the model generates more thinking tokens to process the problem.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png" width="1456" height="875" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:875,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:250596,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This process is useful, but the R1-Zero model, despite scoring high on these reasoning problems, confronts other issues that make it less usable than desired. </p><blockquote><p>Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing.</p></blockquote><p>R1 is meant to be a more usable model. So instead of relying completely on the RL process, it is used in two places as we’ve mentioned earlier in this section:</p><p>1- creating an interim reasoning model to generate SFT data points</p><p>2- Training the R1 model to improve on reasoning and non-reasoning problems (using other types of verifiers)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png" width="1456" height="629" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:629,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>To make the interim reasoning model more useful, it goes through an supervised fine-tuning (SFT) training step on a few thousand examples of reasoning problems (some of which are generated and filtered from R1-Zero). The paper refers to this as cold start data”</p><blockquote><p><strong>2.3.1. Cold Start</strong><br><span>Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and refining the results through post-processing by human annotators.</span></p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png" width="1456" height="468" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:468,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:160514,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>But wait, if we have this data, then why are we relying on the RL process? It’s because of the scale of the data. This dataset might be 5,000 examples (which is possible to source), but to train R1, 600,000 examples were needed. This interim model bridges that gap and allows to synthetically generate that extremely valuable data.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png" width="1456" height="516" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:516,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:244148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>If you’re new to the concept of Supervised Fine-Tuning (SFT), that is the process that presents the model with training examples in the form of prompt and correct completion. This figure from chapter 12 shows a couple of SFT training examples:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png" width="1456" height="851" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:851,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:575809,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This enables R1 to excel at reasoning as well as other non-reasoning tasks. The process is similar to the the RL process we’ve seen before. But since it extends to non-reasoning applications, it utilizes a helpfulnes and a safety reward model (not unlike the Llama models) for prompts that belong to these applications.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png" width="902" height="394" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:394,&quot;width&quot;:902,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:72511,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Just like previous models from the dawn of </span><a href="https://jalammar.github.io/illustrated-gpt2/" rel="">GPT2</a><span> and </span><a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/" rel="">GPT 3</a><span>, DeepSeek-R1 is a stack of </span><a href="https://jalammar.github.io/illustrated-transformer/" rel="">Transformer</a><span> decoder blocks. It’s made up 61 of them. The first three are dense, but the rest are mixture-of-experts layers (See my co-author Maarten’s incredible intro guide here: </span><a href="https://substack.com/home/post/p-148217245" rel="">A Visual Guide to Mixture of Experts (MoE)</a><span>).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png" width="538" height="413" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:413,&quot;width&quot;:538,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:39245,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In terms of model dimension size and other hyperparameters, they look like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png" width="916" height="481" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:481,&quot;width&quot;:916,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:63869,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>More details about the model architecture are presented in their two earlier papers:</p><ul><li><p><a href="https://arxiv.org/pdf/2412.19437v1" rel="">DeepSeek-V3 Technical Report</a></p></li><li><p><a href="https://arxiv.org/pdf/2401.06066" rel="">DeepSeekMoE: Towards Ultimate Expert Specialization in</a></p><p><a href="https://arxiv.org/pdf/2401.06066" rel="">Mixture-of-Experts Language Models</a></p></li></ul><p>With this, you should now have the main intuitions to wrap your head around the DeepSeek-R1 model. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png" width="1456" height="634" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:634,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:341594,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>If you felt needed a little more foundational information to understand this post, I’d suggest you pick up a copy of </span><a href="https://www.llm-book.com/" rel="">Hands-On Large Language Models</a><span> or read it online on </span><a href="https://learning.oreilly.com/library/view/hands-on-large-language/9781098150952/" rel="">O’Reilly</a><span> and check it out on </span><a href="https://github.com/handsOnLLM/Hands-On-Large-Language-Models" rel="">Github</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png" width="158" height="208.49484536082474" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:582,&quot;resizeWidth&quot;:158,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Book Cover&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Book Cover" title="Book Cover" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Other suggested resources are:</p><ul><li><p><a href="https://www.interconnects.ai/p/deepseek-r1-recipe-for-o1" rel="">DeepSeek R1's recipe to replicate o1 and the future of reasoning LMs</a><span> by </span></p><span> </span></li><li><p><a href="https://substack.com/home/post/p-148217245" rel="">A Visual Guide to Mixture of Experts (MoE)</a><span> by </span></p><span> </span></li><li><p><span>Sasha Rush’s YouTube video </span><a href="https://www.youtube.com/watch?v=6PEJ96k1kiw" rel="">Speculations on Test-Time Scaling (o1)</a><span> </span></p></li><li><p><span>Yannis Kilcher’s </span><a href="https://www.youtube.com/watch?v=bAWV_yrqx4w" rel="">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Paper Explained)</a></p></li><li><p><a href="https://github.com/huggingface/open-r1" rel="">Open R1</a><span> is the HuggingFace project to openly reproduce DeepSeek-R1</span></p></li><li><p><a href="https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo" rel="">Putting RL back in RLHF</a></p></li></ul></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Go 1.24's go tool is one of the best additions to the ecosystem in years (262 pts)]]></title>
            <link>https://www.jvt.me/posts/2025/01/27/go-tools-124/</link>
            <guid>42845323</guid>
            <pubDate>Mon, 27 Jan 2025 20:33:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jvt.me/posts/2025/01/27/go-tools-124/">https://www.jvt.me/posts/2025/01/27/go-tools-124/</a>, See on <a href="https://news.ycombinator.com/item?id=42845323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For those that aren't aware, one of the big changes in February's upcoming Go 1.24 release is the new <a href="https://tip.golang.org/doc/go1.24#tools"><code>go tool</code></a> command, and <code>tool</code> directive in the <code>go.mod</code> to manage any tools your project uses. I'm <em>incredibly excited</em> about this, and in my opinion, this is one of the best changes we've had in recent years in the ecosystem as a whole.</p><p>I've been meaning to write this post since the first release candidate for Go 1.24 landed, but after reading Howard John's <a href="https://blog.howardjohn.info/posts/go-tools-command/">Exploring the new "go tool" support in Go 1.24</a> this morning, I thought I should write my thoughts up.</p><h2 id="what-is-it">What is it?</h2><p>Within your Go codebases, there's often some additional tools that you need to have installed to be able to build/test/deploy the project.</p><p>Sometimes this will a dependency that's needed for <code>go generate</code>ing, or it may be that you want to pipe your <code>go test</code> output into a JUnit-compatible format, so your CI platform can provide more useful metadata.</p><p>For each of these, you have two choices:</p><ul><li>require that the user knows how to install them, i.e. by knowing to run <code>make deps</code> or <code>just setup</code> before building anything on the project (which will then i.e. <code>go install</code> the commands)</li><li>use the <a href="https://www.jvt.me/posts/2022/06/15/go-tools-dependency-management/"><code>tools.go</code> pattern</a> to make it so you can <em>just</em> run <code>go generate</code>, and that'll call the right dependency via <code>go run</code></li></ul><p>My preference is <a href="https://www.jvt.me/posts/2022/06/15/go-tools-dependency-management/"><code>tools.go</code> pattern</a>, but there are two key problems with this approach.</p><p>Firstly, there's a performance hit of using a <code>tools.go</code>. It's something that is <em>slightly</em> noticeable, moreso if your project relies upon a lot of <code>go run</code> i.e. with lots of <code>go generate</code>s, because prior to Go 1.24, the <code>go run</code> invocations were not cached.</p><p>Secondly, it also leads to dependency tree bloat, because you have to record your dependency on i.e. <code>github.com/sqlc-dev/sqlc/cmd/sqlc</code> which then gets recorded in your <code>go.mod</code>, and then anyone using <em>your module</em> will then see that as an indirect (transitive) dependency.</p><p>This was something we <a href="https://www.jvt.me/posts/2023/10/23/oapi-codegen-v2-decrease/">worked on for <code>oapi-codegen</code>'s v2 release</a> to further reduce unnecessary dependencies, and make things a bit cleaner for our consumers. This is somewhat mitigated by Go's <a href="https://go.dev/ref/mod#graph-pruning">module graph pruning</a> which won't download dependencies that aren't used, but consumers may still see the dependencies coming in as an indirect dependency, which may not be ideal (especially as it can then bloat their indirect dependencies, which then gets passed on to their consumers and so on .</p><p>Dependency tree bloat can also be further mitigated by splitting your <a href="https://www.jvt.me/posts/2024/09/30/go-tools-module/"><code>tools.go</code> into a separate module</a>, which makes it more awkward to invoke dependencies but makes sure that none of your consumers will be seeing any tool-related dependencies.</p><p>For those who know me as co-maintainer of <a href="https://github.com/oapi-codegen/oapi-codegen">oapi-codegen</a>, you'll know that the <code>tools.go</code> pattern is our <a href="https://github.com/oapi-codegen/oapi-codegen#install">explicit recommendation</a> and we believe is better than installing it as a binary, so it's probably unsurprising that I'm very excited about this as an option to manage dependencies.</p><h2 id="how-does-it-work">How does it work?</h2><p>I've started playing around with this <a href="https://gitlab.com/tanna.dev/dependency-management-data/-/commits/spike/go-tools-124">on a branch</a> of the <a href="https://dmd.tanna.dev/">dependency-management-data</a> project, where I've got a mix of different tools that need to be installed and used.</p><p>Let's take a worked example of how we'd move over calls to <code>oapi-codegen</code> to the new <code>go tool</code> pattern.</p><h3 id="existing-state">Existing state</h3><p>For instance let's say that we have the following <code>tools.go</code> in its own module:</p><pre tabindex="0"><code data-lang="gomod"># tools/go.mod
module dmd.tanna.dev/tools

go 1.22.0

require (
	github.com/99designs/gqlgen v0.17.49
	github.com/oapi-codegen/oapi-codegen/v2 v2.4.1
	github.com/sqlc-dev/sqlc v1.26.0
)
</code></pre><p>We can then see that we invoke this via <code>go run</code>:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>// internal/ecosystems/generate.go
</span></span></span><span><span><span>//go:generate go run -modfile=../../tools/go.mod github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen --config=config.yaml openapi.yaml
</span></span></span></code></pre></div><h3 id="migrating">Migrating</h3><p>To start migrating over to <code>go tool</code>, we need to make sure that we've first pulled in the new version of Go in our top-level Go module:</p><div><pre tabindex="0"><code data-lang="diff"><span><span> module dmd.tanna.dev
</span></span><span><span>
</span></span><span><span><span>-go 1.22.7
</span></span></span><span><span><span></span><span>+go 1.24
</span></span></span><span><span><span></span>
</span></span><span><span><span>-toolchain go1.23.2
</span></span></span><span><span><span></span><span>+toolchain go1.24rc2
</span></span></span></code></pre></div><p>Next, we need to pull in a <code>tool</code> dependency on <code>oapi-codegen</code>'s CLI tool - notice that you need <strong>the full path</strong> to the command that's being invoked:</p><div><pre tabindex="0"><code data-lang="sh"><span><span><span># NOTE the full import path</span>
</span></span><span><span>% go get -tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen@v2.4.1
</span></span></code></pre></div><p>We could also do this by hand, but doing it via <code>go get</code> simplifies this a little.</p><p>From here, we'll notice that our <code>go.mod</code> has a few other changes:</p><div><pre tabindex="0"><code data-lang="diff"><span><span><span>@@ -57,12 +57,16 @@ require (
</span></span></span><span><span><span></span>        github.com/cenkalti/backoff/v4 v4.3.0 // indirect
</span></span><span><span>        github.com/cespare/xxhash/v2 v2.3.0 // indirect
</span></span><span><span>        github.com/charmbracelet/lipgloss v0.10.0 // indirect
</span></span><span><span><span>+       github.com/dprotaso/go-yit v0.0.0-20220510233725-9ba8df137936 // indirect
</span></span></span><span><span><span></span>        github.com/dustin/go-humanize v1.0.1 // indirect
</span></span><span><span>        github.com/felixge/httpsnoop v1.0.4 // indirect
</span></span><span><span><span>+       github.com/getkin/kin-openapi v0.127.0 // indirect
</span></span></span><span><span><span></span>        github.com/go-ini/ini v1.67.0 // indirect
</span></span><span><span>        github.com/go-logfmt/logfmt v0.6.0 // indirect
</span></span><span><span>        github.com/go-logr/logr v1.4.2 // indirect
</span></span><span><span>        github.com/go-logr/stdr v1.2.2 // indirect
</span></span><span><span><span>+       github.com/go-openapi/jsonpointer v0.21.0 // indirect
</span></span></span><span><span><span>+       github.com/go-openapi/swag v0.23.0 // indirect
</span></span></span><span><span><span></span>        github.com/gobwas/glob v0.2.3 // indirect
</span></span><span><span>        github.com/google/go-querystring v1.1.0 // indirect
</span></span><span><span>        github.com/gorilla/mux v1.8.1 // indirect
</span></span><span><span><span>@@ -72,16 +76,22 @@ require (
</span></span></span><span><span><span></span>        github.com/hashicorp/go-retryablehttp v0.7.5 // indirect
</span></span><span><span>        github.com/hashicorp/golang-lru/v2 v2.0.7 // indirect
</span></span><span><span>        github.com/inconshreveable/mousetrap v1.1.0 // indirect
</span></span><span><span><span>+       github.com/invopop/yaml v0.3.1 // indirect
</span></span></span><span><span><span>+       github.com/josharian/intern v1.0.0 // indirect
</span></span></span><span><span><span></span>        github.com/klauspost/compress v1.17.11 // indirect
</span></span><span><span>        github.com/lucasb-eyer/go-colorful v1.2.0 // indirect
</span></span><span><span><span>+       github.com/mailru/easyjson v0.7.7 // indirect
</span></span></span><span><span><span></span>        github.com/mattn/go-isatty v0.0.20 // indirect
</span></span><span><span>        github.com/mattn/go-runewidth v0.0.15 // indirect
</span></span><span><span>        github.com/mitchellh/mapstructure v1.5.0 // indirect
</span></span><span><span><span>+       github.com/mohae/deepcopy v0.0.0-20170929034955-c48cc78d4826 // indirect
</span></span></span><span><span><span></span>        github.com/muesli/reflow v0.3.0 // indirect
</span></span><span><span>        github.com/muesli/termenv v0.15.2 // indirect
</span></span><span><span>        github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
</span></span><span><span>        github.com/ncruces/go-strftime v0.1.9 // indirect
</span></span><span><span><span>+       github.com/oapi-codegen/oapi-codegen/v2 v2.4.1 // indirect
</span></span></span><span><span><span></span>        github.com/olekukonko/tablewriter v0.0.5 // indirect
</span></span><span><span><span>+       github.com/perimeterx/marshmallow v1.1.5 // indirect
</span></span></span><span><span><span></span>        github.com/prometheus/client_golang v1.20.5 // indirect
</span></span><span><span>        github.com/prometheus/client_model v0.6.1 // indirect
</span></span><span><span>        github.com/prometheus/common v0.60.1 // indirect
</span></span><span><span><span>@@ -91,8 +101,10 @@ require (
</span></span></span><span><span><span></span>        github.com/rivo/uniseg v0.4.7 // indirect
</span></span><span><span>        github.com/sirupsen/logrus v1.9.4-0.20230606125235-dd1b4c2e81af // indirect
</span></span><span><span>        github.com/sosodev/duration v1.3.1 // indirect
</span></span><span><span><span>+       github.com/speakeasy-api/openapi-overlay v0.9.0 // indirect
</span></span></span><span><span><span></span>        github.com/spf13/pflag v1.0.5 // indirect
</span></span><span><span>        github.com/tchap/go-patricia/v2 v2.3.1 // indirect
</span></span><span><span><span>+       github.com/vmware-labs/yaml-jsonpath v0.3.2 // indirect
</span></span></span><span><span><span></span>        github.com/xeipuuv/gojsonpointer v0.0.0-20190905194746-02993c407bfb // indirect
</span></span><span><span>        github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415 // indirect
</span></span><span><span>        github.com/yashtewari/glob-intersection v0.2.0 // indirect
</span></span><span><span><span>@@ -110,11 +122,13 @@ require (
</span></span></span><span><span><span></span>        go.opentelemetry.io/otel/metric v1.32.0 // indirect
</span></span><span><span>        go.opentelemetry.io/proto/otlp v1.3.1 // indirect
</span></span><span><span>        golang.org/x/exp v0.0.0-20231108232855-2478ac86f678 // indirect
</span></span><span><span><span>+       golang.org/x/mod v0.18.0 // indirect
</span></span></span><span><span><span></span>        golang.org/x/net v0.30.0 // indirect
</span></span><span><span>        golang.org/x/oauth2 v0.23.0 // indirect
</span></span><span><span>        golang.org/x/sys v0.27.0 // indirect
</span></span><span><span>        golang.org/x/term v0.25.0 // indirect
</span></span><span><span>        golang.org/x/time v0.5.0 // indirect
</span></span><span><span><span>+       golang.org/x/tools v0.22.0 // indirect
</span></span></span><span><span><span></span>        google.golang.org/genproto/googleapis/api v0.0.0-20241104194629-dd2ea8efbc28 // indirect
</span></span><span><span>        google.golang.org/genproto/googleapis/rpc v0.0.0-20241104194629-dd2ea8efbc28 // indirect
</span></span><span><span>        google.golang.org/grpc v1.68.0 // indirect
</span></span><span><span><span>@@ -128,3 +142,5 @@ require (
</span></span></span><span><span><span></span>        modernc.org/token v1.1.0 // indirect
</span></span><span><span>        sigs.k8s.io/yaml v1.4.0 // indirect
</span></span><span><span> )
</span></span><span><span><span>+
</span></span></span><span><span><span>+tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen
</span></span></span></code></pre></div><p>From here, we can see:</p><ul><li>there is a <code>tool</code> directive for <code>github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen</code></li><li>the containing Go module for the CLI, <code>github.com/oapi-codegen/oapi-codegen/v2</code>, is now an <code>indirect</code> dependency</li><li>any other required dependencies of <code>github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen</code> are now <code>indirect</code> dependencies</li></ul><p>Now we've done this, we could run:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>% go tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen --help
</span></span><span><span>Usage of /home/jamie/.cache/go-build/0e/0e04736601c8bbef785d372de02859bf8f39405aae9ccbf371477b0f2d8df755-d/oapi-codegen:
</span></span><span><span><span># ...</span>
</span></span></code></pre></div><p>With this tool set up, we can now modify i.e. <code>internal/ecosystems/generate.go</code> like so to use the new <code>go tool</code>:</p><div><pre tabindex="0"><code data-lang="diff"><span><span> package ecosystems
</span></span><span><span>
</span></span><span><span><span>-//go:generate go run -modfile=../../tools/go.mod github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen --config=config.yaml openapi.yaml
</span></span></span><span><span><span></span><span>+//go:generate go tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen --config=config.yaml openapi.yaml
</span></span></span></code></pre></div><p>Then running <code>go generate ./internal/ecosystems</code> works as it did before 🚀</p><h2 id="performance-implications">Performance implications</h2><p>A less scientific view than Howard John's article above, but we can see a slight improvement in performance:</p><div><pre tabindex="0"><code data-lang="sh"><span><span><span># first time using `go tool`, from a fresh cache directory</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  55.05s user 4.57s system 531% cpu 11.220 total
</span></span><span><span><span># a subsequent call</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  0.59s user 0.18s system 424% cpu 0.181 total
</span></span><span><span><span># another just to see</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  0.57s user 0.25s system 404% cpu 0.202 total
</span></span></code></pre></div><p>Compare this to the previous implementation:</p><div><pre tabindex="0"><code data-lang="sh"><span><span><span># first time using `go run`, from a fresh cache directory</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  50.29s user 3.67s system 536% cpu 10.063 total
</span></span><span><span><span># a subsequent call</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  1.04s user 0.21s system 185% cpu 0.677 total
</span></span><span><span><span># another just to see</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  1.02s user 0.26s system 191% cpu 0.669 total
</span></span></code></pre></div><p>Notice that the first call is similar in speed, but the use of <code>go tool</code>'s subsequent calls are still faster.</p><p>I'm a big fan of the fact that as of Go 1.24+ the <code>go run</code>s will be cached, so even if you don't move over to <code>go tool</code>, you'll get a performance boost!</p><h2 id="concerns">Concerns</h2><p>Now, there are still a few things I've noticed while doing the migration that aren't necessarily what I expected.</p><h3 id="gomod-implications"><code>go.mod</code> implications</h3><p>Something interesting is that the usage of the <code>tool</code> dependencies being treated as an <code>indirect</code> dependency is that they're present in the dependency tree, and treated like any other <code>indirect</code> dependency.</p><p>I'd also have preferred that we had just used <code>// tool</code> instead of <code>// indirect</code>, but I can see why this is likely the choice that's made - so they're treated like any other dependency - but making them less clear as only being required for tools could lead to issues with clashing dependencies, or where you upgrade an <code>indirect</code> dependency and then that breaks other things.</p><p>This means that tools such as Renovate need to be a little more involved in how to do the updates, but <a href="https://github.com/renovatebot/renovate/discussions/33867">that's all in hand</a>.</p><h2 id="gqlgen-fails-to-run-with-go-124rc2"><code>gqlgen</code> fails to run with Go 1.24rc2</h2><p>Something I've noticed while playing around with this is that <a href="https://github.com/99designs/gqlgen/issues/3505"><code>gqlgen</code> struggles to run with Go 1.24rc2</a>, which <a href="https://github.com/golang/go/issues/71448">feels like an upstream Go issue</a>, but it looks like that may be related to the use of <code>/x/tools</code> 🤔</p><p>It may be interesting to find out what else gets affected by this - please give the RC a test!</p><h2 id="closing">Closing</h2><p>Overall, I'm feeling very positive about it, and improving the way that dependencies get installed <em>if they should be built from source</em>, but there are dependencies such as <code>golangci-lint</code> which <a href="https://golangci-lint.run/welcome/install/#install-from-sources">don't recommend building from source</a> and instead using their pre-built binaries, which is fair, and is unlikely to change here.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We're bringing Pebble back (2399 pts)]]></title>
            <link>https://repebble.com/</link>
            <guid>42845091</guid>
            <pubDate>Mon, 27 Jan 2025 20:11:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://repebble.com/">https://repebble.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42845091">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="text-container">
            
            
            
            <div>
                <div>
                    <h2>We're making new Pebble watches</h2>
                    <p>I've tried pretty much every other smartwatch on Earth, yet I still wear my Pebble every day—nothing else matches its features and long battery life. I really, <em>really</em>, <strong><em>really</em></strong> hoped someone else would create a proper replacement, but no one has stepped up, and my stash of old Pebbles is dwindling!</p>
                    <p>It's time to take matters into my own hands. A small team and I are working on a new Pebble-like smartwatch that runs open source PebbleOS, has the same beloved features (plus some fun new stuff), and stays true to the core Pebble vision. If enough people are interested, we'll build it. <a href="https://repebble.com/signup.html">Sign up</a> to get one!</p>
                    <a href="https://ericmigi.com/blog/why-were-bringing-pebble-back" target="_blank">
                        <h3>Read the full blog post</h3>
                        <p>Why We're Bringing Pebble Back</p>
                    </a>
                </div>

                <div>
                    <h2>PebbleOS is now open source</h2>
                    <p>Google (which purchased Fitbit, which had bought Pebble) still owns PebbleOS. Over the last year, a team inside Google (including some amazing ex-Pebblers turned Googlers) has been working on open sourcing the OS! The source code for PebbleOS is now available at <a href="https://github.com/google/pebble" target="_blank">github.com/google/pebble</a>. Read more on their <a href="https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html" target="_blank">blog</a>.</p>
                    <p>Thank you so much, Google! I can't stress how thankful I am to the individuals who did the heavy lifting. This was also made possible by the <a href="https://rebble.io/" target="_blank">Rebble</a> team and community, who have supported Pebble since it shut down. Check out the vibrant <a href="https://reddit.com/r/pebble" target="_blank">r/Pebble</a> and <a href="https://discordapp.com/invite/aRUAYFN" target="_blank">Discord</a>.</p>
                </div>
                
                <div>
                    <h2>What happens now?</h2>
                    <p>The source code that powers each Pebble smartwatch is now freely available to download, modify and improve on <a href="https://github.com/google/pebble" target="_blank">Github</a>. Want a reminder of how awesome Pebble OS is? Dive back into the <a href="https://ericmigi.com/blog/pebbleos-is-awesome" target="_blank">beautiful, retro, pixelated world</a> of Pebble.</p>
                    <p>Anyone can use PebbleOS in any way they want. You can get it working on existing Pebble watches, emulate it, run it on other embedded devices, or create new hardware specifically for it. <a href="https://github.com/pebble-dev/pebbleos" target="_blank">Learn more</a> about PebbleOS.</p>
                    <p>We're setting out to bring Pebble back, we'd love for you to join the fun!</p>
                </div>
            </div>

            <h2 id="do-you-want-one">Do you want a new Pebble?</h2>
            
            <div>
                <p>Eric Migicovsky<br>Founder of Pebble</p>
                <div>
                    <h3>Wait, what is Pebble again?</h3>
                    <p>Pebble is an e-paper smartwatch with simple functionality, long battery life, and fun, quirky design. It first launched on <a href="https://www.kickstarter.com/projects/getpebble/pebble-e-paper-watch-for-iphone-and-android" target="_blank">Kickstarter</a> in 2012 and sold over 2 million watches before the company's IP was sold to Fitbit in 2016.</p>
                </div>
                <p id="smallText">© Copyright <span id="year">----</span> Core Devices LLC. All Rights Reserved.<br><a href="https://repebble.com/privacy.html">Privacy</a> · <a href="https://repebble.com/terms.html">Terms</a> · <a href="https://twitter.com/pebble" target="_blank">Twitter</a></p>
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google open-sources the Pebble OS (1201 pts)]]></title>
            <link>https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html</link>
            <guid>42845070</guid>
            <pubDate>Mon, 27 Jan 2025 20:09:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html">https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html</a>, See on <a href="https://news.ycombinator.com/item?id=42845070">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-5561126018697023258" itemprop="articleBody">
<meta name="twitter:image" content="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHCX-mb_DqHgkNn1By45jRl-t4yGY82D79aFivyvhLIjiW9oglYr2fu7qOXFTEPj4sg-18anq6Aydli437ogx_AfTNI4V8Kq9Wjm1pPpOpqsSG1aiTwNLURTHgzFTeND8VuCxmndTLxT48Hr5RQgWvilKyeI9ORfoRNE40ZyqV49xuxTNarCAIoErsYbw/s1600/Pebble-Smartwatch%20%281%29.png">
<p>

<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHCX-mb_DqHgkNn1By45jRl-t4yGY82D79aFivyvhLIjiW9oglYr2fu7qOXFTEPj4sg-18anq6Aydli437ogx_AfTNI4V8Kq9Wjm1pPpOpqsSG1aiTwNLURTHgzFTeND8VuCxmndTLxT48Hr5RQgWvilKyeI9ORfoRNE40ZyqV49xuxTNarCAIoErsYbw/s1600/Pebble-Smartwatch%20%281%29.png" imageanchor="1"><img data-original-height="800" data-original-width="100%" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHCX-mb_DqHgkNn1By45jRl-t4yGY82D79aFivyvhLIjiW9oglYr2fu7qOXFTEPj4sg-18anq6Aydli437ogx_AfTNI4V8Kq9Wjm1pPpOpqsSG1aiTwNLURTHgzFTeND8VuCxmndTLxT48Hr5RQgWvilKyeI9ORfoRNE40ZyqV49xuxTNarCAIoErsYbw/s1600/Pebble-Smartwatch%20%281%29.png"></a></p><p>We are excited to announce that the source code that powered Pebble smartwatches is now <a href="https://github.com/google/pebble" target="_blank">available for download</a>.</p>

<p>This is part of an effort from Google to help and support the <a href="https://rebble.io/" target="_blank">volunteers</a> who have come together to maintain functionality for Pebble watches after the original company ceased operations in 2016.</p><br>

<h3><b>A quick look back</b></h3>

<p>Pebble was initially launched through a very successful <a href="https://www.kickstarter.com/projects/getpebble/pebble-e-paper-watch-for-iphone-and-android" target="_blank">Kickstarter project</a>. Pebble’s first Kickstarter was the single most funded at the time, and its successor Kickstarter for the <a href="https://www.kickstarter.com/projects/getpebble/pebble-time-awesome-smartwatch-no-compromises" target="_blank">Pebble Time</a> repeated that feat – and remains the second most funded today! Over the course of four years, Pebble sold over two million smartwatches, cultivating a thriving community of thousands of developers who created over ten thousand Pebble apps and watchfaces.</p>

<p>In 2016, Fitbit acquired Pebble, including Pebble’s intellectual property. Later on, Fitbit itself was acquired by Google, taking the Pebble OS with it.</p>

<p>Despite the Pebble hardware and software support being discontinued eight years ago, Pebble still has thousands of dedicated fans.</p><br>

<h3><b>What is being released</b></h3>

<p>We are releasing most of the source code for the Pebble operating system. This repository contains the entire OS, which provides all the standard smartwatch functionality – notifications, media controls, fitness tracking, and support for custom apps and watchfaces – on tiny ARM Cortex-M microcontrollers. Built with <a href="https://www.freertos.org/" target="_blank">FreeRTOS</a>, it contains multiple modules for memory management, graphics, and timekeeping, as well as an extensive framework to load and run custom applications written in C, as well as in Javascript via the <a href="https://jerryscript.net/" target="_blank">Jerryscript</a> Javascript engine. The Pebble architecture allowed for a lightweight system delivering a rich user experience as well as a very long battery life.</p>

<p>It's important to note that some proprietary code was removed from this codebase, particularly for chipset support and the Bluetooth stack. This means the code being released contains all the build system files (using the <a href="https://waf.io/" target="_blank">waf</a> build system), but it will not compile or link as released.</p><br>

<h3><b>The path forward</b></h3>

<p>From here, we are hoping this release will assist the dedicated community and volunteers from the <a href="https://rebble.io/" target="_blank">Rebble project</a> to carry forward the support for Pebble watches that users still love. For someone to build a new firmware update, there is a non-trivial amount of work to do in finding replacements for the pieces that were stripped out of this code, as well as updating this source code that has not been maintained for a few years.</p>

<p><i>By Matthieu Jeanson, Katharine Berry, and Liam McLoughlin</i></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google has open-sourced the Pebble smartwatch operating system (374 pts)]]></title>
            <link>https://rebble.io/2025/01/27/the-future-of-rebble.html</link>
            <guid>42845017</guid>
            <pubDate>Mon, 27 Jan 2025 20:03:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rebble.io/2025/01/27/the-future-of-rebble.html">https://rebble.io/2025/01/27/the-future-of-rebble.html</a>, See on <a href="https://news.ycombinator.com/item?id=42845017">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <div>
	    <p>Jan 27, 2025 • by <a href="https://rebble.io/team#person-Will%20Murphy">Will Murphy</a></p>
        
        <p>Today we’re excited to announce several developments which will affect the future of Rebble. Let’s get straight into it, starting with the big one…</p>

<h2 id="-google-open-sources-tintin">🎉 Google Open Sources Tintin</h2>

<p><img src="https://rebble.io/images/tintin-blog-post/the-loop.png" alt=""></p>

<p>Today Google <a href="https://github.com/google/pebble">announced that they have released the source code to PebbleOS</a>. This is massive for Rebble, 
and will accelerate our efforts to produce new hardware.</p>

<p>Previously, we have been working on our own replacement firmware: <a href="https://github.com/pebble-dev/RebbleOS">RebbleOS</a>. As you can see by the commit history though, progress was slow.
Building a production-ready realtime OS for the Pebble is no small feat, and although we were confident we’d get there given enough time, it was never our ideal path.
Thanks to the hard work of many people both within Google and not, we finally have our hands on the original source code for PebbleOS. You can read <a href="https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html">Google’s blog post on this for even more information.</a></p>

<p>This does <em>not</em> mean we instantly have the ability to start developing updates for PebbleOS though, we first will need to spend some concentrated time getting it to build. 
But before we talk about that, let’s talk about Rebble itself.</p>

<!--more-->



<p>With a long term plan for the Rebble community starting to coalesce, the
longevity of Rebble is more important than ever.  We’re excited to say that
Rebble is transforming into a non-profit to formalize what we’ve all always
hoped for: the community (that’s you!) are the owners of Rebble!  Rebble has
always been about preserving these humble little smartwatches as a little
oasis of user-respectful technology in a desert of big corporations trying
to sell your attention, and we’re excited to have a legal framework that
lets us codify our missions of: educating people about why these are
important; using them as a platform to teach embedded systems; preserving
the history of this quirky little platform; and building open source
software for the public good to keep the dream going long into the future.</p>

<p>It’s still early days, but more information will be available at <a href="https://rebble.foundation/">rebble.foundation</a> as soon<img title=":tm:" alt=":tm:" src="https://github.githubassets.com/images/icons/emoji/unicode/2122.png" height="20" width="20"> as we have it. 
In the mean time, expect more hackathons from us, now that we have a
framework to run them!  Oh, and speaking of which…</p>

<h2 id="-the-rebbleos-hackathon">💻 The RebbleOS Hackathon</h2>

<p><img src="https://rebble.io/images/tintin-blog-post/hackathon-002.gif" alt=""></p>

<p>The <a href="https://rebble.io/2023/05/12/a-look-back-at-the-rebble-hackathon.html">last Rebble hackathon</a> was so much fun, and we’ve been wanting to do another for some time. 
The Rebble project is a fantastic example of what community can achieve, and we intend to build on this in 2025 and beyond.</p>

<p>Writing Pebble apps is a fantastic way to delve into the world of embedded systems, and what better way to do that than with a hackathon?</p>

<p>Mark your calendars for the <strong>1st - 8th of March</strong> as we work on RebbleOS and other apps, and encourage you to do the same!</p>

<p>For more information see <a href="https://rebble.io/hackathon-002">/hackathon-002</a></p>

<h2 id="-old-dog-new-tricks">🐶 Old Dog, New Tricks</h2>

<p><img src="https://rebble.io/images/tintin-blog-post/snowy.png" alt=""></p>

<p>We’re also happy to announce that we’ve acquired the <a href="https://github.com/pebble-dev/snowy">source code for Snowy</a>! 
<a href="https://apps.rebble.io/en_US/application/561960c8a1dd2652af00000d">Snowy</a> was one of the most popular assistants for the Pebble, and is still a useful companion today.
However, given the current landscape of LLMs and voice assistants it is definitely due an upgrade, so expect to see this old dog appear in the hackathon.</p>

<h2 id="️-thats-all-for-now">🗒️ That’s all for now</h2>

<p>Between everything above, and the fact that progress continues on our replacement mobile app, the future of Rebble has never looked so bright. We are committed to an open-source community-owned smartwatch, and these announcements bring that reality even closer.
A huge thank you to everyone in the Pebble-verse who made this happen, especially those internal to Google who have helped ensure PebbleOS’s future. We’d like to especially thank Liam McLoughlin and Matthieu Jeanson, as well as Rebble superstar Katharine Berry. Thank you also to the many other Googlers who made this possible – and a massive shout out to Eric Migicovsky for ensuring this happened (and for creating Pebble in the first place).</p>

<p>One more shoutout: we would like to thank, of course, you!  Without all of you Rebblers who have been entrusting us for the past 8 years to keep the dream of an open-platform user-respectful smartwatch alive, PebbleOS wouldn’t be relevant at all today.  Your cumulative $3s a month have reminded the world that Pebble is worth preserving, and worth building on.  We love this platform, and we’re glad that you do too.  Thank you so much.</p>

<p>Stay tuned for more updates as the Hackathon launches, and when we have the first working versions of the new RebbleOS!</p>

<p>- Will ❤️</p>

<h3 id="clarifications">Clarifications:</h3>

<h4 id="how-can-i-get-involved-with-the-hackathon">How can I get involved with the hackathon?</h4>
<p>See <a href="https://rebble.io/hackathon-002/">here.</a></p>

<h4 id="did-google-gift-pebbleos-to-rebble-specifically">Did Google gift PebbleOS to Rebble specifically?</h4>
<p>No, Google have open sourced the PebbleOS to everyone, Rebble plans to make good use of this.</p>


<p>No. If you’re reading about another PebbleOS project somewhere other than this blog, it does not involve us.</p>

<h4 id="what-if-i-have-more-questions">What if I have more questions?</h4>
<p>Reach out to us at support@rebble.io, or drop a message <a href="https://rebble.io/discord">on Discord.</a></p>

    </div>
</article></div>]]></description>
        </item>
    </channel>
</rss>