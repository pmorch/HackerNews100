<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 30 May 2025 16:30:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[MinIO Removes Web UI Features from Community Version, Pushes Users to Paid Plans (122 pts)]]></title>
            <link>https://biggo.com/news/202505261334_MinIO_Removes_Web_UI_Features</link>
            <guid>44136108</guid>
            <pubDate>Fri, 30 May 2025 13:37:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://biggo.com/news/202505261334_MinIO_Removes_Web_UI_Features">https://biggo.com/news/202505261334_MinIO_Removes_Web_UI_Features</a>, See on <a href="https://news.ycombinator.com/item?id=44136108">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>MinIO, a popular open-source object storage solution, has made significant changes to its community version that have sparked controversy among users. The company has removed key web-based management features from the free version, directing users to either use command-line tools or upgrade to paid plans.</p>
<h3>Major Features Stripped from Community Version</h3>
<p>The latest changelog reveals that MinIO has deprecated several core management features in the web interface. Account and policy management, configuration settings, and other administrative functions are no longer available through the browser-based console. Instead, users must rely on the <code>mc</code> command-line client to perform these tasks.</p>
<p>This change affects how users interact with their MinIO deployments. Previously, administrators could manage their storage systems through an intuitive web interface. Now, they must learn command-line syntax or pay for the commercial version to regain web-based management capabilities.</p>
<p><strong>Deprecated Features in MinIO v2.0.0:</strong></p>
<ul>
<li>Account and policy management (web UI)</li>
<li>Configuration management (web UI)</li>
<li>Bucket management tools (web UI)</li>
<li>Administrative console features</li>
</ul>
<p><strong>Alternative Solutions:</strong></p>
<ul>
<li><strong>SeaweedFS</strong> - Apache 2.0 license</li>
<li><strong>Garage</strong> - AGPL license</li>
<li><strong>Zenko</strong> - Apache 2.0 license</li>
<li><strong>OpenMaxIO</strong> - Community fork of pre-change MinIO</li>
</ul>
<h3>Community Response and Concerns</h3>
<p>The decision has drawn comparisons to Redis's recent licensing changes, with users expressing frustration about the removal of functionality they previously relied on. Many see this as a classic example of enshittification - the gradual degradation of services to drive revenue.</p>
<blockquote>
<p>I think that Deprecated support has another meaning. I hate when this kind of things happens.</p>
</blockquote>
<p>Some community members are already exploring alternatives. A fork called OpenMaxIO has emerged, preserving the last version before these changes were implemented. However, its long-term viability remains uncertain.</p>
<h3>Technical Impact and Alternatives</h3>
<p>While the core storage functionality remains intact, the user experience has significantly changed. Organizations that depend on web-based management may need to retrain staff or consider migration to other solutions.</p>
<p>Several alternatives are gaining attention, including SeaweedFS, Garage, and Zenko. These projects offer S3-compatible storage with varying licensing models and feature sets. Users are actively discussing these options as potential replacements for MinIO in self-hosted environments.</p>
<h3>Looking Forward</h3>
<p>MinIO's strategy appears focused on monetizing enterprise features while maintaining the core storage engine as open source. The company argues this approach helps sustain development while serving both community and commercial users.</p>
<p>However, the timing and execution of these changes have created uncertainty in the community. Users must now decide whether to adapt to command-line management, pay for commercial licenses, or migrate to alternative solutions that better align with their needs and expectations.</p>
<p>Reference: <a href="https://github.com/minio/object-browser/blob/master/CHANGELOG.md">Changelog</a></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Systems Correctness Practices at Amazon Web Services (174 pts)]]></title>
            <link>https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/</link>
            <guid>44135638</guid>
            <pubDate>Fri, 30 May 2025 12:43:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/">https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/</a>, See on <a href="https://news.ycombinator.com/item?id=44135638">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en"><section id="sec1"><p id="p-1">Amazon Web Services (AWS) strives to deliver reliable services that customers can trust completely. This requires maintaining the highest standards of security, durability, integrity, and availability—with systems correctness serving as the cornerstone for achieving these priorities. An April 2015 article published in <i>Communications of the ACM</i>, titled “How Amazon Web Services Uses Formal Methods,” highlighted the approach for ensuring the correctness of critical services that have since become among the most widely used by AWS customers.<a href="#B21" data-jats-ref-type="bibr" data-jats-rid="B21"><sup>21</sup></a></p><p id="p-2">Central to this approach was TLA+,<a href="#B14" data-jats-ref-type="bibr" data-jats-rid="B14"><sup>14</sup></a> a formal specification language developed by Leslie Lamport. Our experience at AWS with TLA+ revealed two significant advantages of applying formal methods in practice. First, we could identify and eliminate subtle bugs early in development—bugs that would have eluded traditional approaches such as testing. Second, we gained the deep understanding and confidence needed to implement aggressive performance optimizations while maintaining systems correctness.</p><p id="p-3">Moreover, 15 years ago, AWS’s software testing practice relied primarily on build-time unit testing, often against mocks, and limited deployment-time integration testing. Since then, we have significantly evolved our correctness practices, integrating both formal and semi-formal approaches into the development process. As AWS has grown, formal methods have become increasingly valuable—not only for ensuring correctness but also for performance improvements, particularly in verifying the correctness of both low- and high-level optimizations. This systematic approach toward systems correctness has become a force multiplier at AWS’s scale, enabling faster development cycles through improved developer velocity while delivering more cost-effective services to customers.</p><p id="p-4">This article surveys the portfolio of formal methods used across AWS to deliver complex services with high confidence in its correctness. We consider an umbrella definition of formal methods that encompasses these rigorous techniques—from traditional formal approaches (such as theorem proving,<a href="#B7" data-jats-ref-type="bibr" data-jats-rid="B7"><sup>7</sup></a><sup>,</sup><a href="#B10" data-jats-ref-type="bibr" data-jats-rid="B10"><sup>10</sup></a> deductive verification,<a href="#B18" data-jats-ref-type="bibr" data-jats-rid="B18"><sup>18</sup></a> and model checking<a href="#B8" data-jats-ref-type="bibr" data-jats-rid="B8"><sup>8</sup></a><sup>,</sup><a href="#B14" data-jats-ref-type="bibr" data-jats-rid="B14"><sup>14</sup></a>) to more lightweight semi-formal approaches (such as property-based testing,<a href="#B6" data-jats-ref-type="bibr" data-jats-rid="B6"><sup>6</sup></a><sup>,</sup><a href="#B19" data-jats-ref-type="bibr" data-jats-rid="B19"><sup>19</sup></a> fuzzing,<a href="#B9" data-jats-ref-type="bibr" data-jats-rid="B9"><sup>9</sup></a> and runtime monitoring<a href="#B11" data-jats-ref-type="bibr" data-jats-rid="B11"><sup>11</sup></a>).</p></section><section id="sec2"><h2>The P Programming Language</h2><p id="p-5">As the use of formal methods was expanded beyond the initial teams at AWS in the early 2010s, we discovered that many engineers struggled to learn and become productive with TLA+. This difficulty seemed to stem from TLA+’s defining feature: It is a high-level, abstract language that more closely resembles mathematics than the imperative programming languages most developers are familiar with. While this mathematical nature is a significant strength of TLA+, and we continue to agree with Lamport’s views on the benefits of mathematical thinking,<a href="#B15" data-jats-ref-type="bibr" data-jats-rid="B15"><sup>15</sup></a> we also sought a language that would allow us to model check (and later prove) key aspects of systems design while being more approachable to programmers.</p><p id="p-6">We found this balance in the P programming language.<a href="#B8" data-jats-ref-type="bibr" data-jats-rid="B8"><sup>8</sup></a> P is a state-machine-based language for modeling and analysis of distributed systems. Using P, developers model their system designs as communicating state machines, a mental model familiar to Amazon’s developer population—most of whom develop systems based on microservices and service-oriented architectures (SOAs). P has been developed at AWS since 2019 and is maintained as a strategic open source project.<a href="#B22" data-jats-ref-type="bibr" data-jats-rid="B22"><sup>22</sup></a> Teams across AWS that build some of its flagship products—from storage (for example, Amazon S3, EBS), to databases (for example, Amazon DynamoDB, MemoryDB, Aurora), to compute (for example, EC2, IoT)—have been using P to reason about the correctness of their system designs.</p><p id="p-7">For example, P was used in migrating Simple Storage Service (S3) from eventual to strong read-after-write consistency.<a href="#B1" data-jats-ref-type="bibr" data-jats-rid="B1"><sup>1</sup></a> A key component of S3 is its index subsystem, an object metadata store that enables fast data lookups. To achieve strong consistency, the S3 team had to make several nontrivial changes to the S3 index protocol stack.<a href="#B25" data-jats-ref-type="bibr" data-jats-rid="B25"><sup>25</sup></a> Because these changes were difficult to get right at S3 scale, and the team wanted to deliver strong consistency with high confidence in correctness, they used P to formally model and validate the protocol design. P helped eliminate several design-level bugs early in the development process and allowed the team to deliver risky optimizations with confidence, as they could be validated using model checking.</p><p id="p-8">In 2023, the P team at AWS built PObserve, which provides a new tool for validating the correctness of distributed systems both during testing and in production. With PObserve, we take structured logs from the execution of distributed systems and validate post hoc that they match behaviors allowed by the formal P specification of the system. This allows for bridging the gap between the P specification of the system design and the production implementation (typically in languages like Rust or Java). While there are significant benefits from verifying protocols at design time, runtime monitoring of the same properties for the implementation makes the investment in formal specification much more valuable and addresses classic concerns with the deployment of formal methods in practice (that is, connecting design-time validation with system implementation).</p></section><section id="sec3"><h2>Lightweight Formal Methods</h2><p id="p-9">Another way that AWS has brought formal methods closer to its engineering teams is through the adoption of <i>lightweight formal methods</i>.</p><section id="sec4"><p data-jats-content-type="inline-heading"><strong>Property-based testing.</strong>&nbsp; The most notable single example of leveraging lightweight formal methods is in Amazon S3’s ShardStore, where the team used property-based testing throughout the development cycle both to test correctness and to speed up development (described in detail by Bornholt, et al.<a href="#B4" data-jats-ref-type="bibr" data-jats-rid="B4"><sup>4</sup></a>). The key idea in their approach was combining property-based testing with developer-provided correctness specifications, coverage-guided fuzzing (an approach where the distribution of inputs is guided by code coverage metrics), failure injection (where hardware and other system failures are simulated during testing), and minimization (where counterexamples are automatically reduced to aid human-guided debugging).</p></section><section id="sec5"><p data-jats-content-type="inline-heading"><strong>Deterministic simulation.</strong>&nbsp; Another lightweight method widely used at AWS is deterministic simulation testing, in which a distributed system is executed on a single-threaded simulator with control over all sources of randomness, such as thread scheduling, timing, and message delivery order. Tests are then written for particular failure or success scenarios, such as the failure of a participant at a particular stage in a distributed protocol. The nondeterminism in the system is controlled by the test framework, allowing developers to specify orderings they believe are interesting (such as ones that have caused bugs in the past). The scheduler in the testing framework can also be extended for fuzzing of orderings or exploring all possible orderings to be tested.</p><p id="p-12">Deterministic simulation testing moves testing of system properties, like behavior under delay and failure, closer to build time instead of integration testing. This accelerates development and provides for more complete behavioral coverage during testing. Some of the work done at AWS on build-time testing of thread ordering and systems failures has been open sourced as part of the shuttle<a href="#FN1" data-jats-rid="FN1" data-jats-ref-type="fn"><sup>a</sup></a> and turmoil<a href="#FN2" data-jats-rid="FN2" data-jats-ref-type="fn"><sup>b</sup></a> projects.</p></section><section id="sec6"><p data-jats-content-type="inline-heading"><strong>Continuous fuzzing or random test-input generation.</strong>&nbsp; Continuous fuzzing, especially coverage-guided scalable test-input generation, is also effective for testing systems correctness at integration time. During the development of Amazon Aurora’s data-sharding feature (Aurora Limitless Database<a href="#B3" data-jats-ref-type="bibr" data-jats-rid="B3"><sup>3</sup></a>), for example, we made extensive use of fuzzing to test two key properties of the system. First, by fuzzing SQL queries (and entire transactions), we validated that the logic partitioning SQL execution over shards is correct. Large volumes of random SQL schemas, datasets, and queries are synthesized and run through the engines under test, and the results compared with an oracle based on the non-sharded version of the engine (as well as other approaches to validation, like those pioneered by SQLancer<a href="#B23" data-jats-ref-type="bibr" data-jats-rid="B23"><sup>23</sup></a>).</p><p id="p-14">Fuzzing, combined with fault-injection testing, is also useful for testing other aspects of database correctness such as atomicity, consistency, and isolation. In database testing, transactions are automatically generated, their correct behavior is defined using a formally specified correctness oracle, and then all possible interleaving of transactions and statements within the transaction is executed against the system under test. We also use post hoc validation of properties such as isolation (following approaches such as Elle<a href="#B13" data-jats-ref-type="bibr" data-jats-rid="B13"><sup>13</sup></a>).</p></section></section><section id="sec7"><h2>Fault Injection as a Service</h2><p id="p-15">In early 2021, AWS launched Fault Injection Service (FIS)<a href="#B2" data-jats-ref-type="bibr" data-jats-rid="B2"><sup>2</sup></a> with the goal of making testing based on fault injection accessible to a wide range of AWS customers. FIS allows customers to inject simulated faults, from API errors to I/O pauses and failed instances, into test or production deployments of their infrastructure on AWS. Injecting faults allows customers to validate that the resiliency mechanisms they have built into their architectures (such as failovers and health checks) actually improve availability and do not introduce correctness problems. Fault-injection testing based on FIS is widely used by AWS customers and internally within Amazon. For example, Amazon.com ran 733 FIS-based fault-injection experiments in preparation for Prime Day 2024.</p><p id="p-16">In 2014, Yuan et al. found that 92% of catastrophic failures in tested distributed systems were triggered by incorrect handling of nonfatal errors. Many distributed-systems practitioners who were told about this research were surprised the percentage was not higher. Happy-case catastrophic failures are rare simply because the happy case of systems is executed often, tested better (both implicitly and explicitly), and is significantly simpler than the error cases. Fault-injection testing and FIS make it much easier for practitioners to test the behavior of their systems under faults and failures, closing the gap between happy-case and error-case bug density.</p><p id="p-17">While fault injection is not considered a formal method, it can be combined with formal specifications. Defining the expected behavior using a formal specification, and then comparing results during and after fault injection to the specified behavior, allows for catching a lot more bugs than simply checking for errors in metrics and logs (or having a human look and say, “Yup, that looks about right”).</p></section><section id="sec8"><h2>Metastability and Emergent System Behavior</h2><p id="p-18">Over the past decade, there has been an emerging interest in a particular class of systems failure: those where some triggering event (like an overload or a cache emptying) causes a distributed system to enter a state where it does not recover without intervention (such as reducing load below normal). This class of failures, dubbed <i>metastable failures</i>,<a href="#B5" data-jats-ref-type="bibr" data-jats-rid="B5"><sup>5</sup></a> is one of the most important contributors to unavailability in cloud systems. The figure, adapted from Bronson et al.,<a href="#B5" data-jats-ref-type="bibr" data-jats-rid="B5"><sup>5</sup></a>&nbsp;illustrates a common type of metastable behavior: Load increases on the system are initially met with increasing goodput, followed by saturation, followed by congestion and goodput dropping to zero (or near zero). From there, the system cannot return to healthy state by slightly reducing load. Instead, it must follow the dotted line and may not recover until load is significantly reduced. This type of behavior is present even in simple systems. For example, it can be triggered in most systems with timeout-and-retry client logic.</p><figure id="UF1"><p><a data-fslightbox="https://cacm.acm.org/wp-content/uploads/2025/04/3729175_fig01.jpg" data-type="image" data-caption="Figure. Metastable system behavior under load." href="https://cacm.acm.org/wp-content/uploads/2025/04/3729175_fig01.jpg">
				<img decoding="async" title="Figure. Metastable system behavior under load." src="https://cacm.acm.org/wp-content/uploads/2025/04/3729175_fig01.jpg" alt="Metastable system behavior under load." data-image-id="UF1" data-image-type="figure">
			</a>
		</p><figcaption><span>Figure.&nbsp;</span> <span>Metastable system behavior under load.</span></figcaption></figure><p id="p-19">Traditional formal approaches to modeling distributed systems typically focus on <i>safety</i> (nothing bad happens) and <i>liveness</i> (something good eventually happens), but metastable failures remind us that systems have a variety of behaviors that cannot be neatly categorized this way. We have increasingly turned to discrete-event simulation to understand the emergent behavior of systems, investing both in custom-built systems simulations and tooling that allow the use of existing system models (built in languages such as TLA+ and P) to simulate system behavior. Extending exhaustive model checkers, like TLA+’s TLC with probabilistic simulations, also allows for the generation of statistical results such as posterior latency distributions, making model checking useful for tasks such as understanding the achievability of latency service-level agreements (SLAs).</p></section><section id="sec9"><h2>Formal Proof</h2><p id="p-20">In some cases, the formal methods enumerated so far in this article are not sufficient. For critical security boundaries such as authorization and virtualization, for example, proofs of correctness can be both desirable and worth the significant investment needed to create them.</p><p id="p-21">In 2023, AWS introduced the Cedar authorization policy language for writing policies that specify fine-grained permissions. Cedar was designed for automated reasoning and formal proof.<a href="#B12" data-jats-ref-type="bibr" data-jats-rid="B12"><sup>12</sup></a><sup>,</sup><a href="#B24" data-jats-ref-type="bibr" data-jats-rid="B24"><sup>24</sup></a> The language was designed to be well-suited for proof, and the implementation was built in the verification-aware programming language Dafny. Using Dafny, the team was able to prove that the implementation satisfies a variety of security properties. This type of proof goes beyond testing. It is a proof in the mathematical sense. The team also applied a differential testing approach using the Dafny code as a correctness oracle to verify the correctness of the production-ready Rust implementation. Publishing the Dafny code and test procedures as open source, along with the Cedar implementation, allows Cedar users to check the team’s work on correctness.</p><p id="p-22">Another example is the Firecracker virtual machine monitor (VMM). Firecracker uses a low-level protocol called <i>virtio</i> to expose emulated hardware devices (such as a network card or solid-state drive) to guest kernels running inside the VM. This emulated device is a critical security boundary because it is the most complex interaction between the untrusted guest and trusted host. The Firecracker team used a tool called Kani<a href="#B20" data-jats-ref-type="bibr" data-jats-rid="B20"><sup>20</sup></a> that can reason formally about Rust code to prove key properties of this security boundary. Again, proof here goes beyond testing and ensures that the critical properties of this boundary are held no matter what the guest attempts to do.</p><p id="p-23">Proofs around the behaviors of programs are an important part of AWS’s software correctness program, so we support development on tools such as Kani, Dafny,<a href="#B18" data-jats-ref-type="bibr" data-jats-rid="B18"><sup>18</sup></a> and Lean,<a href="#B16" data-jats-ref-type="bibr" data-jats-rid="B16"><sup>16</sup></a> and the underlying tools—such as satisfiability modulo theory (SMT) solvers—that power them.</p><p id="p-24">The ability to use formal models and specifications—for model-checking systems at design time, for validating in-production behavior using runtime monitoring by serving as a correctness oracle, for simulating emergent systems behavior, and for building proofs of critical properties—allows AWS to amortize the engineering effort of developing these specifications over a larger amount of business and customer value.</p></section><section id="sec10"><h2>Benefits Beyond Correctness</h2><p id="p-25">Finally, as discussed in the aforementioned 2015 paper, formal methods are a crucial part of safely improving the performance of cloud systems. Modeling a key commit protocol for the Aurora relational database engine in P and TLA+ allowed us to identify an opportunity to reduce the cost of distributed commits from 2 to 1.5 network roundtrips without sacrificing any safety properties. These kinds of stories are usual for teams that adopt formal methods, driven by at least two different dynamics.</p><p id="p-26">First, the act of deeply thinking about and formally writing down distributed protocols forces a structured way of thinking that leads to deeper insights about the structure of protocols and the problem to be solved.&nbsp;Second, having the ability to formally check (and, in some cases, prove) that proposed design optimizations are correct allows naturally conservative distributed-systems engineers to be bolder in their protocol design choices without increasing risk and boosting the developer velocity toward delivering reliable services.</p><p id="p-27">These productivity and cost benefits are limited not only to high-level design optimizations but also to low-level code that normally gets ignored. In one example, the AWS team identified optimizations to the implementation of the Rivest-Shamir-Adleman (RSA) public-key encryption scheme on our ARM-based Graviton 2 processor, which could improve throughput by up to 94%.<a href="#B17" data-jats-ref-type="bibr" data-jats-rid="B17"><sup>17</sup></a></p><p id="p-28">Using the HOL Light interactive theorem prover, the team was able to prove the correctness of these optimizations. Given the high percentage of cloud CPU cycles spent on cryptography, this type of optimization can significantly reduce infrastructure costs and aid sustainability while at the same time improving customer-visible performance.</p></section><section id="sec11"><h2>Challenges and Opportunities for the Future</h2><p id="p-29">Despite significant success in scaling formal and semi-formal testing methods across AWS over the past 15 years, several challenges persist, particularly in industrial adoption of formal methods. The primary barriers for formal methods tools include their steep learning curve and the specialized domain expertise required. Additionally, many of these tools remain academic in nature and lack user-friendly interfaces.</p><p id="p-30">Even well-established semi-formal approaches face adoption challenges. For example, deterministic simulation, a key distributed-systems testing technique used successfully at AWS and in projects like FoundationDB, remains unfamiliar to many experienced distributed-systems developers joining AWS. Similar gaps exist in the adoption of other proven methodologies, such as fault-injection testing, property-based testing, and fuzzing. The challenge is educating distributed-systems developers about these testing methods and tools, teaching the art of rigorous thinking.</p><p id="p-31">The education gap begins at the academic level, where even basic formal reasoning approaches are rarely taught, making it difficult for graduates from top institutions to adopt these tools. Although formal methods and automated reasoning are crucial for industry applications, they continue to be viewed as niche fields. We anticipate that increased industry adoption of formal methods and automated reasoning will attract more talent to this domain.</p><p id="p-32">Metastability and other emergent properties of large-scale systems represent another critical research area facing similar awareness challenges. Common practices that lead to metastable system behavior, such as “retry N times on timeout,” continue to be widely recommended despite their known issues. Current tools and techniques for understanding emergent system behavior are still in their early stages, making system stability modeling expensive and complex. Ongoing research in this area holds promising potential for advancement.</p><p id="p-33">Looking ahead, we believe large language models and AI assistants will significantly help address the adoption challenges of formal methods in practice. Just as AI-assisted unit testing has gained popularity, these tools are expected soon to help developers create formal models and specifications, making these advanced techniques more accessible to the broader developer community.</p></section><section id="sec12"><h2>Conclusion</h2><p id="p-34">Building reliable and secure software requires a range of approaches to reason about systems correctness. Alongside industry-standard testing methods (such as unit and integration testing), AWS has adopted model checking, fuzzing, property-based testing, fault-injection testing, deterministic simulation, event-based simulation, and runtime validation of execution traces. Formal methods have been an important part of the development process—perhaps most importantly, formal specifications as test oracles that provide the correct answers for many of AWS’s testing practices. Correctness testing and formal methods remain key areas of investment at AWS, accelerated by the excellent returns seen on investments in these areas already.</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What is the best LLM for consumer grade hardware? (110 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44134896</link>
            <guid>44134896</guid>
            <pubDate>Fri, 30 May 2025 11:02:19 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44134896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="44135283"><td></td></tr>
                <tr id="44137215"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44137215" href="https://news.ycombinator.com/vote?id=44137215&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; If you want to run LLMs locally then the localllama community is your friend: <a href="https://old.reddit.com/r/LocalLLaMA/" rel="nofollow">https://old.reddit.com/r/LocalLLaMA/</a></p><p>For folks new to reddit, it's worth noting that LocalLlama, just like the rest of the internet but especially reddit, is filled with misinformed people spreading incorrect "facts" as truth, and you really can't use the upvote/downvote count as an indicator of quality or how truthful something is there.</p><p>Something that is more accurate but put in a boring way will often be downvoted, while straight up incorrect but funny/emotional/"fitting the group think" comments usually get upvoted.</p><p>For us who've spent a lot of time on the web, this sort of bullshit detector is basically built-in at this point, but if you're new to places where the group think is so heavy as on reddit, it's worth being careful taking anything at face value.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137262"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44137262" href="https://news.ycombinator.com/vote?id=44137262&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>This is entirely why I can't bring myself to use it. The groupthink and virtue signaling is <i>intense</i>, when it's not just extremely low effort crud that rises to the top. And yes, before anyone says, I know, "curate." No, thank you.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137332"><td></td></tr>
                <tr id="44137419"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44137419" href="https://news.ycombinator.com/vote?id=44137419&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I understand that the core similarities are there, but I disagree. The comparisons have been around since I started browsing HN years ago. The moderation on this site, for one, emphasizes constructive conversation and discussion in a way that most subreddits can only dream of.</p><p>It also helps that the target audience has been filtered with that moderation, so over time this site (on average) skews more technical and informed.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137755"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_44137755" href="https://news.ycombinator.com/vote?id=44137755&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Frankly, no. As an obvious example that can be stated nowadays: musk has always been an over-promising liar.</p><p>Eg just look at the 2012+ videos of thunderf00t.</p><p>Yet people were literally banned here just for pointing out that he hasn't actually delivered on anything in the capacity he promised until he did the salute.</p><p>It's pointless to list other examples, as this page is- as dingnuts pointed out - exactly the same and most people aren't actually willing to change their opinion based on arguments. They're set in their opinions and think everyone else is dumb.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44137702"><td></td></tr>
                <tr id="44137746"><td></td></tr>
                                    <tr id="44136239"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136239" href="https://news.ycombinator.com/vote?id=44136239&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Yes at this point it's starting to become almost a matter of how much you like the model's personality since they're all fairly decent. OP just has to start downloading and trying them out. With 16GB one can do partial DDR5 offloading with llama.cpp and run anything up to about 30B (even dense) or even more at a "reasonable" speed for chat purposes. Especially with tensor offload.</p><p>I wouldn't count Qwen as that much of a conversationalist though. Mistral Nemo and Small are pretty decent. All of Llama 3.X are still very good models even by today's standards. Gemma 3s are great but a bit unhinged. And of course QwQ when you need GPT4 at home. And probably lots of others I'm forgetting.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135941"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44135941" href="https://news.ycombinator.com/vote?id=44135941&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I'd also recommend you go with something like 8b, so you can have the other 8GB of vram for a decent sized context window. There's tons of good 8b ones, as mentioned above. If you go for the largest model you can fit, you'll have slower inference (as you pass in more tokens) and smaller context.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136192"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136192" href="https://news.ycombinator.com/vote?id=44136192&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I think your recommendation falls within</p><p>&gt; all of them will have some strengths and weaknesses</p><p>Sometimes a higher parameter model with less quantization and low context will be the best, sometimes lower parameter model with some quantization and huge context will be the best, sometimes high parameter count + lots of quantization + medium context will be the best.</p><p>It's really hard to say one model is better than another in a general way, since it depends on so many things like your use case, the prompts, the settings, quantization, quantization method and so on.</p><p>If you're building/trying to build stuff depending on LLMs in any capacity, the first step is coming up with your own custom benchmark/evaluation that you can run with your specific use cases being put under test. Don't share this publicly (so it doesn't end up in the training data) and run it in order to figure out what model is best for that specific problem.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137226"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44137226" href="https://news.ycombinator.com/vote?id=44137226&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I’m curious (as someone who knows nothing about this stuff!)—the context window is basically a record of the conversation so far and other info that isn’t part of the model, right?</p><p>I’m a bit surprised that 8GB is useful as a context window if that is the case—it just seems like you could fit a ton of research papers, emails, and textbooks in 2GB, for example.</p><p>But, I’m commenting from a place of ignorance and curiosity. Do models blow up the info in the context window, maybe do some processing to pre-digest it?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137306"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44137306" href="https://news.ycombinator.com/vote?id=44137306&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Yes, every token is expanded into a vector that can be many thousand of dimensions. The vectors are stored for every token and every layer.</p><p>You absolutely can not fill even a single research paper in 2 GB much less an entire book.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44137490"><td></td></tr>
            <tr id="44136037"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136037" href="https://news.ycombinator.com/vote?id=44136037&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>8b is the number of parameters. The most common quant is 4 bits per parameter so 8b params is roughly 4GB of VRAM. (Typically more like 4.5GB)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136708"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44136708" href="https://news.ycombinator.com/vote?id=44136708&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>The number of quantized bits is a trade off between size and quality. Ideally you should be aiming for a 6-bit or 5-bit model. I've seen some models be unstable at 4-bit (where they will either repeat words or start generating random words).</p><p>Anything below 4-bits is usually not worth it unless you want to experiment with running a 70B+ model -- though I don't have any experience of doing that, so I don't know how well the increased parameter size balances the quantization.</p><p>See <a href="https://github.com/ggml-org/llama.cpp/pull/1684">https://github.com/ggml-org/llama.cpp/pull/1684</a> and <a href="https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9" rel="nofollow">https://gist.github.com/Artefact2/b5f810600771265fc1e3944228...</a> for comparisons between quantization levels.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137105"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44137105" href="https://news.ycombinator.com/vote?id=44137105&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; The number of quantized bits is a trade off between size and quality. Ideally you should be aiming for a 6-bit or 5-bit model. I've seen some models be unstable at 4-bit (where they will either repeat words or start generating random words).</p><p>Note that that's a skill issue of whoever quantized the model. In general quantization even as low as 3-bit can be almost loseless when you do quantization-aware finetuning[1] (and apparently you don't even need that many training tokens), but even if you don't want to do any extra training you can be smart as to which parts of the model you're quantizing and by how much to minimize the damage (e.g. in the worst case over-quantizing even a <i>single</i> weight can have disastrous consequences[2])</p><p>Some time ago I ran an experiment where I finetuned a small model while quantizing parts of it to 2-bits to see which parts are most sensitive (the numbers are the final loss; lower is better):</p><pre><code>    1.5275   mlp.downscale
    1.5061   mlp.upscale
    1.4665   mlp.gate
    1.4531   lm_head
    1.3998   attn.out_proj
    1.3962   attn.v_proj
    1.3794   attn.k_proj
    1.3811   input_embedding
    1.3662   attn.q_proj
    1.3397   unquantized baseline
</code></pre><p>
So as you can see quantizing some parts of the model affects it more strongly. The downprojection in the MLP layers is the most sensitive part of the model (which also matches with what [2] found), so it makes sense to quantize this part of the model less and instead quantize other parts more strongly. But if you'll just do the naive "quantize everything in 4-bit" then sure, you might get broken models.</p><p>[1] - <a href="https://arxiv.org/pdf/2502.02631" rel="nofollow">https://arxiv.org/pdf/2502.02631</a>
[2] - <a href="https://arxiv.org/pdf/2411.07191" rel="nofollow">https://arxiv.org/pdf/2411.07191</a></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="44136498"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136498" href="https://news.ycombinator.com/vote?id=44136498&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>With a 16GB GPU you can comfortably run like Qwen3 14B or Mistral Small 24B models at Q4 to Q6 and still have plenty of context space and get much better abilities than an 8B model.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44135524"><td></td></tr>
                <tr id="44135604"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44135604" href="https://news.ycombinator.com/vote?id=44135604&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; Beyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding.</p><p>Thank you for thinking of the vibe coders.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44135928"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44135928" href="https://news.ycombinator.com/vote?id=44135928&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; Released today; probably the best reasoning model in 8B size.</p><p>Actually DeepSeek-R1-0528-Qwen3-8B was uploaded Thursday (yesterday) at 11 AM UTC / 7 PM CST.
I had to check if a new version came out since! I am waiting for the other sizes! ;D</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44137711"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137711" href="https://news.ycombinator.com/vote?id=44137711&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>At 16GB a Q4 quant of Mistral Small 3.1, or Qwen3-14B at FP8, will probably serve you best. You'd be cutting it a little close on context length due to the VRAM usage... If you want longer context, a Q4 quant of Qwen3-14B will be a bit dumber than FP8 but will leave you more breathing room. Mistral Small can take images as input, and Qwen3 will be a bit better at math/coding; YMMV otherwise.</p><p>Going below Q4 isn't worth it IMO. If you want significantly more context, probably drop down to a Q4 quant of Qwen3-8B rather than continuing to lobotomize the 14B.</p><p>Since you're on a Blackwell-generation Nvidia chip, using LLMs quantized to NVFP4 specifically will provide some speed improvements at some quality cost compared to FP8 (and will be faster than Q4 GGUF, although ~equally dumb). Ollama doesn't support NVFP4 yet, so you'd need to use vLLM (which isn't too hard, and will give better token throughput anyway). Finding pre-quantized models at NVFP4 will be more difficult since there's less-broad support, but you can use llmcompressor [1] to statically compress any FP16 LLM to NVFP4 locally — you'll probably need to use accelerate to offload params to CPU during the one-time compression process, which they have documentation for.</p><p>I wouldn't reach for this particular power tool until you've decided on an LLM already, and just want faster perf, since it's a bit more involved than just using ollama and the initial quantization process will be slow due to CPU offload during compression (albeit it's only a one-time cost). But if you land on a Q4 model, it's not a bad choice once you have a favorite.</p><p>1: <a href="https://github.com/vllm-project/llm-compressor">https://github.com/vllm-project/llm-compressor</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137434"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137434" href="https://news.ycombinator.com/vote?id=44137434&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Basic conversations are essentially RP I suppose. You can look at KoboldCPP or SillyTavern reddit.</p><p>I was trying Patricide unslop mell and some of the Qwen ones recently. Up to a point more params is better than worrying about quantization. But eventually you'll hit a compute wall with high params.</p><p>KV cache quantization is awesome (I use q4 for a 32k context with a 1080ti!) and context shifting is also awesome for long conversations/stories/games. I was using ooba but found recently that KoboldCPP not only runs faster for the same model/settings but also Kobold's context shifting works much more consistently than Ooba's "streaming_llm" option, which almost always re-evaluates the prompt when hooked up to something like ST.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137598"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137598" href="https://news.ycombinator.com/vote?id=44137598&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Wow, a 5060Ti.  16gb + I'm guessing &gt;=32gb ram.  And here I am spinning Ye Olde RX 570 4gb + 32gb.</p><p>I'd like to know how many tokens you can get out of the larger models especially (using Ollama + Open WebUI on Docker Desktop, or LM Studio whatever).  I'm probably not upgrading GPU this year, but I'd appreciate an anecdotal benchmark.</p><pre><code>  - gemma3:12b
  - phi4:latest (14b)
  - qwen2.5:14b [I get ~3 t/s on all these small models, acceptably slow]

  - qwen2.5:32b [this is about my machine's limit; verrry slow, ~1 t/s]
  - qwen2.5:72b [beyond my machine's limit, but maybe not yours]</code></pre></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137621"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44137621" href="https://news.ycombinator.com/vote?id=44137621&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I'm guessing you probably also want to include the quantization levels you're using, as otherwise they'll be a huge variance in your comparisons with others :)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44136107"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136107" href="https://news.ycombinator.com/vote?id=44136107&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>What is everyone using their local LLMs for primarily? Unless you have a beefy machine, you'll never approach the level of quality of proprietary models like Gemini or Claude, but I'm guessing these smaller models still have their use cases, just not sure what those are.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136198"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136198" href="https://news.ycombinator.com/vote?id=44136198&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Not everyone is comfortable with sending their data and/or questions and prompts to an external party.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136410"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136410" href="https://news.ycombinator.com/vote?id=44136410&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I generally try a local model first for most prompts. It's good enough surprisingly often (over 50% for sure). Every time I avoid using a cloud service is a win.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136393"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136393" href="https://news.ycombinator.com/vote?id=44136393&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>You still can get decent stuff out of local ones.</p><p>Mostly I use it for testing tools and integrations via API not to spend money on subscriptions. When I see something working I switch it to proprietary one to get best results.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136215"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136215" href="https://news.ycombinator.com/vote?id=44136215&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I'm currently experimenting with Devstral for my own local coding agent I've slowly built together. It's in many ways nicer than Codex in that 1) full access to my hardware so can start VMs, make network requests and everything else I can do, which Codex cannot and 2) it's way faster both in initial setup, working through things and creating a patch.</p><p>Of course, it still isn't at the same level as Codex itself, the model Codex is using is just way better so of course it'll get better results. But Devstral (as I currently use it) is able to make smaller changes and refactors, and I think if I evolve the software a bit more, can start making larger changes too.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136187"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136187" href="https://news.ycombinator.com/vote?id=44136187&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I think that the future of local LLMs is delegation. You give it a prompt and it very quickly identifies what should be used to solve the prompt.</p><p>Can it be solved locally with locally running MCPs? Or maybe it's a system API - like reading your calendar or checking your email. Otherwise it identifies the best cloud model and sends the prompt there.</p><p>Basically Siri if it was good</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136300"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136300" href="https://news.ycombinator.com/vote?id=44136300&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; unless you have a beefy machine</p><p>The average person in r/locallama has a machine that would make r/pcmasterrace users blush.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136406"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136406" href="https://news.ycombinator.com/vote?id=44136406&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>An Apple M1 is decent enough for LMs. My friend wondered why I got so excited about it when it came out five years ago. It wasn't that it was particularly powerful - it's decent. What it did was to set a new bar for "low end".</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136434"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44136434" href="https://news.ycombinator.com/vote?id=44136434&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>A new Mac is easily starting around $1k and quickly goes up from there if you want a storage or RAM upgrade, especially for enough memory to really run some local models. Insane that a $1,000 computer is called "decent" and "low end". My daily driver personal laptop brand new was $300.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137458"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44137458" href="https://news.ycombinator.com/vote?id=44137458&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>You're right - memory size and then bandwidth is imperative for LLMs. Apple currently lacks great memory bandwidth with their unified memory. But it's not a bad option if you can find one for a good price. The prices for new are just bonkers.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136558"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44136558" href="https://news.ycombinator.com/vote?id=44136558&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>An M1 Mac is about 5 years old at this point and can be had for far less than a grand.</p><p>A brand new Mac Mini M4 is only $499.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136703"><td></td></tr>
                <tr id="44136773"><td></td></tr>
                        <tr id="44136478"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44136478" href="https://news.ycombinator.com/vote?id=44136478&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>That's fun to hear given that low end laptops are now $800, mid range is like $1.5k and upper end is $3k+ even for non-Apple vendors. Inflation makes fools of us all.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137659"><td></td></tr>
            <tr id="44136663"><td></td></tr>
                                    <tr id="44136288"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136288" href="https://news.ycombinator.com/vote?id=44136288&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I have a large repository of notes, article drafts, and commonplace book-type stuff.
I experimented a year or so ago with a system using RAG to "ask myself" what I have to say about various topics. (I suppose nowadays I would use MCP instead of RAG?)
I was not especially impressed by the results with the models I was able to run: long-winded responses full of slop and repetition, irrelevant information pulled in from notes that had some semantically similar ideas, and such.
I'm certainly not going to feed the contents of my private notebooks to any of the AI companies.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136425"><td></td></tr>
                <tr id="44136592"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44136592" href="https://news.ycombinator.com/vote?id=44136592&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>To clarify: what I was doing was first querying for the documents via a standard document database query and then feeding the best matching documents to the LLM.
My understanding is that with MCP I'd delegate the document query from the LLM to the tool.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137494"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44137494" href="https://news.ycombinator.com/vote?id=44137494&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>As a beginner, I also haven't had much luck with embedded vector queries either. Firstly, setting it up was a major pain in the ass and I couldn't even get it to ingest anything beyond .txt files. Second, maybe it was my AI system prompt or the lack of outside search capabilities but unless i was very specific with my query the response was essentially "can't find what youre looking for"</p>
              </div></td></tr>
        </tbody></table></td></tr>
                                    <tr id="44137517"><td></td></tr>
                <tr id="44137597"><td></td></tr>
                  <tr id="44135237"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135237" href="https://news.ycombinator.com/vote?id=44135237&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I only have 8gb of vram to work with currently, but I'm running OpenWebUI as a frontend to ollamma and I have a very easy time loading up multiple models and letting them duke it out either at the same time or in a round robin.</p><p>You can even keep track of the quality of the answers over time to help guide your choice.</p><p><a href="https://openwebui.com/" rel="nofollow">https://openwebui.com/</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137017"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137017" href="https://news.ycombinator.com/vote?id=44137017&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Related question: what is everyone using to run a local LLM? I'm using Jan.ai and it's been okay. I also see OpenWebUI mentioned quite often.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137166"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44137166" href="https://news.ycombinator.com/vote?id=44137166&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>LM studio if you just want an app. openwebui is just a front end - you'd need to have either llama.cpp or vllm behind it to serve the model</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137466"><td></td></tr>
            <tr id="44137142"><td></td></tr>
                  <tr id="44135269"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135269" href="https://news.ycombinator.com/vote?id=44135269&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I'm afraid that 1) you are not going to get a definite answer, 2) an objective answer is very hard to give, 3) you really need to try a few most recent models on your own and give them the tasks that seem most useful/meaningful to you. There is drastic difference in output quality depending on the task type.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136421"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136421" href="https://news.ycombinator.com/vote?id=44136421&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Generally speaking, how can you tell how much vram a model will take?  It seems to be a valuable bit of data which is missing from downloadable models (gguf) files.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136515"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136515" href="https://news.ycombinator.com/vote?id=44136515&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Very rougly you can consider the Bs of a model as GBs of memory then it depends on the quantization level. Say for an 8B model:</p><p>- FP16: 2x 8GB = 16GB</p><p>- Q8: 1x 8GB</p><p>- Q4: 0.5x 8GB = 4GB</p><p>It doesn't 100% neatly map like this but this gives you a rough measure. In top of this you need some more memory depending on the context length and some other stuff.</p><p>Rationale for the calculation above: A model is basically a billions of variables with a floating number value. So the size of a model roughly maps to number of variables (weights) x word-precision of each variable (4, 8, 16bits..)</p><p>You don't have to quantize all layers to the same precision this is why sometimes you see fractional quantizations like 1.58bits.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136815"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136815" href="https://news.ycombinator.com/vote?id=44136815&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>The 1.58bit quantization is using 3 values -- -1, 0, 1. The bits number comes from log_2(3) = 1.58....</p><p>For that level you can pack 4 weights in a byte using 2 bits per byte. However, there is one bit configuration in each that is unused.</p><p>More complex packing arrangements are done by grouping weights together (e.g. a group of 3) and assigning a bit configuration to each combination of values into a lookup table. This allows greater compression closer to the 1.68 bits value.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44137485"><td></td></tr>
                  <tr id="44137188"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137188" href="https://news.ycombinator.com/vote?id=44137188&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>It's a bit like asking what flavour of icecream is the best. Try a few and see.</p><p>For 16gb and speed you could try Qwen3-30B-A3B with some offload to system ram or use a dense model Probably a 14B quant</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135367"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135367" href="https://news.ycombinator.com/vote?id=44135367&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I have an RTX 3070 with 8GB VRAM and for me Qwen3:30B-A3B is fast enough. It's not lightning fast, but more than adequate if you have a _little_ patience.</p><p>I've found that Qwen3 is generally really good at following instructions and you can also very easily turn on or off the reasoning by adding "/no_think" in the prompt to turn it off.</p><p>The reason Qwen3:30B works so well is because it's a MoE. I have tested the 14B model and it's noticeably slower because it's a dense model.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136024"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136024" href="https://news.ycombinator.com/vote?id=44136024&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>How are you getting Qwen3:30B-A3B running with 8GB? On my system it takes 20GB of VRAM to launch it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137522"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44137522" href="https://news.ycombinator.com/vote?id=44137522&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Probably offload to regular ram I'd wager. Or really, really, reaaaaaaally quantized to absolute fuck. Qwen3:30B-A3B Q1 with a 1k Q4 context uses 5.84GB of vram.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="44136471"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136471" href="https://news.ycombinator.com/vote?id=44136471&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I’ve had awesome results with Qwen3-30B-A3B compared to other local LMs I’ve tried.  Still not crazy good but a lot better and very fast.  I have 24GB of VRAM though</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135375"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135375" href="https://news.ycombinator.com/vote?id=44135375&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I think you'll find that on that card most models that are approaching the 16G memory size will be more than fast enough and sufficient for chat. You're in the happy position of needing steeper requirements rather than faster hardware! :D</p><p>Ollama is the easiest way to get started trying things out IMO: <a href="https://ollama.com/">https://ollama.com/</a></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44135827"><td></td></tr>
                <tr id="44136218"><td></td></tr>
            <tr id="44135979"><td></td></tr>
                <tr id="44136315"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44136315" href="https://news.ycombinator.com/vote?id=44136315&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Any FOSS solutions that let you browse models and guesstimates for you on whether you have enough VRAM to fully load the model? That's the only selling point to LM Studio for me.</p><p>Ollama's default context length is frustratingly short in the era of 100k+ context windows.</p><p>My solution so far has been to boot up LM Studio to check if a model will work well on my machine, manually download the model myself through huggingface, run llama.cpp, and hook it up to open-webui. Which is less than ideal, and LM Studio's proprietary code has access to my machine specs.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136796"><td></td></tr>
                                    <tr id="44136744"><td></td></tr>
            <tr id="44136016"><td></td></tr>
            <tr id="44135318"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135318" href="https://news.ycombinator.com/vote?id=44135318&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Good question. I've had some success with Qwen2.5-Coder 14B, I did use the quantised version: huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF:latest 
It worked well on my MacBook Pro M1 32Gb. It does get a bit hot on a laptop though.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135841"><td></td></tr>
            <tr id="44135350"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135350" href="https://news.ycombinator.com/vote?id=44135350&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>VEGA64 (8GB) is pretty much obsolete <i>for this AI stuff, right</i> (compared to e.g. M2Pro (16GB))?</p><p>I'll give Qwen2.5 a try on the Apple Silicon, thanks.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135306"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135306" href="https://news.ycombinator.com/vote?id=44135306&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Agree with what others have said: you need to try a few out. But I'd put Qwen3-14B on your list of things to try out.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135824"><td></td></tr>
                      <tr id="44136384"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136384" href="https://news.ycombinator.com/vote?id=44136384&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Phi-4 is scared to talk about anything controversial, as if they're being watched.</p><p>I asked it a question about militias. It thought for a few pages about the answer and whether to tell me, then came back with "I cannot comply".</p><p>Nidum is the name of uncensored Gemma, it does a good job most of the time.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136623"><td></td></tr>
            <tr id="44136132"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136132" href="https://news.ycombinator.com/vote?id=44136132&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>hf.co/bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF:Q6_K is a decent performing model, if you're not looking for blinding speed. It definitely ticks all the boxes in terms of model quality. Try a smaller quant if you need more speed.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135491"><td></td></tr>
                <tr id="44137297"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44137297" href="https://news.ycombinator.com/vote?id=44137297&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>It is feasible to run 7B, 8B models with q6_0 in 8GB VRAM, or q5_k_m/q4_k_m if you have to or want to free up some VRAM for other things. With q4_k_m you can run 10B and even 12B models.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44136267"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136267" href="https://news.ycombinator.com/vote?id=44136267&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Ollama[0] has a collection of models that are either already small or quantized/distilled, and come with hyperparameters that are pretty reasonable, and they make it easy to try them out. I recommend you install it and just try a bunch because they all have different "personalities", different strengths and weaknesses. My personal go-tos are:</p><p>Qwen3 family from Alibaba seem to be the best reasoning models that fit on local hardware right now. Reasoning models on local hardware are annoying in contexts where you just want an immediate response, but vastly outperform non-reasoning models on things where you want the model to be less naive/foolish.</p><p>Gemma3 from google is really good at intuition-oriented stuff, but with an obnoxious HR Boy Scout personality where you basically have to add "please don't add any disclaimers" to the system prompt for it to function. Like, just tell me how long you think this sprain will take to heal, I already know you are not a medical professional, jfc.</p><p>Devstral from Mistral performs the best on my command line utility where I describe the command I want and it executes that for me (e.g. give me a 1-liner to list the dotfiles in this folder and all subfolders that were created in the last month).</p><p>Nemo from Mistral, I have heard (but not tested) is really good for routing-type jobs, where you need something with to make a simple multiple-choice decision competently with low latency, and is easy to fine-tune if you want to get that sophisticated.</p><p>[0] <a href="https://ollama.com/search">https://ollama.com/search</a></p></div></td></tr>
        </tbody></table></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is not our future (202 pts)]]></title>
            <link>https://procreate.com/ai</link>
            <guid>44134798</guid>
            <pubDate>Fri, 30 May 2025 10:45:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://procreate.com/ai">https://procreate.com/ai</a>, See on <a href="https://news.ycombinator.com/item?id=44134798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-content"><main role="main" aria-label="Page Content"><!--[--><!--[--><section role="presentation"><div><h2>Where we stand</h2><div><!--[--><div><h3>No generative AI</h3><p>We deeply respect your hard-earned skills.</p></div><div><h3>Your work belongs to you</h3><p>We do not have access to your art, by design.</p></div><div><h3>We take pride in privacy</h3><p>Your activity is not tracked in our apps.</p></div><!--]--></div></div><div><p>Generative AI is ripping the humanity out of things. Built on a foundation of theft, the technology is steering us toward a barren future.  We think machine learning is a compelling technology with a lot of merit, but the path generative AI is on is wrong for us.</p><p>We're here for the humans. We're not chasing a technology that is a moral threat to our greatest jewel: human creativity. In this technological rush, this might make us an exception or seem at risk of being left behind. But we see this road less travelled as the more exciting and fruitful one for our community.</p></div></section><!----><!--]--><!--]--></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RFK Jr's 'Maha' report found to contain citations to nonexistent studies (143 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2025/may/29/rfk-jr-maha-health-report-studies</link>
            <guid>44133962</guid>
            <pubDate>Fri, 30 May 2025 07:54:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2025/may/29/rfk-jr-maha-health-report-studies">https://www.theguardian.com/us-news/2025/may/29/rfk-jr-maha-health-report-studies</a>, See on <a href="https://news.ycombinator.com/item?id=44133962">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><a href="https://www.theguardian.com/us-news/robert-f-kennedy-jr" data-link-name="in body link">Robert F Kennedy Jr</a>’s flagship <a href="https://www.theguardian.com/us-news/2025/may/22/rfk-jr-maha-health-report-explained" data-link-name="in body link">health commission report</a> contains citations to studies that do not exist, according to an <a href="https://www.notus.org/health-science/make-america-healthy-again-report-citation-errors" data-link-name="in body link">investigation</a> by the US publication Notus.</p><p>The report exposes glaring scientific failures from a health secretary who earlier this week threatened to ban government scientists from publishing in leading medical journals.</p><p>The 73-page “Make America healthy again” report – which was commissioned by the <a href="https://www.theguardian.com/us-news/trump-administration" data-link-name="in body link">Trump administration</a> to examine the causes of chronic illness, and which Kennedy promoted it as “gold-standard” science backed by more than 500 citations – includes references to seven studies that appear to be entirely invented, and others that the researchers say have been mischaracterized.</p><figure id="0992c1a8-3359-4cf1-a406-e28fb87cfb06" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:3,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Key takeaways: RFK Jr’s ‘Maha’ report on chronic disease in children&quot;,&quot;elementId&quot;:&quot;0992c1a8-3359-4cf1-a406-e28fb87cfb06&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/us-news/2025/may/22/rfk-jr-maha-health-report-explained&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>Two supposed studies on ADHD medication advertising simply do not exist in the journals where they are claimed to be published. Virginia Commonwealth University confirmed to Notus that researcher Robert L Findling, listed as an author of one paper, never wrote such an article, while another citation leads only to the Kennedy report itself when searched online.</p><p>Harold J Farber, a pediatric specialist supposedly behind research on asthma overprescribing, told Notus he never wrote the cited paper and had never worked with the other listed authors.</p><p>The US Department of <a href="https://www.theguardian.com/society/health" data-link-name="in body link" data-component="auto-linked-tag">Health</a> and Human Services has not immediately responded to a Guardian request for comment.</p><p>The citation failures come as Kennedy, a noted skeptic of vaccines, <a href="https://www.theguardian.com/us-news/2025/may/28/rfk-jr-medical-journals" data-link-name="in body link">criticized medical publishing</a> this week, branding top journals the Lancet, New England Journal of Medicine and Jama as “corrupt” and alleging they were controlled by pharmaceutical companies. He outlined plans for creating government-run journals instead.</p><p>Beyond the phantom studies in Kennedy’s report, Notus found it systematically misrepresented existing research.</p><figure id="5e068b28-7b23-4d73-bbb4-409abba4a53f" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:9,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;RFK’s health report omits key facts in painting dark vision for US children&quot;,&quot;elementId&quot;:&quot;5e068b28-7b23-4d73-bbb4-409abba4a53f&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/us-news/2025/may/22/rfk-maha-health-report-children&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>For example, one paper was claimed to show that talking therapy was as effective as psychiatric medication, but the statistician Joanne McKenzie said this was impossible, as “we did not include psychotherapy” in the review.</p><p>The sleep researcher Mariana G Figueiro also said her study was mischaracterized, with the report incorrectly stating it involved children rather than college students, and citing the wrong journal entirely.</p><p>The <a href="https://www.theguardian.com/us-news/trump-administration" data-link-name="in body link" data-component="auto-linked-tag">Trump administration</a> asked Kennedy for the report in order to look at chronic illness causes, from pesticides to mobile phone radiation. Kennedy called it a “milestone” that provides “evidence-based foundation” for sweeping policy changes.</p><p>A follow-up “Make our children healthy again strategy” report is due in August, raising concerns about the scientific credibility underpinning the administration’s health agenda.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Buttplug MCP (210 pts)]]></title>
            <link>https://github.com/ConAcademy/buttplug-mcp</link>
            <guid>44133706</guid>
            <pubDate>Fri, 30 May 2025 07:06:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ConAcademy/buttplug-mcp">https://github.com/ConAcademy/buttplug-mcp</a>, See on <a href="https://news.ycombinator.com/item?id=44133706">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">buttplug-mcp - Buttplug.io MCP Server</h2><a id="user-content-buttplug-mcp---buttplugio-mcp-server" aria-label="Permalink: buttplug-mcp - Buttplug.io MCP Server" href="#buttplug-mcp---buttplugio-mcp-server"></a></p>
<p dir="auto"><code>buttplug-mcp</code> is a <a href="https://www.anthropic.com/news/model-context-protocol" rel="nofollow">Model Context Protocol (MCP)</a> server for the <a href="https://buttplug.io/" rel="nofollow">Buttplug.io ecosystem</a>.  It allows Tool-supporting LLM programs like <a href="https://claude.ai/download" rel="nofollow">Claude Desktop</a> query and control your Genital Interface Devices.</p>
<p dir="auto"><em>|insert AI-generated slop image of robots doing nasty things|</em>
<br><code>LLM|=&gt; - - (__(__)</code></p>
<p dir="auto">Once set up, you can prompt your LLM:</p>
<ul dir="auto">
<li>"What are my connected buttplug devices?"</li>
<li>"Set the second motor on my LELO F1S to 50% strength"</li>
<li>"How much battery is left on my Lovense Max 2?"</li>
<li>"Does my WeWibe have weak signal?"</li>
</ul>
<p dir="auto"><strong>NOTE: The above is aspirational and really the <a href="#current-state">current experience</a> is unstable and frustating.</strong></p>
<p dir="auto">It supports the following Resources and Tools:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Resource</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>/devices</code></td>
<td>List of connected Buttplug devices in JSON.</td>
</tr>
<tr>
<td><code>/device/{id}</code></td>
<td>Device information by device ID where<code>id</code> is a number from <code>/devices</code></td>
</tr>
<tr>
<td><code>/device/{id}/rssi</code></td>
<td>RSSI signal level by device ID where <code>id</code> is a number from <code>/devices</code></td>
</tr>
<tr>
<td><code>/device/{id}/battery</code></td>
<td>Battery level by device ID where <code>id</code> is a number from <code>/devices</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Tool</th>
<th>Params</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>device_vibrate</code></td>
<td><code>id</code>, <code>motor</code>, <code>strength</code></td>
<td>Vibrates device by <code>id</code>, selecting <code>strength</code> and optional <code>motor</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<details>
<summary>JSON Schema for Resources.  Click to expand</summary>
<p dir="auto"><a href="https://github.com/ConAcademy/buttplug-mcp/blob/main/schema_resources.json"><code>schema_resources.json</code></a></p>
<div data-snippet-clipboard-copy-content="{
  &quot;jsonrpc&quot;: &quot;2.0&quot;,
  &quot;id&quot;: 1,
  &quot;result&quot;: {
    &quot;resources&quot;: [
      {
        &quot;uri&quot;: &quot;devices&quot;,
        &quot;name&quot;: &quot;Device List&quot;,
        &quot;description&quot;: &quot;List of connected Buttplug devices in JSON&quot;,
        &quot;mimeType&quot;: &quot;application/json&quot;
      }
    ]
  }
}"><pre><code>{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "resources": [
      {
        "uri": "devices",
        "name": "Device List",
        "description": "List of connected Buttplug devices in JSON",
        "mimeType": "application/json"
      }
    ]
  }
}
</code></pre></div>
</details>
<details>
<summary>JSON Schema for Tools.  Click to expand</summary>
<p dir="auto"><a href="https://github.com/ConAcademy/buttplug-mcp/blob/main/schema_tools.json"><code>schema_tools.json</code></a></p>
<div data-snippet-clipboard-copy-content="{
  &quot;jsonrpc&quot;: &quot;2.0&quot;,
  &quot;id&quot;: 1,
  &quot;result&quot;: {
    &quot;tools&quot;: [
      {
        &quot;description&quot;: &quot;Vibrates device by `id`, selecting `strength` and optional `motor`&quot;,
        &quot;inputSchema&quot;: {
          &quot;type&quot;: &quot;object&quot;,
          &quot;properties&quot;: {
            &quot;id&quot;: {
              &quot;description&quot;: &quot;Device ID to query, sourced from `/devices`&quot;,
              &quot;pattern&quot;: &quot;^[0-9]*$&quot;,
              &quot;type&quot;: &quot;number&quot;
            },
            &quot;motor&quot;: {
              &quot;description&quot;: &quot;Motor number to vibrate, defaults to 0&quot;,
              &quot;pattern&quot;: &quot;^[0-9]*$&quot;,
              &quot;type&quot;: &quot;number&quot;
            },
            &quot;strength&quot;: {
              &quot;description&quot;: &quot;Strength from 0.0 to 1.0, with 0.0 being off and 1.0 being full&quot;,
              &quot;pattern&quot;: &quot;^(0(\\.\\d+)?|1(\\.0+)?)$&quot;,
              &quot;type&quot;: &quot;number&quot;
            }
          },
          &quot;required&quot;: [
            &quot;id&quot;,
            &quot;strength&quot;
          ]
        },
        &quot;name&quot;: &quot;device_vibrate&quot;
      }
    ]
  }
}"><pre><code>{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "tools": [
      {
        "description": "Vibrates device by `id`, selecting `strength` and optional `motor`",
        "inputSchema": {
          "type": "object",
          "properties": {
            "id": {
              "description": "Device ID to query, sourced from `/devices`",
              "pattern": "^[0-9]*$",
              "type": "number"
            },
            "motor": {
              "description": "Motor number to vibrate, defaults to 0",
              "pattern": "^[0-9]*$",
              "type": "number"
            },
            "strength": {
              "description": "Strength from 0.0 to 1.0, with 0.0 being off and 1.0 being full",
              "pattern": "^(0(\\.\\d+)?|1(\\.0+)?)$",
              "type": "number"
            }
          },
          "required": [
            "id",
            "strength"
          ]
        },
        "name": "device_vibrate"
      }
    ]
  }
}
</code></pre></div>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Current State</h2><a id="user-content-current-state" aria-label="Permalink: Current State" href="#current-state"></a></p>
<p dir="auto">I started working on this on 2025-04-01, April Fool's Day, after having created another experimental MCP service, <a href="https://github.com/NimbleMarkets/dbn-go/blob/main/cmd/dbn-go-mcp/README.md"><code>dbn-go</code> for financial market data</a>, the day prior.  So it is fresh meat and was intended as a quick, fun educational project.</p>
<p dir="auto">While it does work, I found the underlying <a href="https://github.com/diamondburned/go-buttplug"><code>go-buttplug</code> library</a> to be unstable in connection handling.   I could ask Claude for my devices, but my specific device wouldn't vibrate even just with just Intiface Central -- it was like in read-only mode!    I also wish I had a virtual buttplug.io device for testing, rather than relying on a physical device.</p>
<p dir="auto">So, it has not truly been tested "end-to-end" 😉</p>
<p dir="auto">I will dig more into the <code>go-buttplug</code> library and see why connections are unstable.  I also need to understand the MCP protocol current state of MCP hosts -- it seems they focus on Tools rather than Resources and Resoure Templates.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing the binary</h2><a id="user-content-installing-the-binary" aria-label="Permalink: Installing the binary" href="#installing-the-binary"></a></p>
<p dir="auto">Binaries for multiple platforms are <a href="https://github.com/conacademy/buttplug-mcp/releases">released on GitHub</a> through <a href="https://github.com/conacademy/buttplug-mcp/actions">GitHub Actions</a>.</p>
<p dir="auto">You can also install for various platforms with <a href="https://brew.sh/" rel="nofollow">Homebrew</a> from <a href="https://github.com/conacademy/homebrew-tap"><code>conacademy/homebrew-tap</code></a>:</p>
<div data-snippet-clipboard-copy-content="brew tap conacademy/homebrew-tap
brew install conacademy/tap/buttplug-mcp"><pre><code>brew tap conacademy/homebrew-tap
brew install conacademy/tap/buttplug-mcp
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Download the <a href="https://intiface.com/central/" rel="nofollow">Intiface Central</a> hub application to manage your devices.  Start it and note the server port (default seems to be <code>12345</code>).</p>
<p dir="auto">To use this the <code>buttplug-mcp</code> MCP server, you must configure your host program to use it.  We will illustrate with <a href="https://claude.ai/download" rel="nofollow">Claude Desktop</a>.  We must find the <code>buttplug-mcp</code> program on our system; the example below shows where <code>buttplug-mcp</code> is installed with MacOS Homebrew (perhaps build your own and point at that).</p>
<p dir="auto">The following <a href="https://github.com/ConAcademy/buttplug-mcp/blob/main/claude_desktop_config.json">configuration JSON</a> sets this up:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;mcpServers&quot;: {
    &quot;buttplug&quot;: {
      &quot;command&quot;: &quot;/opt/homebrew/bin/buttplug-mcp&quot;,
      &quot;args&quot;: [
        &quot;--ws-port&quot;, &quot;12345&quot;
      ]
    }
  }
}"><pre>{
  <span>"mcpServers"</span>: {
    <span>"buttplug"</span>: {
      <span>"command"</span>: <span><span>"</span>/opt/homebrew/bin/buttplug-mcp<span>"</span></span>,
      <span>"args"</span>: [
        <span><span>"</span>--ws-port<span>"</span></span>, <span><span>"</span>12345<span>"</span></span>
      ]
    }
  }
}</pre></div>
<p dir="auto">Using Claude Desktop, you can follow <a href="https://modelcontextprotocol.io/quickstart/user" rel="nofollow">their configuration tutorial</a> but substitute the configuration above.  With that in place, you can ask Claude question and it will use the <code>buttplug-mcp</code> server.  Here's example conversations:</p>
<p dir="auto">Perhaps you can use the <a href="https://www.home-assistant.io/integrations/mcp_server/" rel="nofollow">HomeAssistant MCP</a> integration to turn the lights down low...</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Ollama and <code>mcphost</code></h3><a id="user-content-ollama-and-mcphost" aria-label="Permalink: Ollama and mcphost" href="#ollama-and-mcphost"></a></p>
<p dir="auto">For local inferencing, there are MCP hosts that support <a href="https://ollama.com/download" rel="nofollow">Ollama</a>.  You can use any <a href="https://ollama.com/search?c=tools" rel="nofollow">Ollama LLM that supports "Tools"</a>.  We experimented with <a href="https://github.com/mark3labs/mcphost"><code>mcphost</code></a>, authored by the developer of the <a href="https://github.com/mark3labs/mcp-go"><code>mcp-go</code> library</a> that peformed the heavy lifting for us.</p>
<p dir="auto">Here's how to install and run with it with the configuration above, stored in <code>mcp.json</code>:</p>
<div data-snippet-clipboard-copy-content="$ go install github.com/mark3labs/mcphost@latest
$ mcphost -m ollama:llama3.3 --config mcp.json
...chat away..."><pre><code>$ go install github.com/mark3labs/mcphost@latest
$ mcphost -m ollama:llama3.3 --config mcp.json
...chat away...
</code></pre></div>
<p dir="auto">It seems that only "Tools" are supported and not "Resources", so I couldn't enumerate and introspect my device.   But I had this Tool interaction (but as noted <a href="#current-state">above</a>, my device didn't actually vibrate):</p>
<div data-snippet-clipboard-copy-content="$ mcphost -m ollama:phi4-mini --config mcp.json
2025/04/02 09:25:05 INFO Model loaded provider=ollama model=phi4-mini
2025/04/02 09:25:05 INFO Initializing server... name=buttplug
2025/04/02 09:25:05 INFO Server connected name=buttplug
2025/04/02 09:25:05 INFO Tools loaded server=buttplug count=1
2025/04/02 09:28:31 INFO Model loaded provider=ollama model=phi4-mini
2025/04/02 09:28:31 INFO Initializing server... name=buttplug
2025/04/02 09:28:31 INFO Server connected name=buttplug
2025/04/02 09:28:31 INFO Tools loaded server=buttplug count=1
/servers
      # buttplug
      Command /opt/homebrew/bin/buttplug-mcp
      Arguments --ws-port 12345

/tools
  • buttplug
    • device_vibrate
      • Vibrates device by ID, selecting strength and optional motor

  You: buttplug device_vibrate id 0 at strength 1

  Assistant:
  <|tool_call|>[start_processing]

  [{&quot;type&quot;:&quot;function&quot;,&quot;function&quot;:{&quot;name&quot;:&quot;buttplug__device_vibrate&quot;,&quot;description&quot;:&quot;Vibrates device by ID, selecting strength and optional
  motor&quot;,&quot;parameters&quot;:{&quot;id&quot;:0,&quot;strength&quot;:1}}]

  {}

  {&quot;status&quot;:&quot;success&quot;,&quot;message&quot;:&quot;Device with id 0 is vibrating at full strength.&quot;}"><pre><code>$ mcphost -m ollama:phi4-mini --config mcp.json
2025/04/02 09:25:05 INFO Model loaded provider=ollama model=phi4-mini
2025/04/02 09:25:05 INFO Initializing server... name=buttplug
2025/04/02 09:25:05 INFO Server connected name=buttplug
2025/04/02 09:25:05 INFO Tools loaded server=buttplug count=1
2025/04/02 09:28:31 INFO Model loaded provider=ollama model=phi4-mini
2025/04/02 09:28:31 INFO Initializing server... name=buttplug
2025/04/02 09:28:31 INFO Server connected name=buttplug
2025/04/02 09:28:31 INFO Tools loaded server=buttplug count=1
/servers
      # buttplug
      Command /opt/homebrew/bin/buttplug-mcp
      Arguments --ws-port 12345

/tools
  • buttplug
    • device_vibrate
      • Vibrates device by ID, selecting strength and optional motor

  You: buttplug device_vibrate id 0 at strength 1

  Assistant:
  &lt;|tool_call|&gt;[start_processing]

  [{"type":"function","function":{"name":"buttplug__device_vibrate","description":"Vibrates device by ID, selecting strength and optional
  motor","parameters":{"id":0,"strength":1}}]

  {}

  {"status":"success","message":"Device with id 0 is vibrating at full strength."}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">Building is performed with <a href="https://taskfile.dev/" rel="nofollow">task</a>, with the binary available in <code>bin/buttplug-mcp</code>.</p>
<div data-snippet-clipboard-copy-content="$ task
task: [tidy] go mod tidy
task: [build] go build -o bin/buttplug-mcp cmd/buttplug-mcp/main.go"><pre><code>$ task
task: [tidy] go mod tidy
task: [build] go build -o bin/buttplug-mcp cmd/buttplug-mcp/main.go
</code></pre></div>
<p dir="auto">Useful testing tools:</p>
<ul dir="auto">
<li><code>task stdio-schema | jq</code> -- prints out JSON schemas</li>
<li><code>npx @modelcontextprotocol/inspector node build/index.js</code> -- <a href="https://github.com/modelcontextprotocol/inspector">MCP Inspector Web GUI</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">CLI Arguments</h2><a id="user-content-cli-arguments" aria-label="Permalink: CLI Arguments" href="#cli-arguments"></a></p>
<div data-snippet-clipboard-copy-content="R buttplug-mcp --help
usage: buttplug-mcp [opts]

  -h, --help              Show help
  -l, --log-file string   Log file destination (or MCP_LOG_FILE envvar). Default is stderr
  -j, --log-json          Log in JSON (default is plaintext)
      --sse               Use SSE Transport (default is STDIO transport)
      --sse-host string   host:port to listen to SSE connections
  -v, --verbose           Verbose logging
      --ws-port int       port to connect to the Buttplug Websocket server"><pre><code>R buttplug-mcp --help
usage: buttplug-mcp [opts]

  -h, --help              Show help
  -l, --log-file string   Log file destination (or MCP_LOG_FILE envvar). Default is stderr
  -j, --log-json          Log in JSON (default is plaintext)
      --sse               Use SSE Transport (default is STDIO transport)
      --sse-host string   host:port to listen to SSE connections
  -v, --verbose           Verbose logging
      --ws-port int       port to connect to the Buttplug Websocket server
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contribution and Conduct</h2><a id="user-content-contribution-and-conduct" aria-label="Permalink: Contribution and Conduct" href="#contribution-and-conduct"></a></p>
<p dir="auto">As with all ConAcademy projects, pull requests are welcome.  Or fork it.  You do you.</p>
<p dir="auto">Either way, obey our <a href="https://github.com/ConAcademy/buttplug-mcp/blob/main/CODE_OF_CONDUCT.md">Code of Conduct</a>.  Be shady, but don't be a jerk.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits and License</h2><a id="user-content-credits-and-license" aria-label="Permalink: Credits and License" href="#credits-and-license"></a></p>
<p dir="auto">Thanks for <code>go-buttplug</code> for the <a href="https://github.com/diamondburned/go-buttplug">Golang Buttplug.io library</a> and its <a href="https://github.com/diamondburned/go-buttplug/tree/plug/cmd/buttplughttp"><code>buttplughttp</code> example</a>, and <code>go-mcp</code> for the <a href="https://github.com/mark3labs/mcp-go">Golang Model Context Protocol library</a>.</p>
<p dir="auto">Copyright (c) 2025 Neomantra BV.  Authored by Evan Wies for <a href="https://github.com/conacademy">ConAcademy</a>.</p>
<p dir="auto">Released under the <a href="https://en.wikipedia.org/wiki/MIT_License" rel="nofollow">MIT License</a>, see <a href="https://github.com/ConAcademy/buttplug-mcp/blob/main/LICENSE.txt">LICENSE.txt</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[White House releases health report written by LLM, with hallucinated citations (168 pts)]]></title>
            <link>https://www.nytimes.com/2025/05/29/well/maha-report-citations.html</link>
            <guid>44132873</guid>
            <pubDate>Fri, 30 May 2025 04:31:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/05/29/well/maha-report-citations.html">https://www.nytimes.com/2025/05/29/well/maha-report-citations.html</a>, See on <a href="https://news.ycombinator.com/item?id=44132873">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/05/29/well/maha-report-citations.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: MCP Server SDK in Bash (104 pts)]]></title>
            <link>https://github.com/muthuishere/mcp-server-bash-sdk</link>
            <guid>44132823</guid>
            <pubDate>Fri, 30 May 2025 04:25:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/muthuishere/mcp-server-bash-sdk">https://github.com/muthuishere/mcp-server-bash-sdk</a>, See on <a href="https://news.ycombinator.com/item?id=44132823">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">🐚 MCP Server in Bash</h2><a id="user-content--mcp-server-in-bash" aria-label="Permalink: 🐚 MCP Server in Bash" href="#-mcp-server-in-bash"></a></p>
<p dir="auto">A lightweight, zero-overhead implementation of the <a href="https://modelcontextprotocol.io/" rel="nofollow">Model Context Protocol (MCP)</a> server in pure Bash.</p>
<p dir="auto"><strong>Why?</strong> Most MCP servers are just API wrappers with schema conversion. This implementation provides a zero-overhead alternative to Node.js, Python, or other heavy runtimes.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📋 Features</h2><a id="user-content--features" aria-label="Permalink: 📋 Features" href="#-features"></a></p>
<ul dir="auto">
<li>✅ Full JSON-RPC 2.0 protocol over stdio</li>
<li>✅ Complete MCP protocol implementation</li>
<li>✅ Dynamic tool discovery via function naming convention</li>
<li>✅ External configuration via JSON files</li>
<li>✅ Easy to extend with custom tools</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔧 Requirements</h2><a id="user-content--requirements" aria-label="Permalink: 🔧 Requirements" href="#-requirements"></a></p>
<ul dir="auto">
<li>Bash shell</li>
<li><code>jq</code> for JSON processing (<code>brew install jq</code> on macOS)</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Quick Start</h2><a id="user-content--quick-start" aria-label="Permalink: 🚀 Quick Start" href="#-quick-start"></a></p>
<ol dir="auto">
<li><strong>Clone the repo</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/muthuishere/mcp-server-bash-sdk
cd mcp-server-bash-sdk"><pre>git clone https://github.com/muthuishere/mcp-server-bash-sdk
<span>cd</span> mcp-server-bash-sdk</pre></div>
<ol start="2" dir="auto">
<li><strong>Make scripts executable</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="chmod +x mcpserver_core.sh moviemcpserver.sh"><pre>chmod +x mcpserver_core.sh moviemcpserver.sh</pre></div>
<ol start="3" dir="auto">
<li><strong>Try it out</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="echo '{&quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;method&quot;: &quot;tools/call&quot;, &quot;params&quot;: {&quot;name&quot;: &quot;get_movies&quot;}, &quot;id&quot;: 1}' | ./moviemcpserver.sh"><pre><span>echo</span> <span><span>'</span>{"jsonrpc": "2.0", "method": "tools/call", "params": {"name": "get_movies"}, "id": 1}<span>'</span></span> <span>|</span> ./moviemcpserver.sh</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🏗️ Architecture</h2><a id="user-content-️-architecture" aria-label="Permalink: 🏗️ Architecture" href="#️-architecture"></a></p>
<div data-snippet-clipboard-copy-content="┌─────────────┐         ┌────────────────────────┐
│ MCP Host    │         │ MCP Server             │
│ (AI System) │◄──────► │ (moviemcpserver.sh)    │
└─────────────┘ stdio   └────────────────────────┘
                             │
                     ┌───────┴──────────┐
                     ▼                  ▼
              ┌─────────────────┐  ┌───────────────┐
              │ Protocol Layer  │  │ Business Logic│
              │(mcpserver_core.sh)│  │(tool_* funcs)│
              └─────────────────┘  └───────────────┘
                     │                  │
                     ▼                  ▼
              ┌─────────────────┐  ┌───────────────┐
              │ Configuration   │  │ External      │
              │ (JSON Files)    │  │ Services/APIs │
              └─────────────────┘  └───────────────┘"><pre><code>┌─────────────┐         ┌────────────────────────┐
│ MCP Host    │         │ MCP Server             │
│ (AI System) │◄──────► │ (moviemcpserver.sh)    │
└─────────────┘ stdio   └────────────────────────┘
                             │
                     ┌───────┴──────────┐
                     ▼                  ▼
              ┌─────────────────┐  ┌───────────────┐
              │ Protocol Layer  │  │ Business Logic│
              │(mcpserver_core.sh)│  │(tool_* funcs)│
              └─────────────────┘  └───────────────┘
                     │                  │
                     ▼                  ▼
              ┌─────────────────┐  ┌───────────────┐
              │ Configuration   │  │ External      │
              │ (JSON Files)    │  │ Services/APIs │
              └─────────────────┘  └───────────────┘
</code></pre></div>
<ul dir="auto">
<li><strong>mcpserver_core.sh</strong>: Handles JSON-RPC and MCP protocol</li>
<li><strong>moviemcpserver.sh</strong>: Contains business logic functions</li>
<li><strong>assets/</strong>: JSON configuration files</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔌 Creating Your Own MCP Server</h2><a id="user-content--creating-your-own-mcp-server" aria-label="Permalink: 🔌 Creating Your Own MCP Server" href="#-creating-your-own-mcp-server"></a></p>
<ol dir="auto">
<li><strong>Create your business logic file (e.g., <code>weatherserver.sh</code>)</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="#!/bin/bash
# Weather API implementation

# Source the core MCP server
source &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)/mcpserver_core.sh&quot;

# Access environment variables
API_KEY=&quot;${MCP_API_KEY:-default_key}&quot;

# Weather tool implementation
tool_get_weather() {
  local args=&quot;$1&quot;
  local location=$(echo &quot;$args&quot; | jq -r '.location')
  
  # Call external API
  local weather=$(curl -s &quot;https://api.example.com/weather?location=$location&amp;apikey=$API_KEY&quot;)
  echo &quot;$weather&quot;
  return 0
}

# Forecast tool implementation
tool_get_forecast() {
  local args=&quot;$1&quot;
  local location=$(echo &quot;$args&quot; | jq -r '.location')
  local days=$(echo &quot;$args&quot; | jq -r '.days')
  
  local forecast=$(curl -s &quot;https://api.example.com/forecast?location=$location&amp;days=$days&amp;apikey=$API_KEY&quot;)
  echo &quot;$forecast&quot;
  return 0
}

# Start the MCP server
run_mcp_server &quot;$@&quot;"><pre><span><span>#!</span>/bin/bash</span>
<span><span>#</span> Weather API implementation</span>

<span><span>#</span> Source the core MCP server</span>
<span>source</span> <span><span>"</span><span><span>$(</span>dirname <span><span>"</span><span>${BASH_SOURCE[0]}</span><span>"</span></span><span>)</span></span>/mcpserver_core.sh<span>"</span></span>

<span><span>#</span> Access environment variables</span>
API_KEY=<span><span>"</span><span>${MCP_API_KEY<span>:-</span>default_key}</span><span>"</span></span>

<span><span>#</span> Weather tool implementation</span>
<span>tool_get_weather</span>() {
  <span>local</span> args=<span><span>"</span><span>$1</span><span>"</span></span>
  <span>local</span> location=<span><span>$(</span>echo <span><span>"</span><span>$args</span><span>"</span></span> <span>|</span> jq -r <span><span>'</span>.location<span>'</span></span><span>)</span></span>
  
  <span><span>#</span> Call external API</span>
  <span>local</span> weather=<span><span>$(</span>curl -s <span><span>"</span>https://api.example.com/weather?location=<span>$location</span>&amp;apikey=<span>$API_KEY</span><span>"</span></span><span>)</span></span>
  <span>echo</span> <span><span>"</span><span>$weather</span><span>"</span></span>
  <span>return</span> 0
}

<span><span>#</span> Forecast tool implementation</span>
<span>tool_get_forecast</span>() {
  <span>local</span> args=<span><span>"</span><span>$1</span><span>"</span></span>
  <span>local</span> location=<span><span>$(</span>echo <span><span>"</span><span>$args</span><span>"</span></span> <span>|</span> jq -r <span><span>'</span>.location<span>'</span></span><span>)</span></span>
  <span>local</span> days=<span><span>$(</span>echo <span><span>"</span><span>$args</span><span>"</span></span> <span>|</span> jq -r <span><span>'</span>.days<span>'</span></span><span>)</span></span>
  
  <span>local</span> forecast=<span><span>$(</span>curl -s <span><span>"</span>https://api.example.com/forecast?location=<span>$location</span>&amp;days=<span>$days</span>&amp;apikey=<span>$API_KEY</span><span>"</span></span><span>)</span></span>
  <span>echo</span> <span><span>"</span><span>$forecast</span><span>"</span></span>
  <span>return</span> 0
}

<span><span>#</span> Start the MCP server</span>
run_mcp_server <span><span>"</span><span>$@</span><span>"</span></span></pre></div>
<ol start="2" dir="auto">
<li><strong>Create <code>tools_list.json</code> in the assets directory</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;tools&quot;: [
    {
      &quot;name&quot;: &quot;get_weather&quot;,
      &quot;description&quot;: &quot;Get current weather for a location&quot;,
      &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
          &quot;location&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;description&quot;: &quot;City name or coordinates&quot;
          }
        },
        &quot;required&quot;: [&quot;location&quot;]
      }
    },
    {
      &quot;name&quot;: &quot;get_forecast&quot;,
      &quot;description&quot;: &quot;Get weather forecast for multiple days&quot;,
      &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
          &quot;location&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;description&quot;: &quot;City name or coordinates&quot;
          },
          &quot;days&quot;: {
            &quot;type&quot;: &quot;integer&quot;,
            &quot;description&quot;: &quot;Number of days to forecast&quot;
          }
        },
        &quot;required&quot;: [&quot;location&quot;, &quot;days&quot;]
      }
    }
  ]
}"><pre>{
  <span>"tools"</span>: [
    {
      <span>"name"</span>: <span><span>"</span>get_weather<span>"</span></span>,
      <span>"description"</span>: <span><span>"</span>Get current weather for a location<span>"</span></span>,
      <span>"parameters"</span>: {
        <span>"type"</span>: <span><span>"</span>object<span>"</span></span>,
        <span>"properties"</span>: {
          <span>"location"</span>: {
            <span>"type"</span>: <span><span>"</span>string<span>"</span></span>,
            <span>"description"</span>: <span><span>"</span>City name or coordinates<span>"</span></span>
          }
        },
        <span>"required"</span>: [<span><span>"</span>location<span>"</span></span>]
      }
    },
    {
      <span>"name"</span>: <span><span>"</span>get_forecast<span>"</span></span>,
      <span>"description"</span>: <span><span>"</span>Get weather forecast for multiple days<span>"</span></span>,
      <span>"parameters"</span>: {
        <span>"type"</span>: <span><span>"</span>object<span>"</span></span>,
        <span>"properties"</span>: {
          <span>"location"</span>: {
            <span>"type"</span>: <span><span>"</span>string<span>"</span></span>,
            <span>"description"</span>: <span><span>"</span>City name or coordinates<span>"</span></span>
          },
          <span>"days"</span>: {
            <span>"type"</span>: <span><span>"</span>integer<span>"</span></span>,
            <span>"description"</span>: <span><span>"</span>Number of days to forecast<span>"</span></span>
          }
        },
        <span>"required"</span>: [<span><span>"</span>location<span>"</span></span>, <span><span>"</span>days<span>"</span></span>]
      }
    }
  ]
}</pre></div>
<ol start="3" dir="auto">
<li><strong>Update <code>mcpserverconfig.json</code></strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;protocolVersion&quot;: &quot;0.1.0&quot;,
  &quot;serverInfo&quot;: {
    &quot;name&quot;: &quot;WeatherServer&quot;,
    &quot;version&quot;: &quot;1.0.0&quot;
  },
  &quot;capabilities&quot;: {
    &quot;tools&quot;: {
      &quot;listChanged&quot;: true
    }
  },
  &quot;instructions&quot;: &quot;This server provides weather information and forecasts.&quot;
}"><pre>{
  <span>"protocolVersion"</span>: <span><span>"</span>0.1.0<span>"</span></span>,
  <span>"serverInfo"</span>: {
    <span>"name"</span>: <span><span>"</span>WeatherServer<span>"</span></span>,
    <span>"version"</span>: <span><span>"</span>1.0.0<span>"</span></span>
  },
  <span>"capabilities"</span>: {
    <span>"tools"</span>: {
      <span>"listChanged"</span>: <span>true</span>
    }
  },
  <span>"instructions"</span>: <span><span>"</span>This server provides weather information and forecasts.<span>"</span></span>
}</pre></div>
<ol start="4" dir="auto">
<li><strong>Make your file executable</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="chmod +x weatherserver.sh"><pre>chmod +x weatherserver.sh</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🖥️ Using with VS Code &amp; GitHub Copilot</h2><a id="user-content-️-using-with-vs-code--github-copilot" aria-label="Permalink: 🖥️ Using with VS Code &amp; GitHub Copilot" href="#️-using-with-vs-code--github-copilot"></a></p>
<ol dir="auto">
<li><strong>Update VS Code settings.json</strong></li>
</ol>

<ol start="2" dir="auto">
<li><strong>Use with GitHub Copilot Chat</strong></li>
</ol>
<div data-snippet-clipboard-copy-content="/mcp my-weather-server get weather for New York"><pre><code>/mcp my-weather-server get weather for New York
</code></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚫 Limitations</h2><a id="user-content--limitations" aria-label="Permalink: 🚫 Limitations" href="#-limitations"></a></p>
<ul dir="auto">
<li>No concurrency/parallel processing</li>
<li>Limited memory management</li>
<li>No streaming responses</li>
<li>Not designed for high throughput</li>
</ul>
<p dir="auto">For AI assistants and local tool execution, these aren't blocking issues.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📄 License</h2><a id="user-content--license" aria-label="Permalink: 📄 License" href="#-license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the <a href="https://github.com/muthuishere/mcp-server-bash-sdk/blob/main/LICENSE">LICENSE</a> file for details.</p>
<p dir="auto"><strong>The complete code is available at: <a href="https://github.com/muthuishere/mcp-server-bash-sdk">https://github.com/muthuishere/mcp-server-bash-sdk</a></strong></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Triangle splatting: radiance fields represented by triangles (128 pts)]]></title>
            <link>https://trianglesplatting.github.io/</link>
            <guid>44132744</guid>
            <pubDate>Fri, 30 May 2025 04:07:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trianglesplatting.github.io/">https://trianglesplatting.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=44132744">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
            Code will be released soon.
        </p><p>
            Triangle Splatting achieves high-quality novel view synthesis and fast rendering by representing scenes with triangles.
        In contrast, the inherent softness of Gaussian primitives often leads to blurring and a loss of fine details, for example, beneath the bench or at the room’s door, whereas Triangle Splatting preserves sharp edges and accurately captures fine details.        
        </p><div>
                    <h2>Abstract</h2>
                    <p>
                        The field of computer graphics was revolutionized by models such as Neural Radiance Fields and 3D Gaussian Splatting, displacing triangles as the dominant representation for photogrammetry. In this paper, we argue for <span>a triangle come back.</span>. We develop a differentiable renderer that directly optimizes triangles via end-to-end gradients. We achieve this by rendering each triangle as differentiable splats, combining the efficiency of triangles with the adaptive density of representations based on independent primitives. Compared to popular 2D and 3D Gaussian Splatting methods, our approach achieves higher visual fidelity, faster convergence, and increased rendering throughput. On the Mip-NeRF360 dataset, our method outperforms concurrent non-volumetric primitives in visual fidelity and achieves higher perceptual quality than the state-of-the-art Zip-NeRF on indoor scenes. <br> Triangles are <span>simple</span>, <span>compatible</span> with standard graphics stacks and GPU hardware, and <span>highly efficient</span>: for the Garden scene, we achieve over 2,400 FPS at 1280×720 resolution using an off-the-shelf mesh renderer. These results highlight the efficiency and effectiveness of triangle-based representations for high-quality novel view synthesis. Triangles bring us closer to mesh-based optimization by combining classical computer graphics with modern differentiable rendering frameworks.
                    </p>
                </div><div>
            <h2>Methodology</h2>

            <p><img src="https://trianglesplatting.github.io/assets/function.png" alt="Window function visualization">
            </p>

            <p>
                Our rendering pipeline uses <strong>3D triangles</strong> as primitives, each defined by three learnable 3D vertices, color, opacity, and a smoothness parameter \( \sigma \). The triangles are projected onto the image plane using a standard pinhole camera model with known intrinsics and extrinsics.
            </p>

            <p>
                Instead of binary masks, we introduce a <strong>smooth window function</strong> that softly modulates the triangle's influence across pixels. This function is derived from the 2D <strong>signed distance field (SDF)</strong> of the triangle, which measures the distance from a pixel \( \mathbf{p} \) to the triangle’s edges.
            </p>

            <p>
                The signed distance field \( \phi(\mathbf{p}) \) is defined as the maximum of three half-plane distances:
            </p>

            <p>
                \( \phi(\mathbf{p}) = \max\left( L_1(\mathbf{p}),\; L_2(\mathbf{p}),\; L_3(\mathbf{p}) \right) \)
            </p>

            <p>
                where each half-space function is defined as:
            </p>

            <p>
                \( L_i(\mathbf{p}) = \mathbf{n}_i \cdot \mathbf{p} + d_i \)
            </p>

            <p>
                with \( \mathbf{n}_i \) denoting the outward-facing unit normal of the \( i \)-th edge, and \( d_i \) its signed offset from the origin.
            </p>

            <p>
                The final <strong>window function</strong> is:
            </p>

            <p>
                \( I(\mathbf{p}) = \text{ReLU}\left( \frac{\phi(\mathbf{p})}{\phi(\mathbf{s})} \right)^\sigma \)
            </p>

            <p>
                where \( \mathbf{s} \) is the triangle’s incenter, i.e., the point where \( \phi \) is minimized. This function satisfies:
            </p>

            <ul>
                <li><strong>Maximum opacity at the triangle incenter</strong></li>
                <li><strong>Zero influence at and beyond the triangle boundary</strong></li>
                <li><strong>Adjustable sharpness via the parameter \( \sigma \)</strong></li>
            </ul>

            <p>
                The figure above illustrates how the window function behaves in 1D and 2D. As \( \sigma \to 0 \), the function approximates a binary triangle mask. As \( \sigma \) increases, the transition becomes smoother, and in the limit \( \sigma \to \infty \), it becomes a delta function centered at \( \mathbf{s} \).
            </p>

            <p>
                To render an image, we accumulate contributions from all projected triangles using <strong>alpha blending</strong> in <strong>front-to-back depth order</strong>. Since all steps are differentiable, we can optimize the triangle parameters using gradient-based learning.
            </p>
        </div><div>
            <h2>More Visual Results</h2>
            

            <div>
                <div id="example12">
                                <video src="https://trianglesplatting.github.io/assets/video/flowers.mp4" autoplay="" loop="" muted="" playsinline=""></video>
                                <p>
                                    Triangle Splatting (Ours)
                                </p>
                            </div>
                <!-- Example 2 -->
                <div id="example11">
                                <video src="https://trianglesplatting.github.io/assets/video/bicycle_ts.mp4" autoplay="" loop="" muted="" playsinline=""></video>
                                <p>
                                    Triangle Splatting (Ours)
                                </p>
                            </div>
            </div>


            <p>
                Triangle Splatting produces sharper and more detailed images. Notably, it renders the flowers and the background with greater realism and captures finer details compared to 3DGS or 3DCS. (If the videos appear out of sync, please reload the page to ensure proper alignment.)
            </p>
    
        </div><div>
        <hr>
        <h2>Byproduct of the Triangle-Based Representation</h2>
        <div>
            <p>
                <video autoplay="" muted="" loop="" playsinline="">
                    <source src="https://trianglesplatting.github.io/assets/video/room_unity.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </p>
            <p>
                <video autoplay="" muted="" loop="" playsinline="">
                    <source src="https://trianglesplatting.github.io/assets/video/garden_unity.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </p>
        </div>
    </div><div><p>
            Triangle Splatting unifies differentiable scene optimization with traditional graphics pipelines. The triangle soup is compatible with any mesh-based
            renderer, enabling seamless integration into traditional graphics pipelines. 
            In a game engine, we render at 2400+ FPS at 1280×720 resolution on an RTX4090.

            </p><p>
            The current visuals are rendered without shaders and were not specifically trained or optimized for game engine fidelity, which accounts for the limited visual quality. 
            Nevertheless, it demonstrates an important first step toward the direct integration of radiance fields into interactive 3D environments. 
            Future work could explore training strategies specifically tailored to maximize visual fidelity in mesh-based renderers, paving the way for seamless integration of reconstructed scenes into standard game engines for real-time applications such as AR/VR or interactive simulations.
        </p></div><p>
            The triangles are well aligned with the underlying geometry. All triangles share a consistent orientation and lie flat on the surface.
        </p><div>
            <h2>Citation</h2>
                    <p><code>
                        @article{Held2025Triangle,<br>
                            title = {Triangle Splatting for Real-Time Radiance Field Rendering},<br>
                            author = {Held, Jan and Vandeghen, Renaud and Deliege, Adrien and Hamdi, Abdullah and Cioppa, Anthony and Giancola, Silvio and Vedaldi, Andrea and Ghanem, Bernard and Tagliasacchi, Andrea and Van Droogenbroeck, Marc},<br>
                            journal = {arXiv},<br>
                            year = {2025},<br>
                        }</code>
        </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The radix 2^51 trick (2017) (324 pts)]]></title>
            <link>https://www.chosenplaintext.ca/articles/radix-2-51-trick.html</link>
            <guid>44132673</guid>
            <pubDate>Fri, 30 May 2025 03:55:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chosenplaintext.ca/articles/radix-2-51-trick.html">https://www.chosenplaintext.ca/articles/radix-2-51-trick.html</a>, See on <a href="https://news.ycombinator.com/item?id=44132673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p><em><strong>Faster addition and subtraction on modern CPUs</strong></em></p>

<p>Do you remember how to do long addition on paper?</p>

<pre><code> ¹¹ ¹
  6876
+ 3406
------
 10282
</code></pre>

<p>Starting from the “ones” position, we add 6 + 6 = 12, write down a 2 and carry a 1.
We proceed to the left, one position at a time, until there are no more digits
to add.</p>

<p>When implementing addition for large integers (e.g. 2<sup>64</sup> and above), it’s common to write
code that looks quite similar to this algorithm.
Interestingly, there’s a straightforward trick that can speed up this
process enormously on modern CPUs.</p>

<p>But first, a question: why do we start long addition with the “ones”?
Why not start on the left?</p>

<p>The answer, of course, is the carries.
We can’t figure out for sure what a given digit of the answer will be
until we’ve completed all of the additions to the right of that digit.</p>

<p>Imagine if we tried to add left-to-right instead:</p>

<blockquote>
  <p>6 + 3 = 9. So the first digit is 9.<br>
8 + 4 = 12. OK, the second digit is 2… but carry a 1, so the first digit
was actually 9 + 1 = 10… now carry back <em>that</em> 1…</p>
</blockquote>

<p>For mental math, this isn’t too bad (and some people actually prefer it
when working with small enough numbers).
As an algorithm, however, this approach has some fundamental limitations that
become clear when working with larger numbers.
Most importantly, because the later parts of the computation rely on
information from the earlier parts of the computation,
it’s hard to split up and parallelize the work.</p>

<h2 id="what-about-computers">What about computers?</h2>

<p>Computers don’t work in base 10, of course.
Instead, modern desktop and server CPUs expose an interface for operating on
(for the most part) 64-bit integers.</p>

<figure><pre><code data-lang="nasm"><span>; Add the 64-bit value in B to the 64-bit value in A</span>
<span>add</span> <span>A</span><span>,</span> <span>B</span>
<span>; Note: I'll use letters instead of real register names to keep things simple</span></code></pre></figure>

<p>As long as our numbers fit within a single 64-bit value, things are easy.
But what if we want to add, say, two 256-bit integers, <code>x</code> and <code>y</code>?</p>

<p>The obvious solution would be to break up each 256-bit number into four 64-bit
pieces (commonly referred to as “limbs”).
Place the highest 64 bits of <code>x</code> into register A,
the next 64 bits into register B,
and so on for registers C and D.
Do the same for <code>y</code> with registers E, F, G, H.</p>

<p>Now we can add <code>x</code> and <code>y</code> by adding the corresponding parts:</p>

<figure><pre><code data-lang="nasm"><span>; Equivalent to x += y</span>
<span>add</span> <span>A</span><span>,</span> <span>E</span>
<span>add</span> <span>B</span><span>,</span> <span>F</span>
<span>add</span> <span>C</span><span>,</span> <span>G</span>
<span>add</span> <span>D</span><span>,</span> <span>H</span></code></pre></figure>

<p>But wait, this might give us the wrong result!
If one of the last three additions overflow,
then we need to “carry” that extra 1 up to the next 64-bit piece.
Oh hey, does that sound familiar?</p>

<p>Fortunately, x86 has a dedicated instruction for this called “add with carry”.
<code>adc</code> will automatically check if the previous operation overflowed, adding 1
if needed.
Here’s how the proper code would look:</p>

<figure><pre><code data-lang="nasm"><span>add</span> <span>D</span><span>,</span> <span>H</span>
<span>adc</span> <span>C</span><span>,</span> <span>G</span> <span>; include carry from previous op</span>
<span>adc</span> <span>B</span><span>,</span> <span>F</span> <span>; include carry from previous op</span>
<span>adc</span> <span>A</span><span>,</span> <span>E</span> <span>; include carry from previous op</span></code></pre></figure>

<p>Just like with long addition in base 10,
we start with the least-significant “digits” (D and H)
and work our way up to the most-significant “digits” (A and E),
carrying 1s as needed along the way.</p>

<h2 id="but-now-its-slow">But now it’s slow</h2>

<p>Interestingly, our fixed code is slower than the original (incorrect) code.
Much slower.  Why is this?</p>

<p>The first reason is that <code>adc</code> is just slower to execute than a normal <code>add</code> on
most popular x86 CPUs.
Since <code>adc</code> has a third input (the carry flag),
it’s a more complex instruction than <code>add</code>.
It’s also used less often than <code>add</code>,
so there is less incentive for CPU designers to spend chip area on optimizing
<code>adc</code> performance.</p>

<p>The second reason is more interesting.
Let’s look at the Intel Haswell microarchitecture as an example.</p>

<p>On a Haswell CPU, a single <code>add</code> instruction takes 1 cycle to execute.
However, in ideal conditions, Haswell CPUs can execute up to 4 <code>add</code>
instructions in a single cycle.
How is this possible? Parallelism.
Modern processors look ahead at what instructions are coming up and try to
schedule them so that they can be executed in parallel whenever possible.
Since Haswell CPUs have 8 execution ports, and 4 of those ports can execute an
integer <code>add</code> instruction, a Haswell processor can execute up to 4 <code>add</code>
instructions at once.</p>

<p>In our original adding code, all 4 <code>add</code> instructions were independent of one
another, so it was straightforward for the processor to run them in parallel.
<strong>Now, with <code>adc</code>, each instruction depends on an output from the previous
instruction.</strong>
The processor has no choice but to execute the instructions serially, one after
the other, instead of in parallel.</p>

<p>The performance difference is even more dramatic if we use SIMD (Single
Instruction, Multiple Data) instructions.
For example, a single <code>vpaddq</code> (Vector Packed Add Quadword) instruction does
four 64-bit adds simultaneously.
Combine that with the fact that Haswell processors can execute two <code>vpaddq</code>s
per cycle, and you can see that we’re taking a serious performance hit
in order to handle carries properly.</p>

<h2 id="eliminating-carries-part-1-on-paper">Eliminating carries, part 1: on paper</h2>

<p>Back to base 10 for a minute.
How can we eliminate the need for carries?</p>

<p>Let’s make some changes to how the number system works.
First, we’ll extend the range of digits available.
Instead of 0-9, we will use 0-9, A-Z, and *:</p>

<pre><code>0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ*
</code></pre>

<p>(Yeah, I needed an extra character to make the numbers work out nicely. Bear
with me.)</p>

<p>Although we have 37 digits, we are <em>not</em> using base 37.
Numbers will still have “ones”, “tens”, and “hundreds” positions,
just like a normal base 10 system.
29 still means 29, and 29 + 1 is still 30.
The only difference is that
digits happen to be capable of counting past 9:
29 + 1 could also be written as 2A, 1K, or even U.</p>

<p>With this new, more flexible number system,
we can add without needing any carries!</p>

<pre><code>  672415
+ 736606
--------
  DA8A1B
</code></pre>

<p>This trick won’t work for all numbers in our number system (e.g. 9 + W will
need a carry),
but it will work if the numbers we are adding are
<em>normalized</em>, i.e. all of their digits are 9 or below.
In fact, we can add up to four normalized numbers in this notation before any
carries are possible:</p>

<pre><code>  999   &lt;-- largest possible normalized 3-digit number
  999
  999
+ 999
-----
  ***   &lt;-- valid 3-digit result, no carries
            (recall that * is the highest digit)
</code></pre>

<p>So, with some clever tweaks to the number system, we’ve cheated our way out of
some carries.
Of course, at some point, we will need to convert from the
37-digit base 10 system back to normal base 10.
We can do that by <em>normalizing</em> a number such that each of its digits is
between 0 and 9:</p>

<pre><code>  ¹¹ ¹ ¹
   DA8A1B
= 1409021

note:
D = 10 + 3
A = 10 + 0
B = 10 + 1
</code></pre>

<p>We normalize a number starting at the right,
determining how many “tens” are in each digit,
subtracting those “tens”,
and carrying them to the next digit.
672415 and 736606 do in fact sum to 1409021, so the system works!</p>

<p>The key insight here is that we can use this technique to delay carry
propagation until the end.
We can’t avoid carry propagation altogether, but we can avoid it temporarily.
If we save up the carries that occur during the intermediate additions,
we can propagate them all in one go at the end.</p>

<h2 id="eliminating-carries-part-2-computers">Eliminating carries, part 2: computers</h2>

<p>Carry propagation was at the heart of the performance problems we encountered earlier.
As you’ve probably anticipated by now, we can use this technique to help speed
up big number arithmetic!</p>

<p>Previously, we split a 256-bit number into four 64-bit pieces,
since x86_64 processors operate on 64-bit integers.
One way to understand this is to view the pieces as “digits”
in base 2<sup>64</sup>, since each digit has a value
between 0 and 2<sup>64</sup> - 1 (inclusive).</p>

<p>In base 10, we kept the same base, but extended the range of digits that were
allowed in order to prevent carries from occurring.
Unfortunately, we can’t do that here – a 64-bit integer only has so many
possible values, and we can’t change the hardware.
Instead, we can get the same effect by reducing the size of the base.</p>

<p>Instead of splitting 256 bits into four base 2<sup>64</sup> digits,
we’ll split 256 bits into five base 2<sup>51</sup> digits.
Each digit can still range from 0 to 2<sup>64</sup> - 1,
but the smaller base gives us the flexibility needed to prevent digits from
needing a carry.
This technique is generally referred to as “radix 2<sup>51</sup>
representation” in the cryptography literature.</p>

<p>Here’s how it will look when we split 256 bits across five limbs (i.e.
digits):</p>

<pre><code>|            [--------------------- 52 bits --------------------]|
|             [-------------------- 51 bits --------------------]|
|             [-------------------- 51 bits --------------------]|
|             [-------------------- 51 bits --------------------]|
|             [-------------------- 51 bits --------------------]|
</code></pre>

<p>Each limb has 51 (or 52) bits of the original 256-bit number.
The remaining 12 or 13 bits give us the extra “digits” we need for preventing
carries.
Effectively, the highest bits of each limb are reserved as storage for any
carries that occur during the computation.</p>

<p>In our base 10 example,
37 digits allowed us to add up to four normalized numbers before needing to
propagate carries.
In radix 2<sup>51</sup> representation,
2<sup>64</sup> digits allow us to add up to 2<sup>13</sup> normalized
numbers before we need to worry about the high 13 bits overflowing.</p>

<p><em>Aside: Why 13 bits instead of 12?
For our purposes, we’re going to ignore the carries in the most significant limb,
allowing numbers to wrap when they overflow past 2<sup>256</sup> - 1 (just like
how unsigned addition works in C with normal size integer types).
As a result, we can assign 52 bits to the most significant limb and ignore the
fact that it will run out of room for carries before the other limbs do.</em></p>

<p>With this new representation, our addition code now looks like:</p>

<figure><pre><code data-lang="nasm"><span>; Assume x is split across A, B, C, D, E (A = most significant)</span>
<span>; and assume y is split across F, G, H, I, J (F = most significant)</span>
<span>add</span> <span>A</span><span>,</span> <span>F</span>
<span>add</span> <span>B</span><span>,</span> <span>G</span>
<span>add</span> <span>C</span><span>,</span> <span>H</span>
<span>add</span> <span>D</span><span>,</span> <span>I</span>
<span>add</span> <span>E</span><span>,</span> <span>J</span>
<span>; Parallel goodness, yay!</span></code></pre></figure>

<p>Despite the fact that we now need 5 <code>add</code>s instead of 4,
addition is much faster due to the lack of carries.</p>

<p>Of course, we also need code to normalize a number by propagating carries.</p>

<figure><pre><code data-lang="nasm"><span>; Assume x is split across A, B, C, D, E (A = most significant)</span>
<span>; Register T is for temporary storage</span>

<span>mov</span> <span>T</span><span>,</span> <span>E</span> <span>; Copy E into T</span>
<span>shr</span> <span>T</span><span>,</span> <span>51</span> <span>; Shift out everything except the carries</span>
<span>add</span> <span>D</span><span>,</span> <span>T</span> <span>; Add carries from E into D</span>
<span>and</span> <span>E</span><span>,</span> <span>0x0007FFFFFFFFFFFF</span> <span>; Zero out the carries in E</span>

<span>mov</span> <span>T</span><span>,</span> <span>D</span> <span>; Copy D into T</span>
<span>shr</span> <span>T</span><span>,</span> <span>51</span> <span>; Shift out everything except the carries</span>
<span>add</span> <span>C</span><span>,</span> <span>T</span> <span>; Add carries from D into C</span>
<span>and</span> <span>D</span><span>,</span> <span>0x0007FFFFFFFFFFFF</span> <span>; Zero the carries in D</span>

<span>mov</span> <span>T</span><span>,</span> <span>C</span> <span>; Copy C into T</span>
<span>shr</span> <span>T</span><span>,</span> <span>51</span> <span>; Shift out everything except the carries</span>
<span>add</span> <span>B</span><span>,</span> <span>T</span> <span>; Add carries from C into B</span>
<span>and</span> <span>C</span><span>,</span> <span>0x0007FFFFFFFFFFFF</span> <span>; Zero the carries in C</span>

<span>mov</span> <span>T</span><span>,</span> <span>B</span> <span>; Copy B into T</span>
<span>shr</span> <span>T</span><span>,</span> <span>51</span> <span>; Shift out everything except the carries</span>
<span>add</span> <span>A</span><span>,</span> <span>T</span> <span>; Add carries from B into A</span>
<span>and</span> <span>B</span><span>,</span> <span>0x0007FFFFFFFFFFFF</span> <span>; Zero the carries in B</span>

<span>and</span> <span>A</span><span>,</span> <span>0x000FFFFFFFFFFFFF</span> <span>; Zero the carries in A</span></code></pre></figure>

<p>Amazingly, some quick and dirty benchmarks show that
<strong>radix 2<sup>51</sup> addition already outperforms radix 2<sup>64</sup>
addition on my Haswell CPU for as few as three additions – and that’s
including the cost of converting to and from
radix 2<sup>51</sup> representation</strong>.
The performance savings scale up appropriately as the number of additions
increases.</p>

<h2 id="subtraction">Subtraction</h2>

<p>So far we’ve only looked at addition.
It’s straightforward though to extend this technique to subtraction.
The main difference between addition and subtraction is that subtraction has
<em>negative</em> carries.</p>

<p>Previously, we treated all limbs (and their carries) as unsigned
integers.
To support subtraction, we can treat limbs as <em>signed</em> integers,
allowing individual digits to be either positive or negative.
With this change, each limb can store either a positive or negative carry.</p>

<p>A side effect of this is that the most significant bit of each limb is now
reserved as a sign bit.
This lowers the number of operations we can perform between normalizations from
2<sup>13</sup> to 2<sup>12</sup> – a small sacrifice in most cases.</p>

<p>I find this technique rather fascinating because of how counterintuitive it is:
by spreading data across more registers and using more operations, performance
is actually improved.
I hope you found it as interesting as I did!</p>

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. sanctions cloud provider 'Funnull' as top source of 'pig butchering' scams (133 pts)]]></title>
            <link>https://krebsonsecurity.com/2025/05/u-s-sanctions-cloud-provider-funnull-as-top-source-of-pig-butchering-scams/</link>
            <guid>44132075</guid>
            <pubDate>Fri, 30 May 2025 01:58:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2025/05/u-s-sanctions-cloud-provider-funnull-as-top-source-of-pig-butchering-scams/">https://krebsonsecurity.com/2025/05/u-s-sanctions-cloud-provider-funnull-as-top-source-of-pig-butchering-scams/</a>, See on <a href="https://news.ycombinator.com/item?id=44132075">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<div id="attachment_70230"><p><img aria-describedby="caption-attachment-70230" decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2025/01/funnell-ss.png" alt="" width="750" height="452" srcset="https://krebsonsecurity.com/wp-content/uploads/2025/01/funnell-ss.png 1319w, https://krebsonsecurity.com/wp-content/uploads/2025/01/funnell-ss-768x463.png 768w, https://krebsonsecurity.com/wp-content/uploads/2025/01/funnell-ss-782x472.png 782w" sizes="(max-width: 750px) 100vw, 750px"></p><p id="caption-attachment-70230">Image: Shutterstock, ArtHead.</p></div>
<p>The U.S. government today imposed economic sanctions on <strong>Funnull Technology Inc.</strong>, a Philippines-based company that provides computer infrastructure for hundreds of thousands of websites involved in virtual currency investment scams known as “<strong>pig butchering</strong>.” In January 2025, KrebsOnSecurity detailed how Funnull was being used as a content delivery network that catered to cybercriminals seeking to route their traffic through U.S.-based cloud providers.</p>
<p>“Americans lose billions of dollars annually to these cyber scams, with revenues generated from these crimes rising to record levels in 2024,” reads <a href="https://home.treasury.gov/news/press-releases/sb0149" target="_blank" rel="noopener">a statement</a> from the <strong>U.S. Department of the Treasury</strong>, which sanctioned Funnull and its 40-year-old Chinese administrator <strong>Liu Lizhi</strong>. “Funnull has directly facilitated several of these schemes, resulting in over $200 million in U.S. victim-reported losses.”</p>
<p>The Treasury Department said Funnull’s operations are linked to the majority of virtual currency investment scam websites reported to the FBI. The agency said Funnull directly facilitated pig butchering and other schemes that resulted in more than $200 million in financial losses by Americans.</p>
<p>Pig butchering is a rampant form of fraud wherein people are lured by flirtatious strangers online into investing in fraudulent cryptocurrency trading platforms. Victims are coached to invest more and more money into what appears to be an extremely profitable trading platform, only to find their money is gone when they wish to cash out.</p>
<p>The scammers often insist that investors pay additional “taxes” on their crypto “earnings” before they can see their invested funds again (spoiler: they never do), and a shocking number of people <a href="https://krebsonsecurity.com/2022/07/massive-losses-define-epidemic-of-pig-butchering/" target="_blank" rel="noopener">have lost six figures or more through these pig butchering scams</a>.</p>
<p>KrebsOnSecurity’s <a href="https://krebsonsecurity.com/2025/01/infrastructure-laundering-blending-in-with-the-cloud/" target="_blank" rel="noopener">January story on Funnull</a> was based on research from the security firm <strong>Silent Push</strong>, which discovered in October 2024 that a vast number of domains hosted via Funnull were promoting gambling sites that bore the logo of the <strong>Suncity Group</strong>, a Chinese entity named in&nbsp;<a href="https://www.unodc.org/roseap/uploads/documents/Publications/2024/Casino_Underground_Banking_Report_2024.pdf" target="_blank" rel="noopener">a 2024 UN report</a> (PDF) for laundering millions of dollars for the North Korean state-sponsored hacking group <a href="https://en.wikipedia.org/wiki/Lazarus_Group" target="_blank" rel="noopener">Lazarus</a>.</p>
<p>Silent Push found Funnull was a criminal content delivery network (CDN) that carried a great deal of traffic tied to scam websites, funneling the traffic through a dizzying chain of auto-generated domain names and U.S.-based cloud providers before redirecting to malicious or phishous websites. The FBI has released a <a href="https://www.ic3.gov/CSA/2025/250529.pdf" target="_blank" rel="noopener">technical writeup</a> (PDF) of the infrastructure used to manage the malicious Funnull domains between October 2023 and April 2025.</p>
<div id="attachment_71392"><p><img aria-describedby="caption-attachment-71392" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2025/05/funnull-network.png" alt="" width="749" height="464" srcset="https://krebsonsecurity.com/wp-content/uploads/2025/05/funnull-network.png 2556w, https://krebsonsecurity.com/wp-content/uploads/2025/05/funnull-network-768x476.png 768w, https://krebsonsecurity.com/wp-content/uploads/2025/05/funnull-network-1536x952.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2025/05/funnull-network-2048x1269.png 2048w, https://krebsonsecurity.com/wp-content/uploads/2025/05/funnull-network-782x485.png 782w" sizes="(max-width: 749px) 100vw, 749px"></p><p id="caption-attachment-71392">A graphic from the FBI explaining how Funnull generated a slew of new domains on a regular basis and mapped them to Internet addresses on U.S. cloud providers.</p></div>
<p>Silent Push <a href="https://www.silentpush.com/blog/infrastructure-laundering/" target="_blank" rel="noopener">revisited Funnull’s infrastructure</a> in January 2025 and found Funnull was still using many of the same <strong>Amazon</strong> and <strong>Microsoft</strong> cloud Internet addresses identified as malicious in its October report. Both Amazon and Microsoft pledged to rid their networks of Funnull’s presence following that story, but according to Silent Push’s <strong>Zach Edwards</strong> only one of those companies has followed through.</p>
<p>Edwards said Silent Push no longer sees Microsoft Internet addresses showing up in Funnull’s infrastructure, while Amazon continues to struggle with removing Funnull servers, including one that appears to have first materialized in 2023.</p>
<p>“Amazon is doing a terrible job — every day since they made those claims to you and us in our public blog they have had IPs still mapped to Funnull, including some that have stayed mapped for inexplicable periods of time,” Edwards said.</p>
<p>Amazon said its Amazon Web Services (AWS) hosting platform actively counters abuse attempts.</p>
<p>“We have stopped hundreds of attempts this year related to this group and we are looking into the information you shared earlier today,” reads a statement shared by Amazon. “If anyone suspects that AWS resources are being used for abusive activity, they can report it to AWS Trust &amp; Safety using the report abuse form <a href="https://support.aws.amazon.com/#/contacts/report-abuse" target="_blank" rel="noopener">here</a>.”</p>

<p>U.S. based cloud providers remain an attractive home base for cybercriminal organizations because many organizations will not be overly aggressive in blocking traffic from U.S.-based cloud networks, as doing so can result in blocking access to many legitimate web destinations that are also on that same shared network segment or host.</p>
<p>What’s more, funneling their bad traffic so that it appears to be coming out of U.S. cloud Internet providers allows cybercriminals to connect to websites from web addresses that are geographically close(r) to their targets and victims (to sidestep location-based security controls by your bank, for example).</p>
<p>Funnull is not the only cybercriminal infrastructure-as-a-service provider that was sanctioned this month: On May 20, 2025, the <strong>European Union</strong> <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=OJ:L_202500966" target="_blank" rel="noopener">imposed sanctions</a> on <strong>Stark Industries Solutions</strong>, an ISP that materialized at the start of Russia’s invasion of Ukraine and has been used as a global proxy network that conceals the true source of cyberattacks and disinformation campaigns against enemies of Russia.</p>
<p>In May 2024, KrebsOnSecurity published <a href="https://krebsonsecurity.com/2024/05/stark-industries-solutions-an-iron-hammer-in-the-cloud/" target="_blank" rel="noopener">a deep dive on Stark Industries Solutions</a> that found much of the malicious traffic traversing Stark’s network (e.g. vulnerability scanning and password brute force attacks) was being bounced through U.S.-based cloud providers. My reporting showed how deeply Stark had penetrated U.S. ISPs, and that Ivan Neculiti for many years sold “bulletproof” hosting services that told Russian cybercrime forum customers they would proudly ignore any abuse complaints or police inquiries.</p>
<div id="attachment_67471"><p><img aria-describedby="caption-attachment-67471" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/05/stark-industries-solutions.png" alt="" width="748" height="464" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/05/stark-industries-solutions.png 1197w, https://krebsonsecurity.com/wp-content/uploads/2024/05/stark-industries-solutions-768x477.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/05/stark-industries-solutions-782x485.png 782w" sizes="(max-width: 748px) 100vw, 748px"></p><p id="caption-attachment-67471">The homepage of Stark Industries Solutions.</p></div>
<p>That story examined the history of Stark’s co-founders, Moldovan brothers <strong>Ivan</strong> and <strong>Yuri Neculiti</strong>, who each denied past involvement in cybercrime or any current involvement in assisting Russian disinformation efforts or cyberattacks. Nevertheless, the EU sanctioned both brothers as well.</p>
<p>The EU said Stark and the Neculti brothers “enabled various Russian state-sponsored and state-affiliated actors to conduct destabilising activities including coordinated information manipulation and interference and cyber-attacks against the Union and third countries by providing services intended to hide these activities from European law enforcement and security agencies.”</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Practical SDR: Getting started with software-defined radio (207 pts)]]></title>
            <link>https://nostarch.com/practical-sdr</link>
            <guid>44131984</guid>
            <pubDate>Fri, 30 May 2025 01:34:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nostarch.com/practical-sdr">https://nostarch.com/practical-sdr</a>, See on <a href="https://news.ycombinator.com/item?id=44131984">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://nostarch.com/download/PracticalSDR_SampleChapter.pdf" target="_blank">Download Chapter 4: Creating an AM Receiver</a></p>
<p><strong>Look Inside!</strong></p>
<p><a href="https://nostarch.com/images/PracticalSDR_backcover.png"><img alt="Practical SDR back cover" src="https://nostarch.com/images/PracticalSDR_backcover.png" title="Practical SDR back cover"></a><br>
<a href="https://nostarch.com/images/PracticalSDR_marketingspreads_p60-61.png"><img alt="Practical SDR pages 60-61" src="https://nostarch.com/images/PracticalSDR_marketingspreads_p60-61.png" title="Practical SDR pages 60-61"></a><a href="https://nostarch.com/images/PracticalSDR_marketingspreads_p158-159.png"><img alt="Practical SDR pages 158-159" src="https://nostarch.com/images/PracticalSDR_marketingspreads_p158-159.png" title="Practical SDR pages 158-159"></a><a href="https://nostarch.com/images/PracticalSDR_marketingspreads_p222-223.png"><img alt="Practical SDR pages 222-223" src="https://nostarch.com/images/PracticalSDR_marketingspreads_p222-223.png" title="Practical SDR pages 222-223"></a></p>
<p><span><span><span>Whether you’re a hobbyist interested in exploring the airwaves, a student learning about wireless communications, or an engineer looking to prototype RF designs, <em>Practical SDR</em> will help you master the fundamentals of software-defined radio.</span></span></span></p>
<p><span><span><span>You’ll build virtual radio receivers on your computer, then extract audio from real AM and FM signals; learn how amplitude modulation works by building an AM radio; understand signal filtering by crafting clean FM reception; and grasp complex topics like IQ sampling. You’ll use the intuitive GNU Radio Companion interface to create working radio systems piece by piece, then move on to building functional AM and FM receivers, and even design your own radio transmitter.</span></span></span></p>
<p><span><span><span>Along the way, you’ll learn how to:<br>
•&nbsp;&nbsp; &nbsp;Manipulate radio frequencies from 1 MHz to 6 GHz&nbsp;<br>
•&nbsp;&nbsp; &nbsp;Use filters and gain control to extract clear signals from noise<br>
•&nbsp;&nbsp; &nbsp;Maximize your SDR’s performance by choosing the right antennas and RF hardware<br>
•&nbsp;&nbsp; &nbsp;Process complex, real-time IQ data to demodulate actual radio signals<br>
•&nbsp;&nbsp; &nbsp;Build a flexible, virtual radio testing environment on&nbsp;your computer</span></span></span></p>
<p><span><span><span>This isn’t just another theory book. <em>Practical SDR</em> bridges the gap between basic tutorials and advanced applications, providing a solid foundation for diving into modern wireless systems like Wi-Fi, Bluetooth, and cellular communications.</span></span></span></p>
<p><span><span><span>Some projects require SDR hardware, such as a HackRF One, and a compatible antenna.</span></span></span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm starting a social club to solve the male loneliness epidemic (250 pts)]]></title>
            <link>https://wave3.social</link>
            <guid>44131513</guid>
            <pubDate>Thu, 29 May 2025 23:57:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wave3.social">https://wave3.social</a>, See on <a href="https://news.ycombinator.com/item?id=44131513">Hacker News</a></p>
Couldn't get https://wave3.social: Error: getaddrinfo ENOTFOUND wave3.social]]></description>
        </item>
        <item>
            <title><![CDATA[California has got good at building giant batteries (110 pts)]]></title>
            <link>https://www.economist.com/united-states/2025/05/22/california-has-got-really-good-at-building-giant-batteries</link>
            <guid>44129603</guid>
            <pubDate>Thu, 29 May 2025 19:48:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/united-states/2025/05/22/california-has-got-really-good-at-building-giant-batteries">https://www.economist.com/united-states/2025/05/22/california-has-got-really-good-at-building-giant-batteries</a>, See on <a href="https://news.ycombinator.com/item?id=44129603">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><time datetime="2025-05-22T12:47:13.391Z"> <!-- -->May 22nd 2025</time><span>|</span><span>MOJAVE</span></p></div><section><p data-component="paragraph"><span data-caps="initial">A</span> <small>renewable energy</small> corridor is rising in eastern Kern County, California—where the Mojave Desert meets the Sierra Nevada mountains. Among the wind turbines, solar panels and Joshua Trees are giant batteries that look like shipping containers. Tesla workers tinker with the ones at the Eland solar and storage project, developed by Arevon Energy. They wear sun hats and boots and warn your correspondent to watch out for rattlesnakes.</p></section><p><h3 id="article-tags">Explore more</h3><nav aria-labelledby="article-tags"><a href="https://www.economist.com/topics/world" data-analytics="tags:world"><span>World</span></a><a href="https://www.economist.com/topics/renewable-energy" data-analytics="tags:renewable_energy"><span>Renewable energy</span></a><a href="https://www.economist.com/topics/united-states" data-analytics="tags:united_states"><span>United States</span></a></nav></p><p>This article appeared in the United States section of the print edition under the headline “Of volts and jolts”</p><div data-test-id="chapterlist" data-tracking-id="content-well-chapter-list"><div><hr data-testid="rule-accent"><div><h3><a href="https://www.economist.com/united-states" text="United States" data-analytics="chapter_list_header:United States">United States</a></h3><p><span>May 24th 2025</span></p></div></div><ul><li><a href="https://www.economist.com/united-states/2025/05/21/what-happens-if-the-inflation-reduction-act-goes-away" id="257eb444-b2b5-4b2e-b651-1f97ecf98458" data-analytics="article:reports_headline:1" data-test-id="chapterlist-link-0"><span data-testid="right-london-5-false"><span>→</span></span><span>What happens if the Inflation Reduction Act goes away?</span></a></li><li><a href="https://www.economist.com/united-states/2025/05/22/california-has-got-really-good-at-building-giant-batteries" id="c54e3be0-6d15-443d-93b5-adf4050704e5" data-analytics="article:reports_headline:2" data-test-id="chapterlist-link-1"><span data-testid="right-economist-red-false"><span>→</span></span><span>California has got really good at building giant batteries</span></a></li><li><a href="https://www.economist.com/united-states/2025/05/22/how-much-worse-could-americas-measles-outbreak-get" id="3f12a803-7619-4378-9053-e232c536f19a" data-analytics="article:reports_headline:3" data-test-id="chapterlist-link-2"><span data-testid="right-london-5-false"><span>→</span></span><span>How much worse could America’s measles outbreak get?</span></a></li><li><a href="https://www.economist.com/united-states/2025/05/22/a-court-resurrects-the-united-states-institute-of-peace" id="ad142e92-3044-4eb8-8faf-54c152024a06" data-analytics="article:reports_headline:4" data-test-id="chapterlist-link-3"><span data-testid="right-london-5-false"><span>→</span></span><span>A court resurrects the United States Institute of Peace</span></a></li><li><a href="https://www.economist.com/united-states/2025/05/18/the-maga-revolution-threatens-americas-most-innovative-place" id="49de5015-ac8c-47d5-8857-2be62a0903e7" data-analytics="article:reports_headline:5" data-test-id="chapterlist-link-4"><span data-testid="right-london-5-false"><span>→</span></span><span>The MAGA revolution threatens America’s most innovative place</span></a></li><li><a href="https://www.economist.com/united-states/2025/05/19/joe-biden-did-not-decline-alone" id="9bb1b4af-27cc-4166-94b0-7dc8f099b250" data-analytics="article:reports_headline:6" data-test-id="chapterlist-link-5"><span data-testid="right-london-5-false"><span>→</span></span><span>Joe Biden did not decline alone</span></a></li></ul></div><div orientation="vertical" data-test-id="vertical"><div orientation="vertical"><figure><img loading="lazy" width="1280" height="1709" decoding="async" data-nimg="1" sizes="300px" srcset="https://www.economist.com/cdn-cgi/image/width=16,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 16w, https://www.economist.com/cdn-cgi/image/width=32,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 32w, https://www.economist.com/cdn-cgi/image/width=48,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 48w, https://www.economist.com/cdn-cgi/image/width=64,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 64w, https://www.economist.com/cdn-cgi/image/width=96,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 96w, https://www.economist.com/cdn-cgi/image/width=128,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 128w, https://www.economist.com/cdn-cgi/image/width=256,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 256w, https://www.economist.com/cdn-cgi/image/width=360,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 360w, https://www.economist.com/cdn-cgi/image/width=384,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 384w, https://www.economist.com/cdn-cgi/image/width=480,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 480w, https://www.economist.com/cdn-cgi/image/width=600,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 600w, https://www.economist.com/cdn-cgi/image/width=834,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 834w, https://www.economist.com/cdn-cgi/image/width=960,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 960w, https://www.economist.com/cdn-cgi/image/width=1096,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 1096w, https://www.economist.com/cdn-cgi/image/width=1280,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 1280w, https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg 1424w" src="https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20250524_DE_EU.jpg"></figure></div><div orientation="vertical"><h3 orientation="vertical">From the May 24th 2025 edition</h3><p orientation="vertical">Discover stories from this section and more in the list of contents</p><p><a href="https://www.economist.com/weeklyedition/2025-05-24" data-analytics="sidebar:weekly_edition"><span data-testid="right-economist-red-true"><span>⇒</span></span><span>Explore the edition</span></a></p></div></div><div><a href="https://s100.copyright.com/AppDispatchServlet?publisherName=economist&amp;publication=economist&amp;title=California%20has%20got%20really%20good%20at%20building%20giant%20batteries&amp;publicationDate=2025-05-22&amp;contentID=%2Fcontent%2Fsjeh31k2883lvmdr9fqfv22ksgo7e5dt&amp;type=A&amp;orderBeanReset=TRUE" target="_blank" rel="noreferrer" data-analytics="end_of_article:reuse_this_content"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" data-testid="renew-outline"><path fill="var(--mb-colour-base-chicago-45)" d="M5.1 16.05a8.25 8.25 0 0 1-.825-1.95A7.696 7.696 0 0 1 4 12.05c0-2.233.775-4.133 2.325-5.7C7.875 4.783 9.767 4 12 4h.175l-1.6-1.6 1.4-1.4 4 4-4 4-1.4-1.4 1.6-1.6H12c-1.667 0-3.083.588-4.25 1.763C6.583 8.938 6 10.367 6 12.05c0 .433.05.858.15 1.275.1.417.25.825.45 1.225l-1.5 1.5ZM12.025 23l-4-4 4-4 1.4 1.4-1.6 1.6H12c1.667 0 3.083-.587 4.25-1.762C17.417 15.063 18 13.633 18 11.95c0-.433-.05-.858-.15-1.275-.1-.417-.25-.825-.45-1.225l1.5-1.5c.367.633.642 1.283.825 1.95.183.667.275 1.35.275 2.05 0 2.233-.775 4.133-2.325 5.7C16.125 19.217 14.233 20 12 20h-.175l1.6 1.6-1.4 1.4Z"></path></svg><span>Reuse this content</span></a></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Untrusted chatbot AI between you & the internet is a disaster waiting to happen (105 pts)]]></title>
            <link>https://macwright.com/2025/05/29/putting-an-untrusted-chat-layer-is-a-disaster</link>
            <guid>44129529</guid>
            <pubDate>Thu, 29 May 2025 19:42:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://macwright.com/2025/05/29/putting-an-untrusted-chat-layer-is-a-disaster">https://macwright.com/2025/05/29/putting-an-untrusted-chat-layer-is-a-disaster</a>, See on <a href="https://news.ycombinator.com/item?id=44129529">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>This is directly motivated by reading about <a href="https://browsercompany.substack.com/p/letter-to-arc-members-2025">the Browser Company’s pivot</a> to <a href="https://www.diabrowser.com/finals">Dia</a>, a chatbot-centric browser, but there are many similar cases.</p><p>I feel like this is obvious but I’m not hearing it shouted from the rooftops so here it is: <strong>adding an untrusted middleman to your information diet and all of your personal communications will eventually become a disaster that will be obvious in hindsight</strong>.</p><p>You ask OpenAI for a product recommendation, and it recommends a product that they’re associated with, or one that a company is paying them to promote. Or maybe some company detects OpenAI’s web scraper and delivers customized content to win the recommendation. You just don’t know.</p><p>This is obviously going to happen. <a href="https://en.wikipedia.org/wiki/Antitrust_cases_against_Google_by_the_European_Union#Google_Shopping_investigation">Google promoted its own products</a> in search. <a href="https://www.propublica.org/article/amazons-new-competitive-advantage-putting-its-own-products-first">Amazon recommends its own products</a>, eagerly ripping off the branding and terms used by other companies. <a href="https://www.alltechnerd.com/microsoft-expands-copilots-reach-through-bing-search-integration/">Microsoft promotes its own AI, Copilot, when you use Microsoft’s search engine, Bing, to search for Google’s AI, Gemini</a>. This kind of stuff is not illegal enough to attract enforcement in the US and it’s obviously good for business, so companies do it with gusto, even when it’s totally obvious to everyone.</p><p>This is just the ‘economic crimes’ part of the equation, because manipulation there shows up in lawsuits. What about the <a href="https://en.wikipedia.org/wiki/Grok_%28chatbot%29#%22White_genocide_in_South_Africa%22_system_prompt_change">ideological manipulation</a>? There’s plenty of pre-AI evidence for that too: <a href="https://macwright.com/2025/04/06/careless-people">Careless People</a>, the Facebook tell-all book, is chock full of examples of insiders turning the dial to promote some people or silence others on the platform. AI will be this, just harder to detect and more efficient.</p><p>When it comes down to it, the chatbot <em>doesn’t work for you</em>. It works for its maker and it is not responsible for anything it does.</p><p>Becoming dependent on the chatbot is like becoming dependent on a butler for all of your news and communications: convenient at first, but eventually you’re going to get gaslit or snuffed out with a pillow.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Airlines are charging solo passengers higher fares than groups (288 pts)]]></title>
            <link>https://thriftytraveler.com/news/airlines/airlines-charging-solo-travelers-higher-fares/</link>
            <guid>44128901</guid>
            <pubDate>Thu, 29 May 2025 18:39:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thriftytraveler.com/news/airlines/airlines-charging-solo-travelers-higher-fares/">https://thriftytraveler.com/news/airlines/airlines-charging-solo-travelers-higher-fares/</a>, See on <a href="https://news.ycombinator.com/item?id=44128901">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>Yesterday, we discovered that <a href="https://thriftytraveler.com/news/airlines/delta-pricing-quirk/" target="_blank" rel="noopener">Delta was charging solo and business travelers higher fares</a> per ticket than when booking for two or more passengers. But it turns out that pricing quirk goes far beyond just Delta.&nbsp;</p>
<p>Since we published that story, we've searched through hundreds of fares and found plenty of examples that prove it: <strong>All three of the country's largest carriers (American Airlines, United Airlines, and Delta) are penalizing solo travelers with higher ticket prices than you can book when traveling with a group – sometimes, significantly higher.</strong></p>
<p>Our&nbsp;<strong><a href="https://thriftytraveler.com/premium/" target="_blank" rel="noopener noreferrer" data-stringify-link="https://thriftytraveler.com/premium/" data-sk="tooltip_parent">Thrifty Traveler Premium</a></strong> team of flight deal analysts search hundreds of routes each day and confirms it's not exactly widespread – you won't see it on each and every route – but it's undeniable. And while it's unclear how long this pricing tactic has been utilized, it doesn't really matter: Whether it's been just days, months, or even years, it's something that few everyday travelers may realize is happening … or how much it might be costing them.</p>
<p>For example, a search for one passenger flying United from its Chicago-O'Hare (ORD) hub to nearby Peoria (PIA) next month yields a $269 one-way fare.&nbsp;</p>

<p><img fetchpriority="high" decoding="async" src="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-1-pax.jpg" alt="united flight from chicago to peoria for $269" width="750" height="389" title="Exclusive: US Airlines Are Quietly Hitting Solo &amp; Biz Travelers with Higher Fares 1" srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-1-pax.jpg 1872w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-1-pax-300x156.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-1-pax-1024x532.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-1-pax-768x399.jpg 768w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-1-pax-1536x798.jpg 1536w" sizes="(max-width: 750px) 100vw, 750px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20750%20389'%3E%3C/svg%3E" data-lazy-srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-1-pax.jpg 1872w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-1-pax-300x156.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-1-pax-1024x532.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-1-pax-768x399.jpg 768w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-1-pax-1536x798.jpg 1536w" data-lazy-src="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-1-pax.jpg"></p>

<p>But bump that up to two (or three or even four) passengers, and the price drops by almost a third: <strong>Just $181 apiece for that exact same standard economy ticket.</strong> Plus, a <a href="https://thriftytraveler.com/guides/airlines/united-basic-economy/" target="_blank" rel="noopener">United basic economy</a> fare suddenly appears that's even cheaper – something that wasn't even an option when searching for just one passenger.&nbsp;</p>

<p><img decoding="async" src="https://thriftytraveler.com/wp-content/uploads/2025/05/united-ord-pia-2-pax.jpg" alt="united flights from chicago to peoria for $151" width="750" height="384" title="Exclusive: US Airlines Are Quietly Hitting Solo &amp; Biz Travelers with Higher Fares 2" srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/united-ord-pia-2-pax.jpg 1886w, https://thriftytraveler.com/wp-content/uploads/2025/05/united-ord-pia-2-pax-300x154.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/united-ord-pia-2-pax-1024x524.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/united-ord-pia-2-pax-768x393.jpg 768w, https://thriftytraveler.com/wp-content/uploads/2025/05/united-ord-pia-2-pax-1536x787.jpg 1536w" sizes="(max-width: 750px) 100vw, 750px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20750%20384'%3E%3C/svg%3E" data-lazy-srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/united-ord-pia-2-pax.jpg 1886w, https://thriftytraveler.com/wp-content/uploads/2025/05/united-ord-pia-2-pax-300x154.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/united-ord-pia-2-pax-1024x524.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/united-ord-pia-2-pax-768x393.jpg 768w, https://thriftytraveler.com/wp-content/uploads/2025/05/united-ord-pia-2-pax-1536x787.jpg 1536w" data-lazy-src="https://thriftytraveler.com/wp-content/uploads/2025/05/united-ord-pia-2-pax.jpg"></p>

<p>You can see in the fine print below each fare exactly how United is doing this: by opening up different fare buckets based upon how many passengers you're booking for. When searching for one passenger, the lowest economy price is a Q economy fare – United's “discount coach” fare. But by searching for multiple travelers, you can pull in (even cheaper) S class fares, which United considers a “deep discount coach” ticket.</p>
<p>And much like Delta, this isn't a glitch. United spells it all out in the fare rules for these cheaper tickets, which are publicly accessible using an advanced airfare search engine like <a href="https://oldmatrix.itasoftware.com/" target="_blank" rel="nofollow noopener">ITA Matrix</a>. It plainly states: “Must be accompanied on all sectors in same compartment by at least 1 adult 15 or older.”</p>

<p><img decoding="async" src="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-fare-rules.jpg" alt="fare rules for united flight from chicago to peoria" width="750" height="190" title="Exclusive: US Airlines Are Quietly Hitting Solo &amp; Biz Travelers with Higher Fares 3" srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-fare-rules.jpg 1144w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-fare-rules-300x76.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-fare-rules-1024x260.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-fare-rules-768x195.jpg 768w" sizes="(max-width: 750px) 100vw, 750px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20750%20190'%3E%3C/svg%3E" data-lazy-srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-fare-rules.jpg 1144w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-fare-rules-300x76.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-fare-rules-1024x260.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-fare-rules-768x195.jpg 768w" data-lazy-src="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-pia-fare-rules.jpg"></p>

<p>There's no such “accompaniment restriction” on United's higher-priced single traveler fares.</p>
<p>Whether you'll run into pricier fares searching for one passenger instead of two is hit-or-miss, at least for now. So far, we've seen this pricing dynamic mainly on one-way domestic tickets – not roundtrip fares or long-haul international routes. And we haven't seen it on other major U.S. carriers like Alaska, JetBlue, or Southwest.</p>
<p>Airlines are notoriously secretive about the inner workings of how they price their fares and why. Case in point: <strong>No one from American, Delta, or United responded to requests for comment from Thrifty Traveler on this pricing strategy.&nbsp;</strong></p>
<p>In this case, the rationale for charging solo travelers more is fairly clear: It's just another way for airlines to continue “segmenting” their customers, charging business travelers paying with a corporate card more while offering a better deal to families on the exact same flight.</p>
<p>And it's even more egregious on this American Airlines flight from Charlotte (CLT) to Fort Myers (RSW) this fall. Traveling solo, you'll pay at least $422 fare for this one-way flight in economy on Oct. 13.&nbsp;</p>

<p><img decoding="async" src="https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-1-pax.jpg" alt="american airlines flight from charlotte to fort myers for $422" width="750" height="560" title="Exclusive: US Airlines Are Quietly Hitting Solo &amp; Biz Travelers with Higher Fares 4" srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-1-pax.jpg 1652w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-1-pax-300x224.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-1-pax-1024x765.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-1-pax-768x574.jpg 768w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-1-pax-1536x1147.jpg 1536w" sizes="(max-width: 750px) 100vw, 750px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20750%20560'%3E%3C/svg%3E" data-lazy-srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-1-pax.jpg 1652w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-1-pax-300x224.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-1-pax-1024x765.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-1-pax-768x574.jpg 768w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-1-pax-1536x1147.jpg 1536w" data-lazy-src="https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-1-pax.jpg"></p>

<p>But by searching for two passengers, <strong>the ticket cost drops to just $266 per person </strong>– and, again, even cheaper if you book an <a href="https://thriftytraveler.com/guides/airlines/american-airlines-basic-economy/" target="_blank" rel="noopener">American basic economy</a>&nbsp; fare.</p>

<p><img decoding="async" src="https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-2-pax.jpg" alt="american airlines flights from charlotte to fort myers for two" width="750" height="558" title="Exclusive: US Airlines Are Quietly Hitting Solo &amp; Biz Travelers with Higher Fares 5" srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-2-pax.jpg 1650w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-2-pax-300x223.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-2-pax-1024x762.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-2-pax-768x572.jpg 768w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-2-pax-1536x1143.jpg 1536w" sizes="(max-width: 750px) 100vw, 750px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20750%20558'%3E%3C/svg%3E" data-lazy-srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-2-pax.jpg 1650w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-2-pax-300x223.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-2-pax-1024x762.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-2-pax-768x572.jpg 768w, https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-2-pax-1536x1143.jpg 1536w" data-lazy-src="https://thriftytraveler.com/wp-content/uploads/2025/05/aa-clt-rsw-2-pax.jpg"></p>

<p>One last example shows airlines know what their competitors are up to. Searching for one-way flights from Chicago-O'Hare (ORD) to Lexington (LEX) in Kentucky, <a href="https://thriftytraveler.com/guides/google-flights/" target="_blank" rel="noopener">Google Flights</a> shows you can book either American or United for $214.</p>

<p><img decoding="async" src="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-1-pax.jpg" alt="google flights screenshot of flights from chicago to lexington" width="750" height="489" title="Exclusive: US Airlines Are Quietly Hitting Solo &amp; Biz Travelers with Higher Fares 6" srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-1-pax.jpg 1590w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-1-pax-300x195.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-1-pax-1024x667.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-1-pax-768x500.jpg 768w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-1-pax-1536x1001.jpg 1536w" sizes="(max-width: 750px) 100vw, 750px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20750%20489'%3E%3C/svg%3E" data-lazy-srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-1-pax.jpg 1590w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-1-pax-300x195.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-1-pax-1024x667.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-1-pax-768x500.jpg 768w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-1-pax-1536x1001.jpg 1536w" data-lazy-src="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-1-pax.jpg"></p>

<p>Yet the <strong>total price</strong> for two passengers is $215, or just $108 per passenger. Importantly, Google Flights always displays the total price for all passengers – not the cost per ticket. And while that cheapest fare is a basic economy ticket on both carriers, standard economy fares are still considerably cheaper when booking for two instead of just one.</p>

<p><img decoding="async" src="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-2-pax.jpg" alt="chicago to lexington flights for two passengers" width="750" height="492" title="Exclusive: US Airlines Are Quietly Hitting Solo &amp; Biz Travelers with Higher Fares 7" srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-2-pax.jpg 1590w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-2-pax-300x197.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-2-pax-1024x672.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-2-pax-768x504.jpg 768w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-2-pax-1536x1009.jpg 1536w" sizes="(max-width: 750px) 100vw, 750px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20750%20492'%3E%3C/svg%3E" data-lazy-srcset="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-2-pax.jpg 1590w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-2-pax-300x197.jpg 300w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-2-pax-1024x672.jpg 1024w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-2-pax-768x504.jpg 768w, https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-2-pax-1536x1009.jpg 1536w" data-lazy-src="https://thriftytraveler.com/wp-content/uploads/2025/05/ord-to-lex-2-pax.jpg"></p>

<p>Whenever this pricing strategy began, this is a massive change in how airlines set prices – and one that will likely catch many travelers off guard.&nbsp;</p>
<p>Unlike shopping at retail stores or Costco, bulk discounts are unusual for airlines – at least not just for booking just two passengers instead of one. And these higher fares for one passenger are&nbsp;the opposite of what we typically see, where travelers booking for two passengers or more wind up getting charged more per person than a single passenger.</p>
<p>That comes down to the mechanics of how airlines actually sell tickets: Carriers aren't just selling economy, extra legroom, and first class tickets but an alphabet soup of different fare classes, each at a different price. If there's only one fare available at the cheapest $118, searching for two would only yield fares at a higher, $199 price point.</p>
<p>This is a complete reversal. And solo travelers will be the ones who pay the price.&nbsp;</p>

<p><em>This is a breaking news story, check back for updates</em></p>
  </div><div>
      <h2>Stop Overpaying for Travel!</h2>
      <p>Get our daily email for the latest in travel, flight deals, and how to save on your next trip.</p>
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FLUX.1 Kontext (455 pts)]]></title>
            <link>https://bfl.ai/models/flux-kontext</link>
            <guid>44128322</guid>
            <pubDate>Thu, 29 May 2025 17:40:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bfl.ai/models/flux-kontext">https://bfl.ai/models/flux-kontext</a>, See on <a href="https://news.ycombinator.com/item?id=44128322">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="abstract"><h2>Play. Create. Manipulate.</h2><p>FLUX.1 Kontext models go beyond text-to-image. Unlike previous flow models that only allow for pure text based generation, FLUX.1 Kontext models also understand and can create from existing images. With FLUX.1 Kontext you can modify an input image via simple text instructions, enabling flexible and instant image editing - no need for finetuning or complex editing workflows. The core capabilities of the the FLUX.1 Kontext suite are:</p><div><div><h3>Character consistency</h3><p>Preserve unique elements of an image, such as a reference character or object in a picture, across multiple scenes and environments.</p></div><div><h3>Local editing</h3><p>Make targeted modifications of specific elements in an image without affecting the rest.</p></div><div><h3>Style Reference</h3><p>Generate novel scenes while preserving unique styles from a reference image, directed by text prompts.</p></div><div><h3>Interactive Speed</h3><p>Iterate at minimal latency for both image generation and editing.</p></div></div><p>Flux.1 Kontext allows you to iteratively add more instructions and build on previous edits, refining your creation step-by-step with minimal latency, while preserving image quality and character consistency.</p></div><div id="get-started"><div><h2>Get started with FLUX.1 Kontext</h2><p>Redefine what's possible with consistent, context-aware image generation</p></div><div><div><h2>FLUX.1 Kontext [max]</h2><p>Maximum Performance at High Speed</p><p>Our new premium model brings maximum performance across all aspects – greatly improved prompt adherence and typography generation meet premium consistency for editing without compromise on speed.</p></div><div><h2>FLUX.1 Kontext [pro]</h2><p>A pioneer for fast, iterative image editing</p><p>A unified model delivering local editing, generative modifications, and text-to-image generation in FLUX.1 quality. Processes text and image inputs for precise regional edits or full scene transformations at breakthrough speeds, pioneering iterative workflows that maintain character consistency across multiple editing turns.</p></div><div><h2>FLUX.1 Kontext [dev]</h2><p>Open-weights, distilled variant of Kontext, our most advanced generative image editing model.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open-sourcing circuit tracing tools (142 pts)]]></title>
            <link>https://www.anthropic.com/research/open-source-circuit-tracing</link>
            <guid>44128101</guid>
            <pubDate>Thu, 29 May 2025 17:16:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/research/open-source-circuit-tracing">https://www.anthropic.com/research/open-source-circuit-tracing</a>, See on <a href="https://news.ycombinator.com/item?id=44128101">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>In our recent interpretability research, we introduced a new method to <a href="https://www.anthropic.com/research/tracing-thoughts-language-model">trace the thoughts</a> of a large language model. Today, we’re open-sourcing the method so that anyone can build on our research.</p><p>Our approach is to generate <em>attribution graphs</em>, which (partially) reveal the steps a model took internally to decide on a particular output. The open-source <a href="https://github.com/safety-research/circuit-tracer">library</a> we’re releasing supports the generation of attribution graphs on popular open-weights models—and a frontend hosted by Neuronpedia lets you explore the graphs interactively.</p><p>This project was led by participants in our <a href="https://alignment.anthropic.com/2024/anthropic-fellows-program/">Anthropic Fellows</a> program, in collaboration with <a href="https://www.decoderesearch.org/">Decode Research</a>.</p><div><figure><img loading="eager" width="3790" height="1748" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fe370dd79d6246cc1afc45e0b7b872b6d392801cf-3790x1748.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fe370dd79d6246cc1afc45e0b7b872b6d392801cf-3790x1748.png&amp;w=3840&amp;q=75"><figcaption>An overview of the interactive graph explorer UI on Neuronpedia.</figcaption></figure></div><p>To get started, you can visit the <a href="https://www.neuronpedia.org/gemma-2-2b/graph">Neuronpedia interface</a> to generate and view your own attribution graphs for prompts of your choosing. For more sophisticated usage and research, you can view the <a href="https://github.com/safety-research/circuit-tracer">code repository</a>. This release enables researchers to:</p><ol><li><strong>Trace circuits </strong>on supported models, by generating their own attribution graphs;</li><li><strong>Visualize, annotate, and share </strong>graphs in an interactive frontend;</li><li><strong>Test</strong> <strong>hypotheses</strong> by modifying feature values and observing how model outputs change.</li></ol><p>We’ve already used these tools to study interesting behaviors like multi-step reasoning and multilingual representations in Gemma-2-2b and Llama-3.2-1b—see our demo <a href="https://github.com/safety-research/circuit-tracer/blob/main/demos/circuit_tracing_tutorial.ipynb">notebook</a> for examples and analysis. We also invite the community to help us find additional interesting circuits—as inspiration, we provide additional attribution graphs that we haven’t yet analyzed in the demo notebook and on Neuronpedia.</p><p>Our CEO Dario Amodei <a href="https://www.darioamodei.com/post/the-urgency-of-interpretability">wrote recently</a> about the urgency of interpretability research: at present, our understanding of the inner workings of AI lags far behind the progress we’re making in AI capabilities. By open-sourcing these tools, we're hoping to make it easier for the broader community to study what’s going on inside language models. We’re looking forward to seeing applications of these tools to understand model behaviors—as well as extensions that improve the tools themselves.</p><p><em>The open-source-circuit-finding library was developed by <a href="https://alignment.anthropic.com/2024/anthropic-fellows-program/">Anthropic Fellows</a> Michael Hanna and Mateusz Piotrowski with mentorship from Emmanuel Ameisen and Jack Lindsey. The Neuronpedia integration was implemented by <a href="https://www.decoderesearch.org/">Decode Research</a> (Neuronpedia lead: Johnny Lin; Science lead/director: Curt Tigges). Our Gemma graphs are based on transcoders trained as part of the <a href="https://ai.google.dev/gemma/docs/gemma_scope">GemmaScope</a> project. For questions or feedback, please open an issue on GitHub.</em></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Human coders are still better than LLMs (557 pts)]]></title>
            <link>https://antirez.com/news/153</link>
            <guid>44127739</guid>
            <pubDate>Thu, 29 May 2025 16:41:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antirez.com/news/153">https://antirez.com/news/153</a>, See on <a href="https://news.ycombinator.com/item?id=44127739">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article data-comment-id="153-" id="153-"><span><span><a href="https://antirez.com/user/antirez">antirez</a></span> 1 hour ago. 17444 views.  </span><pre>This is a short story of how humans are still so much more capable of LLMs. Note that I'm not anti-AI or alike, you know it if you know me / follow me somewhere. I use LLMs routinely, like I did today, when I want to test my ideas, for code reviews, to understand if there are better approaches than what I had in mind, to explore stuff at the limit of my expertise, and so forth (I wrote a blog post about coding with LLMs almost two years, when it was not exactly cool: I was already using LLMs for coding and never stopped, I'll have to write an update, but that's not the topic of this post).

But, still: the current level of AI is useful, great too, but so incredibly behind human intelligence, and I want to remark this as lately it is impossible to have balanced conversations.

So, today I was working to Vector Sets for Redis, to fix a complicated bug: during the time I stopped working at Redis my colleagues introduced resistance against corruption RDB and RESTORE payloads, even when the checksum of the data passes. This feature is disabled by default, but provides an enhanced layer of safety for people wanting it.

But… there is a but as big as an elephant: In order to make HNSWs fast to save into Redis RDBs and to load back, I serialized the *graph* representation, and not the element-vector pairs, otherwise I would have to re-insert back data into HNSWs, and that would be, like, 100 times slower (!). So I store all the links the nodes have with other nodes, as integers, and then I resolve them into pointers, it’s a nice trick and works great. But if you mix this and random corruptions of the representation, and the fact that my own twist on HNSWs enforce reciprocal links between nodes (I wrote my own implementation of HNSWs with many useful features, but reciprocal links are needed to enable many of them) then this could happen:

1. We load corrupted data that says A links to B, but B no longer links to A (corrupted node IDs).
2. We delete node B: since the reciprocity is violated, we don’t clear the link from A to B.
3. Then we scan the graph and once we are at B we access A: use-after-free :-D :-) :-|

So after loading data, I need to check that every link is reciprocal, and in the vanilla case this is going to be O(N^2), for each node we need to scan all the levels, for each level all the neighbors of the node, and check that it also links to this node by scanning its links at that level. Not good.

# Human vs LLM

To start, I implemented the vanilla approach, to see if the fuzzer could no longer find the bug, and it worked indeed, but loading times for a big vector set with 20 million vectors went from 45 seconds to 90 or something. WTF. So I opened a Gemini 2.5 PRO chat and told the LLM, hey, what we can do here? Is there a super fast way to do so?

The best solution that Gemini could find was to say: order the pointers of the neighbors links, so you can use binary search. Oh, well, sure, I know this, I’m not really sure if in arrays of 16/32 pointers this is going to be faster or slower. So I asked, anything else? Nope, no better solution.

So I told it: look, what about when we see A linking B at level X we store in a hash table A:B:X (but we sort A and B always so that A&gt;B, and links are the same whatever the direction), and when we see the link again we clean it, this time we just scan the whole thing as we are already doing when resolving IDs to pointers in the links, and if at the end the hash table is not empty, we know there is some link that must be non-reciprocal?

Gemini told me it was a nice idea, but there was the snprintf() to create the key and the hashing time and so forth, but yep, it was better than what my original approach (even sorting pointers). I made it notice that snprintf() was not needed. We could just memcpy() pointers in a fixed sized key. It recognized that it was possible to do so, then I realized something…

Hey, I told Gemini, what about using a fixed accumulator for A:B:X? No hash table at all. Each time we see a link (A:B:X, so 8+8+4 bytes) we xor it in the current accumulator of 12 bytes. If we store it twice, it cancels out, so at the end if the register is non-zero, we know something is odd! However I anticipated Gemini that this system was potentially subject to collisions, and to evaluate them. Even if this feature is normally turned off in Redis, when users enable such extra checks they also often expect some more protection against an attacker deliberately crafting bad payloads.

Gemini was quite impressed about the idea, but still told that pointers are… you know, similar in structure, change of a few bits, so if there were three spurious links L1, L2, L3 it could happen that the xor between L1 and L2 was the same as the L3 bits, and we could have a false negative (zero register). I also noticed that allocators tend to be very predictable and externally guessable.

I asked Gemini for ways to improve upon this: it got no great ideas. Then I thought, wait, we can actually hash this with a good enough hash function that is still fast, murmur-128 or alike (we don’t need it to have cryptographic properties for this task), and proposed the following schema to Gemini:

1. Take the link A:B:X, but use a seed obtained via /dev/urandom to prefix all the keys with it, so we actually have S:A:B:X.
2. We just xor the output of murmur-128(S:A:B:X) into the 128 bit register.
3. At the end, we check if the register is 0 (all links reciprocal).

I asked Gemini to do an analysis of that, and it was finally happy, saying that this makes it a lot harder both to casually find orphaned links that happen to xor to 0 together, and even that an external attacker could ever use this in a useful way, since “S” is not known, there is to control the pointers too, and all that it is really hard to put together. Also, this feature is a best effort extra protection that you need to enable, it is normally off and to be practical it should not pose a too big performance penalty.

Well, all this to say: I just finished the analysis and stopped to write this blog post, I’m not sure if I’m going to use this system (but likely yes), but, the creativity of humans still have an edge, we are capable of really thinking out of the box, envisioning strange and imprecise solutions that can work better than others. This is something that is extremely hard for LLMs. Still, to verify all my ideas, Gemini was very useful, and maybe I started to think at the problem in such terms because I had a “smart duck” to talk with.</pre></article></div></div>]]></description>
        </item>
    </channel>
</rss>