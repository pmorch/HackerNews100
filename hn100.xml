(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 20 May 2024 11:00:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Reflections on our Responsible Scaling Policy (120 pts)]]></title>
            <link>https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy</link>
            <guid>40411115</guid>
            <pubDate>Mon, 20 May 2024 01:15:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy">https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy</a>, See on <a href="https://news.ycombinator.com/item?id=40411115">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><img alt="Gavel" loading="eager" width="2880" height="1620" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6276c7f8e14b693c66836810242243bd8dfd03ce-2880x1620.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6276c7f8e14b693c66836810242243bd8dfd03ce-2880x1620.png&amp;w=3840&amp;q=75"></figure><p>Last summer we published our first <a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Responsible Scaling Policy (RSP)</a>, which focuses on addressing catastrophic safety failures and misuse of frontier models. In adopting this policy, our primary goal is to help turn high-level safety concepts into practical guidelines for fast-moving technical organizations and demonstrate their viability as possible standards. As we operationalize the policy, we expect to learn a great deal and plan to share our findings. This post shares reflections from implementing the policy so far. We are also working on an updated RSP and will share this soon.</p><p>We have found having a clearly-articulated policy on catastrophic risks extremely valuable. It has provided a structured framework to clarify our organizational priorities and frame discussions around project timelines, headcount, threat models, and tradeoffs. The process of implementing the policy has also surfaced a range of important questions, projects, and dependencies that might otherwise have taken longer to identify or gone undiscussed.</p><p>Balancing the desire for strong commitments with the reality that we are still seeking the right answers is challenging. In some cases, the original policy is ambiguous and needs clarification. In cases where there are open research questions or uncertainties, setting overly-specific requirements is unlikely to stand the test of time. That said, as industry actors face increasing commercial pressures we hope to move from voluntary commitments to established best practices and then well-crafted regulations.</p><p>As we continue to iterate on and improve the original policy, we are actively exploring ways to incorporate practices from existing risk management and operational safety domains. While none of these domains alone will be perfectly analogous, we expect to find valuable insights from nuclear security, biosecurity, systems safety, autonomous vehicles, aerospace, and cybersecurity. <a href="https://grnh.se/4f33f5958us">We are building an interdisciplinary team</a> to help us integrate the most relevant and valuable practices from each.</p><p>Our current framework for doing so is summarized below, as a set of five high-level commitments.</p><ol><li><strong>Establishing Red Line Capabilities. </strong>We commit to identifying and publishing "Red Line Capabilities" which might emerge in future generations of models and would present too much risk if stored or deployed under our current safety and security practices (referred to as the <em>ASL-2 Standard)</em>. </li><li><strong>Testing for Red Line Capabilities (Frontier Risk Evaluations). </strong>We commit to demonstrating that the Red Line Capabilities are not present in models, or - if we cannot do so - taking action as if they are (more below). This involves collaborating with domain experts to design a range of "Frontier Risk Evaluations"<em> – </em>empirical tests which, if failed, would give strong evidence against a model being at or near a red line capability. We also commit to maintaining a clear evaluation process and a summary of our current evaluations publicly. </li><li><strong>Responding to Red Line Capabilities. </strong>We commit to develop and implement a new standard for safety and security sufficient to handle models that have the <em>Red Line Capabilities. </em>This set of measures is referred to as the <em>ASL-3 Standard</em>. We commit not only to define the risk mitigations comprising this standard, but also detail and follow an assurance process to validate the standard’s effectiveness. Finally, we commit to pause training or deployment if necessary to ensure that models with Red Line Capabilities are only trained, stored and deployed when we are able to apply the ASL-3 standard. </li><li><strong>Iteratively extending this policy. </strong>Before we proceed with activities which require the ASL-3 standard, we commit to publish a clear description of its upper bound of suitability: a new set of <em>Red Line Capabilities</em> for which we must build <em>Frontier Risk Evaluations</em>, and which would require a higher standard of safety and security (ASL-4) before proceeding with training and deployment. This includes maintaining a clear evaluation process and summary of our evaluations publicly.</li><li><strong>Assurance Mechanisms. </strong>We commit to ensuring this policy is executed as intended, by implementing <em>Assurance Mechanisms</em>. These should ensure that our evaluation process is stress-tested; our safety and security mitigations are validated publicly or by disinterested experts; our Board of Directors and <a href="https://www.anthropic.com/news/the-long-term-benefit-trust">Long-Term Benefit Trust</a> have sufficient oversight over the policy implementation to identify any areas of non-compliance; and that the policy itself is updated via an appropriate process.</li></ol><h3>Threat Modeling and Evaluations</h3><p>Our Frontier Red Team and Alignment Science teams have focused on threat modeling and engaging with domain experts. They are primarily focused on (a) improving threat models to determine which capabilities would warrant the ASL-3 standard of security and safety, (b) working with teams developing ASL-3 controls to ensure that those controls are tailored to the correct risks, and (c) mapping capabilities which the ASL-3 standard would be insufficient to handle, and which we would continue to test for even once it is implemented. Some key reflections are:</p><ul><li>Each new generation of models has emergent capabilities, making anticipating properties of future models unusually challenging. There is a serious need for further threat modeling.</li><li>There is reasonable disagreement amongst experts over which risks to prioritize and how new capabilities might cause harm, even in relatively established Chemical, Biological, Radiological, and Nuclear (CBRN) domains. Talking to a wide variety of experts in different sub-domains has been valuable, given the lack of consensus view.</li><li>Attempting to make threat models quantitative has been helpful for deciding which capabilities and scenarios to prioritize.</li></ul><p>Our Frontier Red Team, Alignment Science, Finetuning, and Alignment Stress Testing teams are focused on building evaluations and improving our overall methodology. Currently, we conduct pre-deployment testing in the domains of cybersecurity, CBRN, and Model Autonomy for frontier models which have reached 4x the compute of our most recently tested model (you can read a more detailed description of our most recent set of evaluations on Claude 3 Opus <a href="https://cdn.sanity.io/files/4zrzovbb/website/210523b8e11b09c704c5e185fd362fe9e648d457.pdf">here</a>). We also test models mid-training if they reach this threshold, and re-test our most capable model every 3 months to account for finetuning improvements. Teams are also focused on building evaluations in a number of new domains to monitor for capabilities for which the ASL-3 standard will still be unsuitable, and identifying ways to make the overall testing process more robust. Some key reflections are:</p><ul><li>Fast iteration cycles with domain experts are especially valuable for recognizing when the difficulty level of a test is poorly calibrated or the task is divorced from the threat model in question. </li><li>We should increasingly aim to leverage and encourage the growing ecosystem of researchers and firms in this space. Many of the risks we aim to assess, particularly those involving autonomy or misalignment, are inherently complex and speculative, and our own testing and threat modeling is likely incomplete. It will also be valuable to develop a mature external ecosystem that can adequately assess the quality of our claims, as well as offer accessible evals as a service to less well-resourced companies. We have begun to test partnerships with external organizations in these areas.</li><li>Different evaluation methodologies have their own strengths and weaknesses, and the methods that most compellingly assess a model's capabilities will differ depending on the threat model or domain in question.<ul><li><em>Question &amp; answer datasets</em> are relatively easy to design and run quickly. However, they may not be the most reflective of real-world risk due to their inherently constrained formats. Teams will continue to explore the possibility of designing datasets that are good proxies for more complex sets of tasks, and which could trigger a more comprehensive, time-intensive set of testing.</li><li><em>Human trials</em> comparing the performance of subjects with model access to that of subjects with search engines are valuable for measuring misuse-related domains. However, they are time-intensive, requiring robust, well-documented, and reproducible processes. We have found it especially important to focus on establishing good expert baselines, ensuring sufficient trial sizes, and performing careful statistical inference in order to get meaningful signals from trials. We are exploring ways to scale up our infrastructure to run these types of tests. </li><li><em>Automated task evaluations</em> have proven informative for threat models where models take actions autonomously. However, building realistic virtual environments is one of the more engineering-intensive styles of evaluation. Such tasks also require secure infrastructure and safe handling of model interactions, including manual human review of tool use when the task involves the open internet, blocking potentially harmful outputs, and isolating vulnerable machines to reduce scope. These considerations make scaling the tasks challenging. </li><li>Although less rigorous and reproducible than the approaches described above, <em>expert red-teaming</em> and reviewing model behavior via transcripts have also proven valuable. These methods allow for more open-ended exploration of model capabilities and make it easier to seek expert opinions on the relevance of different evaluation tasks or questions. </li></ul></li><li>There are a number of open research questions on which our teams will focus over the coming months to build a reliable evaluation process. We welcome more exploration in these areas from the broader research community. <ul><li>We aim to collect evidence about model risk and prepare suitable mitigations <em>before</em> reaching dangerous thresholds. This requires extrapolating from current evidence to future risk levels. Ideally, the “scaling laws” that lead to dangerous capabilities would be smooth, making it possible to predict when models might develop dangerous capabilities. In future, we hope to be able to predict precisely how much more capable a next-generation model will be in a given domain. </li><li>Techniques can be used to help models complete tasks more effectively, including domain-specific reinforcement learning training, prompt engineering, and supervised fine-tuning. This makes it impossible to guarantee we are eliciting all the relevant model capabilities during testing. A good testing process involves a concerted effort to pass evaluations and invest in capability elicitation improvements. This is important to simulate scenarios where well-resourced malicious actors bypass security controls and gain access to model weights. However, there is no clear distinction between trying extremely hard to elicit a dangerous capability in some model and simply training a model to have that capability. We hope to make more precise and principled claims about what sufficient elicitation would look like in future versions of the policy. </li><li>There is significant value in making our risk assessment process externally legible. We have therefore aimed to pre-specify test results we think are indicative of an intolerable level of risk when left unmitigated. These clear commitments help avoid production pressures incentivizing the relaxation of standards, although they may inevitably result in somewhat crude or arbitrary thresholds. We would like to explore ways to better aggregate the different sources of evidence described above while maintaining external legibility for verifiable commitments. Similarly, we may explore whether to incorporate other sources of evidence, such as forecasting, which are common in <a href="https://en.wikipedia.org/wiki/Delphi_method#Applications">other domains</a>.</li></ul></li></ul><h3>The ASL-3 Standard</h3><p>Our Security, Alignment Science, and Trust and Safety teams have been focused on developing the ASL-3 standard. Their goal is to design and implement a set of controls that will sufficiently mitigate the risk of the model weights being stolen by non-state actors or models being misused via our product surfaces. This standard would be sufficient for many models with capabilities where even a low rate of misuse could be catastrophic. However, it would not be sufficient to handle capabilities which would enable state groups or groups with substantial state backing and resources. Some key reflections are:</p><ul><li>Our current plans for ensuring models are used safely and responsibly in all of our product surfaces (e.g. Vertex, Bedrock, Claude.ai) involve scaling up research on classifier models for automated detection and response as well as strengthening all aspects of traditional trust and safety practices. <ul><li>For human misuse, we expect a <a href="https://en.wikipedia.org/wiki/Defence_in_depth_(non-military)">defense-in-depth</a> approach to be most promising. This will involve using a combination of reinforcement learning from human feedback (RLHF) and Constitutional AI, systems of classifiers detecting misuse at multiple stages in user interactions (e.g. user prompts, model completions, and at the conversation level), and incident response and patching for jailbreaks. Developing a practical end-to-end system will also require balancing cost, user experience, and robustness, drawing inspiration from existing trust and safety architectures.</li><li>As described in the Responsible Scaling Policy, we will red-team this end-to-end system prior to deployment to ensure robustness against sophisticated attacks. We emphasize the importance of tying risk mitigation efforts directly to threat models, and have found that these risk mitigation objectives are improved via close collaboration between the teams developing our red-teaming approach and the researchers leading our threat modeling and evaluations efforts. </li></ul></li><li>Scaling up our security program and developing a comprehensive roadmap to defend against a wide variety of non-state actors has required a surge of effort: around 8% of all Anthropic employees are now working on security-adjacent areas and we expect that proportion to grow further as models become more economically valuable to attackers. The threat models and security targets articulated in the RSP have been especially valuable for our security team to help prioritize and motivate the necessary changes.<ul><li>Implementing the level of security required by the ASL-3 standard will require changing every aspect of employees' day-to-day workflows. To make these changes in a thoughtful way, our security team has invested significant time in building partnerships with teams, especially researchers, to preserve productivity and apply state-of-the-art cyber security controls to tooling. </li><li>Our threat modeling assumes that insider device compromise is our highest risk vector. Given this, one of our main areas of focus has been implementing multi-party authorization, time-bounded access controls in order to reduce the risk of model weights exfiltration. Under this system, employees are granted temporary access and only via the smallest set of necessary permissions. Fortunately, Anthropic has already adopted a culture of peer review across software engineering, research, comms, and finance teams, and so adopting multi-party controls as we approach the ASL-3 level has been a well-received extension of these existing cultural norms. </li></ul></li><li>In such a fast-moving field, it is often difficult to define risk mitigations, or even the methods we will use to assess their effectiveness, upfront. We want to make binding commitments where possible while still allowing degrees of freedom when new information and situations arise. We expect it will be most practical, for both the ASL-3 standard and future standards, to provide a high-level sketch of expected mitigations and set clear “attestation” standards they must meet before use. For example, with our security standard, we can clarify the goal of defending against non-state actors without specifying detailed controls in advance, and pair this with a sensible attestation process involving detailed control lists, review from disinterested experts, and board approval.</li></ul><h3>Assurance Structures</h3><p>Lastly, our Responsible Scaling, Alignment Stress Testing, and Compliance teams have been focused on exploring possible governance, coordination, and assurance structures. We intend to introduce more independent checks over time and are looking to hire a <a href="https://boards.greenhouse.io/anthropic/jobs/4024668008">Risk Manager</a> to develop these structures, drawing on best practices from other industries and relevant research. Some key reflections are:</p><ul><li>The complexity and cross-functional nature of the workstreams described above requires a high level of central coordination. We will continue to build a Responsible Scaling Team to manage the complex web of work streams and dependencies. Amidst a range of competing priorities, strong executive backing has also been essential in reinforcing that identifying and mitigating risks from frontier models is a company priority, deserving significant resources.</li><li>There is value in creating a “second line of defense” – teams that can take a more adversarial approach to our core work streams. Our Alignment Stress Testing team has begun to stress-test our evaluations, interventions, and overall policy execution. For example, the team provided reflections on potential under-elicitation alongside our Claude 3 Opus evaluations report, which were shared with our Board of Directors and summarized in our report to the U.S. Department of Commerce Bureau of Industry and Security. It may make sense to build out a bespoke internal audit function over time. </li><li>In addition to providing regular updates to our Board of Directors and the <a href="https://www.anthropic.com/news/the-long-term-benefit-trust">Long-Term Benefit Trust</a>, we have shared evaluations reports and quarterly updates on progress towards future mitigations to all employees. Encouraging employees to feel ownership over the RSP and share areas they would like to see us improve the policy has been immensely helpful, with staff drawing on diverse backgrounds to provide valuable insights. We also recently implemented a non-compliance reporting policy that allows employees to anonymously report concerns to our Responsible Scaling Officer about our implementation of our RSP.</li></ul><p>Ensuring future generations of frontier models are trained and deployed responsibly will require serious investment from both Anthropic and others across industry and governments. Our Responsible Scaling Policy has been a powerful rallying point with many teams' objectives over the past months connecting directly back to the major workstreams above. The progress we have made on operationalizing safety during this period has necessitated significant engagement from teams across Anthropic, and there is much more work to be done. Our goal in sharing these reflections ahead of the upcoming AI Seoul Summit is to continue the discussion on creating thoughtful, empirically-grounded frameworks for managing risks from frontier models. We are eager to see more companies adopt their own frameworks and share their own experiences, leading to the development of shared best practices and informing future efforts by governments.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Is Generative Art? (2022) (101 pts)]]></title>
            <link>https://www.amygoodchild.com/blog/what-is-generative-art</link>
            <guid>40410603</guid>
            <pubDate>Sun, 19 May 2024 23:43:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amygoodchild.com/blog/what-is-generative-art">https://www.amygoodchild.com/blog/what-is-generative-art</a>, See on <a href="https://news.ycombinator.com/item?id=40410603">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">
        
          
<article id="sections" data-page-sections="5f37d09b551f154577577f0a">
  
  
    
    


  


<div data-content-field="main-content" data-item-id="" data-test="page-section" data-section-theme="white" data-section-id="5f37d09b551f154577577f0c" data-controller="SectionWrapperController" data-current-styles="{
                              &quot;imageOverlayOpacity&quot;: 0.15,
                              &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                              &quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
                              &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                              &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                              &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                              &quot;sectionTheme&quot;: &quot;white&quot;,
                              &quot;sectionAnimation&quot;: &quot;none&quot;,
                              &quot;backgroundMode&quot;: &quot;image&quot;
                            }" data-current-context="{
                              &quot;video&quot;: {
                                &quot;playbackSpeed&quot;: 0.5,
                                &quot;filter&quot;: 1,
                                &quot;filterStrength&quot;: 0,
                                &quot;zoom&quot;: 0,
                                &quot;videoSourceProvider&quot;: &quot;none&quot;
                              },
                              &quot;backgroundImageId&quot;: null,
                              &quot;backgroundMediaEffect&quot;: null,
                              &quot;divider&quot;: null,
                              &quot;typeName&quot;: &quot;blog-basic-grid&quot;
                            }" data-animation="none">
  <article id="article-">
  
    
    
    
    <div data-layout-label="Post Body" data-type="item" id="item-61ec10d4138e9c5d9dda3279"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-3a93699398ae04ffe0fe">
  <p>An artwork may be generative in some ways and not in others. A piece could have generative aspects even if the artist didn’t have that intention. There are ways in which a painting could be considered generative.</p><p>I prefer definitions to be inclusive, not restrictive, and my definitions are for the purposes of discussing generative art, not for the purposes of ruling on what “is or isn’t” generative art.</p><p>Header image: <a href="https://www.fxhash.xyz/gentk/FX0-1556245">Forecast #23<br></a></p><h3>Some tentative definitions</h3><p>Okay, how about… </p>
</div><div data-block-type="23" id="block-yui_3_17_2_1_1642860757007_15285">
    <p>Generative Art</p>
    <p>Art created using autonomous processes</p>
  </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1642874631254_6102">

<p>We then need a definition for an autonomous process…</p>




















  
  



</div><div data-block-type="23" id="block-yui_3_17_2_1_1642874631254_5626">
    <p>Autonomous Process</p>
    <p>A process not under direct human control</p>
  </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1642860757007_15348">
  <h3>a way to structure the discussion </h3><p>I see there being three different types of autonomous process - randomness, rules and natural systems. The bulk of this article will look at these three things, and examples of how each is implemented in generative art.</p><p>This is a non-perfect categorisation, with crossover and omissions which I’ll stuff in round the edges, but it is a structure for establishing the different types of autonomy, and for exploring generativity. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1642874631254_11293">

<p>
  <h2>1. Randomness</h2>
</p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643218979350_25967">
  <h3>The mainstay of generation</h3><p>Randomness drives most of what we think of as “generative art”. </p><p>An artist writes code which produces different outputs depending on the values of a set of variables. These variables could be things like the number of circles to draw, their positions, what size they are, what colour they are, and so on. </p><p>For example, in the following (work in progress) images, variables define the positions of a collection of nodes, the number of nodes, how many other nodes they connect to and the colours and thickness of the connecting lines.</p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643570775601_86713">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/aca6cd42-80cc-4379-acca-1ea1e018c55b/geometricconstellation1.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/aca6cd42-80cc-4379-acca-1ea1e018c55b/geometricconstellation1.png" data-image-dimensions="754x754" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/aca6cd42-80cc-4379-acca-1ea1e018c55b/geometricconstellation1.png" width="754" height="754" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/aca6cd42-80cc-4379-acca-1ea1e018c55b/geometricconstellation1.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/aca6cd42-80cc-4379-acca-1ea1e018c55b/geometricconstellation1.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/aca6cd42-80cc-4379-acca-1ea1e018c55b/geometricconstellation1.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/aca6cd42-80cc-4379-acca-1ea1e018c55b/geometricconstellation1.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/aca6cd42-80cc-4379-acca-1ea1e018c55b/geometricconstellation1.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/aca6cd42-80cc-4379-acca-1ea1e018c55b/geometricconstellation1.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/aca6cd42-80cc-4379-acca-1ea1e018c55b/geometricconstellation1.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643570775601_89711">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b85ae050-1450-40a9-81bb-df5ac84eb32b/geometricconstellation5.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b85ae050-1450-40a9-81bb-df5ac84eb32b/geometricconstellation5.png" data-image-dimensions="754x754" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b85ae050-1450-40a9-81bb-df5ac84eb32b/geometricconstellation5.png" width="754" height="754" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b85ae050-1450-40a9-81bb-df5ac84eb32b/geometricconstellation5.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b85ae050-1450-40a9-81bb-df5ac84eb32b/geometricconstellation5.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b85ae050-1450-40a9-81bb-df5ac84eb32b/geometricconstellation5.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b85ae050-1450-40a9-81bb-df5ac84eb32b/geometricconstellation5.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b85ae050-1450-40a9-81bb-df5ac84eb32b/geometricconstellation5.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b85ae050-1450-40a9-81bb-df5ac84eb32b/geometricconstellation5.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b85ae050-1450-40a9-81bb-df5ac84eb32b/geometricconstellation5.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643570775601_104308">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/177bb10d-7de3-40cc-bd2a-6644e9fe538d/geometricconstellation2.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/177bb10d-7de3-40cc-bd2a-6644e9fe538d/geometricconstellation2.png" data-image-dimensions="754x754" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/177bb10d-7de3-40cc-bd2a-6644e9fe538d/geometricconstellation2.png" width="754" height="754" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/177bb10d-7de3-40cc-bd2a-6644e9fe538d/geometricconstellation2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/177bb10d-7de3-40cc-bd2a-6644e9fe538d/geometricconstellation2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/177bb10d-7de3-40cc-bd2a-6644e9fe538d/geometricconstellation2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/177bb10d-7de3-40cc-bd2a-6644e9fe538d/geometricconstellation2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/177bb10d-7de3-40cc-bd2a-6644e9fe538d/geometricconstellation2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/177bb10d-7de3-40cc-bd2a-6644e9fe538d/geometricconstellation2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/177bb10d-7de3-40cc-bd2a-6644e9fe538d/geometricconstellation2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643570775601_101166">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/808c8976-36a6-4689-a92e-d45f8671abcd/geometricconstellation6.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/808c8976-36a6-4689-a92e-d45f8671abcd/geometricconstellation6.png" data-image-dimensions="754x754" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/808c8976-36a6-4689-a92e-d45f8671abcd/geometricconstellation6.png" width="754" height="754" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/808c8976-36a6-4689-a92e-d45f8671abcd/geometricconstellation6.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/808c8976-36a6-4689-a92e-d45f8671abcd/geometricconstellation6.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/808c8976-36a6-4689-a92e-d45f8671abcd/geometricconstellation6.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/808c8976-36a6-4689-a92e-d45f8671abcd/geometricconstellation6.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/808c8976-36a6-4689-a92e-d45f8671abcd/geometricconstellation6.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/808c8976-36a6-4689-a92e-d45f8671abcd/geometricconstellation6.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/808c8976-36a6-4689-a92e-d45f8671abcd/geometricconstellation6.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1642874631254_7170">
  <p>Of course we <em>could</em> define those variables’ values in the code, or we <em>could </em>manually drag sliders up and down to alter them - but these would be very human controlled, non-autonomous processes. It would also be tedious to manually define the properties of each of the nodes when there are thousands of them. </p><p>Instead, to create a <em>generative </em>artwork, we can use a function like random() to generate the values. Different images can be generated every time the code is run, producing a variety of outputs. </p><p>Vera Molnar, a pioneer of generative art, made extensive use of randomness in her work, and <a href="https://vimeo.com/372579247">this clip</a> of her talking on the topic is a delight. </p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643552402403_123923">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72038821-e73f-4b66-8d7b-e947219ce4c5/VeraMolnar-Interruptions.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72038821-e73f-4b66-8d7b-e947219ce4c5/VeraMolnar-Interruptions.jpg" data-image-dimensions="2000x1896" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72038821-e73f-4b66-8d7b-e947219ce4c5/VeraMolnar-Interruptions.jpg" width="2000" height="1896" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72038821-e73f-4b66-8d7b-e947219ce4c5/VeraMolnar-Interruptions.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72038821-e73f-4b66-8d7b-e947219ce4c5/VeraMolnar-Interruptions.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72038821-e73f-4b66-8d7b-e947219ce4c5/VeraMolnar-Interruptions.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72038821-e73f-4b66-8d7b-e947219ce4c5/VeraMolnar-Interruptions.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72038821-e73f-4b66-8d7b-e947219ce4c5/VeraMolnar-Interruptions.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72038821-e73f-4b66-8d7b-e947219ce4c5/VeraMolnar-Interruptions.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72038821-e73f-4b66-8d7b-e947219ce4c5/VeraMolnar-Interruptions.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643218979350_26716">
  <h3>Pseudorandom</h3><p>Something that’s super important in a conceptual and technical sense is that, in fact, it’s usually not randomness at all, it’s pseudorandomness. </p><p>Computers generally have no mechanism by which to pick a truly random number, but we can generate numbers which are usually sufficiently random for our purposes as artists using a pseudorandom number generator (prng). </p><p>A prng works by taking an initial value, known as a “seed”, and running it through an algorithm to produce a result. The seed value is often taken from something like the timestamp on the computer. </p><p>If the algorithm is run 10,000 times with 10,000 different seeds, it should produce a set of results with an even distribution from 0 to 1. This p5js code and output shows 10,000 squares which are evenly distributed across the canvas, as their x,y positions were generated using the <strong>random()</strong> function. </p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-aspect-ratio="100" data-block-type="5" id="block-yui_3_17_2_1_1643218979350_47227">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/24ef432c-e618-48c2-861a-cbb32e9f0a95/prng-code.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/24ef432c-e618-48c2-861a-cbb32e9f0a95/prng-code.png" data-image-dimensions="600x600" data-image-focal-point="0.5,0.5" alt="Code using random() in p5js" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/24ef432c-e618-48c2-861a-cbb32e9f0a95/prng-code.png" width="600" height="600" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/24ef432c-e618-48c2-861a-cbb32e9f0a95/prng-code.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/24ef432c-e618-48c2-861a-cbb32e9f0a95/prng-code.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/24ef432c-e618-48c2-861a-cbb32e9f0a95/prng-code.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/24ef432c-e618-48c2-861a-cbb32e9f0a95/prng-code.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/24ef432c-e618-48c2-861a-cbb32e9f0a95/prng-code.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/24ef432c-e618-48c2-861a-cbb32e9f0a95/prng-code.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/24ef432c-e618-48c2-861a-cbb32e9f0a95/prng-code.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643218979350_49238">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/554b31f6-09e2-4457-ae9a-af8245efc62b/prng.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/554b31f6-09e2-4457-ae9a-af8245efc62b/prng.png" data-image-dimensions="600x600" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/554b31f6-09e2-4457-ae9a-af8245efc62b/prng.png" width="600" height="600" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/554b31f6-09e2-4457-ae9a-af8245efc62b/prng.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/554b31f6-09e2-4457-ae9a-af8245efc62b/prng.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/554b31f6-09e2-4457-ae9a-af8245efc62b/prng.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/554b31f6-09e2-4457-ae9a-af8245efc62b/prng.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/554b31f6-09e2-4457-ae9a-af8245efc62b/prng.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/554b31f6-09e2-4457-ae9a-af8245efc62b/prng.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/554b31f6-09e2-4457-ae9a-af8245efc62b/prng.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643218979350_47565">
  <p>The seed can also be set to a fixed number, for testing or for reproducible results. In NFTs on sites like <a href="https://www.fxhash.xyz/u/Amy%20Goodchild">fxhash</a> and <a href="https://artblocks.io/">ArtBlocks</a>, a hash (string of characters) from the blockchain transaction is used as the seed for the randomness in the piece. </p><p>When the algorithm is given the same seed, it produces the same results each time.  This process is therefore deterministic, not really random at all. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643239458609_25673">
  <h3>True Random</h3><p>In some applications like banking and cryptography, it’s necessary to produce truly random numbers. This can be done by leveraging a natural source of chaotic data, for example, <a href="https://www.random.org/">atmospheric noise</a>, picked up by a radio or visual output of <a href="https://en.wikipedia.org/wiki/Lavarand">lava lamps</a>.</p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643239458609_34271">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/bfe85349-159e-4afb-bdfa-101eeb1727a9/thunderstorm.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/bfe85349-159e-4afb-bdfa-101eeb1727a9/thunderstorm.jpg" data-image-dimensions="600x600" data-image-focal-point="0.5,0.5" alt="Lightning in storm clouds" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/bfe85349-159e-4afb-bdfa-101eeb1727a9/thunderstorm.jpg" width="600" height="600" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/bfe85349-159e-4afb-bdfa-101eeb1727a9/thunderstorm.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/bfe85349-159e-4afb-bdfa-101eeb1727a9/thunderstorm.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/bfe85349-159e-4afb-bdfa-101eeb1727a9/thunderstorm.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/bfe85349-159e-4afb-bdfa-101eeb1727a9/thunderstorm.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/bfe85349-159e-4afb-bdfa-101eeb1727a9/thunderstorm.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/bfe85349-159e-4afb-bdfa-101eeb1727a9/thunderstorm.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/bfe85349-159e-4afb-bdfa-101eeb1727a9/thunderstorm.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-aspect-ratio="100" data-block-type="5" id="block-yui_3_17_2_1_1643239458609_40593">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d3f351a9-aee5-46ab-aaac-9d7e4e60c87b/lavalamps.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d3f351a9-aee5-46ab-aaac-9d7e4e60c87b/lavalamps.jpg" data-image-dimensions="600x600" data-image-focal-point="0.5,0.5" alt="Row of lava lamps on a shelf" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d3f351a9-aee5-46ab-aaac-9d7e4e60c87b/lavalamps.jpg" width="600" height="600" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d3f351a9-aee5-46ab-aaac-9d7e4e60c87b/lavalamps.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d3f351a9-aee5-46ab-aaac-9d7e4e60c87b/lavalamps.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d3f351a9-aee5-46ab-aaac-9d7e4e60c87b/lavalamps.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d3f351a9-aee5-46ab-aaac-9d7e4e60c87b/lavalamps.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d3f351a9-aee5-46ab-aaac-9d7e4e60c87b/lavalamps.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d3f351a9-aee5-46ab-aaac-9d7e4e60c87b/lavalamps.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d3f351a9-aee5-46ab-aaac-9d7e4e60c87b/lavalamps.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643241547041_21584">
  <p>Producing truly random numbers like this is much more faff than simply typing random(), but it could be interesting conceptually to explore using a truly random number source in a generative art project. </p><p>How much difference does it make to the final piece if the number is random or pseudorandom? Does it make any difference at all? </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643241547041_46587">
  <h3>A philosophical sidebar</h3><p>We can say a pseudorandom number generator is deterministic and predictable, while numbers generated from a natural source are chaotic and unpredictable.</p><p>In fact though, the latter is only unpredictable if the universe is. If we live in a deterministic universe, where every event is determined by prior events, then theoretically there exists no source of true randomness. Quantum mechanics suggests this is not the case, as subatomic particles can be observed to behave in ways that, as far as we can tell, are truly random and have no cause. However, the debate on <a href="https://en.wikipedia.org/wiki/Determinism">determinism</a> has not reached consensus. </p><p>I’d like to explore this more in a future blog as there are some mysterious conceptual threads to pull at. </p><p>In the meantime - we can certainly say that in practical terms natural sources of chaotic data like atmospheric noise are unpredictable, and <a href="https://phys.org/news/2021-01-unpredictable-nature-quantum-mechanics-random.html">quantum</a> <a href="https://qrng.anu.edu.au/">random</a> <a href="https://www.nature.com/articles/s41598-021-95388-7">number</a> <a href="https://www.wired.com/story/quantum-mechanics-could-solve-cryptographys-random-number-problem/">generators</a> are possibly even theoretically unpredictable. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643293789257_18889">
  <h3>Distribution</h3><p>A good prng should give an even distribution of results but for some artistic applications it can be useful to use mathematical functions to skew those results to an <em>uneven </em>distribution. </p><p><a href="https://editor.p5js.org/amygoodchild/sketches/xHSIoZDbq">This sketch</a> I made shows some examples, using functions from <a href="https://easings.net/">easings.net</a> (that are actually intended for animation). </p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643293789257_24767">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/2703915f-20ea-4f2d-a44a-a3688cee1502/randomdistributions.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/2703915f-20ea-4f2d-a44a-a3688cee1502/randomdistributions.png" data-image-dimensions="926x702" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/2703915f-20ea-4f2d-a44a-a3688cee1502/randomdistributions.png" width="926" height="702" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/2703915f-20ea-4f2d-a44a-a3688cee1502/randomdistributions.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/2703915f-20ea-4f2d-a44a-a3688cee1502/randomdistributions.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/2703915f-20ea-4f2d-a44a-a3688cee1502/randomdistributions.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/2703915f-20ea-4f2d-a44a-a3688cee1502/randomdistributions.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/2703915f-20ea-4f2d-a44a-a3688cee1502/randomdistributions.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/2703915f-20ea-4f2d-a44a-a3688cee1502/randomdistributions.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/2703915f-20ea-4f2d-a44a-a3688cee1502/randomdistributions.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643293789257_27368">
  <p>Here’s a quick sketch that shows how effective and useful that can be. </p><p>The following image is made up of 30,000 low opacity squares. Their x positions are generated randomly and evenly distributed over the width of the canvas. Their y positions are generated randomly but the distribution is skewed towards the top of the canvas using the <strong>easeInCubic()</strong> function. This creates a gradient in the output. </p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643293789257_29366">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8a87b48f-d71a-4147-9c9d-90e56cf5f278/skewdistribution.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8a87b48f-d71a-4147-9c9d-90e56cf5f278/skewdistribution.png" data-image-dimensions="600x600" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8a87b48f-d71a-4147-9c9d-90e56cf5f278/skewdistribution.png" width="600" height="600" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8a87b48f-d71a-4147-9c9d-90e56cf5f278/skewdistribution.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8a87b48f-d71a-4147-9c9d-90e56cf5f278/skewdistribution.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8a87b48f-d71a-4147-9c9d-90e56cf5f278/skewdistribution.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8a87b48f-d71a-4147-9c9d-90e56cf5f278/skewdistribution.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8a87b48f-d71a-4147-9c9d-90e56cf5f278/skewdistribution.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8a87b48f-d71a-4147-9c9d-90e56cf5f278/skewdistribution.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8a87b48f-d71a-4147-9c9d-90e56cf5f278/skewdistribution.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643295861142_21657">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/10abb5ff-31a7-4a6c-af3a-8f4afa4d08f2/skewdistributioncode.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/10abb5ff-31a7-4a6c-af3a-8f4afa4d08f2/skewdistributioncode.png" data-image-dimensions="600x600" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/10abb5ff-31a7-4a6c-af3a-8f4afa4d08f2/skewdistributioncode.png" width="600" height="600" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/10abb5ff-31a7-4a6c-af3a-8f4afa4d08f2/skewdistributioncode.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/10abb5ff-31a7-4a6c-af3a-8f4afa4d08f2/skewdistributioncode.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/10abb5ff-31a7-4a6c-af3a-8f4afa4d08f2/skewdistributioncode.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/10abb5ff-31a7-4a6c-af3a-8f4afa4d08f2/skewdistributioncode.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/10abb5ff-31a7-4a6c-af3a-8f4afa4d08f2/skewdistributioncode.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/10abb5ff-31a7-4a6c-af3a-8f4afa4d08f2/skewdistributioncode.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/10abb5ff-31a7-4a6c-af3a-8f4afa4d08f2/skewdistributioncode.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643293789257_20555">
  <h3>Other Algorithms</h3><p>An alternative to pseudorandom number generators is a procedural generator like Perlin noise. Where a prng gives evenly distributed but disparate results, Perlin noise produces sequentially similar results. </p><p>In the following image, there are two sets of dots, each evenly spaced across the width of the canvas. The y-positions of the top set are generated randomly. The y-positions of the bottom set are generated using Perlin noise.  </p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643295861142_46540">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/c495ccfe-9f96-4a1d-adb8-057b06c974c8/noise.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/c495ccfe-9f96-4a1d-adb8-057b06c974c8/noise.png" data-image-dimensions="600x600" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/c495ccfe-9f96-4a1d-adb8-057b06c974c8/noise.png" width="600" height="600" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/c495ccfe-9f96-4a1d-adb8-057b06c974c8/noise.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/c495ccfe-9f96-4a1d-adb8-057b06c974c8/noise.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/c495ccfe-9f96-4a1d-adb8-057b06c974c8/noise.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/c495ccfe-9f96-4a1d-adb8-057b06c974c8/noise.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/c495ccfe-9f96-4a1d-adb8-057b06c974c8/noise.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/c495ccfe-9f96-4a1d-adb8-057b06c974c8/noise.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/c495ccfe-9f96-4a1d-adb8-057b06c974c8/noise.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643295861142_48992">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0444517e-811f-4d1f-a3b6-b8769ff703a8/noisecode.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0444517e-811f-4d1f-a3b6-b8769ff703a8/noisecode.png" data-image-dimensions="600x600" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0444517e-811f-4d1f-a3b6-b8769ff703a8/noisecode.png" width="600" height="600" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0444517e-811f-4d1f-a3b6-b8769ff703a8/noisecode.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0444517e-811f-4d1f-a3b6-b8769ff703a8/noisecode.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0444517e-811f-4d1f-a3b6-b8769ff703a8/noisecode.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0444517e-811f-4d1f-a3b6-b8769ff703a8/noisecode.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0444517e-811f-4d1f-a3b6-b8769ff703a8/noisecode.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0444517e-811f-4d1f-a3b6-b8769ff703a8/noisecode.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0444517e-811f-4d1f-a3b6-b8769ff703a8/noisecode.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643295861142_57389">

<p>Noise is an effective way to create organic looking movement and shapes. Here are two of my pieces from <a href="https://genuary.art/">Genuary 2022</a> which rely heavily on noise to generate the values for their variables. </p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643819102305_134810">
  <h3>Last thoughts about Randomness</h3><p>There are significant philosophical questions that surround randomness and this area is filled with possibility for conceptual work. </p><p>Even without considering these theoretical ideas, pseudorandomness and procedural number generators like Perlin noise provide us with easy but powerful ways to produce effective and varied results. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643241547041_17761">

<p>
  <h2>2. Rules</h2>
</p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643300717638_24163">
  <h3>Instructions For Art</h3><p>Sol LeWitt’s work is the quintessential example of one type of rule based art. Some of his pieces comprise a list of instructions, rather than the results of those instructions. </p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643300717638_33212">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9e83f612-91eb-4346-bc56-86e0b24b9b81/lewitt-instructions-1-571x790.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9e83f612-91eb-4346-bc56-86e0b24b9b81/lewitt-instructions-1-571x790.jpg" data-image-dimensions="571x790" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9e83f612-91eb-4346-bc56-86e0b24b9b81/lewitt-instructions-1-571x790.jpg" width="571" height="790" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9e83f612-91eb-4346-bc56-86e0b24b9b81/lewitt-instructions-1-571x790.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9e83f612-91eb-4346-bc56-86e0b24b9b81/lewitt-instructions-1-571x790.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9e83f612-91eb-4346-bc56-86e0b24b9b81/lewitt-instructions-1-571x790.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9e83f612-91eb-4346-bc56-86e0b24b9b81/lewitt-instructions-1-571x790.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9e83f612-91eb-4346-bc56-86e0b24b9b81/lewitt-instructions-1-571x790.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9e83f612-91eb-4346-bc56-86e0b24b9b81/lewitt-instructions-1-571x790.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9e83f612-91eb-4346-bc56-86e0b24b9b81/lewitt-instructions-1-571x790.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>Sol LeWitt - <em>Wall Drawing #118 </em>(1967)</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643300717638_44459">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/237a6ce7-78bb-4a4a-8edc-7a71bf42506c/proposal-for-wall-drawing-information-show.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/237a6ce7-78bb-4a4a-8edc-7a71bf42506c/proposal-for-wall-drawing-information-show.jpg" data-image-dimensions="553x800" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/237a6ce7-78bb-4a4a-8edc-7a71bf42506c/proposal-for-wall-drawing-information-show.jpg" width="553" height="800" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/237a6ce7-78bb-4a4a-8edc-7a71bf42506c/proposal-for-wall-drawing-information-show.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/237a6ce7-78bb-4a4a-8edc-7a71bf42506c/proposal-for-wall-drawing-information-show.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/237a6ce7-78bb-4a4a-8edc-7a71bf42506c/proposal-for-wall-drawing-information-show.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/237a6ce7-78bb-4a4a-8edc-7a71bf42506c/proposal-for-wall-drawing-information-show.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/237a6ce7-78bb-4a4a-8edc-7a71bf42506c/proposal-for-wall-drawing-information-show.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/237a6ce7-78bb-4a4a-8edc-7a71bf42506c/proposal-for-wall-drawing-information-show.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/237a6ce7-78bb-4a4a-8edc-7a71bf42506c/proposal-for-wall-drawing-information-show.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>Sol LeWitt - <em>Proposal for Wall Drawing</em> (1970)</p>
          </figcaption>
        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643300717638_39280">

<p>LeWitt’s instructions have been implemented many times over. </p>




















  
  



</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643300717638_71062">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a86f048f-bcd0-4455-91b5-f6e654ad19f4/sol+lewitt+1.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a86f048f-bcd0-4455-91b5-f6e654ad19f4/sol+lewitt+1.png" data-image-dimensions="800x800" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a86f048f-bcd0-4455-91b5-f6e654ad19f4/sol+lewitt+1.png" width="800" height="800" sizes="(max-width: 640px) 100vw, (max-width: 767px) 33.33333333333333vw, 33.33333333333333vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a86f048f-bcd0-4455-91b5-f6e654ad19f4/sol+lewitt+1.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a86f048f-bcd0-4455-91b5-f6e654ad19f4/sol+lewitt+1.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a86f048f-bcd0-4455-91b5-f6e654ad19f4/sol+lewitt+1.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a86f048f-bcd0-4455-91b5-f6e654ad19f4/sol+lewitt+1.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a86f048f-bcd0-4455-91b5-f6e654ad19f4/sol+lewitt+1.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a86f048f-bcd0-4455-91b5-f6e654ad19f4/sol+lewitt+1.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a86f048f-bcd0-4455-91b5-f6e654ad19f4/sol+lewitt+1.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>Genuary Day 7 - Sol LeWitt Wall Drawing</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643300717638_73433">

      

      
        <figure>
          
        
        

        
          <a href="https://igorkorenfeld.com/code/sol-lewitt/">
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dafd205f-ad1f-4de0-8940-52f81cba53c5/sol+lewitt+2.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dafd205f-ad1f-4de0-8940-52f81cba53c5/sol+lewitt+2.png" data-image-dimensions="800x800" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dafd205f-ad1f-4de0-8940-52f81cba53c5/sol+lewitt+2.png" width="800" height="800" sizes="(max-width: 640px) 100vw, (max-width: 767px) 33.33333333333333vw, 33.33333333333333vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dafd205f-ad1f-4de0-8940-52f81cba53c5/sol+lewitt+2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dafd205f-ad1f-4de0-8940-52f81cba53c5/sol+lewitt+2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dafd205f-ad1f-4de0-8940-52f81cba53c5/sol+lewitt+2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dafd205f-ad1f-4de0-8940-52f81cba53c5/sol+lewitt+2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dafd205f-ad1f-4de0-8940-52f81cba53c5/sol+lewitt+2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dafd205f-ad1f-4de0-8940-52f81cba53c5/sol+lewitt+2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dafd205f-ad1f-4de0-8940-52f81cba53c5/sol+lewitt+2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          </a>
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643819102305_66030">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b92e08ef-027c-4492-b960-c0898330572f/sollewitt.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b92e08ef-027c-4492-b960-c0898330572f/sollewitt.jpg" data-image-dimensions="768x614" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b92e08ef-027c-4492-b960-c0898330572f/sollewitt.jpg" width="768" height="614" sizes="(max-width: 640px) 100vw, (max-width: 767px) 33.33333333333333vw, 33.33333333333333vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b92e08ef-027c-4492-b960-c0898330572f/sollewitt.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b92e08ef-027c-4492-b960-c0898330572f/sollewitt.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b92e08ef-027c-4492-b960-c0898330572f/sollewitt.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b92e08ef-027c-4492-b960-c0898330572f/sollewitt.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b92e08ef-027c-4492-b960-c0898330572f/sollewitt.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b92e08ef-027c-4492-b960-c0898330572f/sollewitt.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b92e08ef-027c-4492-b960-c0898330572f/sollewitt.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643311768408_27083">
  <p>The instructions are simultaneously prescriptive and ambiguous. When the executor of the instructions is a human, they are able to follow or ignore parts of the instructions at will. </p><p>Some control over the output lies with LeWitt and some lies with whoever (or whatever) executes the instructions. This is analogous with the way that, in much generative art, some control lies with the artist and some with the machine and algorithms. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643575188542_69300">
  <h3>Following instructions</h3><p>In Studio Moniker’s project <a href="https://studiomoniker.com/projects/red-follows-yellow-follows-blue-follows-red"><em>Red Follows Yellow Follows Blue Follows Red</em></a>, a group of participants wear coloured capes and headphones, on which they are given instructions. For example the participants in a red cape might be told to “follow yellow but avoid blue” while participants in blue are told “follow red but avoid yellow”. </p><p>In this very human experiment, most participants follow the instructions closely but occasionally individuals can be seen to misinterpret or ignore them to varying degrees, shifting the balance of control between the artists and the participants. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643575188542_89167">

<p>As each person behaves under influence of the instructions they’re given individually, patterns emerge across the whole group. </p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643218979350_15434">
  <h3>Emergence</h3><p>When individual components of a system each have their own properties and behaviours, unexpected and interesting results can emerge at the system level. </p><p>Emergence is prevalent throughout nature and can be observed in weather systems, ant colonies, flocks of birds, convection and more. We can generate our own emergent systems by creating rules for individual elements. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643218979350_11721">
  <h3>Cellular Automata</h3><p>Cellular automata are a well-known emergent system, seen in examples such as Conway’s Game of Life and Wolfram’s rules. </p><p>Conway’s Game of Life plays out on a grid of cells, each of which is set to live or dead for the first turn (perhaps randomly, or in some pattern). For each proceeding turn, each cell looks at its 8 neighbours and determines its own new state according to these rules: </p><ol data-rte-list="default"><li><p>Any live cell with fewer than two live neighbours dies, as if caused by underpopulation.</p></li><li><p>Any live cell with two or three live neighbours lives on to the next turn.</p></li><li><p>Any live cell with more than three live neighbours dies, as if by overpopulation.</p></li><li><p>Any dead cell with exactly three live neighbours becomes a live cell, as if by reproduction.</p></li></ol><p>This results in surprisingly complex patterns across the whole grid, which are also determined by the starting pattern.</p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643218979350_21929">
  <p>You can play with my Conway’s Game of Life implementation <a href="https://editor.p5js.org/amygoodchild/sketches/RH_6RNRxs">here</a>. Be sure to click the play button in the top left to start, and check out the controls at the top of the code. </p><p>These systems can be made even more complex and varied by increasing the number of possible states, changing the neighbourhood size and shape and playing with different sets of rules. When the system has multiple states and loops around in a cycle, like 0, 1, 2, 3, 0, 1, 2… it is called a Cyclic Cellular Automaton. </p><p>Here’s one I implemented recently:</p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643218979350_16596">
  <h3>Simulated Ecosystems</h3><p>We can also break out of the grid into more versatile emergent systems made up of moving creatures. Each creature has their own behaviours, which are often inspired by real life, emulating flocking, reproduction, growth, consumption, death and more.</p><p>Here are a couple of examples of systems of creatures I’ve developed with different features. <br></p>
</div><div data-block-type="23" id="block-yui_3_17_2_1_1643376542295_77622">
  <div>
    
  <p>Three elements displaying flocking, avoidance, reproduction, repulsion and consumption.</p>
  </div>
  <div>
    
    <p>These creatures move in a flock and generate sounds when they come close together</p>
  </div>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643552402403_83732">

<p>Interdependence between members of an ecosystem like this is something I plan to explore more in my work. When we incorporate birth and death of creatures, necessity of food sources and threat of predators, we can observe an ecosystem thrive or die out. </p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643384043768_29754">
  <h3>Other simulations</h3><p>Commonly, artists simulate phenomena from the real world, with algorithms based on things like reaction diffusion, diffusion limited aggregation, forces, snowflakes, tree growth, slime mould, to name just a few. </p><p>Many complex real world systems and phenomena can be modelled by simulating the rules which govern their composite parts, and the variables which affect them.</p>
</div><div data-block-type="23" id="block-yui_3_17_2_1_1643384043768_45129">
  <div>
    
    <p>Diffusion limited aggregation simulation by <a href="https://www.instagram.com/andylomasart/">Andy Lomas</a></p>
    
  </div>
  <div>
    
    <p>Tree growth algorithm from my project <a href="https://www.fxhash.xyz/generative/1138">Propagate</a></p>   
  </div>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643575188542_124495">
  <p>Simulations can vary in their level of abstraction, in a way that could be considered a gradient from art to science, with some works being loosely inspired by something from nature, while others seek to closely model and investigate the real world. </p><p>Consider these two sets of snowflakes. The first image shows outputs from <a href="https://www.misha.studio/snowflaker/">Snowflaker</a>, which generates simple and pleasing snowflake svgs. The second shows <a href="https://blogs.scientificamerican.com/sa-visual/in-silico-flurries/">Gravner-Griffeath</a> snowflakes (interactive example <a href="https://cruzgodar.com/applets/gravner-griffeath-snowflakes/gravner-griffeath-snowflakes.html">here</a>), which are generated by accurately modelling crystal growth under different conditions. </p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643727805454_44809">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d4af34b4-6c30-4cf7-abe5-305bcaf9cd06/snowflakes1.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d4af34b4-6c30-4cf7-abe5-305bcaf9cd06/snowflakes1.png" data-image-dimensions="700x700" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d4af34b4-6c30-4cf7-abe5-305bcaf9cd06/snowflakes1.png" width="700" height="700" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d4af34b4-6c30-4cf7-abe5-305bcaf9cd06/snowflakes1.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d4af34b4-6c30-4cf7-abe5-305bcaf9cd06/snowflakes1.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d4af34b4-6c30-4cf7-abe5-305bcaf9cd06/snowflakes1.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d4af34b4-6c30-4cf7-abe5-305bcaf9cd06/snowflakes1.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d4af34b4-6c30-4cf7-abe5-305bcaf9cd06/snowflakes1.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d4af34b4-6c30-4cf7-abe5-305bcaf9cd06/snowflakes1.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d4af34b4-6c30-4cf7-abe5-305bcaf9cd06/snowflakes1.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643727805454_47573">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a5dc260d-739f-4957-86d0-a0dc1b7cda1d/snowflakes2.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a5dc260d-739f-4957-86d0-a0dc1b7cda1d/snowflakes2.png" data-image-dimensions="700x700" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a5dc260d-739f-4957-86d0-a0dc1b7cda1d/snowflakes2.png" width="700" height="700" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a5dc260d-739f-4957-86d0-a0dc1b7cda1d/snowflakes2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a5dc260d-739f-4957-86d0-a0dc1b7cda1d/snowflakes2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a5dc260d-739f-4957-86d0-a0dc1b7cda1d/snowflakes2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a5dc260d-739f-4957-86d0-a0dc1b7cda1d/snowflakes2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a5dc260d-739f-4957-86d0-a0dc1b7cda1d/snowflakes2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a5dc260d-739f-4957-86d0-a0dc1b7cda1d/snowflakes2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a5dc260d-739f-4957-86d0-a0dc1b7cda1d/snowflakes2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643384043768_40174">
  <h3>Mathematics </h3><p>Mathematics is another major player in rule based systems. Geometry, and trigonometry in particular, are necessary for a lot of things I make, with sohcahtoa  popping up all over the place and making me regret my high school eyerolls. </p><p>Maths really sings though, in Dave Whyte’s work:<br></p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643552402403_101476">

<p>I recommend <a href="https://www.goodreads.com/book/show/41739506-math-art">this book</a> if you are interested to investigate mathematical algorithms that lend themselves to visual art. </p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643570775601_38397">
  <h3>Algorithm is the idea</h3><p>Some ideas come to me in a format of “what if it did this, and then this, and then that… what would it look like?” The idea <em>is</em> the algorithm - a list of steps. </p><p>In the following pen-plotted works from Casey Reas’ series <a href="https://reas.com/rgb_3/"><em>RGB-3</em></a>, the three angles in the titles describe the angles of the red, green and blue lines. Some areas of lines are skipped, creating areas of different crossover effects. </p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643570775601_42633">

      

      
        <figure>
          
        
        

        
          <a href="https://reas.com/rgb_3/">
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ad7e1a0-1f98-4c74-8ef6-ade896150e21/caseyreas7.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ad7e1a0-1f98-4c74-8ef6-ade896150e21/caseyreas7.jpg" data-image-dimensions="1210x807" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ad7e1a0-1f98-4c74-8ef6-ade896150e21/caseyreas7.jpg" width="1210" height="807" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ad7e1a0-1f98-4c74-8ef6-ade896150e21/caseyreas7.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ad7e1a0-1f98-4c74-8ef6-ade896150e21/caseyreas7.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ad7e1a0-1f98-4c74-8ef6-ade896150e21/caseyreas7.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ad7e1a0-1f98-4c74-8ef6-ade896150e21/caseyreas7.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ad7e1a0-1f98-4c74-8ef6-ade896150e21/caseyreas7.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ad7e1a0-1f98-4c74-8ef6-ade896150e21/caseyreas7.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ad7e1a0-1f98-4c74-8ef6-ade896150e21/caseyreas7.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          </a>
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643570775601_45697">

      

      
        <figure>
          
        
        

        
          <a href="https://reas.com/rgb_3/">
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/020ef388-178c-4870-b7cf-c139ac8fbebc/caseyreas8.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/020ef388-178c-4870-b7cf-c139ac8fbebc/caseyreas8.jpg" data-image-dimensions="1210x807" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/020ef388-178c-4870-b7cf-c139ac8fbebc/caseyreas8.jpg" width="1210" height="807" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/020ef388-178c-4870-b7cf-c139ac8fbebc/caseyreas8.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/020ef388-178c-4870-b7cf-c139ac8fbebc/caseyreas8.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/020ef388-178c-4870-b7cf-c139ac8fbebc/caseyreas8.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/020ef388-178c-4870-b7cf-c139ac8fbebc/caseyreas8.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/020ef388-178c-4870-b7cf-c139ac8fbebc/caseyreas8.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/020ef388-178c-4870-b7cf-c139ac8fbebc/caseyreas8.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/020ef388-178c-4870-b7cf-c139ac8fbebc/caseyreas8.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          </a>
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div></div><div data-block-type="51" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1684334936847_129054">
  <form data-form-id="6464ec811f98c13d48916eef" autocomplete="on" method="POST" novalidate="" onsubmit="return (function (form) {
    Y.use('squarespace-form-submit', 'node', function usingFormSubmit(Y) {
      (new Y.Squarespace.FormSubmit(form)).submit({
        formId: '6464ec811f98c13d48916eef',
        collectionId: '5f37d09b551f154577577f01',
        objectName: 'item-61ec10d4138e9c5d9dda3279'
      });
    });
    return false;
  })(this);">
    
    
    <p>We respect your privacy.</p>
    <p>Thank you!</p>
    
  </form>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643552402403_37801">
  <h3>Last thoughts about rules</h3><p>In this article and in my work so far I have only scratched the surface of what is possible with rule based generation. </p><p>Some key things I wanted to get across from this section: </p><ul data-rte-list="default"><li><p>There are lots of different ways to set and implement rules.</p></li><li><p>Rules applied at an individual level can result in unexpected emergent phenomena at a system level, and it’s often necessary to actually run the rules to know the result. </p></li><li><p>Rules that govern behaviour are prevalent in natural systems. </p></li><li><p>In generative art there is often a balance of control between the rule setter and the rule follower, or the artist and the machine. </p></li></ul>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643552402403_89921">

<p>
  <h2>3. Natural systems </h2>
</p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643552402403_93584">
  <h3>Origins</h3><p>Light dappling through trees, water bubbling in a pan, birds flocking, ice forming on a windshield… we are surrounded with natural phenomena that generate aesthetic and interesting results. </p><p>In the first section I talked about how true randomness can only be found in a natural source and in the second section I talked about how generative artists sometimes seek to model natural phenomena. </p><p>Some artists go straight to the source and elevate a natural system to the artwork itself.</p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643727805454_163355">
  <h3>The system is the art</h3><p>An institutional example of this kind of art is Hans Haacke’s <a href="https://www.tate.org.uk/art/artworks/haacke-condensation-cube-t13214"><em>Condensation Cube</em></a>. Materially, the work is a sealed Perspex box, containing a small amount of water. Condensation forms and runs down the inner walls of the box. The work interacts with its environment and the results are affected by ambient light and temperature. </p><p>The natural processes and the interaction of these physical and biological systems are the artwork. </p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643727805454_195467">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/64285468-6671-47ea-82fa-38dfa0214778/cube1.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/64285468-6671-47ea-82fa-38dfa0214778/cube1.jpg" data-image-dimensions="750x896" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/64285468-6671-47ea-82fa-38dfa0214778/cube1.jpg" width="750" height="896" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/64285468-6671-47ea-82fa-38dfa0214778/cube1.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/64285468-6671-47ea-82fa-38dfa0214778/cube1.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/64285468-6671-47ea-82fa-38dfa0214778/cube1.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/64285468-6671-47ea-82fa-38dfa0214778/cube1.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/64285468-6671-47ea-82fa-38dfa0214778/cube1.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/64285468-6671-47ea-82fa-38dfa0214778/cube1.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/64285468-6671-47ea-82fa-38dfa0214778/cube1.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643727805454_198435">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a7eb0fa6-8879-47f8-8b69-5369a2fcf11b/cube2.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a7eb0fa6-8879-47f8-8b69-5369a2fcf11b/cube2.jpg" data-image-dimensions="750x896" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a7eb0fa6-8879-47f8-8b69-5369a2fcf11b/cube2.jpg" width="750" height="896" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a7eb0fa6-8879-47f8-8b69-5369a2fcf11b/cube2.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a7eb0fa6-8879-47f8-8b69-5369a2fcf11b/cube2.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a7eb0fa6-8879-47f8-8b69-5369a2fcf11b/cube2.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a7eb0fa6-8879-47f8-8b69-5369a2fcf11b/cube2.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a7eb0fa6-8879-47f8-8b69-5369a2fcf11b/cube2.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a7eb0fa6-8879-47f8-8b69-5369a2fcf11b/cube2.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/a7eb0fa6-8879-47f8-8b69-5369a2fcf11b/cube2.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643727805454_175403">
  <h3>Growing art</h3><p>In <a href="https://magical-contamination.tumblr.com/"><em>Magical Contamination</em></a>, Antoine Bridier-Nahmias curates petri dishes of microorganisms. Just as Gravner-Griffeath’s snowflakes (see above) are defined by a set of modelled variables, here the artist varies the outputs by controlling conditions like oxygen levels, light and temperature, to influence the growth of the mould. </p><p>The key difference of course is that <em>Magical Contamination </em>is of the real world, being less exact, more chaotic and of higher resolution than a simulation. (Or perhaps, the simulation we consider to be the real world is…)</p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643727805454_143944">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0c70f0cb-6ff7-44fb-ab94-c3866ee124fa/mould1.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0c70f0cb-6ff7-44fb-ab94-c3866ee124fa/mould1.jpg" data-image-dimensions="750x1000" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0c70f0cb-6ff7-44fb-ab94-c3866ee124fa/mould1.jpg" width="750" height="1000" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0c70f0cb-6ff7-44fb-ab94-c3866ee124fa/mould1.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0c70f0cb-6ff7-44fb-ab94-c3866ee124fa/mould1.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0c70f0cb-6ff7-44fb-ab94-c3866ee124fa/mould1.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0c70f0cb-6ff7-44fb-ab94-c3866ee124fa/mould1.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0c70f0cb-6ff7-44fb-ab94-c3866ee124fa/mould1.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0c70f0cb-6ff7-44fb-ab94-c3866ee124fa/mould1.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0c70f0cb-6ff7-44fb-ab94-c3866ee124fa/mould1.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643727805454_146855">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/60cdb446-409d-4894-a6c7-d156f521c8a3/mould2.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/60cdb446-409d-4894-a6c7-d156f521c8a3/mould2.jpg" data-image-dimensions="750x1000" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/60cdb446-409d-4894-a6c7-d156f521c8a3/mould2.jpg" width="750" height="1000" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/60cdb446-409d-4894-a6c7-d156f521c8a3/mould2.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/60cdb446-409d-4894-a6c7-d156f521c8a3/mould2.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/60cdb446-409d-4894-a6c7-d156f521c8a3/mould2.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/60cdb446-409d-4894-a6c7-d156f521c8a3/mould2.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/60cdb446-409d-4894-a6c7-d156f521c8a3/mould2.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/60cdb446-409d-4894-a6c7-d156f521c8a3/mould2.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/60cdb446-409d-4894-a6c7-d156f521c8a3/mould2.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643819102305_112803">
  <h3>Last thoughts about Natural systems</h3><p>This section is shorter than the other two because work of this type, that considers itself generative art, is much less prevalent than other types of generative art. </p><p>I felt it warranted its own section because natural systems do influence the field so strongly, and it feels valid that randomness, rules and natural systems form a trifecta of generative art methodologies.</p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643727805454_76076">
  <h2>Other Stuff</h2><p>As I mentioned at the start, there is all kinds of crossover in my categorisation and plenty of absence as well. Let’s go through a few things I’ve missed out so far. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643735920055_52619">
  <h3>Data sources and inputs</h3><p>We’ve already talked about using a chaotic data source as a true random number generator, but what about using a non-chaotic data source? </p><p>In some work of this genre, there is a clear crossover with data visualisation, as in Aaron Koblin’s <a href="http://www.aaronkoblin.com/project/flight-patterns/"><em>Flight Patterns</em></a>, which visualises air traffic across North America. Meanwhile in Maria Takeuchi and Frederico Phillips’ film, <a href="http://www.asphyxia-project.com/"><em>Asphyxia</em></a>, a Kinect captures the movements of dancer Shiho Tanaka and uses them to a generated 3D rendered structure. Here the output is still representational of the input, but in a less data-led way.</p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643735920055_97624">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/ac754b8b-1e5a-4e06-8908-1746048b4a7a/FP_01-US.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/ac754b8b-1e5a-4e06-8908-1746048b4a7a/FP_01-US.jpg" data-image-dimensions="2400x1829" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/ac754b8b-1e5a-4e06-8908-1746048b4a7a/FP_01-US.jpg" width="2400" height="1829" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/ac754b8b-1e5a-4e06-8908-1746048b4a7a/FP_01-US.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/ac754b8b-1e5a-4e06-8908-1746048b4a7a/FP_01-US.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/ac754b8b-1e5a-4e06-8908-1746048b4a7a/FP_01-US.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/ac754b8b-1e5a-4e06-8908-1746048b4a7a/FP_01-US.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/ac754b8b-1e5a-4e06-8908-1746048b4a7a/FP_01-US.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/ac754b8b-1e5a-4e06-8908-1746048b4a7a/FP_01-US.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/ac754b8b-1e5a-4e06-8908-1746048b4a7a/FP_01-US.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643735920055_100747">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d862b25c-99ee-47e9-99c3-d461499a6def/asphyxia-2.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d862b25c-99ee-47e9-99c3-d461499a6def/asphyxia-2.jpg" data-image-dimensions="1100x836" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d862b25c-99ee-47e9-99c3-d461499a6def/asphyxia-2.jpg" width="1100" height="836" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d862b25c-99ee-47e9-99c3-d461499a6def/asphyxia-2.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d862b25c-99ee-47e9-99c3-d461499a6def/asphyxia-2.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d862b25c-99ee-47e9-99c3-d461499a6def/asphyxia-2.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d862b25c-99ee-47e9-99c3-d461499a6def/asphyxia-2.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d862b25c-99ee-47e9-99c3-d461499a6def/asphyxia-2.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d862b25c-99ee-47e9-99c3-d461499a6def/asphyxia-2.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/d862b25c-99ee-47e9-99c3-d461499a6def/asphyxia-2.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643735920055_93061">

<p>A “data source” can also be directly applied to an output in an analogue way, as in Charles Sowers’ <a href="https://www.charlessowers.com/new-page-1"><em>Windswept</em></a>, in which 612 freely rotating metal arrows are spun by the wind, resulting in patterns which happen to be visually reminiscent of Vera Molnar’s Interruptions (see above).</p>




















  
  



</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643741510783_52164">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dfc6748a-0a14-425e-be3c-dd723807ad66/windswept4.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dfc6748a-0a14-425e-be3c-dd723807ad66/windswept4.jpg" data-image-dimensions="1351x750" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dfc6748a-0a14-425e-be3c-dd723807ad66/windswept4.jpg" width="1351" height="750" sizes="(max-width: 640px) 100vw, (max-width: 767px) 58.333333333333336vw, 58.333333333333336vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dfc6748a-0a14-425e-be3c-dd723807ad66/windswept4.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dfc6748a-0a14-425e-be3c-dd723807ad66/windswept4.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dfc6748a-0a14-425e-be3c-dd723807ad66/windswept4.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dfc6748a-0a14-425e-be3c-dd723807ad66/windswept4.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dfc6748a-0a14-425e-be3c-dd723807ad66/windswept4.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dfc6748a-0a14-425e-be3c-dd723807ad66/windswept4.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/dfc6748a-0a14-425e-be3c-dd723807ad66/windswept4.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643741510783_57384">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/83b0215a-2bc9-4bf1-a2ef-54996892506e/windswept2.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/83b0215a-2bc9-4bf1-a2ef-54996892506e/windswept2.jpg" data-image-dimensions="750x500" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/83b0215a-2bc9-4bf1-a2ef-54996892506e/windswept2.jpg" width="750" height="500" sizes="(max-width: 640px) 100vw, (max-width: 767px) 41.66666666666667vw, 41.66666666666667vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/83b0215a-2bc9-4bf1-a2ef-54996892506e/windswept2.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/83b0215a-2bc9-4bf1-a2ef-54996892506e/windswept2.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/83b0215a-2bc9-4bf1-a2ef-54996892506e/windswept2.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/83b0215a-2bc9-4bf1-a2ef-54996892506e/windswept2.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/83b0215a-2bc9-4bf1-a2ef-54996892506e/windswept2.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/83b0215a-2bc9-4bf1-a2ef-54996892506e/windswept2.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/83b0215a-2bc9-4bf1-a2ef-54996892506e/windswept2.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643744579027_255517">
  <h3>Artificial Intelligence</h3><p>Art that uses machine learning stands parallel to, but separate from generative art. It’s almost surprising how little they are considered related, yet outputs from an AI are undoubtedly generative. A machine learning algorithm is a complex system of rules and could perhaps be considered in the Rules section, but it’s fair to say that ML/AI art is its own thing. </p><p>A machine learning algorithm requires a data source to be trained on. In Anna Ridler’s work <a href="http://annaridler.com/myriad-tulips"><em>Myriad (Tulips)</em></a>, she created her data set manually and painstakingly, by taking 10,000 photographs of tulips. Then in her work <a href="http://annaridler.com/mosaic-virus"><em>Mosaic Virus</em></a> she trained a GAN (generative adversarial network - machine learning framework) on this dataset, to generate new images of tulips. </p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-aspect-ratio="77.57166947723441" data-block-type="5" id="block-yui_3_17_2_1_1643744579027_419666">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4a49fc5d-572f-476e-99c6-afd142094827/Myriad.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4a49fc5d-572f-476e-99c6-afd142094827/Myriad.png" data-image-dimensions="1000x776" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4a49fc5d-572f-476e-99c6-afd142094827/Myriad.png" width="1000" height="776" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4a49fc5d-572f-476e-99c6-afd142094827/Myriad.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4a49fc5d-572f-476e-99c6-afd142094827/Myriad.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4a49fc5d-572f-476e-99c6-afd142094827/Myriad.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4a49fc5d-572f-476e-99c6-afd142094827/Myriad.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4a49fc5d-572f-476e-99c6-afd142094827/Myriad.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4a49fc5d-572f-476e-99c6-afd142094827/Myriad.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4a49fc5d-572f-476e-99c6-afd142094827/Myriad.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643744579027_423319">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/26d0d5b3-2785-465c-80e0-542e873137dd/Mosaicvirus.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/26d0d5b3-2785-465c-80e0-542e873137dd/Mosaicvirus.jpg" data-image-dimensions="1000x776" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/26d0d5b3-2785-465c-80e0-542e873137dd/Mosaicvirus.jpg" width="1000" height="776" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/26d0d5b3-2785-465c-80e0-542e873137dd/Mosaicvirus.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/26d0d5b3-2785-465c-80e0-542e873137dd/Mosaicvirus.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/26d0d5b3-2785-465c-80e0-542e873137dd/Mosaicvirus.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/26d0d5b3-2785-465c-80e0-542e873137dd/Mosaicvirus.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/26d0d5b3-2785-465c-80e0-542e873137dd/Mosaicvirus.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/26d0d5b3-2785-465c-80e0-542e873137dd/Mosaicvirus.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/26d0d5b3-2785-465c-80e0-542e873137dd/Mosaicvirus.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643744579027_444096">
  <p>In many (AI and non AI) cases, a data source is a biproduct of something else - naturally occurring information that needs to be captured and organised. When looking at the balance of control in the output, the data source would be largely outside the artist’s control, while they do have control over the way that data is used and manipulated for the output. </p><p>This project is different. Ridler had strong control over the dataset, and left the outputs up to the hidden mechanisms in the GAN. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643741510783_73903">
  <h3>Automatism</h3><p>The <a href="https://en.wikipedia.org/wiki/Surrealism">surrealists</a> used a technique called automatic drawing, wherein the hand is allowed to move “randomly” across a page. By most accounts this is a way to allow the artist’s subconscious to express itself, while for early abstract artist <a href="https://en.wikipedia.org/wiki/Hilma_af_Klint">Hilma Af Klint</a>, it was a way of communicating with the spirit world in séances. </p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643744579027_72979">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/34d0a7ee-aceb-4545-91be-723b57d028db/hilmaafklimt.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/34d0a7ee-aceb-4545-91be-723b57d028db/hilmaafklimt.jpg" data-image-dimensions="924x1024" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/34d0a7ee-aceb-4545-91be-723b57d028db/hilmaafklimt.jpg" width="924" height="1024" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/34d0a7ee-aceb-4545-91be-723b57d028db/hilmaafklimt.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/34d0a7ee-aceb-4545-91be-723b57d028db/hilmaafklimt.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/34d0a7ee-aceb-4545-91be-723b57d028db/hilmaafklimt.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/34d0a7ee-aceb-4545-91be-723b57d028db/hilmaafklimt.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/34d0a7ee-aceb-4545-91be-723b57d028db/hilmaafklimt.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/34d0a7ee-aceb-4545-91be-723b57d028db/hilmaafklimt.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/34d0a7ee-aceb-4545-91be-723b57d028db/hilmaafklimt.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>A collective automatic drawing by The Five, <a href="https://www.hilmaafklint.se/en/">Hilma af Klint</a>'s spiritualist group</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643744579027_78904">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/53406cd1-f35c-4365-a1d5-9a25908250a4/miro3.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/53406cd1-f35c-4365-a1d5-9a25908250a4/miro3.jpg" data-image-dimensions="900x677" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/53406cd1-f35c-4365-a1d5-9a25908250a4/miro3.jpg" width="900" height="677" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/53406cd1-f35c-4365-a1d5-9a25908250a4/miro3.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/53406cd1-f35c-4365-a1d5-9a25908250a4/miro3.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/53406cd1-f35c-4365-a1d5-9a25908250a4/miro3.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/53406cd1-f35c-4365-a1d5-9a25908250a4/miro3.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/53406cd1-f35c-4365-a1d5-9a25908250a4/miro3.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/53406cd1-f35c-4365-a1d5-9a25908250a4/miro3.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/53406cd1-f35c-4365-a1d5-9a25908250a4/miro3.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>The Smile of the Flamboyant Wings</em> (1953) - <a href="https://www.joan-miro.net/">Joan Miró</a></p>
          </figcaption>
        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643744579027_73315">
  <p>Earlier, I defined generative art as being made “not under direct human control”, so perhaps this inclusion stretches that definition. Or perhaps if artists are truly accessing their subconscious (or, indeed, the spirit world) then this work is not under their direct control. Perhaps the subconscious can be considered a random source. </p><p>Thinking about that balance of control present in many generative artworks, between the artist and the computer, or between the artist and a participant; perhaps the artist and their own subconscious can be thought of the same way. When we have ideas, where do they come from? If the idea is a impulse to draw in one direction or the other, is that system any less chaotic than the weather?</p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643744579027_307839">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fd1a2f3e-46e0-41c1-8660-358156bc1579/paulemileborduas.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fd1a2f3e-46e0-41c1-8660-358156bc1579/paulemileborduas.jpg" data-image-dimensions="1311x920" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fd1a2f3e-46e0-41c1-8660-358156bc1579/paulemileborduas.jpg" width="1311" height="920" sizes="(max-width: 640px) 100vw, (max-width: 767px) 58.333333333333336vw, 58.333333333333336vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fd1a2f3e-46e0-41c1-8660-358156bc1579/paulemileborduas.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fd1a2f3e-46e0-41c1-8660-358156bc1579/paulemileborduas.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fd1a2f3e-46e0-41c1-8660-358156bc1579/paulemileborduas.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fd1a2f3e-46e0-41c1-8660-358156bc1579/paulemileborduas.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fd1a2f3e-46e0-41c1-8660-358156bc1579/paulemileborduas.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fd1a2f3e-46e0-41c1-8660-358156bc1579/paulemileborduas.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fd1a2f3e-46e0-41c1-8660-358156bc1579/paulemileborduas.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643744579027_318279">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8c0b4c0a-697f-42e2-9fec-fb8a792d3549/maxernst.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8c0b4c0a-697f-42e2-9fec-fb8a792d3549/maxernst.jpg" data-image-dimensions="750x919" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8c0b4c0a-697f-42e2-9fec-fb8a792d3549/maxernst.jpg" width="750" height="919" sizes="(max-width: 640px) 100vw, (max-width: 767px) 41.66666666666667vw, 41.66666666666667vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8c0b4c0a-697f-42e2-9fec-fb8a792d3549/maxernst.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8c0b4c0a-697f-42e2-9fec-fb8a792d3549/maxernst.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8c0b4c0a-697f-42e2-9fec-fb8a792d3549/maxernst.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8c0b4c0a-697f-42e2-9fec-fb8a792d3549/maxernst.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8c0b4c0a-697f-42e2-9fec-fb8a792d3549/maxernst.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8c0b4c0a-697f-42e2-9fec-fb8a792d3549/maxernst.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8c0b4c0a-697f-42e2-9fec-fb8a792d3549/maxernst.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Forest and Dove</em> (1927) - <a href="https://www.tate.org.uk/art/artworks/ernst-forest-and-dove-t00548">Max Ernst</a><br>Uses grattage technique</p>
          </figcaption>
        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643744579027_238129">
  <p>Some automatists would produce entire pieces purely at the mercy of their subconscious, while other artists would use this technique to begin a piece, and then consciously move more towards figurative or composed imagery as it emerged. <a href="https://www.tate.org.uk/art/art-terms/a/automatism">Automatism</a> led to techniques that sought to introduce chance and spontaneity to creation and mark making, like collage, pencil rubbings (frottage), and paint scraping (grattage). </p><p>This leads us nicely on to the next section: </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643744579027_249594">
  <h3>painting and drawing</h3><p>At the start of this article I said that traditional painting could be considered generative. This is true in two ways - the first is when there is some level of automatism in the artist’s process and the second is when the physical process of laying paint down on a canvas involves chance. </p><p><a href="https://www.tate.org.uk/art/artists/jackson-pollock-1785">Jackson Pollock</a>’s action-paintings are an obvious example of both of these things. His process was influenced by automatism and his work was not made according to any plan. He allowed it to emerge out of the process of painting, surrendering control to existing in the moment. The energetic, unconstrained nature of his drip technique means that natural processes were strongly present in his work, as phenomena like fluid dynamics introduced randomness to the results.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1643744579027_290599">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/df692ca7-617e-4c9b-86e8-5f7a68c5dae8/alchemy.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/df692ca7-617e-4c9b-86e8-5f7a68c5dae8/alchemy.jpg" data-image-dimensions="1000x516" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/df692ca7-617e-4c9b-86e8-5f7a68c5dae8/alchemy.jpg" width="1000" height="516" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/df692ca7-617e-4c9b-86e8-5f7a68c5dae8/alchemy.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/df692ca7-617e-4c9b-86e8-5f7a68c5dae8/alchemy.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/df692ca7-617e-4c9b-86e8-5f7a68c5dae8/alchemy.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/df692ca7-617e-4c9b-86e8-5f7a68c5dae8/alchemy.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/df692ca7-617e-4c9b-86e8-5f7a68c5dae8/alchemy.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/df692ca7-617e-4c9b-86e8-5f7a68c5dae8/alchemy.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/df692ca7-617e-4c9b-86e8-5f7a68c5dae8/alchemy.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643293789257_18204">

<p>Rather than thinking of paintings as either having these generative elements or not, I prefer to think of it as a gradient. Some paintings are tightly and consciously controlled by the artist, some are heavily influenced by chance, and many are somewhere in between. The intent and perspective of the artist is also important, in terms of whether they would consider their work to be generative. </p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1643744579027_304098">
  <h2>Conclusion</h2><p>When we think of generative art, we generally think of p5js sketches, Vera Molnar’s plotter drawings, and flow fields. These are all things I hold dear to my heart but just below the surface of that view, we can find a rich abundance of other avenues to explore. I am confident there are more examples and perspectives I have not included!</p><p>In working with the categories above, I frequently found that examples or ideas could fit into multiple sections. I think a multitude of opportunities for new artworks can be found by looking at ways elements and ideas from each of these categories can be combined and remixed. </p><p>There is a rich conceptual scope in considering the boundaries between what is chance and what is the result of a series of prior steps as well as the questions of where ideas and randomness come from, where generativity appears in nature, what <em>isn’t </em>generative, what is a simulation, and more. </p><p>Generative art isn’t a new genre, but it is booming at the moment, and in some ways we’re still in the early stages of what’s possible. I look forward to seeing (and contributing to!) work that explores these avenues and more, and tackles these conceptual questions. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1683823749922_75555">

<p>Enjoyed this article? Found it useful? You can <a href="https://tiptopjar.com/amygoodchild">tip me</a>!<br>I’d also love if you could give the article a boost on <a href="https://twitterhttps//twitter.com/amygoodchild/status/1647229407314976769.com/amygoodchild/status/1647973978080792578">Twitter</a>. <br>Thanks!</p>




















  
  



</div></div>
  
</article>

</div>

  
</article>


          

          
            
              

            
          
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Lunacy of Artemis (355 pts)]]></title>
            <link>https://idlewords.com/2024/5/the_lunacy_of_artemis.htm</link>
            <guid>40410404</guid>
            <pubDate>Sun, 19 May 2024 23:02:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://idlewords.com/2024/5/the_lunacy_of_artemis.htm">https://idlewords.com/2024/5/the_lunacy_of_artemis.htm</a>, See on <a href="https://news.ycombinator.com/item?id=40410404">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><b>1.1.<a href="https://idlewords.com/2020/">2023</a></b></p><p><a href="https://idlewords.com/2024/5/the_lunacy_of_artemis.htm">The Lunacy of Artemis</a></p>
   
   


<p><img src="https://idlewords.com/images/artemis_cloud.jpg" width="100%" alt="distant photo of Artemis rocket on launch pad"></p>


<p>A little over 51 years ago, a rocket lifted off from Cape Canaveral carrying three astronauts and a space car.  After a three day journey to the moon, two of the astronauts climbed into a spindly lander and made the short trip down to the surface, where for another three days they collected rocks and did donuts in the space car. Then they climbed back into the lander,  rejoined their colleague in orbit, and departed for Earth. Their capsule splashed down in the South Pacific on December 19, 1972.  This mission, Apollo 17, would be the last time human beings ventured beyond low Earth orbit. 

</p><p>If you believe NASA, late in 2026 Americans will walk on the moon again. That proposed mission is called Artemis 3, and its lunar segment looks a lot like Apollo 17 without the space car. Two astronauts will land on the moon, collect rocks, take selfies, and about a week after landing rejoin their orbiting colleagues to go back to Earth. 

</p><p>But where Apollo 17 launched on a single rocket and cost $3.3 billion (in 2023 dollars), the first Artemis landing involves a dozen or two heavy rocket launches and costs so much that NASA refuses to give a figure (one veteran of NASA budgeting estimates it at $7-10 billion).<a id="fn_cost"></a><a href="#cost"><span><sup>[1]</sup></span></a>  The single-use lander for the mission will be the heaviest spacecraft ever flown, and yet the mission's scientific return—a small box of rocks—is less than what came home on Apollo 17. And the whole plan hinges on technologies that haven't been invented yet becoming reliable and practical within the next eighteen months. 

</p><p>You don’t have to be a rocket scientist to wonder what’s going on here. If we can put a man on the moon,  then why can't we just go do it again?  The moon hasn’t changed since the 1960’s, while every technology we used to get there  has seen staggering advances.  It took NASA eight years to go from nothing to a moon landing at the dawn of the Space Age. But today, twenty years and $93 billion after the space agency announced our return to the moon, the goal seems as far out of reach as ever.<a id="fn_nasatime"></a><a href="#nasatime"><span><sup>[2]</sup></span></a> 

</p><p>Articles about Artemis often give the program’s tangled backstory.   But I want to talk about Artemis as a technical design, because there’s just so much to drink in. While NASA is no stranger to complex mission architectures, Artemis goes beyond complex to the just plain incoherent. None of the puzzle pieces seem to come from the  same box. Half the program requires breakthrough technologies that make the other half unnecessary.  The rocket and spacecraft NASA spent two decades building can’t even reach the moon.  And for reasons no one understands, there’s a new space station in the mix.

</p><p>In the past, whatever oddball project NASA came up with, we at least knew they could build the hardware. But Artemis calls the agency’s competence as an engineering organization into question.  For the first time since the early 1960's, it's unclear whether the US space agency is even <i>capable</i> of putting astronauts on the Moon. 

<br>



</p><p><img src="https://idlewords.com/images/apollo_flight_path.jpg" width="100%" alt="Photograph of SLS rocket"></p>

<p><b><a name="apollo">A Note on Apollo</a></b></p> 


<p>In this essay I make a lot of comparisons to Project Apollo. This is not because I think other mission architectures are inferior, but because the early success of that program sets such a useful baseline.  At the dawn of the Space Age, using rudimentary technology, American astronauts landed on the moon six times in seven attempts.  The moon landings were NASA’s greatest achievement and should set a floor for what a modern mission, flying modern hardware, might achieve.

</p><p>Advocates for Artemis insist that the program is more than Apollo 2.0. But as we’ll see, Artemis can't even measure up to Apollo 1.0. It costs more, does less, flies less frequently, and exposes crews to risks that the steely-eyed missile men of the Apollo era found unacceptable.   It's as if Ford in 2024 released a new model car that was slower,  more accident-prone, and ten times more expensive than the Model T.  

</p><p>When a next-generation lunar program can’t meet the cost, performance, or safety standards set three generations earlier, something has gone seriously awry. 

<br>



</p><p><img src="https://idlewords.com/images/sls_image.jpg" width="100%" alt="Photograph of SLS rocket"></p>

<p><a name="sls"><b>I. The Rocket</b></a></p> 

<p>The jewel of Artemis is a big orange rocket with a flavorless name, the Space Launch System (SLS).  SLS looks like someone started building a Space Shuttle and ran out of legos for the orbiter. There is the familiar orange tank, a big white pair of solid rocket boosters, but then the rocket just peters out in a 1960’s style stack of cones and cylinders. 

</p><p>The best way to think of SLS is as a balding guy with a mullet: there are fireworks down below that are meant to distract you from a sad situation up top.  In the case of the rocket, those fireworks are a first stage with more thrust than the Saturn V, enough thrust that the boosted core stage can nearly put itself into orbit.  But on top of this monster sits a second stage so anemic that even its name (the Interim Cryogenic Propulsion Stage) is a kind of apology. For eight minutes SLS roars into the sky on a pillar of fire.  And then, like a cork popping out of a bottle, the tiny ICPS emerges and drifts vaguely moonwards on a wisp of flame. 

</p><p>With this design, the minds behind SLS achieved a first in space flight, creating a rocket that is at the same time more powerful and less capable than the Saturn V.  While the 1960’s giant could send 49 metric tons to the Moon,  SLS only manages 27 tons—not enough to fly an Apollo-style landing, not enough to even put a crew in orbit around the Moon without a lander. The best SLS can do is slingshot the Orion spacecraft once around the moon and back, a mission that will fly under the name Artemis 2.  

</p><p>NASA wants to replace ICPS with an ‘Exploration Upper Stage’  (the project has been held up, among other things, by a <a href="https://oig.nasa.gov/wp-content/uploads/2024/02/IG-22-012.pdf">near-billion dollar cost overrun on a launch pad</a>). But even that upgrade won’t give SLS the power of the Saturn V.  For whatever reason, NASA designed its first heavy launcher in forty years to be unable to fly the simple, proven architecture of the Apollo missions.

</p><p>Of course, plenty of rockets go on to enjoy rewarding, productive careers without being as powerful as the Saturn V.   And if SLS rockets were piling up at the Michoud Assembly Facility like cordwood, or if NASA were willing to let its astronauts fly commercial, it would be a simple matter to split Artemis missions across multiple launches. 

</p><p>But NASA insists that astronauts fly SLS. And SLS is a “one and done” rocket, artisanally hand-crafted by a workforce that likes to get home before traffic gets bad. The rocket can only launch once every two years at a cost of about four billion dollars<a id="fn_slscost"></a><a href="#slscost"><span><sup>[3]</sup></span></a>—about twice what it would cost to light the rocket’s weight in dollar bills on fire<a id="fn_dollarbills"></a><a href="#dollarbills"><span><sup>[4]</sup></span></a>. 

</p><p>Early on, SLS designers made the catastrophic decision to reuse Shuttle hardware, which is like using Fabergé eggs to save money on an omelette.  The SLS core stage recycles Space Shuttle main engines, actual veterans of old Shuttle flights called out of retirement for one last job.  Refurbishing a single such engine to work on SLS costs NASA $40 million, or a bit more than SpaceX spends on all 33 engines on its Superheavy booster.<a id="fn_refurb"></a><a href="#refurb"><span><sup>[5]</sup></span></a> And though the Shuttle engines are designed to be fully reusable (the main reason they're so expensive), every SLS launch throws four of them away. Once all the junkyards are picked clean, NASA will pay Aerojet Rocketdyne to restart production of the classic engine at a cool unit cost of $145 million<a id="fn_rs25restart"></a><a href="#rs25restart"><span><sup>[6]</sup></span></a>. 

</p><p>The story is no better with the solid rocket boosters, the other piece of Shuttle hardware SLS reuses.  Originally a stopgap measure introduced to save the Shuttle budget, these heavy rockets now attach themselves like barnacles to every new NASA launcher design. To no one’s surprise, retrofitting a bunch of heavy steel casings left over from Shuttle days has saved the program nothing. Each SLS booster is now projected to cost $266 million, or about twice the launch cost of a Falcon Heavy.<a id="fn_fheavy"></a><a href="#fheavy"><span><sup>[7]</sup></span></a>  Just replacing the asbestos lining in the boosters with a greener material, a project budgeted at $4.4M, has now cost NASA a quarter of a billion dollars.  And once the leftover segments run out seven rockets from now,  SLS will need a brand new booster design, opening up fertile new vistas of overspending. 

</p><p>Costs on SLS have reached the point where private industry is now able to develop, test, and launch an entire rocket <i>program</i> for less than NASA spends on a single engine<a id="fn_electron"></a><a href="#electron"><span><sup>[8]</sup></span></a>.  Flying SLS is like owning a classic car—everything is hand built, the components cost a fortune, and when you finally get the thing out of the shop, you find yourself constantly overtaken by younger rivals. 

</p><p>But the cost of SLS to NASA goes beyond money.   The agency has committed to an antiquated frankenrocket just as the space industry is entering a period of unprecedented innovation. While other space programs get to romp and play with technologies like reusable stages and exotic alloys, NASA is stuck for years wasting a massive, skilled workforce on a dead-end design. 

</p><p>The SLS program's slow pace also affects safety. Back in the Shuttle era, NASA managers argued that it took three to four launches a year to keep workers proficient enough to build and launch the vehicles safely.  A boutique approach where workers hand-craft one rocket every two years  means having to re-learn processes and procedures with every launch.

</p><p>It also leaves no room in Artemis for test flights. The program simply assumes success, flying all its important 'firsts' with astronauts on board. When there are unanticipated failures, like the extensive heat shield spalling and near burn-through observed in Artemis 1,<a id="fn_heatshield"></a><a href="#heatshield"><span><sup>[9]</sup></span></a> the agency has no way to test a proposed fix without a multi-year delay to the program. So they end up using indirect means to convince themselves that a new design is safe to fly, a process ripe for error and self-delusion.




</p><p><img src="https://idlewords.com/images/orion_oversize.jpg" width="100%" alt="Orion space capsule with OVERSIZE LOAD banner"></p>

<p><b><a name="Orion">II. The Spacecraft</a></b></p> 


<p>Orion, the capsule that launches on top of SLS, is a relaxed-fit reimagining of the Apollo command module suitable for today’s larger astronaut. It boasts modern computers, half again as much volume as the 1960’s design, and a few creature comforts (like not having to poop in a baggie) that would have pleased the Apollo pioneers.

</p><p>The capsule’s official name is the Orion Multipurpose Crew Vehicle, but finding even a single purpose for Orion has greatly challenged NASA.  For twenty years the spacecraft has mostly sat on the ground, chewing through a $1.2 billion annual budget. In 2014, the first Orion flew a brief test flight. Eight short years later, Orion launched again, carrying a crew of instrumented mannequins around the Moon on Artemis 1.  In 2025 the capsule (by then old enough to drink) is supposed to fly  human passengers on Artemis 2. 

</p><p>Orion goes to space attached to a basket of amenities called the European Service Module. The ESM provides Orion with solar panels, breathing gas, batteries, and a small rocket that is the capsule’s principal means of propulsion. But because the ESM was never designed to go to the moon, it carries very little propellant—far too little to get the hefty capsule in and out of lunar orbit.<a id="fn_oriondeltav"></a><a href="#oriondeltav"><span><sup>[10]</sup></span></a>   

</p><p>And Orion is hefty. Originally designed to hold six astronauts, the capsule was never resized when the crew requirement shrank to four.  Like an empty nester’s minivan, Orion now hauls around a bunch of mass and volume that it doesn’t need. Even with all the savings that come from replacing Apollo-era avionics, the capsule weighs almost twice as much as the Apollo Command Module. 

</p><p>This extra mass has knock-on effects across the entire Artemis design. Since a large capsule needs a large abort rocket, SLS has to haul Orion's massive Launch Abort System—seven tons of dead weight—nearly all the way into orbit. And reinforcing the capsule so that abort system won't shake the astronauts into jelly means making it heavier, which puts more demand on the parachutes and heat shield,<a id="fn_heatshield"></a><a href="#heatshield"><span><sup>[11]</sup></span></a> and around and around we go. 

</p><p><img src="https://idlewords.com/images/csm_orion_esm.jpg" width="100%" alt="Orion space capsule with OVERSIZE LOAD banner"></p>

<p>Size comparison of the Apollo command and service module (left) and Orion + European Service Module (right)</p>

<div><p>What’s particularly frustrating is that Orion and ESM together have nearly the same mass as the Apollo command and service modules, which had no trouble reaching the Moon.  The difference is all in the proportions. Where Apollo was built like a roadster, with a small crew compartment bolted onto an oversized engine, Orion is the Dodge Journey of spacecraft—a chunky, underpowered six-seater that advertises to the world that you're terrible at managing money.

</p></div><p><img src="https://idlewords.com/images/nrho.jpg" width="100%" alt="diagram of near-rectilinear halo orbit"></p>

<p><b><a name="orbit">III. The Orbit</a></b></p> 

<p>The fact that neither its rocket or spaceship can get to the Moon creates difficulties for NASA’s lunar program.  So, like an aging crooner transposing old hits into an easier key, the agency has worked to find a ‘lunar-adjacent’ destination that its hardware can get to.

</p><p>Their solution is a bit of celestial arcana called Near Rectilinear Halo Orbit, or NRHO.  A spacecraft in this orbit circles the moon every 6.5 days, passing 1,000 kilometers above the lunar north pole at closest approach, then drifting out about 70,000 kilometers (a fifth of the Earth/Moon distance) at its furthest point.  Getting to NRHO from Earth requires significantly less energy than entering a  useful lunar orbit, putting it just within reach for SLS and Orion.<a id="fn_nrho"></a><a href="#nrho"><span><sup>[12]</sup></span></a>


</p><p>To hear NASA tell it, NRHO is so full of advantages that it’s a wonder we stay on Earth.  Spacecraft in the orbit always have a sightline to Earth and never pass through its shadow. The orbit is relatively stable, so a spacecraft can loiter there for months using only ion thrusters. And the deep space environment is the perfect place to practice going to Mars. 

</p><p>But NRHO is terrible for getting to the moon. The orbit is like one of those European budget airports that leaves you out in a field somewhere, requiring an expensive taxi.  In Artemis, this taxi takes the form of a whole other spaceship—the lunar lander—which launches without a crew a month or two before Orion and is supposed to be waiting in NRHO when the capsule arrives. 

 </p><p>Once these two spacecraft dock together, two astronauts climb into the lander from Orion and begin a day-long descent to the lunar surface. The other two astronauts wait for them in NRHO, playing hearts and quietly absorbing radiation.

</p><p>Apollo landings also divided the crew between lander and orbiter. But those missions kept the command module in a low lunar orbit that brought it over the landing site every two hours.  This proximity between orbiter and lander had enormous implications for safety. At any point in the surface mission, the astronauts on the moon could climb into the ascent rocket, hit the big red button, and be back sipping Tang with the command module pilot by bedtime.  The short orbital period also gave the combined crew a dozen opportunities a day to return directly to Earth. <a id="fn_abort"></a><a href="#abort"><span><sup>[13]</sup></span></a>

</p><p>Sitting in NRHO makes abort scenarios much harder. Depending on when in the mission it happens, a stricken lander might need three or more days to catch up with the orbiting Orion. In the worst case, the crew might find themselves stuck on the lunar surface for hours after an abort is called, forced to wait for Orion to reach a more favorable point in its orbit. And once everyone is back on Orion, more days might pass before the crew can depart for Earth. These long and variable abort times significantly increase risk to the crew, making many scenarios that were survivable on Apollo (like Apollo 13!) lethal on Artemis. <a id="fn_abortnrho"></a><a href="#abortnrho"><span><sup>[14]</sup></span></a>

</p><p>The abort issue is just one example of  NRHO making missions slower.  NASA likes to boast that Orion can stay in space far longer than Apollo, but this is like bragging that you’re in the best shape of your life after the bank repossessed your car.  It's an oddly positive spin to put on bad life choices. The reason Orion needs all that endurance is because transit times from Earth to NRHO are long, and the crew has to waste additional time in NRHO waiting for orbits to line up.  The Artemis 3 mission, for example, will spend 24 days in transit, compared to just 6 days on Apollo 11. 

</p><p>NRHO even dictates how long astronauts stay on the Moon—surface time has to be a multiple of the 6.5 day orbital period. This lack of flexibility means that even early flag-and-footprints missions like Artemis 3 have to spend at least a week on the moon, a constraint that adds considerable risk to the initial landing.  <a id="fn_landingrisk"></a><a href="#landingrisk"><span><sup>[15]</sup></span></a>

</p><p>In spaceflight, brevity is safety. There's no better way to protect astronauts  from the risks of solar storms, mechanical failure, and other mishaps than by minimizing slack time in space. Moreover, a safe architecture should allow for a rapid return to Earth at any point in the mission.   There’s no question astronauts on the first Artemis missions would be better off with Orion in low lunar orbit. The decision to stage from NRHO is an excellent example of NASA designing its lunar program in the wrong direction—letting deficiencies in the hardware dictate the level of mission risk. 

￼




</p><p><img src="https://idlewords.com/images/gateway_diagram.jpg" width="100%" alt="diagram of Gateway"></p>
<p>Early diagram of Gateway. Note that the segment marked 'human lander system' now dwarfs the space station.</p>

<p><b><a name="gateway">IV. Gateway</a></b></p> 

<p>I suppose at some point we have to talk about Gateway.  Gateway is a small modular space station that NASA wants to build in NRHO.  It has been showing up across various missions like a bad smell since before 2012.  

</p><p>Early in the Artemis program, NASA described Gateway as a kind of celestial truck stop, a safe place for the lander to park and for the crew to grab a cup of coffee on their way to the moon.  But when it became clear that Gateway would not be ready in time for Artemis 3, NASA re-evaluated.  Reasoning that two spacecraft could meet up in NRHO just as easily as three, the agency gave permission for the first moon landing to proceed without a space station.

</p><p>Despite this open admission that Gateway is unnecessary, building the space station remains the core activity of the Artemis program. The three missions that follow that first landing are devoted chiefly to Gateway assembly.  In fact, initial plans for Artemis 4 left out a lunar landing entirely, as if it were an inconvenience to the real work being done up in orbit.

</p><p>This is a remarkable situation. It’s like if you hired someone to redo your kitchen and they started building a boat in your driveway. Sure, the boat gives the builders a place to relax, lets them practice tricky plumbing and finishing work, and is a safe place to store their tools. But all those arguments will fail to satisfy. You still want to know what building a boat has to do with kitchen repair, and why you’re the one footing the bill. 

</p><p>NASA has struggled to lay out a technical rationale for Gateway. The space station adds both cost and complexity to Artemis, a program not particularly lacking in either.  Requiring moon-bound astronauts to stop at Gateway also makes missions riskier (by adding docking operations) while imposing a big propellant tax. Aerospace engineer and pundit Robert Zubrin has aptly called the station a tollbooth in space.  

</p><p>Even Gateway defenders struggle to hype up the station. A common argument is that Gateway may not ideal for any one thing, but is good for a whole lot of things. But that is the same line of thinking that got us SLS and Orion, both vehicles designed before anyone knew what to do with them. The truth is that all-purpose designs don't exist in human space flight. The best you can do is build a spacecraft that is equally bad at everything. 
 
</p><p>But to search for technical grounds is to misunderstand the purpose of Gateway.  The station is not being built to shelter astronauts in the harsh environment of space, but to protect Artemis in the harsh environment of Congress. NASA needs Gateway to navigate an uncertain political landscape in the 2030’s. Without a station, Artemis will just be a series of infrequent multibillion dollar moon landings, a red cape waved in the face of the Office of Management and Budget.  Gateway armors Artemis by bringing in international partners, each of whom contributes expensive hardware. As NASA learned building the International Space Station, this combination of sunk costs and international entanglement is a powerful talisman against program death.   

</p><p>Gateway also solves some other problems for NASA.  It gives SLS a destination to fly to, stimulates private industry (by handing out public money to supply Gateway), creates a job for the astronaut corps, and guarantees the continuity of human space flight once the ISS becomes uninhabitable sometime in the 2030’s. <a id="fn_iss"></a><a href="#iss"><span><sup>[16]</sup></span></a>


</p><p>That last goal may sound odd if you don’t see human space flight as an end in itself.  But NASA is a faith-based organization, dedicated to the principle that taxpayers should always keep an American or two in orbit.  it’s a little bit as if the National Oceanic Atmospheric Administration insisted on keeping bathyscapes full of sailors at the bottom of the sea, irrespective of cost or merit, and kneecapped programs that might threaten the continuous human benthic presence. You can’t argue with faith.

</p><p>From a bureaucrat’s perspective, Gateway is NASA’s ticket back to a golden era in the early 2000's when the Space Station and Space Shuttle formed an uncancellable whole, each program justifying the existence of the other.  Recreating this dynamic with Gateway and SLS/Orion would mean predictable budgets and program stability for NASA well into the 2050’s.  

</p><p>But Artemis was supposed to take us back to a different golden age, the golden age of Apollo.  And so there’s an unresolved tension in the program between building Gateway and  doing interesting things on the moon. With Artemis missions two or more years apart, it’s inevitable that Gateway assembly will push aspirational projects like a surface habitat or pressurized rover out into the 2040’s. But those same projects are on the critical path to Mars, where NASA still insists we’re going in the late 2030’s. The situation is awkward.

</p><p>So that is the story of Gateway—unloved, ineradicable, and as we’ll see, likely to become the sole legacy of the Artemis program.


￼


</p><p><img src="https://idlewords.com/images/pointy_rocket.jpg" width="100%" alt="artist's rendering of human landing system'"></p>

<p><b><a name="lander">V. The Lander</a></b></p> 

<p>The lunar lander is the most technically ambitious part of Artemis.  Where SLS, Orion, and Gateway are mostly a compilation of NASA's greatest hits, the lander requires breakthrough technologies with the potential to revolutionize space travel. 

</p><p>Of course, you can’t just call it a lander.  In Artemis speak, this spacecraft is the Human Landing System, or HLS. NASA has delegated its design to two private companies, Blue Origin and SpaceX.  SpaceX is responsible for landing astronauts on Artemis 3 and 4, while Blue Origin is on the hook for Artemis 5 (notionally scheduled for 2030).  After that, the agency will take competitive bids for subsequent missions. 

</p><p>The SpaceX HLS design is based on their experimental Starship spacecraft, an enormous rocket that takes off on and lands on its tail, like 1950’s sci-fi.   There is a strong “emperor’s new clothes” vibe to this design.  On the one hand, it is the brainchild of brilliant SpaceX engineers and passed NASA technical review. On the other hand, the lander seems to go out of its way to create problems for itself to solve with technology. 

</p><p><img src="https://idlewords.com/images/hls_lem.jpg" width="100%" alt="artist's rendering of human landing system'"></p>
<p>An early SpaceX rendering of the Human Landing System, with the Apollo Lunar Module added for scale.</p> 


<p>To start with the obvious, HLS looks more likely to tip over than the last two spacecraft to land on the moon, which <a href="https://www.space.com/intuitive-machines-odysseus-moon-lander-tipped-over">tipped</a> <a href="https://www.cbsnews.com/news/japanese-moon-lander-reaches-surface-but-fate-uncertain/">over</a>.   It is a fifteen story tower that must land on its ass in terrible lighting conditions, on rubble of unknown composition, over a light-second from Earth.  The crew are left suspended so high above the surface that they need a folding space elevator (not the cool kind) to get down.  And yet in the end this single-use lander carries less payload (both up and down) than the tiny Lunar Module on Apollo 17. Using Starship to land two astronauts on the moon is like delivering a pizza with an aircraft carrier. 

</p><p>Amusingly, the sheer size of the SpaceX design leaves it with little room for cargo.  The spacecraft arrives on the Moon laden with something like 200 tons of cryogenic propellant,<a id="fn_hlsprop"></a><a href="#hlsprop"><span><sup>[14]</sup></span></a> and like a fat man leaving an armchair, it needs every drop of that energy to get its bulk back off the surface.  Nor does it help matters that all this cryogenic propellant has to cook for a week in direct sunlight.

</p><p>Other, less daring lander designs reduce their appetite for propellant by using a detachable landing stage. This arrangement also shields the ascent rocket from hypervelocity debris that gets kicked up during landing.  But HLS is a one-piece rocket; the same engines that get sandblasted on their way down to the moon must relight without fail a week later. 

 </p><p>Given this fact, it’s remarkable that NASA’s contract with SpaceX doesn’t require them to demonstrate a lunar takeoff. All SpaceX has to do to satisfy NASA requirements is land an HLS prototype on the Moon.  Questions about ascent can then presumably wait until the actual mission, when we all find out together with the crew whether HLS can take off again.<a id="fn_ascent"></a><a href="#ascent"><span><sup>[15]</sup></span></a>

</p><p>This fearlessness in design is part of a pattern with Starship HLS.  Problems that other landers avoid in the design phase are solved with engineering.  And it’s kind of understandable why SpaceX does it this way. Starship is meant to fly to Mars, a much bigger challenge than landing two people on the Moon. If the basic Starship design can’t handle a lunar landing, it would throw the company’s whole Mars plan into question. SpaceX is committed to making Starship work, which is different from making the best possible lunar lander.

</p><p>Less obvious is why NASA tolerates all this complexity in the most hazardous phase of its first moon mission. Why land a rocket the size of a building packed with moving parts?  It’s hard to look at the HLS design and not think back to other times when a room full of smart NASA people talked themselves into taking major risks because the alternative was not getting to fly at all. 

</p><p>It’s instructive to compare the HLS approach to the design philosophy on Apollo. Engineers on that progam were motivated by terror; no one wanted to make the mistake that would leave astronauts stranded on the moon.  The weapon they used to knock down risk was simplicity. The Lunar Module was a small metal box with a wide stance, built low enough so that the astronauts only needed to climb down a short ladder. The bottom half of the LM was a descent stage that completely covered the ascent rocket (a design that showed its value on Apollo 15, when one of the descent engines got <a href="https://en.wikipedia.org/wiki/Descent_propulsion_system#/media/File:Apollo_15_Engine_Bell.jpg">smushed by a rock</a>).   And that ascent rocket, the most important piece of hardware in the lander, was a caveman design intentionally made so primitive that it  would struggle to find ways to fail.  

</p><p>On Artemis, it's the other way around: the more hazardous the mission phase, the more complex the hardware. It's hard to look at all this lunar machinery and feel reassured, especially when NASA's own Aerospace Safety Advisory Panel estimates that the Orion/SLS portion of a moon mission alone (not including anything to do with HLS) already <a href="https://oiir.hq.nasa.gov/asap/documents/2014_ASAP_Annual_Report.pdf">has a 1:75 chance</a> of killing the crew. 



</p><p><img src="https://idlewords.com/images/fuel_fight.jpg" width="100%" alt="artist's rendering of human landing system'"></p>

<p><b><a name="refueling">VI. Refueling</a></b></p> 



<p>Since NASA’s biggest rocket struggles to get Orion into distant lunar orbit, and HLS weighs fifty times as much as Orion, the curious reader might wonder how the unmanned lander is supposed to get up there.

</p><p>NASA’s answer is, very sensibly, “not our problem”. They are paying Blue Origin and SpaceX the big bucks to figure this out on their own. And as a practical matter, the only way to put such a massive spacecraft into NRHO is to first refuel it in low Earth orbit. 

</p><p>Like a lot of space technology, orbital refueling sounds simple, has never been attempted, and can’t be adequately simulated on Earth.<a id="fn_refuel"></a><a href="#refuel"><span><sup>[18]</sup></span></a>  The crux of the problem is that liquid and gas phases in microgravity jumble up into a three-dimensional mess, so that even measuring the quantity of propellant in a tank becomes difficult.  To make matters harder, Starship uses cryogenic propellants that boil at temperatures about a hundred degrees colder than the plumbing they need to move through.  Imagine trying to pour water from a thermos into a red-hot skillet while falling off a cliff and you get some idea of the difficulties. 

</p><p>To get refueling working, SpaceX will first have to demonstrate propellant transfer between rockets as a proof of concept, and then get the process working reliably and efficiently at a scale of hundreds of tons. (These are two distinct challenges).  Once they can routinely move liquid oxygen and methane from Starship A to Starship B, they’ll be ready to set up the infrastructure they need to launch HLS. 


</p><p><img src="https://idlewords.com/images/fueling_conops.jpg" width="100%" alt="artist's rendering of human landing system'"></p>

<p>The plan for getting HLS to the moon looks like this: a few months before the landing date, SpaceX will launch a special variant of their Starship rocket configured to serve as a propellant depot. Then they'll start launching Starships one by one to fill it up. Each Starship arrives in low Earth orbit with some residual propellant; it will need to dock with the depot rocket and transfer over this remnant fuel.   Once the depot is full, SpaceX will launch HLS, have it fill its tanks at the depot rocket, and send it up to NRHO in advance of Orion. When Orion arrives, HLS will hopefully have enough propellant left on board to take on astronauts and make a single round trip from NRHO to the lunar surface.

</p><p>Getting this plan to work requires solving a second engineering problem, how to keep cryogenic propellants cold in space.  Low earth orbit is a toasty place, and without special measures, the cryogenic propellants Starship uses will quickly vent off into space.  The problem is easy to solve in deep space (use a sunshade), but becomes tricky in low Earth orbit, where a warm rock covers a third of the sky.  (Boil-off is also a big issue for HLS on the moon.)


</p><p> It’s not clear how many Starship launches it will take to refuel HLS.  Elon Musk has said four launches might be enough; NASA Assistant Deputy Associate Administrator Lakiesha Hawkins says the number is in the “high teens”.  Last week, SpaceX's Kathy Lueders <a href="https://youtu.be/vOg49BVhU40?si=6q8R2qvkmEDPGy0V&amp;t=2381">gave a figure of fifteen launches</a>.

</p><p>The real number is unknown and will come down to four factors:

</p><ol>
<li>How much propellant a Starship can carry to low Earth orbit.</li>

<li>What fraction of that can be usably pumped out of the rocket. </li>

<li>How quickly cryogenic propellant boils away from the orbiting depot.</li>

<li>How rapidly SpaceX can launch Starships.</li>
</ol>

<p>SpaceX probably knows the answer to (1), but isn’t talking.  Data for (2) and (3) will have to wait for flight tests that are planned for 2025.  And obviously a lot is riding on (4), also called launch cadence.  

</p><p>The record for heavy rocket launch cadence belongs to Saturn V, which launched three times during a four month period in 1968.  Second place belongs to the Space Shuttle, which flew nine times in the calendar year before the Challenger disaster. In third place is Falcon Heavy, which flew six times in a 13 month period beginning in November 2022. 

</p><p>For the refueling plan to work, Starship will have to break this record by a factor of ten, launching every six days or so across multiple launch facilities. <a id="fn_cadence"></a><a href="#cadence"><span><sup>[1]</sup></span></a>  The refueling program can tolerate a few launch failures, as long as none of them damages a launch pad. 

</p><p>There’s no company better prepared to meet this challenge than SpaceX. Their Falcon 9 rocket has shattered records for both reliability and cadence, and now launches about once every three days.  But it took SpaceX ten years to get from the first orbital Falcon 9 flight to a weekly cadence, and Starship is vastly bigger and more complicated than the Falcon 9. <a id="fn_falcon9"></a><a href="#falcon9"><span><sup>[20]</sup></span></a>

</p><p>Working backwards from the official schedule allows us to appreciate the time pressure facing SpaceX. To make the official Artemis landing date, SpaceX has to land an unmanned HLS prototype on the moon in early 2026. That means tanker flights to fill an orbiting depot would start in late 2025.  This doesn’t leave a lot of time for the company to invent orbital refueling, get it working at scale, make it efficient, deal with boil-off, get Starship launching reliably, begin recovering booster stages,<a id="fn_recovery"></a><a href="#recovery"><span><sup>[21]</sup></span></a> set up additional launch facilities, achieve a weekly cadence,  and at the same time design and test all the other systems that need to go into HLS. 

</p><p>Lest anyone think I’m picking on SpaceX, the development schedule for Blue Origin’s 2029 lander is even more fantastical. That design requires pumping tons of liquid hydrogen between spacecraft in lunar orbit, a challenge perhaps an order of magnitude harder than what SpaceX is attempting. Liquid hydrogen is bulky, boils near absolute zero, and is infamous for its ability to leak through anything (the Shuttle program couldn't get a handle on hydrogen leaks on Earth even after a hundred some launches).  And the rocket Blue Origin needs to test all this technology has never left the ground. 

</p><p>The upshot is that NASA has put a pair of last-minute long-shot technology development programs between itself and the moon.  Particularly striking is the contrast between the ambition of the HLS designs and the extreme conservatism and glacial pace of SLS/Orion.  The same organization that spent 23 years and 20 billion dollars building the world's most vanilla spacecraft demands that SpaceX darken the sky with Starships within four years of signing the initial HLS contract. While thrilling for SpaceX fans, this is pretty unserious behavior from the nation’s space agency, which had several decades' warning that going to the moon would require a lander. 

</p><p>All this to say, it's universally understood that there won’t be a moon landing in 2026. At some point NASA will have to officially slip the schedule, as it did in 2021, 2023, and at the start of this year.  If this accelerating pattern of delays continues, by year’s end we might reach a state of continuous postponement, a kind of scheduling singularity where the landing date for Artemis 3 recedes smoothly and continuously into the future. 

</p><p>Otherwise, it's hard to imagine a manned lunar landing before 2030, if the Artemis program survives that long.


</p><p><img src="https://idlewords.com/images/artemis_cart.jpg" width="100%" alt="Interior of Skylab"></p>

<p><b><a name="conclusion">VII. Conclusion</a></b></p>

<p>I want to stress that there’s nothing wrong with NASA making big bets on technology.  Quite the contrary, the audacious HLS contracts may be the healthiest thing about Artemis. Visionaries at NASA identified a futuristic new energy source (space billionaire egos) and found a way to tap it on a fixed-cost basis.  If SpaceX or Blue Origin figure out how to make cryogenic refueling practical, it will mean a big step forward for space exploration, exactly the thing NASA should be encouraging. And if the technology doesn’t pan out, we’ll have found that out mostly by spending Musk’s and Bezos’s money.  

</p><p>The real problem with Artemis is that it doesn’t think through the consequences of its own success.  A working infrastructure for orbital refueling would make SLS and Orion superfluous.  Instead of waiting two years to go up on a $4 billion rocket, crews and cargo could launch every weekend on cheap commercial rockets, refueling in low Earth orbit on their way to the Moon. A similar logic holds for Gateway.  Why assemble a space station out of habitrail pieces out in lunar orbit, like an animal, when you can build one on Earth and launch it in one piece? Better yet, just spraypaint “<b>GATEWAY</b>” on the side of the nearest Starship, send it out to NRHO, and save NASA and its international partners billions.  Having a working gas station in low Earth orbit fundamentally changes what is possible, in a way the SLS/Orion arm of Artemis doesn't seem to recognize.

</p><p>Conversely, if SpaceX and Blue Origin can’t make cryogenic refueling work, then NASA has no plan B for landing on the moon. All the Artemis program will be able to do is assemble Gateway.  Promising taxpayers the moon only to deliver ISS Jr. does not broadcast a message of national greatness, and is unlikely to get Congress excited about going to Mars.  The hurtful comparisons between American dynamism in the 1960’s and whatever it is we have now will practically write themselves. 

</p><p>What NASA is doing is like an office worker blowing half their salary on lottery tickets while putting the other half in a pension fund.  If the lottery money comes through, then there was really no need for the pension fund. But without the lottery win, there’s not enough money in the pension account to retire on. The two strategies don't make sense together.

</p><p>There’s a ‘realist’ school of space flight that concedes all this but asks us to look at the bigger picture. We’re never going to have the perfect space program, the argument goes, but the important thing is forward progress. And Artemis is the first program in years to survive a presidential transition and have a shot at getting us beyond low Earth orbit.  With Artemis still funded, and Starship making rapid progress, at some point we’ll finally see American astronauts back on the moon.

</p><p>But this argument has two flaws. The first is that it feeds a cycle of dysfunction at NASA that is rapidly making it impossible for us to go anywhere.    Holding human space flight to a different standard than NASA’s science missions has been a disaster for space exploration.  Right now the Exploration Systems Development Mission Directorate (the entity responsible for manned space flight) couldn’t build a toaster for less than a billion dollars.  Incompetence, self-dealing, and mismanagement that end careers on the science side of NASA are not just tolerated but rewarded on the human space flight side.  Before we let the agency build out its third white elephant project in forty years,  it’s worth reflecting on what we're getting in return for half our exploration budget.

</p><p>The second, more serious flaw in the “realist” approach is that it enables a culture of institutional mendacity that must ultimately be fatal at an engineering organization. We've reached a point where NASA lies constantly, to both itself and to the public. It lies about schedules and capabilities. It lies about the costs and the benefits of its human spaceflight program. And above all, it lies about risk. All the institutional pathologies identified in the Rogers Report and the Columbia Accident Investigation Board are alive and well in Artemis—groupthink, management bloat, intense pressure to meet impossible deadlines, and a willingness to manufacture engineering rationales to justify flying unsafe hardware.

</p><div><p>Do we really have to wait for another tragedy, and another beautifully produced Presidential Commission report, to see that Artemis is broken? 

</p></div><p><b><a name="notes">Notes</a></b></p>



<p><a id="cost"></a><a href="#fn_cost">[1]</a>  Without NASA's help, it's hard to put a dollar figure on a mission without making somewhat arbitrary decisions about what to include and exclude. The $7-10 billion estimate comes from a Bush-era official in the Office of Management and Budget commenting <a href="https://forum.nasaspaceflight.com/index.php?topic=60228.msg2559545#msg2559545">on the NASA Spaceflight Forum</a>

</p><blockquote>
And that $7.2B assumes Artemis III stays on schedule.  Based on the FY24 budget request, each additional year between Artemis II and Artemis III adds another $3.5B to $4.0B in Common Exploration to Artemis III.  If Artemis III goes off in 2027, then it will be $10.8B total.  If 2028, then $14.3B.
</blockquote>

 <p>In other words, it's hard to break out an actual cost while the launch dates for both Artemis II and III keep slipping.
 
 </p><p>NASA's own Inspector General estimates the cost of <a href="https://oig.nasa.gov/wp-content/uploads/2024/02/IG-22-003.pdf">just the SLS/Orion portion</a> of a moon landing at $4.1 billion. 


</p><p><a id="nasatime"></a><a href="#fn_nasatime">[2]</a>  The first US suborbital flight, Friendship 7, launched on May 15, 1961. Armstrong and Aldrin landed on the moon eight years and two months later, on July 21, 1969.  President Bush announced the goal of returning to the Moon in a January 2004 speech, setting the target date for the first landing "as early as 2015", and no later than 2020.

</p><p><a id="slscost"></a><a href="#fn_slscost">[3]</a> NASA refuses to track the per-launch cost of SLS, so it's easy to get into nerdfights. Since the main cost driver on SLS is the gigantic workforce employed on the project, something like two or three times the headcount of SpaceX, the cost per launch depends a lot on cadence.    If you assume a yearly launch rate (the official line), then the rocket costs $2.1 billion a launch. If like me you think one launch every two years is optimistic, the cost climbs up into the $4-5 billion range.   


</p><p><a id="dollarbills"></a><a href="#fn_dollarbills">[4]</a> The SLS weighs 2,600 metric tons fully fueled, and conveniently enough a dollar bill weighs about 1 gram. 

</p><p><a id="refurb"></a><a href="#fn_refurb">[5]</a> SpaceX does not disclose the cost, but it's widely assumed the Raptor engine used on Superheavy costs $1 million.   



</p><p><a id="rs25restart"></a><a href="#fn_rs25restart">[6]</a>  The $145 million figure comes from <a href="https://spacenews.com/aerojet-rocketdyne-defends-sls-engine-contract-costs/">dividing the contract cost by the number of engines</a>, caveman style. Others have reached a figure of $100 million for the unit cost of these engines. The important point is not who is right but the fact that NASA is paying vastly more than anyone else for engines of this class.


</p><p><a id="fheavy"></a><a href="#fn_fheavy">[7]</a> $250M is the figure you get by dividing the $3.2 billion Booster Production and Operations contract to Northrop Grumman by the number of boosters (12) in the contract.  Source:  <a href="https://oig.nasa.gov/wp-content/uploads/2024/02/IG-23-015.pdf">Office of the Inspector General</a>. For cost overruns replacing asbestos, see the OIG report on <a href="https://oig.nasa.gov/wp-content/uploads/2024/02/IG-23-015.pdf">NASA’s Management of the Space Launch
System Booster and Engine Contracts</a>.  The Department of Defense paid <a href"https:="" www.nasaspaceflight.com="" 2023="" 12="" otv-7="" "="">$130 million</a> for a Falcon Heavy launch in 2023. 

</p><p><a id="electron"></a><a href="#fn_electron">[8]</a>  Rocket Lab developed, tested, and flew its Electron rocket for a total program cost of <a href="https://twitter.com/Peter_J_Beck/status/1302692025297379328">$100 million</a>. 

</p><p><a id="heatshield"></a><a href="#fn_heatshield">[9]</a> In particular, the separation bolts embedded in the Orion heat shield were built based on a flawed thermal model, and need to be redesigned to safely fly a crew. From <a href="https://oig.nasa.gov/office-of-inspector-general-oig/audit-reports/nasas-readiness-for-the-artemis-ii-crewed-mission-to-lunar-orbit/">the OIG report</a>:

</p><blockquote>
Separation bolt melt beyond the thermal barrier during reentry can expose the vehicle to hot gas ingestion behind the heat shield, exceeding Orion’s structural limits and resulting in the breakup of the vehicle and loss of crew. Post-flight inspections determined there was a discrepancy in the thermal model used to predict the bolts’ performance pre-flight. Current predictions using the correct information suggest the bolt melt exceeds the design capability of Orion.
</blockquote>

<p>The current plan is to work around these problems on Artemis 2, and then redesign the components for Artemis 3. That means astronauts have to fly at least twice with an untested heat shield design. 




</p><p><a id="oriondeltav"></a><a href="#fn_oriondeltav">[10]</a> 
Orion/ESM has a delta V budget of 1340 m/s. Getting into and out of an equatorial low lunar orbit takes about 1800 m/s, more for a polar orbit.  (See <a href="https://www.nasa.gov/wp-content/uploads/2023/10/nrho-artemis-orbit.pdf">source</a>.)

</p><p><a id="nrho"></a><a href="#fn_nrho">[11]</a> 
It takes about 900 m/s of total delta V to get in and out of NHRO, comfortably within Orion/ESM's 1340 m/s budget.  (See <a href="https://www.nasa.gov/wp-content/uploads/2023/10/nrho-artemis-orbit.pdf">source</a>.)

</p><p><a id="abort"></a><a href="#fn_abort">[12]</a> 
In <i>Carrying the Fire</i>, Apollo 11 astronaut Michael Collins recalls carrying a small notebook covering 18 lunar rendezvous scenarios he might be called on to fly in various contingencies. If the Lunar Module could get itself off the surface, there was probably a way to dock with it.

</p><p>For those too young to remember, Tang is a powdered orange drink <a href="https://www.foodandwine.com/lifestyle/how-nasa-made-tang-cool">closely associated</a> with the American space program. 


</p><p><a id="abortnrho"></a><a href="#fn_abortnrho">[13]</a> For a detailed (if somewhat cryptic) discussion of possible Artemis abort modes to NRHO, see <a href="https://ntrs.nasa.gov/api/citations/20230002566/downloads/HLS%20NRHO%20to%20Lunar%20Surface%20and%20Back%20Mission%20Design_STRIVES.pptx.pdf">HLS NRHO to Lunar Surface and
Back Mission Design</a>, NASA 2022.

</p><p><a id="hlsprop"></a><a href="#fn_hlsprop">[14]</a> This is my own speculative guess; the answer is very sensitive to the dry weight of HLS and the boil-off rate of its cryogenic propellants. Delta V from the lunar surface to NRHO is 2,610 m/sec.  Assuming HLS weighs 120 tons unfueled, it would need about 150 metric tons of propellant to get into NRHO from the lunar surface. Adding safety margin, fuel for docking operations, and allowing for a week of boiloff gets me to about 200 tons. 

</p><p><a id="landingrisk"></a><a href="#fn_landingrisk">[15]</a>
The main safety issue is the difficult thermal environment at the landing site, where the Sun sits just above the horizon, heating half the lander. If it weren't for the NRHO constraint, it's very unlikely Artemis 3 would spend more than a day or two on the lunar surface. 

</p><p><a id="iss"></a><a href="#fn_iss">[16]</a>
The ISS program has been repeatedly extended, but the station is coming up against physical limiting factors (like metal fatigue) that will soon make it too dangerous to use.

</p><p><a id="ascent"></a><a href="#fn_ascent">[17]</a>
Recent comments by NASA suggest SpaceX has <a href="https://twitter.com/jeff_foust/status/1783877352432263175">voluntarily added an ascent phase</a> to its landing demo, ending a pretty untenable situation. However, there's still no requirement that the unmanned landing/ascent demo be performed using the same lander design that will fly on the actual mission, another oddity in the HLS contract.


</p><p><a id="refuel"></a><a href="#fn_refuel">[18]</a>
To be precise, I'm talking about moving bulk propellant between rockets in orbit. There are resupply flights to the International Space Station that deliver about 850 kilograms of non-cryogenic propellant to boost the station in its orbit, and there have been small-scale experiments in refueling satellites. But no one has attempted refueling a flown rocket stage in space, cryogenic or otherwise. 



</p><p><a id="cadence"></a><a href="#fn_cadence">[19]</a>

Both SpaceX's <a href="https://youtu.be/vOg49BVhU40?si=6q8R2qvkmEDPGy0V&amp;t=2381" "="">Kathy Lueders</a> and NASA confirm Starship needs to launch from multiple sites.  Here's an excerpt from <a href="https://www.nasa.gov/wp-content/uploads/2024/01/heoc-november-2023-final-v2.pdf">the minutes</a> of the NASA Advisory Council Human Exploration and Operations Committee meeting on November 17 and 20, 2023:

</p><blockquote>
Mr. [Wayne] Hale asked where Artemis III will launch from. [Assistant Deputy AA for Moon to Mars Lakiesha] Hawkins said that launch pads will be used in Florida and potentially Texas. The missions will need quite a number of tankers; in order to meet the schedule, there will need to be a rapid succession of launches of fuel, requiring more than one site for launches on a 6-day rotation schedule, and multiples of launches. 
</blockquote>


<p><a id="falcon9"></a><a href="#fn_falcon9">[20]</a> Falcon 9 first flew in June of 2010 and achieved a weekly launch cadence over a span of six launches starting in November 2020. 


</p><p><a id="recovery"></a><a href="#fn_recovery">[21]</a> Recovering Superheavy stages is not a NASA requirement for HLS, but it's a huge cost driver for SpaceX given the number of launches involved. 


</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hertz Charging a Tesla Renter for Gas Was Not an Isolated Incident (214 pts)]]></title>
            <link>https://www.thedrive.com/news/hertz-charging-a-tesla-renter-for-gas-was-not-an-isolated-incident</link>
            <guid>40410341</guid>
            <pubDate>Sun, 19 May 2024 22:52:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thedrive.com/news/hertz-charging-a-tesla-renter-for-gas-was-not-an-isolated-incident">https://www.thedrive.com/news/hertz-charging-a-tesla-renter-for-gas-was-not-an-isolated-incident</a>, See on <a href="https://news.ycombinator.com/item?id=40410341">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-id="page-content" data-og-area="article-blocks" id="incArticle"><p>Hertz's foray into renting EVs hasn't been a runaway success. The rental agency is <a href="https://www.thedrive.com/news/ex-hertz-tesla-model-3s-cost-as-little-as-14000-would-you-buy-one" target="_blank" rel="noreferrer noopener">liquidating its excess Teslas</a> due to limited demand and tanking values, while those who have rented its EVs haven't always had a good experience. Last week, we reported on a customer who was <a href="https://www.thedrive.com/news/hertz-is-charging-tesla-model-3-renter-277-fee-for-gas-wont-back-down" target="_blank" rel="noreferrer noopener">charged $277 for gasoline his rented Tesla couldn't have possibly used</a>—and now, we've heard from other Hertz customers who say they've been charged even more.</p><p>Hertz caught attention last week for how it handled a customer whom it had charged a "Skip the Pump" fee, which allows renters to pay a premium for Hertz to refill the tank for them. But of course, this customer's rented <a href="https://www.thedrive.com/news/2024-tesla-model-3-with-new-face-and-interior-finally-arrives-in-us" target="_blank" rel="noreferrer noopener">Tesla Model 3</a> didn't use gas—it draws power from a battery—and Hertz has a separate, flat fee for EV recharges. Nevertheless, the customer was charged $277.39 despite returning the car with the exact same charge they left with, and Hertz refused to refund it until after our story ran.</p><p>It's no isolated incident either, as other customers have written in to inform us that it happened to them, too.</p><figure data-og-block-area="article-blocks" data-og-block-nth="1" data-og-block-type="core/image"><span data-rawhtml="1">A lineup of Hertz Polestar 2 EVs. <em>Hertz</em> </span></figure><p>Evan Froehlich told us that when he booked a Tesla Model 3 Long Range, his problems began well before it came time to return the car. On pickup, he was told the Long Range model he'd reserved was unavailable, and that he'd been downgraded to a standard-range model. He had to go to a manager to get any recourse, which ended up being just a $22 discount.</p><p>Froehlich returned the rental at 21 percent charge, expecting to pay a flat $25 recharge fee. (It's ordinarily $35, but Hertz's loyalty program discounts it.) To Froehlich's surprise, he was hit with a $340.97 "Skip the Pump" fee, which can be applied after returning a car if it's not requested beforehand. He says Hertz's customer service was difficult to reach, and that it took making a ruckus on social media to get Hertz's attention.</p><p>In the end, a Hertz representative was able to review the charge and have it reversed. But Froelich was told it could take five to seven business days for the money to transfer, and for his troubles, he was offered just one free day of renting an EV (to be redeemed this calendar year, no less).</p><div data-og-block-area="article-blocks" data-og-block-nth="1" data-og-block-type="core/gallery"><figcaption data-rawhtml="1">Some of Evan Froehlich's interactions with Hertz customer service regarding his fuel charges. <em>Evan Froehlich</em></figcaption></div><p>Fellow Hertz customer Toan Le reported an even worse experience with their Tesla rental earlier this month. They told us they prepaid $329.83 for a week with a Model 3, and returned the car expecting to pay only $25 for Hertz to charge it. Le was then apparently billed $690.32, some of which was redundant billing for the rental they say they'd already paid for.</p><p>To add insult to injury, their invoice (shown here) indicates more than two thirds of that, $475.19, was a fuel charge, which was applied in addition to the $25 charging fee. They also faced a $125.01 "rebill" for using <a href="https://www.thedrive.com/news/oops-tesla-is-re-hiring-some-charging-personnel-musk-laid-off" target="_blank" rel="noreferrer noopener">the Supercharger network</a> during their rental, which <a href="https://www.yellowbullet.com/threads/rented-a-tesla.2679591/" target="_blank" rel="noreferrer noopener">other Hertz customers have expressed surprise and frustration with</a>. Charging costs can vary, but a 75-percent charge from a Supercharger will often cost in the region of just $15.</p><div data-og-block-area="article-blocks" data-og-block-nth="2" data-og-block-type="core/gallery"><figcaption data-rawhtml="1">Toan Le's invoices for their Tesla Model 3 rental, plus an email from Hertz's customer service. <em>Toan Le</em></figcaption></div><p>Le was able to get the fuel charge waived, but was still dissatisfied to see such a large followup bill. Despite reaching the top tier of Hertz's loyalty program, they told us they "might reconsider renting another vehicle from Hertz." How many other Hertz customers have been inappropriately billed is unclear, though <a href="https://www.facebook.com/david.kreeger.9/posts/pfbid0866bUEYfvxJ3vYS2dxDFU7Eyf9uQidgYPvLw7rS7VeVtrrYoApJtAQZgzEXteGCkl" target="_blank" rel="noreferrer noopener">a March 2023 Facebook post</a> documenting a similar case indicates this has been happening for more than a year.</p><p><em>Got a tip or question for the author? You can reach them here: james@thedrive.com</em></p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beating Jeff's 3.14 Ghz Raspberry Pi 5 (153 pts)]]></title>
            <link>https://jonatron.github.io/randomstuff/pivolt/</link>
            <guid>40409718</guid>
            <pubDate>Sun, 19 May 2024 21:02:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jonatron.github.io/randomstuff/pivolt/">https://jonatron.github.io/randomstuff/pivolt/</a>, See on <a href="https://news.ycombinator.com/item?id=40409718">Hacker News</a></p>
<div id="readability-page-1" class="page">

<h2>Beating Jeff's 3.14 Ghz Raspberry Pi 5</h2>

<p>Jeff came up against a 1V limit in his video 
  "<a href="https://www.youtube.com/watch?v=TTIkZBsVJyA&amp;t=931s"> Overclocking Raspberry Pi 5 to 3.14 GHz on Pi Day </a>"
after <a href="https://github.com/raspberrypi/firmware/issues/1876">firmware</a> was released to remove the 3Ghz limit.

</p><p>There's a silicon lottery, and I tried to see what I could get on my particular Pi.
  Geekbench takes ages to run and has a large run-to-run variance.
  <br>
  </p><pre>sysbench cpu run</pre> allows me to iterate more quickly.
  I'm using this cooler, and I haven't tried any other cooling solutions. <br>
  <a href="https://thepihut.com/products/aluminium-armour-heatsink-case-for-raspberry-pi-5">
    <img width="250" height="250" src="https://jonatron.github.io/randomstuff/pivolt/case.webp"></a>

<pre>over_voltage_delta=50000
arm_freq=2900
force_turbo=1

    total number of events:              32951
    total number of events:              32954

over_voltage_delta=50000
arm_freq=3000
force_turbo=1

    total number of events:              34076
    total number of events:              34094

over_voltage_delta=50000
arm_freq=3200
force_turbo=1
    total number of events:              36373
    total number of events:              36365
</pre>

<p>arm_freq=3300 or 3.3Ghz is where it gets very unstable.

</p><h2>Firmware</h2>

<p>The 1V limit is in the firmware. The Raspberry Pi is weird, because it starts by running code on the VPU/GPU. It's an obscure Brodcom <a href="https://en.wikipedia.org/wiki/VideoCore">VideoCore</a> instruction set.

</p><p>There's a <a href="https://pip.raspberrypi.com/categories/685-whitepapers-app-notes/documents/RP-004651-WP/Raspberry-Pi-4-Boot-Security.pdf">PDF</a>
 documenting the Raspberry Pi 4 boot security.

</p><p>Basically the first 3 boot stages are BOOTROM (AKA BL0), bootsys, and bootmain. Bootrom is baked into the CPU, bootsys and bootmain are signed, so I can't modify them without the signing key, which I don't have. Somehow I doubt Raspberry Pi or Broadcom would hand me the keys.
  There's some differences between the
  <a href="https://www.raspberrypi.com/documentation/computers/raspberry-pi.html#raspberry-pi-4-and-raspberry-pi-5-boot-flow">Raspberry Pi 4 and 5 boot process.</a>

</p><p>There are some tools to parse and extract the Raspberry Pi 5 firmware (stored on the eeprom):

  <br><a href="https://github.com/info-beamer/rpi-eeprom-tools">https://github.com/info-beamer/rpi-eeprom-tools</a>
  <br><a href="https://github.com/raspberrypi/rpi-eeprom/">https://github.com/raspberrypi/rpi-eeprom/</a>

</p><p>Handily, a Github user made <a href="https://github.com/NationalSecurityAgency/ghidra/pull/1147">Ghidra support for Videocore</a>.

</p><p>Searching bootmain for "volt", I found what looks a lot like a voltage limiter.

</p><p><img src="https://jonatron.github.io/randomstuff/pivolt/ghid.png">

</p><p>A single mov.cc instruction can be patched to remove the voltage limit. However, it's in bootmain, which is signed, so we can't just patch bootmain and flash the eeprom.
</p><p>However, as a root linux user on Raspberry Pi full access to system memory, including memory used by the videocore. I mmap'd /dev/vc-mem, searched for the instruction and replaced it, but i'll leave that as an exercise to the reader. I don't want people blaming me if their Pi decides to halt and catch fire.

</p><h2>Slowing down before I speed up</h2>

<p>If I set arm_freq=3300, it isn't stable. Also, I can't use force_turbo. To get it usable, I limited the cpu to 2.9Ghz as early as possible:

</p><pre><b>/lib/systemd/system/slowcpu.service</b></pre>
<br>

<pre>[Unit]
Description=Slow CPU
Before=basic.target
After=local-fs.target sysinit.target
DefaultDependencies=no

[Service]
Type=oneshot
ExecStart=/bin/bash /slowcpu

[Install]
WantedBy=basic.target
</pre>
<br>
<pre><b>/slowcpu</b></pre>
<br>
<pre>echo 2900000 | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_max_freq</pre>
<pre>sudo systemctl enable slowcpu.service</pre>

<p>From there, I can remove the voltage limit:
  <br></p><pre>sudo ./removelimit &amp;&amp; vcgencmd cache_flush</pre>

<p>Then I can put the frequency limit back up to 3.3Ghz
  <br></p><pre>echo 3300000 | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_max_freq</pre>

<p>And make it like force_turbo was on:
  <br></p><pre>echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor</pre>

<p>Checking my voltage and clock with over_voltage_delta=60000:
<br>
</p><pre>$ vcgencmd measure_volts
volt=1.0437V
$ vcgencmd measure_clock arm
frequency(0)=3300034816
</pre>

<p>After all that...</p>

  <br>
  <pre>    total number of events:              37713
    total number of events:              37712
  </pre>

<p>...That wasn't worth it.







</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meteor Just Seen in Portugal (259 pts)]]></title>
            <link>https://old.reddit.com/r/interestingasfuck/comments/1cva6j6/meteor_just_seen_in_portugal_23h45/</link>
            <guid>40409710</guid>
            <pubDate>Sun, 19 May 2024 21:01:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/interestingasfuck/comments/1cva6j6/meteor_just_seen_in_portugal_23h45/">https://old.reddit.com/r/interestingasfuck/comments/1cva6j6/meteor_just_seen_in_portugal_23h45/</a>, See on <a href="https://news.ycombinator.com/item?id=40409710">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><hr>

<p>A place to share (almost) anything and everything interesting as fuck.</p>

<hr>

<h2><a href="https://www.reddit.com/r/interestingasfuck/wiki/index#wiki_rules.3A">Please read our rules</a></h2>

<ol>
<li><p><strong>Posts MUST be INTERESTING AS FUCK.</strong> <a href="https://www.reddit.com/r/interestingasfuck/wiki/index#wiki_rule_1_-_posts_must_be_interesting_as_fuck">more&gt;&gt;</a></p></li>
<li><p>Titles must be descriptive and directly related to the content <a href="https://www.reddit.com/r/interestingasfuck/wiki/index#wiki_rule_2_-_titles_must_be_descriptive_and_directly_related_to_the_content">more&gt;&gt;</a></p></li>
<li><p>No porn or gore. <a href="https://www.reddit.com/r/interestingasfuck/wiki/index#wiki_rule_3_-_no_porn.2Fgore">more&gt;&gt;</a></p></li>
<li><p>No personal information, doxing, witch hunt, brigading, or <strong>any subreddit-related meta-drama</strong>. <a href="https://www.reddit.com/r/interestingasfuck/wiki/index#wiki_rule_4_-_no_doxing.2Fwitch_hunts">more&gt;&gt;</a></p></li>
<li><p>Source your claims. <a href="https://www.reddit.com/r/interestingasfuck/wiki/index#wiki_rule_5_-_provide_a_source_when_the_title_is_in_doubt">more&gt;&gt;</a></p></li>
<li><p>No FCoO/flooding. <a href="https://www.reddit.com/r/interestingasfuck/wiki/index#wiki_rule_6_-_no_fcoo.2Fflooding">more&gt;&gt;</a> </p></li>
<li><p>No self-promotion, bots or any kind of spam. <a href="https://www.reddit.com/r/interestingasfuck/wiki/index#wiki_rule_7_-_no_self-promotion">more&gt;&gt;</a></p></li>
<li><p>Comments must be civil. Any racism, bigotry, or any other kind of hate speech is strictly prohibited and will result in a ban. <a href="https://www.reddit.com/r/interestingasfuck/wiki/index#wiki_rule_8_-_civility">more&gt;&gt;</a></p></li>
<li><p>Reposts of images on the front page, or within the set limit of <a href="https://old.reddit.com/r/interestingasfuck/top">/r/interestingasfuck/top</a>, will be removed <a href="https://www.reddit.com/r/interestingasfuck/wiki/index#wiki_rule_9_-_repost_limitations">more&gt;&gt;</a></p></li>
<li><p>No gossip or tabloid-type material <a href="https://www.reddit.com/r/interestingasfuck/wiki/index#wiki_rule_10_-_no_gossip_or_tabloid-type_material">more&gt;&gt;</a></p></li>
</ol>

<hr>

<p><a href="https://www.reddit.com/r/interestingasfuck/wiki/index#wiki_additional.2Ftemporary_rules">Additional Rules</a></p>

<ul>
<li><p>Serial reposters may be filtered or banned.</p></li>
<li><p>Established accounts will get escalating bans if they willfully ignore the posting rules</p></li>
<li><p>We require new users to post original content, not lazy reposts. Reposting as a very new account might get you banned</p></li>
<li><p>All posts by new users require mod approval in order to weed out spammers.</p></li>
<li><p>Please mark spoilers like this:  <code>&gt;!text here!&lt;</code> Click/tap to <span>read</span>.</p></li>
<li><p>All posts concerning the Palestine/Israel conflict will be locked for comments.  Such posts are still subject to all rules above.</p></li>
</ul>

<hr>

<p>If you want a piece of content that belongs to you to be removed from <a href="https://old.reddit.com/r/interestingasfuck">/r/interestingasfuck</a> then please file a copyright notice <a href="https://reddit.zendesk.com/hc/en-us/requests/new?ticket_form_id=73465">here</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Operation CHARM: Car repair manuals for everyone (241 pts)]]></title>
            <link>https://charm.li/</link>
            <guid>40409588</guid>
            <pubDate>Sun, 19 May 2024 20:39:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://charm.li/">https://charm.li/</a>, See on <a href="https://news.ycombinator.com/item?id=40409588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p><b>Operation CHARM</b>: Car repair manuals for everyone.</p>
<div><p><a href="https://charm.li/">Home</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Llama3 implemented from scratch (676 pts)]]></title>
            <link>https://github.com/naklecha/llama3-from-scratch</link>
            <guid>40408880</guid>
            <pubDate>Sun, 19 May 2024 18:42:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/naklecha/llama3-from-scratch">https://github.com/naklecha/llama3-from-scratch</a>, See on <a href="https://news.ycombinator.com/item?id=40408880">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">llama3 implemented from scratch</h2><a id="user-content-llama3-implemented-from-scratch" aria-label="Permalink: llama3 implemented from scratch" href="#llama3-implemented-from-scratch"></a></p>
<p dir="auto">in this file, i implemented llama3 from scratch, one tensor and matrix multiplication at a time.
<br>
also, im going to load tensors directly from the model file that meta provided for llama3, you need to download the weights before running this file.
here is the offical link to download the weights: <a href="https://llama.meta.com/llama-downloads/" rel="nofollow">https://llama.meta.com/llama-downloads/</a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/archi.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/archi.png"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">tokenizer</h2><a id="user-content-tokenizer" aria-label="Permalink: tokenizer" href="#tokenizer"></a></p>
<p dir="auto">im not going to implement a bpe tokenizer (but andrej karpathy has a really clean implementation)
<br>
link to his implementation: <a href="https://github.com/karpathy/minbpe">https://github.com/karpathy/minbpe</a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/karpathyminbpe.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/karpathyminbpe.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="from pathlib import Path
import tiktoken
from tiktoken.load import load_tiktoken_bpe
import torch
import json
import matplotlib.pyplot as plt

tokenizer_path = &quot;Meta-Llama-3-8B/tokenizer.model&quot;
special_tokens = [
            &quot;<|begin_of_text|>&quot;,
            &quot;<|end_of_text|>&quot;,
            &quot;<|reserved_special_token_0|>&quot;,
            &quot;<|reserved_special_token_1|>&quot;,
            &quot;<|reserved_special_token_2|>&quot;,
            &quot;<|reserved_special_token_3|>&quot;,
            &quot;<|start_header_id|>&quot;,
            &quot;<|end_header_id|>&quot;,
            &quot;<|reserved_special_token_4|>&quot;,
            &quot;<|eot_id|>&quot;,  # end of turn
        ] + [f&quot;<|reserved_special_token_{i}|>&quot; for i in range(5, 256 - 5)]
mergeable_ranks = load_tiktoken_bpe(tokenizer_path)
tokenizer = tiktoken.Encoding(
    name=Path(tokenizer_path).name,
    pat_str=r&quot;(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&quot;,
    mergeable_ranks=mergeable_ranks,
    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},
)

tokenizer.decode(tokenizer.encode(&quot;hello world!&quot;))"><pre><span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>
<span>import</span> <span>tiktoken</span>
<span>from</span> <span>tiktoken</span>.<span>load</span> <span>import</span> <span>load_tiktoken_bpe</span>
<span>import</span> <span>torch</span>
<span>import</span> <span>json</span>
<span>import</span> <span>matplotlib</span>.<span>pyplot</span> <span>as</span> <span>plt</span>

<span>tokenizer_path</span> <span>=</span> <span>"Meta-Llama-3-8B/tokenizer.model"</span>
<span>special_tokens</span> <span>=</span> [
            <span>"&lt;|begin_of_text|&gt;"</span>,
            <span>"&lt;|end_of_text|&gt;"</span>,
            <span>"&lt;|reserved_special_token_0|&gt;"</span>,
            <span>"&lt;|reserved_special_token_1|&gt;"</span>,
            <span>"&lt;|reserved_special_token_2|&gt;"</span>,
            <span>"&lt;|reserved_special_token_3|&gt;"</span>,
            <span>"&lt;|start_header_id|&gt;"</span>,
            <span>"&lt;|end_header_id|&gt;"</span>,
            <span>"&lt;|reserved_special_token_4|&gt;"</span>,
            <span>"&lt;|eot_id|&gt;"</span>,  <span># end of turn</span>
        ] <span>+</span> [<span>f"&lt;|reserved_special_token_<span><span>{</span><span>i</span><span>}</span></span>|&gt;"</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>5</span>, <span>256</span> <span>-</span> <span>5</span>)]
<span>mergeable_ranks</span> <span>=</span> <span>load_tiktoken_bpe</span>(<span>tokenizer_path</span>)
<span>tokenizer</span> <span>=</span> <span>tiktoken</span>.<span>Encoding</span>(
    <span>name</span><span>=</span><span>Path</span>(<span>tokenizer_path</span>).<span>name</span>,
    <span>pat_str</span><span>=</span><span>r"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"</span>,
    <span>mergeable_ranks</span><span>=</span><span>mergeable_ranks</span>,
    <span>special_tokens</span><span>=</span>{<span>token</span>: <span>len</span>(<span>mergeable_ranks</span>) <span>+</span> <span>i</span> <span>for</span> <span>i</span>, <span>token</span> <span>in</span> <span>enumerate</span>(<span>special_tokens</span>)},
)

<span>tokenizer</span>.<span>decode</span>(<span>tokenizer</span>.<span>encode</span>(<span>"hello world!"</span>))</pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">reading the model file</h2><a id="user-content-reading-the-model-file" aria-label="Permalink: reading the model file" href="#reading-the-model-file"></a></p>
<p dir="auto">normally, reading this depends on how the model classes are written and the variable names inside them.
<br>
but since we are implementing llama3 from scratch we will read the file one tensor at a time.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/model.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/model.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = torch.load(&quot;Meta-Llama-3-8B/consolidated.00.pth&quot;)
print(json.dumps(list(model.keys())[:20], indent=4))"><pre><span>model</span> <span>=</span> <span>torch</span>.<span>load</span>(<span>"Meta-Llama-3-8B/consolidated.00.pth"</span>)
<span>print</span>(<span>json</span>.<span>dumps</span>(<span>list</span>(<span>model</span>.<span>keys</span>())[:<span>20</span>], <span>indent</span><span>=</span><span>4</span>))</pre></div>
<div data-snippet-clipboard-copy-content="[
    &quot;tok_embeddings.weight&quot;,
    &quot;layers.0.attention.wq.weight&quot;,
    &quot;layers.0.attention.wk.weight&quot;,
    &quot;layers.0.attention.wv.weight&quot;,
    &quot;layers.0.attention.wo.weight&quot;,
    &quot;layers.0.feed_forward.w1.weight&quot;,
    &quot;layers.0.feed_forward.w3.weight&quot;,
    &quot;layers.0.feed_forward.w2.weight&quot;,
    &quot;layers.0.attention_norm.weight&quot;,
    &quot;layers.0.ffn_norm.weight&quot;,
    &quot;layers.1.attention.wq.weight&quot;,
    &quot;layers.1.attention.wk.weight&quot;,
    &quot;layers.1.attention.wv.weight&quot;,
    &quot;layers.1.attention.wo.weight&quot;,
    &quot;layers.1.feed_forward.w1.weight&quot;,
    &quot;layers.1.feed_forward.w3.weight&quot;,
    &quot;layers.1.feed_forward.w2.weight&quot;,
    &quot;layers.1.attention_norm.weight&quot;,
    &quot;layers.1.ffn_norm.weight&quot;,
    &quot;layers.2.attention.wq.weight&quot;
]"><pre><code>[
    "tok_embeddings.weight",
    "layers.0.attention.wq.weight",
    "layers.0.attention.wk.weight",
    "layers.0.attention.wv.weight",
    "layers.0.attention.wo.weight",
    "layers.0.feed_forward.w1.weight",
    "layers.0.feed_forward.w3.weight",
    "layers.0.feed_forward.w2.weight",
    "layers.0.attention_norm.weight",
    "layers.0.ffn_norm.weight",
    "layers.1.attention.wq.weight",
    "layers.1.attention.wk.weight",
    "layers.1.attention.wv.weight",
    "layers.1.attention.wo.weight",
    "layers.1.feed_forward.w1.weight",
    "layers.1.feed_forward.w3.weight",
    "layers.1.feed_forward.w2.weight",
    "layers.1.attention_norm.weight",
    "layers.1.ffn_norm.weight",
    "layers.2.attention.wq.weight"
]
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="with open(&quot;Meta-Llama-3-8B/params.json&quot;, &quot;r&quot;) as f:
    config = json.load(f)
config"><pre><span>with</span> <span>open</span>(<span>"Meta-Llama-3-8B/params.json"</span>, <span>"r"</span>) <span>as</span> <span>f</span>:
    <span>config</span> <span>=</span> <span>json</span>.<span>load</span>(<span>f</span>)
<span>config</span></pre></div>
<div data-snippet-clipboard-copy-content="{'dim': 4096,
 'n_layers': 32,
 'n_heads': 32,
 'n_kv_heads': 8,
 'vocab_size': 128256,
 'multiple_of': 1024,
 'ffn_dim_multiplier': 1.3,
 'norm_eps': 1e-05,
 'rope_theta': 500000.0}"><pre><code>{'dim': 4096,
 'n_layers': 32,
 'n_heads': 32,
 'n_kv_heads': 8,
 'vocab_size': 128256,
 'multiple_of': 1024,
 'ffn_dim_multiplier': 1.3,
 'norm_eps': 1e-05,
 'rope_theta': 500000.0}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">we use this config to infer details about the model like</h2><a id="user-content-we-use-this-config-to-infer-details-about-the-model-like" aria-label="Permalink: we use this config to infer details about the model like" href="#we-use-this-config-to-infer-details-about-the-model-like"></a></p>
<ol dir="auto">
<li>the model has 32 transformer layers</li>
<li>each multi-head attention block has 32 heads</li>
<li>the vocab size and so on</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="dim = config[&quot;dim&quot;]
n_layers = config[&quot;n_layers&quot;]
n_heads = config[&quot;n_heads&quot;]
n_kv_heads = config[&quot;n_kv_heads&quot;]
vocab_size = config[&quot;vocab_size&quot;]
multiple_of = config[&quot;multiple_of&quot;]
ffn_dim_multiplier = config[&quot;ffn_dim_multiplier&quot;]
norm_eps = config[&quot;norm_eps&quot;]
rope_theta = torch.tensor(config[&quot;rope_theta&quot;])"><pre><span>dim</span> <span>=</span> <span>config</span>[<span>"dim"</span>]
<span>n_layers</span> <span>=</span> <span>config</span>[<span>"n_layers"</span>]
<span>n_heads</span> <span>=</span> <span>config</span>[<span>"n_heads"</span>]
<span>n_kv_heads</span> <span>=</span> <span>config</span>[<span>"n_kv_heads"</span>]
<span>vocab_size</span> <span>=</span> <span>config</span>[<span>"vocab_size"</span>]
<span>multiple_of</span> <span>=</span> <span>config</span>[<span>"multiple_of"</span>]
<span>ffn_dim_multiplier</span> <span>=</span> <span>config</span>[<span>"ffn_dim_multiplier"</span>]
<span>norm_eps</span> <span>=</span> <span>config</span>[<span>"norm_eps"</span>]
<span>rope_theta</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>config</span>[<span>"rope_theta"</span>])</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">converting text to tokens</h2><a id="user-content-converting-text-to-tokens" aria-label="Permalink: converting text to tokens" href="#converting-text-to-tokens"></a></p>
<p dir="auto">here we use tiktoken (i think an openai library) as the tokenizer</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/tokens.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/tokens.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="prompt = &quot;the answer to the ultimate question of life, the universe, and everything is &quot;
tokens = [128000] + tokenizer.encode(prompt)
print(tokens)
tokens = torch.tensor(tokens)
prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens]
print(prompt_split_as_tokens)"><pre><span>prompt</span> <span>=</span> <span>"the answer to the ultimate question of life, the universe, and everything is "</span>
<span>tokens</span> <span>=</span> [<span>128000</span>] <span>+</span> <span>tokenizer</span>.<span>encode</span>(<span>prompt</span>)
<span>print</span>(<span>tokens</span>)
<span>tokens</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>tokens</span>)
<span>prompt_split_as_tokens</span> <span>=</span> [<span>tokenizer</span>.<span>decode</span>([<span>token</span>.<span>item</span>()]) <span>for</span> <span>token</span> <span>in</span> <span>tokens</span>]
<span>print</span>(<span>prompt_split_as_tokens</span>)</pre></div>
<div data-snippet-clipboard-copy-content="[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]
['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']"><pre><code>[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]
['&lt;|begin_of_text|&gt;', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">converting tokens to their embedding</h2><a id="user-content-converting-tokens-to-their-embedding" aria-label="Permalink: converting tokens to their embedding" href="#converting-tokens-to-their-embedding"></a></p>
<div dir="auto"><p>IM SORRY but this is the only part of the codebase where i use an inbuilt neural network module
<br>
anyway, so our [17x1] tokens are now [17x4096], i.e. 17 embeddings (one for each token) of length 4096
</p><p>

note: keep track of the shapes, it makes it much easier to understand everything</p></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/embeddings.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/embeddings.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="embedding_layer = torch.nn.Embedding(vocab_size, dim)
embedding_layer.weight.data.copy_(model[&quot;tok_embeddings.weight&quot;])
token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)
token_embeddings_unnormalized.shape"><pre><span>embedding_layer</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>Embedding</span>(<span>vocab_size</span>, <span>dim</span>)
<span>embedding_layer</span>.<span>weight</span>.<span>data</span>.<span>copy_</span>(<span>model</span>[<span>"tok_embeddings.weight"</span>])
<span>token_embeddings_unnormalized</span> <span>=</span> <span>embedding_layer</span>(<span>tokens</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)
<span>token_embeddings_unnormalized</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">we then normalize the embedding using rms normalization</h2><a id="user-content-we-then-normalize-the-embedding-using-rms-normalization" aria-label="Permalink: we then normalize the embedding using rms normalization" href="#we-then-normalize-the-embedding-using-rms-normalization"></a></p>
<p dir="auto">please, note after this step the shapes dont change, the values are just normalized
<br>
things to keep in mind, we need a norm_eps (from config) because we dont want to accidently set rms to 0 and divide by 0
<br>
here is the formula:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/rms.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/rms.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# def rms_norm(tensor, norm_weights):
#     rms = (tensor.pow(2).mean(-1, keepdim=True) + norm_eps)**0.5
#     return tensor * (norm_weights / rms)
def rms_norm(tensor, norm_weights):
    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights"><pre><span># def rms_norm(tensor, norm_weights):</span>
<span>#     rms = (tensor.pow(2).mean(-1, keepdim=True) + norm_eps)**0.5</span>
<span>#     return tensor * (norm_weights / rms)</span>
<span>def</span> <span>rms_norm</span>(<span>tensor</span>, <span>norm_weights</span>):
    <span>return</span> (<span>tensor</span> <span>*</span> <span>torch</span>.<span>rsqrt</span>(<span>tensor</span>.<span>pow</span>(<span>2</span>).<span>mean</span>(<span>-</span><span>1</span>, <span>keepdim</span><span>=</span><span>True</span>) <span>+</span> <span>norm_eps</span>)) <span>*</span> <span>norm_weights</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">building the first first layer of the transformer</h2><a id="user-content-building-the-first-first-layer-of-the-transformer" aria-label="Permalink: building the first first layer of the transformer" href="#building-the-first-first-layer-of-the-transformer"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">normalization</h3><a id="user-content-normalization" aria-label="Permalink: normalization" href="#normalization"></a></p>
<p dir="auto">you will see me accessing layer.0 from the model dict (this is the first layer)
<br>
anyway, so after normalizing our shapes are still [17x4096] same as embedding but normalized</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/norm.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/norm.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="token_embeddings = rms_norm(token_embeddings_unnormalized, model[&quot;layers.0.attention_norm.weight&quot;])
token_embeddings.shape"><pre><span>token_embeddings</span> <span>=</span> <span>rms_norm</span>(<span>token_embeddings_unnormalized</span>, <span>model</span>[<span>"layers.0.attention_norm.weight"</span>])
<span>token_embeddings</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">attention implemented from scratch</h3><a id="user-content-attention-implemented-from-scratch" aria-label="Permalink: attention implemented from scratch" href="#attention-implemented-from-scratch"></a></p>
<p dir="auto">let's load the attention heads of the first layer of the transformer</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/qkv.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/qkv.png" width="600"></a>
</p>

<p dir="auto">&gt; when we load the query, key, value and output vectors from the model we notice the shapes to be [4096x4096], [1024x4096], [1024x4096], [4096x4096]
<br>
&gt; at first glance this is weird because ideally we want each q,k,v and o for each head individually
<br>
&gt; the authors of the code bundled them togeather because its easy it helps parallize attention head multiplication.
<br>
&gt; im going to unwrap everything...</p>
<div dir="auto" data-snippet-clipboard-copy-content="print(
    model[&quot;layers.0.attention.wq.weight&quot;].shape,
    model[&quot;layers.0.attention.wk.weight&quot;].shape,
    model[&quot;layers.0.attention.wv.weight&quot;].shape,
    model[&quot;layers.0.attention.wo.weight&quot;].shape
)"><pre><span>print</span>(
    <span>model</span>[<span>"layers.0.attention.wq.weight"</span>].<span>shape</span>,
    <span>model</span>[<span>"layers.0.attention.wk.weight"</span>].<span>shape</span>,
    <span>model</span>[<span>"layers.0.attention.wv.weight"</span>].<span>shape</span>,
    <span>model</span>[<span>"layers.0.attention.wo.weight"</span>].<span>shape</span>
)</pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])"><pre><code>torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">unwrapping query</h3><a id="user-content-unwrapping-query" aria-label="Permalink: unwrapping query" href="#unwrapping-query"></a></p>
<div dir="auto"><p>in the next section we will unwrap the queries from multiple attention heads, the resulting shape is [32x128x4096]
</p><p>
here, 32 is the number of attention heads in llama3, 128 is the size of the query vector and 4096 is the size of the token embedding</p></div>
<div dir="auto" data-snippet-clipboard-copy-content="q_layer0 = model[&quot;layers.0.attention.wq.weight&quot;]
head_dim = q_layer0.shape[0] // n_heads
q_layer0 = q_layer0.view(n_heads, head_dim, dim)
q_layer0.shape"><pre><span>q_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wq.weight"</span>]
<span>head_dim</span> <span>=</span> <span>q_layer0</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_heads</span>
<span>q_layer0</span> <span>=</span> <span>q_layer0</span>.<span>view</span>(<span>n_heads</span>, <span>head_dim</span>, <span>dim</span>)
<span>q_layer0</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([32, 128, 4096])"><pre><code>torch.Size([32, 128, 4096])
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">im going to implement the first head of the first layer</h3><a id="user-content-im-going-to-implement-the-first-head-of-the-first-layer" aria-label="Permalink: im going to implement the first head of the first layer" href="#im-going-to-implement-the-first-head-of-the-first-layer"></a></p>
<p dir="auto">here i access the query weight matrix first head of the first layer, the size of this query weight matrix is [128x4096]</p>
<div dir="auto" data-snippet-clipboard-copy-content="q_layer0_head0 = q_layer0[0]
q_layer0_head0.shape"><pre><span>q_layer0_head0</span> <span>=</span> <span>q_layer0</span>[<span>0</span>]
<span>q_layer0_head0</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">we now multiply the query weights with the token embedding, to recive a query for the token</h3><a id="user-content-we-now-multiply-the-query-weights-with-the-token-embedding-to-recive-a-query-for-the-token" aria-label="Permalink: we now multiply the query weights with the token embedding, to recive a query for the token" href="#we-now-multiply-the-query-weights-with-the-token-embedding-to-recive-a-query-for-the-token"></a></p>
<p dir="auto">here you can see the resulting shape is [17x128], this is because we have 17 tokens and for each token there is a 128 length query.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/q_per_token.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/q_per_token.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)
q_per_token.shape"><pre><span>q_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>q_layer0_head0</span>.<span>T</span>)
<span>q_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">positioning encoding</h2><a id="user-content-positioning-encoding" aria-label="Permalink: positioning encoding" href="#positioning-encoding"></a></p>
<div dir="auto"><p>we are now at a stage where we have a query vector for each token in our prompt, but if you think about it -- the indivitually query vector has no idea about the position in the prompt.
</p><p>
query: "the answer to the ultimate question of life, the universe, and everything is "
</p><p>
in our prompt we have used "the" three times, we need the query vectors of all 3 "the" tokens to have different query vectors (each of size [1x128]) based on their positions in the query. we perform these rotations using RoPE (rotory positional embedding).
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">RoPE</h3><a id="user-content-rope" aria-label="Permalink: RoPE" href="#rope"></a></p>
<p dir="auto">watch this video (this is what i watched) to understand the math.
<a href="https://www.youtube.com/watch?v=o29P0Kpobz0&amp;t=530s" rel="nofollow">https://www.youtube.com/watch?v=o29P0Kpobz0&amp;t=530s</a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/rope.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/rope.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)
q_per_token_split_into_pairs.shape"><pre><span>q_per_token_split_into_pairs</span> <span>=</span> <span>q_per_token</span>.<span>float</span>().<span>view</span>(<span>q_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)
<span>q_per_token_split_into_pairs</span>.<span>shape</span></pre></div>

<div dir="auto"><p>in the above step, we split the query vectors into pairs, we apply a rotational angle shift to each pair!
</p><p>
we now have a vector of size [17x64x2], this is the 128 length queries split into 64 pairs for each token in the prompt! each of those 64 pairs will be rotated by m*(theta) where m is the position of the token for which we are rotating the query!</p></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/qsplit.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/qsplit.png" width="600"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">using dot product of complex numbers to rotate a vector</h2><a id="user-content-using-dot-product-of-complex-numbers-to-rotate-a-vector" aria-label="Permalink: using dot product of complex numbers to rotate a vector" href="#using-dot-product-of-complex-numbers-to-rotate-a-vector"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/freq_cis.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/freq_cis.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="zero_to_one_split_into_64_parts = torch.tensor(range(64))/64
zero_to_one_split_into_64_parts"><pre><span>zero_to_one_split_into_64_parts</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>range</span>(<span>64</span>))<span>/</span><span>64</span>
<span>zero_to_one_split_into_64_parts</span></pre></div>
<div data-snippet-clipboard-copy-content="tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,
        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,
        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,
        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,
        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,
        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,
        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,
        0.9844])"><pre><code>tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,
        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,
        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,
        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,
        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,
        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,
        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,
        0.9844])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)
freqs"><pre><span>freqs</span> <span>=</span> <span>1.0</span> <span>/</span> (<span>rope_theta</span> <span>**</span> <span>zero_to_one_split_into_64_parts</span>)
<span>freqs</span></pre></div>
<div data-snippet-clipboard-copy-content="tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,
        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,
        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,
        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,
        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,
        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,
        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,
        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,
        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,
        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,
        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])"><pre><code>tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,
        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,
        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,
        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,
        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,
        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,
        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,
        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,
        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,
        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,
        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="freqs_for_each_token = torch.outer(torch.arange(17), freqs)
freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)
freqs_cis.shape

# viewing tjhe third row of freqs_cis
value = freqs_cis[3]
plt.figure()
for i, element in enumerate(value[:17]):
    plt.plot([0, element.real], [0, element.imag], color='blue', linewidth=1, label=f&quot;Index: {i}&quot;)
    plt.annotate(f&quot;{i}&quot;, xy=(element.real, element.imag), color='red')
plt.xlabel('Real')
plt.ylabel('Imaginary')
plt.title('Plot of one row of freqs_cis')
plt.show()"><pre><span>freqs_for_each_token</span> <span>=</span> <span>torch</span>.<span>outer</span>(<span>torch</span>.<span>arange</span>(<span>17</span>), <span>freqs</span>)
<span>freqs_cis</span> <span>=</span> <span>torch</span>.<span>polar</span>(<span>torch</span>.<span>ones_like</span>(<span>freqs_for_each_token</span>), <span>freqs_for_each_token</span>)
<span>freqs_cis</span>.<span>shape</span>

<span># viewing tjhe third row of freqs_cis</span>
<span>value</span> <span>=</span> <span>freqs_cis</span>[<span>3</span>]
<span>plt</span>.<span>figure</span>()
<span>for</span> <span>i</span>, <span>element</span> <span>in</span> <span>enumerate</span>(<span>value</span>[:<span>17</span>]):
    <span>plt</span>.<span>plot</span>([<span>0</span>, <span>element</span>.<span>real</span>], [<span>0</span>, <span>element</span>.<span>imag</span>], <span>color</span><span>=</span><span>'blue'</span>, <span>linewidth</span><span>=</span><span>1</span>, <span>label</span><span>=</span><span>f"Index: <span><span>{</span><span>i</span><span>}</span></span>"</span>)
    <span>plt</span>.<span>annotate</span>(<span>f"<span><span>{</span><span>i</span><span>}</span></span>"</span>, <span>xy</span><span>=</span>(<span>element</span>.<span>real</span>, <span>element</span>.<span>imag</span>), <span>color</span><span>=</span><span>'red'</span>)
<span>plt</span>.<span>xlabel</span>(<span>'Real'</span>)
<span>plt</span>.<span>ylabel</span>(<span>'Imaginary'</span>)
<span>plt</span>.<span>title</span>(<span>'Plot of one row of freqs_cis'</span>)
<span>plt</span>.<span>show</span>()</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/implllama3_30_0.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/implllama3_30_0.png" alt="png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">now that we have a complex number (the angle change vector) for every token's query element</h3><a id="user-content-now-that-we-have-a-complex-number-the-angle-change-vector-for-every-tokens-query-element" aria-label="Permalink: now that we have a complex number (the angle change vector) for every token's query element" href="#now-that-we-have-a-complex-number-the-angle-change-vector-for-every-tokens-query-element"></a></p>
<p dir="auto">we can convert our queries (the one we split into pairs) as complex numbers and then dot product to rotate the query based on the position
<br>
honeslty this is beautiful to think about :)</p>
<div dir="auto" data-snippet-clipboard-copy-content="q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)
q_per_token_as_complex_numbers.shape"><pre><span>q_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>q_per_token_split_into_pairs</span>)
<span>q_per_token_as_complex_numbers</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis
q_per_token_as_complex_numbers_rotated.shape"><pre><span>q_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>q_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>
<span>q_per_token_as_complex_numbers_rotated</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">after rotated vector is obtained</h3><a id="user-content-after-rotated-vector-is-obtained" aria-label="Permalink: after rotated vector is obtained" href="#after-rotated-vector-is-obtained"></a></p>
<p dir="auto">we can get back our the queries as pairs by viewing the complex numbers as real numbers again</p>
<div dir="auto" data-snippet-clipboard-copy-content="q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)
q_per_token_split_into_pairs_rotated.shape"><pre><span>q_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>q_per_token_as_complex_numbers_rotated</span>)
<span>q_per_token_split_into_pairs_rotated</span>.<span>shape</span></pre></div>

<p dir="auto">the rotated pairs are now merged, we now have a new query vector (rotated query vector) that is of the shape [17x128] where 17 is the number of tokens and the 128 is the dim of the query vector</p>
<div dir="auto" data-snippet-clipboard-copy-content="q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)
q_per_token_rotated.shape"><pre><span>q_per_token_rotated</span> <span>=</span> <span>q_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>q_per_token</span>.<span>shape</span>)
<span>q_per_token_rotated</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">keys (almost the same as queries)</h2><a id="user-content-keys-almost-the-same-as-queries" aria-label="Permalink: keys (almost the same as queries)" href="#keys-almost-the-same-as-queries"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/keys.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/keys.png" width="600px"></a>
</p>
im lazy as fuck, so im not going to go through the math for keys, the only things you need to keep in mind are:
<br>
&gt; keys generate key vectors also of dimention 128
<br>
&gt; keys have only 1/4th the number of the weights as queries, this is because the weights for keys are shared across 4 heads at a time, to reduce the number of computations need
<br>
&gt; keys are also rotated to add positional info, just like queries because of the same reasons 
<div dir="auto" data-snippet-clipboard-copy-content="k_layer0 = model[&quot;layers.0.attention.wk.weight&quot;]
k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim)
k_layer0.shape"><pre><span>k_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wk.weight"</span>]
<span>k_layer0</span> <span>=</span> <span>k_layer0</span>.<span>view</span>(<span>n_kv_heads</span>, <span>k_layer0</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)
<span>k_layer0</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([8, 128, 4096])"><pre><code>torch.Size([8, 128, 4096])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="k_layer0_head0 = k_layer0[0]
k_layer0_head0.shape"><pre><span>k_layer0_head0</span> <span>=</span> <span>k_layer0</span>[<span>0</span>]
<span>k_layer0_head0</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T)
k_per_token.shape"><pre><span>k_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>k_layer0_head0</span>.<span>T</span>)
<span>k_per_token</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)
k_per_token_split_into_pairs.shape"><pre><span>k_per_token_split_into_pairs</span> <span>=</span> <span>k_per_token</span>.<span>float</span>().<span>view</span>(<span>k_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)
<span>k_per_token_split_into_pairs</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)
k_per_token_as_complex_numbers.shape"><pre><span>k_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>k_per_token_split_into_pairs</span>)
<span>k_per_token_as_complex_numbers</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)
k_per_token_split_into_pairs_rotated.shape"><pre><span>k_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>k_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>)
<span>k_per_token_split_into_pairs_rotated</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)
k_per_token_rotated.shape"><pre><span>k_per_token_rotated</span> <span>=</span> <span>k_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>k_per_token</span>.<span>shape</span>)
<span>k_per_token_rotated</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">at this stage now have both the rotated values of queries and keys, for each token.</h2><a id="user-content-at-this-stage-now-have-both-the-rotated-values-of-queries-and-keys-for-each-token" aria-label="Permalink: at this stage now have both the rotated values of queries and keys, for each token." href="#at-this-stage-now-have-both-the-rotated-values-of-queries-and-keys-for-each-token"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/keys0.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/keys0.png" width="600px"></a>
</p>
each of the queries and keys are now of shape [17x128]. 
<p dir="auto"><h2 tabindex="-1" dir="auto">in the next step we will multiply the queries and key matrices</h2><a id="user-content-in-the-next-step-we-will-multiply-the-queries-and-key-matrices" aria-label="Permalink: in the next step we will multiply the queries and key matrices" href="#in-the-next-step-we-will-multiply-the-queries-and-key-matrices"></a></p>
<p dir="auto">doing this will give us a score mapping each token with one another
<br>
this score describes how well each token's query relates to the each tokens's key.
THIS IS SELF ATTENTION :)
<br>
the shape of the attention score matrix (qk_per_token) is [17x17] where 17 is the number of tokens in the prompt</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/qkmatmul.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/qkmatmul.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5
qk_per_token.shape"><pre><span>qk_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>q_per_token_rotated</span>, <span>k_per_token_rotated</span>.<span>T</span>)<span>/</span>(<span>head_dim</span>)<span>**</span><span>0.5</span>
<span>qk_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">we now have to mask query key scores</h2><a id="user-content-we-now-have-to-mask-query-key-scores" aria-label="Permalink: we now have to mask query key scores" href="#we-now-have-to-mask-query-key-scores"></a></p>
<p dir="auto">during the training process of llama3, the future token qk scores are masked.
<br>
why? because during training we only learn to predict tokens using past tokens.
<br>
as a result, during inference we set the future tokens to zero.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/mask.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/mask.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="def display_qk_heatmap(qk_per_token):
    _, ax = plt.subplots()
    im = ax.imshow(qk_per_token.to(float).detach(), cmap='viridis')
    ax.set_xticks(range(len(prompt_split_as_tokens)))
    ax.set_yticks(range(len(prompt_split_as_tokens)))
    ax.set_xticklabels(prompt_split_as_tokens)
    ax.set_yticklabels(prompt_split_as_tokens)
    ax.figure.colorbar(im, ax=ax)
    
display_qk_heatmap(qk_per_token)"><pre><span>def</span> <span>display_qk_heatmap</span>(<span>qk_per_token</span>):
    <span>_</span>, <span>ax</span> <span>=</span> <span>plt</span>.<span>subplots</span>()
    <span>im</span> <span>=</span> <span>ax</span>.<span>imshow</span>(<span>qk_per_token</span>.<span>to</span>(<span>float</span>).<span>detach</span>(), <span>cmap</span><span>=</span><span>'viridis'</span>)
    <span>ax</span>.<span>set_xticks</span>(<span>range</span>(<span>len</span>(<span>prompt_split_as_tokens</span>)))
    <span>ax</span>.<span>set_yticks</span>(<span>range</span>(<span>len</span>(<span>prompt_split_as_tokens</span>)))
    <span>ax</span>.<span>set_xticklabels</span>(<span>prompt_split_as_tokens</span>)
    <span>ax</span>.<span>set_yticklabels</span>(<span>prompt_split_as_tokens</span>)
    <span>ax</span>.<span>figure</span>.<span>colorbar</span>(<span>im</span>, <span>ax</span><span>=</span><span>ax</span>)
    
<span>display_qk_heatmap</span>(<span>qk_per_token</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/implllama3_50_0.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/implllama3_50_0.png" alt="png"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="mask = torch.full((len(tokens), len(tokens)), float(&quot;-inf&quot;), device=tokens.device)
mask = torch.triu(mask, diagonal=1)
mask"><pre><span>mask</span> <span>=</span> <span>torch</span>.<span>full</span>((<span>len</span>(<span>tokens</span>), <span>len</span>(<span>tokens</span>)), <span>float</span>(<span>"-inf"</span>), <span>device</span><span>=</span><span>tokens</span>.<span>device</span>)
<span>mask</span> <span>=</span> <span>torch</span>.<span>triu</span>(<span>mask</span>, <span>diagonal</span><span>=</span><span>1</span>)
<span>mask</span></pre></div>
<div data-snippet-clipboard-copy-content="tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"><pre><code>tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="qk_per_token_after_masking = qk_per_token + mask
display_qk_heatmap(qk_per_token_after_masking)"><pre><span>qk_per_token_after_masking</span> <span>=</span> <span>qk_per_token</span> <span>+</span> <span>mask</span>
<span>display_qk_heatmap</span>(<span>qk_per_token_after_masking</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/implllama3_52_0.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/implllama3_52_0.png" alt="png"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/softmax.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/softmax.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)
display_qk_heatmap(qk_per_token_after_masking_after_softmax)"><pre><span>qk_per_token_after_masking_after_softmax</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>functional</span>.<span>softmax</span>(<span>qk_per_token_after_masking</span>, <span>dim</span><span>=</span><span>1</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)
<span>display_qk_heatmap</span>(<span>qk_per_token_after_masking_after_softmax</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/implllama3_54_0.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/implllama3_54_0.png" alt="png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">values (almost the end of attention)</h2><a id="user-content-values-almost-the-end-of-attention" aria-label="Permalink: values (almost the end of attention)" href="#values-almost-the-end-of-attention"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/value.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/value.png" width="600px"></a>
</p>
these scores (0-1) are used to determine how much of value matrix is used per token
<br>
&gt; just like keys, value weights are also shared acorss every 4 attention heads (to save computation)
<br>
&gt; as a result, the shape of the value weight matrix below is [8x128x4096]
<div dir="auto" data-snippet-clipboard-copy-content="v_layer0 = model[&quot;layers.0.attention.wv.weight&quot;]
v_layer0 = v_layer0.view(n_kv_heads, v_layer0.shape[0] // n_kv_heads, dim)
v_layer0.shape"><pre><span>v_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wv.weight"</span>]
<span>v_layer0</span> <span>=</span> <span>v_layer0</span>.<span>view</span>(<span>n_kv_heads</span>, <span>v_layer0</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)
<span>v_layer0</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([8, 128, 4096])"><pre><code>torch.Size([8, 128, 4096])
</code></pre></div>
<p dir="auto">the first layer, first head value weight matrix is given below</p>
<div dir="auto" data-snippet-clipboard-copy-content="v_layer0_head0 = v_layer0[0]
v_layer0_head0.shape"><pre><span>v_layer0_head0</span> <span>=</span> <span>v_layer0</span>[<span>0</span>]
<span>v_layer0_head0</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">value vectors</h2><a id="user-content-value-vectors" aria-label="Permalink: value vectors" href="#value-vectors"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/v0.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/v0.png" width="600px"></a>
</p>
we now use the value weghts to get the attention values per token, this is of size [17x128] where 17 is the number of tokens in the prompt and 128 is the dim of the value vector per token
<div dir="auto" data-snippet-clipboard-copy-content="v_per_token = torch.matmul(token_embeddings, v_layer0_head0.T)
v_per_token.shape"><pre><span>v_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>v_layer0_head0</span>.<span>T</span>)
<span>v_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">attention</h2><a id="user-content-attention" aria-label="Permalink: attention" href="#attention"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/attention.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/attention.png" width="600px"></a>
</p>
the resultant attention vector after multipying with the values per token is of shape [17*128]
<div dir="auto" data-snippet-clipboard-copy-content="qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)
qkv_attention.shape"><pre><span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)
<span>qkv_attention</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">multi head attention</h2><a id="user-content-multi-head-attention" aria-label="Permalink: multi head attention" href="#multi-head-attention"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/heads.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/heads.png" width="600px"></a>
</p>
WE NOW HAVE THE ATTENTION VALUE OF THE FIRST LAYER AND FIRST HEAD
<br>
now im going to run a loop and perform the exact same math as the cells above but for every head in the first layer
<div dir="auto" data-snippet-clipboard-copy-content="qkv_attention_store = []

for head in range(n_heads):
    q_layer0_head = q_layer0[head]
    k_layer0_head = k_layer0[head//4] # key weights are shared across 4 heads
    v_layer0_head = v_layer0[head//4] # value weights are shared across 4 heads
    q_per_token = torch.matmul(token_embeddings, q_layer0_head.T)
    k_per_token = torch.matmul(token_embeddings, k_layer0_head.T)
    v_per_token = torch.matmul(token_embeddings, v_layer0_head.T)

    q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)
    q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)
    q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis[:len(tokens)])
    q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)

    k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)
    k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)
    k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis[:len(tokens)])
    k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)

    qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5
    mask = torch.full((len(tokens), len(tokens)), float(&quot;-inf&quot;), device=tokens.device)
    mask = torch.triu(mask, diagonal=1)
    qk_per_token_after_masking = qk_per_token + mask
    qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)
    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)
    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)
    qkv_attention_store.append(qkv_attention)

len(qkv_attention_store)"><pre><span>qkv_attention_store</span> <span>=</span> []

<span>for</span> <span>head</span> <span>in</span> <span>range</span>(<span>n_heads</span>):
    <span>q_layer0_head</span> <span>=</span> <span>q_layer0</span>[<span>head</span>]
    <span>k_layer0_head</span> <span>=</span> <span>k_layer0</span>[<span>head</span><span>//</span><span>4</span>] <span># key weights are shared across 4 heads</span>
    <span>v_layer0_head</span> <span>=</span> <span>v_layer0</span>[<span>head</span><span>//</span><span>4</span>] <span># value weights are shared across 4 heads</span>
    <span>q_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>q_layer0_head</span>.<span>T</span>)
    <span>k_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>k_layer0_head</span>.<span>T</span>)
    <span>v_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>v_layer0_head</span>.<span>T</span>)

    <span>q_per_token_split_into_pairs</span> <span>=</span> <span>q_per_token</span>.<span>float</span>().<span>view</span>(<span>q_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)
    <span>q_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>q_per_token_split_into_pairs</span>)
    <span>q_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>q_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>[:<span>len</span>(<span>tokens</span>)])
    <span>q_per_token_rotated</span> <span>=</span> <span>q_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>q_per_token</span>.<span>shape</span>)

    <span>k_per_token_split_into_pairs</span> <span>=</span> <span>k_per_token</span>.<span>float</span>().<span>view</span>(<span>k_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)
    <span>k_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>k_per_token_split_into_pairs</span>)
    <span>k_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>k_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>[:<span>len</span>(<span>tokens</span>)])
    <span>k_per_token_rotated</span> <span>=</span> <span>k_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>k_per_token</span>.<span>shape</span>)

    <span>qk_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>q_per_token_rotated</span>, <span>k_per_token_rotated</span>.<span>T</span>)<span>/</span>(<span>128</span>)<span>**</span><span>0.5</span>
    <span>mask</span> <span>=</span> <span>torch</span>.<span>full</span>((<span>len</span>(<span>tokens</span>), <span>len</span>(<span>tokens</span>)), <span>float</span>(<span>"-inf"</span>), <span>device</span><span>=</span><span>tokens</span>.<span>device</span>)
    <span>mask</span> <span>=</span> <span>torch</span>.<span>triu</span>(<span>mask</span>, <span>diagonal</span><span>=</span><span>1</span>)
    <span>qk_per_token_after_masking</span> <span>=</span> <span>qk_per_token</span> <span>+</span> <span>mask</span>
    <span>qk_per_token_after_masking_after_softmax</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>functional</span>.<span>softmax</span>(<span>qk_per_token_after_masking</span>, <span>dim</span><span>=</span><span>1</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)
    <span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)
    <span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)
    <span>qkv_attention_store</span>.<span>append</span>(<span>qkv_attention</span>)

<span>len</span>(<span>qkv_attention_store</span>)</pre></div>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/stacked.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/stacked.png" width="600px"></a>
</p>
we now have a the qkv_attention matrix for all 32 heads on the first layer, next im going to merge all attention scores into one large matrix of size [17x4096]
<br>
we are almost at the end :)
<div dir="auto" data-snippet-clipboard-copy-content="stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)
stacked_qkv_attention.shape"><pre><span>stacked_qkv_attention</span> <span>=</span> <span>torch</span>.<span>cat</span>(<span>qkv_attention_store</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)
<span>stacked_qkv_attention</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">weight matrix, one of the final steps</h2><a id="user-content-weight-matrix-one-of-the-final-steps" aria-label="Permalink: weight matrix, one of the final steps" href="#weight-matrix-one-of-the-final-steps"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/weightmatrix.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/weightmatrix.png" width="600px"></a>
</p>
one of the last things to do for a layer 0 attention is, is to multiply the weight matrix of the 
<div dir="auto" data-snippet-clipboard-copy-content="w_layer0 = model[&quot;layers.0.attention.wo.weight&quot;]
w_layer0.shape"><pre><span>w_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wo.weight"</span>]
<span>w_layer0</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">this is a simple linear layer, so we just matmul</h3><a id="user-content-this-is-a-simple-linear-layer-so-we-just-matmul" aria-label="Permalink: this is a simple linear layer, so we just matmul" href="#this-is-a-simple-linear-layer-so-we-just-matmul"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="embedding_delta = torch.matmul(stacked_qkv_attention, w_layer0.T)
embedding_delta.shape"><pre><span>embedding_delta</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>stacked_qkv_attention</span>, <span>w_layer0</span>.<span>T</span>)
<span>embedding_delta</span>.<span>shape</span></pre></div>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/afterattention.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/afterattention.png" width="600px"></a>
</p>
we now have the change in the embedding value after attention, that should be adding to the original token embeddings
<div dir="auto" data-snippet-clipboard-copy-content="embedding_after_edit = token_embeddings_unnormalized + embedding_delta
embedding_after_edit.shape"><pre><span>embedding_after_edit</span> <span>=</span> <span>token_embeddings_unnormalized</span> <span>+</span> <span>embedding_delta</span>
<span>embedding_after_edit</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">we normalize and then run a feed forward neural network through the embedding delta</h2><a id="user-content-we-normalize-and-then-run-a-feed-forward-neural-network-through-the-embedding-delta" aria-label="Permalink: we normalize and then run a feed forward neural network through the embedding delta" href="#we-normalize-and-then-run-a-feed-forward-neural-network-through-the-embedding-delta"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/norm_after.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/norm_after.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[&quot;layers.0.ffn_norm.weight&quot;])
embedding_after_edit_normalized.shape"><pre><span>embedding_after_edit_normalized</span> <span>=</span> <span>rms_norm</span>(<span>embedding_after_edit</span>, <span>model</span>[<span>"layers.0.ffn_norm.weight"</span>])
<span>embedding_after_edit_normalized</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">loading the ff weights and implementing the feed forward network</h2><a id="user-content-loading-the-ff-weights-and-implementing-the-feed-forward-network" aria-label="Permalink: loading the ff weights and implementing the feed forward network" href="#loading-the-ff-weights-and-implementing-the-feed-forward-network"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/swiglu.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/swiglu.png" width="600px"></a>
</p>
in llama3, they used a SwiGLU feedforward network, this network architecture is really good at adding non linearity when needed by the model.
<br>
its pretty standard to use this feed forward network architecture in llms these days
<div dir="auto" data-snippet-clipboard-copy-content="w1 = model[&quot;layers.0.feed_forward.w1.weight&quot;]
w2 = model[&quot;layers.0.feed_forward.w2.weight&quot;]
w3 = model[&quot;layers.0.feed_forward.w3.weight&quot;]
output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)
output_after_feedforward.shape"><pre><span>w1</span> <span>=</span> <span>model</span>[<span>"layers.0.feed_forward.w1.weight"</span>]
<span>w2</span> <span>=</span> <span>model</span>[<span>"layers.0.feed_forward.w2.weight"</span>]
<span>w3</span> <span>=</span> <span>model</span>[<span>"layers.0.feed_forward.w3.weight"</span>]
<span>output_after_feedforward</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>torch</span>.<span>functional</span>.<span>F</span>.<span>silu</span>(<span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w1</span>.<span>T</span>)) <span>*</span> <span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w3</span>.<span>T</span>), <span>w2</span>.<span>T</span>)
<span>output_after_feedforward</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">WE FINALLY HAVE NEW EDITED EMBEDDINGS FOR EACH TOKEN AFTER THE FIRST LAYER</h2><a id="user-content-we-finally-have-new-edited-embeddings-for-each-token-after-the-first-layer" aria-label="Permalink: WE FINALLY HAVE NEW EDITED EMBEDDINGS FOR EACH TOKEN AFTER THE FIRST LAYER" href="#we-finally-have-new-edited-embeddings-for-each-token-after-the-first-layer"></a></p>
<p dir="auto">just 31 more layers to go before we are done (one for loop away)
<br>
you can imagine this edited embedding as having information about all queries asked on the first layer
<br>
now each layer will encode more and more complex queries on the quesions asked, until we have an embedding that knows everything about the next token that we need.</p>
<div dir="auto" data-snippet-clipboard-copy-content="layer_0_embedding = embedding_after_edit+output_after_feedforward
layer_0_embedding.shape"><pre><span>layer_0_embedding</span> <span>=</span> <span>embedding_after_edit</span><span>+</span><span>output_after_feedforward</span>
<span>layer_0_embedding</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">god, everything all at once</h2><a id="user-content-god-everything-all-at-once" aria-label="Permalink: god, everything all at once" href="#god-everything-all-at-once"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/god.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/god.png" width="600px"></a>
</p>
yep, this is it. everything we did before, all at once, for every single layer.

<p dir="auto"><h2 tabindex="-1" dir="auto">have fun reading :)</h2><a id="user-content-have-fun-reading-" aria-label="Permalink: have fun reading :)" href="#have-fun-reading-"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="final_embedding = token_embeddings_unnormalized
for layer in range(n_layers):
    qkv_attention_store = []
    layer_embedding_norm = rms_norm(final_embedding, model[f&quot;layers.{layer}.attention_norm.weight&quot;])
    q_layer = model[f&quot;layers.{layer}.attention.wq.weight&quot;]
    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)
    k_layer = model[f&quot;layers.{layer}.attention.wk.weight&quot;]
    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)
    v_layer = model[f&quot;layers.{layer}.attention.wv.weight&quot;]
    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)
    w_layer = model[f&quot;layers.{layer}.attention.wo.weight&quot;]
    for head in range(n_heads):
        q_layer_head = q_layer[head]
        k_layer_head = k_layer[head//4]
        v_layer_head = v_layer[head//4]
        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)
        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)
        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)
        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)
        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)
        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis)
        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)
        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)
        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)
        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)
        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)
        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5
        mask = torch.full((len(token_embeddings_unnormalized), len(token_embeddings_unnormalized)), float(&quot;-inf&quot;))
        mask = torch.triu(mask, diagonal=1)
        qk_per_token_after_masking = qk_per_token + mask
        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)
        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)
        qkv_attention_store.append(qkv_attention)

    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)
    w_layer = model[f&quot;layers.{layer}.attention.wo.weight&quot;]
    embedding_delta = torch.matmul(stacked_qkv_attention, w_layer.T)
    embedding_after_edit = final_embedding + embedding_delta
    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f&quot;layers.{layer}.ffn_norm.weight&quot;])
    w1 = model[f&quot;layers.{layer}.feed_forward.w1.weight&quot;]
    w2 = model[f&quot;layers.{layer}.feed_forward.w2.weight&quot;]
    w3 = model[f&quot;layers.{layer}.feed_forward.w3.weight&quot;]
    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)
    final_embedding = embedding_after_edit+output_after_feedforward"><pre><span>final_embedding</span> <span>=</span> <span>token_embeddings_unnormalized</span>
<span>for</span> <span>layer</span> <span>in</span> <span>range</span>(<span>n_layers</span>):
    <span>qkv_attention_store</span> <span>=</span> []
    <span>layer_embedding_norm</span> <span>=</span> <span>rms_norm</span>(<span>final_embedding</span>, <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention_norm.weight"</span>])
    <span>q_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wq.weight"</span>]
    <span>q_layer</span> <span>=</span> <span>q_layer</span>.<span>view</span>(<span>n_heads</span>, <span>q_layer</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_heads</span>, <span>dim</span>)
    <span>k_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wk.weight"</span>]
    <span>k_layer</span> <span>=</span> <span>k_layer</span>.<span>view</span>(<span>n_kv_heads</span>, <span>k_layer</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)
    <span>v_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wv.weight"</span>]
    <span>v_layer</span> <span>=</span> <span>v_layer</span>.<span>view</span>(<span>n_kv_heads</span>, <span>v_layer</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)
    <span>w_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wo.weight"</span>]
    <span>for</span> <span>head</span> <span>in</span> <span>range</span>(<span>n_heads</span>):
        <span>q_layer_head</span> <span>=</span> <span>q_layer</span>[<span>head</span>]
        <span>k_layer_head</span> <span>=</span> <span>k_layer</span>[<span>head</span><span>//</span><span>4</span>]
        <span>v_layer_head</span> <span>=</span> <span>v_layer</span>[<span>head</span><span>//</span><span>4</span>]
        <span>q_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>layer_embedding_norm</span>, <span>q_layer_head</span>.<span>T</span>)
        <span>k_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>layer_embedding_norm</span>, <span>k_layer_head</span>.<span>T</span>)
        <span>v_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>layer_embedding_norm</span>, <span>v_layer_head</span>.<span>T</span>)
        <span>q_per_token_split_into_pairs</span> <span>=</span> <span>q_per_token</span>.<span>float</span>().<span>view</span>(<span>q_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)
        <span>q_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>q_per_token_split_into_pairs</span>)
        <span>q_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>q_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>)
        <span>q_per_token_rotated</span> <span>=</span> <span>q_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>q_per_token</span>.<span>shape</span>)
        <span>k_per_token_split_into_pairs</span> <span>=</span> <span>k_per_token</span>.<span>float</span>().<span>view</span>(<span>k_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)
        <span>k_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>k_per_token_split_into_pairs</span>)
        <span>k_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>k_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>)
        <span>k_per_token_rotated</span> <span>=</span> <span>k_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>k_per_token</span>.<span>shape</span>)
        <span>qk_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>q_per_token_rotated</span>, <span>k_per_token_rotated</span>.<span>T</span>)<span>/</span>(<span>128</span>)<span>**</span><span>0.5</span>
        <span>mask</span> <span>=</span> <span>torch</span>.<span>full</span>((<span>len</span>(<span>token_embeddings_unnormalized</span>), <span>len</span>(<span>token_embeddings_unnormalized</span>)), <span>float</span>(<span>"-inf"</span>))
        <span>mask</span> <span>=</span> <span>torch</span>.<span>triu</span>(<span>mask</span>, <span>diagonal</span><span>=</span><span>1</span>)
        <span>qk_per_token_after_masking</span> <span>=</span> <span>qk_per_token</span> <span>+</span> <span>mask</span>
        <span>qk_per_token_after_masking_after_softmax</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>functional</span>.<span>softmax</span>(<span>qk_per_token_after_masking</span>, <span>dim</span><span>=</span><span>1</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)
        <span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)
        <span>qkv_attention_store</span>.<span>append</span>(<span>qkv_attention</span>)

    <span>stacked_qkv_attention</span> <span>=</span> <span>torch</span>.<span>cat</span>(<span>qkv_attention_store</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)
    <span>w_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wo.weight"</span>]
    <span>embedding_delta</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>stacked_qkv_attention</span>, <span>w_layer</span>.<span>T</span>)
    <span>embedding_after_edit</span> <span>=</span> <span>final_embedding</span> <span>+</span> <span>embedding_delta</span>
    <span>embedding_after_edit_normalized</span> <span>=</span> <span>rms_norm</span>(<span>embedding_after_edit</span>, <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.ffn_norm.weight"</span>])
    <span>w1</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.feed_forward.w1.weight"</span>]
    <span>w2</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.feed_forward.w2.weight"</span>]
    <span>w3</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.feed_forward.w3.weight"</span>]
    <span>output_after_feedforward</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>torch</span>.<span>functional</span>.<span>F</span>.<span>silu</span>(<span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w1</span>.<span>T</span>)) <span>*</span> <span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w3</span>.<span>T</span>), <span>w2</span>.<span>T</span>)
    <span>final_embedding</span> <span>=</span> <span>embedding_after_edit</span><span>+</span><span>output_after_feedforward</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">we now have the final embedding, the best guess the model could make about the next token</h2><a id="user-content-we-now-have-the-final-embedding-the-best-guess-the-model-could-make-about-the-next-token" aria-label="Permalink: we now have the final embedding, the best guess the model could make about the next token" href="#we-now-have-the-final-embedding-the-best-guess-the-model-could-make-about-the-next-token"></a></p>
<p dir="auto">the shape of the embedding is the same as regular token embeddings [17x4096] where 17 is the number of tokens and 4096 is the embedding dim</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/last_norm.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/last_norm.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="final_embedding = rms_norm(final_embedding, model[&quot;norm.weight&quot;])
final_embedding.shape"><pre><span>final_embedding</span> <span>=</span> <span>rms_norm</span>(<span>final_embedding</span>, <span>model</span>[<span>"norm.weight"</span>])
<span>final_embedding</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">finally, lets decode the embedding into the token value</h2><a id="user-content-finally-lets-decode-the-embedding-into-the-token-value" aria-label="Permalink: finally, lets decode the embedding into the token value" href="#finally-lets-decode-the-embedding-into-the-token-value"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/finallayer.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/finallayer.png" width="600px"></a>
</p>
we will use the output decoder to convert the final embedding into a token
<div dir="auto" data-snippet-clipboard-copy-content="model[&quot;output.weight&quot;].shape"><pre><span>model</span>[<span>"output.weight"</span>].<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([128256, 4096])"><pre><code>torch.Size([128256, 4096])
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">we use the embedding of the last token to predict the next value</h2><a id="user-content-we-use-the-embedding-of-the-last-token-to-predict-the-next-value" aria-label="Permalink: we use the embedding of the last token to predict the next value" href="#we-use-the-embedding-of-the-last-token-to-predict-the-next-value"></a></p>
<p dir="auto">hopefully in our case, 42 :)
note: 42 is the answer to "the answer to the ultimate question of life, the universe, and everything is ", according to the book "hitchhiker's guide to the galaxy", most mordern llms would answer with 42 here, which should validate our entire code! wish me luck :)</p>
<div dir="auto" data-snippet-clipboard-copy-content="logits = torch.matmul(final_embedding[-1], model[&quot;output.weight&quot;].T)
logits.shape"><pre><span>logits</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>final_embedding</span>[<span>-</span><span>1</span>], <span>model</span>[<span>"output.weight"</span>].<span>T</span>)
<span>logits</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">the model predicted token number 2983 as the next token, is this the token number for 42?</h3><a id="user-content-the-model-predicted-token-number-2983-as-the-next-token-is-this-the-token-number-for-42" aria-label="Permalink: the model predicted token number 2983 as the next token, is this the token number for 42?" href="#the-model-predicted-token-number-2983-as-the-next-token-is-this-the-token-number-for-42"></a></p>
<p dir="auto">IM HYPING YOU UP, this is the last cell of code, hopefully you had fun :)</p>
<div dir="auto" data-snippet-clipboard-copy-content="next_token = torch.argmax(logits, dim=-1)
next_token"><pre><span>next_token</span> <span>=</span> <span>torch</span>.<span>argmax</span>(<span>logits</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)
<span>next_token</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">lets fucking go</h2><a id="user-content-lets-fucking-go" aria-label="Permalink: lets fucking go" href="#lets-fucking-go"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/42.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/42.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="tokenizer.decode([next_token.item()])"><pre><span>tokenizer</span>.<span>decode</span>([<span>next_token</span>.<span>item</span>()])</pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">thank you, i love you :)</h2><a id="user-content-thank-you-i-love-you-" aria-label="Permalink: thank you, i love you :)" href="#thank-you-i-love-you-"></a></p>
<p dir="auto">This is the end. Hopefully you enjoyed reading it!</p>
<p dir="auto">If you want to support my work</p>
<ol dir="auto">
<li>follow me on twitter <a href="https://twitter.com/naklecha" rel="nofollow">https://twitter.com/naklecha</a></li>
<li>or, buy me a coffee <a href="https://www.buymeacoffee.com/naklecha" rel="nofollow">https://www.buymeacoffee.com/naklecha</a></li>
</ol>
<p dir="auto">Honestly, if you made it this far you already made my day :)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">what motivates me?</h2><a id="user-content-what-motivates-me" aria-label="Permalink: what motivates me?" href="#what-motivates-me"></a></p>
<p dir="auto">My friends and I are on a mission - to make research more accessible!
We created a research lab called A10 - <a href="http://aaaaaaaaaa.org/" rel="nofollow">AAAAAAAAAA.org</a></p>
<p dir="auto">A10 twitter - <a href="https://twitter.com/aaaaaaaaaaorg" rel="nofollow">https://twitter.com/aaaaaaaaaaorg</a></p>
<p dir="auto">our thesis:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/a10.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/a10.png" width="600px"></a>
</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Video streaming is expensive yet YouTube "seems" to do it for free. How? (255 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40408515</link>
            <guid>40408515</guid>
            <pubDate>Sun, 19 May 2024 17:51:41 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40408515">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="40411040"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40411040" href="https://news.ycombinator.com/vote?id=40411040&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>The economics fundamentally changed a couple of years ago when cloudflare released R2. I don't know if anyone has built a streaming client for it yet, but R2 takes the largest expense (outgoing bandwidth) and zeroes it out.<p>Yor business would be wholly dependent on cloudflare. But if you don't have to pay for bandwidth, the economics aren't that bad.</p><p>(I ran a large porn site a lifetime ago, long before cdns were ubiquitous. If I was in the business today, I would absolutely put everything on R2 and make it work no matter how much client development it took)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40409320"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409320" href="https://news.ycombinator.com/vote?id=40409320&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Disclaimer: I used to work at a live video streaming company as a financial analyst so quite familiar with this<p>The biggest cost is as you imagine the streaming - getting the video to the viewer.  It was a large part of our variable cost and we had a (literal) mad genius dev ops person holed up in his own office cave that managed the whole operation.</p><p>Ive long forgotten the special optimizations he did but he would keep finding ways to improve margin / efficiency.</p><p>Encoding is a cost but I don’t recall it being significant</p><p>Storage isnt generally expensive. Think about how cheap you as a consumer can go get 2 TB of storage, and extrapolate.</p><p>The other big expense - people!  All those engineers to build back and front end systems. That’s what ruined us - too many people were needed and not enough money coming in so we were burning cash.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40409451"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40409451" href="https://news.ycombinator.com/vote?id=40409451&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>I'm guessing live video looks a lot different from a more static video site. I think encoding and storage are both quite expensive. You want to encode videos that are likely to be watched in the most efficient ways possible to reduce network bandwidth usage, and every video needs at least some encoding.<p>Based on some power laws etc., I would guess most videos have only a handful of views, so storing them forever and the cost to encode them initially is probably significant.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410469"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40410469" href="https://news.ycombinator.com/vote?id=40410469&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Encoding and storage aren't significant, relative to the bandwidth costs. Bandwidth is the high order bit.<p>The primary difference between live and static video is the bursts -- get to a certain scale as a static video provider, and you can roughly estimate your bandwidth 95th percentiles. But one big live event can blow you out of the water, and push you over into very expensive tiers that will kill your economics.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410696"><td></td></tr>
                <tr id="40410737"><td></td></tr>
            <tr id="40410960"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40410960" href="https://news.ycombinator.com/vote?id=40410960&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>AFAICT, the answer to "why does Google do X" is basically always "because someone needed a launch to point at when they're up for promotion".</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40410739"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40410739" href="https://news.ycombinator.com/vote?id=40410739&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Because significance varies, as does optimisation. At YouTube scale it might matter more, or the benefits might be bigger, even if just to save some energy or carbon footprint (and even that might be just for a compliance or marketing line).</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40410735"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40410735" href="https://news.ycombinator.com/vote?id=40410735&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Because when you are Youtube, even relatively marginal cost improvements can be huge in absolute. There is also the UX of having to wait X minutes for an uploaded video to be ready that is improved by this.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="40410638"><td></td></tr>
            <tr id="40409586"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40409586" href="https://news.ycombinator.com/vote?id=40409586&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Interesting background. I worked twice in digital video, once ~2000-2001 (ancient history - early IP, ISDN, the dead-end of H.323, bonded GSM channels, etc.) and once ~2009-2010. The second episode was fascinating, we specialised in mobile video at a time when it was just appearing on the consumer market. <i>Most</i> of the global mobile device manufacturers were clients. It got to the point where they would build the hardware and we would get airdropped in to their R&amp;D to make it work - they had no idea how performant the architecture was going to be, because they'd never tried it. We also built the server side, the billing architecture with revenue share, carrier billing support (only possible with device preloaded apps due to <i>Google Play</i> (then "Google Apps"?) store restrictions on third party payment mechanisms), etc.<p>Encoding, scaling and transcoding are relatively cheap for stored content, and relatively expensive if you want real or near-real time.</p><p>If you want DRM (digital rights management = ~ineffective copy protection) then you need to add a bit more overhead for that, both in terms of processing and latency. If you need multi-DRM (different DRM systems for different devices the consumer owns) and a good cross-device experience (like pause and resume across devices), it gets real hard real fast.</p><p>It helps to be targeting a standard platform, for example a modern widescreen TV with H.265 support and solid 4K decoding. Otherwise you need a different version for every resolution, a different version for every CODEC, a different version for every bitrate, etc. We had great experience adjusting bitrates and encoding parameters for different device categories, for example if you had a certain phone and you ran it at max spec it might look great but if you were looking to preserve battery and were running on battery save mode the decode would fail and you'd get choppy performance and stuttering audio. This sort of thing was rife then.</p><p>As a series of specialist video providers emerged, ~all the cloud providers went and added these services, basically 95% of which are frontends to ffmpeg and some internal cloud storage scheme or similar.</p><p>Finally, billing is hard. Users have an expectation of free content now.</p><p>No experience with real time stream economics, but saw the inside of LA's stadium video control center one day. Didn't look inexpensive, I'll tell you that much. Probably for events with multiple cameras you're mostly paying site fees, ie. reliable bandwidth, humans, mixing desk if required. For studio broadcast these costs will be reduced. Both will have a slight real time encoding tax vs. stored content. If you want to figure out how to do it cheaply, look at the porn industry.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410306"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40410306" href="https://news.ycombinator.com/vote?id=40410306&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>&gt; basically 95% of which are frontends to ffmpeg<p>I wonder what the approximate net global economic benefit of ffmpeg is to this point?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410496"><td></td></tr>
                        <tr id="40409336"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40409336" href="https://news.ycombinator.com/vote?id=40409336&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Curious: was your distribution client-server or peer-to-peer?<p>Or both, similar to Skype's supernode model?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40409612"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40409612" href="https://news.ycombinator.com/vote?id=40409612&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>The overwhelming majority of "legitimate" video streaming sites operate on a client-server model, which allows videos to be watched in web browsers, and on mobile devices (which don't generally do well in P2P as they find uploading difficult).<p>And generally torrent-based streamers don't hire financial analysts :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="40409790"><td></td></tr>
            <tr id="40410183"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40410183" href="https://news.ycombinator.com/vote?id=40410183&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>It’s not that expensive at YouTube scale. We are talking fractions of a penny per GiB transferred.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40410264"><td></td></tr>
                <tr id="40410289"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40410289" href="https://news.ycombinator.com/vote?id=40410289&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Yup! This is the reason why its so cheap for them. Other companies in similar positions have cache nodes in the ISPs and this dramatically lowers the cost</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40410442"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40410442" href="https://news.ycombinator.com/vote?id=40410442&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Which would help for the crazy popular meme videos, but I bet the long tail on YouTube is insanely big, even if you did have the “watch next” engine getting in on the game steering you toward content already present in your nearby caches.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40410330"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40410330" href="https://news.ycombinator.com/vote?id=40410330&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>"not that expensive" is relative; it's still a lot of money.  Sure, it's not trillions of dollars, but it's still billions of dollars.  YouTube has historically not returned a net profit (and I haven't heard of that situation changing).</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40410433"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40410433" href="https://news.ycombinator.com/vote?id=40410433&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Do you have a public source for that? From what I’ve heard YouTube has been profitable year years at this point.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40410530"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40410530" href="https://news.ycombinator.com/vote?id=40410530&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>&gt; Do you have a public source for that?<p>YT financials and P&amp;L were not broken out in audited financial statements back in the day.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                    <tr id="40409260"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409260" href="https://news.ycombinator.com/vote?id=40409260&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>For the streaming factor specifically, I can at least offer something resembling an answer: Google. In the early 2000s they bought up a bunch of dark fiber and peered with all the major US ISPs, and they were able to do this because no ISP wants to be the one that blocks or degrades Google. As a result they were able to host video streaming on their network without immediately being shut down by Comcast and co. Instead they had to go after Netflix.<p>Google has a lot of custom encoding silicon, too, AFAIK.</p><p>Storage is the biggest question of the three. Linus Sebastian specifically called this out when YouTube started really pushing to make the non-Premium experience <i>dreadful</i>. There isn't really some secret special sauce you can buy or make for storage. Literally everything is being stored with the same hard drives, SSDs, discs, or tapes you can just go out and buy. The only specialization you can do is build or buy equipment to handle extreme numbers of them. Google <i>does</i> buy these in bulk, so they probably get a discount on storage, but it's not something that would make storage costs just go away.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40409418"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40409418" href="https://news.ycombinator.com/vote?id=40409418&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>The bandwidth costs are the key. Good luck getting rates anywhere near what Google’s effectively are. Spoiler: you can’t. You probably can’t realistically get to 5x their costs, byte-for-byte.<p>Which makes competing with them effectively impossible except for a very-few other megacorps.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40409516"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40409516" href="https://news.ycombinator.com/vote?id=40409516&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Bandwidth costs are actually free, so this isn't exactly accurate.<p>Most bandwidth is via settlement-free peering with thousands of ISPs around the world. At least that's how we did it at Twitch, and how we did it when I worked at a large CDN before that. There are still costs for backhaul, interconnect, colocation space, dark fiber, network hardware, and transit to fill the gaps. But this talk about how "Google can magically do it 5x cheaper" is nonsense.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410663"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40410663" href="https://news.ycombinator.com/vote?id=40410663&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>&gt; There are still costs for backhaul, interconnect, colocation space, dark fiber, network hardware, and transit to fill the gaps.<p>Genuine question - aren't those gaps essentially what make a video streaming service operate at scale though? It'd be like saying "ya this bus can get everyone from NYC to Philly at $10 but doesn't stop anywhere in between", or am I missing something about all of those gap filling components?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40409640"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40409640" href="https://news.ycombinator.com/vote?id=40409640&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Don't Comcast and friends throttle any peering points you use, until you hand over $x per subscriber per month for them to stop doing so?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40410835"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40410835" href="https://news.ycombinator.com/vote?id=40410835&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>That isn't free. For every terabit of bandwidth, you have to physically build out a terabit of network. Not even remotely free. Having already built the network, and being already paying to run it, you can then use it for free, yes.<p>This is the model for all networks, indeed, most businesses - they pay a big upfront and moderate recurring cost to make a fast network (or restaurant or widget factory) and then sell it in slices with a large freedom to choose a pricing model. Pay per terabyte is a pretty reasonable way to pass on the network's fixed cost to consumers, just like part of the cost of the restaurant meal covers the interior decorations, even though the decorations don't actually cost more the more people eat, until the restaurant gets so busy it needs to expand.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="40409525"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40409525" href="https://news.ycombinator.com/vote?id=40409525&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Bandwidth costs are really not that bad.<p>95th billing, adaptive, progressive playing and just cap buffer to the minimum to keep playing. Equals ~$1M/month for +10 tbit/s egress.</p><p>Source: Worked at one of the largest bandwidth consumers in the world.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410080"><td></td></tr>
                <tr id="40411015"><td></td></tr>
            <tr id="40411014"><td></td></tr>
                  <tr id="40409610"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40409610" href="https://news.ycombinator.com/vote?id=40409610&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>My home connection uploads at 681mbit/s (just did the test over wifi) for 40 euros/months.
At that price, I'd get 13tbit/s for 800k euros.<p>It's a bit surprising that you were not getting significantly better prices than individuals.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410056"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40410056" href="https://news.ycombinator.com/vote?id=40410056&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Price business pipes where they won’t cut you off for saturating that 24/7.<p>[edit] and that have good deliverability worldwide, no weird paths to other consumer IPs that intermittently fail to route or inexplicably have dial-up transfer speeds. And have anything like a real SLA.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="40409568"><td></td></tr>
                <tr id="40410065"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40410065" href="https://news.ycombinator.com/vote?id=40410065&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>I am so confused here. Either I’ve been doing high-bandwidth bit slinging extremely wrong for quite a while or a lot of HN has never done it at all and is opining on it anyway. It’s real money, IME.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40409600"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40409600" href="https://news.ycombinator.com/vote?id=40409600&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Anyone who says "bandwidth costs are not really that bad" should spend 2 minutes playing with the AWS cost calculator.<p>You would think the VMs are the expensive part, but no, egress is easily multiple times the cost of the compute.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410286"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40410286" href="https://news.ycombinator.com/vote?id=40410286&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Cloud bandwidth pricing has nothing with do with costs and everything to do with lock in .<p>You can get 100x cheaper and unmetered at a low cost provider like OVH or hetzner or similar bare metal data centers .</p><p>It doesn’t even need significant monthly commits to get that pricing if you are running video streaming at scale you are not running on AWS or even tier 2 like OVH for sure
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410865"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_40410865" href="https://news.ycombinator.com/vote?id=40410865&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>But anyone doing things at great scale, isn’t going with OVH. You could use it as an origin I goes but you’d still need CDN for decent content delivery.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40410717"><td></td></tr>
                <tr id="40410894"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_40410894" href="https://news.ycombinator.com/vote?id=40410894&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>It's unmetered*<p>* if our upstream is saturated we're going to look at our biggest users and if the number is really big we'll send them a polite email to please reduce it or pay more.</p><p>There are reports of people getting emails from Hetzner after sending  multiple Gbps continuously for several months. That's the level you have to reach before the * kicks in. Only 1Gbps servers are unmetered, so you'd have to have several.</p><p>If you want to know a better approximation of their true cost just look at their non-unlimited plans: 20TB/month included for free; 1€/TB (excl VAT) after that.</p><p>I have one more interesting data point to add: I was quoted 950€/month for a dedicated 10Gbps between Berlin and Amsterdam (about 600km) plus peering at AMS-IX, or 300€ for 1Gbps. (They're not secretive and you can just ask for a quote using their sales contact form). Extrapolating, it seems that 1€ is worth about 2.5 petabyte-kilometers, at least within the dense interconnections of continental Europe. About twice the price of shipping a petabyte of hard drives the same distance.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="40409778"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40409778" href="https://news.ycombinator.com/vote?id=40409778&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>To be fair, that is the penalty of being in a shared cloud. They are incetivized to keep their customers from using everything, everywhere, all at once.<p>Jump into a 'bare metal' datacenter and things can get much different.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40410049"><td></td></tr>
                  <tr id="40409634"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40409634" href="https://news.ycombinator.com/vote?id=40409634&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Assuming 10 Mbit/s per stream that would serve 1 million concurrent streams 24/7. If we assume people watch 2.4 hours per day on average, it could support 10 million active users. Or a cost to serve for bandwidth alone of 1 USD/user per year.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40409310"><td></td></tr>
                <tr id="40409338"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40409338" href="https://news.ycombinator.com/vote?id=40409338&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>So long as storage costs decrease exponentially I think they'll keep everything.<p>Especially if the amount of content uploaded keeps going up, so the relative benefit of deleting old stuff is small.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40409368"><td></td></tr>
                  <tr id="40409535"><td></td></tr>
                <tr id="40409704"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40409704" href="https://news.ycombinator.com/vote?id=40409704&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Exactly! This was my go-to approach for reducing storage costs. Customers don't get spooked when they get an extra 1s delay for something they search once in a month. However, an extra 30ms delay in "everyday content" is a sure way to loose your users.<p>However, implementing this in practice is non-trivial. Knowing what is "everyday content" versus what is "once a month content".</p><p>To add more complexity -- you have these semi-predictable hype-waves especially two peaks in case of most YT videos where a "once-a-month" content becomes an "everyday" content before again becoming a "once-a-month" content. It feels you could specifically optimise for this -- reduce storage costs without sacrificing UX.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40409583"><td></td></tr>
                <tr id="40409721"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40409721" href="https://news.ycombinator.com/vote?id=40409721&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Think about what you actually need to start a video. Maybe a dozen MB?<p>After that, you can plunge into colder storages and warm things up as you stream. Additionally, if you need longer to 'defrost' things, just cache a few more MB at the front. Cheat a bit by assuming 480p to start with if you need to; even less to store.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410249"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_40410249" href="https://news.ycombinator.com/vote?id=40410249&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>There is also location location location.<p>Maybe Google holds your content in 7 data centers round the world (~1 per continent for planned maintenance + latency + reduced oceanic fiber usage).</p><p>But with old rarely streamed content they might cut that down to just 3.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="40409648"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40409648" href="https://news.ycombinator.com/vote?id=40409648&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Speed/latency doesn't tell you much, because it's all on a hard drive <i>somewhere</i>.<p>The question is whether YT is serving up the <i>one</i> (redundantly-backed storage) copy they have of your almost-never-watched video, or whether it's serving it up from one of 1,000+ copies it's made across the globe for currently popular videos.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40409608"><td></td></tr>
                        <tr id="40409539"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40409539" href="https://news.ycombinator.com/vote?id=40409539&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Old content has value for training AI models even if there are no human viewers anymore.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40409639"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40409639" href="https://news.ycombinator.com/vote?id=40409639&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Yes, it's inevitable.<p>My guess is that the first step will be to re-encode all the non-popular videos with severe lossy compression.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410266"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40410266" href="https://news.ycombinator.com/vote?id=40410266&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>They still keep the original uploaded files.<p>You know that because when they release a new format (eg. HDR or a different resolution), they re-encode from the original.   Various people have tested that with moire patterns and various other ways to demonstrate if something was encoded more than once.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="40408644"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40408644" href="https://news.ycombinator.com/vote?id=40408644&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>YouTube generated <i>$31.5 billion</i> in advertising revenue in 2023.<p><a href="https://www.businessofapps.com/data/youtube-statistics/#:~:text=YouTube%20improved%20its%20percentage%20revenue,generated%20%2431.5%20billion%20in%202023" rel="nofollow">https://www.businessofapps.com/data/youtube-statistics/#:~:t...</a>.</p><p>That's... a lot! Plenty of historical precedent for fully advertising-supported media with high expenses, from OTA network television and radio to free weekly newspapers... or inexpensive subscriptions to daily newspapers subsidized by advertising. Advertising has been paying the bills for electronic media for a century now.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410248"><td></td></tr>
                <tr id="40410322"><td></td></tr>
            <tr id="40410297"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40410297" href="https://news.ycombinator.com/vote?id=40410297&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Netflix spends over 50% of it's revenue on content production and licensing ($17B out of $30B), and they made $6B net profit in the past year.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40410354"><td></td></tr>
                <tr id="40410503"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40410503" href="https://news.ycombinator.com/vote?id=40410503&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Netflix serves a slightly larger portion of internet traffic than YouTube on the same amount of revenue. So whatever subsidization youtube is providing for those videos, is clearly outweighed by the monetization of the remaining videos. YouTube has higher revenue per GB of bandwidth served than Netflix.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="40410241"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40410241" href="https://news.ycombinator.com/vote?id=40410241&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Yes. For comparison, Netflix has about $30B in revenue... paid up front for all of its content (something youtube doesnt) and accounts for a larger percentage of internet traffic (likely because of higher quality streams)... and they still made $6B net profit.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40410275"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40410275" href="https://news.ycombinator.com/vote?id=40410275&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Netflix content is highly coachable (tiny library Vs YouTube), which dramatically reduces the cost of serving the data to users.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40409531"><td></td></tr>
                <tr id="40409596"><td></td></tr>
                        <tr id="40410516"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40410516" href="https://news.ycombinator.com/vote?id=40410516&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>It's purely a scale problem.<p>On the "Revenue" side you will quite probably need to have enough eyeballs that advertisers come to you directly to display ads and do so in volume.</p><p>On the "Costs" side you'd want to be big enough that you can just store your content in ~3 of your own datacentres, cache the "hot content" in a site or two per country then give away caches to ISPs (who will gladly host them in their own network for free).</p><p>Biggest cost will be bandwidth/streaming servers. Encoding/storage is comparatively cheap. If you were small you would likely start to do this from a few 100Gbps dedicated servers per continent. <a href="https://www.fdcservers.net/configurator?fixedFilter=15&amp;fixedFilterType=bandwidth_option" rel="nofollow">https://www.fdcservers.net/configurator?fixedFilter=15&amp;fixed...</a> If we set an average of 3-4Mbps per stream you're looking at each server handling 20,000 videos served and the hourly cost of the server would be around say $4/hour so you're looking at around $0.20 per 1000 video hours in theory, in practice it will be higher. Worst case closer to $0.50 per 1000 video hours due to utilisation rates.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410994"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40410994" href="https://news.ycombinator.com/vote?id=40410994&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Aren't big players colocating their CDNs at internet exchanges? Bandwidth should be essentially free for content delivery.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40410587"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40410587" href="https://news.ycombinator.com/vote?id=40410587&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>It may be different at this scale, but in my experience fast-ish redundant file storage has always been extremely expensive even at lower storage sizes, while getting a 100 Gbps line is relatively cheap.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40410665"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40410665" href="https://news.ycombinator.com/vote?id=40410665&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>it's different at scale, and you mostly don't need SSDs since you have cache boxes, most your storage is videos that are very rarely used. You're looking at probably a rack of SSDs for every 5-10 racks of spinning disks X 3 datacentres. Even that would give you approx 50-90PB of usable storage (replicated so it's in all three sites) for a few tens of thousands a month.<p>Even if you just put it all on S3 infrequent, which would be one of the most expensive ways of doing it, it's still not really expensive compared to serving the content.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="40410648"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40410648" href="https://news.ycombinator.com/vote?id=40410648&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>For most players streaming are not profitable.  Youtube is likely profitable, but since Google doesn’t report separate income, we don’t know how much.<p>Netflix is.</p><p>Disney, peacock, Paramount, Max , etc are not profitable with the hope they can capture future monopoly standing.</p><p>Prime Video is likely also not profitable or break even given their studio investments (e.g. MGM, first party content).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410676"><td></td></tr>
                  <tr id="40409573"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409573" href="https://news.ycombinator.com/vote?id=40409573&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Disclaimer: I worked at Google and occasionally did things for YT, but it was 2015 and before. I did look at their P&amp;L, <i>somewhat</i>.<p>egress costs were enormous and YT was not profitable. I don't know if it is now, but I wouldn't be surprised to find it is. They sure have enough ads.</p><p>As several people say below, caching content around the world is key, so that not all requests are serviced in NoCal.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40409589"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40409589" href="https://news.ycombinator.com/vote?id=40409589&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Bingo, YT has always been a loss leader for Google dominance. Only recently have they squeezed the ads knob to maybe generate a profit but I’d bet it’s nothing like the high margin AdWords cash cow.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40410362"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40410362" href="https://news.ycombinator.com/vote?id=40410362&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>It's so wild to see people so confidently being wrong in a comment. YouTube has seen profit since 2013, with margins growing each year.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40410890"><td></td></tr>
            <tr id="40410520"><td></td></tr>
                <tr id="40410577"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40410577" href="https://news.ycombinator.com/vote?id=40410577&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>I was there as well, and Youtube was wildly profitable.<p>See how easy it is to make random statements on the internet.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410692"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_40410692" href="https://news.ycombinator.com/vote?id=40410692&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>&gt; random statements<p>Yep. I'd be happy to stand in front of an arbiter who knows the facts and let him/her judge which one of us knows what he's talking about.</p><p>or we can ask ChatGPT:  "was YouTube profitable prior to 2017"</p><p>Before 2017, YouTube's profitability status was not officially disclosed by its parent company, Google (now Alphabet Inc.). While Google acquired YouTube in 2006 for $1.65 billion, the platform's journey to profitability was complex and prolonged due to the high costs associated with its massive data storage, bandwidth, and content licensing needs. Despite its immense popularity and growing user base, these operational costs made turning a profit challenging.</p><p>Revenue Growth and Challenges:</p><p>Revenue Generation: YouTube generated significant revenue through advertising. By 2015, its ad revenue was estimated to be over $4 billion annually. Despite this, high operational costs offset these revenues.</p><p>Content and Infrastructure Costs: The expenses for server maintenance, bandwidth, and licensing for music and other content were substantial. YouTube also invested heavily in developing new features and expanding its global reach.
Monetization Strategies: YouTube introduced various monetization strategies over the years, including TrueView ads, channel memberships, Super Chat, and YouTube Premium (previously YouTube Red). These aimed to diversify revenue streams beyond traditional advertising.</p><p>Estimates and Market Analysts:</p><p>While exact profitability figures were not publicly available, market analysts and industry insiders speculated about YouTube's financial health. For instance:</p><p>In 2015, The Wall Street Journal reported that despite generating substantial revenue, YouTube barely broke even due to high costs.
Analysts suggested that YouTube's profitability improved gradually as its revenue grew and operational efficiencies increased.</p><p>Official Confirmation of Profitability:</p><p>It wasn't until 2017 that there were stronger indications of YouTube's profitability. During an earnings call in 2017, Alphabet's CFO Ruth Porat hinted at YouTube being a significant contributor to the company's revenue growth, suggesting improved profitability.</p><p>In summary, while YouTube likely faced profitability challenges for much of its early existence, it was making significant strides towards profitability by the mid-2010s, achieving a more stable financial status around 2017.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410880"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_40410880" href="https://news.ycombinator.com/vote?id=40410880&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>I don't know why you're getting downvoted.  Efficiency projects like the transcoding ASIC are a big part of pushing YouTube to profitability, as well as the alternate revenue streams and heavy increases in monetization.  Video serving is extremely expensive and difficult compared to everything else Google does.<p>Ruth Porat has been on record many times indicating that YouTube wasn't profitable in the 2010's.  I think her public statements have only indicated that YouTube was free cash flow positive as of the 2020's, but I haven't found exactly where that happened - Google has experimented with a lot of different kinds of breakdowns of its finances.  I assume that hiding the economics of YouTube is part of this (as well as protection against a zealous DOJ saying that Google's businesses are separable).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410998"><td><table>  <tbody><tr>    <td indent="7"><img src="https://news.ycombinator.com/s.gif" height="1" width="280"></td><td>
      <center><a id="up_40410998" href="https://news.ycombinator.com/vote?id=40410998&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>&gt; I don't know why you're getting downvoted.<p>Because I'm not here to read a wall of text generated by ChatGPT.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                    <tr id="40410499"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40410499" href="https://news.ycombinator.com/vote?id=40410499&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>No. 2013? See GP, see Google financial statements, etc. I do think it started turning a profit over the last 3 years, but can't confirm that.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="40409205"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409205" href="https://news.ycombinator.com/vote?id=40409205&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>For streaming, google have caches in like every ISP network. Also majority of the people watches the latest and same type of content that is mainly served from the homepage, which is easier to cache and serve.<p>If you have the ipvfoo extension, you can see it in action. (its easier to see with IPv6)</p><p><a href="https://github.com/pmarks-net/ipvfoo">https://github.com/pmarks-net/ipvfoo</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40409275"><td></td></tr>
                <tr id="40409355"><td></td></tr>
                <tr id="40409829"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40409829" href="https://news.ycombinator.com/vote?id=40409829&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Although note the map is only city addresses. For example the "London" pin is on the notional point where "London" is, ie Charing Cross. There is no giant network interchange at Charing Cross, it's just a convenient place co-ordinate for "London".</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40409253"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40409253" href="https://news.ycombinator.com/vote?id=40409253&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>I wonder if that's part of the reason for their algorithms to push the same videos to everyone, they're already cached at the edge so it costs them nothing?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40409282"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40409282" href="https://news.ycombinator.com/vote?id=40409282&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>More popular works are more likely to be enjoyable to more people. There is no really objective measure of quality for any creative work, and taste doesn't scale, so publishers bias for popularity as it's one of the few things they can understand.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40409670"><td></td></tr>
            <tr id="40410355"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40410355" href="https://news.ycombinator.com/vote?id=40410355&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>The majority will enjoy and like whatever is pushed on them. Decades of radio and TV should be enough evidence for this. Music or video is not popular because people like it, but because somebody decided to make it popular by pushing it on people.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40409402"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40409402" href="https://news.ycombinator.com/vote?id=40409402&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>I would imagine that's too insignificant to factor into that particular calculation.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40410108"><td></td></tr>
                      <tr id="40409279"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40409279" href="https://news.ycombinator.com/vote?id=40409279&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>I would imagine it's an unintended side effect of the "people recently watched this so it's relevant" part of the algorithm.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40409174"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409174" href="https://news.ycombinator.com/vote?id=40409174&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Encoding is largely super-linear for a single stream, so you just need enough cores for the intake * formats.  Streaming is mostly chunking and a smart player that loads the right chunks at the right time.  Storage is bottom dollar, use whatever the cheapest disks you've got that you can attach to fiber, then cache the hell out of everything.<p>So in short, the only "on-demand" component is encoding, and if you don't have an 'available in an instant' promise, you can do it on spot instances on the cheapest cloud you can find;  The rest is just storage and distribution - if you own a world-wide network of datacenters for your successful advertising service, that's kinda an already solved problem for you - just allocate a few racks to a new service.</p><p>I of course downplay everything and simplify massively - but at a high level, it's just a lot of ffmpeg -&gt; S3 -&gt; html5 player.  The harder problems are in the long tail - high latency, content licensing &amp; geo fencing, etc.</p><p>Source: used to SRE for a video streaming provider (not YT), also former GG
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40410531"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40410531" href="https://news.ycombinator.com/vote?id=40410531&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Video streaming is getting cheaper all the time. Bandwidth costs are dropping every year (substantially in most cases) and bitrates aren't keeping up (the few exceptions probably being AppleTV+ and BBC iPlayer which do 30-50mbit/sec 4K HDR streams).<p>You can do this for so much cheaper than AWS etc price for bandwidth. You can get 100gigE transit from he.net for list price $4500/month. Add probably the same again for colo + hardware (don't need much hardware these days to saturate 100gigE) and you can probably stream videos to 20,000 concurrent users at 5mbit/sec for ~$10k/month.</p><p>Another way would be to use someone like OVH who offers dedicated servers with 10gigE (supposedly 'guaranteed') for about $800/month each list price, without having to bother with colo and ip transit setup.</p><p>Obviously this is highly simplified as you will require encoding resource and storage, but again with someone like OVH you'd be able to spin up a lot of cheap boxes to do this. How much this will cost will depend on how many videos you get and how many views per video etc.</p><p>So IMO the actual bandwidth is a bit of a non issue. The far bigger issue is getting users to use your platform (marketing is MUCH more expensive than IP transit) and then having advertisers on your site. This is a much harder problem to solve and where the real barrier to entry is.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40410558"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40410558" href="https://news.ycombinator.com/vote?id=40410558&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>I worked on YouTube transcoding about 12 years ago. First, the scale is mostly reused - what’s doing transcoding now is doing a different compute job later. Transcoding was also done for most videos only on idle compute. Second, Google had 300k+ caches around the world, in many surprising places (buses, cruise ships) as well as many thousands of other larger but not full data center locations; get the content as close as possible to the user. (I imagine now all transcoding from the mezzanine format is done in real time on an edge GPU for all but the most popular platforms and content). Tl;dr: build out a huge amount of infrastructure to serve ads very quickly and you can piggyback video serving on that at little marginal cost.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40410982"><td></td></tr>
            <tr id="40409214"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409214" href="https://news.ycombinator.com/vote?id=40409214&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>It probably costs them a ton of money, but they probably make a ton more so that’s OK.<p>Also: bandwidth gets a lot cheaper when you own the pipes.</p><p>And one that is less obvious: despite having hundreds of millions of videos available, a large contingent of people are watching the same ultra-popular ones. There are some economies of scale to be had there.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40409334"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409334" href="https://news.ycombinator.com/vote?id=40409334&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>DIY (instead of the cloud) is the answer. If you're pushing terabytes+ from day 1 on a shoestring, you're going to want your own CDN. If you can manage a queueing system and the occasional wait, run your own (or rented physical) hardware for transcoding at as high a utilisation as you can.<p>Build vs buy pushes you to "build" early on when your margins are slim and your volume is huge.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40409603"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409603" href="https://news.ycombinator.com/vote?id=40409603&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>The economics are simple:<p>&gt; <i>I know advertisements are a thing for YT, but is it enough?</i></p><p>Yes, it is -- virtually certainly. We can assume YouTube is profitable. It's not broken out directly in quarterly reports, but it doesn't make any sense that Google would still be running it after all these years (almost 20) if it weren't.</p><p>But obviously YouTube didn't <i>start out</i> as profitable. You need scale, which provides two things:</p><p>1) Marginal storage and streaming costs go down (Google is big enough to save huge amounts of money by running its own data centers, peering agreements, caching near customers, etc.)</p><p>2) More advertisers running more ads that can be targeted to more users whose preferences you know more about</p><p>So no, <i>you</i> can't run it profitably.</p><p>This is a classic example of a business that is only profitable at scale, that needs to lose a lot of money at first as it grows until it achieves scale. And it's not just scale on the traditional tech/users side, it's scale on the advertising side as well -- advertisers aren't going to bother running ads on your platform until you have enough users for them to care.</p><p>It's also pretty strongly a "winner-takes-all" network effects situation, where video publishers want to put up their videos where the viewers are, and viewers want to visit the site where all the content is. So if you wanted to create a YT competitor, I don't know how you'd convince content creators to post their videos to your site in addition to YT, or how to convince consumers to watch said videos on your site instead of YT.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40410048"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40410048" href="https://news.ycombinator.com/vote?id=40410048&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Also scale on the supply side. It took time to get to a place where obviously the video should be on Youtube, and that's hard to replicate.<p>This applies to the content which would exist anyway, and then doubly to content created for Youtube. Grand Pooh Bear would be on Twitch anyway, but it doesn't really make sense for a Tom Scott, let alone "Corrections" which is a Youtube-only addition to Seth Myers "Late Night" show.</p><p>Likewise until it gets fairly "big" it doesn't make sense to <i>officially</i> put your music videos on Youtube. Today that's basically the main way they're getting seen.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="40409297"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409297" href="https://news.ycombinator.com/vote?id=40409297&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>The value of YouTube isn't purely monetary. Controlling YouTube is extremely advantageous for Google, even if it isn't particularly profitable.<p>Besides the general advantage of having control over such a massive platform, which definitely plays an important part in the lives of hundreds of millions of people, Google likely views YouTube as important to control.
If YouTube were a separate entity, it could e.g. freely choose their ad providers or even provides ads themselves, essentially creating competition for Google. Google also has trivial access to the data there and therefore the easiest access if they want to train AI on that data. Last but not least I think Google sees YouTube as vital for their corporate image and their social mission presented in Google Jigsaw.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40410585"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40410585" href="https://news.ycombinator.com/vote?id=40410585&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Yes it costs a lot, and YouTube also makes a lot of money via ads, subscriptions, partnerships. Whether it is ultimately profitable or not is anyone's guess, since they don't report the numbers publicly.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40409157"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409157" href="https://news.ycombinator.com/vote?id=40409157&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>&gt; especially at scale<p>The marginal costs go down a bit if your scale is truly immense. Google can afford to design/manufacture/deploy hyper-efficient custom silicon ASICs for encoding. Also because their critical mass of users provide valuable network effects, they can get away with particularly poor quality encoding (IMHO) and the vast majority of users still won't switch to other platforms with higher visual quality - but other (non-pornographic) video platforms generally don't have that luxury.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40410012"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40410012" href="https://news.ycombinator.com/vote?id=40410012&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Is there any way that I can pay YouTube so that the videos I post do not have ads when people view them for free?  I think there is a clear answer to this question for video, and it’s ~$60/month.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40410790"><td></td></tr>
                  <tr id="40409443"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409443" href="https://news.ycombinator.com/vote?id=40409443&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Storage, transcoding/encoding, and any other compute operations (rendering, etc) are small compared to data transfer costs.<p>At the scale of the largest streaming apps (Disney, Netflix, YouTube, etc) you are moving petabytes of data PER DAY. At that size, you have access to significant savings on CDNs, backbone providers, etc. in many cases the discounts will be 90% - I have seen as high as 99% - or higher off the “list” price (which are usually never paid by anyone anyway).</p><p>You also tend to own your own backbone and can link in whichever ISP wherever you want for the “final mile.”</p><p>Final note, when you have been doing this long enough, you can start shaping the traffic based off previous patterns. I remember an eBay listing years ago for a Netflix local storage device that was meant to store shows at an ISP’s data center.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40409775"><td></td></tr>
                  <tr id="40409269"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409269" href="https://news.ycombinator.com/vote?id=40409269&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>Bandwidth/Transport costs for live streaming, especially in group conference call scenarios (where N streams needs to be broadcasted to N-1 participants) become prohibitively expensive after about 8 or so participants unless you can offload those bandwidth requirements to other places (e.g., like in a peer-to-peer architecture).<p>How Zoom manages to do this in a client-server fashion and is still financially solvent is also a question I've had for a while, but like others say, discounts on the transport and peering arrangements will be a key part in making those economics work, as compression and storage are relatively solved problems here.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40409664"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409664" href="https://news.ycombinator.com/vote?id=40409664&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>&gt; Streaming, encoding, and storage demands enormous costs -- especially at scale<p>When you look at costs per unit, then it gets cheaper at scale, not more expensive.</p><p>For streaming, at scale you can afford to do peering yourself, instead of buying bandwidth.</p><p>For encoding, at scale you can afford special purpose encoding hardware, instead of using general purpose hardware.</p><p>For storage, at scale you can get cheap bulk deals with drive manufacturers.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40409065"><td></td></tr>
                <tr id="40409248"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40409248" href="https://news.ycombinator.com/vote?id=40409248&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>I realized this as I saw many of my favorite gaming Youtubers chase the algorithm and change their content to stay relevant. It all eventually converges on content that children would find entertaining. They're the biggest demographic spending the most time looking at ads. That's where following the algorithm leads.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40409192"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409192" href="https://news.ycombinator.com/vote?id=40409192&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><br><div>
                  <p><span>Video streaming is expensive <i>for you</i>. Google is incredible at making things cheap that we all thought were expensive.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40409734"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40409734" href="https://news.ycombinator.com/vote?id=40409734&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>&gt;Google is incredible at making things cheap<p>As Curly would say, "Eh, a big cheapskate, nyuk nyuk !" ;)</p><p>It's true they do have a "cheapening" effect, especially over time, but Curly's a knucklehead, I wouldn't want to compete with them on their own terms.  That's a big gorilla.</p><p>&gt;If tomorrow I want to start a platform</p><p>Seems like one approach would be to start out with what you can easily afford to begin with.</p><p>Which brings me exactly to the bare bones of storing, encoding, streaming and nothing else.</p><p>If nothing else to minimize complexity and cost of getting started.</p><p>And to possibly obviate the need for monetization up to a point.</p><p>To launch, just pick one fairly popular &amp; accessible format/bit-rate and encode all your raw content (or a test portion of content) the exact same way in advance.  Afterward, you're done with that phase and free of any need for real-time encoding.  You still need to store and subsequently "outstream" your ready-to-deliver content.</p><p>It may actually cost you nothing to store a working copy of your encoded content "library" on your own private server on your own designated premises, especially if you already own the storage devices and there is plenty of unused storage space.  There are also alternatives that are not without cost, only you could decide if it was worth money or not.</p><p>Naturally you will be limited by the bandwidth and infrastructure at each storage location, as to how many viewers at what resolution you can directly serve at one time, and whether or not the ISP/router can be configured to allow outside access to your server.</p><p>If you're going to use 100Mbps of surplus upload bandwidth from a business internet account for instance, and your content was encoded at 1.5Mbps (don't even think about 4K), it may be no additional cost to start serving viewers directly from that server, but you would not be able to serve more than about 50 viewers at one time.</p><p>That might get you started (at an appropriate scale) with no cash outlay whatsoever, and if the demand was there beyond a few dozen viewers then you could decide to pass your stream along to a more capable content delivery network of some kind, at various incremental cost.</p><p>Alternatively, the whole thing could be outsourced and hosted for world-wide access in a turnkey operation where all you do is supply the content.  Cost may be a prohibiting factor, it does seem like there are hosting plans with a free tier but not with enough bandwidth to serve a meaningful number of viewers compared to YT.</p><p>Fortunately for YT, when they got started they didn't have competition already showing 4K stuff to compare to.</p><p>But if the action you take, has cost within the range of what you can <i>easily</i> bear, you could then afford to deliver a completely superior, ad-free experience for your fortunate few viewers.  If you wanted to.  Something a multi-billion-dollar company seems to be less and less able to afford.  What a position to be in.  If the whole thing actually was costing you no cash at all you'd be free to make it seem as free and frictionless as YT, probably more so because it was free from the ground up.</p><p>&gt;a platform that is supported with Advert revenues</p><p>If you did decide to go this route and were sustainable without ads to begin with, you could very judiciously choose your sponsors to be ones that did not conflict with any feature that is more meaningful to the visitors.  You would also be financially ahead beginning with the first ad you decided to run.  And you could decide to stop at any time.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="40409590"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409590" href="https://news.ycombinator.com/vote?id=40409590&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>It sure doesn’t feel free. YouTube has cranked the ads up so frickin’ high that I swear they’re quietly in panic mode about profitability.<p>Every month I notice the temperature of the pot is up a few degrees. This month it’s unskippable 15 second ads before most videos. Last month it was the first search result now being an ad.  Before that it was how 5 second ads are now 7 seconds.</p><p>If I thought to write them all down I’d have a dozen more steps to share.</p><p>My kids now call it “the Bad YouTube” vs. YouTube Kids because the former is flooded with ads.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40410479"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40410479" href="https://news.ycombinator.com/vote?id=40410479&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>As someone who works in a large cloud company, there is a lot going on to create a data center. By data center I mean compute and storage. These large companies have perfected the economics and engineering needed to create a data center. They spend billions on R&amp;D. They basically own everything in their supply chain. Smaller companies can't compete.<p>Additionally, they don't pay the same electricity and water bill that others pay for their data center. They get a discount because they are creating jobs.</p><p>Getting streaming to be cost effective starts from decades of R&amp;D investment + getting low cost electricity and water + owning supply chain.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40410283"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40410283" href="https://news.ycombinator.com/vote?id=40410283&amp;how=up&amp;goto=item%3Fid%3D40408515"></a></center>    </td><td><p><span>YouTube is vertically integrated.<p>They're not paying a margin to advertising companies because they are the advertising company, they're not paying a margin to datacenters because they are the datacenter.</p><p>The data gleaned from YT views helps them to run search and vice versa.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40410317"><td></td></tr>
            <tr id="40409428"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coding My Handwriting (447 pts)]]></title>
            <link>https://www.amygoodchild.com/blog/cursive-handwriting-in-javascript</link>
            <guid>40408291</guid>
            <pubDate>Sun, 19 May 2024 17:15:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amygoodchild.com/blog/cursive-handwriting-in-javascript">https://www.amygoodchild.com/blog/cursive-handwriting-in-javascript</a>, See on <a href="https://news.ycombinator.com/item?id=40408291">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">
        
          
<article id="sections" data-page-sections="5f37d09b551f154577577f0a">
  
  
    
    


  


<div data-content-field="main-content" data-item-id="" data-test="page-section" data-section-theme="white" data-section-id="5f37d09b551f154577577f0c" data-controller="SectionWrapperController" data-current-styles="{
&quot;imageOverlayOpacity&quot;: 0.15,
&quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
&quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
&quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
&quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
&quot;contentWidth&quot;: &quot;content-width--wide&quot;,
&quot;sectionTheme&quot;: &quot;white&quot;,
&quot;sectionAnimation&quot;: &quot;none&quot;,
&quot;backgroundMode&quot;: &quot;image&quot;
}" data-current-context="{
&quot;video&quot;: {
&quot;playbackSpeed&quot;: 0.5,
&quot;filter&quot;: 1,
&quot;filterStrength&quot;: 0,
&quot;zoom&quot;: 0,
&quot;videoSourceProvider&quot;: &quot;none&quot;
},
&quot;backgroundImageId&quot;: null,
&quot;backgroundMediaEffect&quot;: null,
&quot;divider&quot;: null,
&quot;typeName&quot;: &quot;blog-basic-grid&quot;
}" data-animation="none">
  <article id="article-">
  
    
    
    
    <div data-layout-label="Post Body" data-type="item" id="item-6642095b62643354c29c66e0"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-8156bbeef2534803659c">
  <p>A familiar theme for me is dismissing an idea as being too much work and then later finding myself doing it anyway. That’s what happened here.</p><p>A little while ago I created a block script in JavaScript, thinking that cursive would be too complex. But here I am, two months later, ready to talk about the cursive handwriting I’ve created. There is perhaps a lesson in that but let’s not dwell on it.  </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715603804147_8094">
  <h3>Block script</h3><p><a href="https://www.amygoodchild.com/blog/generating-the-alphabet" target="_blank">This previous article</a> is about my block printed version of the alphabet. As a summary, I created it by: </p><ul data-rte-list="default"><li><p>Writing code to define key points in each letter’s paths (~10 points per letter).</p></li><li><p>Smoothing those paths using Chaikin’s curve algorithm.</p></li><li><p>Turning the path into a shape for variable thickness along the length.</p></li><li><p>Draw the shape paths using p5js.</p></li></ul><p>It looked like this: </p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715603804147_5236">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/eba9ea4a-a805-4c1d-8630-c2bcf0772453/printed+handwriting.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/eba9ea4a-a805-4c1d-8630-c2bcf0772453/printed+handwriting.png" data-image-dimensions="777x1010" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/eba9ea4a-a805-4c1d-8630-c2bcf0772453/printed+handwriting.png" width="777" height="1010" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/eba9ea4a-a805-4c1d-8630-c2bcf0772453/printed+handwriting.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/eba9ea4a-a805-4c1d-8630-c2bcf0772453/printed+handwriting.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/eba9ea4a-a805-4c1d-8630-c2bcf0772453/printed+handwriting.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/eba9ea4a-a805-4c1d-8630-c2bcf0772453/printed+handwriting.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/eba9ea4a-a805-4c1d-8630-c2bcf0772453/printed+handwriting.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/eba9ea4a-a805-4c1d-8630-c2bcf0772453/printed+handwriting.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/eba9ea4a-a805-4c1d-8630-c2bcf0772453/printed+handwriting.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715603804147_6047">

<p>By the way, an article about my system for generating these sentences is coming soon, sign up to my newsletter to hear about it.</p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715603804147_7576">

<p>Defining the original paths for those letters was a very manual process of writing their positions into the code and then nudging the points back and forth until the letters looked right. When it came to coding cursive, I streamlined the process. </p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715603804147_12260">
  <h3>Designing Letters</h3><p>In the p5js editor, for easy access, I <a href="https://editor.p5js.org/amygoodchild/sketches/GZzkh4cWt" target="_blank">created a tool</a> to define and output the key points in the paths. </p><p>It displays a sample letter (for scale and context) next to an area in which to design the new letter with these steps:</p><ul data-rte-list="default"><li><p>Click to place key points for the path - the resultant Chaikin-curved path is shown.</p></li><li><p>Tap ‘p’ to switch to editing mode.</p></li><li><p>Select points and drag them into position. </p></li><li><p>Tap ‘enter’ to output the path to the console. </p></li></ul><p>I created 2-3 options for each letter. </p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715608887114_6458">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b22794d7-33d0-4a7e-b5f8-ce4b8b2aa363/letter+designer.gif" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b22794d7-33d0-4a7e-b5f8-ce4b8b2aa363/letter+designer.gif" data-image-dimensions="600x353" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b22794d7-33d0-4a7e-b5f8-ce4b8b2aa363/letter+designer.gif" width="600" height="353" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b22794d7-33d0-4a7e-b5f8-ce4b8b2aa363/letter+designer.gif?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b22794d7-33d0-4a7e-b5f8-ce4b8b2aa363/letter+designer.gif?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b22794d7-33d0-4a7e-b5f8-ce4b8b2aa363/letter+designer.gif?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b22794d7-33d0-4a7e-b5f8-ce4b8b2aa363/letter+designer.gif?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b22794d7-33d0-4a7e-b5f8-ce4b8b2aa363/letter+designer.gif?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b22794d7-33d0-4a7e-b5f8-ce4b8b2aa363/letter+designer.gif?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/b22794d7-33d0-4a7e-b5f8-ce4b8b2aa363/letter+designer.gif?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715608887114_11164">

<p>The path that results looks like this: </p>




















  
  



</div><div data-block-type="23" id="block-yui_3_17_2_1_1715608887114_11555">
<pre><code>[{x:0.7,y:22.5},{x:8.2,y:18.1},{x:8.9,y:11.2},{x:3.7,y:11.4},{x:1.7,y:18.9},{x:8.4,y:22.4},{x:17.7,y:22.0}] </code></pre></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_6768">
  <p>I wanted to use my own handwriting as a guide, so I wrote out a range of examples of lower and uppercase letters and loaded the image directly into my letter building tool for tracing.</p><p>The w/a/s/d keys are used to place the image in the right spot and r/e zooms the image in and out. The blurry ‘e’ you see in the gif above is the sample image. </p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715610756359_8038">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/93889136-57d1-4f0a-a508-7aa8ce5209b2/handwritingsample.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/93889136-57d1-4f0a-a508-7aa8ce5209b2/handwritingsample.jpg" data-image-dimensions="2904x2148" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/93889136-57d1-4f0a-a508-7aa8ce5209b2/handwritingsample.jpg" width="2904" height="2148" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/93889136-57d1-4f0a-a508-7aa8ce5209b2/handwritingsample.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/93889136-57d1-4f0a-a508-7aa8ce5209b2/handwritingsample.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/93889136-57d1-4f0a-a508-7aa8ce5209b2/handwritingsample.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/93889136-57d1-4f0a-a508-7aa8ce5209b2/handwritingsample.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/93889136-57d1-4f0a-a508-7aa8ce5209b2/handwritingsample.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/93889136-57d1-4f0a-a508-7aa8ce5209b2/handwritingsample.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/93889136-57d1-4f0a-a508-7aa8ce5209b2/handwritingsample.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_13825">

<p>The numbers noted on the paper are the x y coordinates to get that area to be in the letter creation window.</p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_11950">

<p>After creating all the paths, curving them and turning them into shapes with variable width,  (check the <a href="https://www.amygoodchild.com/blog/generating-the-alphabet" target="_blank">previous article</a> for more detail), here’s how the characters look individually. </p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715610756359_12692">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/5e99bed5-e27f-40bb-b95b-4c76d198e2ea/cursive+letters+individual.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/5e99bed5-e27f-40bb-b95b-4c76d198e2ea/cursive+letters+individual.png" data-image-dimensions="1208x337" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/5e99bed5-e27f-40bb-b95b-4c76d198e2ea/cursive+letters+individual.png" width="1208" height="337" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/5e99bed5-e27f-40bb-b95b-4c76d198e2ea/cursive+letters+individual.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/5e99bed5-e27f-40bb-b95b-4c76d198e2ea/cursive+letters+individual.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/5e99bed5-e27f-40bb-b95b-4c76d198e2ea/cursive+letters+individual.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/5e99bed5-e27f-40bb-b95b-4c76d198e2ea/cursive+letters+individual.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/5e99bed5-e27f-40bb-b95b-4c76d198e2ea/cursive+letters+individual.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/5e99bed5-e27f-40bb-b95b-4c76d198e2ea/cursive+letters+individual.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/5e99bed5-e27f-40bb-b95b-4c76d198e2ea/cursive+letters+individual.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_11339">
  <h3>Cursive-ifying, Cursifying (?) </h3><p>Sometimes joining letters is easy, you just go straight from one path of key points to the next before Chaikin curving them all in one go. But some letter pairs do not work nicely together.</p><p>Consider the letter pair <strong>na</strong>. In red we can see the last point of the letter <strong>n</strong>, which is low, and in green, the first point of the letter <strong>a</strong>, which is high. This causes the joining path to go diagonally through the <strong>a</strong>, making it look a bit like an <strong>e</strong>. </p><p>Meanwhile in the pair <strong>ti</strong>, the <strong>t</strong> ends just above the baseline and the <strong>i</strong> starts on it, causing an unnatural ridge. </p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715610756359_14483">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/7984a9cc-0079-4dc4-94e6-d323f25c05fb/na+-+1.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/7984a9cc-0079-4dc4-94e6-d323f25c05fb/na+-+1.png" data-image-dimensions="500x500" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/7984a9cc-0079-4dc4-94e6-d323f25c05fb/na+-+1.png" width="500" height="500" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/7984a9cc-0079-4dc4-94e6-d323f25c05fb/na+-+1.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/7984a9cc-0079-4dc4-94e6-d323f25c05fb/na+-+1.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/7984a9cc-0079-4dc4-94e6-d323f25c05fb/na+-+1.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/7984a9cc-0079-4dc4-94e6-d323f25c05fb/na+-+1.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/7984a9cc-0079-4dc4-94e6-d323f25c05fb/na+-+1.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/7984a9cc-0079-4dc4-94e6-d323f25c05fb/na+-+1.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/7984a9cc-0079-4dc4-94e6-d323f25c05fb/na+-+1.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715610756359_32131">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/f333ede0-cb98-4db3-99e3-feadcdcbda39/ti.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/f333ede0-cb98-4db3-99e3-feadcdcbda39/ti.png" data-image-dimensions="500x500" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/f333ede0-cb98-4db3-99e3-feadcdcbda39/ti.png" width="500" height="500" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/f333ede0-cb98-4db3-99e3-feadcdcbda39/ti.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/f333ede0-cb98-4db3-99e3-feadcdcbda39/ti.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/f333ede0-cb98-4db3-99e3-feadcdcbda39/ti.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/f333ede0-cb98-4db3-99e3-feadcdcbda39/ti.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/f333ede0-cb98-4db3-99e3-feadcdcbda39/ti.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/f333ede0-cb98-4db3-99e3-feadcdcbda39/ti.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/f333ede0-cb98-4db3-99e3-feadcdcbda39/ti.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_19361">

<p>To fix these issues we can add an extra point to the start of the <strong>a</strong>, and delete the last two points on the <strong>t</strong>. </p>




















  
  



</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715610756359_20290">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/e3456092-c6ba-4ade-b4a8-2e925de89e90/na+-+2.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/e3456092-c6ba-4ade-b4a8-2e925de89e90/na+-+2.png" data-image-dimensions="500x500" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/e3456092-c6ba-4ade-b4a8-2e925de89e90/na+-+2.png" width="500" height="500" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/e3456092-c6ba-4ade-b4a8-2e925de89e90/na+-+2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/e3456092-c6ba-4ade-b4a8-2e925de89e90/na+-+2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/e3456092-c6ba-4ade-b4a8-2e925de89e90/na+-+2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/e3456092-c6ba-4ade-b4a8-2e925de89e90/na+-+2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/e3456092-c6ba-4ade-b4a8-2e925de89e90/na+-+2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/e3456092-c6ba-4ade-b4a8-2e925de89e90/na+-+2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/e3456092-c6ba-4ade-b4a8-2e925de89e90/na+-+2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715610756359_60784">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/edec8a82-c9eb-4467-b731-ec4cc27afb80/ti+-+2.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/edec8a82-c9eb-4467-b731-ec4cc27afb80/ti+-+2.png" data-image-dimensions="500x500" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/edec8a82-c9eb-4467-b731-ec4cc27afb80/ti+-+2.png" width="500" height="500" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/edec8a82-c9eb-4467-b731-ec4cc27afb80/ti+-+2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/edec8a82-c9eb-4467-b731-ec4cc27afb80/ti+-+2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/edec8a82-c9eb-4467-b731-ec4cc27afb80/ti+-+2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/edec8a82-c9eb-4467-b731-ec4cc27afb80/ti+-+2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/edec8a82-c9eb-4467-b731-ec4cc27afb80/ti+-+2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/edec8a82-c9eb-4467-b731-ec4cc27afb80/ti+-+2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/edec8a82-c9eb-4467-b731-ec4cc27afb80/ti+-+2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_23305">
  <p>But we can’t just change the letters like that for all scenarios. </p><p>For example, if the <strong>a</strong> is at the start of a word, the additional point will be out of place and if the <strong>a</strong> preceded by a letter like <strong>w</strong>, it creates a line that crosses through the <strong>a </strong>in a different way. If the <strong>t</strong> is paired with a <strong>k</strong>, it becomes deformed. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_28307">
  <p>The points at the beginning and ends of letter paths need to vary depending on which other letters they are next to. </p><p>At first I tried calling out particular “problem” pairs and writing rules for them specifically but, in the end, I added a single number to the beginning and end of each path which states if it: </p><ul data-rte-list="default"><li><p>Cannot join another letter (0)</p></li><li><p>Joins another letter around the base line (1)</p></li><li><p>Joins another letter just above the base line (2)</p></li><li><p>Joins another letter around the x-height (3)</p></li></ul><p>Here are some examples:</p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715610756359_86058">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4c290819-b063-4d0a-b083-804b7036d783/join+heights+-+0.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4c290819-b063-4d0a-b083-804b7036d783/join+heights+-+0.png" data-image-dimensions="693x470" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4c290819-b063-4d0a-b083-804b7036d783/join+heights+-+0.png" width="693" height="470" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4c290819-b063-4d0a-b083-804b7036d783/join+heights+-+0.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4c290819-b063-4d0a-b083-804b7036d783/join+heights+-+0.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4c290819-b063-4d0a-b083-804b7036d783/join+heights+-+0.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4c290819-b063-4d0a-b083-804b7036d783/join+heights+-+0.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4c290819-b063-4d0a-b083-804b7036d783/join+heights+-+0.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4c290819-b063-4d0a-b083-804b7036d783/join+heights+-+0.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4c290819-b063-4d0a-b083-804b7036d783/join+heights+-+0.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715610756359_87117">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/daa228f1-48f9-4594-88b9-8e696cf0d176/join+heights+-+1.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/daa228f1-48f9-4594-88b9-8e696cf0d176/join+heights+-+1.png" data-image-dimensions="693x470" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/daa228f1-48f9-4594-88b9-8e696cf0d176/join+heights+-+1.png" width="693" height="470" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/daa228f1-48f9-4594-88b9-8e696cf0d176/join+heights+-+1.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/daa228f1-48f9-4594-88b9-8e696cf0d176/join+heights+-+1.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/daa228f1-48f9-4594-88b9-8e696cf0d176/join+heights+-+1.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/daa228f1-48f9-4594-88b9-8e696cf0d176/join+heights+-+1.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/daa228f1-48f9-4594-88b9-8e696cf0d176/join+heights+-+1.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/daa228f1-48f9-4594-88b9-8e696cf0d176/join+heights+-+1.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/daa228f1-48f9-4594-88b9-8e696cf0d176/join+heights+-+1.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715610756359_91110">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/962005c8-c702-417a-9d26-7d0ed69ad611/join+heights+-+2.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/962005c8-c702-417a-9d26-7d0ed69ad611/join+heights+-+2.png" data-image-dimensions="693x448" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/962005c8-c702-417a-9d26-7d0ed69ad611/join+heights+-+2.png" width="693" height="448" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/962005c8-c702-417a-9d26-7d0ed69ad611/join+heights+-+2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/962005c8-c702-417a-9d26-7d0ed69ad611/join+heights+-+2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/962005c8-c702-417a-9d26-7d0ed69ad611/join+heights+-+2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/962005c8-c702-417a-9d26-7d0ed69ad611/join+heights+-+2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/962005c8-c702-417a-9d26-7d0ed69ad611/join+heights+-+2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/962005c8-c702-417a-9d26-7d0ed69ad611/join+heights+-+2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/962005c8-c702-417a-9d26-7d0ed69ad611/join+heights+-+2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715610756359_92175">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9b0091c0-e647-4800-83e7-33c6419eb13e/join+heights+-+3.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9b0091c0-e647-4800-83e7-33c6419eb13e/join+heights+-+3.png" data-image-dimensions="693x448" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9b0091c0-e647-4800-83e7-33c6419eb13e/join+heights+-+3.png" width="693" height="448" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9b0091c0-e647-4800-83e7-33c6419eb13e/join+heights+-+3.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9b0091c0-e647-4800-83e7-33c6419eb13e/join+heights+-+3.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9b0091c0-e647-4800-83e7-33c6419eb13e/join+heights+-+3.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9b0091c0-e647-4800-83e7-33c6419eb13e/join+heights+-+3.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9b0091c0-e647-4800-83e7-33c6419eb13e/join+heights+-+3.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9b0091c0-e647-4800-83e7-33c6419eb13e/join+heights+-+3.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/9b0091c0-e647-4800-83e7-33c6419eb13e/join+heights+-+3.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_83859">

<p>Each letter path now looks something like this, note the single digits at the beginning and end:</p>




















  
  



</div><div data-block-type="23" id="block-yui_3_17_2_1_1715610756359_82950">
<pre><code>[0,{x:12.2,y:13.2},{x:13.5,y:11.0},{x:6.2,y:8.4},{x:1.1,y:13.0},{x:1.8,y:19.0},{x:7.0,y:23.4},{x:15.2,y:23.6},{x:18.4,y:22.1},1],</code></pre></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715752260728_99684">

<p>I tested all of the letter pairs, like so:</p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715752260728_104149">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8e7c8f57-54e5-4aae-8d94-41c9826d11de/letterpairs.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8e7c8f57-54e5-4aae-8d94-41c9826d11de/letterpairs.png" data-image-dimensions="3164x3165" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8e7c8f57-54e5-4aae-8d94-41c9826d11de/letterpairs.png" width="3164" height="3165" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8e7c8f57-54e5-4aae-8d94-41c9826d11de/letterpairs.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8e7c8f57-54e5-4aae-8d94-41c9826d11de/letterpairs.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8e7c8f57-54e5-4aae-8d94-41c9826d11de/letterpairs.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8e7c8f57-54e5-4aae-8d94-41c9826d11de/letterpairs.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8e7c8f57-54e5-4aae-8d94-41c9826d11de/letterpairs.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8e7c8f57-54e5-4aae-8d94-41c9826d11de/letterpairs.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/8e7c8f57-54e5-4aae-8d94-41c9826d11de/letterpairs.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715752260728_106462">

<p>Here you can also see some of the variation, created by having multiple paths for each letter and also by editing the letters depending on what letter they are next to. Ideally I would have at least 5 or 6 options of paths for each letter but there is a balance to be drawn against file size. </p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_99517">
  <h3>Creating words</h3><p>When a word is created: </p><ul data-rte-list="default"><li><p>a basic path is chosen for each letter from the 2-3 different options for that character.</p></li><li><p>the information about the ends of the paths are passed to the adjacent letters<br>(the letter paths all have to be chosen first as, in some cases, different path options for the same letter have different end points)</p></li><li><p>The basic paths are adjusted in response to their neighbours. <br>E.g. if the previous letter’s end height is 2, remove 1 point from the start of this path, or if the next letter’s start height is 1, add an additional point in a certain location. </p></li></ul><p>The adjustment functions can get a bit complicated, for example here is the one for the letter q:</p>
</div><div data-block-type="23" id="block-yui_3_17_2_1_1715610756359_131824">
<pre><code>// ip = path 
// pc = previous char's end info 
// nc = next char's start info 
// n = index of path that was chosen for this letter
adjust: (ip, pc, nc, n) =&gt; {
  // randomly adds in a break at the end for 70% of this letter
  if (rand() &lt; 0.7 ) ip.splice(-1, 1, 0);

   // if [2] was chosen for this path from the 4 options, 
   if (n &lt; 2) {

     // Swap out first two points for a different point if the previous char ends at 3
     if (pc == 3) ip.splice(1, 2, {x:10,y:12});

     // Otherwise, as long as it's not a 0, add a point at the beginning
     else if (pc &gt; 0) ip.splice(1, 0, {x:10,y:20});
  }

  // If there's no break (0) between this character and the next
  if (nc &gt; 0 &amp;&amp; ip[ip.length-1] != 0){
    // Swap out the last two points for a different one 
    ip.splice(-3, 2, {x:16,y:34})
  }
}</code></pre></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_133889">

<p>But often they are fairly short, here is the one for the letter n:</p>




















  
  



</div><div data-block-type="23" id="block-yui_3_17_2_1_1715610756359_138034">
<pre><code>adjust: (ip, pc, nc) =&gt; {
  // If the next letter starts at a 3, randomly either create a break or move the last point 
  if (nc == 3) rand() &lt; 0.3 ? ip.splice(-1, 1, 0) : ip.splice(-2, 1, {x:17,y:23.8})
}</code></pre></div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_140399">

<p>Next the basic paths for all the letters are joined together. While doing this, it ignores 1, 2 and 3’s in the letter paths but whenever there is a 0 it creates a break by starting a new path. </p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_152142">

<p>After curving those paths, turning them into varied width shapes, and adding some jittering around using Perlin noise, here’s what the cursive writing looks like.</p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715610756359_153981">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4555438f-8f38-4571-8fc5-fd1baafcf559/Cursive+text.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4555438f-8f38-4571-8fc5-fd1baafcf559/Cursive+text.png" data-image-dimensions="1274x1284" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4555438f-8f38-4571-8fc5-fd1baafcf559/Cursive+text.png" width="1274" height="1284" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4555438f-8f38-4571-8fc5-fd1baafcf559/Cursive+text.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4555438f-8f38-4571-8fc5-fd1baafcf559/Cursive+text.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4555438f-8f38-4571-8fc5-fd1baafcf559/Cursive+text.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4555438f-8f38-4571-8fc5-fd1baafcf559/Cursive+text.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4555438f-8f38-4571-8fc5-fd1baafcf559/Cursive+text.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4555438f-8f38-4571-8fc5-fd1baafcf559/Cursive+text.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4555438f-8f38-4571-8fc5-fd1baafcf559/Cursive+text.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715610756359_155793">

<p>An article about generating these sentences will be coming soon, you can sign up to my newsletter to get a heads up when it’s out. </p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715688327725_64868">

<p>For fun, here’s a side by side comparison of the coded handwriting run through my plotter, next to my actual handwriting. </p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715610756359_159523">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fc2b53c5-879a-4dd0-b568-6957eff56693/handwriting+comparison.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fc2b53c5-879a-4dd0-b568-6957eff56693/handwriting+comparison.jpg" data-image-dimensions="2794x1946" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fc2b53c5-879a-4dd0-b568-6957eff56693/handwriting+comparison.jpg" width="2794" height="1946" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fc2b53c5-879a-4dd0-b568-6957eff56693/handwriting+comparison.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fc2b53c5-879a-4dd0-b568-6957eff56693/handwriting+comparison.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fc2b53c5-879a-4dd0-b568-6957eff56693/handwriting+comparison.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fc2b53c5-879a-4dd0-b568-6957eff56693/handwriting+comparison.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fc2b53c5-879a-4dd0-b568-6957eff56693/handwriting+comparison.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fc2b53c5-879a-4dd0-b568-6957eff56693/handwriting+comparison.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/fc2b53c5-879a-4dd0-b568-6957eff56693/handwriting+comparison.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715688327725_67491">
  <h3>WHAT DOES IT WEIGH?</h3><p>The letter class for the block print was 9.7kb. The letter class for the cursive handwriting (after being run through a minifier) is currently 26.1kb. </p><p>This one is larger because there are multiple paths for each letter as well as the function for adjust the points to meet the letter’s neighbours, but I have made some other savings. I’m sure further savings could be made - I am not a code golf wizard but I have a few ideas. </p><p>For example, currently the letters are designed around a default font size of 20 and then resized, meaning lots of the points are defined as e.g. x: 14.5, but if I switch this to a default size of 200, the point could be defined as 145, removing one character (the decimal place). I need to make this change carefully, so it’s on the To Do list for later. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715752260728_44848">
  <h3>How I’m Using it</h3><p>The main purpose for this handwriting is for the titles, labels and scribbled notes on these <a href="https://x.com/amygoodchild/status/1770433357777973273" target="_blank">diagrams</a> I’ve been working on. But I’m also have a lot of fun playing around with the text itself. </p><p>One of the best things about having encoded paths instead of using a font is that I can mess around with those paths. Changing the position of letters and changing the thickness across an individual letter and so on. </p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715752260728_54208">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0ffc0b11-4c4f-4fa1-be64-2d8f1f43878b/handwriting-text-1.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0ffc0b11-4c4f-4fa1-be64-2d8f1f43878b/handwriting-text-1.png" data-image-dimensions="2352x2352" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0ffc0b11-4c4f-4fa1-be64-2d8f1f43878b/handwriting-text-1.png" width="2352" height="2352" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0ffc0b11-4c4f-4fa1-be64-2d8f1f43878b/handwriting-text-1.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0ffc0b11-4c4f-4fa1-be64-2d8f1f43878b/handwriting-text-1.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0ffc0b11-4c4f-4fa1-be64-2d8f1f43878b/handwriting-text-1.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0ffc0b11-4c4f-4fa1-be64-2d8f1f43878b/handwriting-text-1.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0ffc0b11-4c4f-4fa1-be64-2d8f1f43878b/handwriting-text-1.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0ffc0b11-4c4f-4fa1-be64-2d8f1f43878b/handwriting-text-1.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/0ffc0b11-4c4f-4fa1-be64-2d8f1f43878b/handwriting-text-1.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715752260728_56521">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/18f126d9-96c3-4835-9fbe-d1f80a328d89/handwriting-text-3.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/18f126d9-96c3-4835-9fbe-d1f80a328d89/handwriting-text-3.png" data-image-dimensions="2352x2352" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/18f126d9-96c3-4835-9fbe-d1f80a328d89/handwriting-text-3.png" width="2352" height="2352" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/18f126d9-96c3-4835-9fbe-d1f80a328d89/handwriting-text-3.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/18f126d9-96c3-4835-9fbe-d1f80a328d89/handwriting-text-3.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/18f126d9-96c3-4835-9fbe-d1f80a328d89/handwriting-text-3.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/18f126d9-96c3-4835-9fbe-d1f80a328d89/handwriting-text-3.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/18f126d9-96c3-4835-9fbe-d1f80a328d89/handwriting-text-3.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/18f126d9-96c3-4835-9fbe-d1f80a328d89/handwriting-text-3.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/18f126d9-96c3-4835-9fbe-d1f80a328d89/handwriting-text-3.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715752260728_61966">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ef17597-1b2d-4d59-8ee3-eb412188c50f/handwriting-text-6.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ef17597-1b2d-4d59-8ee3-eb412188c50f/handwriting-text-6.png" data-image-dimensions="2352x2352" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ef17597-1b2d-4d59-8ee3-eb412188c50f/handwriting-text-6.png" width="2352" height="2352" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ef17597-1b2d-4d59-8ee3-eb412188c50f/handwriting-text-6.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ef17597-1b2d-4d59-8ee3-eb412188c50f/handwriting-text-6.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ef17597-1b2d-4d59-8ee3-eb412188c50f/handwriting-text-6.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ef17597-1b2d-4d59-8ee3-eb412188c50f/handwriting-text-6.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ef17597-1b2d-4d59-8ee3-eb412188c50f/handwriting-text-6.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ef17597-1b2d-4d59-8ee3-eb412188c50f/handwriting-text-6.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/4ef17597-1b2d-4d59-8ee3-eb412188c50f/handwriting-text-6.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715752260728_67735">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/1289f9d9-f244-4e4f-9a86-a72dd2ab4339/handwriting-text-5.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/1289f9d9-f244-4e4f-9a86-a72dd2ab4339/handwriting-text-5.png" data-image-dimensions="2352x2352" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/1289f9d9-f244-4e4f-9a86-a72dd2ab4339/handwriting-text-5.png" width="2352" height="2352" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/1289f9d9-f244-4e4f-9a86-a72dd2ab4339/handwriting-text-5.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/1289f9d9-f244-4e4f-9a86-a72dd2ab4339/handwriting-text-5.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/1289f9d9-f244-4e4f-9a86-a72dd2ab4339/handwriting-text-5.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/1289f9d9-f244-4e4f-9a86-a72dd2ab4339/handwriting-text-5.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/1289f9d9-f244-4e4f-9a86-a72dd2ab4339/handwriting-text-5.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/1289f9d9-f244-4e4f-9a86-a72dd2ab4339/handwriting-text-5.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/1289f9d9-f244-4e4f-9a86-a72dd2ab4339/handwriting-text-5.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715752260728_69016">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72f0234a-05dc-44d5-aa0e-cfe012d6e4e2/handwriting-text-4.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72f0234a-05dc-44d5-aa0e-cfe012d6e4e2/handwriting-text-4.png" data-image-dimensions="2352x2352" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72f0234a-05dc-44d5-aa0e-cfe012d6e4e2/handwriting-text-4.png" width="2352" height="2352" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72f0234a-05dc-44d5-aa0e-cfe012d6e4e2/handwriting-text-4.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72f0234a-05dc-44d5-aa0e-cfe012d6e4e2/handwriting-text-4.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72f0234a-05dc-44d5-aa0e-cfe012d6e4e2/handwriting-text-4.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72f0234a-05dc-44d5-aa0e-cfe012d6e4e2/handwriting-text-4.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72f0234a-05dc-44d5-aa0e-cfe012d6e4e2/handwriting-text-4.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72f0234a-05dc-44d5-aa0e-cfe012d6e4e2/handwriting-text-4.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/72f0234a-05dc-44d5-aa0e-cfe012d6e4e2/handwriting-text-4.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715752260728_55380">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/30b1f065-edb6-4cfe-ae9c-4eb898256c42/handwriting-text-2.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/30b1f065-edb6-4cfe-ae9c-4eb898256c42/handwriting-text-2.png" data-image-dimensions="2352x2352" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/30b1f065-edb6-4cfe-ae9c-4eb898256c42/handwriting-text-2.png" width="2352" height="2352" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 50vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/30b1f065-edb6-4cfe-ae9c-4eb898256c42/handwriting-text-2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/30b1f065-edb6-4cfe-ae9c-4eb898256c42/handwriting-text-2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/30b1f065-edb6-4cfe-ae9c-4eb898256c42/handwriting-text-2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/30b1f065-edb6-4cfe-ae9c-4eb898256c42/handwriting-text-2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/30b1f065-edb6-4cfe-ae9c-4eb898256c42/handwriting-text-2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/30b1f065-edb6-4cfe-ae9c-4eb898256c42/handwriting-text-2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/30b1f065-edb6-4cfe-ae9c-4eb898256c42/handwriting-text-2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div></div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1715951246919_69251">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/de2fe388-555d-49b2-84ac-c06d3e3002cf/handwriting-text-10.png" data-image="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/de2fe388-555d-49b2-84ac-c06d3e3002cf/handwriting-text-10.png" data-image-dimensions="2352x2352" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/de2fe388-555d-49b2-84ac-c06d3e3002cf/handwriting-text-10.png" width="2352" height="2352" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/de2fe388-555d-49b2-84ac-c06d3e3002cf/handwriting-text-10.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/de2fe388-555d-49b2-84ac-c06d3e3002cf/handwriting-text-10.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/de2fe388-555d-49b2-84ac-c06d3e3002cf/handwriting-text-10.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/de2fe388-555d-49b2-84ac-c06d3e3002cf/handwriting-text-10.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/de2fe388-555d-49b2-84ac-c06d3e3002cf/handwriting-text-10.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/de2fe388-555d-49b2-84ac-c06d3e3002cf/handwriting-text-10.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5f33cddd6aff255aabb0c6cd/de2fe388-555d-49b2-84ac-c06d3e3002cf/handwriting-text-10.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715951246919_82022">

<p>Next up I’m going to be incorporating the handwriting into those diagrams, but I’m definitely intending to come back and create something focused on the text itself as well, as I’m finding it super beautiful and there’s a lot of possibility there!</p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1715752260728_120438">
  <p>😍 Enjoyed this article? I’d love it if you could <a href="https://x.com/amygoodchild/status/1791475650190102834" target="_blank">give a boost on Twitter</a>, thanks!</p><p>✨ And don’t forget to sign up for my weekly newsletter, filled with updates. </p>
</div></div>
  
</article>

</div>

  
</article>


          

          
            
              

            
          
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI doppelgänger experiment – Part 1: The training (176 pts)]]></title>
            <link>https://julienposture.substack.com/p/the-ai-doppelganger-experiment-part</link>
            <guid>40407927</guid>
            <pubDate>Sun, 19 May 2024 16:19:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://julienposture.substack.com/p/the-ai-doppelganger-experiment-part">https://julienposture.substack.com/p/the-ai-doppelganger-experiment-part</a>, See on <a href="https://news.ycombinator.com/item?id=40407927">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>&nbsp;‘And only&nbsp;one&nbsp;for birthday presents, you know.&nbsp;There’s glory for you!’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</em></p><p><em>&nbsp;‘I don’t know what you mean by “glory”,’ Alice said.</em></p><p><em>&nbsp;‘Humpty Dumpty smiled contemptuously. ‘Of course, you don’t–till I tell you.&nbsp;I meant “there’s a nice knock-down argument for you!”’</em></p><p><em>&nbsp;‘But “glory” doesn’t mean “a nice knock-down argument”, Alice objected.</em></p><p><em>&nbsp;‘When&nbsp;I&nbsp;use a word,’ Humpty Dumpty said in rather a scornful tone, ‘it means just what I choose it to mean–neither more nor less.’</em></p><p><em>&nbsp;‘The question is,’ said Alice, ‘whether you&nbsp;can&nbsp;make words mean different things–that’s all.’</em></p><p><em>‘The question is,’ said Humpty Dumpty, ‘which is to be master–that’s all’</em></p><p>This passage of Alice Through the Looking Glass is often cited in philosophy of language classes to introduce the idea of language as a social activity that rely on shared meaning. If the same word means different things every time we use it, or different things for different speakers, how could we communicate at all? As I’m researching the social life of images through the fields of illustration, design, law, machine learning, and so on, I’m often reminded of Humpty Dumpty. While illustrators, designers, lawyers, and computer scientists might all use the same words —images, style, art— they rarely share the same meaning. But unlike Alice and Humpty Dumpty, instead of having a conversation about what they mean when they use a word, the people talking about AI and art continue to yap at each other endlessly in mutual misunderstanding. The goal of course, is not mutual understanding, but power, or as Humpty Dumpty puts it, to decide “which is to be master”. Without ever addressing the linguistic nature of the debate, proponents and opponents of generative AI are indeed fighting for the power to define what “image”, “style”, “art” mean.</p><p><span>Communication relies on shared language, and when this communication is about what we see, it also relies on shared ways of seeing. Yet, different people learn to see the same things differently and these ways of seeing, as </span><a href="https://www.youtube.com/watch?v=0pDE4VX_9Kk" rel="">John Berger pointed out</a><span>, are socially constructed. What does an artist see when they look at their style? How does it differ from the ways machine learning models see styles? And which way of seeing has more power?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ab196f5-1f2c-41a5-b45a-906a99c0e667_500x357.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ab196f5-1f2c-41a5-b45a-906a99c0e667_500x357.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ab196f5-1f2c-41a5-b45a-906a99c0e667_500x357.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ab196f5-1f2c-41a5-b45a-906a99c0e667_500x357.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ab196f5-1f2c-41a5-b45a-906a99c0e667_500x357.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ab196f5-1f2c-41a5-b45a-906a99c0e667_500x357.png" width="48" height="34.272" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5ab196f5-1f2c-41a5-b45a-906a99c0e667_500x357.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:357,&quot;width&quot;:500,&quot;resizeWidth&quot;:48,&quot;bytes&quot;:6882,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ab196f5-1f2c-41a5-b45a-906a99c0e667_500x357.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ab196f5-1f2c-41a5-b45a-906a99c0e667_500x357.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ab196f5-1f2c-41a5-b45a-906a99c0e667_500x357.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ab196f5-1f2c-41a5-b45a-906a99c0e667_500x357.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>To start answering these questions, I interviewed computer scientists working on generative AI about “style transfer”, the process of extracting the stylistic features of an image to apply them to another. Both as an illustrator and an anthropologist, I was fascinated by the idea. When I ask illustrators to describe their style, they usually go on long narrative tangents about what they love, how they grew up, who they know, etc. Style is a deeply lived category, a repository of experiences and influences that resists simple definitions. I wondered how computer scientists could create systems that translated this in mathematical vectors that could be extracted from a mere image. How they defined “style”, how they trained a model to pick up on it, and how much of our own perception and the ways we learn to look at images, are all very exciting questions to me.</p><p><span>The main takeaway of these (ongoing) interviews so far has been that computer scientists are interested in style not in itself, but for the challenges that it confronts machine learning models with. Training a model to transfer the style of an image to another involves for that model to take a whole entity, the image, and divides it in layers that are not pre-given in the data itself. In this case, the layers are the </span><em>denotational</em><span> content of the image, what it represents, and its </span><em>stylistic</em><span>, or formal treatment. This is an impressive feat for a machine learning model to be able to achieve this. But the very notion that artworks can be separated in these two aspects, denotational and stylistic, is itself an old idea that can be traced to early twentieth century </span><a href="http://arthistoryresources.net/baroque-art-theory-2013/wolfflin-renaissance-baroque.html" rel="">art historian Heinrich Wölfflin (1864-1945)</a><span> work on formalism. For him, it was important to study artwork from a formal point of view, without considering their subject.</span></p><p><span>We rarely hear about the historical ideas that shape the thinking of technologists, that’s the whole point. Technology is meant to be innovative, disruptive, and the lobbying of AI company is in part of cultural lobbying to erase the historicity of their ideas in order to </span><a href="https://www.effectivealtruism.org/articles/cause-profile-long-run-future" rel="">constantly situate their work in the future.</a><span> So how does an early twentieth century idea about art shows up in AI work today?</span></p><p>As often, it shows up in the unquestioned parts of the process of training a model. Computer scientists have often told me how none of them really has a working definition of style they consciously encode in a model:</p><blockquote><p><em>“It's not very easy to define exactly what style is. But then you have these papers that say ‘Okay, what we're doing is, without defining exactly what style means we're showing you a method that can take an image, which we'll say we'll copy the style from, and we can transplant this style onto another image’. And you get a result that you look at and intuitively you say, ‘Okay, it's another object in the same style’ even if you don't know how to define its parts.”</em></p></blockquote><p>The success or failure of style transfer relies not only on an algorithm, but on the “intuitive” perception of a beholder. This means both machine and human ways of seeing are co-constitutive of the resulting model. Another computer scientist shared that when training their own model, they worked with a designer whose job was to evaluate the quality of the output of a style transfer. The success of the model here became entangled with the taste of the perceiver parsing through outputs, selecting the best, i.e. the output that fit best our culturally specific idea of style. Far from an emergent inexplicable feature of the model, separating content from style is most often the result of tinkering, subjective perception, selection, and more tinkering, all steeped in very human ideas about what images are made of. So, how can I study this very subjective perception?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4499739e-b13c-4d83-ac0f-8fb0da4e3a51_500x357.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4499739e-b13c-4d83-ac0f-8fb0da4e3a51_500x357.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4499739e-b13c-4d83-ac0f-8fb0da4e3a51_500x357.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4499739e-b13c-4d83-ac0f-8fb0da4e3a51_500x357.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4499739e-b13c-4d83-ac0f-8fb0da4e3a51_500x357.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4499739e-b13c-4d83-ac0f-8fb0da4e3a51_500x357.png" width="48" height="34.272" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4499739e-b13c-4d83-ac0f-8fb0da4e3a51_500x357.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:357,&quot;width&quot;:500,&quot;resizeWidth&quot;:48,&quot;bytes&quot;:6882,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4499739e-b13c-4d83-ac0f-8fb0da4e3a51_500x357.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4499739e-b13c-4d83-ac0f-8fb0da4e3a51_500x357.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4499739e-b13c-4d83-ac0f-8fb0da4e3a51_500x357.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4499739e-b13c-4d83-ac0f-8fb0da4e3a51_500x357.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Ways of seeing are a slippery object of study. Unlike the linguistic anthropologists who trained me to study language, I (realistically) cannot record and transcribe “looks” like I would with words. Instead, I’m chasing ways of seeing in various corners of multiple meaning-making practices, from the moodboard designers create to the feedback illustrators get from art directors, from the captioning of a dataset to the prompting of a model. As AI companies are </span><a href="https://www.theatlantic.com/technology/archive/2024/01/ai-transparency-meta-microsoft/677022/" rel="">not exactly as open as they promised to be</a><span>, I’ve decided that instead of spending months negotiating access to secret spaces protected by NDAs, I might as well take the experimental road, and train a model myself. &nbsp;</span></p><p><span>To do this, I contacted an amazing PhD researcher in Human-Computer Interaction at the University of Columbia, Sitong, to work together on this. Working with another researcher was not only a way for me to ensure I would have someone who can explain to me what’s going on, but also to work with someone with the same ethical concerns I have about working with machine learning and people. Together we created an experimental protocol that would allow illustrators to participate and train the model on their own work in a safe environment, and get the opportunity to generate images themselves, thus placing human and machine in interaction to answer the question: </span><em>What is generated in the encounter between human and machine ways of seeing?</em></p><p>Here's how we went about setting up a Human-Computer Interaction (HCI) experiment, within the larger context of my ongoing ethnographic fieldwork with New York creatives, and some insights we gathered along the way.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03c1617a-df51-4751-b6bf-19e0a89ed3ca_500x357.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03c1617a-df51-4751-b6bf-19e0a89ed3ca_500x357.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03c1617a-df51-4751-b6bf-19e0a89ed3ca_500x357.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03c1617a-df51-4751-b6bf-19e0a89ed3ca_500x357.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03c1617a-df51-4751-b6bf-19e0a89ed3ca_500x357.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03c1617a-df51-4751-b6bf-19e0a89ed3ca_500x357.png" width="48" height="34.272" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/03c1617a-df51-4751-b6bf-19e0a89ed3ca_500x357.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:357,&quot;width&quot;:500,&quot;resizeWidth&quot;:48,&quot;bytes&quot;:6882,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03c1617a-df51-4751-b6bf-19e0a89ed3ca_500x357.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03c1617a-df51-4751-b6bf-19e0a89ed3ca_500x357.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03c1617a-df51-4751-b6bf-19e0a89ed3ca_500x357.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03c1617a-df51-4751-b6bf-19e0a89ed3ca_500x357.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Firstly, Sitong found the simplest and safest way to fine-tune a model and ended up choosing to do so with a </span><a href="https://www.youtube.com/watch?app=desktop&amp;v=7m522D01mh0&amp;ab_channel=kasukanra" rel="">LoRa, or Low Rank Adaptation</a><span>. One of the main limitations of regular models like Stable Diffusion or MidJourney is that one cannot maintain a subject through several generations. This means that the description of a character would lead to wildly different results from one output to the next. LoRa was initially created to teach a model to learn a concept, for example a given character, that could then be generated consistently through many images. Interestingly, the model has been since then widely used to teach models not to recognize the subject of an image, but its style, leading to the now infamous case of </span><a href="https://waxy.org/2022/11/invasive-diffusion-how-one-unwilling-illustrator-found-herself-turned-into-an-ai-model/" rel="">Hollie Mengert’s work getting ripped off</a><span>.</span></p><p><span>Once the LoRa was installed and set up, we had to feed it our first dataset, a.k.a 30 of my own precious illustrations. But the images alone are not enough for the model to learn to see their style, I had to caption each illustration describing </span><em>what</em><span> it was showing, leaving unwritten </span><em>how</em><span> it looked. Captioning a dataset is a process fraught with arbitrary choices that while seemingly “intuitive”, was for me a very complex task to wrap my mind around. The goal of captioning is to teach the model that everything described in the caption will be the replaceable features of a dataset, what will be flexibly generated by prompting it later. This means that everything else, that is not described in captions, must be retained as something to apply to all output. As one Youtuber explains it, captioning is therefore a powerful semiotic action:</span></p><blockquote><p><em>“[…] because the more details you input the more precise the model is going to be. […]If you have only 20 images it is going to be pretty fast but if you have more images that's really gonna take a while. I really wish that we had some sort of software that could do it automatically but as of right now the best tool is human eyes.”</em></p></blockquote><p>So much for an automation.</p><p>My favourite way to think about this problem of captioning is Gregory Bateson’s boot. Bateson was a famous anthropologist, known for his interdisciplinary work spanning biology, psychiatry and social sciences and his founding role in the field of cybernetics in the 1950s. He was interested in the patterns that govern our lives, and the relation between parts and whole. To teach this to his student, he would use this figure:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febafd4d9-cdd6-4a06-bea9-c116b2b4e80c_690x652.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febafd4d9-cdd6-4a06-bea9-c116b2b4e80c_690x652.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febafd4d9-cdd6-4a06-bea9-c116b2b4e80c_690x652.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febafd4d9-cdd6-4a06-bea9-c116b2b4e80c_690x652.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febafd4d9-cdd6-4a06-bea9-c116b2b4e80c_690x652.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febafd4d9-cdd6-4a06-bea9-c116b2b4e80c_690x652.png" width="322" height="304.26666666666665" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ebafd4d9-cdd6-4a06-bea9-c116b2b4e80c_690x652.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:652,&quot;width&quot;:690,&quot;resizeWidth&quot;:322,&quot;bytes&quot;:36357,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febafd4d9-cdd6-4a06-bea9-c116b2b4e80c_690x652.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febafd4d9-cdd6-4a06-bea9-c116b2b4e80c_690x652.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febafd4d9-cdd6-4a06-bea9-c116b2b4e80c_690x652.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febafd4d9-cdd6-4a06-bea9-c116b2b4e80c_690x652.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Bateson Boot, reproduced from Mind and Nature, 1979</figcaption></figure></div><p>He would ask the students to describe it, and then compare the results. I’m not going to describe all the answers as you can watch a short video focusing on the exercise here: </p><div id="youtube2-bM5N9RDfPco" data-attrs="{&quot;videoId&quot;:&quot;bM5N9RDfPco&quot;,&quot;startTime&quot;:&quot;389s&quot;,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/bM5N9RDfPco?start=389s&amp;rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>As different students describe the figure using different strategies (the way it looks, the way it can be broken into parts, etc.), Bateson tells us that “we can get a certain amount of agreement about what's really there but we cannot get an agreement about ways of describing it and we use in the description a whole mess of concepts of intervening variables and mentionables to get our stuff across.” In other words, when using words and concepts to “describe” something we see, we inevitably impose on this object the limitations that come with description.</p><p>A dataset is like a massive Bateson boot, full of complex images (expressed as numerical vectors) tied to simple words (expressed as numerical vectors). In this pairing between the sprawling complexity of our visual world and its neat description by words, much is lost. As I’m sitting at my laptop trying to come up for a caption for each of my illustration, I panic. My brain can’t seem to be able to precisely locate where meaning happens in my work, nor can it separate in neat layers the images I’ve made between style and content. The message is the medium. I try to think about how I work, my relationship to a brief, the process of sketching and the role of language in it, maybe there’s in the process of making an image the key to understand why it’s so hard to describe it.</p><p>I realize that, despite working with texts (briefs, articles, etc.), there’s always a moment when I let go of language. I usually read the text once, and then work with whatever afterimage I have left of its meaning. Part of that meaning gets transferred in the things I draw, maybe a corpse and a bookshelf because the text is about corpses in literature.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18bd5685-50ca-4388-a86d-1d4458e1d749_4018x1563.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18bd5685-50ca-4388-a86d-1d4458e1d749_4018x1563.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18bd5685-50ca-4388-a86d-1d4458e1d749_4018x1563.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18bd5685-50ca-4388-a86d-1d4458e1d749_4018x1563.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18bd5685-50ca-4388-a86d-1d4458e1d749_4018x1563.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18bd5685-50ca-4388-a86d-1d4458e1d749_4018x1563.png" width="1456" height="566" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/18bd5685-50ca-4388-a86d-1d4458e1d749_4018x1563.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:566,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:809437,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18bd5685-50ca-4388-a86d-1d4458e1d749_4018x1563.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18bd5685-50ca-4388-a86d-1d4458e1d749_4018x1563.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18bd5685-50ca-4388-a86d-1d4458e1d749_4018x1563.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18bd5685-50ca-4388-a86d-1d4458e1d749_4018x1563.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The long suffering cadaver - The New York Times Book Review</figcaption></figure></div><p><span>But also, the way I choose to depict this corpse is a cut out, which I only realize as I’m sketching, because the art director suggested a two-image layout which allows for a sense of time, a sequence. Suddenly the corpse is not represented </span><em>in</em><span> the image but cut out </span><em>from</em><span> it, and only becomes the subject of the second image. All of this happens silently, keeping language at bay, to allow visual meaning-making to happen on its own. We’ll return to language later, as I get feedback from the art director and we fine tune the final image, but that process of making images from text is in large part non-linguistic, and therefore hardly contained by the linearity of neither captioning nor prompting.</span></p><p>In taking the relationship between words and images as a simple 1:1 ratio, machine learning models sacrifice much of what illustration is really about, i.e. the space between text and image. This is not to say they do a bad job, they just do a very different one than illustrators do. They see images in a very different ways illustrators see them. The division between words and images is not the only one that is being flattened in most discussions of generative AI, another crucial one is that between images and people.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf0ee00f-2cb6-4002-b6da-bc13863b6b6f_500x357.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf0ee00f-2cb6-4002-b6da-bc13863b6b6f_500x357.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf0ee00f-2cb6-4002-b6da-bc13863b6b6f_500x357.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf0ee00f-2cb6-4002-b6da-bc13863b6b6f_500x357.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf0ee00f-2cb6-4002-b6da-bc13863b6b6f_500x357.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf0ee00f-2cb6-4002-b6da-bc13863b6b6f_500x357.png" width="48" height="34.272" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/af0ee00f-2cb6-4002-b6da-bc13863b6b6f_500x357.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:357,&quot;width&quot;:500,&quot;resizeWidth&quot;:48,&quot;bytes&quot;:6882,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf0ee00f-2cb6-4002-b6da-bc13863b6b6f_500x357.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf0ee00f-2cb6-4002-b6da-bc13863b6b6f_500x357.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf0ee00f-2cb6-4002-b6da-bc13863b6b6f_500x357.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf0ee00f-2cb6-4002-b6da-bc13863b6b6f_500x357.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Once she trained the model on my work, Sitong and I meet up on Zoom. She shares her screen and shows me a series of 10 images of a woman wearing a cap. Put together on a spectrum, the images shows how the original image (a photorealistic AI image of the woman) gradually turns into a Julien Posture version of itself.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8132c89-0b0f-4c69-9d3e-78a6c9a9c9cf_4000x678.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8132c89-0b0f-4c69-9d3e-78a6c9a9c9cf_4000x678.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8132c89-0b0f-4c69-9d3e-78a6c9a9c9cf_4000x678.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8132c89-0b0f-4c69-9d3e-78a6c9a9c9cf_4000x678.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8132c89-0b0f-4c69-9d3e-78a6c9a9c9cf_4000x678.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8132c89-0b0f-4c69-9d3e-78a6c9a9c9cf_4000x678.png" width="1456" height="247" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b8132c89-0b0f-4c69-9d3e-78a6c9a9c9cf_4000x678.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:247,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2670033,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8132c89-0b0f-4c69-9d3e-78a6c9a9c9cf_4000x678.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8132c89-0b0f-4c69-9d3e-78a6c9a9c9cf_4000x678.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8132c89-0b0f-4c69-9d3e-78a6c9a9c9cf_4000x678.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8132c89-0b0f-4c69-9d3e-78a6c9a9c9cf_4000x678.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Progression of the model learning my style</figcaption></figure></div><p><span>As I’m contemplating this Cronenberg-like transformation of the image, I can’t help to be struck by the triviality of my own work. There’s something confronting in facing a computational doppelgänger, something akin to the uncanny valley. I’m surprised at how much this affects me, even though my whole schtick is to be reflexive and critical about style, what surprises me the most is that even though the output if “objectively” a failure</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-144663529" href="https://julienposture.substack.com/p/the-ai-doppelganger-experiment-part#footnote-1-144663529" target="_self" rel="">1</a></span><span>, I see myself in it. But maybe what I see in the generation, what I find actually disturbing, is the part of my work that has already been objectified and commodified, the parts of my style I spent years making digestible for clients, consistent for social media, and reproducible for easy production.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7cdd15c-99c4-4ab6-9f60-e8e8b22867f6_512x768.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7cdd15c-99c4-4ab6-9f60-e8e8b22867f6_512x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7cdd15c-99c4-4ab6-9f60-e8e8b22867f6_512x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7cdd15c-99c4-4ab6-9f60-e8e8b22867f6_512x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7cdd15c-99c4-4ab6-9f60-e8e8b22867f6_512x768.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7cdd15c-99c4-4ab6-9f60-e8e8b22867f6_512x768.png" width="324" height="486" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d7cdd15c-99c4-4ab6-9f60-e8e8b22867f6_512x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:512,&quot;resizeWidth&quot;:324,&quot;bytes&quot;:488606,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7cdd15c-99c4-4ab6-9f60-e8e8b22867f6_512x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7cdd15c-99c4-4ab6-9f60-e8e8b22867f6_512x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7cdd15c-99c4-4ab6-9f60-e8e8b22867f6_512x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7cdd15c-99c4-4ab6-9f60-e8e8b22867f6_512x768.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Early result from the model</figcaption></figure></div><p>As I extend the invitations to various illustrators, some categorically refuse, seeing one’s life work so easily reproduced would be too challenging they tell me. I understand. I went in this experiment confident about the distance I have with my own style, only to be shaken by the uncanniness of existing in a computational form. Is this the way people see my work? A mere surface, a bundle of shapes and colours slapped onto any idea.</p><p><span>On April 30</span><sup>th</sup><span>, </span><a href="https://www.youtube.com/watch?v=-C0dU2OV5F4&amp;ab_channel=PBSNewsHour" rel="">British artist FKA Twigs testified in front of US senate</a><span> about her experience and thoughts about AI. Her testimony weaves in the same breath issues of personhood and economic livelihood, identity and intellectual property, essence and superficiality:</span></p><blockquote><p>“I am here because my music, my dancing, my acting, the way that my body moves in front of a camera and the way that my voice resonates through a microphone is not by chance; they are essential reflections of who I am. My art is the canvas on which I paint my identity and the sustaining foundation of my livelihood. It is the essence of my being.”</p></blockquote><p>As many artists navigating the AI waters, Twigs must negotiate a delicate balance. On the one hand she mobilizes deeply emotional registers showing the inherent entanglement of her identity with her work, the “essence of [her] being”, on the other hand, she must clarify she’s not a luddite, and that her argument is mostly a rational, economic one. In fact, later she reveals having trained her own model in her likeness “to extend my reach and handle my online social media interactions, whilst I continue to focus on my art from the comfort and solace of my studio.”</p><p><a href="https://julienposture.substack.com/p/phd-journal-1" rel="">As I’ve written before</a><span>, illustration is a practice that turns artists into sort of hybrids between people and images. Illustrators style is both a very personal thing and a valuable commodity, and as illustrators’ images circulate online, fragments of themselves do too. Returning to the court, this ambiguity is best captured by Adobe’s proposition of the FAIR act (Federal Anti-Impersonation Right) which is a blend of copyright and right of publicity, conceptualizing style mimicry as a form of impersonation (see what I mean by hybrid between people and image?).</span></p><p>In lawsuits between AI companies and artists, these differences become flattened in the supposedly neutral gaze of the law. Machine learning models are constantly personified, the processes in their black box likened to that of artistic inspiration, meanwhile, artistic processes are mechanized, made simple and linear. The AI doppelgänger experiment is a way to zoom in on these tensions and ask, with artists, what is the difference between a person-created image, and a machine-generated one? And can we qualify this difference in ways that are productive for technological, legal, and social conversations about AI and creativity?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b27f482-dec3-4ece-bddb-b8e88b6416d3_500x357.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b27f482-dec3-4ece-bddb-b8e88b6416d3_500x357.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b27f482-dec3-4ece-bddb-b8e88b6416d3_500x357.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b27f482-dec3-4ece-bddb-b8e88b6416d3_500x357.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b27f482-dec3-4ece-bddb-b8e88b6416d3_500x357.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b27f482-dec3-4ece-bddb-b8e88b6416d3_500x357.png" width="48" height="34.272" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b27f482-dec3-4ece-bddb-b8e88b6416d3_500x357.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:357,&quot;width&quot;:500,&quot;resizeWidth&quot;:48,&quot;bytes&quot;:6882,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b27f482-dec3-4ece-bddb-b8e88b6416d3_500x357.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b27f482-dec3-4ece-bddb-b8e88b6416d3_500x357.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b27f482-dec3-4ece-bddb-b8e88b6416d3_500x357.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b27f482-dec3-4ece-bddb-b8e88b6416d3_500x357.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>A notification chimes on my phone, it’s D., an illustrator who will be testing the model soon, “when are you finally gonna copy my art — Can’t wait to see it”. The model has now been trained enough times, it’s ready to be used with prompts. D. will be the first one to try it. I’m looking forward to it. I’m also a bit worried. Will this be a triggering experience for the illustrators I work with? Or will this be vindicating? What will be generated in the encounter between human and machine ways of seeing?</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Make timelapses easily using FFmpeg (243 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40407526</link>
            <guid>40407526</guid>
            <pubDate>Sun, 19 May 2024 15:24:12 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40407526">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="40408852"><td></td></tr>
            <tr id="40408345"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40408345" href="https://news.ycombinator.com/vote?id=40408345&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>Nice, thanks! I tried using ffmpeg for a minor video editing task I had a few months ago - just a cut, crop, rescale, and volume adjust. I've tried a few of the mainstream GUI video editing tools, and IMO, they all have incomprehensible UIs, are way too bloated, and usually far too expensive for what I'm trying to do. FFmpeg may not be dead simple, but I find it much easier to skim the command line flag list to figure out how to do what I want. And once I do, I can save down a handful of useful sets of flags and refer to them next time. Cheers to ffmpeg, one of the kings of FOSS! If you ever feel the need to do any kind of video conversion or editing, definitely try to do it in ffmpeg first.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40408757"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40408757" href="https://news.ycombinator.com/vote?id=40408757&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>Indeed. I find it baffling how hard it is to just make lightweight edits to videos on Windows. At a bare minimum I would like to clip a video, or crop or change audio tracks. My cheat sheet of ffmpeg commands still remains to be the easiest way for me to do this.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40409236"><td></td></tr>
            <tr id="40408744"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40408744" href="https://news.ycombinator.com/vote?id=40408744&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><p><span>To be fair, pulling out a professional video editor for small changes is like learning emacs to edit some config files. You don't need 99% of the features.<p>Also as an FYI to everyone, FFmpeg does support nVidia GPU acceleration but it might not be enabled in your build. So check if you use it a lot.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40409152"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40409152" href="https://news.ycombinator.com/vote?id=40409152&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>Probably true, but ffmpeg seems to have a ton of features too. It seems to me that CLI apps are inherently better at not distracting you with things you don't need. A CLI flag that you don't use is invisible outside of the man pages, not so for a menu or toolbar of a zillion options with names and icons you don't understand.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40408620"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40408620" href="https://news.ycombinator.com/vote?id=40408620&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>Davinci Resolve has a free (as in beer) version that is quite capable and easy to use, even as someone who'd only used iMovie before. The only problem is that "how to do X in Davinci Resolve" has been taken over by slop.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40409138"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40409138" href="https://news.ycombinator.com/vote?id=40409138&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>Davinci Resolve is actually the first thing that came to mind on the subject of, okay it's free, that's nice, but I can't for the life of me figure out how to do anything in it. I suppose it's not necessarily their fault that the search results for how to do basic things are garbage, but I guess an advantage of CLI apps is how-to results for them don't seem to attract nearly as much SEOified clickbait.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40409019"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409019" href="https://news.ycombinator.com/vote?id=40409019&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>back when computers were hard, tips like this were gold. but these days,
for for a well trod/documented thing like ffmpeg,
asking ChatGPT to make the ffmpeg command you want works really well, eg "give me ffmpeg to make a video from a series of jpegs" and iterate from there.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40409249"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40409249" href="https://news.ycombinator.com/vote?id=40409249&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>Please don't let this line of thinking put you (the reader) off sharing tips. Here we now have a thread containing other information we may not have thought to ask anyone/thing about, discussion, history, etc.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40408878"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40408878" href="https://news.ycombinator.com/vote?id=40408878&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><p><span>Where FFMPEG really shines is stabilising video.<p>Unfortunately not all versions have "vidstab".</p><p>ffmpeg -i "$1" -vf vidstabdetect=shakiness=5:show=1  dummy.avi</p><p>ffmpeg -i "$1" -vf yadif, format=yuv420p, vidstabtransform=zoom=2:optzoom=0:crop=black -c:v libx264 -b:a 32k  stabilized264.mp4</p><p>Yesterweek's shaky video shot from a kayak: <a href="https://youtu.be/4pM0VeH4NE0?si=H2qTJfcvis3QmFlj" rel="nofollow">https://youtu.be/4pM0VeH4NE0?si=H2qTJfcvis3QmFlj</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40409023"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40409023" href="https://news.ycombinator.com/vote?id=40409023&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><p><span>If you really wish to install all the available options, you can run:<p>brew install homebrew-ffmpeg/ffmpeg/ffmpeg $(brew options homebrew-ffmpeg/ffmpeg/ffmpeg --compact)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="40409221"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409221" href="https://news.ycombinator.com/vote?id=40409221&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><p><span>Is there a variant that encodes ProRes lossless?<p>I usually open them up in a new project just to create a lossless input video to work with in After Effects, and use that (if I use image sequence directly, DaVinci Resolve acts in weird ways).</p><p>ffmpeg might ease that AE part.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40409183"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40409183" href="https://news.ycombinator.com/vote?id=40409183&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>Maybe someone should collect all those commands and create a website or a gist that list them with search possible. They are gems !</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40407826"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40407826" href="https://news.ycombinator.com/vote?id=40407826&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><p><span>ffmpeg is such a great tool!<p>Be aware that <i>-pattern_type glob</i> is not supported on Windows, though, iirc. A workaround is to name your jpegs with consecutive numbers (not necessarily starting at 0) and use a pattern with a counter placeholder in it instead.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40408208"><td></td></tr>
                <tr id="40408566"><td></td></tr>
                  <tr id="40408025"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40408025" href="https://news.ycombinator.com/vote?id=40408025&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><p><span>Obligatory humor: <a href="https://youtu.be/9kaIXkImCAM" rel="nofollow">https://youtu.be/9kaIXkImCAM</a><p>Note of support: ffmpeg supported many of the transcoding needs of my former employer back in 2007, being a "friendly" tool to the team. Yes it had/s issue. Being open source gave us a lifeline, to be able to fix our own stuff, and build up our video and audio live streaming and video watching white label service.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40408317"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40408317" href="https://news.ycombinator.com/vote?id=40408317&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>wsl --cd=%cd% ffmpeg -framerate 30 -pattern_type glob -i '*.JPG' -c:v libx264 -r 30 -pix_fmt yuv420p timelapse.mp4</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40407905"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40407905" href="https://news.ycombinator.com/vote?id=40407905&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>Hmm, I wonder why `-pattern_type glob` doesn't work on Windows. Perhaps it is something that could easily be programmed into the source code?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40408151"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40408151" href="https://news.ycombinator.com/vote?id=40408151&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>If I were yo guess, it might be using the GNU libc (or compatible) glob functionality under the hood.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40408421"><td></td></tr>
                  <tr id="40408813"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40408813" href="https://news.ycombinator.com/vote?id=40408813&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>Does this add any interframe blur or are you controlling that based on exposure time ? Very important for quality Timelapse's</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40408023"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40408023" href="https://news.ycombinator.com/vote?id=40408023&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><p><span>I recently wrote a blog post about doing this to create timelapses of Rimworld colonies. I didn’t realize -pattern_type glob didn’t work on windows though… I’ll have to update it.<p>Also, an assumption in your command is that all the images are the same aspect ratio. If they’re not, you can use this to dynamically pad it out with black bars on either size:</p><p>‘-vf "scale=1920:1080:force_original_aspect_ratio=decrease:eval=frame,pad=1920:1080:-1:-1:eval=frame"’</p><p><a href="https://mpeyton.com/posts/rimworld_timelapse_ffmpeg/" rel="nofollow">https://mpeyton.com/posts/rimworld_timelapse_ffmpeg/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40408529"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40408529" href="https://news.ycombinator.com/vote?id=40408529&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>I'd recommend Da Vinci Resolve for making timelapses. It performs really well and let's you scrub through before rendering anything which lets you clip just the part that you need. Plus you get the benefit of high export quality which can be fiddly with ffmpeg.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40409303"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40409303" href="https://news.ycombinator.com/vote?id=40409303&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>Resolve is insanely heavyweight for such a simple task. Those video editor UIs are incredibly hard to understand for people not using them every day.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40408723"><td></td></tr>
                <tr id="40409033"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40409033" href="https://news.ycombinator.com/vote?id=40409033&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>there are a bunch of flags to get exactly right in order to get it to give you a high quality image out. there are wrappers to do this more easily for you, ffmpeg is a low level tool.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40408972"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40408972" href="https://news.ycombinator.com/vote?id=40408972&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>I looked into using ffmpeg to “compress” video podcasts by lowering the framerate a lot, but it didn’t seem to do as much as I thought (about 50% size reduction). The theory was that a video podcast is mostly talking heads with an occasional chart on the screen, so you really only need a frame every second, or five seconds.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40409006"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40409006" href="https://news.ycombinator.com/vote?id=40409006&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>AV1 exceeds at these type of videos. It's why so many anime people use it.<p>Try encoding the video to AV1 with OPUS audio. You'll get ridiculous gainz!</p><p>My command is:</p><pre><code>    $ffmpegPath -i $_.FullName -r 23.976 -vf scale=1280:720 -c:v libsvtav1 -pix_fmt yuv420p10le -crf 30 -preset 10 -g 300 -c:a libopus -b:a 96k -ac 2 -c:s copy -map 0 $destPath</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40409059"><td></td></tr>
                        <tr id="40408801"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40408801" href="https://news.ycombinator.com/vote?id=40408801&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><p><span>I wanted to print out one of those flipbooks I had as a kid, where the frames are printed and as the pages are flipped it looks like a movie.<p>Is that something ffmpeg could do?</p><p>Is there any good resource for recipes like these?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40408995"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40408995" href="https://news.ycombinator.com/vote?id=40408995&amp;how=up&amp;goto=item%3Fid%3D40407526"></a></center>    </td><td><br><div>
                  <p><span>try<pre><code>    ffmpeg -ss 00:01:00 -i input.avi -t 30 -vf "fps=1,scale=320:-1:flags=lanczos" output_%04d.jpg
</code></pre>
00:01:00 is where to start the flip book, 30 is thirty seconds worth, and 1 fps is how many frames per second. this'll make output_XX.jpg from the Avi which you can then print</span></p></div></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Swarming Proxima Centauri: Picospacecraft Swarms over Interstellar Distances (200 pts)]]></title>
            <link>https://astrobiology.com/2024/05/swarming-proxima-centauri-coherent-picospacecraft-swarms-over-interstellar-distances.html</link>
            <guid>40407228</guid>
            <pubDate>Sun, 19 May 2024 14:33:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://astrobiology.com/2024/05/swarming-proxima-centauri-coherent-picospacecraft-swarms-over-interstellar-distances.html">https://astrobiology.com/2024/05/swarming-proxima-centauri-coherent-picospacecraft-swarms-over-interstellar-distances.html</a>, See on <a href="https://news.ycombinator.com/item?id=40407228">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                            <figure>
                                    <img src="https://astrobiology.com/wp-content/uploads/2024/05/Swarming-Proxima-Centauri.png" alt="Swarming Proxima Centauri: Coherent Picospacecraft Swarms Over Interstellar Distances">
                                                                            <figcaption>
                                            <div>
                                                <p>
                                                                                                            Graphic depiction of Swarming Proxima Centauri: Coherent Picospacecraft Swarms Over Interstellar Distances
Thomas Eubanks                                                                                                    </p>
                                                                                            </div>
                                        </figcaption>
                                                                    </figure>
                                                        
<p>Tiny gram-scale interstellar probes pushed by laser light are likely to be the only technology capable of reaching another star this century. We presuppose availability by mid-century of a laser beamer powerful enough (~100-GW) to boost a few grams to relativistic speed, lasersails robust enough to survive launch, and terrestrial light buckets (~1-sq.km) big enough to catch our optical signals. Then our proposed representative mission, around the third quarter of this century, is to fly by our nearest neighbor, the potentially habitable world Proxima b, with a large autonomous swarm of 1000s of tiny probes.</p>



<p>Given extreme constraints on launch mass (grams), onboard power (milliwatts), and coms aperture (centimeters to meters), our team determined in our work over the last 3 years that only a large swarm of many probes acting in unison can generate an optical signal strong enough to cross the immense distance back to Earth. The 8-year round-trip time lag eliminates any practical control by Earth, therefore the swarm must possess an extraordinary degree of autonomy, for example, in order to prioritize which data is returned to Earth. Thus, the reader will see that coordinating the swarming of individuals into an effective whole is the dominant challenge for our representative mission to Proxima Centauri b. Coordination in turn rests on establishing a mesh network via low-power optical links and synchronizing probes’ on-board clocks with Earth and with each other to support accurate position-navigation-timing (PNT).</p>



<p>Our representative mission begins with a long string of probes launched one at a time to ~0.2c. After launch, the drive laser is used for signaling and clock synchronization, providing a continual time signal like a metronome. Initial boost is modulated so the tail of the string catches up with the head (“time on target”). Exploiting drag imparted by the interstellar medium (“velocity on target”) over the 20-year cruise keeps the group together once assembled. An initial string 100s to 1000s of AU long dynamically coalesces itself over time into a lens-shaped mesh network #100,000 km across, sufficient to account for ephemeris errors at Proxima, ensuring at least some probes pass close to the target.</p>



<p>A swarm whose members are in known spatial positions relative to each other, having state-of-the-art microminiaturized clocks to keep synchrony, can utilize its entire population to communicate with Earth, periodically building up a single short but extremely bright contemporaneous laser pulse from all of them. Operational coherence means each probe sends the same data but adjusts its emission time according to its relative position, such that all pulses arrive simultaneously at the receiving arrays on Earth. This effectively multiplies the power from any one probe by the number N of probes in the swarm, providing orders of magnitude greater data return.</p>



<p>A swarm would tolerate significant attrition en route, mitigating the risk of “putting all your eggs in one basket,” and enabling close observation of Proxima b from multiple vantage points. Fortunately, we don’t have to wait until mid-century to make practical progress – we can explore and test swarming techniques now in a simulated environment, which is what we propose to do in this work. We anticipate our innovations would have a profound effect on space exploration, complementing existing techniques and enabling entirely new types of missions, for example picospacecraft swarms covering all of cislunar space, or instrumenting an entire planetary magnetosphere. Well before mid-century we foresee a number of such missions, starting in Earth or lunar orbit, but in time extending deep into the outer Solar system. For example, such a swarm could explore the rapidly receding interstellar object 1I/’Oumuamua or the solar gravitational lens. These would both be precursors to the ultimate interstellar mission, but also scientifically valuable in their own right.</p>



<p>— Thomas Eubanks Space Initiatives, Inc.:</p>



<p><em><strong><a href="https://www.nasa.gov/general/niac-2024-selections/" target="_blank" rel="noreferrer noopener">2024 NIAC Phase I Selection</a>, NASA</strong></em></p>



<p>Astrobiology, Interstellar,</p>
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Compilers for free with weval (187 pts)]]></title>
            <link>https://bernsteinbear.com/blog/weval/</link>
            <guid>40406194</guid>
            <pubDate>Sun, 19 May 2024 11:33:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bernsteinbear.com/blog/weval/">https://bernsteinbear.com/blog/weval/</a>, See on <a href="https://news.ycombinator.com/item?id=40406194">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><a href="https://cfallin.org/">Chris Fallin</a> came and gave a talk to the Northeastern
<a href="https://prl.khoury.northeastern.edu/">Programming Research Laboratory</a>
last month. He talked about his work on a new project called
<a href="https://github.com/cfallin/weval">weval</a>, a WebAssembly partial evaluator (and
then helped me write this post!).</p>

<p>Partial evaluation is neat. In short, it’s all about taking an existing
program, modifying it to hold some of its inputs as constants, and then letting
the compiler/optimizer go hog wild on it. The result is still a program—not a
value—and it’s usually faster than the original program.</p>

<p>The usual small example is the power function. If you have a function that takes
two arguments, <code>x</code> and <code>y</code>, and returns <code>x^y</code>:</p>

<div><pre><code><span>int</span> <span>power</span><span>(</span><span>int</span> <span>x</span><span>,</span> <span>int</span> <span>y</span><span>)</span> <span>{</span>
  <span>int</span> <span>result</span> <span>=</span> <span>1</span><span>;</span>
  <span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>y</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>result</span> <span>*=</span> <span>x</span><span>;</span>
  <span>}</span>
  <span>return</span> <span>result</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>If you partially evaluate this function with respect to <code>y</code> at <code>y = 5</code>, you get
a new function that takes one argument, <code>x</code>, and returns <code>x^5</code>:</p>

<div><pre><code><span>int</span> <span>power_5</span><span>(</span><span>int</span> <span>x</span><span>)</span> <span>{</span>
  <span>int</span> <span>result</span> <span>=</span> <span>1</span><span>;</span>
  <span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>5</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>result</span> <span>*=</span> <span>x</span><span>;</span>
  <span>}</span>
  <span>return</span> <span>result</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Now, to you, this might not look that different from the original function. But
to an optimizer, it is a new world of opportunity. The optimizer can unroll the
loop and remove the conditional:</p>

<div><pre><code><span>int</span> <span>power_5</span><span>(</span><span>int</span> <span>x</span><span>)</span> <span>{</span>
  <span>return</span> <span>x</span> <span>*</span> <span>x</span> <span>*</span> <span>x</span> <span>*</span> <span>x</span> <span>*</span> <span>x</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>weval does that for entire WebAssembly modules. WebAssembly modules that are
normally much bigger than a small <code>power</code> function. You might want to use it
if, for example, your WebAssembly module is an interpreter. Imagine a world
where you have compiled a runtime such as SpiderMonkey or CPython to
WebAssembly. You could then run your Python or JavaScript programs on the
WebAssembly runtime, but they would be slower than if you had compiled them
directly to WebAssembly. And even if you compiled the JS/Python directly to
Wasm, it would probably be slow unless your compiler did some fancy static
analysis. This is where weval comes in.</p>

<h2 id="enter-weval">Enter weval</h2>

<p>SpiderMonkey and CPython are both huge. Instead, we’re going to do a little
demo of a tiny interpreter that I wrote with Chris. Our interpreter doesn’t do
much—local variables, an accumulator, arithmetic, and branching. But it’s
enough to show off the performance boosts that come with weval.</p>

<div><pre><code><span>#define FOR_EACH_INSTRUCTION(V)                                                \
  V(LOAD_IMMEDIATE)                                                            \
  V(STORE_LOCAL)                                                               \
  V(LOAD_LOCAL)                                                                \
  V(PRINT)                                                                     \
  V(PRINTI)                                                                    \
  V(JMPNZ)                                                                     \
  V(INC)                                                                       \
  V(DEC)                                                                       \
  V(ADD)                                                                       \
  V(HALT)
</span></code></pre></div>

<p>It’s designed as a little loop that reads the next instructions and dispatches
with a <code>switch</code>. It’s not the fastest design<sup id="fnref:computed-goto" role="doc-noteref"><a href="#fn:computed-goto" rel="footnote">1</a></sup>, but that’s okay.</p>

<div><pre><code><span>uword</span> <span>Execute</span><span>(</span><span>uword</span> <span>*</span><span>program</span><span>)</span> <span>{</span>
  <span>// ...</span>
  <span>while</span> <span>(</span><span>true</span><span>)</span> <span>{</span>
    <span>Instruction</span> <span>op</span> <span>=</span> <span>(</span><span>Instruction</span><span>)</span><span>program</span><span>[</span><span>pc</span><span>++</span><span>];</span>
    <span>switch</span> <span>(</span><span>op</span><span>)</span> <span>{</span>
    <span>case</span> <span>LOAD_IMMEDIATE</span><span>:</span> <span>{</span>
      <span>uword</span> <span>value</span> <span>=</span> <span>program</span><span>[</span><span>pc</span><span>++</span><span>];</span>
      <span>accumulator</span> <span>=</span> <span>(</span><span>Object</span><span>)</span><span>value</span><span>;</span>
      <span>break</span><span>;</span>
    <span>}</span>
    <span>case</span> <span>STORE_LOCAL</span><span>:</span> <span>{</span>
      <span>uword</span> <span>idx</span> <span>=</span> <span>program</span><span>[</span><span>pc</span><span>++</span><span>];</span>
      <span>LOCAL_AT_PUT</span><span>(</span><span>idx</span><span>,</span> <span>accumulator</span><span>);</span>
      <span>break</span><span>;</span>
    <span>}</span>
    <span>case</span> <span>LOAD_LOCAL</span><span>:</span> <span>{</span>
      <span>uword</span> <span>idx</span> <span>=</span> <span>program</span><span>[</span><span>pc</span><span>++</span><span>];</span>
      <span>accumulator</span> <span>=</span> <span>LOCAL_AT</span><span>(</span><span>idx</span><span>);</span>
      <span>break</span><span>;</span>
    <span>}</span>
    <span>case</span> <span>PRINT</span><span>:</span> <span>{</span>
      <span>const</span> <span>char</span> <span>*</span><span>msg</span> <span>=</span> <span>(</span><span>const</span> <span>char</span> <span>*</span><span>)</span><span>program</span><span>[</span><span>pc</span><span>++</span><span>];</span>
      <span>printf</span><span>(</span><span>"%s"</span><span>,</span> <span>msg</span><span>);</span>
      <span>break</span><span>;</span>
    <span>}</span>
    <span>case</span> <span>PRINTI</span><span>:</span> <span>{</span>
      <span>printf</span><span>(</span><span>"%"</span> <span>PRIu64</span><span>,</span> <span>accumulator</span><span>);</span>
      <span>break</span><span>;</span>
    <span>}</span>
    <span>case</span> <span>HALT</span><span>:</span> <span>{</span>
      <span>return</span> <span>accumulator</span><span>;</span>
    <span>}</span>
    <span>case</span> <span>JMPNZ</span><span>:</span> <span>{</span>
      <span>uword</span> <span>offset</span> <span>=</span> <span>program</span><span>[</span><span>pc</span><span>++</span><span>];</span>
      <span>if</span> <span>(</span><span>accumulator</span> <span>!=</span> <span>0</span><span>)</span> <span>{</span>
        <span>pc</span> <span>=</span> <span>offset</span><span>;</span>
      <span>}</span>
      <span>break</span><span>;</span>
    <span>}</span>
    <span>case</span> <span>INC</span><span>:</span> <span>{</span>
      <span>accumulator</span><span>++</span><span>;</span>
      <span>break</span><span>;</span>
    <span>}</span>
    <span>case</span> <span>DEC</span><span>:</span> <span>{</span>
      <span>accumulator</span><span>--</span><span>;</span>
      <span>break</span><span>;</span>
    <span>}</span>
    <span>case</span> <span>ADD</span><span>:</span> <span>{</span>
      <span>uword</span> <span>idx1</span> <span>=</span> <span>program</span><span>[</span><span>pc</span><span>++</span><span>];</span>
      <span>uword</span> <span>idx2</span> <span>=</span> <span>program</span><span>[</span><span>pc</span><span>++</span><span>];</span>
      <span>accumulator</span> <span>=</span> <span>LOCAL_AT</span><span>(</span><span>idx1</span><span>)</span> <span>+</span> <span>LOCAL_AT</span><span>(</span><span>idx2</span><span>);</span>
      <span>break</span><span>;</span>
    <span>}</span>
    <span>// ...</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<p>Using this bytecode, we can write a simple program that adds up all the numbers
from 1 to 100 million:</p>

<div><pre><code><span>enum</span> <span>{</span>
  <span>result</span> <span>=</span> <span>0</span><span>,</span>
  <span>loopc</span> <span>=</span> <span>1</span><span>,</span>
<span>};</span>
<span>uword</span> <span>program</span><span>[]</span> <span>=</span> <span>{</span>
  <span>// result = 0</span>
  <span>LOAD_IMMEDIATE</span><span>,</span> <span>0</span><span>,</span>
  <span>STORE_LOCAL</span><span>,</span> <span>result</span><span>,</span>
  <span>// loopc = 100_000_000</span>
  <span>LOAD_IMMEDIATE</span><span>,</span> <span>100000000</span><span>,</span>
  <span>STORE_LOCAL</span><span>,</span> <span>loopc</span><span>,</span>

  <span>// loop:</span>
  <span>// result += loopc</span>
  <span>ADD</span><span>,</span> <span>result</span><span>,</span> <span>loopc</span><span>,</span>
  <span>STORE_LOCAL</span><span>,</span> <span>result</span><span>,</span>
  <span>// loopc--</span>
  <span>LOAD_LOCAL</span><span>,</span> <span>loopc</span><span>,</span>
  <span>DEC</span><span>,</span>
  <span>STORE_LOCAL</span><span>,</span> <span>loopc</span><span>,</span>
  <span>// if loopc != 0, jump to loop</span>
  <span>JMPNZ</span><span>,</span> <span>8</span><span>,</span>

  <span>// print result</span>
  <span>PRINT</span><span>,</span> <span>(</span><span>uword</span><span>)</span><span>"Result: "</span><span>,</span>
  <span>LOAD_LOCAL</span><span>,</span> <span>result</span><span>,</span>
  <span>PRINTI</span><span>,</span>
  <span>PRINT</span><span>,</span> <span>(</span><span>uword</span><span>)</span><span>"</span><span>\n</span><span>"</span><span>,</span>
  <span>HALT</span><span>,</span>
<span>};</span>
</code></pre></div>

<p>We can compile this interpreter program with any C or C++ compiler, feed the
interpreter the bytecode, and it will print the result after about 350ms:</p>

<div><pre><code><span>$</span><span> </span>c++ <span>-O2</span> peval.cc <span>-o</span> peval.out
<span>$</span><span> </span>./peval.out
<span>Result: 5000000050000000
</span><span>$</span><span>
</span></code></pre></div>

<p>But let’s assume you want to sandbox this program with WebAssembly. Thankfully,
there’s this project called <a href="https://github.com/webAssembly/wasi-sdk">wasi-sdk</a>
that provides near drop-in replacements for Clang that target WebAssembly. We
can compile the interpreter with wasi-sdk and run it with <code>wasmtime</code> or any
other WebAssembly runtime that provides a WASI polyfill<sup id="fnref:polyfill" role="doc-noteref"><a href="#fn:polyfill" rel="footnote">2</a></sup>. This runs
in about 530ms:</p>

<div><pre><code><span>$</span><span> </span>/opt/wasi-sdk/bin/clang++ <span>-O2</span> peval.cc <span>-o</span> peval.normal.wasm
<span>$</span><span> </span>wasmtime peval.normal.wasm
<span>Result: 5000000050000000
</span><span>$</span><span>
</span></code></pre></div>

<p>But really what we wanted all along was to deploy the program—not the
interpreter too, <em>just</em> the program—in the sandbox. We can do that by
smushing the bytecode and the interpreter together with weval. This runs in
about 40ms:</p>

<div><pre><code><span>$</span><span> </span>/opt/wasi-sdk/bin/clang++ <span>-O2</span> <span>-DDO_WEVAL</span> <span>-I</span> include peval.cc <span>-o</span> peval.wasm
<span>$</span><span> </span>weval weval <span>-i</span> peval.wasm <span>-o</span> peval.wevaled.wasm <span>-w</span>
<span>$</span><span> </span>wasmtime peval.wevaled.wasm
<span>Result: 5000000050000000
</span><span>$</span><span>
</span></code></pre></div>

<p>First of all: let’s step back. We had an interpreter written in C++ that took
350ms to run. We made it a little slower (530ms) by compiling it to
WebAssembly. Then we got a <strong>8.5x speedup</strong> by using weval. That’s nuts. That’s
probably close to what we would get if we hand-wrote a little compiler for our
bytecode machine, but I did not have to write a compiler.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Time (ms)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>C++</td>
      <td>350</td>
    </tr>
    <tr>
      <td>WASI</td>
      <td>530</td>
    </tr>
    <tr>
      <td>WASI+weval</td>
      <td><strong>40</strong>  (!!)</td>
    </tr>
  </tbody>
</table>

<p>Check out an <a href="https://asciinema.org/a/EesYdO0GFxkTKyAJu2vuVXjTS">asciicast</a> if
you want to feel that difference.</p>

<p>You might notice that I added some sneaky flags like <code>-DDO_WEVAL</code> and <code>-I
include</code> in there. What’s going on?</p>

<h2 id="specializing-the-interpreter">Specializing the interpreter</h2>

<p><strong>Big picture:</strong> give the interpreter function access to constant bytecode.</p>

<p>Well, while weval works <em>correctly</em> on any combination of WebAssembly module
and its input, it works <em>best</em> when you give it a little help and tell it what
data is constant. In order to do that, we pre-initialize the WebAssembly module
using a project called <a href="https://github.com/bytecodealliance/wizer">wizer</a>. It
gives you, the programmer, hooks to set up some memory before turning the
running state back into a WebAssembly module. Let’s look at a diagram of the
situation as it is right now:</p>

<figure>

<figcaption>This is too many levels of nesting</figcaption>
</figure>

<p>Right now, at run-time, the interpreter loads the bytecode and runs it. The
bytecode is not known ahead of time, so the interpreter has to be general.</p>

<p>In order to specialize the interpreter, we do three steps <em>at WebAssembly
module initialization time</em>:</p>

<ol>
  <li>Load the bytecode</li>
  <li>Create a specialized version of the interpreter function with constant
arguments</li>
  <li>Run constant propagation and other compiler passes on the function</li>
</ol>

<p>In this example, we know the bytecode is constant. We can tell weval this by
using one of its helper intrinsics. In this case, we create a copy of the
<code>Execute</code> function with constant arguments (the <code>program</code>). Now we have two
functions: <code>Execute</code> and <code>ExecuteSpecialized</code>. All of this happens in the
<code>init</code> function:</p>

<div><pre><code><span>template</span> <span>&lt;</span><span>bool</span> <span>IsSpecialized</span><span>&gt;</span>
<span>uword</span> <span>Execute</span><span>(</span><span>uword</span> <span>*</span><span>program</span><span>)</span> <span>{</span>
    <span>// ...</span>
<span>}</span>

<span>#ifdef DO_WEVAL
</span><span>Object</span> <span>(</span><span>*</span><span>ExecuteSpecialized</span><span>)(</span><span>uword</span> <span>*</span><span>)</span> <span>=</span> <span>0</span><span>;</span>

<span>void</span> <span>init</span><span>()</span> <span>{</span>
  <span>uword</span> <span>result</span> <span>=</span> <span>0</span><span>;</span>
  <span>uword</span> <span>loopc</span> <span>=</span> <span>1</span><span>;</span>
  <span>weval</span><span>::</span><span>weval</span><span>(</span><span>&amp;</span><span>ExecuteSpecialized</span><span>,</span> <span>&amp;</span><span>Execute</span><span>&lt;</span><span>true</span><span>&gt;</span><span>,</span> <span>/*func_id=*/</span><span>123</span><span>,</span>
               <span>weval</span><span>::</span><span>SpecializeMemory</span><span>&lt;</span><span>uword</span> <span>*&gt;</span><span>(</span><span>program</span><span>,</span> <span>sizeof</span> <span>program</span><span>));</span>
<span>}</span>

<span>WIZER_INIT</span><span>(</span><span>init</span><span>);</span>
<span>WEVAL_DEFINE_GLOBALS</span><span>();</span>
<span>#endif
</span>
<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span> <span>**</span><span>argv</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>ExecuteSpecialized</span><span>)</span> <span>{</span>
    <span>ExecuteSpecialized</span><span>(</span><span>nullptr</span><span>);</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>Execute</span><span>&lt;</span><span>false</span><span>&gt;</span><span>(</span><span>program</span><span>);</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<p>Now that the code has been loaded and marked constant, the picture looks more
like this:</p>

<figure>

</figure>

<p>While the code is constant, weval isn’t magic. It won’t modify control flow, by
default, except simplifying branches that become constant (it doesn’t even know
what an interpreter is!).</p>

<p>In order to start making <code>ExecuteSpecialized</code> faster, we have to drop little
hints into the interpreter for weval to pick up. We want to actually specialize
the control flow—make the control flow in the bytecode become control flow in
the new function itself—so we tell weval about the PC to let it expand out
the code.</p>

<h2 id="modifying-the-interpreter">Modifying the interpreter</h2>

<p><strong>Big picture:</strong> unroll the loop by specializing on the program counter.</p>

<p>We can start off by telling weval what variable to use as a <em>specialization
context</em>. In this case, since we know that the bytecode is constant, we can
specialize on the <code>pc</code>—the program counter. This lets weval completely unroll
the interpreter loop.</p>

<div><pre><code><span>uword</span> <span>Execute</span><span>(</span><span>uword</span> <span>*</span><span>program</span><span>)</span> <span>{</span>
  <span>while</span> <span>(</span><span>true</span><span>)</span> <span>{</span>
    <span>// ...</span>
    <span>switch</span> <span>(</span><span>op</span><span>)</span> <span>{</span>
      <span>// ...</span>
    <span>}</span>
    <span>weval</span><span>::</span><span>update_context</span><span>(</span><span>pc</span><span>);</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<p>After running weval on the bundled module, and letting weval unroll the loop,
the picture looks like this:</p>

<figure>

</figure>

<p>This means that from this point forward, we have used weval to turn our
interpreter into a compiler. There are only two optimizations so
far—unrolling the loop and constant propagation—but they are very
effective. The result is a fully WebAssembly module with no interpreter and no
bytecode.</p>

<p>Weval’s compiler passes are not magic. They are the same passes that any
compiler would run on your code. They can unroll the interpreter loop and turn
bytecode into straight-line WebAssembly code. But that code still has local
variable writes push and local variable reads and all the other overhead of the
interpreter. So there’s more to be done…</p>

<h2 id="but-what-if-we-modified-it-more">But what if we modified it more?</h2>

<p><strong>Big picture:</strong> unroll interpreter local variables into WebAssembly local
variables by telling weval where they are.</p>

<p>Memory can be hard to reason about in a compiler. Weval isn’t a whole program
optimizing compiler and might not be able to prove that a memory location (in
this case, the locals array) never escapes or aliases something else. But we,
the interpreter authors, know that. So we can add more hints.</p>

<p>Right now, <code>LOCAL_AT</code> and <code>LOCAL_AT_PUT</code> are macros that read and write to the
locals array:</p>

<div><pre><code><span>#define LOCAL_AT(idx) (locals[idx])
#define LOCAL_AT_PUT(idx, val) (locals[idx] = val)
</span></code></pre></div>

<p>That’s all well and good for the interpreter, but it’s not great for the
compiled code. What we really want is to give weval the ability to reason about
each memory location—each local index—separately as an SSA value.</p>

<p>In order to do that, we use weval intrinsics: <code>weval_read_reg</code> and
<code>weval_write_reg</code>. For maximum flexibility, we have a couple of macros that
switch between the two:</p>

<div><pre><code><span>#ifdef DO_WEVAL
#define LOCAL_AT(idx) (IsSpecialized ? weval_read_reg(idx) : locals[idx])
#define LOCAL_AT_PUT(idx, val)                                                 \
  if (IsSpecialized) {                                                         \
    weval_write_reg(idx, val);                                                 \
  } else {                                                                     \
    locals[idx] = val;                                                         \
  }
#else
#define LOCAL_AT(idx) (locals[idx])
#define LOCAL_AT_PUT(idx, val) (locals[idx] = val)
#endif
</span></code></pre></div>

<p>Now, weval can reason about each local variable separately and they get
eventually compiled to normal WebAssembly locals.</p>

<h2 id="wrapping-up">Wrapping up</h2>

<p>The big idea here is that it’s possible to incrementally unravel an interpreter
into a compiler by specializing on constant data and then doing normal compiler
passes. The more you can specialize at build-time, the faster the resulting
generated code will be.</p>

<p>Check out the code in my <a href="https://github.com/tekknolagi/weval">already old fork of
weval</a>. It includes surprise benchmarks of
Wasm JITs in different JS runtimes, too!</p>

<h2 id="looking-forward">Looking forward</h2>

<p>Chris gave some more detail about how weval works in <a href="https://bernsteinbear.com/assets/img/weval_slides.pdf">this
talk</a> (PDF), including a description of how the
interpreter function is actually combined with the bytecode. The main idea is
to use the PC values as a “context” in a context-sensitive dataflow analysis,
so regular constant propagation will see the PC value and opcode for just one
interpreter loop iteration, rather than the union of all of them (as a static
analysis normally would). There are a bunch of fiddly details to make it work
well, and Chris also plans to write a blog post covering weval and its
application to SpiderMonkey soon.</p>

<p>Also, our interpreter is tiny and not very interesting on its own. It’s only
useful to explain some weval concepts to you. But the same principles apply to
much larger interpreters, too! There’s SpiderMonkey, yes, and the same could
also probably be done for CPython, the main Python runtime. CPython even has
support for being compiled to WebAssembly already!</p>

<p>Imagine compiling Python directly down to WebAssembly… maybe coming soon?</p>

<h2 id="wilder-ideas">Wilder ideas</h2>

<p>CPython already has support for vectorcall function pointers. This is a way to
add a JIT compiler in a portable way. We could also maybe use this to turn
weval into a Wasm JIT for CPython.</p>

<h2 id="similar-projects">Similar projects</h2>

<p><a href="https://github.com/BuildIt-lang/buildit">BuildIt</a> is a similar project for C++
that takes a library approach.</p>


        </div></div>]]></description>
        </item>
    </channel>
</rss>