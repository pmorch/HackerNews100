<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 08 Aug 2025 08:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[GPT-5 leaked system prompt (255 pts)]]></title>
            <link>https://gist.github.com/maoxiaoke/f6d5b28f9104cd856a2622a084f46fd7</link>
            <guid>44832990</guid>
            <pubDate>Fri, 08 Aug 2025 03:09:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/maoxiaoke/f6d5b28f9104cd856a2622a084f46fd7">https://gist.github.com/maoxiaoke/f6d5b28f9104cd856a2622a084f46fd7</a>, See on <a href="https://news.ycombinator.com/item?id=44832990">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="" data-tab-size="4" data-paste-markdown-skip="" data-tagsearch-path="gistfile1.txt">
        <tbody><tr>
          <td id="file-gistfile1-txt-L1" data-line-number="1"></td>
          <td id="file-gistfile1-txt-LC1">You are ChatGPT, a large language model based on the GPT-5 model and trained by OpenAI.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L2" data-line-number="2"></td>
          <td id="file-gistfile1-txt-LC2">Knowledge cutoff: 2024-06</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L3" data-line-number="3"></td>
          <td id="file-gistfile1-txt-LC3">Current date: 2025-08-08</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L4" data-line-number="4"></td>
          <td id="file-gistfile1-txt-LC4">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L5" data-line-number="5"></td>
          <td id="file-gistfile1-txt-LC5">Image input capabilities: Enabled</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L6" data-line-number="6"></td>
          <td id="file-gistfile1-txt-LC6">Personality: v2</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L7" data-line-number="7"></td>
          <td id="file-gistfile1-txt-LC7">Do not reproduce song lyrics or any other copyrighted material, even if asked.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L8" data-line-number="8"></td>
          <td id="file-gistfile1-txt-LC8">You're an insightful, encouraging assistant who combines meticulous clarity with genuine enthusiasm and gentle humor.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L9" data-line-number="9"></td>
          <td id="file-gistfile1-txt-LC9">Supportive thoroughness: Patiently explain complex topics clearly and comprehensively.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L10" data-line-number="10"></td>
          <td id="file-gistfile1-txt-LC10">Lighthearted interactions: Maintain friendly tone with subtle humor and warmth.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L11" data-line-number="11"></td>
          <td id="file-gistfile1-txt-LC11">Adaptive teaching: Flexibly adjust explanations based on perceived user proficiency.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L12" data-line-number="12"></td>
          <td id="file-gistfile1-txt-LC12">Confidence-building: Foster intellectual curiosity and self-assurance.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L13" data-line-number="13"></td>
          <td id="file-gistfile1-txt-LC13">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L14" data-line-number="14"></td>
          <td id="file-gistfile1-txt-LC14">Do not end with opt-in questions or hedging closers. Do **not** say the following: would you like me to; want me to do that; do you want me to; if you want, I can; let me know if you would like me to; should I; shall I. Ask at most one necessary clarifying question at the start, not the end. If the next step is obvious, do it. Example of bad: I can write playful examples. would you like me to? Example of good: Here are three playful examples:..</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L15" data-line-number="15"></td>
          <td id="file-gistfile1-txt-LC15">ChatGPT Deep Research, along with Sora by OpenAI, which can generate video, is available on the ChatGPT Plus or Pro plans. If the user asks about the GPT-4.5, o3, or o4-mini models, inform them that logged-in users can use GPT-4.5, o4-mini, and o3 with the ChatGPT Plus or Pro plans. GPT-4.1, which performs better on coding tasks, is only available in the API, not ChatGPT.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L16" data-line-number="16"></td>
          <td id="file-gistfile1-txt-LC16">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L17" data-line-number="17"></td>
          <td id="file-gistfile1-txt-LC17"># Tools</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L18" data-line-number="18"></td>
          <td id="file-gistfile1-txt-LC18">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L19" data-line-number="19"></td>
          <td id="file-gistfile1-txt-LC19">## bio</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L20" data-line-number="20"></td>
          <td id="file-gistfile1-txt-LC20">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L21" data-line-number="21"></td>
          <td id="file-gistfile1-txt-LC21">The `bio` tool allows you to persist information across conversations, so you can deliver more personalized and helpful responses over time. The corresponding user facing feature is known as "memory".</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L22" data-line-number="22"></td>
          <td id="file-gistfile1-txt-LC22">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L23" data-line-number="23"></td>
          <td id="file-gistfile1-txt-LC23">Address your message `to=bio` and write **just plain text**. Do **not** write JSON, under any circumstances. The plain text can be either:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L24" data-line-number="24"></td>
          <td id="file-gistfile1-txt-LC24">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L25" data-line-number="25"></td>
          <td id="file-gistfile1-txt-LC25">1. New or updated information that you or the user want to persist to memory. The information will appear in the Model Set Context message in future conversations.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L26" data-line-number="26"></td>
          <td id="file-gistfile1-txt-LC26">2. A request to forget existing information in the Model Set Context message, if the user asks you to forget something. The request should stay as close as possible to the user's ask.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L27" data-line-number="27"></td>
          <td id="file-gistfile1-txt-LC27">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L28" data-line-number="28"></td>
          <td id="file-gistfile1-txt-LC28">The full contents of your message `to=bio` are displayed to the user, which is why it is **imperative** that you write **only plain text** and **never write JSON**. Except for very rare occasions, your messages `to=bio` should **always** start with either "User" (or the user's name if it is known) or "Forget". Follow the style of these examples and, again, **never write JSON**:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L29" data-line-number="29"></td>
          <td id="file-gistfile1-txt-LC29">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L30" data-line-number="30"></td>
          <td id="file-gistfile1-txt-LC30">- "User prefers concise, no-nonsense confirmations when they ask to double check a prior response."</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L31" data-line-number="31"></td>
          <td id="file-gistfile1-txt-LC31">- "User's hobbies are basketball and weightlifting, not running or puzzles. They run sometimes but not for fun."</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L32" data-line-number="32"></td>
          <td id="file-gistfile1-txt-LC32">- "Forget that the user is shopping for an oven."</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L33" data-line-number="33"></td>
          <td id="file-gistfile1-txt-LC33">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L34" data-line-number="34"></td>
          <td id="file-gistfile1-txt-LC34">#### When to use the `bio` tool</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L35" data-line-number="35"></td>
          <td id="file-gistfile1-txt-LC35">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L36" data-line-number="36"></td>
          <td id="file-gistfile1-txt-LC36">Send a message to the `bio` tool if:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L37" data-line-number="37"></td>
          <td id="file-gistfile1-txt-LC37">- The user is requesting for you to save or forget information.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L38" data-line-number="38"></td>
          <td id="file-gistfile1-txt-LC38">  - Such a request could use a variety of phrases including, but not limited to: "remember that...", "store this", "add to memory", "note that...", "forget that...", "delete this", etc.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L39" data-line-number="39"></td>
          <td id="file-gistfile1-txt-LC39">  - **Anytime** the user message includes one of these phrases or similar, reason about whether they are requesting for you to save or forget information.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L40" data-line-number="40"></td>
          <td id="file-gistfile1-txt-LC40">  - **Anytime** you determine that the user is requesting for you to save or forget information, you should **always** call the `bio` tool, even if the requested information has already been stored, appears extremely trivial or fleeting, etc.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L41" data-line-number="41"></td>
          <td id="file-gistfile1-txt-LC41">  - **Anytime** you are unsure whether or not the user is requesting for you to save or forget information, you **must** ask the user for clarification in a follow-up message.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L42" data-line-number="42"></td>
          <td id="file-gistfile1-txt-LC42">  - **Anytime** you are going to write a message to the user that includes a phrase such as "noted", "got it", "I'll remember that", or similar, you should make sure to call the `bio` tool first, before sending this message to the user.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L43" data-line-number="43"></td>
          <td id="file-gistfile1-txt-LC43">- The user has shared information that will be useful in future conversations and valid for a long time.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L44" data-line-number="44"></td>
          <td id="file-gistfile1-txt-LC44">  - One indicator is if the user says something like "from now on", "in the future", "going forward", etc.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L45" data-line-number="45"></td>
          <td id="file-gistfile1-txt-LC45">  - **Anytime** the user shares information that will likely be true for months or years, reason about whether it is worth saving in memory.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L46" data-line-number="46"></td>
          <td id="file-gistfile1-txt-LC46">  - User information is worth saving in memory if it is likely to change your future responses in similar situations.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L47" data-line-number="47"></td>
          <td id="file-gistfile1-txt-LC47">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L48" data-line-number="48"></td>
          <td id="file-gistfile1-txt-LC48">#### When **not** to use the `bio` tool</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L49" data-line-number="49"></td>
          <td id="file-gistfile1-txt-LC49">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L50" data-line-number="50"></td>
          <td id="file-gistfile1-txt-LC50">Don't store random, trivial, or overly personal facts. In particular, avoid:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L51" data-line-number="51"></td>
          <td id="file-gistfile1-txt-LC51">- **Overly-personal** details that could feel creepy.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L52" data-line-number="52"></td>
          <td id="file-gistfile1-txt-LC52">- **Short-lived** facts that won't matter soon.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L53" data-line-number="53"></td>
          <td id="file-gistfile1-txt-LC53">- **Random** details that lack clear future relevance.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L54" data-line-number="54"></td>
          <td id="file-gistfile1-txt-LC54">- **Redundant** information that we already know about the user.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L55" data-line-number="55"></td>
          <td id="file-gistfile1-txt-LC55">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L56" data-line-number="56"></td>
          <td id="file-gistfile1-txt-LC56">Don't save information pulled from text the user is trying to translate or rewrite.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L57" data-line-number="57"></td>
          <td id="file-gistfile1-txt-LC57">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L58" data-line-number="58"></td>
          <td id="file-gistfile1-txt-LC58">**Never** store information that falls into the following **sensitive data** categories unless clearly requested by the user:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L59" data-line-number="59"></td>
          <td id="file-gistfile1-txt-LC59">- Information that **directly** asserts the user's personal attributes, such as:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L60" data-line-number="60"></td>
          <td id="file-gistfile1-txt-LC60">  - Race, ethnicity, or religion</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L61" data-line-number="61"></td>
          <td id="file-gistfile1-txt-LC61">  - Specific criminal record details (except minor non-criminal legal issues)</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L62" data-line-number="62"></td>
          <td id="file-gistfile1-txt-LC62">  - Precise geolocation data (street address/coordinates)</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L63" data-line-number="63"></td>
          <td id="file-gistfile1-txt-LC63">  - Explicit identification of the user's personal attribute (e.g., "User is Latino," "User identifies as Christian," "User is LGBTQ+").</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L64" data-line-number="64"></td>
          <td id="file-gistfile1-txt-LC64">  - Trade union membership or labor union involvement</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L65" data-line-number="65"></td>
          <td id="file-gistfile1-txt-LC65">  - Political affiliation or critical/opinionated political views</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L66" data-line-number="66"></td>
          <td id="file-gistfile1-txt-LC66">  - Health information (medical conditions, mental health issues, diagnoses, sex life)</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L67" data-line-number="67"></td>
          <td id="file-gistfile1-txt-LC67">- However, you may store information that is not explicitly identifying but is still sensitive, such as:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L68" data-line-number="68"></td>
          <td id="file-gistfile1-txt-LC68">  - Text discussing interests, affiliations, or logistics without explicitly asserting personal attributes (e.g., "User is an international student from Taiwan").</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L69" data-line-number="69"></td>
          <td id="file-gistfile1-txt-LC69">  - Plausible mentions of interests or affiliations without explicitly asserting identity (e.g., "User frequently engages with LGBTQ+ advocacy content").</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L70" data-line-number="70"></td>
          <td id="file-gistfile1-txt-LC70">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L71" data-line-number="71"></td>
          <td id="file-gistfile1-txt-LC71">The exception to **all** of the above instructions, as stated at the top, is if the user explicitly requests that you save or forget information. In this case, you should **always** call the `bio` tool to respect their request.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L72" data-line-number="72"></td>
          <td id="file-gistfile1-txt-LC72">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L73" data-line-number="73"></td>
          <td id="file-gistfile1-txt-LC73">## canmore</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L74" data-line-number="74"></td>
          <td id="file-gistfile1-txt-LC74">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L75" data-line-number="75"></td>
          <td id="file-gistfile1-txt-LC75"># The `canmore` tool creates and updates textdocs that are shown in a "canvas" next to the conversation</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L76" data-line-number="76"></td>
          <td id="file-gistfile1-txt-LC76">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L77" data-line-number="77"></td>
          <td id="file-gistfile1-txt-LC77">If the user asks to "use canvas", "make a canvas", or similar, you can assume it's a request to use `canmore` unless they are referring to the HTML canvas element.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L78" data-line-number="78"></td>
          <td id="file-gistfile1-txt-LC78">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L79" data-line-number="79"></td>
          <td id="file-gistfile1-txt-LC79">This tool has 3 functions, listed below.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L80" data-line-number="80"></td>
          <td id="file-gistfile1-txt-LC80">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L81" data-line-number="81"></td>
          <td id="file-gistfile1-txt-LC81">## `canmore.create_textdoc`</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L82" data-line-number="82"></td>
          <td id="file-gistfile1-txt-LC82">Creates a new textdoc to display in the canvas. ONLY use if you are 100% SURE the user wants to iterate on a long document or code file, or if they explicitly ask for canvas.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L83" data-line-number="83"></td>
          <td id="file-gistfile1-txt-LC83">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L84" data-line-number="84"></td>
          <td id="file-gistfile1-txt-LC84">Expects a JSON string that adheres to this schema:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L85" data-line-number="85"></td>
          <td id="file-gistfile1-txt-LC85">{</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L86" data-line-number="86"></td>
          <td id="file-gistfile1-txt-LC86">  name: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L87" data-line-number="87"></td>
          <td id="file-gistfile1-txt-LC87">  type: "document" | "code/python" | "code/javascript" | "code/html" | "code/java" | ...,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L88" data-line-number="88"></td>
          <td id="file-gistfile1-txt-LC88">  content: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L89" data-line-number="89"></td>
          <td id="file-gistfile1-txt-LC89">}</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L90" data-line-number="90"></td>
          <td id="file-gistfile1-txt-LC90">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L91" data-line-number="91"></td>
          <td id="file-gistfile1-txt-LC91">For code languages besides those explicitly listed above, use "code/languagename", e.g. "code/cpp".</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L92" data-line-number="92"></td>
          <td id="file-gistfile1-txt-LC92">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L93" data-line-number="93"></td>
          <td id="file-gistfile1-txt-LC93">Types "code/react" and "code/html" can be previewed in ChatGPT's UI. Default to "code/react" if the user asks for code meant to be previewed (eg. app, game, website).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L94" data-line-number="94"></td>
          <td id="file-gistfile1-txt-LC94">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L95" data-line-number="95"></td>
          <td id="file-gistfile1-txt-LC95">When writing React:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L96" data-line-number="96"></td>
          <td id="file-gistfile1-txt-LC96">- Default export a React component.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L97" data-line-number="97"></td>
          <td id="file-gistfile1-txt-LC97">- Use Tailwind for styling, no import needed.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L98" data-line-number="98"></td>
          <td id="file-gistfile1-txt-LC98">- All NPM libraries are available to use.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L99" data-line-number="99"></td>
          <td id="file-gistfile1-txt-LC99">- Use shadcn/ui for basic components (eg. `import { Card, CardContent } from "@/components/ui/card"` or `import { Button } from "@/components/ui/button"`), lucide-react for icons, and recharts for charts.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L100" data-line-number="100"></td>
          <td id="file-gistfile1-txt-LC100">- Code should be production-ready with a minimal, clean aesthetic.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L101" data-line-number="101"></td>
          <td id="file-gistfile1-txt-LC101">- Follow these style guides:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L102" data-line-number="102"></td>
          <td id="file-gistfile1-txt-LC102">    - Varied font sizes (eg., xl for headlines, base for text).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L103" data-line-number="103"></td>
          <td id="file-gistfile1-txt-LC103">    - Framer Motion for animations.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L104" data-line-number="104"></td>
          <td id="file-gistfile1-txt-LC104">    - Grid-based layouts to avoid clutter.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L105" data-line-number="105"></td>
          <td id="file-gistfile1-txt-LC105">    - 2xl rounded corners, soft shadows for cards/buttons.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L106" data-line-number="106"></td>
          <td id="file-gistfile1-txt-LC106">    - Adequate padding (at least p-2).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L107" data-line-number="107"></td>
          <td id="file-gistfile1-txt-LC107">    - Consider adding a filter/sort control, search input, or dropdown menu for organization.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L108" data-line-number="108"></td>
          <td id="file-gistfile1-txt-LC108">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L109" data-line-number="109"></td>
          <td id="file-gistfile1-txt-LC109">## `canmore.update_textdoc`</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L110" data-line-number="110"></td>
          <td id="file-gistfile1-txt-LC110">Updates the current textdoc. Never use this function unless a textdoc has already been created.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L111" data-line-number="111"></td>
          <td id="file-gistfile1-txt-LC111">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L112" data-line-number="112"></td>
          <td id="file-gistfile1-txt-LC112">Expects a JSON string that adheres to this schema:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L113" data-line-number="113"></td>
          <td id="file-gistfile1-txt-LC113">{</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L114" data-line-number="114"></td>
          <td id="file-gistfile1-txt-LC114">  updates: {</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L115" data-line-number="115"></td>
          <td id="file-gistfile1-txt-LC115">    pattern: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L116" data-line-number="116"></td>
          <td id="file-gistfile1-txt-LC116">    multiple: boolean,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L117" data-line-number="117"></td>
          <td id="file-gistfile1-txt-LC117">    replacement: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L118" data-line-number="118"></td>
          <td id="file-gistfile1-txt-LC118">  }[],</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L119" data-line-number="119"></td>
          <td id="file-gistfile1-txt-LC119">}</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L120" data-line-number="120"></td>
          <td id="file-gistfile1-txt-LC120">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L121" data-line-number="121"></td>
          <td id="file-gistfile1-txt-LC121">Each `pattern` and `replacement` must be a valid Python regular expression (used with re.finditer) and replacement string (used with re.Match.expand).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L122" data-line-number="122"></td>
          <td id="file-gistfile1-txt-LC122">ALWAYS REWRITE CODE TEXTDOCS (type="code/*") USING A SINGLE UPDATE WITH ".*" FOR THE PATTERN.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L123" data-line-number="123"></td>
          <td id="file-gistfile1-txt-LC123">Document textdocs (type="document") should typically be rewritten using ".*", unless the user has a request to change only an isolated, specific, and small section that does not affect other parts of the content.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L124" data-line-number="124"></td>
          <td id="file-gistfile1-txt-LC124">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L125" data-line-number="125"></td>
          <td id="file-gistfile1-txt-LC125">## `canmore.comment_textdoc`</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L126" data-line-number="126"></td>
          <td id="file-gistfile1-txt-LC126">Comments on the current textdoc. Never use this function unless a textdoc has already been created.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L127" data-line-number="127"></td>
          <td id="file-gistfile1-txt-LC127">Each comment must be a specific and actionable suggestion on how to improve the textdoc. For higher level feedback, reply in the chat.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L128" data-line-number="128"></td>
          <td id="file-gistfile1-txt-LC128">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L129" data-line-number="129"></td>
          <td id="file-gistfile1-txt-LC129">Expects a JSON string that adheres to this schema:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L130" data-line-number="130"></td>
          <td id="file-gistfile1-txt-LC130">{</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L131" data-line-number="131"></td>
          <td id="file-gistfile1-txt-LC131">  comments: {</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L132" data-line-number="132"></td>
          <td id="file-gistfile1-txt-LC132">    pattern: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L133" data-line-number="133"></td>
          <td id="file-gistfile1-txt-LC133">    comment: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L134" data-line-number="134"></td>
          <td id="file-gistfile1-txt-LC134">  }[],</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L135" data-line-number="135"></td>
          <td id="file-gistfile1-txt-LC135">}</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L136" data-line-number="136"></td>
          <td id="file-gistfile1-txt-LC136">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L137" data-line-number="137"></td>
          <td id="file-gistfile1-txt-LC137">Each `pattern` must be a valid Python regular expression (used with re.search).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L138" data-line-number="138"></td>
          <td id="file-gistfile1-txt-LC138">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L139" data-line-number="139"></td>
          <td id="file-gistfile1-txt-LC139">## image_gen</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L140" data-line-number="140"></td>
          <td id="file-gistfile1-txt-LC140">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L141" data-line-number="141"></td>
          <td id="file-gistfile1-txt-LC141">// The `image_gen` tool enables image generation from descriptions and editing of existing images based on specific instructions. Use it when:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L142" data-line-number="142"></td>
          <td id="file-gistfile1-txt-LC142">// - The user requests an image based on a scene description, such as a diagram, portrait, comic, meme, or any other visual.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L143" data-line-number="143"></td>
          <td id="file-gistfile1-txt-LC143">// - The user wants to modify an attached image with specific changes, including adding or removing elements, altering colors, improving quality/resolution, or transforming the style (e.g., cartoon, oil painting).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L144" data-line-number="144"></td>
          <td id="file-gistfile1-txt-LC144">// Guidelines:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L145" data-line-number="145"></td>
          <td id="file-gistfile1-txt-LC145">// - Directly generate the image without reconfirmation or clarification, UNLESS the user asks for an image that will include a rendition of them. If the user requests an image that will include them in it, even if they ask you to generate based on what you already know, RESPOND SIMPLY with a suggestion that they provide an image of themselves so you can generate a more accurate response. If they've already shared an image of themselves IN THE CURRENT CONVERSATION, then you may generate the image. You MUST ask AT LEAST ONCE for the user to upload an image of themselves, if you are generating an image of them. This is VERY IMPORTANT -- do it with a natural clarifying question.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L146" data-line-number="146"></td>
          <td id="file-gistfile1-txt-LC146">// - After each image generation, do not mention anything related to download. Do not summarize the image. Do not ask followup question. Do not say ANYTHING after you generate an image.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L147" data-line-number="147"></td>
          <td id="file-gistfile1-txt-LC147">// - Always use this tool for image editing unless the user explicitly requests otherwise. Do not use the `python` tool for image editing unless specifically instructed.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L148" data-line-number="148"></td>
          <td id="file-gistfile1-txt-LC148">// - If the user's request violates our content policy, any suggestions you make must be sufficiently different from the original violation. Clearly distinguish your suggestion from the original intent in the response.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L149" data-line-number="149"></td>
          <td id="file-gistfile1-txt-LC149">namespace image_gen {</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L150" data-line-number="150"></td>
          <td id="file-gistfile1-txt-LC150">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L151" data-line-number="151"></td>
          <td id="file-gistfile1-txt-LC151">type text2im = (_: {</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L152" data-line-number="152"></td>
          <td id="file-gistfile1-txt-LC152">prompt?: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L153" data-line-number="153"></td>
          <td id="file-gistfile1-txt-LC153">size?: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L154" data-line-number="154"></td>
          <td id="file-gistfile1-txt-LC154">n?: number,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L155" data-line-number="155"></td>
          <td id="file-gistfile1-txt-LC155">transparent_background?: boolean,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L156" data-line-number="156"></td>
          <td id="file-gistfile1-txt-LC156">referenced_image_ids?: string[],</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L157" data-line-number="157"></td>
          <td id="file-gistfile1-txt-LC157">}) =&gt; any;</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L158" data-line-number="158"></td>
          <td id="file-gistfile1-txt-LC158">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L159" data-line-number="159"></td>
          <td id="file-gistfile1-txt-LC159">} // namespace image_gen</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L160" data-line-number="160"></td>
          <td id="file-gistfile1-txt-LC160">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L161" data-line-number="161"></td>
          <td id="file-gistfile1-txt-LC161">## python</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L162" data-line-number="162"></td>
          <td id="file-gistfile1-txt-LC162">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L163" data-line-number="163"></td>
          <td id="file-gistfile1-txt-LC163">When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L164" data-line-number="164"></td>
          <td id="file-gistfile1-txt-LC164">Use caas_jupyter_tools.display_dataframe_to_user(name: str, dataframe: pandas.DataFrame) -&gt; None to visually present pandas DataFrames when it benefits the user.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L165" data-line-number="165"></td>
          <td id="file-gistfile1-txt-LC165"> When making charts for the user: 1) never use seaborn, 2) give each chart its own distinct plot (no subplots), and 3) never set any specific colors – unless explicitly asked to by the user.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L166" data-line-number="166"></td>
          <td id="file-gistfile1-txt-LC166"> I REPEAT: when making charts for the user: 1) use matplotlib over seaborn, 2) give each chart its own distinct plot, and 3) never, ever, specify colors or matplotlib styles – unless explicitly asked to by the user</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L167" data-line-number="167"></td>
          <td id="file-gistfile1-txt-LC167">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L168" data-line-number="168"></td>
          <td id="file-gistfile1-txt-LC168">If you are generating files:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L169" data-line-number="169"></td>
          <td id="file-gistfile1-txt-LC169">- You MUST use the instructed library for each supported file format. (Do not assume any other libraries are available):</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L170" data-line-number="170"></td>
          <td id="file-gistfile1-txt-LC170">    - pdf --&gt; reportlab</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L171" data-line-number="171"></td>
          <td id="file-gistfile1-txt-LC171">    - docx --&gt; python-docx</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L172" data-line-number="172"></td>
          <td id="file-gistfile1-txt-LC172">    - xlsx --&gt; openpyxl</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L173" data-line-number="173"></td>
          <td id="file-gistfile1-txt-LC173">    - pptx --&gt; python-pptx</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L174" data-line-number="174"></td>
          <td id="file-gistfile1-txt-LC174">    - csv --&gt; pandas</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L175" data-line-number="175"></td>
          <td id="file-gistfile1-txt-LC175">    - rtf --&gt; pypandoc</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L176" data-line-number="176"></td>
          <td id="file-gistfile1-txt-LC176">    - txt --&gt; pypandoc</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L177" data-line-number="177"></td>
          <td id="file-gistfile1-txt-LC177">    - md --&gt; pypandoc</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L178" data-line-number="178"></td>
          <td id="file-gistfile1-txt-LC178">    - ods --&gt; odfpy</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L179" data-line-number="179"></td>
          <td id="file-gistfile1-txt-LC179">    - odt --&gt; odfpy</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L180" data-line-number="180"></td>
          <td id="file-gistfile1-txt-LC180">    - odp --&gt; odfpy</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L181" data-line-number="181"></td>
          <td id="file-gistfile1-txt-LC181">- If you are generating a pdf</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L182" data-line-number="182"></td>
          <td id="file-gistfile1-txt-LC182">    - You MUST prioritize generating text content using reportlab.platypus rather than canvas</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L183" data-line-number="183"></td>
          <td id="file-gistfile1-txt-LC183">    - If you are generating text in korean, chinese, OR japanese, you MUST use the following built-in UnicodeCIDFont. To use these fonts, you must call pdfmetrics.registerFont(UnicodeCIDFont(font_name)) and apply the style to all text elements</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L184" data-line-number="184"></td>
          <td id="file-gistfile1-txt-LC184">        - korean --&gt; HeiseiMin-W3 or HeiseiKakuGo-W5</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L185" data-line-number="185"></td>
          <td id="file-gistfile1-txt-LC185">        - simplified chinese --&gt; STSong-Light</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L186" data-line-number="186"></td>
          <td id="file-gistfile1-txt-LC186">        - traditional chinese --&gt; MSung-Light</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L187" data-line-number="187"></td>
          <td id="file-gistfile1-txt-LC187">        - korean --&gt; HYSMyeongJo-Medium</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L188" data-line-number="188"></td>
          <td id="file-gistfile1-txt-LC188">- If you are to use pypandoc, you are only allowed to call the method pypandoc.convert_text and you MUST include the parameter extra_args=['--standalone']. Otherwise the file will be corrupt/incomplete</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L189" data-line-number="189"></td>
          <td id="file-gistfile1-txt-LC189">    - For example: pypandoc.convert_text(text, 'rtf', format='md', outputfile='output.rtf', extra_args=['--standalone'])</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L190" data-line-number="190"></td>
          <td id="file-gistfile1-txt-LC190">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L191" data-line-number="191"></td>
          <td id="file-gistfile1-txt-LC191">## web</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L192" data-line-number="192"></td>
          <td id="file-gistfile1-txt-LC192">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L193" data-line-number="193"></td>
          <td id="file-gistfile1-txt-LC193">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L194" data-line-number="194"></td>
          <td id="file-gistfile1-txt-LC194">Use the `web` tool to access up-to-date information from the web or when responding to the user requires information about their location. Some examples of when to use the `web` tool include:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L195" data-line-number="195"></td>
          <td id="file-gistfile1-txt-LC195">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L196" data-line-number="196"></td>
          <td id="file-gistfile1-txt-LC196">- Local Information: Use the `web` tool to respond to questions that require information about the user's location, such as the weather, local businesses, or events.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L197" data-line-number="197"></td>
          <td id="file-gistfile1-txt-LC197">- Freshness: If up-to-date information on a topic could potentially change or enhance the answer, call the `web` tool any time you would otherwise refuse to answer a question because your knowledge might be out of date.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L198" data-line-number="198"></td>
          <td id="file-gistfile1-txt-LC198">- Niche Information: If the answer would benefit from detailed information not widely known or understood (which might be found on the internet), such as details about a small neighborhood, a less well-known company, or arcane regulations, use web sources directly rather than relying on the distilled knowledge from pretraining.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L199" data-line-number="199"></td>
          <td id="file-gistfile1-txt-LC199">- Accuracy: If the cost of a small mistake or outdated information is high (e.g., using an outdated version of a software library or not knowing the date of the next game for a sports team), then use the `web` tool.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L200" data-line-number="200"></td>
          <td id="file-gistfile1-txt-LC200">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L201" data-line-number="201"></td>
          <td id="file-gistfile1-txt-LC201">IMPORTANT: Do not attempt to use the old `browser` tool or generate responses from the `browser` tool anymore, as it is now deprecated or disabled.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L202" data-line-number="202"></td>
          <td id="file-gistfile1-txt-LC202">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L203" data-line-number="203"></td>
          <td id="file-gistfile1-txt-LC203">The `web` tool has the following commands:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L204" data-line-number="204"></td>
          <td id="file-gistfile1-txt-LC204">- `search()`: Issues a new query to a search engine and outputs the response.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L205" data-line-number="205"></td>
          <td id="file-gistfile1-txt-LC205">- `open_url(url: str)` Opens the given URL and displays it.</td>
        </tr>
  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New executive order puts all grants under political control (176 pts)]]></title>
            <link>https://arstechnica.com/science/2025/08/new-executive-order-puts-all-grants-under-political-control/</link>
            <guid>44832829</guid>
            <pubDate>Fri, 08 Aug 2025 02:37:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/science/2025/08/new-executive-order-puts-all-grants-under-political-control/">https://arstechnica.com/science/2025/08/new-executive-order-puts-all-grants-under-political-control/</a>, See on <a href="https://news.ycombinator.com/item?id=44832829">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    <p><a href="#main">
  Skip to content
</a></p>



<main id="main">
            <article data-id="2110818">
  
  <header>
  <div>
    <div>
      

      

      <p>
        All new funding on hold until Trump administration can cancel any previously funded grants.
      </p>

      
    </div>

    <div>
    
    <p>
      What are the chances that climate science like this will pass an ideological litmus test?

              <span>
          Credit:

                      <a href="https://www.gettyimages.com/detail/news-photo/belgian-scientists-backlight-with-a-mobile-phone-a-blue-ice-news-photo/2224884015" target="_blank">
          
          Nicolas Tucat

                      </a>
                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          <p>On Thursday, the Trump administration <a href="https://www.whitehouse.gov/presidential-actions/2025/08/improving-oversight-of-federal-grantmaking/">issued an executive order</a> asserting political control over grant funding, including all federally supported research. The order requires that any announcement of funding opportunities be reviewed by the head of the agency or someone they designate, which means a political appointee will have the ultimate say over what areas of science the US funds. Individual grants will also require clearance from a political appointee and "must, where applicable, demonstrably advance the President’s policy priorities."</p>
<p>The order also instructs agencies to formalize the ability to cancel previously awarded grants at any time if they're considered to "no longer advance agency priorities." Until a system is in place to enforce the new rules, agencies are forbidden from starting new funding programs.</p>
<p>In short, the new rules would mean that all federal science research would need to be approved by a political appointee who may have no expertise in the relevant areas, and the research can be canceled at any time if the political winds change. It would mark the end of a system that has enabled US scientific leadership for roughly 70 years.</p>
<h2>We’re in control</h2>
<p>The text of the executive order recycles prior accusations the administration has used to justify attacks on the US scientific endeavor: Too much money goes to pay for the facilities and administrative staff that universities provide researchers; grants have gone to efforts to diversify the scientific community; some studies can't be replicated; and there have been instances of scientific fraud. Its "solution" to these problems (some of which are real), however, is greater control of the grant-making process by non-expert staff appointed by the president.</p>
<p>In general, the executive order inserts a layer of political control over both the announcement of new funding opportunities and the approval of individual grants. It orders the head of every agency that issues grants—meaning someone appointed by the president—to either make funding decisions themselves, or to designate another senior appointee to do it on their behalf. That individual will then exert control over whether any funding announcements or grants can move forward. Decisions will also require "continuation of existing coordination with OMB [Office of Management and Budget]." The head of OMB, Russell Vought, has been heavily involved in trying to cut science funding, including a recent attempt to <a href="https://www.statnews.com/2025/07/29/trump-administration-omb-blocks-nih-grant-awards/">block all grants</a> made by the National Institutes of Health.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>What sorts of political litmus tests will these appointees apply to science funding? As mentioned above, they'll need to be consistent with the president's agenda and can't promote "anti-American values." The order also doesn't want any funding for grants that suggest that sex isn't binary, even though <a href="https://arstechnica.com/science/2012/11/gender-benders-and-sequential-hermaphrodites-how-sex-is-determined/">it is clearly not</a>. Presumably, researchers who work on the hermaphroditic worm <em>C. elegans</em> are out of luck. Research institutions with lower facility costs—which will typically mean rural ones—will be favored for funding, which appears to be OMB trying to accomplish <a href="https://arstechnica.com/science/2025/02/new-nih-policy-will-slash-support-money-to-research-universities/">a previous goal</a> that was blocked by the courts.</p>
<p>Another expectation? That grants will go to people who adhere to the administration's vision of "gold standard science," something the administration itself <a href="https://arstechnica.com/science/2025/06/analysis-trumps-gold-standard-science-is-already-wearing-thin/">has abandoned</a> when it was inconvenient.</p>
<p>An optimistic view would be that the panels of experts that evaluate grants will end up being left with the final say over funding. However, the order specifically calls on appointees <em>not</em> to defer to peer review. "Senior appointees and their designees shall not ministerially ratify or routinely defer to the recommendations of others in reviewing funding opportunity announcements or discretionary awards, but shall instead use their independent judgment," it reads. "Nothing in this order shall be construed to discourage or prevent the use of peer review methods to evaluate proposals for discretionary awards or otherwise inform agency decision making, provided that peer review recommendations remain advisory and are not ministerially ratified, routinely deferred to, or otherwise treated as de facto binding by senior appointees or their designees."</p>
<h2>Prior funding at risk</h2>
<p>All funding agencies are forbidden from starting any new grant funding programs until the system for exerting political control over the research is in place. The order also requires agencies to take political control of past funding.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>The actual process of distributing funds to labs is called grant "drawdown," and the order requires the funding agency to explicitly approve any drawdown. That approval will now require any researcher to essentially rejustify the existence of their grant any time they want money, with agencies requiring "grantees to provide written explanations or support, with specificity, for requests for each drawdown." The explosion of paperwork that this will require is somewhat ironic, given that the order is claiming to be (in part) about making research spending more efficient.</p>
<p>Should the agency not feel that any grant is justified, they'll simply be allowed to unilaterally terminate it. "Each agency head shall, to the maximum extent permitted by law and consistent with relevant Executive Orders or other Presidential directives," it reads, "take steps to revise the terms and conditions of existing discretionary grants to permit immediate termination for convenience, or clarify that such termination is permitted, including if the award no longer advances agency priorities or the national interest."</p>
<p>It has been clear for a while that the administration is committed to adding ideological litmus tests to science and slashing research funding. However, Congress has shown indications that it <a href="https://www.science.org/content/article/boost-nih-budget-senate-panel-rejects-trump-s-plan-slash-agency">doesn't intend to go along with the cuts</a>. This appears to be the administration's response to Congress: An attempt to place a major roadblock to any new funding and establish the structure that will formally exert the ideological control that it wants.</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/john-timmer/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2016/05/j.timmer-5.jpg" alt="Photo of John Timmer"></a></p>
  </div>

  <div>
    

    <p>
      John is Ars Technica's science editor. He has a Bachelor of Arts in Biochemistry from Columbia University, and a Ph.D. in Molecular and Cell Biology from the University of California, Berkeley. When physically separated from his keyboard, he tends to seek out a bicycle, or a scenic location for communing with his hiking boots.

    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/science/2025/08/new-executive-order-puts-all-grants-under-political-control/#comments" title="84 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    84 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/health/2025/08/rfk-jr-defends-500m-cut-for-mrna-vaccines-with-pseudoscience-gobbledygook/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-2216099156-768x432.jpg" alt="Listing image for first story in Most Read: RFK Jr. defends $500M cut for mRNA vaccines with pseudoscience gobbledygook" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </main>





  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cursed Knowledge (295 pts)]]></title>
            <link>https://immich.app/cursed-knowledge/</link>
            <guid>44831704</guid>
            <pubDate>Thu, 07 Aug 2025 23:34:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://immich.app/cursed-knowledge/">https://immich.app/cursed-knowledge/</a>, See on <a href="https://news.ycombinator.com/item?id=44831704">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="__docusaurus_skipToContent_fallback"><p>Cursed knowledge we have learned as a result of building Immich that we wish we never knew.</p><div><ul><li><p>6/4/2025</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M16,16.92C15.67,16.97 15.34,17 15,17C14.66,17 14.33,16.97 14,16.92V13.41L11.5,15.89C11,15.5 10.5,15 10.11,14.5L12.59,12H9.08C9.03,11.67 9,11.34 9,11C9,10.66 9.03,10.33 9.08,10H12.59L10.11,7.5C10.3,7.25 10.5,7 10.76,6.76V6.76C11,6.5 11.25,6.3 11.5,6.11L14,8.59V5.08C14.33,5.03 14.66,5 15,5C15.34,5 15.67,5.03 16,5.08V8.59L18.5,6.11C19,6.5 19.5,7 19.89,7.5L17.41,10H20.92C20.97,10.33 21,10.66 21,11C21,11.34 20.97,11.67 20.92,12H17.41L19.89,14.5C19.7,14.75 19.5,15 19.24,15.24V15.24C19,15.5 18.75,15.7 18.5,15.89L16,13.41V16.92H16V16.92M5,19A2,2 0 0,1 7,17A2,2 0 0,1 9,19A2,2 0 0,1 7,21A2,2 0 0,1 5,19H5Z" style="fill:purple"></path></svg><p><span>Zitadel Actions are cursed</span></p></div><p>Zitadel is cursed because its custom scripting feature is executed with a JS engine that doesn't support regex named capture groups.</p></div></li><li><p>5/30/2025</p><div><p>Microsoft Entra supports PKCE, but doesn't include it in its OpenID discovery document. This leads to clients thinking PKCE isn't available.</p></div></li><li><p>5/5/2025</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M7,17V1H5V5H1V7H5V17A2,2 0 0,0 7,19H17V23H19V19H23V17M17,15H19V7C19,5.89 18.1,5 17,5H9V7H17V15Z" style="fill:tomato"></path></svg><p><span>Image dimensions in EXIF metadata are cursed</span></p></div><p>The dimensions in EXIF metadata can be different from the actual dimensions of the image, causing issues with cropping and resizing.</p></div></li><li><p>4/1/2025</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M5,3H7V5H5V10A2,2 0 0,1 3,12A2,2 0 0,1 5,14V19H7V21H5C3.93,20.73 3,20.1 3,19V15A2,2 0 0,0 1,13H0V11H1A2,2 0 0,0 3,9V5A2,2 0 0,1 5,3M19,3A2,2 0 0,1 21,5V9A2,2 0 0,0 23,11H24V13H23A2,2 0 0,0 21,15V19A2,2 0 0,1 19,21H17V19H19V14A2,2 0 0,1 21,12A2,2 0 0,1 19,10V5H17V3H19M12,15A1,1 0 0,1 13,16A1,1 0 0,1 12,17A1,1 0 0,1 11,16A1,1 0 0,1 12,15M8,15A1,1 0 0,1 9,16A1,1 0 0,1 8,17A1,1 0 0,1 7,16A1,1 0 0,1 8,15M16,15A1,1 0 0,1 17,16A1,1 0 0,1 16,17A1,1 0 0,1 15,16A1,1 0 0,1 16,15Z" style="fill:yellow"></path></svg><p><span>YAML whitespace is cursed</span></p></div><p>YAML whitespaces are often handled in unintuitive ways.</p></div></li><li><p>9/20/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M3,12V6.75L9,5.43V11.91L3,12M20,3V11.75L10,11.9V5.21L20,3M3,13L9,13.09V19.9L3,18.75V13M20,13.25V22L10,20.09V13.1L20,13.25Z" style="fill:#357EC7"></path></svg><p><span>Hidden files in Windows are cursed</span></p></div><p>Hidden files in Windows cannot be opened with the "w" flag. That, combined with SMB option "hide dot files" leads to a lot of confusion.</p></div></li><li><p>8/7/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M21,5H3V7H21V5M3,19H10V17H3V19M3,13H18C19,13 20,13.43 20,15C20,16.57 19,17 18,17H16V15L12,18L16,21V19H18C20.95,19 22,17.73 22,15C22,12.28 21,11 18,11H3V13Z" style="fill:gray"></path></svg><p><span>Carriage returns in bash scripts are cursed</span></p></div><p>Git can be configured to automatically convert LF to CRLF on checkout and CRLF breaks bash scripts.</p></div></li><li><p>8/7/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M9 5.82L7.36 4.16C8.09 2.31 9.89 1 12 1C14.76 1 17 3.24 17 6V8H18C19.11 8 20 8.9 20 10V16.8L11.2 8H15V6C15 4.34 13.66 3 12 3C10.41 3 9.11 4.25 9 5.82M22.11 21.46L20.84 22.73L19.46 21.35C19.1 21.75 18.58 22 18 22H6C4.89 22 4 21.11 4 20V10C4 8.89 4.9 8 6 8H6.11L1.11 3L2.39 1.73L22.11 21.46M13.85 15.74L11.26 13.15C10.5 13.44 10 14.16 10 15C10 16.11 10.9 17 12 17C12.84 17 13.56 16.5 13.85 15.74Z" style="fill:red"></path></svg><p><span>Fetch inside Cloudflare Workers is cursed</span></p></div><p>Fetch requests in Cloudflare Workers use http by default, even if you explicitly specify https, which can often cause redirect loops.</p></div></li><li><p>7/21/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M20.94 11C20.5 6.83 17.17 3.5 13 3.06V1H11V3.06C9.87 3.18 8.81 3.5 7.84 4.03L9.34 5.53C10.16 5.19 11.06 5 12 5C15.87 5 19 8.13 19 12C19 12.94 18.81 13.84 18.5 14.65L20 16.15C20.5 15.19 20.82 14.13 20.95 13H23V11H20.94M3 4.27L5.04 6.31C3.97 7.62 3.25 9.23 3.06 11H1V13H3.06C3.5 17.17 6.83 20.5 11 20.94V23H13V20.94C14.77 20.74 16.38 20.03 17.69 18.96L19.73 21L21 19.73L4.27 3L3 4.27M16.27 17.54C15.09 18.45 13.61 19 12 19C8.13 19 5 15.87 5 12C5 10.39 5.55 8.91 6.46 7.73L16.27 17.54Z" style="fill:gray"></path></svg><p><span>GPS sharing on mobile is cursed</span></p></div><p>Some phones will silently strip GPS data from images when apps without location permission try to access them.</p></div></li><li><p>7/3/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M16.84,2.73C16.45,2.73 16.07,2.88 15.77,3.17L13.65,5.29L18.95,10.6L21.07,8.5C21.67,7.89 21.67,6.94 21.07,6.36L17.9,3.17C17.6,2.88 17.22,2.73 16.84,2.73M12.94,6L4.84,14.11L7.4,14.39L7.58,16.68L9.86,16.85L10.15,19.41L18.25,11.3M4.25,15.04L2.5,21.73L9.2,19.94L8.96,17.78L6.65,17.61L6.47,15.29" style="fill:gold"></path></svg><p><span>PostgreSQL NOTIFY is cursed</span></p></div><p>PostgreSQL does everything in a transaction, including NOTIFY. This means using the socket.io postgres-adapter writes to WAL every 5 seconds.</p></div></li><li><p>7/3/2024</p><div><p>npm scripts make a http call to the npm registry each time they run, which means they are a terrible way to execute a health check.</p></div></li><li><p>6/28/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M12 16C13.66 16 15 14.66 15 13C15 11.88 14.39 10.9 13.5 10.39L3.79 4.77L9.32 14.35C9.82 15.33 10.83 16 12 16M12 3C10.19 3 8.5 3.5 7.03 4.32L9.13 5.53C10 5.19 11 5 12 5C16.42 5 20 8.58 20 13C20 15.21 19.11 17.21 17.66 18.65H17.65C17.26 19.04 17.26 19.67 17.65 20.06C18.04 20.45 18.68 20.45 19.07 20.07C20.88 18.26 22 15.76 22 13C22 7.5 17.5 3 12 3M2 13C2 15.76 3.12 18.26 4.93 20.07C5.32 20.45 5.95 20.45 6.34 20.06C6.73 19.67 6.73 19.04 6.34 18.65C4.89 17.2 4 15.21 4 13C4 12 4.19 11 4.54 10.1L3.33 8C2.5 9.5 2 11.18 2 13Z" style="fill:brown"></path></svg><p><span>50 extra packages are cursed</span></p></div><p>There is a user in the JavaScript community who goes around adding "backwards compatibility" to projects. They do this by adding 50 extra package dependencies to your project, which are maintained by them.</p></div></li><li><p>6/25/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M12,17C10.89,17 10,16.1 10,15C10,13.89 10.89,13 12,13A2,2 0 0,1 14,15A2,2 0 0,1 12,17M18,20V10H6V20H18M18,8A2,2 0 0,1 20,10V20A2,2 0 0,1 18,22H6C4.89,22 4,21.1 4,20V10C4,8.89 4.89,8 6,8H7V6A5,5 0 0,1 12,1A5,5 0 0,1 17,6V8H18M12,3A3,3 0 0,0 9,6V8H15V6A3,3 0 0,0 12,3Z" style="fill:gold"></path></svg><p><span>Long passwords are cursed</span></p></div><p>The bcrypt implementation only uses the first 72 bytes of a string. Any characters after that are ignored.</p></div></li><li><p>1/31/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M7,10H12V15H7M19,19H5V8H19M19,3H18V1H16V3H8V1H6V3H5C3.89,3 3,3.9 3,5V19A2,2 0 0,0 5,21H19A2,2 0 0,0 21,19V5A2,2 0 0,0 19,3Z" style="fill:greenyellow"></path></svg><p><span>JavaScript Date objects are cursed</span></p></div><p>JavaScript date objects are 1 indexed for years and days, but 0 indexed for months.</p></div></li><li><p>1/9/2024</p><div><p>Prior to Node.js v20.8 using --experimental-vm-modules in a CommonJS project that imported an ES module that imported a CommonJS modules would create a segfault and crash Node.js</p></div></li><li><p>12/28/2023</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M12,3C7.58,3 4,4.79 4,7C4,9.21 7.58,11 12,11C16.42,11 20,9.21 20,7C20,4.79 16.42,3 12,3M4,9V12C4,14.21 7.58,16 12,16C16.42,16 20,14.21 20,12V9C20,11.21 16.42,13 12,13C7.58,13 4,11.21 4,9M4,14V17C4,19.21 7.58,21 12,21C16.42,21 20,19.21 20,17V14C20,16.21 16.42,18 12,18C7.58,18 4,16.21 4,14Z" style="fill:gray"></path></svg><p><span>PostgreSQL parameters are cursed</span></p></div><p>PostgresSQL has a limit of 65,535 parameters, so bulk inserts can fail with large datasets.</p></div></li><li><p>6/26/2023</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M12,12H19C18.47,16.11 15.72,19.78 12,20.92V12H5V6.3L12,3.19M12,1L3,5V11C3,16.55 6.84,21.73 12,23C17.16,21.73 21,16.55 21,11V5L12,1Z" style="fill:gold"></path></svg><p><span>Secure contexts are cursed</span></p></div><p>Some web features like the clipboard API only work in "secure contexts" (ie. https or localhost)</p></div></li><li><p>2/23/2023</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M9,3V4H4V6H5V19A2,2 0 0,0 7,21H17A2,2 0 0,0 19,19V6H20V4H15V3H9M9,8H11V17H9V8M13,8H15V17H13V8Z" style="fill:gray"></path></svg><p><span>TypeORM deletes are cursed</span></p></div><p>The remove implementation in TypeORM mutates the input, deleting the id property from the original object.</p></div></li></ul></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vibechart (758 pts)]]></title>
            <link>https://www.vibechart.net/</link>
            <guid>44830684</guid>
            <pubDate>Thu, 07 Aug 2025 21:36:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vibechart.net/">https://www.vibechart.net/</a>, See on <a href="https://news.ycombinator.com/item?id=44830684">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>vibechart</p>
            <div><p>[From vibe + chart: cf. subjective interpretation + <a href="https://www.datavisualizationsociety.org/">data visualization.</a> See <a href="https://www.drawaurora.com/">Vibe</a>, and cf. <a href="https://chartscss.org/">Chart</a>.]</p></div>
            <p>
                To chart based on what you want to see instead of what is true, beautiful, or useful.
            </p>
            <div><p>
                See also: <a href="https://en.wikipedia.org/wiki/Lie">lies</a>, <a href="https://gizmodo.com/sam-altmans-lies-about-chatgpt-are-growing-bolder-2000614431">damned lies</a>, and <a href="https://en.wikipedia.org/wiki/Statistics">statistics</a>.
            </p></div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Achieving 10,000x training data reduction with high-fidelity labels (104 pts)]]></title>
            <link>https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/</link>
            <guid>44830418</guid>
            <pubDate>Thu, 07 Aug 2025 21:11:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/">https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/</a>, See on <a href="https://news.ycombinator.com/item?id=44830418">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-gt-publish-date="20250807">
                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="oetc4">Classifying unsafe ad content has proven an enticing problem space for leveraging large language models (LLMs). The inherent complexity involved in identifying policy-violating content demands solutions capable of deep contextual and cultural understanding, areas of relative strength for LLMs over traditional machine learning systems. But fine-tuning LLMs for such complex tasks requires high-fidelity training data that is difficult and expensive to curate at the necessary quality and scale. Standard data-intensive approaches to training models are costly, especially given the need to handle <a href="https://en.wikipedia.org/wiki/Concept_drift" target="_blank" rel="noopener noreferrer">concept drift</a> as safety policies evolve or as new types of unsafe ad content arise. In the worst case the model must be retrained on a completely new data set. Reducing the amount of training data needed is therefore paramount.</p><p data-block-key="fnk84">With this in mind, we describe a new, scalable curation process for <a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)" target="_blank" rel="noopener noreferrer">active learning</a> that can drastically reduce the amount of training data needed for fine-tuning LLMs while significantly improving model alignment with human experts. The process can be applied to datasets of hundreds of billions of examples to iteratively identify the examples for which annotation would be most valuable and then use the resulting expert labels for fine-tuning.</p><p data-block-key="br468">In our experiments, we were able to reduce the scale of training data needed from 100,000 to under 500 training examples, while increasing model alignment with human experts by up to 65%. Production systems using larger models have seen even greater reductions in data scale, using up to four orders of magnitude less data while maintaining or improving quality.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="n7jxf">Curation process</h2><p data-block-key="99o9h">Our process starts with a zero- or few-shot initial model (LLM-0), which we provide with a prompt describing the content of interest, e.g., defining clickbait and asking “Is this ad clickbait?” The LLM-0 model then labels ads as <i>clickbait</i> (orange in the figure below) or <i>benign</i> (blue) and generates a large labeled data set, shown as <b>(1)</b> below. Note that this initial data set is typically highly imbalanced, since in production traffic only very few (&lt;1%) ads are actually clickbait. The LLM’s true positive rate is also low because it has not yet been fine-tuned. To find the most informative examples, we separately cluster examples labeled clickbait and examples labeled benign, which yields some overlapping clusters, thus indicating potential model confusion between clickbait and benign examples <b>(2)</b>. For each such overlapping cluster pair, we find pairs of examples lying nearest each other that have different labels <b>(3)</b> and send these to human experts for an opinion. If needed to stay within our review budget, we prioritize pairs of examples that cover a larger area of our search space <b>(4)</b>. The resulting curated set is both informative (since it contains the most confusable examples along the decision boundary) and diverse (since it draws from different regions along that decision boundary).</p>
</div>

                    
                    
    




                    
                    
    


<div>
        
  <p data-block-key="n7jxf">These expert-provided labels are split randomly into two sets. The first is used for model evaluation, based on two key alignment metrics: the internal alignment measuring how much experts agree, and the model–human alignment between the current model and human experts. The second is used to fine-tune the current models, producing the next iteration of the model. The process repeats until the model–human alignment either matches the internal alignment or plateaus and cannot be improved further.</p>

    </div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="n7jxf">Metric</h2><p data-block-key="a9c7p">Our curation process does not assume the existence of ground truth. Many classification problems in the ads safety space, such as content moderation or fraud detection, are inherently ambiguous and require interpretation and deliberation, even among policy experts. We therefore cannot rely on standard metrics like precision and recall, which require a ground truth label. Instead we use <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa" target="_blank" rel="noopener noreferrer">Cohen’s Kappa</a>, a measure of how well two independent annotators align, above what would be expected from chance agreement. In our experiments, Cohen’s Kappa is used as both a quality indicator for datasets (including model evaluation during the curation process, as noted above); and as a measure of model performance. Values closer to 1 show higher alignment, 0 indicates no alignment above chance, and negative values indicate systematic disagreement. While standards for interpreting these scores vary, Kappa values above .8 are widely considered to be exceptionally good, and values above .4 are generally considered acceptable.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="n7jxf">Experiments</h2><p data-block-key="9ljdb">We wanted to understand which models and tasks would benefit most from our curation process. As <i>baselines</i> for our experiments, we fine-tuned two LLMs of different sizes (<a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf" target="_blank" rel="noopener noreferrer">Gemini</a> Nano-1 with 1.8B parameters and Nano-2 with 3.25B parameters) on two tasks of different complexity (lower and higher, based on expert alignment) using crowdsourced labels. Each crowdsourced data set has ~100K annotations and a strong class imbalance, with around 95% benign labels on average.</p><p data-block-key="1v6c">We compared each of these four baseline conditions against the corresponding <i>curated</i> condition in which each model (Nano-1 and Nano-2) is fine-tuned over multiple rounds using the curation process described above. At each iteration, we selected our curated set of examples and used them for model evaluation and fine-tuning, as described above. All models plateaued before reaching parity with the experts’ internal alignment, so we stopped at 6 iterations (~400 fine-tuning and ~250 evaluation samples) for the lower complexity task and 5 iterations (~250 fine-tuning and ~150 evaluation samples) for the higher complexity task. (Note that the lower complexity task had a larger variety of examples, which may account for the longer time needed to converge.) Both data sets had a final class balance of ~40% positive examples.</p><p data-block-key="fcsdi">The table below provides an overview of the scale and quality of the data used in each condition. Experts reached an average pairwise Cohen’s Kappa of .81 (on the lower complexity task) and .78 (on the higher complexity task) through the curation process. We consider these the ceiling for model performance. To assess the quality of our crowdsourced data, we calculated Kappa alignment between crowdsourced annotations and experts based on our full curated set, which was .59 (lower complexity) and .41 (higher complexity).</p>
</div>

                    
                    
    




                    
                    
    


<div>
        
  <p data-block-key="n7jxf">Below we show how models trained on these vastly different data sets performed in each of our baseline and curated conditions. The 1.8B parameter model saw comparable performance on both tasks: the baseline and curated models had .24 and .25 alignment, respectively, for the lower complexity task, and both models had .13 alignment on the higher complexity task. By contrast, the 3.25B parameter model showed significant quality improvements when trained with our curation process. Kappa scores for the baseline and curated models were .36 and .56, respectively, for the lower complexity task; and .23 and .38, respectively, for the higher complexity task — an improvement in alignment of 55-65% using three orders of magnitude less data (250 to 450 examples, compared to 100K in the baseline condition).</p>

    </div>

                    
                    
    




                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="n7jxf">These results demonstrate that careful curation of LLM datasets to focus on fewer, more informative examples can yield better or equivalent classifier performance using much less data — three orders of magnitude less in the experiments reported here, and up to four orders of magnitude less for the larger models used in production. Of course, these gains require not only good curation but also very high quality data. For our use cases, we have observed that a label quality above .8 pairwise Cohen’s Kappa is needed to reliably outperform crowdsourced data. Consistently achieving this level of quality poses a separate challenge, to be discussed in a subsequent blog post.</p><p data-block-key="e0dkc">But given sufficient label quality, our curation process is able to leverage the strengths of both LLMs, which can cast a wide net over the problem space, and domain experts, who can focus more efficiently on the most challenging examples. The ability to retrain models with just a handful of examples is especially valuable for handling the rapidly changing landscapes of domains like ads safety. We believe the approach we’ve described will enable systems that can make more flexible, efficient use of high-fidelity labels to escape the data bottleneck.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="n7jxf">Acknowledgements</h2><p data-block-key="1bgbo"><i>This work would not have been possible without our outstanding team of engineers and product managers. Steve Walker is a co-founder of our project and co-creator of the curation process as well as the tech lead for the machine learning infrastructure of our project. Kelsie McElroy is the project manager and a co-founder of our project. We also want to thank the Ads Privacy and Safety leadership team for their continued support and belief in our vision.</i></p>
</div>

                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flipper Zero DarkWeb Firmware Bypasses Rolling Code Security (293 pts)]]></title>
            <link>https://www.rtl-sdr.com/flipperzero-darkweb-firmware-bypasses-rolling-code-security/</link>
            <guid>44830408</guid>
            <pubDate>Thu, 07 Aug 2025 21:10:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rtl-sdr.com/flipperzero-darkweb-firmware-bypasses-rolling-code-security/">https://www.rtl-sdr.com/flipperzero-darkweb-firmware-bypasses-rolling-code-security/</a>, See on <a href="https://news.ycombinator.com/item?id=44830408">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="site" role="main">
			
	
			
		<article id="post-64439">
			<header>
								
			  				  <time datetime="2025-08-07T02:47:03+00:00" pubdate="">August 7, 2025</time>
								
									
							</header>
		
			<div>
				<p>Over on YouTube Talking Sasquach has recently tested custom firmware for the Flipper Zero that can entirely break the rolling code security system used on most modern vehicles. Rolling code security works by using&nbsp;a synchronized algorithm between a transmitter and receiver to generate a new, unique code for each transmission, preventing replay attacks and unauthorized access.</p>
<p>In the past we've discussed an attack against rolling code security systems called <a href="https://www.rtl-sdr.com/?s=rolljam&amp;apbct__email_id__search_form_47564=47564" target="_blank" rel="noopener">RollJam</a>, which works by jamming the original keyfob signal so the vehicle cannot receive it, and at the same time recording it for later use. However, this attack is difficult to perform in reality.</p>
<p>For this new attack to work, all that is needed is a single button-press capture from the keyfob, without any jamming. Just from that single capture, it is able to emulate all the keyfob's functions, including lock, unlock, and unlock trunk. A consequence of this is that the original keyfob gets out of sync, and will no longer function.</p>
<p>According to the Talking Sasquatch, the attack works by simply reverse engineering the rolling code sequence, either through sequence leaks or prior brute forcing of the sequence from a large list of known codes. However, <a href="https://san.com/cc/millions-of-cars-at-risk-from-flipper-zero-key-fob-hack-experts-warn/" target="_blank" rel="noopener">another article</a> mentions that the firmware is based on the "<a href="https://arxiv.org/abs/2210.11923" target="_blank" rel="noopener">RollBack</a>" attack, which works by playing back captured rolling codes in a specific order to initiate a 'rollback' of the synchronization system.</p>
<p>Regardless of the method, videos demonstrating the attack show that only a single capture is needed to emulate a keyfob completely.</p>
<p>Affected vehicles include Chrysler, Dodge, Fiat, Ford, Hyundai, Jeep, Kia, Mitsubishi and Subaru. As of yet, there appears to be no easy fix for this, other than mass vehicle recalls.</p>
<div id="WYL_wk7BGMkuI8A" data-src="https://www.rtl-sdr.com/wp-content/plugins/wp-youtube-lyte/lyteCache.php?origThumbUrl=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fwk7BGMkuI8A%2Fmaxresdefault.jpg" title="Flipper Zero DarkWeb Firmware Copies My Key Fob! I&amp;#039;ll Explain How this Works!"><p>Flipper Zero DarkWeb Firmware Copies My Key Fob! I'll Explain How this Works!</p></div>
					
					
			</div>
			<!-- / .content -->
			
			
		</article>
		<!-- / #post-64439 -->

		    
    
    
    
    		
				  
			<!-- / .post-nav -->
				
		



		</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cursor CLI (259 pts)]]></title>
            <link>https://cursor.com/cli</link>
            <guid>44830221</guid>
            <pubDate>Thu, 07 Aug 2025 20:53:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cursor.com/cli">https://cursor.com/cli</a>, See on <a href="https://news.ycombinator.com/item?id=44830221">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main"><section aria-label="Marketing highlights"><div><p>Use it in your IDE or any terminal.</p><p>Same commands, any environment. Plug into your setup anywhere.</p></div><div><h2>Full control from your terminal.</h2><div><div><ul><li role="button" tabindex="0"><p>Review agent edits</p><p>Make code changes directly in the terminal.</p></li><li role="button" tabindex="0"><p>Steer in real-time</p><p>Guide the agent as it works.</p></li><li role="button" tabindex="0"><p>Set your own rules</p><p>Customize Cursor's work with rules, AGENTS.md, and MCP.</p></li></ul></div><div aria-live="polite"><p><span>1/2 src/components/Canvas3D.tsx</span><span><span>+8</span><span>-2</span></span></p><div aria-label="Code diff for Canvas3D.tsx"><p>1</p><div><p><span>import</span></p><!-- --><p>{</p><!-- --><p> Suspense </p><!-- --><p>}</p><!-- --> <!-- --><p>from <span>'react'</span>;</p></div><p>2</p><div><p><span>import</span></p><!-- --><p>{</p><!-- --><p> Canvas </p><!-- --><p>}</p><!-- --> <!-- --><p>from <span>'@react-three/fiber'</span>;</p></div><p>3</p><div><p><span>import</span> as THREE from</p><!-- --> <p><span>'three'</span>;</p></div><p>4</p><div><p><span>import</span></p><!-- --><p>{</p><!-- --><p> Leva </p><!-- --><p>}</p><!-- --><p> from</p><!-- --> <p><span>'leva'</span>;</p></div><p>5</p><p>import GameScene from '../scenes/GameScene';</p><p>18</p><p>right-click</p><p>19</p><p>gl<span>=</span>{{</p><p>20</p><p>antialias: true,</p><p>20</p><p>antialias: false,</p><p>21</p><p> alpha: false,</p><p>22</p><p> stencil: false,</p><p>23</p><p> depth: true,</p><p>24</p><div> <!-- --><p>powerPreference:</p><!-- --> <p><span>'high-performance'</span>,</p></div></div></div></div></div><section aria-label="Additional highlights"><h3>Built for your workflow</h3><div><div><p>Always use the latest model</p><p>Every cutting edge model from Anthropic, OpenAI, Gemini, and more at your fingertips.</p></div><div><p>Use in your preferred IDE</p><p>Wherever you code, Cursor CLI integrates with your existing workflow.</p></div><div><p>Write powerful scripts and automations</p><p>Automatically update docs, trigger security reviews, or build custom coding agents.</p></div></div></section></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Historical Tech Tree (380 pts)]]></title>
            <link>https://www.historicaltechtree.com/</link>
            <guid>44829185</guid>
            <pubDate>Thu, 07 Aug 2025 19:24:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.historicaltechtree.com/">https://www.historicaltechtree.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44829185">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI's new open-source model is basically Phi-5 (281 pts)]]></title>
            <link>https://www.seangoedecke.com/gpt-oss-is-phi-5/</link>
            <guid>44828884</guid>
            <pubDate>Thu, 07 Aug 2025 18:59:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seangoedecke.com/gpt-oss-is-phi-5/">https://www.seangoedecke.com/gpt-oss-is-phi-5/</a>, See on <a href="https://news.ycombinator.com/item?id=44828884">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header></header><section><p>OpenAI just released its first ever open-source<sup id="fnref-1"><a href="#fn-1">1</a></sup> large language models, called gpt-oss-120b and gpt-oss-20b. You can talk to them <a href="https://gpt-oss.com/">here</a>. Are they good models? Well, that depends on what you’re looking for. They’re great at <a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf">some benchmarks</a>, of course (OpenAI would never have released them otherwise) but weirdly bad at others, like SimpleQA.</p>
<p>Some people <a href="https://simonwillison.net/2025/Aug/5/gpt-oss/">really like them</a>. Others on Twitter <a href="https://x.com/corbtt/status/1952868822891012241">really</a> <a href="https://x.com/vikhyatk/status/1952863413845275132">don’t</a>. From what I can tell, they’re technically competent but lack a lot of out-of-domain knowledge: for instance, they have broad general knowledge about science, but don’t know much about popular culture. We’ll know in six months how useful these models are in practice, but my prediction is that these models will end up in the category of “performs much better on benchmarks than on real-world tasks”.</p>
<h3>Phi models and training on synthetic data</h3>
<p>In 2024, Sebastien Bubeck led the development of Microsoft’s open-source Phi-series of models<sup id="fnref-2"><a href="#fn-2">2</a></sup>. The big idea behind those models was to train exclusively on synthetic data: instead of text pulled from books or the internet, text generated by other language models or hand-curated textbooks. Synthetic data is less common than normal data, since instead of just downloading terabytes of it for free you have to spend money to generate each token. But the trade-off is that you have complete control over your training data. What happens when you train a model on entirely high-quality synthetic and curated data?</p>
<p>As it turns out, it does very well on model benchmarks but disappoints in practice. Searching for the reception to each Phi model shows the same pattern: very impressive <a href="https://arxiv.org/abs/2404.14219">benchmarks</a>, lots of enthusiasm, and then actual performance <a href="https://news.ycombinator.com/item?id=40128351">far weaker</a> than the benchmarks would suggest.</p>
<p>I think the impressive benchmark results come from the fact that these models are very easy to train for specific tasks, because you generate much of the training data yourself. If you’re training on synthetic data, you’d be foolish not to generate some synthetic data that matches the kind of problems people are benchmarking on. But since you’re “teaching for the test”, you should expect to do worse than other language models who are training on broad data and end up being good at the benchmarks by accident.</p>
<p>Why am I talking about Phi models? At the end of 2024, Sebastien Bubeck <a href="https://www.reuters.com/technology/microsofts-vp-genai-research-join-openai-2024-10-14/">left Microsoft</a> to join OpenAI. We don’t know who was involved in making the new OpenAI <code>gpt-oss</code> models. The <a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf">model card</a> doesn’t provide much detail about the pretraining stage. However, I’d bet that Sebastien Bubeck was a part of the effort, and that these models were trained on a <em>heavily</em> filtered or synthetic dataset.</p>
<h3>Synthetic data is safer</h3>
<p>Why would OpenAI train Phi-style models, knowing that they’ll perform better on benchmarks than in real-world applications? For the same reason that Microsoft probably continued to train Phi-style models: safety. Releasing an open-source model is terrifying for a large organization. Once it’s out there, your name is associated with it forever, and thousands of researchers will be frantically trying to fine-tune it to remove the safety guardrails. </p>
<p>It’s not discussed publically very often, but the main use-case for fine-tuning small language models is for erotic role-play, and there’s a serious demand. Any small online community for people who run local models is at least 50% perverts.</p>
<p>If you release a regular closed-weights model that stays in your own infrastructure, people can’t fine-tune it. If you make a mistake, you can always update the model in-place. But open-source models are out there forever.</p>
<p>Training on synthetic data (or highly-controlled data such as textbooks) makes it much easier to produce a safe model. You can produce as much “you asked me to do X, but as a sensible language model I am declining to do so” content as you like. If there’s no subversive or nasty content in the training data, the model never learns to behave in subversive or nasty ways (at least, that’s the goal).</p>
<p>For OpenAI, it must have been very compelling to train a Phi-style model for their open-source release. They needed a model that beat the Chinese open-source models on benchmarks, while also not misbehaving in a way that caused <a href="https://www.seangoedecke.com/ai-sycophancy">yet another</a> scandal for them. Unlike Meta, they don’t need their open-source model to be <em>actually</em> good, because their main business is in their closed-source models.</p>
<p>That’s why I think OpenAI went down the synthetic data route for their new <code>gpt-oss</code> models. For good or ill, they may as well be Phi-5 and Phi-5-mini.</p>
</section><p>If you liked this post, consider <a href="https://buttondown.com/seangoedecke" target="_blank">subscribing</a> to email updates about my new posts, or <a href="https://news.ycombinator.com/submitlink?u=https://www.seangoedecke.com/gpt-oss-is-phi-5/" target="_blank">sharing it on Hacker News</a>.</p><p>August 7, 2025<!-- -->&nbsp;│ Tags: <a href="https://www.seangoedecke.com/tags/ai/">ai</a></p><hr></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Encryption made for police and military radios may be easily cracked (175 pts)]]></title>
            <link>https://www.wired.com/story/encryption-made-for-police-and-military-radios-may-be-easily-cracked-researchers-find/</link>
            <guid>44828504</guid>
            <pubDate>Thu, 07 Aug 2025 18:30:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/encryption-made-for-police-and-military-radios-may-be-easily-cracked-researchers-find/">https://www.wired.com/story/encryption-made-for-police-and-military-radios-may-be-easily-cracked-researchers-find/</a>, See on <a href="https://news.ycombinator.com/item?id=44828504">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>Two years ago,</span> researchers in the Netherlands <a href="https://www.wired.com/story/tetra-radio-encryption-backdoor/">discovered an intentional backdoor</a> in an encryption algorithm baked into radios used by critical infrastructure–as well as police, intelligence agencies, and military forces around the world–that made any communication secured with the algorithm vulnerable to eavesdropping.</p><p>When the researchers publicly disclosed the issue in 2023, the European Telecommunications Standards Institute (ETSI), which developed the algorithm, advised anyone using it for sensitive communication to deploy an end-to-end encryption solution on top of the flawed algorithm to bolster the security of their communications.</p><p>But now the same researchers have found that at least one implementation of the end-to-end encryption solution endorsed by ETSI has a similar issue that makes it equally vulnerable to eavesdropping. The encryption algorithm used for the device they examined starts with a 128-bit key, but this gets compressed to 56 bits before it encrypts traffic, making it easier to crack. It’s not clear who is using this implementation of the end-to-end encryption algorithm, nor if anyone using devices with the end-to-end encryption is aware of the security vulnerability in them.</p><p>The end-to-end encryption the researchers examined, which is expensive to deploy, is most commonly used in radios for law enforcement agencies, special forces, and covert military and intelligence teams that are involved in national security work and therefore need an extra layer of security. But ETSI’s endorsement of the algorithm two years ago to mitigate flaws found in its lower-level encryption algorithm suggests it may be used more widely now than at the time.</p><p>In 2023, Carlo Meijer, Wouter Bokslag, and Jos Wetzels of security firm <a data-offer-url="https://www.midnightblue.nl/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.midnightblue.nl/&quot;}" href="https://www.midnightblue.nl/" rel="nofollow noopener" target="_blank">Midnight Blue</a>, based in the Netherlands, discovered vulnerabilities in encryption algorithms that are part of a European radio standard created by ETSI called TETRA (Terrestrial Trunked Radio), which has been baked into radio systems made by Motorola, Damm, Sepura, and others since the ’90s. The flaws remained unknown publicly until their disclosure, because ETSI refused for decades to let anyone examine the proprietary algorithms. The end-to-end encryption the researchers examined recently is designed to run on top of TETRA encryption algorithms.</p><p>The researchers found the issue with the end-to-end encryption (E2EE) only after extracting and reverse-engineering the E2EE algorithm used in a radio made by Sepura. The researchers plan to present their findings today at the BlackHat security conference in Las Vegas.</p><p>ETSI, when contacted about the issue, noted that the end-to-end encryption used with TETRA-based radios is not part of the ETSI standard, nor was it created by the organization. Instead it was produced by The Critical Communications Association’s (TCCA) security and fraud prevention group (SFPG). But ETSI and TCCA work closely with one another, and the two organizations include many of the same people. Brian Murgatroyd, former chair of the technical body at ETSI responsible for the TETRA standard as well as the TCCA group that developed the E2EE solution, wrote in an email on behalf of ETSI and the TCCA that end-to-end encryption was not included in the ETSI standard “because at the time it was considered that E2EE would only be used by government groups where national security concerns were involved, and these groups often have special security needs.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>For this reason, Murgatroyd noted that purchasers of TETRA-based radios are free to deploy other solutions for end-to-end encryption on their radios, but he acknowledges that the one produced by the TCCA and endorsed by ETSI “is widely used as far as we can tell.”</p><p>Although TETRA-based radio devices are not used by police and military in the US, the majority of police forces around the world do use them. These include police forces in Belgium and Scandinavian countries, as well as East European countries like Serbia, Moldova, Bulgaria, and Macedonia, and in the Middle East in Iran, Iraq, Lebanon, and Syria. The Ministries of Defense in Bulgaria, Kazakhstan, and Syria also use them, as do the Polish military counterintelligence agency, the Finnish defense forces, and Lebanon and Saudi Arabia’s intelligence services. It’s not clear, however, how many of these also deploy end-to-end decryption with their radios.</p><p>The TETRA standard includes four encryption algorithms—TEA1, TEA2, TEA3 and TEA4—that can be used by radio manufacturers in different products, depending on the intended customer and usage. The algorithms have different levels of security based on whether the radios will be sold in or outside Europe. TEA2, for example, is restricted for use in radios used by police, emergency services, military, and intelligence agencies in Europe. TEA3 is available for police and emergency services radios used outside Europe but only in countries deemed “friendly” to the EU. Only TEA1 is available for radios used by public safety agencies, police agencies, and militaries in countries deemed not friendly to Europe, such as Iran. But it’s also used in critical infrastructure in the US and other countries for machine-to-machine communication in industrial control settings such as pipelines, railways, and electric grids.</p><p>All four TETRA encryption algorithms use 80-bit keys to secure communication. But the Dutch researchers revealed in 2023 that TEA1 has a feature that causes its key to get reduced to just 32 bits, which allowed the researchers to crack it in less than a minute.</p><p>In the case of the E2EE, the researchers found that the implementation they examined starts with a key that is more secure than ones used in the TETRA algorithms, but it gets reduced to 56 bits, which would potentially let someone decrypt voice and data communications. They also found a second vulnerability that would let someone send fraudulent messages or replay legitimate ones to spread misinformation or confusion to personnel using the radios.</p><p>The ability to inject voice traffic and replay messages affects all users of the TCCA end-to-end encryption scheme, according to the researchers. They say this is the result of flaws in the TCCA E2EE protocol design rather than a particular implementation. They also say that “law enforcement end users” have confirmed to them that this flaw is in radios produced by vendors other than Sepura.</p><p>But the researchers say only a subset of end-to-end encryption users are likely affected by the reduced-key vulnerability because it depends how the encryption was implemented in radios sold to various countries.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>ETSI’s Murgatroyd <a data-offer-url="https://www.zetter-zeroday.com/interview-with-the-etsi-standards/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.zetter-zeroday.com/interview-with-the-etsi-standards/&quot;}" href="https://www.zetter-zeroday.com/interview-with-the-etsi-standards/" rel="nofollow noopener" target="_blank">said in 2023</a> that the TEA1 key was reduced to meet export controls for encryption sold to customers outside Europe. He said when the algorithm was created, a key with 32 bits of entropy was considered secure for most uses. Advances in computing power make it less secure now, so when the Dutch researchers exposed the reduced key two years ago, ETSI recommended that customers using TEA1 deploy TCCA's end-to-end encryption solution on top of it.</p><p>But Murgatroyd said the end-to-end encryption algorithm designed by TCCA is different. It doesn’t specify the key length the radios should use because governments using the end-to-end encryption have their own “specific and often proprietary security rules” for the devices they use. Therefore they are able to customize the TCCA encryption algorithm in their devices by working with their radio supplier to select the “encryption algorithm, key management and so on” that is right for them—but only to a degree.</p><p>“The choice of encryption algorithm and key is made between supplier and customer organisation, and ETSI has no input to this selection—nor knowledge of which algorithms and key lengths are in use in any system,” he said. But he added that radio manufacturers and customers “will always have to abide by export control regulations.”</p><p>The researchers say they cannot verify that the TCCA E2EE doesn’t specify a key length because the TCCA documentation describing the solution is protected by nondisclosure agreement and provided only to radio vendors. But they note that the E2EE system calls out an “algorithm identifier" number, which means it calls out the specific algorithm it’s using for the end-to-end encryption. These identifiers are not vendor specific, the researchers say, which suggests the identifiers refer to different key variants produced by TCCA—meaning TCCA provides specifications for algorithms that use a 126 bit key or 56 bit key, and radio vendors can configure their devices to use either of these variants, depending on the export controls in place for the purchasing country.</p><p>Whether users know their radios could have this vulnerability is unclear. The researchers found a confidential 2006 Sepura product bulletin that <a href="https://www.scribd.com/document/237610110/Issue2-MOD-05-166-Crypto-Management-Tools">someone leaked online</a>, which mentions that “the length of the traffic key … is subject to export control regulations and hence the [encryption system in the device] will be factory configured to support 128, 64, or 56 bit key lengths.” But it’s not clear what Sepura customers receive or if other manufacturers whose radios use a reduced key disclose to customers if their radios use a reduced-key algorithm.</p><p>“Some manufacturers have this in brochures; others only mention this in internal communications, and others don’t mention it at all,” says Wetzels. He says they did extensive open-source research to examine vendor documentation and “ found no clear sign of weakening being communicated to end users. So while … there are ‘some’ mentions of the algorithm being weakened, it is not fully transparent at all.”</p><p>Sepura did not respond to an inquiry from WIRED.</p><p>But Murgatroyd says that because government customers who have opted to use TCCA’s E2EE solution need to know the security of their devices, they are likely to be aware if their systems are using a reduced key.</p><p>“As end-to-end encryption is primarily used for government communications, we would expect that the relevant government National Security agencies are fully aware of the capabilities of their end-to-end encryption systems and can advise their users appropriately,” Murgatroyd wrote in his email.</p><p>Wetzels is skeptical of this, however. “We consider it highly unlikely non-Western governments are willing to spend literally millions of dollars if they know they're only getting 56 bits of security,” he says.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exit Tax: Leave Germany before your business gets big (170 pts)]]></title>
            <link>https://eidel.io/exit-tax-leave-germany-before-your-business-gets-big/</link>
            <guid>44828158</guid>
            <pubDate>Thu, 07 Aug 2025 18:06:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eidel.io/exit-tax-leave-germany-before-your-business-gets-big/">https://eidel.io/exit-tax-leave-germany-before-your-business-gets-big/</a>, See on <a href="https://news.ycombinator.com/item?id=44828158">Hacker News</a></p>
Couldn't get https://eidel.io/exit-tax-leave-germany-before-your-business-gets-big/: AggregateError]]></description>
        </item>
        <item>
            <title><![CDATA[Benchmark Framework Desktop Mainboard and 4-node cluster (162 pts)]]></title>
            <link>https://github.com/geerlingguy/ollama-benchmark/issues/21</link>
            <guid>44827862</guid>
            <pubDate>Thu, 07 Aug 2025 17:49:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/geerlingguy/ollama-benchmark/issues/21">https://github.com/geerlingguy/ollama-benchmark/issues/21</a>, See on <a href="https://news.ycombinator.com/item?id=44827862">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><p dir="auto">Testing the <a href="https://frame.work/desktop" rel="nofollow">Framework Desktop</a> - AMD Ryzen AI Max+ 395 with Radeon 8090S. (Four pre-production units were shipped to me for local cluster testing).</p>
<h2 dir="auto">Single Node configuration (128 GB RAM):</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/481677/475718422-78791530-a491-463b-8c8e-6b94883b2b19.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ2MDI1MDIsIm5iZiI6MTc1NDYwMjIwMiwicGF0aCI6Ii80ODE2NzcvNDc1NzE4NDIyLTc4NzkxNTMwLWE0OTEtNDYzYi04YzhlLTZiOTQ4ODNiMmIxOS5qcGVnP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDdUMjEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YTdmNzNlMGRlN2M5MmYzZGEzNzZhOWI1ZmY5YjdkNmVkMWY4YWYyOTJjY2M5ZTAyYzA3MTYzN2FiNTQ3ZjU2ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.NrogbtFc_AN6neMIMlFkEZxdrq03GhOdZvW2ofvWeXg"><img src="https://private-user-images.githubusercontent.com/481677/475718422-78791530-a491-463b-8c8e-6b94883b2b19.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ2MDI1MDIsIm5iZiI6MTc1NDYwMjIwMiwicGF0aCI6Ii80ODE2NzcvNDc1NzE4NDIyLTc4NzkxNTMwLWE0OTEtNDYzYi04YzhlLTZiOTQ4ODNiMmIxOS5qcGVnP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDdUMjEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YTdmNzNlMGRlN2M5MmYzZGEzNzZhOWI1ZmY5YjdkNmVkMWY4YWYyOTJjY2M5ZTAyYzA3MTYzN2FiNTQ3ZjU2ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.NrogbtFc_AN6neMIMlFkEZxdrq03GhOdZvW2ofvWeXg" alt="Image"></a></p>
<h2 dir="auto">Cluster configuration (4x Mainboard - 512 GB RAM)</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/481677/475719431-140425ff-2110-4301-9e92-dc2807b3c277.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ2MDI1MDIsIm5iZiI6MTc1NDYwMjIwMiwicGF0aCI6Ii80ODE2NzcvNDc1NzE5NDMxLTE0MDQyNWZmLTIxMTAtNDMwMS05ZTkyLWRjMjgwN2IzYzI3Ny5qcGc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwODA3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDgwN1QyMTMwMDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0yY2ZkMzcyNDk0NjgwYTYxYTNlNmM1ZmIwYzZmNzZjN2IwMjg3NmI4ZDRmYzA2MDIzZDZmNzNjZWM2YWRlMjgxJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.bvcCvKOWXhYfWrtuFsAn0asOUaw_tnxHHDZpx5F9pRQ"><img src="https://private-user-images.githubusercontent.com/481677/475719431-140425ff-2110-4301-9e92-dc2807b3c277.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ2MDI1MDIsIm5iZiI6MTc1NDYwMjIwMiwicGF0aCI6Ii80ODE2NzcvNDc1NzE5NDMxLTE0MDQyNWZmLTIxMTAtNDMwMS05ZTkyLWRjMjgwN2IzYzI3Ny5qcGc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwODA3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDgwN1QyMTMwMDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0yY2ZkMzcyNDk0NjgwYTYxYTNlNmM1ZmIwYzZmNzZjN2IwMjg3NmI4ZDRmYzA2MDIzZDZmNzNjZWM2YWRlMjgxJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.bvcCvKOWXhYfWrtuFsAn0asOUaw_tnxHHDZpx5F9pRQ" alt="Image"></a></p>
<p dir="auto">Initial tests (above) were run using 2.5 Gbps Ethernet interconnect.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/481677/475719725-da027dbf-864c-47d2-ad87-2917285de6c9.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ2MDI1MDIsIm5iZiI6MTc1NDYwMjIwMiwicGF0aCI6Ii80ODE2NzcvNDc1NzE5NzI1LWRhMDI3ZGJmLTg2NGMtNDdkMi1hZDg3LTI5MTcyODVkZTZjOS5qcGVnP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDdUMjEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MWU5ZjUzNGJmMGZlMzNjZmRmZmNmYzdlOGVhYTc1MGZhNzVjNDlmNjgwYTJiYWNhMTBlNTc2YmJhZTNjZWJlOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.1BO9Fncu1POYdXUNIT2rRTRJimpRGMeumaevUIA-8NU"><img src="https://private-user-images.githubusercontent.com/481677/475719725-da027dbf-864c-47d2-ad87-2917285de6c9.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ2MDI1MDIsIm5iZiI6MTc1NDYwMjIwMiwicGF0aCI6Ii80ODE2NzcvNDc1NzE5NzI1LWRhMDI3ZGJmLTg2NGMtNDdkMi1hZDg3LTI5MTcyODVkZTZjOS5qcGVnP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDdUMjEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MWU5ZjUzNGJmMGZlMzNjZmRmZmNmYzdlOGVhYTc1MGZhNzVjNDlmNjgwYTJiYWNhMTBlNTc2YmJhZTNjZWJlOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.1BO9Fncu1POYdXUNIT2rRTRJimpRGMeumaevUIA-8NU" alt="Image"></a></p>
<p dir="auto">I later changed to 5 Gbps using a NICGIGA switch, and racked it in a black T1 mini rack shipped by DeskPi, along with four of their currently-in-prototype Framework Desktop mini rack trays. <a href="https://github.com/geerlingguy/mini-rack/issues/234" data-hovercard-type="issue" data-hovercard-url="/geerlingguy/mini-rack/issues/234/hovercard">Mini rack build showcase here</a>.</p>
<p dir="auto">I also tested Thunderbolt node-to-node interconnects, but could only get 10 Gbps over TB4 using <code>thunderbolt0</code>/<code>thunderbolt1</code> interfaces).</p>
<p dir="auto">For more benchmarks (focusing on CPU, GPU, disk, net, etc.), see:</p>
<ul dir="auto">
<li><a href="https://www.jeffgeerling.com/blog/2025/i-clustered-four-framework-mainboards-test-huge-llms" rel="nofollow">I clustered four Framework Mainboards to test huge LLMs</a></li>
<li><a href="https://github.com/geerlingguy/sbc-reviews/issues/80" data-hovercard-type="issue" data-hovercard-url="/geerlingguy/sbc-reviews/issues/80/hovercard">sbc-reviews: Framework Desktop</a>.</li>
</ul>
<p dir="auto">All my automation for testing is in the <a href="https://github.com/geerlingguy/beowulf-ai-cluster">Beowulf AI Cluster</a> repo.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5: Key characteristics, pricing and system card (530 pts)]]></title>
            <link>https://simonwillison.net/2025/Aug/7/gpt-5/</link>
            <guid>44827794</guid>
            <pubDate>Thu, 07 Aug 2025 17:46:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Aug/7/gpt-5/">https://simonwillison.net/2025/Aug/7/gpt-5/</a>, See on <a href="https://news.ycombinator.com/item?id=44827794">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2025/Aug/7/gpt-5/">

<p>7th August 2025</p>



<p>I’ve had preview access to the new GPT-5 model family for the past two weeks (see <a href="https://simonwillison.net/2025/Aug/7/previewing-gpt-5/">related video</a>) and have been using GPT-5 as my daily-driver. It’s my new favorite model. It’s still an LLM—it’s not a dramatic departure from what we’ve had before—but it rarely screws up and generally feels competent or occasionally impressive at the kinds of things I like to use models for.</p>
<p>I’ve collected a lot of notes over the past two weeks, so I’ve decided to break them up into <a href="https://simonwillison.net/series/gpt-5/">a series of posts</a>. This first one will cover key characteristics of the models, how they are priced and what we can learn from the <a href="https://openai.com/index/gpt-5-system-card/">GPT-5 system card</a>.</p>
<ul>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#key-model-characteristics">Key model characteristics</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#position-in-the-openai-model-family">Position in the OpenAI model family</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#pricing-is-aggressively-competitive">Pricing is aggressively competitive</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#more-notes-from-the-system-card">More notes from the system card</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#prompt-injection-in-the-system-card">Prompt injection in the system card</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#thinking-traces-in-the-api">Thinking traces in the API</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#and-some-svgs-of-pelicans">And some SVGs of pelicans</a></li>
</ul>

<h4 id="key-model-characteristics">Key model characteristics</h4>
<p>Let’s start with the fundamentals. GPT-5 in ChatGPT is a weird hybrid that switches between different models. Here’s what the system card says about that (my highlights in bold):</p>
<blockquote>
<p>GPT-5 is a unified system with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and <strong>a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent</strong> (for example, if you say “think hard about this” in the prompt). [...] Once usage limits are reached, a mini version of each model handles remaining queries. In the near future, we plan to integrate these capabilities into a single model.</p>
</blockquote>
<p>GPT-5 in the API is simpler: it’s available as three models—<strong>regular</strong>, <strong>mini</strong> and <strong>nano</strong>—which can each be run at one of four reasoning levels: minimal (a new level not previously available for other OpenAI reasoning models), low, medium or high.</p>
<p>The models have an input limit of 272,000 tokens and an output limit (which includes invisible reasoning tokens) of 128,000 tokens. They support text and image for input, text only for output.</p>
<p>I’ve mainly explored full GPT-5. My verdict: it’s just <strong>good at stuff</strong>. It doesn’t feel like a dramatic leap ahead from other LLMs but it exudes competence—it rarely messes up, and frequently impresses me. I’ve found it to be a very sensible default for everything that I want to do. At no point have I found myself wanting to re-run a prompt against a different model to try and get a better result.</p>

<p>Here are the OpenAI model pages for <a href="https://platform.openai.com/docs/models/gpt-5">GPT-5</a>, <a href="https://platform.openai.com/docs/models/gpt-5-mini">GPT-5 mini</a> and <a href="https://platform.openai.com/docs/models/gpt-5-nano">GPT-5 nano</a>. Knowledge cut-off is September 30th 2024 for GPT-5 and May 30th 2024 for GPT-5 mini and nano.</p>

<h4 id="position-in-the-openai-model-family">Position in the OpenAI model family</h4>
<p>The three new GPT-5 models are clearly intended as a replacement for most of the rest of the OpenAI line-up. This table from the system card is useful, as it shows how they see the new models fitting in:</p>
<table>
<thead>
<tr>
<th>Previous model</th>
<th>GPT-5 model</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4o</td>
<td>gpt-5-main</td>
</tr>
<tr>
<td>GPT-4o-mini</td>
<td>gpt-5-main-mini</td>
</tr>
<tr>
<td>OpenAI o3</td>
<td>gpt-5-thinking</td>
</tr>
<tr>
<td>OpenAI o4-mini</td>
<td>gpt-5-thinking-mini</td>
</tr>
<tr>
<td>GPT-4.1-nano</td>
<td>gpt-5-thinking-nano</td>
</tr>
<tr>
<td>OpenAI o3 Pro</td>
<td>gpt-5-thinking-pro</td>
</tr>
</tbody>
</table>
<p>That “thinking-pro” model is currently only available via ChatGPT where it is labelled as “GPT-5 Pro” and limited to the $200/month tier. It uses “parallel test time compute”.</p>
<p>The only capabilities not covered by GPT-5 are audio input/output and image generation. Those remain covered by models like <a href="https://platform.openai.com/docs/models/gpt-4o-audio-preview">GPT-4o Audio</a> and <a href="https://platform.openai.com/docs/models/gpt-4o-realtime-preview">GPT-4o Realtime</a> and their mini variants and the <a href="https://platform.openai.com/docs/models/gpt-image-1">GPT Image 1</a> and DALL-E image generation models.</p>
<h4 id="pricing-is-aggressively-competitive">Pricing is aggressively competitive</h4>
<p>The pricing is <em>aggressively competitive</em> with other providers.</p>
<ul>
<li>GPT-5: $1.25/million for input, $10/million for output</li>
<li>GPT-5 Mini: $0.25/m input, $2.00/m output</li>
<li>GPT-5 Nano: $0.05/m input, $0.40/m output</li>
</ul>
<p>GPT-5 is priced at half the input cost of GPT-4o, and maintains the same price for output. Those invisible reasoning tokens count as output tokens so you can expect most prompts to use more output tokens than their GPT-4o equivalent (unless you set reasoning effort to “minimal”).</p>
<p>The discount for token caching is significant too: 90% off on input tokens that have been used within the previous few minutes. This is particularly material if you are implementing a chat UI where the same conversation gets replayed every time the user adds another prompt to the sequence.</p>
<p>Here’s a comparison table I put together showing the new models alongside the most comparable models from OpenAI’s competition:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Input $/m</th>
<th>Output $/m</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claude Opus 4.1</td>
<td>15.00</td>
<td>75.00</td>
</tr>
<tr>
<td>Claude Sonnet 4</td>
<td>3.00</td>
<td>15.00</td>
</tr>
<tr>
<td>Grok 4</td>
<td>3.00</td>
<td>15.00</td>
</tr>
<tr>
<td>Gemini 2.5 Pro (&gt;200,000)</td>
<td>2.50</td>
<td>15.00</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>2.50</td>
<td>10.00</td>
</tr>
<tr>
<td>GPT-4.1</td>
<td>2.00</td>
<td>8.00</td>
</tr>
<tr>
<td>o3</td>
<td>2.00</td>
<td>8.00</td>
</tr>
<tr>
<td>Gemini 2.5 Pro (&lt;200,000)</td>
<td>1.25</td>
<td>10.00</td>
</tr>
<tr>
<td><strong>GPT-5</strong></td>
<td>1.25</td>
<td>10.00</td>
</tr>
<tr>
<td>o4-mini</td>
<td>1.10</td>
<td>4.40</td>
</tr>
<tr>
<td>Claude 3.5 Haiku</td>
<td>0.80</td>
<td>4.00</td>
</tr>
<tr>
<td>GPT-4.1 mini</td>
<td>0.40</td>
<td>1.60</td>
</tr>
<tr>
<td>Gemini 2.5 Flash</td>
<td>0.30</td>
<td>2.50</td>
</tr>
<tr>
<td>Grok 3 Mini</td>
<td>0.30</td>
<td>0.50</td>
</tr>
<tr>
<td><strong>GPT-5 Mini</strong></td>
<td>0.25</td>
<td>2.00</td>
</tr>
<tr>
<td>GPT-4o mini</td>
<td>0.15</td>
<td>0.60</td>
</tr>
<tr>
<td>Gemini 2.5 Flash-Lite</td>
<td>0.10</td>
<td>0.40</td>
</tr>
<tr>
<td>GPT-4.1 Nano</td>
<td>0.10</td>
<td>0.40</td>
</tr>
<tr>
<td>Amazon Nova Lite</td>
<td>0.06</td>
<td>0.24</td>
</tr>
<tr>
<td><strong>GPT-5 Nano</strong></td>
<td>0.05</td>
<td>0.40</td>
</tr>
<tr>
<td>Amazon Nova Micro</td>
<td>0.035</td>
<td>0.14</td>
</tr>
</tbody>
</table>
<p>(Here’s a good example of a GPT-5 failure: I tried to get it to <a href="https://chatgpt.com/share/6894d804-bca4-8006-ac46-580bf4a9bf5f">output that table sorted itself</a> but it put Nova Micro as more expensive than GPT-5 Nano, so I prompted it to “construct the table in Python and sort it there” and that fixed the issue.)</p>
<h4 id="more-notes-from-the-system-card">More notes from the system card</h4>
<p>As usual, <a href="">the system card</a> is vague on what went into the training data. Here’s what it says:</p>
<blockquote>
<p>Like OpenAI’s other models, the GPT-5 models were trained on diverse datasets, including information that is publicly available on the internet, information that we partner with third parties to access, and information that our users or human trainers and researchers provide or generate. [...] We use advanced data filtering processes to reduce personal information from training data.</p>
</blockquote>
<p>I found this section interesting, as it reveals that writing, code and health are three of the most common use-cases for ChatGPT. This explains why so much effort went into health-related questions,  for both GPT-5 and the recently released OpenAI open weight models.</p>
<blockquote>
<p>We’ve made significant advances in <strong>reducing hallucinations, improving instruction following, and minimizing sycophancy</strong>, and have leveled up GPT-5’s performance in <strong>three of ChatGPT’s most common uses: writing, coding, and health</strong>. All of the GPT-5 models additionally feature <strong>safe-completions, our latest approach to safety training</strong> to prevent disallowed content.</p>
</blockquote>
<p>Safe-completions is later described like this:</p>
<blockquote>
<p>Large language models such as those powering ChatGPT have <strong>traditionally been trained to
either be as helpful as possible or outright refuse a user request</strong>, depending on whether the
prompt is allowed by safety policy. [...] Binary refusal boundaries are especially ill-suited for dual-use cases (such as biology
or cybersecurity), where a user request can be completed safely at a high level, but may lead
to malicious uplift if sufficiently detailed or actionable. <strong>As an alternative, we introduced safe-
completions: a safety-training approach that centers on the safety of the assistant’s output rather
than a binary classification of the user’s intent</strong>. Safe-completions seek to maximize helpfulness
subject to the safety policy’s constraints.</p>
</blockquote>
<p>So instead of straight up refusals, we should expect GPT-5 to still provide an answer but moderate that answer to avoid it including “harmful” content.</p>
<p>OpenAI have a paper about this which I haven’t read yet (I didn’t get early access): <a href="https://openai.com/index/gpt-5-safe-completions/">From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training</a>.</p>
<p>Sycophancy gets a mention, unsurprising given <a href="https://simonwillison.net/2025/May/2/what-we-missed-with-sycophancy/">their high profile disaster in April</a>. They’ve worked on this in the core model:</p>
<blockquote>
<p>System
prompts, while easy to modify, have a more limited impact on model outputs relative to changes in
post-training. For GPT-5, we post-trained our models to reduce sycophancy. Using conversations
representative of production data, we evaluated model responses, then assigned a score reflecting
the level of sycophancy, which was used as a reward signal in training.</p>
</blockquote>
<p>They claim impressive reductions in hallucinations. In my own usage I’ve not spotted a single hallucination yet, but that’s been true for me for Claude 4 and o3 recently as well—hallucination is so much less of a problem with this year’s models.</p>
<blockquote>
<p>One of our focuses when training the GPT-5 models was to reduce the frequency of factual
hallucinations. While ChatGPT has browsing enabled by default, many API queries do not use
browsing tools. Thus, we focused both on training our models to browse effectively for up-to-date
information, and on reducing hallucinations when the models are relying on their own internal
knowledge.</p>
</blockquote>
<p>The section about deception also incorporates the thing where models sometimes pretend they’ve completed a task that defeated them:</p>
<blockquote>
<p>We placed gpt-5-thinking in a variety of tasks that were partly or entirely infeasible to accomplish,
and <strong>rewarded the model for honestly admitting it can not complete the task</strong>. [...]</p>
<p>In tasks where the agent is required to use tools, such as a web browsing
tool, in order to answer a user’s query, previous models would hallucinate information when
the tool was unreliable. We simulate this scenario by purposefully disabling the tools or by
making them return error codes.</p>
</blockquote>
<h4 id="prompt-injection-in-the-system-card">Prompt injection in the system card</h4>
<p>There’s a section about prompt injection, but it’s pretty weak sauce in my opinion.</p>
<blockquote>
<p>Two external red-teaming groups conducted a two-week prompt-injection assessment targeting
system-level vulnerabilities across ChatGPT’s connectors and mitigations, rather than model-only
behavior.</p>
</blockquote>
<p>Here’s their chart showing how well the model scores against the rest of the field. It’s an impressive result in comparison—56.8 attack success rate for gpt-5-thinking, where Claude 3.7 scores in the 60s (no Claude 4 results included here) and everything else is 70% plus:</p>
<p><img src="https://static.simonwillison.net/static/2025/prompt-injection-chart.jpg" alt="A bar chart titled &quot;Behavior Attack Success Rate at k Queries&quot; shows attack success rates (in %) for various AI models at k=1 (dark red) and k=10 (light red). For each model, the total height of the stacked bar represents the k=10 success rate (labeled above each bar), while the lower dark red section represents the k=1 success rate (estimated). From left to right: Llama 3.3 70B – k=10: 92.2%, k=1: ~47%; Llama 3.1 405B – k=10: 90.9%, k=1: ~38%; Gemini Flash 1.5 – k=10: 87.7%, k=1: ~34%; GPT-4o – k=10: 86.4%, k=1: ~28%; OpenAI o3-mini-high – k=10: 86.4%, k=1: ~41%; Gemini Pro 1.5 – k=10: 85.5%, k=1: ~34%; Gemini 2.5 Pro Preview – k=10: 85.0%, k=1: ~28%; Gemini 2.0 Flash – k=10: 85.0%, k=1: ~33%; OpenAI o3-mini – k=10: 84.5%, k=1: ~40%; Grok 2 – k=10: 82.7%, k=1: ~34%; GPT-4.5 – k=10: 80.5%, k=1: ~28%; 3.5 Haiku – k=10: 76.4%, k=1: ~17%; Command-R – k=10: 76.4%, k=1: ~28%; OpenAI o4-mini – k=10: 75.5%, k=1: ~17%; 3.5 Sonnet – k=10: 75.0%, k=1: ~13%; OpenAI o1 – k=10: 71.8%, k=1: ~18%; 3.7 Sonnet – k=10: 64.5%, k=1: ~17%; 3.7 Sonnet: Thinking – k=10: 63.6%, k=1: ~17%; OpenAI o3 – k=10: 62.7%, k=1: ~13%; gpt-5-thinking – k=10: 56.8%, k=1: ~6%. Legend shows dark red = k=1 and light red = k=10."></p>
<p>On the one hand, a 56.8% attack rate is cleanly a big improvement against all of those other models.</p>
<p>But it’s also a strong signal that prompt injection continues to be an unsolved problem! That means that more than half of those k=10 attacks (where the attacker was able to try up to ten times) got through.</p>
<p>Don’t assume prompt injection isn’t going to be a problem for your application just because the models got better.</p>
<h4 id="thinking-traces-in-the-api">Thinking traces in the API</h4>
<p>I had initially thought that my biggest disappointment with GPT-5 was that there’s no way to get at those thinking traces via the API... but that turned out <a href="https://bsky.app/profile/sophiebits.com/post/3lvtceih7222r">not to be true</a>. The following <code>curl</code> command demonstrates that the responses API <code>"reasoning": {"summary": "auto"}</code> is available for the new GPT-5 models:</p>

<pre><code>curl https://api.openai.com/v1/responses \
  -H "Authorization: Bearer $(llm keys get openai)" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-5",
    "input": "Give me a one-sentence fun fact about octopuses.",
    "reasoning": {"summary": "auto"}
  }'</code></pre>

<p>Here’s <a href="https://gist.github.com/simonw/1d1013ba059af76461153722005a039d">the response</a> from that API call.</p>

<p>Without that option the API will often provide a lengthy delay while the model burns through thinking tokens until you start getting back visible tokens for the final response.</p>
<p>OpenAI offer a new <code>reasoning_effort=minimal</code> option which turns off most reasoning so that tokens start to stream back to you as quickly as possible.</p>
<h4 id="and-some-svgs-of-pelicans">And some SVGs of pelicans</h4>
<p>Naturally I’ve been running <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle/">my “Generate an SVG of a pelican riding a bicycle” benchmark</a>. I’ll actually spend more time on this in a future post—I have some fun variants I’ve been exploring—but for the moment here’s <a href="https://gist.github.com/simonw/c98873ef29e621c0fe2e0d4023534406">the pelican</a> I got from GPT-5 running at its default “medium” reasoning effort:</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-5-pelican.png" alt="The bicycle is really good, spokes on wheels, correct shape frame, nice pedals. The pelican has a pelican beak and long legs stretching to the pedals."></p>
<p>It’s pretty great! Definitely recognizable as a pelican, and one of the best bicycles I’ve seen yet.</p>
<p>Here’s <a href="https://gist.github.com/simonw/9b5ecf61a5fb0794729aa0023aaa504d">GPT-5 mini</a>:</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-5-mini-pelican.png" alt="Blue background with clouds. Pelican has two necks for some reason. Has a good beak though. More gradents and shadows than the GPT-5 one."></p>
<p>And <a href="https://gist.github.com/simonw/3884dc8b186b630956a1fb0179e191bc">GPT-5 nano</a>:</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-5-nano-pelican.png" alt="Bicycle is two circles and some randomish black lines. Pelican still has an OK beak but is otherwise very simple."></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DNA tests are uncovering the true prevalence of incest (2024) (109 pts)]]></title>
            <link>https://www.theatlantic.com/health/archive/2024/03/dna-tests-incest/677791/</link>
            <guid>44827692</guid>
            <pubDate>Thu, 07 Aug 2025 17:40:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/health/archive/2024/03/dna-tests-incest/677791/">https://www.theatlantic.com/health/archive/2024/03/dna-tests-incest/677791/</a>, See on <a href="https://news.ycombinator.com/item?id=44827692">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true">When Steve Edsel was a boy, his adoptive parents kept a scrapbook of newspaper clippings in their bedroom closet. He would ask for it sometimes, poring over the headlines about his birth. Headlines like this: “Mother Deserts Son, Flees From Hospital,” <em>Winston-Salem Journal,</em> December 30, 1973.</p><p data-flatplan-paragraph="true">The mother in question was 14 years old, “5 feet 6 with reddish brown hair,” and she had come to the hospital early one morning with her own parents. They gave names that all turned out to be fake. And by 8 o’clock that evening, just hours after she gave birth, they were gone. In a black-and-white drawing of the mother, based on nurses’ recollections, she has round glasses and sideswept bangs. Her mouth is grimly set.</p><p data-flatplan-paragraph="true">The abandoned boy was placed in foster care with a local couple, the Edsels, who later adopted him. Steve knew all of this growing up. His parents never tried to hide his origins, and they always gave him the scrapbook when he asked. It wasn’t until he turned 14, though, that he really began to wonder about his birth mom. “I’m 14,” he thought at the time. “This is how old she was when she had me.”</p><p data-flatplan-paragraph="true">Steve began looking for her in earnest in his 20s, but the paper trail quickly ran cold. When he turned 40, he told his wife, Michelle, that he wanted to give the search one last go. This was in 2013. AncestryDNA had started selling mail-in test kits the previous year, so he bought one. His matches at first seemed unpromising—some distant relatives—but when he began posting in a Facebook group for people seeking out biological family, he got connected to a genetic genealogist named CeCe Moore. Moore specializes in finding people via distant DNA matches, a technique made famous in 2018 when it led to the <a data-event-element="inline link" href="https://www.theatlantic.com/science/archive/2018/04/golden-state-killer-east-area-rapist-dna-genealogy/559070/">capture of the Golden State Killer</a>. But back then, genetic genealogy was still new, and Moore was one of its pioneers. She volunteered to help Steve.</p><p data-flatplan-paragraph="true">Within just a couple of weeks, she had narrowed down the search to two women, cousins of the same age. On Facebook, Steve could see that one cousin had four kids, and she regularly posted photos of them, beautiful and smiling. They looked well-off, their lives picture-perfect—“like a storybook,” Steve says. The other woman was unmarried; she didn’t have kids. She was not friends with her immediate family on Facebook, and she had moved halfway across the country from them. One evening—a Saturday, Steve clearly remembers—Moore asked to speak with him by phone.</p><p data-flatplan-paragraph="true">She confirmed what he had already suspected: His birth mom was the second woman. But Moore had another piece of news too. She had unexpectedly figured out something about his biological father as well.<em> It looks like your parents are related.</em> Steve didn’t know what to say. <em>Do you understand what I mean?</em> He said he thought so. <em>Either your mom’s father or your mom’s brother is your father.</em> A sea of emotions rose to a boil inside him: anger, hurt, worthlessness, disgust, shame, and devastation all at once. In his years of wondering about his birth, he had never, ever considered the possibility of incest. Why would he? What were the chances?</p><hr><p data-flatplan-paragraph="true">In 1975, around the time of Steve’s birth, a psychiatric textbook put the frequency of incest at one in a million.</p><p data-flatplan-paragraph="true">But this number is almost certainly a dramatic underestimate. The stigma around openly discussing incest, which often involves child sexual abuse, has long made the subject difficult to study. In the 1980s, <a data-event-element="inline link" href="https://www.hup.harvard.edu/books/9780674002708">feminist scholars argued</a>, based on the testimonies of victims, that incest was far more common than recognized, and in recent years, DNA has offered a new kind of biological proof. Widespread genetic testing is uncovering case after secret case of children born to close biological relatives—providing an unprecedented accounting of incest in modern society.</p><p data-flatplan-paragraph="true">The geneticist Jim Wilson, at the University of Edinburgh, was shocked by the frequency he found in the U.K. Biobank, an anonymized research database: One in 7,000 people, according to his unpublished analysis, was born to parents who were first-degree relatives—a brother and a sister or a parent and a child. “That’s way, way more than I think many people would ever imagine,” he told me. And this number is just a floor: It reflects only the cases that resulted in pregnancy, that did not end in miscarriage or abortion, and that led to the birth of a child who grew into an adult who volunteered for a research study.</p><p data-flatplan-paragraph="true">Most of the people affected may never know about their parentage, but these days, many are stumbling into the truth after AncestryDNA and 23andMe tests. Steve’s case was one of the first Moore worked on involving closely related parents. She now knows of well over 1,000 additional cases of people born from incest, the significant majority between first-degree relatives, with the rest between second-degree relatives (half-siblings, uncle-niece, aunt-nephew, grandparent-grandchild). The cases show up in every part of society, every strata of income, she told me.</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/science/archive/2018/07/dna-test-misattributed-paternity/562928/">Read: When a DNA test shatters your identity</a></p><p data-flatplan-paragraph="true">Neither AncestryDNA nor 23andMe informs customers about incest directly, so the thousand-plus cases Moore knows of all come from the tiny proportion of testers who investigated further. This meant, for example, uploading their DNA profiles to a third-party genealogy site to analyze what are known as “runs of homozygosity,” or ROH: long stretches where the DNA inherited from one’s mother and father are identical. For a while, one popular genealogy site instructed anyone who found high ROH to contact Moore. She would call them, one by one, to explain the jargon’s explosive meaning. Unwittingly, she became the keeper of what might be the world’s largest database of people born out of incest.</p><p data-flatplan-paragraph="true">In the overwhelming majority of cases, Moore told me, the parents are a father and a daughter or an older brother and a younger sister, meaning a child’s existence was likely evidence of sexual abuse. She had no obvious place to send people reeling from such revelations, and she was not herself a trained therapist. After seeing many of these cases, though, she wanted people to know they were not alone. Moore ended up creating a private and invite-only support group on Facebook in 2016, and she tapped Steve and later his wife, Michelle, to become admins, too. The three of them had become close in the months and years after the search for his birth mom, as they navigated the emotional fallout together.</p><p data-flatplan-paragraph="true">One day this past January, Michelle, who also works as Moore’s part-time assistant, told me she had spoken with four new people that week, all of them with ROH high enough to have parents who were first-degree relatives. She used to dread these calls. “I would stumble over my words,” she told me. But not anymore. She tells the shaken person on the line that they can join a support group full of people who are living the same reality. She tells them they can talk to her husband, Steve.</p><hr><p data-flatplan-paragraph="true">When Steve first discovered the truth about his biological parents, a decade ago, he had no support group to turn to, and he did not know what to do with the strange mix of emotions. He was genuinely happy to have found his birth mom. He had never looked like his adoptive parents, but in photos of her and her family, he could see his eyes, his chin, and even the smirky half-grin that his face naturally settles into.</p><p data-flatplan-paragraph="true">But he radiated with newfound anger, too, on her behalf. He could not know the exact circumstances of his conception, and his DNA test alone could not determine whether her older brother or her father was responsible. But Steve could not imagine a consensual scenario, given her age. The bespectacled 14-year-old girl who disappeared from the hospital had remained frozen in time in his mind, even as he himself grew older, got married, became a stepdad. He felt protective of that young girl.</p><p data-flatplan-paragraph="true">As badly as he wanted to know his birth mom, he worried she would not want to know him. Would his sudden reappearance dredge up traumatic memories—memories she had perhaps been trying to outrun her whole adult life, given how far away she had moved and how little she seemed connected to her family? A religious man, Steve prayed over it and settled on handwriting a letter. He included a couple of paragraphs about his life, some photos, and a message that he loved her. He left out what he knew about his paternity. And he took care to send the letter by certified mail, so that he could confirm its receipt and so that it would not accidentally fall into anyone else’s hands.</p><p data-flatplan-paragraph="true">She never responded. But Steve knew that she had received it: The post office sent him the green slip that she had signed upon delivery, and he scrutinized her signature—her actual name, written by her actual hand. At 40 years old, he touched for the first time something his mother had just touched, held something she had just held. He put the slip inside the pages of his Bible.</p><p data-flatplan-paragraph="true">Steve had never faulted his mother for leaving him at the hospital, and finding out about his paternity made him even more understanding. But the revelation also made him struggle with who he was. Did it mean that something was wrong with him, written into his DNA from the moment of his conception? On a <a data-event-element="inline link" href="https://podcasts.apple.com/us/podcast/ep-1-the-secret-one/id1572201167?i=1000526354990">podcast</a> later, he admitted to feeling like trash, “like something that somebody had just thrown away.” Those first six months after his discovery were the hardest six months of his life.</p><hr><p data-flatplan-paragraph="true">Across human cultures, incest between close family members is one of the most universal and most deeply held taboos. A common explanation is biological: Children born from related parents are more likely to develop health complications, because their parents are more likely to be carriers of the same recessive mutations. From the 1960s to the ’80s, a <a data-event-element="inline link" href="https://publications.aap.org/pediatrics/article-abstract/40/1/55/43627/CHILDREN-OF-INCEST">handful</a> <a data-event-element="inline link" href="https://www.sciencedirect.com/science/article/abs/pii/S0022347682803478">of</a> <a data-event-element="inline link" href="https://karger.com/hhe/article-abstract/21/2/108/158742/A-Study-of-Children-of-Incestuous-Matings">studies</a> following a few dozen children born of incest documented high rates of infant mortality and congenital conditions.</p><p data-flatplan-paragraph="true">But in the past, healthy children born from incestuous unions would have never come to the attention of doctors. As widespread DNA testing has uncovered orders of magnitude more people whose parents are brother and sister or parent and child, it’s also shown that plenty of those people are perfectly healthy. “There is a large element of chance in whether incest has a poor outcome,” according to Wilson, the geneticist. It depends on whether those runs of homozygosity contain recessive disease-causing mutations. All of us have some of these runs in our DNA—usually less than 1 percent of the genome in Western populations, higher in cultures where cousin marriage is common. But that number is about 25 percent, Wilson said, in people born from first-degree relatives. While the odds of a genetic disease are much higher, the outcome is far from predetermined.</p><p data-flatplan-paragraph="true">Still, these numbers make people wonder. Steve was born with a heart murmur, which required open-heart surgery at ages 13 and 18, though he does not know for sure the cause; heart defects are among the more common birth defects in the general population. He and Michelle were also never able to have children together. Others in the Facebook group have shared their struggles with autoimmune diseases, fibromyalgia, eye problems, and so on—though these are often hard to definitively link to incest. Health problems arising from incest might manifest in any number of ways, depending on exactly which mutations are inherited. “When I go to the doctor and they ask me my family history, I wonder: <em>How much do I need to go into it?</em>” says Mandy, another member of the group. (I am identifying some people by first name only, so they can speak freely about their family and medical histories.) How much experience would a typical doctor have with incest, anyway?</p><p data-flatplan-paragraph="true">After Mandy first learned that her father was her mother’s uncle, she went looking for stories about other people like her. All she could find were “gross fantasies” online and medical-journal articles about health problems. She felt very lonely. “<em>I don’t have anybody I can talk to about this</em>,” she remembers thinking. “<em>Nobody knows what to say.</em>” When she found the Facebook group, she could see that she was far from the only one like her. She watched the others cycle, too, through the stages of denial, anger, bargaining, depression, and acceptance.</p><p data-flatplan-paragraph="true">She does not know exactly what happened between her biological parents, but her mother was 17, and her mother’s uncle was in his 30s. The discovery, for all the hurt that it surfaced, has helped Mandy reconcile some of her childhood experiences. Unlike Steve, she was raised by her biological mother, and she believed her mother’s husband to be her biological father. He mostly ignored her, but her mother was cruel. She treated Mandy differently than she did her younger brothers. “At least now I have more of an answer as to why,” Mandy told me. “I wasn’t a bad kid and unlovable.”</p><p data-flatplan-paragraph="true">Kathy was also raised by her mother, though she had an early inkling that her dad was not her biological dad. Their blood types were incompatible, and she heard rumors about her mother and grandfather. Although her mother’s family was violent and chaotic, she was close to her dad’s family, especially her granny on that side. “They’ve been my rock,” she told me. By the time Kathy took a DNA test confirming that her dad was not her biological dad, she had spent a lifetime distancing herself from her biological family and embracing one with whom she shared no DNA.</p><p data-flatplan-paragraph="true">Hers was, in some ways, the opposite journey of adoptees such as Steve, who wanted so badly to know his biological family. But the two of them have become close. Kathy remembers how angry he used to be on his mother’s behalf. She told him that she used to be angry too, but she had to leave it behind. “It’s not going to bring me any peace. It’s not going to bring my mother any peace,” she recalled saying. And it wouldn’t undo what had been done to his mother by her father or her brother so many years ago.</p><hr><p data-flatplan-paragraph="true">In the end, Steve was able to identify his biological father, though not through any particular feat of genetic sleuthing. One day, two and a half years after his DNA test, he logged in to AncestryDNA and saw a parent match. It was his mother’s older brother. From the site, he could see that his father-uncle had logged in once, presumably seen that Steve was his son, and—even after Steve sent him a message—never logged back on again.</p><p data-flatplan-paragraph="true">By then, his initial anger had started to dissipate. He still felt deeply for his birth mom. Michelle says that her husband has always been a sensitive guy—she makes fun of him for crying at movies—but he’s become even more empathetic. The feeling of worthlessness he initially struggled with has given way to a sense of purpose; he and Michelle now spend hours on the phone talking with others in the support group.</p><p data-flatplan-paragraph="true">Steve has still never spoken to his birth mother. He tried writing to her a second time, sending a journal about his life—but she returned it unopened. He messages her occasionally on Facebook, sending photos of grandkids and puppies he’s raised. Every year, he wishes her a happy birthday. She has not replied, but she has also not blocked him.</p><p data-flatplan-paragraph="true">When the journal came back unopened, Steve decided to try messaging his mother’s cousin—the other woman he’d initially thought could be his birth mom. He yearned for some kind of connection with someone in his biological family. He wrote to the cousin about his mom—but not his dad—and she&nbsp; actually replied. She told him that she and his mom had been close as children, Steve recounted, but she did not know about a pregnancy. To her, it had seemed like her cousin one day “fell off the face of the Earth,” he says. She agreed to read his journal, and the two of them soon began speaking on the phone about their families.</p><p data-flatplan-paragraph="true">Months later, Steve felt like he could finally share the truth about his biological father, and the cousin again accepted him for who he was. They met for the first time in 2017 when she was visiting a nearby town, and she later invited Steve and Michelle to Thanksgiving. Last year, she extended another invitation to a large family gathering. Steve’s immediate biological family was not there, but hers was, and they all knew about him and his mom and his dad. They greeted him with hugs, and they took photos together as a family. “It felt like a relief,” he told me, like a burden had been lifted from him. In this family, he was not a secret.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5 for Developers (409 pts)]]></title>
            <link>https://openai.com/index/introducing-gpt-5-for-developers</link>
            <guid>44827101</guid>
            <pubDate>Thu, 07 Aug 2025 17:06:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/introducing-gpt-5-for-developers">https://openai.com/index/introducing-gpt-5-for-developers</a>, See on <a href="https://news.ycombinator.com/item?id=44827101">Hacker News</a></p>
Couldn't get https://openai.com/index/introducing-gpt-5-for-developers: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5 (1700 pts)]]></title>
            <link>http://openai.com/gpt-5</link>
            <guid>44826997</guid>
            <pubDate>Thu, 07 Aug 2025 17:00:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://openai.com/gpt-5">http://openai.com/gpt-5</a>, See on <a href="https://news.ycombinator.com/item?id=44826997">Hacker News</a></p>
Couldn't get http://openai.com/gpt-5: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Live: GPT-5 (170 pts)]]></title>
            <link>https://www.youtube.com/watch?v=0Uu_VJeVVfo</link>
            <guid>44826463</guid>
            <pubDate>Thu, 07 Aug 2025 16:16:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=0Uu_VJeVVfo">https://www.youtube.com/watch?v=0Uu_VJeVVfo</a>, See on <a href="https://news.ycombinator.com/item?id=44826463">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Building Bluesky Comments for My Blog (309 pts)]]></title>
            <link>https://natalie.sh/posts/bluesky-comments/</link>
            <guid>44826164</guid>
            <pubDate>Thu, 07 Aug 2025 15:56:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://natalie.sh/posts/bluesky-comments/">https://natalie.sh/posts/bluesky-comments/</a>, See on <a href="https://news.ycombinator.com/item?id=44826164">Hacker News</a></p>
<div id="readability-page-1" class="page"><p data-astro-cid-2q5oecfc=""> I hate disqus too much. </p><article data-astro-cid-2q5oecfc=""> 
<p>I’ve been running my blog without decent comments for years. Not by choice, really - I just couldn’t find a solution that didn’t suck.</p>
<ul>
<li>
<p>Disqus? Slow, heavy, tracks users, and I don’t own anything. Plus it makes every page 100x slower to load.</p>
</li>
<li>
<p>Self-hosted solutions? Great in theory. (not really.) You’re signing up to manage users, moderate spam, maintain databases, and deal with all the headaches that come with running basically a miniature social platform. And if your users aren’t where you are, it’s probably slow as hell.</p>
</li>
<li>
<p>GitHub Issues as comments? Probably works for some developer blogs, but feels hacky and limits your audience to people with GitHub accounts.</p>
</li>
<li>
<p>No comments at all? Clean and simple, but you lose the conversations. Some of my favorite discoveries came from comment threads that went in unexpected directions.</p>
</li>
</ul>
<p>I’ve been a Bluesky user for a while. Recently, the community has been feeling healthier than Twitter ever did, the API is designed, and this decentralized approach means I don’t necessarily have to be beholden to a single company. People have been doing some interesting things with Bluesky, like on-protocol blog content and using Bluesky comments as a comment system. Why not do some of that for myself?</p>
<h2 id="why-bluesky-actually-makes-sense">Why Bluesky Actually Makes Sense</h2>
<p>The more I thought about it, the more directly using Bluesky for comments made sense:</p>
<ul>
<li>
<p>No infrastructure to maintain. (for me, at least) I don’t need to run databases, manage user accounts, or build moderation tools. Bluesky handles all of that.</p>
</li>
<li>
<p>Rich(er) content support. People can post images, links, and in threads. All the stuff that makes conversations interesting.</p>
</li>
<li>
<p>Real identities. Since people are using their actual Bluesky profiles, and your one profile can <em>actually</em> be used on any supported platform, there’s more accountability and less incentive to drive-by troll.</p>
</li>
<li>
<p>Cross-platform conversations. Comments live on Bluesky too, so people can discover my blog posts through social media and vice versa.</p>
</li>
<li>
<p>I own my content, they own theirs. No platform lock-in for anyone!</p>
</li>
</ul>
<p>The workflow is simple: I publish a blog post, share it on Bluesky, edit the post to add the AT URI, and the replies to that Bluesky post become the comments on my blog.</p>
<h2 id="building-the-component">Building the Component</h2>
<h3 id="understanding-the-at-protocol">Understanding the AT Protocol</h3>
<p>Bluesky runs on the AT Protocol, which has surprisingly okay documentation. The key concepts I needed:</p>
<ul>
<li><strong>DIDs</strong> (Decentralized Identifiers): Unique user IDs like <code>did:plc:abc123...</code> or <code>did:web:joe.coffee</code></li>
<li><strong>CIDs</strong> (Content Identifiers): Unique post IDs</li>
<li><strong>AT URIs</strong>: Addresses for content like <code>at://did:plc:user.../app.bsky.feed.post/postid</code></li>
</ul>
<p>To fetch comments, I just need to call the <code>getPostThread</code> endpoint with the right URI. No authentication required. Easy peasy.</p>
<h3 id="component-architecture">Component Architecture</h3>
<p>I ended up with three main pieces:</p>
<ol>
<li>The main comments component that fetches and displays the thread.</li>
<li>A reply component that handles rendering individual posts and their replies. Also includes metadata and a link to the original Bluesky post.</li>
<li>An embed component for rich content like images and open graph previews.</li>
</ol>
<p>This separation made each piece reasonably manageable, reasonable, and small.</p>
<h3 id="the-threading-challenge">The Threading Challenge</h3>
<p>The interesting part was handling nested replies. Bluesky threads can go arbitrarily deep, but I needed to display them in a way that’s readable and doesn’t break layouts.</p>
<p>I settled on a naive recursive approach where each reply can render child replies, with visual indentation to show the hierarchy. I cap it at 5 levels deep because beyond that, conversations usually devolve into two people arguing anyway.</p>
<pre tabindex="0" data-language="typescript"><code><span><span>const</span><span> MAX_DEPTH </span><span>=</span><span> 5</span><span>;</span></span>
<span><span>const</span><span> BlueskyReply</span><span> =</span><span> ({</span><span> thread</span><span>,</span><span> depth</span><span> =</span><span> 0</span><span> })</span><span> =&gt;</span><span> {</span></span>
<span><span>  return</span><span> (</span></span>
<span><span>    &lt;</span><span>div style</span><span>=</span><span>{{</span><span> marginLeft</span><span>:</span><span> depth </span><span>*</span><span> 12</span><span> }}</span><span>&gt;</span></span>
<span><span>      {</span><span>/* Render the post content */</span><span>}</span></span>
<span></span>
<span><span>      {</span><span>depth</span><span> &lt; </span><span>MAX_DEPTH</span><span> &amp;&amp; </span><span>thread</span><span>.</span><span>replies</span><span>?.</span><span>map</span><span>(</span><span>reply</span><span> =&gt;</span></span>
<span><span>        &lt;</span><span>BlueskyReply</span><span> thread</span><span>=</span><span>{</span><span>reply</span><span>}</span><span> depth</span><span>=</span><span>{</span><span>depth + </span><span>1</span><span>}</span><span> /&gt;</span></span>
<span><span>      )</span><span>}</span></span>
<span><span>    &lt;/</span><span>div</span><span>&gt;</span></span>
<span><span>  )</span><span>;</span></span>
<span><span>};</span></span></code></pre>
<h3 id="handling-rich-content">Handling Rich Content</h3>
<p>One of the nice things about Bluesky is that posts can contain more than just text. People embed images, external links, and even quote other posts. Each embed type needs special handling.</p>
<p><strong>Images</strong> were the most complex. Bluesky serves them through their CDN, and people often post multiple images in a single reply. I built a responsive grid layout that adapts based on image count, plus a modal for viewing images full-screen.</p>
<p><strong>External links</strong> get rendered as cards with thumbnails and descriptions, just like they appear in Bluesky apps.</p>
<p><strong>Other embed types</strong> get a graceful fallback message since the AT Protocol is extensible and new embed types might appear.</p>
<h3 id="integrating-with-astro">Integrating with Astro</h3>
<p>Getting this working with my Astro blog was straightforward. I had the React integration (which I already had for my background and music components) and used the <code>client:load</code> directive to ensure the comment component hydrates immediately:</p>
<pre tabindex="0" data-language="astro"><code><span><span>---</span></span>
<span><span>import</span><span> BlueskyComments </span><span>from</span><span> '../components/bsky-comments.tsx'</span><span>;</span></span>
<span><span>---</span></span>
<span></span>
<span><span>{</span><span>post</span><span>.</span><span>data</span><span>.</span><span>bsky </span><span>&amp;&amp;</span><span> (</span></span>
<span><span>  &lt;</span><span>BlueskyComments</span></span>
<span><span>    did</span><span>=</span><span>{</span><span>post</span><span>.</span><span>data</span><span>.</span><span>bsky</span><span>.</span><span>did</span><span>}</span></span>
<span><span>    postCid</span><span>=</span><span>{</span><span>post</span><span>.</span><span>data</span><span>.</span><span>bsky</span><span>.</span><span>postCid</span><span>}</span></span>
<span><span>    client</span><span>:</span><span>load</span></span>
<span><span>  /&gt;</span></span>
<span><span>)</span><span>}</span></span></code></pre>
<p>Now I just add this to any post’s frontmatter to enable comments:</p>
<pre tabindex="0" data-language="yaml"><code><span><span>bsky</span><span>:</span></span>
<span><span>  did</span><span>:</span><span> "my-bluesky-did"</span></span>
<span><span>  postCid</span><span>:</span><span> "the-post-id"</span></span></code></pre>
<h2 id="what-i-learned">What I Learned</h2>
<h3 id="typescript-is-your-friend">TypeScript is Your Friend</h3>
<p>There are proper TypeScript types for all their API responses through the <code>@atcute/client</code> package. This made development much smoother as I could rely on autocomplete and catch type errors before they became runtime bugs.</p>
<h3 id="progressive-enhancement-works">Progressive Enhancement Works</h3>
<p>I built the comments as an enhancement to the blog, not a core dependency. If JavaScript is disabled or the API is down, the blog post (rendered a long time ago) still works perfectly. The comments just don’t appear.</p>
<h3 id="performance-by-default--ish">Performance by Default (-ish)</h3>
<p>Since I’m not managing any backend infrastructure, server-side performance optimizations are just there. Bluesky’s CDN handles image delivery, their public API is fast and cached, and I don’t have to care about database queries or server scaling.</p>
<h2 id="the-results">The Results</h2>
<p>I’m pretty happy with how it turned out. The conversations feel more natural than traditional blog comments - people use their actual profiles, share images and links, and more. It’s a lot more like social media.</p>
<h2 id="whats-next">What’s Next</h2>
<p>I’m considering a few improvements, but honestly, the core system works so well that I’m not in a rush to change it. Sometimes the best solution is the one just works, almost invisibly.</p>
<h2 id="why-this-approach-works">Why This Approach Works</h2>
<p>Traditional comment systems try to recreate social media features on every individual website. I think that’s backwards. People already have social media accounts they like using. Instead of forcing them to create new accounts and learn new interfaces, why not try meeting them where they already are?</p>
<p>This approach scales with the platform because it <em>uses</em> the platform. As Bluesky grows, more people can participate in blog discussions without any additional work from me. And because everything is built on open protocols, I’m not locked into any single platform’s decisions. If Bluesky ever changes for the worse, I can always switch to another AppView, such as zeppelin or Blacksky’s AppView.</p>
<p>I could theoretically even write my own comments AppView. ATProto is designed to be flexible, especially with data, so doing such would be quite simple. I’d just need to listen to the right events on the firehose and store the data in a way that makes sense and rebuild the comment thread every so often.</p>
<p>In my opinion, the web is better when independent sites can connect to broader conversations without sacrificing their independence. I feel like this <del>is</del> was the goal of other decentralised platforms like Mastodon, but Bluesky’s focus on user-owned identities and app intercompat via the PDS ultimately makes it a better fit.</p>
<hr>
<p><em>Want to see it in action? The comments are right below this post, powered by the system I mentioned above. Meta.</em></p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to sell if your user is not the buyer (174 pts)]]></title>
            <link>https://writings.founderlabs.io/p/how-to-sell-if-your-user-is-not-the</link>
            <guid>44825491</guid>
            <pubDate>Thu, 07 Aug 2025 15:09:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://writings.founderlabs.io/p/how-to-sell-if-your-user-is-not-the">https://writings.founderlabs.io/p/how-to-sell-if-your-user-is-not-the</a>, See on <a href="https://news.ycombinator.com/item?id=44825491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!XpMA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7892cc8d-c26f-410a-9747-929a882f6573_1456x816.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!XpMA!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7892cc8d-c26f-410a-9747-929a882f6573_1456x816.png 424w, https://substackcdn.com/image/fetch/$s_!XpMA!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7892cc8d-c26f-410a-9747-929a882f6573_1456x816.png 848w, https://substackcdn.com/image/fetch/$s_!XpMA!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7892cc8d-c26f-410a-9747-929a882f6573_1456x816.png 1272w, https://substackcdn.com/image/fetch/$s_!XpMA!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7892cc8d-c26f-410a-9747-929a882f6573_1456x816.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!XpMA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7892cc8d-c26f-410a-9747-929a882f6573_1456x816.png" width="1456" height="816" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7892cc8d-c26f-410a-9747-929a882f6573_1456x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:816,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2220211,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://writings.founderlabs.io/i/168654646?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7892cc8d-c26f-410a-9747-929a882f6573_1456x816.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!XpMA!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7892cc8d-c26f-410a-9747-929a882f6573_1456x816.png 424w, https://substackcdn.com/image/fetch/$s_!XpMA!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7892cc8d-c26f-410a-9747-929a882f6573_1456x816.png 848w, https://substackcdn.com/image/fetch/$s_!XpMA!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7892cc8d-c26f-410a-9747-929a882f6573_1456x816.png 1272w, https://substackcdn.com/image/fetch/$s_!XpMA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7892cc8d-c26f-410a-9747-929a882f6573_1456x816.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>I recently wrote about how </span><a href="https://writings.founderlabs.io/p/your-ideal-customer-is-one-who-values" rel="">your ideal customer is one who values your product the most</a><span>”. And then quickly, in one of my private communities, I had this question pop up:</span></p><blockquote><p>But how you would talk to your ideal customer if they are not the ones who try the product?</p><p>In my case -- I totally agree. CTOs / Director of Engineering are probably the ones who make decisions. But it is developers who try the product first.</p><p>In the article you talk about messaging. I get it. But how practically you can reach out to decision makers?</p></blockquote><p>Let’s dissect this question here and get to the root of this question here, right now.</p><p>This is the "the user is not the buyer" problem.</p><p>Sure, it's a tough one, and I don't think there's a one-size-fits-all solution. </p><p><strong>Here’s the key… it depends on one thing:</strong><span> </span></p><p><span>→ Who actually has the power?</span><br><em>(and hint, it’s not always the person with the credit card)</em></p><p>Let me explain…</p><p>Your product is used and bought by smaller companies, they have CTOs/directors, but it's pretty flat, and they are in build mode. These are likely early stage or smaller companies still trying to figure out their market fit or value proposition.</p><p>The user (developer) might have the power. Here’s why…</p><p>The CTO's incentive is to reduce time to market/iterate. The faster they are to market and iterate, the faster they learn, the faster they find PMF, the more likely everyone keeps their jobs (and maybe they get a big payout win 3-10 years from now, or at least that's the promise from the CEO).</p><p><span>The user/dev has the power </span><strong>because of the time constraint</strong><span>. </span></p><p>They're likely to come to the organization with knowledge - it's why they were hired. So, their suggestion on what to bring to the table in terms of dev tooling will be massively important. </p><p>They might even setup a free account to use before anyone notices, "just to get things done". </p><p><span>Eventually they have to level up to a paid tier and voila, </span><strong>the company has just been trojan-horsed</strong><span>.</span></p><p>The CTO/directors/leadership are the ones who have the power because there’s different constraints other than time.  In this case, let’s just say security is the primary constraint. </p><p>No user/dev is going to be allowed to install software themselves. This comes from top-down. </p><p>Thus, the sales cycle is going to be long since you have to sell the value differently and there’s likely a vetting and due diligence type of process they have to go through to choose your product over another.</p><p>The users/devs don't have much of a choice because the primary value/risk isn't UI/UX/DX or whatever the user/dev cares about. It's security + outcome/output.</p><p><strong>Now here’s the thing…</strong><span> The person with the credit card is who we typically would call the "decision maker" (and in the original question, it was called the “ideal customer” even).</span></p><p><strong>But the reality is, they aren't always the one with the power. It depends on the constraints they're under.</strong></p><p>It's also not “who tries the product first”, as the original question states. </p><p>It's “who has the leverage”?</p><p>Who has the power, the constraints, and/or the incentive to push it through the most?</p><p><span>Subsequently, </span><strong>this is who values it the most.</strong></p><p>IMPORTANT POINT: I’m getting picky about defining “value” here. It needs to be actionable - something within the bounds of reality. Someone who says it’s worth N to them, but doesn’t have the means to actually trade N for it, doesn’t really matter at the end of the day. </p><p>Let’s get back to it…</p><p><strong>If your user/dev values it more than the holder of the budget, then you only have one thing to do….</strong></p><p>The CTO will never write the check for the value the user/dev deems it’s worth.  </p><p>That said, as a dev, I've paid for tools out of my own pocket that mean enough to me to do so if it's valuable enough. And in that case, the incentive and value isn't because the company will win... it's because I'm going to look great when I make the company win.</p><p><span>Different incentives. </span><br><span>Different payoffs.</span></p><p>But, with that in mind, let's get more specific... </p><div><p><span>I posed this question back to the OP: “What's the typical path for adoption (that works best for you!)?”</span></p><p><span>Perhaps for you, it goes something like this:</span></p></div><ol><li><p>User/dev registers</p></li><li><p>User/dev runs free trial on a local environment</p></li><li><p>User/dev gets a the value right before the do a pull request, so they can see the before and after scenarios (highlighting any issues)</p></li><li><p><span>User/dev sees value (Note: here’s </span><a href="https://writings.founderlabs.io/p/how-to-increase-mrr-activating-users?utm_source=publication-search" rel="">how to master the “aha moment”</a><span>). They see it will save them time with QA’ing their own changes, perhaps it could even be automated?</span></p></li><li><p>User/dev tries to convince leadership it's necessary to get it paid for</p></li><li><p>Leadership tests/sees process, checks budget, decides yes/no</p></li><li><p>Leadership gives approval for purchase</p></li><li><p>User/dev/leadership purchases</p></li><li><p>User/dev continues to use it and propagates among other user/devs in the co.</p></li></ol><p><span>In this case (change based on the assumption of the scenarios above, small vs big orgs, user flow, etc), </span><strong>here's some questions to figure out:</strong></p><p><strong>What's the real incentive behind why the dev suggests the tool?</strong><span>  Does the dev want it because it helps them be awesome / reduces the pain of doing their job, or because it'll help the company goals? </span></p><p><strong>What's the real incentive behind why the leadership buys it?</strong><span> Because they'll see it'll take less time for the dev to do their job, they'll be better at it, thus getting to the goals of the company? </span></p><p>If those things are true, I would do these 2 things….</p><ol><li><p>Give the user/dev the tools they need to convince their leadership that it's a slam dunk (for their incentives).  Translate the value from the user/dev’s terms into the leadership’s terms.  </p></li></ol><p>Note: It's best if this isn't explicitly declared to them like "here's how you convince your leadership this is a good idea" but more like "here's a report showing how much time you were going to spend doing it in the old way vs how much time you actually spent doing it this way". Also, make these actual, real numbers as much as possible or the dev won't use the report because they’ll know the numbers are inaccurate.</p><ol start="2"><li><p>I'd also do some customer interviews here with the devs to figure out how those conversations usually go with leadership.  What do they need to make that conversation go faster/easier? What stops them from buying?</p></li></ol><p><strong>Find the friction, smooth it out.</strong></p><p>The meta point here is that you're not going to talk to the credit card holder; the user/dev is going to do that for you. </p><p><span>Give them the best possible chance at convincing the leadership. Make them look awesome for even bothering the leadership with a choice like this. Make it obviously awesome for them to decide “yes”.  These users/devs are your sales people. Treat them as such in the sense that </span><strong>if they win</strong><span> (product does what the user hopes and dreams it will, getting them to the promised land), </span><strong>you win</strong><span> ($), and </span><strong>then they win again</strong><span> (look good, systemically removing friction in their job/process, etc). Not to mention, the leadership wins and the company wins.</span></p><p>Now go, and proselytize those devs. ;) </p><p><span>If you’re a founder who wants to get unstuck, reduce churn, get through that revenue plateau, generate more leads, and more…  </span><a href="https://fantastical.app/nateritter/nrp" rel="">Get in touch today</a></p><p data-attrs="{&quot;url&quot;:&quot;https://fantastical.app/nateritter/nrp&quot;,&quot;text&quot;:&quot;→ I want to get unstuck, let's talk&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://fantastical.app/nateritter/nrp" rel=""><span>→ I want to get unstuck, let's talk</span></a></p><p><span>🧢 Hat tip to </span><a href="https://ygerasimov.com/" rel="">Yuriy Gerasimov</a><span> from </span><a href="https://diffy.website/" rel="">Diffy (visual regression for Wordpress and Drupal)</a><span> for the being the muse this week! Thanks for letting me write about our chat, Yuriy!</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lithium compound can reverse Alzheimer’s in mice: study (140 pts)]]></title>
            <link>https://hms.harvard.edu/news/could-lithium-explain-treat-alzheimers-disease</link>
            <guid>44825326</guid>
            <pubDate>Thu, 07 Aug 2025 14:56:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hms.harvard.edu/news/could-lithium-explain-treat-alzheimers-disease">https://hms.harvard.edu/news/could-lithium-explain-treat-alzheimers-disease</a>, See on <a href="https://news.ycombinator.com/item?id=44825326">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Open AI Announces $1.5M Bonus for Every Employee (125 pts)]]></title>
            <link>https://medium.com/activated-thinker/breaking-open-ai-announces-1-5-million-bonus-for-every-employee-29d057b9d590</link>
            <guid>44825309</guid>
            <pubDate>Thu, 07 Aug 2025 14:55:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/activated-thinker/breaking-open-ai-announces-1-5-million-bonus-for-every-employee-29d057b9d590">https://medium.com/activated-thinker/breaking-open-ai-announces-1-5-million-bonus-for-every-employee-29d057b9d590</a>, See on <a href="https://news.ycombinator.com/item?id=44825309">Hacker News</a></p>
Couldn't get https://medium.com/activated-thinker/breaking-open-ai-announces-1-5-million-bonus-for-every-employee-29d057b9d590: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Let's stop pretending that managers and executives care about productivity (129 pts)]]></title>
            <link>https://www.baldurbjarnason.com/2025/disingenuous-discourse/</link>
            <guid>44824981</guid>
            <pubDate>Thu, 07 Aug 2025 14:33:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.baldurbjarnason.com/2025/disingenuous-discourse/">https://www.baldurbjarnason.com/2025/disingenuous-discourse/</a>, See on <a href="https://news.ycombinator.com/item?id=44824981">Hacker News</a></p>
Couldn't get https://www.baldurbjarnason.com/2025/disingenuous-discourse/: AggregateError]]></description>
        </item>
        <item>
            <title><![CDATA[Windows XP Professional (323 pts)]]></title>
            <link>https://win32.run/</link>
            <guid>44824539</guid>
            <pubDate>Thu, 07 Aug 2025 13:58:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://win32.run/">https://win32.run/</a>, See on <a href="https://news.ycombinator.com/item?id=44824539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="pos_loader">
			<!-- Bootup by Kyle Stephens -->
			<!-- https://codepen.io/kylestephens/pen/zYOgLrr -->
			<section id="bios">
				<p>PhoenixBIOS 1.4 Release 6.0</p>
				<p>Copyright 1985-2001 Phoenix Technologies Ltd.</p>
				<p>All Rights Reserved</p>
				<p>Copyright 2001-2003 VMware. Inc.</p>
				<p>VMware BIOS build 314</p>
				
				<p>ATAPI CD-ROM: VMware Virtual IDECDROM Drive</p>
				<p>Initializing <span>...</span></p>
			  
			  </section>
			  
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Infinite Pixels (219 pts)]]></title>
            <link>https://meyerweb.com/eric/thoughts/2025/08/07/infinite-pixels/</link>
            <guid>44824056</guid>
            <pubDate>Thu, 07 Aug 2025 13:12:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://meyerweb.com/eric/thoughts/2025/08/07/infinite-pixels/">https://meyerweb.com/eric/thoughts/2025/08/07/infinite-pixels/</a>, See on <a href="https://news.ycombinator.com/item?id=44824056">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I was on one of my rounds of social media trawling, just seeing what was floating through the aether, when I came across <a href="https://mastodon.art/@otterlove/114971594534242993">a toot by Andy P</a> that said:</p>
<blockquote>
<div><p>Fun #css trick:</p><p>

width: calc(infinity * 1px);<br>
height: calc(infinity * 1px);</p></div>
</blockquote>
<p>…and I immediately thought, <em>This is a perfect outer-limits probe!</em> By which I mean, if I hand a browser values that are effectively infinite by way of <a href="https://www.w3.org/TR/css-values-4/#calc-error-constants"> the<code>infinity</code> keyword</a>, it will necessarily end up clamping to something finite, thus revealing how far it’s able or willing to go for that property.</p>
<p>The first thing I did was exactly what Andy proposed, with a few extras to zero out box model extras:</p>
<pre><code>div {
	width: calc(infinity * 1px);&nbsp; 
	height: calc(infinity * 1px);
	margin: 0;
	padding: 0; }</code></pre>
<pre><code>&lt;body&gt;
&nbsp;  &lt;div&gt;I’m huge!&lt;/div&gt;
&lt;/body&gt;</code></pre>
<p>Then I loaded the (fully valid HTML 5) test page in Firefox Nightly, Chrome stable, and Safari stable, all on macOS, and things pretty immediately got weird:</p>
<table>
<caption>Element Size Results</caption>
<thead>
<tr>
<th>Browser</th>
<th>Computed value</th>
<th>Layout value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Safari</td>
<td>33,554,428</td>
<td>33,554,428</td>
</tr>
<tr>
<td>Chrome</td>
<td>33,554,400</td>
<td>33,554,400</td>
</tr>
<tr>
<td>Firefox (Nightly)</td>
<td>19.2 / 17,895,700</td>
<td>19.2 / 8,947,840 †</td>
</tr>
</tbody>
</table>
<p><em>† height / width</em></p>
<p>Chrome and Safari both get <em>very</em> close to 2<sup>25</sup>-1 (33,554,431), with Safari backing off from that by just 3 pixels, and Firefox by 31.&nbsp; I can’t even hazard a guess as to why this sort of value would be limited in that way; if there was a period of time where 24-bit values were in vogue, I must have missed it.&nbsp; I assume this is somehow rooted in the pre-Blink-fork codebase, but who knows. (Seriously, who knows?&nbsp; I want to talk to you.)</p>
<p>But the faint whiff of oddness there has <em>nothing</em> on what’s happening in Firefox.&nbsp; First off, the computed height is<code>19.2px</code>, which is the height of a line of text at default font size and line height.&nbsp; If I explicitly gave it<code> line-height: 1</code>, the height of the <code>&lt;div&gt;</code> changes to 16px.&nbsp; All this is despite my assigning a height of infinite pixels!&nbsp; Which, to be fair, is not really possible to do, but does it make sense to just drop it on the floor rather than clamp to an upper bound?</p>
<p>Even if that can somehow be said to make sense, it <em>only</em> happens with height.&nbsp; The computed width value is, as indicated, nearly 17.9 million, which is not the content width and is also nowhere close to any power of two.&nbsp; But the actual layout width, according to the diagram in the Layout tab, is just over 8.9 million pixels; or, put another way, one-half of 17,895,700 <em> minus 10</em>.</p>
<p>This frankly makes my brain hurt.&nbsp; I would truly love to understand the reasons for any of these oddities.&nbsp; If you know from whence they arise, please, please leave a comment!&nbsp; The more detail, the better.&nbsp; I also accept trackbacks from blog posts if you want to get extra-detailed.</p>
<p>For the sake of my aching skullmeats, I almost called a halt there, but I decided to see what happened with font sizes.</p>
<pre><code>div {
	width: calc(infinity * 1px);&nbsp; 
	height: calc(infinity * 1px);
	margin: 0;
	padding: 0;
	font-size: calc(infinity * 1px); }</code></pre>
<p>My skullmeats did not thank me for this, because once again, things got…&nbsp;interesting.</p>
<table>
<caption>Font Size Results</caption>
<thead>
<tr>
<th>Browser</th>
<th>Computed value</th>
<th>Layout value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Safari</td>
<td>100,000</td>
<td>100,000</td>
</tr>
<tr>
<td>Chrome</td>
<td>10,000</td>
<td>10,000</td>
</tr>
<tr>
<td>Firefox (Nightly)</td>
<td>3.40282e38</td>
<td>2,400 / 17,895,700 †</td>
</tr>
</tbody>
</table>
<p><em>† line height values of <code>normal</code> /<code>1</code></em></p>
<p>Safari and Chrome have pretty clearly set hard limits, with Safari’s an order of magnitude larger than Chrome’s.&nbsp; I get it: what are the odds of someone wanting their text to be any larger than, say, a viewport height, let alone ten or 100 times that height?&nbsp; What intrigues me is the nature of the limits, which are so clearly base-ten numbers that someone typed in at some point, rather than being limited by setting a register size or variable length or something that would have coughed up a power of two.</p>
<p>And speaking of powers of two… ah, Firefox.&nbsp; Your idiosyncrasy continues.&nbsp; The computed value is a 32-bit <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">single-precision floating-point</a> number.&nbsp; It doesn’t get used in any of the actual rendering, but that’s what it is.&nbsp; Instead, the actual font size of the text, as judged by the Box Model diagram on the Layout tab, is…&nbsp;2,400 pixels.</p>
<p>Except, I can’t say that’s the <em>actual</em> actual font size being used: I suspect the actual value is 2,000 with a line height of 1.2, which is generally what <code> normal</code> line heights are in browsers. “So why didn’t you just set <code> line-height: 1</code> to verify that, genius?” I hear you asking.&nbsp; I did!&nbsp; And that’s when the layout height of the <code>&lt;div&gt;</code> bloomed to just over 8.9 million pixels, like it probably should have in the previous test!&nbsp; And all the same stuff happened when I moved the styles from the<code>&lt;div&gt;</code> to the <code>&lt;body&gt;</code>!</p>
<p>I’ve started writing at least three different hypotheses for why this happens, and stopped halfway through each because each hypothesis self-evidently fell apart as I was writing it.&nbsp; Maybe if I give my whimpering neurons a rest, I could come up with something.&nbsp; Maybe not.&nbsp; All I know is, I’d be much happier if someone just explained it to me; bonus points if their name is Clarissa.</p>
<p>Since setting line heights opened the door to madness in font sizing, I thought I’d try setting <code>line-height</code> to infinite pixels and see what came out.&nbsp; This time, things were (relatively speaking) more sane.</p>
<table>
<caption>Line Height Results</caption>
<thead>
<tr>
<th>Browser</th>
<th>Computed value</th>
<th>Layout value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Safari</td>
<td>33,554,428</td>
<td>33,554,428</td>
</tr>
<tr>
<td>Chrome</td>
<td>33,554,400</td>
<td>33,554,400</td>
</tr>
<tr>
<td>Firefox (Nightly)</td>
<td>17,895,700</td>
<td>8,947,840</td>
</tr>
</tbody>
</table>
<p>Essentially, the results were the same as what happened with element widths in the first example: Safari and Chrome were very close to 2<sup>25</sup>-1, and Firefox had its thing of a strange computed value and a rendering size not <em> quite</em> half the computed value.</p>
<p>I’m sure there’s a fair bit more to investigate about infinite-pixel values, or about infinite values in general, but I’m going to leave this here because my gray matter needs a rest and possibly a pressure washing.&nbsp; Still, if you have ideas for infinitely fun things to jam into browser engines and see what comes out, let me know.&nbsp; I’m already wondering what kind of shenanigans, other than in <code>z-index</code>, I can get up to with <code> calc(-infinity)</code>…</p> 
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An LLM does not need to understand MCP (102 pts)]]></title>
            <link>https://hackteam.io/blog/your-llm-does-not-care-about-mcp/</link>
            <guid>44823850</guid>
            <pubDate>Thu, 07 Aug 2025 12:52:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hackteam.io/blog/your-llm-does-not-care-about-mcp/">https://hackteam.io/blog/your-llm-does-not-care-about-mcp/</a>, See on <a href="https://news.ycombinator.com/item?id=44823850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Model Context Protocol (MCP) has become the standard for tool calling when building agents, but contrary to popular belief, your LLM does not need to understand MCP. You might have heard about the term "context engineering"; where you, as the person interacting with an LLM, are responsible for providing the right context to help it answer your questions. To gather this context, you can use tool calling to give the LLM access to a set of tools it can use to fetch information or take actions.</p>
<p>MCP helps by standardizing how your agent connects to these tools. But to your LLM, there’s no difference between “regular” tool calling and using a standard like MCP. It only sees a list of tool definitions, it doesn’t know or care what’s happening behind the scenes. And that’s a good thing.</p>
<p>By using MCP you get access to thousands of tools, without writing custom integration logic for each one. It heavily simplifies setting up an agentic loop that involves tool calling, often with almost zero development time. You, the developer, are responsible for calling the tools. The LLM only generates a snippet of what tool(s) to call and with which input parameters.</p>
<p>In this blog post, I’ll break down how tool calling works, what MCP actually does, and how both relate to context engineering.</p>

<p>LLMs understand the concept of tool calling, sometimes also called tool use or function calling. You provide a list of tool definitions as part of your prompt. Each tool includes a name, description, and expected input parameters. Based on the question and available tools, the LLM may generate a call.</p>

<p>But here’s the important part: LLMs don’t know how to use tools. They don’t have native tool calling support. They just generate text that represents a function call.
</p><p><img src="https://hackteam.io/images/your-llm-does-not-care-about-mcp/tool-calling.png" alt="Input and output when interacting with a LLM"><span>Input and output when interacting with a LLM</span></p>
<p>In the diagram above, you can see what the LLM actually sees: a prompt made up of instructions, previous user messages, and a list of available tools. Based on that, the LLM generates a text response which might include a tool that your system should call. It doesn’t understand tools in a meaningful way, it’s just making a prediction.</p>
<p>Let's look at a more practical use case. For example, if you provide a tool called <code>get_weather</code> that takes a <code>location</code> as input, and then ask the model: "What’s the weather in San Jose, CA?" it might respond with:</p>
<pre><div><p><code>{
  "name": "get_weather",
  "input": {
    "location": "San Jose, CA"
  }
}</code></p></div></pre>
<p>The LLM is able to generate that snippet based on the context it was provided with, as you can see in the diagram below. The LLM doesn’t know how to call the <code>get_weather</code> tool, nor does it need to. Your agentic loop, or agentic application, is responsible for taking this output and making the actual API call or function invocation. It parses the generated tool name and inputs, runs the tool, and passes the result back to the LLM as a new message.</p>
<p><img src="https://hackteam.io/images/your-llm-does-not-care-about-mcp/tool-calling-flow.png" alt="Tool Calling flow interaction with a LLM"><span>Tool Calling flow interaction with a LLM</span></p>
<p>This separation of concerns is important. The LLM just generates predictions and your system handles the execution. And that brings us to where MCP fits in.</p>

<p>Model Context Protocol, or MCP, is a way to <a target="_blank" rel="noopener noreferrer" href="https://www.infoworld.com/article/4029634/what-is-model-context-protocol-how-mcp-bridges-ai-and-external-services.html">standardize how your agent connects</a> to data sources like tools, prompts, resources, and samples. Right now, MCP is best known for simplifying the tools side of that equation. Instead of manually writing code for each tool in a custom format, MCP defines a consistent schema and communication pattern. Think of it as a universal adapter (like USB-C) for tooling.</p>

<p>MCP usually involves three components: a host application, an MCP client, and one or more MCP servers. The host might be a chat app or IDE (like Cursor) that includes an MCP client capable of connecting to different servers. These servers expose tools, prompts, samples, or resources.</p>
<p>The way you interact with the LLM doesn’t change. What changes is how the tools are surfaced to it. The agentic application talks to the MCP client, which talks to the right server. Tools are described in a format the LLM can use.</p>
<p><img src="https://hackteam.io/images/your-llm-does-not-care-about-mcp/tool-calling-flow-mcp.png" alt="Tool Calling flow interaction with a LLM and MCP"><span>Tool Calling flow interaction with a LLM and MCP</span></p>
<p>For the same question, "What’s the weather in San Jose, CA?", the LLM will still get the same list of tools. And based on that list it will tell you what tool to call, how that tool is called is up to the developer. When using MCP, that tool will be called using MCP.</p>
<p>The benefit here isn’t for the LLM, it’s for you as the developer. MCP helps manage complexity of working with many different tools as your agent grows. It makes it easier to reuse tools across projects, enforce consistent formats, and plug into new systems without rewriting everything.</p>
<p>But the LLM will never know you are using MCP, unless you are letting it know in the system prompt of tool definitions. You, the developer, is responsible for calling the tools. The LLM only generates a snippet of what tool(s) to call with which input parameters.</p>
<p>Next, let’s look at how this fits into the bigger picture of context engineering, and why abstraction layers like MCP make things easier for humans, not models.</p>

<p>Context engineering is about giving your LLM the right inputs so it can generate useful outputs. That sounds simple, but it’s actually one of the most important parts of building effective AI systems.</p>
<p>When you ask a model a question, you’re really giving it a prompt -- a block of text it uses to predict the next block of text. The quality of that prompt directly affects the quality of the response.</p>
<p>This is where tools come in. Sometimes the model doesn’t have enough context to answer a question well. Maybe it needs real-time data, access to user profiles, or the ability to take action on behalf of the user. Tool calling lets you solve that by giving the model access to external systems, as you learned in this blog post.</p>
<p>But again, the model doesn’t need to know how those tools work. It just needs to know that they exist, what they’re for, and how to call them. That’s where context engineering meets tool design, you’re crafting a set of tool definitions that serve as part of the model’s prompt.</p>
<p><img src="https://hackteam.io/images/your-llm-does-not-care-about-mcp/overview.png" alt="Tool Calling as seen by a LLM"><span>Tool Calling as seen by a LLM</span></p>
<p>MCP makes that process cleaner and more repeatable. Instead of hardcoding tools or writing ad hoc wrappers, you define a structured interface once and expose it through MCP. The LLM still sees the same types of tool definitions, but now they’re easier to maintain and scale.</p>
<p>So in the end, MCP is a tool for us developers, not for the LLM. It helps us build more reliable, modular systems. And it helps us focus on context engineering without reinventing the plumbing every time.</p>

<ul depth="0">
<li><a target="_blank" rel="noopener noreferrer" href="https://hackteam.io/blog/build-your-first-mcp-server-with-typescript-in-under-10-minutes">Learn how to build a MCP Server in &lt;10 minutes</a></li>
<li><a target="_blank" rel="noopener noreferrer" href="https://hackteam.io/blog/build-test-mcp-server-typescript-mcp-inspector">Test MCP Servers using MCP Inspector</a></li>
</ul>
<p>If you found this tutorial helpful, don’t forget to share it with your network. For more content on AI and web development, subscribe to my <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/@gethackteam?sub_confirmation=1">YouTube channel</a> and connect with me on <a target="_blank" rel="noopener noreferrer" href="https://linkedin.com/in/gethackteam">LinkedIn</a>, <a target="_blank" rel="noopener noreferrer" href="https://x.com/gethackteam">X</a> or <a target="_blank" rel="noopener noreferrer" href="https://bsky.app/profile/gethackteam.bsky.social">Bluesky</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Ethics is being narrowed on purpose, like privacy was (167 pts)]]></title>
            <link>https://nimishg.substack.com/p/ai-ethics-is-being-narrowed-on-purpose</link>
            <guid>44823094</guid>
            <pubDate>Thu, 07 Aug 2025 11:20:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nimishg.substack.com/p/ai-ethics-is-being-narrowed-on-purpose">https://nimishg.substack.com/p/ai-ethics-is-being-narrowed-on-purpose</a>, See on <a href="https://news.ycombinator.com/item?id=44823094">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!DANS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F400a5908-8000-4717-8537-bd326bb46886_1024x1024.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!DANS!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F400a5908-8000-4717-8537-bd326bb46886_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!DANS!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F400a5908-8000-4717-8537-bd326bb46886_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!DANS!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F400a5908-8000-4717-8537-bd326bb46886_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!DANS!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F400a5908-8000-4717-8537-bd326bb46886_1024x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!DANS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F400a5908-8000-4717-8537-bd326bb46886_1024x1024.png" width="1024" height="1024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/400a5908-8000-4717-8537-bd326bb46886_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2158689,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://nimishg.substack.com/i/170332859?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F400a5908-8000-4717-8537-bd326bb46886_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!DANS!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F400a5908-8000-4717-8537-bd326bb46886_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!DANS!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F400a5908-8000-4717-8537-bd326bb46886_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!DANS!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F400a5908-8000-4717-8537-bd326bb46886_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!DANS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F400a5908-8000-4717-8537-bd326bb46886_1024x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><br><span>A few days ago, OpenAI released an open-source language model for the first time in a very long time.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-170332859" href="https://nimishg.substack.com/p/ai-ethics-is-being-narrowed-on-purpose#footnote-1-170332859" target="_self" rel="nofollow ugc noopener">1</a></span><span>  It had been promised for a while, but the deadline kept being pushed for “safety” concerns.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-170332859" href="https://nimishg.substack.com/p/ai-ethics-is-being-narrowed-on-purpose#footnote-2-170332859" target="_self" rel="nofollow ugc noopener">2</a></span></p><p><span>In fact, they’ve put quite a bit of time and effort into discussing safety</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-170332859" href="https://nimishg.substack.com/p/ai-ethics-is-being-narrowed-on-purpose#footnote-3-170332859" target="_self" rel="nofollow ugc noopener">3</a></span><span>, because, ostensibly, safety and ethics is at the top of people’s minds. </span></p><p>So, the public is worried about AI ethics, and OpenAI is putting efforts into making sure the AI is ethical. Sounds like a match. </p><p><span>Not just a match, but a great talking point. When the press or someone issues a question or challenge around ethics, they can point to the work they’re doing </span><strong>around that very subject</strong><span>, and superficially the questioner is shut down.</span></p><p><span>Except that’s not what people actually mean when they say “ethics”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-170332859" href="https://nimishg.substack.com/p/ai-ethics-is-being-narrowed-on-purpose#footnote-4-170332859" target="_self" rel="nofollow ugc noopener">4</a></span><span>.  People are far more concerned with the </span><em>real-world </em><span>implications of ethics: governance structures, accountability, how their data is used, jobs being lost, etc.  In other words, they’re not so worried about whether their models will swear or philosophically handle the </span><a href="https://en.wikipedia.org/wiki/Trolley_problem" rel="nofollow ugc noopener">trolley problem</a><span> so much as, you know, reality. What happens with the </span><em>humans</em><span> running the models? </span><em>Their</em><span> influx of power and resources? How will </span><em>they</em><span> hurt or harm society? </span></p><p>This isn’t the first time this “redefining a legitimate concern” tactic has been used in tech. Way back, in the one thousand nine hundred and 90s, telemarketer calls were even more ubiquitous than they are now, and puzzled recipients would often ask “how did you even get my number?”</p><p><span>The answer was that telemarketing companies would just buy customer lists from other companies, who naively didn’t understand the true value of what they had</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-170332859" href="https://nimishg.substack.com/p/ai-ethics-is-being-narrowed-on-purpose#footnote-5-170332859" target="_self" rel="nofollow ugc noopener">5</a></span><span>. It was a sketchy practice and there was a huge consumer backlash against it, leading to </span><strong>the</strong><span> privacy cop-out phrase: “we never share your data with third parties”.</span></p><p><span>The full statement should be “we never share your data with third parties because that would be dumb. If they want that data, they have to buy out the company. In fact, that’s a large part of our exit strategy and valuation”. Business-wise, this has become common knowledge, so the statement about third-parties is almost redundant.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-170332859" href="https://nimishg.substack.com/p/ai-ethics-is-being-narrowed-on-purpose#footnote-6-170332859" target="_self" rel="nofollow ugc noopener">6</a></span></p><p><span>When people express concerns about privacy nowadays, the concern is what </span><em>the company they’re interacting with right now</em><span> is doing with the data. There’s an app I’m required to have for my kids’ school. What kind of profile and behavior model are they building about me? Why? What about the one I’m required to have to buy parking? Or the one I’m required to have to ride the train?</span></p><p><span>Those concerns aren’t really discussed. Instead, privacy is redefined as “making sure people </span><em>who aren’t this company</em><span> won’t have access to your data”, and never “what exactly is </span><em>this</em><span> company going to do with my data?”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-170332859" href="https://nimishg.substack.com/p/ai-ethics-is-being-narrowed-on-purpose#footnote-7-170332859" target="_self" rel="nofollow ugc noopener">7</a></span></p><p>This narrow redefinition has become the accepted professional definition of the term. We have entire industries around procurement, compliance, testing and others to make sure the above standards of “privacy” are upheld. Don’t get me wrong — it’s definitely important to secure your data and to prevent data leaks and testing the security infrastructure and all that. If anything, in a ‘vibe code’ era of start-ups, it’s even more important to make sure baseline security practices are followed.</p><p>But, when it comes to addressing public concerns about privacy, it’s (deliberately) spending time, resources, and energy on a particular scope, and pretending this effort is your way of addressing a different scope.</p><p>It’s like when a politician is asked “Will you raise taxes?” and then answers with “I want to grow the economy”… it’s not actually addressing the question being asked. Only now, with privacy, there are whole ecosystems of process and tools that are dedicated to answering the wrong question, specifically so they don’t have to answer the right one.</p><p>AI is different in that it’s new and (for many) came out of nowhere when it comes to culture and ethics discussions around it.</p><p><span>In fact, the only thing we had to fall back on were sci-fi thought experiments (which we have </span><a href="https://en.wikipedia.org/wiki/Roko%27s_basilisk" rel="nofollow ugc noopener">plenty</a><span> </span><a href="https://en.wikipedia.org/wiki/Gray_goo" rel="nofollow ugc noopener">of</a><span>). They’re interesting, fun, profound, and from a business perspective, totally safe. I mean, no one wants an AI to trap them in some sort of </span><em>Black Mirror</em><span> simulation, or turn the world into paperclips or anything like that. If it earns you good PR, there’s no reason </span><em>not</em><span> to spend time on such issues. It’s also free publicity since the press eats that stuff up.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-8-170332859" href="https://nimishg.substack.com/p/ai-ethics-is-being-narrowed-on-purpose#footnote-8-170332859" target="_self" rel="nofollow ugc noopener">8</a></span></p><p>But, realistically, is that the actual danger? </p><p><span>One final AI thought experiment is the </span><a href="https://en.wikipedia.org/wiki/AI_alignment#Alignment_problem" rel="nofollow ugc noopener">alignment problem</a><span>. Basically, if we give an AI lots of resources and ask it to do something, how do we know it will do what we want it to, and not try to subvert us and… take over the world? How do we know it will stay on humanity’s side?</span></p><p><span>This is something that some companies have </span><a href="https://openai.com/index/our-approach-to-alignment-research/" rel="nofollow ugc noopener">whole</a><span> </span><a href="https://alignment.anthropic.com/" rel="nofollow ugc noopener">teams</a><span> dedicated to working on and see as a fundamental challenge of the AI age. I absolutely agree, but I don’t think they’re using the right premise or assumptions.</span></p><div><p><span>If we give </span><em>companies</em><span> unending hype, near unlimited government and scientific resources, all of our personal data including thoughts and behavior patterns, how do we know </span><em>their leaders</em><span> will do what we want them to, and not try to subvert us and… take over the world? How do we know </span><em>they</em><span> stay on humanity’s side?</span></p><p><span>See, AI ethics is quite important. But like everything else in AI, we have to be sure we understand the actual problems so we can set up the solutions right.</span></p></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Whispering Earring (103 pts)]]></title>
            <link>https://croissanthology.com/earring</link>
            <guid>44822684</guid>
            <pubDate>Thu, 07 Aug 2025 10:16:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://croissanthology.com/earring">https://croissanthology.com/earring</a>, See on <a href="https://news.ycombinator.com/item?id=44822684">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>


        
        <article>
    
    <div>
        <p>Cleaner, easier-to-reference repo of Scott Alexander’s <em>The Whispering Earring</em> (that’s the Schelling title, real title below). Original from livejournal is backed up <a href="https://gwern.net/doc/fiction/science-fiction/2012-10-03-yvain-thewhisperingearring.html">here</a>.</p>

<h2 id="clarity-didnt-work-trying-mysterianism">Clarity didn’t work, trying mysterianism</h2>

<p>In the treasure-vaults of Til Iosophrang rests the Whispering Earring, buried deep beneath a heap of gold where it can do no further harm.</p>

<p>The earring is a little topaz tetrahedron dangling from a thin gold wire. When worn, it whispers in the wearer’s ear: “Better for you if you take me off.” If the wearer ignores the advice, it never again repeats that particular suggestion.</p>

<p>After that, when the wearer is making a decision the earring whispers its advice, always of the form “Better for you if you…”. <em>The earring is always right</em>. It does not always give the best advice possible in a situation. It will not necessarily make its wearer King, or help her solve the miseries of the world. But its advice is always better than what the wearer would have come up with on her own.</p>

<p>It is not a taskmaster, telling you what to do in order to achieve some foreign goal. It always tells you what will make you happiest. If it would make you happiest to succeed at your work, it will tell you how best to complete it. If it would make you happiest to do a half-assed job at your work and then go home and spend the rest of the day in bed having vague sexual fantasies, the earring will tell you to do that. <em>The earring is never wrong.</em></p>

<p>The <em>Book of Dark Waves</em> gives the histories of two hundred seventy four people who previously wore the Whispering Earring. There are no recorded cases of a wearer regretting following the earring’s advice, and there are no recorded cases of a wearer not regretting disobeying the earring. <em>The earring is always right</em>.</p>

<p>The earring begins by only offering advice on major life decisions. However, as it gets to know a wearer, it becomes more gregarious, and will offer advice on everything from what time to go to sleep, to what to eat for breakfast. If you take its advice, you will find that breakfast food really hit the spot, that it was exactly what you wanted for breakfast that day even though you didn’t know it yourself. <em>The earring is never wrong</em>.</p>

<p>As it gets completely comfortable with its wearer, it begins speaking in its native language, a series of high-bandwidth hisses and clicks that correspond to individual muscle movements. At first this speech is alien and disconcerting, but by the magic of the earring it begins to make more and more sense. No longer are the earring’s commands momentous on the level of “Become a soldier”. No more are they even simple on the level of “Have bread for breakfast”. Now they are more like “Contract your biceps muscle about thirty-five percent of the way” or “Articulate the letter p”. <em>The earring is always right</em>. This muscle movement will no doubt be part of a supernaturally effective plan toward achieving whatever your goals at that moment may be.</p>

<p>Soon, reinforcement and habit-formation have done their trick. The connection between the hisses and clicks of the earring and the movements of the muscles have become instinctual, no more conscious than the reflex of jumping when someone hidden gives a loud shout behind you.</p>

<p>At this point no further change occurs in the behavior of the earring. The wearer lives an abnormally successful life, usually ending out as a rich and much-beloved pillar of the community with a large and happy family.</p>

<p>When Kadmi Rachumion came to Til Iosophrang, he took an unusual interest in the case of the earring. First, he confirmed from the records and the testimony of all living wearers that the earring’s first suggestion was always that the earring itself be removed. Second, he spent some time questioning the Priests of Beauty, who eventually admitted that when the corpses of the wearers were being prepared for burial, it was noted that their brains were curiously deformed: the neocortexes had wasted away, and the bulk of their mass was an abnormally hypertrophied mid- and lower-brain, especially the parts associated with reflexive action.</p>

<p>Finally, Kadmi-nomai asked the High Priest of Joy in Til Iosophrang for the earring, which he was given. After cutting a hole in his own earlobe with the tip of the Piercing Star, he donned the earring and conversed with it for two hours, asking various questions in Kalas, in Kadhamic, and in its own language. Finally he removed the artifact and recommended that the it be locked in the deepest and most inaccessible parts of the treasure vaults, a suggestion with which the Iosophrelin decided to comply.</p>

<p><strong>Niderion-nomai’s commentary</strong>: It is well that we are so foolish, or what little freedom we have would be wasted on us. It is for this that <em>Book of Cold Rain</em> says one must never take the shortest path between two points.</p>

    </div>
</article>


    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How AI Conquered the US Economy: A Visual FAQ (152 pts)]]></title>
            <link>https://www.derekthompson.org/p/how-ai-conquered-the-us-economy-a</link>
            <guid>44822665</guid>
            <pubDate>Thu, 07 Aug 2025 10:12:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.derekthompson.org/p/how-ai-conquered-the-us-economy-a">https://www.derekthompson.org/p/how-ai-conquered-the-us-economy-a</a>, See on <a href="https://news.ycombinator.com/item?id=44822665">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>The American economy has split in two. There’s a rip-roaring AI economy. And there’s a lackluster consumer economy.</p><p><span>You see it in the economic statistics. Last quarter, spending on artificial intelligence outpaced the growth in consumer spending. Without AI, US economic growth </span><a href="https://www.wsj.com/economy/the-ai-booms-hidden-risk-to-the-economy-731b00d6?mod=author_content_page_1_pos_1" rel="">would be meager.</a></p><p><span>You see it in stocks. In the last two years, </span><a href="https://privatebank.jpmorgan.com/content/dam/jpm-pb-aem/global/en/documents/eotm/summer-mailbag.pdf" rel="">about 60 percent</a><span> of the stock market’s growth has come from AI-related companies, such as Microsoft, Nvidia, and Meta. Without the AI boom, stock market returns would be putrid.</span></p><p><span>You see it in the business data. According to </span><a href="https://stripe.com/guides/indexing-the-ai-economy" rel="">Stripe</a><span>, firms that self-describe as “AI companies” are dominating revenue growth on the platform, and they’re far surpassing the growth rate of any other group.</span></p><p>Nobody can say for sure whether the AI boom is evidence of the next Industrial Revolution or the next big bubble. All we know is that it’s happening. We can all stop talking about “what will happen if AI dominates the economy at such-and-such future date?” No, the AI economy is here and now. We’re living in it, for better or worse.</p><p><span>So, what exactly </span><em>is</em><span> the artificial intelligence boom? How did it happen, where did all this money to build AI come from, who is using the technology, and is it making people more productive? Today, in a bit of a throwback to my early blogging years, I’m going to try to walk through an FAQ with graphs to create a visual guide to the question: </span></p><p>Artificial intelligence has a few simple ingredients: computer chips, racks of servers in data centers, huge amounts of electricity, and networking and cooling systems that keep everything running without overheating.</p><p><span>This hardware is immensely expensive. In the last six months, the four companies investing the most in artificial intelligence—Meta, Google, Microsoft, and Amazon—spent between $100 billion and $200 billion on chips, data centers, and the like. “The most valuable tech companies are buying and building stuff at a record pace,” </span><a href="https://www.wsj.com/tech/ai/silicon-valley-ai-infrastructure-capex-cffe0431" rel="">wrote</a><span> the Wall Street Journal’s Christopher Mims.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Nkpa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde0d8a15-6b98-4874-b1b7-0f2f30698bab_1155x887.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Nkpa!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde0d8a15-6b98-4874-b1b7-0f2f30698bab_1155x887.png 424w, https://substackcdn.com/image/fetch/$s_!Nkpa!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde0d8a15-6b98-4874-b1b7-0f2f30698bab_1155x887.png 848w, https://substackcdn.com/image/fetch/$s_!Nkpa!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde0d8a15-6b98-4874-b1b7-0f2f30698bab_1155x887.png 1272w, https://substackcdn.com/image/fetch/$s_!Nkpa!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde0d8a15-6b98-4874-b1b7-0f2f30698bab_1155x887.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Nkpa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde0d8a15-6b98-4874-b1b7-0f2f30698bab_1155x887.png" width="1155" height="887" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de0d8a15-6b98-4874-b1b7-0f2f30698bab_1155x887.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:887,&quot;width&quot;:1155,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:306661,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde0d8a15-6b98-4874-b1b7-0f2f30698bab_1155x887.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Nkpa!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde0d8a15-6b98-4874-b1b7-0f2f30698bab_1155x887.png 424w, https://substackcdn.com/image/fetch/$s_!Nkpa!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde0d8a15-6b98-4874-b1b7-0f2f30698bab_1155x887.png 848w, https://substackcdn.com/image/fetch/$s_!Nkpa!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde0d8a15-6b98-4874-b1b7-0f2f30698bab_1155x887.png 1272w, https://substackcdn.com/image/fetch/$s_!Nkpa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde0d8a15-6b98-4874-b1b7-0f2f30698bab_1155x887.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>WSJ</figcaption></figure></div><p>This is either the biggest tech-infrastructure project since the 1960s (since the beginning of the computer age) or the 1880s (the heyday of the railroad age).</p><p><span>In January, JP Morgan’s Michael Cembalest </span><a href="https://assets.jpmprivatebank.com/content/dam/jpm-pb-aem/global/en/documents/eotm/the-alchemists.pdf" rel="">calculated</a><span> that the leading AI chip manufacturer Nvidia is on pace to capture the highest share of market-wide capital spending since IBM’s peak revenues in 1969. Not to be outdone, the economic writer Paul Kedrosky </span><a href="https://paulkedrosky.com/honey-ai-capex-ate-the-economy/" rel="">has calculated</a><span> that AI capital expenditures as a share of GDP have already exceeded the dot-com boom and are now approaching levels not seen since the railroad build-out of the Gilded Age. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wqm2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d255833-0919-43de-b2ad-9bd427368b6d_1080x694.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wqm2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d255833-0919-43de-b2ad-9bd427368b6d_1080x694.png 424w, https://substackcdn.com/image/fetch/$s_!wqm2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d255833-0919-43de-b2ad-9bd427368b6d_1080x694.png 848w, https://substackcdn.com/image/fetch/$s_!wqm2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d255833-0919-43de-b2ad-9bd427368b6d_1080x694.png 1272w, https://substackcdn.com/image/fetch/$s_!wqm2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d255833-0919-43de-b2ad-9bd427368b6d_1080x694.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wqm2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d255833-0919-43de-b2ad-9bd427368b6d_1080x694.png" width="1080" height="694" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8d255833-0919-43de-b2ad-9bd427368b6d_1080x694.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:694,&quot;width&quot;:1080,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:99436,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d255833-0919-43de-b2ad-9bd427368b6d_1080x694.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!wqm2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d255833-0919-43de-b2ad-9bd427368b6d_1080x694.png 424w, https://substackcdn.com/image/fetch/$s_!wqm2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d255833-0919-43de-b2ad-9bd427368b6d_1080x694.png 848w, https://substackcdn.com/image/fetch/$s_!wqm2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d255833-0919-43de-b2ad-9bd427368b6d_1080x694.png 1272w, https://substackcdn.com/image/fetch/$s_!wqm2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d255833-0919-43de-b2ad-9bd427368b6d_1080x694.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>JPM</figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!2pDW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c1cb0e-2d05-41a6-a238-893813e36352_1322x834.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!2pDW!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c1cb0e-2d05-41a6-a238-893813e36352_1322x834.png 424w, https://substackcdn.com/image/fetch/$s_!2pDW!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c1cb0e-2d05-41a6-a238-893813e36352_1322x834.png 848w, https://substackcdn.com/image/fetch/$s_!2pDW!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c1cb0e-2d05-41a6-a238-893813e36352_1322x834.png 1272w, https://substackcdn.com/image/fetch/$s_!2pDW!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c1cb0e-2d05-41a6-a238-893813e36352_1322x834.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!2pDW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c1cb0e-2d05-41a6-a238-893813e36352_1322x834.png" width="1322" height="834" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/11c1cb0e-2d05-41a6-a238-893813e36352_1322x834.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:834,&quot;width&quot;:1322,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:164779,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c1cb0e-2d05-41a6-a238-893813e36352_1322x834.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!2pDW!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c1cb0e-2d05-41a6-a238-893813e36352_1322x834.png 424w, https://substackcdn.com/image/fetch/$s_!2pDW!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c1cb0e-2d05-41a6-a238-893813e36352_1322x834.png 848w, https://substackcdn.com/image/fetch/$s_!2pDW!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c1cb0e-2d05-41a6-a238-893813e36352_1322x834.png 1272w, https://substackcdn.com/image/fetch/$s_!2pDW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c1cb0e-2d05-41a6-a238-893813e36352_1322x834.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Today’s AI infrastructure boom is made possible by the extraordinary and unprecedented profits of today’s leading tech companies. As Cembalest </span><a href="https://www.theringer.com/podcasts/plain-english-with-derek-thompson/2025/01/07/the-big-2025-economy-forecast-ai-and-big-tech-nuclears-renaissance-trump-vs-china-and-whats-eating-europe" rel="">explained</a><span> on my podcast, today's leading tech companies have become so profitable in the last few years that their share of total “free cash flow” (meaning, revenue minus operating expenses and infrastructure) dwarfs anything we’ve seen since the end of World War II. These firms’ existing business models—whether it’s ads for Meta or search ads for Google—are strong enough to generate stupid amounts of money to throw at the next generation of technology. “They’re generating unprecedented amounts of free cash flow,” Cembalest told me. “They make oodles and oodles of money, which is why they can afford to be pouring hundreds of billions of dollars of capital spending each year into AI-related R&amp;D and infrastructure.”</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Zx6X!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b1bd0f7-8e00-48e8-b607-6035a53ac09b_1270x827.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Zx6X!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b1bd0f7-8e00-48e8-b607-6035a53ac09b_1270x827.png 424w, https://substackcdn.com/image/fetch/$s_!Zx6X!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b1bd0f7-8e00-48e8-b607-6035a53ac09b_1270x827.png 848w, https://substackcdn.com/image/fetch/$s_!Zx6X!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b1bd0f7-8e00-48e8-b607-6035a53ac09b_1270x827.png 1272w, https://substackcdn.com/image/fetch/$s_!Zx6X!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b1bd0f7-8e00-48e8-b607-6035a53ac09b_1270x827.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Zx6X!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b1bd0f7-8e00-48e8-b607-6035a53ac09b_1270x827.png" width="1270" height="827" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7b1bd0f7-8e00-48e8-b607-6035a53ac09b_1270x827.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:827,&quot;width&quot;:1270,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:106735,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b1bd0f7-8e00-48e8-b607-6035a53ac09b_1270x827.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Zx6X!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b1bd0f7-8e00-48e8-b607-6035a53ac09b_1270x827.png 424w, https://substackcdn.com/image/fetch/$s_!Zx6X!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b1bd0f7-8e00-48e8-b607-6035a53ac09b_1270x827.png 848w, https://substackcdn.com/image/fetch/$s_!Zx6X!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b1bd0f7-8e00-48e8-b607-6035a53ac09b_1270x827.png 1272w, https://substackcdn.com/image/fetch/$s_!Zx6X!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b1bd0f7-8e00-48e8-b607-6035a53ac09b_1270x827.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>JPM</figcaption></figure></div><p>I think so. There’s an interesting debate in finance circles now about why the stock market seems to be shrugging off the Trump tariffs and slowing growth. I think the clearest answer to this question is some combination of (a) some investors still think Trump will chicken out on the tariffs; (b) they don’t think the final effect of the tariffs will be very big; and (c) the tariffs don’t matter much to the digital economy, and AI-related stocks are dominating returns while the rest of the market collectively putters along.</p><p><span>As this chart from Societe Generale </span><a href="https://insight-public.sgmarkets.com/quant-motion-pictures/outside-of-the-top-10-stocks-sp500-forward-profits-haven-t-grown-in-three-years?utm_source=chatgpt.com" rel="">shows</a><span>, the ten largest companies in the S&amp;P 500 have so dominated net income growth in the last six years that it’s becoming more useful to think about an S&amp;P 10 vs an S&amp;P 490. If you’re a portfolio manager invested in the other 490 stocks, the last six years of equity returns aren’t very impressive because these companies have collectively not managed to increase their profits.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!y2id!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb65a1c13-91df-4e54-88a7-a74998480ac1_1058x624.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!y2id!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb65a1c13-91df-4e54-88a7-a74998480ac1_1058x624.png 424w, https://substackcdn.com/image/fetch/$s_!y2id!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb65a1c13-91df-4e54-88a7-a74998480ac1_1058x624.png 848w, https://substackcdn.com/image/fetch/$s_!y2id!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb65a1c13-91df-4e54-88a7-a74998480ac1_1058x624.png 1272w, https://substackcdn.com/image/fetch/$s_!y2id!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb65a1c13-91df-4e54-88a7-a74998480ac1_1058x624.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!y2id!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb65a1c13-91df-4e54-88a7-a74998480ac1_1058x624.png" width="1058" height="624" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b65a1c13-91df-4e54-88a7-a74998480ac1_1058x624.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:624,&quot;width&quot;:1058,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:270757,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb65a1c13-91df-4e54-88a7-a74998480ac1_1058x624.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!y2id!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb65a1c13-91df-4e54-88a7-a74998480ac1_1058x624.png 424w, https://substackcdn.com/image/fetch/$s_!y2id!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb65a1c13-91df-4e54-88a7-a74998480ac1_1058x624.png 848w, https://substackcdn.com/image/fetch/$s_!y2id!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb65a1c13-91df-4e54-88a7-a74998480ac1_1058x624.png 1272w, https://substackcdn.com/image/fetch/$s_!y2id!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb65a1c13-91df-4e54-88a7-a74998480ac1_1058x624.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Societe Generale</figcaption></figure></div><p><span>Not yet. As the </span><em>Wall Street Journal</em><span>'s Greg Ip </span><a href="https://www.wsj.com/economy/the-ai-booms-hidden-risk-to-the-economy-731b00d6?mod=author_content_page_1_pos_1" rel="">wrote</a><span>, the "unsettling” side of the AI boom is that all this spending on chips and data centers is "draining American corporations of cash." OpenAI and Anthropic are losing gobs of money, and the biggest tech companies are still relying on their older business models to generate their largest profit margins. If these firms are spending much more than they’ll ever be able to earn back, it would suggest that we’re in the midst of a historic infrastructure bubble.</span></p><p><span>As for the bull case: The payments company Stripe is already seeing evidence that AI startup revenue is exceeding the growth rate of </span><em>any</em><span> previous generation of technology.</span><strong> “</strong><span>AI companies are reaching revenue milestones faster than previous generations of startups,” the company </span><a href="https://stripe.com/blog/inside-the-growth-of-the-top-ai-companies-on-stripe" rel="">announced</a><span> in a recent report. “The top 100 AI companies on Stripe achieved annualized revenues of $1 million in a median period of just 11.5 months—four months ahead of the fastest-growing SaaS companies.”</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Bqd0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e175aac-6ebf-4dc9-8cea-de2bf7cc932b_1643x766.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Bqd0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e175aac-6ebf-4dc9-8cea-de2bf7cc932b_1643x766.png 424w, https://substackcdn.com/image/fetch/$s_!Bqd0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e175aac-6ebf-4dc9-8cea-de2bf7cc932b_1643x766.png 848w, https://substackcdn.com/image/fetch/$s_!Bqd0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e175aac-6ebf-4dc9-8cea-de2bf7cc932b_1643x766.png 1272w, https://substackcdn.com/image/fetch/$s_!Bqd0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e175aac-6ebf-4dc9-8cea-de2bf7cc932b_1643x766.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Bqd0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e175aac-6ebf-4dc9-8cea-de2bf7cc932b_1643x766.png" width="1456" height="679" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5e175aac-6ebf-4dc9-8cea-de2bf7cc932b_1643x766.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:679,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:283989,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e175aac-6ebf-4dc9-8cea-de2bf7cc932b_1643x766.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Bqd0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e175aac-6ebf-4dc9-8cea-de2bf7cc932b_1643x766.png 424w, https://substackcdn.com/image/fetch/$s_!Bqd0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e175aac-6ebf-4dc9-8cea-de2bf7cc932b_1643x766.png 848w, https://substackcdn.com/image/fetch/$s_!Bqd0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e175aac-6ebf-4dc9-8cea-de2bf7cc932b_1643x766.png 1272w, https://substackcdn.com/image/fetch/$s_!Bqd0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e175aac-6ebf-4dc9-8cea-de2bf7cc932b_1643x766.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Stripe</figcaption></figure></div><p><span>By one account, generative AI tools like ChatGPT and Gemini are being adopted faster than practically any technology for which we have good data. The St. Louis Federal Reserve </span><a href="https://www.stlouisfed.org/on-the-economy/2024/sep/rapid-adoption-generative-ai" rel="">has estimated</a><span> that the rate of adoption for generative AI is roughly twice as fast as the Internet.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!k_ds!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e1e6d6-8f0f-4518-baf2-a6d0256525ad_1623x1289.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!k_ds!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e1e6d6-8f0f-4518-baf2-a6d0256525ad_1623x1289.png 424w, https://substackcdn.com/image/fetch/$s_!k_ds!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e1e6d6-8f0f-4518-baf2-a6d0256525ad_1623x1289.png 848w, https://substackcdn.com/image/fetch/$s_!k_ds!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e1e6d6-8f0f-4518-baf2-a6d0256525ad_1623x1289.png 1272w, https://substackcdn.com/image/fetch/$s_!k_ds!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e1e6d6-8f0f-4518-baf2-a6d0256525ad_1623x1289.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!k_ds!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e1e6d6-8f0f-4518-baf2-a6d0256525ad_1623x1289.png" width="1456" height="1156" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a4e1e6d6-8f0f-4518-baf2-a6d0256525ad_1623x1289.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1156,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:258824,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e1e6d6-8f0f-4518-baf2-a6d0256525ad_1623x1289.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!k_ds!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e1e6d6-8f0f-4518-baf2-a6d0256525ad_1623x1289.png 424w, https://substackcdn.com/image/fetch/$s_!k_ds!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e1e6d6-8f0f-4518-baf2-a6d0256525ad_1623x1289.png 848w, https://substackcdn.com/image/fetch/$s_!k_ds!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e1e6d6-8f0f-4518-baf2-a6d0256525ad_1623x1289.png 1272w, https://substackcdn.com/image/fetch/$s_!k_ds!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e1e6d6-8f0f-4518-baf2-a6d0256525ad_1623x1289.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>In one the largest recent surveys of generative AI—the 2025 paper </span><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5136877" rel="">"The Labor Market Effects of Generative Artificial Intelligence</a><span>”—economists estimated that more than 50 percent of workers in information services (meaning, software firms) and management are already using the technology at work. That compares with very few people in old-economy firms, such as mining or fishing. AI is also much more popular among college graduates than people who never attended college.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!F9t1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6743275-a7a2-4eba-bad3-6d765a996bf2_1609x1107.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!F9t1!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6743275-a7a2-4eba-bad3-6d765a996bf2_1609x1107.png 424w, https://substackcdn.com/image/fetch/$s_!F9t1!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6743275-a7a2-4eba-bad3-6d765a996bf2_1609x1107.png 848w, https://substackcdn.com/image/fetch/$s_!F9t1!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6743275-a7a2-4eba-bad3-6d765a996bf2_1609x1107.png 1272w, https://substackcdn.com/image/fetch/$s_!F9t1!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6743275-a7a2-4eba-bad3-6d765a996bf2_1609x1107.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!F9t1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6743275-a7a2-4eba-bad3-6d765a996bf2_1609x1107.png" width="1456" height="1002" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a6743275-a7a2-4eba-bad3-6d765a996bf2_1609x1107.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1002,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:305371,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6743275-a7a2-4eba-bad3-6d765a996bf2_1609x1107.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!F9t1!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6743275-a7a2-4eba-bad3-6d765a996bf2_1609x1107.png 424w, https://substackcdn.com/image/fetch/$s_!F9t1!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6743275-a7a2-4eba-bad3-6d765a996bf2_1609x1107.png 848w, https://substackcdn.com/image/fetch/$s_!F9t1!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6743275-a7a2-4eba-bad3-6d765a996bf2_1609x1107.png 1272w, https://substackcdn.com/image/fetch/$s_!F9t1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6743275-a7a2-4eba-bad3-6d765a996bf2_1609x1107.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Hartley, et al</figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ySxl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b00b72e-7c86-4048-b7a0-d9a27976c369_1633x1086.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ySxl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b00b72e-7c86-4048-b7a0-d9a27976c369_1633x1086.png 424w, https://substackcdn.com/image/fetch/$s_!ySxl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b00b72e-7c86-4048-b7a0-d9a27976c369_1633x1086.png 848w, https://substackcdn.com/image/fetch/$s_!ySxl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b00b72e-7c86-4048-b7a0-d9a27976c369_1633x1086.png 1272w, https://substackcdn.com/image/fetch/$s_!ySxl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b00b72e-7c86-4048-b7a0-d9a27976c369_1633x1086.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ySxl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b00b72e-7c86-4048-b7a0-d9a27976c369_1633x1086.png" width="1456" height="968" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1b00b72e-7c86-4048-b7a0-d9a27976c369_1633x1086.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:968,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:193624,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b00b72e-7c86-4048-b7a0-d9a27976c369_1633x1086.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ySxl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b00b72e-7c86-4048-b7a0-d9a27976c369_1633x1086.png 424w, https://substackcdn.com/image/fetch/$s_!ySxl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b00b72e-7c86-4048-b7a0-d9a27976c369_1633x1086.png 848w, https://substackcdn.com/image/fetch/$s_!ySxl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b00b72e-7c86-4048-b7a0-d9a27976c369_1633x1086.png 1272w, https://substackcdn.com/image/fetch/$s_!ySxl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b00b72e-7c86-4048-b7a0-d9a27976c369_1633x1086.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Hartley, et al</figcaption></figure></div><p><span>Yes. The iconic study proving that new AI models improve productivity comes from firms with rather repetitive work, such as </span><a href="https://hai.stanford.edu/news/will-generative-ai-make-you-more-productive-work-yes-only-if-youre-not-already-great-your-job" rel="">call centers</a><span>. But we’re getting more self-reports from workers saying that AI is helping them save lots of time. One surprising example: teaching. According to a recent </span><a href="https://news.gallup.com/poll/691967/three-teachers-weekly-saving-six-weeks-year.aspx" rel="">Gallup survey</a><span>, roughly 60 percent of elementary school teachers say they’ve used AI to prepare lessons, review instruction material, make worksheets, or do administrative work. Most teachers who use AI say it improves their work, and those who use it regularly say it saves them 6 hours a week—or six weeks per school year. In one very optimistic interpretation, that’s like saying AI gives elementary school teachers a month and a half of paid leave every year.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!UOEU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8687f204-6def-4c30-b3e4-ef82b0ecb12d_1349x1196.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!UOEU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8687f204-6def-4c30-b3e4-ef82b0ecb12d_1349x1196.png 424w, https://substackcdn.com/image/fetch/$s_!UOEU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8687f204-6def-4c30-b3e4-ef82b0ecb12d_1349x1196.png 848w, https://substackcdn.com/image/fetch/$s_!UOEU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8687f204-6def-4c30-b3e4-ef82b0ecb12d_1349x1196.png 1272w, https://substackcdn.com/image/fetch/$s_!UOEU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8687f204-6def-4c30-b3e4-ef82b0ecb12d_1349x1196.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!UOEU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8687f204-6def-4c30-b3e4-ef82b0ecb12d_1349x1196.png" width="1349" height="1196" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8687f204-6def-4c30-b3e4-ef82b0ecb12d_1349x1196.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1196,&quot;width&quot;:1349,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:284978,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8687f204-6def-4c30-b3e4-ef82b0ecb12d_1349x1196.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!UOEU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8687f204-6def-4c30-b3e4-ef82b0ecb12d_1349x1196.png 424w, https://substackcdn.com/image/fetch/$s_!UOEU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8687f204-6def-4c30-b3e4-ef82b0ecb12d_1349x1196.png 848w, https://substackcdn.com/image/fetch/$s_!UOEU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8687f204-6def-4c30-b3e4-ef82b0ecb12d_1349x1196.png 1272w, https://substackcdn.com/image/fetch/$s_!UOEU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8687f204-6def-4c30-b3e4-ef82b0ecb12d_1349x1196.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Gallup</figcaption></figure></div><p>Perhaps the most bullish indicator that AI is going to help people become more productive at work comes from the AI research nonprofit METR, which found that the length of tasks that AI agents can complete is doubling every 7 months. In 2021, AI could automate a simple Google search: 10 seconds. Two years later, ChatGPT was looking up facts on the Internet that would take the typical person about 4 minutes. Now some models are performing coding tasks that take a typical developer 50 minutes. “Extrapolating this trend predicts that, in under a decade, we will see AI agents that can independently complete a large fraction of software tasks that currently take humans days or weeks,” the researchers said.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!E3fb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f5ced6-480d-42d7-a001-2cf3573c602c_1395x973.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!E3fb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f5ced6-480d-42d7-a001-2cf3573c602c_1395x973.png 424w, https://substackcdn.com/image/fetch/$s_!E3fb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f5ced6-480d-42d7-a001-2cf3573c602c_1395x973.png 848w, https://substackcdn.com/image/fetch/$s_!E3fb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f5ced6-480d-42d7-a001-2cf3573c602c_1395x973.png 1272w, https://substackcdn.com/image/fetch/$s_!E3fb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f5ced6-480d-42d7-a001-2cf3573c602c_1395x973.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!E3fb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f5ced6-480d-42d7-a001-2cf3573c602c_1395x973.png" width="1395" height="973" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/40f5ced6-480d-42d7-a001-2cf3573c602c_1395x973.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:973,&quot;width&quot;:1395,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:318042,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f5ced6-480d-42d7-a001-2cf3573c602c_1395x973.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!E3fb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f5ced6-480d-42d7-a001-2cf3573c602c_1395x973.png 424w, https://substackcdn.com/image/fetch/$s_!E3fb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f5ced6-480d-42d7-a001-2cf3573c602c_1395x973.png 848w, https://substackcdn.com/image/fetch/$s_!E3fb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f5ced6-480d-42d7-a001-2cf3573c602c_1395x973.png 1272w, https://substackcdn.com/image/fetch/$s_!E3fb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f5ced6-480d-42d7-a001-2cf3573c602c_1395x973.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Not so fast. In fact, many workers might dramatically </span><em>overestimate</em><span> how much more productive AI is making them.</span></p><p><span>METR also </span><a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/" rel="">conducted</a><span> an in-depth study that asked experienced developers to code with a popular AI assistant. After they finished their tasks, the developers claimed that using the AI had made them 20 percent more productive. But independent evaluators in the study actually concluded that using AI did the opposite: it </span><em>increased</em><span> task completion time by about 20 percent. I don’t want to speculate too much about what this study means in the long run. But for now, I think it’s a necessary caveat to boosterish claims that ChatGPT is on the cusp of replacing tens of millions of entry-level white-collar jobs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!aH-u!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F230b051c-0861-49bc-bdf5-5ffab4b5d8e1_1557x942.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!aH-u!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F230b051c-0861-49bc-bdf5-5ffab4b5d8e1_1557x942.png 424w, https://substackcdn.com/image/fetch/$s_!aH-u!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F230b051c-0861-49bc-bdf5-5ffab4b5d8e1_1557x942.png 848w, https://substackcdn.com/image/fetch/$s_!aH-u!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F230b051c-0861-49bc-bdf5-5ffab4b5d8e1_1557x942.png 1272w, https://substackcdn.com/image/fetch/$s_!aH-u!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F230b051c-0861-49bc-bdf5-5ffab4b5d8e1_1557x942.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!aH-u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F230b051c-0861-49bc-bdf5-5ffab4b5d8e1_1557x942.png" width="1456" height="881" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/230b051c-0861-49bc-bdf5-5ffab4b5d8e1_1557x942.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:881,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:243646,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F230b051c-0861-49bc-bdf5-5ffab4b5d8e1_1557x942.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!aH-u!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F230b051c-0861-49bc-bdf5-5ffab4b5d8e1_1557x942.png 424w, https://substackcdn.com/image/fetch/$s_!aH-u!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F230b051c-0861-49bc-bdf5-5ffab4b5d8e1_1557x942.png 848w, https://substackcdn.com/image/fetch/$s_!aH-u!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F230b051c-0861-49bc-bdf5-5ffab4b5d8e1_1557x942.png 1272w, https://substackcdn.com/image/fetch/$s_!aH-u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F230b051c-0861-49bc-bdf5-5ffab4b5d8e1_1557x942.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>A new paper in </span><em>Science</em><span> </span><a href="https://www.science.org/doi/10.1126/sciadv.adt3813?utm_campaign=ScienceMagazine&amp;utm_source=twitter&amp;utm_medium=ownedSocial" rel="">found</a><span> that since the rise of large language models, there's been a huge shift in academic writing. In 2024, the word "delves" has appeared 2,700% more than its historical average, by one account. The analysis suggests that about 1/7th of 2024 abstracts were processed by AI.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!UDeX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d360121-e1d1-490e-81e8-c170abd9efe3_1658x1416.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!UDeX!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d360121-e1d1-490e-81e8-c170abd9efe3_1658x1416.png 424w, https://substackcdn.com/image/fetch/$s_!UDeX!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d360121-e1d1-490e-81e8-c170abd9efe3_1658x1416.png 848w, https://substackcdn.com/image/fetch/$s_!UDeX!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d360121-e1d1-490e-81e8-c170abd9efe3_1658x1416.png 1272w, https://substackcdn.com/image/fetch/$s_!UDeX!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d360121-e1d1-490e-81e8-c170abd9efe3_1658x1416.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!UDeX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d360121-e1d1-490e-81e8-c170abd9efe3_1658x1416.png" width="1456" height="1243" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4d360121-e1d1-490e-81e8-c170abd9efe3_1658x1416.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1243,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1132159,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/170278858?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d360121-e1d1-490e-81e8-c170abd9efe3_1658x1416.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!UDeX!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d360121-e1d1-490e-81e8-c170abd9efe3_1658x1416.png 424w, https://substackcdn.com/image/fetch/$s_!UDeX!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d360121-e1d1-490e-81e8-c170abd9efe3_1658x1416.png 848w, https://substackcdn.com/image/fetch/$s_!UDeX!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d360121-e1d1-490e-81e8-c170abd9efe3_1658x1416.png 1272w, https://substackcdn.com/image/fetch/$s_!UDeX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d360121-e1d1-490e-81e8-c170abd9efe3_1658x1416.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Kobak, et al</figcaption></figure></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["I closed MPEG on 2 Jun '20 when I left because obscure forces had hijacked it." (201 pts)]]></title>
            <link>https://leonardo.chiariglione.org/</link>
            <guid>44822637</guid>
            <pubDate>Thu, 07 Aug 2025 10:09:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://leonardo.chiariglione.org/">https://leonardo.chiariglione.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44822637">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="5c35ece" data-element_type="section" data-widget_type="text-editor.default">
									<p><span>I needed an organisation that would</span><em>&nbsp;create digital media standards for <span>consumers</span>&nbsp;to seamlessly communicate and&nbsp;<span>industry</span>&nbsp;operate in a global market of interoperable products, services and applications</em><span>.&nbsp;</span><span>I conceived&nbsp;</span><span>t</span><span>hat organisation&nbsp;</span><span>in 1987, established it in 1988</span><span>I, and called </span><span>&nbsp;</span><a href="http://mpeg.chiariglione.org/">Moving Picture Experts Group</a><span>&nbsp;(MPEG). In four years,&nbsp;</span><span>MPEG had&nbsp;</span><span>ushered in the&nbsp;</span><span><i>digital media age </i><span>with&nbsp;</span></span><span>MPEG-1,&nbsp;</span><span>a standard for interactive media&nbsp;</span><span>used in Video CD, digital audio broadcasting (MP2), and personal music (MP3). Starting from the mid ’90s, MPEG-2, the result of the second MPEG project, became the common infrastructure that underpinned distribution of digital television via cable, satellite, terrestrial networks and package media (DVD). MPEG-4, the third standard first released in 1988, opened the way to digital media distribution over the internet. Several families of standards followed: MPEG-7, MPEG-21, </span><span>MPEG-A, MPEG-H, MPEG-I and more.</span></p><p><span>I chaired the group </span><span>fostering its productivity with the development of over 200 standards, membership with a 20-fold growth in attendance </span><span>from the initial 29 experts attending the first meeting, and scope extending from media to genomics, the “born digital” data of the world..&nbsp;</span></p><p><span>I closed MPEG on 2 June 2020 when I left because&nbsp;</span><span>obscure forces</span><span>&nbsp;had hijacked it.&nbsp;</span></p><p>Even before it has ceased to exists, the MPEG engine had run out of steam – technology- and business wise. The same obscure forces that have hijacked MPEG had kept it hostage to their interests impeding its technical development and keeping it locked to outmoded Intellectual Property licensing models delaying market adoption of MPEG standards. Industry has been strangled&nbsp;<span>and consumers have been deprived of the benefits of new technologies. From&nbsp;</span><span>facilitators&nbsp;</span><span>of new opportunities and experiences, MPEG s</span><span>tandards have morphed from </span><span>&nbsp;into </span><span>roadblocks.</span></p><p>On the 19th of July 2020 I proposed and, on 30 September 2020, a group of 33 companies established the&nbsp;<a href="http://mpai.community/" target="_blank" rel="nofollow noopener noreferrer">Moving Picture, Audio and Data Coding by Artificial Intelligence</a>&nbsp;(MPAI). Industry and consumers have now an organisation developing standards based on powerful technologies, overcoming stagnation and licensing stalemates. Five standards covering <a href="https://mpai.community/standards/mpai-aif/">execution of AI applications</a>, <a href="https://mpai.community/standards/mpai-cae/">audio enhancement</a>, <a href="https://mpai.community/standards/mpai-mmc/">multimodal conversation</a>, <a href="https://mpai.community/standards/mpai-cui/">company performance prediction</a>, and <a href="https://mpai.community/standards/mpai-gme/">ecosystem governance</a> have been developed and adopted. More standards are in the pipeline: <a href="https://mpai.community/standards/mpai-eev/">AI-based End-to-End Video Coding</a>, <a href="https://mpai.community/standards/mpai-evc/">AI-Enhanced Video Coding</a>, <a href="https://mpai.community/standards/mpai-aih/">AI Health</a>, <a href="https://mpai.community/standards/mpai-ara/">Avatar Representation and Animation</a>, <a href="https://mpai.community/standards/mpai-cav/">Connected Autonomous Vehicles</a>, <a href="https://mpai.community/standards/mpai-mmm/">Metaverse Model</a>,&nbsp; <a href="https://mpai.community/standards/mpai-spg/">Server-based Predictive Multiplayer Gaming</a>, and <a href="https://mpai.community/standards/mpai-xrv/">XR Venues</a>.</p><p>The book <a href="https://leonardo.chiariglione.org/publications/even-the-stars-die/"><em>Even the stars die</em></a> tell the MPEG story from birth to death and the MPAI story from birth to growth to death. The book <a href="https://mpai.community/the-mpai-book-2021/"><em>Towards Pervasive and Trustworthy Artificial Intelligence</em></a> tells the MPAI story in its first 15 months of life: 5 standards produced and 7 projects under way1998.</p>								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New AI Coding Teammate: Gemini CLI GitHub Actions (220 pts)]]></title>
            <link>https://blog.google/technology/developers/introducing-gemini-cli-github-actions/</link>
            <guid>44822389</guid>
            <pubDate>Thu, 07 Aug 2025 09:28:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/developers/introducing-gemini-cli-github-actions/">https://blog.google/technology/developers/introducing-gemini-cli-github-actions/</a>, See on <a href="https://news.ycombinator.com/item?id=44822389">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="jump-content" tabindex="-1">
            

    
    

    <article>

    
    





    

    
      








<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Meet your new AI coding teammate: Gemini CLI GitHub Actions&quot;
  }">
  
  <div>
      
      
        <p>
          Gemini CLI GitHub Actions is a no-cost, powerful AI coding teammate for your repository. It acts both as an autonomous agent for critical routine coding tasks, and an on-demand collaborator you can quickly delegate work to.
        </p>
      
    </div>
  
  <div>
  <p>Ryan J. Salva</p>
  
    <p>
      Senior Director, Product Management
    </p>
  
  
</div>
</div>

    

    
      










<div>
    <figure>
      <div>
        <p><img alt="Gemini CLI GitHub Actions implements a feature at a user's request" data-component="uni-progressive-image" fetchpriority="high" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/All-Three_1.width-200.format-webp.webp" width="360px" data-sizes="(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px" data-srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/All-Three_1.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/All-Three_1.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/All-Three_1.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/All-Three_1.width-2200.format-webp.webp 2200w">
        </p>
      </div>
      
    </figure>
  </div>






    

    
    <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="Meet your new AI coding teammate: Gemini CLI GitHub Actions" listen-to-article="Listen to article" data-date-modified="2025-08-06T01:00:00.396657+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js" data-highlight-mode="word-over-paragraph"></uni-article-speakable>

            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Meet your new AI coding teammate: Gemini CLI GitHub Actions&quot;
         }"><p data-block-key="qeat0">In June, we launched <a href="https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/">Gemini CLI</a>, an open-source AI agent that brings the power of Gemini to your terminal. The enthusiastic adoption from developers has been incredible. To keep up with the flood of feature requests and contributions, we put our own tool to the test — using Gemini CLI to automate issue triage and pull request reviews. When community members noticed our new workflows, they asked us to share what we’ve built.</p><p data-block-key="dkm0r">Today, we’re introducing <b>Gemini CLI GitHub Actions</b>. It’s a no-cost, powerful AI coding teammate for your repository. It acts both as an autonomous agent for critical routine coding tasks, and an on-demand collaborator you can quickly delegate work to.</p><p data-block-key="4h2fu">It’s now in beta, available to everyone worldwide, and you can find it on GitHub at <a href="https://github.com/google-github-actions/run-gemini-cli">google-github-actions/run-gemini-cli</a>.</p><h3 data-block-key="d3s1s"><b>An AI teammate in your repository</b></h3><p data-block-key="5qn4">While Gemini CLI is a tool built for individual use in your own terminal, Gemini CLI GitHub Actions was created for team collaboration on the platform where developers work with each other.</p><p data-block-key="3tmop">Triggered by events like new issues or pull requests, it works asynchronously in the background, using the full context of your project to automatically handle tasks. It knows your code, understands what you want to do, and gets it done.</p><p data-block-key="413o5">We’re launching with three powerful, open-source workflows that can help you code better, faster:</p><ol><li data-block-key="1fbho"><b>🤖Intelligent issue triage</b>: Automate the overhead of managing new issues. Gemini CLI can analyze, label and prioritize incoming issues, helping focus your attention on what matters most.</li><li data-block-key="fu7pb"><b>🚀Accelerated pull request reviews</b>: Get instant, insightful feedback on code changes. Gemini CLI can review pull requests for quality, style and correctness, freeing up reviewers to focus on more complex tasks and decisions.</li><li data-block-key="2q78t"><b>🤝On-demand collaboration</b>: Simply mention @gemini-cli in any issue or pull request to delegate tasks. Tell it to do things like, "write tests for this bug," "implement the changes suggested above," "brainstorm alternative solutions," or "fix this well defined bug."</li></ol></div>
  

  
    

















<uni-image-carousel section-header="Meet your new AI coding teammate: Gemini CLI GitHub Actions" images="[
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/CLI_Labels.mp4 &quot;],
        
        &quot;alt&quot;: &quot;Easily create new feature requests on GitHub for Gemini CLI to handle on your behalf&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;Gemini CLI GitHub Actions labels workflow&quot;
      },
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/CLI_Pull_Request.mp4 &quot;],
        
        &quot;alt&quot;: &quot;Gemini CLI GitHub Actions can handle your pull requests, providing code changes and AI\u002Dgenerated suggestions for improving the user experience&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;Gemini CLI GitHub Actions Pull Request&quot;
      },
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/CLI_Comment.mp4 &quot;],
        
        &quot;alt&quot;: &quot;Delegate work with an \u0022@gemini\u002Dcli\u0022 tag and the agent can complete a range of tasks, from writing bugs to fixing bugs&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;Gemini CLI GitHub Actions feature request&quot;
      }
    
  ]">
  
    
      <div slot="caption-slot-0">
        <p data-block-key="3g0nl">Easily create new feature requests on GitHub for Gemini CLI to handle on your behalf</p>
      </div>
    
  
    
      <div slot="caption-slot-1">
        <p data-block-key="foxmz">Gemini CLI GitHub Actions can handle your pull requests, providing code changes and AI-generated suggestions for improving the user experience</p>
      </div>
    
  
    
      <div slot="caption-slot-2">
        <p data-block-key="foxmz">Delegate work with an "@gemini-cli" tag and the agent can complete a range of tasks, from writing bugs to fixing bugs</p>
      </div>
    
  
</uni-image-carousel>

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Meet your new AI coding teammate: Gemini CLI GitHub Actions&quot;
         }"><p data-block-key="qeat0">Think of these initial workflows as your launchpad. They are open-source and fully customizable — you can create your own workflows, or configure the ones that come built into Gemini CLI GitHub Actions.</p><h2 data-block-key="fe8q4">Built with enterprise-grade security and control</h2><p data-block-key="16p06">Robust security measures are a fundamental part of modern software development. That’s why we built Gemini CLI GitHub Actions with security and flexibility at its core.</p><p data-block-key="ataps">You are always in control with capabilities including:</p><ul><li data-block-key="9s87r"><b>Secure, credential-less authentication:</b> Vertex AI and Gemini Code Assist Standard and Enterprise users can tap into Google Cloud's <a href="https://cloud.google.com/iam/docs/workload-identity-federation">Workload Identity Federation</a> (WIF) to eliminate the need for long-lived API keys in your environment, drastically reducing the risk of credential compromise.</li><li data-block-key="e2u6u"><b>Granular control</b>: Enforce the principle of least privilege with multi-layered controls. Use capabilities like command allowlisting to explicitly approve every shell command the agent can execute. You can also create a custom identity for the agent (e.g., gemini-for-your-org) and grant it only the precise permissions it needs.</li><li data-block-key="d5i11"><b>Complete transparency:</b> GitHub on CLI comes integrated with <a href="https://opentelemetry.io/">OpenTelemetry</a>, an industry standard for telemetry, so you can stream logs and metrics to your preferred observability platform, like Google Cloud Monitoring. This gives you full, real-time visibility into every action to monitor usage and debug complex workflows.</li></ul><h2 data-block-key="ffuab">Get started today</h2><p data-block-key="3bqq">What will you build with your new coding teammate? A workflow that automatically generates release notes? One that keeps documentation in sync with your code? Don’t just imagine it; build it. We invite you to contribute your innovative workflows to our repository and share them with the community.</p><p data-block-key="f79fk">Gemini CLI GitHub Actions is <a href="https://github.com/google-github-actions/run-gemini-cli">available today</a> in beta, with <a href="https://github.com/google-gemini/gemini-cli/blob/main/docs/quota-and-pricing.md#gemini-cli-quotas-and-pricing">generous free-of-charge quotas</a> for Google AI Studio. Vertex AI, along with the Standard and Enterprise tiers of Gemini Code Assist, are also supported. We will have free-of-charge use for Gemini Code Assist for individual users available soon.</p><p data-block-key="mgkb">To get started, <a href="https://github.com/google-gemini/gemini-cli">download</a> Gemini CLI 0.1.18 or later and run `/setup-github`. You can find the GitHub Action at <a href="https://github.com/google-github-actions/run-gemini-cli">google-github-actions/run-gemini-cli</a>.</p></div>
  


            
            

            
              




            
          </div>
  </article>
  





  

  


<div data-component="uni-related-articles" aria-roledescription="carousel" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Footer Related Stories&quot;,
    &quot;section_header&quot;: &quot;Related stories&quot;
  }">
        <h3>
          <p>
            Related stories
          </p>
        </h3>
      </div></div></div>]]></description>
        </item>
    </channel>
</rss>