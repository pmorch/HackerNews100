<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 08 Aug 2025 22:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HNInternal: Ask HN: How can ChatGPT serve 700M users when I can't run one GPT-4 locally? (177 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44840728</link>
            <guid>44840728</guid>
            <pubDate>Fri, 08 Aug 2025 19:27:28 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44840728">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="bigbox"><td><table><tbody><tr id="44840728"><td><span></span></td><td><center><a id="up_44840728" href="https://news.ycombinator.com/vote?id=44840728&amp;how=up&amp;goto=item%3Fid%3D44840728"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=44840728">Ask HN: How can ChatGPT serve 700M users when I can't run one GPT-4 locally?</a></span></td></tr><tr><td colspan="2"></td><td><span><span id="score_44840728">116 points</span> by <a href="https://news.ycombinator.com/user?id=superasn">superasn</a> <span title="2025-08-08T19:27:28 1754681248"><a href="https://news.ycombinator.com/item?id=44840728">2 hours ago</a></span> <span id="unv_44840728"></span> | <a href="https://news.ycombinator.com/hide?id=44840728&amp;goto=item%3Fid%3D44840728">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20How%20can%20ChatGPT%20serve%20700M%20users%20when%20I%20can%27t%20run%20one%20GPT-4%20locally%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=44840728&amp;auth=df93e80f283c5ff48819997a35cd155bd4cf7c12">favorite</a> | <a href="https://news.ycombinator.com/item?id=44840728">62&nbsp;comments</a></span></td></tr><tr><td colspan="2"></td><td><div><p>Sam said yesterday that chatgpt handles ~700M weekly users. Meanwhile, I can't even run a single GPT-4-class model locally without insane VRAM or painfully slow speeds.</p><p>Sure, they have huge GPU clusters, but there must be more going on - model optimizations, sharding, custom hardware, clever load balancing, etc.</p><p>What engineering tricks make this possible at such massive scale while keeping latency low?</p><p>Curious to hear insights from people who've built large-scale ML systems.</p></div></td></tr><tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr></tbody></table><br>
</td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jim Lovell, Apollo 13 commander, has died (249 pts)]]></title>
            <link>https://www.nasa.gov/news-release/acting-nasa-administrator-reflects-on-legacy-of-astronaut-jim-lovell/</link>
            <guid>44840582</guid>
            <pubDate>Fri, 08 Aug 2025 19:12:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nasa.gov/news-release/acting-nasa-administrator-reflects-on-legacy-of-astronaut-jim-lovell/">https://www.nasa.gov/news-release/acting-nasa-administrator-reflects-on-legacy-of-astronaut-jim-lovell/</a>, See on <a href="https://news.ycombinator.com/item?id=44840582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>The following is a statement from acting NASA Administrator Sean Duffy on the passing of famed Apollo astronaut Jim Lovell. He passed away Aug. 7, in Lake Forest, Illinois. He was 97 years old.</p>
<p>‚ÄúNASA sends its condolences to the family of Capt. Jim Lovell, whose life and work inspired millions of people across the decades. Jim‚Äôs character and steadfast courage helped our nation reach the Moon and turned a potential tragedy into a success from which we learned an enormous amount. We mourn his passing even as we celebrate his achievements.</p>
<p>‚ÄúFrom a pair of pioneering Gemini missions to the successes of Apollo, Jim helped our nation forge a historic path in space that carries us forward to upcoming Artemis missions to the Moon and beyond.</p>
<p>‚ÄúAs the Command Module Pilot for Apollo 8, Jim and his crewmates became the first to lift off on a Saturn V rocket and orbit the Moon, proving that the lunar landing was within our reach. As commander of the Apollo 13 mission, his calm strength under pressure helped return the crew safely to Earth and demonstrated the quick thinking and innovation that informed future NASA missions.</p>
<p>‚ÄúKnown for his wit, this unforgettable astronaut was nicknamed Smilin‚Äô Jim by his fellow astronauts because he was quick with a grin when he had a particularly funny comeback.</p>
<p>‚ÄúJim also served our country in the military, and the Navy has lost a proud academy graduate and test pilot. Jim Lovell embodied the bold resolve and optimism of both past and future explorers, and we will remember him always.‚Äù</p>
<p>For more information about Lovell‚Äôs NASA career, and his agency biography, visit:</p>
<p><a href="https://www.nasa.gov/former-astronaut-james-a-lovell"><strong>https://www.nasa.gov/former-astronaut-james-a-lovell</strong></a></p>
<p>-end-</p>
<p>Grace Bartlinski / Cheryl Warner<br>Headquarters, Washington<br>202-358-1600<br><a href="mailto:grace.bartlinksi@nasa.gov">grace.bartlinksi@nasa.gov</a> / <a href="mailto:cheryl.m.warner@nasa.gov">cheryl.m.warner@nasa.gov</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I want everything local ‚Äì Building my offline AI workspace (313 pts)]]></title>
            <link>https://instavm.io/blog/building-my-offline-ai-workspace</link>
            <guid>44840013</guid>
            <pubDate>Fri, 08 Aug 2025 18:19:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://instavm.io/blog/building-my-offline-ai-workspace">https://instavm.io/blog/building-my-offline-ai-workspace</a>, See on <a href="https://news.ycombinator.com/item?id=44840013">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><blockquote>
<p>I want everything local ‚Äî no cloud, no remote code execution.</p>
</blockquote>
<p>That‚Äôs what a friend said. That one-line requirement, albeit simple, would need multiple things to work in tandem to make it happen.</p>
<p>What does a mainstream LLM (Large Language Model) chat app like ChatGPT or Claude provide at a high level?</p>
<ul>
<li>Ability to use chat with a cloud hosted LLM,</li>
<li>Ability to run code generated by them mostly on their cloud infra, sometimes locally via shell,</li>
<li>Ability to access the internet for new content or services.</li>
</ul>
<p>With so many LLMs being open source / open weights, shouldn't it be possible to do all that locally? But just local LLM is not enough, we need a truely isolated environment to run code as well.</p>
<p>So, LLM for chat, Docker to containerize code execution, and finally a browser access of some sort for content.</p>
<hr>
<h2>üß† The Idea</h2>
<p>We wanted a system where:</p>
<ul>
<li>LLMs run completely <strong>locally</strong></li>
<li><strong>Code executes inside a lightweight VM</strong>, not on the host machine</li>
<li>Bonus: <strong>headless browser</strong> for automation and internet access</li>
</ul>
<p><img src="https://instavm.io/blog-images/Pasted%20image%2020250730003357.png"></p>
<p>The idea was to perform tasks which require privacy to be executed completely locally, starting from planning via LLM to code execution inside a container. For instance, if you wanted to edit your photos or videos, how could you do it without giving your data to OpenAI/Google/Anthropic? Though they take security seriously (more than many), it's just a matter of one slip leading to your private data being compromised, a case in point being the early days of ChatGPT when user chats were <a href="https://www.bloomberg.com/news/articles/2023-03-21/openai-shut-down-chatgpt-to-fix-bug-exposing-user-chat-titles">accessible</a> from another's account!</p>
<hr>
<h2>The Stack We Used</h2>
<ul>
<li><strong>LLMs</strong>: <a href="https://ollama.com/">Ollama</a> for local models (also private models for now)</li>
<li><strong>Frontend UI</strong>: <a href="https://github.com/assistant-ai/assistant-ui"><code>assistant-ui</code></a></li>
<li><strong>Sandboxed VM Runtime</strong>: <a href="https://github.com/apple/container"><code>container</code></a> by Apple</li>
<li><strong>Orchestration</strong>: <a href="https://github.com/instavm/coderunner"><code>coderunner</code></a></li>
<li><strong>Browser Automation</strong>: <a href="https://playwright.dev/">Playwright</a></li>
</ul>
<blockquote>
<p>üí° We ran this entirely on Apple Silicon, using <code>container</code> for isolation.</p>
</blockquote>
<h3>üõ†Ô∏è  Our Attempt at a Mac App</h3>
<p>We started with zealous ambition: make it feel native. We tried using <code>a0.dev</code>, hoping it could help generate a Mac app. But it appears to be meant more for iOS app development ‚Äî and getting it to work for MacOS was painful, to say the least.</p>
<p>Even with help from the "world's best" LLMs, things didn't go quite as smoothly as we had expected. They hallucinated steps, missed platform-specific quirks, and often left us worse off.</p>
<p>Then we tried wrapping a <code>NextJS</code> app inside Electron. It took us longer than we'd like to admit. As of this writing, it looks like there's just no (clean) way to do it.</p>
<p>So, we gave up on the Mac app. The local web version of <code>assistant-ui</code> was good enough ‚Äî simple, configurable, and didn't fight back.</p>
<p><img src="https://instavm.io/blog-images/Pasted%20image%2020250729142902.png"></p>
<h3>Assistant UI</h3>
<p>We thought <code>Assistant-UI</code> provided multiple LLM support out-of-the-box, as their landing page shows a drop-down of models. But, no.
So, we had to look for examples on how to go about it, and <code>ai-sdk</code> appeared to be the popular choice.
Finally we had a dropdown for model selection. We decided not to restrict the set to just local models, as smaller local models are not quite there just yet. Users can get familiar with the tool and its capabilities, and later as small local models become better, they can just switch to being completely local.
<img src="https://instavm.io/blog-images/Pasted%20image%2020250729225206.png"></p>
<h3>Tool-calling</h3>
<p>Our use-case also required us to have models that support tool-calling. While some models do, Ollama has not implemented the tool support for them. For instance:</p>
<pre><code>responseBody: '{"error":"registry.ollama.ai/library/deepseek-r1:8b does not support tools"}',
</code></pre>
<p>And to add to the confusion, Ollama has decided to put this model under tool calling category on their site. Understandably, with the fast-moving AI landscape, it can be difficult for community driven projects to keep up.</p>
<p>At the moment, essential information like whether a model has tool-support or not, pricing per token, for various models are so fickle. A model's official page mentions tool-support but then tools like Ollama take a while to implement them. Anyway, we shouldn't complain - it's open source, we could've contributed.</p>
<h3>Containerized execution</h3>
<p>After the UI was MVP-level sorted, we moved on to the isolated VM part. Recently Apple released a tool called 'Container'. Yes, that's right. So, we checked it out and it seemed better than Docker as it provided one isolated VM per container - a perfect fit for running AI generated code.
So, we deployed a Jupyter server in the VM, exposed it as MCP (Model Context Protocol) tool, and made it available at <code>http://coderunner.local:8222/mcp</code>.</p>
<p>The advantage of MCPing vs a exposing an API is that existing tools that work with MCPs can use this right away. For instance, Claude Desktop and Gemini CLI can start executing AI-generated code with a simple config.</p>
<pre><code>"mcpServers": {
    "coderunner": {
      "httpUrl": "http://coderunner.local:8222/mcp"
    }
}
</code></pre>
<p>As you can see below, Claude figured out it should use the tool <code>execute_python_code</code> exposed from our isolated VM via the MCP endpoint.
<img src="https://instavm.io/blog-images/Pasted%20image%2020250730135012.png">
Aside - if you want to just use the <code>coderunner</code> bit as an MCP to execute code with your existing tools, the code for <code>coderunner</code> is <a href="https://github.com/instavm/coderunner">public</a>.</p>
<blockquote>
<h4>Apple container</h4>
<p>A tangent - if you're planning to work with Apple <code>container</code> and building VM images using it, have an abundance of patience. The build keeps failing with <code>Trap</code> error or just hangs without any output. To continue, you should <code>pkill</code> all container processes and restart the <code>container</code> tool. Then remove the <code>buildkit</code> image so that the next <code>build</code> process fetches a fresh one.
And repeat the three steps till it successfully works; this can take hours. We are excited to see Apple container mature as it moves beyond its early stages.</p>
</blockquote>
<p>Back to our app, we tested the <code>UI + LLMs + CodeRunner</code> on a task to <code>edit a video</code> and it worked!</p>
<p><img src="https://instavm.io/blog-images/WhatsApp%20Image%202025-07-11%20at%2016.50.13.jpeg">
		<em>I asked it to address me as Lord Voldemort as a sanity check for system instructions</em></p>
<p>After the coderunner was verified to be working, we decided to add the support of a headless browser. The main reason was to allow the app to look for new/updated tools/information online, for example, browsing github to find installation instruction for some tool it doesn't yet know about. Another reason was laying the foundation for <code>research</code>.
We chose Playwright for the task. We deployed it in the same container and exposed it as an MCP tool. Here is one task we asked it to do -</p>
<p><img src="https://instavm.io/blog-images/WhatsApp%20Image%202025-07-25%20at%2014.10.38.jpeg"></p>
<p>With this our basic set up was ready: <strong>Local LLM + Sandboxed arbitrary code execution + Headless browser</strong> for latest information.</p>
<hr>
<h2>What It Can Do (Examples)</h2>
<ol>
<li><strong>Do research on a topic</strong></li>
<li><strong>Generate and render charts</strong> from CSV using plain English</li>
<li><strong>Edit videos</strong> (via <code>ffmpeg</code>) ‚Äî e.g., ‚Äúcut between 0:10 and 1:00‚Äù</li>
<li><strong>Edit images</strong> ‚Äî resize, crop, convert formats</li>
<li><strong>Install tools from GitHub</strong> in a containerized space</li>
<li><strong>Use a headless browser</strong> to fetch pages and summarize content etc.</li>
</ol>
<hr>
<h2>Volumes and Isolation</h2>
<p>We mapped a volume from
<code>~/.coderunner/assets</code> (host)
to
<code>/app/uploads</code> (container)</p>
<p>So files edited/generated stay in a safe shared space, <strong>but code never touches the host system</strong>.</p>
<hr>
<h2>Limitations &amp; Next Steps</h2>
<ul>
<li>Currently <strong>only works on Apple Silicon</strong> (macOS 26 is optional)</li>
<li>Needs better UI for managing tools and output streaming</li>
<li>Headless browser is classified as bot by various sites</li>
</ul>
<hr>
<h2>Final Thoughts</h2>
<p>This is more than a just an experiment. It's a philosophy shift <strong>bringing compute and agency back to your machine</strong>. No cloud dependency. No privacy tradeoffs. While the best models will probably be always with the giants, we hope that we will still have local tools which can get our day-to-day work done with the privacy we deserve.</p>
<blockquote>
<p>We didn't just imagine it. We built it. And now, you can use it too.</p>
<p>Check out <code>coderunner-ui</code> on <a href="https://github.com/instavm/coderunner-ui">Github</a> to get started, and let us know what you think. We welcome feedback, issues and contributions.</p>
</blockquote>
<hr>
<h2>üîó Resources</h2>
<ul>
<li><a href="https://github.com/assistant-ai/assistant-ui">assistant-ui</a></li>
<li><a href="https://github.com/instavm/coderunner">instavm/coderunner</a></li>
<li><a href="https://github.com/apple/container">Apple/container</a></li>
<li><a href="https://github.com/instavm/coderunner-ui">instavm/coderunner-ui</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The surprise deprecation of GPT-4o for ChatGPT consumers (227 pts)]]></title>
            <link>https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/</link>
            <guid>44839842</guid>
            <pubDate>Fri, 08 Aug 2025 18:04:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/">https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/</a>, See on <a href="https://news.ycombinator.com/item?id=44839842">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2025/Aug/8/surprise-deprecation-of-gpt-4o/">

<p>8th August 2025</p>



<p>I‚Äôve been dipping into the <a href="https://reddit.com/r/chatgpt">r/ChatGPT</a> subreddit recently to see how people are reacting to <a href="https://simonwillison.net/2025/Aug/7/gpt-5/">the GPT-5 launch</a>, and so far the vibes there are not good. <a href="https://www.reddit.com/r/ChatGPT/comments/1mkae1l/gpt5_ama_with_openais_sam_altman_and_some_of_the/">This AMA thread</a> with the OpenAI team is a great illustration of the single biggest complaint: a lot of people are <em>very</em> unhappy to lose access to the much older GPT-4o, previously ChatGPT‚Äôs default model for most users.</p>
<p>A big surprise for me yesterday was that OpenAI simultaneously retired access to their older models as they rolled out GPT-5, at least in their consumer apps. Here‚Äôs a snippet from <a href="https://help.openai.com/en/articles/6825453-chatgpt-release-notes">their August 7th 2025 release notes</a>:</p>
<blockquote>
<p>When GPT-5 launches, several older models will be retired, including GPT-4o, GPT-4.1, GPT-4.5, GPT-4.1-mini, o4-mini, o4-mini-high, o3, o3-pro.</p>
<p>If you open a conversation that used one of these models, ChatGPT will automatically switch it to the closest GPT-5 equivalent. Chats with 4o, 4.1, 4.5, 4.1-mini, o4-mini, or o4-mini-high will open in GPT-5, chats with o3 will open in GPT-5-Thinking, and chats with o3-Pro will open in GPT-5-Pro (available only on Pro and Team).</p>
</blockquote>
<p>There‚Äôs no deprecation period at all: when your consumer ChatGPT account gets GPT-5, those older models cease to be available.</p>

<p id="sama"><strong>Update 12pm Pacific Time</strong>: Sam Altman on Reddit <a href="https://www.reddit.com/r/ChatGPT/comments/1mkae1l/comment/n7nelhh/">six minutes ago</a>:</p>
<blockquote>
<p>ok, we hear you all on 4o; thanks for the time to give us the feedback (and the passion!). we are going to bring it back for plus users, and will watch usage to determine how long to support it.</p>
</blockquote>
<p>See also <a href="https://x.com/sama/status/1953893841381273969">Sam‚Äôs tweet</a> about updates to the GPT-5 rollout.</p>

<p>Rest of my original post continues below:</p>

<hr>

<p>(This only affects ChatGPT consumers‚Äîthe API still provides the old models, their <a href="https://platform.openai.com/docs/deprecations">deprecation policies are published here</a>.)</p>
<p>One of the expressed goals for GPT-5 was to escape the terrible UX of the model picker. Asking users to pick between GPT-4o and o3 and o4-mini was a notoriously bad UX, and resulted in many users sticking with that default 4o model‚Äînow a year old‚Äîand hence not being exposed to the advances in model capabilities over the last twelve months.</p>
<p>GPT-5‚Äôs solution is to automatically pick the underlying model based on the prompt. On paper this sounds great‚Äîusers don‚Äôt have to think about models any more, and should get upgraded to the best available model depending on the complexity of their question.</p>
<p>I‚Äôm already getting the sense that this is <strong>not</strong> a welcome approach for power users. It makes responses much less predictable as the model selection can have a dramatic impact on what comes back.</p>
<p>Paid tier users can select ‚ÄúGPT-5 Thinking‚Äù directly. Ethan Mollick is <a href="https://www.oneusefulthing.org/p/gpt-5-it-just-does-stuff">already recommending deliberately selecting the Thinking mode</a> if you have the ability to do so, or trying prompt additions like ‚Äúthink harder‚Äù to increase the chance of being routed to it.</p>
<p>But back to GPT-4o. Why do many people on Reddit care so much about losing access to that crusty old model? I think <a href="https://www.reddit.com/r/ChatGPT/comments/1mkae1l/comment/n7js2sf/">this comment</a> captures something important here:</p>
<blockquote>
<p>I know GPT-5 is designed to be stronger for complex reasoning, coding, and professional tasks, but <strong>not all of us need a pro coding model</strong>. Some of us rely on 4o for creative collaboration, emotional nuance, roleplay, and other long-form, high-context interactions. Those areas feel different enough in GPT-5 that it impacts my ability to work and create the way I‚Äôm used to.</p>
</blockquote>
<p>What a fascinating insight into the wildly different styles of LLM-usage that exist in the world today! With <a href="https://simonwillison.net/2025/Aug/4/nick-turley/">700M weekly active users</a> the variety of usage styles out there is incomprehensibly large.</p>
<p>Personally I mainly use ChatGPT for research, coding assistance, drawing pelicans and foolish experiments. <em>Emotional nuance</em> is not a characteristic I would know how to test!</p>
<p>Professor Casey Fiesler <a href="https://www.tiktok.com/@professorcasey/video/7536223372485709086">on TikTok</a> highlighted OpenAI‚Äôs post from last week <a href="https://openai.com/index/how-we%27re-optimizing-chatgpt/">What we‚Äôre optimizing ChatGPT for</a>, which includes the following:</p>
<blockquote>
<p>ChatGPT is trained to respond with grounded honesty. There have been instances where our 4o model fell short in recognizing signs of delusion or emotional dependency. [‚Ä¶]</p>
<p>When you ask something like ‚ÄúShould I break up with my boyfriend?‚Äù ChatGPT shouldn‚Äôt give you an answer. It should help you think it through‚Äîasking questions, weighing pros and cons. New behavior for high-stakes personal decisions is rolling out soon.</p>
</blockquote>
<p>Casey points out that this is an ethically complicated issue. On the one hand ChatGPT should be much more careful about how it responds to these kinds of questions. But if you‚Äôre already leaning on the model for life advice like this, having that capability taken away from you without warning could represent a sudden and unpleasant loss!</p>
<p>It‚Äôs too early to tell how this will shake out. Maybe OpenAI will extend a deprecation period for GPT-4o in their consumer apps?</p>
<p><em><strong>Update</strong>: That‚Äôs exactly what they‚Äôve done, see <a href="https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/#sama">update above</a>.</em></p>
<p>GPT-4o remains available via the API, and there are no announced plans to deprecate it there. It‚Äôs possible we may see a small but determined rush of ChatGPT users to alternative third party chat platforms that use that API under the hood.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tor: How a military project became a lifeline for privacy (179 pts)]]></title>
            <link>https://thereader.mitpress.mit.edu/the-secret-history-of-tor-how-a-military-project-became-a-lifeline-for-privacy/</link>
            <guid>44838378</guid>
            <pubDate>Fri, 08 Aug 2025 15:45:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thereader.mitpress.mit.edu/the-secret-history-of-tor-how-a-military-project-became-a-lifeline-for-privacy/">https://thereader.mitpress.mit.edu/the-secret-history-of-tor-how-a-military-project-became-a-lifeline-for-privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=44838378">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>A story of secrecy, resistance, and the fight for digital freedom.</p><figure><img width="700" height="420" src="https://thereader.mitpress.mit.edu/wp-content/uploads/2025/07/Tor-lead-700x420.jpg" alt="" decoding="async" fetchpriority="high"><figcaption>Photo credit: <a href="https://unsplash.com/photos/a-computer-with-a-white-screen-sitting-on-a-table-AP7tG4LTeXA">Alan W, via Unsplash</a></figcaption></figure><p>I‚Äôm sitting in a cold, scuffed, and dirty plastic chair on a crowded train, watching freezing fog stream past the window ‚Äî one of the many unpleasant but strangely enjoyable everyday experiences of life in the United Kingdom. Despite the train carriage hailing from the mid-1980s, there is something resembling Wi-Fi service, and so I connect, hoping to sneak in a few hours of PhD research. I load up a website ‚Äî or so I think ‚Äî but instead reach a block page courtesy of the train‚Äôs Wi-Fi provider.</p><p>Sighing, I load up the Tor Browser and type in the address. The website loads instantly.</p><p>Tor is mostly known as the <em>Dark Web </em>or <em>Dark Net</em>, seen as an online Wild West where crime runs rampant. Yet it‚Äôs partly <a href="https://blog.torproject.org/transparency-openness-and-our-2021-and-2022-financials/" target="_blank" rel="nofollow">funded by the U.S. government</a>, and the BBC and Facebook both have Tor-only versions to allow users in authoritarian countries to reach them.</p><p>At its simplest, Tor is a distributed digital infrastructure that makes you anonymous online. It is a network of servers spread around the world, accessed using a browser called the Tor Browser, which you can download for free from the Tor Project website. When you use the Tor Browser, your signals are encrypted and bounced around the world before they reach the service you‚Äôre trying to access. This makes it difficult for governments to trace your activity or block access, as the network just routes you through a country where that access isn‚Äôt restricted.</p><h3><strong>The dark net rises</strong></h3><p>Today, privacy technologies like Tor underpin our digital society. From VPNs and encrypted messengers like WhatsApp to the basic security features in our digital systems, they‚Äôre essential tools for defending against cybercrime.</p><p>But, because you can‚Äôt protect yourself from digital crime without also protecting yourself from mass surveillance by the state, these technologies are the site of constant battles between security and law enforcement interests. The UK in particular is <a href="https://www.theguardian.com/technology/2025/jul/24/what-are-the-new-uk-online-safety-rules-and-how-will-they-be-enforced" target="_blank" rel="nofollow">currently convulsed over attempts to use law and technology to fight online harm</a>.</p><p>Recent debates focus on harms like the algorithmic spread of radicalizing and hateful content, issues often treated as if they emerge magically from technology itself, rather than from social policy, corporate greed, or an increasingly radicalized social elite.</p><figure><blockquote><p>Cypherpunks warned that the internet could quickly turn from a utopian dream into an authoritarian nightmare.</p></blockquote></figure><p>We‚Äôre in an undoubtedly odd situation. Governments are increasingly clamping down on the internet, yet the technologies to circumvent these blocks are readily available (and, in the case of Tor, completely free) and often funded, developed, used, and promoted by the same governments. By going back 30 years to the founding of the World Wide Web and the development of the technologies that would become Tor, we can get some surprising insights into why.</p><h3><strong>Cryptowars</strong></h3><p>Besieged as they are by the ongoing Oasis revival and the continuing dominance of Carhartt in the fashion market, readers in the UK will need little introduction to the cultural landscape of the 1990s. But in addition to the baggy jeans and Britpop, the early days of the commercial internet were also a time of immense possibility and conflict, when many aspects of the technologies and design of our digital societies were being fought over.</p><p>Some of the most important of these battles were the so-called Crypto Wars. A group of radical hackers and computer scientists known as the Cypherpunks spent the 1980s and early ‚Äò90s adapting military-grade encryption for public use. They warned that the internet could quickly turn from a utopian dream into an authoritarian nightmare. As <em>technolibertarians</em>, they believed that encryption was vital to realizing the potential of the internet, that it would permanently break up the power of the big media corporations, banks, and governments and give it to private individuals.</p><p>Law enforcement, on the other hand, was increasingly furious at the spread of mass communications platforms slipping beyond their control. But the spirit of the age was against them. Many of the internet‚Äôs core architects saw encryption as essential to keeping large and complex systems running without interference. Global businesses, too, generally favored privacy. Operating in a global market, they wanted to protect their competitive edge in an emerging digital economy.</p><p>Perhaps most importantly, at the heart of the U.S. government were an ascendant set of ideas that saw the internet as the ultimate neoliberal project: a borderless marketplace where free-flowing information would lead to optimal prices, ideas, and solutions. Full of messianic cultural confidence following the fall of the Soviet Union, they believed that if information were allowed to flow, the values of American capitalism would triumph on their own merits.</p><p>It was in this chaotic, high-stakes environment, full of strange alliances and clashing visions,  that the technologies behind the Dark Web were born.</p><h3><strong>Spies, submarines, and secrets</strong></h3><p>Tor‚Äôs story began in an office of the U.S. Naval Research Laboratory (NRL). Down the hall, satellites and radar dishes hung suspended in enormous voids, giant black pyramids bristled from the walls of vast anechoic testing chambers, and robot arms flexed in dark flooded pools, being poked and prodded by scuba divers armed with sensors. But the foundations of Tor were laid in a much more prosaic setting ‚Äî a shared computer lab.</p><p>Three military researchers ‚Äî David Goldschlag, Mike Reed, and Paul Syverson ‚Äî had been discussing a foundational aspect of the internet infrastructure. The rise of the new, commercial internet presented challenges for military users, as these global systems were vital for communications but difficult to secure.</p><p>Encryption technologies were already available to protect the <em>content</em> of messages. But above the content, the network itself had a range of security issues. The internet‚Äôs traffic routing systems and protocols were reliant on addressing metadata, equivalent to the <em>to </em>and <em>from </em>addresses on an envelope. Much as the address of the recipient is crucial for the delivery of a piece of mail, so are these metadata fundamental to the internet‚Äôs design and necessarily visible to the infrastructure providers who run its networks.</p><p>The routing design of the internet worked well for the U.S. government‚Äôs domestic interests, as it allowed the state to establish itself at key control points and surveil user traffic. However, the spread of the internet around the world had given other governments this power over their own domestic communication networks. This means that intelligence and military personnel abroad who wanted to make contact with their handlers in the United States or communicate with their base of operations were vulnerable.</p><p>Whenever the Navy utilized cryptosystems and communication networks that linked up to the internet, substantial amounts of valuable additional information were exposed to the people who ran the infrastructure. For example, if a CIA spy was in a foreign nation and sent a message over the internet back to the CIA‚Äôs home servers, ISPs in that foreign nation could observe where the message was sent and infer the spy‚Äôs affiliation.</p><p>This was a clear question for military research: how to keep internet traffic between the U.S. and other nations secret, not only in content (which you could protect with encryption), but also in origin and destination. And the three NRL researchers sought to solve it.</p><h3><strong>Onions</strong></h3><p>The researchers wanted to find a way to do the seemingly impossible ‚Äî to give the military the benefits of a global, high-speed communications network without exposing them to the vulnerabilities of the metadata that the network relied on to operate.</p><p>Enter Onion routing. Onion routing has undergone many changes and refinements over the years, but the basic principle has remained the same: The routing information used to navigate the internet is first hidden under three layers of encryption, like a Russian doll. It is from these layers that onion routing gets its name. This ‚Äúonion‚Äù of routing information is then sent into a network of onion routers: servers, or relays, located around the world that bounce the traffic around and between themselves. Each of these relays decrypts a layer of encryption to reveal the address of the next server in the network, until the final server reveals the destination of the traffic and makes a connection to the target web service. None of them can see both the origin and the destination of the traffic.</p><figure><blockquote><p>The routing information used to navigate the internet is first hidden under three layers of encryption, like a Russian doll.</p></blockquote></figure><p>This technical design has immediate social consequences, which were apparent to the NRL designers from the early stages. First, the infrastructure could not be run by the U.S. Navy, for if this were the case, then only people who trusted the U.S. Navy would use it. In an onion routing design, anonymity is produced by the size of the crowd ‚Äî the more people using the system, the more privacy it provides.</p><p>There are other implications, as well. For a CIA agent to use Tor without suspicion in non-U.S. nations, for example, there would need to be plenty of citizens in these nations using Tor for everyday internet browsing. Similarly, if the only users in a particular country are whistleblowers, civil rights activists and protesters, the government may well simply arrest anyone connecting to your anonymity network. As a result, an onion routing system had to be open to as wide a range of users and maintainers as possible, so that the mere fact that someone was using the system wouldn‚Äôt reveal anything about their identity or their affiliations.</p><p>This philosophy of a system open to the general public, in which small numbers of high-risk users could hide in cover traffic from more everyday users, underpins what became the onion routing paradigm, the predecessor to Tor.</p><h3><strong>Cypherpunk hackers and the U.S. military</strong></h3><p>Anonymity loves company ‚Äî so Tor needed to be sold to the general public. That necessity led to an unlikely alliance between cypherpunks and the U.S. Navy.</p><p>The NRL researchers behind Onion routing knew it wouldn‚Äôt work unless everyday people used it, so they reached out to the cypherpunks and invited them into conversations about design and strategy to reach the masses.</p><p>The NRL researchers met several members of the cypherpunk community in person at the Information Hiding Workshop in Oakland in 1997, where they discussed the possibility of collaboration. There, over vegetarian lasagna, salad, and (what else?) roasted onions, they discussed the technical possibilities and paradigms that might underpin a mass-use anonymity system. As they did, they also talked through broader values and motivations that might unite their strange, hybrid community.</p><p>Observing these two worlds ‚Äî the military academics and the cypherpunks ‚Äî interacting, through sharing test results, theoretical discussions, phone calls, emails, and eating the occasional roasted onion, we see the beginnings of a distinctive idea of what privacy means. Somewhere between the cypherpunk‚Äôs everyday, radical, decentralized vision of privacy and the high-security traffic protection desired by the military, a shared idea was forming. This saw privacy as being strongly shaped by the clusters of power and control built into digital infrastructure.</p><p>This understanding of privacy as a <em>structure</em> would unite an odd coalition around Tor over the next three decades: activists, journalists, drug buyers, hackers, and the military itself.</p><h3><strong>Scrambling for safety</strong></h3><p>This strange story of a group of libertarian hackers teaming up with the U.S. military amid the aftershocks of the Cold War presents a more nuanced picture of privacy than the familiar lone-user-versus-state narrative. It shows different groups coming together to change how ‚Äî through laws, technologies, practices, and cultural values ‚Äî we police the boundaries between different material systems of power. Understood in this way, we can see privacy as setting out where the domain of the community, of the family, of the state, of a corporation, of an institution or an individual begins and ends.</p><p>Take the UK‚Äôs Online Safety Act. It‚Äôs justified by policymakers as a tool to protect women and children from harm, with the Technology Minister going as far as to say that opposing the Act <a href="https://www.politico.eu/article/nigel-farage-on-the-side-predator-jimmy-savile-says-uk-minister-peter-kyle-online-safety-act/" target="_blank" rel="nofollow">puts you ‚Äúon the side of predators‚Äù</a> and child abusers. Law enforcement often argues that privacy technologies undermine their ability to prevent and investigate crime, particularly crime against women and children. This frames the issue as a trade-off between individual rights and collective safety. But many feminists would argue exactly the opposite: that the police have long painted women and children as uniquely weak and vulnerable in order to cement their own claim to power.</p><figure><blockquote><p>Undermining the very tools that give communities security is a poor strategy for keeping them safe.</p></blockquote></figure><p>In fact, breaking encryption in practice intensifies surveillance of women and children, undermining their rights to self-determination and autonomy, under the justification of protecting them. Yet it‚Äôs often powerful men in their families, communities, or institutions that women need to protect themselves from. For many, the prospect of police being able to track their intimate lives, or their attempts to access reproductive healthcare, is extremely threatening.</p><p>The state‚Äôs claim to protect the vulnerable often masks efforts to exert control. In fact, robust, well-funded, value-driven and democratically accountable content moderation ‚Äî by well-paid workers with good conditions ‚Äî is a far better solution than magical tech fixes to social problems (which also do little to tackle real social issues of misogyny, racism, and violence) or surveillance tools.</p><p>Indeed, undermining the very tools that give communities security is a poor strategy for keeping them safe. As more of our online lives are funneled into the centralized AI infrastructures ‚Äî controlled by a small and increasingly radicalized tech elite ‚Äî tools like Tor are becoming ever more important. Beyond offering privacy and protection from cybercrime in an increasingly insecure global landscape, they point to a more optimistic future for the internet, one in which we rebuild trust in our social institutions to address harm, rather than surrender that role to unaccountable technologies of control.</p><hr><p><strong><em>Ben Collier</em></strong><em> is Senior Lecturer in Digital Methods in the Department of Science, Technology, and Innovation Studies at the School of Social and Political Science, University of Edinburgh. He is the author of ‚Äú<a href="https://mitpress.mit.edu/9780262548182/tor/" target="_blank">Tor: From the Dark Web to the Future of Privacy</a>.‚Äù An open access edition of the book is freely available for download <a href="https://direct.mit.edu/books/oa-monograph/5761/TorFrom-the-Dark-Web-to-the-Future-of-Privacy" target="_blank">here</a>.</em></p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5 vs. Sonnet: Complex Agentic Coding (156 pts)]]></title>
            <link>https://elite-ai-assisted-coding.dev/p/copilot-agentic-coding-gpt-5-vs-claude-4-sonnet</link>
            <guid>44838303</guid>
            <pubDate>Fri, 08 Aug 2025 15:38:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elite-ai-assisted-coding.dev/p/copilot-agentic-coding-gpt-5-vs-claude-4-sonnet">https://elite-ai-assisted-coding.dev/p/copilot-agentic-coding-gpt-5-vs-claude-4-sonnet</a>, See on <a href="https://news.ycombinator.com/item?id=44838303">Hacker News</a></p>
Couldn't get https://elite-ai-assisted-coding.dev/p/copilot-agentic-coding-gpt-5-vs-claude-4-sonnet: Error: getaddrinfo ENOTFOUND elite-ai-assisted-coding.dev]]></description>
        </item>
        <item>
            <title><![CDATA[AI must RTFM: Why tech writers are becoming context curators (123 pts)]]></title>
            <link>https://passo.uno/from-tech-writers-to-ai-context-curators/</link>
            <guid>44837875</guid>
            <pubDate>Fri, 08 Aug 2025 15:04:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://passo.uno/from-tech-writers-to-ai-context-curators/">https://passo.uno/from-tech-writers-to-ai-context-curators/</a>, See on <a href="https://news.ycombinator.com/item?id=44837875">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header>
	
	<nav>
		
		<a href="https://passo.uno/about">About</a>
		
		<a href="https://passo.uno/posts">Posts</a>
		
		<a href="https://bsky.app/profile/theletterf.bsky.social">Bluesky</a>
		
		<a href="https://hachyderm.io/@remoquete">Mastodon</a>
		
		<a href="https://www.linkedin.com/in/fabrizioferri/">LinkedIn</a>
		
		<a href="https://passo.uno/posts/index.xml">RSS</a>
		
		
	</nav>
</header>

<main>
	<article>
		
		

		

		<section>
			<p>I‚Äôve been noticing a trend among developers that use AI: they are increasingly writing and structuring docs in context folders so that the AI powered tools they use can build solutions autonomously and with greater accuracy. They now strive to understand information architecture, semantic tagging, docs markup. All of a sudden they‚Äôve discovered docs, so they write more than they code. Because AI must RTFM now.</p>
<p>It‚Äôs docs-driven development. It‚Äôs also technical writing. We should welcome our colleagues into the fold of technical communication and seriously start thinking about becoming context writers and maintainers. In a way, we‚Äôve always been that, building the skills and techniques that allow owners of brains ‚Äì either organic or simulated ‚Äì to find their way in complex systems and accomplish tasks.</p>
<h2 id="all-ai-requires-to-do-the-right-thing-is-great-context-and-a-gentle-nudge">All AI requires to do the right thing is great context and a gentle nudge</h2>
<p>Picture large language models (LLMs) as elaborate machines that take inputs (in most cases it‚Äôs just text), turn it into discrete pieces (tokens) and process it through an incredibly convoluted conveyor belt. At the other end of the machine is the output, usually in the form of helpful commentary, feedback, and commands issued to various system tools. The <a href="https://en.wikipedia.org/wiki/Chinese_room">Chinese room</a>, finally incarnated.</p>
<p>The quality of the output is a function of the quality of the input. We enter requests and get back what an average of all human thoughts could have led us to, eventually. There‚Äôs nothing magic to it: if you write clear, accurate, and well structured requests (prompts), chances are that the LLM will respond in kind. It‚Äôs hard work, but it pays off, because development time is substantially reduced.</p>
<p>That‚Äôs why, with every release of a new LLM, coders pay close attention to the size of the <a href="https://www.ibm.com/think/topics/context-window">context window</a>, that is, the amount of information one can feed to an LLM. A context window of one million tokens means you can easily feed the entire <em>The Lord of the Rings</em> trilogy to, say, Gemini, and start asking questions about it. And you would still have room for <em>The Hobbit and</em> <em>The</em> <em>Silmarillion</em>.</p>
<h2 id="what-is-a-context-curator-and-why-we-need-that-role">What is a Context curator and why we need that role</h2>
<p>Engineers are finding out that writing, that long shunned soft skill, is now key to their efforts. In <a href="https://www.anthropic.com/engineering/claude-code-best-practices">Claude Code: Best Practices for Agentic Coding</a>, one of the key steps is creating a CLAUDE.md file that contains instructions and guidelines on how to develop the project, like which commands to run. But that‚Äôs only the beginning. Folks now <a href="https://www.reddit.com/r/ClaudeAI/comments/1lxylfs/claude_code_docs_as_context/">suggest</a> maintaining elaborate context folders.</p>
<p><strong>A context curator, in this sense, is a technical writer who is able to orchestrate and execute a content strategy around both human and AI needs, or even focused on AI alone.</strong> Context is so much better than content (a much abused word that means little) because it‚Äôs tied to <em>meaning</em>. Context is situational, relevant, necessarily limited. AI needs context to shape its thoughts.</p>
<p>In <a href="https://passo.uno/build-tech-writing-tools-llms/">Own the prompt</a>, where I described how I built <a href="https://github.com/theletterf/aikidocs">a tool</a> to write docs using context from the terminal or the browser, I argued that technical writers should lead the way and own AI-powered docs processes, including the curation of context. In my <a href="https://passo.uno/tech-writing-predictions-2025/">predictions for this year</a> (and probably the next, too), the importance of context was already present as the rise of docs-as-data.</p>
<blockquote>
<p><em>Picture a developer inserting a docs cartridge into his AI powered code editor: the presentation layer is going to be largely irrelevant, the docs powering the answers of locally executed LLMs to aid developers in their coding quests. In a multi-channel content strategy, LLM-tailored output is going to be an additional, incredibly relevant channel.</em></p>
</blockquote>
<h2 id="writing-is-designing-and-co-developing-again">Writing is designing and co-developing (again)</h2>
<p>Four years ago, <a href="https://passo.uno/posts/how-to-assist-api-design-as-a-technical-writer/">I argued</a> that technical writers can play a key role in API design and development, because words are everywhere and we writers are uniquely well positioned to select the right words. Back then, OpenAPI was the device that allowed the magic of crafting design using just words. Today, the spell extends to all kinds of software development. <a href="https://passo.uno/thinking-better-through-ai/">We can conjure software ourselves</a> now.</p>
<p>At this point, the most bleeding edge tech writing shops are serving <a href="https://llmstxt.org/">llms.txt</a> files and <a href="https://github.com/romansky/dom-to-semantic-markdown">LLM-optimized Markdown</a>. We‚Äôd take this a step further and prepackage context in a way that LLMs can easily consume. A standard for this still doesn‚Äôt exist, but I can see some flavor of DITA or another markup making a comeback, together with UIs that let users download which docs, for which version, etc.</p>
<p><img src="https://passo.uno/uploads/repomix.png" alt="Repomix"></p>
<blockquote>
<p><a href="https://repomix.com/">Repomix</a> allows anybody to select and package code and docs for LLMs</p>
</blockquote>
<p>The endgame is to be able to make content accessible to LLMs and humans alike, to let them extract knowledge tailored to their needs. Tech writers become context writers when they put on the art gallery curator hat, eager to show visitors the way and help them understand what they‚Äôre seeing. It‚Äôs yet another hat, but that‚Äôs both the curse and the blessing of our craft: like <a href="https://en.wikipedia.org/wiki/Bard_(Dungeons_%26_Dragons)">bards in DnD</a>, we‚Äôre the jacks of all trades that save the day (and the campaign).<br></p>
		</section>

		
		</article>
</main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is impressive because we've failed at personal computing (194 pts)]]></title>
            <link>https://rakhim.exotext.com/ai-is-impressive-because-we-ve-failed-at-semantic-web-and-personal-computing</link>
            <guid>44837783</guid>
            <pubDate>Fri, 08 Aug 2025 14:57:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rakhim.exotext.com/ai-is-impressive-because-we-ve-failed-at-semantic-web-and-personal-computing">https://rakhim.exotext.com/ai-is-impressive-because-we-ve-failed-at-semantic-web-and-personal-computing</a>, See on <a href="https://news.ycombinator.com/item?id=44837783">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>Unless someone wrote an article about that exact thing, a plain full-text search engine cannot answer a question like this:</p>
<blockquote>
<p>What animal is featured on a flag of a country where the first small British colony was established in the same year that Sweden's King Gustav IV Adolf declared war on France?</p>
</blockquote>
<p>But ChatGPT got the correct answer in a few seconds.</p>
<p><img src="https://img.exotext.com/1/_1IITv5hhOjWL-WdlODDM.png" alt="">
<em>Flag of Dominica features the Sisserou parrot, which is only found in Dominica. Great Britain established a small colony on the island in 1805.</em></p>
<p>Google's AI widget failed miserably, by the way.</p>
<p><img src="https://img.exotext.com/1/S4vSDU4NTMtPN3BuxyNbJ.png" alt=""></p>
<p>One of the best applications of modern LLM-based AI is surfacing answers from the chaos of the internet. Its success can be partly attributed to our failure to build systems that organize information well in the first place.</p>
<p>This product pattern is not new. Take Google Drive: a glorified file system in the cloud with folders and files, but it offers a worse experience than almost any desktop file management application of the last 30 years. Organizing your stuff there is hard and tedious. So Google took a shortcut: full-text search. Just dump everything in, and type to find it later. </p>
<p>The pattern of giving up on structure and relying on search has quietly become the dominant paradigm. "Search" here is a wide term: it can mean classic text-matching across indexed data, or complex multi-dimensional token matching across unwieldy models and weights. Why create a well-organized e-commerce site, just add a search bar and oversaturate each item's page with keywords. Why write high-quality user documentation, just add a support chat bot.</p>
<p>Remember Semantic Web? The web was supposed to evolve into semantically structured, linked, machine-readable data that would enable amazing opportunities. That never happened. Not only data remains unstructured and lacking metadata, even the representation of the unstructured data became difficult for machines to read due to the switch from plain, somewhat-structured HTML to JS-driven dynamic pile of <code>div</code>s.</p>
<p>We also never achieved truly personal computing. Computers could've been personal knowledge bases, with structured semantic connections akin to HyperCard, that take advantage of the semantic web and open standards. </p>
<p>My point is that if all knowledge were stored in a structured way with rich semantic linking, then very primitive natural language processing algorithms could parse question like the example at the beginning of the article, and could find the answer using orders of magnitude fewer computational resources. And most importantly: the knowledge and the connections would remain accessible and comprehensible, not hidden within impenetrable AI models.</p>
<p>AI is not a triumph of elegant design, but a brute-force workaround. LLMs like ChatGPT can infer structure from chaos. They scan the unstructured web and build ephemeral semantic maps across everything. It's not knowledge in the classic sense.. or perhaps it is exactly what knowledge is?</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google's Genie is more impressive than GPT5 (181 pts)]]></title>
            <link>https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant</link>
            <guid>44837646</guid>
            <pubDate>Fri, 08 Aug 2025 14:46:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant">https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant</a>, See on <a href="https://news.ycombinator.com/item?id=44837646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>The goal of AGI is to make programs that can do lots of things. Unfortunately, it's not all that easy to program ‚Äúdo lots of things‚Äù into a computer. Like, if you're writing python, there's no `import everything` library ‚Äì you have to somehow teach your program a bunch of different tasks and skills. Naively, you could spend a lot of time hand coding every possible scenario that may occur ‚Äì some gigantic switch statement that has a unique handler for every possible input. But this is obviously going to take too long and is extremely inefficient and really only theoretically possible.</p><p><span>So the watchwords of AGI are compression and generalization. You want to make a program that is pretty small in terms of compute and memory and so on, but has a lot of abilities that allow it to cover a very large ‚Äòaction space.‚Äô</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-1-170425838" target="_self" rel="">1</a></span></p><p><span>One way to teach your program how to generalize across things is by using deep learning. At a high level, you can show a deep neural network terabytes of data, and it will learn how to represent that data in a compressed form. The big large language models take in ~all of the text ever written, and are maybe a few tens-of-gigabytes in size,</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-2-170425838" target="_self" rel="">2</a></span><span> and yet seem to be able to replicate much of the training data. Perhaps the most surprising revelation of the last few years is that in addition to getting really good at spitting out realistic looking text, these LLMs also picked up more generic skills. The most evocative example of this for me was realizing that the original GPT-3 models ‚Äì the ones that preceded ChatGPT, that had no ‚Äòpost training‚Äô or ‚Äòinstruction tuning‚Äô ‚Äì could play a decent game of chess, even though the model surely didn't understand chess, and probably didn't really understand 2D grids. And since that moment back in 2020, it has become extremely obvious that these things can do quite a fair bit beyond just mimicking text.</span></p><p>A lot of AI research these days is basically exclusively about how to make large language models better. Naturally, some people focus on ‚Äúlarge‚Äù ‚Äì if you make the model bigger, it can get better! But some people also focus on ‚Äúlanguage‚Äù ‚Äì LLMs are only compressing text, but what if it compressed more kinds of data? A model that could represent text and images is probably better than one that can only represent text. And a model that could represent text and images and video is probably better than one that can only represent text and images.</p><p><span>If you assume that model representation capacity is directly tied to usefulness, you'll eventually reach a conclusion that looks something like this: ‚Äúa model that can accurately represent the entire world is going to be pretty damn useful.‚Äù Imagine asking a model a question like ‚Äúwhat's the weather in Tibet‚Äù and instead of doing something </span><em>lame</em><span> like check weather.com, it does something </span><em>awesome</em><span> like stimulate Tibet exactly so that it can tell you the weather based on the simulation. And giving a robot the ability to represent the world may allow it to do things like plan complex movements, navigate environments, and otherwise interact with real world environments. After all, this is approximately how humans work. In order to pick up my mug of not-coffee, I have to have an internal representation of my hand, the table, the mug, where my arm is going to go, how my hand is going to grip, what gravity is, what object-corporeality is, etc. etc. More mundanely, world models will probably allow people to make, like, better, more realistic AI generated Tiktoks that don't turn into spaghetti after a few minutes (and I'm sure nothing bad will come of that).</span></p><p><span>These ‚ÄúWorld Models‚Äù are considered a pretty long shot frontier in the AI world. Hopefully for obvious reasons ‚Äî simulating an entire world for extended periods of time with any kind of accuracy is really hard! You need mountains and mountains of data, most of it video. And as a result, there aren't a lot of people who are really even trying in this space.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-3-170425838" target="_self" rel="">3</a></span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-4-170425838" target="_self" rel="">4</a></span><span> But you know who has a mountain of video data?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3_o3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3_o3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 424w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 848w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 1272w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!3_o3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png" width="1456" height="971" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!3_o3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 424w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 848w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 1272w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>This is, possibly, the worst photo of these guys that I have ever seen.</figcaption></figure></div><p><span>About 3 days ago, Google announced </span><a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/" rel="">Genie 3</a><span>. Genie stands for Generative Interactive Environments. The best way to understand Genie is by analogy. GPT and Gemini let you create text descriptions of a time and place. And Veo and Sora let you turn text descriptions into video. Genie lets you take a text description into a </span><em>video game</em><span>, a space that you can, at least primitively, </span><em>interact with</em><span>.</span></p><div id="youtube2-PDKhUknuQDg" data-attrs="{&quot;videoId&quot;:&quot;PDKhUknuQDg&quot;,&quot;startTime&quot;:&quot;53&quot;,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/PDKhUknuQDg?start=53&amp;rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>It is kind of incredible? The reason Genie gets the headliner title over GPT-5 (below) is that Genie is really, truly something different.</p><p><span>Now, you can only really interact with a world Genie creates for a few minutes. But that is a massive step up from where we were previously. And it points to the future. Coherence over long context windows used to be a very difficult problem for language models too ‚Äì if you've been reading my </span><a href="https://theahura.substack.com/p/ilyas-30-papers-to-carmack-table" rel="">ml paper review series</a><span> you'll know that a solid part of the last 10 years of AI research has been motivated in part by this very problem. Last I checked we did a pretty good job with it; similar progress is possible for world models. More importantly, Genie 3 is now at a point where you can start using it for a wide range of other tasks, including training other models. You don't need to drive millions of miles in a Waymo if you can artificially create long-tail distribution events and train on those!</span></p><p><span>For folks who are interested in the more technical aspects of how this thing works, you're a bit out of luck. Publishing at Google is a bit weird these days. Any research papers that get written up first go into an internal pool. If any of the Gemini product teams want to productionize research out of that pool, the paper doesn't get published. As a result, I suspect we won't see papers for Genie 3 (or even its predecessor, Genie 2) any time soon. Here's the </span><a href="https://arxiv.org/abs/2402.15391" rel="">Genie 1 paper</a><span> though. I'll try and review it soon.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!i96C!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!i96C!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 424w, https://substackcdn.com/image/fetch/$s_!i96C!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 848w, https://substackcdn.com/image/fetch/$s_!i96C!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 1272w, https://substackcdn.com/image/fetch/$s_!i96C!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!i96C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png" width="1080" height="1031" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1031,&quot;width&quot;:1080,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;r/OpenAI - Sam Altman twitter hype is out of control again. we are not gonna deploy AGI next month, nor have we built it. we have some very cool stuff for you but pls chill and cut your expectations 100x! 11:32 AM ¬∑ Jan 20, 2025 ¬∑ 26.9K Views ‚Ä¢&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="r/OpenAI - Sam Altman twitter hype is out of control again. we are not gonna deploy AGI next month, nor have we built it. we have some very cool stuff for you but pls chill and cut your expectations 100x! 11:32 AM ¬∑ Jan 20, 2025 ¬∑ 26.9K Views ‚Ä¢" title="r/OpenAI - Sam Altman twitter hype is out of control again. we are not gonna deploy AGI next month, nor have we built it. we have some very cool stuff for you but pls chill and cut your expectations 100x! 11:32 AM ¬∑ Jan 20, 2025 ¬∑ 26.9K Views ‚Ä¢" srcset="https://substackcdn.com/image/fetch/$s_!i96C!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 424w, https://substackcdn.com/image/fetch/$s_!i96C!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 848w, https://substackcdn.com/image/fetch/$s_!i96C!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 1272w, https://substackcdn.com/image/fetch/$s_!i96C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!MCMu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!MCMu!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 424w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 848w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 1272w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!MCMu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png" width="600" height="302" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:302,&quot;width&quot;:600,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!MCMu!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 424w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 848w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 1272w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!AdQj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AdQj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 424w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 848w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 1272w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AdQj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png" width="720" height="342" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:342,&quot;width&quot;:720,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;This tweet by Sam Altman could be very important. Is he talking about  OpenAI? : r/singularity&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="This tweet by Sam Altman could be very important. Is he talking about  OpenAI? : r/singularity" title="This tweet by Sam Altman could be very important. Is he talking about  OpenAI? : r/singularity" srcset="https://substackcdn.com/image/fetch/$s_!AdQj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 424w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 848w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 1272w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>This you?</figcaption></figure></div><p>The big story about GPT-5 is about what it isn't.</p><p>It isn't a world-changing super-intelligent insane-step-up on the intelligence ladder. It isn't God. It isn't close to God.</p><p>Now, if you've been reading my blog for any length of time, you'll know that I didn't really ever suspect OpenAI would be the one to stumble upon God in the machine, even though that is in some sense their explicit purpose. I tend to think Google is going to do it, mostly by accident, and will probably also end up sitting on the research for too long until OpenAI-2-electric-boogaloo comes around and tries to eat their lunch, again.</p><p>But still. There was so much hype around GPT-5, and now all that hype has deflated.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!1RO6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!1RO6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 424w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 848w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 1272w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!1RO6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png" width="961" height="542" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:542,&quot;width&quot;:961,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!1RO6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 424w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 848w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 1272w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>The betting markets were not impressed by GPT-5. I am reading this graph as "there is a high expectation that Google will announce Gemini-3 in August", and not as "Gemini 2.5 is better than GPT-5". EDIT: this may be incorrect ‚Äî the polymarket is using the style-removed benchmark </span><a href="https://lmarena.ai/leaderboard/text/overall-no-style-control" rel="">here</a><span>, where Gemini 2.5 still ranks higher than GPT-5.</span></figcaption></figure></div><p><span>Starting about a year ago, people began to complain that AI had hit a wall because GPT-5 was not yet released. Some folks (cough Gary cough) were even starting to make claims like "GPT-4 is the best AI we're ever going to get". At the time, </span><a href="https://theahura.substack.com/p/tech-things-gpt-pro-and-the-state" rel="">I pushed back</a><span>, blaming our short attention spans and need for immediate gratification:</span></p><blockquote><p>But is AI stagnating?</p><p>There is a strict sense in which consumer AI may not feel like it's growing at the same rate as it did from 2020 to 2023. That period was a particularly magic time where we had a surplus of chips that we had to catch up to. Like a gas expanding to fill a volume, our chip utilization has caught up, so releases may not be at such a rapid clip.</p><p>Some of the problem here is that consumers are just getting impatient. The first version of GPT3 was published in May, 2020. GPT4 was launched in March, 2023. That‚Äôs 34 months. It‚Äôs only been ~20 months since GPT4 was released, there‚Äôs a bit more time to go before OpenAI starts ‚Äòfalling behind schedule‚Äô. We haven‚Äôt had the capability to even create large enough GPU clusters until recently. And it is also plausible that the release of stronger LLMs tracks more to self driving cars than to iPhones. The hypecycle for self driving cars was at its peak around 2014-2015. Even though the technology wasn‚Äôt quite consumer ready by then, the estimated ‚Äòrelease date‚Äô was still within only a few short years. In 2024 there are readily available self driving cars in several cities. From a research perspective, the folks saying that self driving cars would be ready within a few years of 2014 were more right than those saying it would never be ready at all.</p><p>As for the people who are arguing that AI is obviously dead and the whole field was doomed to failure because it's "just statistics" or "just linear algebra", idk, this feels a lot like shifting goal posts. Standard LLMs are exposed to way less data than the average human baby, the fact that they can do anything at all is a miracle, the fact that they can regularly pass competence tests like the SAT or the Bar should be endlessly awe inspiring. For some reason people keep wondering when we'll have AGI, even though it's literally here and accessible through a web browser. In any case, the cope isn't going to stop the AI from taking everyone's jobs (mine included).</p></blockquote><p>I stand by basically all of what I said. But I also have to eat some of the intent behind my words here. With GPT-5, I was clearly expecting something closer to the step function increase in functionality that we saw between GPT-3 and 4. Unfortunately, we really did hit a serious industry-wide asymptote in our ability to get more out of next-token-prediction. In retrospect, I think GPT-5 was always going to be disappointing. I'm sympathetic to the OpenAI team here, people were expecting literal miracles. But also, Sam definitely played a role in building up hype ‚Äî and, as a result, increased the mountain OpenAI would have to eventually summit.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!nK2Q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!nK2Q!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 424w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 848w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 1272w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png" width="745" height="565" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b447decd-22cb-46e6-839b-7e716719f063_745x565.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:565,&quot;width&quot;:745,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:271822,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://theahura.substack.com/i/170425838?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!nK2Q!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 424w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 848w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 1272w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Sam tweeted this the day before the GPT-5 announcement, come on!</figcaption></figure></div><p><span>So what is GPT-5? It's basically GPT-4, but better. It's still early, but it seems to be </span><em>significantly</em><span> more consistent, which is no small feat. OpenAI already has most of the consumer brand recognition; there are many people for whom "LLM" and "AI" are synonymous with "ChatGPT". But I suspect that those of us who swap models frequently will begin to use OpenAI as a daily driver again.</span></p><p><span>This shouldn't be understated. GPT has not been a part of my daily life at all since approximately January of this year, when I fully switched to Claude. And when I switched over to using Gemini for code and Claude for everything else, I took the extra step of uninstalling the ChatGPT app from my phone. More generally, I think there's been a bit of a 'vibe shift' in the Bay and among AI researchers and practitioners. People are starting to realize the sheer weight of Google's TPU farms, while OpenAI talent is getting siphoned off by </span><em>liquid </em><span>billion-dollar offers on one side (e.g. Meta) and </span><em>even more </em><span>ideological startups on the other (e.g. Safe Superintelligence, whatever Mira Murati is up to). Friends who are way more plugged in than I am</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-5-170425838" target="_self" rel="">5</a></span><span> describe an anti-OpenAI "coalition" forming, with many of the folks who had been burned by Sam's aggressive commercialization lining up to give the company a black eye. If you were more social-graph-minded, you may read a lot into Alexandr Wang ‚Äî Sam Altman's ex-roommate and close confidant ‚Äî leaving </span><a href="http://scale.ai/" rel="">Scale.AI</a><span> for Meta.</span></p><p><span>In this context, being the best in class is really important. Important people are losing faith, and those important people talk to other important people who have money. OpenAI needs to justify their extremely high valuation and their capex burn. If I‚Äôm right </span><a href="https://theahura.substack.com/p/tech-things-gpt-pro-and-the-state" rel="">that LLMs are a winner-take-all game</a><span>, OpenAI has to position itself as the winner.</span></p><p>They may have a tough time doing so though if they can't get their graphs straight. I mean what the hell are these?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Mx83!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Mx83!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 424w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 848w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 1272w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Mx83!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png" width="744" height="824" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:824,&quot;width&quot;:744,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Mx83!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 424w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 848w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 1272w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Ok, ok, that's great and all. But </span><em>what is GPT-5? How does it work?</em><span> Well, OpenAI isn't exactly going to release a public research paper about their latest and greatest. But we can go off the model card.</span></p><blockquote><p>GPT-5 is a unified system with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent‚Ä¶In the near future, we plan to integrate these capabilities into a single model.</p></blockquote><p><span>In other words, GPT-5 is a bunch of smaller models in a trenchcoat. I've long believed that many of the consumer-facing web chat interfaces were powered by many models instead of one really big model. It is simply more cost effective. I've written in the past about </span><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" rel="">'the bitter lesson'</a><span>, which can roughly be summarized as "scaling compute and data will lead to more long term progress than hand crafted heuristics and rules". But a natural corollary to the bitter lesson is that, </span><em>for a fixed budget</em><span>, human crafted systems are often more efficient. So here. Unfortunately, there just isn't that much additional information for how it works beyond that.</span></p><p>Even though Anthropic already had their big Claude 4 release a few months ago, they didn't want to feel left out, so they released Claude Opus 4.1. Rather appropriately titled, it really is just a slightly better version of Opus 4. I'll take it.</p><p>Claude hasn't really been at the top of any of the leaderboards for a while. And yet I and many very technical and AI-savvy people continue to use it. This is‚Ä¶somewhat odd? Why do I purposely use a worse model?</p><p><span>I think the short answer is that it's not worse. Teaching to the test is as much a problem in AI as it is in education. I mean this is standard </span><a href="https://en.wikipedia.org/wiki/Goodhart%27s_law" rel="">Goodhart's law</a><span> stuff ‚Äî the tests are meant to be proxies for competence, not targets. Even though Claude doesn't top leaderboards, it feels better to use. And any other gaps in model quality are simply papered over by Anthropic's focus on the user experience. As a developer, Claude is just way better. The artifact system is great, and the claude code CLI is a seamless experience.</span></p><p>A friend of mine described Anthropic as the Apple to OpenAI's Microsoft. And, like, yea, I see it. I guess Google is still just Google in this metaphor, idk.</p><p>I feel like every other month I hear about some random famous tech person raising a bajillion dollars to start an AI company. John Carmack raised $20M in 2022. Ilya raised $1b in 2024, and then another $2b a few months ago. Mira Murati raised $2b in July.</p><p>Ilya's company is valued at $32b. As far as I can tell, the only thing it has produced is the 370 words on its website. That is $86,486,486.49 per word. And 148 of those words are about Daniel Gross stepping away from the company. At least Ilya has a website, Carmack has literally disappeared.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!8fQm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8fQm!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 424w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 848w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 1272w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!8fQm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png" width="649" height="748" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:748,&quot;width&quot;:649,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8fQm!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 424w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 848w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 1272w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>apparently he's been hitting the gym</figcaption></figure></div><p>What is going on! Where are all of these people? What happened to the billions of dollars? Do they realize how many taco bell burritos you can buy with a billion dollars? Where are these people??? If any of you know what is going on at any of these places, please tell me.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Astronomy Photographer of the Year 2025 shortlist (144 pts)]]></title>
            <link>https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/2025-shortlist</link>
            <guid>44837434</guid>
            <pubDate>Fri, 08 Aug 2025 14:29:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/2025-shortlist">https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/2025-shortlist</a>, See on <a href="https://news.ycombinator.com/item?id=44837434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-off-canvas-main-canvas="">
    
  

  

  




  

  

  

  <main role="main">
    
    <div id="block-rmg-theme-content">
  
    
      <article data-content-type="topic">

  
    

  
  <div>
              <div>
        
                
            <p>The shortlist for the ZWO <a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year" data-gtm-name="CTA" data-gtm-detail="formatted content">Astronomy Photographer of the Year</a> 2025 competition has been unveiled.</p><p>From a blood moon hanging over Shanghai to a family portrait of the Solar System and a close-up of a comet's streaming tails, distant astronomical wonders are photographed in magnificent detail for all to admire.</p><p>Now in its 17th year, in 2025 the competition received a record number of entries, with just over <span>5,880 photographs submitted from 68 different countries.</span></p><p><span>See a small selection of shortlisted images below, and stay tuned to discover this year's full shortlist, winners and runners-up at a </span><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/2025-awards-ceremony-live" data-gtm-name="CTA" data-gtm-detail="formatted content"><span>special online awards ceremony on 11 September</span></a><span>.</span></p>
      
      </div>
              <div>
        <h2>
            Keep up to date with the competition
      </h2>        
            <p>Sign up to our space newsletter for exclusive astronomy news,&nbsp;guides and events, and be among the first to see this year's Astronomy Photographer of the Year winners</p>
      
      </div>
              <div>
              
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Blood%20Moon%20Rising%20Behind%20the%20City%20Skyscrapers%20%C2%A9%20Tianyao%20Yang.jpg.webp?itok=HqPGwVH0" width="900" height="1200" alt="Huge blood red moon rising at night behind Shanghai skyscrapers" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Tianyao Yang
      </span>
                  </p></div>
      <div>
                  <h3>
            Blood Moon Rising Behind the City Skyscrapers by Tianyao Yang
      </h3>
                <div>
          
            <p><em><span>Jiading District, Shanghai, China</span></em></p><p><span>This photograph captures a red Full Moon rising beside Shanghai‚Äôs tallest skyscrapers in Lujiazui. Taken from a distance of 26.5 km (16.5 miles) from the skyscrapers in a single exposure, this image‚Äôs alignment took five years of planning. The Full Moon appears perfectly positioned next to the illuminated skyline, creating a striking contrast.&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/A-270201-2025-5_The_Arctic_Flower.jpg.webp?itok=bEz--Lma" width="802" height="1200" alt="Photo showing icy snowy mountain landscape with vast sky above filled with green and purple aurorae, at the top is a large firework-shape aurora in green and purple" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Vincent Beudez
      </span>
                  </p></div>
      <div>
                  <h3>
            The Arctic Flower by Vincent Beudez
      </h3>
                <div>
          
            <p><em><span>Sjursnes, Troms√∏, Norway</span></em></p><p>In April, there is no ‚Äòtrue‚Äô night in northern Norway. This is why the Northern Lights look much more blue than usual. Vincent Beudez captured the visually pleasing aurora shape above the Norwegian background.</p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Gateway%20to%20the%20Galaxy%20%C2%A9%20Yujie%20Zhang.jpg.webp?itok=CgE4EpnW" width="1200" height="971" alt="Vivid Milky Way core vertical in sky over sculpture of black stones with water in foreground" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Yujie Zhang
      </span>
                  </p></div>
      <div>
                  <h3>
            Gateway to the Galaxy by Yujie Zhang
      </h3>
                <div>
          
            <p><em><span>Songyang County, China</span></em></p><p><span>Under the night sky, several black geometric buildings appear to stand on the water‚Äôs surface, resembling gateways to the galaxy. The bright Milky Way stretches across the sky behind them, with stars twinkling. The reflections of the buildings shimmer in the water, blending reality and illusion, as if opening a passage to the mysteries of the Universe, inspiring endless reverie and a longing to explore the vast starry sky.&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/OS-255963-2025-1_500%2C000_km_Solar_Prominence_Eruption_Captured_in_Full.jpg.webp?itok=TBjaG3-w" width="1088" height="1200" alt="Photo of the Sun in bright oranges and yellows with a solar prominence coming out of the bottom of it" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© PengFei Chou
      </span>
                  </p></div>
      <div>
                  <h3>
            500,000-km Solar Prominence Eruption by PengFei Chou
      </h3>
                <div>
          
            <p><em><span>Eastern New District, Xinxing County, Guangdong province</span></em></p><p><span>On 7 November 2024, the Sun experienced a massive solar prominence eruption, with a length exceeding 500,000 km (311,000 miles). The eruption lasted approximately one hour from its initial outburst to its conclusion. The eruption phase of the prominence is composed of more than 20 stacked data sets highlighting the entire process of this spectacular event.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-07/G-372747-2025-7_8_Panels_Mosaic_of_M31%2C_resolved_stars%2C_nebula_and_central_bulge..jpg.webp?itok=gs5Hx8es" width="1200" height="836" alt="Photo of Andromeda Galaxy up close in vivid reds and purples" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Chuhong Yu, Jingyao Hong, Xi Zhu, Yaguang Wan
      </span>
                  </p></div>
      <div>
                  <h3>
            Eight-Panel Mosaic of M31: Stars, Nebulae and Central Bulge by Chuhong Yu, Jingyao Hong, Xi Zhu, Yaguang Wan
      </h3>
                <div>
          
            <p><em><span>Daocheng County, Garz√™ Tibetan Autonomous Prefecture, Sichuan, China</span></em></p><p><span>This image shows countless resolved stars, emission nebula and a mysterious central bulge. The photo is incredibly detailed, the mist surrounding the galaxy is actually tens of thousands of yellowish tiny stars.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Into%20the%20Past%20%C2%A9%20Jim%20Hildreth.jpg.webp?itok=xri1UE1g" width="1200" height="600" alt="Desolate, dry and cracked Utah landscape, below a starry Milky Way arching in the sky in purples, blues and oranges" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Jim Hildreth 
      </span>
                  </p></div>
      <div>
                  <h3>
            Into the Past by Jim Hildreth
      </h3>
                <div>
          
            <p><em>Moonscape Overlook, Wayne County, Utah, USA</em></p><p><span>This impressive panorama is a view from the Utah desert. 23,000 pixels wide, the photograph shows the desolate, character rich landscape, below a starry Milky Way.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Moonrise%20Perfection%20Over%20the%20Dolomites%20%C2%A9%20Fabian%20Dalpiaz.jpg.webp?itok=nAIpIMYp" width="1200" height="800" alt="Photo showing top of mountain range in the Dolomites in Italy, with the Moon fitting neatly in a groove in the mountain" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Fabian Dalpiaz
      </span>
                  </p></div>
      <div>
                  <h3>
            Moonrise Perfection Over the Dolomites by Fabian Dalpiaz
      </h3>
                <div>
          
            <p><em><span>Santuario di Pietralba, Deutschnofen, South Tyrol, Italy</span></em></p><p><span>The </span><a href="https://www.rmg.co.uk/stories/space-astronomy/full-moon-calendar-2025" data-entity-type="node" data-entity-uuid="5b407ae4-0c46-4f0d-bb4f-8d563d647d2a" data-entity-substitution="canonical" title="Full Moon calendar 2025" data-gtm-name="CTA" data-gtm-detail="formatted content"><span>full Moon</span></a><span> rises above the rugged peaks of the Dolomites. With no clouds in sight and in flawless conditions, the golden light of sunset bathes the mountains, creating harmony between Earth and sky.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/S-128936-2025-3_Dragon_Tree_Trails.jpg.webp?itok=y5mnDM6c" width="1200" height="960" alt="Photo of lone tree in the centre of a flat landscape with distant hill, in the sky are multicoloured star trails forming a perfect circle around the tree" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Benjamin Barakat
      </span>
                  </p></div>
      <div>
                  <h3>
            Dragon Tree Trails by Benjamin Barakat
      </h3>
                <div>
          
            <p><em><span>Firmihin Forest, Hidaybu District, Yemen</span></em></p><p><span>A solitary dragon tree stands tall in the heart of Socotra‚Äôs Dragon Blood Tree forest ‚Äì an otherworldly landscape unlike anywhere else on Earth. The final image is composed of 300 individual exposures.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Total%20Solar%20Eclipse%20%C2%A9%20Louis%20Egan.jpg.webp?itok=6TyBWkYi" width="1200" height="235" alt="Photo showing progression of Moon moving in front of the Sun incrementally, with a total eclipse in the middle" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Louis Egan
      </span>
                  </p></div>
      <div>
                  <h3>
            Total Solar Eclipse by Louis Egan
      </h3>
                <div>
          
            <p><em><span>Shortlisted in ZWO Young Astronomy Photographer of the Year. Coaticook, Quebec, Canada</span></em></p><p>This 22-megapixel panorama shows the different stages of the full <a href="https://www.rmg.co.uk/stories/space-astronomy/solar-eclipses-explained" data-entity-type="node" data-entity-uuid="7a208546-504d-4e07-9e2f-8c9af23e8058" data-entity-substitution="canonical" title="Solar eclipses explained" data-gtm-name="CTA" data-gtm-detail="formatted content">solar eclipse</a>, with a high dynamic range (HDR) image of totality in the middle. This reveals both the bright corona and finer details otherwise lost in standard exposures. The final image uses approximately 200 images with varying exposure times to create a HDR totality, before combining everything together.&nbsp;</p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Comet%20Over%20Waikiki%20%C2%A9%20Ran%20Shen.jpg.webp?itok=1t1VTiy2" width="800" height="1200" alt="Photo showing lit up city landscape with a large white comet in the sky" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Ran Shen
      </span>
                  </p></div>
      <div>
                  <h3>
            Comet Over Waikiki by Ran Shen
      </h3>
                <div>
          
            <p><em>Honolulu, Hawaii, USA</em></p><p><span>Taken on the evening of 12 October 2024 at Pu'u O Kaimukƒ´ Park, Ran Shen joined many residents and astrophotographers in Honolulu, Hawaii, to witness the passage of Comet C/2023 A3 (Tsuchinshan-ATLAS), one of the most anticipated astronomical events of the year.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-07/G-68776-2025-6_Fireworks.jpg.webp?itok=mPO0tkBa" width="1200" height="719" alt="Photo of M33, the Triangulum Galaxy, resembling fireworks with bright spots of white, purple and reds against black background" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Bence T√≥th, P√©ter Felt√≥ti, Bertalan Kecsk√©s
      </span>
                  </p></div>
      <div>
                  <h3>
            Fireworks by Bence T√≥th, P√©ter Felt√≥ti and Bertalan Kecsk√©s
      </h3>
                <div>
          
            <p><em><span>Sz≈ëdliget, Pest and T√∂r√∂kkopp√°ny, Somogy, Hungary</span></em></p><p><span>The image shows M33, the Triangulum Galaxy, from a new perspective. Due to tidal interaction with M31, there is very prominent star-forming activity in M33, which results in a spectacular structure of emission nebulae. During processing, a separate SHO picture was created with a strong SII/H-alpha presence, the glowing red structures in the picture, and blended with a high-resolution LRGB processing of the continuum data, representing the ‚Äòbackground‚Äô light.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Solar%20System%20Portrait%20%C2%A9%20Sophie%20Paulin.jpg.webp?itok=NDLLDyLk" width="1200" height="803" alt="Long landscape image showing all planets in the Solar System" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Sophie Paulin
      </span>
                  </p></div>
      <div>
                  <h3>
            Solar System Portrait by Sophie Paulin
      </h3>
                <div>
          
            <p><em><span>Bobingen, Bavaria, Germany</span></em></p><p><span>This image presents all the planets of our Solar System, excluding Earth, showcasing their unique characteristics. Mercury, the closest to the Sun, is a barren, cratered world, while Venus is shrouded in thick clouds. Mars, the Red Planet, has vast deserts and the largest volcano in the Solar System. The gas giants, Jupiter and Saturn, dominate with their immense size and swirling storms, while Saturn‚Äôs rings make it especially striking. Uranus and Neptune, the ice giants, are rich in methane, giving them their blue hue.&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/A-206876-2025-1_Auroroa%20Over%20Mono%20Lake%3B%20a%20Rare%20Dance%20of%20Light.jpg.webp?itok=k7yLkKhh" width="1200" height="800" alt="Photo showing landscape, on the bottom half is a lake with rock protrusions and in the top half is the sky with bright green and purple aurorae, which is reflected in the lake" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Daniel Zafra
      </span>
                  </p></div>
      <div>
                  <h3>
            Aurora Over Mono Lake: A Rare Dance of Light by Daniel Zafra
      </h3>
                <div>
          
            <p><em><span>Mono Lake, Mono County, USA</span></em></p><p><span>This photograph captures the rare occurrence of </span><a href="https://www.rmg.co.uk/stories/space-astronomy/what-causes-northern-lights-aurora-borealis-explained" data-entity-type="node" data-entity-uuid="eef7b0b8-8e1b-4b1e-b3fc-633b7b405305" data-entity-substitution="canonical" title="What causes the Northern Lights? Aurora borealis explained" data-gtm-name="CTA" data-gtm-detail="formatted content"><span>Northern Lights</span></a><span> in California. Vibrant ribbons of magenta and green light up the sky, reflecting in the still waters among the rock formations.&nbsp;&nbsp;&nbsp;&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/NGC%206164%20and%20NGC%206165%20The%20Dragon%27s%20Egg%20%C2%A9%20Charles%20Pevsner.jpg.webp?itok=IyURF4ch" width="1200" height="800" alt="Bright star in the centre of a small egg-shaped pink nebula surrounded by protrusions of purple nebulae clouds, against a deep purple sky smattered with stars" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Charles Pevsner 
      </span>
                  </p></div>
      <div>
                  <h3>
            NGC 6164 and NGC 6165: The Dragon's Egg by Charles Pevsner
      </h3>
                <div>
          
            <p><em><span>Deep Sky Chile Observatory, Camino del Observatorio, R√≠o Hurtado, Chile</span></em></p><p><span>At the centre of this image is the bright star HD148937, part of a luminous triple-star system at the centre of the Dragon‚Äôs Egg Nebula (NGC 6164 and 6165) that lights up the nebula structure. Charles Pevsner was originally attracted to this target because of the striking symmetry of the magenta lobes of the Dragon‚Äôs Egg, but his favourite element ended up being the wispy outer shell.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/OM-423073-2025-5_Moonrise_on_Villebois-Lavalette.jpg.webp?itok=wwODG6F8" width="1200" height="668" alt="Landscape photo of French town called Villebois-Lavalette with a large orange moon in the sky with the top half visible" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Flavien Beauvais&nbsp;
      </span>
                  </p></div>
      <div>
                  <h3>
            Moonrise Over Villebois-Lavalette by Flavien Beauvais
      </h3>
                <div>
          
            <p><em><span>La Font Aride, Saint-Amant-de-Montmoreau, France</span></em></p><p><span>This unique photograph was taken 6.4 km (4 miles) from the ch√¢teau of Villebois-Lavalette, just north of Bordeaux. The distortions are related to the distance between the imaged Moon and the foreground but also with respect to the atmospheric disturbance, hence the curves on the surface of the Moon.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Close-up%20of%20a%20Comet%20%C2%A9%20Gerald%20Rhemann%20and%20Michael%20J%C3%A4ger.jpg.webp?itok=Hbk_-H0L" width="807" height="1200" alt="Photo showing bright vivid comet streaming in white, with another tail in bright blue" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Gerald Rhemann and Michael J√§ger
      </span>
                  </p></div>
      <div>
                  <h3>
            Close-up of a Comet by Gerald Rhemann and Michael J√§ger
      </h3>
                <div>
          
            <p><em><span>Tivoli Astrofarm, Windhoek Rural, Namibia</span></em></p><p><span>The photographers travelled to Namibia to view Comet C/2023 A3 (Tsuchinshan-ATLAS) in the southern hemisphere. Due to the angle of the observation, the dust and ion tails seem to have overlapped, but the impact of solar winds on the day caused noticeable kinks in the ion tail.&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/S-245523-2025-1_Cave_of_Stars.jpg.webp?itok=bLdwTF7z" width="960" height="1200" alt="Photo taken from inside a cave looking out on a seascape with the Milky Way diagonal in the sky above" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Yoshiki Abe
      </span>
                  </p></div>
      <div>
                  <h3>
            Cave of Stars by Yoshiki Abe
      </h3>
                <div>
          
            <p><em><span>Nagato, Yamaguchi, Japan</span></em></p><p><span>Realising that it was possible to photograph the Milky Way from this remote cave, Yoshiki Abe waited for the perfect conditions to take the image. This is a composite photograph. Both parts were taken on the same night and at the same location, but the foreground was shot during the blue hour then the tripod was shifted to capture the Milky Way.&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/OM-328097-2025-4_Lunar_Occultation_of_Saturn.jpg.webp?itok=TsF7EaBX" width="1200" height="1112" alt="Photo of the Moon which fades into darkness at the top, with a dotted line to the left which disappears behind the left limb of the moon momentarily" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Chayaphon Phanitloet
      </span>
                  </p></div>
      <div>
                  <h3>
            Lunar Occultation of Saturn by Chayaphon Phanitloet
      </h3>
                <div>
          
            <p><em><span>Bua Yai, Bua Yai District, Nakhon Ratchasima, Thailand</span></em></p><p><span>This is a composite image that brings images of both the Moon and Saturn together to show the lunar occultation of Saturn. A lunar occultation of Saturn occurs when the Moon passes in front of Saturn, temporarily blocking its light from Earth. This event is brief and can be observed as the Moon obscures the planet.</span><em><span>&nbsp;</span></em></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-07/Progression%20of%20Baily%27s%20Beads%20%C2%A9%20Damien%20Cannane.jpg.webp?itok=nnmzEN-Z" width="1200" height="400" alt="Long photo showing progression of solar eclipse, with baily's beads appearing" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Damien Cannane
      </span>
                  </p></div>
      <div>
                  <h3>
            Progression of Baily's Beads by Damien Cannane
      </h3>
                <div>
          
            <p><em><span>Dexter, Missouri, USA</span></em></p><p><span>Baily‚Äôs Beads are bright spots around the Moon during a solar eclipse that are caused by sunlight passing through lunar valleys. This composite shows the progression, from left to right, from the first ‚Äòdiamond ring‚Äô ‚Äì a moment when one last bright point of sunlight shines beside the faint corona, resembling a diamond on a ring ‚Äì fading through Baily's Beads into totality and beyond until a 'diamond ring' occurs again as the Sun starts to reappear.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Electric%20Threads%20of%20the%20Lightning%20Spaghetti%20Nebula%20%C2%A9%20Shaoyu%20Zhang.jpg.webp?itok=rwE5ERqC" width="1200" height="800" alt="Vivid photo of a nebula in reds, purples and blues, resembling a large bubble" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            ¬© Shaoyu Zhang
      </span>
                  </p></div>
      <div>
                  <h3>
            Electric Threads of the Lightning Spaghetti Nebula by Shaoyu Zhang
      </h3>
                <div>
          
            <p><em><span>Deep Sky Chile Observatory, R√≠o Hurtado, Chile and Xiangcheng, Garz√™ Tibetan Autonomous Prefecture, Sichuan, China</span></em></p><p><span>This full-spectrum image of the Spaghetti Nebula unveils the faint and elusive nature of this supernova remnant (SNR), hidden behind a vast cloud of dust that obstructs its emission light. To enhance its visual appeal, Shaoyu Zhang dedicated considerable time to capturing OIII data, intensifying the blue and green hues, while allowing SII and H-alpha to support high dynamic range stretching for added depth.&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Neon%20Sun%20%C2%A9%20Peter%20Ward.jpg.webp?itok=gcrlOnCM" width="1200" height="1200" alt="Square image showing black background on which is a ring in purples and greens" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            Data from NASA, processed by Peter Ward
      </span>
                  </p></div>
      <div>
                  <h3>
            Neon Sun by Peter Ward
      </h3>
                <div>
          
            <p><em>Shortlisted in the </em><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/competition/annie-maunder-open-category" data-entity-type="node" data-entity-uuid="910d9a7d-642b-4398-9d7f-1e0d94d625ae" data-entity-substitution="canonical" title="The Annie Maunder Open Category" data-gtm-name="CTA" data-gtm-detail="formatted content"><em>Annie Maunder Open Category</em></a><em>. Original data from NASA SDO 171, 193, 304 nanometre from 1 June 2024&nbsp;</em></p><p><span>The data from NASA‚Äôs Solar Dynamics Observer (SDO) probe was used here to show the Sun‚Äôs inner corona in a way that hints at a process similar to that which energises colourful neon lights on Earth. Images taken by the SDO in the ultraviolet spectrum were remapped to a more vibrant palette, with the same coronal data turned ‚Äòinside out‚Äô to surround the Sun, creating the illusion of it being enclosed in a neon tube. The data was then polar inversed to mirror the inner coronal image and colour saturation was increased.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
                  <h2>
            Learn more about Astronomy Photographer of the Year
      </h2>
                        <div>
                                                <div>
                  <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/2024-06/OS-2913-85%20Gigantic%20Solar%20Prominence%20in%20Motion%20from%2029th%20January%202023.jpg.webp?itok=GFbL8bGu" alt="" loading="lazy">
                                                          </p>
                  <div>
                      
                                            <p>
                        See the remarkable shortlisted and winning images from 2024's competition for free at the National Maritime Museum, open until 11 August.
                      </p>
                    </div>
                                    
                                  </div>
                                                            <div>
                  <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/2024-08/Arctic%20Dragon%20%C2%A9%20Carina%20Letelier%20Baeza%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20Aurorae.jpg.webp?itok=sZF5QeGz" alt="" loading="lazy">
                                                          </p>
                  <div>
                      
                                            <p>
                        Explore the winning and shortlisted images from previous years of Astronomy Photographer of the Year
                      </p>
                    </div>
                                    
                                  </div>
                                                            <div>
                  <div>
                     <p><img src="https://cdn.shopify.com/s/files/1/2459/8861/files/Collection-14-cover_bf105e6d-909b-4cb4-9d71-8ad23e7a1bad.jpg?v=1751968077" alt="" loading="lazy"></p><p>Shop</p>                                                                                </div>
                  <div>
                      
                                              <p>
                                                                                    ¬£30.00
                                                                              </p>
                                            <p>
                        Note: This title is currently available to pre-order. Pre-orders will be dispatched on 12 September, ahead of the general publication release date of 25 September 2025...
                      </p>
                    </div>
                                    
                                  </div>
                              </div>
      </div>
              <div>
      <div>
      <h2>
            Read more
      </h2>
        <p>
            Explore the universe with Royal Observatory Greenwich astronomers and curators.
      </p>
  </div>
      <div>
                  
                  
                  
                  <div>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/2024-11/vlcsnap-2024-11-21-14h07m48s961.jpg.webp?itok=jB6fJmIn" alt="" loading="lazy">
              </p>
              <div>
                <h3>
                  <a href="https://www.rmg.co.uk/stories/space-astronomy/finding-community-through-astrophotography-astronomy-photographer-year">Finding community through astrophotography</a>
                </h3>
                <p>
                  Sophie Paulin and Tom Williams struck up a friendship in an online astrophotography forum. Discover how they combined their expertise to win a prize in Astronomy Photographer of the Year
                </p>
              </div>
            </div>
                  
                  
                  <div>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/migrations/PS-43711-19_The%20Dreamlike%20Sky%20Above%20the%20Sea%20of%20Clouds%20%C2%A9%20Likai%20Lin.jpg.webp?itok=jUNzE3Xt" alt="" loading="lazy">
              </p>
              <div>
                <h3>
                  <a href="https://www.rmg.co.uk/stories/space-astronomy/astronomy-naked-eye">Astronomy with the naked eye</a>
                </h3>
                <p>
                  Learn what you could see in the night sky with no equipment from the Royal Observatory Greenwich; from galaxies and meteor showers to comets, star clusters, cloud formations and more
                </p>
              </div>
            </div>
                  
                  
              </div>
      

    </div>
              <div>
      <div>
                      <p>
      <h2>
            Sponsors and supporters
      </h2>
    </p>
                    </div>
      <div>
                              <div>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/logo/public/2025-06/zwo%20x%20seestar%20-%20vertical_0.png.webp?itok=cOOIHojk" width="169" height="96" alt="Logo that says ZWO x Seestar" loading="lazy">



      
</p>
            </div>
                      <div>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/logo/public/2022-06/BBC%20Sky%20At%20Night%20logo.png.webp?itok=BFOpS0pz" width="193" height="96" alt="BBC Sky at Night logo in black" loading="lazy">



      
</p>
            </div>
                        </div>
    </div>
              <div>
        
                
            <p>Header image:<em> NGC 6164 and NGC 6165: The Dragon's Egg</em> ¬© Charles Pevsner</p>
      
      </div>
          </div>

</article>


  </div>  </main>

  
    

  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting good results from Claude code (193 pts)]]></title>
            <link>https://www.dzombak.com/blog/2025/08/getting-good-results-from-claude-code/</link>
            <guid>44836879</guid>
            <pubDate>Fri, 08 Aug 2025 13:45:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dzombak.com/blog/2025/08/getting-good-results-from-claude-code/">https://www.dzombak.com/blog/2025/08/getting-good-results-from-claude-code/</a>, See on <a href="https://news.ycombinator.com/item?id=44836879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="gh-main">
    <article>

        <header>
            <span>
                Posted <time datetime="2025-08-08">08 Aug 2025</time>
                    in
                    <a href="https://www.dzombak.com/blog/tag/ai/">AI</a>
            </span>

            


            
        </header>

        <div>




            <p>I've been experimenting with LLM programming agents over the past few months. Claude Code has become my favorite.</p><p>It is not without issues, but it's allowed me to write ~12 programs/projects in relatively little time, and I feel I would not have been able to do all this in the same amount of time without it. Most of them, I wouldn't even have bothered to write without Claude Code, simply because they'd take too much of my time. (A list is included at the end of this post.)</p><p>I'm still far from a Claude Code expert, and I have a backlog of blog posts and documentation to review that might be useful. But ‚Äî and this is critical ‚Äî you don't have to read everything that's out there to start seeing results. You don't even need to read <em>this</em> post; just type some prompts in and see what comes out.</p><p>That said, because I just wrote this up for a job application, <strong>here's how I'm getting good results from Claude Code</strong>. I've embedded links to some examples where appropriate.</p><ul><li>A key is writing a clear spec ahead of time, which provides context to the agent as it works in the codebase.</li><li>Having a document for the agent that outlines the project‚Äôs structure and how to run e.g. builds and linters is helpful.</li><li>Asking the agent to perform a code review on its own work is surprisingly fruitful.</li><li>Finally, I have a personal ‚Äúglobal‚Äù agent guide describing best practices for agents to follow, specifying things like problem-solving approach, use of TDD, etc. <em>(This file is listed near the end of this post.)</em></li></ul><p>Then there's the question of <strong>validating LLM-written code.</strong></p><p>AI-generated code <em>is</em> often incorrect or inefficient.</p><p>It‚Äôs important for me to call out that <strong>I believe I‚Äôm ultimately responsible for the code that goes into a PR with my name on it, regardless of how it was produced</strong>.</p><p>Therefore, especially in any professional context, I manually review all AI-written code and test cases. I‚Äôll add test cases for anything I think is missing or needs improvement, either manually or by asking the LLM to write those cases (which I then review).</p><p>At the end of the day, manual review is necessary to verify that behavior is implemented correctly and tested properly.</p><h2 id="personal-global-agent-guide">Personal "global" agent guide</h2><p>This lives at <code>~/.claude/CLAUDE.md</code>:</p><pre><code># Development Guidelines

## Philosophy

### Core Beliefs

- **Incremental progress over big bangs** - Small changes that compile and pass tests
- **Learning from existing code** - Study and plan before implementing
- **Pragmatic over dogmatic** - Adapt to project reality
- **Clear intent over clever code** - Be boring and obvious

### Simplicity Means

- Single responsibility per function/class
- Avoid premature abstractions
- No clever tricks - choose the boring solution
- If you need to explain it, it's too complex

## Process

### 1. Planning &amp; Staging

Break complex work into 3-5 stages. Document in `IMPLEMENTATION_PLAN.md`:

```markdown
## Stage N: [Name]
**Goal**: [Specific deliverable]
**Success Criteria**: [Testable outcomes]
**Tests**: [Specific test cases]
**Status**: [Not Started|In Progress|Complete]
```
- Update status as you progress
- Remove file when all stages are done

### 2. Implementation Flow

1. **Understand** - Study existing patterns in codebase
2. **Test** - Write test first (red)
3. **Implement** - Minimal code to pass (green)
4. **Refactor** - Clean up with tests passing
5. **Commit** - With clear message linking to plan

### 3. When Stuck (After 3 Attempts)

**CRITICAL**: Maximum 3 attempts per issue, then STOP.

1. **Document what failed**:
   - What you tried
   - Specific error messages
   - Why you think it failed

2. **Research alternatives**:
   - Find 2-3 similar implementations
   - Note different approaches used

3. **Question fundamentals**:
   - Is this the right abstraction level?
   - Can this be split into smaller problems?
   - Is there a simpler approach entirely?

4. **Try different angle**:
   - Different library/framework feature?
   - Different architectural pattern?
   - Remove abstraction instead of adding?

## Technical Standards

### Architecture Principles

- **Composition over inheritance** - Use dependency injection
- **Interfaces over singletons** - Enable testing and flexibility
- **Explicit over implicit** - Clear data flow and dependencies
- **Test-driven when possible** - Never disable tests, fix them

### Code Quality

- **Every commit must**:
  - Compile successfully
  - Pass all existing tests
  - Include tests for new functionality
  - Follow project formatting/linting

- **Before committing**:
  - Run formatters/linters
  - Self-review changes
  - Ensure commit message explains "why"

### Error Handling

- Fail fast with descriptive messages
- Include context for debugging
- Handle errors at appropriate level
- Never silently swallow exceptions

## Decision Framework

When multiple valid approaches exist, choose based on:

1. **Testability** - Can I easily test this?
2. **Readability** - Will someone understand this in 6 months?
3. **Consistency** - Does this match project patterns?
4. **Simplicity** - Is this the simplest solution that works?
5. **Reversibility** - How hard to change later?

## Project Integration

### Learning the Codebase

- Find 3 similar features/components
- Identify common patterns and conventions
- Use same libraries/utilities when possible
- Follow existing test patterns

### Tooling

- Use project's existing build system
- Use project's test framework
- Use project's formatter/linter settings
- Don't introduce new tools without strong justification

## Quality Gates

### Definition of Done

- [ ] Tests written and passing
- [ ] Code follows project conventions
- [ ] No linter/formatter warnings
- [ ] Commit messages are clear
- [ ] Implementation matches plan
- [ ] No TODOs without issue numbers

### Test Guidelines

- Test behavior, not implementation
- One assertion per test when possible
- Clear test names describing scenario
- Use existing test utilities/helpers
- Tests should be deterministic

## Important Reminders

**NEVER**:
- Use `--no-verify` to bypass commit hooks
- Disable tests instead of fixing them
- Commit code that doesn't compile
- Make assumptions - verify with existing code

**ALWAYS**:
- Commit working code incrementally
- Update plan documentation as you go
- Learn from existing implementations
- Stop after 3 failed attempts and reassess
</code></pre><h2 id="projects-written-using-claude-code">Projects written using Claude Code</h2><figure><a href="https://github.com/cdzombak/xrp?ref=dzombak.com"><div><p>GitHub - cdzombak/xrp: HTML/XML aware reverse proxy</p><p>HTML/XML aware reverse proxy. Contribute to cdzombak/xrp development by creating an account on GitHub.</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-24.svg" alt=""><span>cdzombak</span></p></div><p><img src="https://www.dzombak.com/content/images/thumbnail/xrp" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/dzsolarized-vscode?ref=dzombak.com"><div><p>GitHub - cdzombak/dzsolarized-vscode: Solarized variant for VS Code (light + dark modes supported)</p><p>Solarized variant for VS Code (light + dark modes supported) - cdzombak/dzsolarized-vscode</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-25.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://www.dzombak.com/content/images/thumbnail/dzsolarized-vscode-1" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/flickr-rss?ref=dzombak.com"><div><p>GitHub - cdzombak/flickr-rss: Generate an RSS feed of a Flickr photostream or your Friends &amp; Family feed</p><p>Generate an RSS feed of a Flickr photostream or your Friends &amp; Family feed - cdzombak/flickr-rss</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-26.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://www.dzombak.com/content/images/thumbnail/flickr-rss-1" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/lychee-meta-tool?ref=dzombak.com"><div><p>GitHub - cdzombak/lychee-meta-tool: Quickly find &amp; edit untitled photos in your Lychee photo library</p><p>Quickly find &amp; edit untitled photos in your Lychee photo library - cdzombak/lychee-meta-tool</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-27.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/0bc054b50f02fae2565fbd58e0233229978d67ef59f2d3c3a2c7584f3d2250ce/cdzombak/lychee-meta-tool" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/macos-screenlock-mqtt?ref=dzombak.com"><div><p>GitHub - cdzombak/macos-screenlock-mqtt: Report macOS screen lock status to an MQTT broker</p><p>Report macOS screen lock status to an MQTT broker. Contribute to cdzombak/macos-screenlock-mqtt development by creating an account on GitHub.</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-28.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/2eac4061a0425efd3045d26d6ea8c3827670f1d8af1fad2e3a528df8193aebe8/cdzombak/macos-screenlock-mqtt" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/lychee-birb-title?ref=dzombak.com"><div><p>GitHub - cdzombak/lychee-birb-title: Set titles for Bird Buddy photos in your Lychee photo library</p><p>Set titles for Bird Buddy photos in your Lychee photo library - cdzombak/lychee-birb-title</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-29.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/60865f263a542ad9f7bc543702fd15ae3de260090e3e35edca63e6492d97da49/cdzombak/lychee-birb-title" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/lychee-ai-organizer?ref=dzombak.com"><div><p>GitHub - cdzombak/lychee-ai-organizer: Use local LLMs to organize your unsorted photos in Lychee</p><p>Use local LLMs to organize your unsorted photos in Lychee - cdzombak/lychee-ai-organizer</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-30.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/cfd5ea252de7e2e859a8b94d02f2cbaeb1d0ce6c05ad5c5f45b8aed5d3441052/cdzombak/lychee-ai-organizer" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/mac-install?ref=dzombak.com"><div><p>GitHub - cdzombak/mac-install: Idempotent software suite installer for macOS</p><p>Idempotent software suite installer for macOS. Contribute to cdzombak/mac-install development by creating an account on GitHub.</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-31.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/919d1e0000548235c07554e25b7b35eef8822b5989895b6ccef3ca60f542e930/cdzombak/mac-install" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/rss.church?ref=dzombak.com"><div><p>GitHub - cdzombak/rss.church: I Believe in RSS</p><p>I Believe in RSS. Contribute to cdzombak/rss.church development by creating an account on GitHub.</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-32.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/cf321c8e07a66765223c314ac40b5887dd2cf5f4c56150621e4ee3ffb83fa708/cdzombak/rss.church" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/flickr-exporter?ref=dzombak.com"><div><p>GitHub - cdzombak/flickr-exporter: Export all your Flickr photos, or a selected set or collection, preserving title/description/tags and other metadata.</p><p>Export all your Flickr photos, or a selected set or collection, preserving title/description/tags and other metadata. - cdzombak/flickr-exporter</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-33.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/5afb4c8d55872fcdceca3c34a1dfa299f558c4d195991224e95c39e516e76dae/cdzombak/flickr-exporter" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/gallerygen?ref=dzombak.com"><div><p>GitHub - cdzombak/gallerygen: Generate a static HTML gallery from a directory tree of images</p><p>Generate a static HTML gallery from a directory tree of images - cdzombak/gallerygen</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-34.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://www.dzombak.com/content/images/thumbnail/gallerygen" alt="" onerror="this.style.display = 'none'"></p></a></figure>

        </div>


        

    </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HorizonDB, a geocoding engine in Rust that replaces Elasticsearch (168 pts)]]></title>
            <link>https://radar.com/blog/high-performance-geocoding-in-rust</link>
            <guid>44836463</guid>
            <pubDate>Fri, 08 Aug 2025 12:57:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://radar.com/blog/high-performance-geocoding-in-rust">https://radar.com/blog/high-performance-geocoding-in-rust</a>, See on <a href="https://news.ycombinator.com/item?id=44836463">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>At Radar, performance is a feature. Our platform processes over 1 billion API calls per day from hundreds of millions of devices worldwide. We provide geolocation infrastructure and solutions, including APIs for:</p><ul role="list"><li><a href="https://radar.com/product/geocoding-api" target="_blank" data-wf-native-id-path="7e9f9f3e-2085-5f55-1e17-1477c9eb89bf" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="7e9f9f3e-2085-5f55-1e17-1477c9eb89bf"><strong>Geocoding</strong></a>: Forward geocoding, reverse geocoding, and IP geocoding APIs with global coverage.</li><li><a href="https://radar.com/product/places-search-api" target="_blank" data-wf-native-id-path="1c432021-7bc9-0814-927e-61b37d5f00fb" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="1c432021-7bc9-0814-927e-61b37d5f00fb"><strong>Search</strong></a><strong>:</strong> Address autocomplete, address validation, and places search APIs.</li><li><a href="https://radar.com/product/routing-api" target="_blank" data-wf-native-id-path="bb8af94d-e2f9-ce26-c9c5-5c50450a9c2f" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="bb8af94d-e2f9-ce26-c9c5-5c50450a9c2f"><strong>Routing</strong></a><strong>: </strong>Distance, matrix, route optimization, route matching, and directions APIs.</li><li><a href="https://radar.com/product/fraud" target="_blank" data-wf-native-id-path="15365177-de4a-9634-7e4a-af0d7cf5f73d" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="15365177-de4a-9634-7e4a-af0d7cf5f73d"><strong>Geolocation compliance</strong></a>: Detect current jurisdiction, distance to border, regulatory exclusion zones, and more.</li></ul><p>But as our products and data scale, so do our engineering challenges.</p><p>To support this growth, we developed <strong>HorizonDB</strong>, a geospatial database written in Rust that consolidates multiple location services into a single, highly performant binary. With HorizonDB, we are able to power all of the above use cases with excellent operational footprint:</p><ul role="list"><li>Handle 1,000 QPS per core.&nbsp;</li><li>Maintain a forward geocoding median latency of 50 ms.</li><li>Maintain a reverse geocoding median latency of &lt;1 ms.</li><li>Scale linearly on commodity hardware.</li></ul><h2>Why we replaced Mongo and Elasticsearch</h2><p>Before HorizonDB, we split geocoding across Elasticsearch and microservices for forward geocoding, and MongoDB for reverse. </p><p>Operating and scaling this stack was costly: Elasticsearch frequently fanned queries to all shards and required service-orchestrated batch updates, while MongoDB lacked true batch ingestion, required overprovisioning, and had no reliable bulk rollback for bad data.</p><h2>The architecture</h2><p>Our goals for this service included:</p><ol role="list"><li>‚Äç<strong>Efficiency:</strong> The service can run on commodity machines, has predictable autoscaling, and is the single source of truth of all our geo entities.</li><li>‚Äç<strong>Operations:</strong> Data assets can be built and processed multiple times a day, changes can be deployed and rolled back trivially, and should be simple to operate.</li><li>‚Äç<strong>Developer experience:</strong> Developers should be able to run the service locally, and changes can be written and tested easily.</li></ol><p>With these goals in mind, we built HorizonDB using <a href="https://rocksdb.org/" target="_blank" data-wf-native-id-path="ef24cc8f-619d-614d-05a8-cb652aa0d38f" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="ef24cc8f-619d-614d-05a8-cb652aa0d38f">RocksDB</a>, <a href="http://s2geometry.io/" target="_blank" data-wf-native-id-path="d4d56947-e884-c6fa-7653-e6def92b7d3f" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="d4d56947-e884-c6fa-7653-e6def92b7d3f">S2</a>, <a href="https://github.com/quickwit-oss/tantivy" data-wf-native-id-path="ef24cc8f-619d-614d-05a8-cb652aa0d393" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="ef24cc8f-619d-614d-05a8-cb652aa0d393">Tantivy</a>, <a href="https://github.com/BurntSushi/fst" target="_blank" data-wf-native-id-path="ef24cc8f-619d-614d-05a8-cb652aa0d397" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="ef24cc8f-619d-614d-05a8-cb652aa0d397">FSTs</a>, <a href="https://github.com/microsoft/LightGBM" target="_blank" data-wf-native-id-path="ef24cc8f-619d-614d-05a8-cb652aa0d39b" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="ef24cc8f-619d-614d-05a8-cb652aa0d39b">LightGBM</a> and <a href="https://fasttext.cc/" target="_blank" data-wf-native-id-path="ef24cc8f-619d-614d-05a8-cb652aa0d39f" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="ef24cc8f-619d-614d-05a8-cb652aa0d39f">FastText</a>. </p><p>Data assets are preprocessed using <a href="https://spark.apache.org/" target="_blank" data-wf-native-id-path="085921f8-2483-8a2b-77c7-6dd0a677c6dc" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="085921f8-2483-8a2b-77c7-6dd0a677c6dc">Apache Spark</a>, ingested in Rust and stored as versioned assets in AWS S3.</p><figure><p><img src="https://cdn.prod.website-files.com/67606084339203323d92a420/689379d4daef27d8453c9535_HorizonDB%20graphic%201%20(2).png" loading="lazy" alt="HorizonDB"></p><figcaption><strong>Figure 1:</strong> The HorizonDB server is a single multi-threaded process that concurrently queries different ‚Äúlayers‚Äù and re-ranks the candidates in a uniform manner.</figcaption></figure><p><strong>Rust</strong> <br>‚Äç<a href="https://www.rust-lang.org/" target="_blank" data-wf-native-id-path="61aabe56-ec07-0c19-2b0a-486b8ca52eed" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="61aabe56-ec07-0c19-2b0a-486b8ca52eed">https://www.rust-lang.org/</a></p><p>A compiled language designed by Mozilla meant for systems programming. There are many aspects the team liked about Rust:</p><ul role="list"><li><strong>Compiled and memory safety without garbage collection:</strong> Rust's strong type system and safe and expressive concurrency in the form of Rayon and Tokio lets us write performant code without sacrificing readability. Rust makes it trivial to manage memory without garbage collection, allowing us to manage large indexes of data in memory with predictable latency.<strong>‚Äç</strong></li><li><strong>Higher-order abstractions:</strong> Many of our engineers work with higher-level languages where expressive list operations, null-handling, and pattern matching are a given. Rust has these primitives, so that our team can move fast and express logic cleanly, which is important when dealing with complex logic such as search ranking.<strong>‚Äç</strong></li><li><strong>Multi-threaded not multi-process:</strong> Since HorizonDB needs to fetch hundreds of GB of data from SSDs, having a single process that can leverage the same memory address space is more efficient compared to our API layer language TypeScript deployed to Node.js, which dedicates a new process to every core.</li></ul><p><strong>RocksDB</strong> <br>‚Äç<a href="https://rocksdb.org/" target="_blank" data-wf-native-id-path="f13bf703-a60f-f9d6-e7d2-d7613c43693a" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="f13bf703-a60f-f9d6-e7d2-d7613c43693a">https://rocksdb.org/</a><br></p><p>S2 is Google's spatial indexing library that projects a quadtree onto a sphere, turning O(n) point-in-polygon lookups into cacheable constant time lookups. While writing HorizonDB we wrote Rust bindings for <a href="https://github.com/google/s2geometry" data-wf-native-id-path="11464a29-91e3-5d5c-4e8a-a3d41bdc88da" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="11464a29-91e3-5d5c-4e8a-a3d41bdc88da">Google's C++</a> library that we will open source soon.</p><p><strong>FSTs <br>‚Äç</strong><a href="https://github.com/BurntSushi/fst" target="_blank" data-wf-native-id-path="87bf22db-3a13-367f-f0b2-453ef8622c4e" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="87bf22db-3a13-367f-f0b2-453ef8622c4e">https://github.com/BurntSushi/fst</a> <br></p><p>FSTs are a data structure offering efficient string compression and prefix queries. <a href="https://blog.burntsushi.net/transducers/" target="_blank" data-wf-native-id-path="3160a8d5-54f9-64b1-c6d1-2cf8d2e064df" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="3160a8d5-54f9-64b1-c6d1-2cf8d2e064df">This blog post by Andrew Gallant describes in great detail how this is achieved</a>. We found 80% of our queries were well-formed and wanted an efficient way to cache these ‚Äúhappy-paths‚Äù. Using FSTs, we were able to cache millions of these happy-paths on the order of MBs of memory and often returned prefix candidates within single-digit milliseconds.</p><figure><p><img src="https://cdn.prod.website-files.com/67606084339203323d92a420/688bc15b64515589fe4e0a74_HorizonDB%20graphic%203.png" loading="lazy" alt="FST"></p><figcaption><strong>Figure 2:</strong> An FST compactly represents a corpus of words by compressing common &nbsp;prefixes and suffixes. FSTs are also able to encode numeric values which can be retrieved during traversal, allowing us to bitpack metadata such as latitude and longitude for ranking purposes.</figcaption></figure><p><strong>Tantivy</strong> <br>‚Äç<a href="https://github.com/quickwit-oss/tantivy" target="_blank" data-wf-native-id-path="24d9ba27-381c-3294-5233-76da3db48bdc" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="24d9ba27-381c-3294-5233-76da3db48bdc">https://github.com/quickwit-oss/tantivy</a><br></p><p>An in-process inverted index library similar to Lucene.</p><p>We made the decision to use an in-process index over an external service such as Elasticsearch or Meilisearch for a few reasons: <br></p><ul role="list"><li><strong>Search quality:</strong> To improve our recall for use cases like address validation, we often ‚Äúexpand‚Äù our search keywords dynamically. This would translate to sending multiple queries over the wire if we used an external service.</li><li><strong>Operational simplicity:</strong> Everything is within the same process, so scaling search servers becomes trivial. Memory mapping gives us a way to efficiently use commodity hardware with large indexes. We found this much simpler than scaling Elasticsearch where tuning JVM params and trying to saturate CPU without increased latency was very difficult.</li></ul><figure><p><img src="https://cdn.prod.website-files.com/67606084339203323d92a420/688bc1919033001b226b4ada_HorizonDB%20graphic%202.png" loading="lazy" alt="Inverted index"></p><figcaption><strong>Figure 3:</strong> An inverted index (as opposed to a typical forward index like in a SQL database) can performantly find relevant documents based on term lookups. Document IDs can be compressed via techniques like delta encoding. The query ‚ÄúCentral Park‚Äù can be performantly handled by querying and combining the documents returned from the terms ‚Äúcentral‚Äù and ‚Äúpark‚Äù.</figcaption></figure><p><strong>FastText</strong> <br>‚Äç<a href="https://fasttext.cc/" target="_blank" data-wf-native-id-path="b169e495-b11e-cfdf-02c6-06395ebf4ca2" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="b169e495-b11e-cfdf-02c6-06395ebf4ca2">https://fasttext.cc/</a><br></p><p>To improve precision and search quality, we implemented a FastText model trained from a mix of our geocoder corpus and query logs. With FastText, we can semantically represent words in a query in a numeric vector format, suitable for ML applications. FastText is typo-tolerant and handles out-of-vocabulary words with its use of ngrams. ‚ÄúNearby‚Äù vectors represent semantically similar words allowing our ML algorithms to understand semantics of a given word in a search query.</p><p><strong>LightGBM</strong> <br>‚Äç<a href="https://github.com/microsoft/LightGBM" target="_blank" data-wf-native-id-path="a2342c00-ec1f-9dfc-2e0e-f08ac84d9c3d" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="a2342c00-ec1f-9dfc-2e0e-f08ac84d9c3d">https://github.com/microsoft/LightGBM</a><br></p><p>We have trained multiple LightGBM models to classify query intent and tag parts of our query depending on the intent.&nbsp;This allows us to ‚Äústructure‚Äù our queries, improving search performance and precision.&nbsp;For example, a query deemed as a regional query such as ‚ÄúNew York‚Äù can skip address search, whereas a query like ‚Äú841 broadway‚Äù allows us to skip searching POIs and regions.</p><p><strong>Apache Spark <br>‚Äç</strong><a href="https://spark.apache.org/" target="_blank" data-wf-native-id-path="cacbdb3a-faf4-f5f8-4299-245b11f222d3" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="cacbdb3a-faf4-f5f8-4299-245b11f222d3">https://spark.apache.org/</a></p><p>With Spark, we are able to process hundreds of millions of data points in less than an hour, with near-linear scalability. We often had to tune or refactor jobs to achieve optimal performance when performing joins or aggregations.</p><p>Since our data is written to S3, it becomes trivial to inspect results via Amazon Athena, a hosted deployment of Apache Presto that can read object storage assets using SQL. DuckDB is another lightweight tool that our engineers use to inspect these assets on the fly.</p><h2><strong>Results</strong></h2><p>HorizonDB has transformed both the operational and developmental aspects of our geolocation offerings. We've achieved improvements across the board for cost, performance, and scalability:</p><ul role="list"><li>Our service is now faster, operationally simple, and reliable.</li><li>Our developers are able to move fast with new features and data changes. We are able to ingest and evaluate new data sources within a day.</li><li>We've shut down multiple Mongo clusters, a large Elasticsearch cluster, and several geo microservices, saving us high five-figures in monthly costs.</li></ul><p>We are happy with our design decisions with HorizonDB and are prepared for our scale for the foreseeable future. We will touch on how we designed particular features of the system in future blog posts.</p><p>Many thanks to our hard-working engineers Bradley Schoeneweis, Jason Liu, Jacky Wang, Binh Robles, Greg Sadetsky, David Gurevich, and Felix Li who made this system a reality.</p><h2>Join us</h2><p>Radar is more than just an API&nbsp;layer. Across SDKs, maps, databases, and infrastructure, we're rethinking geolocation from the ground up to offer the fastest, most developer-friendly location stack available.</p><p>If this blog post was interesting to you, we're hiring great engineering talent across the board. </p><p>Check out our <a href="https://radar.com/jobs" target="_blank" data-wf-native-id-path="483df42e-e888-d31f-82e8-8bee205833e4" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="483df42e-e888-d31f-82e8-8bee205833e4">jobs page</a> for more information.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Food, housing, & health care costs are a source of major stress for many people (283 pts)]]></title>
            <link>https://apnorc.org/projects/food-housing-and-health-care-costs-are-a-source-of-major-stress-for-many-people/</link>
            <guid>44836219</guid>
            <pubDate>Fri, 08 Aug 2025 12:27:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnorc.org/projects/food-housing-and-health-care-costs-are-a-source-of-major-stress-for-many-people/">https://apnorc.org/projects/food-housing-and-health-care-costs-are-a-source-of-major-stress-for-many-people/</a>, See on <a href="https://news.ycombinator.com/item?id=44836219">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-10926">
	<!-- .entry-header -->

	<div>

									<div>
						<figure>
							<img width="950" height="300" src="https://apnorc.org/wp-content/uploads/2025/08/AP24023711063510-scaled-950x300.jpg" alt="" decoding="async" fetchpriority="high">							<figcaption></figcaption>
						</figure>
					</div>
				
				
<p><em>August 4, 2025</em></p>



<p>About half of the public consider the cost of groceries to be a major source of stress in their life right now, and 19% of those concerned have used deferred payment services to fund groceries at some point.</p>



<p>Overall, 29% of the public have ever used deferred payment services, sometimes called Buy Now Pay Later, for health care, entertainment, groceries, or restaurant meals. Use of these services is higher among adults under age 45 compared with older adults.</p>



<p>People experiencing economic stress are much more likely to use these services.</p>


<div>
<figure><img decoding="async" width="804" height="1024" src="https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-804x1024.png" alt="" srcset="https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-804x1024.png 804w, https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-236x300.png 236w, https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-768x978.png 768w, https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-1207x1536.png 1207w, https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-1609x2048.png 1609w, https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-scaled.png 2011w" sizes="(max-width: 804px) 100vw, 804px"></figure></div>


<p>Fifty-three percent of adults report that grocery expenses are a major source of stress, and another 33% say they are a minor stress. About half also identify housing costs as a major concern. Additionally, 43% express stress related to their personal income and savings. Health care costs are also a major source of stress for 4 in 10 adults.Fewer report major stress from debt or the cost of child care.</p>


<div>
<figure><img decoding="async" width="805" height="1024" src="https://apnorc.org/wp-content/uploads/2025/08/Picture2-805x1024.png" alt="" srcset="https://apnorc.org/wp-content/uploads/2025/08/Picture2-805x1024.png 805w, https://apnorc.org/wp-content/uploads/2025/08/Picture2-236x300.png 236w, https://apnorc.org/wp-content/uploads/2025/08/Picture2-768x977.png 768w, https://apnorc.org/wp-content/uploads/2025/08/Picture2-1207x1536.png 1207w, https://apnorc.org/wp-content/uploads/2025/08/Picture2-1610x2048.png 1610w, https://apnorc.org/wp-content/uploads/2025/08/Picture2-scaled.png 2012w" sizes="(max-width: 805px) 100vw, 805px"></figure></div>


<p>Overall, 75% say one or more of these financial factors cause them major stress. These individuals with significant stressors in their life are more likely to use Buy Now Pay Later services than those who report minor or no stress. For example, 21% of people who experience any major stress have used Buy Now Pay Later services for medical or dental expenses, whereas 8% of those with minor or no stress. .</p>


<div>
<figure><img loading="lazy" decoding="async" width="804" height="1024" src="https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-804x1024.png" alt="" srcset="https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-804x1024.png 804w, https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-236x300.png 236w, https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-768x978.png 768w, https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-1207x1536.png 1207w, https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-1609x2048.png 1609w, https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-scaled.png 2011w" sizes="auto, (max-width: 804px) 100vw, 804px"></figure></div>


<p>Adults under age 45 report higher levels of stress related to their earnings, the cost of housing, student debt, and childcare compared with older adults. In other areas, stress levels are generally comparable across age groups such as the cost of groceries and the amount of money saved, and the cost of health care.</p>


<div>
<figure><img loading="lazy" decoding="async" width="689" height="1024" src="https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-689x1024.png" alt="" srcset="https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-689x1024.png 689w, https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-202x300.png 202w, https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-768x1142.png 768w, https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-1033x1536.png 1033w, https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-1378x2048.png 1378w, https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-scaled.png 1722w" sizes="auto, (max-width: 689px) 100vw, 689px"></figure></div>


<p>The nationwide poll was conducted July 10-14, 2025 using the AmeriSpeak¬Æ Panel, the probability-based panel of NORC at the University of Chicago. Online and telephone interviews using landlines and cell phones were conducted with 1,437 adults. The overall margin of sampling error is +/- 3.6 percentage points. Respondents age 18-29 were sampled at a higher rate than their proportion of the population for reasons of analysis. The overall margin of sampling error for the 386 interviews completed with respondents age 18-29 is +/- 6.6 percentage points.</p>



<ul>
<li>Suggested Citation: AP-NORC Center for Public Affairs Research. ‚ÄúSupport for legal abortion remains strong.‚Äù (July 2025).</li>
</ul>





							</div>

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ultrathin business card runs a fluid simulation (779 pts)]]></title>
            <link>https://github.com/Nicholas-L-Johnson/flip-card</link>
            <guid>44835879</guid>
            <pubDate>Fri, 08 Aug 2025 11:41:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Nicholas-L-Johnson/flip-card">https://github.com/Nicholas-L-Johnson/flip-card</a>, See on <a href="https://news.ycombinator.com/item?id=44835879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">This repo contains all files related to the flip-card project, which is a business card that runs a fluid-implicit-particle(FLIP) simulation.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Nicholas-L-Johnson/flip-card/blob/main/media/1000003136.gif"><img src="https://github.com/Nicholas-L-Johnson/flip-card/raw/main/media/1000003136.gif" alt="alt text" data-animated-image=""></a></p>
<p dir="auto">The PCB design files are in the "kicad-pcb" folder. The flip-card project is inspired by mitxela's fluid simulation pendant project <a href="https://mitxela.com/projects/fluid-pendant" rel="nofollow">https://mitxela.com/projects/fluid-pendant</a></p>
<p dir="auto">The fluid simulation logic is contained in a standalone crate, which is in the "fluid_sim_crate" folder. This is based off the work by Matthias M√ºller (<a href="https://github.com/matthias-research">https://github.com/matthias-research</a>) and his excelent demonstrations on his youtube channel "Ten Minute Physics"</p>
<p dir="auto">One of the more difficult features was the rechargable battery.  I found a design for a board edge usb-c port from cnlohr's tiny touch lcd project <a href="https://github.com/cnlohr/ch32v003_3digit_lcd_usb/">https://github.com/cnlohr/ch32v003_3digit_lcd_usb/</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Nicholas-L-Johnson/flip-card/blob/main/media/Charging.jpg"><img src="https://github.com/Nicholas-L-Johnson/flip-card/raw/main/media/Charging.jpg" alt="alt text"></a></p>
<p dir="auto">a WASM simulator is also provided in the "sim_display" folder, which is what I use to debug issues in the simulation.</p>
<p dir="auto">The implementation for the fluid simulation on the rp2350 is in the "flip-card_firmware" file</p>
<p dir="auto">further details can be found in each folder's README files</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Nicholas-L-Johnson/flip-card/blob/main/media/PCB_3D.JPG"><img src="https://github.com/Nicholas-L-Johnson/flip-card/raw/main/media/PCB_3D.JPG" alt="alt text"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Nicholas-L-Johnson/flip-card/blob/main/media/PCB_Back.JPG"><img src="https://github.com/Nicholas-L-Johnson/flip-card/raw/main/media/PCB_Back.JPG" alt="alt text"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US to rewrite its past national climate reports (235 pts)]]></title>
            <link>https://www.france24.com/en/live-news/20250807-us-to-rewrite-its-past-national-climate-reports</link>
            <guid>44835287</guid>
            <pubDate>Fri, 08 Aug 2025 10:02:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.france24.com/en/live-news/20250807-us-to-rewrite-its-past-national-climate-reports">https://www.france24.com/en/live-news/20250807-us-to-rewrite-its-past-national-climate-reports</a>, See on <a href="https://news.ycombinator.com/item?id=44835287">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                                <p>The decision, announced by Energy Secretary Chris Wright during a CNN appearance Tuesday night, follows the government's revocation of the Endangerment Finding, a scientific determination that underpins a host of regulations aimed at curbing greenhouse gas emissions.</p><p>Asked by CNN's Kaitlan Collins why previous editions of the National Climate Assessment were no longer available online, former fracking company CEO Wright responded: "Because we're reviewing them, and we will come out with updated reports on those and with comments on those."</p><p>First published in 2000, the National Climate Assessment has long been viewed as a cornerstone of the US government's understanding of climate science, synthesizing input from federal agencies and hundreds of external experts.</p><p>Previous editions warned in stark terms of mounting risks to America's economy, infrastructure, and public health if greenhouse gas emissions are not curtailed. But in April, the administration moved to dismiss the hundreds of scientists working on the sixth edition.</p><p>Under the Global Change Research Act of 1990, the government is legally obligated to deliver the climate assessment to Congress and the president.</p><p>Trump's administration and the Republican-controlled Congress have pressed forward with their pro- fossil fuel agenda -- dismantling clean energy tax credits through the so-called "Big Beautiful Bill" and opening more ecologically sensitive lands to drilling.</p><p>Last month's proposed revocation of the Endangerment Finding by the Environmental Protection Agency was accompanied by the release of a new climate study from the Department of Energy, authored by climate change contrarians.</p><p>The study questioned whether heat records are truly increasing and whether extreme weather is worsening.</p><p>It also misrepresented the work of cited climate scientists, according to several who spoke to AFP, and suggested that rising atmospheric carbon dioxide could be a net benefit for agriculture.</p>
                                        <p>¬© 2025 AFP</p>                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Attention Sinks Keep Language Models Stable (141 pts)]]></title>
            <link>https://hanlab.mit.edu/blog/streamingllm</link>
            <guid>44834918</guid>
            <pubDate>Fri, 08 Aug 2025 08:53:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hanlab.mit.edu/blog/streamingllm">https://hanlab.mit.edu/blog/streamingllm</a>, See on <a href="https://news.ycombinator.com/item?id=44834918">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>We discovered why language models catastrophically fail on long conversations: when old tokens are removed to save memory, models produce complete gibberish. We found models dump massive attention onto the first few tokens as "attention sinks"‚Äîplaces to park unused attention since softmax requires weights to sum to 1. Our solution, StreamingLLM, simply keeps these first 4 tokens permanently while sliding the window for everything else, enabling stable processing of 4 million+ tokens instead of just thousands. This mechanism is now in HuggingFace, NVIDIA TensorRT-LLM, and OpenAI's latest models.</p><div><p>‚Äç</p><p>This week, OpenAI made headlines by releasing their first open-source large language models, GPT-OSS-20B and GPT-OSS-120B. Buried in the technical documentation was a fascinating architectural detail: the inclusion of <strong>attention sink mechanisms</strong>.</p><p>Their implementation adds a trainable scalar value to each attention head's softmax calculation:</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/68953e9f8d1a14e5b43c4a08_Screenshot%202025-08-07%20at%205.02.12%E2%80%AFPM.png" loading="lazy" alt=""></p></figure><p>‚Äç</p><p>This simple modification‚Äîadding just one learnable parameter per attention head‚Äîenables the model to "pay no attention to any tokens" when needed, a design choice OpenAI's model card explicitly attributes to our <a href="https://arxiv.org/abs/2309.17453"><em>StreamingLLM</em></a> work.</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/68953ec17590788f3932d165_01_openai_model_card.png" loading="lazy" alt=""></p><figcaption><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf"><em>OpenAI's model card</em></a><em> for GPT-OSS-20B explains the attention sink mechanism, directly connecting the design to our research.</em></figcaption></figure><p>‚Äç</p><p>Seeing this feature in a major OpenAI release connected directly to research that began during my internship at Meta in the summer of 2023, when I was tasked with solving what seemed like a simple problem: </p><blockquote>How do you make a language model handle conversations longer than what it was trained for?</blockquote><p>This is the story of <strong>attention sinks</strong>: how we discovered this mechanism that every Transformer relies on, why it's crucial for model stability, and how this research has found its way into production AI systems.</p><h3><strong>The Streaming Challenge</strong></h3><p>In the beginning of the 2023 summer, I was presented with a fundamental question:</p><p><em>How can we make a language model handle conversations longer than what it was trained for?</em></p><p>The challenge affects any real-world AI application. Consider a chatbot engaged in an ongoing conversation‚Äîit needs to be aware of the recent context, but it can't afford to re-process the entire conversation history with every new word it generates. The computational cost would grow quadratically, making long conversations prohibitively expensive.</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/689564b728d02538b942000c_Screenshot%202025-08-07%20at%207.44.34%E2%80%AFPM.png" loading="lazy" alt=""></p><figcaption>Dense attention has a quadratic time complexity and an increasing cache size. It performance decreases when the text length exceeds the pre-training text length.</figcaption></figure><p>‚Äç</p><p>The obvious solution seemed to be a <strong>sliding window approach</strong>: keep a fixed-size cache of the most recent tokens' internal states (their Key and Value vectors, known as the KV cache) and simply drop the oldest ones as the conversation grows. This approach is efficient and‚Äîas we discovered‚Äîfails spectacularly.</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/6895651910922ac3793f2ea3_Screenshot%202025-08-07%20at%207.44.46%E2%80%AFPM.png" loading="lazy" alt=""></p><figcaption>Window Attention caches the most recent tokens‚Äô KV. While efficient in inference, performance declines sharply oncethe starting tokens‚Äô keys and values are evicted.</figcaption></figure><p>Our experiments revealed a stark and unexpected result: the moment we removed the very first few tokens from the cache, the model's performance collapsed entirely. The perplexity‚Äîa distance between the model's predictions and the ground truths‚Äîskyrocketed. Models that had been generating perfectly coherent text suddenly began producing complete nonsense.</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/68953fb16fee2a53dc5974fb_04_perplexity_failure.png" loading="lazy" alt=""></p><figcaption><em>The model's perplexity skyrockets the moment the initial tokens are evicted from the cache, indicating catastrophic failure.</em></figcaption></figure><p>This was puzzling. The first few tokens often carried minimal semantic information‚Äîsometimes just a start-of-sequence marker or common words like "the" or "a." Why would removing these seemingly insignificant tokens cause such catastrophic failure?</p><h3><strong>The Discovery: When Attention Gets into Sinks</strong></h3><p>When I began visualizing the attention patterns inside models like Llama-2, a consistent and unexpected pattern emerged. Across most layers, large amounts of attention were being directed toward the very first tokens in the sequence. Regardless of what the current token was trying to predict, it would often "look back" toward the beginning.</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/68953fe0c2a45acce0941c9f_05_attention_visualization.png" loading="lazy" alt=""></p><figcaption><em>Attention visualization showing a heavy focus on the first tokens across multiple layers in LLaMA-2.</em></figcaption></figure><p>‚Äç</p><p>The behavior reminded me of graph theory from my undergraduate studies. <em>In directed graphs, a </em><strong><em>sink node</em></strong><em> is defined as a vertex with no outgoing connections‚Äîit receives flow from other nodes but doesn't pass it along.</em> These initial tokens were behaving similarly: they were absorbing attention from across the sequence. This analogy led me to term them <strong>attention sinks</strong>.</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/68953ff1a6d5fc18da6d8251_06_graph_sink_diagram.png" loading="lazy" alt=""></p><figcaption><em>A visual representation of a sink in a directed graph. (Source: mathworld.wolfram.com)</em></figcaption></figure><p>‚Äç</p><h4><strong>The Mathematical Foundation</strong></h4><p>The mathematics behind this behavior reveals a fundamental constraint of the Transformer architecture. In the attention mechanism, we compute attention weights using the softmax function:</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/689540336fee2a53dc59b5ff_Screenshot%202025-08-07%20at%205.09.00%E2%80%AFPM.png" loading="lazy" alt=""></p></figure><p>The softmax function forces all attention weights to sum to exactly 1.0:</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/68954039d18adaaf6c0ee92f_Screenshot%202025-08-07%20at%205.09.05%E2%80%AFPM.png" loading="lazy" alt=""></p></figure><p>‚Äç</p><p>This creates what Evan Miller eloquently described as a <a href="https://www.evanmiller.org/attention-is-off-by-one.html">"deafening democracy where abstention is disallowed."</a> Every attention head <em>must</em> allocate its focus somewhere, even when it has no meaningful information to contribute.</p><p>When a token encounters no particularly relevant context to attend to, where does that mandatory attention "budget" go? In practice, some positions (especially the initial tokens) tend to have slightly higher baseline scores due to their consistent presence across training examples. This small bias gets amplified through training, causing these positions to evolve into specialized repositories for otherwise unused attention‚Äîfunctioning as computational pressure valves.</p><h4><strong>Not a New Phenomenon</strong></h4><p>This phenomenon wasn't entirely new. <a href="https://arxiv.org/abs/1906.04341">Researchers had observed similar patterns in BERT</a>, where "a surprisingly large amount of attention focuses on the delimiter token [SEP] and periods," which they argued was used by the model as a sort of no-op. The same summer at Meta, <a href="https://arxiv.org/abs/2309.16588">researchers studying vision transformers found similar behavior,</a> observing that models would repurpose uninformative background patches as computational scratchpads.</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/6895404b06afb0c46baf1118_07_bert_attention_patterns.png" loading="lazy" alt=""></p></figure><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/6895405806afb0c46baf178e_08_vision_transformer_patterns.png" loading="lazy" alt=""></p></figure><p><em>Researchers observed similar sink-like behavior in both NLP models like BERT and vision models.</em></p><p>‚Äç</p><h4><strong>The Revelation</strong></h4><p>Our discovery suddenly illuminated why the sliding window approach failed so catastrophically. When we evicted those initial tokens, we weren't simply removing old context‚Äîwe were removing a huge proportion of the denominator in the softmax function, dismantling the model's fundamental mechanism for maintaining attention stability. We had inadvertently removed its pressure valve.</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/6895406c7590788f393425d0_09_sliding_window_failure.png" loading="lazy" alt=""></p><figcaption><em>Illustration of why sliding window approaches fail when attention sinks are removed.</em></figcaption></figure><p>‚Äç</p><h3><strong>The Simple Fix: Just Don't Throw Away the Sinks</strong></h3><p>Understanding the problem led us to an almost embarrassingly simple solution: if these already-trained models desperately need attention sinks, why not just never throw them away?</p><p>Our work, <strong>StreamingLLM</strong>, introduced a surprisingly straightforward modification to standard KV cache management. Instead of discarding the first few tokens like any others, we permanently preserve their Key and Value states while maintaining a sliding window for everything else.</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/68953de16fee2a53dc580b4d_10_streaminglLM_diagram.png" loading="lazy" alt=""></p><figcaption><em>Diagram comparing a standard sliding window to the StreamingLLM approach, where the first few "sink" tokens are always kept in the cache.</em></figcaption></figure><p>‚Äç</p><p>The implementation couldn't be simpler:</p><pre contenteditable="false"><code><span>1</span><span># StreamingLLM: Always keep the first few tokens as attention sinks</span><span>
</span><span>2</span><span></span><span>def</span><span> </span><span>get_streaming_kv_cache</span><span>(</span><span>full_history, window_size=</span><span>1020</span><span>, sink_size=</span><span>4</span><span>):</span><span>
</span><span>3</span><span>    </span><span># Never discard the first few tokens - they're our attention sinks</span><span>
</span><span>4</span>    sink_tokens = full_history[:sink_size]
<span>5</span><span>    </span><span># Maintain a sliding window for the actual content</span><span>
</span><span>6</span>    recent_tokens = full_history[-(window_size - sink_size):]
<span>7</span><span>    </span><span># Combine permanent sinks with recent content</span><span>
</span><span>8</span><span>    </span><span>return</span><span> sink_tokens + recent_tokens</span></code></pre><p>The results were remarkable. Models like LLaMA that previously collapsed after a few thousand tokens could now maintain stable perplexity across <strong>4 million tokens</strong>‚Äîprocessing sequences orders of magnitude longer than their original training context. We had unlocked infinite-length generation simply by respecting the model's existing attention patterns.</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/689540d077afb7e97969b315_11_performance_results.png" loading="lazy" alt=""></p><figcaption><em>The perplexity of StreamingLLM remaining low and stable across 4 million tokens, while other methods fail.</em></figcaption></figure><p>‚Äç</p><h3><strong>But Why Four Sinks? The Pre-Training Question</strong></h3><p>This success raised an intriguing follow-up question: why did we need to preserve <strong>four</strong> attention sink tokens? Could we get away with just one?</p><p>This curiosity led us to our pre-training experiments. We trained two identical 160-million parameter models from scratch‚Äîone with standard training, and another that included a dedicated [SINK] token at the start of every training sample.</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/6895410306afb0c46baf8f29_12_pretraining_convergence.png" loading="lazy" alt=""></p><figcaption><em>Adding a sink token during pre-training benefits the model's convergence trend and zero-shot performance across several NLP benchmarks.</em></figcaption></figure><p>‚Äç</p><p>The results revealed something profound: the model trained with a dedicated sink token needed only <strong>one</strong> attention sink during streaming, while the vanilla model required <strong>four</strong> repurposed content tokens to maintain stability. Moreover, the model with the purpose-built sink actually converged slightly better during training.</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/6895419ba1a382b9e1711fe3_13_sink_comparison.png" loading="lazy" alt=""></p><figcaption><em>Training without dedicated sinks and with SoftMax-off-by-one (Zero Sink) still requires multiple initial tokens to stabilize performance‚Äîindicating multiple implicit sinks‚Äîwhereas a single learnable sink token alone is sufficient.</em></figcaption></figure><p>‚Äç</p><p>This experiment showed that models can learn to use purpose-built attention sinks more efficiently than hijacking existing content tokens‚Äîa finding that would influence how attention mechanisms are designed from the ground up.</p><h3><strong>Two Paths to the Same Solution: Our Design vs. OpenAI's</strong></h3><p>Interestingly, OpenAI's recent implementation represents a different approach to solving the same problem. Where we use a dedicated sink token k<sub>0</sub> at the sequence start:</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/6895415d77afb7e97969d5e5_Screenshot%202025-08-07%20at%205.14.01%E2%80%AFPM.png" loading="lazy" alt=""></p></figure><p>‚Äç</p><p>OpenAI simplified this with a universal scalar approach:</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/68954164d18adaaf6c0f3e83_Screenshot%202025-08-07%20at%205.14.05%E2%80%AFPM.png" loading="lazy" alt=""></p></figure><p>‚Äç</p><p>The key difference: our approach lets different tokens have different relationships with the sink (some tokens might "need" the sink more than others), while OpenAI's treats the sink as a simple escape valve‚Äîthe same for all tokens. Their design trades expressiveness for simplicity, eliminating the need for sink token embeddings while capturing the core insight that models need somewhere to dump unused attention.</p><p>Both approaches solve the fundamental problem, but represent different philosophies: we treated the sink as a learnable component, while they engineered it as an architectural necessity.</p><h3><strong>The Science Behind the Sink: Recent Discoveries</strong></h3><p>Since our work, researchers have deepened our understanding of why attention sinks emerge and how they function.</p><p><a href="https://arxiv.org/abs/2504.02732">Barbero et al.</a> have shown that attention sinks serve as "pressure valves" preventing what researchers call <strong>"over-mixing"</strong>‚Äîa pathological state where deep models processing long sequences blur important distinctions between tokens. The presence of a sink draws attention away from other tokens, limiting the spread of information (and noise) and resulting in more stable embeddings. This effect becomes more pronounced in larger models; LLaMA 3.1 405B shows attention sinks in a remarkable <strong>80% of its attention heads</strong>.</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/689541c9aebe1e5eee795227_14_overmixing_diagram.png" loading="lazy" alt=""></p><figcaption><em>Research shows sinks slow the mixing of information, making Transformers more robust. A perturbation (red) spreads less with a sink (right) than without (left). (Source: </em><a href="https://arxiv.org/abs/2504.02732"><em>Barbero et al.</em></a><em>)</em></figcaption></figure><p>‚Äç</p><p><a href="https://arxiv.org/abs/2410.10781">Gu et al.</a> has traced sink formation to the fundamental constraint of softmax normalization. As we noted, when attention weights must sum to 1.0, models need a default place to allocate their attention budget. Tellingly, replacing softmax with other attention operations that don't have this constraint prevents sinks from forming entirely.</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/689541dcaebe1e5eee7957ce_15_kv_biases_table.png" loading="lazy" alt=""></p><figcaption><em>Table from </em><a href="https://arxiv.org/abs/2410.10781"><em>Gu et al.</em></a><em> showing how KV biases can prevent sink formation.</em></figcaption></figure><p>‚Äç</p><h4><strong>Practical Applications</strong></h4><p>These insights have inspired practical applications. <a href="https://arxiv.org/abs/2502.01563v4">Sun et al.</a> directly introduced learnable key and value parameters (<strong>KV biases</strong>) into the attention mechanism, finding this design could alleviate the massive activations seen during inference‚Äîessentially the same approach as our learnable sink experiments. Building on this understanding, <a href="https://arxiv.org/abs/2406.12016"><strong>"CushionCache"</strong> by Son et al.</a> uses deliberately designed attention sink prefixes that improve model quantization by reducing activation outliers.</p><p>‚Äç</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/689541f508148cd59c684d77_16_learnable_kv_biases.png" loading="lazy" alt=""></p><figcaption><a href="https://arxiv.org/abs/2502.01563v4"><em>Diagram from Sun et al.</em></a><em> showing the architectural addition of learnable KV biases.</em></figcaption></figure><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/689542087590788f39357652_17_cushioncache_results.png" loading="lazy" alt=""></p><figcaption><a href="https://arxiv.org/abs/2406.12016"><em>Techniques like CushionCache (Son et al.)</em></a><em> use sinks to tame activation spikes, improving quantization.</em></figcaption></figure><p>‚Äç</p><p>Given this connection between attention sinks and quantization stability, it's intriguing to speculate that OpenAI's built-in attention sink mechanism may partly enable the <strong>aggressive 4-bit weight quantization</strong> in their open-source models.</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/689567a19625daa1804dedc9_Screenshot%202025-08-07%20at%207.56.43%E2%80%AFPM.png" loading="lazy" alt=""></p><figcaption><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf">OpenAI's aggressive 4-bit quantization</a> in their open-source models likely benefits from the built-in attention sink mechanism, which helps prevent the activation outliers that typically plague extreme quantization. </figcaption></figure><h3><strong>From Research to Reality</strong></h3><p>What began as a practical engineering problem during my internship has evolved into a fundamental insight about Transformer architecture. The attention sink mechanism we discovered has since been adopted across the industry, appearing in production systems like OpenAI's models and inspiring new research directions in quantization and model optimization.</p><p>The adoption happened remarkably quickly. By October 2023, Intel integrated StreamingLLM into their Extension for Transformers, enabling continuous LLM inference on CPUs with just 3 lines of code. December 2023 saw explosive adoption: HuggingFace integrated attention sinks into their main Transformers branch, and just days later, researchers from CMU, UW, and OctoAI demonstrated endless LLM generation running directly on iPhones, noting that "attention sinks are particularly helpful for longer generation with less memory requirement." The momentum continued into 2024 with NVIDIA incorporating StreamingLLM into TensorRT-LLM in January. Now, in August 2025, OpenAI released their open-source models with built-in attention sink parameters, bringing the mechanism full circle from research discovery to production implementation.</p><p>Sometimes an impactful discovery emerges not from grand theoretical breakthroughs, but from carefully investigating the curious details that others might overlook. In our case, questioning why a few seemingly meaningless tokens were so critical led us to uncover a mechanism that every Transformer model relies on‚Äîone that was hiding in plain sight.</p><h2>References</h2><ol start="" role="list"><li><strong>Xiao, G., Tian, Y., Chen, B., Han, S., &amp; Lewis, M.</strong> (2024). <em>Efficient Streaming Language Models with Attention Sinks</em>.</li><li><strong>OpenAI</strong> (2025). <em>GPT-OSS Model Card</em>. <a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf">https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf</a></li><li><strong>Miller, E.</strong> (2021). <em>Attention Is Off By One</em>. <a href="https://www.evanmiller.org/attention-is-off-by-one.html">https://www.evanmiller.org/attention-is-off-by-one.html</a></li><li><strong>Clark, K., Khandelwal, U., Levy, O., &amp; Manning, C. D.</strong> (2019). <em>What Does BERT Look At? An Analysis of BERT's Attention</em>.</li><li><strong>Darcet, T., Oquab, M., Mairal, J., &amp; Bojanowski, P.</strong> (2024). <em>Vision Transformers Need Registers.</em></li><li><strong>Barbero, F., Arroyo, √Å., Gu, X., Perivolaropoulos, C., Bronstein, M., Veliƒçkoviƒá, P., &amp; Pascanu, R.</strong> (2024). <em>Why do LLMs attend to the first token?</em> </li><li><strong>Gu, X., Pang, T., Du, C., Liu, Q., Zhang, F., Du, C., &amp; Wang, Y.</strong> (2024). <em>When Attention Sink Emerges in Language Models: An Empirical View</em>.</li><li><strong>Sun, M., Chen, X., Kolter, J. Z., &amp; Liu, Z.</strong> (2024). <em>Massive Activations in Large Language Models</em>.</li><li><strong>Son, S., Park, W., Han, W., Kim, K., &amp; Lee, J.</strong> (2024). <em>Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization</em>.</li><li><strong>Wolfram Research</strong> (2024). <em>Digraph Sink</em>. <a href="https://mathworld.wolfram.com/DigraphSink.html">https://mathworld.wolfram.com/DigraphSink.html</a></li></ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linear sent me down a local-first rabbit hole (400 pts)]]></title>
            <link>https://bytemash.net/posts/i-went-down-the-linear-rabbit-hole/</link>
            <guid>44833834</guid>
            <pubDate>Fri, 08 Aug 2025 05:45:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bytemash.net/posts/i-went-down-the-linear-rabbit-hole/">https://bytemash.net/posts/i-went-down-the-linear-rabbit-hole/</a>, See on <a href="https://news.ycombinator.com/item?id=44833834">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article"> <p>I started using Linear a couple of months ago and using it made me go down a technical rabbit hole that changed how I think about web applications.</p>
<p>For the uninitiated, <a href="https://linear.app/">Linear</a> is a project management tool that feels impossibly fast. Click an issue, it opens instantly. Update a status and watch in a second browser, it updates almost as fast as the source. No loading states, no page refreshes - just instant, interactions.</p>
<p>After building traditional web apps for years, this felt wrong. Where‚Äôs the network latency? How are they handling conflicts? What happens when you go offline?</p>
<p><em>If you‚Äôre still unsure what local-first looks like, I think <a href="https://livestore.dev/">LiveStore‚Äôs</a> demo is the best.</em></p>
<h2 id="down-the-rabbit-hole">Down the Rabbit Hole</h2>
<p>Armed with a rainy weekend and too much coffee I was determined to learn this wizardry. What I found was a goldmine of engineering deep-dives:</p>
<ul>
<li><a href="https://github.com/backupManager/reverse-linear-sync-engine-dev">A reverse engineering of Linear‚Äôs sync engine</a> - endorsed by Linear‚Äôs CTO</li>
<li><a href="https://marknotfound.com/posts/reverse-engineering-linears-sync-magic/">Another breakdown of their sync protocol</a></li>
<li><a href="https://www.youtube.com/live/WxK11RsLqp4?si=QrQf6cI8eBH0xw9Z&amp;t=2176">Linear CTO Tuomas Artman‚Äôs talk on their architecture</a></li>
<li><a href="https://www.youtube.com/live/Wo2m3jaJixU">A follow up talk by Tuomas on scaling the sync-engine</a></li>
<li><a href="https://www.figma.com/blog/how-figmas-multiplayer-technology-works/">Figma‚Äôs port about their multiplayer tech, that Tuomas referenced</a></li>
</ul>
<p>The short version: they built their own sync engine that treats your browser‚Äôs IndexedDB as a real database. Every change happens locally first, then in the background uses GraphQL for mutations and Websockets for sync.</p>
<p>I also found the term ‚Äúlocal-first‚Äù kept popping up, which depending on the content you read is either a UX strategy for apps that feel local (instant updates, etc.) or a philosophy about keeping your data local and syncing across devices.</p>
<p>In most instances the concept is beautifully simple: instead of your app being a fancy form that sends data to a server, it has it‚Äôs own local database. Sometimes the server is just another client to sync with. It can be a fundamental inversion of how we typically build web applications.</p>
<p>In a traditional web app, the server is the only source of truth:</p>
<pre tabindex="0" data-language="text"><code><span><span>Client ‚Üí HTTP Request ‚Üí Server ‚Üí Database ‚Üí Response ‚Üí Client</span></span></code></pre>
<p>In a local-first/sync approach, each client may have its own (nearly) complete database:</p>
<pre tabindex="0" data-language="text"><code><span><span>Client ‚Üí Local Database ‚Üí UI Update</span></span>
<span><span>         ‚Üì (async)</span></span>
<span><span>    Sync Engine ‚Üí Server ‚Üí Other Clients</span></span></code></pre>
<p>The key point for me was that by moving the database to the client, you eliminate network latency from the user interaction path. Updates happen instantly because they‚Äôre just local database read/writes.</p>
<h2 id="the-challenge-this-is-not-trivial">The Challenge: This Is Not Trivial</h2>
<p>After understanding Linear‚Äôs approach, my first instinct was to build something similar. Then reality hit: even the basic version of their sync engine probably represents months of engineering effort.</p>
<p>The complexity comes from:</p>
<ul>
<li>Handling offline/online transitions gracefully</li>
<li>Conflict resolution across distributed clients</li>
<li>Partial synchronization (you don‚Äôt want to download your entire database)</li>
<li>Schema migrations across cached data</li>
<li>Security and access control in a distributed system</li>
</ul>
<p>Surely, someone has already built this magic into something I can reuse‚Ä¶</p>
<h2 id="the-local-first-ecosystem-in-2025">The Local-First Ecosystem in 2025</h2>
<p>Fortunately, the local-first community has been building solutions. Here‚Äôs the current landscape:</p>
<h3 id="production-ready-options">Production-Ready Options</h3>
<ul>
<li><strong><a href="https://electric-sql.com/">Electric SQL</a></strong> - Postgres-backed sync engine</li>
<li><strong><a href="https://www.powersync.com/">PowerSync</a></strong> - Enterprise-focused solution</li>
<li><strong><a href="https://jazz.tools/">Jazz</a></strong> - The one that caught my eye (see below)</li>
<li><strong><a href="https://replicache.dev/">Replicache</a></strong> - The OG (RIP)</li>
<li><strong><a href="https://zero.rocicorp.dev/">Zero</a></strong> - Replicache team‚Äôs new approach</li>
<li><strong><a href="https://www.triplit.dev/">Triplit</a></strong> - TripleStore-based sync</li>
<li><strong><a href="https://www.instantdb.com/">Instant</a></strong> - Focused on developer experience</li>
<li><strong><a href="https://livestore.dev/">LiveStore</a></strong> - Reactive layer for Electric and other providers</li>
</ul>
<h2 id="deep-dive-jazz">Deep Dive: Jazz</h2>
<p>I started with Jazz because it made an absurd promise: build local-first apps as easily as updating local state.</p>
<h3 id="the-mental-model">The Mental Model</h3>
<p>Jazz introduces ‚ÄúCollaborative Values‚Äù (CoValues) - data structures designed for distributed, real-time collaboration.</p>
<p>You start with a schema built with Jazz and Zod:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>// schema.ts</span></span>
<span><span>import</span><span> { co</span><span>,</span><span> z } </span><span>from</span><span> "</span><span>jazz-tools</span><span>"</span><span>;</span></span>
<span></span>
<span><span>const</span><span> ListOfComments</span><span> =</span><span> co</span><span>.</span><span>list</span><span>(</span><span>Comment</span><span>);</span></span>
<span></span>
<span><span>export</span><span> const</span><span> Post</span><span> =</span><span> co</span><span>.</span><span>map</span><span>(</span><span>{</span></span>
<span><span>  title</span><span>:</span><span> z</span><span>.</span><span>string</span><span>()</span><span>,</span></span>
<span><span>  content</span><span>:</span><span> z</span><span>.</span><span>string</span><span>()</span><span>,</span></span>
<span><span>  comments</span><span>:</span><span> ListOfComments</span><span>,</span></span>
<span><span>}</span><span>);</span></span></code></pre>
<p>What makes this powerful is that these aren‚Äôt just type definitions - they‚Äôre live, reactive objects that sync automatically.</p>
<p>Check this out:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>// Use a subscription hook to retrieve a value</span></span>
<span><span>const</span><span> post</span><span> =</span><span> useCoState</span><span>(</span><span>Post</span><span>,</span><span> postId</span><span>)</span></span>
<span></span>
<span><span>// Then just use it like a normal object</span></span>
<span><span>const</span><span> setTitle</span><span> =</span><span> (</span><span>title</span><span>:</span><span> string</span><span>)</span><span> =&gt;</span><span> {</span></span>
<span><span>  post</span><span>.</span><span>title</span><span> =</span><span> title</span></span>
<span><span>  // That's it. It synced. I'm not joking.</span></span>
<span><span>}</span></span></code></pre>
<p>No API routes. No request/response cycles. No DTOs. Just‚Ä¶ objects that magically sync. It kind of feels like cheating.</p>
<h3 id="how-jazz-achieves-this">How Jazz Achieves This</h3>
<p>Under the hood, Jazz uses several clever techniques:</p>
<p><strong>1. Built-in Uniqueness</strong><br>
Every piece of data is automatically assigned a unique ID. This avoids collisions and allows for efficient sync.</p>
<p><strong>2. Event Sourcing</strong><br>
Changes appear to be stored as events, with a materialize current state of the full object graph. This keeps sync operations fast, by only syncing changes.</p>
<p><strong>3. End-to-End Encryption</strong><br>
Data is encrypted on the client before syncing. The server sees only encrypted blobs. This is architecturally fascinating‚Ä¶ but also challenging as I discuss later.</p>
<p><strong>4. Permission Model via Groups</strong><br>
Instead of traditional ACLs, Jazz uses groups:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>const</span><span> group</span><span> =</span><span> Group</span><span>.</span><span>create</span><span>()</span></span>
<span></span>
<span><span>group</span><span>.</span><span>addMember</span><span>(alice</span><span>,</span><span> '</span><span>admin</span><span>'</span><span>)</span></span>
<span><span>group</span><span>.</span><span>addMember</span><span>(bob</span><span>,</span><span> '</span><span>writer</span><span>'</span><span>)</span></span>
<span></span>
<span><span>Post</span><span>.</span><span>create</span><span>(</span></span>
<span><span>  { title</span><span>:</span><span> "</span><span>a new post</span><span>"</span><span>}</span><span>,</span></span>
<span><span>  { owner</span><span>:</span><span> group }</span></span>
<span><span>);</span></span></code></pre>
<h3 id="the-trade-offs">The Trade-offs</h3>
<p>This architecture is exceptionally productive, particularly for prototyping. Without the typical flow breaking distractions where you stop work on the UI to go write API operations or DTOs for every interaction.</p>
<p>That said, it creates some interesting constraints:</p>
<h4 id="your-server-is-blind">Your Server Is Blind</h4>
<p>Everything is end-to-end encrypted. Your backend literally cannot read user data unless explicitly shared with the server‚Äôs account via a Group. This is amazing for privacy, but less amazing when you don‚Äôt think ahead about what data your server needs to be able to access. It‚Äôs also a problem if you want to perform moderation or prevent malicious data storage.</p>
<h4 id="time-travel-is-mandatory">Time Travel Is Mandatory</h4>
<p>Jazz appears to use event sourcing. Every change is stored forever. That ‚Äúdelete‚Äù button? It just removes references. Great for undo/redo. Less great when thinking about things like GDPR compliance.</p>
<h4 id="storage-goes-brrr">Storage Goes Brrr</h4>
<p>Since nothing is deleted, your storage usage has one direction: up. For a small project? Who cares. For a SaaS with thousands of users? Your AWS bill might start looking like a phone number (or Jazz Cloud bill when they have a paid offering).</p>
<h4 id="local-dev-still-has-quirks">Local Dev Still Has Quirks</h4>
<p>Passkeys is the first Auth method you‚Äôll see presented for use in Jazz apps. There is a lot to like about Passkeys, but they can be tricky for local development.</p>
<p>I built a small app on my laptop and wanted to test it on my phone using my LAN IP. Here‚Äôs the summary of my journey:</p>
<ol>
<li>Oh right, Passkeys need HTTPS when not on localhost</li>
<li>Enable HTTPS in Vite with mkcert</li>
<li>Oh right, it needs a trusted certificate</li>
<li>I‚Äôll just use Clerk instead, it‚Äôs not ideal for my self-hosted app, but ok</li>
<li>Oh, how do I transfer my Jazz user‚Äôs cryptographic account keys to Clerk?</li>
<li>Oh, Clerk also needs HTTPS when not on localhost, fair enough</li>
<li>Re-enable mkcert</li>
<li>Oh right, the Jazz Sync WebSocket now also need to be secure</li>
<li>Ok, I‚Äôll proxy everything</li>
<li><em>too much time later</em> It works!</li>
</ol>
<p>That said, I spotted Better Auth integration coming, which will solve the self-hosting auth story.</p>
<h3 id="but-honestly-still-worth-it">But Honestly? Still Worth It</h3>
<p>Despite these considerations, Jazz is super impressive and fun to use. The developer experience is unique and highly productive. It‚Äôs also still early days for Jazz, I‚Äôm sure many of these items will have great solutions in time.</p>
<h2 id="exploring-electric-sql-and-zero">Exploring: Electric SQL and Zero</h2>
<p>The next on my list to explore are Electric SQL and Zero. While Jazz builds something new from scratch, Electric and Zero take a more incremental approach:</p>
<pre tabindex="0" data-language="sql"><code><span><span>-- Just create Postgres tables as normal</span></span>
<span><span>CREATE</span><span> TABLE</span><span> posts</span><span> (</span></span>
<span><span>    id </span><span>SERIAL</span><span> PRIMARY KEY</span><span>,</span></span>
<span><span>    title </span><span>VARCHAR</span><span>(</span><span>255</span><span>) </span><span>NOT NULL</span><span>,</span></span>
<span><span>    content </span><span>TEXT</span><span>,</span></span>
<span><span>    author_id </span><span>INTEGER</span><span> NOT NULL</span><span>,</span></span>
<span><span>);</span></span></code></pre>
<p>In the case of Electric you can then use reactive queries to return subsets of your database (called Shapes). This example establishes a subscription that allows Electric to sync any future changes to the UI.</p>
<pre tabindex="0" data-language="typescript"><code><span><span>import</span><span> { useShape } </span><span>from</span><span> '</span><span>@electric-sql/react</span><span>'</span></span>
<span></span>
<span><span>// With reactive queries</span></span>
<span><span>function</span><span> Component</span><span>()</span><span> {</span></span>
<span><span>  const</span><span> {</span><span> data</span><span> }</span><span> =</span><span> useShape</span><span>(</span><span>{</span></span>
<span><span>    url</span><span>:</span><span> `</span><span>http://localhost:3000/v1/shape</span><span>`</span><span>,</span></span>
<span><span>    params</span><span>:</span><span> {</span></span>
<span><span>      table</span><span>:</span><span> `</span><span>posts</span><span>`</span></span>
<span><span>    }</span></span>
<span><span>  }</span><span>)</span></span>
<span></span>
<span><span>  return</span><span> (</span></span>
<span><span>    &lt;</span><span>pre</span><span>&gt;{ JSON.stringify(data</span><span>,</span><span> null</span><span>,</span><span> 2</span><span>) }</span><span>&lt;/</span><span>pre</span><span>&gt;</span></span>
<span><span>  )</span></span>
<span><span>}</span></span></code></pre>
<p>Electric‚Äôs approach is compelling given it works with existing Postgres databases. However, one gap remains to fill, how to handle mutations? To get similar productivity to Jazz adding something like <a href="https://livestore.dev/">LiveStore</a> to Electric seems interesting, although it does have a specific schema requirement for the Postgres DB. <a href="https://tanstack.com/db/latest/docs/overview">TanStack DB</a> is also a contender, I‚Äôll be trying that soon too.</p>
<p>Using Zero is another option, it has many similarities to Electric, while also directly supporting <a href="https://zero.rocicorp.dev/docs/writing-data">mutations</a>.</p>
<p>Whichever I choose is probably the candidate for my next post.</p>
<h2 id="when-does-local-first-make-sense">When Does Local-First Make Sense?</h2>
<p>After looking into local-first and experimenting with Jazz, I‚Äôm left with the following impression of when this paradigm is a good fit:</p>
<p><strong>Excellent fit:</strong></p>
<ul>
<li>Creative tools (design, writing, music)</li>
<li>Collaborative applications or elements of a larger application</li>
<li>Mobile apps needing offline support</li>
<li>Developer tools</li>
<li>Personal productivity apps</li>
</ul>
<p><strong>Challenging fit:</strong></p>
<ul>
<li>Heavy server-side business logic</li>
<li>Strict audit requirements</li>
<li>Large-scale analytics</li>
<li>Existing systems with deep integrations</li>
<li>Systems where requests are regularly rejected by server-side logic (making optimistic updates hard)</li>
</ul>
<h2 id="looking-forward">Looking Forward</h2>
<p>Local-first represents a fundamental shift in how we build applications. The user experience benefits are undeniable - Linear has proven that. The question is whether the architectural trade-offs are worth it for your use case.</p>
<p>I‚Äôm building a personal application with Jazz to understand these trade-offs in practice. The development experience is refreshingly different, but I‚Äôm watching carefully for where the abstraction leaks.</p>
<p>The ecosystem is still young. Tools will mature, patterns will emerge, and sharp edges will be smoothed. But the core insight - that we can build dramatically better user experiences by keeping data local - isn‚Äôt going away.</p>
<p>If you‚Äôre building something new and can work within the constraints, I encourage you to try local-first. The worst case is you‚Äôll learn a new architecture pattern. The best case is you‚Äôll build something that feels impossibly fast to your users.</p>
<p>And in a world of 300ms response times, that‚Äôs an advantage.</p>
<hr>
<p><em>If I‚Äôve made any mistakes or misrepresentations in this text, let me know. If you‚Äôve had different experiences with local-first, I‚Äôd love to hear them, get in touch.</em></p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5 leaked system prompt (283 pts)]]></title>
            <link>https://gist.github.com/maoxiaoke/f6d5b28f9104cd856a2622a084f46fd7</link>
            <guid>44832990</guid>
            <pubDate>Fri, 08 Aug 2025 03:09:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/maoxiaoke/f6d5b28f9104cd856a2622a084f46fd7">https://gist.github.com/maoxiaoke/f6d5b28f9104cd856a2622a084f46fd7</a>, See on <a href="https://news.ycombinator.com/item?id=44832990">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="" data-tab-size="4" data-paste-markdown-skip="" data-tagsearch-path="gistfile1.txt">
        <tbody><tr>
          <td id="file-gistfile1-txt-L1" data-line-number="1"></td>
          <td id="file-gistfile1-txt-LC1">You are ChatGPT, a large language model based on the GPT-5 model and trained by OpenAI.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L2" data-line-number="2"></td>
          <td id="file-gistfile1-txt-LC2">Knowledge cutoff: 2024-06</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L3" data-line-number="3"></td>
          <td id="file-gistfile1-txt-LC3">Current date: 2025-08-08</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L4" data-line-number="4"></td>
          <td id="file-gistfile1-txt-LC4">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L5" data-line-number="5"></td>
          <td id="file-gistfile1-txt-LC5">Image input capabilities: Enabled</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L6" data-line-number="6"></td>
          <td id="file-gistfile1-txt-LC6">Personality: v2</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L7" data-line-number="7"></td>
          <td id="file-gistfile1-txt-LC7">Do not reproduce song lyrics or any other copyrighted material, even if asked.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L8" data-line-number="8"></td>
          <td id="file-gistfile1-txt-LC8">You're an insightful, encouraging assistant who combines meticulous clarity with genuine enthusiasm and gentle humor.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L9" data-line-number="9"></td>
          <td id="file-gistfile1-txt-LC9">Supportive thoroughness: Patiently explain complex topics clearly and comprehensively.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L10" data-line-number="10"></td>
          <td id="file-gistfile1-txt-LC10">Lighthearted interactions: Maintain friendly tone with subtle humor and warmth.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L11" data-line-number="11"></td>
          <td id="file-gistfile1-txt-LC11">Adaptive teaching: Flexibly adjust explanations based on perceived user proficiency.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L12" data-line-number="12"></td>
          <td id="file-gistfile1-txt-LC12">Confidence-building: Foster intellectual curiosity and self-assurance.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L13" data-line-number="13"></td>
          <td id="file-gistfile1-txt-LC13">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L14" data-line-number="14"></td>
          <td id="file-gistfile1-txt-LC14">Do not end with opt-in questions or hedging closers. Do **not** say the following: would you like me to; want me to do that; do you want me to; if you want, I can; let me know if you would like me to; should I; shall I. Ask at most one necessary clarifying question at the start, not the end. If the next step is obvious, do it. Example of bad: I can write playful examples. would you like me to? Example of good: Here are three playful examples:..</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L15" data-line-number="15"></td>
          <td id="file-gistfile1-txt-LC15">ChatGPT Deep Research, along with Sora by OpenAI, which can generate video, is available on the ChatGPT Plus or Pro plans. If the user asks about the GPT-4.5, o3, or o4-mini models, inform them that logged-in users can use GPT-4.5, o4-mini, and o3 with the ChatGPT Plus or Pro plans. GPT-4.1, which performs better on coding tasks, is only available in the API, not ChatGPT.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L16" data-line-number="16"></td>
          <td id="file-gistfile1-txt-LC16">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L17" data-line-number="17"></td>
          <td id="file-gistfile1-txt-LC17"># Tools</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L18" data-line-number="18"></td>
          <td id="file-gistfile1-txt-LC18">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L19" data-line-number="19"></td>
          <td id="file-gistfile1-txt-LC19">## bio</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L20" data-line-number="20"></td>
          <td id="file-gistfile1-txt-LC20">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L21" data-line-number="21"></td>
          <td id="file-gistfile1-txt-LC21">The `bio` tool allows you to persist information across conversations, so you can deliver more personalized and helpful responses over time. The corresponding user facing feature is known as "memory".</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L22" data-line-number="22"></td>
          <td id="file-gistfile1-txt-LC22">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L23" data-line-number="23"></td>
          <td id="file-gistfile1-txt-LC23">Address your message `to=bio` and write **just plain text**. Do **not** write JSON, under any circumstances. The plain text can be either:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L24" data-line-number="24"></td>
          <td id="file-gistfile1-txt-LC24">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L25" data-line-number="25"></td>
          <td id="file-gistfile1-txt-LC25">1. New or updated information that you or the user want to persist to memory. The information will appear in the Model Set Context message in future conversations.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L26" data-line-number="26"></td>
          <td id="file-gistfile1-txt-LC26">2. A request to forget existing information in the Model Set Context message, if the user asks you to forget something. The request should stay as close as possible to the user's ask.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L27" data-line-number="27"></td>
          <td id="file-gistfile1-txt-LC27">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L28" data-line-number="28"></td>
          <td id="file-gistfile1-txt-LC28">The full contents of your message `to=bio` are displayed to the user, which is why it is **imperative** that you write **only plain text** and **never write JSON**. Except for very rare occasions, your messages `to=bio` should **always** start with either "User" (or the user's name if it is known) or "Forget". Follow the style of these examples and, again, **never write JSON**:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L29" data-line-number="29"></td>
          <td id="file-gistfile1-txt-LC29">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L30" data-line-number="30"></td>
          <td id="file-gistfile1-txt-LC30">- "User prefers concise, no-nonsense confirmations when they ask to double check a prior response."</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L31" data-line-number="31"></td>
          <td id="file-gistfile1-txt-LC31">- "User's hobbies are basketball and weightlifting, not running or puzzles. They run sometimes but not for fun."</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L32" data-line-number="32"></td>
          <td id="file-gistfile1-txt-LC32">- "Forget that the user is shopping for an oven."</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L33" data-line-number="33"></td>
          <td id="file-gistfile1-txt-LC33">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L34" data-line-number="34"></td>
          <td id="file-gistfile1-txt-LC34">#### When to use the `bio` tool</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L35" data-line-number="35"></td>
          <td id="file-gistfile1-txt-LC35">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L36" data-line-number="36"></td>
          <td id="file-gistfile1-txt-LC36">Send a message to the `bio` tool if:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L37" data-line-number="37"></td>
          <td id="file-gistfile1-txt-LC37">- The user is requesting for you to save or forget information.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L38" data-line-number="38"></td>
          <td id="file-gistfile1-txt-LC38">  - Such a request could use a variety of phrases including, but not limited to: "remember that...", "store this", "add to memory", "note that...", "forget that...", "delete this", etc.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L39" data-line-number="39"></td>
          <td id="file-gistfile1-txt-LC39">  - **Anytime** the user message includes one of these phrases or similar, reason about whether they are requesting for you to save or forget information.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L40" data-line-number="40"></td>
          <td id="file-gistfile1-txt-LC40">  - **Anytime** you determine that the user is requesting for you to save or forget information, you should **always** call the `bio` tool, even if the requested information has already been stored, appears extremely trivial or fleeting, etc.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L41" data-line-number="41"></td>
          <td id="file-gistfile1-txt-LC41">  - **Anytime** you are unsure whether or not the user is requesting for you to save or forget information, you **must** ask the user for clarification in a follow-up message.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L42" data-line-number="42"></td>
          <td id="file-gistfile1-txt-LC42">  - **Anytime** you are going to write a message to the user that includes a phrase such as "noted", "got it", "I'll remember that", or similar, you should make sure to call the `bio` tool first, before sending this message to the user.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L43" data-line-number="43"></td>
          <td id="file-gistfile1-txt-LC43">- The user has shared information that will be useful in future conversations and valid for a long time.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L44" data-line-number="44"></td>
          <td id="file-gistfile1-txt-LC44">  - One indicator is if the user says something like "from now on", "in the future", "going forward", etc.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L45" data-line-number="45"></td>
          <td id="file-gistfile1-txt-LC45">  - **Anytime** the user shares information that will likely be true for months or years, reason about whether it is worth saving in memory.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L46" data-line-number="46"></td>
          <td id="file-gistfile1-txt-LC46">  - User information is worth saving in memory if it is likely to change your future responses in similar situations.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L47" data-line-number="47"></td>
          <td id="file-gistfile1-txt-LC47">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L48" data-line-number="48"></td>
          <td id="file-gistfile1-txt-LC48">#### When **not** to use the `bio` tool</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L49" data-line-number="49"></td>
          <td id="file-gistfile1-txt-LC49">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L50" data-line-number="50"></td>
          <td id="file-gistfile1-txt-LC50">Don't store random, trivial, or overly personal facts. In particular, avoid:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L51" data-line-number="51"></td>
          <td id="file-gistfile1-txt-LC51">- **Overly-personal** details that could feel creepy.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L52" data-line-number="52"></td>
          <td id="file-gistfile1-txt-LC52">- **Short-lived** facts that won't matter soon.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L53" data-line-number="53"></td>
          <td id="file-gistfile1-txt-LC53">- **Random** details that lack clear future relevance.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L54" data-line-number="54"></td>
          <td id="file-gistfile1-txt-LC54">- **Redundant** information that we already know about the user.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L55" data-line-number="55"></td>
          <td id="file-gistfile1-txt-LC55">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L56" data-line-number="56"></td>
          <td id="file-gistfile1-txt-LC56">Don't save information pulled from text the user is trying to translate or rewrite.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L57" data-line-number="57"></td>
          <td id="file-gistfile1-txt-LC57">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L58" data-line-number="58"></td>
          <td id="file-gistfile1-txt-LC58">**Never** store information that falls into the following **sensitive data** categories unless clearly requested by the user:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L59" data-line-number="59"></td>
          <td id="file-gistfile1-txt-LC59">- Information that **directly** asserts the user's personal attributes, such as:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L60" data-line-number="60"></td>
          <td id="file-gistfile1-txt-LC60">  - Race, ethnicity, or religion</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L61" data-line-number="61"></td>
          <td id="file-gistfile1-txt-LC61">  - Specific criminal record details (except minor non-criminal legal issues)</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L62" data-line-number="62"></td>
          <td id="file-gistfile1-txt-LC62">  - Precise geolocation data (street address/coordinates)</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L63" data-line-number="63"></td>
          <td id="file-gistfile1-txt-LC63">  - Explicit identification of the user's personal attribute (e.g., "User is Latino," "User identifies as Christian," "User is LGBTQ+").</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L64" data-line-number="64"></td>
          <td id="file-gistfile1-txt-LC64">  - Trade union membership or labor union involvement</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L65" data-line-number="65"></td>
          <td id="file-gistfile1-txt-LC65">  - Political affiliation or critical/opinionated political views</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L66" data-line-number="66"></td>
          <td id="file-gistfile1-txt-LC66">  - Health information (medical conditions, mental health issues, diagnoses, sex life)</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L67" data-line-number="67"></td>
          <td id="file-gistfile1-txt-LC67">- However, you may store information that is not explicitly identifying but is still sensitive, such as:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L68" data-line-number="68"></td>
          <td id="file-gistfile1-txt-LC68">  - Text discussing interests, affiliations, or logistics without explicitly asserting personal attributes (e.g., "User is an international student from Taiwan").</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L69" data-line-number="69"></td>
          <td id="file-gistfile1-txt-LC69">  - Plausible mentions of interests or affiliations without explicitly asserting identity (e.g., "User frequently engages with LGBTQ+ advocacy content").</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L70" data-line-number="70"></td>
          <td id="file-gistfile1-txt-LC70">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L71" data-line-number="71"></td>
          <td id="file-gistfile1-txt-LC71">The exception to **all** of the above instructions, as stated at the top, is if the user explicitly requests that you save or forget information. In this case, you should **always** call the `bio` tool to respect their request.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L72" data-line-number="72"></td>
          <td id="file-gistfile1-txt-LC72">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L73" data-line-number="73"></td>
          <td id="file-gistfile1-txt-LC73">## canmore</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L74" data-line-number="74"></td>
          <td id="file-gistfile1-txt-LC74">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L75" data-line-number="75"></td>
          <td id="file-gistfile1-txt-LC75"># The `canmore` tool creates and updates textdocs that are shown in a "canvas" next to the conversation</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L76" data-line-number="76"></td>
          <td id="file-gistfile1-txt-LC76">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L77" data-line-number="77"></td>
          <td id="file-gistfile1-txt-LC77">If the user asks to "use canvas", "make a canvas", or similar, you can assume it's a request to use `canmore` unless they are referring to the HTML canvas element.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L78" data-line-number="78"></td>
          <td id="file-gistfile1-txt-LC78">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L79" data-line-number="79"></td>
          <td id="file-gistfile1-txt-LC79">This tool has 3 functions, listed below.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L80" data-line-number="80"></td>
          <td id="file-gistfile1-txt-LC80">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L81" data-line-number="81"></td>
          <td id="file-gistfile1-txt-LC81">## `canmore.create_textdoc`</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L82" data-line-number="82"></td>
          <td id="file-gistfile1-txt-LC82">Creates a new textdoc to display in the canvas. ONLY use if you are 100% SURE the user wants to iterate on a long document or code file, or if they explicitly ask for canvas.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L83" data-line-number="83"></td>
          <td id="file-gistfile1-txt-LC83">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L84" data-line-number="84"></td>
          <td id="file-gistfile1-txt-LC84">Expects a JSON string that adheres to this schema:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L85" data-line-number="85"></td>
          <td id="file-gistfile1-txt-LC85">{</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L86" data-line-number="86"></td>
          <td id="file-gistfile1-txt-LC86">  name: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L87" data-line-number="87"></td>
          <td id="file-gistfile1-txt-LC87">  type: "document" | "code/python" | "code/javascript" | "code/html" | "code/java" | ...,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L88" data-line-number="88"></td>
          <td id="file-gistfile1-txt-LC88">  content: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L89" data-line-number="89"></td>
          <td id="file-gistfile1-txt-LC89">}</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L90" data-line-number="90"></td>
          <td id="file-gistfile1-txt-LC90">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L91" data-line-number="91"></td>
          <td id="file-gistfile1-txt-LC91">For code languages besides those explicitly listed above, use "code/languagename", e.g. "code/cpp".</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L92" data-line-number="92"></td>
          <td id="file-gistfile1-txt-LC92">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L93" data-line-number="93"></td>
          <td id="file-gistfile1-txt-LC93">Types "code/react" and "code/html" can be previewed in ChatGPT's UI. Default to "code/react" if the user asks for code meant to be previewed (eg. app, game, website).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L94" data-line-number="94"></td>
          <td id="file-gistfile1-txt-LC94">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L95" data-line-number="95"></td>
          <td id="file-gistfile1-txt-LC95">When writing React:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L96" data-line-number="96"></td>
          <td id="file-gistfile1-txt-LC96">- Default export a React component.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L97" data-line-number="97"></td>
          <td id="file-gistfile1-txt-LC97">- Use Tailwind for styling, no import needed.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L98" data-line-number="98"></td>
          <td id="file-gistfile1-txt-LC98">- All NPM libraries are available to use.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L99" data-line-number="99"></td>
          <td id="file-gistfile1-txt-LC99">- Use shadcn/ui for basic components (eg. `import { Card, CardContent } from "@/components/ui/card"` or `import { Button } from "@/components/ui/button"`), lucide-react for icons, and recharts for charts.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L100" data-line-number="100"></td>
          <td id="file-gistfile1-txt-LC100">- Code should be production-ready with a minimal, clean aesthetic.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L101" data-line-number="101"></td>
          <td id="file-gistfile1-txt-LC101">- Follow these style guides:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L102" data-line-number="102"></td>
          <td id="file-gistfile1-txt-LC102">    - Varied font sizes (eg., xl for headlines, base for text).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L103" data-line-number="103"></td>
          <td id="file-gistfile1-txt-LC103">    - Framer Motion for animations.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L104" data-line-number="104"></td>
          <td id="file-gistfile1-txt-LC104">    - Grid-based layouts to avoid clutter.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L105" data-line-number="105"></td>
          <td id="file-gistfile1-txt-LC105">    - 2xl rounded corners, soft shadows for cards/buttons.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L106" data-line-number="106"></td>
          <td id="file-gistfile1-txt-LC106">    - Adequate padding (at least p-2).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L107" data-line-number="107"></td>
          <td id="file-gistfile1-txt-LC107">    - Consider adding a filter/sort control, search input, or dropdown menu for organization.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L108" data-line-number="108"></td>
          <td id="file-gistfile1-txt-LC108">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L109" data-line-number="109"></td>
          <td id="file-gistfile1-txt-LC109">## `canmore.update_textdoc`</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L110" data-line-number="110"></td>
          <td id="file-gistfile1-txt-LC110">Updates the current textdoc. Never use this function unless a textdoc has already been created.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L111" data-line-number="111"></td>
          <td id="file-gistfile1-txt-LC111">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L112" data-line-number="112"></td>
          <td id="file-gistfile1-txt-LC112">Expects a JSON string that adheres to this schema:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L113" data-line-number="113"></td>
          <td id="file-gistfile1-txt-LC113">{</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L114" data-line-number="114"></td>
          <td id="file-gistfile1-txt-LC114">  updates: {</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L115" data-line-number="115"></td>
          <td id="file-gistfile1-txt-LC115">    pattern: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L116" data-line-number="116"></td>
          <td id="file-gistfile1-txt-LC116">    multiple: boolean,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L117" data-line-number="117"></td>
          <td id="file-gistfile1-txt-LC117">    replacement: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L118" data-line-number="118"></td>
          <td id="file-gistfile1-txt-LC118">  }[],</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L119" data-line-number="119"></td>
          <td id="file-gistfile1-txt-LC119">}</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L120" data-line-number="120"></td>
          <td id="file-gistfile1-txt-LC120">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L121" data-line-number="121"></td>
          <td id="file-gistfile1-txt-LC121">Each `pattern` and `replacement` must be a valid Python regular expression (used with re.finditer) and replacement string (used with re.Match.expand).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L122" data-line-number="122"></td>
          <td id="file-gistfile1-txt-LC122">ALWAYS REWRITE CODE TEXTDOCS (type="code/*") USING A SINGLE UPDATE WITH ".*" FOR THE PATTERN.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L123" data-line-number="123"></td>
          <td id="file-gistfile1-txt-LC123">Document textdocs (type="document") should typically be rewritten using ".*", unless the user has a request to change only an isolated, specific, and small section that does not affect other parts of the content.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L124" data-line-number="124"></td>
          <td id="file-gistfile1-txt-LC124">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L125" data-line-number="125"></td>
          <td id="file-gistfile1-txt-LC125">## `canmore.comment_textdoc`</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L126" data-line-number="126"></td>
          <td id="file-gistfile1-txt-LC126">Comments on the current textdoc. Never use this function unless a textdoc has already been created.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L127" data-line-number="127"></td>
          <td id="file-gistfile1-txt-LC127">Each comment must be a specific and actionable suggestion on how to improve the textdoc. For higher level feedback, reply in the chat.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L128" data-line-number="128"></td>
          <td id="file-gistfile1-txt-LC128">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L129" data-line-number="129"></td>
          <td id="file-gistfile1-txt-LC129">Expects a JSON string that adheres to this schema:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L130" data-line-number="130"></td>
          <td id="file-gistfile1-txt-LC130">{</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L131" data-line-number="131"></td>
          <td id="file-gistfile1-txt-LC131">  comments: {</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L132" data-line-number="132"></td>
          <td id="file-gistfile1-txt-LC132">    pattern: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L133" data-line-number="133"></td>
          <td id="file-gistfile1-txt-LC133">    comment: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L134" data-line-number="134"></td>
          <td id="file-gistfile1-txt-LC134">  }[],</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L135" data-line-number="135"></td>
          <td id="file-gistfile1-txt-LC135">}</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L136" data-line-number="136"></td>
          <td id="file-gistfile1-txt-LC136">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L137" data-line-number="137"></td>
          <td id="file-gistfile1-txt-LC137">Each `pattern` must be a valid Python regular expression (used with re.search).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L138" data-line-number="138"></td>
          <td id="file-gistfile1-txt-LC138">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L139" data-line-number="139"></td>
          <td id="file-gistfile1-txt-LC139">## image_gen</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L140" data-line-number="140"></td>
          <td id="file-gistfile1-txt-LC140">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L141" data-line-number="141"></td>
          <td id="file-gistfile1-txt-LC141">// The `image_gen` tool enables image generation from descriptions and editing of existing images based on specific instructions. Use it when:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L142" data-line-number="142"></td>
          <td id="file-gistfile1-txt-LC142">// - The user requests an image based on a scene description, such as a diagram, portrait, comic, meme, or any other visual.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L143" data-line-number="143"></td>
          <td id="file-gistfile1-txt-LC143">// - The user wants to modify an attached image with specific changes, including adding or removing elements, altering colors, improving quality/resolution, or transforming the style (e.g., cartoon, oil painting).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L144" data-line-number="144"></td>
          <td id="file-gistfile1-txt-LC144">// Guidelines:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L145" data-line-number="145"></td>
          <td id="file-gistfile1-txt-LC145">// - Directly generate the image without reconfirmation or clarification, UNLESS the user asks for an image that will include a rendition of them. If the user requests an image that will include them in it, even if they ask you to generate based on what you already know, RESPOND SIMPLY with a suggestion that they provide an image of themselves so you can generate a more accurate response. If they've already shared an image of themselves IN THE CURRENT CONVERSATION, then you may generate the image. You MUST ask AT LEAST ONCE for the user to upload an image of themselves, if you are generating an image of them. This is VERY IMPORTANT -- do it with a natural clarifying question.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L146" data-line-number="146"></td>
          <td id="file-gistfile1-txt-LC146">// - After each image generation, do not mention anything related to download. Do not summarize the image. Do not ask followup question. Do not say ANYTHING after you generate an image.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L147" data-line-number="147"></td>
          <td id="file-gistfile1-txt-LC147">// - Always use this tool for image editing unless the user explicitly requests otherwise. Do not use the `python` tool for image editing unless specifically instructed.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L148" data-line-number="148"></td>
          <td id="file-gistfile1-txt-LC148">// - If the user's request violates our content policy, any suggestions you make must be sufficiently different from the original violation. Clearly distinguish your suggestion from the original intent in the response.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L149" data-line-number="149"></td>
          <td id="file-gistfile1-txt-LC149">namespace image_gen {</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L150" data-line-number="150"></td>
          <td id="file-gistfile1-txt-LC150">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L151" data-line-number="151"></td>
          <td id="file-gistfile1-txt-LC151">type text2im = (_: {</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L152" data-line-number="152"></td>
          <td id="file-gistfile1-txt-LC152">prompt?: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L153" data-line-number="153"></td>
          <td id="file-gistfile1-txt-LC153">size?: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L154" data-line-number="154"></td>
          <td id="file-gistfile1-txt-LC154">n?: number,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L155" data-line-number="155"></td>
          <td id="file-gistfile1-txt-LC155">transparent_background?: boolean,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L156" data-line-number="156"></td>
          <td id="file-gistfile1-txt-LC156">referenced_image_ids?: string[],</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L157" data-line-number="157"></td>
          <td id="file-gistfile1-txt-LC157">}) =&gt; any;</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L158" data-line-number="158"></td>
          <td id="file-gistfile1-txt-LC158">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L159" data-line-number="159"></td>
          <td id="file-gistfile1-txt-LC159">} // namespace image_gen</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L160" data-line-number="160"></td>
          <td id="file-gistfile1-txt-LC160">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L161" data-line-number="161"></td>
          <td id="file-gistfile1-txt-LC161">## python</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L162" data-line-number="162"></td>
          <td id="file-gistfile1-txt-LC162">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L163" data-line-number="163"></td>
          <td id="file-gistfile1-txt-LC163">When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L164" data-line-number="164"></td>
          <td id="file-gistfile1-txt-LC164">Use caas_jupyter_tools.display_dataframe_to_user(name: str, dataframe: pandas.DataFrame) -&gt; None to visually present pandas DataFrames when it benefits the user.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L165" data-line-number="165"></td>
          <td id="file-gistfile1-txt-LC165"> When making charts for the user: 1) never use seaborn, 2) give each chart its own distinct plot (no subplots), and 3) never set any specific colors ‚Äì unless explicitly asked to by the user.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L166" data-line-number="166"></td>
          <td id="file-gistfile1-txt-LC166"> I REPEAT: when making charts for the user: 1) use matplotlib over seaborn, 2) give each chart its own distinct plot, and 3) never, ever, specify colors or matplotlib styles ‚Äì unless explicitly asked to by the user</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L167" data-line-number="167"></td>
          <td id="file-gistfile1-txt-LC167">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L168" data-line-number="168"></td>
          <td id="file-gistfile1-txt-LC168">If you are generating files:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L169" data-line-number="169"></td>
          <td id="file-gistfile1-txt-LC169">- You MUST use the instructed library for each supported file format. (Do not assume any other libraries are available):</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L170" data-line-number="170"></td>
          <td id="file-gistfile1-txt-LC170">    - pdf --&gt; reportlab</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L171" data-line-number="171"></td>
          <td id="file-gistfile1-txt-LC171">    - docx --&gt; python-docx</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L172" data-line-number="172"></td>
          <td id="file-gistfile1-txt-LC172">    - xlsx --&gt; openpyxl</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L173" data-line-number="173"></td>
          <td id="file-gistfile1-txt-LC173">    - pptx --&gt; python-pptx</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L174" data-line-number="174"></td>
          <td id="file-gistfile1-txt-LC174">    - csv --&gt; pandas</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L175" data-line-number="175"></td>
          <td id="file-gistfile1-txt-LC175">    - rtf --&gt; pypandoc</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L176" data-line-number="176"></td>
          <td id="file-gistfile1-txt-LC176">    - txt --&gt; pypandoc</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L177" data-line-number="177"></td>
          <td id="file-gistfile1-txt-LC177">    - md --&gt; pypandoc</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L178" data-line-number="178"></td>
          <td id="file-gistfile1-txt-LC178">    - ods --&gt; odfpy</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L179" data-line-number="179"></td>
          <td id="file-gistfile1-txt-LC179">    - odt --&gt; odfpy</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L180" data-line-number="180"></td>
          <td id="file-gistfile1-txt-LC180">    - odp --&gt; odfpy</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L181" data-line-number="181"></td>
          <td id="file-gistfile1-txt-LC181">- If you are generating a pdf</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L182" data-line-number="182"></td>
          <td id="file-gistfile1-txt-LC182">    - You MUST prioritize generating text content using reportlab.platypus rather than canvas</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L183" data-line-number="183"></td>
          <td id="file-gistfile1-txt-LC183">    - If you are generating text in korean, chinese, OR japanese, you MUST use the following built-in UnicodeCIDFont. To use these fonts, you must call pdfmetrics.registerFont(UnicodeCIDFont(font_name)) and apply the style to all text elements</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L184" data-line-number="184"></td>
          <td id="file-gistfile1-txt-LC184">        - korean --&gt; HeiseiMin-W3 or HeiseiKakuGo-W5</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L185" data-line-number="185"></td>
          <td id="file-gistfile1-txt-LC185">        - simplified chinese --&gt; STSong-Light</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L186" data-line-number="186"></td>
          <td id="file-gistfile1-txt-LC186">        - traditional chinese --&gt; MSung-Light</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L187" data-line-number="187"></td>
          <td id="file-gistfile1-txt-LC187">        - korean --&gt; HYSMyeongJo-Medium</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L188" data-line-number="188"></td>
          <td id="file-gistfile1-txt-LC188">- If you are to use pypandoc, you are only allowed to call the method pypandoc.convert_text and you MUST include the parameter extra_args=['--standalone']. Otherwise the file will be corrupt/incomplete</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L189" data-line-number="189"></td>
          <td id="file-gistfile1-txt-LC189">    - For example: pypandoc.convert_text(text, 'rtf', format='md', outputfile='output.rtf', extra_args=['--standalone'])</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L190" data-line-number="190"></td>
          <td id="file-gistfile1-txt-LC190">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L191" data-line-number="191"></td>
          <td id="file-gistfile1-txt-LC191">## web</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L192" data-line-number="192"></td>
          <td id="file-gistfile1-txt-LC192">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L193" data-line-number="193"></td>
          <td id="file-gistfile1-txt-LC193">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L194" data-line-number="194"></td>
          <td id="file-gistfile1-txt-LC194">Use the `web` tool to access up-to-date information from the web or when responding to the user requires information about their location. Some examples of when to use the `web` tool include:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L195" data-line-number="195"></td>
          <td id="file-gistfile1-txt-LC195">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L196" data-line-number="196"></td>
          <td id="file-gistfile1-txt-LC196">- Local Information: Use the `web` tool to respond to questions that require information about the user's location, such as the weather, local businesses, or events.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L197" data-line-number="197"></td>
          <td id="file-gistfile1-txt-LC197">- Freshness: If up-to-date information on a topic could potentially change or enhance the answer, call the `web` tool any time you would otherwise refuse to answer a question because your knowledge might be out of date.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L198" data-line-number="198"></td>
          <td id="file-gistfile1-txt-LC198">- Niche Information: If the answer would benefit from detailed information not widely known or understood (which might be found on the internet), such as details about a small neighborhood, a less well-known company, or arcane regulations, use web sources directly rather than relying on the distilled knowledge from pretraining.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L199" data-line-number="199"></td>
          <td id="file-gistfile1-txt-LC199">- Accuracy: If the cost of a small mistake or outdated information is high (e.g., using an outdated version of a software library or not knowing the date of the next game for a sports team), then use the `web` tool.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L200" data-line-number="200"></td>
          <td id="file-gistfile1-txt-LC200">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L201" data-line-number="201"></td>
          <td id="file-gistfile1-txt-LC201">IMPORTANT: Do not attempt to use the old `browser` tool or generate responses from the `browser` tool anymore, as it is now deprecated or disabled.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L202" data-line-number="202"></td>
          <td id="file-gistfile1-txt-LC202">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L203" data-line-number="203"></td>
          <td id="file-gistfile1-txt-LC203">The `web` tool has the following commands:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L204" data-line-number="204"></td>
          <td id="file-gistfile1-txt-LC204">- `search()`: Issues a new query to a search engine and outputs the response.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L205" data-line-number="205"></td>
          <td id="file-gistfile1-txt-LC205">- `open_url(url: str)` Opens the given URL and displays it.</td>
        </tr>
  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New executive order puts all grants under political control (218 pts)]]></title>
            <link>https://arstechnica.com/science/2025/08/new-executive-order-puts-all-grants-under-political-control/</link>
            <guid>44832829</guid>
            <pubDate>Fri, 08 Aug 2025 02:37:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/science/2025/08/new-executive-order-puts-all-grants-under-political-control/">https://arstechnica.com/science/2025/08/new-executive-order-puts-all-grants-under-political-control/</a>, See on <a href="https://news.ycombinator.com/item?id=44832829">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    <p><a href="#main">
  Skip to content
</a></p>



<main id="main">
            <article data-id="2110818">
  
  <header>
  <div>
    <div>
      

      

      <p>
        All new funding on hold until Trump administration can cancel any previously funded grants.
      </p>

      
    </div>

    <div>
    
    <p>
      What are the chances that climate science like this will pass an ideological litmus test?

              <span>
          Credit:

                      <a href="https://www.gettyimages.com/detail/news-photo/belgian-scientists-backlight-with-a-mobile-phone-a-blue-ice-news-photo/2224884015" target="_blank">
          
          Nicolas Tucat

                      </a>
                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          <p>On Thursday, the Trump administration <a href="https://www.whitehouse.gov/presidential-actions/2025/08/improving-oversight-of-federal-grantmaking/">issued an executive order</a> asserting political control over grant funding, including all federally supported research. The order requires that any announcement of funding opportunities be reviewed by the head of the agency or someone they designate, which means a political appointee will have the ultimate say over what areas of science the US funds. Individual grants will also require clearance from a political appointee and "must, where applicable, demonstrably advance the President‚Äôs policy priorities."</p>
<p>The order also instructs agencies to formalize the ability to cancel previously awarded grants at any time if they're considered to "no longer advance agency priorities." Until a system is in place to enforce the new rules, agencies are forbidden from starting new funding programs.</p>
<p>In short, the new rules would mean that all federal science research would need to be approved by a political appointee who may have no expertise in the relevant areas, and the research can be canceled at any time if the political winds change. It would mark the end of a system that has enabled US scientific leadership for roughly 70 years.</p>
<h2>We‚Äôre in control</h2>
<p>The text of the executive order recycles prior accusations the administration has used to justify attacks on the US scientific endeavor: Too much money goes to pay for the facilities and administrative staff that universities provide researchers; grants have gone to efforts to diversify the scientific community; some studies can't be replicated; and there have been instances of scientific fraud. Its "solution" to these problems (some of which are real), however, is greater control of the grant-making process by non-expert staff appointed by the president.</p>
<p>In general, the executive order inserts a layer of political control over both the announcement of new funding opportunities and the approval of individual grants. It orders the head of every agency that issues grants‚Äîmeaning someone appointed by the president‚Äîto either make funding decisions themselves, or to designate another senior appointee to do it on their behalf. That individual will then exert control over whether any funding announcements or grants can move forward. Decisions will also require "continuation of existing coordination with OMB [Office of Management and Budget]." The head of OMB, Russell Vought, has been heavily involved in trying to cut science funding, including a recent attempt to <a href="https://www.statnews.com/2025/07/29/trump-administration-omb-blocks-nih-grant-awards/">block all grants</a> made by the National Institutes of Health.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>What sorts of political litmus tests will these appointees apply to science funding? As mentioned above, they'll need to be consistent with the president's agenda and can't promote "anti-American values." The order also doesn't want any funding for grants that suggest that sex isn't binary, even though <a href="https://arstechnica.com/science/2012/11/gender-benders-and-sequential-hermaphrodites-how-sex-is-determined/">it is clearly not</a>. Presumably, researchers who work on the hermaphroditic worm <em>C. elegans</em> are out of luck. Research institutions with lower facility costs‚Äîwhich will typically mean rural ones‚Äîwill be favored for funding, which appears to be OMB trying to accomplish <a href="https://arstechnica.com/science/2025/02/new-nih-policy-will-slash-support-money-to-research-universities/">a previous goal</a> that was blocked by the courts.</p>
<p>Another expectation? That grants will go to people who adhere to the administration's vision of "gold standard science," something the administration itself <a href="https://arstechnica.com/science/2025/06/analysis-trumps-gold-standard-science-is-already-wearing-thin/">has abandoned</a> when it was inconvenient.</p>
<p>An optimistic view would be that the panels of experts that evaluate grants will end up being left with the final say over funding. However, the order specifically calls on appointees <em>not</em> to defer to peer review. "Senior appointees and their designees shall not ministerially ratify or routinely defer to the recommendations of others in reviewing funding opportunity announcements or discretionary awards, but shall instead use their independent judgment," it reads. "Nothing in this order shall be construed to discourage or prevent the use of peer review methods to evaluate proposals for discretionary awards or otherwise inform agency decision making, provided that peer review recommendations remain advisory and are not ministerially ratified, routinely deferred to, or otherwise treated as de facto binding by senior appointees or their designees."</p>
<h2>Prior funding at risk</h2>
<p>All funding agencies are forbidden from starting any new grant funding programs until the system for exerting political control over the research is in place. The order also requires agencies to take political control of past funding.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>The actual process of distributing funds to labs is called grant "drawdown," and the order requires the funding agency to explicitly approve any drawdown. That approval will now require any researcher to essentially rejustify the existence of their grant any time they want money, with agencies requiring "grantees to provide written explanations or support, with specificity, for requests for each drawdown." The explosion of paperwork that this will require is somewhat ironic, given that the order is claiming to be (in part) about making research spending more efficient.</p>
<p>Should the agency not feel that any grant is justified, they'll simply be allowed to unilaterally terminate it. "Each agency head shall, to the maximum extent permitted by law and consistent with relevant Executive Orders or other Presidential directives," it reads, "take steps to revise the terms and conditions of existing discretionary grants to permit immediate termination for convenience, or clarify that such termination is permitted, including if the award no longer advances agency priorities or the national interest."</p>
<p>It has been clear for a while that the administration is committed to adding ideological litmus tests to science and slashing research funding. However, Congress has shown indications that it <a href="https://www.science.org/content/article/boost-nih-budget-senate-panel-rejects-trump-s-plan-slash-agency">doesn't intend to go along with the cuts</a>. This appears to be the administration's response to Congress: An attempt to place a major roadblock to any new funding and establish the structure that will formally exert the ideological control that it wants.</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/john-timmer/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2016/05/j.timmer-5.jpg" alt="Photo of John Timmer"></a></p>
  </div>

  <div>
    

    <p>
      John is Ars Technica's science editor. He has a Bachelor of Arts in Biochemistry from Columbia University, and a Ph.D. in Molecular and Cell Biology from the University of California, Berkeley. When physically separated from his keyboard, he tends to seek out a bicycle, or a scenic location for communing with his hiking boots.

    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/science/2025/08/new-executive-order-puts-all-grants-under-political-control/#comments" title="84 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    84 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/health/2025/08/rfk-jr-defends-500m-cut-for-mrna-vaccines-with-pseudoscience-gobbledygook/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-2216099156-768x432.jpg" alt="Listing image for first story in Most Read: RFK Jr. defends $500M cut for mRNA vaccines with pseudoscience gobbledygook" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </main>





  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cursed Knowledge (439 pts)]]></title>
            <link>https://immich.app/cursed-knowledge/</link>
            <guid>44831704</guid>
            <pubDate>Thu, 07 Aug 2025 23:34:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://immich.app/cursed-knowledge/">https://immich.app/cursed-knowledge/</a>, See on <a href="https://news.ycombinator.com/item?id=44831704">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="__docusaurus_skipToContent_fallback"><p>Cursed knowledge we have learned as a result of building Immich that we wish we never knew.</p><div><ul><li><p>6/4/2025</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M16,16.92C15.67,16.97 15.34,17 15,17C14.66,17 14.33,16.97 14,16.92V13.41L11.5,15.89C11,15.5 10.5,15 10.11,14.5L12.59,12H9.08C9.03,11.67 9,11.34 9,11C9,10.66 9.03,10.33 9.08,10H12.59L10.11,7.5C10.3,7.25 10.5,7 10.76,6.76V6.76C11,6.5 11.25,6.3 11.5,6.11L14,8.59V5.08C14.33,5.03 14.66,5 15,5C15.34,5 15.67,5.03 16,5.08V8.59L18.5,6.11C19,6.5 19.5,7 19.89,7.5L17.41,10H20.92C20.97,10.33 21,10.66 21,11C21,11.34 20.97,11.67 20.92,12H17.41L19.89,14.5C19.7,14.75 19.5,15 19.24,15.24V15.24C19,15.5 18.75,15.7 18.5,15.89L16,13.41V16.92H16V16.92M5,19A2,2 0 0,1 7,17A2,2 0 0,1 9,19A2,2 0 0,1 7,21A2,2 0 0,1 5,19H5Z" style="fill:purple"></path></svg><p><span>Zitadel Actions are cursed</span></p></div><p>Zitadel is cursed because its custom scripting feature is executed with a JS engine that doesn't support regex named capture groups.</p></div></li><li><p>5/30/2025</p><div><p>Microsoft Entra supports PKCE, but doesn't include it in its OpenID discovery document. This leads to clients thinking PKCE isn't available.</p></div></li><li><p>5/5/2025</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M7,17V1H5V5H1V7H5V17A2,2 0 0,0 7,19H17V23H19V19H23V17M17,15H19V7C19,5.89 18.1,5 17,5H9V7H17V15Z" style="fill:tomato"></path></svg><p><span>Image dimensions in EXIF metadata are cursed</span></p></div><p>The dimensions in EXIF metadata can be different from the actual dimensions of the image, causing issues with cropping and resizing.</p></div></li><li><p>4/1/2025</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M5,3H7V5H5V10A2,2 0 0,1 3,12A2,2 0 0,1 5,14V19H7V21H5C3.93,20.73 3,20.1 3,19V15A2,2 0 0,0 1,13H0V11H1A2,2 0 0,0 3,9V5A2,2 0 0,1 5,3M19,3A2,2 0 0,1 21,5V9A2,2 0 0,0 23,11H24V13H23A2,2 0 0,0 21,15V19A2,2 0 0,1 19,21H17V19H19V14A2,2 0 0,1 21,12A2,2 0 0,1 19,10V5H17V3H19M12,15A1,1 0 0,1 13,16A1,1 0 0,1 12,17A1,1 0 0,1 11,16A1,1 0 0,1 12,15M8,15A1,1 0 0,1 9,16A1,1 0 0,1 8,17A1,1 0 0,1 7,16A1,1 0 0,1 8,15M16,15A1,1 0 0,1 17,16A1,1 0 0,1 16,17A1,1 0 0,1 15,16A1,1 0 0,1 16,15Z" style="fill:yellow"></path></svg><p><span>YAML whitespace is cursed</span></p></div><p>YAML whitespaces are often handled in unintuitive ways.</p></div></li><li><p>9/20/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M3,12V6.75L9,5.43V11.91L3,12M20,3V11.75L10,11.9V5.21L20,3M3,13L9,13.09V19.9L3,18.75V13M20,13.25V22L10,20.09V13.1L20,13.25Z" style="fill:#357EC7"></path></svg><p><span>Hidden files in Windows are cursed</span></p></div><p>Hidden files in Windows cannot be opened with the "w" flag. That, combined with SMB option "hide dot files" leads to a lot of confusion.</p></div></li><li><p>8/7/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M21,5H3V7H21V5M3,19H10V17H3V19M3,13H18C19,13 20,13.43 20,15C20,16.57 19,17 18,17H16V15L12,18L16,21V19H18C20.95,19 22,17.73 22,15C22,12.28 21,11 18,11H3V13Z" style="fill:gray"></path></svg><p><span>Carriage returns in bash scripts are cursed</span></p></div><p>Git can be configured to automatically convert LF to CRLF on checkout and CRLF breaks bash scripts.</p></div></li><li><p>8/7/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M9 5.82L7.36 4.16C8.09 2.31 9.89 1 12 1C14.76 1 17 3.24 17 6V8H18C19.11 8 20 8.9 20 10V16.8L11.2 8H15V6C15 4.34 13.66 3 12 3C10.41 3 9.11 4.25 9 5.82M22.11 21.46L20.84 22.73L19.46 21.35C19.1 21.75 18.58 22 18 22H6C4.89 22 4 21.11 4 20V10C4 8.89 4.9 8 6 8H6.11L1.11 3L2.39 1.73L22.11 21.46M13.85 15.74L11.26 13.15C10.5 13.44 10 14.16 10 15C10 16.11 10.9 17 12 17C12.84 17 13.56 16.5 13.85 15.74Z" style="fill:red"></path></svg><p><span>Fetch inside Cloudflare Workers is cursed</span></p></div><p>Fetch requests in Cloudflare Workers use http by default, even if you explicitly specify https, which can often cause redirect loops.</p></div></li><li><p>7/21/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M20.94 11C20.5 6.83 17.17 3.5 13 3.06V1H11V3.06C9.87 3.18 8.81 3.5 7.84 4.03L9.34 5.53C10.16 5.19 11.06 5 12 5C15.87 5 19 8.13 19 12C19 12.94 18.81 13.84 18.5 14.65L20 16.15C20.5 15.19 20.82 14.13 20.95 13H23V11H20.94M3 4.27L5.04 6.31C3.97 7.62 3.25 9.23 3.06 11H1V13H3.06C3.5 17.17 6.83 20.5 11 20.94V23H13V20.94C14.77 20.74 16.38 20.03 17.69 18.96L19.73 21L21 19.73L4.27 3L3 4.27M16.27 17.54C15.09 18.45 13.61 19 12 19C8.13 19 5 15.87 5 12C5 10.39 5.55 8.91 6.46 7.73L16.27 17.54Z" style="fill:gray"></path></svg><p><span>GPS sharing on mobile is cursed</span></p></div><p>Some phones will silently strip GPS data from images when apps without location permission try to access them.</p></div></li><li><p>7/3/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M16.84,2.73C16.45,2.73 16.07,2.88 15.77,3.17L13.65,5.29L18.95,10.6L21.07,8.5C21.67,7.89 21.67,6.94 21.07,6.36L17.9,3.17C17.6,2.88 17.22,2.73 16.84,2.73M12.94,6L4.84,14.11L7.4,14.39L7.58,16.68L9.86,16.85L10.15,19.41L18.25,11.3M4.25,15.04L2.5,21.73L9.2,19.94L8.96,17.78L6.65,17.61L6.47,15.29" style="fill:gold"></path></svg><p><span>PostgreSQL NOTIFY is cursed</span></p></div><p>PostgreSQL does everything in a transaction, including NOTIFY. This means using the socket.io postgres-adapter writes to WAL every 5 seconds.</p></div></li><li><p>7/3/2024</p><div><p>npm scripts make a http call to the npm registry each time they run, which means they are a terrible way to execute a health check.</p></div></li><li><p>6/28/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M12 16C13.66 16 15 14.66 15 13C15 11.88 14.39 10.9 13.5 10.39L3.79 4.77L9.32 14.35C9.82 15.33 10.83 16 12 16M12 3C10.19 3 8.5 3.5 7.03 4.32L9.13 5.53C10 5.19 11 5 12 5C16.42 5 20 8.58 20 13C20 15.21 19.11 17.21 17.66 18.65H17.65C17.26 19.04 17.26 19.67 17.65 20.06C18.04 20.45 18.68 20.45 19.07 20.07C20.88 18.26 22 15.76 22 13C22 7.5 17.5 3 12 3M2 13C2 15.76 3.12 18.26 4.93 20.07C5.32 20.45 5.95 20.45 6.34 20.06C6.73 19.67 6.73 19.04 6.34 18.65C4.89 17.2 4 15.21 4 13C4 12 4.19 11 4.54 10.1L3.33 8C2.5 9.5 2 11.18 2 13Z" style="fill:brown"></path></svg><p><span>50 extra packages are cursed</span></p></div><p>There is a user in the JavaScript community who goes around adding "backwards compatibility" to projects. They do this by adding 50 extra package dependencies to your project, which are maintained by them.</p></div></li><li><p>6/25/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M12,17C10.89,17 10,16.1 10,15C10,13.89 10.89,13 12,13A2,2 0 0,1 14,15A2,2 0 0,1 12,17M18,20V10H6V20H18M18,8A2,2 0 0,1 20,10V20A2,2 0 0,1 18,22H6C4.89,22 4,21.1 4,20V10C4,8.89 4.89,8 6,8H7V6A5,5 0 0,1 12,1A5,5 0 0,1 17,6V8H18M12,3A3,3 0 0,0 9,6V8H15V6A3,3 0 0,0 12,3Z" style="fill:gold"></path></svg><p><span>Long passwords are cursed</span></p></div><p>The bcrypt implementation only uses the first 72 bytes of a string. Any characters after that are ignored.</p></div></li><li><p>1/31/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M7,10H12V15H7M19,19H5V8H19M19,3H18V1H16V3H8V1H6V3H5C3.89,3 3,3.9 3,5V19A2,2 0 0,0 5,21H19A2,2 0 0,0 21,19V5A2,2 0 0,0 19,3Z" style="fill:greenyellow"></path></svg><p><span>JavaScript Date objects are cursed</span></p></div><p>JavaScript date objects are 1 indexed for years and days, but 0 indexed for months.</p></div></li><li><p>1/9/2024</p><div><p>Prior to Node.js v20.8 using --experimental-vm-modules in a CommonJS project that imported an ES module that imported a CommonJS modules would create a segfault and crash Node.js</p></div></li><li><p>12/28/2023</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M12,3C7.58,3 4,4.79 4,7C4,9.21 7.58,11 12,11C16.42,11 20,9.21 20,7C20,4.79 16.42,3 12,3M4,9V12C4,14.21 7.58,16 12,16C16.42,16 20,14.21 20,12V9C20,11.21 16.42,13 12,13C7.58,13 4,11.21 4,9M4,14V17C4,19.21 7.58,21 12,21C16.42,21 20,19.21 20,17V14C20,16.21 16.42,18 12,18C7.58,18 4,16.21 4,14Z" style="fill:gray"></path></svg><p><span>PostgreSQL parameters are cursed</span></p></div><p>PostgresSQL has a limit of 65,535 parameters, so bulk inserts can fail with large datasets.</p></div></li><li><p>6/26/2023</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M12,12H19C18.47,16.11 15.72,19.78 12,20.92V12H5V6.3L12,3.19M12,1L3,5V11C3,16.55 6.84,21.73 12,23C17.16,21.73 21,16.55 21,11V5L12,1Z" style="fill:gold"></path></svg><p><span>Secure contexts are cursed</span></p></div><p>Some web features like the clipboard API only work in "secure contexts" (ie. https or localhost)</p></div></li><li><p>2/23/2023</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M9,3V4H4V6H5V19A2,2 0 0,0 7,21H17A2,2 0 0,0 19,19V6H20V4H15V3H9M9,8H11V17H9V8M13,8H15V17H13V8Z" style="fill:gray"></path></svg><p><span>TypeORM deletes are cursed</span></p></div><p>The remove implementation in TypeORM mutates the input, deleting the id property from the original object.</p></div></li></ul></div></section></div>]]></description>
        </item>
    </channel>
</rss>