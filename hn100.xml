<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 26 Aug 2023 09:00:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Companies That Union-Bust Must Now Automatically Recognize Union, NLRB Rules (112 pts)]]></title>
            <link>https://www.vice.com/en/article/dy3xej/companies-that-union-bust-must-now-automatically-recognize-union-nlrb-rules</link>
            <guid>37269909</guid>
            <pubDate>Sat, 26 Aug 2023 04:18:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vice.com/en/article/dy3xej/companies-that-union-bust-must-now-automatically-recognize-union-nlrb-rules">https://www.vice.com/en/article/dy3xej/companies-that-union-bust-must-now-automatically-recognize-union-nlrb-rules</a>, See on <a href="https://news.ycombinator.com/item?id=37269909">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>NLRB sign. Image Credit: Getty Images</p></div><div data-component="BodyComponentRenderer"><p><span data-component="TextBlock"><p>The National Labor Relations Board issued a ruling on Friday that <a href="https://www.nlrb.gov/news-outreach/news-story/board-issues-decision-announcing-new-framework-for-union-representation" target="_blank"><span>changes the framework</span></a> for unionizations, making it easier for workers to organize and harder for companies to fight back against them.&nbsp;</p></span><span data-component="TextBlock"><p>The new process comes as part of a decision in the case between Cemex Construction Materials Pacific, LLC and the International Brotherhood of Teamsters, where the Board found that the employer had committed over 20 “instances of objectionable or unlawful misconduct” between the filing of the union election petition and the election itself, intending to dissuade workers from organizing.&nbsp;</p></span></p><p><span data-component="TextBlock"><p>The decision requires that if a majority of workers ask a company for voluntary recognition of their union, the company must either immediately recognize them or promptly file a petition asking the Board to hold a union election.&nbsp;</p></span><span data-component="TextBlock"><p>“However, if an employer who seeks an election commits any unfair labor practice that would require setting aside the election, the petition will be dismissed, and—rather than re-running the election—the Board will order the employer to recognize and bargain with the union,” an <a href="https://www.nlrb.gov/news-outreach/news-story/board-issues-decision-announcing-new-framework-for-union-representation" target="_blank"><span>NLRB press release stated</span></a>. If the company neither recognizes the union nor files a petition, the Board will issue a bargaining order forcing the company to come to the table.&nbsp;</p></span></p><p><span data-component="TextBlock"><p>“This is a very important ruling that will help workers to be able to unionize free of coercion, especially at companies like Trader Joe's, Starbucks and Amazon,” said Seth Goldstein, a partner at Julien, Mirer, Singla and Goldstein, who has represented workers at those companies attempting to unionize and <a href="https://www.nytimes.com/2023/07/21/opinion/starbucks-union-strikes-labor-movement.html" target="_blank"><span>facing union-busting efforts</span></a>. “This has been the law of the land for 80 years, so it really goes back to what should have been all along.”&nbsp;</p></span><span></span><span data-component="TextBlock"><p>Goldstein was referring to a 1949 Supreme Court decision known as <a href="https://www.politico.com/news/magazine/2022/06/07/the-lie-that-helped-kill-the-labor-movement-00037459" target="_blank"><span>Joy Silk</span></a>, which stated that an employer had to voluntarily recognize and bargain with a majority of workers who wanted a union, and could only contest the unionization if it had “good faith doubt” about the union’s majority status. Joy Silk was <a href="https://supreme.justia.com/cases/federal/us/395/575/" target="_blank"><span>abandoned in 1969</span></a> in favor of giving companies more leverage in the unionization process.&nbsp;</p></span><span data-component="TextBlock"><p>But NLRB General Counsel Jennifer Abruzzo issued a memo earlier this year <a href="https://www.jdsupra.com/legalnews/nlrb-general-counsel-calls-for-4375993/" target="_blank"><span>demanding that the Board revive Joy Silk</span></a>, something that <a href="https://www.politico.com/news/magazine/2022/06/07/the-lie-that-helped-kill-the-labor-movement-00037459" target="_blank"><span>labor activists have been fighting</span></a> for since it was overturned. The Cemex decision issued on Friday is a partial step in that direction.&nbsp;</p></span><span data-component="TextBlock"><p>“What this new decision does is, it's a compromise,” said Eric Blanc, an assistant professor of labor studies at Rutgers University. “It's not a return to ‘card check,’” the unionization process in the 1930s and ‘40s that said if a majority of workers signed cards stating they wanted a union, the company was obligated to recognize and bargain with them—which Joy Silk had upheld.&nbsp;</p></span><span data-component="TextBlock"><p>“If there's intense illegal union busting, as is very often the case, the NLRB can force the employers to immediately recognize the union rather than have to go through another union election,” Blanc said. “But it's far short of what many union organizers were hoping for. By not making ‘card check’ the norm, [it] still opens up the process to all sorts of legal appeals and delays, which is ultimately one of the main tactics of employers—to delay the union first and then hold things up in endless appeals. This unfortunately doesn't avoid that dynamic, but it does get the NLRB more powers to require employers to recognize unions, and that should be at least a partial deterrent on employers’ willingness to break the law.”&nbsp;</p></span><span data-component="TextBlock"><p>Goldstein said that despite the incomplete return to Joy Silk, he thought it was a “big step forward.”&nbsp;</p></span><span data-component="TextBlock"><p>“The Board is taking a proactive stance, that unfair labor violations are going to be challenged,” he said. “We haven’t seen this kind of activism from the Board since the 1940s. I think this is going to really help workers.”</p></span></p></div><div><p><h3>ORIGINAL REPORTING ON EVERYTHING THAT MATTERS IN YOUR INBOX.</h3></p><p>By signing up, you agree to the<!-- --> <a href="https://vice-web-statics-cdn.vice.com/privacy-policy/en_us/page/terms-of-use.html">Terms of Use</a> <!-- -->and<!-- --> <a href="https://vice-web-statics-cdn.vice.com/privacy-policy/en_us/page/privacy-policy.html">Privacy Policy</a> <!-- -->&amp; to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We are not empty: The concept of the atomic void is a mistake (107 pts)]]></title>
            <link>https://aeon.co/essays/why-the-empty-atom-picture-misunderstands-quantum-theory</link>
            <guid>37268886</guid>
            <pubDate>Sat, 26 Aug 2023 00:55:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aeon.co/essays/why-the-empty-atom-picture-misunderstands-quantum-theory">https://aeon.co/essays/why-the-empty-atom-picture-misunderstands-quantum-theory</a>, See on <a href="https://news.ycombinator.com/item?id=37268886">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The camera zooms in on the person’s arm to reveal the cells, then a cell nucleus. A DNA strand grows on the screen. The camera focuses on a single atom within the strand, dives into a frenetic cloud of rocketing particles, crosses it, and leaves us in oppressive darkness. An initially imperceptible tiny dot grows smoothly, revealing the atomic nucleus. The narrator lectures that the nucleus of an atom is tens of thousands of times smaller than the atom itself, and poetically concludes that we are made from emptiness.</p>
<p>How often have you seen such a scene or read something equivalent to it in popular science? I am sure plenty, if you are fans of this genre like me. However, the narrative is wrong. Atomic nuclei in a molecule are not tiny dots, and there are no empty spaces within the atom.</p>
<p>The <em>empty atom</em> picture is likely the most repeated mistake in popular science. It is unclear who created this myth, but it is sure that Carl Sagan, in his classic TV series <em>Cosmos</em> (1980), was crucial in popularising it. After wondering how small the nuclei are compared with the atom, Sagan concluded that</p>
<blockquote>[M]ost of the mass of an atom is in its nucleus; the electrons are by comparison just clouds of moving fluff. Atoms are mainly empty space. Matter is composed chiefly of nothing.</blockquote>
<p>I still remember how deeply these words spoke to me when I heard them as a kid in the early 1980s. Today, as a professional theoretical chemist, I know that Sagan’s statements failed to recognise some fundamental features of atoms and molecules.</p>
<p>Yet his reasoning is still influential. While preparing this essay, I ran a poll on Twitter asking whether people agreed with Sagan’s quote above. Of the 180 voters, <span>43 per</span> cent answered that they mostly agreed, and <span>27 per</span> cent fully agreed. Google ‘atoms empty space’, and you will find tens of essays, blog posts and YouTube videos concluding that atoms are <span>99.9 per</span> cent empty space. To be fair, you will also find a reasonable share of articles debunking the idea.</p>
<p>Misconceptions feeding the idea of the empty atom can be dismantled by carefully interpreting quantum theory, which describes the physics of molecules, atoms and subatomic particles. According to quantum theory, the building blocks of matter – like electrons, nuclei and the molecules they form – can be portrayed either as waves or particles. Leave them to evolve by themselves without human interference, and they act like delocalised waves in the shape of continuous clouds. On the other hand, when we attempt to observe these systems, they appear to be localised particles, something like bullets in the classical realm. But accepting the quantum predictions that nuclei and electrons fill space as continuous clouds has a daring conceptual price: it implies that these particles do not vibrate, spin or orbit. They inhabit a motionless microcosmos where time only occasionally plays a role.</p>
<p>Most problems surrounding the description of the submolecular world come from frustrated attempts to reconcile conflicting pictures of waves and particles, leaving us with inconsistent chimeras such as particle-like nuclei surrounded by wave-like electrons. This image doesn’t capture quantum theory’s predictions. To compensate, our conceptual reconstruction of matter at the submolecular level should consistently describe how nuclei and electrons behave when not observed – like the proverbial sound of a tree falling in the forest without anyone around.</p>
<p><span>H</span>ere’s a primer on how to think of the fundamental components of matter: a molecule is a stable collection of nuclei and electrons. If the collection contains a single nucleus, it is called an atom. Electrons are elementary particles with no internal structure and a negative electric charge. On the other hand, each nucleus is a combined system composed of several protons and a roughly equal number of neutrons. Each proton and neutron is 1,836 times more massive than an electron. The proton has a positive charge of the same magnitude as an electron’s negative charge, while neutrons, as their name hints, have no electric charge. Usually, but not necessarily, the total number of protons in a molecule equals the number of electrons, making molecules electrically neutral.</p>
<p>The interior of the protons and neutrons is likely the most complex place in the Universe. I like to consider each of them a hot soup of three permanent elementary particles known as quarks boiling along inside, with an uncountable number of virtual quarks popping into existence and disappearing almost immediately. Other elementary particles called gluons hold the soup within a pot of <span>0.9 femtometres</span> radius. (A femtometre, abbreviated fm, is a convenient scale that measures systems tens of thousands of times smaller than an atom. Corresponding to <span>10</span><span><sup>‑15</sup></span><span> m,</span> we must juxtapose <span>1 trillion</span> femtometres to make one millimetre.)</p>
<p>Instead of localised bullets in empty space, matter delocalises into continuous quantum clouds</p>
<p>Particles with the same electric charge sign repel each other. So additional interactions are required to hold protons close-packed in the nucleus. These interactions arise from quark and antiquark pairs called pions that constantly spill out of each proton and neutron to be absorbed by another such particle nearby. The energy exchanged in this transfer is big enough to compensate for the electric repulsion between protons and, thus, bind together protons and neutrons, storing the immense energy that may be released in nuclear fission processes.</p>
<p>However, the extremely short lifetime of the pions limits how far protons and neutrons may be from each other, curbing the nucleus size to a 1 to <span>10 fm</span> radius. Thus, from a particle perspective, the nucleus is tiny compared with an atom. A nitrogen nucleus, composed of seven protons and seven neutrons, has a radius of about <span>3 fm.</span> In contrast, nitrogen’s atomic <a href="https://doi.org/10.1002/chem.201602949" target="_blank" rel="noreferrer noopener">radius</a> is <span>179,000 fm.</span> At the scale of atoms and molecules, nuclei are no more than heavy, point-like positive charges without any apparent internal structure. So are the electrons: they are just light, point-like negative charges.</p>
<p>If atoms and molecules remained a collection of point-like particles, they <em>would</em> be mostly empty space. But at their size scale, they must be described by quantum theory. And this theory predicts that the wave-like picture predominates until a measurement disturbs it. Instead of localised bullets in empty space, matter delocalises into continuous quantum clouds.</p>
<p><span>M</span>atter is fundamentally quantum. Molecules cannot be assembled under the rules of classical physics. The classical electrical interactions between nuclei and electrons are insufficient to build a stable molecule. Due to the electric attraction of charges of opposite signs, the negatively charged electrons would quickly spiral toward the positively charged nuclei and glue to them. The resulting combined particles with no net charge would fly apart, preventing any molecule from forming.</p>
<p>Two quantum properties avoid this bleak fate.</p>
<p>The first property arises from the Heisenberg <a href="https://aeon.co/essays/our-simple-magic-free-recipe-for-quantum-entanglement" target="_blank" rel="noopener">uncertainty principle</a>, which holds that a quantum particle cannot simultaneously be at a precise position and also have zero speed. This implies that an electron cannot glue to a nucleus because both particles would be in a well-defined place and at rest to each other – defying a central rule of the quantum world.</p>
<p>The second quantum property is the Pauli exclusion principle. The fundamental components of matter are split into two types, bosons and fermions. The gluons inside the proton are examples of bosons. We can have as many of them as we want, sharing the same position simultaneously. On the other hand, fermions – such as electrons, quarks, protons and neutrons – obey a much more restrictive rule named the Pauli exclusion principle: no two identical fermions can simultaneously occupy the same space and have the same spin (a quantum property analogous to a classical rotation of a particle about its axis).</p>
<p>In the quantum world, the wave function represents more than a mere lack of knowledge</p>
<p>With all those effects encoded into the Schrödinger equation, the master equation of quantum theory, it predicts that our point-like nuclei and electrons must, in fact, behave like waves. They delocalise in quantum clouds much bigger than their particle-picture size to satisfy the Heisenberg uncertainty principle, with electrons shaped into different clouds to satisfy the Pauli exclusion principle. The lighter the particles are, the bigger the delocalisation. Thus, a single electron cloud may spread over multiple nuclei, forming a chemical bond and stabilising the molecule.</p>
<p>Take an ammonia molecule, NH<sub>3</sub>, illustrated below. The small blue smudge in the middle is the nitrogen nucleus cloud, while the three green blobs are the proton (hydrogen nuclei) clouds. The 10 electrons of the ammonia molecule delocalise into the fat yellow cloud, tying the party together.</p>
<figure><img alt="" loading="lazy" width="2400" height="2400" decoding="async" data-nimg="1" srcset="https://aeon.co/_next/image?url=https%3A%2F%2Fd2e1bqvws99ptg.cloudfront.net%2Fuser_image_upload%2F2662%2Finsert-scatterplot-HR.jpg&amp;w=3840&amp;q=90 1x" src="https://aeon.co/_next/image?url=https%3A%2F%2Fd2e1bqvws99ptg.cloudfront.net%2Fuser_image_upload%2F2662%2Finsert-scatterplot-HR.jpg&amp;w=3840&amp;q=90"><figcaption><p>Figure 1: Electronic and nuclear quantum clouds in an ammonia molecule. The yellow cloud represents the 10 electrons in this molecule. The small blue cloud is the nitrogen nucleus, while the three green clouds indicate each hydrogen nucleus. Electronic points in front of the nuclei were made transparent so as not to hide the nuclear clouds. Technical details are explained in <a href="http://dx.doi.org/10.1039/D3CP00247K" target="_blank" rel="noreferrer noopener">Toldo et al 2023</a>. Courtesy the author</p></figcaption></figure>
<p>A particle-like nitrogen nucleus has a <span>3 fm</span> radius. However, in the ammonia molecule, the nitrogen nucleus grows to a respectable <span>3,000 fm</span> radius due to delocalisation. The delocalisation of the hydrogen nuclei is even more impressive. They grow from a radius of <span>0.9 fm</span> when seen as particles to clouds of about <span>23,000 fm.</span> But the electrons take the cake. Due to their tiny mass, they grow from particles much smaller than a nucleus into a cloud that defines the molecular volume.</p>
<p>Nuclei and electrons, however, are not atomic giants. If the nitrogen nucleus is measured (for instance, by throwing fast electrons against it and observing them bounce back), the nuclear cloud would immediately collapse into the initial <span>3 fm</span> dot. The same is true for each electron.</p>
<p>Indeed, quantum theory prescribes a precise relationship between the wave and particle pictures. The clouds of the wave picture are mathematically described by a wave function, essentially an equation that attributes an intensity to every point in space and how these intensities change with time. The wave function is analogous to mathematical functions describing conventional sound or water waves, but with the peculiarity that it has an <a href="https://aeon.co/essays/how-imaginary-numbers-describe-the-fundamental-shape-of-nature" target="_blank" rel="noopener">imaginary-number component</a>, which is negative when squared.</p>
<p>The square of the wave function modulus (a mathematical operation that always yields positive numbers) gives the probability of finding the particle at each point in space if we attempt to observe it. The denser the cloud, the bigger the odds of observing the particle there. Thus, if we try to measure the point-like nitrogen nucleus, we are sure that it will be somewhere in the region of the delocalised nitrogen nucleus cloud, the blue smudge in the figure.</p>
<p>However, interpreting the quantum cloud as probability does not mean it is just a measure of a lack of knowledge about the system. If I left my keys in one of my jacket’s two pockets, but I am unsure which one, I may write a probability function with a <span>50 per</span> cent value at each pocket and zero value at every other point of my office. This function obviously does not imply that my keys are delocalised over the two pockets. It just states my ignorance, which can be easily fixed by checking the jacket.</p>
<p>In the quantum world, the wave function represents more than a mere lack of knowledge. Delocalised systems – like nuclear and electronic clouds – cause phenomena that localised particles cannot explain. The existence of chemical bonds forming molecules is a direct example of the effect of electronic delocalisation. In the case of nuclear delocalisation, one of its main effects is to boost the chances of a hydrogen nucleus (a single proton) flowing from one molecule to another nearby. This kind of enhanced proton transfer has dramatic biological consequences, like <a href="https://doi.org/10.1073/pnas.1417923111" target="_blank" rel="noreferrer noopener">increasing</a> the acidity of specific enzymes compared with how acidic they would be if hydrogen nuclei behaved as particles.</p>
<p><span>A</span>lthough electron clouds are commonly depicted in popular science and chemistry, delocalisation of the nucleus is often interpreted as vibrations and rotations. But these are only classical, albeit helpful, analogies. From a quantum perspective and for conceptual consistency, nuclei should be depicted on the same footing as electrons, as clouds as well.</p>
<p>Yet another misconception is that atoms are empty because their mass is in their nucleus. The atomic mass is indeed highly localised. In an ammonia molecule, <span>82 per</span> cent of the mass is in the blue smudge of the nitrogen nucleus shown in <span>Figure 1</span> above. If we add the masses of the three green proton clouds, they account for <span>99.97 per</span> cent of the total. Thus, the big yellow cloud of the electrons carries only <span>0.03 per</span> cent of the mass.</p>
<p>The association between this mass concentration and the idea that atoms are empty stems from a flawed view that mass is the property of matter that fills a space. However, this concept does not hold up to close inspection, not even in our human-scale world. When we pile objects on top of each other, what keeps them separated is not their masses but the electric repulsion between the outmost electrons at their touching molecules. (The electrons cannot collapse under pressure due to the Heisenberg uncertainty and Pauli exclusion principles.) Therefore, the electron’s electric charge ultimately fills the space.</p>
<p>Anyone taking Chemistry 101 is likely to be faced with diagrams of electrons orbiting in shells</p>
<p>In atoms and molecules, electrons are everywhere! Look how the yellow cloud permeates the entire molecular volume in <span>Figure 1.</span> Thus, when we see that atoms and molecules are packed with electrons, the only reasonable conclusion is that they are filled with matter, not the opposite.</p>
<p>Despite all this, anyone taking Chemistry 101 is likely to be faced with diagrams of electrons orbiting in shells, like concentric and separated layers with empty space between them. The idea that these diagrams represent physical reality is a third common misconception. Electrons do not literally orbit around the atomic nucleus in the shape of these shells.</p>
<p>In atoms and molecules, electrons must have specific energies, each energy associated with a particular cloud shape. Consider, for example, an atom with a single electron. In the lowest possible energy, the ground energy level, this electron delocalises into a spherical cloud, dense at the centre of the atom and gradually fading out. The single-electron wave functions describing these clouds are called orbitals.</p>
<p>At higher energy levels, the single electron delocalises into more complex clouds with nested spheres, multiple blobs or even doughnut shapes. Thus, when speaking of atoms and molecules, electrons are not little particles chaotically rocketing around the nuclei until they become a fuzzy cloud, as often depicted. And electrons are not <em>in</em> the orbitals, nor do they <em>populate</em> them. Electrons <em>are</em> the orbitals. They are delocalised clouds.</p>
<p><span>W</span>ith multiple electrons, which have been terra incognita in popular science, things get much more complicated. This is hardly a surprise since even professional theoretical chemists are uncomfortable describing them, despite their exceptional competence in predicting the properties of multi-electron systems.</p>
<p>Like ill-fitting clothes, chemistry vernacular is filled with awkward analogies and descriptions. Chemists may say that an electron <em>occupies</em> or <em>populates</em> an orbital as if orbitals were pre-existing places where electrons are put. Chemists often draw diagrams where orbitals are represented as short horizontal lines and electrons as small vertical arrows on those lines, like objects on shelves. All these verbal and visual metaphors fail to translate what quantum theory tells us about atoms and molecules.</p>
<p>When dealing with multi-electron systems (encompassing virtually all molecules), quantum theory no longer distinguishes between each electron; they are all described by a single wave function, a single cloud. Nevertheless, single electron orbitals are still a valid approximation that chemists constantly use to rationalise chemical reactions. The multi-electron wave function resembles a composition of these individual clouds overlapping within the volume defining the molecule. They feel each other; they recombine into new shapes; some bulge and others shrink; the clouds skew, stretch and twist until they comfortably adapt, occupying every available space. It may look like a messy sock drawer.</p>
<p>For a fraction of a picosecond, the tempest rages and reshapes the molecular landscape until stillness is restored</p>
<p>A molecule is a static object without any internal motion. The quantum clouds of all nuclei and electrons remain absolutely still for a molecule with a well-defined energy. Time is irrelevant. Quantum theory does not predict vibrating nuclei or orbiting and spinning electrons; those dynamic features are classical analogues to intrinsic quantum properties. Angular momentum, for instance, which in classical physics quantifies rotational speed, manifests as blobs in the wave function. The more numerous the blobs, the bigger the angular momentum, even though nothing rotates.</p>
<p>Time, however, comes into play when a molecule collides with another one, triggering a chemical reaction. Then, a storm strikes. The quantum steadiness bursts when the sections of the electronic cloud pour from one molecule upon another. The clouds mix, reshape, merge, and split. The nuclear clouds rearrange to accommodate themselves within the new electronic configuration, sometimes even migrating between molecules. For a fraction of a picosecond <span>(10</span><span><sup>-12</sup></span><span> seconds</span> or a billionth of a millisecond), the tempest rages and reshapes the molecular landscape until stillness is restored in the newly formed compounds.</p>
<p>In the Flammarion engraving <span>(Figure 2</span> below), a person at the edge of Earth dares to look beyond the firmament dome to uncover the marvellous machinery of clouds controlling the heavens. They could well be looking at a molecule instead. Then, this non-disturbing observer would find that nuclei and electrons are majestic, stable, structured, closed-packed clouds, driving every aspect of matter as we <span>know it.</span></p>
<figure><img alt="" loading="lazy" width="2400" height="2400" decoding="async" data-nimg="1" srcset="https://aeon.co/_next/image?url=https%3A%2F%2Fd2e1bqvws99ptg.cloudfront.net%2Fuser_image_upload%2F2661%2Finsert-The_Flammarion_Engraving_(ca._1888).jpg&amp;w=3840&amp;q=90 1x" src="https://aeon.co/_next/image?url=https%3A%2F%2Fd2e1bqvws99ptg.cloudfront.net%2Fuser_image_upload%2F2661%2Finsert-The_Flammarion_Engraving_(ca._1888).jpg&amp;w=3840&amp;q=90"><figcaption><p>Figure 2: Wood engraving from <em>L’atmosphère: météorologie populaire</em> (1888) by Camille Flammarion. Courtesy Wikipedia</p></figcaption></figure>
<p><span>M</span>y criticism of the <em>empty atom</em> picture isn’t meant to shame people’s previous attempts to describe atoms and molecules to the public. On the contrary, I applaud their effort in this challenging enterprise. Our common language, intuitions and even basic reasoning processes are not adapted to face quantum theory, this alien world of strangeness surrounded by quirky landscapes we mostly cannot make <span>sense of.</span></p>
<p>And there is so much we do not understand. We have yet to learn how to reconcile the dual wave-like and particle-like behaviour of matter. We do not even know whether wave functions have objective reality. Our brains melt, facing the multiple potential interpretations of quantum theory to the point that outstanding scientists seemingly <a href="https://aeon.co/essays/materialism-alone-cannot-explain-the-riddle-of-consciousness" target="_blank" rel="noopener">gave up hope</a> that we may reach a scientific consensus. We turn a blind eye to the dirty tricks we carry from the conceptual construction of quantum theory to the actual predictions.</p>
<p>The account of the quantum molecular world I presented is on comfortably safe grounds</p>
<p>We could conform to the unsatisfying ‘Shut up and calculate!’ attitude that has accompanied the increasingly weird predictions of quantum theory, which enabled the outstanding technological advancements of the past <span>100 years,</span> from lasers to microprocessors. However, we do not want to make only useful predictions. Our ultimate goal is to tell stories about our Universe. Thus, we calculate but <a href="https://aeon.co/essays/shut-up-and-calculate-does-a-disservice-to-quantum-mechanics" target="_blank" rel="noopener">do not shut up</a>. Generations of scientists and science popularisers do their best to translate all this strangeness into friendly metaphors of a theoretical body still full of mystery. We build new mental images of the quantum world one step at a time, even under the risk of tripping up here and there.</p>
<p>The account of the quantum molecular world I presented is on comfortably safe grounds. It is based on a quantum theory domain that is highly consensual among specialists. It is the town square of what the Nobel laureate <a href="https://aeon.co/ideas/perspective-is-a-lesson-in-how-science-collaborates-with-art" target="_blank" rel="noopener">Frank Wilczek</a> called the Core Theory, the physics framework describing fundamental particles, their interactions and Albert Einstein’s general relativity. Physicists are so confident about this core’s stability that they believe it should persist within any new theories of matter developed in the future.</p>
<p>Breathing this confidence and realising we are not made of empty space may be a soothing thought.</p>
<p><em>This Essay was made possible through the support of a grant to Aeon+Psyche from the John Templeton Foundation. The opinions expressed in this publication are those of the author and do not necessarily reflect the views of the Foundation. Funders to Aeon+Psyche are not involved in editorial decision-making.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sidewalk Garden (156 pts)]]></title>
            <link>https://zachklein.com/Sidewalk+Garden</link>
            <guid>37268616</guid>
            <pubDate>Sat, 26 Aug 2023 00:14:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zachklein.com/Sidewalk+Garden">https://zachklein.com/Sidewalk+Garden</a>, See on <a href="https://news.ycombinator.com/item?id=37268616">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Deep Neural Nets: 33 years ago and 33 years from now (2022) (149 pts)]]></title>
            <link>http://karpathy.github.io/2022/03/14/lecun1989/</link>
            <guid>37268610</guid>
            <pubDate>Sat, 26 Aug 2023 00:13:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://karpathy.github.io/2022/03/14/lecun1989/">http://karpathy.github.io/2022/03/14/lecun1989/</a>, See on <a href="https://news.ycombinator.com/item?id=37268610">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  

<p>The Yann LeCun et al. (1989) paper <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf">Backpropagation Applied to Handwritten Zip Code Recognition</a> is I believe of some historical significance because it is, to my knowledge, the earliest real-world application of a neural net trained end-to-end with backpropagation. Except for the tiny dataset (7291 16x16 grayscale images of digits) and the tiny neural network used (only 1,000 neurons), this paper reads remarkably modern today, 33 years later - it lays out a dataset, describes the neural net architecture, loss function, optimization, and reports the experimental classification error rates over training and test sets. It’s all very recognizable and type checks as a modern deep learning paper, except it is from 33 years ago. So I set out to reproduce the paper 1) for fun, but 2) to use the exercise as a case study on the nature of progress in deep learning.</p>

<p><img src="http://karpathy.github.io/assets/lecun/lecun1989.png" width="100%"></p>

<p><strong>Implementation</strong>. I tried to follow the paper as close as possible and re-implemented everything in PyTorch in this <a href="https://github.com/karpathy/lecun1989-repro">karpathy/lecun1989-repro</a> github repo. The original network was implemented in Lisp using the Bottou and LeCun 1988 <a href="https://leon.bottou.org/papers/bottou-lecun-88">backpropagation simulator SN</a> (later named Lush). The paper is in french so I can’t super read it, but from the syntax it looks like you can specify neural nets using higher-level API similar to what you’d do in something like PyTorch today. As a quick note on software design, modern libraries have adopted a design that splits into 3 components: 1) a fast (C/CUDA) general Tensor library that implements basic mathematical operations over multi-dimensional tensors, and 2) an autograd engine that tracks the forward compute graph and can generate operations for the backward pass, and 3) a scriptable (Python) deep-learning-aware, high-level API of common deep learning operations, layers, architectures, optimizers, loss functions, etc.</p>

<p><strong>Training</strong>. During the course of training we have to make 23 passes over the training set of 7291 examples, for a total of 167,693 presentations of (example, label) to the neural network. The original network trained for 3 days on a <a href="https://en.wikipedia.org/wiki/Sun-4">SUN-4/260</a> workstation. I ran my implementation on my MacBook Air (M1) CPU, which crunched through it in about 90 seconds (~<strong>3000X naive speedup</strong>). My conda is setup to use the native arm64 builds, rather than Rosetta emulation. The speedup may have been more dramatic if PyTorch had support for the full capability of the M1 (including the GPU and the NPU), but this seems to still be in development. I also tried naively running the code on an A100 GPU, but the training was actually <em>slower</em>, most likely because the network is so tiny (4 layer convnet with up to 12 channels, total of 9760 params, 64K MACs, 1K activations), and the SGD uses only a single example at a time. That said, if one really wanted to crush this problem with modern hardware (A100) and software infrastructure (CUDA, PyTorch), we’d need to trade per-example SGD for full-batch training to maximize GPU utilization and most likely achieve another ~100X speedup of training latency.</p>

<p><strong>Reproducing 1989 performance</strong>. The original paper reports the following results:</p>

<div><pre><code>eval: split train. loss 2.5e-3. error 0.14%. misses: 10
eval: split test . loss 1.8e-2. error 5.00%. misses: 102
</code></pre></div>

<p>While my training script repro.py in its current form prints at the end of the 23rd pass:</p>

<div><pre><code>eval: split train. loss 4.073383e-03. error 0.62%. misses: 45
eval: split test . loss 2.838382e-02. error 4.09%. misses: 82
</code></pre></div>

<p>So I am reproducing the numbers <em>roughly</em>, but not exactly. Sadly, an exact reproduction is most likely not possible because the original dataset has, I believe, been lost to time. Instead, I had to simulate it using the larger MNIST dataset (hah never thought I’d say that) by taking its 28x28 digits, scaling them down to 16x16 pixels with bilinear interpolation, and randomly without replacement drawing the correct number of training and test set examples from it. But I am sure there are other culprits at play. For example, the paper is a bit too abstract in its description of the weight initialization scheme, and I suspect that there are some formatting errors in the pdf file that, for example, erase dots “.”, making “2.5” look like like “2 5”, and potentially (I think?) erasing square roots. E.g. we’re told that the weight init is drawn from uniform “2 4 / F” where F is the fan-in, but I am guessing this surely (?) means “2.4 / sqrt(F)”, where the sqrt helps preserve the standard deviation of outputs. The specific sparse connectivity structure between the H1 and H2 layers of the net are also brushed over, the paper just says it is “chosen according to a scheme that will not be discussed here”, so I had to make some some sensible guesses here with an overlapping block sparse structure. The paper also claims to use tanh non-linearity, but I am worried this may have actually been the “normalized tanh” that maps ntanh(1) = 1, and potentially with an added scaled-down skip connection, which was trendy at the time to ensure there is at least a bit of gradient in the flat tails of the tanh. Lastly, the paper uses a “special version of Newton’s algorithm that uses a positive, diagonal approximation of Hessian”, but I only used SGD because it is significantly simpler and, according to the paper, “this algorithm is not believed to bring a tremendous increase in learning speed”.</p>

<p><strong>Cheating with time travel</strong>. Around this point came my favorite part. We are living here 33 years in the future and deep learning is a highly active area of research. How much can we improve on the original result using our modern understanding and 33 years of R&amp;D? My original result was:</p>

<div><pre><code>eval: split train. loss 4.073383e-03. error 0.62%. misses: 45
eval: split test . loss 2.838382e-02. error 4.09%. misses: 82
</code></pre></div>

<p>The first thing I was a bit sketched out about is that we are doing simple classification into 10 categories, but at the time this was modeled as a <a href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html">mean squared error</a> (MSE) regression into targets -1 (for negative class) or +1 (for positive class), with output neurons that also had the tanh non-linearity. So I deleted the tanh on output layers to get class logits and swapped in the standard (multiclass) <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">cross entropy loss</a> function. This change dramatically improved the training error, completely overfitting the training set:</p>

<div><pre><code>eval: split train. loss 9.536698e-06. error 0.00%. misses: 0
eval: split test . loss 9.536698e-06. error 4.38%. misses: 87
</code></pre></div>

<p>I suspect one has to be much more careful with weight initialization details if your output layer has the (saturating) tanh non-linearity and an MSE error on top of it. Next, in my experience a very finely-tuned SGD can work very well, but the modern <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">Adam optimizer</a> (learning rate of 3e-4, of course :)) is almost always a strong baseline and needs little to no tuning. So to improve my confidence that optimization was not holding back performance, I switched to AdamW with LR 3e-4, and decay it down to 1e-4 over the course of training, giving:</p>

<div><pre><code>eval: split train. loss 0.000000e+00. error 0.00%. misses: 0
eval: split test . loss 0.000000e+00. error 3.59%. misses: 72
</code></pre></div>

<p>This gave a slightly improved result on top of SGD, except we also have to remember that a little bit of weight decay came in for the ride as well via the default parameters, which helps fight the overfitting situation. As we are still heavily overfitting, next I introduced a simple data augmentation strategy where I shift the input images by up to 1 pixel horizontally or vertically. However, because this simulates an increase in the size of the dataset, I also had to increase the number of passes from 23 to 60 (I verified that just naively increasing passes in original setting did not substantially improve results):</p>

<div><pre><code>eval: split train. loss 8.780676e-04. error 1.70%. misses: 123
eval: split test . loss 8.780676e-04. error 2.19%. misses: 43
</code></pre></div>

<p>As can be seen in the test error, that helped quite a bit! Data augmentation is a fairly simple and very standard concept used to fight overfitting, but I didn’t see it mentioned in the 1989 paper, perhaps it was a more recent innovation (?). Since we are still overfitting a bit, I reached for another modern tool in the toolbox, <a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html">Dropout</a>. I added a weak dropout of 0.25 just before the layer with the largest number of parameters (H3). Because dropout sets activations to zero, it doesn’t make as much sense to use it with tanh that has an active range of [-1,1], so I swapped all non-linearities to the much simpler <a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html">ReLU</a> activation function as well. Because dropout introduces even more noise during training, we also have to train longer, bumping up to 80 passes, but giving:</p>

<div><pre><code>eval: split train. loss 2.601336e-03. error 1.47%. misses: 106
eval: split test . loss 2.601336e-03. error 1.59%. misses: 32
</code></pre></div>

<p>Which brings us down to only 32 / 2007 mistakes on the test set! I verified that just swapping tanh -&gt; relu in the original network did not give substantial gains, so most of the improvement here is coming from the addition of dropout. In summary, if I time traveled to 1989 I’d be able to cut the rate of errors by about 60%, taking us from ~80 to ~30 mistakes, and an overall error rate of ~1.5% on the test set. This gain did not come completely free because we also almost 4X’d the training time, which would have increased the 1989 training time from 3 days to almost 12. But the inference latency would not have been impacted. The remaining errors are here:</p>

<p><img src="http://karpathy.github.io/assets/lecun/errors32.png" width="100%"></p>

<p><strong>Going further</strong>. However, after swapping MSE -&gt; Softmax, SGD -&gt; AdamW, adding data augmentation, dropout, and swapping tanh -&gt; relu I’ve started to taper out on the low hanging fruit of ideas. I tried a few more things (e.g. weight normalization), but did not get substantially better results. I also tried to miniaturize a <a href="https://arxiv.org/abs/2010.11929">Visual Transformer (ViT)</a>) into a “micro-ViT” that roughly matches the number of parameters and flops, but couldn’t match the performance of a convnet. Of course, many other innovations have been made in the last 33 years, but many of them (e.g. residual connections, layer/batch normalizations) only become relevant in much larger models, and mostly help stabilize large-scale optimization. Further gains at this point would likely have to come from scaling up the size of the network, but this would bloat the test-time inference latency.</p>

<p><strong>Cheating with data</strong>. Another approach to improving the performance would have been to scale up the dataset, though this would come at a dollar cost of labeling. Our original reproduction baseline, again for reference, was:</p>

<div><pre><code>eval: split train. loss 4.073383e-03. error 0.62%. misses: 45
eval: split test . loss 2.838382e-02. error 4.09%. misses: 82
</code></pre></div>

<p>Using the fact that we have all of MNIST available to us, we can simply try scaling up the training set by ~7X (7,291 to 50,000 examples). Leaving the baseline training running for 100 passes already shows some improvement from the added data alone:</p>

<div><pre><code>eval: split train. loss 1.305315e-02. error 2.03%. misses: 60
eval: split test . loss 1.943992e-02. error 2.74%. misses: 54
</code></pre></div>

<p>But further combining this with the innovations of modern knowledge (described in the previous section) gives the best performance yet:</p>

<div><pre><code>eval: split train. loss 3.238392e-04. error 1.07%. misses: 31
eval: split test . loss 3.238392e-04. error 1.25%. misses: 24
</code></pre></div>

<p>In summary, simply scaling up the dataset in 1989 would have been an effective way to drive up the performance of the system, at no cost to inference latency.</p>

<p><strong>Reflections</strong>. Let’s summarize what we’ve learned as a 2022 time traveler examining state of the art 1989 deep learning tech:</p>

<ul>
  <li>First of all, not much has changed in 33 years on the macro level. We’re still setting up differentiable neural net architectures made of layers of neurons and optimizing them end-to-end with backpropagation and stochastic gradient descent. Everything reads remarkably familiar, except it is smaller.</li>
  <li>The dataset is a baby by today’s standards: The training set is just 7291 16x16 greyscale images. Today’s vision datasets typically contain a few hundred million high-resolution color images from the web (e.g. Google has JFT-300M, <a href="https://openai.com/blog/clip/">OpenAI CLIP</a> was trained on a 400M), but grow to as large as a small few billion. This is approx. ~1000X pixel information per image (384*384*3/(16*16)) times 100,000X the number of images (1e9/1e4), for a rough 100,000,000X more pixel data at the input.</li>
  <li>The neural net is also a baby: This 1989 net has approx. 9760 params, 64K MACs, and 1K activations. <a href="https://arxiv.org/abs/2106.04560">Modern (vision) neural nets</a> are on the scale of small few billion parameters (1,000,000X) and O(~1e12) MACs (~10,000,000X). Natural language models can reach into trillions of parameters.</li>
  <li>A state of the art classifier that took 3 days to train on a workstation now trains in 90 seconds on my fanless laptop (3,000X naive speedup), and further ~100X gains are very likely possible by switching to full-batch optimization and utilizing a GPU.</li>
  <li>I was, in fact, able to tune the model, augmentation, loss function, and the optimization based on modern R&amp;D innovations to cut down the error rate by 60%, while keeping the dataset and the test-time latency of the model unchanged.</li>
  <li>Modest gains were attainable just by scaling up the dataset alone.</li>
  <li>Further significant gains would likely have to come from a larger model, which would require more compute, and additional R&amp;D to help stabilize the training at increasing scales. In particular, if I was transported to 1989, I would have ultimately become upper-bounded in my ability to further improve the system without a bigger computer.</li>
</ul>

<p>Suppose that the lessons of this exercise remain invariant in time. What does that imply about deep learning of 2022? What would a time traveler from 2055 think about the performance of current networks?</p>

<ul>
  <li>2055 neural nets are basically the same as 2022 neural nets on the macro level, except bigger.</li>
  <li>Our datasets and models today look like a joke. Both are somewhere around 10,000,000X larger.</li>
  <li>One can train 2022 state of the art models in ~1 minute by training naively on their personal computing device as a weekend fun project.</li>
  <li>Today’s models are not optimally formulated, and just changing some of the details of the model, loss function, augmentation or the optimizer we can about halve the error.</li>
  <li>Our datasets are too small, and modest gains would come from scaling up the dataset alone.</li>
  <li>Further gains are actually not possible without expanding the computing infrastructure and investing into some R&amp;D on effectively training models on that scale.</li>
</ul>

<p>But the most important trend I want to comment on is that the whole setting of training a neural network from scratch on some target task (like digit recognition) is quickly becoming outdated due to finetuning, especially with the emergence of <a href="https://arxiv.org/abs/2108.07258">foundation models</a> like GPT. These foundation models are trained by only a few institutions with substantial computing resources, and most applications are achieved via lightweight finetuning of part of the network, prompt engineering, or an optional step of data or model distillation into smaller, special-purpose inference networks. I think we should expect this trend to be very much alive, and indeed, intensify. In its most extreme extrapolation, you will not want to train any neural networks at all. In 2055, you will ask a 10,000,000X-sized neural net megabrain to perform some task by speaking (or thinking) to it in English. And if you ask nicely enough, it will oblige. Yes you could train a neural net too… but why would you?</p>



  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CVE-2020-19909 is everything that is wrong with CVEs (146 pts)]]></title>
            <link>https://daniel.haxx.se/blog/2023/08/26/cve-2020-19909-is-everything-that-is-wrong-with-cves/</link>
            <guid>37267940</guid>
            <pubDate>Fri, 25 Aug 2023 22:43:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daniel.haxx.se/blog/2023/08/26/cve-2020-19909-is-everything-that-is-wrong-with-cves/">https://daniel.haxx.se/blog/2023/08/26/cve-2020-19909-is-everything-that-is-wrong-with-cves/</a>, See on <a href="https://news.ycombinator.com/item?id=37267940">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>This is a story consisting of several little building blocks and they occurred spread out in time and in different places. It is a story that shows with clarity how our current system with CVE Ids and lots of power given to NVD is a completely broken system.</p>







<p>On August 25 2023, we get <a href="https://curl.se/mail/lib-2023-08/0031.html">an email to the curl-library mailing list</a> that informs us that “someone” has recently created a CVE, a security vulnerability identification number and report really, for a curl problem.</p>



<pre>I wanted to let you know that there's a recent curl CVE published and it doesn't look like it was acknowledged by the curl authors since it's not mentioned in the curl website: CVE-2020-19909</pre>



<p>We can’t tell who filed it. We just know that it is now there.</p>



<h2>We own our curl issues</h2>



<p>In the curl project we work hard and fierce on security and we always work with security researchers who report problems. We file our own CVEs, we document them and we make sure to tell the world about them. <a href="https://curl.se/docs/security.html">We list over 140 of them</a> with every imaginable detail about them provided. We aim at providing gold-level documentation for <em>everything</em> and that includes our past security vulnerabilities.</p>



<p>That someone else suddenly has submitted a CVE for curl is a surprise. We have not been told about this and we would <em>really</em> have liked to.  Now there is a new CVE out there reporting a curl issue and we have no details to say about it on the website. Not good.</p>



<p>I bet curl users soon would like to know the details about this.</p>



<h2>Wait 2020?</h2>



<p>The new CVE has an ID containing 2020 and that is weird. When you register a CVE you typically get it with the year you request it. Unless you get an ID for an <em>old</em> problem of the past. Is that what they did?</p>



<p>Sources seem to indicate that this was published just days ago.</p>



<h2>What is this CVE?</h2>



<p>Of course the top link when you search for this CVE is to NVD. <a href="https://daniel.haxx.se/blog/2023/06/12/nvd-damage-continued/" data-type="post" data-id="22645">Not the most reliable organization</a>, but now we can’t be too picky. On their site <a href="https://nvd.nist.gov/vuln/detail/CVE-2020-19909">they explain this</a> with very few details:</p>



<pre>Integer overflow vulnerability in tool_operate.c in curl 7.65.2 via crafted value as the retry delay.</pre>



<p>And then the craziest statement of the year. They grade it a <strong>9.8 CRITICAL</strong> issue. With 10 as a maximum, this is close to the worst case possible, right?</p>



<h2>The code</h2>



<p>Let’s pause NVD in their panic state for a moment because I immediately recognized this description. Brief as it is.</p>



<p>I spend a lot of time in the curl security team receiving reports, reviewing reports, reviewing source code, assessing claims and figuring out curl security issues.<em> I had seen this claim before!</em></p>



<p>On July 27, 2019, a Jason Lee file an <a href="https://hackerone.com/reports/661847">issue on hackerone</a>, where he reported that there was an integer overflow problem in curl’s <code>--retry-delay</code> command line option. The option accepts number of seconds and then internally converts to milliseconds by multiplying the value by 1000. The option sets how long time curl should wait until it makes a retry if the previous transfer failed with a transient error.</p>



<p>This means that on a 64 bit machine, if you write </p>



<pre>curl --retry-delay 18446744073709552 ...</pre>



<p>The number will overflow the math and instead of waiting until the end of the universe, it might retry again within the next few seconds. The above example apparently made it 384 seconds instead. On Windows, which uses 32 bit longs, you can get the problem already by asking for more than two million seconds (roughly 68 years).</p>



<p>A bug, sure. Security problem? No. I told Jason that in 2019 and then we closed the security report. I then filed <a href="https://github.com/curl/curl/pull/4166">a pull-request and fixed the bug</a>. Credits to Jason for the report. We moved on. The fix was shipped in curl 7.66.0, released in September 2019.</p>



<h2>Grading issues</h2>



<p>In previous desperate attempts from me to reason with NVD and stop their scaremongering and their grossly inflating the severity level of issues, they have insisted that they take in all publicly available data about the problem and make an assessment.</p>



<p>It was obvious already before that NVD really does not try very hard to actually understand or figure out the problem they grade. In this case it is quite impossible for me to understand how they could come up with this severity level. It’s like they saw “integer overflow” and figure that <em>wow, yeah that is the most horrible flaw we can imagine</em>, but clearly nobody at NVD engaged their brains nor looked at the “vulnerable” code or the patch that fixed the bug. Anyone that looks can see that this is not a security problem.</p>



<p>The issue listed by NVD even links to <a href="https://github.com/curl/curl/pull/4166">my pull request</a> I mention above. There is no doubt that it is the exact same bug they refer to.</p>



<h2>Spreading like a virus</h2>



<p>NVD hosts a CVE database and there is an entire world and eco system now that pulls the records from them.</p>



<p>NVD now has this CVE-2020-19909 entry in there, rated 9.8 CRITICAL and now this disinformation spreads across the world. Now when we search for this CVE number we find numerous sites that repeats the same data. “This is a 9.8 CRITICAL problem in curl” – when it is not.</p>



<h2>I will object</h2>



<p>I learned about this slap in my face just a few hours ago, but I intend to do what I can to reject this CVE.</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beating GPT-4 on HumanEval with a fine-tuned CodeLlama-34B (488 pts)]]></title>
            <link>https://www.phind.com/blog/code-llama-beats-gpt4</link>
            <guid>37267597</guid>
            <pubDate>Fri, 25 Aug 2023 22:08:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phind.com/blog/code-llama-beats-gpt4">https://www.phind.com/blog/code-llama-beats-gpt4</a>, See on <a href="https://news.ycombinator.com/item?id=37267597">Hacker News</a></p>
Couldn't get https://www.phind.com/blog/code-llama-beats-gpt4: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Fifth Circuit: Law enforcement doesn’t need warrants to search phones at border (157 pts)]]></title>
            <link>https://www.techdirt.com/2023/08/25/fifth-circuit-says-law-enforcement-doesnt-need-warrants-to-search-phones-at-the-border/</link>
            <guid>37267299</guid>
            <pubDate>Fri, 25 Aug 2023 21:32:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2023/08/25/fifth-circuit-says-law-enforcement-doesnt-need-warrants-to-search-phones-at-the-border/">https://www.techdirt.com/2023/08/25/fifth-circuit-says-law-enforcement-doesnt-need-warrants-to-search-phones-at-the-border/</a>, See on <a href="https://news.ycombinator.com/item?id=37267299">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-418955">


<h3>from the <i>never-mind-the-precedent</i> dept</h3>

<p>In 2014, <a href="https://www.techdirt.com/2014/06/25/supreme-court-says-law-enforcement-cant-search-mobile-phones-without-warrant/" data-type="link" data-id="https://www.techdirt.com/2014/06/25/supreme-court-says-law-enforcement-cant-search-mobile-phones-without-warrant/">the Supreme Court made it clear</a>: phone searches require warrants. While it did note the case involved a search “incident to an arrest,” the precedent was undeniable. If a phone search attached to an arrest requires a warrant, it would logically follow that <em>any</em> phone search by law enforcement — even those not subsequent to an arrest — requires a warrant. </p>
<p>Since then, <a href="https://www.techdirt.com/2016/04/22/court-border-search-warrant-exception-beats-riley-constitution-free-zone/" data-type="link" data-id="https://www.techdirt.com/2016/04/22/court-border-search-warrant-exception-beats-riley-constitution-free-zone/">multiple federal courts</a> have come to the opposite conclusion in cases involving searches of phones at borders or international airports. According to these judges, the <em>Riley</em> decision simply doesn’t apply when border security is in play. And it doesn’t matter whether the searched device belongs to an American citizen or a resident of a foreign country.</p>
<p>The law shouldn’t be unsettled, but it is. There’s no consensus at the appellate level. Nor is there one at the lower levels. All we have is a lack of clarity to work with. <a href="https://www.techdirt.com/2023/05/23/federal-judge-says-riley-applies-at-border-warrants-are-needed-for-some-cell-phone-searches/" data-type="link" data-id="https://www.techdirt.com/2023/05/23/federal-judge-says-riley-applies-at-border-warrants-are-needed-for-some-cell-phone-searches/">One federal judge</a> (Jed Rakoff) said warrants are needed for “some” border phone searches — specifically “forensic” searches in which the government makes itself a copy of <em>all</em> data on a person’s phone. </p>
<p>The Fourth Circuit Appeals Court also made a limited finding in favor of <em>Riley’s</em> warrant requirement, stating that border law enforcement officers must have at least articulable suspicion to engage in forensic searches. That’s still a long way from probable cause, but it’s more than the “nothing at all” standard CBP and Border Patrol officers have been held to. </p>
<p>The Seventh Circuit <a href="https://www.techdirt.com/2019/04/01/7th-circuit-punts-border-smartphone-searches-says-riley-decision-doesnt-affect-anything/" data-type="link" data-id="https://www.techdirt.com/2019/04/01/7th-circuit-punts-border-smartphone-searches-says-riley-decision-doesnt-affect-anything/">had a chance to set precedent</a> in another border device search case, but instead chose to save the question for a later day, leaving the “because border security” rationale for warrantless searches undisturbed.</p>
<p>This indecision has led to a <a href="https://www.techdirt.com/2017/08/23/border-device-searches-continue-to-increase-threatening-more-than-just-4th-amendment/" data-type="link" data-id="https://www.techdirt.com/2017/08/23/border-device-searches-continue-to-increase-threatening-more-than-just-4th-amendment/">steady increase</a> in <a href="https://www.techdirt.com/articles/20170314/08063936914/phone-searches-now-default-mode-border-more-searches-last-month-than-all-2015.shtml" data-type="link" data-id="https://www.techdirt.com/articles/20170314/08063936914/phone-searches-now-default-mode-border-more-searches-last-month-than-all-2015.shtml">border device searches</a>, driven by the ubiquity of smartphones and no one at the judicial level willing to decisively tell border officers a warrant is needed.</p>
<p>Texas immigration lawyer Adam Malik has given the Fifth Circuit a chance to set precedent more aligned with the Supreme Court’s 2014 <em>Riley</em> decision. Unfortunately, the Fifth Circuit has decided a forensic search of Malik’s device (which was held by the CBP for three weeks) isn’t a violation of the Fourth Amendment. (via <a href="http://fourthamendment.com/?p=55616" data-type="link" data-id="http://fourthamendment.com/?p=55616">FourthAmendment.com</a>)</p>
<p><a href="https://www.techdirt.com/2021/02/02/texas-immigration-lawyer-sues-dhs-cbp-over-seizure-search-his-work-phone/" data-type="link" data-id="https://www.techdirt.com/2021/02/02/texas-immigration-lawyer-sues-dhs-cbp-over-seizure-search-his-work-phone/">Malik sued the DHS and CBP</a> in early 2021, after his phone was seized, searched, and held by the CBP. One of Malik’s many concerns was the government’s warrantless access to privileged attorney-client information. This is what happened during the search Malik sued over. </p>
<blockquote>
<p><em>In response to Mr. Malik’s assertion of privilege, Officer Sullivan informed Mr. Malik that DHS was seizing the iPhone and that the digital contents would be searched. Officer Sullivan did not disconnect the iPhone from the internet or the communications network. He failed to take action that would protect the iPhone from accessing the internet or a communications network. Officer Sullivan ordered Mr. Malik to leave the deferred inspection area without the iPhone while the iPhone still was connected to the internet and a communications network.</em></p>
<p><em>Neither Officer Sullivan nor any other employee of Defendants asked Mr. Malik to disable connectivity of the iPhone to the internet or to any network. Had Officer Sullivan or any employee of Defendants offered to permit Mr. Malik to place the iPhone in airplane mode upon or after seizure of the iPhone, Mr. Malik would have done so immediately.</em></p>
</blockquote>
<p>On top of not preventing the phone from continuing to collect data and communications, the DHS held on to Malik’s phone for five months. According to this, from the Fifth Circuit’s <a href="https://s3.documentcloud.org/documents/23919671/riley-border-search-5th.pdf" data-type="link" data-id="https://s3.documentcloud.org/documents/23919671/riley-border-search-5th.pdf">decision</a> [PDF], part of that time was given over to the government’s efforts to avoid accessing privileged information. </p>
<blockquote>
<p><em>The phone’s passcode feature prevented the border officers from accessing the phone, and thus from searching it, so they sent it to a forensics lab. The lab bypassed the phone’s security features, extracted the phone’s data, and returned the phone and the data to DHS. All of that took about three months. DHS then used a “filter team” to screen the extracted data for any privileged materials. That took about two more months. Once the filter team had finished, they provided the border officers in Dallas with “two thumb drives . . . consisting of the data that the filter team determined [the officers] were authorized to search.” DHS then conducted a border search of that data, and DHS returned the phone to Malik on May 21, 2021.</em></p>
</blockquote>
<p>What’s not explained here is <em>what</em> the DHS was searching <em>for</em>. That it has the power to engage in warrantless searches of devices doesn’t automatically create reasons for it to do so. Very little was explained to Malik, other than that the officers <em>could</em> do this and, therefore, they were going to do this. All of this happened despite Malik being a government-approved member of the CBP’s Global Entry Trusted Traveler Program, which should have seen him subjected to less scrutiny when crossing borders, rather than what he was actually subjected to.</p>
<p>Following <a href="https://www.techdirt.com/2017/02/23/sen-wyden-wants-answers-new-dhs-head-introducing-legislation-to-create-warrant-requirement-border-phone-searches/" data-type="link" data-id="https://www.techdirt.com/2017/02/23/sen-wyden-wants-answers-new-dhs-head-introducing-legislation-to-create-warrant-requirement-border-phone-searches/">some</a><a href="https://www.techdirt.com/2022/03/15/sen-ron-wyden-catches-ice-illegally-collecting-americans-financial-data/" data-type="link" data-id="https://www.techdirt.com/2017/02/23/sen-wyden-wants-answers-new-dhs-head-introducing-legislation-to-create-warrant-requirement-border-phone-searches/"> </a><a href="https://www.techdirt.com/2017/02/23/sen-wyden-wants-answers-new-dhs-head-introducing-legislation-to-create-warrant-requirement-border-phone-searches/" data-type="link" data-id="https://www.techdirt.com/2017/02/23/sen-wyden-wants-answers-new-dhs-head-introducing-legislation-to-create-warrant-requirement-border-phone-searches/">hard questions</a> posed to the DHS by Senator Ron Wyden, Malik sought to obtain more information to use in his lawsuit against the agency.</p>
<blockquote>
<p><em>Discovery closed on February 11, 2022. Malik moved to reopen discovery a few weeks later, citing a public letter that United States Senator Ron Wyden sent to DHS’s Inspector General. Among other things, the letter alleges that DHS conducted “bulk surveillance of Americans’ financial records” by collecting troves of “transaction data” from Western Union. While the letter asks DHS to investigate these allegations, it does not address individual border searches, phone records, decryption, or DHS’s data-retention policies. The district court denied Malik’s motion.</em></p>
</blockquote>
<p>The court denied this motion. Then it decided in favor of the DHS, ruling it had not violated Malik’s rights with this search of his phone.</p>
<p>Unfortunately, the Fifth Circuit Appeals Court (which splits the US-Mexico border with the Ninth Circuit) agrees with the lower court. No rights violation here, not when border security is on the line.</p>
<blockquote>
<p><em>Malik argues that we should extend Riley v. California to border searches. Yet, for “[routine] cell phone searches at the border, our sister circuits have uniformly held that Riley does not require either a warrant or reasonable suspicion.” We have held the same. Even for non-routine searches, our sister circuit “have differed only as to whether reasonable</em> <em>suspicion is required.” We are not aware of any circuit court that has extended Riley’s warrant requirement to the border.</em></p>
<p><em>“Ordinarily, we would expect a party encouraging us to adopt a new constitutional [theory] to convincingly distinguish adverse authorities” and “to discuss the contours of the doctrine [he] wishes us to adopt. Malik has not done any of that. He has not even attempted to argue that the search was anything other than routine. He also has not discussed or analyzed Riley at any length, nor has he addressed the fact that “[e]very circuit that has faced this question has agreed that Riley does not mandate a warrant requirement for border searches of electronic devices, whether basic or advanced. </em></p>
<p><em>Instead, Malik has asked us to “intervene” and hold “that a judicial warrant is required at this time for the search of an attorney’s confidential client files and communications at the border.” Malik’s request for our “intervention” is itself a tacit concession that our precedent does not currently require a warrant for cell-phone searches at the border. We express no view on how the border-search exception may develop or be clarified in future cases, but we do expressly decline to address it further here.</em></p>
</blockquote>
<p>So, like the Seventh Circuit, the Fifth Circuit decides analyzing <em>Riley</em> in terms of border searches is a question for another day. And, by passing on this opportunity, it ensures the next time someone asks the same question, it will be able to refer to its previous punt to kick the constitutional can further down the road.</p>
<p>That being said, the lawsuit isn’t entirely dead. Malik also wants to ensure the DHS destroys all the data it pulled from his phone, which includes plenty of privileged material. Not only were there attorney-client communications, but likely information dealing with ongoing immigration litigation against the government — work product that is likewise shielded from government snooping.</p>
<p>The Fifth Circuit agrees Malik is right to demand this form of expungement and the DHS is wrong to refuse to destroy this seized data <em>unless</em> Malik drops his lawsuit. (All emphasis in the original.)</p>
<blockquote>
<p><em>In the district court, DHS argued that “the information is being retained <strong>only</strong> because Malik requested a litigation hold,” and that Malik cannot not rely on this self-inflicted injury to show standing. And, on appeal, DHS has represented that it will “destroy the remaining data in its possession and will be happy to provide an appropriate certification to Malik that <strong>all</strong> data in the government’s possession has been destroyed and that <strong>no data was transferred</strong> to any other governmental or nongovernmental entity or person” as soon as these “proceedings” conclude. DHS made similar representations to the district court. In other words, DHS argues that this lawsuit is the only obstacle separating Malik from the expungement that he seeks.</em></p>
<p><em>We do not agree that Malik’s injury is self-inflicted. The injury is that DHS still possesses privileged information that it unlawfully seized from his phone. Malik did not volunteer that data to DHS, and he has no control over how DHS handles it. That is why Malik came to court. DHS argues that it will delete the data if Malik non-suits this case. But while the possibility of an alternate form of relief confirms that Malik has suffered an injury, it does not mean that he caused the injury. That is especially true here, where Malik lacks any power to redress his injury. Instead, the most he can do is non-suit this case and trust DHS to delete the data. Where redress cannot be self-actuated, we are hesitant to conclude that an injury is self-inflicted.</em></p>
<p><em>We also do not agree that DHS can moot Malik’s suit merely by promising to delete the data once the suit is over. By its very nature, a promise of some future action cannot redress Malik’s injury <strong>now</strong>. DHS’s promise, then, supports no more than a prediction that this case could be moot in the future. But it is not presently moot. Rather, DHS still has Malik’s data. Just as we will not rely on “conjectural or hypothetical” facts to find that standing</em> <em>is present, so too we will not rely on predictions and what-ifs to find that standing is absent. We hold that Malik has standing to seek expungement.</em></p>
</blockquote>
<p>That’s great but there’s nothing in here for Malik, other US citizens, or their constitutional rights. When it comes to the border, the house always wins. What Malik is being given here is nothing more than existing precedent regarding expungement of privileged material. What <em>no one</em> is being given is any more protection from their own government just because they cross borders or seek to board international flights. When it comes to anything the government calls a “border,” the rights we were guaranteed are mostly null and void.</p>

<p>
Filed Under: <a href="https://www.techdirt.com/tag/4th-amendment/" rel="tag">4th amendment</a>, <a href="https://www.techdirt.com/tag/5th-circuit/" rel="tag">5th circuit</a>, <a href="https://www.techdirt.com/tag/adam-malik/" rel="tag">adam malik</a>, <a href="https://www.techdirt.com/tag/border-searches/" rel="tag">border searches</a>, <a href="https://www.techdirt.com/tag/phone-searches/" rel="tag">phone searches</a>, <a href="https://www.techdirt.com/tag/riley/" rel="tag">riley</a>, <a href="https://www.techdirt.com/tag/warrants/" rel="tag">warrants</a>
<br>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Giving up the iPad-only travel dream (227 pts)]]></title>
            <link>https://sixcolors.com/post/2023/08/why-i-gave-up-on-the-ipad-only-dream/</link>
            <guid>37266957</guid>
            <pubDate>Fri, 25 Aug 2023 20:58:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sixcolors.com/post/2023/08/why-i-gave-up-on-the-ipad-only-dream/">https://sixcolors.com/post/2023/08/why-i-gave-up-on-the-ipad-only-dream/</a>, See on <a href="https://news.ycombinator.com/item?id=37266957">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



					



<figure><img decoding="async" src="https://i0.wp.com/sixcolors.com/wp-content/uploads/2023/08/ipad-macbook-travel-6c.jpg?ssl=1" alt="MacBook and iPad" data-image-w="" data-image-h="" data-recalc-dims="1"><figcaption></figcaption></figure>
<p>Every time any of us packs a bag, we are making some very specific tech-focused decisions. It starts with what devices we need (or can live without) and cascades into charging bricks and cords and anything else that will keep us powered up and not feeling regret about having left an essential device behind.</p>
<p>I’m spending a few days this week visiting my mom, and it’s the fourth or fifth time this summer I’ve needed to pack a bag as a part of a busy travel schedule. For many years, I tried very hard to travel with only an iPad. (Why bring two devices? And I’m not leaving my iPad at home.) Since the arrival of Apple silicon, however, I’ve gone back to traveling with both an iPad and a MacBook Air.</p>
<p>As the Mac picked up speed (and the <a href="https://sixcolors.com/post/2022/07/m2-macbook-air-review-a-new-era/">M2 MacBook Air</a> packed even more power into a delightful new design), the iPad seemed to evolve slowly when it evolved at all. I’ve noticed that a lot of my colleagues who were previously working hard to integrate the iPad into their professional work have backed off, retreating to the more flexible and powerful Mac side of the house.</p>
<p>In the battle between iPad and Mac, I’m a longtime member of <a href="https://sixcolors.com/offsite/2016/02/ipad-vs-mac-im-on-team-both/">Team Both</a>—I use my Mac most of the day at my desk, but when I write elsewhere in the house or backyard, I switch to an iPad Pro in the Magic Keyboard case. And that iPad (in a regular case) is my primary computing device when I’m not in work mode.</p>
<p>I’m not at all ready to declare the “use iPad to get work done” experiment dead. With the forthcoming release of iPadOS 17, Stage Manager has thrown in a bunch of improvements that suggest the iPad’s progression to more functional status continues, albeit at a pace that’s a bit too slow for my liking.</p>
<p>But here I sit at my mother’s dining room table, typing on a MacBook Air. Something has changed in my approach to travel, and I’m trying to understand just what it is and what it tells me about the trajectory of the iPad as a productivity tool.</p>
<p>My productivity needs are clearly unlike those of most people, but the truth is that everyone’s got different productivity needs. The problem with the iPad continues to be that as it builds functionality, it has failed to build in flexibility—or at least the flexibility offered by a platform like macOS. If the iPad doesn’t support it, you’ve hit a brick wall. Your choices are to find a workaround or give up.</p>
<p>As I’ve <a href="https://sixcolors.com/post/2019/02/a-week-of-podcasting-with-only-an-ipad-pro/">written about</a> for the <a href="https://sixcolors.com/post/2016/12/recording-a-podcast-locally-on-ios-without-a-mac/">better part of a decade</a>, I’ve tried endlessly to find a solid workflow to record podcasts on the iPad. Oh, sure, there are plenty of workarounds, but the bottom line is still this: the iPad’s audio system is so inflexible that it just can’t do the job.</p>
<p>Sure, it would be swell if a utility like <a href="https://sixcolors.com/post/2022/03/audio-hijack-4-arrives-the-definitive-mac-audio-utility-just-got-better/">Audio Hijack</a> could run on the iPad. But even a simpler solution, like being able to record microphone audio in one app while simultaneously using Zoom to have a conversation, would make this approach viable. Speaking of Zoom, that app’s built-in recording feature will save a recording of your local microphone audio… on every platform <a href="https://support.zoom.us/hc/en-us/articles/201362473-Enabling-and-starting-local-recordings">except on iOS and iPadOS</a>.</p>
<p>But it’s not just podcasting. Take the <a href="https://sixcolors.com/post/2022/05/a-year-with-the-elgato-stream-deck/">Stream Deck</a>, a clever external device that lets you press buttons to kick off all sorts of tasks. I’ve come to rely on it, so much so that in a moment of weakness, I bought a <em>second</em> Stream Deck (for travel and potential use in a backup office I’m setting up) on Prime Day the other month.</p>
<p>While there’s an iPad app for Stream Deck, it’s not what you think. It turns the iPad <em>into</em> a Stream Deck, so you can tap on its screen and execute macros on a Mac or PC. If you connect a Stream Deck to an iPad directly, nothing happens. The Stream Deck software on Mac and Windows runs in the background, looks for button presses, and then runs macros. That kind of software just doesn’t fit how Apple envisions the iPad experience.</p>
<p>Sure, the iPad has the Shortcuts app, and many of my Keyboard Maestro macros really just execute shortcuts… but this very useful accessory just doesn’t work with it. How could it? Even if there were a market for it, I don’t think the platform is robust enough to support it.</p>
<p>This is where the iPad is today. It’s good enough for what it does. If it doesn’t do it, it doesn’t do it. This is the fundamental difference between the Mac (a platform that basically lets developers and users do anything they want) and the iPad (where if Apple doesn’t specifically allow it, it can’t be done).</p>
<p>The beauty of the Mac as a platform is that Apple doesn’t have to think of every use case and doesn’t have to build out every single esoteric detail in order to enable new features. It empowers developers and users to build what they need, and by extending the Mac’s functionality, they incrementally increase the Mac’s value as a computing platform.</p>
<p>On the iPad, advancement doesn’t work like that. Instead, it’s decided in various meetings inside Apple where specific features will get prioritized or deprioritized for the next operating system cycle. Once every year or two, we will hear about some (legitimately exciting!) new features that will extend the usability of the platform. And that will basically be it. The waiting begins again.</p>
<p>I’m tired of waiting. I’m tired of creating more inconvenience for myself in order to push the iPad past the boundaries Apple has set for it. For the cost of an extra 2.75 pounds in my backpack, I can travel with a MacBook Air knowing that more or less anything I need to do while on the road can be accomplished without requiring weird workarounds or risking a catastrophic tech failure.</p>
<p>I <em>want</em> to do it all on my iPad. I hope that one day I’ll be able to. But for now, I’m done pushing the envelope. Apple will determine what I can do with my iPad, and when that changes, I’m sure they’ll let me know. Until then, all any of us can do is wait.</p>





	<p><strong>If you appreciate articles like this one, support us by <a href="https://sixcolors.com/subscribe/">becoming a Six Colors subscriber</a>. Subscribers get access to an exclusive podcast, members-only stories, and a special community.</strong></p>

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Slime Molds [video] (114 pts)]]></title>
            <link>https://www.youtube.com/watch?v=gpt9cJrEZ_Y</link>
            <guid>37265664</guid>
            <pubDate>Fri, 25 Aug 2023 18:48:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=gpt9cJrEZ_Y">https://www.youtube.com/watch?v=gpt9cJrEZ_Y</a>, See on <a href="https://news.ycombinator.com/item?id=37265664">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Big Ass Data Broker Opt-Out List (145 pts)]]></title>
            <link>https://github.com/yaelwrites/Big-Ass-Data-Broker-Opt-Out-List</link>
            <guid>37265346</guid>
            <pubDate>Fri, 25 Aug 2023 18:25:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/yaelwrites/Big-Ass-Data-Broker-Opt-Out-List">https://github.com/yaelwrites/Big-Ass-Data-Broker-Opt-Out-List</a>, See on <a href="https://news.ycombinator.com/item?id=37265346">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Big Ass Data Broker Opt-Out List</h2>
<table>
<thead>
<tr>
<th>Symbols</th>
<th>Meanings</th>
</tr>
</thead>
<tbody>
<tr>
<td>☠</td>
<td>high priority</td>
</tr>
<tr>
<td>🎫</td>
<td>requires driver’s license (cross out your ID #!)</td>
</tr>
<tr>
<td>📫</td>
<td>must use snail mail</td>
</tr>
<tr>
<td>💰</td>
<td>site charges money for access or removal (whaaaat?)</td>
</tr>
</tbody>
</table>
<p dir="auto">This list was started on September 29, 2017 and was most recently updated in May 2023 to add information on sites that require you to click links sent via email or to receive an automated call and enter a four-digit number on your phone in order to complete an opt-out request.</p>
<p dir="auto">Please send corrections or updates to <a href="mailto:yael@yaelwrites.com">yael@yaelwrites.com</a>, or file a pull request. Screenshots in emails are incredibly helpful. I will add opt-outs where users can verify that the data broker has their information before providing it, and where removal is not limited to GDPR/CCPA/etc.</p>
<p dir="auto"><em>Disclaimers: Some of these opt-outs take a long time to go through. Sometimes, information is pulled from other sources and you’ll need to opt out multiple times for the same site. Data brokers come and go (and are bought out by others), and they also often change their opt-out pages. I try to update this ~every six months, but it’s not always current. Finally, even opting out of these sites doesn’t mean that your address is secure. In many states, real estate data and voter registration information is public (or easy to obtain). And, of course, location data can be found by physical means (e.g., following you home) and through other people who know it (i.e., social engineering). That said, removing your home address from data broker sites can significantly lower your attack surface and make it harder for people to find it. 💕</em></p>
<p dir="auto">CC BY-NC-SA: This license allows reusers to distribute, remix, adapt, and build upon the material in any medium or format for noncommercial purposes only, and only so long as attribution is given to the creator. If you remix, adapt, or build upon the material, you must license the modified material under identical terms.</p>
<h2 tabindex="-1" dir="auto">Search Engines</h2>
<p dir="auto">If you've been doxed, you can remove your contact info from Google Search and Bing.</p>
<p dir="auto"><a href="https://support.google.com/websearch/troubleshooter/3111061" rel="nofollow">https://support.google.com/websearch/troubleshooter/3111061</a></p>
<p dir="auto"><a href="https://www.microsoft.com/en-ca/concern/bing" rel="nofollow">https://www.microsoft.com/en-ca/concern/bing</a></p>
<p dir="auto">Google further allows you to remove non-consensual or intimate personal images, involuntary fake pornography, irrelevant pornography from Google search results for your name, content about you on sites with exploitative removal practices, and several other categories of information.</p>
<h2 tabindex="-1" dir="auto">People Search Sites</h2>
<h3 tabindex="-1" dir="auto">☠ Acxiom</h3>
<p dir="auto">It is hard to find your own info for free, but people in some countries can opt out using the following links:</p>
<p dir="auto"><a href="https://gdpr-fr.eu.acxiom.com/" rel="nofollow">https://gdpr-fr.eu.acxiom.com/</a> (France)</p>
<p dir="auto"><a href="https://marketing.acxiom.com/AcxiomDE-Verbraucheranfragen.html" rel="nofollow">https://marketing.acxiom.com/AcxiomDE-Verbraucheranfragen.html</a> (Germany)</p>
<p dir="auto"><a href="https://gdpr-it.eu.acxiom.com/" rel="nofollow">https://gdpr-it.eu.acxiom.com</a> (Italy)</p>
<p dir="auto"><a href="https://gdpr-es.eu.acxiom.com/" rel="nofollow">https://gdpr-es.eu.acxiom.com</a>(Spain)</p>
<p dir="auto"><a href="https://isapps.acxiom.com/optout/optout.aspx#section8" rel="nofollow">https://isapps.acxiom.com/optout/optout.aspx#section8</a> (U.S.)</p>
<p dir="auto">Users in Argentina, Australia, Canada and Mexico may email <a href="mailto:Consumer.Privacy@acxiom.com">Consumer.Privacy@acxiom.com</a>.</p>
<p dir="auto">Users in Austria, India and Switzerland may email <a href="mailto:Datenschutz@acxiom.com">Datenschutz@acxiom.com</a>.</p>
<p dir="auto">If you don’t have an email address, call (877) 774-2094 and follow the prompts.</p>
<h3 tabindex="-1" dir="auto">☠ Advanced Background Checks</h3>
<p dir="auto">Search for your information here: <a href="https://www.advancedbackgroundchecks.com/" rel="nofollow">https://www.advancedbackgroundchecks.com/</a>. (Remember that you can leave your city and state and your age blank.) If your information shows up, remove your data here: <a href="https://www.advancedbackgroundchecks.com/removal" rel="nofollow">https://www.advancedbackgroundchecks.com/removal</a></p>
<h3 tabindex="-1" dir="auto">Archives</h3>
<p dir="auto">To find your information, you must sign up for a free trial here:</p>
<p dir="auto"><a href="https://www.archives.com/search/ancestor/checkout" rel="nofollow">https://www.archives.com/search/ancestor/checkout</a>(and then cancel)</p>
<p dir="auto">Opt out here: <a href="http://www.archives.com/Optout" rel="nofollow">http://www.archives.com/Optout</a></p>
<h3 tabindex="-1" dir="auto">🎫 BackgroundAlert</h3>
<p dir="auto">Find your information and opt out here: <a href="https://www.backgroundalert.com/optout/" rel="nofollow">https://www.backgroundalert.com/optout/</a>.</p>
<h3 tabindex="-1" dir="auto">☠ BeenVerified</h3>
<p dir="auto">Find your information and opt out here: <a href="https://www.beenverified.com/app/optout/search" rel="nofollow">https://www.beenverified.com/app/optout/search</a>. Opt out of property search here: <a href="https://www.beenverified.com/app/optout/address-search" rel="nofollow">https://www.beenverified.com/app/optout/address-search</a>. BeenVerified also owns PeopleLooker and PeopleSmart. Be aware that BeenVerified only allows one opt-out per email address, so you may need to reach out via email for additional opt-outs. You will need to confirm your opt-out request by clicking on a link sent to you via email.</p>
<h3 tabindex="-1" dir="auto">CheckPeople</h3>
<p dir="auto">Look for your contact info here: <a href="https://checkpeople.com/" rel="nofollow">https://checkpeople.com/</a>. If it shows up, opt out here: <a href="https://checkpeople.com/do-not-sell-info" rel="nofollow">https://checkpeople.com/do-not-sell-info</a>. You will need to fill out a form and captcha, select the record for removal, fill out your name and email, and solve another captcha. You will receive an email which will require you to confirm your opt-out request.</p>
<h3 tabindex="-1" dir="auto">Classmates.com</h3>
<p dir="auto">Cancel your free or paid membership following the instructions here:
<a href="https://help.classmates.com/hc/en-us/articles/115002224171-How-can-I-cancel-my-membership-" rel="nofollow">https://help.classmates.com/hc/en-us/articles/115002224171-How-can-I-cancel-my-membership-</a>.</p>
<h3 tabindex="-1" dir="auto">ClustrMaps</h3>
<p dir="auto">Find your information here: <a href="https://clustrmaps.com/" rel="nofollow">https://clustrmaps.com/</a>. To remove it, go here: <a href="https://clustrmaps.com/bl/opt-out" rel="nofollow">https://clustrmaps.com/bl/opt-out</a>. It will ask you for a mailing address, but doesn’t seem to verify it. It will then ask you to check off anything associated with your account that you want removed.</p>
<h3 tabindex="-1" dir="auto">CocoFinder</h3>
<p dir="auto">Look for your information here: <a href="https://cocofinder.com/search" rel="nofollow">https://cocofinder.com/search</a>. Opt out here: <a href="https://cocofinder.com/remove-my-info" rel="nofollow">https://cocofinder.com/remove-my-info</a>. It will send you to this Google form: <a href="https://docs.google.com/forms/d/e/1FAIpQLScEgLH9ro7oQR4c0V9E19ug1Gb7wVMrFFSj0Sgx4sjba_ET4Q/viewform" rel="nofollow">https://docs.google.com/forms/d/e/1FAIpQLScEgLH9ro7oQR4c0V9E19ug1Gb7wVMrFFSj0Sgx4sjba_ET4Q/viewform</a>.</p>
<h3 tabindex="-1" dir="auto">CyberBackgroundChecks</h3>
<p dir="auto">Find your information here: <a href="https://www.cyberbackgroundchecks.com/" rel="nofollow">https://www.cyberbackgroundchecks.com/</a>. If it’s listed, remove it here. <a href="https://www.cyberbackgroundchecks.com/removal" rel="nofollow">https://www.cyberbackgroundchecks.com/removal</a>. You will need to confirm by clicking a link sent via email.</p>
<h3 tabindex="-1" dir="auto">Dataveria</h3>
<p dir="auto">Find your information here: <a href="https://dataveria.com/" rel="nofollow">https://dataveria.com/</a>. Opt out by entering the specific profile URL you find here: <a href="https://dataveria.com/ng/control/privacy" rel="nofollow">https://dataveria.com/ng/control/privacy</a>. You will need to confirm your opt-out request by clicking on a link sent to you via email.</p>
<h3 tabindex="-1" dir="auto">☠ FamilyTreeNow</h3>
<p dir="auto">Search for yourself and remove your information here:</p>
<p dir="auto"><a href="https://www.familytreenow.com/optout" rel="nofollow">https://www.familytreenow.com/optout</a></p>
<h3 tabindex="-1" dir="auto">☠ FastPeopleSearch</h3>
<p dir="auto">Look for and remove your information here: <a href="https://www.fastpeoplesearch.com/removal" rel="nofollow">https://www.fastpeoplesearch.com/removal</a>. You may need to solve a captcha.</p>
<h3 tabindex="-1" dir="auto">FreePeopleDirectory</h3>
<p dir="auto">Search for your information at <a href="https://www.freepeopledirectory.com/" rel="nofollow">https://www.freepeopledirectory.com</a>. If you find it, you can opt out on Spokeo at <a href="https://www.spokeo.com/privacy/control" rel="nofollow">https://www.spokeo.com/privacy/control</a>. Make sure to scroll down to the grammatically incorrect “opt out your information” section.</p>
<h3 tabindex="-1" dir="auto">Glad I Know</h3>
<p dir="auto">Find your information here: <a href="https://gladiknow.com/" rel="nofollow">https://gladiknow.com/</a>. Follow these instructions to remove your data: <a href="https://gladiknow.com/opt-out" rel="nofollow">https://gladiknow.com/opt-out</a>. You can remove your information without downloading your free report, and email <a href="mailto:support@gladiknow.com">support@gladiknow.com</a> for assistance.</p>
<h3 tabindex="-1" dir="auto">Homemetry</h3>
<p dir="auto">Look for your home address here: <a href="https://homemetry.com/" rel="nofollow">https://homemetry.com/</a>. If your name is listed alongside your address, click on the “information control” link. Opt out of applicable phone numbers and names. There's a limit per each opt out and each email you use to opt out. You will need to click the verification link they email you to finalize the opt out.</p>
<h3 tabindex="-1" dir="auto">IDTrue</h3>
<p dir="auto">Find your information and out here: <a href="https://www.idtrue.com/optout/" rel="nofollow">https://www.idtrue.com/optout/</a></p>
<h3 tabindex="-1" dir="auto">InfoTracer</h3>
<p dir="auto">Find your information here: <a href="https://www.infotracer.com/" rel="nofollow">https://www.infotracer.com/</a>. You can opt out by filling out this form: <a href="https://www.infotracer.com/optout" rel="nofollow">https://www.infotracer.com/optout</a>. You can also mail in this form: <a href="https://members.infotracer.com/tspec/shared/assets/data_opt_out_form.pdf" rel="nofollow">https://members.infotracer.com/tspec/shared/assets/data_opt_out_form.pdf</a> or fax it to 1-617-933-9946.</p>
<h3 tabindex="-1" dir="auto">☠ Intelius</h3>
<p dir="auto">Find your information here: <a href="https://www.intelius.com/" rel="nofollow">https://www.intelius.com</a>. Opt out here: <a href="https://www.intelius.com/suppression-center" rel="nofollow">https://www.intelius.com/suppression-center</a>. You will need to enter your birthdate and any information you want suppressed, and you’ll also have to confirm your email address and enter a verification code emailed to you. (Intelius owns Addresses.com, Addresssearch.com, Anywho, Classmates, DateCheck, Instant Checkmate, InstantPeopleFinder, iSearch, LookUpAnyone, Peopleconnect.us, PeopleFinder, PeopleLookup, Phonesbook, Publicrecords, Spock, US Search, and Zabasearch.) If you have trouble opting out on the website, email <a href="mailto:privacy@intelius.com">privacy@intelius.com</a>. If the person you're emailing says they can't find your account, you may need to confirm your mailing address, phone number, old email addresses, or other information via email. (As always, make sure Intelius already has the information before providing it.</p>
<h3 tabindex="-1" dir="auto">LocatePeople</h3>
<p dir="auto">Find your information and Record ID here: <a href="https://www.locatepeople.org/" rel="nofollow">https://www.locatepeople.org/</a>. Opt out here with your Record ID: <a href="https://www.locatepeople.org/optout/" rel="nofollow">https://www.locatepeople.org/optout/</a></p>
<h3 tabindex="-1" dir="auto">☠ 📞 MyLife</h3>
<p dir="auto">Find your information here: <a href="https://www.mylife.com/" rel="nofollow">https://www.mylife.com</a>.
Opt out using the following URL: <a href="https://www.mylife.com/ccpa/index.pubview" rel="nofollow">https://www.mylife.com/ccpa/index.pubview</a>. Although this is CCPA-focused, non-California folks have successfully removed their profiles. However, another option is to email your removal request to <a href="mailto:privacy@mylife.com">privacy@mylife.com</a> with your name and a link to your profile. MyLife will try to get you to create an account with a copy of your driver’s license to submit a profile removal request, and we’ve received reports that emailing the <a href="mailto:info@mylife.com">info@mylife.com</a> or <a href="mailto:removalrequests@mylife.com">removalrequests@mylife.com</a> email addresses will require a phone call to&nbsp;(888) 704-1900. Previously, calling that number and pressing 2 allowed you to be removed from MyLife (and Wink.com) after providing your name, age, date of birth, email address, current mailing address, and a previous mailing address, but this no longer appears to be the case. (It also appears that you can sign up for a free membership to remove your reputation profile, though not your personal information such as your home address.)</p>
<h3 tabindex="-1" dir="auto">NeighborReport</h3>
<p dir="auto">Search for your name, address or phone number at <a href="https://neighbor.report/" rel="nofollow">https://neighbor.report/</a>. To remove a report, go to <a href="https://neighbor.report/remove" rel="nofollow">https://neighbor.report/remove</a>. You will need to verify the opt-out request via email.</p>
<h3 tabindex="-1" dir="auto">Nuwber</h3>
<p dir="auto">Find your information here: <a href="https://nuwber.com/" rel="nofollow">https://nuwber.com/</a>. Enter the URL of the profile page you found and your email address here: <a href="https://nuwber.com/removal/link" rel="nofollow">https://nuwber.com/removal/link</a>. You will need to confirm your opt-out request by clicking on a link sent to you via email. Your profile will look like this "<a href="https://nuwber.com/person/563a151aa219445d5225ff38" rel="nofollow">https://nuwber.com/person/563a151aa219445d5225ff38</a>." Be careful not to accidentally click on a Truthfinder link (conveniently up top). You will need to confirm your opt-out request by clicking on a link sent to you via email.</p>
<h3 tabindex="-1" dir="auto">☠ PeekYou</h3>
<p dir="auto">Find your information here:<a href="https://www.peekyou.com/" rel="nofollow">https://www.peekyou.com/</a>, opt out here: <a href="https://www.peekyou.com/about/contact/optout" rel="nofollow">https://www.peekyou.com/about/contact/optout</a>. You will need the letters at the end of your profile’s URL in the "unique ID" field. Make sure to check "remove my entire listing." You can also remove your information from the sites that it says it has aggregated information from.</p>
<h3 tabindex="-1" dir="auto">PeopleByName</h3>
<p dir="auto">Look up your name here: <a href="http://www.peoplebyname.com/people" rel="nofollow">http://www.peoplebyname.com/people</a>. Or, you can fill in your name in the URL like this: <a href="http://www.peoplebyname.com/people/%5BLast_Name%5D/%5BFirst_Name%5D" rel="nofollow">http://www.peoplebyname.com/people/[Last_Name]/[First_Name]</a>. Opt out for each record here: <a href="http://www.peoplebyname.com/remove.php" rel="nofollow">http://www.peoplebyname.com/remove.php</a>. This site, unfortunately, does not support HTTPS.</p>
<h3 tabindex="-1" dir="auto">☠ PeopleFinders</h3>
<p dir="auto">Find your information here: <a href="https://www.peoplefinders.com/" rel="nofollow">https://www.peoplefinders.com/</a> and opt out here:<a href="https://www.peoplefinders.com/opt-out" rel="nofollow">https://www.peoplefinders.com/opt-out</a> (includes Advanced People Search and PublicRecordsNow). The opt-out link also includes information on finding your profile, after you hit next. You will need to confirm your opt-out request by clicking on a link sent to you via email.</p>
<h3 tabindex="-1" dir="auto">PeopleSearchNow</h3>
<p dir="auto">Find your information and opt out here: <a href="https://www.peoplesearchnow.com/opt-out" rel="nofollow">https://www.peoplesearchnow.com/opt-out</a>.</p>
<h3 tabindex="-1" dir="auto">PeopleSearchSite</h3>
<p dir="auto">PLook for your information here: <a href="https://www.peoplesearchsite.com/" rel="nofollow">https://www.peoplesearchsite.com</a>. Opt out by clicking the link at the bottom of the profile you want to remove. You can also try emailing <a href="mailto:info@peoplesearchsite.com">info@peoplesearchsite.com</a>.</p>
<h3 tabindex="-1" dir="auto">Persopo</h3>
<p dir="auto">Search for your information here: <a href="https://persopo.com/search" rel="nofollow">https://persopo.com/search</a>. Cut and paste the text of it (which is hard to do because of the 1990s site format, but is required by the site) and email it along with your opt-out request to<a href="mailto:support@persopo.com">support@persopo.com</a>. You can also mail your request to Persopo.com,
Attention: Opt Out Dept., 848 N. Rainbow Blvd., Suite 20, Las Vegas, NV 89107. (Opt-out instructions are posted here: <a href="http://info.persopo.com/opt-out.html" rel="nofollow">http://info.persopo.com/opt-out.html</a>—this site unfortunately does not use HTTPS).</p>
<h3 tabindex="-1" dir="auto">☠ Pipl</h3>
<p dir="auto">Go to <a href="https://pipl.com/" rel="nofollow">https://pipl.com</a> to find your data. Opt out from Pipl at <a href="https://pipl.com/personal-information-removal-request" rel="nofollow">https://pipl.com/personal-information-removal-request</a>. Then, remove your information from any sites it’s aggregated from. You can also email support at [<a href="mailto:support@pipl.com">support@pipl.com</a>] (<a href="mailto:support@pipl.com">mailto:support@pipl.com</a>).</p>
<h3 tabindex="-1" dir="auto">Private Eye</h3>
<p dir="auto">Look for your info here: <a href="https://www.privateeye.com/static/view/optout/" rel="nofollow">https://www.privateeye.com/</a> Opt out here: <a href="https://www.privateeye.com/static/view/optout/" rel="nofollow">https://www.privateeye.com/static/view/optout/</a>. You can also skip the first part and head to the second.</p>
<h3 tabindex="-1" dir="auto">PrivateRecords</h3>
<p dir="auto">Search for your name and state and opt out here: <a href="https://www.privaterecords.net/optOut/name/landing" rel="nofollow">https://www.privaterecords.net/optOut/name/landing</a>.</p>
<h3 tabindex="-1" dir="auto">PublicDataUSA</h3>
<p dir="auto">Find your information here: <a href="https://www.publicdatausa.com/optout.php" rel="nofollow">https://www.publicdatausa.com/optout.php</a>. Then, select the items you’d like removed, and fill out the form at the bottom of the page.</p>
<h3 tabindex="-1" dir="auto">PublicRecordsNow</h3>
<p dir="auto">Find your information here: [<a href="https://www.publicrecordsnow.com/" rel="nofollow">https://www.publicrecordsnow.com/</a>]. Click on the orange “click here to opt out” button on  <a href="https://www.publicrecordsnow.com/static/view/contact/" rel="nofollow">https://www.publicrecordsnow.com/static/view/contact/</a>, fill out the form, complete the captcha and send it in.</p>
<h3 tabindex="-1" dir="auto">☠ Radaris</h3>
<p dir="auto">Find your information here: <a href="http://radaris.com/" rel="nofollow">http://radaris.com</a>. Click on "control information" and "remove information" by clicking on the X next to it or by removing specific listings. You can also privatize your account. You unfortunately need to register to remove information, and are limited to removing six items at a time. (I recommend removing items even if you privatize your account because these settings don’t always stick.) Details are here: <a href="http://radaris.com/page/how-to-remove" rel="nofollow">http://radaris.com/page/how-to-remove</a>. You will need to confirm your opt-out request by clicking on a link sent to you via email.</p>
<h3 tabindex="-1" dir="auto">Rehold</h3>
<p dir="auto">Look up your home address here: <a href="https://rehold.com/" rel="nofollow">https://rehold.com/</a>. If there is a number or name next to it you’d like removed, click on  the red “remove” button next to it. You will need to enter your name and email address and solve a captcha.</p>
<h3 tabindex="-1" dir="auto">💰 SearchBug</h3>
<p dir="auto">Unfortunately, you have to sign up with a credit card to search for your information at <a href="https://www.searchbug.com/" rel="nofollow">https://www.searchbug.com</a>, though your first search is free. You can contact SearchBug using the contact form or chat link on this page <a href="https://www.searchbug.com/contact-us.aspx" rel="nofollow">https://www.searchbug.com/contact-us.aspx</a> or via phone or text at (760) 652-4050 or via fax at (760) 454-7341 to to ask them to block the display of your personal information.</p>
<h3 tabindex="-1" dir="auto">SearchPeopleFree</h3>
<p dir="auto">Find your information here: <a href="https://www.searchpeoplefree.com/" rel="nofollow">https://www.searchpeoplefree.com/</a>, making sure to search by name, phone number, address and email address: Opt out here: <a href="https://www.searchpeoplefree.com/opt-out" rel="nofollow">https://www.searchpeoplefree.com/opt-out</a>.</p>
<h3 tabindex="-1" dir="auto">SearchQuarry</h3>
<p dir="auto">Find your information here: <a href="https://www.searchquarry.com/" rel="nofollow">https://www.searchquarry.com/</a>. If it’s listed, opt out here: <a href="https://www.searchquarry.com/opt-out/" rel="nofollow">https://www.searchquarry.com/opt-out/</a> You will need to click the verification link in your inbox to finalize the opt out.</p>
<h3 tabindex="-1" dir="auto">SmartBackground Checks</h3>
<p dir="auto">Find your information here: <a href="https://www.smartbackgroundchecks.com/" rel="nofollow">https://www.smartbackgroundchecks.com/</a>. Opt out here: <a href="https://www.smartbackgroundchecks.com/optout" rel="nofollow">https://www.smartbackgroundchecks.com/optout</a></p>
<h3 tabindex="-1" dir="auto">Social Catfish</h3>
<p dir="auto">Find your information here: <a href="https://socialcatfish.com/" rel="nofollow">https://socialcatfish.com/</a>. Record the URL of each page that has your information. Then, go to <a href="https://socialcatfish.com/" rel="nofollow">https://socialcatfish.com/opt-out/</a> and scroll down to the instructions for removing URLs, email, phone number, and address. (Make sure to only ask for information to be removed if you already see it on the site.) You’ll need to provide your email address and click on a confirmation link to complete the opt-out process.</p>
<h3 tabindex="-1" dir="auto">Spoke</h3>
<p dir="auto">Find your information here: <a href="http://www.spoke.com/" rel="nofollow">http://www.spoke.com/</a>. Find the “people site map” on the bottom, and it’ll take you here (with a different initial at the end, depending on your last name): <a href="https://www.spoke.com/dir/people/name/a" rel="nofollow">https://www.spoke.com/dir/people/name/a</a>. Click the links to search for your last name. Then do the same for your first name. You will get to a results page. Click each name listing, and suppress it by clicking the "Request Removal" link at the bottom of the page, entering the URL in the contact form it takes you to, and providing your email address. You can also email your removal request to <a href="mailto:feedback@readwritelabs.com">feedback@readwritelabs.com</a>. Unfortunately, Spoke’s opt-out page is working—its link from the FAQ page is dead.</p>
<h3 tabindex="-1" dir="auto">☠ Spokeo</h3>
<p dir="auto">Find your information here <a href="https://www.spokeo.com/search" rel="nofollow">https://www.spokeo.com/search</a> and remove your information by entering the URL of the profile you want removed and your email address here: <a href="https://www.spokeo.com/optout" rel="nofollow">https://www.spokeo.com/optout</a>. Make sure to scroll down to the grammatically incorrect “opt out your information” section. You will need to confirm your opt-out request by clicking on a link sent to you via email.</p>
<h3 tabindex="-1" dir="auto">SpyFly</h3>
<p dir="auto">Look for your data here: [<a href="https://www.spyfly.com](" rel="nofollow">https://www.spyfly.com](</a><a href="https://www.spyfly.com/" rel="nofollow">https://www.spyfly.com</a>. Opt out here: <a href="https://www.spyfly.com/help-center/remove-my-public-record" rel="nofollow">https://www.spyfly.com/help-center/remove-my-public-record</a>. You can also try emailing <a href="mailto:support@spyfly.com">support@spyfly.com</a>. You’ll need to provide your name, age, address, and email address. Make sure SpyFly has information you'd like removed before volunteering these things.</p>
<h3 tabindex="-1" dir="auto">TruePeopleSearch</h3>
<p dir="auto">Opt out here: <a href="https://www.truepeoplesearch.com/removal" rel="nofollow">https://www.truepeoplesearch.com/removal</a>.</p>
<h3 tabindex="-1" dir="auto">That’s Them</h3>
<p dir="auto">Find your information here: <a href="https://thatsthem.com/" rel="nofollow">https://thatsthem.com/</a>, if it shows up, try to opt out here: <a href="https://thatsthem.com/optout" rel="nofollow">https://thatsthem.com/optout</a>. Do not click on the Spokeo identity theft protection link, as it is a paid product.</p>
<h3 tabindex="-1" dir="auto">☠ TruthFinder</h3>
<p dir="auto">First, request your public data at <a href="https://www.truthfinder.com/suppression-center" rel="nofollow">https://www.truthfinder.com/suppression-center</a>. Then, fill out a suppression request form. You can also send your removal request in the mail to TruthFinder, attn: OptOut, 2534 State St., Suite 473, San Diego, CA 92101. If you find your information after you have opted out, contact Truthfinder at (800) 699-8081 or <a href="mailto:help@truthfinder.com">help@truthfinder.com</a>. You can also contact TruthFinder by filling out a web form at <a href="https://www.truthfinder.com/contact/" rel="nofollow">https://www.truthfinder.com/contact/</a>. If you are working through this list, you may have already removed this data at Intelius’ suppression center site.</p>
<h3 tabindex="-1" dir="auto">United States Phone Book</h3>
<p dir="auto">Search for your information here: <a href="https://www.unitedstatesphonebook.com/search.php" rel="nofollow">https://www.unitedstatesphonebook.com/search.php</a>.
If your address is listed and you want it removed, enter the telephone number and zip code listed on the above site in this page <a href="https://www.unitedstatesphonebook.com/contact.php" rel="nofollow">https://www.unitedstatesphonebook.com/contact.php</a>.</p>
<h3 tabindex="-1" dir="auto">Unmask</h3>
<p dir="auto">Look for your data here: <a href="https://unmask.com/" rel="nofollow">https://unmask.com/</a>.
Opt out here: <a href="https://unmask.com/opt-out/" rel="nofollow">https://unmask.com/opt-out/</a>.</p>
<h3 tabindex="-1" dir="auto">USA People Search</h3>
<p dir="auto">Find and remove your information at <a href="https://www.usa-people-search.com/manage" rel="nofollow">https://www.usa-people-search.com/manage</a>.</p>
<h3 tabindex="-1" dir="auto">US Search</h3>
<p dir="auto">First, you must request your public data. Then you can apply to have the information removed. The form for both is here: <a href="https://www.ussearch.com/suppression-center/" rel="nofollow">https://www.ussearch.com/suppression-center/</a>. If you are working through this list, you may have already removed this data at Intelius’ suppression center site.</p>
<h3 tabindex="-1" dir="auto">☠ USPhoneBook</h3>
<p dir="auto">Look for your number and opt out here:<a href="https://www.usphonebook.com/opt-out/" rel="nofollow">https://www.usphonebook.com/opt-out/</a>.</p>
<h3 tabindex="-1" dir="auto">Verecor</h3>
<p dir="auto">Find your information at <a href="https://verecor.com/" rel="nofollow">https://verecor.com</a>. To opt out, enter the profile URL (along with your name and email) at <a href="https://verecor.com/ng/control/privacy" rel="nofollow">https://verecor.com/ng/control/privacy</a>. You will need to fill out a captcha, and confirm the email link to finalize your request. If it doesn’t go through, you may need to email <a href="mailto:assist@verecor.com">assist@verecor.com</a>.</p>
<h3 tabindex="-1" dir="auto">Vericora</h3>
<p dir="auto">Find your information here: <a href="https://vericora.com/profile/search" rel="nofollow">https://vericora.com/profile/search</a>. Opt out here: <a href="https://vericora.com/ng/control/privacy" rel="nofollow">https://vericora.com/ng/control/privacy</a>.</p>
<h3 tabindex="-1" dir="auto">VoterRecords</h3>
<p dir="auto">Go to <a href="https://voterrecords.com/" rel="nofollow">VoterRecords.com</a>, find your information, scroll to the bottom of the page and click on the "record opt-out" link. This will lead to an opt-out form, which you’ll need to submit. You may receive a verification link you’ll need to click on to verify your opt-out request.</p>
<h3 tabindex="-1" dir="auto">☠ 📞 💰 White Pages</h3>
<p dir="auto">Find your information here: <a href="https://www.whitepages.com/" rel="nofollow">https://www.whitepages.com/</a>, and then opt out here: <a href="https://www.whitepages.com/suppression_requests" rel="nofollow">https://www.whitepages.com/suppression_requests</a>. You will need to provide a phone number and enter an opt-out code when you receive the phone call. Opt-out information is available here: <a href="https://support.whitepages.com/hc/en-us/articles/115010106908-How-do-I-edit-or-remove-a-personal-listing" rel="nofollow">https://support.whitepages.com/hc/en-us/articles/115010106908-How-do-I-edit-or-remove-a-personal-listing</a>. You can also send a request here <a href="https://support.whitepages.com/hc/en-us/requests/new" rel="nofollow">https://support.whitepages.com/hc/en-us/requests/new</a>. (Including multiple options because these sites have a tendency to disappear.) Sometimes, information is removed from White Pages but not White Pages Premium, so make sure to double-check for your listing. Unfortunately, it may not be possible to opt out of White Page Premium pages without an account. White Pages also owns <a href="https://www.411.com/" rel="nofollow">411.com</a>, so check for your information there. The opt-out is the same. You may need to verify your opt-out by receiving an automated voice call and entering a four-digit code into your phone.</p>
<h3 tabindex="-1" dir="auto">Xlek</h3>
<p dir="auto">(formerly Cubib?) Look up your information at <a href="https://www.xlek.com/" rel="nofollow">https://www.xlek.com/</a>. Opt out at <a href="https://www.xlek.com/optout.php" rel="nofollow">https://www.xlek.com/optout.php</a>. You’ll need to submit a separate opt-out request for each item you’d like removed.</p>
<h3 tabindex="-1" dir="auto">ZoomInfo</h3>
<p dir="auto">Check to see if your information is on ZoomInfo by filling out this form:<a href="https://www.zoominfo.com/update/remove" rel="nofollow">https://www.zoominfo.com/update/remove</a>. Opt out if it has an email on record—they will email you a code. ZoomInfo appears to have moved towards focusing primarily on business rather than individuals.</p>
<h2 tabindex="-1" dir="auto">Special Circumstances</h2>
<p dir="auto">If you’re a victim of violent crime or identity theft, it’s often possible to opt out of information that others can’t opt out of. You may, however, need to provide court orders or other information. It may be worth checking more complete data broker lists which include those: <a href="https://www.privacyrights.org/data-brokers" rel="nofollow">https://www.privacyrights.org/data-brokers</a> See this resource: <a href="https://nnedv.org/mdocs-posts/people-searches-data-brokers/" rel="nofollow">https://nnedv.org/mdocs-posts/people-searches-data-brokers/</a>.</p>
<p dir="auto">If you are experiencing or have experienced domestic violence, sexual abuse, or stalking, you may be able to keep your address private through state programs that provide a substitute address and mail forwarding. To see if a program exists in your state, look up “address confidentiality program” along with the name of your state in a search engine.</p>
<p dir="auto">If you are a healthcare worker with a National Provider Index (NPI) number, your personal cell phone number or home address may be showing up on various online directories and websites because they were listed as primary contact information when you filled out your NPI profile (or when someone else filled it out on your behalf).
Look up your information here: <a href="https://opennpi.com/provider/" rel="nofollow">https://opennpi.com/provider/</a>. You can update or remove information by filling out this form: <a href="https://opennpi.com/contact" rel="nofollow">https://opennpi.com/contact</a>. You’ll also need to reach out to any sites aggregating this data, like Doximity, where you may be able to <a href="https://support.doximity.com/hc/en-us/articles/9349631650323-How-to-Update-Edit-Your-Profile-on-the-Doximity-Website" rel="nofollow">edit</a> or <a href="https://support.doximity.com/hc/en-us/articles/360039340393-How-to-Delete-Your-Doximity-Profile" rel="nofollow">delete</a> your profile.</p>
<h2 tabindex="-1" dir="auto">Preventing Identity Theft &amp; Opting Out of Marketing Sites</h2>
<h3 tabindex="-1" dir="auto">📞 Consider freezing your credit, which prevents creditors from accessing your credit report. (It also prevents credit, loans, and services from being approved in your name, but you will receive a pin to unfreeze your credit, if needed.) You will need to provide personal information such as your name, address, date of birth, and social security number to freeze your credit. While you can sometimes initiate this online, it’s better to do it by phone. Fees for freezing your credit <a href="https://advocacy.consumerreports.org/research/consumers-unions-guide-to-security-freeze-protection-2/" rel="nofollow">vary by state</a>. To freeze your credit, contact:</h3>
<ul dir="auto">
<li>Equifax: 1 (800) 349-9960, <a href="https://www.equifax.com/personal/credit-report-services/credit-freeze/" rel="nofollow">https://www.equifax.com/personal/credit-report-services/credit-freeze/</a></li>
<li>Experian: 1 (888) 397‑3742, <a href="https://www.experian.com/freeze/center.html" rel="nofollow">https://www.experian.com/freeze/center.html</a>.</li>
<li>TransUnion: 1 (888) 909-8872, <a href="https://www.transunion.com/credit-freeze/place-credit-freeze" rel="nofollow">https://www.transunion.com/credit-freeze/place-credit-freeze</a> or <a href="https://service.transunion.com/dss/orderStep1_form.page" rel="nofollow">https://service.transunion.com/dss/orderStep1_form.page</a></li>
<li>Innovis: 1 (800) 540-2505, <a href="https://www.innovis.com/personal/securityFreeze" rel="nofollow">https://www.innovis.com/personal/securityFreeze</a> or <a href="https://www.innovis.com/securityFreeze/index" rel="nofollow">https://www.innovis.com/securityFreeze/index</a></li>
</ul>
<h3 tabindex="-1" dir="auto">Explore any privacy options offered by your state’s motor vehicle department.</h3>
<h3 tabindex="-1" dir="auto">Contact your home and mobile phone companies to see if they offer privacy options, such as not listing your number on caller ID or allowing you to opt out of sales of phone numbers.</h3>
<h3 tabindex="-1" dir="auto">Consider opting out of direct marketing and telemarketing.</h3>
<p dir="auto">This list, compiled by Griffin Boyce, has a lot of information on doing so: <a href="https://github.com/glamrock/data-brokers/blob/master/data-brokers.md">https://github.com/glamrock/data-brokers/blob/master/data-brokers.md</a>. It was last updated in 2019.</p>
<p dir="auto">You can also opt out of targeted ads using cookies and similar technology at the browser level at <a href="https://optout.aboutads.info/" rel="nofollow">https://optout.aboutads.info</a>.</p>
<h3 tabindex="-1" dir="auto">Opt out of prescreened credit offers</h3>
<p dir="auto">Follow the instructions at <a href="https://www.optoutprescreen.com/" rel="nofollow">https://www.optoutprescreen.com/</a></p>
<h3 tabindex="-1" dir="auto">Get on the Do Not Call Registry</h3>
<p dir="auto"><a href="https://www.donotcall.gov/register/reg.aspx" rel="nofollow">https://www.donotcall.gov/register/reg.aspx</a></p>
<h2 tabindex="-1" dir="auto">Paid options</h2>
<p dir="auto"><a href="https://brandyourself.com/" rel="nofollow">Brand Yourself</a>, <a href="https://joindeleteme.com/" rel="nofollow">DeleteMe</a>, <a href="https://easyoptouts.com/" rel="nofollow">EasyOptOuts</a>, <a href="https://www.idx.us/idx-privacy" rel="nofollow">IDX Privacy</a>, <a href="https://www.thekanary.com/" rel="nofollow">Kanary</a>, <a href="https://onerep.com/" rel="nofollow">OneRep</a>, <a href="https://www.optery.com/" rel="nofollow">Optery</a>, <a href="https://privacypros.io/" rel="nofollow">Privacy Pros</a>, <a href="https://github.com/yaelwrites/Big-Ass-Data-Broker-Opt-Out-List/blob/master/Reputation.com">Reputation.com</a>, <a href="https://www.reputationdefender.com/" rel="nofollow">Reputation Defender</a>, and <a href="https://spartacus.com/" rel="nofollow">Spartacus</a> all offer different opt-out services. Of these options, I have personally paid for and had good experiences with DeleteMe and Kanary. In general, note that removal services are not comprehensive, as some data brokers do not allow third parties to remove listings, and removal services do not include every data broker on this list, let alone outside of it.</p>
<p dir="auto">There are services for media mail as well. For a fee, <a href="https://www.paperkarma.com/product/paperkarma-subscription/" rel="nofollow">PaperKarma</a> will remove your name from common direct mail vendors, including charity solicitations, direct mail and catalogs. Also for a fee, <a href="https://dmachoice.org/static/consumer_choice_tools.php" rel="nofollow">DMAChoice</a>, will remove your name from prospective mailers, not including political mail or mail from companies you’ve done business with in the past two years. You&nbsp;can also remove your name from <a href="https://www.valpak.com/remove-address" rel="nofollow">Valpak coupon packs</a>, for free.</p>
<h2 tabindex="-1" dir="auto">Further reading</h2>
<p dir="auto"><a href="https://www.eff.org/deeplinks/2020/12/doxxing-tips-protect-yourself-online-how-minimize-harm" rel="nofollow">Doxing: Tips To Protect Yourself Online &amp; How to Minimize Harm</a> (EFF)</p>
<p dir="auto"><a href="https://www.fastcompany.com/90310803/here-are-the-data-brokers-quietly-buying-and-selling-your-personal-information" rel="nofollow">Here are the data brokers quietly buying and selling your personal information</a> (Fast Company)</p>
<p dir="auto"><a href="https://inteltechniques.com/data/workbook.pdf" rel="nofollow">Personal Data Removal Workbook and Credit Freeze Guide</a> (Michael Bazzell)</p>
<p dir="auto"><a href="https://medium.com/cr-digital-lab/preliminary-results-are-in-ccpa-testers-provide-important-insights-into-the-landmark-privacy-law-c299f733de09" rel="nofollow">Preliminary results are in! CCPA testers provide important insights into the landmark privacy law</a> (Medium/Consumer Reports)</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Htmx Does Not Have a Build Step (163 pts)]]></title>
            <link>https://htmx.org/essays/no-build-step/</link>
            <guid>37265097</guid>
            <pubDate>Fri, 25 Aug 2023 18:08:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htmx.org/essays/no-build-step/">https://htmx.org/essays/no-build-step/</a>, See on <a href="https://news.ycombinator.com/item?id=37265097">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

  
  
    <address>Alexander Petros</address>
    <p><time>August 19, 2023</time></p><p>A recurring question from some htmx contributors is why htmx isn’t written in TypeScript, or, for that matter, why htmx lacks any build step at all. The full htmx source is a single 3,500-line JavaScript file; if you want to contribute to htmx, you do so by modifying the <code>htmx.js</code> file, the same file that gets sent to browsers in production, give or take minification and compression.</p>
<p>I do not speak for the htmx project, but I have made a few nontrivial contributions to it, and have been a vocal advocate for retaining this no-build setup every time the issue has arisen. From my perspective, here’s why htmx does not have a build step.</p>
<h2 id="write-once-run-forever"><a href="#write-once-run-forever" aria-label="Anchor link for: write-once-run-forever">🔗</a>Write Once, Run Forever</h2>
<p>The best reason to write a library in plain JavaScript is that it lasts forever. This is arguably JavaScript’s single most underrated feature. While I’m sure there are some corner cases, JavaScript from 1999 that ran in Netscape Navigator will run unaltered, alongside modern code, in Google Chrome downloaded yesterday. That is true for very few programming environments. It’s certainly not true for Python, or Java, or C, which all have versioning mechanisms where opting for new language features will force you off of deprecated APIs.</p>
<p>Of course, most people’s experience with JavaScript is that it ages like milk. Reopen a node repository after 3 months and you’ll find that your project is mired in a flurry of security warnings, backwards-incompatible library “upgrades,” and a frontend framework whose cultural peak was the exact moment you started the project and is now widely considered tech debt. Who’s to blame for this situation is for someone else to decide, but, in any case, you can eliminate this entire problem class by not having any dependencies beyond the JavaScript runtime.</p>
<p>A popular way to write JavaScript today is compile it from TypeScript (which I will use frequently as an example, because TypeScript is probably the <a rel="noopener" target="_blank" href="https://en.wikipedia.org/wiki/Straw_man#Steelmanning">best reason</a> to use a build system). TypeScript does not run natively in web browsers, so TypeScript code is not protected by <a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Glossary/ECMA">ECMA’s</a> fanatical devotion to backwards compatibility. Like any dependency, new major TypeScript versions are not guaranteed to be backwards compatible with the previous ones. They might be! But if they aren’t, then you need to do maintenance if you want to use the modern development toolchain.</p>
<p>Maintenance is a cost paid for with labor, and open-source codebases are the projects that can least afford to pay it. Opting not to use a build step drastically minimizes the labor required to keep htmx up-to-date. This experience has been borne out by <a rel="noopener" target="_blank" href="https://intercoolerjs.org/">intercooler.js</a>, the predecessor to htmx which is maintained indefinitely with (as I understand) very little effort. When htmx 1.0 was released, TypeScript was at version 4.1; when intercooler.js was released, TypeScript was pre-1.0. Would code written in those TypeScript versions compile unmodified in today’s TypeScript compiler (version 5.1 at the time of writing)? Maybe, maybe not.</p>
<p>But htmx is written in JavaScript, with no dependencies, so it will run unmodified for as long as web browsers remain relevant. Let the browser vendors do the hard work for you.</p>
<h2 id="developer-experience"><a href="#developer-experience" aria-label="Anchor link for: developer-experience">🔗</a>Developer Experience</h2>
<p>It is true that the TypeScript developer experience (DX) is better than the JavaScript developer experience in many respects. It is not true that the TypeScript DX is better in <em>every</em> respect, and the tendency of software engineers to view progress as a teleology of capability rather than choices with tradeoffs sometimes blinds them to the cost paid for the DX aspects they like. For instance, a small tradeoff you make for using TypeScript is that compiling it takes time, and you have to wait for it to recompile to test a change. Usually this cost is negligible, and well worth paying, but it is nonetheless a cost.</p>
<p>A more significant cost for using TypeScript is that the code running in the browser is not the code you wrote, which makes the browser’s developer tools harder to use. When your TypeScript code throws an exception, you have to figure out how the stack trace (with its JavaScript line numbers, JavaScript function signatures, and so forth) maps to the TypeScript code that you wrote; when your JavaScript code throws an exception, you can click straight through to the source code, read the thing you wrote, and set a breakpoint in the debugger. This is <em>tremendous</em> DX. For many younger web developers who have never worked this way, it can be a revelatory experience.</p>
<p>Build step advocates point out that TypeScript can generate <a rel="noopener" target="_blank" href="https://firefox-source-docs.mozilla.org/devtools-user/debugger/how_to/use_a_source_map/index.html">source maps</a>, which tell your browser what TypeScript corresponds to what JavaScript, and that’s true! But now you have <em>another</em> thing to keep track of—the TypeScript you wrote, the JavaScript it generated, and the source map that connects these two. The hot-reloading development server you’re now dependent on will keep these up to date for you on localhost—but what about on your staging server? What about in production? Bugs that appear in these environments will be harder to track down, because you’ve lost a lot of information about where they come from. These are solvable problems, but they’re problems you created; they are a cost.</p>
<p>The htmx DX is very simple—your browser loads a single file, which in every environment is the exact same file you wrote. The tradeoffs required to maintain that experience are real, but they’re tradeoffs that make sense for this project.</p>
<h2 id="enforced-clarity"><a href="#enforced-clarity" aria-label="Anchor link for: enforced-clarity">🔗</a>Enforced Clarity</h2>
<p>Modularization is one of the <a rel="noopener" target="_blank" href="https://legacy.python.org/dev/peps/pep-0020/">honking great ideas</a> of software. Modules make it possible to solve incredibly complex problems by breaking down code into well-contained substructures that solve smaller problems. Modules are really useful.</p>
<p>Sometimes, however, you want to solve simple problems, or at least relatively simple problems. In those cases, it can be helpful <em>not</em> to use the building blocks of more complex software, lest you emulate their complexity without creating commensurate value. At its core, htmx solves a relatively simple problem: it adds a handful of attributes to HTML that make it easier to replace DOM elements using the declarative character of hypertext. Requiring that htmx remain in a single file (again, around 3,500 LOC) enforces a degree of intention on the library; there is a real pressure when working on the htmx source to justify the addition of new code, a pressure which maintains an equilibrium of relative simplicity.</p>
<p>While the DX costs are obvious, there are also surprising DX benefits. If you search a function name in the source file, you’ll instantly find every invocation of that function (this also mitigates the need for more-advanced code introspection). The lack of places for functionality to hide makes working on htmx a lot more approachable. Far, far more complex projects use aspects of this approach as well: SQLite3 compiles from a <a rel="noopener" target="_blank" href="https://www.sqlite.org/amalgamation.html">single-file source amalgamation</a> (though they use separate files for development, they’re not <em>crazy</em>) which makes hacking on it <a rel="noopener" target="_blank" href="https://jvns.ca/blog/2019/10/28/sqlite-is-really-easy-to-compile/">significantly easier</a>. You could never build the linux kernel this way—but htmx is not the linux kernel.</p>
<h2 id="costs"><a href="#costs" aria-label="Anchor link for: costs">🔗</a>Costs</h2>
<p>Like any technology decision, choosing to forgo a build step has advantages and disadvantages. It’s important to acknowledge those tradeoffs so that you can make an informed decision, and revisit that decision if some of the benefits or costs no longer apply. With the advantages of writing plain JavaScript in mind, let’s consider some of the pain points it introduces.</p>
<h2 id="no-static-types"><a href="#no-static-types" aria-label="Anchor link for: no-static-types">🔗</a>No static types</h2>
<p>TypeScript is a strict superset of JavaScript, and some of the features it adds are very useful. TypeScript has… types, which make your <a rel="noopener" target="_blank" href="https://grugbrain.dev/#grug-on-type-systems">IDE better at suggesting code</a> and pointing out where you might have used methods incorrectly. The tools for automatically renaming and refactoring code are much more reliable for TypeScript than they are for JavaScript. The htmx code does have to be written in JavaScript, though, because browsers run JavaScript. And <a rel="noopener" target="_blank" href="https://github.com/tc39/proposal-type-annotations">as long as JavaScript is dynamically typed</a>, the tradeoffs required to get true static typing in the htmx source are not worth it (htmx <em>users</em> can still take advantage of typed APIs, declared with <code>.d.ts</code> files).</p>
<p>Future versions of htmx might use JSDoc to get some of the same guarantees without the build step. Other libraries, like <a rel="noopener" target="_blank" href="https://github.com/sveltejs/svelte/pull/8569">Svelte</a>, have been trending in this direction as well, in part due to the <a rel="noopener" target="_blank" href="https://news.ycombinator.com/item?id=35892250">debugging friction</a> that TypeScript files introduce.</p>
<h2 id="no-es6"><a href="#no-es6" aria-label="Anchor link for: no-es6">🔗</a>No ES6</h2>
<p>Because htmx maintains support for Internet Explorer 11, and because it does not have a build step, every line of htmx has to be written in IE11-compatible JavaScript, which means no <a rel="noopener" target="_blank" href="https://262.ecma-international.org/6.0/">ES6</a>. When people like me say that JavaScript is pretty good now, they are usually referring to language features that were introduced with ES6, like <code>async/await</code>, anonymous functions, and functional array methods (i.e. <code>.map</code>, <code>.forEach</code>)—none of which can be used in the htmx source.</p>
<p>While this is incredibly annoying, in practice it is not a huge impediment. The lack of some nice language features doesn’t prevent you from writing code with functional paradigms. Would it be nice not to write a custom <a rel="noopener" target="_blank" href="https://github.com/bigskysoftware/htmx/blob/b4a61c543b283eb2315a47708006783efb78f563/src/htmx.js#L375-L381">forEach method</a>? Of course. But until all the browsers targeted by htmx support ES6, it’s not hard to supplement ES5 with a few helper functions. If you are used to ES6, you will automatically write better ES5.</p>
<p>IE11 support is going to be dropped in htmx 2.0, at which point ES6 will be allowed in the source code.</p>
<h2 id="no-modules-in-core"><a href="#no-modules-in-core" aria-label="Anchor link for: no-modules-in-core">🔗</a>No modules in core</h2>
<p>This point is obvious, but it’s worth re-stating: the htmx source would be a lot tidier if it could be split it into modules. There are other factors that affect code quality <a rel="noopener" target="_blank" href="https://www.steveonstuff.com/2022/01/27/no-such-thing-as-clean-code">besides tidiness</a>, but to the extent that the htmx source is high-quality, it is not because it is tidy.</p>
<p>This makes doing certain things with htmx very difficult. The <a rel="noopener" target="_blank" href="https://github.com/bigskysoftware/idiomorph">idiomorph algorithm</a> might be included in the htmx 2.0 core, but it’s also maintained as a separate package so that people can use the DOM-morphing algorithm without using htmx. If the core could include multiple files, one could easily accomplish this with any number of mirroring schemes, such as git submodules. But the core is a single file, so the idiomorph code will have to live there as well.</p>
<h2 id="final-thoughts"><a href="#final-thoughts" aria-label="Anchor link for: final-thoughts">🔗</a>Final Thoughts</h2>
<p>This essay might be better titled “Why htmx Doesn’t Have a Build Step <em>Right Now</em>.” As previously mentioned, circumstances change and these tradeoffs can be revisited at any time! One issue we’re exploring at the moment has to do with releases. When htmx cuts releases, it uses a few different shell commands to populate the <code>dist</code> directory with minified and compressed versions of <code>htmx.js</code> (pedants are welcome to point out that this is obviously, in some sense, a build step). In the future, we might expand that script to auto-generate the <a rel="noopener" target="_blank" href="https://github.com/umdjs/umd">Universal Module Definition</a>. Or we might have new distribution needs that require an even more involved setup. Who knows!</p>
<p>One of the core values of htmx is that it gives you <em>choice</em> in a web development ecosystem that has for the last decade been dominated by an increasingly complex JavaScript stack. Once you no longer have an enormous codebase of frontend JavaScript, there is far less pressure to adopt JavaScript on the backend. You can write backends in Python, Go, even NodeJS, and it doesn’t matter to htmx—every mainstream language has mature solutions for formatting HTML. This is the principle of <a rel="noopener" target="_blank" href="https://htmx.org/essays/hypermedia-on-whatever-youd-like/">Hypermedia On Whatever you’d Like (HOWL)</a>.</p>
<p>Writing JavaScript with no build process is one of the options available to you once you no longer require NextJS or SvelteKit to manage the spiraling complexity of SPA frameworks. That choice makes sense for htmx development today, and it may or may not make sense for your app too.</p>

  <p>
    &lt;/&gt;
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Web scraping for me, but not for thee (460 pts)]]></title>
            <link>https://blog.ericgoldman.org/archives/2023/08/web-scraping-for-me-but-not-for-thee-guest-blog-post.htm</link>
            <guid>37264676</guid>
            <pubDate>Fri, 25 Aug 2023 17:42:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ericgoldman.org/archives/2023/08/web-scraping-for-me-but-not-for-thee-guest-blog-post.htm">https://blog.ericgoldman.org/archives/2023/08/web-scraping-for-me-but-not-for-thee-guest-blog-post.htm</a>, See on <a href="https://news.ycombinator.com/item?id=37264676">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-25442">
<p>by guest blogger <a href="https://mccarthylg.com/attorneys/">Kieran McCarthy</a></p>
<p><a href="https://blog.ericgoldman.org/wp-content/uploads/2023/08/rules-for-thee-29eb361473.jpg"><img decoding="async" src="https://blog.ericgoldman.org/wp-content/uploads/2023/08/rules-for-thee-29eb361473-300x164.jpg" alt="" width="300" height="164" srcset="https://blog.ericgoldman.org/wp-content/uploads/2023/08/rules-for-thee-29eb361473-300x164.jpg 300w, https://blog.ericgoldman.org/wp-content/uploads/2023/08/rules-for-thee-29eb361473.jpg 600w" sizes="(max-width: 300px) 100vw, 300px"></a>There are few, if any, legal domains where hypocrisy is as baked into the ecosystem as it is with web scraping.</p>
<p>Some of the biggest companies on earth—including Meta and Microsoft—take aggressive, litigious approaches to prohibiting web scraping on their own properties, while taking liberal approaches to scraping data on other companies’ properties.</p>
<p>When we talk about web scraping, what we’re really talking about is data access. All the world’s knowledge is available for the taking on the Internet, and web scraping is how companies acquire it at scale. But the question of who can access and use that data, and for what purposes, is a tricky legal question, which gets trickier the deeper you dig.</p>
<p>Some forms of data are protected by copyright, trademark, or another cognizable forms of intellectual property. But most of the data on the Internet isn’t easily protectible as intellectual property by those who might have an incentive to protect it.</p>
<p>For example, the most aggressive companies in pursuing web-scraping litigation are the social media companies. LinkedIn and Facebook, most notably, have done as much as anyone to shape the law of web scraping. But the content that they’re trying to protect isn’t theirs—it belongs to their users. It’s user-generated content. And while their terms of use provide the social media companies a license to use that user-generated content, it is their users who typically have a copyright interest in their content. The social media companies have no cognizable property right to assert in this content/data.</p>
<p>But make no mistake, these companies view this data, generated by their users on their platforms, as <em>their</em> property. This is true even though the law does not recognize that they have a property interest in it, and even though they expressly disclaim any property rights in that data in their terms of use.</p>
<p>Since the law does not give them a cognizable property interest in this data, they must resort to other legal theories to prevent others from taking it and using it.</p>
<p>In the early days of the Internet, the primary legal theory that companies used to stop scrapers was something called trespass to chattels. This is why Eric—who has been covering this issue for a good while now—tags all scraping posts as “Trespass to Chattels.”</p>
<p>The idea behind this legal theory is that web scraping—often high-volume, unwanted data requests—are a form of trespass on private tangible property—computer servers. But the thing about trespass to chattels is that it requires both a trespass to private tangible property and an element of damages. In the early days of the Internet, when Internet connections sounded <a href="https://www.youtube.com/watch?v=gsNaR6FRuO0">like this</a>, it didn’t take a lot of extra traffic to damage someone’s server or the ability to provide a functioning website. Many web scrapers were clumsy and didn’t realize the impact of their additional requests on servers. In the late 1990s and early 2000s, web scraping often did burden or shut down websites.</p>
<p>But as technology improved, this legal theory stopped making as much sense. Server capacity improved by many orders of magnitude, and most scrapers became savvy enough to limit their requests in a way that became imperceptible or at least inconsequential to the host servers. Now, one of elements of the trespass to chattels legal claim—damage to the servers or other tangible property of the host, very rarely happens.</p>
<p>Next, from the early 2000s until 2017, the primary legal theory that was used to deter web scraping was the Computer Fraud and Abuse Act or the CFAA. The CFAA prohibits accessing a “protected computer” without authorization. In the context of web scraping, the question is whether, once a web scraper gets its authorization revoked (usually via cease-and-desist letter, but often in the form of various anti-bot protections), any further scraping and use of a website’s data is “without authorization” within the meaning of the CFAA.</p>
<p>From 2001 to 2017, the simplistic answer was yes, any form of revocation of authorization was typically sufficient to trigger CFAA liability, if the scraper continued to access the site without permission. And then, in 2017, the famous <em>hiQ Labs, Inc. v. LinkedIn Corp</em>. case came out, which affirmed a plaintiff web scraper’s right to access public LinkedIn data under the CFAA.&nbsp; The Ninth Circuit affirmed, holding:</p>
<blockquote><p>We agree with the district court that giving companies like LinkedIn free rein to decide, on any basis, who can collect and use data—data that the companies do not own, that they otherwise make publicly available to viewers, and that the companies themselves collect and use—risks the possible creation of information monopolies that would disserve the public interest.</p></blockquote>
<p>Many interpreted this as allowing an affirmative right to scrape public data, <a href="https://blog.ericgoldman.org/archives/2022/12/hello-youve-been-referred-here-because-youre-wrong-about-web-scraping-laws-guest-blog-post-part-2-of-2.htm">even if that was not the correct reading of the law and the reality was always more nuanced</a>.</p>
<p>In the end, it was a pyrrhic victory. <a href="https://blog.ericgoldman.org/archives/2022/12/as-everyone-expected-years-ago-hiqs-cfaa-wins-dont-mean-it-can-freely-scrape-hiq-v-linkedin-guest-blog-post-part-1-of-2.htm">hiQ Labs lost that case, and at summary judgment the district court held that</a> “LinkedIn’s User Agreement unambiguously prohibits scraping and the unauthorized use of scraped data.” LinkedIn obtained a permanent injunction and damages against hiQ Labs on that basis.</p>
<p>Now, the primary vehicle to stop web scraping is with breach of contract claims.</p>
<p>For example, in just the last few weeks, Twitter/X Corp. has filed multiple lawsuits against web scrapers, <a href="https://www.bloomberglaw.com/public/desktop/document/XCorpvBrightDataLtdDocketNo323cv03698NDCalJul262023CourtDocket?doc_id=X2RNFIJA37E8BIOJFN5G1P36COT">including against Bright Data</a>, which is perhaps the biggest web-scraping company in the world.</p>
<p>Ten years ago, in web-scraping cases, you’d typically see plaintiffs in scraping cases file 10-15 legal claims, with law firms exploring any legal theory that might stick. Now, in its case against Bright Data, Twitter’s lawyers filed three claims: breach of contract, tortious interference with a contract, and unjust enrichment. Lawyers are increasingly confident that courts will enforce the breach of contract claim against scrapers and obtain the relief thy want. They don’t need or seek alternative legal theories.</p>
<p>And it is this legal reality—web scraping legal enforcement through breach of contract—that allows companies to assert property rights regarding how people use and access data—through the domain of contract law.</p>
<p>Mark Lemley observed this happening nearly 20 years ago, in his prescient, seminal article, “Terms of Use.”</p>
<blockquote><p>The problem is that the shift from property law to contract law takes the job of defining the Web site owner’s rights out of the hands of the law and into the hands of the site owner. Property law may or may not prohibit a particular “intrusion” on a Web site, but it is the law that determines the answer to that question. The reason my “no-trespassing” sign is effective in the real world is not because there is any sort of agreement to abide by it, but because the law already protects my land against intrusion by another. If the sign read “no walking on the road outside my property,” no one would think of it as an enforceable agreement. If we make the conceptual leap to assuming that refusing to act in the way the site owner wants is also a breach of contract, it becomes the site owner rather than the law that determines what actions are forbidden. The law then enforces that private decision. [citations omitted]</p></blockquote>
<p>Mark Lemley, 2006 Minnesota Law Review, <a href="https://www.minnesotalawreview.org/wp-content/uploads/2011/11/Lemley_Final.pdf">Terms of Use</a> at 471.</p>
<p>With the breach-of-contract-as-property legal regime, host websites are free to define their rights in online data however they want, in the form of online terms of use agreements.</p>
<p>Rather than creating a new intellectual property regime with general rules for data use—or even simpler—deciding cases using existing intellectual property rules, courts have allowed host websites to create their own intellectual property rights in website data, through the mere act of declaring such data to be property through an online contract. Companies have almost complete liberty to declare data that is not entitled to intellectual property protection to be “proprietary,” and courts allow them to enforce this ad hoc intellectual property regime through breach of contract claims (as long as they aren’t so foolish as to do it in a way that is co-terminus with copyright protections).</p>
<p>—</p>
<p>And this is where the hypocrisy comes in: the breach-of-contract-as-property legal regime has no legal requirement for intellectual honesty or consistency. It has no requirement to respect others’ IP akin to trademarks or patents in the same way that you do your own. Companies are free to press their advantage on what is deemed “proprietary” on their sites while simultaneously asserting what is free for the taking on others. It is easy to criticize this, but this is what smart lawyers and legal teams do.</p>
<p>—</p>
<p>Let’s look at what Microsoft is doing right now, as an example.</p>
<p>In the last couple of weeks, Microsoft updated its general terms of use to <a href="https://fagenwasanni.com/news/microsoft-adds-rules-and-restrictions-for-ai-offerings-in-terms-of-service/184341/">prohibit scraping, harvesting, or similar extraction methods of its AI services</a>.</p>
<p>Also in the couple of weeks, Microsoft affiliate OpenAI released a product called GPTbot, <a href="https://www.marktechpost.com/2023/08/10/openai-introduces-gptbot-a-web-crawler-designed-to-scrape-data-from-the-entire-internet-automatically/">which is designed to scrape the entire internet</a>.</p>
<p>And while they don’t admit this publicly, OpenAI has almost certainly already scraped the entire non-authwalled-Internet and used it is training data for GPT-3, ChatGPT, and GPT-4.</p>
<p>Nonetheless, without any obvious hints of irony, OpenAI’s own <a href="https://openai.com/policies/terms-of-use">terms of use prohibits scraping</a>.</p>
<p>Last year, Microsoft subsidiary <a href="https://www.linkedin.com/posts/sarahgwight_user-agreement-linkedin-activity-6994402330884386816-UAkW/">LinkedIn loudly and proudly declared victory in the most high-profile web-scraping litigation in US history</a>, imposing a permanent injunction on a former business rival to prevent it from scraping and accessing its private and public data forever. VP of Legal Sarah Wright declared, “The Court’s ruling helps us better protect everyone in our professional community from unauthorized use of profile data, and it establishes important precedent to stop this kind of abuse in the future.”</p>
<p>—</p>
<p>I’m picking on Microsoft, as it is the most flagrant offender here. But I could pick on hundreds of others who are also hypocritical on this issue. Notably, <a href="https://www.theregister.com/2023/02/02/meta_web_scraping/">Meta is also famously suing a company right now for scraping and selling its public content, even though Meta once paid the same scraper to scrape public data for them.</a></p>
<p>As I said at the start of this post, hypocrisy is endemic to this legal regime.</p>
<p>—</p>
<p>I, for one, don’t blame Microsoft or Meta or any of the other companies that take hypocritical stances on scraping. That’s what smart legal teams do when courts allow them to do it.</p>
<p>I blame the courts.</p>
<p><a href="https://en.wikipedia.org/wiki/Register.com_v._Verio">I blame the court in <em>Register.com v. Verio, Inc</em>. that paved the way for contracts of adhesion in the absence of assent</a>. I blame <a href="https://www.techdirt.com/2018/01/19/southwests-bullshit-lawsuit-over-site-that-made-45-helping-people-book-cheaper-flights/">the Northern District of Texas for enabling Southwest Airlines</a> to sue anyone that publishes public information about their flights. I blame the court in the <em>hiQ Labs</em> case that <a href="https://blog.ericgoldman.org/archives/2022/12/as-everyone-expected-years-ago-hiqs-cfaa-wins-dont-mean-it-can-freely-scrape-hiq-v-linkedin-guest-blog-post-part-1-of-2.htm">made no attempt to explain the disconnect or inconsistency on why hiQ Labs</a> was entitled to a preliminary injunction on its CFAA claim, but LinkedIn was entitled to a permanent injunction on its breach of contract claim on the exact same facts a few years later.</p>
<p>Courts need to realize that if you allow private companies to invent intellectual property rights through online contracts of adhesion, courts will be at the mercy of private decision-makers on questions that should be questions of public interest.</p>
<p>But given the fact that contracts, even online contracts, are a state-law issue, it’s hard to imagine a simple resolution to this problem. One possible solution might be a more all-encompassing interpretation of the copyright preemption doctrine, but the current law of copyright preemption is a muddled mess of a circuit split and <a href="https://blog.ericgoldman.org/archives/2023/07/contractual-control-over-information-goods-after-ml-genius-v-google-guest-blog-post.htm">the Supreme Court just declined an opportunity to resolve it.</a></p>
<p>—</p>
<p>But regardless of what you and I think about this legal regime, that is the current state of the law.</p>
<p>The next testing ground for it will be <a href="https://blog.ericgoldman.org/archives/2023/06/how-can-ai-models-legally-obtain-training-data-doe-1-v-github-guest-blog-post.htm">with these generative AI cases</a>.</p>
<p>I’ve long said we have not yet reached a stable equilibrium on these issues, because this kind of inconsistency in the law cannot be sustained. That means we are likely to see plenty of fireworks on these issues in the next few years.</p>
								
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why does all() return True if the iterable is empty? (126 pts)]]></title>
            <link>https://blog.carlmjohnson.net/post/2020/python-square-of-opposition/</link>
            <guid>37264149</guid>
            <pubDate>Fri, 25 Aug 2023 17:01:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.carlmjohnson.net/post/2020/python-square-of-opposition/">https://blog.carlmjohnson.net/post/2020/python-square-of-opposition/</a>, See on <a href="https://news.ycombinator.com/item?id=37264149">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>The other day on Reddit someone asked this:</p><blockquote><p>Why does <code>all()</code> return <code>True</code> if the iterable is empty? Shouldn’t it return <code>False</code> just like <code>if my_list:</code> would evaluate to <code>False</code> if the list is empty? What’s the thinking behind it returning <code>True</code>?</p></blockquote><p>They have since removed their question, but it sparked a long discussion thread, and <a href="https://www.reddit.com/r/Python/comments/fhygph/why_does_all_return_true_if_the_iterable_is_empty/fked1ha/">my comment</a> was heavily upvoted, so I thought I would record it here:</p><blockquote><p>This is literally a 2,500 year old debate in philosophy. The ancients thought “all unicorns are blue” should be false because there are no unicorns, but modern logic says it is true because there are no unicorns that aren’t blue. Python is just siding with modern predicate logic, but your intuition is also quite common and was the orthodox position until the last few hundred years.</p></blockquote><hr><p>Here is a little more explanation. Western logic is generally thought of as having two main periods: Aristotelean syllogistic logic and modern predicate logic. The classic Aristotelean syllogism runs along the lines of:</p><ol><li>Socrates is a man.</li><li>All men are mortal.</li><li>Therefore, Socrates is mortal.</li></ol><p>Logic in the Aristotelean system is centered around statements that all, some, or no X is Y. This creates a <a href="https://en.wikipedia.org/wiki/Square_of_opposition#The_problem_of_existential_import">square of opposition</a> in which “all men are mortal” contradicts “some men are not mortal”, and “Some men are just” contradicts “no man is just.” <a href="https://en.wikipedia.org/wiki/Peter_Abelard">Peter Abelard</a> (12c.) pointed out a problem with this system: “All stonemen are made of stone” is true by definition, but “no men are made of stone” is also true and seems to contradict it.</p><p>Debate about this topic raged without every really being settled. Instead, a new system of logic arose in the nineteenth century that replaced the old one. In predicate logic, <em>Fa</em> means “<em>a</em> is an <em>F</em>” and <em>∀x Fx</em> “all <em>x</em> are <i>F</i>s” while <em>∃x Fx</em> means “there exists an <em>x</em> which is an <em>F</em>”. In this system, there is no contradiction between “all <em>x</em> are <em>F</em>” and “there does not exist an <em>x</em> which is <em>F</em>” when there is no <em>x</em>. Because the new logic only talks about individuals and predicates rather than about universals and essences, it has no problem with saying “all stonemen are made of bologna”, since there are no stonemen, so no predicates apply. On the other hand, in the new logic, you can’t soundly argue that “all dogs are mammals; all mammals have fur; therefore, all dogs have fur” because one person shaving their dog suddenly makes “all mammals have fur” untrue.</p><p>Several commenters on Reddit thought that the problem with <code>all([])</code> was that we were restricting ourselves to either true or false answers. In logic, this is called the <a href="https://en.wikipedia.org/wiki/Principle_of_bivalence">principle of bivalence</a>: there are only two values that a proposition can have, true or false. But Python has <code>None</code>, many other programming languages have <code>null</code> or <code>nil</code>, and Zen has <a href="https://en.wikipedia.org/wiki/Mu_(negative)">mu</a>. Maybe adding another value would help?</p><p>On one level, the principle of bivalence clearly does not apply to all speech, because not all speech is “truth apt”. If I say, “Go Eagles!” I can mean it seriously or sarcastically, but I can’t be said to be saying something true or false. Logic is only supposed to be “truth preserving” for “propositions”: speech acts that are truth apt. However, <a href="https://plato.stanford.edu/entries/logic-manyvalued/">many-valued logics</a> go further and try to mathematically define the ways in which we can preserve desired properties (“truth” or perhaps “justification”) through arguments. This is still an area of active research and no system of many valued logic has yet become standard.</p><p>For workaday Pythonistas, this basically doesn’t matter. The Python core team chose to make <code>all([])</code> return <code>True</code>, and whatever their reasons, you can program your way around by adding wrapper functions or <code>if</code> tests. But it is interesting how even extremely practical twenty-first century programming can get drawn into millennia old philosophical controversies, intentionally or not.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UBC device uses wood dust to trap up to 99.9 per cent of microplastics in water (170 pts)]]></title>
            <link>https://news.ubc.ca/2023/08/16/microplastic-pollution-plants-could-be-the-answer/</link>
            <guid>37263827</guid>
            <pubDate>Fri, 25 Aug 2023 16:42:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.ubc.ca/2023/08/16/microplastic-pollution-plants-could-be-the-answer/">https://news.ubc.ca/2023/08/16/microplastic-pollution-plants-could-be-the-answer/</a>, See on <a href="https://news.ycombinator.com/item?id=37263827">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

            		

								 									<h5>Science, Health &amp; Technology</h5>
 								
								
																											  																			  

							</div><div>
                						<h2>UBC device uses wood dust to trap up to 99.9 per cent of microplastics in water</h2>
<p>Could plants be the answer to the looming threat of microplastic pollution? Scientists at UBC’s <a href="https://bpi.ubc.ca/" target="_blank" rel="noopener">BioProducts Institute</a> found that if you add tannins—natural plant compounds that make your mouth pucker if you bite into an unripe fruit—to a layer of wood dust, you can create a filter that traps virtually all microplastic particles present in water.</p>
<p>While the experiment remains a lab set-up at this stage, the team is convinced that the solution can be scaled up easily and inexpensively once they find the right industry partner.</p>
<p>Microplastics are tiny pieces of plastic debris resulting from the breakdown of consumer products and industrial waste. Keeping them out of water supplies is a huge challenge, says Dr. Orlando Rojas, the institute’s scientific director and the Canada Excellence Research Chair in Forest Bioproducts.</p>
<div id="attachment_268794"><p><a href="http://news.ubc.ca/wp-content/uploads/2023/08/Orlando-Rojas-e1555452538923.jpeg"><img aria-describedby="caption-attachment-268794" decoding="async" src="https://news.ubc.ca/wp-content/uploads/2023/08/Orlando-Rojas-e1555452538923-150x150.jpeg" alt="Dr. Orlando Rojas" width="150" height="150"></a></p><p id="caption-attachment-268794">Dr. Orlando Rojas</p></div>
<p>He noted one study which <a href="https://orbmedia.org/the-invisibles" target="_blank" rel="noopener">found</a> that virtually all tap water is contaminated by microplastics, and other research which <a href="https://www.thelancet.com/journals/ebiom/article/PIIS2352-3964(22)00372-3/fulltext#bib0001" target="_blank" rel="noopener">states</a> that more than 10 billion tons of mismanaged plastic waste will be dispersed in the environment by 2025.</p>
<p>“Most solutions proposed so far are costly or difficult to scale up. We’re proposing a solution that could potentially be scaled down for home use or scaled up for municipal treatment systems. Our filter, unlike plastic filters, does not contribute to further pollution as it uses renewable and biodegradable materials: tannic acids from plants, bark, wood and leaves, and wood sawdust—a forestry byproduct that is both widely available and renewable.”</p>
<h2><strong>Captures a wide variety of plastics</strong></h2>
<p>For their <a href="https://doi.org/10.1002/adma.202301531" target="_blank" rel="noopener">study</a>, the team analyzed microparticles released from popular tea bags made of polypropylene. They found that their method (they’re calling it “bioCap”) trapped from 95.2 per cent to as much as 99.9 per cent of plastic particles in a column of water, depending on plastic type. When tested in mouse models, the process was proved to prevent the accumulation of microplastics in the organs.</p>
<p>Dr. Rojas, a professor in the departments of wood science, chemical and biological engineering, and chemistry at UBC, adds that it’s difficult to capture all the different kinds of microplastics in a solution, as they come in different sizes, shapes and electrical charges.</p>
<p>“There are microfibres from clothing, microbeads from cleansers and soaps, and foams and pellets from utensils, containers and packaging. By taking advantage of the different molecular interactions around tannic acids, our bioCap solution was able to remove virtually all of these different microplastic types.”</p>
<h2><strong>Collaborating on sustainable solutions</strong></h2>
<p>The UBC method was developed in collaboration with Dr. Junling Guo, a professor at the Center of Biomass Materials and Nanointerfaces at Sichuan University in China. <a href="https://www.linkedin.com/in/marina-mehling/" target="_blank" rel="noopener">Marina Mehling</a> <em>(she/her),</em> a PhD student at UBC’s department of chemical and biological engineering, and <a href="https://www.linkedin.com/in/tianyu-guo-50a0371b3/?originalSubdomain=ca" target="_blank" rel="noopener">Dr. Tianyu Guo</a> <em>(she/her)</em>, a postdoctoral researcher at the BioProducts Institute, also contributed to the work.</p>
<p>“Microplastics pose a growing threat to aquatic ecosystems and human health, demanding innovative solutions. We’re thrilled that the BioProducts Institute’s multidisciplinary collaboration has brought us closer to a sustainable approach to combat the challenges posed by these plastic particles,” said Dr. Rojas.</p>
<p><em>Interview language(s): English (Rojas, Tianyu Guo, Mehling), Mandarin (Tianyu Guo)</em></p>
<h4><em>Multimedia assets:</em></h4>
<ul>
<li><em><a href="https://doi.org/10.1002/adma.202301531" target="_blank" rel="noopener">Link to study</a></em></li>
<li><em><a href="https://www.dropbox.com/sh/8mv07a3hgcmcd7d/AABDNWHaoVS5nB8W-NsAUeT-a?dl=0" target="_blank" rel="noopener">Dropbox (images and b-roll)</a></em></li>
</ul>
<h4><em>Related links:</em></h4>
<ul>
<li><em><a href="https://bpi.ubc.ca/about-us" target="_blank" rel="noopener">Learn more about the BioProducts Institute</a></em></li>
<li><em><a href="https://mp.ubc.ca/news/may-05-2023/ubc-microplastics-cluster-symposium-highlights-groundbreaking-research-and" target="_blank" rel="noopener">UBC microplastics cluster symposium highlights groundbreaking research and discussions on microplastic pollution</a></em></li>
<li><em><a href="https://news.ubc.ca/2022/05/26/ubc-entrepreneurs-join-fight-against-plastic-pollution/" target="_blank" rel="noopener">UBC entrepreneurs join fight against plastic pollution</a></em></li>
</ul>


					
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Email Authentication: A Developer's Guide (167 pts)]]></title>
            <link>https://resend.com/blog/email-authentication-a-developers-guide</link>
            <guid>37263708</guid>
            <pubDate>Fri, 25 Aug 2023 16:34:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://resend.com/blog/email-authentication-a-developers-guide">https://resend.com/blog/email-authentication-a-developers-guide</a>, See on <a href="https://news.ycombinator.com/item?id=37263708">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Proper email authentication can be the difference between <strong>reaching the human or the spam folder</strong>, but it is often overlooked or misunderstood.</p>
<p>Think of your <strong>emails as a startup getting into a competitive accelerator program</strong>.</p>
<h2>SPF (Receiving Applications)</h2>
<p>Competitive startup programs will receive 10's of thousands of applications. Their first step is to see which of these applications can be thrown out without being considered.</p>
<p><a href="https://www.open-spf.org/">SPF (Sender Policy Framework)</a> is similar. It's the first triage of the emails coming to an inbox, checking to make sure that each email should even be considered for delivery.</p>
<p>The DNS record for SPF declares a list of origins (servers) that are allowed to send email for this domain, and the inbox will confirm that the message they received matches one of them. If a server isn't on the list, it's like an application being tossed out because it wasn't fully filled or the business idea is illegal.</p>
<p>Every domain or subdomain can only have one SPF policy, and policies on the root/apex domain (domain.com) are <strong>not applied to subdomains</strong> (sub.domain.com).</p>
<p>Your SPF policy, specified in a TXT record, probably looks like this:</p>
<p><img src="https://resend.com/static/posts/email-authentication-spf.jpg" alt="Diagram explain SPF record"></p>

<ul>
<li><strong>v=spf1</strong>: The version of SPF</li>
<li><strong>include:_spf.google.com</strong>: Allows Google servers to send emails on your domain</li>
<li><strong>include:amazonses.com</strong>: Allows AWS servers to send emails on your domain</li>
<li><strong>~all</strong>: The policy which tells the server what to do if the SPF check fails</li>
</ul>
<p>When a mailbox receives a message from you, it will look at the <strong>Return-Path</strong> in the email header and expects it to map back to one of the origins specified in the record.</p>
<h2>DKIM (Application Vetting)</h2>
<p>If the application passes that initial check, then the vetting process begins to make sure all the claims the applicants made are true.</p>
<p><a href="https://dkim.org/">DKIM (DomainKeys Identified Mail)</a> plays a similar role to confirm the legitimacy of the message by adding a signature on each message that verifies the email sender is who they say they are.</p>
<p>DKIM is set with a private/public key pairing.</p>
<ol>
<li>You set a public key in your DNS records (usually a CNAME or TXT record)</li>
<li>Each email you send includes a DKIM signature</li>
<li>When an inbox receives your message, it compares the signature with the public record to confirm a pair</li>
</ol>
<p>Especially as your company becomes more well known, there are more incentives for <strong>hackers to send an email as if it is from you</strong>.</p>
<p>The DKIM, like a strong login password, is an essential way to prove who you are by providing information only you know.</p>
<p>It is common to have multiple DKIM records, usually one or more per email provider.</p>
<h2>DMARC (Selection Policy)</h2>
<p>What if an applicant fails one of these steps? How should their application be handled?</p>
<p><a href="https://resend.com/docs/dashboard/domains/dmarc">DMARC (Domain-based Message Authentication, Reporting &amp; Conformance)</a> is <strong>the selection policy</strong>. It sets rules for what happens if an applicant lies on an application (DKIM) vs. not demonstrate enough traction (SPF). For email, DMARC establishes your policy as a sender for what should happen to your messages if they fail DKIM or SPF.</p>
<p>You would likely have one DMARC policy set for your entire domain, including subdomains, in a TXT record that looks like this:</p>
<p><img src="https://resend.com/static/posts/email-authentication-dmarc.jpg" alt="Diagram explain SPF record"></p>

<ul>
<li><strong>v</strong>: The version of DMARC</li>
<li><strong>p</strong>: What the mailbox should do (policy) if SPF or DKIM fails (none, quarantine, reject)</li>
<li><strong>pct</strong>: The percentage of failed messages that should be affected by the policy.</li>
<li><strong>rua</strong>: A valid inbox where the providers should send their DMARC reports</li>
</ul>
<p>Implementing DMARC, particularly with a policy of <strong>quarantine</strong> or <strong>reject</strong>, enhances your domain's reputation. This is because inbox providers can rely on your commitment to prevent the delivery of suspicious messages, thereby improving their user experience within the inbox.</p>
<p>Check out our full guide on <a href="https://resend.com/docs/dashboard/domains/dmarc">how to set up DMARC</a>.</p>
<h2>BIMI (Exclusive Access)</h2>
<p>Making it into a startup accelerator is an amazing feat, but if you want to be exceptional, you need to gain the attention of the industry leaders and pioneers. There are no hacks or shortcuts to this, you simply need to prove yourself.</p>
<p><a href="https://resend.com/docs/dashboard/domains/bimi">BIMI (Brand Indicators for Message Identification)</a> is this kind of access in the inbox. It sets you apart from all the others by showcasing your brand and legitimacy to your users in the inbox by displaying your logo and, in some cases, a verified checkmark.</p>
<p>With over 347 billion emails sent every day, this is an exceptional way to stand out.</p>
<p><img src="https://resend.com/static/posts/email-authentication-bimi-inbox.png" alt="BIMI in the inbox"></p>
<p>Obtaining BIMI is exclusive because of the long, hard process it takes to complete the verification. Here are a few things you need:</p>
<ul>
<li><strong>DMARC</strong>: The DMARC policy must be at quarantine or reject and at 100%</li>
<li><strong>Trademarked Logo</strong>: The logo you want to showcase must be trademarked</li>
<li><strong>VMC</strong>: The certificate which verifies your identity, domain, and trademark</li>
</ul>
<p><img src="https://resend.com/static/posts/email-authentication-bimi.jpg" alt="Diagram explain BIMI record"></p>

<ul>
<li><strong>v</strong>: The version of BIMI</li>
<li><strong>l</strong>: The location of the SVG logo</li>
<li><strong>a</strong>: The location of the Verified Mark Certificate (VMC)</li>
</ul>
<p>Check out our full guide on <a href="https://resend.com/docs/dashboard/domains/bimi">how to set up BIMI</a>.</p>
<h2>Delivery is the Goal</h2>
<p>The good news is that <strong>SPF and DKIM are handled for you when using Resend</strong>. All you need to do is <a href="https://resend.com/domains">add a domain</a> and we take care of the rest.</p>
<p>Ultimately, inbox providers aim to only show the emails their users want to see, and spoofed or compromised emails are not on the list.</p>
<p>Without these protocols, <strong>they can't tell you from a spammer</strong>.</p>
<p>Assure them you're legit, and they'll prioritize your emails. It's a win-win.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: Release AI – Talk to Your Infrastructure (122 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37263473</link>
            <guid>37263473</guid>
            <pubDate>Fri, 25 Aug 2023 16:20:09 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37263473">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37266162"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37266162" href="https://news.ycombinator.com/vote?id=37266162&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>"Hello database, why are you slow?" /s<p>Serious feedback:</p><p>* Support Digital Ocean (currently only AWS supported)</p><p>* Even read-only access is scary... it's not clear if that means you have access to my sensitive user data or just the infra metadata.</p><p>Normally I would just include relevant info about my AWS infra in my prompt... how is this better than that?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37266231"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37266231" href="https://news.ycombinator.com/vote?id=37266231&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>Release engineer here.<p>- Support Digital Ocean: We are always looking to add more integrations!
- Read-only access to user data: You are welcome to check on the policy and/or adjust it. We default in the recommended case to Read-Only, but you can improve on that with a permissions boundary
- How is it better than gathering output from AWS, copy/pasting it into a browser, reading the answer and copy/pasting back into terminal? Well, it's all in one place and it allows a tighter loop between gathering, analysing, and giving responses in one place. It gives lay people better access to data they might not know how to or be able to access, and it gives advanced users smoother, tighter loops on data and access they might already have. Hopefully that gives you some ideas.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37267503"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37267503" href="https://news.ycombinator.com/vote?id=37267503&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>"Hello AWS, why is there this subnet and who added it?"<p>I'm seriously curious how is asking a chatbot better than looking at a well designed diagram...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37267663"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37267663" href="https://news.ycombinator.com/vote?id=37267663&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>You're very lucky if there's an up to date diagram and you got confidence in it's correctness.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37267962"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37267962" href="https://news.ycombinator.com/vote?id=37267962&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>Like a hand-made diagram? That's a blast from the past...<p>I mean a diagram generated directly from your actual infra using the same APIs as the chatbot
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37268367"><td></td></tr>
                <tr id="37269235"><td></td></tr>
                                          <tr id="37265345"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37265345" href="https://news.ycombinator.com/vote?id=37265345&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>You're basically coming for my job as an SRE/Platarch but great documentation you've covered a lot of scenarios and in a pretty thorough manner.<p>You should figure out a way to import current infra state and control it from your dashboard and build off of that. That'd be really interesting. Like, take over a tfstate file or whatever pulumi/cdk uses. And definitely just build off of aws/gcp/azure/oracle without IAC like terraformer does.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37266143"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37266143" href="https://news.ycombinator.com/vote?id=37266143&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Release engineer here. I believe you have hit on some of the ideas we've certainly discussed way down the road (or sooner than you'd think?). For example, can we inform the AI about your existing infrastructure patterns, tooling, configs, etc. and then ask it to "suggest best practices", "compare to security and compliance policies", "improve" it, "analyse optimisations", "identify unused/overprovisioned pieces", etc. etc.!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37266571"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37266571" href="https://news.ycombinator.com/vote?id=37266571&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>I'll throw you a big bone, I just cut our nearly $million/month aws infra costs down 45% by switching from x86 to arm64. So factor that in for sure. I was <i>hoping</i> for 15-25%.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37267435"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37267435" href="https://news.ycombinator.com/vote?id=37267435&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Fantastic, we've already begun switching RDS instances to Arm64 (simple, easy, and effective), but still haven't cracked (yet) building for Arm64 compute containers in a safe and effective way. I love it!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37267916"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37267916" href="https://news.ycombinator.com/vote?id=37267916&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>What about arm64 makes building containers in a safe and effective way harder compared to x86?</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="37266345"><td></td></tr>
                <tr id="37266584"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37266584" href="https://news.ycombinator.com/vote?id=37266584&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>As mentioned platform architect. I work on the research, design and construction of PaaS systems (which usually include SaaS as well, the backend of what lambdas or "functions" etc run on).<p>PaaS being Platform as a Service (think GKE/EKS/A...KS?)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37268079"><td></td></tr>
                <tr id="37268258"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37268258" href="https://news.ycombinator.com/vote?id=37268258&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>Not the person you asked, but I needed a place to run my shitty python scripts and the minimal requirement in my org was 'stand up an ec2 and get the OS managed by (insert other team)'<p>I complained about it and designed and built an MVP for a k8s SaaS in a week or so and pitched it to leadership and got some buy-in.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37266357"><td></td></tr>
                        <tr id="37265145"><td></td></tr>
                <tr id="37265265"><td></td></tr>
                  <tr id="37264457"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37264457" href="https://news.ycombinator.com/vote?id=37264457&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>FYI you may want to add something to the homepage to clarify - I was confused initially, because I clicked through to the Launch YC post, and then I clicked the URL at the top and saw the homepage, which talked about Environments as a Service rather than 'talking to your infra'.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37265060"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37265060" href="https://news.ycombinator.com/vote?id=37265060&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Release engineer here, we see this as complementary to environments. When you spin up your infrastructure and environments using our service, how do you interact with them? How do you see what is happening? How can you (easily, as a lay person) figure out how to diagnose, maybe even fix, problems? That is the idea, hope you like it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37265709"><td></td></tr>
                  <tr id="37265435"><td></td></tr>
                <tr id="37266170"><td></td></tr>
                  <tr id="37268263"><td></td></tr>
                <tr id="37268407"><td></td></tr>
                  <tr id="37269099"><td></td></tr>
            <tr id="37268710"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37268710" href="https://news.ycombinator.com/vote?id=37268710&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>I cannot tell how much this company can change your world in this space. Efficiency is paramount in all we do in life and work.<p>Let's get Release wider on the map.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37268069"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37268069" href="https://news.ycombinator.com/vote?id=37268069&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Really curious to know what led you to this project? When ChatGPT4 released did Bella start choking in your head with all of these possibilities?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37267421"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37267421" href="https://news.ycombinator.com/vote?id=37267421&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Thanks for all of the feedback. I would love to get everyones ideas on what they would want Release AI to do for them. Please reply in the comments.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37265596"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37265596" href="https://news.ycombinator.com/vote?id=37265596&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>... You seriously just tell people to deploy a stack with your IAM role that "has the least permissions possible"  good luck lol.  Another thin wrapper around open-ai that doesn't even do the heavy lifting (deploying a secure and trusted authenticated role to access resources that can be easily audited etc)</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37266022"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37266022" href="https://news.ycombinator.com/vote?id=37266022&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>For what it’s worth, literally every vendor that operates in its customers’ AWS accounts does this. Create a cross account role, trust the vendor’s account, and give it read only permissions (although don’t use the built in “read only” role since that includes access to things like S3 objects.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37266164"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37266164" href="https://news.ycombinator.com/vote?id=37266164&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>At my current job I can trigger a pipeline to deploy a complex set of cloud formation stacks but my account doesn’t have access to logs of those deployments. Devops!</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37266310"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37266310" href="https://news.ycombinator.com/vote?id=37266310&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Release engineer here. You can pretty easily audit the policy we use (Read-Only) and you can also add a permissions boundary if you want. We would love to get any feedback and improvements you can offer if you are inclined. We have a slack workspace users can join, check it out. <a href="https://release-ai.slack.com/" rel="nofollow noreferrer">https://release-ai.slack.com</a></span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37266818"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37266818" href="https://news.ycombinator.com/vote?id=37266818&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>Looks cool! A bit hesitant to try it out, though. Can you talk a bit about your privacy and security practices?<p>Do you (or do you plan to) use customer data (or prompts/results) for training? Do you ever read any of the customer's AWS data beyond what's strictly necessary for the functionality of the tool? What data do you retain?</p><p>I looked at your privacy policy but it's pretty generic.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37267314"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37267314" href="https://news.ycombinator.com/vote?id=37267314&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>I do the infra from our company. I _want_ them to train on our infra and everyone else's infra and to tell me what actually works and will solve my problem, not what my account manager recommends because AWS approve it. That's basically the value add of the tool.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37268125"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37268125" href="https://news.ycombinator.com/vote?id=37268125&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>But how will it learn "what works" just based on different company's infra? I don't see how that would help an AI learn without things being labelled, and the answer can't be "lower latency means better". I'd actually be more trusting of the results of the current approach (GPT-4), because at least it's trained on every blog, forum response, and book ever.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37268648"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37268648" href="https://news.ycombinator.com/vote?id=37268648&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>&gt; But how will it learn "what works" just based on different company's infra?<p>The same way as the current data was trained. I'm not expecting magic, I'm expecting help.</p><p>&gt;  because at least it's trained on every blog, forum response, and book ever.</p><p>Random blog posts that people write as examples as they're learning how to structure their infrastructure is not what I want to mirror our infrastructure on. I don't blindly trust the AWS documentation, I don't blindly trust a blog post, so why will I blindly trust an AI tool?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37268057"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37268057" href="https://news.ycombinator.com/vote?id=37268057&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>Exactly.<p>It would be interesting if just like an AI coding assistant, this would be a DevOPs/SRE/Infra assistant, whereby you can also just do your regular CI/CD/deployments/whatever - and have it make recommendations on caveats based on other architectures, or such.</p><p>e.g.</p><p>"describe examples for an architecture to accomplish X" and it spits out some examples - then you can choose, and save these "Infra-Prompts" and then later say</p><p>"generate a new sand-box based on X and connect it to such-and-such and notify USERA when their environment is available, include a status page, and alert cron and a costing line in the daily CFO report" etc
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37268661"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37268661" href="https://news.ycombinator.com/vote?id=37268661&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Couldn't agree more. Imagine if it could look at all the other 30-50 person companies who are running a SAAS and say "teams like yours use ECS + Fargate instead of Kubernetes" (as an example)</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37267417"><td></td></tr>
                        <tr id="37264949"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37264949" href="https://news.ycombinator.com/vote?id=37264949&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>I like the concept, but would love to hear if it's a feature or some long-term product vision.<p>It looks like the main business is Environment as a Service.</p><p>Do you see ReleaseAI as a new product? Is it related to Env as a service?</p><p>Or maybe you are considering to pivot?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37265271"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37265271" href="https://news.ycombinator.com/vote?id=37265271&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Tommy, CEO here. We believe it is incredibly complimentary to our core service. What we've found in building Release through our support channels is we spend a tremendous amount of time consulting with the DevOps teams of our customers answering questions like what can be answered with ReleaseAI. It helps us help ourselves and our customers. In the long run we see this being integrated into our core product in really interesting ways as well. There will always be a standalone version.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37264328"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37264328" href="https://news.ycombinator.com/vote?id=37264328&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>I like the idea, but how do you handle hallucinations? E.g. when the user asks about their AWS bill, how can they be sure the numbers they get are accurate?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37264486"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37264486" href="https://news.ycombinator.com/vote?id=37264486&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Would be good if any user interface that uses these kinds of llm solutions always include the raw data from the backing services when asked for. Like how in chatgpt you can open and inspect its interaction with a plugin.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37265146"><td></td></tr>
                  <tr id="37265100"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37265100" href="https://news.ycombinator.com/vote?id=37265100&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Release engineer here. That's an excellent question, and we worry about it all the time. The AI "seems" authoritative, but it can't even add 1+1 sometimes :crying-emoji:. We've tried to engineer the prompts and tooling so that it will say "I don't know" if it doesn't know. But we've still seen it say some crazy things, like "Your cluster is fine" when it clearly wasn't. :tounge-sticking-out-emoji: I guess the only real answer is you have to trust but verify.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37266695"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37266695" href="https://news.ycombinator.com/vote?id=37266695&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>&gt; But we've still seen it say some crazy things, like "Your cluster is fine" when it clearly wasn't. :tounge-sticking-out-emoji:<p>It’s difficult to take you seriously when you write like this about show-stopping bugs.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37267457"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37267457" href="https://news.ycombinator.com/vote?id=37267457&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>I was referring to problems we found during initial development, but I appreciate that I didn't clarify that well.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37265751"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37265751" href="https://news.ycombinator.com/vote?id=37265751&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>You need to engineer a system when the AI state something it has to give a command that should support what it says and explain how the command shows that it is true. At this point the command should be really executed and its output or error fed yo the AI so that it can confirm the statements or correct it.<p>I am crazy how they think a system with no feedback loop can be always accurate. Only perfect mathematics can work like this, any -like system need to have a feedback loop.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37266285"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37266285" href="https://news.ycombinator.com/vote?id=37266285&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Excellent idea, we do internally feed the answers back to the system to improve its own inputs and outputs. The funniest part of some of this experience has been to find cases where even humans were hallucinating: "Hey, I thought this was shutdown?!" or "I can't find the bucket!" Even on a bad day, the humans are still ahead though.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37265674"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37265674" href="https://news.ycombinator.com/vote?id=37265674&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Tommy, CEO here. We also have some ideas on reporting hallucinations and feeding wrong answers back into the prompts automatically to help reduce instances of hallucinations. We have a few other ideas and would welcome any ideas folks have to help with this problem.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37268174"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37268174" href="https://news.ycombinator.com/vote?id=37268174&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>After thinking about it for a bit, I have an idea that might help. The writeup is probably too long for an HN comment, though. Could I email you?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37268454"><td></td></tr>
                        <tr id="37266264"><td></td></tr>
            <tr id="37265504"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37265504" href="https://news.ycombinator.com/vote?id=37265504&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Thanks for the answer. Yeah, that's pretty much what I expected would be the case. Speaking as another dev in the AI space, it seems like reliability and consistency are the hardest issues when it comes to making AI genuinely useful in production vs. just a neat toy, and there's no stock solution.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37267134"><td></td></tr>
            <tr id="37267439"><td></td></tr>
            <tr id="37263635"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37263635" href="https://news.ycombinator.com/vote?id=37263635&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>Love it.<p>Funny, while I was reading the site, I was wondering if you could build crons on it - and that was the last example :-)</p><p>Some cool things I'd like to try would be:</p><p>"Create an alert whenever [EVENT] happens and contact [whomever]"</p><p>"Give me a system summary every morning at 9:15 AM called "Stand-up Status"</p><p>"Summarize all events for [OBJECT] each [TIME PERIOD] and send a report daily at X"</p><p>"Give the CFO a daily spend report each morning"</p><p>-</p><p>Can it do things like this?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37263946"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37263946" href="https://news.ycombinator.com/vote?id=37263946&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>Release engineer here.<p>With the exception of the first example (I'm not sure how good it would be at event monitoring like that, though we're absolutely going to try it out!), I think all of these should work. We've tested the daily spend report already.</p><p>Give it a shot and let us know how it works for you!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37264131"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37264131" href="https://news.ycombinator.com/vote?id=37264131&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>For the first one, maybe an Intrusion Detection<p>"Alert me anytime someon attempts to login to [SENSITIVE SYSTEM]</p><p>-</p><p>Can this do SPLUNK like log analytics?</p><p>"Give me a table of all activity of [TYPE] in [THESE LOGS/SYSTEMS]" (for whatever metric youre tracking)</p><p>"Give me a cron of uptime every hour for [system, site, cluster, whatever]"</p><p>"create a status page for critical systems A B C X Y Z"</p><p>--</p><p>I dont have any AWS infra to throw this at right now - but I do love this</p><p>As a Dir. Of DevOps in my career - these were very common questions thrown at me on the regular from PMs, C-suite, engineering etc...</p><p>So if this were a self-service query portal for teams with permissions/roles on what sort of questions could be asked from other teams that would be cool.</p><p>this is read only? It cant deploy/launch/buy services can it?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37265103"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37265103" href="https://news.ycombinator.com/vote?id=37265103&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><p><span>Since this is currently talking to AWS and k8s directly, unless you are setup in a way that would let AWS know about the intrusion detection, then this is likely out of scope for now.<p>Similar to the logs/systems access. If AWS or k8s can read the logs, there is a chance we can crunch them, if they are in a separate logging platform, we would currently be unable to fetch that information. Great ideas for future features though!</p><p>It is currently read only yes.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37265832"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37265832" href="https://news.ycombinator.com/vote?id=37265832&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>re:splunk (and opensearch, databricks, etc), we're already doing those with louie.ai and running early self-hosted + saas cohorts. Your questions are very much the type we'd interested in exploring with you! Feel free to signup on our early access program on the site or reach out directly (leo@graphistry.com).</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="37264992"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37264992" href="https://news.ycombinator.com/vote?id=37264992&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Congratulations on the launch. I own and run a DevOps consulting company and being able to ask AWS billing questions or create CloudWatch notifications in human language is gonna be a game changer. Looking forward to using and following Releases progress. The .com domain name must have cost a fortune. lol</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37266381"><td></td></tr>
                <tr id="37266468"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37266468" href="https://news.ycombinator.com/vote?id=37266468&amp;how=up&amp;goto=item%3Fid%3D37263473"></a></center>    </td><td><br><div>
                  <p><span>Release engineer here. No, this tool merely complements our core offerings and also has some use as a standalone tool. We can currently use any of the AI API offerings that are compatible; OpenAI is the one that is most common though. If they shut it down, a lot of people would have to migrate to a different provider but our tool could still work.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just 23% of Americans Know the US Has Failed to Pass an Internet-Era Privacy Law (171 pts)]]></title>
            <link>https://www.techdirt.com/2023/08/25/just-23-of-americans-know-the-u-s-has-failed-to-pass-an-internet-era-privacy-law/</link>
            <guid>37263459</guid>
            <pubDate>Fri, 25 Aug 2023 16:19:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2023/08/25/just-23-of-americans-know-the-u-s-has-failed-to-pass-an-internet-era-privacy-law/">https://www.techdirt.com/2023/08/25/just-23-of-americans-know-the-u-s-has-failed-to-pass-an-internet-era-privacy-law/</a>, See on <a href="https://news.ycombinator.com/item?id=37263459">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-419090">


<h3>from the <i>not-the-sharpest-knife</i> dept</h3>

<p>We’ve noted repeatedly how the hyperventilation about TikTok privacy is <a href="https://www.techdirt.com/2023/03/21/forget-a-tiktok-ban-we-need-to-regulate-data-brokers-and-pass-a-real-privacy-law/" data-type="link" data-id="https://www.techdirt.com/2023/03/21/forget-a-tiktok-ban-we-need-to-regulate-data-brokers-and-pass-a-real-privacy-law/">largely just a distraction</a> from the U.S.’ ongoing failure to pass even a basic privacy law or meaningfully regulate data brokers. </p>
<p>We haven’t done those things for two reasons. One, the dysfunctional status quo (where companies mindlessly over-collect data and fail to secure it, resulting in endless privacy scandals) is hugely profitable to everybody in the chain. Two, the government long ago realized it can abuse the barely regulated info-hoovering user tracking system we’ve built <a href="https://www.techdirt.com/2023/03/16/fbi-latest-to-admit-to-bypassing-warrant-requirements-by-purchasing-location-info-from-data-brokers/" data-type="link" data-id="https://www.techdirt.com/2023/03/16/fbi-latest-to-admit-to-bypassing-warrant-requirements-by-purchasing-location-info-from-data-brokers/">to avoid having to get warrants</a>. </p>
<p>There’s simply no meaningful incentive for reform. </p>
<p>None of this is helped by the fact that an ad-based, wealth-obsessed tech press is financially incentivized to prioritize engagement clickbait (billionaire cage matches! Poorly-made blockchain-based ape art will change the world!), over nuance and deeper analysis. A media ecosystem owned by billionaires that seems to have an <a href="https://www.techdirt.com/2023/02/08/cnet-insiders-say-tech-outlet-softened-coverage-to-please-advertisers/" data-type="link" data-id="https://www.techdirt.com/2023/02/08/cnet-insiders-say-tech-outlet-softened-coverage-to-please-advertisers/">ever-dwindling interest</a> in <a href="https://www.techdirt.com/2023/07/06/gq-clowns-itself-weakens-story-critical-of-incompetent-discovery-ceo-david-zaslav/" data-type="link" data-id="https://www.techdirt.com/2023/07/06/gq-clowns-itself-weakens-story-critical-of-incompetent-discovery-ceo-david-zaslav/">meaningfully challenging</a> money, power, or the status quo. </p>
<p>The result of our collective superficiality isn’t hard to find when looking at the tech knowledge of the broader public. A <a href="https://www.pewresearch.org/internet/2023/08/17/what-americans-know-about-ai-cybersecurity-and-big-tech/" data-type="link" data-id="https://www.pewresearch.org/internet/2023/08/17/what-americans-know-about-ai-cybersecurity-and-big-tech/">recent Pew survey of 5,101 U.S. adults</a> found that 80 percent of Americans know that Elon Musk now owns Tesla and Twitter, but <em>just 23 percent were aware that the United States lacks a meaningful privacy law</em> addressing how companies can use the data they collect:</p>
<figure><img decoding="async" fetchpriority="high" width="1024" height="890" src="https://i0.wp.com/www.techdirt.com/wp-content/uploads/2023/08/1-1.jpg?resize=1024%2C890&amp;ssl=1" alt="" srcset="https://i0.wp.com/www.techdirt.com/wp-content/uploads/2023/08/1-1.jpg?resize=1024%2C890&amp;ssl=1 1024w, https://i0.wp.com/www.techdirt.com/wp-content/uploads/2023/08/1-1.jpg?resize=300%2C261&amp;ssl=1 300w, https://i0.wp.com/www.techdirt.com/wp-content/uploads/2023/08/1-1.jpg?resize=768%2C667&amp;ssl=1 768w, https://i0.wp.com/www.techdirt.com/wp-content/uploads/2023/08/1-1.jpg?resize=600%2C521&amp;ssl=1 600w, https://i0.wp.com/www.techdirt.com/wp-content/uploads/2023/08/1-1.jpg?w=1326&amp;ssl=1 1326w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></figure>
<p>52 percent of the public wasn’t sure if we had a privacy law. At the same time, while 77 percent of the public knows that Facebook changed its name to Meta in 2021, less than half (48 percent) of those surveyed know what <a href="https://en.wikipedia.org/wiki/Multi-factor_authentication" data-type="link" data-id="https://en.wikipedia.org/wiki/Multi-factor_authentication">two-factor authentication</a> is. And while 87 percent know that more complicated passwords are better, just 32 percent have a basic understanding of how “AI” (LLMs) function. </p>
<p>When the press covers consumer privacy, the fact that the U.S. government has proven too corrupt to pass even a <strong>basic</strong> internet-era privacy law rarely gets mentioned. The idea that the government has been lobbied into apathy on this subject for 30 years by a broad coalition of industries (opposed to anything but the most toothless oversight) rarely even warrants a mention in mainstream tech coverage. </p>
<p>While I’m sure a superficial, clickbait obsessed tech press isn’t the only culprit here (our <a href="https://www.thebalancemoney.com/the-u-s-is-losing-its-competitive-advantage-3306225" data-type="link" data-id="https://www.thebalancemoney.com/the-u-s-is-losing-its-competitive-advantage-3306225">shaky education standards</a> surely play a role), I can’t imagine it helps much. As a tech reporter I’ve watched a long, long line of quality independent tech news outlets get dismantled in favor of <a href="https://www.techdirt.com/2023/02/08/cnet-insiders-say-tech-outlet-softened-coverage-to-please-advertisers/" data-type="link" data-id="https://www.techdirt.com/2023/02/08/cnet-insiders-say-tech-outlet-softened-coverage-to-please-advertisers/">superficial clickbait machines</a>, terrified of offending anyone in power, whose output is now being <a href="https://www.techdirt.com/2023/07/07/ai-journalism-continues-to-be-a-lazy-error-prone-mess/" data-type="link" data-id="https://www.techdirt.com/2023/07/07/ai-journalism-continues-to-be-a-lazy-error-prone-mess/">clumsily supercharged by “AI”</a>. </p>
<p>Tech journalism’s failure to accurately portray the sorry state of U.S. privacy was perfectly exemplified by coverage over the TikTok privacy scandals. Endless outlets parroted worries that a single app might share U.S. consumer data with the Chinese government; few if any could be bothered to note that same Chinese government can buy endless reams of consumer data from barely regulated data brokers. </p>
<p>As a broadband and telecom beat reporter in particular, I’ve similarly seen how when press outlets cover substandard broadband, the real underlying problem (consolidated monopoly power has lobbied a corrupt government into apathy) again rarely warrants a mention. It’s systemic, and until we dedicate some serious time toward creatively funding independent journalism, it’s simply not getting better. </p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/corruption/" rel="tag">corruption</a>, <a href="https://www.techdirt.com/tag/federal-privacy-law/" rel="tag">federal privacy law</a>, <a href="https://www.techdirt.com/tag/legislation/" rel="tag">legislation</a>, <a href="https://www.techdirt.com/tag/pew/" rel="tag">pew</a>, <a href="https://www.techdirt.com/tag/privacy/" rel="tag">privacy</a>, <a href="https://www.techdirt.com/tag/survey/" rel="tag">survey</a>, <a href="https://www.techdirt.com/tag/tech/" rel="tag">tech</a>, <a href="https://www.techdirt.com/tag/two-factor-authentication/" rel="tag">two factor authentication</a>
<br>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The EU's war on behavioral advertising (229 pts)]]></title>
            <link>https://thisisunpacked.substack.com/p/the-eu-war-on-behavioral-advertising</link>
            <guid>37263322</guid>
            <pubDate>Fri, 25 Aug 2023 16:07:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thisisunpacked.substack.com/p/the-eu-war-on-behavioral-advertising">https://thisisunpacked.substack.com/p/the-eu-war-on-behavioral-advertising</a>, See on <a href="https://news.ycombinator.com/item?id=37263322">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Every few decades, major shifts happen that change how an industry fundamentally operates, and who the new winners and losers are. We are in the midst of one such shift in the digital advertising industry. This has been driven by three critical changes in the ecosystem, all privacy-related:</p><ul><li><p><span>Apple announced the </span><a href="https://www.adexchanger.com/privacy/ios-14-5-is-live-att-enforcement-begins-and-heres-how-we-got-here/" rel="">App Tracking Transparency framework</a><span> in 2021</span></p></li><li><p><span>Google is </span><a href="https://privacysandbox.com/open-web/#the-privacy-sandbox-timeline" rel="">deprecating third party cookies</a><span> in 2024</span></p></li><li><p><span>A slew of privacy legislation is either already in place (like the </span><a href="https://oag.ca.gov/privacy/ccpa" rel="">California Privacy Rights Act</a><span>, </span><a href="https://gdpr-info.eu/" rel="">EU General Data Protection Regulation</a><span>) or is about to be in place (like the </span><a href="https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package" rel="">EU Digital Services Act</a><span>)</span></p></li></ul><p><span>While the effects of these changes have been felt by the advertising industry globally, the </span><strong>EU in particular has been at war with behavioral advertising</strong><span> for a few years now. </span><strong>And winning</strong><span>. In the past three weeks, there have been significant announcements from </span><a href="https://techcrunch.com/2023/08/01/meta-says-yes-to-consent/" rel="">Meta</a><span>, </span><a href="https://techcrunch.com/2023/08/23/snapchat-dsa-compliance/" rel="">Snap</a><span> and </span><a href="https://techcrunch.com/2023/08/04/tiktok-algorithm-dsa/" rel="">TikTok</a><span> about upcoming changes to their products in the EU. It’s probably fair to say these changes mark the end of an era for behavioral advertising in the EU, and the platforms have made peace with that.</span></p><p>In this piece, we’ll dive into:</p><ul><li><p>How the advertising ecosystem runs (or used to run)</p></li><li><p>Privacy interventions and impact on behavioral advertising</p></li><li><p>EU’s war on behavioral advertising and why platforms are conceding</p></li><li><p>Possible future paths</p></li></ul><p>Let’s start with some simple concepts. An ad shown to you can be contextual or behavioral (sometimes both).</p><p><span>A </span><strong>contextual ad</strong><span> is something that is shown alongside organic results (non-ads) in the context of whatever you are doing. For example, if you are on a food delivery app like DoorDash and you see an ad for a restaurant alongside organic results, that is a contextual ad. In that case, your identity doesn’t really matter. DoorDash knows your location and what you’re looking for, and therefore shows you a relevant ad. Knowing your identity and interests does help improve relevance of these ads but it’s secondary to the context itself.</span></p><p><span>A </span><strong>behavioral ad</strong><span> is something that’s shown to you because of your past behavior. For example, you might have visited an office chair brand’s page on Instagram and therefore you get served ads for another office chair brand; that’s exactly what happened to me with the ad on the right. This can either be based on direct behavior (like the office chair example), or inferred behavior - people like you that looked for chairs also looked for monitors. One step further, people that looked at chairs are also likely to work from home more and therefore open to a lunch service. All of those are potential ads that can be shown to you.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e6f5b81-3bce-456f-8bae-5894b75c6ca1_1119x970.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e6f5b81-3bce-456f-8bae-5894b75c6ca1_1119x970.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e6f5b81-3bce-456f-8bae-5894b75c6ca1_1119x970.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e6f5b81-3bce-456f-8bae-5894b75c6ca1_1119x970.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e6f5b81-3bce-456f-8bae-5894b75c6ca1_1119x970.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e6f5b81-3bce-456f-8bae-5894b75c6ca1_1119x970.png" width="452" height="391.8141197497766" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2e6f5b81-3bce-456f-8bae-5894b75c6ca1_1119x970.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:970,&quot;width&quot;:1119,&quot;resizeWidth&quot;:452,&quot;bytes&quot;:1169040,&quot;alt&quot;:&quot;Contextual ad on DoorDash (left) vs Behavioral ad on Instagram Stories (right)&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="Contextual ad on DoorDash (left) vs Behavioral ad on Instagram Stories (right)" title="Contextual ad on DoorDash (left) vs Behavioral ad on Instagram Stories (right)" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e6f5b81-3bce-456f-8bae-5894b75c6ca1_1119x970.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e6f5b81-3bce-456f-8bae-5894b75c6ca1_1119x970.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e6f5b81-3bce-456f-8bae-5894b75c6ca1_1119x970.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e6f5b81-3bce-456f-8bae-5894b75c6ca1_1119x970.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Contextual ad on DoorDash (left) vs Behavioral ad on Instagram Stories (right)</figcaption></figure></div><p>A subset of products, most often search products, are perfect for contextual ads - Google Maps, Google Search, DoorDash, Yelp, Thumbtack, TripAdvisor, Zocdoc to name a few. The user has high intent to do something specific. However, most high intent products are low-medium frequency. You are not looking for plumbers every day on Thumbtack and you are not looking to order food delivery every hour of the day on DoorDash, and therefore the amount of contextual ad inventory is lower - highly effectively but lower in volume.</p><p>On the other hand, social media platforms where users spend hours doomscrolling have a large volume of non-contextual inventory. Random ads are annoying and ineffective, and therefore the most effective way to monetize this inventory is through behavioral ads.</p><p><span>Unsurprisingly, </span><strong>behavioral ads require a lot of data, and specifically data about you</strong><span>. Before ~2018 (when EU’s GDPR kicked in), there was essentially </span><strong>free flow of data</strong><span> about you, collected through a long list of highly effective AdTech mechanisms. Some notable examples:</span></p><ul><li><p><span>Every iOS or Android phone had a unique “</span><a href="https://kb.narrative.io/mobile-advertising-ids" rel="">mobile advertising ID</a><span>” assigned to it (and by association, to you). This ID was accessible to all apps on your device, i.e. your Uber app and your Google Maps app both knew you are mobile advertising ID 123, and could easily cross-identify you across apps.</span></p></li><li><p><span>Most browsers (except Safari and Firefox) supported “</span><a href="https://cookie-script.com/all-you-need-to-know-about-third-party-cookies.html" rel="">third party cookies</a><span>”, which is essentially a piece of data that one site could place on your browser and another site could access. There would be a “cookie ID” associated with you and you could be easily cross-identified across websites.</span></p></li></ul><p><span>Note that </span><strong>these identifiers, for all practical purposes, were permanent</strong><span>. You could go deep inside your settings and reset these IDs but most people never do that. This resulted in a couple of second order effects:</span></p><ul><li><p><span>Companies that are “</span><strong>data brokers</strong><span>” started purchasing data from several different data providers and </span><strong>building “profiles” about you</strong><span>; for example, app 1 could tell the broker that you bought an expensive piece of furniture, website 2 could tell them you have an account with a high end bank, and the data broker could put that together and categorize you as “high propensity of spend” person; note that when I say “you”, that refers to your identifiers (mobile advertising ID or third party cookie ID)</span></p><ul><li><p>As more and more data became available, the profiles had more fidelity to them and identification became more sophisticated; for example, if app 1 had your mobile advertising ID and website 2 had your Chrome third party cookie ID, a data broker or an ad platform could compare your your IP addresses and know it is the same person.</p></li><li><p>Take this one step further. A data broker could also use a third data source and find an email associated with your IDs. One more step further, they could buy your address from a utility or telecom company and find your address.</p></li><li><p><span>This might sound far-fetched but it’s not - </span><a href="https://www.onaudience.com/resources/how-to-choose-a-device-graph-vendor/#:~:text=A%20device%20graph%20(a.k.a.%20identity,also%20an%20iPhone%20and%20iPad." rel="">device graphs</a><span> are a very commonly used data product in the AdTech stack today, and telecom companies are </span><a href="https://cybernews.com/privacy/us-telecom-companies-selling-users-data/#:~:text=Telecom%20companies%20actually%20have%20the,without%20any%20court%20order%20instantly." rel="">notorious for selling your data</a><span>. If you need a more entertaining take, check out </span><a href="https://www.youtube.com/watch?v=wqn3gR1WTcA" rel="">John Oliver’s episode on data brokers</a><span>.</span></p></li></ul></li><li><p><strong>Attribution / measurement of advertising campaigns became more and more precise</strong><span>; through a combination of technical mechanisms, an advertiser could say you first saw an ad on Facebook, then an ad on Google Search, then a display ad on NYTimes, and eventually bought an item from their website, so each of those three advertising platforms get shared credit</span></p></li></ul><p>While this sounds privacy invasive (and it is), this resulted in a highly efficient advertising ecosystem. Advertisers knew exactly which users they were targeting and since they had all these extra behavioral signals to know how likely a user was to engage, they were willing to pay higher cost per impressions (CPM) for ad inventory, thereby generating more revenue for a media publisher. Precise attribution / measurement turbo charged this further.</p><p><span>However, you can see how this was </span><strong>becoming the wild west - highly effective advertising no doubt but also an uncontrolled orgy of data acquired with non-existent or questionable user consent</strong><span>. This rightfully raised concerns about data consolidation in the hands of both data brokers and large technology companies, and an intervention was inevitable. It was less a matter of if and more a matter of when.</span></p><p>Let’s dive into each of the three privacy interventions that arose in an attempt to tame the wild west.</p><p><span>First, Apple introduced the </span><a href="https://www.adexchanger.com/privacy/ios-14-5-is-live-att-enforcement-begins-and-heres-how-we-got-here/" rel="">App Tracking Transparency (ATT) framework</a><span>. It sounds jargon-y but the change is relatively simple. Prior to ATT, every app by default had access to your advertising ID, i.e. it was opt-in by default. This meant you could easily be tracked across apps, and therefore shown effective behavioral ads. For example, you installed the Strava app to track your runs, you are now on Facebook, and you are shown an ad for Strava Premium. After ATT, the access to this </span><strong>identifier became opt-out by default</strong><span>, i.e. an app had to show you a pretty aggressive prompt to get access to your identifier and you explicitly needed to say yes. The average opt-in rate ended up close to </span><a href="https://www.adjust.com/blog/app-tracking-transparency-opt-in-rates/" rel="">~34%</a><span> (with a lot of caveats).</span></p><p><span>We won’t go into much detail here but this prompt was launched by Apple in the guise of embracing privacy - a smart chess move. The consensus opinion today is that this was an opportunistic move from Apple, which no doubt improves privacy but also heavily hurts Apple’s competitors as they </span><a href="https://www.forbes.com/sites/daviddoty/2023/02/08/apple-the-story-behind-its-new-ad-offerings-to-retailers-restaurants-hotels-other-location-based-businesses/?sh=69dd5a994701" rel="">prop up their own ads business</a><span>. The impact  was that user identity was available much less often.</span></p><p><span>Note here that for behavioral advertising to happen in the Strava-Facebook example, there is not one but two apps that needs to have received opt-in from you, i.e. the addressable market does not drop to 34%, it drops to 34% * 34% = ~12%. Therefore, </span><strong>cross-app behavioral advertising on iOS is no longer effective at scale</strong><span>.</span></p><p><span>Second, Google announced that they will </span><a href="https://blog.google/products/chrome/updated-timeline-privacy-sandbox-milestones/" rel="">deprecate third-party cookies in 2024</a><span>. The consensus opinion is that the change helps Google achieve a dual purpose: appease regulators who are breathing down their neck for </span><a href="https://www.justice.gov/opa/pr/justice-department-sues-google-monopolizing-digital-advertising-technologies" rel="">potential anti-trust behavior in AdTech</a><span>, while taking control back from what’s now a </span><a href="https://www.sovrn.com/wp-content/uploads/2015/05/ProgramaticBuyingEcosystem_transparent-1024x791.png" rel="">fairly bloated advertising tech stack</a><span>. Google’s </span><a href="https://developer.chrome.com/en/docs/privacy-sandbox/fledge/" rel="">new mechanisms</a><span> post third-party cookies will still allow cross-site retargeting but in a more private way where all information is stored on-device within the browser, i.e. there are no more cross-site “cookie IDs” assigned to you. While the new mechanism preserves some of the status quo, </span><strong>cross-site behavioral advertising is</strong><span> </span><strong>going to have much less fidelity and therefore effectiveness</strong><span>.</span></p><p><span>Which brings us to the third intervention - </span><strong>privacy regulations</strong><span>. The most aggressive of these is EU’s GDPR, which went into effect in 2018. The California Consumer Privacy Act (CCPA) went into effect in 2020. While the progression has been gradual, the reason these laws matter for advertising companies today more that ever is because the laws take aim at the </span><strong>only remaining and mission critical advertising mechanism</strong><span> -</span><strong> behavioral advertising within companies’ own apps</strong><span> (i.e. you do a bunch of different things inside the Facebook app and Facebook gets to use that data to show you behavioral ads within the app).</span></p><p><span>A primary feature that makes the California laws (arguably the strictest privacy law in the US) less aggressive than EU’s GDPR is that it </span><strong>does not require explicit opt-ins and only requires platforms to provide opt-outs</strong><span>. For example, the 2020 California Consumer Privacy Act (CCPA) requires companies that are considered “data sellers” under the law to provide explicit opt-outs on web pages, but the </span><strong>default is still opt-in</strong><span>.</span></p><p><span>EU’s GDPR takes this up another notch and requires </span><strong>explicit opt-in / consent </strong><span>for behavioral advertising. This consent needs to be </span><a href="https://gdpr.eu/gdpr-consent-requirements/#:~:text=Article%204(11)%20defines%20consent,relating%20to%20him%20or%20her." rel="">freely given, specific, informed and unambiguous</a><span>. For example, Meta cannot gate content behind a behavioral advertising consent prompt.</span></p><p>So, the simplistic inference from this is that Meta needs to get explicit consent for all behavioral understanding, including the last remaining mechanism - showing ads within their own platform. If Meta is forced to do this, the opt in will likely be small (the Apple opt in rates were ~34%) and this majorly shrinks Meta’s addressable advertising market in the EU.</p><p>To not meet that fate, Meta made a creative legal argument:</p><ul><li><p><span>GDPR requires any company to have one of </span><strong><a href="https://gdpr-info.eu/art-6-gdpr/" rel="">six legal bases</a><span> </span></strong><span>if the company needs to process personal data. Paraphrasing, these are - consent, necessity to fulfill a contract, necessity for legal compliance, necessity to protect vital interests of the user, necessity to perform a public interest task, and necessity for legitimate interests pursued by the company</span></p></li><li><p><span>Of these six, Meta held the stance that they were processing personal data using “</span><strong>legitimate interests” (the last of the six) as the legal basis, and therefore  does not need to show an explicit opt-in prompt</strong></p></li></ul><p><span>Last month, the Norwegian Data Protection Authority </span><a href="https://techcrunch.com/2023/07/17/norway-meta-ads-ban/" rel="">provided their enforcement decision</a><span> that </span><strong>Meta’s use of “legitimate interests” as the legal basis is not valid</strong><span>. Paraphrasing the </span><a href="https://www.datatilsynet.no/contentassets/36ad4a92100943439df9a8a3a7015c19/urgent-and-provisional-measures--meta_redacted.pdf" rel="">enforcement decision</a><span>:</span></p><ul><li><p>Based on several past court rulings, Meta must prove that legitimate interest “cannot reasonably be achieved just as effectively by other means less restrictive of the fundamental rights and freedoms of data subjects”</p></li><li><p>Based on another past court ruling, despite the face that Meta provides a free service, the user of that network cannot reasonably expect that the operator of the social network will process that user’s personal data, without his or her consent, for the purposes of personalized advertising</p></li></ul><p><span>Four days after the Norwegian Data Protection Authority’s ruling, Meta </span><a href="https://about.fb.com/news/2023/01/how-meta-uses-legal-bases-for-processing-ads-in-the-eu/" rel="">announced</a><span> that they will be changing their legal basis from “legitimate interests” to “consent”. In practice, what this means is that </span><strong>Meta is conceding that behavioral advertising within their own app in the EU can no longer be opt-in by default</strong><span>. While Apple’s changes significantly cut down the ability to advertise cross-app, Meta was holding on to hope that it would be able to preserve all in-app behavioral advertising (including in the EU), and the writing is now on the wall. </span><strong><a href="https://techcrunch.com/2023/08/23/snapchat-dsa-compliance/" rel="">Snap</a><span>, </span><a href="https://techcrunch.com/2023/08/04/tiktok-algorithm-dsa/" rel="">TikTok</a><span> followed suit</span></strong><span> shortly after with their own announcements that non-personalized versions of their products will soon be available to users. A total of </span><a href="https://techcrunch.com/2023/04/25/europe-names-19-platforms-that-must-report-algorithmic-risks-under-dsa/" rel="">19 platforms that are in scope</a><span> are likely to follow suit.</span></p><p>It is still to be seen what percent of users opt into behavioral advertising. If the Apple opt-in rates are any indication, it may get to the ~34% rate seen there but it may also go higher if platforms are allowed to be creative about opt-in language and user interfaces.</p><p>It is hard to say what exactly the long-term effects of ending the opt-in-by-default regime for behavioral advertising in the EU will be, but here are some educated guesses:</p><ul><li><p><strong>Explicit preferences instead of inferred interests</strong><span> - Meta has made </span><a href="https://www.socialmediatoday.com/news/meta-provides-insight-evolving-feed-algorithms-ai/654367/" rel="">some progress</a><span> towards letting users provide inputs to the feed algorithm. I would expect the creation of more explicit preferences / interests that users explicitly select. While it has worked to a smaller extent for some products like </span><a href="https://searchengineland.com/reddit-interest-targeting-categories-ads-431085" rel="">Reddit</a><span>, it is likely that the quality of content provided by just preferences will be nowhere close to the relevance of a system that is constantly ingesting data and inferring interests.</span></p></li><li><p><strong>Non-personalized feeds</strong><span>: TikTok has announced that they will create a </span><a href="https://techcrunch.com/2023/08/04/tiktok-algorithm-dsa/" rel="">non-personalized feed</a><span> that is based on what’s popular in your region. Most social feed-based platforms will also (re)introduce simplistic ranking options like chronological. I am not bullish any of these will create an engaging user experience.</span></p></li><li><p><strong>Degraded user experience, leading to more opt-in</strong><span>: My (potentially contentious) take is that opted out users will see a significantly degraded experience and eventually decide to provide platforms consent to personalize. The platforms will likely use this mechanism to receive content for all personalization (including behavioral advertising) - it’s unclear if there are any regulatory constraints that force them to not bundle consent for user experience personalization and behavioral advertising. I don’t expect the consent rate to go anywhere close to status quo but I think it will be a meaningful bump to what’s seen with Apple’s opt in rates.</span></p></li><li><p><strong>Experiments with subscriptions</strong><span>: Twitter launched Twitter Verified a few months back, and Meta is </span><a href="https://about.fb.com/news/2023/02/testing-meta-verified-to-help-creators/" rel="">experimenting</a><span> with a similar concept. I am not bullish that a majority (or even a minority) of users will be willing to pay for social media products after years of being conditioned to getting these products for free.</span></p></li><li><p><strong>De-prioritization of EU markets: </strong><span>If platforms are not able to effectively monetize the EU market, it is likely that they will start de-prioritizing investments for the EU market, which is a real risk of an aggressive regulatory regime. I don’t expect them to stop supporting EU markets but every new feature built is an investment, and companies will start heavily questioning if they are willing to do any extra work for this market. For example, if there’s additional work to make a feature comply with GDPR, that might not be worth it anymore. We have seen other equivalent situations of this - several apps have a much better experience on iOS than Android because iOS apps monetize better.</span></p></li></ul><p>The advertising market exploded over the last several decades due to availability of essentially infinite user data, which brought both large efficiencies in advertising and major privacy risks. Regulation was necessary. Great regulation is about finding the balance between promoting innovation / letting new businesses emerge that move society forward, and having guardrails so people are protected. Did the EU go too far? I personally think they did, and if that is true, the effects will start showing in a 5-10 year horizon and course correction will follow. </p><p>Until then, this is the new reality of behavioral advertising in the EU. The regulators and legislators came all guns blazing, and they won.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HelloSystem: A graphical OS built on FreeBSD (213 pts)]]></title>
            <link>https://hellosystem.github.io/docs/</link>
            <guid>37262857</guid>
            <pubDate>Fri, 25 Aug 2023 15:32:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hellosystem.github.io/docs/">https://hellosystem.github.io/docs/</a>, See on <a href="https://news.ycombinator.com/item?id=37262857">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody" role="main" itemscope="itemscope" itemtype="http://schema.org/Article">
             
  <p><a href="https://raw.githubusercontent.com/helloSystem/hello/1d1e69be8a689c5e0a176df821c14f0b49b241a4/branding/hello_variation.svg"><img alt="The helloSystem logo" src="https://raw.githubusercontent.com/helloSystem/hello/1d1e69be8a689c5e0a176df821c14f0b49b241a4/branding/hello_variation.svg" width="300px"></a></p><p>
<strong>Willkommen • Welcome • Bienvenue • Benvenuto • Bienvenido • ようこそ • Mabuhay • Tervetuloa • Välkommen • Добро пожаловать • Hoş geldiniz • Bonvenon • 歡迎</strong></p><div id="hello">

<p><strong>helloSystem</strong> is a desktop system for creators with a focus on simplicity, elegance, and usability. Its design follows the “Less, but better” philosophy. It is intended as a system for “mere mortals”, welcoming to switchers from the Mac. <a href="https://www.freebsd.org/">FreeBSD</a> is used as the core operating system. Please refer to <a href="https://github.com/helloSystem/hello">https://github.com/helloSystem/hello</a> if you would like to learn more about the ideas and principles behind hello.</p>
<p><img alt="Screenshot" src="https://github.com/helloSystem/hello/blob/master/screenshots/20220121-desktop-0.8.png?raw=true"></p>



</div>


           </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenTF Announces Fork of Terraform (1349 pts)]]></title>
            <link>https://opentf.org/announcement</link>
            <guid>37262440</guid>
            <pubDate>Fri, 25 Aug 2023 14:56:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://opentf.org/announcement">https://opentf.org/announcement</a>, See on <a href="https://news.ycombinator.com/item?id=37262440">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <a href="https://opentf.org/">
          <picture>
              <source srcset="https://opentf.org/images/on-dark%402x.png" media="(prefers-color-scheme: dark)">
              <img width="342px" height="200px" src="https://opentf.org/images/on-light%402x.png" alt="OpenTF Foundation">
          </picture>
      </a>

      <section id="announcements">
        <h2>OpenTF Announces Fork of Terraform</h2>
        <article>
          <p>Posted on: <time datetime="2023-08-25">August 25, 2023</time></p>
          <p> 
            Two weeks ago, HashiCorp announced they are changing the license to all their core products, including Terraform, to the Business Source License (BSL). In an attempt to keep Terraform open source, we published the <a href="https://opentf.org/">OpenTF manifesto</a>, and the community response was huge! Over 100 companies, 10 projects, and 400 individuals pledged their time and resources to keep Terraform open-source. The <a href="https://github.com/opentffoundation/manifesto">GitHub repository</a> for the manifesto already has over 2.5k stars, and the number is growing quickly!
          </p>
          <p>
            The manifesto outlined the intent of the OpenTF initiative in two steps — the first was to appeal to HashiCorp to return Terraform to the community and revert the license change they were making for this project. The second, in case the license was not reverted, was to fork the Terraform project as OpenTF.
          </p>
          <h4>The time is now!</h4>
          <p>
            Since no reversal has been done, and no intent to do one has been communicated, we’re proud to announce that <b>we have created a fork of Terraform called OpenTF</b>. Many engineers across a number of companies, sometimes even competing companies, have been working together over the last week to make this possible. It’s been an incredible experience, really!
          </p>
          <p>
            As outlined in our manifesto, we are keeping OpenTF:
          </p>
          <ul>
            <li><b>Truly open source</b> - under a well-known and widely-accepted license that companies can trust, that won't suddenly change in the future
            </li>
            <li><b>Community-driven</b> - so that the community governs the project for the community, where pull requests are regularly reviewed and accepted on their merit and changes are proposed through a public RFC process
            </li>
            <li><b>Impartial</b> - so that valuable features and fixes are accepted based on their value to the community, regardless of their impact on any particular vendor
            </li>
            <li><b>Layered and modular</b> -  with a programmer-friendly project structure to encourage building on top, enabling a new vibrant ecosystem of tools and integrations</li>
            <li><b>Backwards-compatible</b> - so that the existing code can drive value for years to come</li>
          </ul>
          <h4>Becoming part of a foundation</h4>
          <p>
            <b>We completed all documents required for OpenTF to become part of the Linux Foundation</b> with the end goal of having <b>OpenTF as part of Cloud Native Computing Foundation</b>. By making a foundation responsible for the project, we will ensure the tool stays truly open-source and vendor-neutral.
          </p>
          <p>
            If Terraform wasn’t open-source from the beginning, many of the tools that you are using right now for your Terraform workflows simply wouldn’t exist, thus, we believe the future for Terraform is OpenTF, developed fully in the open.
          </p>
          <h4>Roadmap</h4>
          
          <p>
            As previously outlined, we’ve been working on this fork for a number of days already, with over 10 engineers across multiple companies working on it.
          </p>
          <p>
            <b>In short, here’s the current status</b>:
          </p>
          <ul>
            <li>Almost done with the repository-wide rename to OpenTF</li>
            <li>Selected initial steering committee members</li>
            <li>Performed initial adjustments and cleanup of community documents.</li>
            <li>Got CI/CD pipelines and multiple testing harnesses of end-to-end and snapshot tests to work and be green, to make sure that we stay backwards-compatible.
            </li>
          </ul>
          <p>
            <b>Expect the repository to be published very soon</b>, once we’re officially part of a foundation and have some basic community guardrails and processes in place.
          </p>
          <p>
            <b>You might wonder why we already started work on this project so early?</b> It’s quite simple, really. If HashiCorp were to reverse their decision, worst case we’d just lose a week of work. But if, and that is what indeed happened, HashiCorp wasn’t to reverse their decision, we didn’t want to lose any time, so that <b>we could have a working OpenTF 1.6.0 release ready for you as soon as possible</b>. And that’s why we started work on this over a week ago.
          </p>
          <p>
            In the spirit of being as open as possible, <b>we’ve created a</b> <a href="https://github.com/opentffoundation/roadmap/milestones">public repository tracking our progress towards important milestones.</a> You can subscribe to the issues there to be notified as soon as the fork is public. If you have any questions, feel free to create additional issues on that repository - we’ll try to respond as quickly as possible.
          </p>
        </article>
      </section>

      <section>
        <h2>FAQ</h2>
        <p>
          <b>When will I be able to see the fork?</b>
        </p>
        <p>
          We’re planning to publish the fork in the next 1-2 weeks. That doesn’t mean there will be a release by then, but the repository will be open. Releases (alpha and stable) should follow shortly.
        </p>
        <p>
          <b>Will I be able to use OpenTF as a drop-in replacement for legacy Terraform?</b>
        </p>
        <p>
          Yes
        </p>
        <p>
          <b>Will OpenTF work with all the providers and modules Terraform works with?</b>
        </p>
        <p>
          Yes
        </p>
        <p>
          <b>What will be the first release of OpenTF?
          </b>
        </p>
        <p>
          The first release will be 1.6.0-alpha, forked from the most recent commit that was still MPL-licensed.
        </p>
        <p>
          <b>Who is maintaining OpenTF? Is there enough firepower behind the project?</b>
        </p>
        <p>
          So far, four companies pledged the equivalent of 14 full-time engineers (FTEs) to the OpenTF initiative. We expect this number to at least double in the following few weeks. To give you some perspective, Terraform was effectively maintained by about 5 FTEs from HashiCorp in the last 2 years. If you don’t believe us, look at their repository.
        </p>
        <p>
          <b>How can I contribute to OpenTF?</b>
        </p>
        <p>
          If you’d like to pledge resources as a company, please get in touch via email or PR on the manifesto repository. Otherwise, the best way to contribute right now is to spread the word! Once the repository is public and we have a contribution process in place, you’ll be able to do code contributions.
        </p>
        <p>
          <b>Does OpenTF have a public roadmap? Can I affect it?</b>
        </p>
        <p>
          You can find current public roadmap on this <a href="https://github.com/opentffoundation/roadmap/milestones">GitHub repository</a>. We will put an RFC process in place for you to be able to contribute to the roadmap as soon as possible.
        </p>
        <p>
          <b>Will OpenTF be compatible with future Terraform releases?</b>
        </p>
        <p>
          OpenTF will be 100% interoperable with future Terraform releases until the community wishes otherwise.
        </p>
        <p>
          <b>Can OpenTF change its licensing model in the future?</b>
        </p>
        <p>
          Once OpenTF becomes part of the foundation, it’s governed by the foundation and the project community. Its licensing model cannot change.
        </p>
      </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Throttles 404Media Investigation on Drug Ads, Continues to Advertise Drugs (170 pts)]]></title>
            <link>https://www.404media.co/instagram-throttles-404-media-investigation-into-drug-ads-on-instagram-continues-to-let-people-advertise-drugs/</link>
            <guid>37262370</guid>
            <pubDate>Fri, 25 Aug 2023 14:51:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/instagram-throttles-404-media-investigation-into-drug-ads-on-instagram-continues-to-let-people-advertise-drugs/">https://www.404media.co/instagram-throttles-404-media-investigation-into-drug-ads-on-instagram-continues-to-let-people-advertise-drugs/</a>, See on <a href="https://news.ycombinator.com/item?id=37262370">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
          <div>
              <p>Instagram limited the reach of a <a href="https://www.404media.co/instagram-ads-illegal-content-drugs-guns-hackers/">404 Media investigation</a> into ads for drugs, guns, counterfeit money, hacked credit cards, and other illegal content on the platform within hours of us posting it. Instagram said it did this because the content, which was about Instagram’s content it failed to moderate on its own platform, didn’t follow its “<a href="https://help.instagram.com/313829416281232?ref=404media.co">Recommendation Guidelines</a>.” Later that evening, while that post was being throttled, I got an ad for “MDMA,” and Meta’s ad library is still full of illegal content that can be found within seconds.</p><p>This means Meta continues to take money from people blatantly advertising drugs on the platform while limiting the reach of reporting about that content moderation failure. Instagram's Recommendation Guidelines limit the reach of posts that "promotes the use of certain regulated products such as tobacco or vaping products, adult products and services, or pharmaceutical drugs."</p><figure><img src="https://www.404media.co/content/images/2023/08/Screenshot_20230823-170129--1--1.png" alt="" loading="lazy" width="1080" height="2340" srcset="https://www.404media.co/content/images/size/w600/2023/08/Screenshot_20230823-170129--1--1.png 600w, https://www.404media.co/content/images/size/w1000/2023/08/Screenshot_20230823-170129--1--1.png 1000w, https://www.404media.co/content/images/2023/08/Screenshot_20230823-170129--1--1.png 1080w" sizes="(min-width: 720px) 720px"></figure><p>Meta reversed its decision <a href="https://www.instagram.com/p/CwS47_OOeIg/?img_index=1&amp;ref=404media.co">on 404 Media’s post</a> after we appealed it and deleted the words “guns, meth, pills, and weapons” from our caption, which was describing the types of content that Instagram is taking money for and injecting into users’ feeds. </p><figure><img src="https://www.404media.co/content/images/2023/08/Screen-Shot-2023-08-24-at-8.41.35-PM.png" alt="" loading="lazy" width="2000" height="1232" srcset="https://www.404media.co/content/images/size/w600/2023/08/Screen-Shot-2023-08-24-at-8.41.35-PM.png 600w, https://www.404media.co/content/images/size/w1000/2023/08/Screen-Shot-2023-08-24-at-8.41.35-PM.png 1000w, https://www.404media.co/content/images/size/w1600/2023/08/Screen-Shot-2023-08-24-at-8.41.35-PM.png 1600w, https://www.404media.co/content/images/size/w2400/2023/08/Screen-Shot-2023-08-24-at-8.41.35-PM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>A screengrab of the post. We had to edit out the words "guns, meth, pills, and weapons" in order to comply with Instagram's rules.</figcaption></figure><p>As I reported earlier this week, most of these ads are for blatantly illegal products and services and link directly to Telegram accounts where drugs/guns/hacked accounts can be bought. A sampling of these ads can be <a href="https://www.facebook.com/ads/library/?active_status=all&amp;ad_type=all&amp;country=US&amp;q=%22t.me%22&amp;sort_data[direction]=desc&amp;sort_data[mode]=relevancy_monthly_grouped&amp;search_type=keyword_exact_phrase&amp;media_type=all">found in seconds by searching for “t.me” on Meta’s Ad Library</a>.</p><p>I have reported on content moderation for many years, and understand how difficult moderating content at scale can be. But the fact remains that the company has a massive, obvious problem with how it reviews and approves ads. An expert who studies content moderation told us that our investigation suggests Meta is not as sophisticated at reviewing and approving ads as it is at moderating normal posts on its platforms.</p><p>“You need to think about how much of an issue, historically, general content moderation has been,” Karan Lala of the Integrity Institute, made up of former integrity team workers at companies like Facebook told me. “The maturity of the different integrity teams is relevant to the kind of problems platforms have had in the past.”</p><figure><img src="https://www.404media.co/content/images/2023/08/Screenshot-2023-08-25-at-7.29.56-AM.png" alt="" loading="lazy" width="864" height="1686" srcset="https://www.404media.co/content/images/size/w600/2023/08/Screenshot-2023-08-25-at-7.29.56-AM.png 600w, https://www.404media.co/content/images/2023/08/Screenshot-2023-08-25-at-7.29.56-AM.png 864w" sizes="(min-width: 720px) 720px"><figcaption>An ad we got hours after being throttled by Instagram.</figcaption></figure><p>In Meta’s case, its biggest ads-related issue has been with political ads. During the 2020 election campaign, it simply <a href="https://www.nytimes.com/2020/09/04/technology/facebooks-political-ads-block-election.html?ref=404media.co">banned all political ads</a> (months after the presidential election, <a href="https://www.nytimes.com/2021/03/03/technology/facebook-ends-ban-on-political-advertising.html?ref=404media.co#:~:text=The%20social%20network%20had%20prohibited,been%20criticized%20for%20spreading%20misinformation.">it began allowing them again</a>).</p><p>“With ads, a lot of the issue has been regulatory: Election ads spreading misinformation,” Lala said. “Ads for spam, ads for low quality content. Moderation of that seems a little less developed. In theory, these integrity teams should be feeding into the same systems. Ads should theoretically be going through a review process … a lot of these things are something that just should be getting caught [at the review process.]”</p><p>Meta did not respond to a request for comment. At the time of this writing, there are thousands of ads on Instagram and Facebook that link to Telegram channels and groups. Many of the most recent ones are for groups selling acid, MDMA, and hacked credit cards.</p><!--kg-card-begin: html--><!--kg-card-end: html-->
          </div>
        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leaked Wipeout source code leads to near-total rewrite and remaster (217 pts)]]></title>
            <link>https://arstechnica.com/gaming/2023/08/developer-rewrites-original-wipeout-from-abysmal-leaked-windows-source/</link>
            <guid>37262258</guid>
            <pubDate>Fri, 25 Aug 2023 14:42:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gaming/2023/08/developer-rewrites-original-wipeout-from-abysmal-leaked-windows-source/">https://arstechnica.com/gaming/2023/08/developer-rewrites-original-wipeout-from-abysmal-leaked-windows-source/</a>, See on <a href="https://news.ycombinator.com/item?id=37262258">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      Better living through code reduction    —
</h4>
            
            <h2 itemprop="description">"Either let it be, or shut this thing down and get a real remaster going."</h2>
                    </header>
        <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/Screenshot-2023-08-24-at-2.11.05-PM-800x450.png" alt="Screenshot from Wipeout Rewrite">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/Screenshot-2023-08-24-at-2.11.05-PM.png" data-height="2160" data-width="3840">Enlarge</a> <span>/</span> Do you remember how fast <em>Wipeout</em> was in 1995? It's just as fast now. You, however, may not be as fast. Just saying.</p><p>Psygnosis / Dominic Szablewski</p></figcaption>  </figure>

  




<!-- cache hit 289:single/related:9cff5691a050b2a853572e1840b45af8 --><!-- empty -->
<p>There have been a lot of <i>Wipeout</i> games released since the 1995 original, including <a href="https://arstechnica.com/gaming/2008/09/when-graphics-matter-the-case-for-wipeout-hd/"><i>Wipeout HD</i></a> and the <i><a href="https://arstechnica.com/gaming/2016/12/playstation-experience-2016-in-photos-and-games-psx-marks-the-spot/2/">Omega Collection</a></i>, but only the original has the distinction of having <a href="https://twitter.com/forestillusion/status/1508048268176990209">its Windows port source code leaked</a> by (<a href="https://www.thegamer.com/forest-of-illusion-game-preservation-shut-down-six-years/">since defunct</a>) archive Forest of Illusion.</p>
<p>Dominic Szablewski grabbed that code before it disappeared and set about creating a version that’s not just a port. He rewrote the game’s rendering, physics, sound, and generally “everything everywhere.” He <a href="https://phoboslab.org/log/2023/08/rewriting-wipeout">documented the project</a>, put <a href="https://github.com/phoboslab/wipeout-rewrite">his code on GitHub</a>, and has some version of a justification. “So let's just pretend that the leak was intentional, a <b>rewrite</b> of the source falls under fair use and the whole thing is abandonware anyway,” Szablewski writes.</p>
<p>Most of the code seemed to come from <i>Wipeout ATI 3D Rage Edition</i>,&nbsp;a “lackluster port for Windows” that was bundled with ATI GPUs, Szablewski wrote. It is a mess. There are fragments of code versions from DOS, PlayStation, Windows 95, and Windows 98, with lots of things shakily patched in, including some kludgey 25-to-30 frames-per-second physics calculations in moving from European PAL to North American NTSC. The result was bad geometry, sluggish performance, and even goofed text rendering.</p>
<p>Not that he doesn’t have sympathy. “The code may not be pretty, but the result justifies it all,” he wrote. The PSX launch title braved unseen hardware and 3D models and physics and holds up today. But this pack-in version “is some caffeine-induced nightmare code written under immense time pressure. The 5000 lines of <code>if else</code> that handles the menu state is a striking witness to this insanity.”</p>                                            
                                                        
  <div>
    <ul>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout1-150x150.png" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout1.png" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout1-980x551.png 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout1-1440x810.png 2560" data-sub-html="#caption-1963050">
          <figure>
            
                          <figcaption id="caption-1963050">
                <span></span>
                                  <p>
                    The menu seems so simple, you'd never guess it was once 5,000 lines of code.                  </p>
                                                  <p><span></span>
                                          Psygnosis / Dominic Szablewski                                      </p>
                              </figcaption>
                      </figure>
        </li>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout4-150x150.png" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout4.png" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout4-980x551.png 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout4-1440x810.png 2560" data-sub-html="#caption-1963049">
          <figure>
            
                          <figcaption id="caption-1963049">
                <span></span>
                                  <p>
                    Cruising along, hearing someone in a British voice mutter "Missile" at me, as if he was shopping for one at Tesco.                  </p>
                                                  <p><span></span>
                                          Psygnosis / Dominic Szablewski                                      </p>
                              </figcaption>
                      </figure>
        </li>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout2-150x150.png" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout2.png" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout2-980x550.png 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout2-1440x808.png 2560" data-sub-html="#caption-1963048">
          <figure>
            
                          <figcaption id="caption-1963048">
                <span></span>
                                  <p>
                    The first-person view in <em>Wipeout</em> is like a little difficulty boost.                  </p>
                                                  <p><span></span>
                                          Psygnosis / Dominic Szablewski                                      </p>
                              </figcaption>
                      </figure>
        </li>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout3-150x150.png" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout3.png" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout3-980x566.png 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/08/wipeout3-1440x832.png 2560" data-sub-html="#caption-1963047">
          <figure>
            
                          <figcaption id="caption-1963047">
                <span></span>
                                  <p>
                    Your author will not be completing this game for full review, as it is both nearly 30 years old and also he is bad at it.                  </p>
                                                  <p><span></span>
                                          Psygnosis / Dominic Szablewski                                      </p>
                              </figcaption>
                      </figure>
        </li>
          </ul>
  </div>

<p>As he digs into the specifics of his work, Szablewski takes the reader on a tour of PSX dev kits and how they handled Z-levels, how to translate yesterday’s triangles to today’s OpenGL, breaking the 30 FPS cap on a game that explicitly forbade that, and more. He takes the code from 40,699 lines to 7,731 and notably loved an excuse to work in C. “I had an absolute blast cleaning up this mess!”</p>
<p>Szablewski’s <i>Wipeout</i> rewrite can be compiled for Windows, Linux, Mac, and WASM (Web Assembly). You can even play it <a href="http://phoboslab.org/wipegame/">in your browser on his server</a> (please be gentle). I spent some time in it this morning, and let me tell you: I am not ready for anti-gravity racing in the year 2052. It was a struggle to even get to fourth place, but those struggles were due entirely to skill, not system. The web version feels buttery smooth, even when you’re continually clunking into walls. I had misremembered this game as having a lot more to it, but it’s all feel: the trance/prog music, the physics, the controls, and the sense that you’re always just slightly out of control.</p>
<p>What about Sony and their legions of lawyers? Szablewski writes that Sony has “demonstrated a lack of interest in the original” <i>Wipeout</i>, so he doesn’t expect to hear much. “If anyone at Sony is reading this, please consider that you have (in my opinion) two equally good options: either let it be, or shut this thing down and get a real remaster going. I’d love to help!”</p>

                                                </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How do domain-specific chatbots work? A retrieval augmented generation overview (119 pts)]]></title>
            <link>https://scriv.ai/guides/retrieval-augmented-generation-overview/</link>
            <guid>37261198</guid>
            <pubDate>Fri, 25 Aug 2023 13:00:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scriv.ai/guides/retrieval-augmented-generation-overview/">https://scriv.ai/guides/retrieval-augmented-generation-overview/</a>, See on <a href="https://news.ycombinator.com/item?id=37261198">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><small>Aug. 25, 2023</small></p><div>
      <p>There’s a popular open-source library called <a href="https://python.langchain.com/">LangChain</a> that can create chatbots that—among other things—do Q&amp;A over any website/document in 3 lines of code. Here’s <a href="https://python.langchain.com/docs/use_cases/question_answering/">an example of that from the langchain docs</a>.</p>
<div>
<pre><code id="target-element-current">from langchain.document_loaders import WebBaseLoader
from langchain.indexes import VectorstoreIndexCreator
loader = WebBaseLoader("http://www.paulgraham.com/greatwork.html")
index = VectorstoreIndexCreator().from_loaders([loader])
index.query("What should I work on?")</code></pre>
</div>
<div><p>Which outputs an answer specific to <a href="http://www.paulgraham.com/greatwork.html">Paul Graham’s essay</a>:</p>
<blockquote>
<p>The work you choose should ideally have three qualities: it should be something you have a natural aptitude for, something you have a deep interest in, and something that offers scope to do great work. If you're unsure, you could start by working on your own projects that seem excitingly ambitious to you. It's also beneficial to be curious, try lots of things, meet lots of people, read lots of books, and ask lots of questions. When in doubt, optimize for interestingness. It's okay to guess and be wrong sometimes, as this can lead to discovering what you're truly good at or interested in.</p>
</blockquote>
<p><em>Note: if you’re interested you can try <a href="https://scriv.ai/a/scriv/bots/b/pg-bot/chat/">the chatbot built from Paul Graham’s essay here</a>.</em></p>
<p>The first time you run this it feels like pure magic. How the heck does this work?</p>
<p>The answer is a process called <strong>retrieval augmented generation</strong>, or RAG for short. It is a remarkably simple concept, though also has incredible depth in the details of its implementation.</p>
<p>This post will provide a high-level overview of RAG. We’ll start from the big picture workflow of what’s happening, and then zoom in on all the individual pieces. By the end of it, you should have a solid understanding of how those three magic lines of code work, and all the principles involved in creating these Q&amp;A bots.</p>
<p>If you’re a developer trying to build bots like this, you’ll learn which knobs you can tweak and how to tweak them. If you’re a non-developer hoping to use AI tools on your dataset, you'll gain knowledge that will help you get the most out of them. And, if you’re just a curious mind, you’ll hopefully learn a thing or two about some of the technology that's upending our lives.</p>
<p>Let’s dive in.</p>
<h2>What is Retrieval Augmented Generation?</h2>
<p><strong>Retrieval augmented generation is the process of supplementing a user’s input to a large language model (LLM) like ChatGPT with additional information that you have <em>retrieved</em> from somewhere else. The LLM can then use that information to <em>augment</em> the response that it <em>generates</em>.</strong></p>
<p>The following diagram shows how it works in practice:</p></div>
<p><img alt="Retrieval Augmented Generation: An Overview" height="436" src="https://communitykeeper-media.s3.amazonaws.com/media/images/rag-overview.original.png" width="993"></p>
<div><p>It starts with a user’s question. For example “How do I do &lt;something&gt;?”</p>
<p>The first thing that happens is the <em>retrieval</em> step. This is the process that takes the user’s question and searches for the most relevant content from a knowledge base that might answer it. The retrieval step is by far the most important, and most complex part of the RAG chain. But for now, just imagine a black box that knows how to pull out the best chunks of relevant information related to the user’s query.</p></div>
<div>
      
        <p>Can't we just give the LLM the whole knowledge base?</p>
      
      <div>
        <p data-block-key="945eb">You might be wondering why we bother with retrieval instead of just sending the whole knowledge base to the LLM. One reason is that models have built-in limits on how much text they can consume at a time (though these are quickly increasing). A second reason is cost—sending huge amounts of text gets quite expensive. Finally, there is <a href="https://arxiv.org/abs/2307.03172">evidence</a> suggesting that sending small amounts of relevant information results in better answers.</p>
      </div>
    </div>
<div><p>Once we’ve gotten the relevant information out of our knowledge base, we send it, along with the user’s question, to the <em>large language model</em> (LLM). The LLM—most commonly ChatGPT—then “reads” the provided information and answers the question. This is the <em>augmented generation</em> step.</p>
<p>Pretty simple, right?</p>
<h2>Working backwards: Giving an LLM extra knowledge to answer a question</h2>
<p>We’ll start at the last step: <em>answer generation</em>. That is, let’s assume we already have the relevant information pulled from our knowledge base that we think answers the question. How do we use that to generate an answer?</p></div>
<p><img alt="Augmented Answer Generation" height="434" src="https://communitykeeper-media.s3.amazonaws.com/media/images/generation.original.png" width="658"></p>
<div><p>This process may feel like black magic, but behind the scenes it is <em>just a language model</em>. So in broad strokes, the answer is “just ask the LLM”. How do we get an LLM to do something like this?</p>
<p>We'll use ChatGPT as an example. And just like regular ChatGPT, it all comes down to prompts and messages.</p>
<h3>Giving the LLM custom instructions with the system prompt</h3>
<p>The first component is the <em>system prompt</em>. The system prompt gives the language model its overall guidance. For ChatGPT, the system prompt is something like “You are a helpful assistant.”</p>
<p>In this case we want it to do something more specific. And, since it’s a language model, we can just <em>tell it what we want it to do</em>. Here’s an example short system prompt that gives the LLM more detailed instructions:</p>
<blockquote>
<p>You are a Knowledge Bot. You will be given the extracted parts of a knowledge base (labeled with DOCUMENT) and a question. Answer the question using information from the knowledge base.</p>
</blockquote>
<p>We’re basically saying, <em>“Hey AI, we’re gonna give you some stuff to read. Read it and then answer our question, k? Thx.”</em> And, because AIs are great at following our instructions, it kind of just...  <em>works</em>.</p>
<h3>Giving the LLM our specific knowledge sources</h3>
<p>Next we need to give the AI its reading material. And again—the latest AIs are really good at just <em>figuring stuff out</em>. But, we can help it a bit with a bit of structure and formatting.</p>
<p>Here’s an example format you can use to pass documents to the LLM:</p>
<div><pre><span></span><code>------------ DOCUMENT 1 -------------

This document describes the blah blah blah...

------------ DOCUMENT 2 -------------

This document is another example of using x, y and z...

------------ DOCUMENT 3 -------------

[more documents here...]
</code></pre></div>

<p>Do you need all this formatting? Probably not, but it’s nice to make things as explicit as possible. You can also use a machine-readable format like JSON or YAML. Or, if you're feeling frisky, you can just dump everything in a giant blob of text. But, having some consistent formatting becomes important in more advanced use-cases, for example, if you want the LLM to cite its sources.</p>
<p>Once we’ve formatted the documents we just send it as a normal chat message to the LLM. Remember, in the system prompt we told it we were gonna give it some documents, and that’s all we’re doing here.</p>
<h3>Putting everything together and asking the question</h3>
<p>Once we’ve got our system prompt and our “documents” message, we just send the user’s question to the LLM alongside them. Here’s how that looks in Python code, using the OpenAI <a href="https://platform.openai.com/docs/guides/gpt/chat-completions-api">ChatCompletion API</a>:</p></div>
<div>
<pre><code id="target-element-current">openai_response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "system",
            "content": get_system_prompt(),  # the system prompt as per above
        },
        {
            "role": "system",
            "content": get_sources_prompt(),  # the formatted documents as per above
        },
        {
            "role": "user",
            "content": user_question,  # the question we want to answer
        },
    ],
)</code></pre>
</div>
<p><figcaption>Python code for doing a retrieval-augmented answer with OpenAI's ChatGPT</figcaption></p>
<div><p>That’s it! A custom system prompt, two messages, and you have context-specific answers!</p>
<p>This is a simple use case, and it can be expanded and improved on. One thing we haven't done is told the AI what to do if it can't find an answer in the sources. We can add these instructions to the system prompt—typically either telling it to refuse to answer, or to use its general knowledge, depending on your bot's desired behavior. You can also get the LLM it to cite the specific sources it used to answer the question. We’ll talk about those tactics in future posts but for now, that’s the basics of answer generation.</p>
<p>With the easy part out of the way, it’s time to come back to that black box we skipped over...</p>
<h2>The retrieval step: getting the right information out of your knowledge base</h2>
<p>Above we assumed we had the right knowledge snippets to send to the LLM. But how do we actually get these from the user’s question? This is the <em>retrieval step</em>, and it is the core piece of infrastructure in any “chat with your data” system.</p></div>
<p><img alt="Retrieval" height="201" src="https://communitykeeper-media.s3.amazonaws.com/media/images/retrieval.original.png" width="697"></p>
<div><p>At its core, retrieval is a search operation—we want to look up the most relevant information based on a user’s input. And just like search, there are two main pieces:</p>
<ol>
<li><strong>Indexing:</strong> Turning your knowledge base into something that can be searched/queried.</li>
<li><strong>Querying:</strong> Pulling out the most relevant bits of knowledge from a search term.</li>
</ol>
<p>It’s worth noting that <em>any search process could be used for retrieval</em>. Anything that takes a user input and returns some results would work. So, for example, you could just try to find text that matches the user's question and send that to the LLM, or you could Google the question and send the top results across—which, incidentally, is approximately how Bing's chatbot works.</p>
<p>That said, <em>most</em> RAG systems today rely on something called <em>semantic search</em>, which uses another core piece of AI technology: <em>embeddings</em>. Here we’ll focus on that use case.</p>
<p>So...what <em>are</em> embeddings?</p>
<h2>What are embeddings? And what do they have to do with knowledge retrieval?</h2>
<p>LLMs are weird. One of the weirdest things about them is that nobody really knows <em>how</em> they understand language. Embeddings are a big piece of that story.</p>
<p>If you ask a person how they turn words into <em>meaning</em>, they will likely fumble around and say something vague and self-referential like "because I know what they mean". Somewhere deep in our brains there is a complex structure that knows "child" and "kid" are basically the same, "red" and "green" are both colors, and "pleased," "happy," and "elated" represent the same emotion with varying magnitudes. We can't <em>explain</em> how this works, we just <em>know</em> it.</p>
<p>Language models have a similarly complex understanding of language, except, since they are <em>computers</em>  it's not in their brains, but made up of <em>numbers</em>. In an LLM's world, any piece of human language can be represented as a vector (list) of numbers. This vector of numbers is an <em>embedding</em>.</p>
<p>A critical piece of LLM technology is a <em>translator</em>  that goes from human <em>word-language</em> to AI <em>number-language</em>. We'll call this translator an "embedding machine", though under the hood it's just an API call. Human language goes in, AI numbers come out.</p></div>
<p><img alt="Embeddings Multiple" height="540" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Screenshot_from_2023-08-25_09-52-18.original.png" width="866"></p>
<div><p>What do these numbers mean? No human knows! They are only “meaningful” to the AI. But, what we do know is that <em>similar words end up with similar sets of numbers</em>. Because behind the scenes, the AI uses these numbers to “read” and “speak”. So the numbers have some kind of magic comprehension baked into them in AI-language—even if we don't understand it. The embedding machine is our translator.</p>
<p>Now, since we have these magic AI numbers, we can plot them. A simplified plot of the above examples might look something like this—where the axes are just some abstract representation of human/AI language:</p></div>
<p><img alt="Embedding Plot" height="525" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Embedding_Plot1.original.png" width="641"></p>
<div><p>Once we’ve plotted them, we can see that the closer two points are to each other in this hypothetical language space, the more similar they are. “Hello, how are you?” and “Hey, how’s it going?” are practically on top of each other. “Good morning,” another greeting, is not too far from those. And “I like cupcakes” is on a totally separate island from the rest.</p>
<p>Naturally, you can’t represent the entirety of human language on a two-dimensional plot, but the theory is the same. In practice, embeddings have many more coordinates (1,536 for the current model used by OpenAI). <strong>But you can still do basic math to determine how close two embeddings—and therefore two pieces of text—are to each other</strong>.</p>
<p>These embeddings, and determining “closeness” are the core principle behind <em>semantic search</em>, which powers the retrieval step.</p></div>
<div>
  <p>Like what you're reading?</p>
  <p>Sign up to get notified when there are new posts about building applications with LLMs. No spam, unsubscribe anytime.</p>
  
</div>
<div><h2>Finding the best pieces of knowledge using embeddings</h2>
<p>Once we understand how search with embeddings works, we can construct a high-level picture of the retrieval step.</p>
<p>On the indexing side, first we have to break up our knowledge base into chunks of text. This process is an entire optimization problem in and of itself, and we’ll cover it next, but for now just assume we know how to do it.</p></div>
<p><img alt="Knowledge Splitting" height="255" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Screenshot_from_2023-08-24_11-14-15.original.png" width="749"></p>
<p>Once we’ve done that, we pass each knowledge snippet through the embedding machine (which is actually an OpenAI API or similar) and get back our embedding representation of that text. Then we save the snippet, along with the embedding in a <em>vector database</em>—a database that is optimized for working with vectors of numbers.</p>
<p><img alt="Embedding Knowledge Snippets" height="323" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Screenshot_from_2023-08-24_11-19-35.original.png" width="582"></p>
<p>Now we have a database with the embeddings of all our content in it. Conceptually, you can think of it as a plot of our entire knowledge base on our “language” graph:</p>
<p><img alt="Knowledge Snippet Plot" height="533" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Screenshot_from_2023-08-24_15-35-04.original.png" width="659"></p>
<p>Once we have this graph, on the query side, we do a similar process. First we get the embedding for the user’s input:</p>
<p><img alt="Embedding the user query" height="180" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Screenshot_from_2023-08-22_14-09-41.original.png" width="856"></p>
<p>Then we plot it in the same vector-space and find the closest snippets (in this case 1 and 2):</p>
<p><img alt="Knowledge Snippet Plot with Query" height="523" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Screenshot_from_2023-08-24_15-35-42.original.png" width="667"></p>
<div><p>The magic embedding machine thinks these are the most related answers to the question that was asked, so these are the snippets that we pull out to send to the LLM!</p>
<p>In practice, this “what are the closest points” question is done via a query into our vector database. So the actual process looks more like this:</p></div>
<p><img alt="Retrieval with Embeddings Final" height="377" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Screenshot_from_2023-08-22_14-13-59.original.png" width="937"></p>
<p>The query itself involves some semi-complicated math—usually using something called a cosine distance, though there are other ways of computing it. The math is a whole space you can get into, but is out of scope for the purposes of this post, and from a practical perspective can largely be offloaded to a library or database.</p>
<div>
      
        <p>Back to LangChain</p>
      
      <div><p data-block-key="3saec">In our LangChain example from the beginning, we have now covered everything done by this single line of code. That little function call is hiding a whole lot of complexity!</p><p data-block-key="2ehku"><code>index.query("What should I work on?")</code></p></div>
    </div>
<div><h2>Indexing your knowledge base</h2>
<p>Alright, we’re almost there. We now understand how we can use embeddings to find the most relevant bits of our knowledge base, pass everything to the LLM, and get our augmented answer back. The final step we’ll cover is creating that initial index from your knowledge base. In other words, the “knowledge splitting machine” from this picture:</p></div>
<p><img alt="Knowledge Splitting High-Level" height="283" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Screenshot_from_2023-08-22_14-17-37.original.png" width="767"></p>
<div><p>Perhaps surprisingly, indexing your knowledge base is usually the hardest and most important part of the whole thing. And unfortunately, it’s more art than science and involves lots of trial and error. </p>
<p>Big picture, the indexing process comes down to two high-level steps.</p>
<ol>
<li><strong>Loading</strong>: Getting the contents of your knowledge base out of wherever it is normally stored.</li>
<li><strong>Splitting</strong>: Splitting up the knowledge into snippet-sized chunks that work well with embedding searches.</li>
</ol></div>
<div>
      
        <p>Technical clarification</p>
      
      <div><p data-block-key="oiv49">Technically, the distinction between "loaders" and "splitters" is somewhat arbitrary. You could imagine a single component that does all the work at the same time, or break the loading stage into multiple sub-components.</p><p data-block-key="8tt3h">That said, "loaders" and "splitters" are how it is done in LangChain, and they provide a useful abstraction on top of the underlying concepts.</p></div>
    </div>
<p>Let’s use my own use-case as an example. I wanted to build a chatbot to answer questions about my <a href="https://www.saaspegasus.com/">saas boilerplate product, SaaS Pegasus</a>. The first thing I wanted to add to my knowledge base was <a href="https://docs.saaspegasus.com/">the documentation site</a>. The <em>loader</em> is the piece of infrastructure that goes to my docs, figures out what pages are available, and then pulls down each page. When the loader is finished it will output individual <em>documents</em>—one for each page on the site.</p>
<p><img alt="Loader" height="279" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Screenshot_from_2023-08-22_14-20-12.original.png" width="901"></p>
<p>Inside the loader a lot is happening! We need to crawl all the pages, scrape each one’s content, and then format the HTML into usable text. And loaders for other things—e.g. PDFs or Google Drive—have different pieces. There’s also parallelization, error handling, and so on to figure out. Again—this is a topic of nearly infinite complexity, but one that we’ll mostly offload to a library for the purposes of this write up. So for now, once more, we’ll just assume we have this magic box where a “knowledge base” goes in, and individual “documents” come out.</p>
<div>
      
        <p>LangChain Loaders</p>
      
      <div><p data-block-key="dbdqd">Built in loaders are one of the most useful pieces of LangChain. They provide <a href="https://python.langchain.com/docs/integrations/document_loaders/">a long list of built-in loaders</a> that can be used to extract content from anything from a Microsoft Word doc to an entire Notion site.</p><p data-block-key="8c7gm">The interface to LangChain loaders is exactly the same as depicted above. A "knowledge base" goes in, and a list of "documents" comes out.</p></div>
    </div>
<div><p>Coming out of the loader, we’ll have a collection of documents corresponding to each page in the documentation site. Also, ideally at this point the extra markup has been removed and just the underlying structure and text remains.</p>
<p>Now, we <em>could</em> just pass these whole webpages to our embedding machine and use those as our knowledge snippets. But, each page might cover a lot of ground! And, the more content in the page, the more “unspecific” the embedding of that page becomes. This means that our “closeness” search algorithm may not work so well.</p>
<p>What’s more likely is that the topic of a user’s question matches some piece of text <em>inside</em> the page. This is where splitting enters the picture. With splitting, we take any single document, and split it up into bite-size, embeddable chunks, better-suited for searches.</p></div>
<p><img alt="Splitter" height="224" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Screenshot_from_2023-08-22_14-21-05.original.png" width="715"></p>
<p>Once more, there’s an entire art to splitting up your documents, including how big to make the snippets on average (too big and they don’t match queries well, too small and they don’t have enough useful context to generate answers), how to split things up (usually by headings, if you have them), and so on. But—a few sensible defaults are good enough to start playing with and refining your data.</p>
<div>
      
        <p>Splitters in LangChain</p>
      
      <div>
        <p data-block-key="5bysr">In LangChain, splitters fall under a larger category of things called "<a href="https://python.langchain.com/docs/modules/data_connection/document_transformers/">document transformers</a>". In addition to providing various strategies for splitting up documents, they also have tools for removing redundant content, translation, adding metadata, and so on. We only focus on splitters here as they represent the overwhelming majority of document transformations.</p>
      </div>
    </div>
<div><p>Once we have the document snippets, we save them into our vector database, as described above, and we’re finally done!</p>
<p>Here’s the complete picture of indexing a knowledge base.</p></div>
<p><img alt="Knowledge Indexing Complete" height="556" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Knowledge_Indexing_Complete.original.png" width="1055"></p>
<div>
      
        <p>Back to LangChain</p>
      
      <div><p data-block-key="9n2db">In LangChain, the entire indexing process is encapsulated in these two lines of code. First we initialize our website loader and tell it what content we want to use:</p><p data-block-key="38vfa"><code>loader = WebBaseLoader("http://www.paulgraham.com/greatwork.html")</code></p><p data-block-key="5bvlf">Then we build the entire index from the loader and save it to our vector database:</p><p data-block-key="1rusd"><code>index = VectorstoreIndexCreator().from_loaders([loader])</code></p><p data-block-key="2nksm">The loading, splitting, embedding, and saving is all happening behind the scenes.</p></div>
    </div>
<div><h2>Recapping the whole process</h2>
<p>At last we can fully flesh out the entire RAG pipeline. Here’s how it looks:</p></div>
<p><img alt="RAG: Complete" height="934" src="https://communitykeeper-media.s3.amazonaws.com/media/images/Complete.original.png" width="1075"></p>
<div><p>First we <em>index</em> our knowledge base. We get the knowledge and turn into individual documents using a <em>loader</em>, and then use a <em>splitter</em> to turn it into bite-size chunks or <em>snippets</em>. Once we have those, we pass them to our <em>embedding machine</em>, which turns them into vectors that can be used for semantic searching. We save these embeddings, alongside their text snippets in our <em>vector database</em>.</p>
<p>Next comes <em>retrieval</em>. It starts with the question, which is then sent through the same embedding machine and passed into our vector database to determine the closest matched snippets, which we’ll use to answer the question.</p>
<p>Finally, <em>augmented answer generation</em>. We take the snippets of knowledge, format them alongside a custom system prompt and our question, and, finally, get our <em>context-specific</em> answer.</p>
<p>Whew!</p>
<p>Hopefully you now have a basic understanding of how retrieval augmented generation works. If you’d like to try it out on a knowledge base of your own, without all the work of setting it up, check out <a href="https://scriv.ai/">Scriv.ai</a>, which lets you build domain-specific chatbots in just a few minutes with no coding skills required.</p>
<p>In future posts we’ll expand on many of these concepts, including all the ways you can improve on the “default” set up outlined here. As I mentioned, there is nearly infinite depth to each of these pieces, and in the future we’ll dig into those one at a time. If you’d like to be notified when those posts come out, sign up to receive updates here. I don’t spam, and you can unsubscribe whenever you want.</p></div>
<div>
  <p>Interested in learning about building with LLMs?</p>
  <p>Sign up for our email list to get notified when there are new posts like this, no spam, unsubscribe anytime.</p>
  
</div>
<p><em>Thanks to <a href="https://canvasapp.com/blog">Will Pride</a> and Rowena Luk for reviewing drafts of this.</em></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[None of us are truly living off the grid (119 pts)]]></title>
            <link>https://www.okdoomer.io/youre-not-going-to-make-it/</link>
            <guid>37261126</guid>
            <pubDate>Fri, 25 Aug 2023 12:52:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.okdoomer.io/youre-not-going-to-make-it/">https://www.okdoomer.io/youre-not-going-to-make-it/</a>, See on <a href="https://news.ycombinator.com/item?id=37261126">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
              <p>I read about this one family...</p>
<p>They were tired of society. They thought civilization was unraveling. They wanted to live off the grid. Authorities found their mummified remains a few months later.</p>
<p>The family died from exposure and malnutrition.</p>
<p>They didn't make it.</p>
<p>A while back, a prepper tried to ride out an arctic blast with canned food and survival gear. He didn't realize his electric can opener wasn't going to work until <em>after </em>the power had gone out.</p>
<p>He was helpless.</p>
<p>Prepping and homesteading have entered the mainstream over the last few years, helped by doomsday shows along with a growing sense of dread about the future. A lot of people have no idea what it's about.</p>
<p>They think they're living off the grid.</p>
<p>They're not.</p>
<p>None of us are truly living off the grid. We take it for granted. We watch homesteaders and bushcraft masters post videos about living in the woods in huts, but we don't see all the production that goes into making it look fun and simple. We don't think about where we'd go to the bathroom. We don't think about how we'd filter our water. We don't think about what we'd do without all these survival tools made in a factory somewhere.</p>
<p>There's nothing wrong with watching bushcraft videos and fantasizing about life in a mudhole. It only becomes a problem when millions of people become convinced they can all do it, and they forget that their mudhole isn't going to have bluetooth. If we had any idea what living off the grid was like, I think a lot of people would be trying a lot harder to protect it.</p>
<p>Prepping can get you through an emergency. It can get you through a disaster. It can get you through shortages and supply chain problems. It's not designed for life on an uninhabitable planet.</p>
<p>We need to talk about this, because a lot of newbie preppers and bunker babies seem to think it's possible.</p>
<p>It's not.</p>
<p>If you know what you're doing, you're probably not going to die making some rookie mistake. You could build a little cabin out in the woods with a solar power system, a root cellar, a rain harvesting system, a well, a composting toilet, and a small farm. You'd be okay for a while. It would still cost a lot. You would still need the luxury of time to get it all set up.</p>
<p>You still wouldn't make it.</p>
<p>There's people out there who know how to make their own batteries. They know how to take a car apart. They know how to build a solar power system without any help. They know how to dig an earth shelter.</p>
<p>They won't make it either.</p>
<p>In Texas in the 1950s, it didn't rain for seven years. People barely had enough water to rinse their dishes. Dust storms and heat waves brought life to a standstill. Those kinds of droughts lie in our future.</p>
<p>I've often daydreamed about having a cabin in the woods where my family can go as the unwinding of global industrial civilization accelerates. It's ironic that homesteading now evokes a sense of privilege. Rural hipsters want to homestead like their great grandparents, but they also want the comfort of the grid, along with its hospitals and drivable roads.</p>
<p>They like indoor plumbing.</p>
<p>And wifi.</p>
<p>The original homesteaders of the 1860s got their land for dirt cheap, even free. They filed claims under the Homestead Act. All they had to do was promise to live on the land and "improve it." There was a small registration fee. That was it. Now, nobody just <em>gives </em>you land anymore.</p>
<p>Talk about privilege...</p>
<p>Of course, the original homesteaders didn't have an internet where they could look things up. They didn't have indoor plumbing. They didn't have central heating. They didn't have hospitals where they could go if they fell and broke their arm. They didn't have antibiotics. They knew a simple harsh truth. If you truly lived off the grid, then you also died off the grid.</p>
<p>Most of us aren't ready for that.</p>
<p>Some of us have delved into prepping out of necessity, as a hedge against a world on fire. The more I've learned, the more I've come to an uncomfortable conclusion. It's no match for climate change.</p>
<p>It's not going to save you.</p>
<p>A rain harvesting system can't help if it doesn't rain for seven years. You're not going to hide from wildfires in a bunker.</p>
<p>You're not going to wait out social collapse in a cabin in the woods. You're not going to grow food if it's raining microplastics. You're not going to live off those emergency food kits, either.</p>
<p>Look at the sodium.</p>
<p>Your solar panels and batteries will eventually need replacing.</p>
<p>You can't smelt your own silicone.</p>
<p>You're not going to shoot your way through the apocalypse. There's always someone with a bigger gun and a better aim. There won't be an end to the number of hungry families who want your stuff and don't share your warm fuzzy view of humanity. Even if you killed them all, you'd have to bury them somewhere, and probably not next to your victory garden.</p>
<p>I've done the survival calculus.</p>
<p>You can't store 20 years of food in your home. You can't protect it from the elements. You can't build a bunker on an uninhabitable planet. Even if you did, someone would take it from you.</p>
<p>At any rate, you probably aren't going to die from famine or thirst if you're living in the first world. You're probably going to die from heat stroke. You're probably going to die from a disease. You're probably going to die from a hospital acquired infection. You're probably going to die from a heart attack or cancer from all the toxins you don't even know you're ingesting.</p>
<p>Eight billion people can't live off the grid.</p>
<p>We tried that with a far smaller global population. It didn't work. That many people can only live in societies where we carefully manage our resources, including food, water, and medicine.</p>
<p>Once you've constructed a network of people to provide all of each other's needs, you've basically built... a grid.</p>
<p>Look at history. People formed cities and societies because they got tired of living and dying off the grid. The grid is a wonderful thing when it's not run by kleptocrats.</p>
<p>It's good to be ready for emergencies.</p>
<p>It's good to have food and water for yourself and your neighbors. It's good to have a backup power source. It's good to know how to change a tire. It's good to know how to get through a heat wave without air conditioning. It's good to know how to prepare for a storm or a flood. It's good to have alternate transportation. It's good to have outdoor camping skills.</p>
<p>It's good to have matches and a compass.</p>
<p>It's good to know first aid.</p>
<p>It's good to know how to garden. It's good to be okay eating rice and beans every meal without complaining. It's good to know just how much you can depend on yourself, and the limits to your own resilience.</p>
<p>That's prepping.</p>
<p>The more I've learned about it, and the more I've read into the scope of our oncoming climate dystopia, one simple truth has been waiting down at the bottom. The best kind of prepping is emotional.</p>
<p>You aren't going to escape.</p>
<p>This disastrous summer has made one thing painfully clear. Nowhere on earth is safe from the consequences of climate change. If it's not wildfires, it's the smoke from the wildfires. If it's not drought, it's floods. You can build a community, but you also have to remember that it's hard to get along with people. They can be irritating, even when they're not batshit crazy. All we can do is be ready, and then be ready for when it's not enough.</p>
<p>One day, it won't.</p>
<p>You will die.</p>
<p>None of us are guaranteed a quick and painless exit from life. None of us are entitled to go peacefully in our sleep.</p>
<p>As for all the fantasy survivalists out there who play commando on the weekends and then stop at Chick-fil-A on the way home...</p>
<p>If you think you're going to defect from the social contract but still get to enjoy all the benefits of society, I have bad news.</p>
<p>You're not going to make it.</p>


            </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[50TB IBM tape drive more than doubles LTO-9 capacity (126 pts)]]></title>
            <link>https://blocksandfiles.com/2023/08/23/50tb-ibm-tape/</link>
            <guid>37261001</guid>
            <pubDate>Fri, 25 Aug 2023 12:36:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blocksandfiles.com/2023/08/23/50tb-ibm-tape/">https://blocksandfiles.com/2023/08/23/50tb-ibm-tape/</a>, See on <a href="https://news.ycombinator.com/item?id=37261001">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <!-- image --><p><a href="https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive.jpg" data-caption=""><img width="696" height="347" src="https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive-696x347.jpg" srcset="https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive-696x347.jpg 696w, https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive-300x150.jpg 300w, https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive-768x383.jpg 768w, https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive-842x420.jpg 842w, https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive.jpg 950w" sizes="(max-width: 696px) 100vw, 696px" alt="" title="IBM-TS1170-tape-drive"></a></p>
            <!-- content -->
<p>IBM has announced its latest <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiTwZPWzPKAAxVhxgIHHerOCfwQFnoECB8QAQ&amp;url=https%3A%2F%2Fwww.ibm.com%2Fdownloads%2Fcas%2FWQX5N4RJ&amp;usg=AOvVaw0IJVJ4YHU88QUH0hX6To-R&amp;opi=89978449">TS1170</a> tape drive with 50TB cartridges, more than 2.5 times larger than LTO-9 cartridges.</p>



<p>IBM, which manufactures the LTO tape drives sold by itself, HPE, and Quantum, has its own proprietary tape formats used in its TS1100 series drives and Jx format media. The latest TS1170 drive supports 50TB JF media cartridges with 150TB compressed capacity through 3:1 compression. The prior <a href="https://blocksandfiles.com/2019/08/23/big-blue-developing-tape-drive-to-repel-quantum-computer-attacks/">TS1160</a>, announced in 2018, supported JE media with capacities of 20TB raw and 60TB compressed.</p>


<div>
<figure><img decoding="async" src="https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive.jpg" alt="IBM TS1170 tape drive" width="700" srcset="https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive.jpg 950w, https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive-300x150.jpg 300w, https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive-768x383.jpg 768w, https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive-696x347.jpg 696w, https://blocksandfiles.com/wp-content/uploads/2023/08/IBM-TS1170-tape-drive-842x420.jpg 842w" sizes="(max-width: 950px) 100vw, 950px"><figcaption>IBM TS1170 tape drive</figcaption></figure></div>


<p>The TS1170 drive operates at 400MBps raw throughput and supports both 12gig SAS and 16gig FC connectivity. It is also recognized as LTFS-ready. However, JF media is not backwards-compatible with the earlier JE specification.</p>



<p>The new tape cartridge media, also called 3592 70F, uses <a href="https://blocksandfiles.com/2020/06/29/fujifilm-400tb-magnetic-tape-cartridge-future/">Strontium Ferrite</a> particle technology. Fujifilm has demonstrated a <a href="https://blocksandfiles.com/2020/12/16/ibm-and-fujifilm-580tb-tape-capacity-record/">580TB</a> raw capacity tape using Strontium Ferrite particles so there is a lot of runway for future capacity increases.</p>



<p>LTO-9 tapes hold 18TB raw and 45TB compressed, less than IBM’s JE format tapes. The <a href="https://blocksandfiles.com/2020/09/10/lto-changes-tape-roadmap-strategy-cuts-capacity-to-build-air-gapped-ransomware-defence-storage/">LTO tape roadmap</a> has a coming LTO-10 specification with 36TB raw/90TB compressed capacity. Where IBM uses a 3:1 compression ratio, the LTO group uses 2.5:1. An IBM TS4500 tape library, using the JF cartridges, will be able to hold the same amount of data as an LTO-9-based library but with more than three times fewer tapes thanks to IBM’s large capacity tape and greater compression. Even when LTO-10 tape technology arrives, the TS4500 will still have a better density in terms of TB per cartridge.</p>



<p>The LTO (Linear Tape-Open) organization was founded to provide an open industry standard format to replace various proprietary tape formats such as Quantum’s S-DLT and DLT. But since only IBM makes LTO tape drives and these drives use media from Sony and Fujifilm, IBM has a lock on the LTO tape format.</p>



<p>LTO group members HPE and Quantum are dependent upon IBM and can do nothing to get LTO tape cartridge capacity up to the IBM TS1170/JF level unless they develop their own drives. If they do this, they may have to lose some backwards-compatibility. With IBM’s dominant presence in the LTO space, competitors would need significant investment to establish a foothold.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Factorio: Space Age (1271 pts)]]></title>
            <link>https://factorio.com/blog/post/fff-373</link>
            <guid>37260637</guid>
            <pubDate>Fri, 25 Aug 2023 11:50:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://factorio.com/blog/post/fff-373">https://factorio.com/blog/post/fff-373</a>, See on <a href="https://news.ycombinator.com/item?id=37260637">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>
  Hello, long time no see!<br>
  Today we are going to talk about the expansion which is called <strong><i>Factorio: Space Age</i></strong>.
</p>

<p>
  <video autoplay="" muted="" loop="" playsinline="">
      <source src="https://cdn.factorio.com/assets/img/blog/fff-373-platform-HR.mp4" type="video/mp4">
    Mp4 playback not supported on your device.
  </video>
</p>
<p><a href="https://cdn.factorio.com/assets/img/blog/fff-373-platform-static-2240.png" target="_blank">(Click here for static image version)</a></p>


<hr>

<h3>What is Factorio: Space Age?<author>kovarex</author></h3>

<p>
  <strong>Factorio: Space Age</strong> continues the player's journey after launching rockets into space. Discover new worlds with unique challenges, exploit their novel resources for advanced technological gains, and manage your fleet of interplanetary space platforms.
</p>

<p>
  Vanilla Factorio ends by launching the rocket into space, so I always expected it to be quite obvious what we are going to do next.
  Honestly the space platform related gameplay was actually planned a very long time ago, and we had shown a little bit in <a href="https://www.factorio.com/blog/post/fff-74">FFF-74</a>.
  We had a similar kind of story with the Spidertron. Its concept was teased for the first time in <a href="https://www.factorio.com/blog/post/fff-120">FFF-120</a>, just to be quietly abandoned and then revived 4 years later.
</p>

<p>
  But as you might or might not know, the name 'WUBE' is an abbreviation of Wszystko będzie, which means something like "Everything will be done eventually". We didn't abandon the plan, we just realized it would be way too ambitious to try to fit it into the 1.0 Factorio release, so it was eventually just postponed to the expansion. In retrospect, it was clearly a good idea.
</p>

<h4>The main structure</h4>
<p>
  Instead of sending one rocket to space, you have to send many, because you need to use them to transport materials to build big space platforms in orbit.
  The rocket is cheaper in Space Age to not make it drag forever, but you still are expected to build bigger in preparation of what is to come.
</p>
<ul>
    <li>The space platforms are used on their own to generate space science, but mainly, they are eventually used to travel to different planets, and for automated interplanetary logistics.</li>
    <li>We decided to go with the approach of having a small number of predefined planets, which represent your progression through the game.</li>
    <li>The expansion contains 4 additional planets. Each of which has its own unique theme, resource, challenges and gameplay mechanics. Most of them also have different military targets.</li>
    <li>Every planet leads to production of specific science packs and its technologies.</li>
    <li>The order in which you exploit the planets is an impactful strategic choice.</li>
  </ul>


<p>
  <img width="258px" src="https://cdn.factorio.com/assets/img/blog/fff-373-technology-graph-smaller.png">
</p>

<h4>The rewards</h4>
<p>
  Since we have a lot of new challenges, we also had to balance the set of rewards you get for each stage of the game.
  We have a set of cool new things in the expansion, but the player is quite omnipotent at the end of the vanila playthrough so we had to make some tough decisions.
</p>

<p>
  Since the goal was to make the overall expansion experience as good as possible, we have rebalanced the tech tree. This means, that with Space Age enabled, some items that are available in vanilla are unlocked later on some planet. This specifically applies to artillery, cliff explosives (this is the masochist part of me speaking), Spidertron, best tier of modules, and some personal equipment upgrades.
</p>

<p>
  Based on testing, these changes made the choice of where and when to go even more meaningful.
  On the other hand, space will be available sooner and there will be some nice additions available directly on Nauvis (the vanilla planet).
</p>

<p>
  This implies that technically, you could just take your vanilla base, activate the expansion, and continue playing. But the best way to experience it will be to play with Space Age from start to finish.
</p>

<h4>Version 2.0 is not just about the expansion</h4>
<p>
  In the previous news about the expansion <a href="https://www.factorio.com/blog/post/fff-367">FFF-367</a>, we declared that the content will be technically a mod taking advantage of the updated engine. What this means is that a lot of the improvements will be for all players, regardless of them having the expansion or not.
</p>
<p>
  Examples of things that will be in the 2.0 update are: the ability to control train systems better, better blueprint building, better flying robot behaviour and many more. It will all be covered in more details in future FFFs.
</p>

<hr>

<h3>The good news and the bad news<author>kovarex</author></h3>

<p>
  Lets start with the (probably) bad news. Since we know where we are and what is the goal, we can start to approximate
  the date of releasing the expansion, which is planned to be about one year from now.
  It is still a long time, but at least it is getting more specific.
</p>

<h4>Current state</h4>
<p>
  In the previous post <a href="https://www.factorio.com/blog/post/fff-367">FFF-367</a>, we declared a 7 step plan, where we were at step 4.
  Now we are at the end of step 5, and have started some things from step 6 already.
</p>

<p>
  It has been playable from start to finish for more than a year. This means that we have already made several improvement iterations based on the playthroughs behind us.
</p>

<p>
  We are doing 3 types of testing which were all very useful:
  </p><ul>
    <li>Just playing the game on our own and contemplating how it feels.</li>
    <li>Office LAN party, which is also a great way to test multiplayer.</li>
    <li>Individual testers. We had a few outside testers who played it from start to finish and gave us detailed feedback.</li>
  </ul>


<p>
  The feedback led us to make different kinds of changes, for example:
  </p><ul>
    <li>Two planets were way too repetitive and similar to Nauvis, and we had to do a complete overhaul to make them more different.</li>
    <li>The game was a little bit too slow and grindy, so we were speeding things up, which doesn't mean dumbing it down.
        The goal was to be able to finish it in non-speedrun mode in less than 80 hours for an experienced player.
        We were trying to keep the mechanics and just cut down on the recipe counts and costs.
    </li>
    <li>We improved many UIs related to the new (and sometimes old) parts of the game many times.</li>
  </ul><p>
  It was kind of annoying to redo so much, but based on the result and reactions, it was definitely worth it.
</p>

<p>
  This means, that we have something we are happy with, is pretty stable, and we don't expect any further brutal changes to come.
  We have a relatively stable list of tasks to be finished both for programmers and artists, which makes us confident that we can release a polished product in about a year.
</p>

<h4>The good news</h4>

<p>
  But then there is the (hopefully) good news! From now on, we are stopping the embargo on the expansion content, and <strong>we
  will be publishing Friday Facts every week</strong> about all the different aspects of the expansion until release!
</p>

<p>
  We will mostly show what we have done, or what we are working on. We are also looking forward to the feedback from the
  community which has proved useful so many times already. With the quantity of things we have, I'm confident that we
  won't run out of topics until the release.
</p>

<hr>

<h3>
  There's a mod for that<author>Earendel</author>
</h3>

<p>
  A lot of people are going to make comparisons between the Space Age expansion and the <a href="https://mods.factorio.com/mod/space-exploration" target="_blank">Space Exploration</a> mod.
  I've worked on the game design for both: On Space Age I made the first space + planets prototype builds and plus I've been involved in most of the gameplay discussions since.
  On Space Exploration, well it's my mod, I made it.
</p>

<p>
  I think that makes me the most qualified to talk about how these two things are very different from the ground up.
  Sure there's some overlap in the broader topics: you go to space, you visit planets; but these similarities end up being fairly superficial when all of the decisions along the way are made with different design goals:
</p>

<ul>
  <li>
    <strong>Target Audience:</strong>
    Space Exploration is targeted at a small set of challenge-seekers, it's not for everyone by design. Space Age is targeted at all Factorio players with better approachability in mind.
  </li>

  <li>
    <strong>Game Length:</strong>
    Space Exploration is a long-form adventure designed for players to gradually uncover many interconnected challenges over 150-500 hours (or more).
    Space Age challenges are more streamlined and self-contained for a faster pace of 60-100 hours (rough estimate).
  </li>

  <li>
    <strong>Complexity:</strong>
    Space Exploration challenges ramp up to a very high difficulty towards the end.
    Some of the last challenges cannot be automated without a set of arithmetic combinators, so it is not expected that all players will be able to complete it.
    Space Age has a lot of different challenges but they never get too extreme and the usage of combinators is quite lightweight, so we expect that most players will be able to complete it.
  </li>

  <li>
    <strong>Engine Support:</strong>
    There are so many things that Space Exploration just can't do because it's a mod.
    Space Age has very different capabilities because the game engine can be made to support it.
    Large parts of the expansion are focused on game mechanics that just aren't possible otherwise, so the gameplay will be unique and refreshing even if you're a Space Exploration veteran.
  </li>
</ul>

<p>
  There are many more differences that will become apparent when we look at individual features of the expansion in upcoming FFFs.
</p>

<hr>

<p>
  As always, let us know what you think at the usual places. We look forward to seeing you all in the weeks to come :) .
</p>

        

        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The College Board Tells TikTok and Facebook Your SAT Scores (175 pts)]]></title>
            <link>https://gizmodo.com/sat-college-board-tells-facebook-tiktok-your-scores-gpa-1850768077</link>
            <guid>37260553</guid>
            <pubDate>Fri, 25 Aug 2023 11:38:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/sat-college-board-tells-facebook-tiktok-your-scores-gpa-1850768077">https://gizmodo.com/sat-college-board-tells-facebook-tiktok-your-scores-gpa-1850768077</a>, See on <a href="https://news.ycombinator.com/item?id=37260553">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Many students have no choice about working with the College Board, the company that administers the SAT test and Advanced Placement exams. Part of that relationship involves a long history of privacy issues. Tests by Gizmodo<!-- --> found if you use some of the handy tools promoted by College Board’s website, the organization<!-- --> sends details about your SAT scores, GPA, and other data to Facebook, TikTok, and a variety of<!-- --> companies. <br></p><div data-video-id="192514" data-monetizable="true" data-position="sidebar" data-video-title="Mourning the Loss of Addison Rae’s Debut Album | The Meme Machine" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="259" data-playlist="192514,192167,191385" data-current="192514"><div><p>Mourning the Loss of Addison Rae’s Debut Album | The Meme Machine</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/192514/192514_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/192514/192514_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/192514/192514_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/192514/192514_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/17970.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>Gizmodo observed the College Board’s website sharing data with Facebook and TikTok when a user fills in information about their GPA and SAT scores. When this reporter<!-- --> used the College Board’s search filtering tools to find colleges that might accept a student with a C+ grade-point average and a SAT score of 420 out of 1600, the site<!-- --> let the social media companies know. Whether a student <!-- -->is acing their tests <!-- -->or struggling<!-- -->, Facebook and TikTok get the details. </p><p>The College Board shares this data via<!-- --> “pixels,”<!-- --> invisible tracking technology used to facilitate targeted advertising on platforms such as Facebook and TikTok. The data is shared along with unique user IDs to identify the students, along with other information about how you use the College Board’s site. </p><p>Organizations use pixels and other tools to share data so they can send targeted ads to people who use their apps and websites on other platforms, such as Google, Facebook, and TikTok. <br></p><p>“We do not share SAT scores or GPAs with Facebook or TikTok, and any  other third parties using pixel or cookies,” said a College Board spokesperson. “In fact, we do not send <em>any</em> personally identifiable information (PII) through our pixels  on the site. In addition, we do not use SAT scores or GPAs for any  targeting.”</p><p>After receiving this comment, Gizmodo shared a screenshot of the College Board sending GPAs<!-- --> and SAT scores to TikTok using a pixel. The spokesperson then acknowledged that the College Board’s website actually does share this data. <br></p><p>“Pixels are simply a means to measure the effectiveness of College Board advertising,” the spokesperson said. “If a student uses the college search tool on CB.org, the student can add  a GPA and SAT score range to the search filters. Those values are  passed in the pixel, not because we configured  the pixel that way but because that’s how the pixel works.”</p><p>The spokesperson stressed that no personally identifiable information is shared using pixels or cookies. Our tests didn’t show<!-- --> the College Board sharing information like names or phones numbers, which fall in the category of personal info<!-- -->. However, pixels and cookies typically contain unique stings of letters and numbers meant to identify and track users. For years, experts have argued and demonstrated this poses privacy risks and is far from anonymous. <br></p><p>This kind of data sharing is common on the internet. <!-- -->For example, Gizmodo found <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/tiktok-ban-joe-biden-28000-apps-sdk-data-china-1850174019&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/tiktok-ban-joe-biden-28000-apps-sdk-data-china-1850174019">28,000 apps sending TikTok data</a></span> in March, a number that likely under-counts the company’s actual data harvesting empire. In December, 2022, an investigation found Amazon, FBI.gov, and 70,000 other <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/elon-musk-twitter-amazon-fbi-70000-sites-data-security-1849867489&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/elon-musk-twitter-amazon-fbi-70000-sites-data-security-1849867489">websites sending user data to Twitter</a></span>, the company now known as X.</p><p>However, many privacy advocates have argued the College Board and companies which handle data about students and minors should be held to a higher standard—especially when many of these services are all but mandatory in the American education system.</p><p>The College Board has a long and troubled history when it comes to student privacy. In 2018 and 2019, the organization<!-- --> was <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.nytimes.com/2018/07/29/business/for-sale-survey-data-on-millions-of-high-school-students.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.nytimes.com/2018/07/29/business/for-sale-survey-data-on-millions-of-high-school-students.html" target="_blank" rel="noopener noreferrer">caught selling data about students</a></span>, including the names of SAT test takers, for <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.wsj.com/articles/for-sale-sat-takers-names-colleges-buy-student-data-and-boost-exclusivity-11572976621&quot;,{&quot;metric25&quot;:1}]]" href="https://www.wsj.com/articles/for-sale-sat-takers-names-colleges-buy-student-data-and-boost-exclusivity-11572976621" target="_blank" rel="noopener noreferrer">as little as 47 cents a piece</a></span>. An investigation in 2020 <!-- -->by this reporter found <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.consumerreports.org/colleges-universities/college-board-is-sharing-student-data-once-again/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.consumerreports.org/colleges-universities/college-board-is-sharing-student-data-once-again/" target="_blank" rel="noopener noreferrer">similar data sharing practices</a></span>, in which the College Board told Google, Facebook, and numerous  other companies about nearly everything you did on the company’s  website. </p><p>At the time, this violated explicit  commitments the College Board made to users, including the “Student  Privacy Pledge,” a voluntary commitment between education technology  companies. By signing the pledge, the College Board promised not to “use  or disclose student information collected through an educational/school  service (whether personal information or otherwise) for behavioral  targeting of advertisements to students.”<br></p><p>Since  then, the College Board appears to have scrubbed references to the  Student Privacy Pledge from its website and is no longer listed as a  signatory. <br></p><p>The College Board Spokesperson said the company uses student data, with permission, to help students access and succeed in college. “Transparency is among our key data privacy principles,” the spokesperson said, providing a link to the company’s <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://privacy.collegeboard.org/.&quot;,{&quot;metric25&quot;:1}]]" href="https://privacy.collegeboard.org/" target="_blank" rel="noopener noreferrer">privacy policy</a></span>.<br></p><p>If you want to attain <!-- -->higher education in the United States, the College Board is hard to avoid. The organization writes and administers the SAT test and Advance Placement (AP) exams, which students take to earn college credit and bolster applications. The College Board also runs standardized tests taken by children as young as kindergartners, and essentially writes the curriculum in some school districts. </p><p>The College Board, as powerful as <!-- -->a governmental institution in some regards, is a non-profit. But that doesn’t mean it isn’t profitable for the people who run it. According to <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://projects.propublica.org/nonprofits/organizations/131623965&quot;,{&quot;metric25&quot;:1}]]" href="https://projects.propublica.org/nonprofits/organizations/131623965" target="_blank" rel="noopener noreferrer">tax forms</a></span>, 14 of the College Board’s 17 executives made more than $300,000 in 2021. Together, CEO David Coleman and President Jeremy Singer made $1,782,254.</p><p>TikTok’s ad library shows that the College Board ran <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://library.tiktok.com/ads?region=all&amp;start_time=1664596800000&amp;end_time=1692635242918&amp;adv_name=COLLEGE%20BOARD&amp;adv_biz_ids=6876462926002127618&amp;query_type=2&amp;sort_type=impression,desc&quot;,{&quot;metric25&quot;:1}]]" href="https://library.tiktok.com/ads?region=all&amp;start_time=1664596800000&amp;end_time=1692635242918&amp;adv_name=COLLEGE%20BOARD&amp;adv_biz_ids=6876462926002127618&amp;query_type=2&amp;sort_type=impression,desc" target="_blank" rel="noopener noreferrer">a number of ads</a></span> on the platform in fall of 2022, but TikTok doesn’t disclose whether or not these ads were targeted at students using data collected from pixels. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Finetuning of Falcon-7B LLM Using QLoRA on Mental Health Conversational Dataset (147 pts)]]></title>
            <link>https://github.com/iamarunbrahma/finetuned-qlora-falcon7b-medical</link>
            <guid>37259753</guid>
            <pubDate>Fri, 25 Aug 2023 09:34:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/iamarunbrahma/finetuned-qlora-falcon7b-medical">https://github.com/iamarunbrahma/finetuned-qlora-falcon7b-medical</a>, See on <a href="https://news.ycombinator.com/item?id=37259753">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Finetuning of Falcon-7B LLM using QLoRA on Mental Health Conversational Dataset</h2>
<p dir="auto"><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/78f47a09877ba9d28da1887a93e5c3bc2efb309c1e910eb21135becd2998238a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a>&nbsp;&nbsp;
<a href="https://colab.research.google.com/github/iamarunbrahma/finetuned-qlora-falcon7b-medical/blob/main/funetuned_qlora_falcon7b.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
<h2 tabindex="-1" dir="auto">Introduction:</h2>
<p dir="auto">Mental health issues are often misunderstood or not fully grasped by the general public. This lack of understanding can lead to fear, discomfort, and negative perceptions about mental health conditions. Media portrayals of mental health often perpetuate negative stereotypes, leading to misconceptions and fear. Overcoming mental health stigma requires a multi-faceted approach that involves education, raising awareness, promoting empathy and understanding, challenging stereotypes, and ensuring accessible and quality mental health care.
Mental health directly impacts an individual's overall well-being, quality of life, and ability to function effectively in daily life. Good mental health is essential for experiencing happiness, fulfilment, and a sense of purpose. Mental health and physical health are closely intertwined. Untreated mental health issues can lead to or worsen physical health problems, such as cardiovascular diseases, weakened immune systems, and chronic conditions.</p>
<h2 tabindex="-1" dir="auto">Core Rationale:</h2>
<div dir="auto"><p>Chatbots offer a readily available and accessible platform for individuals seeking support. They can be accessed anytime and anywhere, providing immediate assistance to those in need. Chatbots can offer empathetic and non-judgmental responses, providing emotional support to users. While they cannot replace human interaction entirely, they can be a helpful supplement, especially in moments of distress.</p><p>
NOTE: <em>It is important to note that while mental health chatbots can be helpful, they are not a replacement for professional mental health care. They can complement existing mental health services by providing additional support and resources.</em></p></div>
<h2 tabindex="-1" dir="auto">Dataset:</h2>
<div dir="auto"><p>The dataset was curated from online FAQs related to mental health, popular healthcare blogs like WebMD, Mayo Clinic and Healthline, and other wiki articles related to mental health. The dataset was pre-processed in a conversational format such that both questions asked by the patient and responses given by the doctor are in the same text. The dataset for this mental health conversational AI can be found here: <a href="https://huggingface.co/datasets/heliosbrahma/mental_health_chatbot_dataset" rel="nofollow">heliosbrahma/mental_health_chatbot_dataset</a>.</p><p>
NOTE: <em>All questions and answers have been anonymized to remove any PII data and preprocessed to remove any unwanted characters.</em></p></div>
<h2 tabindex="-1" dir="auto">Model Finetuning:</h2>
<div dir="auto"><p>This is the major step in the entire project. I have used sharded Falcon-7B pre-trained model and finetuned it to using the QLoRA technique on my custom mental health dataset. The entire finetuning process took less than an hour and it was finetuned entirely on Nvidia A100 from Google Colab Pro. But, it could also be trained on free-tier GPU using Nvidia T4 provided by Colab. In that case, we have to ensure to use max_steps less than 150.
The rationale behind using sharded pre-trained model is mentioned in my blog post: <a href="https://medium.com/@iamarunbrahma/fine-tuning-of-falcon-7b-large-language-model-using-qlora-on-mental-health-dataset-aa290eb6ec85" rel="nofollow">Fine-tuning of Falcon-7B Large Language Model using QLoRA on Mental Health Dataset</a><br>
Adding here the training loss metrics tracking report from WandB monitoring logs for 180 steps training run: <a href="https://api.wandb.ai/links/heliosbrahma/iv8s4frw" rel="nofollow">train/loss logs for Falcon-7B PEFT</a></p><p>
NOTE: <em>Try changing hyperparameters in TrainingArguments and LoraConfig based on your requirements. With the settings mentioned in notebook, I achieved 0.031 training loss after 320 steps.</em></p></div>
<h2 tabindex="-1" dir="auto">Model Inference:</h2>
<p dir="auto">PEFT fine-tuned model has been updated here: <a href="https://huggingface.co/heliosbrahma/falcon-7b-sharded-bf16-finetuned-mental-health-conversational" rel="nofollow">heliosbrahma/falcon-7b-sharded-bf16-finetuned-mental-health-conversational</a>. <br>
Run <code>gradio_chatbot_app.ipynb</code> notebook to get a chatbot like interface using Gradio as frontend for demo. Play around with different hyperparameter config settings for answer generation and run multiple queries to check for the quality of generated response.</p>
<p dir="auto">It takes less than 3 minutes to generate the model response. Compare the PEFT model response with the original model response in <code>funetuned_qlora_falcon7b.ipynb</code> notebook.</p>
<h2 tabindex="-1" dir="auto">Conclusion:</h2>
<p dir="auto">I have written a detailed technical blog explaining key concepts of QLoRA and PEFT fine-tuning method: <a href="https://medium.com/@iamarunbrahma/fine-tuning-of-falcon-7b-large-language-model-using-qlora-on-mental-health-dataset-aa290eb6ec85" rel="nofollow">Fine-tuning of Falcon-7B Large Language Model using QLoRA on Mental Health Dataset</a>. If you still have any queries, you can open an issue on this repo or comment on my blog.</p>
<p dir="auto"><em>If you like this project, please ⭐ this repository</em>.</p>
</article>
          </div></div>]]></description>
        </item>
    </channel>
</rss>