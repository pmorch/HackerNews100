<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 27 Dec 2024 19:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Parents of OpenAI Whistleblower Don't Believe He Died by Suicide, Order Autopsy (132 pts)]]></title>
            <link>https://sfist.com/2024/12/26/parents-of-openai-whistleblower-dont-believe-he-died-by-suicide-order-second-autopsy/</link>
            <guid>42524190</guid>
            <pubDate>Fri, 27 Dec 2024 17:38:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sfist.com/2024/12/26/parents-of-openai-whistleblower-dont-believe-he-died-by-suicide-order-second-autopsy/">https://sfist.com/2024/12/26/parents-of-openai-whistleblower-dont-believe-he-died-by-suicide-order-second-autopsy/</a>, See on <a href="https://news.ycombinator.com/item?id=42524190">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-main">

        <article>

            <figure>
                
            </figure>

            <div>
                    <p>There is a bit more intrigue around the tragic death late last month of former OpenAI researcher Suchir Balaji, who was found dead in his Lower Haight apartment of an apparent suicide.</p><p>Balaji had only recently become a whistleblower about OpenAI and its use of copyrighted material to train its ChatGPT AI model, <a href="https://www.nytimes.com/2024/10/23/technology/openai-copyright-law.html">speaking to the New York Times</a> in an interview published in October. In addition to the legal issues around consuming copyrighted material, Balaji told the Times it was also "not a sustainable model for the internet ecosystem as a whole."</p><p>On November 26, two days before Thanksgiving, Balaji was <a href="https://sfist.com/2024/12/13/openai-whistleblower-found-dead-in-his-sf-apartment/">found dead in his apartment on Buchanan Street</a>. The medical examiner deemed it a death by suicide, and the SFPD has said there is "no evidence of foul play."</p><p>Balaji's mother, Poornima Ramarao, who lives in the Bay Area, tells Bay Area News Group this week that she and her husband are "demanding a thorough investigation" into their son's death. They do not believe he would have taken his own life, and they say there had been zero indication in his mental state that this could be a possibility.</p><p>"No one believes that he could do that," Ramarao tells the news group. The mother says that she and her husband had last spoken to their son on November 22, in a 10-minute phone call in which he had not indicated anything was wrong. He said he was heading out to get dinner.</p><p>Ramarao said that when she had asked Balaji, who had quit his job at OpenAI over the summer, how he would make a living, he assured he wasn't concerned, and said "money is not important to me — I want to offer a service to humanity."</p><p>Balaji was reportedly working to establish a nonprofit that was centered on machine learning.</p><p>Balaji also reportedly reassurred his parents about his decision to go public with his concerns about OpenAI, and they say they were proud of him and that he was untroubed by his decision. Days before his death, Balaji was named in a legal filing by the New York Times as a person with significant documents to support their case. The Times is among multiple companies suing OpenAI over the use of copyrighted materials.</p><p>"He was very happy," Ramarao tells Bay Area News Group, adding that he had just turned 26 less than a week earlier, and he had spent his birthday backpacking with high school friends in the Catalina Islands. "He had a blast. He had one of the best times of his life," the mother says.</p><p>The parents have hired an attorney, Phil Kearney, and they have commissioned a second, independent autopsy.</p><p><strong>Previously:</strong> <a href="https://sfist.com/2024/12/13/openai-whistleblower-found-dead-in-his-sf-apartment/">OpenAI Whistleblower Found Dead In His SF Apartment</a></p><p><em>If you or someone you know is struggling with feelings of depression or suicidal thoughts, the 988 Suicide &amp; Crisis Lifeline offers free, round-the-clock support, information and resources for help. Call or text the lifeline at 988, or see the 988lifeline.org website, where chat is available.</em></p>
                </div>

            
            

        </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scale Model of Boeing 777-300ER, Made from Manila Folders (208 pts)]]></title>
            <link>https://www.lucaiaconistewart.com/model-777</link>
            <guid>42523745</guid>
            <pubDate>Fri, 27 Dec 2024 16:53:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lucaiaconistewart.com/model-777">https://www.lucaiaconistewart.com/model-777</a>, See on <a href="https://news.ycombinator.com/item?id=42523745">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site">

        
        
          
            

  


          
        

        <main id="page" role="main">

          

          <div data-type="page" data-updated-on="1561691883296" id="page-5cef0d874437940001629f3f" data-content-field="main-content"><div data-block-type="2" id="block-yui_3_17_2_1_1559268252785_9085">

<p>
  <h2><em>1:60 SCALE MODEL OF AN AIR INDIA BOEING 777-300ER, MADE ENTIRELY FROM MANILA FOLDERS</em></h2>
</p>






















</div><div data-block-type="2" id="block-yui_3_17_2_1_1559266243734_6718">
  <p>This project traces its beginnings to an <a href="https://www.lucaiaconistewart.com/architecture"><span>architecture class</span></a> in high school where we learned to use manila file folders to roughly model our building ideas. The more I worked with paper, the more I fell in love with its versatility. At some point, I got the idea to make a model of an airplane as a way of challenging myself with an unconventional shape.</p><p>Though the project began on a much smaller and simpler scale in mid-2008, it has since evolved through multiple revisions to become a highly detailed, true-to-life representation of a Boeing 777. I originally drew my plans by hand, but my desire to increase the accuracy and amount of detail led me to start using Adobe Illustrator to design and print increasingly intricate parts directly onto the folder.</p><p><em>The project has been in progress since May 2008</em></p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1559270348562_9530">

<p>
  <h2>HOW THE *#%! DO I DO IT?</h2>
</p>






















</div><div data-block-type="2" id="block-yui_3_17_2_1_1559711406501_11179">
  <h2>RESEARCH</h2><p>I start with as much source material as I can. Often that means photos and videos found on the web, but sometimes I’m lucky enough to get my hands on technical drawings as well. I study these materials to form an understanding of the intrinsic shape and function of the particular section I’m working on. Once I feel confident in my basic understanding of the part, I can begin the design process.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1559711406501_13965">
  <h2>DESIGN</h2><p>Much of my work consists of figuring out how to flatten 3-D shapes into 2-D slices that can be printed onto paper and assembled. I work in Adobe Illustrator to create 2-D plans for all the parts of which the final product will be comprised. I typically start the process by working out the general shape and dimensions of the piece, then I drill down, adding more detail and functionality. Once the design has been “frozen,” I work to break it down into individual pieces and arrange them for printing. Often, if a piece is complex and I’m unsure of how well it will function in reality, I build a small test section to verify my designs.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1559711406501_16684">
  <h2>PRINTING &amp; ASSEMBLY</h2><p>This stage is as time-consuming as the design process. I cut folders down to printer-friendly sizes and print my plans directly onto them. Then, I cut out the individual pieces using an <a href="http://www.xacto.com/products/cutting-solutions/knives/detail/X3201" target="_blank"><span>Xacto knife</span></a>, arrange them into sections, and glue them together with <a href="https://www.aleenes.com/aleenes-clear-gel-tacky-glue" target="_blank"><span>Tacky Glue</span></a>. Often, a complex piece is actually a collection of much smaller sub-sections that each needs to be assembled separately before being joined together; this is especially true of something like the wing, which contains many articulating functions and thousands of parts.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1561353870131_383468">
  <h2>TOOLS</h2><p>My tools of choice include the aforementioned Xacto knife, a cutting mat, straight edges, squares, and a toothpick for the precise application of glue.</p>
</div><div data-block-json="{&quot;hSize&quot;:null,&quot;floatDir&quot;:null,&quot;url&quot;:&quot;https://www.youtube.com/watch?v=77oRSCxGmYA&quot;,&quot;html&quot;:&quot;<iframe src=\&quot;//www.youtube.com/embed/77oRSCxGmYA?wmode=opaque&amp;amp;enablejsapi=1\&quot; height=\&quot;480\&quot; width=\&quot;854\&quot; scrolling=\&quot;no\&quot; frameborder=\&quot;0\&quot; allowfullscreen=\&quot;\&quot;>\n</iframe>&quot;,&quot;resolveObject&quot;:&quot;Video&quot;,&quot;resolvedBy&quot;:&quot;youtube&quot;,&quot;resolved&quot;:true,&quot;description&quot;:&quot;Many people have asked how I design and build the parts for the manila folder 777 I'm putting together, and this video is perhaps the best answer yet. This covers the design and build process for the main landing gear, and was made from almost 130 hours of raw footage.&quot;,&quot;title&quot;:&quot;Manila Folder 777 - Main Landing Gear - Design+Build Time-lapse with Info! **60FPS**&quot;,&quot;height&quot;:480,&quot;thumbnail_width&quot;:480,&quot;width&quot;:854,&quot;version&quot;:&quot;1.0&quot;,&quot;type&quot;:&quot;video&quot;,&quot;thumbnail_height&quot;:360,&quot;authorName&quot;:&quot;Luca Iaconi-Stewart&quot;,&quot;authorUrl&quot;:&quot;https://www.youtube.com/user/lucaiaconistewart&quot;,&quot;providerName&quot;:&quot;YouTube&quot;,&quot;providerUrl&quot;:&quot;https://www.youtube.com/&quot;,&quot;thumbnailUrl&quot;:&quot;https://i.ytimg.com/vi/77oRSCxGmYA/hqdefault.jpg&quot;}" data-block-type="22" id="block-yui_3_17_2_1_1559834399264_13753"><p><iframe scrolling="no" data-image-dimensions="854x480" allowfullscreen="" src="//www.youtube.com/embed/77oRSCxGmYA?wmode=opaque&amp;enablejsapi=1" width="854" data-embed="true" frameborder="0" height="480">
</iframe></p><div><p>Many people have asked how I design and build the parts for the manila folder 777 I'm putting together, and this video is perhaps the best answer yet. This covers the design and build process for the main landing gear, and was made from almost 130 hours of raw footage.</p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1559269142757_26027">

<p>
  <h2>EXPLORE</h2><h3>CLICK BELOW FOR MORE INFO ON EACH SECTION</h3>
</p>






















</div><div data-block-type="2" id="block-yui_3_17_2_1_1559870775542_20405">
  <h2>FOLLOW ME FOR UPDATES</h2><p>Check out an expansive catalog of photos and videos about the project, and stay tuned for further updates!</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1559833812352_13795">

<p>
  <h2>NOTABLE PRESS</h2><h3>CLICK TO VIEW ARTICLES</h3>
</p>






















</div></div>

          

          

        </main>

        

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Notice of Service Closure (109 pts)]]></title>
            <link>https://bench.co/</link>
            <guid>42523061</guid>
            <pubDate>Fri, 27 Dec 2024 15:43:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bench.co/">https://bench.co/</a>, See on <a href="https://news.ycombinator.com/item?id=42523061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-sckkx6r4="">  <p><img src="https://bench.co/images/logo.svg" alt="Bench Logo"> </p> <div> <h2>Notice of Service Closure</h2> <p>
We regret to inform you that as of December 27, 2024, the Bench platform
    will no longer be accessible. We know this news is abrupt and may cause
    disruption, so we’re committed to helping Bench customers navigate through
    the transition.
</p> <p>
From the entire team at Bench, it has been an absolute privilege to serve
    small businesses for the last 13 years. Thank you for being part of our
    journey.
</p> <p>The Bench Team</p> </div> <div> <h2>What happens next?</h2> <hr> <p>
On this website, by December 30th, customers will receive further
    information about how to access their Bench data.
</p> </div> <div> <h2>Our Recommendation</h2> <hr> <p><img src="https://bench.co/images/logo.svg" alt="arrow"> <img src="https://bench.co/images/arrow.svg" alt="arrow"> <img src="https://bench.co/images/kick-logo.svg" alt="arrow"> </p> <p>
For continued support with your bookkeeping, we recommend <a href="https://kick.co/bench?utm_source=bench&amp;utm_medium=website&amp;utm_campaign=benchreferral" target="_blank">Kick</a>, a modern accounting software, which has created an exclusive offer to
    handle your ongoing needs.
</p> <p><a href="https://kick.co/bench?utm_source=bench&amp;utm_medium=website&amp;utm_campaign=benchreferral" target="_blank">Learn more</a> </p></div> <div> <h2>Frequently asked questions</h2> <astro-island uid="12vuKB" prefix="r0" component-url="/_astro/FAQSection.DBBdZUlW.js" component-export="default" renderer-url="/_astro/client.DDQftVcU.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;FAQSection&quot;,&quot;value&quot;:true}" await-children=""><dl></dl><!--astro:end--></astro-island> </div> <p>© Bench 2011-2024</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fake Nintendo lawyer is scaring YouTubers, and its not clear YouTube can stop it (177 pts)]]></title>
            <link>https://www.theverge.com/2024/12/27/24326278/nintendo-fake-takedowns-youtube-domtendo-dmca</link>
            <guid>42521873</guid>
            <pubDate>Fri, 27 Dec 2024 13:25:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/12/27/24326278/nintendo-fake-takedowns-youtube-domtendo-dmca">https://www.theverge.com/2024/12/27/24326278/nintendo-fake-takedowns-youtube-domtendo-dmca</a>, See on <a href="https://news.ycombinator.com/item?id=42521873">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In late September, Dominik “Domtendo” Neumayer received a troubling email. He had just featured <em>The</em> <em>Legend of Zelda: Echoes of Wisdom</em> in a series of videos on his YouTube channel. Now, those videos were gone.&nbsp;</p><p>“Some of your videos have been removed,” YouTube explained matter-of-factly. The email said that Domtendo had now received a pair of copyright strikes. He was now just one copyright strike away from losing his 17-year-old channel and the over 1.5 million subscribers he’d built up.&nbsp;</p><p>At least, he would have been, if Domtendo hadn’t spotted something fishy about the takedown notice —&nbsp;something YouTube had missed.&nbsp;</p><p>Domtendo had been a little bit confused right from the start;<strong> </strong>the strikes didn’t make sense. Like countless other creators, Domtendo specializes in “Let’s Play” videos, a well-established genre where streamers play through the entirety of a game on camera.</p><div><p>“The next copyright strike will close your channel”</p></div><p>Nintendo has a complicated relationship with the fans who use its copyrighted works, infamously shutting down all sorts of unauthorized projects by sending cease-and-desists. <a href="https://www.theverge.com/games/24272743/nintendo-retro-game-corps-russ-crandall-profile-youtube-emulation-dmca-takedown-copyright-strike">It has gone after YouTubers</a>, too. But both the Japanese gaming giant and the broader gaming industry typically leave Let’s Plays alone, because they serve as free marketing for their games.</p><p>And yet, YouTube had received a legit-looking request apparently justifying these takedowns under the Digital Millennium Copyright Act (DMCA), signed “Tatsumi Masaaki, Nintendo Legal Department, Nintendo of America.”</p><p>It was in a second email from YouTube that Domtendo spotted something off. The takedown requests came from a <em>personal</em> account at an encrypted email service: “tatsumi-masaaki@protonmail.com”.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="YouTube took action on Domtendo’s videos, even though the requests cited a personal email address." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/376x352/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/384x359/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/415x388/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/480x449/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/540x505/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/640x599/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/750x702/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/828x775/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/1080x1011/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/1200x1123/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/1440x1347/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/1920x1796/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/2048x1916/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/2400x2246/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:948x887/2400x2246/filters:focal(474x444:475x445):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25799995/domtendo_tatsumi_protonmail_hint.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>YouTube took action on Domtendo’s videos, even though the requests cited a personal email address.</em></figcaption> <p><cite>Image: Domtendo</cite></p></div></div><p>Fake takedowns are real. <a href="https://transparencyreport.google.com/youtube-copyright/summary?hl=en">YouTube says</a> over six percent of takedown requests through its public webform are likely fake, and the company accepts requests via plain email too, meaning anyone can file them. Fighting fake takedowns can cost creators time, money, and stress. But creators can’t easily be sure that a takedown is fake — and they can lose their entire channel if they get it wrong and clash with a company that has a legitimate copyright claim.</p><p><a href="https://www.theverge.com/games/24272743/nintendo-retro-game-corps-russ-crandall-profile-youtube-emulation-dmca-takedown-copyright-strike">When the well-respected Retro Game Corps</a> received his second Nintendo copyright strike, he publicly declared he would self-censor all his future work to hopefully escape the company’s wrath. But first, he checked that Nintendo’s threat was real. He checked to see who YouTube listed as the complaining party, and that it came from a Nintendo email address. Then he checked with his YouTube Partner Manager to be extra safe.</p><div><p>Rumors of fake Nintendo takedowns have swirled in the past. Earlier this year, <em>Garry’s Mod </em>developer Garry Newman removed<a href="https://www.theverge.com/2024/4/25/24140246/garrys-mod-nintendo-copyright-takedowns-dmca"> 20 years’ worth of Nintendo-related fan content from his sandbox video game</a> over takedown threats. Fans speculated that it may actually have been someone posing as a Nintendo lawyer. But Newman eventually revealed <a href="https://garry.net/posts/aaron-peters">Nintendo was legitimately behind those takedowns</a> despite using seemingly suspicious names and emails.</p></div><p>Domtendo thought he might have an actual case of a Nintendo faker. So he decided to push back. At first, it seemed to work. He emailed YouTube, and it soon reinstated his videos. But Tatsumi was back the next day —&nbsp;this time, emailing Domtendo directly.&nbsp;</p><p>“Dear Domtendo, I represent Nintendo of America Inc. (“Nintendo”) in intellectual property matters,” the first email began. After a bunch of legalese, Tatsumi eventually explains why he’s reaching out: “I submitted a notice through YouTube’s legal copyright system, but the infringing content still appears.”</p><p>He wouldn’t let it go.&nbsp;</p><p>Domtendo wasn’t about to risk his livelihood just in case Tatsumi was real. He got spooked, and began voluntarily pulling his videos off YouTube. But his new pen pal just kept asking for more removals. Tatsumi reached out day after day, sometimes multiple times a day, according to emails shared with <em>The Verge</em>. The threats got weirder, too:</p><p><strong>October 3rd:</strong></p><div><blockquote><p>I ask for your expeditious removal of all infringing material that use Nintendo Switch game emulators by 6th October 2024. Please note that the amount of videos infringing Nintendo’s copyrights is too high to be able to list them all in this e-mail and we hope that you will conscientiously remove all infringing videos before the next week.&nbsp;</p></blockquote></div><p><strong>October 8th:&nbsp;</strong></p><div><blockquote><p>Nintendo hereby prohibits you from any future use of its intellectual and copyrighted property. Existing content may remain as long as there is no request to remove it. Nintendo of America Inc. would like to avoid further legal action and therefore hopes that their intellectual property will no longer be used by you. This cease-and-desist declaration is valid immediately and has been approved by President of Nintendo of America Doug Spencer Bowser.</p></blockquote></div><p><strong>October 12th:</strong>&nbsp;</p><div><blockquote><p>Nintendo of America Inc. (“Nintendo”) will no longer tolerate this behavior and is now on the verge of filing a lawsuit. Note that we work closely with our subsidiary Nintendo of Europe, located in Germany and therefore already have your address from the time you have been Nintendo Partner and/or will receive your new address from the residents’ registration office.”</p></blockquote></div><p>Domtendo began reaching out to friends and fellow content creators, and discovered he wasn’t alone. <a href="https://www.youtube.com/@Waikuteru">Waikuteru</a>, a streamer who develops Zelda mods, had been targeted by Tatsumi as well. Only that time, the takedown notices were filed in Japanese, and YouTube claimed they’d come from a seemingly real email address: <a href="mailto:anti-piracy3@nintendo.co.jp">anti-piracy3@nintendo.co.jp</a>. Whoever submitted those notices claimed to be a “Group Sub-Manager” in Nintendo’s “Intellectual Property Department.”&nbsp;</p><p>Could Tatsumi be legit? Was Domtendo staring down a real threat?&nbsp;</p><p><em>The Verge</em> could find no public record of a Tatsumi Masaaki working for Nintendo of America or Nintendo’s legal team, period. Nintendo did not respond to <em>The Verge</em>’s repeated requests to fact-check whether such a lawyer even exists.&nbsp;</p><p>But there <em>was </em>a person by a similar name <a href="https://patents.google.com/?inventor=Masaaki+Tatsumi);&amp;assignee=Nintendo&amp;oq=inventor:(Masaaki+Tatsumi);+assignee:(Nintendo)">working on Nintendo technology patents</a> in its home country of Japan, public records show, and Domtendo was dismayed to find a Nintendo email address for that person on the public web.&nbsp;</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="This is the name of a real person (真章 辰己) who worked for Nintendo out of Kyoto, Japan, but the company wouldn’t tell The Verge one word about it." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/376x186/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/384x190/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/415x205/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/480x237/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/540x267/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/640x316/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/750x371/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/828x409/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/1080x534/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/1200x593/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/1440x712/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/1920x949/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/2048x1013/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/2400x1187/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2605x1288/2400x1187/filters:focal(1303x644:1304x645):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800013/tatsumi_patent.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>This is the name of a real person (</em>真章 辰己) <em>who worked for Nintendo out of Kyoto, Japan</em>, <em>but the company wouldn’t tell The Verge one word about it.</em></figcaption> <p><cite>Image: USPTO</cite></p></div></div><p>To a trained eye, there were signs that Domtendo’s “Tatsumi” was probably a fake. What business would a Japanese game technology inventor have individually chasing down a German YouTuber and threatening them with the laws of the United States? If they were a real lawyer, wouldn’t they know that threatening Domtendo with DMCA 512 is laughable, because that’s the portion of the law that protects platforms like YouTube rather than individual creators?&nbsp;</p><p>But Domtendo didn’t want to take the risk, not without proof. His livelihood was at stake. So as Tatsumi’s email threats rolled in, he reached out to Nintendo himself.&nbsp;</p><p>To his great surprise, Nintendo replied.&nbsp;</p><p>“Please note that <a href="mailto:tatsumi-masaaki@protonmail.com">tatsumi-masaaki@protonmail.com</a> is not a legitimate Nintendo email address and the details contained within the communication do not align with Nintendo of America Inc.’s enforcement practices. We are investigating further,” the company’s legal department wrote on October 10th, according to a screenshot shared with <em>The Verge</em>.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Here’s how Nintendo replied to Domtendo." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/376x254/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/384x260/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/415x281/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/480x324/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/540x365/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/640x433/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/750x507/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/828x560/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/1080x730/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/1200x811/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/1440x973/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/1920x1298/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/2048x1384/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/2400x1622/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1074x726/2400x1622/filters:focal(537x363:538x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800023/domtendo_nintendo_screenshot.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>Here’s how Nintendo replied to Domtendo.</em></figcaption></p></div><p>Even then, Domtendo didn’t feel safe. He’d seen how Waikuteru had received a legal threat that seemingly came from a legitimate Nintendo email. Perhaps Tatsumi just wasn’t using his proper email account? Domtendo tried emailing “tasumi_masaaki@nintendo.co.jp” to find out.&nbsp;</p><p>His anxiety ratcheted even higher when Tatsumi’s next email arrived, asking him not to send email to that address. “Please understand that matters are not currently handled from there,” he wrote. Even though it seemed impossible that Tatsumi could be real, he somehow knew things that he shouldn’t.</p><p>Then, on October 18th, Tatsumi suddenly changed his tune: “Dear Domtendo, I hereby retract all of my preceding claims.”</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="The end?" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/376x309/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/384x315/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/415x341/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/480x394/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/540x443/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/640x525/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/750x616/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/828x680/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/1080x886/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/1200x985/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/1440x1182/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/1920x1576/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/2048x1681/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/2400x1970/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:987x810/2400x1970/filters:focal(494x405:495x406):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25800026/the_end_o.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>The end?</em></figcaption></p></div><p>Tatsumi wasn’t done with Domtendo quite yet. Two more emails arrived the same day, explaining that while Nintendo had “suspended” him from filing copyright infringement claims, his Nintendo colleagues would now file them on his behalf. Hours later, Domtendo received what was in some ways the most legit-looking email yet, seemingly sent from anti-piracy3@nintendo.co.jp rather than a personal email address.&nbsp;</p><p>But that email turned out to be Tatsumi’s undoing, when Domtendo checked the headers and discovered they’d spoofed Nintendo’s email address using a publicly available tool on the web. I took the tool for a spin, and sure enough — <a href="https://support.google.com/mail/answer/29436?hl=en">unless you check</a>, anyone can make an email look like it was sent from Nintendo that way.&nbsp;</p><p>Domtendo still doesn’t understand how “Tatsumi” knew he’d emailed the real Tatsumi at Nintendo. He changed his passwords and reformatted his computer, just to be safe. Today, his best guess is that the troll was lurking in his personal Discord channel.&nbsp;</p><p>He’s angry at YouTube for letting this happen. “It’s their fault,” he tells me. “Every idiot can strike every YouTuber and there is nearly no problem to do so. It’s insane,” he writes. “It has to change NOW.”</p><div><p>“Every idiot can strike every YouTuber”</p></div><p>It’s true there isn’t a terribly high bar to submit a YouTube copyright claim, something that YouTube itself admits. Currently, bad actors just need to fill in a form on a website, a place where YouTube sees a “10 times higher attempted abuse rate” than tools with more limited access. Or they can just email YouTube’s copyright department directly. And while <a href="https://www.law.cornell.edu/uscode/text/17/512#:~:text=Elements%20of%20notification">the law</a> technically requires a copyright holder to provide their name and address and state “under penalty of perjury” that they’re authorized to complain on Nintendo’s behalf, there’s nothing compelling YouTube to check they aren’t lying before slapping creators with penalties.&nbsp;</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Photo collage of Blue shells from Mario Kart launching at an anonymous figure." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/376x251/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/384x256/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/415x277/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/480x320/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/540x360/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/640x427/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/750x500/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/828x552/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1080x720/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1200x800/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1440x960/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1920x1280/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2048x1365/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25703633/247348_Nintendo_targeting_Youtuber_CVirginia_A.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><h2><a href="https://www.theverge.com/games/24272743/nintendo-retro-game-corps-russ-crandall-profile-youtube-emulation-dmca-takedown-copyright-strike">Why is Nintendo targeting this YouTuber?</a></h2></p><p>Russ Crandall, aka Retro Game Corps, was actually targeted by Nintendo. <a href="https://www.theverge.com/games/24272743/nintendo-retro-game-corps-russ-crandall-profile-youtube-emulation-dmca-takedown-copyright-strike">Here’s the story</a> of  how the Japanese game company (and YouTube) are threatening his livelihood.</p><p>If you have first-hand information about Nintendo, you can reach me securely on Signal at <strong>seanhollister.01</strong>. </p></div><p>The thing to remember is the DMCA’s “Safe Harbor” isn’t here to protect creators, EFF legal director Corynne McSherry explained to me in 2022. When rightsholders realized they wouldn’t be able to sue every uploader, and internet platforms realized they wouldn’t be able to survive under an onslaught of uploader lawsuits, the law became a compromise to protect platforms from liability as long as they remove infringing content fast. </p><p>“It creates a situation where service providers have very strong incentives to respond,” said McSherry. “They don’t want to mess around and try to figure out if they might be liable or not.”</p><p>Waikuteru and Rimea, a pair of other creators harassed by Tatsumi, agree that the YouTube system is unfair. Neither know for sure whether trolls were responsible for all the takedown notices they’re received, and that’s part of the problem. “The idea that months of worries were caused by a single troll as opposed to a big untouchable company is a hard pill to swallow either way,” says Rimea.&nbsp;</p><p>But they also claim YouTube doesn’t allow smaller channels to challenge copyright strikes in the first place, arguing that it automatically and arbitrarily rejects the legal notices that would let them reinstate their videos. “YouTube decides whether someone loses his channel based on channel size,” says Waikuteru.&nbsp;</p><p>YouTube isn’t particularly interested in talking about any of this, though.&nbsp;</p><p>While YouTube spokesperson Jack Malon did confirm that “Tatsumi” made false claims, the company wouldn’t explain why the company even briefly accepted false claims from a protonmail.com email address as legitimate, and repeatedly dodged questions about whether Tatsumi made false claims on other creators’ videos, too. </p><p>YouTube wouldn’t even tell me whether Domtendo was still in danger of false copyright claims from this specific individual, or offer assurances that it would take any new action to prevent this sort of behavior in the future.&nbsp;</p><p>Malon does claim that YouTube has “dedicated teams working to detect and prevent abuse,” however, and “work to ensure that any associated strikes are reversed” when bad actors make false claims.&nbsp;</p><p>As for the troll, Tatsumi declined <em>The Verge</em>’s interview request. “Dear Sean, I am an authorized agent for Nintendo of America Inc,” they replied, staying in character to the very end.&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Does current AI represent a dead end? (338 pts)]]></title>
            <link>https://www.bcs.org/articles-opinion-and-research/does-current-ai-represent-a-dead-end/</link>
            <guid>42521865</guid>
            <pubDate>Fri, 27 Dec 2024 13:24:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bcs.org/articles-opinion-and-research/does-current-ai-represent-a-dead-end/">https://www.bcs.org/articles-opinion-and-research/does-current-ai-represent-a-dead-end/</a>, See on <a href="https://news.ycombinator.com/item?id=42521865">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <!--Header-->
        
        <!--header-->


        





    <main id="main-content-hub">
        


            <div>
                                <p>Eerke Boiten, Professor of Cyber Security at De Montfort University Leicester, explains his belief that current AI should not be used for serious applications.</p>
                            </div>


            <section>


<div>
                    <p>From the perspective of <a href="https://www.bcs.org/articles-opinion-and-research/how-to-become-a-software-engineer/">software engineering</a>, current AI systems are unmanageable, and as a consequence their use in serious contexts is irresponsible. For foundational reasons (rather than any temporary technology deficit), the tools we have to manage complexity and scale are just not applicable.</p>
<p>By ‘software engineering’, I mean developing software to align with the principle that impactful software systems need to be trustworthy, which implies their development needs to be managed, transparent and accountable. I don’t suggest any particular methodologies or tools (before you know it, I’d have someone explaining why ‘waterfall’ is wrong!) — but there are some principles which I believe to be both universal and unsatisfiable with current AI systems.</p>
<p>When I last gave talks about AI ethics, around 2018, my sense was that AI development was taking place alongside the abandonment of responsibility in two dimensions. Firstly, and following on from what was already happening in ‘big data’, the world stopped caring about where AI got its data — fitting in nicely with ‘<a href="https://www.amazon.co.uk/Age-Surveillance-Capitalism-Future-Frontier/dp/1781256845">surveillance capitalism</a>. And secondly, contrary to what professional organisations like BCS and ACM had been preaching for years, the outcomes of AI algorithms were no longer viewed as the responsibility of their designers — or anybody, really.</p>
<p>‘Explainable AI’ and some ideas about mitigating bias in AI were developed in response to this, and for a while this looked promising — but unfortunately, the data responsibility issue has not gone away, and the major developments in AI since then have made responsible engineering only more difficult.</p>
<h2>How neural networks work</h2>
<p>When I say ‘current AI systems’, I mean systems based on large neural networks, including most generative AI, large language models (LLMs) like ChatGPT, most of what DeepMind and OpenAI are producing and developing, and so on. An extremely optimistic view of these is what I would call ‘LLM-functionalism’: the idea that a natural language description of the required functionality fed to an LLM, possibly with some prompt engineering, establishes a meaningful implementation of the functionality.</p>
<p>The neural networks underlying these systems have millions of ‘nodes’ or ‘neurons’. Each has one output and multiple inputs, which either originate externally to the entire network or are taken from other nodes’ outputs. A node’s output is determined by the inputs, the weights put on each input, and an ‘activation function’ that decides how the weighted inputs translate into an output. The connection structure is fixed, using connected ‘layers’ or other structures such as recurrent networks or transformers, as is the activation function. The fixed structure is relevant to what type of problems the network can deal with, but the network’s functionality is almost entirely introduced by ‘training’, which means setting and modifying the weights of each input until the outputs achieve an objective satisfactorily on a set of training data.</p>
<p>The training of the huge networks that make up current AI systems has typically taken an astronomical amount of compute, measurable in the millions of dollars or kWh, and will necessarily have been mostly unsupervised or self-supervised. Put bluntly, it will have required no human input – though there may have been some human tuning afterwards (such as reinforcement learning from human feedback (RLHF) or ‘guard rails’), or when the system runs (such as context and prompt engineering).</p>
<h2>Emergence and compositionality</h2>
<p>Many of these neural network systems are stochastic, meaning that providing the same input will not always lead to the same output. The behaviour of such AI systems is ‘emergent’ — which means despite the fact that the behaviour of each neuron is given by a precise mathematical formula, neither this behaviour nor the way the nodes are connected are of much help in explaining the network’s overall behaviour.</p>
<p>My first 20 years of research were in formal methods, where <a href="https://link.springer.com/book/10.1007/978-3-319-92711-4">mathematics and logic are used to ensure systems operate according to precise formal specifications</a>, or at least to support verification of implemented systems. Software engineering, and particularly formal methods, <a href="https://www.sciencedirect.com/science/article/pii/S1571066105050966">has not been as successful in managing emergent behaviour</a> or even the aspects of ‘traditional’ systems that have emergent tendencies such as resource usage or security. This is for foundational reasons rather than for a lack of scientific effort.</p>
                </div>

<div>

                        <blockquote>

                                            <p>
                                                For you
                                            </p>
<p>Be part of something bigger,<span>&nbsp;</span><a href="https://www.bcs.org/membership-and-registrations/become-a-member/" title="Become a member">join BCS, The Chartered Institute for IT</a>.</p>

                        </blockquote>
<p><a href="https://link.springer.com/book/10.1007/3-540-49213-5">A central property in formal software engineering is compositionality</a>: the idea that composite systems can be understood in terms of the meanings of their parts and the nature of the composition, rather than by having to look at the parts themselves.</p>
<p>This idea lies at the heart of piecewise development: parts can be engineered (and verified) separately and hence in parallel, and reused in the form of modules, libraries and the like in a ‘black box’ way, with re-users being able to rely on any verification outcomes of the component and only needing to know their interfaces and their behaviour at an abstract level. Reuse of components not only provides increased confidence through multiple and diverse use, but also saves costs.</p>
<h2>Issues arising from emergent over compositional</h2>
<p>Unfortunately, my informal definitions of ‘emergent’ and ‘compositionality’ are almost exact opposites, and this raises several issues:</p>
<ul>
<li>Current AI systems have no internal structure that relates meaningfully to their functionality. They cannot be developed, or reused, as components. There can be no separation of concerns or piecewise development. A related issue is that most current AI systems do not create explicit models of knowledge — in fact, many of these systems developed from techniques in image analysis, where humans have been notably unable to create knowledge models for computers to use, and all learning is by example (‘<a href="https://www.acluohio.org/en/cases/jacobellis-v-ohio-378-us-184-1964#:~:text=TheU.S.SupremeCourtreversed,tohardcorepornography...">I know it when I see it</a>’). This has multiple consequences for development and verification.</li>
<li>There are no intermediate models at different levels of abstraction to describe the system. There is no possibility for stepwise development — using either informal or formal methods.</li>
<li>Systems are not explainable, as they have no model of knowledge and no representation of any ‘reasoning’.</li>
<li>Even a ‘human in the loop’ adds little explainability, as they can only explain system outcomes (and learn from them) by doing their own reasoning on the input data from scratch.</li>
</ul>
<h2>Verification</h2>
<p>Verification comes with a subset of issues following from the above. The only verification that is possible is of the system in its entirety; if there are no handles for generating confidence in the system during its development, we have to put all our eggs in the basket of post-hoc verification. Unfortunately, that is severely hampered, following from the issues listed above:</p>
<ul>
<li>Current AI systems have input and state spaces too large for exhaustive testing.</li>
<li>A correct output on a test of a stochastic system only evidences that the system has the capability to respond correctly to this input, but not that it will do this always or frequently enough.</li>
<li>Lacking components, current AI systems do not allow verification by parts (unit testing, integration testing, etc).</li>
<li>As the entire system is involved in every computation, there are no meaningful notions of coverage to gain confidence from non-exhaustive whole system testing.</li>
</ul>
<p>So, whole system testing is the only verification tool available, but it can never represent more than a drop in the ocean.</p>
<h2>Faults</h2>
<p>There are serious additional issues around faults and fixing. Faults may arise from unreliable input data, which certainly applies to many generative AI systems, but also from training data being sparse in parts of the input domain.</p>
<ul>
<li>Current AI systems have faults, but even their error behaviour is likely emergent, and certainly hard to predict or eradicate.</li>
<li>Given the relative efforts of unsupervised training versus human error correction and feedback learning, there can never be confidence in correctness, arguing from scale alone.</li>
<li>Fixes of errors through retraining are not localised and regression testing is not possible, so newly introduced errors are likely, but not easily discoverable.</li>
</ul>
<h2>Conclusions</h2>
<p>In my mind, all this puts even state-of-the-art current AI systems in a position where professional responsibility dictates the avoidance of them in any serious application. When all its techniques are based on testing, AI safety is an intellectually dishonest enterprise.</p>
<p>So, is there hope? I believe — though I would be happy to be proved wrong on this — that current generative AI systems represent a dead end, where exponential increases of training data and effort will give us modest increases in impressive plausibility but no foundational increase in reliability. I would love to see compositional approaches to neural networks, hard as it appears.</p>
<p>However, <a href="https://aisafety.dance/">hybrids between symbolic and intuition-based AI</a> should be possible — systems that do generate some explicit knowledge models or confidence levels, or that are coupled with more traditional data retrieval or theorem proving. Current AI systems also have a role to play as components of larger systems in limited scopes where their potentially erroneous outputs can be reliably detected and managed, or in contexts such as weather prediction where we had always expected stochastic predictions rather than certainty.</p>                </div>

            </section>



    
        <!-- topics -->
        <!-- related -->
        <div>
                        <p>
                            <h2>Related</h2>
                        </p>
                    </div>


        <!--page sharing modal-->


            
        <!--modal-->
    </main>







<!--footer-->

<!--footer-->
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I send myself automated emails to practice Dutch (106 pts)]]></title>
            <link>https://github.com/ThReinecke/dutch_vocabulary</link>
            <guid>42521773</guid>
            <pubDate>Fri, 27 Dec 2024 13:05:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ThReinecke/dutch_vocabulary">https://github.com/ThReinecke/dutch_vocabulary</a>, See on <a href="https://news.ycombinator.com/item?id=42521773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Daily Dutch Vocabulary Email Automation</h2><a id="user-content-daily-dutch-vocabulary-email-automation" aria-label="Permalink: Daily Dutch Vocabulary Email Automation" href="#daily-dutch-vocabulary-email-automation"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">This project automates the daily delivery of an email containing three C1-level Dutch words, their English translations, and example sentences. The email looks like this:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ThReinecke/dutch_vocabulary/blob/main/images/email.png"><img src="https://github.com/ThReinecke/dutch_vocabulary/raw/main/images/email.png" alt="Screenshot of email"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Motivation</h2><a id="user-content-motivation" aria-label="Permalink: Motivation" href="#motivation"></a></p>
<p dir="auto">I created this project because I couldn't find a suitable app to help me build a C1-level Dutch vocabulary. I discovered that ChatGPT provides good word suggestions and decided to automate the process. Additionally, I know that I check emails more consistently than apps, making this method more effective for learning.</p>
<p dir="auto">This project also provided an opportunity to refresh my skills in <strong>Terraform</strong> and <strong>Python</strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Simplified Architecture</h2><a id="user-content-simplified-architecture" aria-label="Permalink: Simplified Architecture" href="#simplified-architecture"></a></p>
<p dir="auto">A CloudWatch Event Rule triggers a Lambda each morning at 7:00. The Lambda retrieves all previously sent Dutch words from DynamoDB. It then retrieves three new words from ChatGPT, stores them in DynamoDB, and sends them to SES. SES delivers them to the end user's email.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ThReinecke/dutch_vocabulary/blob/main/images/architecture.jpg"><img src="https://github.com/ThReinecke/dutch_vocabulary/raw/main/images/architecture.jpg" alt="Picture of architecture"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<p dir="auto">To deploy this project, ensure the following tools and configurations are in place:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Tools Installed:</strong></p>
<ul dir="auto">
<li>Python (Tested with Python 3.8)</li>
<li>pip (Tested with pip 19.2.3)</li>
<li>Terraform (Tested with Terraform 1.10.3)</li>
<li>AWS CLI (Tested with 2.15.58)</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Permissions:</strong>
Your AWS CLI user must have the appropriate permissions to deploy the resources. Refer to the Terraform files and apply the principle of least privilege.</p>
</li>
<li>
<p dir="auto"><strong>Amazon SES Verified Email:</strong>
You need a verified email address in Amazon SES. This email must match the one used in the project.<br>
Reference: <a href="https://docs.aws.amazon.com/ses/latest/dg/creating-identities.html#verify-email-addresses-procedure" rel="nofollow">Verifying Email Addresses in Amazon SES</a>.</p>
</li>
<li>
<p dir="auto"><strong>Optional:</strong><br>
You can zip the Lambda deployment package manually if you like:</p>
<ul dir="auto">
<li>Use the provided <code>setup.sh</code> script or follow the steps in the script manually (might need small modifications if on Mac/Linux)</li>
<li>Alternatively, use the pre-zipped package: <code>deployment_package.zip</code>.</li>
</ul>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deployment Steps</h3><a id="user-content-deployment-steps" aria-label="Permalink: Deployment Steps" href="#deployment-steps"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Prepare Configuration:</strong></p>
<ul dir="auto">
<li>Copy <code>terraform.tfvars.example</code> to <code>terraform.tfvars</code>.</li>
<li>Fill out the required values in <code>terraform.tfvars</code>.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Run the Terraform Workflow:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="terraform init
terraform plan
terraform apply"><pre>terraform init
terraform plan
terraform apply</pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Considerations</h2><a id="user-content-considerations" aria-label="Permalink: Considerations" href="#considerations"></a></p>
<p dir="auto">This project was intended as a <strong>weekend project</strong>, so there is room for improvement. Potential enhancements include:</p>
<ul dir="auto">
<li>Refactoring the Python code to be asynchronous for better performance and robustness.</li>
<li>Splitting the <code>lambda_function.py</code> file into smaller modules for better organization and maintainability.</li>
</ul>
<p dir="auto">However, since the project fulfills its purpose and is unlikely to grow further, I kept the implementation simple.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why OpenAI's Structure Must Evolve to Advance Our Mission (206 pts)]]></title>
            <link>http://openai.com/index/why-our-structure-must-evolve-to-advance-our-mission</link>
            <guid>42521744</guid>
            <pubDate>Fri, 27 Dec 2024 12:57:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://openai.com/index/why-our-structure-must-evolve-to-advance-our-mission">http://openai.com/index/why-our-structure-must-evolve-to-advance-our-mission</a>, See on <a href="https://news.ycombinator.com/item?id=42521744">Hacker News</a></p>
Couldn't get http://openai.com/index/why-our-structure-must-evolve-to-advance-our-mission: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Missiles Are Now the Biggest Killer of Airline Passengers (389 pts)]]></title>
            <link>https://www.wsj.com/world/flight-deaths-shot-from-sky-rising-798fd31e</link>
            <guid>42521598</guid>
            <pubDate>Fri, 27 Dec 2024 12:21:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/world/flight-deaths-shot-from-sky-rising-798fd31e">https://www.wsj.com/world/flight-deaths-shot-from-sky-rising-798fd31e</a>, See on <a href="https://news.ycombinator.com/item?id=42521598">Hacker News</a></p>
Couldn't get https://www.wsj.com/world/flight-deaths-shot-from-sky-rising-798fd31e: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Building AI Products–Part I: Back-End Architecture (144 pts)]]></title>
            <link>https://philcalcado.com/2024/12/14/building-ai-products-part-i.html</link>
            <guid>42521241</guid>
            <pubDate>Fri, 27 Dec 2024 10:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://philcalcado.com/2024/12/14/building-ai-products-part-i.html">https://philcalcado.com/2024/12/14/building-ai-products-part-i.html</a>, See on <a href="https://news.ycombinator.com/item?id=42521241">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  

  
  <article>
      <p>In 2023, we launched an AI-powered Chief of Staff for engineering leaders—an assistant that unified information across team tools and tracked critical project developments. Within a year, we attracted 10,000 users, <a href="https://outropy.ai/blog/2024-04-19-outropy_vs_slack_ai/">outperforming even deep-pocketed incumbents such as Salesforce and Slack AI</a>. Here is an early demo:</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/8mr5eZNXDlo?si=-IIK5uO5cTN9FFhi" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>By May 2024, we realized something interesting: while our AI assistant was gaining traction, there was overwhelming demand for the technology we built to power it. Engineering leaders using the platform were reaching out non-stop to ask not about the tool but how we made our agents work so reliably at scale and be, you know, <em>actually useful</em>. This led us to pivot to <a href="https://outropy.ai/">Outropy</a>, a developer platform that enables software engineers to build AI products.</p>

<p>Building with Generative AI at breakneck pace while the industry was finding its footing taught us invaluable lessons—lessons that now form the core of the Outropy platform. While LinkedIn overflows with thought leaders declaring every new research paper a “game changer,” few explain what the game actually is. This series aims to change that.</p>

<p>This three-part series will cover:</p>

<ul>
  <li>How we built the AI agents powering the assistant</li>
  <li>How we constructed and operate our inference pipelines</li>
  <li>The AI-specific tools and techniques that made it all work</li>
</ul>

<p>This order is intentional. So much content out there fixates on choosing the best reranker or chasing the latest shiny technology, and few discuss how to build useful AI software. This is a report from the trenches, not the ivory tower.</p>

<h3 id="structuring-an-ai-application">Structuring an AI Application</h3>

<p>Working with AI presents exciting opportunities and unique frustrations for a team like ours, with decades of experience building applications and infrastructure.</p>

<p>AI’s stochastic (probabilistic) nature fundamentally differs from traditional deterministic software development—but that’s only part of the story. With years of experience handling distributed systems and <a href="https://nighthacks.com/jag/res/Fallacies.html">their inherent uncertainties</a>, we’re no strangers to unreliable components.</p>

<p>The biggest open questions lie in structuring GenAI systems for long-term evolution and operation, moving beyond the quick-and-dirty prompt chaining that suffices for flashy demos.</p>

<p>In my experience, there are two major types of components in a GenAI system:</p>

<ul>
  <li><strong>Inference Pipelines:</strong> A deterministic sequence of operations that transforms inputs through one or more AI models to produce a specific output. Think of RAG pipelines generating answers from documents—each step follows a fixed path despite the AI’s probabilistic nature.</li>
  <li><strong>Agents:</strong> Autonomous software entities that maintain state while orchestrating AI models and tools to accomplish complex tasks. These agents can reason about their progress and adjust their approach across multiple steps, making them suitable for longer-running operations.</li>
</ul>

<p>Our journey began with <a href="https://www.youtube.com/watch?v=ePFEpU5crN0&amp;ab_channel=PhilCal%C3%A7ado">a simple Slack bot</a>. This focused approach let us explore GenAI’s possibilities and iterate quickly without getting bogged down in architectural decisions. During this period, we only used distinct inference pipelines and tied their results together manually.</p>

<p>This approach served us well until we expanded our integrations and features. As the application grew, our inference pipelines became increasingly complex and brittle, struggling to reconcile data from different sources and formats while maintaining coherent semantics.</p>

<p>This complexity drove us to adopt a <em>multi-agentic system</em>.</p>

<h3 id="what-are-agents-really">What are agents, really?</h3>

<p>The industry has poured billions into AI agents, yet most discussions focus narrowly on <a href="https://en.wikipedia.org/wiki/Robotic_process_automation">RPA</a>-style, no-code and low-code automation tools. Yes, frameworks like CrewAI, AutoGen, Microsoft Copilot Studio, and Salesforce’s Agentforce serve an important purpose—they give business users the same power that shell scripts give Linux admins. But just like you wouldn’t build a production system in Bash, these frameworks are just scratching the surface of what agents can be.</p>

<p>The broader concept of agents has a rich history in academia and AI research, offering much more interesting possibilities for product development. Still, as a tiny startup on a tight deadline, rather than get lost in theoretical debates, we distilled practical traits that guided our implementation:</p>

<ul>
  <li><strong>Semi-autonomous:</strong> Functions independently with minimal supervision, making local decisions within defined boundaries.</li>
  <li><strong>Specialized:</strong> Masters specific tasks or domains rather than attempting general-purpose intelligence.</li>
  <li><strong>Reactive:</strong> Responds intelligently to requests and environmental changes, maintaining situational awareness.</li>
  <li><strong>Memory-driven:</strong> Maintains and leverages both immediate context and historical information to inform decisions.</li>
  <li><strong>Decision-making:</strong> Analyzes situations, evaluates options, and executes actions aligned with objectives.</li>
  <li><strong>Tool-using:</strong> Effectively employs various tools, systems, and APIs to accomplish tasks.</li>
  <li><strong>Goal-oriented:</strong> Adapts behavior and strategies to achieve defined objectives while maintaining focus.</li>
</ul>

<p>While these intelligent components are powerful, we quickly learned that not everything needs to be an agent. Could we have built our Slackbot and productivity tool connectors using agents? Sure, but the traditional design patterns worked perfectly well, and our limited resources were better spent elsewhere. The same logic applied to standard business operations—user management, billing, permissions, and other commodity functions worked better with conventional architectures.</p>

<p>This meant that we had the following <a href="https://learning.oreilly.com/library/view/software-architecture-patterns/9781491971437/ch01.html#idm46407728082304">layered architecture</a> inside our application:</p>

<p><img src="https://philcalcado.com/img/building-ai-products-i/arch-mono.png" alt=""></p>

<h3 id="agents-are-not-microservices">Agents are not Microservices</h3>

<p>I’ve spent the last decade deep in microservices—from pioneering work at ThoughtWorks to helping underdogs like SoundCloud, DigitalOcean, SeatGeek, and Meetup punch above their weight. So naturally, that’s where we started with our agent architecture.</p>

<p>Initially, we implemented agents as <a href="https://martinfowler.com/eaaCatalog/serviceLayer.html">a service layer</a> with traditional request/response cycles:</p>

<p><img src="https://philcalcado.com/img/building-ai-products-i/arch-agents-service-layer.png" alt=""></p>

<p>One of the biggest appeals of this architecture was that, even if we expected our application to be a monolith for a long time, it creates an easier path to extracting services as needed and benefit from <em>horizontal scalability</em> when the time comes.</p>

<p>Unfortunately, the more we went down the path, the more obvious it became that stateless microservices and AI agents just don’t play nice together. Microservices are all about splitting a particular feature into small units of work that need minimal context to perform the task at hand. The same traits that make agents powerful create a significant impedance mismatch with these expectations:</p>

<ul>
  <li><strong>Stateful Operation</strong>: Agents must maintain rich context across interactions, including conversation history and planning states. This fundamentally conflicts with microservices’ stateless nature and complicates scaling and failover.</li>
  <li><strong>Non-deterministic Behavior</strong>: Unlike traditional services, agents are basically state machines with unbounded states. They behave completely differently depending on context and various probabilistic responses. This breaks core assumptions about caching, testing, and debugging.</li>
  <li><strong>Data-Intensive with Poor Locality</strong>: Agents process massive amounts of data through language models and embeddings, with poor data locality. This contradicts microservices’ efficiency principle.</li>
  <li><strong>Unreliable External Dependencies</strong>: Heavy reliance on external APIs such as LLMs, embedding services, and tool endpoints creates complex dependency chains with unpredictable latency, reliability, and costs.</li>
  <li><strong>Implementation Complexity</strong>: The combination of prompt engineering, planning algorithms, and tool integrations creates debugging challenges that compound with distribution.</li>
</ul>

<p>Not only did this impedance mismatch cause a lot of pain while writing and maintaining the code, but agentic systems are so far away from the ubiquitous <a href="https://12factor.net/">12-factor</a> model that attempting to leverage existing microservice tooling became an exercise in fitting square pegs into round holes.</p>

<h3 id="agents-are-more-like-objects">Agents are more like objects</h3>

<p>If microservices weren’t the right fit, another classic software engineering paradigm offered a more natural abstraction for agents: object-oriented programming.</p>

<p>Agents naturally align with OOP principles: they maintain encapsulated state (their memory), expose methods (their tools and decision-making capabilities via inference pipelines), and communicate through message passing. <a href="https://userpage.fu-berlin.de/~ram/pub/pub_jf47ht81Ht/doc_kay_oop_en">This mirrors Alan Kay’s original vision</a>:</p>

<blockquote>
  <p>OOP to me means only messaging, local retention and protection and hiding of state-process, and extreme late-binding of all things.</p>
</blockquote>

<p><img src="https://philcalcado.com/img/building-ai-products-i/uml.png" alt=""></p>

<p>We’ve been in the industry long enough to remember the nightmares of distributed objects and the fever dreams of CORBA and J2EE. Yet, objects offered us a pragmatic way to quickly iterate on our product and defer the scalability question until we actually need to solve that.</p>

<p>We evolved our agents from <a href="https://martinfowler.com/bliki/EvansClassification.html">stateless Services to Entities</a>, giving them distinct identities and lifecycles. This meant each user or organization maintained their own persistent agent instances, managed through <a href="https://philcalcado.com/2010/12/23/how_to_write_a_repository.html">Repositories</a> in our database.</p>

<p>This drastically simplified our function signatures by eliminating the need to pass extensive context as arguments on every agent call. It also lets us leverage battle-tested tools like SQLAlchemy and Pydantic to build our agents, while enabling unit tests with stubs/mocks instead of complicated integration tests.</p>

<h3 id="implementing-agentic-memory">Implementing Agentic Memory</h3>

<p>Agents’ memories can be as simple as a single value to as complicated as keeping track of historical information since the beginning of times. In our assistant, we have both types and more.</p>

<p>For simple, narrow-focused agents such as the “Today’s Priorities” agents had to remember nothing more than a list of high-priority things they were monitoring and eventually taking action, such as sending a notification if they weren’t happy with the progress. Others, like our “Org Chart Keeper” had to keep track of all interactions between everyone in the organizations and use that to infer reporting lines and teams people belonged to.</p>

<p>The agents with simpler persistence needs would usually just store their data on a dedicated table using <a href="https://docs.sqlalchemy.org/en/20/orm/">SQLAlchemy’s ORM</a>. This obviously wasn’t an option for the more complicated memory needs, so we had to apply a different model</p>

<p>After some experimentation, we adopted <a href="https://martinfowler.com/bliki/CQRS.html">CQRS</a> with <a href="https://martinfowler.com/eaaDev/EventSourcing.html">Event Sourcing</a>. In essence, every state change—whether creating a meeting or updating team members—was represented as a <em>Command</em>, a discrete event recorded chronologically—much like a database <a href="https://en.wikipedia.org/wiki/Transaction_log">transaction log</a>. The current state of any object could then be reconstructed by replaying all its associated events in sequence.</p>

<p>While this approach has clear benefits, replaying events solely to respond to a query is slow and cumbersome, especially when most queries focus on the current state rather than historical data. To address, CQRS suggests that we maintain a continuously updated, query-optimized representation of the data, similar to materialized views in a relational database. This ensured quick reads without sacrificing the advantages of event sourcing. We started off storing events and query models in Postgres, planning to move them to DynamoDB when we started having issues.</p>

<p>One big challenge in this model is that only an agent knows what matters to them. For example, if a user would change cancel a scheduled meeting, which agents should care about this event? The scheduling agent for sure, but if this meeting was about a specific project you might also want the project management agent to know about it as it might impact the roadmap.</p>

<p>Rather than building an all-knowing router to dispatch events to the right agents—risking the creation of a <a href="https://en.wikipedia.org/wiki/God_object">God object</a>—we took inspiration from <a href="https://developers.soundcloud.com/blog/building-products-at-soundcloud-part-1-dealing-with-the-monolith">my experience at SoundCloud</a>. There, we developed a semantic event bus enabling interested parties to publish and observe events for relevant entities:</p>

<blockquote>
  <p>Soon enough, we realized that there was a big problem with this model; as our microservices needed to react to user activity. The push-notifications system, for example, needed to know whenever a track had received a new comment so that it could inform the artist about it.  […] over several iterations we developed a model called Semantic Events, where changes in the domain objects result in a message being dispatched to a broker and consumed by whichever microservice finds the message interesting.</p>
</blockquote>

<p><img src="https://philcalcado.com/img/building-ai-products-i/semantic-events.png" alt=""></p>

<p>Following this model, all state-change events were posted to an event bus that agents could subscribe to. Each agent filtered out irrelevant events independently, removing the need for external systems to know what they cared about. Since we were working within a single monolith at the time, we implemented a straightforward <a href="https://en.wikipedia.org/wiki/Observer_pattern">Observer pattern</a> using <a href="https://docs.sqlalchemy.org/en/20/orm/events.html">SQLAlchemy’s native event system</a>, with plans to eventually migrate to <a href="https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/">DynamoDB Streams</a>.</p>

<p>Inside our monolith, the architecture looked like this:</p>

<p><img src="https://philcalcado.com/img/building-ai-products-i/memory1.png" alt=""></p>

<p>Managing both the ORM approach for simpler objects and CQRS for more complex needs grew increasingly cumbersome. Small refactorings or shared logic across all agents became harder than necessary. Ultimately, we decided the simplicity of ORM wasn’t worth the complexity of handling two separate persistence models. We converted all agents to the CQRS style but retained ORM for non-agentic components.</p>

<h3 id="handling-events-in-natural-language">Handling Events in Natural Language</h3>

<p>CQRS and its supporting tools excel with well-defined data structures. At SoundCloud, events like <em>UploadTrack</em> or <em>CreateTrackComment</em> were straightforward and unambiguous. AI systems, however, present a very different challenge.</p>

<p>Most AI systems deal with the uncertainty of natural language. This makes the process of consolidating the Commands into a “materialized view” hard. For example, what events correspond to someone posting a Slack message like <em>“I am feeling sick and can’t come to the office tomorrow, can we reschedule the project meeting?”</em></p>

<p>We started with the naive approach most agentic systems use: running every message through an inference pipeline to extract context, make decisions, and take actions via tool calling. This approach faced two problems: first, reliably doing all this work in a single pipeline is hard even with frontier models—more on this in part II. Second, we ran into the God object problem discussed earlier—our logic was spread across many agents, and no single pipeline could handle everything.</p>

<p>One option involved sending each piece of content—Slack messages, GitHub reviews, Google Doc comments, emails, calendar event descriptions…—to every agent for processing. While this was straightforward to implement via our event bus, each agent would need to run its inference pipeline for every piece of content. This would offer all sorts of performance and cost issues due to frequent calls to LLMs and other models, especially considering that the vast majority of content wouldn’t be relevant to a particular agent.</p>

<p>We wrestled with this problem for a while, exploring some initially promising but ultimately unsuccessful attempts at <a href="https://www.mathworks.com/discovery/feature-extraction.html">Feature Extraction</a> using simpler ML models instead of LLMs. That said, I believe this approach can work well in constrained domains—indeed, we use it in Outropy to route requests within the platform.</p>

<p>Our solution built on <a href="https://arxiv.org/pdf/2312.06648">Tong Chen’s Proposition-Based Retrieval research</a>. We already <a href="https://www.linkedin.com/posts/pcalcado_ai-vs-human-readability-the-unnecessary-activity-7275345861222535168-3fIB?utm_source=share&amp;utm_medium=member_desktop">used this approach to ingest structured content like CSV file</a>s, where instead of directly embedding it into a vector database, we first use an LLM to generate natural language factoids about the content. While these factoids add no new information, their natural language format makes vector similarity search much more effective than the original spreadsheet-like structure.</p>

<p>Our solution was to use an LLM to generate propositions for every message, structured according to a format inspired by <a href="https://github.com/amrisi/amr-guidelines/blob/master/amr.md">Abstract Meaning Representation</a>, a technique from natural language processing.</p>

<p>This way, if user <em>Bob</em> sends a message like <em>“I am feeling sick and can’t come to the office tomorrow, can we reschedule the project meeting?”</em> on the <code>#project-lavender</code> channel we would get structured propositions such as:</p>

<p><img src="https://philcalcado.com/img/building-ai-products-i/amr.jpg" alt=""></p>

<p>Naturally, we had to carefully batch messages and discussions to minimize costs and latency. This necessity became a major driver behind developing Outropy’s automated pipeline optimization using Reinforcement Learning.</p>

<h3 id="scaling-to-10000-users">Scaling to 10,000 Users</h3>

<p>As mentioned a few times, Throughout this whole process, it was very important to us to minimize the amount of time and energy invested in technical topics unrelated to learning about our users and how to use AI to build products.</p>

<p>We kept our assistant as a single component, with a single code base and a single container image that we deployed using AWS Elastic Container Service. Our agents were simple Python classes using SQLAlchemy and Pydantic, and we relied on FastAPI and asyncio’s excellent features to handle the load. Keeping things simple allowed us to make massive progress on the product side, to a point we went from 8 to 2,000 users in about two months.</p>

<p><img src="https://philcalcado.com/img/building-ai-products-i/arch-mono.png" alt=""></p>

<p>That’s when things started breaking down. Our personal daily briefings—our flagship feature—went from taking minutes to hours per user. We’d trained our assistant to learn each user’s login time and generate reports an hour before, ensuring fresh updates. But as we scaled, we had to abandon this personalization and batch process everything at midnight, hoping reports would be ready when users logged in.</p>

<p>As an early startup, growth had to continue, so we needed a quick solution. We implemented organization-based sharding with a simple configuration file: smaller organizations shared a container pool, while those with thousands of users got dedicated resources. This isolation allowed us to keep scaling while maintaining performance across our user base.</p>

<p><img src="https://philcalcado.com/img/building-ai-products-i/arch-shards.png" alt=""></p>

<p>This simple change gave us breathing room by preventing larger accounts from blocking smaller ones. We also added priority processing, deprioritizing inactive users and those we learned were away from work.</p>

<p>While sharding gave us parallelism, we quickly hit the fundamental scaling challenges of GenAI systems. Traditional microservices can scale horizontally because their external API calls are mostly for data operations. But in AI systems, these slow and unpredictable third-party API calls are your critical path. They make the core decisions, and this means everything is blocked until you get a response.</p>

<p>Python’s async features proved invaluable here. We restructured our agent-model interactions using <a href="https://refactoring.guru/design-patterns/chain-of-responsibility">Chain of Responsibility</a>, which let us properly separate CPU-bound and IO-bound work. Combined with some classic systems tuning—increasing container memory and <code>ulimit</code> for more open sockets—we saw our request backlog start to plummet.</p>

<p><a href="https://platform.openai.com/docs/guides/rate-limits">OpenAI rate limits</a> became our next bottleneck. We responded with a token budgeting system that <a href="https://dagster.io/glossary/data-backpressure">applied backpressure</a> while hardening our LLM calls with exponential backoffs, caching, and fallbacks. Moving the heaviest processing to off-peak hours gave us extra breathing room.</p>

<p>Our final optimization on the architectural: moving from OpenAI’s APIs to Azure’s GPT deployments. The key advantage was <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits">Azure’s per-deployment quotas</a>, unlike OpenAI’s organization-wide limits. This let us scale by load-balancing across multiple deployments. To manage the shared quota, we extracted our GPT calling code into a dedicated service rather than adding distributed locks</p>

<p><img src="https://philcalcado.com/img/building-ai-products-i/arch-gpt-proxy.png" alt=""></p>

<h3 id="the-zero-one-infinity-rule">The Zero-one-infinity rule</h3>

<p>One of my favorite adages in computer science is <a href="https://en.wikipedia.org/wiki/Zero_one_infinity_rule">“There are only three numbers: zero, one, and infinity.”</a> In software engineering, this manifests as having either zero modules, a monolith, or an arbitrary and always-growing number. As such, extracting the <code>GPTProxy</code> as our first remote service paved the way for similar changes.</p>

<p>The most obvious opportunity to simplify our monolith and squeeze more performance from the system was extracting the logic that pulled data from our users’ connected productivity tools. The extraction was straightforward, except for one challenge: our event bus needed to work across services. We kept using SQLAlchemy’s event system, but replaced our simple observer loop with a proper <a href="https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern">pub/sub</a> implementation using Postgres as a queue.</p>

<p><img src="https://philcalcado.com/img/building-ai-products-i/arch-int-worker.png" alt=""></p>

<p>This change dramatically simplified things—we should have done it from the start. It isolated a whole class of errors to a single service, making debugging easier, and let developers run only the components they were working on.</p>

<p>Encouraged by this success, we took the next logical step: extracting our agents and inference pipelines into their own component.</p>

<p><img src="https://philcalcado.com/img/building-ai-products-i/arch-agents-worker.png" alt=""></p>

<p>This is where my familiar service extraction playbook stopped working. I’ll cover the details of our inference pipelines in the next article, but first, let’s talk about how we distributed our agents.</p>

<h3 id="agents-as-distributed-objects">Agents as Distributed Objects</h3>

<p>As successful as we were with modeling agents as objects, we’d always been wary of distributing them. My ex-colleague <a href="https://martinfowler.com/bliki/FirstLaw.html">Martin Fowler’s First Law of Distributed Objects</a> puts it best: <strong>don’t</strong>.</p>

<p>Still, I think that <a href="https://martinfowler.com/articles/distributed-objects-microservices.html">Martin’s “exception” for microservices</a> applies just as well for agents:</p>

<blockquote>
  <p>[My objection is that] although you can encapsulate many things behind object boundaries, you can’t encapsulate the remote/in-process distinction. An in-process function call is fast and always succeeds […] Remote calls, however, are orders of magnitude slower, and there’s always a chance that the call will fail due to a failure in the remote process or the connection.</p>
</blockquote>

<p>The problem with the distributed objects craze of the 90s was its promise that fine-grained operations—like iterating through a list of <code>user</code> objects and setting <code>is_enabled</code> to false—could work transparently across processes or servers. Microservices and agents avoid this trap by exposing coarse-grained APIs specifically designed for remote calls and error scenarios.</p>

<p>We kept modeling our agents as objects even as we distributed them, just using <a href="https://en.wikipedia.org/wiki/Data_transfer_object">Data Transfer Objects</a> for their APIs instead of domain model objects. This worked well since not everything needs to be an object. Inference pipelines, for instance, are a poor candidate for object orientation and benefit from different abstractions.</p>

<p>At this stage, our system consisted of multiple instances of a few docker images on ECS. Each container exposed FastAPI HTTP endpoints, with some continuously polling our event bus.</p>

<p>This model broke down when we added backpressure and <a href="https://learn.microsoft.com/en-us/dotnet/architecture/cloud-native/application-resiliency-patterns">resilience patterns</a> to our agents. We faced new challenges: what happens when the third of five LLM calls fails during an agent’s decision process? Should we retry everything? Save partial results and retry just the failed call? When do we give up and error out?”</p>

<p>Rather than build a custom orchestrator from scratch, we started exploring existing solutions to this problem.</p>

<p>We first looked at ETL tools like Apache Airflow. While great for data engineering, Airflow’s focus on stateless, scheduled tasks wasn’t a good fit for our agents’ stateful, event-driven operations.</p>

<p>Being in the AWS ecosystem, we looked at Lambda and other serverless options. But while serverless has evolved significantly, it’s still optimized for stateless, short-lived tasks—the opposite of what our agents need.</p>

<p>I’d heard great things about Temporal from my previous teams at DigitalOcean. It’s built for long-running, stateful workflows, offering the durability and resilience we needed out of the box. The multi-language support was a bonus, as we didn’t want to be locked into Python for every component.</p>

<p>After a quick experiment, we were sold. We migrated our agents to run all their computations through Temporal workflows.</p>

<p>Temporal’s core abstractions mapped perfectly to our object-oriented agents. It splits work between side-effect-free workflows and flexible activities. We implemented our agents’ main logic as Workflows, while tool and API interactions—like AI model calls—became Activities. This structure let Temporal’s runtime handle retries, durability, and scalability automatically.</p>

<p>The framework wasn’t perfect though. Temporal’s Python SDK felt like a second-class citizen—even using standard libraries like Pydantic was a challenge, as the framework favors data classes. We had to build quite a few converters and exception wrappers, but ultimately got everything working smoothly.</p>

<p>Temporal Cloud was so affordable we never considered self-hosting. It just works—no complaints. For local development and builds, we use their Docker image, which is equally reliable. We were so impressed that Temporal became core to both our inference pipelines and Outropy’s evolution into a developer platform!</p>

<p>Stay tuned for a deeper dive into Temporal and inference pipelines in the next installment of this series!</p>

  </article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quiver: A Modern Commutative Diagram Editor (276 pts)]]></title>
            <link>https://github.com/varkor/quiver</link>
            <guid>42520151</guid>
            <pubDate>Fri, 27 Dec 2024 05:27:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/varkor/quiver">https://github.com/varkor/quiver</a>, See on <a href="https://news.ycombinator.com/item?id=42520151">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">quiver: a modern commutative diagram editor</h2><a id="user-content-quiver-a-modern-commutative-diagram-editor" aria-label="Permalink: quiver: a modern commutative diagram editor" href="#quiver-a-modern-commutative-diagram-editor"></a></p>
<p dir="auto"><a href="https://q.uiver.app/" rel="nofollow"><img src="https://github.com/varkor/quiver/raw/master/screenshots/title.png" alt="quiver" title="quiver: a modern commutative diagram editor"></a></p>
<p dir="auto"><strong>quiver</strong> is a modern, graphical editor for <a href="https://en.wikipedia.org/wiki/Commutative_diagram" rel="nofollow">commutative</a> and <a href="https://ncatlab.org/nlab/show/pasting+diagram" rel="nofollow">pasting diagrams</a>, capable of
rendering high-quality diagrams for screen viewing, and exporting to LaTeX via <a href="https://github.com/astoff/tikz-cd">tikz-cd</a>.</p>
<p dir="auto">Creating and modifying diagrams with <strong>quiver</strong> is orders of magnitude faster than writing the
equivalent LaTeX by hand and, with a little experience, competes with pen-and-paper.</p>
<p dir="auto">Try <strong>quiver</strong> out: <a href="https://q.uiver.app/" rel="nofollow">q.uiver.app</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features &amp; screenshots</h2><a id="user-content-features--screenshots" aria-label="Permalink: Features &amp; screenshots" href="#features--screenshots"></a></p>
<p dir="auto"><strong>quiver</strong> features an efficient, intuitive interface for creating complex commutative diagrams and
pasting diagrams. It's easy to draw diagrams involving pullbacks and pushouts,</p>
<p dir="auto"><a href="http://q.uiver.app/?q=WzAsNSxbMSwyLCJBIl0sWzIsMSwiQiJdLFsyLDIsIkMiXSxbMSwxLCJBIFxcdGltZXNfQyBCIl0sWzAsMCwiWCJdLFswLDIsImYiLDJdLFsxLDIsImciXSxbMywwLCJcXHBpXzEiLDJdLFszLDEsIlxccGlfMiJdLFs0LDAsImEiLDIseyJjdXJ2ZSI6M31dLFs0LDEsImIiLDAseyJjdXJ2ZSI6LTN9XSxbNCwzLCJcXGxhbmdsZSBhLCBiIFxccmFuZ2xlIiwxLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoiZGFzaGVkIiwibGV2ZWwiOjF9fX1dLFszLDIsIiIsMSx7InN0eWxlIjp7Im5hbWUiOiJjb3JuZXIifX1dXQ==" rel="nofollow"><img src="https://github.com/varkor/quiver/raw/master/screenshots/pullback.png" alt="Pullback" title="Pullback"></a></p>
<p dir="auto">adjunctions,</p>
<p dir="auto"><a href="https://q.uiver.app/?q=WzAsMixbMCwwLCJcXG1hdGhzY3IgQyJdLFsyLDAsIlxcbWF0aHNjciBEIl0sWzAsMSwiRiIsMCx7ImN1cnZlIjotMn1dLFsxLDAsIkciLDAseyJjdXJ2ZSI6LTJ9XSxbMiwzLCIiLDAseyJsZXZlbCI6MSwic3R5bGUiOnsibmFtZSI6ImFkanVuY3Rpb24ifX1dXQ==" rel="nofollow"><img src="https://github.com/varkor/quiver/raw/master/screenshots/adjunction.png" alt="Adjunction" title="Adjunction"></a></p>
<p dir="auto">and higher cells.</p>
<p dir="auto"><a href="https://q.uiver.app/?q=WzAsNCxbMCwwLCJYJyJdLFsxLDAsIlgiXSxbNCwwLCJZIl0sWzUsMCwiWSciXSxbMCwxLCJ4Il0sWzIsMywieSIsMl0sWzEsMiwiIiwwLHsiY3VydmUiOi0yfV0sWzEsMiwiIiwyLHsiY3VydmUiOjJ9XSxbMCwzLCJmJyIsMCx7ImN1cnZlIjotNX1dLFswLDMsImcnIiwyLHsiY3VydmUiOjV9XSxbOCw2LCJcXHZhcnBoaSIsMix7Imxlbmd0aCI6NzB9XSxbNyw5LCJcXGdhbW1hIiwwLHsibGVuZ3RoIjo3MH1dLFs2LDcsIlxcY2hpIiwyLHsib2Zmc2V0Ijo1LCJsZW5ndGgiOjcwfV0sWzYsNywiXFx1cHNpbG9uIiwwLHsib2Zmc2V0IjotNSwibGVuZ3RoIjo3MH1dLFsxMiwxMywiXFxtdSIsMCx7Imxlbmd0aCI6NzB9XV0=" rel="nofollow"><img src="https://github.com/varkor/quiver/raw/master/screenshots/3-cell.png" alt="3-cell" title="3-cell"></a></p>
<p dir="auto">Object placement is based on a flexible grid that resizes according to the size of the labels.</p>
<p dir="auto"><a href="https://q.uiver.app/?q=WzAsMyxbMCwwLCIoQSBcXG90aW1lcyBJKSBcXG90aW1lcyBCIl0sWzIsMCwiQSBcXG90aW1lcyAoSSBcXG90aW1lcyBCKSJdLFsxLDEsIkEgXFxvdGltZXMgQiJdLFswLDIsIlxccmhvX0EgXFxvdGltZXMgQiIsMl0sWzEsMiwiQSBcXG90aW1lcyBcXGxhbWJkYV9CIl0sWzAsMSwiXFxhbHBoYV97QSwgSSwgQn0iXV0=" rel="nofollow"><img src="https://github.com/varkor/quiver/raw/master/screenshots/flexible-grid.png" alt="Flexible grid" title="Flexible grid"></a></p>
<p dir="auto"><a href="https://q.uiver.app/?q=WzAsMTYsWzAsMCwiXFxidWxsZXQiXSxbMSwwLCJcXGJ1bGxldCJdLFswLDEsIlxcYnVsbGV0Il0sWzEsMSwiXFxidWxsZXQiXSxbMCwyLCJcXGJ1bGxldCJdLFsxLDIsIlxcYnVsbGV0Il0sWzAsMywiXFxidWxsZXQiXSxbMSwzLCJcXGJ1bGxldCJdLFszLDAsIlxcYnVsbGV0Il0sWzIsMCwiXFxidWxsZXQiXSxbMywxLCJcXGJ1bGxldCJdLFsyLDEsIlxcYnVsbGV0Il0sWzMsMiwiXFxidWxsZXQiXSxbMiwyLCJcXGJ1bGxldCJdLFszLDMsIlxcYnVsbGV0Il0sWzIsMywiXFxidWxsZXQiXSxbMCwxLCIiLDAseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJtb25vIn0sImJvZHkiOnsibmFtZSI6ImRhc2hlZCIsImxldmVsIjoxfX19XSxbMiwzLCIiLDAseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJtYXBzIHRvIn0sImJvZHkiOnsibmFtZSI6ImRvdHRlZCIsImxldmVsIjoxfSwiaGVhZCI6eyJuYW1lIjoiZXBpIn19fV0sWzQsNSwiIiwwLHsic3R5bGUiOnsidGFpbCI6eyJuYW1lIjoiaG9vayIsInNpZGUiOiJ0b3AifSwiYm9keSI6eyJuYW1lIjoic3F1aWdnbHkiLCJsZXZlbCI6MX0sImhlYWQiOnsibmFtZSI6ImhhcnBvb24iLCJzaWRlIjoidG9wIn19fV0sWzYsNywiIiwwLHsic3R5bGUiOnsidGFpbCI6eyJuYW1lIjoiaG9vayIsInNpZGUiOiJib3R0b20ifSwiYm9keSI6eyJuYW1lIjoiYmFycmVkIiwibGV2ZWwiOjF9LCJoZWFkIjp7Im5hbWUiOiJoYXJwb29uIiwic2lkZSI6ImJvdHRvbSJ9fX1dLFs4LDksImYiLDIseyJvZmZzZXQiOjJ9XSxbOCw5LCJnIiwwLHsib2Zmc2V0IjotMn1dLFsxMCwxMSwiIiwwLHsibGV2ZWwiOjIsInN0eWxlIjp7ImJvZHkiOnsibmFtZSI6InNxdWlnZ2x5IiwibGV2ZWwiOjF9fX1dLFsxMiwxMywiIiwwLHsibGVuZ3RoIjoyMCwic3R5bGUiOnsiYm9keSI6eyJsZXZlbCI6MX19fV0sWzE0LDE1LCIiLDAseyJjdXJ2ZSI6MiwibGV2ZWwiOjMsInN0eWxlIjp7ImJvZHkiOnsibmFtZSI6ImJhcnJlZCIsImxldmVsIjoxfSwiaGVhZCI6eyJuYW1lIjoiZXBpIn19fV1d" rel="nofollow"><img src="https://github.com/varkor/quiver/raw/master/screenshots/styles.png" alt="Arrow styles" title="Arrow styles"></a></p>
<p dir="auto">There is a wide range of composable arrow styles.</p>
<p dir="auto"><a href="https://q.uiver.app/?q=WzAsNSxbMSwxLCJBIFxcdGltZXNfQyBCIixbMCw2MCw2MCwxXV0sWzIsMSwiQiIsWzI0MCw2MCw2MCwxXV0sWzEsMiwiQSIsWzI0MCw2MCw2MCwxXV0sWzIsMiwiQyIsWzI0MCw2MCw2MCwxXV0sWzAsMCwiWCIsWzEyMCw2MCw2MCwxXV0sWzAsMSwiIiwwLHsiY29sb3VyIjpbMCw2MCw2MF19XSxbMCwyLCIiLDIseyJjb2xvdXIiOlswLDYwLDYwXX1dLFsyLDMsImciLDIseyJjb2xvdXIiOlsyNDAsNjAsNjBdfSxbMTgwLDYwLDYwLDFdXSxbMSwzLCJmIiwwLHsiY29sb3VyIjpbMjQwLDYwLDYwXX0sWzE4MCw2MCw2MCwxXV0sWzAsMywiIiwwLHsiY29sb3VyIjpbMCw2MCw2MF0sInN0eWxlIjp7Im5hbWUiOiJjb3JuZXIifX1dLFs0LDIsImEiLDIseyJjdXJ2ZSI6MiwiY29sb3VyIjpbMTIwLDYwLDYwXX0sWzYwLDYwLDYwLDFdXSxbNCwxLCJiIiwwLHsiY3VydmUiOi0yLCJjb2xvdXIiOlsxMjAsNjAsNjBdfSxbNjAsNjAsNjAsMV1dLFs0LDAsIiIsMSx7ImNvbG91ciI6WzMwMCw2MCw2MF0sInN0eWxlIjp7ImJvZHkiOnsibmFtZSI6ImRhc2hlZCJ9fX1dXQ" rel="nofollow"><img src="https://github.com/varkor/quiver/raw/master/screenshots/colour-picker.png" alt="Colours" title="Colours"></a></p>
<p dir="auto">And full use of colour for labels and arrows.</p>
<p dir="auto"><a href="https://q.uiver.app/?q=WzAsOCxbMCwwLCJHeCJdLFsxLDAsIkZ4Il0sWzIsMCwiR3giXSxbMywwLCJGeCJdLFswLDEsIkd5Il0sWzEsMSwiRnkiXSxbMiwxLCJHeSJdLFszLDEsIkZ5Il0sWzAsMSwiXFxiZXRhX3giXSxbMSwyLCJcXGFscGhhX3giXSxbMiwzLCJcXGJldGFfeCJdLFswLDQsIkdmIiwyXSxbNCw1LCJcXGJldGFfeSIsMl0sWzUsNiwiXFxhbHBoYV95IiwyXSxbNiw3LCJcXGJldGFfeSIsMl0sWzMsNywiRmYiXSxbMSw1LCJGZiIsMV0sWzIsNiwiR2YiLDFdLFs0LDEsIlxcYmV0YV9mIiwwLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfSwibGV2ZWwiOjJ9XSxbNSwyLCJcXGFscGhhX2YiLDAseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9LCJsZXZlbCI6Mn1dLFs2LDMsIlxcYmV0YSdmIiwwLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfSwibGV2ZWwiOjJ9XSxbMCwyLCIxIiwwLHsiY3VydmUiOi00fV0sWzUsNywiMSIsMix7ImN1cnZlIjo0fV0sWzEsMjEsIlxcdmFyZXBzaWxvbl94IiwwLHsic2hvcnRlbiI6eyJ0YXJnZXQiOjMwfX1dLFsyMiw2LCJcXGV0YV95IiwyLHsic2hvcnRlbiI6eyJzb3VyY2UiOjMwfX1dXQ==" rel="nofollow"><img src="https://github.com/varkor/quiver/raw/master/screenshots/grid-hidden.png" alt="Screenshot mode" title="Screenshot mode"></a></p>
<p dir="auto"><strong>quiver</strong> is intended to look good for screenshots, as well as to export LaTeX that looks as close
as possible to the original diagram.</p>
<p dir="auto"><a href="https://q.uiver.app/?q=WzAsMixbMCwwLCJBIl0sWzEsMCwiQiJdLFswLDEsImYiXV0=" rel="nofollow"><img src="https://github.com/varkor/quiver/raw/master/screenshots/hints.png" alt="Keyboard hints" title="Keyboard hints"></a></p>
<p dir="auto">Diagrams may be created and modified using either the mouse, by clicking and dragging, or using the keyboard, with a complete set of keyboard shortcuts for performing any action.</p>
<p dir="auto"><a href="https://q.uiver.app/?q=WzAsOCxbMCwwLCJHeCJdLFsxLDAsIkZ4Il0sWzIsMCwiR3giXSxbMywwLCJGeCJdLFswLDEsIkd5Il0sWzEsMSwiRnkiXSxbMiwxLCJHeSJdLFszLDEsIkZ5Il0sWzAsMSwiXFxiZXRhX3giXSxbMSwyLCJcXGFscGhhX3giXSxbMiwzLCJcXGJldGFfeCJdLFswLDQsIkdmIiwyXSxbNCw1LCJcXGJldGFfeSIsMl0sWzUsNiwiXFxhbHBoYV95IiwyXSxbNiw3LCJcXGJldGFfeSIsMl0sWzMsNywiRmYiXSxbMSw1LCJGZiIsMV0sWzIsNiwiR2YiLDFdLFs0LDEsIlxcYmV0YV9mIiwwLHsibGVuZ3RoIjo1MCwibGV2ZWwiOjJ9XSxbNSwyLCJcXGFscGhhX2YiLDAseyJsZW5ndGgiOjUwLCJsZXZlbCI6Mn1dLFs2LDMsIlxcYmV0YSdmIiwwLHsibGVuZ3RoIjo1MCwibGV2ZWwiOjJ9XSxbMCwyLCIxIiwwLHsiY3VydmUiOi00fV0sWzUsNywiMSIsMix7ImN1cnZlIjo0fV0sWzEsMjEsIlxcdmFyZXBzaWxvbl94IiwwLHsibGVuZ3RoIjo1MH1dLFsyMiw2LCJcXGV0YV95IiwyLHsibGVuZ3RoIjo1MH1dXQ==" rel="nofollow"><img src="https://github.com/varkor/quiver/raw/master/screenshots/export.png" alt="Export to LaTeX" title="Export to LaTeX"></a></p>
<p dir="auto">When you export diagrams to LaTeX, <strong>quiver</strong> will embed a link to the diagram, which will allow you
to return to it later if you decide it needs to be modified, or to share it with others.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Other features</h3><a id="user-content-other-features" aria-label="Permalink: Other features" href="#other-features"></a></p>
<ul dir="auto">
<li>Multiple selection, making mass changes easy and fast.</li>
<li>A history system, allowing you to undo/redo actions.</li>
<li>Support for custom macro definitions: simply paste a URL corresponding to the file containing your <code>\newcommand</code>s.</li>
<li>Export embeddable diagrams to HTML.</li>
<li>Panning and zooming, for large diagrams.</li>
<li>Smart label alignment and edge offset.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Importing macros and colours</h2><a id="user-content-importing-macros-and-colours" aria-label="Permalink: Importing macros and colours" href="#importing-macros-and-colours"></a></p>
<p dir="auto">To use custom macros and colours in <strong>quiver</strong>, create a file containing the definitions, like the
following.</p>
<div dir="auto" data-snippet-clipboard-copy-content="\newcommand{\cat}{\mathscr}
\newcommand{\psh}{\widehat}
\newcommand{\smcat}{\mathbb}
\newcommand{\yo}{よ}"><pre><span>\newcommand</span>{<span>\cat</span>}{<span>\mathscr</span>}
<span>\newcommand</span>{<span>\psh</span>}{<span>\widehat</span>}
<span>\newcommand</span>{<span>\smcat</span>}{<span>\mathbb</span>}
<span>\newcommand</span>{<span>\yo</span>}{よ}</pre></div>
<p dir="auto">Upload the file to a publicly accessible URL (for instance,
<a href="https://gist.github.com/">gist.github.com</a>), and paste the URL for the raw text into the "Macros"
input at the bottom of <strong>quiver</strong>.</p>
<p dir="auto">Currently, macros may be defined using <code>\newcommand</code>, <code>\newcommand*</code>, <code>\renewcommand</code>,
<code>\renewcommand*</code>, <code>\DeclareMathOperator</code>, and <code>\DeclareMathOperator*</code>; and colours may be defined
using <code>\definecolor</code> (using the colour modes: <code>rgb</code>, <code>RGB</code>, <code>HTML</code>, <code>gray</code>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Editor integration</h2><a id="user-content-editor-integration" aria-label="Permalink: Editor integration" href="#editor-integration"></a></p>
<p dir="auto">See <a href="https://github.com/varkor/quiver/wiki/Editor-integration">Editor integration</a> on the <a href="https://github.com/varkor/quiver/wiki">quiver
wiki</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">Run <code>make</code> from the command line, and then open <code>src/index.html</code> in your favourite web browser.</p>
<p dir="auto">If this fails, you might be using an incompatible version of Make or Bash. In this case, you can
manually download the <a href="https://github.com/KaTeX/KaTeX/releases">latest release</a> of KaTeX and place
it under <code>src/</code> as <code>src/KaTeX/</code>. If KaTeX has not been given the correct path, you will get an
error telling you that KaTeX failed to load.</p>
<p dir="auto"><strong>quiver</strong> must be run through <code>localhost</code>. If you have Python installed, an easy solution is to
run:</p>

<p dir="auto">in the <strong>quiver</strong> <code>src</code> directory and then open <code>localhost:8000</code> in browser.</p>
<p dir="auto">If you have any other problems building <strong>quiver</strong>, <a href="https://github.com/varkor/quiver/issues/new">open an
issue</a> detailing the problem and I'll try to help.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Thanks to</h2><a id="user-content-thanks-to" aria-label="Permalink: Thanks to" href="#thanks-to"></a></p>
<ul dir="auto">
<li><a href="https://www.cl.cam.ac.uk/~scs62/" rel="nofollow">S. C. Steenkamp</a>, for helpful discussions regarding the
aesthetic rendering of arrows.</li>
<li><a href="https://tex.stackexchange.com/users/138900/andr%c3%a9c" rel="nofollow">AndréC</a>, for the custom TikZ style for
curves of a fixed height.</li>
<li><a href="https://github.com/doctorn">Nathan Corbyn</a>, for adding the ability to export embeddable diagrams
to HTML.</li>
<li><a href="https://github.com/paolobrasolin">Paolo Brasolin</a>, for adding offline support.</li>
<li><a href="https://github.com/davidson16807">Carl Davidson</a>, for discussing and prototyping loop rendering.</li>
<li>Everyone who has improved <strong>quiver</strong> by reporting issues or suggesting improvements.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bill requiring US agencies to share source code with each other becomes law (418 pts)]]></title>
            <link>https://fedscoop.com/agencies-must-share-custom-source-code-under-new-share-it-act/</link>
            <guid>42518833</guid>
            <pubDate>Thu, 26 Dec 2024 23:57:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fedscoop.com/agencies-must-share-custom-source-code-under-new-share-it-act/">https://fedscoop.com/agencies-must-share-custom-source-code-under-new-share-it-act/</a>, See on <a href="https://news.ycombinator.com/item?id=42518833">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

<p>Agencies will have to share custom-developed code amongst each other in an effort to prevent duplicative software development contracts under a new bill <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2024/12/24/press-release-bill-signed-hr-663/">signed</a> into law by President Joe Biden.</p>



<p>The bipartisan Source Code Harmonization And Reuse in Information Technology (<a href="https://www.congress.gov/bill/118th-congress/house-bill/9566">H.R. 9566</a>), or SHARE IT Act, takes aim at reducing the roughly $12 billion that lawmakers estimated the federal government spends each year on software purchases by requiring agencies to publicly list custom code and share that code with other agencies.</p>



<p>Doing so, bill sponsors said, will address the inefficiency that can happen when agencies unknowingly hire contractors to develop code that has already been developed for another agency. The new law doesn’t apply to classified code, national security systems or code that would post privacy risks if shared.</p>



<p>The legislation was sponsored by Sens. Ted Cruz, R-Texas, and Gary Peters, D-Mich., in the Senate and Reps. Nicholas Langworthy, R-N.Y., and William Timmons, R-S.C., in the House. Both chambers approved the bill with overwhelming support in <a href="https://www.congress.gov/bill/118th-congress/house-bill/9566/all-actions">December</a>, without recorded up or down votes.</p>



<p>Under the law, agency chief information officers are required to develop policies within 180 days of enactment that implement the act. Those policies need to ensure that custom-developed code aligns with best practices, establish a process for making the metadata for custom code publicly available, and outline a standardized reporting process.&nbsp;</p>



<p>Per the new law, metadata includes information about whether custom code was developed under a contract or shared in a repository, the contract number, and a hyperlink to the repository where the code was shared.</p>



<p>The legislation also had industry support. According to an announcement from Langworthy on the bill’s House introduction in September, collaborative software companies Atlassian and GitLab Inc. backed the legislation.</p>



<p>At the time, Stan Shepard, Atlassian’s general counsel, said in a statement in the release that the company shares “the belief that greater collaboration and sharing of custom code will promote openness, efficiency, and innovation across the federal enterprise.”&nbsp;</p>


							
						</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The CAP theorem of Clustering: Why Every Algorithm Must Sacrifice Something (188 pts)]]></title>
            <link>https://blog.codingconfessions.com/p/the-cap-theorem-of-clustering</link>
            <guid>42518562</guid>
            <pubDate>Thu, 26 Dec 2024 23:04:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.codingconfessions.com/p/the-cap-theorem-of-clustering">https://blog.codingconfessions.com/p/the-cap-theorem-of-clustering</a>, See on <a href="https://news.ycombinator.com/item?id=42518562">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>As software engineers, we use clustering algorithms all the time. Whether it's grouping similar users, categorizing content, or detecting patterns in data, clustering seems deceptively simple: just group similar things together, right? You might have used k-means, DBSCAN, or agglomerative clustering, thinking you just need to pick the right algorithm for your use case.</p><p><span>But here's what most tutorials won't tell you: every clustering algorithm you choose is fundamentally flawed. Not because of poor implementation or wrong parameters, but because of the math itself. In 2002, </span><a href="https://www.cs.cornell.edu/home/kleinber/" rel="">Jon Kleinberg</a><span> (in a </span><a href="https://www.cs.cornell.edu/home/kleinber/nips15.pdf" rel="">paper</a><span> published at NIPS 2002) proved something that should make every developer pause: it's impossible for any clustering algorithm to have all three properties we'd naturally want it to have.</span></p><p><span>It can be thought of as the </span><a href="https://en.wikipedia.org/wiki/CAP_theorem" rel="">CAP theorem</a><span> of clustering. Just as distributed systems force you to choose between consistency, availability, and partition tolerance, Kleinberg showed that clustering algorithms force you to pick between scale invariance, richness, and consistency. You can't have all three – ever, it’s a mathematical impossibility. </span></p><p>Before you deploy your next clustering solution in production, you need to understand what you're giving up. Let's dive into these three properties and see why you'll always have to choose what to sacrifice.</p><p>Before we talk about the theorem, we need a precise definition of clustering. The paper defines it in terms of the set of data points and the distance function as defined below:</p><p><span>We refer to the data being clustered as the set </span><code>S</code><span> of n points.</span></p><p>In order to perform the clustering, the clustering model needs to compute pairwise distance between each data point and for that it needs a distance function.</p><p><span>The distance function is a mathematical function which takes two data points </span><code>i</code><span> and </span><code>j</code><span> as parameters, and computes the distance between them. If the parameters </span><code>i</code><span> and </span><code>j</code><span> are the same data points then the distance between them as computed by this function should be 0. </span></p><p><span>Finally, the paper defines clustering as a function of the distance function </span><code>d</code><span>, and the set of data points </span><code>S</code><span> such that it partitions </span><code>S</code><span> into smaller subsets where each subset represent a cluster. Mathematically speaking:</span></p><blockquote><p><em>In terms of the distance function d, the clustering function can be defined as a function ƒ that takes a distance function d on S and returns a partition Γ of S.</em></p></blockquote><p><span>For instance, the k-means algorithm takes the number of clusters k, a distance function (such as the </span><a href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="">Euclidean distance function</a><span>), and the set of data points as input, and results in k clusters as its output. These k clusters are essentially a partition of the original dataset </span><code>S</code><span>.</span></p><p>We want our clustering algorithm to exhibit three desirable properties, which are termed as scale-invariance, richness, and consistency. Let’s understand what these properties mean.</p><p>Scale invariance is a property that sounds almost too obvious: if you take your data points and scale all distances between them by the same factor, your clusters shouldn't change.</p><p>For instance, if for any two points i,j in the dataset the distance between them is d(i,j). Then, if we scale this distance by a factor 𝛼 such that it becomes 𝛼.d(i,j) then the clustering result should remain unchanged. This means that the clustering algorithm is invariant to the scale.</p><p>In the real world, this matters more than you might think. Have you ever:</p><ul><li><p>Changed your measurement units (like feet to meters)</p></li><li><p>Normalized your data differently</p></li><li><p>Applied a different scaling to your features only to find your clustering results completely changed? That's what happens when your algorithm isn't scale invariant.</p></li></ul><p>Richness is about possibilities — a clustering algorithm should be capable of producing any grouping that might make sense for your data.</p><p>Imagine you're sorting your wardrobe. Sometimes you might want to group clothes by color (creating many clusters), other times by season (four clusters), or simply into 'wear now' and 'store away' (two clusters). A truly rich clustering algorithm should be able to handle all these possibilities, not force you into a predetermined number of groups.</p><p>But many popular algorithms fail this requirement. Take k-means, for instance. The moment you specify k=3, you've already ruled out any possibility of finding two clusters or four clusters, even if that's what your data naturally suggests. It's like forcing your wardrobe into exactly three groups, even if that doesn't make sense for your clothes.</p><p><span>Mathematically speaking: if </span><code>f</code><span> is our clustering function and </span><code>S</code><span> is our dataset, richness means that </span><code>f</code><span> should be able to produce any possible partitioning of </span><code>S</code><span>. In other words, </span><code>range(f)</code><span> should equal the set of all possible ways to partition </span><code>S</code><span>.</span></p><p>While this flexibility sounds great in theory, you can probably see why many practical algorithms sacrifice it. When you're analyzing real data, you often want to control the number of clusters to make the results interpretable</p><p>The third property, consistency, means that if your existing clusters are good, making similar points more similar and different points more different shouldn't change these clusters.</p><p>Let's break this down with a simple example. Imagine you've clustered movies into genres based on their characteristics. Now:</p><ul><li><p>Two action movies add even more explosive scenes, making them more similar to each other</p></li><li><p>Meanwhile, a romance movie adds more romantic scenes, making it even more different from the action movies</p></li></ul><p>Consistency means that if these changes only reinforce the existing grouping, your clustering algorithm shouldn't suddenly decide to reorganize the groups.</p><p><span>Mathematically speaking: if </span><code>Γ</code><span> is a clustering of points using distance function </span><code>d</code><span>, and we create a new distance function </span><code>d'</code><span> where:</span></p><ul><li><p><span>For any two points </span><code>i</code><span>,</span><code>j</code><span> in the same cluster: </span><code>d'(i,j) &lt; d(i,j)</code><span> (similar things become more similar)</span></p></li><li><p><span>For any two points </span><code>i</code><span>,</span><code>j</code><span> in different clusters: </span><code>d'(i,j) &gt; d(i,j)</code><span> (different things become more different)</span></p></li></ul><p><span>Then a consistent clustering algorithm should produce the same clustering </span><code>Γ</code><span> with </span><code>d'</code><span> as it did with </span><code>d</code><span>. This new distance function </span><code>d'</code><span> is called a </span><code>Γ</code><span> transformation of </span><code>d</code><span>.</span></p><p><strong>There is no clustering function which satisfies all three properties: </strong><em><strong>scale-invariance</strong></em><strong>, </strong><em><strong>richness</strong></em><strong> and </strong><em><strong>consistency</strong></em><strong>.</strong></p><p>While Kleinberg proved this mathematically (check out his paper for the full proof), let's see how this 'pick two out of three' limitation shows up in algorithms you might be using today.</p><p>Single linkage is a form of hierarchical clustering. It starts simple: every point is its own cluster, and we gradually merge the closest clusters. The interesting part is deciding how to stop the algorithm. One of the three common criterion are used to stop the algorithm and each one has its trade off rooted in the impossibility theorem.</p><ul><li><p>What we do: Stop clustering after we have k clusters</p></li><li><p>What we sacrifice: Richness</p></li><li><p>Why? We've locked ourselves into exactly k groups. Our algorithm will never discover any other groupings of size smaller or larger than k.</p></li></ul><ul><li><p>What we do: We keep merging clusters as long as their distance &lt;= some distance r. When all clusters are at a distance larger than r, the algorithm automatically stops.</p></li><li><p>What we sacrifice: Scale-invariance</p></li><li><p>Why? If we scale up our data by 2x (or some other factor), then clusters which were previously mergeable are suddenly too far apart and will not be merged. This changes the clustering output.</p></li></ul><ul><li><p><span>What we do: We calculate the maximum pairwise distance </span><code>ρ</code><span> within our dataset using some distance function </span><code>d1</code><span>. After that we only merge two clusters if their distance is &lt;= </span><code>αρ</code><span>, where </span><code>α</code><span> &lt; 1.</span></p></li><li><p>What we sacrifice: Consistency</p></li><li><p><span>Why?  Let’s say we change our distance function from </span><code>d1</code><span> to </span><code>d2</code><span>, such that d2 makes similar points more similar, and dissimilar points more dissimilar. More formally, </span><code>d2</code><span> is a </span><code>Γ</code><span> transformation of </span><code>d1</code><span>. Then by definition, the maximum pairwise distance obtained using </span><code>d2</code><span> will be larger than </span><code>ρ</code><span>, and as a result the clustering output obtained using </span><code>d2</code><span> will also be very different than the original clustering.</span></p></li></ul><p><span>Centroid based clustering refers to the commonly used </span><em>k-means</em><span> and </span><em>k-median</em><span> algorithms. Where we start with a predefined </span><em>k</em><span> number of clusters by selecting </span><em>k</em><span> points in the data as centroids and then assigning each point to their nearest cluster. The algorithm iteratively optimizes the centroids of the k clusters by computing the mean (or the median) of each cluster, and then redistributing the points based on their nearest cluster centroid. The algorithm normally stops when the clusters become stable.</span></p><p><span>These algorithms suffer with the problem of not satisfying the </span><em>richness</em><span> property because as soon as we fix the number of clusters k, automatically it eliminates the possibility of achieving all the possible clusterings as the output.</span></p><p><span>The paper proves that these algorithms don't satisfy the </span><em>consistency</em><span> property as well. For instance, for k=2, k-means may come up with clusters X and Y. However, if we decrease the distance between the points within X, and Y, while increasing the distance between the points in X and Y, then the algorithm might come up with two completely different clusters as its output. The paper has a formal proof for this specific example which generalizes to k &gt; 2 as well.</span></p><p>Now you know why clustering algorithms force you to make sacrifices. It's not a flaw in implementation or a limitation we'll eventually overcome – it's mathematically impossible to have it all. Every clustering algorithm must give up either scale-invariance, richness, or consistency. There's no escape from this fundamental trade-off.</p><p>But once you understand what you're giving up, you can make this limitation work for you. Just like engineers choose between consistency and availability in distributed systems, you can strategically choose which clustering property to sacrifice:</p><ul><li><p>Need your algorithm to handle data regardless of scale? You might have to give up richness.</p></li><li><p>Want the flexibility to discover any possible grouping? Be prepared to lose scale-invariance.</p></li><li><p>Need results to stay stable when cluster patterns become more pronounced? You'll probably sacrifice richness.</p></li></ul><p>Instead of fighting these limitations, use them as a guide. Ask yourself:</p><ul><li><p>What property matters most for your specific use case?</p></li><li><p>Which trade-off can your application tolerate?</p></li><li><p>How can you design your system knowing these inherent limitations?</p></li></ul><p>Understanding Kleinberg's theorem doesn't just make you a better theorist – it makes you a more effective engineer. Because in the real world, success isn't about finding the perfect clustering algorithm (it doesn't exist). It's about choosing the right sacrifices for your specific needs.</p><p>If you find my work interesting and valuable, you can support me by opting for a paid subscription (it’s $6 monthly/$60 annual). As a bonus you get access to monthly live sessions, and all the past recordings. </p><p><span>Many people report failed payments, or don’t want a recurring subscription. For that I also have a </span><a href="https://buymeacoffee.com/codeconfessions" rel="">buymeacoffee page</a><span>. Where you can buy me coffees or become a member. I will upgrade you to a paid subscription for the equivalent duration here.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://buymeacoffee.com/codeconfessions&quot;,&quot;text&quot;:&quot;Buy me a coffee&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://buymeacoffee.com/codeconfessions" rel=""><span>Buy me a coffee</span></a></p><p>I also have a GitHub Sponsor page. You will get a sponsorship badge, and also a complementary paid subscription here.</p><p data-attrs="{&quot;url&quot;:&quot;https://github.com/sponsors/abhinav-upadhyay&quot;,&quot;text&quot;:&quot;Sponsor me on GitHub&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://github.com/sponsors/abhinav-upadhyay" rel=""><span>Sponsor me on GitHub</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse Engineering the Stream Deck+ (157 pts)]]></title>
            <link>https://den.dev/blog/reverse-engineer-stream-deck-plus/</link>
            <guid>42518444</guid>
            <pubDate>Thu, 26 Dec 2024 22:46:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://den.dev/blog/reverse-engineer-stream-deck-plus/">https://den.dev/blog/reverse-engineer-stream-deck-plus/</a>, See on <a href="https://news.ycombinator.com/item?id=42518444">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <header>
      
        <ol>
  
  
    
  
    
  
  <li>
    <a href="https://den.dev/">Den Delimarsky 🔐</a><span>/</span>
  </li>

  
  <li>
    <a href="https://den.dev/blog/">Writing</a><span>/</span>
  </li>

  
  <li>
    <a href="https://den.dev/blog/reverse-engineer-stream-deck-plus/">Reverse Engineering The Stream Deck Plus</a><span>/</span>
  </li>

</ol>


      
      
      <div>
        





  
  



  

  
  
    
  

  

  
    
  

  

  

  <p><time datetime="2024-12-26 00:00:00 +0000 UTC">December 26, 2024</time><span>·</span><span>6495 words</span>
    

    
    
  </p>

  
  
    
  


      </div>
      
    </header>
    <div>
        <p>Close to 4 years ago I <a href="https://den.dev/blog/reverse-engineering-stream-deck/">talked about reverse engineering the Stream Deck</a> to gain full control of the device and remove the dependence on the Stream Deck software. Well, I still really enjoy the hardware, but the software has gotten worse - it now goes as far as to <em>requiring users for an account</em> to download extensions.</p>

  
  
  
  
  
  <figure>
    
    <img alt="You now need to create and log in with an account to install Stream Deck software extensions." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/software-marketplace.webp">
    
    <figcaption>You now need to create and log in with an account to install Stream Deck software extensions.</figcaption>
  </figure>


<p>If we’ve interacted in the past, I am big on respecting customer privacy and choices, and that means - if I want to use a device without an account, I better be damn able to do that. Luckily, building on my past work with <a href="https://deck.surf/" target="_blank" rel="noreferrer noopener">DeckSurf</a>, I finally was determined to push the pedal to the metal and make my project a viable alternative to proprietary software for this extremely versatile and flexible button box.</p>
<p>This post is going to be looking at the workings of the <a href="https://www.elgato.com/us/en/p/stream-deck-plus-black" target="_blank" rel="noreferrer noopener">Stream Deck Plus</a>, a $179.99 (discounted by $20 at the time of this writing) and how you, dear reader, can use it even if you don’t want to install Elgato’s own software.</p>
<h2 id="what-are-we-working-with">What are we working with <span><a href="#what-are-we-working-with" aria-label="Anchor">#</a></span></h2>
<p>Here is the device we’ll be talking about:</p>

  
  
  
  
  
  <figure>
    
    <img alt="A Stream Deck Plus on a desk." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/sdp.webp">
    
    <figcaption>A Stream Deck Plus on a desk.</figcaption>
  </figure>


<p>Stream Deck Plus sports a few capabilities that I should outline before I go more in-depth about the actual reverse-engineering process:</p>
<ol>
<li><strong>8 buttons</strong>. This is effectively the same as with all the other Stream Deck products. Numbers vary, but the behavior is consistent.</li>
<li><strong>A narrow screen</strong>. Right below the buttons is a narrow color screen band that can be used to provide auxiliary contextual information.</li>
<li><strong>4 dials</strong>. Each dial can turn right or left an unlimited number of times. They can also be pressed down (click).</li>
</ol>
<p>We’re going to dive into each of the features and how they work. I will also mention that to actually reverse-engineer this device, I used the following tooling:</p>
<ul>
<li><a href="https://www.wireshark.org/" target="_blank" rel="noreferrer noopener">Wireshark</a> with <a href="https://desowin.org/usbpcap/" target="_blank" rel="noreferrer noopener">USBPcap</a>.</li>
<li>A virtual machine where Stream Deck Plus is the only USB device connected.</li>
<li><a href="https://help.elgato.com/hc/en-us/sections/5162671529357-Elgato-Stream-Deck-Software-Release-Notes" target="_blank" rel="noreferrer noopener">Stream Deck software</a>, to inspect the traffic it sends over the wire.</li>
</ul>
<p>The way Stream Deck is built, it’s a generic HID device and it does not require you to have Stream Deck software installed to function. That is, once I reverse-engineer the protocol for the hardware, I can build my own client software that does whatever I want and doesn’t depend in any capacity on Elgato’s software stack.</p>
<h2 id="setting-up-the-inspection-process">Setting up the inspection process <span><a href="#setting-up-the-inspection-process" aria-label="Anchor">#</a></span></h2>
<p>To get started, let’s launch Wireshark and select a USB capture interface.</p>

  
  
  
  
  
  <figure>
    
    <img alt="Selecting a USB interface in Wireshark." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/wireshark-capture.webp">
    
    <figcaption>Selecting a USB interface in Wireshark.</figcaption>
  </figure>


<p>Depending on your machine configuration, you may have more than one interface available. You might need to try opening a few until you’re able to spot the connected Stream Deck.</p>
<p>To <em>find</em> the Stream Deck Plus device in my case, I can filter by the product ID (PID), that is <code>0x0084</code> (the Elgato VID, shall you need it, is <code>0x0FD9</code>). Narrowing down the traffic to just the connected Stream Deck Plus device can be done by applying the following filter string:</p>
<p>And just like that, the Stream Deck lights up in the list (in the likely <em>sea</em> of other USB traffic):</p>

  
  
  
  
  
  <figure>
    
    <img alt="Spotting the Stream Deck Plus in Wireshark." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/stream-deck-plus-wireshark.webp">
    
    <figcaption>Spotting the Stream Deck Plus in Wireshark.</figcaption>
  </figure>


<p>Based on the above, the source I need to look for is <code>3.5.0</code>. That value is also the <em>destination</em> - the “address” of the device that we can use to inspect outbound traffic that actually sets things like brightness or images on the different surfaces available on the Stream Deck.</p>
<p>With this data at hand, I can now set up the filter string like this:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>usb.dst matches <span>"3\\.5\\..*"</span>
</span></span></code></pre></div><div>
  <p><img src="https://assets.den.dev/images/shared/thinking.gif" alt="Mole out of the ground, thinking.">
  </p>
  <p>Hold on a second... You said that the address is <code>3.5.0</code> above, but in your second filter string it looks like you're matching to <i>everything</i> that follows the <code>3.5.</code> pattern. Why is that?</p>
</div>
<p>Aha! Keen eye. Indeed, I am not filtering just by <code>3.5.0</code>. The USB address is made of three components - the <strong>bus</strong>, <strong>device</strong>, and <strong>endpoint</strong>. In our case, the Stream Deck Plus is operating as <strong>bus 3</strong>, <strong>device 5</strong>, and <strong>endpoint 0</strong>, but what we also need to know is that a single device can have multiple endpoints. So for us to properly look at <em>all</em> Stream Deck Plus device traffic, we exclude the endpoint identifier.</p>
<p>We can also simplify the filter string like this:</p>
<p>This is a bit cleaner and you don’t need to worry about RegEx-ing a relatively constraint.</p>
<p>With these basics out of the way, let’s take a look at how the actual hardware interacts with my computer, and vice-versa.</p>
<h2 id="stream-deck-hardware">Stream Deck hardware <span><a href="#stream-deck-hardware" aria-label="Anchor">#</a></span></h2>
<h3 id="the-buttons">The buttons <span><a href="#the-buttons" aria-label="Anchor">#</a></span></h3>
<p>The behavior for the buttons is the same as I’ve <a href="https://den.dev/blog/reverse-engineering-stream-deck/">outlined with the Stream Deck XL</a>. I really appreciate the consistency here, and I guess from a supportability perspective this makes sense - once you have an API more or less working, why change how things act from a new hardware release to another? Kudos to Elgato on that.</p>
<p>Every button supports a 120x120 color image. The content is not dynamically updated by the device itself but rather by the host - that is, whatever computer you’re connecting it to. If there is updated status displayed on the button, that’s only because the computer is pushing new images to the Stream Deck constantly. On Windows and macOS, that responsibility <em>typically</em> falls on the Stream Deck software (that I am aiming to fully replace with DeckSurf).</p>
<h4 id="setting-images">Setting images <span><a href="#setting-images" aria-label="Anchor">#</a></span></h4>
<p>When images are set on the computer, a JPEG-encoded and usually compressed (if you use a larger resolution) image is being sent over the wire, along with some other <a href="https://den.dev/blog/reverse-engineering-stream-deck/#-decoding-the-packets">generic packet metadata</a>.</p>
<p>The packets that set the image can be recognized by looking for the following pattern in the header (values are hexadecimal):</p>
<pre tabindex="0"><code data-lang="binary">+-------+----+----+----+----+----+----+----+----+
| Byte  |  0 |  1 |  2 |  3 |  4 |  5 |  6 |  7 |
+-------+----+----+----+----+----+----+----+----+
| Value | 02 | 07 | 18 | 00 | F8 | 03 | 00 | 00 |
+-------+----+----+----+----+----+----+----+----+
</code></pre><p>The header can be described like this:</p>
<table>
<thead>
<tr>
<th>Byte Index</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>0</code></td>
<td>Always <code>02</code></td>
</tr>
<tr>
<td><code>1</code></td>
<td>Always <code>07</code></td>
</tr>
<tr>
<td><code>2</code></td>
<td>Hexadecimal ID of the button for which the image is set. This value is zero-indexed.</td>
</tr>
<tr>
<td><code>3</code></td>
<td>Determines whether the current packet is the <em>final</em> packet that sets an image. Larger images are broken down into multiple packets, and this value can be either <code>00</code> or <code>01</code>.</td>
</tr>
<tr>
<td><code>4</code> and <code>5</code></td>
<td>16-bit Little Endian representation of the image payload length in the <em>current packet</em>.</td>
</tr>
<tr>
<td><code>6</code> and <code>7</code></td>
<td>16-bit Little Endian representation of the zero-based iteration (or, page) for cases where the image is split in multiple packets.</td>
</tr>
</tbody>
</table>
<p>Everything that follows the header is the <em>image payload</em>. If the image is split, we will see several packets, like this:</p>

  
  
  
  
  
  <figure>
    
    <img alt="Setting an image in Stream Deck software and inspecting the packets in Wireshark." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/set-image-stream-deck.webp">
    
    <figcaption>Setting an image in Stream Deck software and inspecting the packets in Wireshark.</figcaption>
  </figure>


<p>Those <code>URB_INTERRUPT out</code> packets is what we’re after. Don’t worry if you haven’t learned about this terminology yet. <code>URB</code> stands for “USB Request Block” and is used as a structure to describe a USB transfer between the host and the device. <code>INTERRUPT</code> refers to the transfer type. USB interrupt transfers are designed for devices that require low-latency communication, typically for small amounts of data. These devices can be keyboards, mice, or, in our case, a Stream Deck Plus. Basically, anything that sends or receives frequent updates. Lastly, <code>out</code> indicates the direction of the transfer - <em>out</em> of the host and to the device. <code>URB_INTERRUPT out</code> means that the host (my computer) is sending data to a device using an interrupt transfer.</p>
<p>Now, notice that the packets being sent are uniform - they are <strong>1,051 bytes</strong> long. The first packet contains the JPEG header (starting bytes of the image), and every subsequent packet contains the rest of the image, split in chunks. The header is always <strong>8 bytes</strong> and the content (image payload) is declared in the header, but is usually <strong>1,016 bytes</strong>, making the total payload <strong>1,024 bytes</strong> long.</p>
<p>Do always check the header for the real length, though - never rely on these kind of assumptions as the de-facto truth, as things may change in the future.</p>
<p>To verify what image is being set, we can use a bit of command line magic. In Wireshark, select the <code>URB_INTERRUPT out</code> packets that contain the image. To make it easier to spot them, you can apply a more restrictive filter:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>usb.dst ~ <span>"3.5"</span> <span>&amp;&amp;</span> _ws.col.info <span>==</span> <span>"URB_INTERRUPT out"</span>
</span></span></code></pre></div><p>This will look for <code>URB_INTERRUPT out</code> packets to the specific destination only. As I mentioned, select the packets, and then from the <strong>File</strong> menu select <strong>Export Specified Packets…</strong>.</p>

  
  
  
  
  
  <figure>
    
    <img alt="Exporting a subset of packets in Wireshark." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/export-packets.webp">
    
    <figcaption>Exporting a subset of packets in Wireshark.</figcaption>
  </figure>


<p>In the following dialog, in the <strong>Packet Range</strong> section, click on <strong>Selected packets only</strong>.</p>

  
  
  
  
  
  <figure>
    
    <img alt="Dialog prompting to export a subset of packets in Wireshark." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/image-extraction-screen.webp">
    
    <figcaption>Dialog prompting to export a subset of packets in Wireshark.</figcaption>
  </figure>


<p>Give the file a descriptive name and store it somewhere on disk. Next, we will use a command-line tool that comes with Wireshark, called <a href="https://www.wireshark.org/docs/man-pages/tshark.html" target="_blank" rel="noreferrer noopener"><code>tshark</code></a>.</p>
<p>On Windows, <code>tshark</code> is <em>typically</em> located in the Wireshark installation folder. In my case, it was in <code>C:\Program Files\Wireshark</code>:</p>

  
  
  
  
  
  <figure>
    
    <img alt="Location of the TShark tool." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/tshark-location.webp">
    
    <figcaption>Location of the TShark tool.</figcaption>
  </figure>


<p>For easier consumption of <code>tshark</code>, you can add the Wireshark path to your system <code>PATH</code> <a href="https://superuser.com/a/284351/421794" target="_blank" rel="noreferrer noopener">environment variable</a>. Assuming that is done, we can now invoke this from the terminal:</p>
<div><pre tabindex="0"><code data-lang="powershell"><span><span><span>tshark</span> <span>-r</span> <span>.\</span><span>test-image</span><span>-extraction</span><span>.</span><span>pcapng</span> <span>-T</span> <span>fields</span> <span>-e</span> <span>usb</span><span>.</span><span>capdata</span> <span>&gt;</span> <span>data</span><span>.</span><span>txt</span>
</span></span></code></pre></div><p>What this command does is extract the HID data and dump it all in a text file. Because we’re already operating on a <code>*.pcapng</code> file that <em>only</em> contains the image packets we’re interested in, we don’t need to fiddle more with filtering, and just put everything in a text file.</p>
<p>The content will look like this:</p>

  
  
  
  
  
  <figure>
    
    <img alt="Text representation of the HID data dump." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/pcapng-dump-txt.webp">
    
    <figcaption>Text representation of the HID data dump.</figcaption>
  </figure>


<p>Not super helpful, but we spot the things I mentioned earlier - the <code>02 07</code> header starter, for example. As a quick and dirty “hack” to dump image data from this text file, I have a PowerShell script:</p>
<div><pre tabindex="0"><code data-lang="powershell"><span><span><span>param</span> <span>(</span>
</span></span><span><span>    <span>[</span><span>string</span><span>]</span><span>$DataFile</span><span>,</span>
</span></span><span><span>    <span>[</span><span>string</span><span>]</span><span>$OutputFileName</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>function</span><span> </span><span>Process-HIDData</span> <span>{</span>
</span></span><span><span>    <span>param</span> <span>(</span>
</span></span><span><span>        <span>[</span><span>string</span><span>]</span><span>$DataFile</span><span>,</span>
</span></span><span><span>        <span>[</span><span>string</span><span>]</span><span>$OutputFileName</span>
</span></span><span><span>    <span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>$lines</span> <span>=</span> <span>Get-Content</span> <span>-Path</span> <span>$DataFile</span>
</span></span><span><span>
</span></span><span><span>    <span>$imageBytes</span> <span>=</span> <span>@</span><span>()</span>
</span></span><span><span>
</span></span><span><span>    <span>foreach</span> <span>(</span><span>$line</span> <span>in</span> <span>$lines</span><span>)</span> <span>{</span>
</span></span><span><span>        <span>$hexBytes</span> <span>=</span> <span>$line</span><span>.</span><span>Trim</span><span>()</span>
</span></span><span><span>
</span></span><span><span>        <span>if</span> <span>(</span><span>$hexBytes</span><span>.</span><span>Length</span> <span>-gt</span> <span>16</span><span>)</span> <span>{</span>
</span></span><span><span>            <span>$processedBytes</span> <span>=</span> <span>$hexBytes</span><span>.</span><span>Substring</span><span>(</span><span>16</span><span>)</span>
</span></span><span><span>            <span>$byteArray</span> <span>=</span> <span>for</span> <span>(</span><span>$i</span> <span>=</span> <span>0</span><span>;</span> <span>$i</span> <span>-lt</span> <span>$processedBytes</span><span>.</span><span>Length</span><span>;</span> <span>$i</span> <span>+=</span> <span>2</span><span>)</span> <span>{</span>
</span></span><span><span>                <span>[</span><span>Convert</span><span>]::</span><span>ToByte</span><span>(</span><span>$processedBytes</span><span>.</span><span>Substring</span><span>(</span><span>$i</span><span>,</span> <span>2</span><span>),</span> <span>16</span><span>)</span>
</span></span><span><span>            <span>}</span>
</span></span><span><span>            <span>$imageBytes</span> <span>+=</span> <span>$byteArray</span>
</span></span><span><span>        <span>}</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>
</span></span><span><span>    <span>$binaryData</span> <span>=</span> <span>[</span><span>byte[]</span><span>]::</span><span>new</span><span>(</span><span>$imageBytes</span><span>.</span><span>Length</span><span>)</span>
</span></span><span><span>    <span>[</span><span>System.Array</span><span>]::</span><span>Copy</span><span>(</span><span>$imageBytes</span><span>,</span> <span>$binaryData</span><span>,</span> <span>$imageBytes</span><span>.</span><span>Length</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>$scriptDirectory</span> <span>=</span> <span>$PSScriptRoot</span>
</span></span><span><span>    <span>$outputFilePath</span> <span>=</span> <span>Join-Path</span> <span>-Path</span> <span>$scriptDirectory</span> <span>-ChildPath</span> <span>$OutputFileName</span>
</span></span><span><span>
</span></span><span><span>    <span>[</span><span>System.IO.File</span><span>]::</span><span>WriteAllBytes</span><span>(</span><span>$outputFilePath</span><span>,</span> <span>$binaryData</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>Write-Output</span> <span>"Image saved as </span><span>$outputFilePath</span><span>"</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>if</span> <span>(</span><span>-not</span> <span>$DataFile</span><span>)</span> <span>{</span>
</span></span><span><span>    <span>Write-Error</span> <span>"Data file path is required."</span>
</span></span><span><span>    <span>exit</span> <span>1</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>if</span> <span>(</span><span>-not</span> <span>$OutputFileName</span><span>)</span> <span>{</span>
</span></span><span><span>    <span>Write-Error</span> <span>"Output file name is required."</span>
</span></span><span><span>    <span>exit</span> <span>1</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>Process-HIDData</span> <span>-DataFile</span> <span>$DataFile</span> <span>-OutputFileName</span> <span>$OutputFileName</span>
</span></span></code></pre></div><p>All it really does is strips out the <em>first 8 bytes</em> from each line (one line represents one batch of HID data) and stores the binary representation as a JPEG file. It can be invoked as such:</p>
<div><pre tabindex="0"><code data-lang="powershell"><span><span><span>.\</span><span>exportimage</span><span>.</span><span>ps1</span> <span>-DataFile</span> <span>.\</span><span>data</span><span>.</span><span>txt</span> <span>-OutputFile</span> <span>image</span><span>.</span><span>jpg</span>
</span></span></code></pre></div><p>Once the script executes, we can see my test hedgehog image:</p>

  
  
  
  
  
  <figure>
    
    <img alt="Test image that is sent to the Stream Deck device." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/generated-image.webp">
    
    <figcaption>Test image that is sent to the Stream Deck device.</figcaption>
  </figure>


<p>Nice! We have an idea of how images are passed over the wire. But another characteristic of Stream Deck buttons is that, just like any other buttons, they can be pressed. I’ve also <a href="https://den.dev/blog/reverse-engineering-stream-deck/#-listening-to-key-presses">talked about this in my previous blog post</a>, but the gist is that you have to look at the <em>reverse</em> of what we were doing with images.</p>
<p>That is, your filter string is now this (make sure to adjust the <code>src</code> argument):</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>_ws.col.info <span>==</span> <span>"URB_INTERRUPT in"</span> <span>&amp;&amp;</span> usb.src <span>==</span> 3.5.1
</span></span></code></pre></div><p>I am looking for interrupt data flowing <em>in</em> (to the host) from the USB device at <code>3.5.1</code>. That data, as it turns out, is very easy to parse because what happens is that with each button <em>press</em> and <em>release</em> we get the <em>entire button map</em> in the HID data.</p>
<p>The first four bytes are the header and we can ignore those. The third byte always indicates the <em>number of buttons on the panel</em> - in the Stream Deck Plus case, that’s 8. For the Stream Deck XL, that is 32. The third byte also indicates <em>how many bytes after the header</em> contain the button map. So, if I press the 4th button on the Stream Deck Plus, the data it will send to my computer is this:</p>
<pre tabindex="0"><code data-lang="binary">0000  01 00 08 00 00 00 00 01 00 00 00 00 00 00 00 00  ................ 
0010  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0020  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0030  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0040  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0050  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0060  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0070  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0080  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0090  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
00A0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
00B0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
00C0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
00D0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
00E0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
00F0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0100  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0110  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0120  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0130  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0140  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0150  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0160  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0170  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0180  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
0190  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
01A0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
01B0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
01C0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
01D0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
01E0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
01F0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................ 
</code></pre><p>Yet another piece of hardware infrastructure that is not different across SKUs, which I really appreciate Elgato doing.</p>
<h3 id="the-screen">The screen <span><a href="#the-screen" aria-label="Anchor">#</a></span></h3>
<p>Let’s now talk about the second component on a Stream Deck Plus - the screen. It’s a narrow band that we can use to display a bunch of information. By default it is used to display information about the knobs right below it. From this statement, we can <em>assume</em> that there are four distinct sections of this screen associated with each knob, but that would be <em>only a partially correct assumption</em>.</p>
<p>In practice, the entire screen area is just one big image. Well, big is relative here - it’s a 800x100 image. The way I found this is by setting the background to the screen and then doing the PowerShell “hack” I mentioned above (but with a header offset of 16 bytes instead of 8) to inspect the traffic from my computer to the connected Stream Deck Plus. What I saw was this:</p>

  
  
  
  
  
  <figure>
    
    <img alt="Background image on a Stream Deck Plus screen." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/narrow-band-image.webp">
    
    <figcaption>Background image on a Stream Deck Plus screen.</figcaption>
  </figure>


<p>What the Stream Deck software does is create a composite image of all the things you’re associating with the knobs, and then pass it to the device as one blob. If we inspect the <em>outbound</em> traffic from the host to the device, we see packets like this:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span>0000</span>  <span>02</span> 0C <span>00</span> <span>00</span> <span>00</span> <span>00</span> <span>20</span> <span>03</span> <span>64</span> <span>00</span> <span>00</span> <span>00</span> <span>00</span> F0 <span>03</span> <span>00</span>  ...... .d....ð.. 
</span></span><span><span><span>0010</span>  FF D8 FF E0 <span>00</span> <span>10</span> 4A <span>46</span> <span>49</span> <span>46</span> <span>00</span> <span>01</span> <span>01</span> <span>00</span> <span>00</span> <span>01</span>  ÿØÿà..JFIF...... 
</span></span><span><span><span>0020</span>  <span>00</span> <span>01</span> <span>00</span> <span>00</span> FF DB <span>00</span> <span>43</span> <span>00</span> <span>03</span> <span>02</span> <span>02</span> <span>03</span> <span>02</span> <span>02</span> <span>03</span>  ....ÿÛ.C........ 
</span></span><span><span><span>0030</span>  <span>03</span> <span>03</span> <span>03</span> <span>04</span> <span>03</span> <span>03</span> <span>04</span> <span>05</span> <span>08</span> <span>05</span> <span>05</span> <span>04</span> <span>04</span> <span>05</span> 0A <span>07</span>  ................ 
</span></span><span><span><span>0040</span>  <span>07</span> <span>06</span> <span>08</span> 0C 0A 0C 0C 0B 0A 0B 0B 0D 0E <span>12</span> <span>10</span> 0D  ................ 
</span></span><span><span><span>0050</span>  0E <span>11</span> 0E 0B 0B <span>10</span> <span>16</span> <span>10</span> <span>11</span> <span>13</span> <span>14</span> <span>15</span> <span>15</span> <span>15</span> 0C 0F  ................ 
</span></span><span><span><span>0060</span>  <span>17</span> <span>18</span> <span>16</span> <span>14</span> <span>18</span> <span>12</span> <span>14</span> <span>15</span> <span>14</span> FF DB <span>00</span> <span>43</span> <span>01</span> <span>03</span> <span>04</span>  .........ÿÛ.C... 
</span></span><span><span><span>0070</span>  <span>04</span> <span>05</span> <span>04</span> <span>05</span> <span>09</span> <span>05</span> <span>05</span> <span>09</span> <span>14</span> 0D 0B 0D <span>14</span> <span>14</span> <span>14</span> <span>14</span>  ................ 
</span></span></code></pre></div><p>We now have a 16-byte header, that is structured like this (judging from the starting packet that is used to send the image):</p>
<pre tabindex="0"><code data-lang="binary">+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+
| Byte  |  0 |  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |  9 | 10 | 11 | 12 | 13 | 14 | 15 |
+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+
| Value | 02 | 0C | 00 | 00 | 00 | 00 | 20 | 03 | 64 | 00 | 00 | 00 | 00 | F0 | 03 | 00 |
+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+
</code></pre><p>Looking at <em>all</em> packets for a single image, I started jotting down the following assumptions for the screen image setting headers:</p>
<table>
<thead>
<tr>
<th>Byte Index</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>0</code></td>
<td>Always <code>02</code></td>
</tr>
<tr>
<td><code>1</code></td>
<td>Always <code>0C</code></td>
</tr>
<tr>
<td><code>2</code> to <code>5</code></td>
<td>Always <code>00</code></td>
</tr>
<tr>
<td><code>6</code></td>
<td>Always <code>20</code></td>
</tr>
<tr>
<td><code>7</code></td>
<td>Always <code>03</code></td>
</tr>
<tr>
<td><code>8</code></td>
<td>Always <code>64</code></td>
</tr>
<tr>
<td><code>9</code></td>
<td>Always <code>00</code></td>
</tr>
<tr>
<td><code>10</code></td>
<td>Indicates whether this is the final chunk (i.e., “page”) when setting an image via a multi-part packet. Can be <code>00</code> or <code>01</code>.</td>
</tr>
<tr>
<td><code>11</code></td>
<td>Chunk (i.e., “page”) index when setting an image via a multi-part packet.</td>
</tr>
<tr>
<td><code>12</code></td>
<td>Always <code>00</code></td>
</tr>
<tr>
<td><code>13</code> and <code>14</code></td>
<td>Little Endian representation of the payload length.</td>
</tr>
<tr>
<td><code>15</code></td>
<td>Always <code>00</code></td>
</tr>
</tbody>
</table>
<p>And this would seem <em>good enough</em>, except there is a twist. I mentioned earlier that the assumption that the whole screen is always managed as a single image is only partially correct, because when you use one of the knobs (whether you press it or turn it), there is an overlay displayed briefly on the screen showing what is happening.</p>

  
  
  
  
  
  <figure>
    
    <img alt="A screen overlay shown on Stream Deck Plus." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/overlay.webp">
    
    <figcaption>A screen overlay shown on Stream Deck Plus.</figcaption>
  </figure>


<p>These overlays, just like everything else, are managed by the Stream Deck software. What was peculiar about it, though, was that it was sent as a <em>segment</em> - that is, the image that you see above is what was sent from my PC to Stream Deck Plus. The software didn’t send the full composite, but rather just that one part of the screen.</p>
<p>Could it be that every segment of the screen is addressable? I started comparing the headers between overlay sets. The data below shows the <em>headers</em> as individual bytes, followed by a short extract of the image payload (trust me, you don’t want the whole thing here). To make it easier to analyze things, I’ve split the screen in four segments, from left to right - <strong>A</strong>, <strong>B</strong>, <strong>C</strong>, and <strong>D</strong>.</p>
<h4 id="segment-a">Segment A <span><a href="#segment-a" aria-label="Anchor">#</a></span></h4>
<pre tabindex="0"><code data-lang="binary">02 0c 00 00 00 00 c8 00 64 00 00 00 00 f0 03 00 ffd8ffe000104a46
02 0c 00 00 00 00 c8 00 64 00 00 01 00 f0 03 00 62719049eff74526
02 0c 00 00 00 00 c8 00 64 00 00 02 00 f0 03 00 6b8844808f976955
02 0c 00 00 00 00 c8 00 64 00 00 03 00 f0 03 00 e7d81a2c2bbb151b
02 0c 00 00 00 00 c8 00 64 00 00 04 00 f0 03 00 0f7c678f7a666ddf
02 0c 00 00 00 00 c8 00 64 00 00 05 00 f0 03 00 434b7255d6e66e9d
02 0c 00 00 00 00 c8 00 64 00 00 06 00 f0 03 00 fc54da1adeecad78
02 0c 00 00 00 00 c8 00 64 00 00 07 00 f0 03 00 7e4095f72f5c6bc2
02 0c 00 00 00 00 c8 00 64 00 00 08 00 f0 03 00 918ba9de8b9d4edf
02 0c 00 00 00 00 c8 00 64 00 00 09 00 f0 03 00 12ebd3dd2116a218
02 0c 00 00 00 00 c8 00 64 00 01 0a 00 0a 00 00 d38bb9152295be67
</code></pre><h4 id="segment-b">Segment B <span><a href="#segment-b" aria-label="Anchor">#</a></span></h4>
<pre tabindex="0"><code data-lang="binary">02 0c c8 00 00 00 c8 00 64 00 00 00 00 f0 03 00 ffd8ffe000104a46
02 0c c8 00 00 00 c8 00 64 00 00 01 00 f0 03 00 7662eade285d4963
02 0c c8 00 00 00 c8 00 64 00 00 02 00 f0 03 00 23241008207b8c96
02 0c c8 00 00 00 c8 00 64 00 00 03 00 f0 03 00 920734728fdaf35d
02 0c c8 00 00 00 c8 00 64 00 00 04 00 f0 03 00 f41d3afd723f98ed
02 0c c8 00 00 00 c8 00 64 00 00 05 00 f0 03 00 4b14e7701b82b291
02 0c c8 00 00 00 c8 00 64 00 00 06 00 f0 03 00 78f61540d6b165b6
02 0c c8 00 00 00 c8 00 64 00 00 07 00 f0 03 00 481bb70eb8fc7fcf
02 0c c8 00 00 00 c8 00 64 00 01 08 00 05 02 00 fce4dbc726e6ea09
</code></pre><h4 id="segment-c">Segment C <span><a href="#segment-c" aria-label="Anchor">#</a></span></h4>
<pre tabindex="0"><code data-lang="binary">02 0c 90 01 00 00 c8 00 64 00 00 00 00 f0 03 00 ffd8ffe000104a46
02 0c 90 01 00 00 c8 00 64 00 00 01 00 f0 03 00 1260fa52b10e4d00
02 0c 90 01 00 00 c8 00 64 00 00 02 00 f0 03 00 e52b2649911c1ce1
02 0c 90 01 00 00 c8 00 64 00 00 03 00 f0 03 00 8cadbf5d1a6bf53d
02 0c 90 01 00 00 c8 00 64 00 00 04 00 f0 03 00 51e403cc601c8127
02 0c 90 01 00 00 c8 00 64 00 00 05 00 f0 03 00 bfd26ffc7fa25e69
02 0c 90 01 00 00 c8 00 64 00 00 06 00 f0 03 00 91d352f3fe933f3d
02 0c 90 01 00 00 c8 00 64 00 00 07 00 f0 03 00 e90ffb69fc22540d
02 0c 90 01 00 00 c8 00 64 00 01 08 00 13 01 00 9f434156b8d8df76
</code></pre><h4 id="segment-d">Segment D <span><a href="#segment-d" aria-label="Anchor">#</a></span></h4>
<pre tabindex="0"><code data-lang="binary">02 0c 58 02 00 00 c8 00 64 00 00 00 00 f0 03 00 ffd8ffe000104a46
02 0c 58 02 00 00 c8 00 64 00 00 01 00 f0 03 00 9e49ea2ad547ccd3
02 0c 58 02 00 00 c8 00 64 00 00 02 00 f0 03 00 33d453a105caa44e
02 0c 58 02 00 00 c8 00 64 00 00 03 00 f0 03 00 2d7d0e89d08caad3
02 0c 58 02 00 00 c8 00 64 00 00 04 00 f0 03 00 d7b45dcc67503a1e
02 0c 58 02 00 00 c8 00 64 00 00 05 00 f0 03 00 9e0ed72437f7b6f6
02 0c 58 02 00 00 c8 00 64 00 00 06 00 f0 03 00 b8b9d1aea3d7a351
02 0c 58 02 00 00 c8 00 64 00 01 07 00 a4 00 00 4e735a459338a458
</code></pre><h4 id="spotting-the-delta">Spotting the delta <span><a href="#spotting-the-delta" aria-label="Anchor">#</a></span></h4>
<p>Between the packets I listed above, the only things that changed in the header are the third and fourth bytes. This makes me think that those are the <em>screen segment addresses</em>. We have:</p>
<table>
<thead>
<tr>
<th>Segment</th>
<th>Address</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>A</code></td>
<td><code>00 00</code></td>
</tr>
<tr>
<td><code>B</code></td>
<td><code>C8 00</code></td>
</tr>
<tr>
<td><code>C</code></td>
<td><code>90 01</code></td>
</tr>
<tr>
<td><code>D</code></td>
<td><code>58 02</code></td>
</tr>
</tbody>
</table>
<p>If we convert the values to decimal, the table suddenly will start making more sense:</p>
<table>
<thead>
<tr>
<th>Segment</th>
<th>Address</th>
<th>Little-Endian Address</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>A</code></td>
<td><code>00 00</code></td>
<td>0</td>
</tr>
<tr>
<td><code>B</code></td>
<td><code>C8 00</code></td>
<td>200</td>
</tr>
<tr>
<td><code>C</code></td>
<td><code>90 01</code></td>
<td>400</td>
</tr>
<tr>
<td><code>D</code></td>
<td><code>58 02</code></td>
<td>600</td>
</tr>
</tbody>
</table>
<p>Amazing. The third and fourth byte in the header represent the <em>pixel offset</em> (remember how I mentioned that the full image is 800x100). Now, let’s compare all those packets to what we see when we set the <em>full</em> image:</p>
<pre tabindex="0"><code data-lang="binary">02 0c 00 00 00 00 20 03 64 00 00 00 00 f0 03 00 ffd8ffe000104a46
02 0c 00 00 00 00 20 03 64 00 00 01 00 f0 03 00 89b062cee0cd8e84
02 0c 00 00 00 00 20 03 64 00 00 02 00 f0 03 00 5805c9c63771d327
02 0c 00 00 00 00 20 03 64 00 00 03 00 f0 03 00 3dab5a71737721b3
02 0c 00 00 00 00 20 03 64 00 00 04 00 f0 03 00 618246734b6342fe
02 0c 00 00 00 00 20 03 64 00 00 05 00 f0 03 00 aa159464a0cce71d
02 0c 00 00 00 00 20 03 64 00 00 06 00 f0 03 00 8996e51482344335
02 0c 00 00 00 00 20 03 64 00 00 07 00 f0 03 00 45ae683e24bbff00
02 0c 00 00 00 00 20 03 64 00 00 08 00 f0 03 00 8bc53378a7c07e1c
02 0c 00 00 00 00 20 03 64 00 00 09 00 f0 03 00 ff005ff807e3af0b
02 0c 00 00 00 00 20 03 64 00 00 0a 00 f0 03 00 b299080709c9c02a
02 0c 00 00 00 00 20 03 64 00 00 0b 00 f0 03 00 4fb45746ef817c56
02 0c 00 00 00 00 20 03 64 00 00 0c 00 f0 03 00 c62390ac84897272
02 0c 00 00 00 00 20 03 64 00 00 0d 00 f0 03 00 27030d8e36a8e001
02 0c 00 00 00 00 20 03 64 00 00 0e 00 f0 03 00 d95ab7fa562dd956
02 0c 00 00 00 00 20 03 64 00 00 0f 00 f0 03 00 ba2eaf650de69fa4
02 0c 00 00 00 00 20 03 64 00 00 10 00 f0 03 00 efe19c9f0f3c1da1
02 0c 00 00 00 00 20 03 64 00 00 11 00 f0 03 00 7e140462b850800c
02 0c 00 00 00 00 20 03 64 00 00 12 00 f0 03 00 ec37b1e8da7b416d
02 0c 00 00 00 00 20 03 64 00 00 13 00 f0 03 00 ee1636963dec088e
02 0c 00 00 00 00 20 03 64 00 00 14 00 f0 03 00 3f06e9f2c02596f4
02 0c 00 00 00 00 20 03 64 00 00 15 00 f0 03 00 49a0d8d94b7b55d3
02 0c 00 00 00 00 20 03 64 00 00 16 00 f0 03 00 8a2ea63df238c374
02 0c 00 00 00 00 20 03 64 00 00 17 00 f0 03 00 f4b8d34433bc65c4
02 0c 00 00 00 00 20 03 64 00 00 18 00 f0 03 00 2033824b02a3712a
02 0c 00 00 00 00 20 03 64 00 00 19 00 f0 03 00 1c8ee4f526bf4da7
02 0c 00 00 00 00 20 03 64 00 00 1a 00 f0 03 00 e3da788745f19f88
02 0c 00 00 00 00 20 03 64 00 00 1b 00 f0 03 00 2755734755b7dc43
02 0c 00 00 00 00 20 03 64 00 00 1c 00 f0 03 00 e0cad297dd8f5c35
02 0c 00 00 00 00 20 03 64 00 00 1d 00 f0 03 00 c00217a1cfd8f259
02 0c 00 00 00 00 20 03 64 00 00 1e 00 f0 03 00 c1cfc8ec33f91ac7
02 0c 00 00 00 00 20 03 64 00 00 1f 00 f0 03 00 b6f30aecddb72492
02 0c 00 00 00 00 20 03 64 00 00 20 00 f0 03 00 ff00b3fb552c8b08
02 0c 00 00 00 00 20 03 64 00 00 21 00 f0 03 00 baf3fc47d99f25fe
02 0c 00 00 00 00 20 03 64 00 00 22 00 f0 03 00 c6adc4510f3644f3
02 0c 00 00 00 00 20 03 64 00 00 23 00 f0 03 00 dcb5a79842477124
02 0c 00 00 00 00 20 03 64 00 00 24 00 f0 03 00 73950000a48af069
02 0c 00 00 00 00 20 03 64 00 00 25 00 f0 03 00 032c919120c6eca0
02 0c 00 00 00 00 20 03 64 00 00 26 00 f0 03 00 63f0d3e1a787127d
02 0c 00 00 00 00 20 03 64 00 00 27 00 f0 03 00 6efb25e5add3b7a9
02 0c 00 00 00 00 20 03 64 00 00 28 00 f0 03 00 7874fe1883c53e1d
02 0c 00 00 00 00 20 03 64 00 00 29 00 f0 03 00 20d4632b5ceda74e
02 0c 00 00 00 00 20 03 64 00 01 2a 00 92 02 00 bf32e9c5367927c4
</code></pre><p>The seventh and eight bytes all of a sudden became <code>20 03</code>, and that is because, once again - we need to look at the Little Endian representation for the values. For each segment, this value has been <code>C8 00</code>, which translates to <code>200</code>. <code>20 03</code> is <code>800</code>. This is the image width. <code>64 00</code> is <code>100</code>, so it’s the image height.</p>
<p>My assumed table now took a much better shape:</p>
<table>
<thead>
<tr>
<th>Byte Index</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>0</code></td>
<td>Always <code>02</code></td>
</tr>
<tr>
<td><code>1</code></td>
<td>Always <code>0C</code></td>
</tr>
<tr>
<td><code>2</code> and <code>3</code></td>
<td>Offset from the left corner.</td>
</tr>
<tr>
<td><code>4</code> and <code>5</code></td>
<td>Always <code>00 00</code>.</td>
</tr>
<tr>
<td><code>6</code> and <code>7</code></td>
<td>Image width.</td>
</tr>
<tr>
<td><code>8</code> and <code>9</code></td>
<td>Image height.</td>
</tr>
<tr>
<td><code>10</code></td>
<td>Indicates whether this is the final chunk (i.e., “page”) when setting an image via a multi-part packet. Can be <code>00</code> or <code>01</code>.</td>
</tr>
<tr>
<td><code>11</code> and <code>12</code></td>
<td>Chunk (i.e., “page”) index when setting an image via a multi-part packet.</td>
</tr>
<tr>
<td><code>13</code> and <code>14</code></td>
<td>Little Endian representation of the payload length.</td>
</tr>
<tr>
<td><code>15</code></td>
<td>Always <code>00</code></td>
</tr>
</tbody>
</table>
<p>And that’s it, we now know how screen data is set! Not overly complicated once I started looking at the delta between packets.</p>
<p>The last thing we need to talk about here is the fact that the screen <strong>is a touch screen</strong>, so we also need to be able to spot the user pressing on a segment. Pressing on any of the screen parts are functionally equivalent to pressing the knob - the same overlay will be shown if you’re using the Stream Deck software. But how does it show up in Wireshark?</p>
<p>To check, let’s once again set the filter to this, because we want to track events that originate on the device and go to the PC:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>usb.src ~ <span>"3.5"</span> <span>&amp;&amp;</span> _ws.col.info <span>==</span> <span>"URB_INTERRUPT in"</span>
</span></span></code></pre></div><p>Here is the data we get, tapping from left to right.</p>
<pre tabindex="0"><code data-lang="binary">01 02 0e 00 01 01 47 00 40 00 00000000000000000
01 02 0e 00 01 01 07 01 1c 00 00000000000000000
01 02 0e 00 01 01 ee 01 32 00 00000000000000000
01 02 0e 00 01 01 b3 02 29 00 00000000000000000
</code></pre><p>That looks very random. A little too varied for us to make any definitive conclusions. But you know what, let’s try tapping the same segment in different parts of the touch screen:</p>
<pre tabindex="0"><code data-lang="binary">01 02 0e 00 01 01 b2 02 33 00 00000000000000000
01 02 0e 00 01 01 13 03 20 00 00000000000000000
01 02 0e 00 01 01 a9 02 46 00 00000000000000000
01 02 0e 00 01 01 06 03 26 00 00000000000000000
01 02 0e 00 01 01 bb 02 20 00 00000000000000000
01 02 0e 00 01 01 00 03 52 00 00000000000000000
01 02 0e 00 01 01 b6 02 3e 00 00000000000000000
01 02 0e 00 01 01 f7 02 2f 00 00000000000000000
01 02 0e 00 01 01 f1 02 1c 00 00000000000000000
01 02 0e 00 01 01 8d 02 1c 00 00000000000000000
</code></pre><p>This variability in values instantly gave me a clue - we’re looking at coordinates on the screen! The structure ends up being this:</p>
<pre tabindex="0"><code data-lang="binary">+-------+----+----+----+----+----+----+----+-----+----+-----+
| Byte  |  0 |  1 |  2 |  3 |  4 |  5 |   6 -  7 |   8 -  9 |
+-------+----+----+----+----+----+----+----+-----+----+-----+
| Value | 01 | 02 | 0E | 00 | 01 | 01 | X coord. | Y coord. |
+-------+----+----+----+----+----+----+----+-----+----+-----+
</code></pre><p>Unlike the buttons, there is no <em>release</em> event - we just get a tap, and that’s it. That means that once we get an event marked by the header above, we can compute based on the X and Y coordinates <em>which part of the screen</em> was tapped and react accordingly.</p>
<h3 id="the-knobs">The knobs <span><a href="#the-knobs" aria-label="Anchor">#</a></span></h3>
<p>Last but not least, the one thing we should talk about are the knobs. Each knob can be used in three ways:</p>
<ol>
<li>Turn right.</li>
<li>Turn left.</li>
<li>Press.</li>
</ol>
<p>To make sure I log things properly, for each knob, labeled similar to how I labeled screen segments, I took four turns to the right, four turns to the left, and then a press. The data I captured is below.</p>
<h4 id="knob-a">Knob A <span><a href="#knob-a" aria-label="Anchor">#</a></span></h4>
<h5 id="right-turns">Right turns <span><a href="#right-turns" aria-label="Anchor">#</a></span></h5>
<pre tabindex="0"><code data-lang="binary">01 03 05 00 01 01 00 00 00 0000000000
01 03 05 00 01 01 00 00 00 0000000000
01 03 05 00 01 01 00 00 00 0000000000
01 03 05 00 01 01 00 00 00 0000000000
</code></pre><h5 id="left-turns">Left turns <span><a href="#left-turns" aria-label="Anchor">#</a></span></h5>
<pre tabindex="0"><code data-lang="binary">01 03 05 00 01 ff 00 00 00 0000000000
01 03 05 00 01 ff 00 00 00 0000000000
01 03 05 00 01 ff 00 00 00 0000000000
01 03 05 00 01 ff 00 00 00 0000000000
</code></pre><h5 id="press">Press <span><a href="#press" aria-label="Anchor">#</a></span></h5>
<pre tabindex="0"><code data-lang="binary">01 03 05 00 00 01 00 00 00 0000000000
01 03 05 00 00 00 00 00 00 0000000000
</code></pre><h4 id="knob-b">Knob B <span><a href="#knob-b" aria-label="Anchor">#</a></span></h4>
<h5 id="right-turns-1">Right turns <span><a href="#right-turns-1" aria-label="Anchor">#</a></span></h5>
<pre tabindex="0"><code data-lang="binary">01 03 05 00 01 00 01 00 00 0000000000
01 03 05 00 01 00 01 00 00 0000000000
01 03 05 00 01 00 01 00 00 0000000000
01 03 05 00 01 00 01 00 00 0000000000
</code></pre><h5 id="left-turns-1">Left turns <span><a href="#left-turns-1" aria-label="Anchor">#</a></span></h5>
<pre tabindex="0"><code data-lang="binary">01 03 05 00 01 00 ff 00 00 0000000000
01 03 05 00 01 00 ff 00 00 0000000000
01 03 05 00 01 00 ff 00 00 0000000000
01 03 05 00 01 00 ff 00 00 0000000000
</code></pre><h5 id="press-1">Press <span><a href="#press-1" aria-label="Anchor">#</a></span></h5>
<pre tabindex="0"><code data-lang="binary">01 03 05 00 00 00 01 00 00 0000000000
01 03 05 00 00 00 00 00 00 0000000000
</code></pre><h4 id="knob-c">Knob C <span><a href="#knob-c" aria-label="Anchor">#</a></span></h4>
<h5 id="right-turns-2">Right turns <span><a href="#right-turns-2" aria-label="Anchor">#</a></span></h5>
<pre tabindex="0"><code data-lang="binary">01 03 05 00 01 00 00 01 00 0000000000
01 03 05 00 01 00 00 01 00 0000000000
01 03 05 00 01 00 00 01 00 0000000000
01 03 05 00 01 00 00 01 00 0000000000
</code></pre><h5 id="left-turns-2">Left turns <span><a href="#left-turns-2" aria-label="Anchor">#</a></span></h5>
<pre tabindex="0"><code data-lang="binary">01 03 05 00 01 00 00 ff 00 0000000000
01 03 05 00 01 00 00 ff 00 0000000000
01 03 05 00 01 00 00 ff 00 0000000000
01 03 05 00 01 00 00 ff 00 0000000000
</code></pre><h5 id="press-2">Press <span><a href="#press-2" aria-label="Anchor">#</a></span></h5>
<pre tabindex="0"><code data-lang="binary">01 03 05 00 00 00 00 01 00 0000000000
01 03 05 00 00 00 00 00 00 0000000000
</code></pre><h4 id="knob-d">Knob D <span><a href="#knob-d" aria-label="Anchor">#</a></span></h4>
<h5 id="right-turns-3">Right turns <span><a href="#right-turns-3" aria-label="Anchor">#</a></span></h5>
<pre tabindex="0"><code data-lang="binary">01 03 05 00 01 00 00 00 01 0000000000
01 03 05 00 01 00 00 00 01 0000000000
01 03 05 00 01 00 00 00 01 0000000000
01 03 05 00 01 00 00 00 01 0000000000
</code></pre><h5 id="left-turns-3">Left turns <span><a href="#left-turns-3" aria-label="Anchor">#</a></span></h5>
<pre tabindex="0"><code data-lang="binary">01 03 05 00 01 00 00 00 ff 0000000000
01 03 05 00 01 00 00 00 ff 0000000000
01 03 05 00 01 00 00 00 ff 0000000000
01 03 05 00 01 00 00 00 ff 0000000000
</code></pre><h5 id="press-3">Press <span><a href="#press-3" aria-label="Anchor">#</a></span></h5>
<pre tabindex="0"><code data-lang="binary">01 03 05 00 00 00 00 00 01 0000000000
01 03 05 00 00 00 00 00 00 0000000000
</code></pre><h4 id="slicing-and-dicing">Slicing and dicing <span><a href="#slicing-and-dicing" aria-label="Anchor">#</a></span></h4>
<p>Looking at the signals above, commonalities emerged rather quickly because the same values kept shifting to the right.</p>

  
  
  
  
  
  <figure>
    
    <img alt="All the patterns eventually come back to repeat themselves." src="https://assets.den.dev/images/postmedia/reverse-engineer-stream-deck-plus/ive-seen-this.gif">
    
    <figcaption>All the patterns eventually come back to repeat themselves.</figcaption>
  </figure>


<p>That’s basically the button press pattern that we’ve seen with, well, button presses!</p>
<pre tabindex="0"><code data-lang="binary">+-------+----+----+----+----+------------+---------------+---------------+---------------+---------------+
| Byte  |  0 |  1 |  2 |  3 |          4 |             5 |             6 |             7 |             8 |
+-------+----+----+----+----+------------+---------------+---------------+---------------+---------------+
| Value | 01 | 03 | 05 | 00 | Is turning | Knob A action | Knob B action | Knob C action | Knob D action |
+-------+----+----+----+----+------------+---------------+---------------+---------------+---------------+
</code></pre><p>For each knob, we can get a combo of:</p>
<ol>
<li>Byte <code>4</code> set to <code>01</code> (turning), button-specific bytes set to <code>01</code> (turn right) or <code>FF</code> (turn left).</li>
<li>Byte <code>4</code> set to <code>00</code> (presset), button-specific bytes set to <code>01</code> (pressed) or <code>00</code> (released).</li>
</ol>
<p>Very nice - if you followed along all this time, you now know how the binary data is set for every control on the Stream Deck Plus.</p>
<h2 id="writing-a-wrapper">Writing a wrapper <span><a href="#writing-a-wrapper" aria-label="Anchor">#</a></span></h2>
<p>Alright, now that we went through the slog that is binary data analysis, it’s time to have a nicer way to deal with all this mess. To do that, <a href="https://github.com/dend/decksurf-sdk/pull/4" target="_blank" rel="noreferrer noopener">I updated the DeckSurf SDK</a> to support the Stream Deck Plus.</p>
<p>With the latest release of the <a href="https://www.nuget.org/packages/DeckSurf.SDK" target="_blank" rel="noreferrer noopener">DeckSurf SDK on NuGet</a> (0.0.4 at the time of this writing), you can use it to managed a Stream Deck Plus device!</p>
<p>Here is a fully-functioning sample in C# that shows how to handle events and set images on a Stream Deck Plus:</p>
<div><pre tabindex="0"><code data-lang="csharp"><span><span><span>using</span> <span>DeckSurf.SDK.Core</span><span>;</span>
</span></span><span><span><span>using</span> <span>DeckSurf.SDK.Models</span><span>;</span>
</span></span><span><span><span>using</span> <span>DeckSurf.SDK.Util</span><span>;</span>
</span></span><span><span><span>using</span> <span>System</span><span>;</span>
</span></span><span><span><span>using</span> <span>System.Collections.Generic</span><span>;</span>
</span></span><span><span><span>using</span> <span>System.IO</span><span>;</span>
</span></span><span><span><span>using</span> <span>System.Threading</span><span>;</span>
</span></span><span><span>
</span></span><span><span><span>namespace</span> <span>DeckSurf.SDK.StartBoard</span>
</span></span><span><span><span>{</span>
</span></span><span><span>    <span>class</span> <span>Program</span>
</span></span><span><span>    <span>{</span>
</span></span><span><span>        <span>static</span> <span>void</span> <span>Main</span><span>(</span><span>string</span><span>[]</span> <span>args</span><span>)</span>
</span></span><span><span>        <span>{</span>
</span></span><span><span>            <span>var</span> <span>exitSignal</span> <span>=</span> <span>new</span> <span>ManualResetEvent</span><span>(</span><span>false</span><span>);</span>
</span></span><span><span>            <span>var</span> <span>devices</span> <span>=</span> <span>DeviceManager</span><span>.</span><span>GetDeviceList</span><span>();</span>
</span></span><span><span>
</span></span><span><span>            <span>Console</span><span>.</span><span>WriteLine</span><span>(</span><span>"The following Stream Deck devices are connected:"</span><span>);</span>
</span></span><span><span>
</span></span><span><span>            <span>foreach</span> <span>(</span><span>var</span> <span>connectedDevice</span> <span>in</span> <span>devices</span><span>)</span>
</span></span><span><span>            <span>{</span>
</span></span><span><span>                <span>Console</span><span>.</span><span>WriteLine</span><span>(</span><span>connectedDevice</span><span>.</span><span>Name</span><span>);</span>
</span></span><span><span>            <span>}</span>
</span></span><span><span>
</span></span><span><span>            <span>var</span> <span>device</span> <span>=</span> <span>((</span><span>List</span><span>&lt;</span><span>ConnectedDevice</span><span>&gt;)</span><span>devices</span><span>)[</span><span>0</span><span>];</span>
</span></span><span><span>            <span>device</span><span>.</span><span>StartListening</span><span>();</span>
</span></span><span><span>            <span>device</span><span>.</span><span>OnButtonPress</span> <span>+=</span> <span>Device_OnButtonPress</span><span>;</span>
</span></span><span><span>
</span></span><span><span>            <span>byte</span><span>[]</span> <span>testImage</span> <span>=</span> <span>File</span><span>.</span><span>ReadAllBytes</span><span>(</span><span>args</span><span>[</span><span>0</span><span>]);</span>
</span></span><span><span>
</span></span><span><span>            <span>var</span> <span>image</span> <span>=</span> <span>ImageHelpers</span><span>.</span><span>ResizeImage</span><span>(</span><span>testImage</span><span>,</span> <span>device</span><span>.</span><span>ScreenWidth</span><span>,</span> <span>device</span><span>.</span><span>ScreenHeight</span><span>,</span> <span>device</span><span>.</span><span>IsButtonImageFlipRequired</span><span>);</span>
</span></span><span><span>
</span></span><span><span>            <span>device</span><span>.</span><span>SetScreen</span><span>(</span><span>image</span><span>,</span> <span>250</span><span>,</span> <span>device</span><span>.</span><span>ScreenWidth</span><span>,</span> <span>device</span><span>.</span><span>ScreenHeight</span><span>);</span>
</span></span><span><span>
</span></span><span><span>            <span>var</span> <span>keyImage</span> <span>=</span> <span>ImageHelpers</span><span>.</span><span>ResizeImage</span><span>(</span><span>testImage</span><span>,</span> <span>device</span><span>.</span><span>ButtonResolution</span><span>,</span> <span>device</span><span>.</span><span>ButtonResolution</span><span>,</span> <span>device</span><span>.</span><span>IsButtonImageFlipRequired</span><span>);</span>
</span></span><span><span>            <span>device</span><span>.</span><span>SetKey</span><span>(</span><span>1</span><span>,</span> <span>keyImage</span><span>);</span>
</span></span><span><span>
</span></span><span><span>            <span>device</span><span>.</span><span>SetBrightness</span><span>(</span><span>29</span><span>);</span>
</span></span><span><span>
</span></span><span><span>            <span>Console</span><span>.</span><span>WriteLine</span><span>(</span><span>"Done"</span><span>);</span>
</span></span><span><span>            <span>exitSignal</span><span>.</span><span>WaitOne</span><span>();</span>
</span></span><span><span>        <span>}</span>
</span></span><span><span>
</span></span><span><span>        <span>private</span> <span>static</span> <span>void</span> <span>Device_OnButtonPress</span><span>(</span><span>object</span> <span>source</span><span>,</span> <span>ButtonPressEventArgs</span> <span>e</span><span>)</span>
</span></span><span><span>        <span>{</span>
</span></span><span><span>            <span>Console</span><span>.</span><span>WriteLine</span><span>(</span><span>$"Button with ID {e.Id} was pressed. It's identified as {e.ButtonKind}. Event is {e.EventKind}. If this is a touch screen, coordinates are {e.TapCoordinates.X} and {e.TapCoordinates.Y}. Is knob rotated: {e.IsKnobRotating}. Rotation direction: {e.KnobRotationDirection}."</span><span>);</span>
</span></span><span><span>        <span>}</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Now, of course - this sample makes the assumption that the Stream Deck Plus device is the first one connected (index zero), so you might want to tweak that if you have more than one Stream Deck plugged in. But nonetheless, this shows you how <em>easy</em> I try to make Stream Deck interactions with the DeckSurf SDK. It’s all still in early preview, so things might break with future releases until I get it to a stable version, but until then - feel free to experiment!</p>
<p>As always, the latest documentation is available on <a href="https://docs.deck.surf/" target="_blank" rel="noreferrer noopener"><code>https://docs.deck.surf</code></a>.</p>
<h2 id="discussions">Discussions <span><a href="#discussions" aria-label="Anchor">#</a></span></h2>
<p>If you’d like to see more discussions on this, check out <a href="https://hackaday.com/2024/12/26/stream-deck-plus-reverse-engineered/" target="_blank" rel="noreferrer noopener">HACKADAY</a> and <a href="https://news.ycombinator.com/item?id=42518444" target="_blank" rel="noreferrer noopener">Hacker News</a>.</p>

      </div>
    
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sub-pixel distance transform (2023) (176 pts)]]></title>
            <link>https://acko.net/blog/subpixel-distance-transform/</link>
            <guid>42517685</guid>
            <pubDate>Thu, 26 Dec 2024 20:48:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://acko.net/blog/subpixel-distance-transform/">https://acko.net/blog/subpixel-distance-transform/</a>, See on <a href="https://news.ycombinator.com/item?id=42517685">Hacker News</a></p>
Couldn't get https://acko.net/blog/subpixel-distance-transform/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Ghostty 1.0 (1963 pts)]]></title>
            <link>https://ghostty.org/</link>
            <guid>42517447</guid>
            <pubDate>Thu, 26 Dec 2024 20:14:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ghostty.org/">https://ghostty.org/</a>, See on <a href="https://news.ycombinator.com/item?id=42517447">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Write a Shell in C (2015) (183 pts)]]></title>
            <link>https://brennan.io/2015/01/16/write-a-shell-in-c/</link>
            <guid>42517303</guid>
            <pubDate>Thu, 26 Dec 2024 19:53:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brennan.io/2015/01/16/write-a-shell-in-c/">https://brennan.io/2015/01/16/write-a-shell-in-c/</a>, See on <a href="https://news.ycombinator.com/item?id=42517303">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  
<p><em>Stephen Brennan • 16 January 2015</em></p><p>It’s easy to view yourself as “not a <em>real</em> programmer.”  There are programs out
there that everyone uses, and it’s easy to put their developers on a pedestal.
Although developing large software projects isn’t easy, many times the basic
idea of that software is quite simple.  Implementing it yourself is a fun way to
show that you have what it takes to be a real programmer.  So, this is a
walkthrough on how I wrote my own simplistic Unix shell in C, in the hopes that
it makes other people feel that way too.</p>

<p>The code for the shell described here, dubbed <code>lsh</code>, is available on
<a href="https://github.com/brenns10/lsh">GitHub</a>.</p>

<p><strong>University students beware!</strong> Many classes have assignments that ask you to
write a shell, and some faculty are aware of this tutorial and code.  If you’re
a student in such a class, you shouldn’t copy (or copy then modify) this code
without permission.  And even then, I would <a href="https://brennan.io/2016/03/29/dishonesty/">advise</a> against heavily relying on this tutorial.</p>

<h2 id="basic-lifetime-of-a-shell">Basic lifetime of a shell</h2>

<p>Let’s look at a shell from the top down.  A shell does three main things in its
lifetime.</p>

<ul>
  <li><strong>Initialize</strong>: In this step, a typical shell would read and execute its
configuration files.  These change aspects of the shell’s behavior.</li>
  <li><strong>Interpret</strong>: Next, the shell reads commands from stdin (which could be
interactive, or a file) and executes them.</li>
  <li><strong>Terminate</strong>: After its commands are executed, the shell executes any
shutdown commands, frees up any memory, and terminates.</li>
</ul>

<p>These steps are so general that they could apply to many programs, but we’re
going to use them for the basis for our shell.  Our shell will be so simple that
there won’t be any configuration files, and there won’t be any shutdown command.
So, we’ll just call the looping function and then terminate.  But in terms of
architecture, it’s important to keep in mind that the lifetime of the program is
more than just looping.</p>

<div><pre><code><span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span> <span>**</span><span>argv</span><span>)</span>
<span>{</span>
  <span>// Load config files, if any.</span>

  <span>// Run command loop.</span>
  <span>lsh_loop</span><span>();</span>

  <span>// Perform any shutdown/cleanup.</span>

  <span>return</span> <span>EXIT_SUCCESS</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Here you can see that I just came up with a function, <code>lsh_loop()</code>, that will
loop, interpreting commands.  We’ll see the implementation of that next.</p>

<h2 id="basic-loop-of-a-shell">Basic loop of a shell</h2>

<p>So we’ve taken care of how the program should start up.  Now, for the basic
program logic: what does the shell do during its loop?  Well, a simple way to
handle commands is with three steps:</p>

<ul>
  <li><strong>Read</strong>: Read the command from standard input.</li>
  <li><strong>Parse</strong>: Separate the command string into a program and arguments.</li>
  <li><strong>Execute</strong>: Run the parsed command.</li>
</ul>

<p>Here, I’ll translate those ideas into code for <code>lsh_loop()</code>:</p>

<div><pre><code><span>void</span> <span>lsh_loop</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
  <span>char</span> <span>*</span><span>line</span><span>;</span>
  <span>char</span> <span>**</span><span>args</span><span>;</span>
  <span>int</span> <span>status</span><span>;</span>

  <span>do</span> <span>{</span>
    <span>printf</span><span>(</span><span>"&gt; "</span><span>);</span>
    <span>line</span> <span>=</span> <span>lsh_read_line</span><span>();</span>
    <span>args</span> <span>=</span> <span>lsh_split_line</span><span>(</span><span>line</span><span>);</span>
    <span>status</span> <span>=</span> <span>lsh_execute</span><span>(</span><span>args</span><span>);</span>

    <span>free</span><span>(</span><span>line</span><span>);</span>
    <span>free</span><span>(</span><span>args</span><span>);</span>
  <span>}</span> <span>while</span> <span>(</span><span>status</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>Let’s walk through the code.  The first few lines are just declarations.  The
do-while loop is more convenient for checking the status variable, because it
executes once before checking its value.  Within the loop, we print a prompt,
call a function to read a line, call a function to split the line into args, and
execute the args.  Finally, we free the line and arguments that we created
earlier.  Note that we’re using a status variable returned by <code>lsh_execute()</code> to
determine when to exit.</p>

<h2 id="reading-a-line">Reading a line</h2>

<p>Reading a line from stdin sounds so simple, but in C it can be a hassle.  The
sad thing is that you don’t know ahead of time how much text a user will enter
into their shell.  You can’t simply allocate a block and hope they don’t exceed
it.  Instead, you need to start with a block, and if they do exceed it,
reallocate with more space.  This is a common strategy in C, and we’ll use it to
implement <code>lsh_read_line()</code>.</p>

<div><pre><code><span>#define LSH_RL_BUFSIZE 1024
</span><span>char</span> <span>*</span><span>lsh_read_line</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
  <span>int</span> <span>bufsize</span> <span>=</span> <span>LSH_RL_BUFSIZE</span><span>;</span>
  <span>int</span> <span>position</span> <span>=</span> <span>0</span><span>;</span>
  <span>char</span> <span>*</span><span>buffer</span> <span>=</span> <span>malloc</span><span>(</span><span>sizeof</span><span>(</span><span>char</span><span>)</span> <span>*</span> <span>bufsize</span><span>);</span>
  <span>int</span> <span>c</span><span>;</span>

  <span>if</span> <span>(</span><span>!</span><span>buffer</span><span>)</span> <span>{</span>
    <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: allocation error</span><span>\n</span><span>"</span><span>);</span>
    <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
  <span>}</span>

  <span>while</span> <span>(</span><span>1</span><span>)</span> <span>{</span>
    <span>// Read a character</span>
    <span>c</span> <span>=</span> <span>getchar</span><span>();</span>

    <span>// If we hit EOF, replace it with a null character and return.</span>
    <span>if</span> <span>(</span><span>c</span> <span>==</span> <span>EOF</span> <span>||</span> <span>c</span> <span>==</span> <span>'\n'</span><span>)</span> <span>{</span>
      <span>buffer</span><span>[</span><span>position</span><span>]</span> <span>=</span> <span>'\0'</span><span>;</span>
      <span>return</span> <span>buffer</span><span>;</span>
    <span>}</span> <span>else</span> <span>{</span>
      <span>buffer</span><span>[</span><span>position</span><span>]</span> <span>=</span> <span>c</span><span>;</span>
    <span>}</span>
    <span>position</span><span>++</span><span>;</span>

    <span>// If we have exceeded the buffer, reallocate.</span>
    <span>if</span> <span>(</span><span>position</span> <span>&gt;=</span> <span>bufsize</span><span>)</span> <span>{</span>
      <span>bufsize</span> <span>+=</span> <span>LSH_RL_BUFSIZE</span><span>;</span>
      <span>buffer</span> <span>=</span> <span>realloc</span><span>(</span><span>buffer</span><span>,</span> <span>bufsize</span><span>);</span>
      <span>if</span> <span>(</span><span>!</span><span>buffer</span><span>)</span> <span>{</span>
        <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: allocation error</span><span>\n</span><span>"</span><span>);</span>
        <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<p>The first part is a lot of declarations.  If you hadn’t noticed, I prefer to
keep the old C style of declaring variables before the rest of the code.  The
meat of the function is within the (apparently infinite) <code>while (1)</code> loop.  In
the loop, we read a character (and store it as an <code>int</code>, not a <code>char</code>, that’s
important!  EOF is an integer, not a character, and if you want to check for it,
you need to use an <code>int</code>.  This is a common beginner C mistake.).  If it’s the
newline, or EOF, we null terminate our current string and return it.  Otherwise,
we add the character to our existing string.</p>

<p>Next, we see whether the next character will go outside of our current buffer
size.  If so, we reallocate our buffer (checking for allocation errors) before
continuing.  And that’s really it.</p>

<p>Those who are intimately familiar with newer versions of the C library may note
that there is a <code>getline()</code> function in <code>stdio.h</code> that does most of the work we
just implemented.  To be completely honest, I didn’t know it existed until after
I wrote this code.  This function was a GNU extension to the C library until
2008, when it was added to the specification, so most modern Unixes should have
it now.  I’m leaving my existing code the way it is, and I encourage people to
learn it this way first before using <code>getline</code>.  You’d be robbing yourself of a
learning opportunity if you didn’t!  Anyhow, with <code>getline</code>, the function
becomes easier:</p>

<div><pre><code><span>char</span> <span>*</span><span>lsh_read_line</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
  <span>char</span> <span>*</span><span>line</span> <span>=</span> <span>NULL</span><span>;</span>
  <span>ssize_t</span> <span>bufsize</span> <span>=</span> <span>0</span><span>;</span> <span>// have getline allocate a buffer for us</span>

  <span>if</span> <span>(</span><span>getline</span><span>(</span><span>&amp;</span><span>line</span><span>,</span> <span>&amp;</span><span>bufsize</span><span>,</span> <span>stdin</span><span>)</span> <span>==</span> <span>-</span><span>1</span><span>){</span>
    <span>if</span> <span>(</span><span>feof</span><span>(</span><span>stdin</span><span>))</span> <span>{</span>
      <span>exit</span><span>(</span><span>EXIT_SUCCESS</span><span>);</span>  <span>// We recieved an EOF</span>
    <span>}</span> <span>else</span>  <span>{</span>
      <span>perror</span><span>(</span><span>"readline"</span><span>);</span>
      <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
    <span>}</span>
  <span>}</span>

  <span>return</span> <span>line</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>This is not 100% trivial because we still need to check for EOF or errors while
reading. EOF (end of file) means that either we were reading commands from a
text file which we’ve reached the end of, or the user typed Ctrl-D, which
signals end-of-file. Either way, it means we should exit successfully, and if
any other error occurs, we should fail after printing the error.</p>

<h2 id="parsing-the-line">Parsing the line</h2>

<p>OK, so if we look back at the loop, we see that we now have implemented
<code>lsh_read_line()</code>, and we have the line of input.  Now, we need to parse that
line into a list of arguments.  I’m going to make a glaring simplification here,
and say that we won’t allow quoting or backslash escaping in our command line
arguments.  Instead, we will simply use whitespace to separate arguments from
each other.  So the command <code>echo "this message"</code> would not call echo with a
single argument <code>this message</code>, but rather it would call echo with two
arguments: <code>"this</code> and <code>message"</code>.</p>

<p>With those simplifications, all we need to do is “tokenize” the string using
whitespace as delimiters.  That means we can break out the classic library
function <code>strtok</code> to do some of the dirty work for us.</p>

<div><pre><code><span>#define LSH_TOK_BUFSIZE 64
#define LSH_TOK_DELIM " \t\r\n\a"
</span><span>char</span> <span>**</span><span>lsh_split_line</span><span>(</span><span>char</span> <span>*</span><span>line</span><span>)</span>
<span>{</span>
  <span>int</span> <span>bufsize</span> <span>=</span> <span>LSH_TOK_BUFSIZE</span><span>,</span> <span>position</span> <span>=</span> <span>0</span><span>;</span>
  <span>char</span> <span>**</span><span>tokens</span> <span>=</span> <span>malloc</span><span>(</span><span>bufsize</span> <span>*</span> <span>sizeof</span><span>(</span><span>char</span><span>*</span><span>));</span>
  <span>char</span> <span>*</span><span>token</span><span>;</span>

  <span>if</span> <span>(</span><span>!</span><span>tokens</span><span>)</span> <span>{</span>
    <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: allocation error</span><span>\n</span><span>"</span><span>);</span>
    <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
  <span>}</span>

  <span>token</span> <span>=</span> <span>strtok</span><span>(</span><span>line</span><span>,</span> <span>LSH_TOK_DELIM</span><span>);</span>
  <span>while</span> <span>(</span><span>token</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
    <span>tokens</span><span>[</span><span>position</span><span>]</span> <span>=</span> <span>token</span><span>;</span>
    <span>position</span><span>++</span><span>;</span>

    <span>if</span> <span>(</span><span>position</span> <span>&gt;=</span> <span>bufsize</span><span>)</span> <span>{</span>
      <span>bufsize</span> <span>+=</span> <span>LSH_TOK_BUFSIZE</span><span>;</span>
      <span>tokens</span> <span>=</span> <span>realloc</span><span>(</span><span>tokens</span><span>,</span> <span>bufsize</span> <span>*</span> <span>sizeof</span><span>(</span><span>char</span><span>*</span><span>));</span>
      <span>if</span> <span>(</span><span>!</span><span>tokens</span><span>)</span> <span>{</span>
        <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: allocation error</span><span>\n</span><span>"</span><span>);</span>
        <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
      <span>}</span>
    <span>}</span>

    <span>token</span> <span>=</span> <span>strtok</span><span>(</span><span>NULL</span><span>,</span> <span>LSH_TOK_DELIM</span><span>);</span>
  <span>}</span>
  <span>tokens</span><span>[</span><span>position</span><span>]</span> <span>=</span> <span>NULL</span><span>;</span>
  <span>return</span> <span>tokens</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>If this code looks suspiciously similar to <code>lsh_read_line()</code>, it’s because it
is!  We are using the same strategy of having a buffer and dynamically expanding
it.  But this time, we’re doing it with a null-terminated array of pointers
instead of a null-terminated array of characters.</p>

<p>At the start of the function, we begin tokenizing by calling <code>strtok</code>.  It
returns a pointer to the first token.  What <code>strtok()</code> actually does is return
pointers to within the string you give it, and place <code>\0</code> bytes at the end of
each token.  We store each pointer in an array (buffer) of character
pointers.</p>

<p>Finally, we reallocate the array of pointers if necessary.  The process repeats
until no token is returned by <code>strtok</code>, at which point we null-terminate the
list of tokens.</p>

<p>So, once all is said and done, we have an array of tokens, ready to execute.
Which begs the question, how do we do that?</p>

<h2 id="how-shells-start-processes">How shells start processes</h2>

<p>Now, we’re really at the heart of what a shell does.  Starting processes is the
main function of shells.  So writing a shell means that you need to know exactly
what’s going on with processes and how they start.  That’s why I’m going to take
us on a short diversion to discuss processes in Unix-like operating systems.</p>

<p>There are only two ways of starting processes on Unix.  The first one (which
almost doesn’t count) is by being Init.  You see, when a Unix computer boots,
its kernel is loaded.  Once it is loaded and initialized, the kernel starts only
one process, which is called Init.  This process runs for the entire length of
time that the computer is on, and it manages loading up the rest of the
processes that you need for your computer to be useful.</p>

<p>Since most programs aren’t Init, that leaves only one practical way for
processes to get started: the <code>fork()</code> system call.  When this function is
called, the operating system makes a duplicate of the process and starts them
both running.  The original process is called the “parent”, and the new one is
called the “child”.  <code>fork()</code> returns 0 to the child process, and it returns to
the parent the process ID number (PID) of its child.  In essence, this means
that the only way for new processes is to start is by an existing one
duplicating itself.</p>

<p>This might sound like a problem.  Typically, when you want to run a new process,
you don’t just want another copy of the same program – you want to run a
different program.  That’s what the <code>exec()</code> system call is all about.  It
replaces the current running program with an entirely new one.  This means that
when you call <code>exec</code>, the operating system stops your process, loads up the new
program, and starts that one in its place.  A process never returns from an
<code>exec()</code> call (unless there’s an error).</p>

<p>With these two system calls, we have the building blocks for how most programs
are run on Unix.  First, an existing process forks itself into two separate
ones.  Then, the child uses <code>exec()</code> to replace itself with a new program.  The
parent process can continue doing other things, and it can even keep tabs on its
children, using the system call <code>wait()</code>.</p>

<p>Phew!  That’s a lot of information, but with all that background, the following
code for launching a program will actually make sense:</p>

<div><pre><code><span>int</span> <span>lsh_launch</span><span>(</span><span>char</span> <span>**</span><span>args</span><span>)</span>
<span>{</span>
  <span>pid_t</span> <span>pid</span><span>,</span> <span>wpid</span><span>;</span>
  <span>int</span> <span>status</span><span>;</span>

  <span>pid</span> <span>=</span> <span>fork</span><span>();</span>
  <span>if</span> <span>(</span><span>pid</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
    <span>// Child process</span>
    <span>if</span> <span>(</span><span>execvp</span><span>(</span><span>args</span><span>[</span><span>0</span><span>],</span> <span>args</span><span>)</span> <span>==</span> <span>-</span><span>1</span><span>)</span> <span>{</span>
      <span>perror</span><span>(</span><span>"lsh"</span><span>);</span>
    <span>}</span>
    <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
  <span>}</span> <span>else</span> <span>if</span> <span>(</span><span>pid</span> <span>&lt;</span> <span>0</span><span>)</span> <span>{</span>
    <span>// Error forking</span>
    <span>perror</span><span>(</span><span>"lsh"</span><span>);</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>// Parent process</span>
    <span>do</span> <span>{</span>
      <span>wpid</span> <span>=</span> <span>waitpid</span><span>(</span><span>pid</span><span>,</span> <span>&amp;</span><span>status</span><span>,</span> <span>WUNTRACED</span><span>);</span>
    <span>}</span> <span>while</span> <span>(</span><span>!</span><span>WIFEXITED</span><span>(</span><span>status</span><span>)</span> <span>&amp;&amp;</span> <span>!</span><span>WIFSIGNALED</span><span>(</span><span>status</span><span>));</span>
  <span>}</span>

  <span>return</span> <span>1</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Alright.  This function takes the list of arguments that we created earlier.
Then, it forks the process, and saves the return value.  Once <code>fork()</code> returns,
we actually have <em>two</em> processes running concurrently.  The child process will
take the first if condition (where <code>pid == 0</code>).</p>

<p>In the child process, we want to run the command given by the user.  So, we use
one of the many variants of the <code>exec</code> system call, <code>execvp</code>.  The different
variants of <code>exec</code> do slightly different things.  Some take a variable number of
string arguments.  Others take a list of strings.  Still others let you specify
the environment that the process runs with.  This particular variant expects a
program name and an array (also called a vector, hence the ‘v’) of string
arguments (the first one has to be the program name).  The ‘p’ means that
instead of providing the full file path of the program to run, we’re going to
give its name, and let the operating system search for the program in the path.</p>

<p>If the exec command returns -1 (or actually, if it returns at all), we know
there was an error.  So, we use <code>perror</code> to print the system’s error message,
along with our program name, so users know where the error came from.  Then, we
exit so that the shell can keep running.</p>

<p>The second condition (<code>pid &lt; 0</code>) checks whether <code>fork()</code> had an error.  If so,
we print it and keep going – there’s no handling that error beyond telling the
user and letting them decide if they need to quit.</p>

<p>The third condition means that <code>fork()</code> executed successfully. The parent
process will land here.  We know that the child is going to execute the process,
so the parent needs to wait for the command to finish running.  We use
<code>waitpid()</code> to wait for the process’s state to change.  Unfortunately,
<code>waitpid()</code> has a lot of options (like <code>exec()</code>).  Processes can change state in
lots of ways, and not all of them mean that the process has ended.  A process
can either exit (normally, or with an error code), or it can be killed by a
signal.  So, we use the macros provided with <code>waitpid()</code> to wait until either
the processes are exited or killed.  Then, the function finally returns a 1, as
a signal to the calling function that we should prompt for input again.</p>

<h2 id="shell-builtins">Shell Builtins</h2>

<p>You may have noticed that the <code>lsh_loop()</code> function calls <code>lsh_execute()</code>, but
above, we titled our function <code>lsh_launch()</code>.  This was intentional!  You see,
most commands a shell executes are programs, but not all of them.  Some of them
are built right into the shell.</p>

<p>The reason is actually pretty simple.  If you want to change directory, you need
to use the function <code>chdir()</code>.  The thing is, the current directory is a
property of a process.  So, if you wrote a program called <code>cd</code> that changed
directory, it would just change its own current directory, and then terminate.
Its parent process’s current directory would be unchanged.  Instead, the shell
process itself needs to execute <code>chdir()</code>, so that its own current directory is
updated.  Then, when it launches child processes, they will inherit that
directory too.</p>

<p>Similarly, if there was a program named <code>exit</code>, it wouldn’t be able to exit the
shell that called it.  That command also needs to be built into the shell.
Also, most shells are configured by running configuration scripts, like
<code>~/.bashrc</code>.  Those scripts use commands that change the operation of the shell.
These commands could only change the shell’s operation if they were implemented
within the shell process itself.</p>

<p>So, it makes sense that we need to add some commands to the shell itself.  The
ones I added to my shell are <code>cd</code>, <code>exit</code>, and <code>help</code>.  Here are their function
implementations below:</p>

<div><pre><code><span>/*
  Function Declarations for builtin shell commands:
 */</span>
<span>int</span> <span>lsh_cd</span><span>(</span><span>char</span> <span>**</span><span>args</span><span>);</span>
<span>int</span> <span>lsh_help</span><span>(</span><span>char</span> <span>**</span><span>args</span><span>);</span>
<span>int</span> <span>lsh_exit</span><span>(</span><span>char</span> <span>**</span><span>args</span><span>);</span>

<span>/*
  List of builtin commands, followed by their corresponding functions.
 */</span>
<span>char</span> <span>*</span><span>builtin_str</span><span>[]</span> <span>=</span> <span>{</span>
  <span>"cd"</span><span>,</span>
  <span>"help"</span><span>,</span>
  <span>"exit"</span>
<span>};</span>

<span>int</span> <span>(</span><span>*</span><span>builtin_func</span><span>[])</span> <span>(</span><span>char</span> <span>**</span><span>)</span> <span>=</span> <span>{</span>
  <span>&amp;</span><span>lsh_cd</span><span>,</span>
  <span>&amp;</span><span>lsh_help</span><span>,</span>
  <span>&amp;</span><span>lsh_exit</span>
<span>};</span>

<span>int</span> <span>lsh_num_builtins</span><span>()</span> <span>{</span>
  <span>return</span> <span>sizeof</span><span>(</span><span>builtin_str</span><span>)</span> <span>/</span> <span>sizeof</span><span>(</span><span>char</span> <span>*</span><span>);</span>
<span>}</span>

<span>/*
  Builtin function implementations.
*/</span>
<span>int</span> <span>lsh_cd</span><span>(</span><span>char</span> <span>**</span><span>args</span><span>)</span>
<span>{</span>
  <span>if</span> <span>(</span><span>args</span><span>[</span><span>1</span><span>]</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
    <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: expected argument to </span><span>\"</span><span>cd</span><span>\"\n</span><span>"</span><span>);</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>if</span> <span>(</span><span>chdir</span><span>(</span><span>args</span><span>[</span><span>1</span><span>])</span> <span>!=</span> <span>0</span><span>)</span> <span>{</span>
      <span>perror</span><span>(</span><span>"lsh"</span><span>);</span>
    <span>}</span>
  <span>}</span>
  <span>return</span> <span>1</span><span>;</span>
<span>}</span>

<span>int</span> <span>lsh_help</span><span>(</span><span>char</span> <span>**</span><span>args</span><span>)</span>
<span>{</span>
  <span>int</span> <span>i</span><span>;</span>
  <span>printf</span><span>(</span><span>"Stephen Brennan's LSH</span><span>\n</span><span>"</span><span>);</span>
  <span>printf</span><span>(</span><span>"Type program names and arguments, and hit enter.</span><span>\n</span><span>"</span><span>);</span>
  <span>printf</span><span>(</span><span>"The following are built in:</span><span>\n</span><span>"</span><span>);</span>

  <span>for</span> <span>(</span><span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>lsh_num_builtins</span><span>();</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>printf</span><span>(</span><span>"  %s</span><span>\n</span><span>"</span><span>,</span> <span>builtin_str</span><span>[</span><span>i</span><span>]);</span>
  <span>}</span>

  <span>printf</span><span>(</span><span>"Use the man command for information on other programs.</span><span>\n</span><span>"</span><span>);</span>
  <span>return</span> <span>1</span><span>;</span>
<span>}</span>

<span>int</span> <span>lsh_exit</span><span>(</span><span>char</span> <span>**</span><span>args</span><span>)</span>
<span>{</span>
  <span>return</span> <span>0</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>There are three parts to this code.  The first part contains <em>forward
declarations</em> of my functions.  A forward declaration is when you declare (but
don’t define) something, so that you can use its name before you define it.  The
reason I do this is because <code>lsh_help()</code> uses the array of builtins, and the
arrays contain <code>lsh_help()</code>.  The cleanest way to break this dependency cycle is
by forward declaration.</p>

<p>The next part is an array of builtin command names, followed by an array of
their corresponding functions.  This is so that, in the future, builtin commands
can be added simply by modifying these arrays, rather than editing a large
“switch” statement somewhere in the code.  If you’re confused by the declaration
of <code>builtin_func</code>, that’s OK!  I am too.  It’s an array of function pointers
(that take array of strings and return an int).  Any declaration involving
function pointers in C can get really complicated.  I still look up how function
pointers are declared myself!<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>Finally, I implement each function.  The <code>lsh_cd()</code> function first checks that
its second argument exists, and prints an error message if it doesn’t.  Then, it
calls <code>chdir()</code>, checks for errors, and returns.  The help function prints a
nice message and the names of all the builtins.  And the exit function returns
0, as a signal for the command loop to terminate.</p>

<h2 id="putting-together-builtins-and-processes">Putting together builtins and processes</h2>

<p>The last missing piece of the puzzle is to implement <code>lsh_execute()</code>, the
function that will either launch a builtin, or a process.  If you’re reading
this far, you’ll know that we’ve set ourselves up for a really simple function:</p>

<div><pre><code><span>int</span> <span>lsh_execute</span><span>(</span><span>char</span> <span>**</span><span>args</span><span>)</span>
<span>{</span>
  <span>int</span> <span>i</span><span>;</span>

  <span>if</span> <span>(</span><span>args</span><span>[</span><span>0</span><span>]</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
    <span>// An empty command was entered.</span>
    <span>return</span> <span>1</span><span>;</span>
  <span>}</span>

  <span>for</span> <span>(</span><span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>lsh_num_builtins</span><span>();</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>strcmp</span><span>(</span><span>args</span><span>[</span><span>0</span><span>],</span> <span>builtin_str</span><span>[</span><span>i</span><span>])</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
      <span>return</span> <span>(</span><span>*</span><span>builtin_func</span><span>[</span><span>i</span><span>])(</span><span>args</span><span>);</span>
    <span>}</span>
  <span>}</span>

  <span>return</span> <span>lsh_launch</span><span>(</span><span>args</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>All this does is check if the command equals each builtin, and if so, run it.
If it doesn’t match a builtin, it calls <code>lsh_launch()</code> to launch the process.
The one caveat is that <code>args</code> might just contain NULL, if the user entered an
empty string, or just whitespace.  So, we need to check for that case at the
beginning.</p>

<h2 id="putting-it-all-together">Putting it all together</h2>

<p>That’s all the code that goes into the shell.  If you’ve read along, you should
understand completely how the shell works.  To try it out (on a Linux machine),
you would need to copy these code segments into a file (<code>main.c</code>), and compile
it.  Make sure to only include one implementation of <code>lsh_read_line()</code>.  You’ll
need to include the following headers at the top.  I’ve added notes so that you
know where each function comes from.</p>

<ul>
  <li><code>#include &lt;sys/wait.h&gt;</code>
    <ul>
      <li><code>waitpid()</code> and associated macros</li>
    </ul>
  </li>
  <li><code>#include &lt;unistd.h&gt;</code>
    <ul>
      <li><code>chdir()</code></li>
      <li><code>fork()</code></li>
      <li><code>exec()</code></li>
      <li><code>pid_t</code></li>
    </ul>
  </li>
  <li><code>#include &lt;stdlib.h&gt;</code>
    <ul>
      <li><code>malloc()</code></li>
      <li><code>realloc()</code></li>
      <li><code>free()</code></li>
      <li><code>exit()</code></li>
      <li><code>execvp()</code></li>
      <li><code>EXIT_SUCCESS</code>, <code>EXIT_FAILURE</code></li>
    </ul>
  </li>
  <li><code>#include &lt;stdio.h&gt;</code>
    <ul>
      <li><code>fprintf()</code></li>
      <li><code>printf()</code></li>
      <li><code>stderr</code></li>
      <li><code>getchar()</code></li>
      <li><code>perror()</code></li>
    </ul>
  </li>
  <li><code>#include &lt;string.h&gt;</code>
    <ul>
      <li><code>strcmp()</code></li>
      <li><code>strtok()</code></li>
    </ul>
  </li>
</ul>

<p>Once you have the code and headers, it should be as simple as running <code>gcc -o
main main.c</code> to compile it, and then <code>./main</code> to run it.</p>

<p>Alternatively, you can get the code from
<a href="https://github.com/brenns10/lsh/tree/407938170e8b40d231781576e05282a41634848c">GitHub</a>.
That link goes straight to the current revision of the code at the time of this
writing– I may choose to update it and add new features someday in the future.
If I do, I’ll try my best to update this article with the details and
implementation ideas.</p>

<h2 id="wrap-up">Wrap up</h2>

<p>If you read this and wondered how in the world I knew how to use those system
calls, the answer is simple: man pages.  In <code>man 3p</code> there is thorough
documentation on every system call.  If you know what you’re looking for, and
you just want to know how to use it, the man pages are your best friend.  If you
don’t know what sort of interface the C library and Unix offer you, I would
point you toward the
<a href="http://pubs.opengroup.org/onlinepubs/9699919799/">POSIX Specification</a>,
specifically Section 13, “Headers”.  You can find each header and everything it
is required to define in there.</p>

<p>Obviously, this shell isn’t feature-rich.  Some of its more glaring omissions
are:</p>

<ul>
  <li>Only whitespace separating arguments, no quoting or backslash escaping.</li>
  <li>No piping or redirection.</li>
  <li>Few standard builtins.</li>
  <li>No globbing.</li>
</ul>

<p>The implementation of all of this stuff is really interesting, but way more than
I could ever fit into an article like this.  If I ever get around to
implementing any of them, I’ll be sure to write a follow-up about it.  But I’d
encourage any reader to try implementing this stuff yourself.  If you’re met
with success, drop me a line in the comments below, I’d love to see the code.</p>

<p>And finally, thanks for reading this tutorial (if anyone did).  I enjoyed
writing it, and I hope you enjoyed reading it.  Let me know what you think in
the comments!</p>

<p><strong>Edit:</strong> In an earlier version of this article, I had a couple nasty bugs in
<code>lsh_split_line()</code>, that just happened to cancel each other out.  Thanks to
/u/munmap on Reddit (and other commenters) for catching them!  Check
<a href="https://github.com/brenns10/lsh/commit/486ec6dcdd1e11c6dc82f482acda49ed18be11b5">this diff</a>
to see exactly what I did wrong.</p>

<p><strong>Edit 2:</strong> Thanks to user ghswa on GitHub for contributing some null checks for
  <code>malloc()</code> that I forgot.  He/she also pointed out that the
  <a href="http://pubs.opengroup.org/onlinepubs/9699919799/functions/getline.html">manpage</a>
  for <code>getline()</code> specifies that the first argument should be freeable, so
  <code>line</code> should be initialized to <code>NULL</code> in my <code>lsh_read_line()</code> implementation
  that uses <code>getline()</code>.</p>

<p><strong>Edit 3:</strong> It’s 2020 and we’re still finding bugs, this is why software is
hard. Credit to <a href="https://github.com/harishankarv">harishankarv</a> on Github, for
finding an issue with my “simple” implementation of <code>lsh_read_line()</code> that
depends on <code>getline()</code>. See <a href="https://github.com/brenns10/lsh/issues/14">this issue</a>
for details – the text of the blog is updated.</p>

<h4 id="footnotes">Footnotes</h4>





<hr>



  
  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI is Visa – Buttering up the government to retain a monopoly (250 pts)]]></title>
            <link>https://sherwood.news/tech/openai-is-visa/</link>
            <guid>42517260</guid>
            <pubDate>Thu, 26 Dec 2024 19:44:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sherwood.news/tech/openai-is-visa/">https://sherwood.news/tech/openai-is-visa/</a>, See on <a href="https://news.ycombinator.com/item?id=42517260">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="article-body-content"><p>OpenAI is on the verge of becoming the Visa of artificial intelligence. Visa’s success wasn’t just about building a payments network; it was about creating barriers that locked in customers and locked out competitors.<b> </b>And just as <span><a href="https://robinhood.com/us/en/stocks/V/"><span>Visa</span></a></span> faced threats from national payment networks and tech giants, OpenAI must contend with competitors like <span><a href="https://robinhood.com/us/en/stocks/GOOGL/"><span>Google</span></a></span>, <span><a href="https://robinhood.com/stocks/META"><span>Meta</span></a></span>, and <span><a href="https://robinhood.com/stocks/AMZN"><span>Amazon</span></a></span>.</p><p>In 1958, 60,000 Californians got a fully working credit card in the mail. It was the <a href="https://www.washingtonpost.com/archive/lifestyle/magazine/1994/11/04/the-day-the-credit-card-was-born/d42da27b-0437-4a67-b753-bf9b440ad6dc/" target="_blank" rel="noopener">first unsolicited credit-card drop,</a> and it led to massive fraud and delinquency problems. <span><a href="https://robinhood.com/us/en/stocks/BAC/"><span>Bank of America</span></a></span>, which ran the campaign, realized it had to build a payments network with account verification and fraud detection. The network it built and licensed to other banks eventually became Visa, which IPO’d in 2008 at a $44 billion valuation. Today the company is <a href="https://www.fool.com/research/largest-companies-by-market-cap/" target="_blank" rel="noopener">worth about $600 billion</a>.</p><p>But in the 2010s Visa faced numerous threats to its core business. Digital providers like <span><a href="https://robinhood.com/us/en/stocks/PYPL/"><span>PayPal</span></a></span> and well-capitalized businesses like <span><a href="https://robinhood.com/us/en/stocks/JPM/"><span>JPMorgan Chase</span></a></span> and Apple were poised to threaten Visa’s de facto monopoly on payment processing. On top of that, other developed countries were rolling out their own national real-time gross settlement programs, facilitating instant interbank transfers at scale for free. Payment processing had become commoditized.</p><p>What Visa did in response recently got it sued by the Justice Department, which accused Visa of using aggressive tactics<b> </b>with companies like <span><a href="https://robinhood.com/us/en/stocks/COST/"><span>Costco</span></a></span> and <span><a href="https://robinhood.com/us/en/stocks/AAPL/"><span>Apple</span></a></span> to guarantee that a competitive payment network would not develop, The Wall Street Journal <a href="https://www.wsj.com/finance/banking/visa-wanted-a-vast-empire-first-it-had-to-beat-back-its-foes-3b3067f3" target="_blank" rel="noopener">reported</a>. It also spent tens of millions of dollars lobbying Washington for more favorable payment regulations. That could help explain why it took the US until 2023 to launch <a href="https://www.barrons.com/articles/visa-mastercard-stock-fed-debit-bcfb6db3" target="_blank" rel="noopener">its own national payment network</a> (FedNow), whereas countries such as Poland launched theirs in 2012, Denmark in 2014, and India in 2016.&nbsp;</p><p>As Visa’s technological moat dried up, it built a legal moat, and there are already signs OpenAI is doing the same.</p><p>OpenAI’s revenue is projected to reach $100 billion by 2029, <a href="https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html" target="_blank" rel="noopener">according to The New York Times</a>, but there’s a major risk factor. The underlying technology behind its revenue growth is the large language model, or LLM, but similar to what happened with payment processing, such models will soon become so ubiquitous that they might as well be free. Earlier this month, OpenAI boss Sam Altman essentially <a href="https://www.youtube.com/watch?v=tn0XpTAD_8Q" target="_blank" rel="noopener">conceded</a> this API business would dry up: “There will be shockingly capable models widely available, used for everything… the AI itself — the reasoning engine — will become commoditized.”</p><p>Dozens of other companies, including Google, Meta, and most recently Amazon, have come out with their own foundational models. Some, including Meta’s Llama and Mistral’s 7B, are open source, meaning they can be downloaded and used in other companies’ products free of charge. Apple <a href="https://www.theverge.com/2024/11/21/24302685/apple-llm-siri-compete-chatgpt" target="_blank" rel="noopener">is rumored</a> to be working on an LLM that can fit on your iPhone.</p><p>To counteract these threats, OpenAI appears to be taking a page from Visa’s playbook. Last year, Altman “<a href="https://www.nytimes.com/2023/06/07/technology/sam-altman-ai-regulations.html" target="_blank" rel="noopener">stormed Washington</a>,” urging lawmakers to <a href="https://www.nytimes.com/2023/05/16/technology/openai-altman-artificial-intelligence-regulation.html" target="_blank" rel="noopener">regulate AI</a>. And, in OpenAI’s latest funding round, participating investors were asked not only to abstain from investing in competitors including Anthropic and SSI but <a href="https://www.reuters.com/technology/openai-tells-investor-not-invest-five-ai-startups-including-sutskevers-ssi-2024-10-02/" target="_blank" rel="noopener">not to fund any application-layer companies</a> such as Glean and Perplexity.</p><p>These efforts signal an attempt to dominate the market, not through superior technology, but by limiting competition through exclusivity deals, government contracts, and licensing requirements for advanced AI models.&nbsp;</p><p>OpenAI could prevent rivals from competing fairly through securing government deals that would mandate it as the arbiter of AI procurement; limiting competitors’ access to talent, chips, data centers, or energy through exclusivity arrangements with partners; or long-term exclusivity contracts with large customers (OpenAI already claims 92% of the Fortune 500 as customers, <a href="https://www.cnbc.com/2024/04/12/openais-altman-pitches-chatgpt-enterprise-to-large-firms-including-some-microsoft-customers.html" target="_blank" rel="noopener">according to CNBC</a>).</p><p>But this strategy may face political and competitive roadblocks. With Elon Musk emerging as Altman’s <a href="https://sherwood.news/business/musk-wants-the-court-to-slow-openais-roll/" target="_blank" rel="noopener">chief rival</a> and exerting influence through figures like White House AI czar David Sacks, government regulation is likely to relax rather than tighten. If OpenAI can’t build strong barriers to entry soon, it risks losing its edge in an increasingly crowded and democratized AI landscape. Either way, Visa and OpenAI seem to agree on one thing: that “<a href="https://www.wsj.com/articles/peter-thiel-competition-is-for-losers-1410535536" target="_blank" rel="noopener">competition is for losers</a>.”</p><p><i></i><a href="https://sherwood.news/tech/what-companys-past-reveals-the-future-of-openai/" target="_blank" rel="noopener"><i>Read the other arguments for OpenAI's future </i><b><i>here.</i></b></a></p><hr><p><i>Taylor Lorenz publishes </i><a href="https://www.usermag.co/" target="_blank" rel="noopener"><i>User Magazine</i></a><i>, a tech and online culture newsletter. She is the author of the book “Extremely Online: The Untold Story of Fame, Influence, and Power on the Internet” and host of the podcast Power User. She has previously written for The New York Times, The Atlantic, The Washington Post, and more. </i><br></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Write Your Own Virtual Machine (2022) (295 pts)]]></title>
            <link>https://www.jmeiners.com/lc3-vm/</link>
            <guid>42517164</guid>
            <pubDate>Thu, 26 Dec 2024 19:30:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jmeiners.com/lc3-vm/">https://www.jmeiners.com/lc3-vm/</a>, See on <a href="https://news.ycombinator.com/item?id=42517164">Hacker News</a></p>
Couldn't get https://www.jmeiners.com/lc3-vm/: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>