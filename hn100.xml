<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 14 Oct 2024 04:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Zero-latency SQLite storage in every Durable Object (141 pts)]]></title>
            <link>https://simonwillison.net/2024/Oct/13/zero-latency-sqlite-storage-in-every-durable-object/</link>
            <guid>41832547</guid>
            <pubDate>Sun, 13 Oct 2024 23:07:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2024/Oct/13/zero-latency-sqlite-storage-in-every-durable-object/">https://simonwillison.net/2024/Oct/13/zero-latency-sqlite-storage-in-every-durable-object/</a>, See on <a href="https://news.ycombinator.com/item?id=41832547">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p><strong><a href="https://blog.cloudflare.com/sqlite-in-durable-objects/">Zero-latency SQLite storage in every Durable Object</a></strong> (<a href="https://lobste.rs/s/kjx2vk/zero_latency_sqlite_storage_every" title="lobste.rs">via</a>) Kenton Varda introduces the next iteration of Cloudflare's <a href="https://developers.cloudflare.com/durable-objects/">Durable Object</a> platform, which recently upgraded from a key/value store to a full relational system based on SQLite.</p>
<p>For useful background on the first version of Durable Objects take a look at <a href="https://digest.browsertech.com/archive/browsertech-digest-cloudflares-durable/">Cloudflare's durable multiplayer moat</a> by Paul Butler, who digs into its popularity for building WebSocket-based realtime collaborative applications.</p>
<p>The new SQLite-backed Durable Objects is a fascinating piece of distributed system design, which advocates for a really interesting way to architect a large scale application.</p>
<p>The key idea behind Durable Objects is to colocate application logic with the data it operates on. A Durable Object comprises code that executes on the same physical host as the SQLite database that it uses, resulting in blazingly fast read and write performance.</p>
<p>How could this work at scale?</p>
<blockquote>
<p>A single object is inherently limited in throughput since it runs on a single thread of a single machine. To handle more traffic, you create more objects. This is easiest when different objects can handle different logical units of state (like different documents, different users, or different "shards" of a database), where each unit of state has low enough traffic to be handled by a single object</p>
</blockquote>
<p>Kenton presents the example of a flight booking system, where each flight can map to a dedicated Durable Object with its own SQLite database - thousands of fresh databases per airline per day.</p>
<p>Each DO has a unique name, and Cloudflare's network then handles routing requests to that object wherever it might live on their global network.</p>
<p>The technical details are fascinating. Inspired by <a href="https://litestream.io/">Litestream</a>, each DO constantly streams a sequence of WAL entries to object storage - batched every 16MB or every ten seconds. This also enables point-in-time recovery for up to 30 days through replaying those logged transactions.</p>
<p>To ensure durability within that ten second window, writes are also forwarded to five replicas in separate nearby data centers as soon as they commit, and the write is only acknowledged once three of them have confirmed it.</p>
<p>The JavaScript API design is interesting too: it's blocking rather than async, because the whole point of the design is to provide fast single threaded persistence operations:</p>
<div><pre><span>let</span> <span>docs</span> <span>=</span> <span>sql</span><span>.</span><span>exec</span><span>(</span><span>`</span>
<span>  SELECT title, authorId FROM documents</span>
<span>  ORDER BY lastModified DESC</span>
<span>  LIMIT 100</span>
<span>`</span><span>)</span><span>.</span><span>toArray</span><span>(</span><span>)</span><span>;</span>

<span>for</span> <span>(</span><span>let</span> <span>doc</span> <span>of</span> <span>docs</span><span>)</span> <span>{</span>
  <span>doc</span><span>.</span><span>authorName</span> <span>=</span> <span>sql</span><span>.</span><span>exec</span><span>(</span>
    <span>"SELECT name FROM users WHERE id = ?"</span><span>,</span>
    <span>doc</span><span>.</span><span>authorId</span><span>)</span><span>.</span><span>one</span><span>(</span><span>)</span><span>.</span><span>name</span><span>;</span>
<span>}</span></pre></div>

<p>This one of their examples deliberately exhibits the N+1 query pattern, because that's something SQLite is <a href="https://www.sqlite.org/np1queryprob.html">uniquely well suited to handling</a>.</p>
<p>The system underlying Durable Objects is called Storage Relay Service, and it's been powering Cloudflare's existing-but-different <a href="https://developers.cloudflare.com/d1/">D1 SQLite system</a> for over a year.</p>
<p>I was curious as to where the objects are created. <a href="https://developers.cloudflare.com/durable-objects/reference/data-location/#provide-a-location-hint">According to this</a> (via <a href="https://news.ycombinator.com/item?id=41832547#41832812">Hacker News</a>):</p>
<blockquote>
<p>Durable Objects do not currently change locations after they are created. By default, a Durable Object is instantiated in a data center close to where the initial <code>get()</code> request is made. [...] To manually create Durable Objects in another location, provide an optional <code>locationHint</code> parameter to <code>get()</code>.</p>
</blockquote>
<p>And in a footnote:</p>
<blockquote>
<p>Dynamic relocation of existing Durable Objects is planned for the future.</p>
</blockquote>
<p><a href="https://where.durableobjects.live/">where.durableobjects.live</a> is a neat site that tracks where in the Cloudflare network DOs are created - I just visited it and it said:</p>
<blockquote>
<p>This page tracks where new Durable Objects are created; for example, when you loaded this page from <strong>Half Moon Bay</strong>, a worker in <strong>San Jose, California, United States (SJC)</strong> created a durable object in <strong>San Jose, California, United States (SJC)</strong>.</p>
</blockquote>
<p><img alt="Where Durable Objects Live.    Created by the wonderful Jed Schmidt, and now maintained with ❤️ by Alastair. Source code available on Github.    Cloudflare Durable Objects are a novel approach to stateful compute based on Cloudflare Workers. They aim to locate both compute and state closest to end users.    This page tracks where new Durable Objects are created; for example, when you loaded this page from Half Moon Bay, a worker in San Jose, California, United States (SJC) created a durable object in Los Angeles, California, United States (LAX).    Currently, Durable Objects are available in 11.35% of Cloudflare PoPs.    To keep data fresh, this application is constantly creating/destroying new Durable Objects around the world. In the last hour, 394,046 Durable Objects have been created(and subsequently destroyed), FOR SCIENCE!    And a map of the world showing lots of dots." src="https://static.simonwillison.net/static/2024/where-durable-objects.jpg"></p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introducing Our New Name (105 pts)]]></title>
            <link>https://blog.minetest.net/2024/10/13/Introducing-Our-New-Name/</link>
            <guid>41832215</guid>
            <pubDate>Sun, 13 Oct 2024 22:22:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.minetest.net/2024/10/13/Introducing-Our-New-Name/">https://blog.minetest.net/2024/10/13/Introducing-Our-New-Name/</a>, See on <a href="https://news.ycombinator.com/item?id=41832215">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
		

		<p>“Is this a Minecraft clone? Is it like Minecraft Alpha?” If you’ve been a member of our community for some time, you are
probably aware of how often these questions remind us of what people think of when they hear the name “Minetest”: a
rip-off of a similar genre-defining game. Truth be told, that was celeron55’s initial plan. But things change, and
Minetest evolved. Now, the time has come for Minetest to assume a new identity and prove it has moved beyond its
original purpose.</p>

<!-- more -->

<h2 id="a-bit-of-history">A Bit of History</h2>

<p>After seeing Minecraft’s alpha debut, celeron55 “took it as a challenge” to explore “the mystery of how a game world
like this works under the hood”<sup id="fnref:wikinews" role="doc-noteref"><a href="#fn:wikinews" rel="footnote">1</a></sup>. The name wasn’t very important; it was a test after all— an experiment to
see what it could do differently. An experiment that began attracting more users and contributors throughout the years
until Minetest eventually evolved from a game to what it is today: a game platform.</p>

<p>Through the efforts of the community over the past decade, Minetest has developed a unique identity with an approachable
API and easy scripting language, support for community-created games and mods, a central library to easily find
community content, and more recently a shift to promoting community creations instead of the original Minetest Game.
Despite these advancements, one anachronistic holdover has remained to remind us of the past: an idea that no longer
describes the purpose of this project and community— its name.</p>

<p>After more than a year of public and internal discussions, that era is over. “Minetest” is no more; the “Minecraft
clone” is no more. With great thanks to the community for their input, we wave goodbye to Minetest and introduce a new
era. Welcome 🥁 … <strong>Luanti!</strong></p>

<h2 id="luanwhat">Luanwhat?</h2>

<p>“Luanti” is a wordplay on the Finnish word <em>luonti</em> (“creation”) and the programming language <del>Minetest</del> Luanti employs
for games and mods, <em>Lua</em>. The goal was to avoid yet another plain English word (good luck finding something unique…)
and highlight the core principles of the project. The fusion of celeron55’s Finnish nationality and the platform’s focus
on content creation resulted in the birth of “Luanti”.</p>

<p>We decided to avoid using “free” or “libre” in the name because we don’t think it does the project justice. Luanti will
always be free software, but that core principle is not everything it has to offer. Projects like Blender, Krita, or
Godot are awesome, and they don’t need to convince you about their libre nature by putting it in their names. They are
libre, but they’re also much, much more!</p>

<p>Luanti is a lot of things. Many of us are familiar with the voxel nature of the engine and the library of content it
supports, and name suggestions related to that idea were certainly plentiful. But describing that in the new name would
limit the platform in the same way as the original: users and developers would come to expect it to be an engine
exclusive to cube games when it can be more than that. Luanti is a platform that is built to help you create. It’s a
platform where anyone can make something. It’s a platform that will keep evolving to support your creations.</p>

<h2 id="moving-forward">Moving Forward</h2>

<p>With the new name, a few changes will be made in the engine and community. Obviously, all the repositories and community
hubs will adopt the Luanti name in some form. You’ll be able to find the website at luanti.org and repositories at
<a href="https://github.com/luanti-org">github.com/luanti-org</a> (many old links will redirect). Our social platforms will change
similarly. The logo will be staying, and you’ll probably still hear “Minetest” occasionally in reference to <a href="https://content.minetest.net/packages/Minetest/minetest_game/">Minetest
Game</a> which will remain a testament to the project’s
roots. Otherwise, Luanti now represents the future of the platform.</p>

<p>For developers, the engine won’t actually change much. For those who aren’t aware, <code>core</code> is actually the real Lua
namespace used internally, <code>minetest</code> is just an alias (which we will keep for compatibility). Rather than rename all
the namespaces to <code>luanti</code>, <code>core</code> will be the official namespace to use. It’s shorter to type, easier to remember, and
backwards-compatible. It’s also friendlier to forks and future name changes (which we don’t plan to do, promise!)</p>

<p>We hope that, free from the ghost of its past, Luanti can bloom into something more and bring life to adventures and
experiences for years to come. It can now live and breathe on its own, explore its nature and potential, and overcome
its limits. At the very least, we can finally say to those who once asked “Is this a Minecraft clone?”: No, not anymore.
It came from humble beginnings and has grown into a powerful game platform where you, and anyone else, can start
creating. And with your help, it will continue to grow into something even better.</p>

<p>What do you think about the new name? Let us know in the forum thread or on any of our community platforms!</p>

<hr>

<h5 id="sources">Sources:</h5>




		
			<p>
				
					Thanks to our post contributors this month:
					Zughy, GreenXenith, FreshReplicant.
				
				
					Cover image by Zughy.
				
			</p>
		
	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CRLF is obsolete and should be abolished (331 pts)]]></title>
            <link>https://fossil-scm.org/home/ext/crlf-harmful.md</link>
            <guid>41830717</guid>
            <pubDate>Sun, 13 Oct 2024 19:16:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fossil-scm.org/home/ext/crlf-harmful.md">https://fossil-scm.org/home/ext/crlf-harmful.md</a>, See on <a href="https://news.ycombinator.com/item?id=41830717">Hacker News</a></p>
Couldn't get https://fossil-scm.org/home/ext/crlf-harmful.md: Error: Parse Error: Invalid header value char]]></description>
        </item>
        <item>
            <title><![CDATA[Making the Tibetan language a first-class citizen in the digital world (183 pts)]]></title>
            <link>https://www.bdrc.io/blog/2024/10/10/tech-innovations-to-make-the-tibetan-language-a-first-class-citizen-in-the-digital-world/</link>
            <guid>41829926</guid>
            <pubDate>Sun, 13 Oct 2024 17:43:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bdrc.io/blog/2024/10/10/tech-innovations-to-make-the-tibetan-language-a-first-class-citizen-in-the-digital-world/">https://www.bdrc.io/blog/2024/10/10/tech-innovations-to-make-the-tibetan-language-a-first-class-citizen-in-the-digital-world/</a>, See on <a href="https://news.ycombinator.com/item?id=41829926">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								<article id="post-11685">
			<div>
					<p>
						
                        						<h2 itemprop="name"><span itemprop="dateCreated">October 10, 2024</span></h2>
						
					</p>
				</div>
			<div>
        <div><p><img decoding="async" itemprop="image" src="https://www.bdrc.io/wp-content/uploads/2024/10/0001-scaled-1.jpg" alt=""></p><p><span></span><i><span>Om Mani Padme Hum mantra scroll printed by Schilling von Canstadt in 1835 in St Petersburg, for use in monasteries in Buryatia. </span></i></p></div><div>
			<p><span>An important part of BDRC's mission is technological innovation to make the Tibetan language a first-class citizen in the digital world. A major milestone in that enterprise was passed recently, as the open source software suite LibreOffice now supports an important feature of Tibetan: very long paragraphs.</span></p>
<p><span>Tibetans have invested tremendous energy and resources into writing, innovation, and technology since the introduction&nbsp; of Buddhism in the eighth century. Many of you will know the story of how the Tibetan script and classical language were invented for the purpose of translating Buddhist texts from Sanskrit (and Chinese) into a language Tibetans would understand. In the 14th Century Tibetans widely adopted the technology of woodblock printing as a means of mass producing the Tibetan translation of the scriptures and also the thousands of volumes of Buddhist texts written directly in Tibetan by Himalayan authors. Fast forwarding to more recent times, the creation of Tibetan computer fonts was a quantum leap in the evolution of Tibetan and its integration in the digital world. Of particular note was the inclusion of Tibetan in the Unicode Standard, the global standard for the codes that underlie computer fonts for all of the world's writing systems.&nbsp;</span></p>
<p><span>However, written Tibetan has important features that are still not supported by all tools or applications. One that may be surprising is very long paragraphs: in fact, the typographical notion of the paragraph does not really exist in a Tibetan text the way it does in European languages. As a result, Tibetan texts often need to be processed as a long stream of uninterrupted text with no forced line breaks, sometimes over hundreds or thousands of pages.&nbsp;</span></p>

		</div><div><div><p><img decoding="async" itemprop="image" src="https://www.bdrc.io/wp-content/uploads/2024/10/c5MCRRhVCKIYkaOtm9FfAA-copy.jpg" alt=""></p></div><div><p><img decoding="async" itemprop="image" src="https://www.bdrc.io/wp-content/uploads/2024/10/mantra-roll.webp" alt=""></p><p><span></span><i><span>Examples of long Tibetan scrolls of unbroken text that are placed inside prayer wheels.&nbsp;</span></i></p></div></div><div>	


	<div>
			<p><span>In general, word processing software programs were originally designed with English text in mind, meaning that they support relatively short paragraphs (possibly up to a few pages), with words separated by spaces that can have a flexible width. These assumptions don't work for written Tibetan, however, where paragraphs are virtually limitless and contain very few spaces. The result is that such a word processor performs very poorly, if it works at all, when opening a long Tibetan text, and it will therefore be impossible to use for any serious publication project.&nbsp;</span></p>
<p><span>LibreOffice is one of the only mature and stable open source word processors (i.e. inspired by MS Word) that is available for free on all possible platforms, including Linux. It is relevant to Tibetan Studies and the Tibetan community because it is free and open source, while popular commercial software programs such as Word or InDesign which can handle long Tibetan texts, are expensive and in many parts of Asia often used in pirated versions. So long as LibreOffice could not handle long paragraphs there was essentially no free tool to publish Tibetan.&nbsp;</span></p>
<p><span>In 2015, BDRC's CTO, Elie Roux, reported the issue to LibreOffice</span><span>. Unfortunately an intervention in the code of LibreOffice is a big project that would have required weeks of research and development, and therefore there was no evolution on that front. That is until a few weeks ago, when a developer named Jonathan Clark tackled the issue and fixed it! A long text like Longchenpa's Yishindzö (view on the BDRC archive at: <a href="http://purl.bdrc.io/resource/WA00KG02677">yid bzhin mdzod</a>), which consists of one "paragraph" of 153 letter size pages, now opens quickly and can be edited in LibreOffice.</span><span> The conversion of the RDF file of this particular text into a PDF used to stall after 45 minutes and it is now completed in 13 seconds! This is a remarkable improvement of several orders of magnitude, something quite rare in software optimization.</span></p>

		</div> 	

<div><p><img decoding="async" itemprop="image" src="https://www.bdrc.io/wp-content/uploads/2024/10/Untitled-design.png" alt=""></p>
<p><em><span>A long text like Longchenpa's Yishindzö, which consists of one "paragraph" of 153 letter size pages, now opens quickly and can be edited in LibreOffice. (View on the BDRC archive at: <a href="http://purl.bdrc.io/resource/WA00KG02677">yid bzhin mdzod</a>).</span></em></p>
</div></div><div>
			<p><span>BDRC's modest contribution to this breakthrough is just to have planted the seed many years ago, but we want to acknowledge and rejoice in Jonathan's good work, and the consequent strengthening of Tibetan in open source publication tools!</span></p>
<p><span>The support for very long paragraphs was integrated in <a href="https://www.libreoffice.org/download/download-libreoffice/">LibreOffice 24.8.2</a>, released on September 27 2024. We encourage you to try it out and send us comments on your favorite software to edit Tibetan, or the most important shortcomings you experience in this type of tool for Tibetan. Working together as a community, we can continue to innovate and keep Tibetan up to speed with the digital technology of other languages.&nbsp;</span></p>

		</div>
       </div>

			
	    	</article>													
 
 						                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Restic: Backups done right (167 pts)]]></title>
            <link>https://restic.net/</link>
            <guid>41829913</guid>
            <pubDate>Sun, 13 Oct 2024 17:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restic.net/">https://restic.net/</a>, See on <a href="https://news.ycombinator.com/item?id=41829913">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <h2 id="introduction">Introduction</h2>
<p>Restic is a modern backup program that can back up your files:</p>
<ul>
<li>
<p>from <strong>Linux, BSD, Mac and Windows</strong></p>
</li>
<li>
<p>to <strong>many different storage types</strong>, including self-hosted and online services</p>
</li>
<li>
<p><strong>easily</strong>, being a single executable that you can run without a server or complex setup</p>
</li>
<li>
<p><strong>effectively</strong>, only transferring the parts that actually changed in the files you back up</p>
</li>
<li>
<p><strong>securely</strong>, by careful use of cryptography in every part of the process</p>
</li>
<li>
<p><strong>verifiably</strong>, enabling you to make sure that your files can be restored when needed</p>
</li>
<li>
<p><strong>freely</strong> - restic is entirely free to use and completely open source</p>
</li>
</ul>
<h2 id="quickstart">Quickstart</h2>
<p>A short recorded demo of restic:</p>

<p>To learn more about restic, checkout the user manual:</p>
<ul>
<li><a href="https://restic.readthedocs.io/en/stable/">Manual for restic (released version)</a></li>
<li><a href="https://restic.readthedocs.io/en/latest/">Manual for restic (latest development version)</a></li>
</ul>
<h2 id="installation">Installation</h2>
<p>To install, please follow the <a href="https://restic.readthedocs.io/en/stable/020_installation.html">Installation Instructions Page</a> in the manual or download the latest native binary on the <a href="https://github.com/restic/restic/releases/latest">GitHub Download Page</a>.</p>
<h2 id="compatibility">Compatibility</h2>
<p>Backward compatibility for backups is important so that our users are always able to restore saved data. Therefore restic follows <a href="http://semver.org/">Semantic Versioning</a> to clearly define which versions are compatible. The repository and data structures contained therein are considered the “Public API” in the sense of Semantic Versioning.</p>
<p>Once version 1.0.0 is released, we guarantee backward compatibility of all repositories within one major version; as long as we do not increment the major version, data can be read and restored. We strive to be fully backward compatible to all prior versions.</p>
<p>During initial development (versions prior to 1.0.0), maintainers and developers will do their utmost to keep backwards compatibility and stability, although there might be breaking changes without increasing the major version.</p>
<h2 id="contributing">Contributing</h2>
<p>Contributions are welcome! More information can be found in <a href="https://github.com/restic/restic/blob/master/CONTRIBUTING.md">the restic contribution guidelines</a>. A document describing the design of restic and the data structures stored on disc is contained in <a href="http://restic.readthedocs.io/en/latest/100_references.html#design">the design document</a>.</p>

<p>There are several ways to contact the restic project and its community:</p>
<ul>
<li>If you have <strong>usage or support questions</strong>, please post in <a href="https://forum.restic.net/">the <strong>forum</strong></a>.</li>
<li>If you discover a <strong>bug</strong> or have a <strong>feature</strong> suggestion, feel free to <a href="https://github.com/restic/restic/issues/new/choose">open a <strong>GitHub issue</strong></a>. Please make sure to fill out the issue template you are presented with when doing so.</li>
<li>If you would like to <strong>chat</strong> about restic with other users there is also the IRC channel <code>#restic</code> on <code>irc.libera.chat</code>, which you can <a href="https://web.libera.chat/gamja/#restic">access through your browser</a> if you don’t have an IRC client installed. Please note that support questions are preferably asked in the forum.</li>
<li>For <strong>other project related inquiries</strong> (<strong>not</strong> support requests), feel free to just write an e-mail to <code>alexander@bumpern.de</code> :)</li>
</ul>
<p><strong>Important</strong>: If you discover something that you believe to be a possible critical security problem, please do <strong>not</strong> open a GitHub issue but send an email directly to <code>alexander@bumpern.de</code>. If possible, please encrypt your email using PGP (<a href="https://restic.net/gpg-key-alex.asc">CF8F18F2844575973F79D4E191A6868BD3F7A907</a>).</p>
<h2 id="talks">Talks</h2>
<p>The following talks have been given about restic:</p>
<ul>
<li>2021-04-02: <a href="https://changelog.com/podcast/434">The Changelog: Restic has your backup (Podcast)</a></li>
<li>2016-01-31: Lightning Talk at the Go Devroom at FOSDEM 2016, Brussels, Belgium</li>
<li>2016-01-29: <a href="https://media.ccc.de/v/c4.openchaos.2016.01.restic">restic - Backups mal richtig</a>: Public lecture in German at <a href="https://koeln.ccc.de/">CCC Cologne e.V.</a> in Cologne, Germany</li>
<li>2015-08-23: <a href="https://media.ccc.de/browse/conferences/froscon/2015/froscon2015-1515-a_solution_to_the_backup_inconvenience.html">A Solution to the Backup Inconvenience</a>: Lecture at <a href="https://www.froscon.de/">FROSCON 2015</a> in Bonn, Germany</li>
<li>2015-02-01: <a href="https://www.youtube.com/watch?v=oM-MfeflUZ8&amp;t=11m40s">Lightning Talk at FOSDEM 2015</a>: A short introduction (with slightly outdated command line)</li>
<li>2015-01-27: <a href="https://video.fsmpi.rwth-aachen.de/cccac/4442">Talk about restic at CCC Aachen</a> (in German)</li>
</ul>
<h2 id="blog">Blog</h2>
<p>For more information regarding restic development, have a look at <a href="https://restic.net/blog">our blog</a>. The latest posts are:</p>
<ul>
<li>
<p>05 Sep 2024 » <a href="https://restic.net/blog/2024-09-05/restic-0.17.1-released/">Restic 0.17.1 Released</a></p>
</li>
<li>
<p>26 Jul 2024 » <a href="https://restic.net/blog/2024-07-26/restic-0.17.0-released/">Restic 0.17.0 Released</a></p>
</li>
<li>
<p>26 Jul 2024 » <a href="https://restic.net/blog/2024-07-26/rest-server-0.13.0-released/">REST-server 0.13.0 released</a></p>
</li>
</ul>
<h2 id="license">License</h2>
<p>Restic is licensed under “BSD 2-Clause License”. You can find the complete text in the file <code>LICENSE</code>.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ACF Plugin no longer available on WordPress.org (184 pts)]]></title>
            <link>https://www.advancedcustomfields.com/blog/acf-plugin-no-longer-available-on-wordpress-org/</link>
            <guid>41828958</guid>
            <pubDate>Sun, 13 Oct 2024 15:44:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.advancedcustomfields.com/blog/acf-plugin-no-longer-available-on-wordpress-org/">https://www.advancedcustomfields.com/blog/acf-plugin-no-longer-available-on-wordpress-org/</a>, See on <a href="https://news.ycombinator.com/item?id=41828958">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrap">

			<p>We were saddened and appalled by Matt Mullenweg’s actions this morning appropriating the Advanced Custom Fields plugin that our ACF team has been actively developing for the WordPress community since 2011.</p>

<p>Advanced Custom Fields is a sophisticated plugin with over 200,000 lines of code, which we continually develop, enhance, support and invest in to meet the needs of our users across WordPress.&nbsp; We’ve made 15+ releases over the past two years, since joining WP Engine, and added significant new functionality to the free plugin as well as continually improving performance and our security and testing practices to meet the ‘enterprise grade’ that our users deserve.</p>

<p>The change to our published distribution, and under our ‘slug’ which uniquely identifies the ACF plugin and code that our users trust in the WordPress.org plugin repository, is inconsistent with open source values and principles.&nbsp; The change made by Mullenweg is maliciously being used to update millions of existing installations of ACF with code that is unapproved and untrusted by the Advanced Custom Fields team.</p>

<p>We are directly able to protect WP Engine, Flywheel hosting and ACF PRO customers –&nbsp; you are not impacted and do not need to take any action. You will continue to get the latest innovations and updates from the experts in the ACF team. The ACF code on wordpress.org is no longer controlled by the ACF team.</p>

<blockquote data-width="500" data-dnt="true"><div lang="en" dir="ltr"><p>We have been made aware that the Advanced Custom Fields plugin on the WordPress directory has been taken over by WordPress dot org.</p><p>A plugin under active development has never been unilaterally and forcibly taken away from its creator without consent in the 21 year history of… <a href="https://t.co/eV0qakURLc">pic.twitter.com/eV0qakURLc</a></p></div>— Advanced Custom Fields (@wp_acf) <a href="https://twitter.com/wp_acf/status/1845169499064107049?ref_src=twsrc%5Etfw">October 12, 2024</a></blockquote>

<p>If you have a site managed elsewhere using the free version of ACF, in order to get genuine ACF updates you <a href="https://www.advancedcustomfields.com/blog/installing-and-upgrading-to-the-latest-version-of-acf/">must perform a one-time download</a> of the 6.3.8 version via advancedcustomfields.com to remain safe in the future. After this one-time download you will be able to update as usual via the WP Admin panel.</p>

<p>You can also follow the same process if your site has already been upgraded to the modified “Secure Custom Fields” plugin, to get back to a genuine version of ACF.</p>

<p>Mullenweg’s actions are extraordinarily concerning and pose the grave risk of upending and irreparably harming the entire WordPress ecosystem.&nbsp; His attempt to unilaterally take control of this open platform that we and so many other plugin developers and contributors have relied on, in the spirit of sharing plugins for all, provides further evidence of his serious abuse of trust, manifold conflicts of interest, and breach of the promises of openness and integrity in the community.</p>


			<h2>About the Author</h2>
<div>
	<p><img alt="" src="https://secure.gravatar.com/avatar/30ac8d2fc7fd828088a702342a3f3cea?s=96&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/30ac8d2fc7fd828088a702342a3f3cea?s=192&amp;d=mm&amp;r=g 2x" height="96" width="96" decoding="async">	</p>
	<p>Iain is the Product Manager for Advanced Custom Fields. He has a long history of building and growing WordPress plugins. Moonlights as a PhpStorm evangelist.</p>
</div>

			

							<p><i>For plugin support, please <a href="https://www.advancedcustomfields.com/contact/">contact our support team</a> directly, as comments aren't actively monitored.</i></p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ward Christensen (of BBS and XMODEM fame) has died (234 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Ward_Christensen</link>
            <guid>41828923</guid>
            <pubDate>Sun, 13 Oct 2024 15:39:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Ward_Christensen">https://en.wikipedia.org/wiki/Ward_Christensen</a>, See on <a href="https://news.ycombinator.com/item?id=41828923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">
<table><tbody><tr><th colspan="2"><p>Ward Christensen</p></th></tr><tr><td colspan="2"><span typeof="mw:File/Frameless"><a href="https://en.wikipedia.org/wiki/File:Ward_Christensen_and_the_First_BBS.jpg"><img alt="Ward Christensen and the First BBS" src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Ward_Christensen_and_the_First_BBS.jpg/220px-Ward_Christensen_and_the_First_BBS.jpg" decoding="async" width="220" height="105" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Ward_Christensen_and_the_First_BBS.jpg/330px-Ward_Christensen_and_the_First_BBS.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Ward_Christensen_and_the_First_BBS.jpg/440px-Ward_Christensen_and_the_First_BBS.jpg 2x" data-file-width="2048" data-file-height="975"></a></span><p>Ward Christensen and the First BBS</p></td></tr><tr><th scope="row">Born</th><td>October 23, 1945<br><div><p><a href="https://en.wikipedia.org/wiki/West_Bend,_Wisconsin" title="West Bend, Wisconsin">West Bend, Wisconsin</a>, United States</p></div></td></tr><tr><th scope="row">Died</th><td>October 11, 2024 (aged&nbsp;78)<br><div><p><a href="https://en.wikipedia.org/wiki/Rolling_Meadows,_Illinois" title="Rolling Meadows, Illinois">Rolling Meadows, Illinois</a>, United States</p></div></td></tr><tr><th scope="row">Known&nbsp;for</th><td>first <a href="https://en.wikipedia.org/wiki/Bulletin_board_system" title="Bulletin board system">bulletin board system</a> (BBS)<br><a href="https://en.wikipedia.org/wiki/XMODEM" title="XMODEM">XMODEM</a> Protocol</td></tr></tbody></table>
<p><b>Ward Christensen</b> (born 1945 in <a href="https://en.wikipedia.org/wiki/West_Bend,_Wisconsin" title="West Bend, Wisconsin">West Bend, Wisconsin</a>, United States) was the co-founder of the <a href="https://en.wikipedia.org/wiki/CBBS" title="CBBS">CBBS</a> bulletin board, the first <a href="https://en.wikipedia.org/wiki/Bulletin_board_system" title="Bulletin board system">bulletin board system</a> (BBS) ever brought online.<sup id="cite_ref-1"><a href="#cite_note-1"><span>[</span>1<span>]</span></a></sup> Christensen, along with partner <a href="https://en.wikipedia.org/wiki/Randy_Suess" title="Randy Suess">Randy Suess</a>,<sup id="cite_ref-2"><a href="#cite_note-2"><span>[</span>2<span>]</span></a></sup> members of the Chicago Area Computer Hobbyists' Exchange (CACHE), started development during a blizzard in <a href="https://en.wikipedia.org/wiki/Chicago" title="Chicago">Chicago</a>, <a href="https://en.wikipedia.org/wiki/Illinois" title="Illinois">Illinois</a>, and officially established CBBS four weeks later, on February 16, 1978. CACHE members frequently shared programs and had long been discussing some form of file transfer, and the two used the downtime during the blizzard to implement it.<sup id="cite_ref-3"><a href="#cite_note-3"><span>[</span>3<span>]</span></a></sup><sup id="cite_ref-4"><a href="#cite_note-4"><span>[</span>4<span>]</span></a></sup><sup id="cite_ref-5"><a href="#cite_note-5"><span>[</span>5<span>]</span></a></sup>
</p><p>Christensen was noted for building <a href="https://en.wikipedia.org/wiki/Software" title="Software">software</a> tools for his needs. He wrote a cassette-based <a href="https://en.wikipedia.org/wiki/Operating_system" title="Operating system">operating system</a> before <a href="https://en.wikipedia.org/wiki/Floppy_disk" title="Floppy disk">floppies</a> and <a href="https://en.wikipedia.org/wiki/Hard_disk_drive" title="Hard disk drive">hard disks</a> were common. When he lost track of the <a href="https://en.wikipedia.org/wiki/Source_code" title="Source code">source code</a> for some <a href="https://en.wikipedia.org/wiki/Computer_program" title="Computer program">programs</a>, he wrote ReSource, an iterative <a href="https://en.wikipedia.org/wiki/Disassembler" title="Disassembler">disassembler</a> for the <a href="https://en.wikipedia.org/wiki/Intel_8080" title="Intel 8080">Intel 8080</a>, to help him regenerate the <a href="https://en.wikipedia.org/wiki/Source_code" title="Source code">source code</a>. When he needed to send files to Randy Suess, he wrote <a href="https://en.wikipedia.org/wiki/XMODEM" title="XMODEM">XMODEM</a>.
</p><p><a href="https://en.wikipedia.org/wiki/Jerry_Pournelle" title="Jerry Pournelle">Jerry Pournelle</a> wrote in 1983 of a collection of <a href="https://en.wikipedia.org/wiki/CP/M" title="CP/M">CP/M</a> <a href="https://en.wikipedia.org/wiki/Public-domain_software" title="Public-domain software">public-domain software</a> that "probably 50 percent of the really good programs were written by Ward Christensen, a public benefactor."<sup id="cite_ref-pournelle198307_6-0"><a href="#cite_note-pournelle198307-6"><span>[</span>6<span>]</span></a></sup> Christensen received two 1992 <a href="https://en.wikipedia.org/wiki/Dvorak_Awards" title="Dvorak Awards">Dvorak Awards for Excellence in Telecommunications</a>, one with Randy Suess for developing the first BBS, and a lifetime achievement award "for outstanding contributions to PC telecommunications."<sup id="cite_ref-7"><a href="#cite_note-7"><span>[</span>7<span>]</span></a></sup> In 1993, he received the <a href="https://en.wikipedia.org/wiki/EFF_Pioneer_Award" title="EFF Pioneer Award">Pioneer Award</a> from the <a href="https://en.wikipedia.org/wiki/Electronic_Frontier_Foundation" title="Electronic Frontier Foundation">Electronic Frontier Foundation</a>.<sup id="cite_ref-8"><a href="#cite_note-8"><span>[</span>8<span>]</span></a></sup>
</p><p>Christensen worked at <a href="https://en.wikipedia.org/wiki/IBM" title="IBM">IBM</a> from 1968<sup id="cite_ref-9"><a href="#cite_note-9"><span>[</span>9<span>]</span></a></sup> until his retirement in 2012.  His last position with IBM was field technical sales specialist.
</p><p>In May 2005, Christensen and Suess were both featured in <i><a href="https://en.wikipedia.org/wiki/BBS:_The_Documentary" title="BBS: The Documentary">BBS: The Documentary</a></i>.<sup id="cite_ref-10"><a href="#cite_note-10"><span>[</span>10<span>]</span></a></sup>
</p>

<div><ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite id="CITEREFZelchenko1998">Zelchenko, Peter (30 October 1998). <a rel="nofollow" href="https://www.chicagotribune.com/news/ct-xpm-1998-10-30-9901080059-story.html">"Jack Rickard, editor of Boardwatch magazine, saw it coming"</a>. <i><a href="https://en.wikipedia.org/wiki/Chicago_Tribune" title="Chicago Tribune">Chicago Tribune</a></i><span>. Retrieved <span>8 October</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Chicago+Tribune&amp;rft.atitle=Jack+Rickard%2C+editor+of+Boardwatch+magazine%2C+saw+it+coming&amp;rft.date=1998-10-30&amp;rft.aulast=Zelchenko&amp;rft.aufirst=Peter&amp;rft_id=https%3A%2F%2Fwww.chicagotribune.com%2Fnews%2Fct-xpm-1998-10-30-9901080059-story.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span><cite id="CITEREFMetz2019">Metz, Cade (2019-12-20). <a rel="nofollow" href="https://www.nytimes.com/2019/12/20/technology/randy-suess-dead.html">"Randy Suess, Computer Bulletin Board Inventor, Dies at 74"</a>. <i>The New York Times</i>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://search.worldcat.org/issn/0362-4331">0362-4331</a><span>. Retrieved <span>2021-10-03</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Randy+Suess%2C+Computer+Bulletin+Board+Inventor%2C+Dies+at+74&amp;rft.date=2019-12-20&amp;rft.issn=0362-4331&amp;rft.aulast=Metz&amp;rft.aufirst=Cade&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2019%2F12%2F20%2Ftechnology%2Frandy-suess-dead.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-3"><span><b><a href="#cite_ref-3">^</a></b></span> <span><cite id="CITEREFBarry">Barry, Rey. <a rel="nofollow" href="http://www.freewarehof.org/ward.html">"The Origin of Computer Bulletin Boards"</a>. Freeware Hall of Fame.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Origin+of+Computer+Bulletin+Boards&amp;rft.pub=Freeware+Hall+of+Fame&amp;rft.aulast=Barry&amp;rft.aufirst=Rey&amp;rft_id=http%3A%2F%2Fwww.freewarehof.org%2Fward.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-4"><span><b><a href="#cite_ref-4">^</a></b></span> <span><cite id="CITEREFGoodwins"><a href="https://en.wikipedia.org/wiki/Rupert_Goodwins" title="Rupert Goodwins">Goodwins, Rupert</a>. <a rel="nofollow" href="http://resources.zdnet.co.uk/articles/comment/0,1000002985,2130537,00.htm">"Online communities turn twenty-five"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Online+communities+turn+twenty-five&amp;rft.aulast=Goodwins&amp;rft.aufirst=Rupert&amp;rft_id=http%3A%2F%2Fresources.zdnet.co.uk%2Farticles%2Fcomment%2F0%2C1000002985%2C2130537%2C00.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20110607175741/http://www.smartcomputing.com/editorial/dictionary/detail.asp?guid=&amp;searchtype=&amp;DicID=19535&amp;RefType=Encyclopedia">"Ward Christensen"</a>. <i>Smart Computing Encyclopedia</i>. Archived from <a rel="nofollow" href="http://www.smartcomputing.com/editorial/dictionary/detail.asp?guid=&amp;searchtype=&amp;DicID=19535&amp;RefType=Encyclopedia">the original</a> on June 7, 2011.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Ward+Christensen&amp;rft.btitle=Smart+Computing+Encyclopedia&amp;rft_id=http%3A%2F%2Fwww.smartcomputing.com%2Feditorial%2Fdictionary%2Fdetail.asp%3Fguid%3D%26searchtype%3D%26DicID%3D19535%26RefType%3DEncyclopedia&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-pournelle198307-6"><span><b><a href="#cite_ref-pournelle198307_6-0">^</a></b></span> <span><cite id="CITEREFPournelle1983">Pournelle, Jerry (July 1983). <a rel="nofollow" href="https://archive.org/stream/byte-magazine-1983-07-rescan/1983_07_BYTE_08-07_Videotex#page/n325/mode/2up">"Interstellar Drives, Osborne Accessories, DEDICATE/32, and Death Valley"</a>. <i>BYTE</i>. p.&nbsp;323<span>. Retrieved <span>28 August</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BYTE&amp;rft.atitle=Interstellar+Drives%2C+Osborne+Accessories%2C+DEDICATE%2F32%2C+and+Death+Valley&amp;rft.pages=323&amp;rft.date=1983-07&amp;rft.aulast=Pournelle&amp;rft.aufirst=Jerry&amp;rft_id=https%3A%2F%2Farchive.org%2Fstream%2Fbyte-magazine-1983-07-rescan%2F1983_07_BYTE_08-07_Videotex%23page%2Fn325%2Fmode%2F2up&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-7"><span><b><a href="#cite_ref-7">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20160306152326/http://citivu.com/dvorak/index.html#1992">"Dvorak Awards for Excellence in Telecommunications"</a>. <i>citivu</i>. Archived from <a rel="nofollow" href="http://citivu.com/dvorak/index.html#1992">the original</a> on 2016-03-06.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=citivu&amp;rft.atitle=Dvorak+Awards+for+Excellence+in+Telecommunications&amp;rft_id=http%3A%2F%2Fcitivu.com%2Fdvorak%2Findex.html%231992&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><cite><a rel="nofollow" href="https://w2.eff.org/Misc/EFF/Pioneer_Awards/2nd_pioneer_awards.announce">"Second Annual EFF Pioneer Awards"</a>. Electronic Frontier Foundation.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Second+Annual+EFF+Pioneer+Awards&amp;rft.pub=Electronic+Frontier+Foundation&amp;rft_id=http%3A%2F%2Fw2.eff.org%2FMisc%2FEFF%2FPioneer_Awards%2F2nd_pioneer_awards.announce&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><a rel="nofollow" href="http://www.bbsdocumentary.com/software/AAA/AAA/CBBS/memories.txt"><i>re: R/1ST BBS QUESTIONS</i> (Msg 46394)</a> from Ward Christensen to Steve Culver, July 31, 1993.</span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.bbsdocumentary.com/">"BBS: TheDocumentary"</a>. <i>BBS: The Documentary</i><span>. Retrieved <span>15 September</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=BBS%3A+The+Documentary&amp;rft.atitle=BBS%3A+TheDocumentary&amp;rft_id=http%3A%2F%2Fwww.bbsdocumentary.com%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
</ol></div>

<ul><li><a rel="nofollow" href="https://x.com/wardxmodem">Ward Christensen</a> on <a href="https://en.wikipedia.org/wiki/Twitter" title="Twitter">Twitter</a> <span typeof="mw:File/Frameless"><a href="https://www.wikidata.org/wiki/Q7968985#P2002" title="Edit this at Wikidata"><img alt="Edit this at Wikidata" src="https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png" decoding="async" width="10" height="10" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/15px-OOjs_UI_icon_edit-ltr-progressive.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png 2x" data-file-width="20" data-file-height="20"></a></span></li></ul>

<!-- 
NewPP limit report
Parsed by mw‐web.eqiad.main‐c595d44d9‐k22dq
Cached time: 20241012105402
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.459 seconds
Real time usage: 0.791 seconds
Preprocessor visited node count: 2284/1000000
Post‐expand include size: 28505/2097152 bytes
Template argument size: 1699/2097152 bytes
Highest expansion depth: 17/100
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 41202/5000000 bytes
Lua time usage: 0.303/10.000 seconds
Lua memory usage: 5908833/52428800 bytes
Number of Wikibase entities loaded: 1/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  745.019      1 -total
 41.60%  309.921      1 Template:Infobox_person
 26.83%  199.864      1 Template:Reflist
 20.60%  153.452      4 Template:Br_separated_entries
 20.29%  151.169      3 Template:Cite_news
 17.37%  129.374      1 Template:Birth_date
 15.41%  114.813      1 Template:BBS
 14.81%  110.314      1 Template:Navbox
  9.85%   73.366      1 Template:Short_description
  8.43%   62.831      1 Template:Wikidata_image
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:355056-0!canonical and timestamp 20241012105402 and revision id 1250720023. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[America's new millionaire class: Plumbers and HVAC entrepreneurs (125 pts)]]></title>
            <link>https://www.wsj.com/business/entrepreneurship/plumbers-hvac-skilled-trades-millionaires-2b62bf6c</link>
            <guid>41828896</guid>
            <pubDate>Sun, 13 Oct 2024 15:35:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/business/entrepreneurship/plumbers-hvac-skilled-trades-millionaires-2b62bf6c">https://www.wsj.com/business/entrepreneurship/plumbers-hvac-skilled-trades-millionaires-2b62bf6c</a>, See on <a href="https://news.ycombinator.com/item?id=41828896">Hacker News</a></p>
Couldn't get https://www.wsj.com/business/entrepreneurship/plumbers-hvac-skilled-trades-millionaires-2b62bf6c: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[DARPA is funding reef structures that will be colonized by corals and bivalves (121 pts)]]></title>
            <link>https://www.wired.com/story/darpa-thinks-walls-of-oysters-could-protect-shores-against-hurricanes/</link>
            <guid>41828864</guid>
            <pubDate>Sun, 13 Oct 2024 15:32:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/darpa-thinks-walls-of-oysters-could-protect-shores-against-hurricanes/">https://www.wired.com/story/darpa-thinks-walls-of-oysters-could-protect-shores-against-hurricanes/</a>, See on <a href="https://news.ycombinator.com/item?id=41828864">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>On October 10,</span> 2018, Tyndall Air Force Base on the Gulf of Mexico—a pillar of American air superiority—found itself under aerial attack. Hurricane Michael, first spotted as a Category 2 storm off the Florida coast, unexpectedly hulked up to a Category 5. Sustained winds of 155 miles per hour whipped into the base, flinging power poles, flipping F-22s, and totaling more than 200 buildings. The sole saving grace: Despite sitting on a peninsula, Tyndall avoided flood damage. Michael’s 9-to-14-foot storm surge swamped other parts of Florida. Tyndall’s main defense was luck.</p><p>That $5 billion disaster at Tyndall was just one of a mounting number of extreme-weather events that convinced the US Department of Defense that it needed new ideas to protect the 1,700 coastal bases it’s responsible for globally. As hurricanes Helene and Milton have just shown, beachfront residents face compounding threats from climate change, and the Pentagon is no exception. Rising oceans are chewing away the shore. Stronger storms are more capable of flooding land.</p><p>In response, Tyndall will later this month test a new way to protect shorelines from intensified waves and storm surges: a prototype artificial reef, designed by a team led by Rutgers University scientists. The 50-meter-wide array, made up of three chevron-shaped structures each weighing about 46,000 pounds, can take 70 percent of the <em>oomph</em> out of waves, according to tests. But this isn’t your grandaddy’s seawall. It’s specifically designed to be colonized by oysters, some of nature’s most effective wave-killers.</p><p>If researchers can optimize these creatures to work in tandem with new artificial structures placed at sea, they believe the resulting barriers can take 90 percent of the energy out of waves. David Bushek, who directs the Haskin Shellfish Research Laboratory at Rutgers, swears he’s not hoping for a megastorm to come and show what his team’s unit is made of. But he’s not <em>not</em> hoping for one. “Models are always imperfect. They’re always a replica of something,” he says. “They’re not the real thing.”</p><p>The project is one of three being developed under a $67.6 million program launched by the US government’s Defense Advanced Research Projects Agency, or Darpa. Cheekily called Reefense, the initiative is the Pentagon’s effort to test if “hybrid” reefs, combining manmade structures with oysters or corals, can perform as well as a good ol’ seawall. Darpa chose three research teams, all led by US universities, in 2022. After two years of intensive research and development, their prototypes are starting to go into the water, with Rutgers’ first up.</p><p>Today, the Pentagon protects its coastal assets much as civilians do: by hardening them. Common approaches involve armoring the shore with retaining walls or arranging heavy objects, like rocks or concrete blocks, in long rows. But hardscape structures come with tradeoffs. They deflect rather than absorb wave energy, so protecting one’s own shoreline means exposing someone else’s. They’re also static: As sea levels rise and storms get stronger, it’s getting easier for water to surmount these structures. This wears them down faster and demands constant, expensive repairs.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In recent decades, a new idea has emerged: using nature as infrastructure. Restoring coastal habitats like marshes and mangroves, it turns out, helps hold off waves and storms. “Instead of armoring, you’re using nature’s natural capacity to absorb wave energy,” says Donna Marie Bilkovic, a professor at the Virginia Institute for Marine Science. Darpa is particularly interested in two creatures whose numbers have been decimated by humans but which are terrific wave-breakers when allowed to thrive: oysters and corals.</p><p>Oysters are effective wave-killers because of how they grow. The bivalves pile onto each other in large, sturdy mounds. The resulting structure, unlike a smooth seawall, is replete with nooks, crannies, and convolutions. When a wave strikes, its energy gets diffused into these gaps, and further spent on the jagged, complex surfaces of the oysters. Also unlike a seawall, an oyster wall can grow. Oysters have been shown to be capable of building vertically at a rate that matches sea-level rise—which suggests they’ll retain some protective value against higher tides and stronger storms.</p><p>Today hundreds of human-tended oyster reefs, particularly on America’s Atlantic coast, use these principles to protect the shore. They take diverse approaches; some look much like natural reefs, while others have an <a data-offer-url="https://www.wfae.org/2024-06-20/sugarloaf-island-morehead-city-oysters-wave-attenuation-devices" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.wfae.org/2024-06-20/sugarloaf-island-morehead-city-oysters-wave-attenuation-devices&quot;}" href="https://www.wfae.org/2024-06-20/sugarloaf-island-morehead-city-oysters-wave-attenuation-devices" rel="nofollow noopener" target="_blank">artificial component</a>. Some cultivate oysters for food, with coastal protection a nice co-benefit; others are built specifically to preserve shorelines. What’s missing amid all this experimentation, says Bilkovic, is systematic performance data—the kind that could validate which approaches are most effective and cost-effective. “Right now the innovation is outpacing the science,” she says. “We need to have some type of systematic monitoring of projects, so we can better understand where the techniques work the best. There just isn’t funding, frankly.”</p><p>Rather than wait for the data needed to engineer the perfect reef, Darpa wants to rapidly innovate them through a burst of R&amp;D. Reefense has given awardees five years to deploy hybrid reefs that take up to 90 percent of the energy out of waves, without costing significantly more than traditional solutions. The manmade component should block waves immediately. But it should be quickly enhanced by organisms that build, in months or years, a living structure that would take nature decades.</p><p>The Rutgers team has built its prototype out of 788 interlocked concrete modules, each 2 feet wide and ranging in height from 1 to 2 feet tall. They have a scalloped appearance, with shelves jutting in all directions. Internally, all these shelves are connected by holes.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><figure><p><span><p>A Darpa-funded team will install sea barriers, made of hundreds of concrete modules, near a Florida military base. The scalloped shape should not only dissipate wave energy but invite oysters to build their own structures.</p>
</span><span>Photograph: David Bushek</span></p></figure><p>What this means is that when a wave strikes this structure, it smashes into the internal geometry, swirls around, and exits with less energy. This effect alone weakens the wave by 70 percent, according to the US Army Corps of Engineers, which tested a scale model in a wave simulator in Mississippi. But the effect should only improve as oysters colonize the structure. Bushek and his team have tried to design the shelves with the right hardness, texture, and shading to entice them.</p><p>But the reef’s value would be diminished if, say, disease were to wipe the mollusks out. This is why Darpa has tasked Rutgers with also engineering oysters resistant to dermo, a protozoan that’s dogged Atlantic oysters for decades. Darpa prohibited them using genetic-modification techniques. But thanks to recent advances in genomics, the Rutgers team can rapidly identify individual oysters with disease-resistant traits. It exposes these oysters to dermo in a lab, and crossbreeds the survivors, producing hardier mollusks. Traditionally it takes about three years to breed a generation of oysters for better disease resistance; Bushek says his team has done it in one.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><figure><p><span><p>Oysters cluster onto the manmade concrete structures designed by the research team led by Rutgers University.</p>
</span><span>Photograph: David Bushek</span></p></figure><p>Oysters may suit the DoD’s needs in temperate waters, but for bases in tropical climates, it’s coral that builds the best seawalls. Hawaii, for instance, enjoys the protection of “fringing” coral reefs that extend offshore for hundreds of yards in a gentle slope along the seabed. The colossal, complex, and porous character of this surface exhausts wave energy over long distances, says Ben Jones, an oceanographer for the Applied Research Laboratory at the University of Hawaii—and head of the university’s Reefense project. He said it’s not unusual to see ocean swells of 6 to 8 feet way offshore, while the water at the seashore laps gently.</p><figure><p><span><p>A Marine base in Hawaii will test out a new approach to coastal protection inspired by local coral reefs: A forward barrier will take the first blows of the waves, and a scattering of pyramids will further weaken waves before they get to shore.</p>
</span><span>Photograph:  Kevin Chun, Makai Ocean Engineering</span></p></figure></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Inspired by this effect, Jones and a team of researchers are designing an array that they’ll deploy near a US Marine Corps base in Oahu whose shoreline is rapidly receding. While the final design isn’t set yet, the broad strokes are: It will feature two 50-meter-wide barriers laid in rows, backed by 20 pyramid-like obstacles. All of these are hollow, thin-walled structures with sloping profiles and lots of big holes. Waves that crash into them will lose energy by crawling up the sides, but two design aspects of the structure—the width of the holes and the thinness of the walls—will generate turbulence in the water, causing it to spin off more energy as heat.</p><figure><p><span><p>The manmade structures in Hawaii will be studded with concrete domes meant to encourage coral colonization. Though at grave risk from global warming, coral reefs are thought to provide coastal-protection benefits worth billions of dollars.</p>
</span><span>Photograph: R3D Consortium</span></p></figure><p>In the team’s full vision, the units are bolstered by about a thousand small coral colonies. Jones’ group plans to cover the structures with concrete modules that are about 20 inches in diameter. These have grooves and crevices that offer perfect shelters for coral larvae. The team will initially implant them with lab-bred coral. But they’re also experimenting with enticements, like light and sound, that help attract coral larvae from the wild—the better to build a wall that nature, not the Pentagon, will tend.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>A third Reefense team, led by scientists at the University of Miami, takes its inspiration from a different sort of coral. Its design has a three-tiered structure. The foundation is made of long, hexagonal logs punctured with large holes; atop it is a dense layer with smaller holes—“imagine a sponge made of concrete,” says Andrew Baker, director of the university’s Coral Reef Futures Lab and the Reefense team lead.</p><p>The team thinks these artificial components will soak up plenty of wave energy—but it’s a crest of elkhorn coral at the top that will finish the job. Native to Florida, the Bahamas, and the Caribbean, elkhorn like to build dense reefs in shallow-water areas with high-intensity waves. They don’t mind getting whacked by water because it helps them harvest food; this whacking keeps wave energy from getting to shore.</p><p>Disease has ravaged Florida’s elkhorn populations in recent decades, and now ocean heat waves are dealing further damage. But their critical condition has also motivated policymakers to pursue options to save this iconic state species—including Baker’s, which is to develop an elkhorn more rugged against disease, higher temperatures, and nastier waves. Under Reefense, Baker says, his lab has developed elkhorn with 1.5 to 2 degrees Celsius more heat tolerance than their ancestors. They also claim to have boosted the heat thresholds of symbiotic algae—an existentially important occupant of any healthy reef—and cross-bred local elkhorn with those from Honduras, where reefs have mysteriously withstood scorching waters.</p><p>An unexpected permitting issue, though, will force the Miami team to exit Reefense in 2025, without building the test unit it hoped to deploy near a Florida naval base. The federal permitting authority wanted a pot of money set aside to uninstall the structure if needed; Darpa felt it couldn’t do that in a timely way, according to Baker. (Darpa told WIRED every Reefense project has unique permitting challenges, so the Miami team’s fate doesn’t necessarily speak to anything broader. Representatives for the other two Reefense projects said Baker’s issue hasn’t come up for them.)</p><p>Though his team’s work with Reefense is coming to a premature end, Baker says, he’s confident their innovations will get deployed elsewhere. He’s been working with Key Biscayne, an island village near Miami whose shorelines have been chewed up by storms. Roland Samimy, the village’s chief resilience and sustainability officer, says they spend millions of dollars every few years importing sand for their rapidly receding beaches. He’s eager to see if a hybrid structure, like the University of Miami design, could offer protection at far lower cost. “People are realizing their manmade structures aren’t as resilient as nature is,” he says.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>By no means is Darpa the only one experimenting in these areas. Around the world, there are efforts tackling various pieces of the puzzle, like breeding coral for greater heat resistance, or combining coral and oysters with artificial reefs, or designing low-carbon concrete that makes building these structures less environmentally damaging. Bilkovic, of the Virginia Institute for Marine Science, says Reefense will be a success if it demonstrates better ways of doing things than the prevailing methods—and has the data to back this up. “I’m looking forward to seeing what their findings are,” she says. “They’re systematically assessing the effectiveness of the project. Those lessons learned can be translated to other areas, and if the techniques are effective and work well, they can easily be translated to other regions.”</p><p>As for Darpa, though the Reefense prototypes are just starting to go in the water, the work is just beginning. All of these first-generation units will be scrutinized—both by the research teams and independent government auditors—to see whether their real-world performance matches what was in the models. Reefense is scheduled to conclude with a final report to the DoD in 2027. It won’t have a “winner” <em>per se</em>; as the Pentagon has bases around the world, it’s likely these three projects will all produce learnings that are relevant elsewhere.</p><p>Although their client has the largest military budget in the world, the three Reefense teams have been asked to keep an eye on the economics. Darpa has asked that project costs “not greatly exceed” those of conventional solutions, and tasked government monitors with checking the teams’ math. Catherine Campbell, Reefense’s program manager at Darpa, says affordability doesn’t just make it more likely the Pentagon will employ the technology—but that civilians can, too.</p><p>“This isn’t something bespoke for the military … we need to be in line with those kinds of cost metrics [in the civilian sector],” Campbell said in an email. “And that gives it potential for commercialization.”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lessons learned from profiling an algorithm in Rust (109 pts)]]></title>
            <link>https://blog.mapotofu.org/blogs/rabitq-bench/</link>
            <guid>41828611</guid>
            <pubDate>Sun, 13 Oct 2024 15:03:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mapotofu.org/blogs/rabitq-bench/">https://blog.mapotofu.org/blogs/rabitq-bench/</a>, See on <a href="https://news.ycombinator.com/item?id=41828611">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Recently, I've been working on a new approximate nearest neighbor search algorithm called <a href="https://arxiv.org/abs/2405.12497">RaBitQ</a>. The author has already provided a <a href="https://github.com/gaoj0017/RaBitQ">C++ implementation</a> that runs quite fast. I tried to <a href="https://github.com/kemingy/rabitq">rewrite it in Rust</a> (yet another RiiR). But I found that my implementation is much slower than the original one. Here is how I improve the performance step by step.</p>
<h2 id="prepare-the-environment">Prepare the environment</h2>
<h3 id="datasets">Datasets</h3>
<p>The most important thing is to have some reasonable datasets. Since the paper already demonstrate some results on the <code>sift_dim128_1m_l2</code> and <code>gist_dim960_1m_l2</code> datasets, 128 and 960 dimensions are typical and 1_000_000 vectors should be sufficient for benchmark purpose. I decided to use them as well. The datasets can be downloaded from <a href="http://corpus-texmex.irisa.fr/">here</a>. (Yes, I know this site doesn't have TLS and it only provides FTP downloads).</p>
<p>The format used by these datasets is called <code>fvecs/ivecs</code>, which is a common vector format:</p>
<pre data-lang="python"><code data-lang="python"><span>| </span><span>dim </span><span>(</span><span>4 </span><span>bytes</span><span>) </span><span>| </span><span>vector </span><span>(</span><span>4 </span><span>* </span><span>dim </span><span>bytes</span><span>) </span><span>|
</span><span>| </span><span>dim </span><span>(</span><span>4 </span><span>bytes</span><span>) </span><span>| </span><span>vector </span><span>(</span><span>4 </span><span>* </span><span>dim </span><span>bytes</span><span>) </span><span>|
</span><span>...
</span><span>| </span><span>dim </span><span>(</span><span>4 </span><span>bytes</span><span>) </span><span>| </span><span>vector </span><span>(</span><span>4 </span><span>* </span><span>dim </span><span>bytes</span><span>) </span><span>|
</span></code></pre>
<p>You can get the read/write script from my <a href="https://gist.github.com/kemingy/2f503fcfff86b9e0197e975c02359157">gist</a>.</p>
<h3 id="profiling-tool">Profiling tool</h3>
<p>I use <a href="https://github.com/mstange/samply">samply</a> to profile the Rust code. It has a nice integration with the <a href="https://profiler.firefox.com/">Firefox Profiler</a>. You can also share the profiling results with others by uploading them to the cloud. Here is <a href="https://share.firefox.dev/3Y4Hppz">an example of the C++ version profiling on GIST</a>. The FlameGraph and CallTree are the most common views. Remember to grant the performance event permission and increase the <code>mlock</code> limit:</p>
<pre data-lang="bash"><code data-lang="bash"><span>echo </span><span>'1' </span><span>| </span><span>sudo</span><span> tee /proc/sys/kernel/perf_event_paranoid
</span><span>sudo</span><span> sysctl kernel.perf_event_mlock_kb=2048
</span></code></pre>
<p>The <a href="https://godbolt.org/">GodBolt</a> compiler explorer is also useful for comparing the assembly function code between C++ and Rust.</p>
<h3 id="cargo-profile">Cargo profile</h3>
<p>To include the debug information in the release build, you can add another profile to the <code>Cargo.toml</code>:</p>
<pre data-lang="toml"><code data-lang="toml"><span>[</span><span>profile.perf</span><span>]
</span><span>inherits </span><span>= </span><span>"release"
</span><span>debug </span><span>= </span><span>true
</span><span>codegen-units </span><span>= </span><span>16
</span></code></pre>
<p>The compiling cost and runtime speed can greatly affect the profiling user experience.</p>
<ul>
<li><code>cargo build</code> has a faster compile speed, but the code may be slower than pure Python</li>
<li><code>cargo build --release</code> runs fast but it might take a long time to compile</li>
</ul>
<p>For benchmarking, we have no choice but to use the <code>opt-level = 3</code>.</p>
<p>I saw some advice to use the following settings:</p>
<pre data-lang="toml"><code data-lang="toml"><span>codegen-units </span><span>= </span><span>1
</span><span>lto </span><span>= </span><span>"fat"
</span><span>panic </span><span>= </span><span>"abort"
</span></code></pre>
<p>In my case, this only slows down the compilation speed and doesn't improve the performance at all.</p>
<h3 id="benchmark-tool">Benchmark tool</h3>
<p><a href="https://github.com/bheisler/criterion.rs">Criterion</a> is a good statistics-driven benchmark tool. I create another <a href="https://github.com/kemingy/rs_bench">repo</a> to store all the related benchmark code. It turns out that I should put them in the same repo.</p>
<p>One thing to note is that the benchmark results are not very stable. I have seen <strong><code>±10%</code></strong> differences without modifying the code. If you're using your laptop, this could be even worse since the CPU might be underclocked due to the high temperature.</p>
<p>I suggest to benchmark the function with several different parameters. In this case, I use different vector dimensions. If the results for all the dimensions are positive, it usually means that the improvement is effective.</p>
<h3 id="metrics">Metrics</h3>
<p>Remember to add some metrics from the start. Many bugs and performance issues can be found by checking the metrics. I use <code>AtomicU64</code> directly since the current requirements are simple. I may switch to the <a href="https://github.com/prometheus/client_rust">Prometheus metrics</a> later.</p>
<p>Note that too many metrics/logging/traces can also affect the performance. So be careful when adding them.</p>
<h3 id="resources">Resources</h3>
<p>During the benchmark, I noticed that the end-to-end QPS is extremely unstable. I could get a <strong>15%</strong> improvement or deterioration the nex day morning without recompiling the code. Then I found that the CPUs are not completely idle as I have VSCode + Rust Analyzer, it seems they don't consume much CPU but they do affect the benchmark results heavily. Even though I'm using <a href="https://www.intel.com/content/www/us/en/products/sku/230500/intel-core-i713700k-processor-30m-cache-up-to-5-40-ghz/specifications.html">Intel Core i7-13700K</a>, which has 8 performance cores and 8 efficient cores, also the program is single-threaded.</p>
<p>I use <a href="https://www.man7.org/linux/man-pages/man1/taskset.1.html"><code>taskset</code></a> to bind the process to a specific CPU. This way it won't be affected by mixed cores scheduling.</p>
<p>Note that Intel Core 13th/14th CPUs are affected by the instability problem due to the extremely high voltage. I have fixed this in the BIOS.</p>
<p>Cloud VMs may not be affected by the CPU temperature, but the cloud providers may have their own CPU throttling and overbooking policies.</p>
<h2 id="step-by-step-improvement">Step by Step Improvement</h2>
<h3 id="start-with-an-naive-implementation">Start with an naive implementation</h3>
<p>My <a href="https://github.com/kemingy/rabitq/tree/dbfd54bd5d739b0729dc28e6fbd8d5413b019561">first release</a> implemented the RaBitQ algorithm based on an algebra library called <a href="https://docs.rs/nalgebra">nalgebra</a>. The main reason is that I need to use the QR decomposition to obtain the orthogonal matrix, which is the key step in the RaBitQ algorithm. Also, a mature linear algebra library provides many useful functions for manipulating the matrix and vectors, making it easier for me to implement the algorithm. Imagine that implementing an algorithm involving matrix multiplication, projection and decomposition in Python without <code>numpy</code>, it's a nightmare.</p>
<p>I thought that the performance should be good since <code>nalgebra</code> is optimized for such kind of scenarios. But the benchmark shows that is much slower than I expected. I guess reimplementing it in <code>numpy</code> would be much faster :(</p>
<p>According to the <a href="https://share.firefox.dev/3AwiVNR">profiling</a>, there are lots of <code>f32::clone()</code> calls. It takes about 33% of the total time, or 44% if you focus on the <code>query_one</code> function. This reminds me that I can preallocate the memory for some vectors and reuse it in the iteration, a very common trick. So instead of using <code>(x - y).norm_squared()</code>, I need to pre-declare another vector that stores the result of <code>(x - y)</code>, which ends up being <code>x.sub_to(y, &amp;mut z); z.norm_squared()</code>. See the <a href="https://github.com/kemingy/rabitq/commit/23f9aff4c8b3303c0a03ac9a7472ada8cc915a3b">commit 23f9aff</a>.</p>
<p>Like most of the algebra libraries, it stores the matrix in the column-major order, which means iterating over the column could be faster than over the row. It's a bit annoying because I have to transpose the matrix before the iteration, and not all the vector/matrix multiplications can detect the dimension mismatch error (<code>1 x dyn</code> or <code>dyn x 1</code>) during compilation.</p>
<h3 id="cpu-target">CPU target</h3>
<p>RaBitQ uses the binary dot product distance to estimate the approximate distance, which is computed by:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>binary_dot_product</span><span>(</span><span>x</span><span>: </span><span>&amp;</span><span>[</span><span>u64</span><span>], </span><span>y</span><span>: </span><span>&amp;</span><span>[</span><span>u64</span><span>]) -&gt; </span><span>u32 </span><span>{
</span><span>    assert_eq!(x.</span><span>len</span><span>(), y.</span><span>len</span><span>());
</span><span>    </span><span>let mut</span><span> res </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>for</span><span> i </span><span>in </span><span>0</span><span>..</span><span>x.</span><span>len</span><span>() {
</span><span>        res </span><span>+= </span><span>(x[i] </span><span>&amp;</span><span> y[i]).</span><span>count_ones</span><span>();
</span><span>    }
</span><span>    res
</span><span>}
</span></code></pre>
<p>Here the <a href="https://doc.rust-lang.org/std/primitive.u64.html#method.count_ones"><code>u64::count_ones()</code></a> would use intrinsics directly, I thought. It turns out that I still need to enable the <code>popcnt</code> feature during the compilation. This could be done by using the <code>RUSTFLAGS="-C target-feature=+popcnt"</code>, but I prefer <code>RUSTFLAGS="-C target-cpu=native"</code>, which enables all the CPU features supported by the current CPU, but also makes the binary non-portable, which is fine for now. The following sections also require this <code>env</code> to enable the AVX2 features.</p>
<p>You can use the following command to check your CPU features:</p>
<pre data-lang="bash"><code data-lang="bash"><span>rustc</span><span> --print</span><span>=</span><span>cfg</span><span> -C</span><span> target-cpu=native </span><span>| </span><span>rg</span><span> target_feature
</span></code></pre>
<h3 id="simd">SIMD</h3>
<p>The key function for the nearest neighbor search is the distance function, which in this case is the Euclidean distance. We usually use the L2 square distance to avoid the square root computation. The naive implementation is as follows:</p>
<pre data-lang="rust"><code data-lang="rust"><span>{
</span><span>    y.</span><span>sub_to</span><span>(x, </span><span>&amp;</span><span>mut</span><span> residual);
</span><span>    residual.</span><span>norm_squared</span><span>()
</span><span>}
</span></code></pre>
<p>After the profiling, I found that it still has <code>f32::clone()</code>. By checking the source code of <code>nalgebra</code>, I found that there are many <code>clone</code> for some reasons I don't know. I decide to write the SIMD by hand. Fortunately, <a href="https://github.com/nmslib/hnswlib">hnswlib</a> (a popular HNSW implementation) already implements <a href="https://github.com/nmslib/hnswlib/blob/master/hnswlib/space_l2.h">this</a>.</p>
<p>This eliminates the <code>f32::clone()</code> in the distance computation and improves the QPS by <strong>28%</strong> for SIFT. Check the <a href="https://github.com/kemingy/rabitq/commit/5f82fccf8b39964ef1f66e9927fb126fd6886765">commit 5f82fcc</a>.</p>
<p>My CPU doesn't support AVX512, so I use the AVX2 version. You can check the <a href="https://store.steampowered.com/hwsurvey/">Steam Hardware Stats</a>, it lists the SIMD support in the "<em>Other Settings</em>". <strong>100%</strong> users have SSE3, <strong>94.61%</strong> users have AVX2, only <strong>13.06%</strong> users have AVX512F. Of course this statistic is biased, most of the cloud Intel CPUs have AVX512 support, game players cannot represent all the users.</p>
<p>To use SIMD, the most useful guide is the <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#">Intel Intrinsics Guide</a>. It's better to download the website as the online experience is not good. Remember to check the "<strong>latency</strong>" and "<strong>throughput</strong>" of the intrinsics, otherwise your code may be slower than the normal version.</p>
<p>Another resource is the <a href="https://db.in.tum.de/~finis/x86%20intrinsics%20cheat%20sheet%20v1.0.pdf">x86 Intrinsics Cheat Sheet</a>. This is good for newbies like me.</p>
<p><a href="https://github.com/ashvardanian">@ashvardanian</a> has a <a href="https://ashvardanian.com/posts/simsimd-faster-scipy/#tails-of-the-past-the-significance-of-masked-loads">post</a> about the "mask load" that solves the tail elements problem (requires AVX512).</p>
<p>To make the code work on other platforms:</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[</span><span>cfg</span><span>(</span><span>any</span><span>(target_arch </span><span>= </span><span>"x86_64"</span><span>, target_arch </span><span>= </span><span>"x86"</span><span>)</span><span>)]
</span><span>{
</span><span>    </span><span>if </span><span>is_x86_feature_detected!(</span><span>"avx2"</span><span>) {
</span><span>        </span><span>// AVX2 version
</span><span>    } </span><span>else </span><span>{
</span><span>        </span><span>// normal version
</span><span>    }
</span><span>}
</span></code></pre>
<p>There are some useful crates for writing better <code>cfg</code> for the SIMD, let's keep it simple for now.</p>
<h3 id="more-simd">More SIMD</h3>
<p>SIMD is like a hammer, now I need to find more nails in the code.</p>
<ul>
<li>rewrite the <code>binarize_vector</code> function with AVX2 in <a href="https://github.com/kemingy/rabitq/commit/f114fc1ec58686596ade0df02a96fcf04b0bf828">commit f114fc1</a> improves the QPS by <strong>32%</strong> for GIST.</li>
</ul>
<p><del>Compared to the original C++ version, this implementation is also branchless.</del> When enabling <code>opt-level=3</code>, this can be optimied by the compiler. See the <a href="https://godbolt.org/z/hjP5qjabz">assembly</a>.</p>
<pre data-lang="diff"><code data-lang="diff"><span>- let shift = if (i / 32) % 2 == 0 { 32 } else { 0 };
</span><span>+ let shift = ((i &gt;&gt; 5) &amp; 1) &lt;&lt; 5;
</span></code></pre>
<h3 id="scalar-quantization">Scalar quantization</h3>
<p>To eliminate more <code>f32::clone()</code> in the code, I decided to replace more <code>nalgebra</code> functions with the manual implementation. The <code>min</code> and <code>max</code> functions are the most common ones. The <code>nalgebra</code> version is like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>let</span><span> lower_bound </span><span>=</span><span> residual.</span><span>min</span><span>();
</span><span>let</span><span> upper_bound </span><span>=</span><span> residual.</span><span>max</span><span>();
</span></code></pre>
<p>This can be done by:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>min_max</span><span>(</span><span>vec</span><span>: </span><span>&amp;</span><span>[</span><span>f32</span><span>]) -&gt; (</span><span>f32</span><span>, </span><span>f32</span><span>) {
</span><span>    </span><span>let mut</span><span> min </span><span>= </span><span>f32</span><span>::</span><span>MAX</span><span>;
</span><span>    </span><span>let mut</span><span> max </span><span>= </span><span>f32</span><span>::</span><span>MIN</span><span>;
</span><span>    </span><span>for</span><span> v </span><span>in</span><span> vec.</span><span>iter</span><span>() {
</span><span>        </span><span>if </span><span>*</span><span>v </span><span>&lt;</span><span> min {
</span><span>            min </span><span>= *</span><span>v;
</span><span>        }
</span><span>        </span><span>if </span><span>*</span><span>v </span><span>&gt;</span><span> max {
</span><span>            max </span><span>= *</span><span>v;
</span><span>        }
</span><span>    }
</span><span>    (min, max)
</span><span>}
</span></code></pre>
<p>I used to use <code>f32::min()</code> and <code>f32::max()</code> because they are convenient. But for non-(asc/desc) vectors, <code>if</code> has a better performance.</p>
<p>Instead of iterating through the vector several times in a function chain and computing the scalar quantization with sum in different iterations:</p>
<pre data-lang="rust"><code data-lang="rust"><span>let</span><span> y_scaled </span><span>=</span><span> residual.</span><span>add_scalar</span><span>(</span><span>-</span><span>lower_bound) </span><span>*</span><span> one_over_delta </span><span>+ &amp;</span><span>self</span><span>.rand_bias;
</span><span>let</span><span> y_quantized </span><span>=</span><span> y_scaled.</span><span>map</span><span>(|</span><span>v</span><span>| v.</span><span>to_u8</span><span>().</span><span>expect</span><span>(</span><span>"convert to u8 error"</span><span>));
</span><span>let</span><span> scalar_sum </span><span>=</span><span> y_quantized.</span><span>iter</span><span>().</span><span>fold</span><span>(</span><span>0</span><span>u32</span><span>, |</span><span>acc</span><span>, </span><span>&amp;</span><span>v</span><span>| acc </span><span>+</span><span> v </span><span>as </span><span>u32</span><span>);
</span></code></pre>
<p>We can do this in one loop:</p>
<pre data-lang="rust"><code data-lang="rust"><span>{
</span><span>    </span><span>let mut</span><span> sum </span><span>= </span><span>0</span><span>u32</span><span>;
</span><span>    </span><span>for</span><span> i </span><span>in </span><span>0</span><span>..</span><span>vec.</span><span>len</span><span>() {
</span><span>        </span><span>let</span><span> q </span><span>= </span><span>((vec[i] </span><span>-</span><span> lower_bound) </span><span>*</span><span> multiplier </span><span>+</span><span> bias[i]) </span><span>as </span><span>u8</span><span>;
</span><span>        quantized[i] </span><span>=</span><span> q;
</span><span>        sum </span><span>+=</span><span> q </span><span>as </span><span>u32</span><span>;
</span><span>    }
</span><span>    sum
</span><span>}
</span></code></pre>
<p>For scalar quantization, we are sure that the <code>f32</code> can be converted to <code>u8</code>, so we can use <code>as u8</code> instead of <code>to_u8().unwrap()</code>.</p>
<p>The <a href="https://github.com/kemingy/rabitq/commit/af39c1ce47eb8ea32e11f47b99548e77846397ea">commit af39c1c</a> &amp; <a href="https://github.com/kemingy/rabitq/commit/d2d51b0785f0234df4d83a60eea96a36486a1120">commit d2d51b0</a> improved the QPS by <strong>31%</strong> for GIST.</p>
<p>The following part can also be rewritten with SIMD, which improves the QPS by <strong>12%</strong> for GIST:</p>
<ul>
<li>min/max: <a href="https://github.com/kemingy/rabitq/commit/c97be68c13c7b4498b564afe3de2a1f6d8bca5ce">commit c97be68</a> &amp; <a href="https://github.com/kemingy/rabitq/commit/e5a4af05433bf724da6902d34a745b4b2bdefd8d">commit e5a4af0</a></li>
<li>scalar quantization: <a href="https://github.com/kemingy/rabitq/commit/28efe097a46696bb1a5469db22e500bafdc04514">commit 28efe09</a></li>
</ul>
<p>I also tried replacing <code>tr_mul</code> with SIMD, which is a vector projection. It turns out that <code>nalgebra</code> uses <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms"><code>BLAS</code></a> here, so the performance stays the same.</p>
<h3 id="yet-another-algebra-crate-faer">Yet another algebra crate: faer</h3>
<p>I found another Rust algebra crate called <a href="https://github.com/sarah-quinones/faer-rs">faer</a> while investigating the <code>f32::clone()</code> problem. It's optimized with lots of SIMD and provides better row/column iteration performance. The QR decomposition is also much faster than <code>nalgebra</code>. This <a href="https://github.com/kemingy/rabitq/commit/04118219d28bd0d43594c98c71e752faa81ff79d">commit 0411821</a> makes the training part faster.</p>
<p>Also, I can now use these vectors as a normal slice without the <code>ColRef</code> or <code>RowRef</code> wrapper after <a href="https://github.com/kemingy/rabitq/commit/0d969bdcfb331f87e938e043e01acc648e1cf963">commit 0d969bd</a>.</p>
<p>I have to admit that if I used <code>faer</code> from the beginning, I could avoid lots of troubles. Anyway, I learned a lot from this experience.</p>
<h3 id="binary-dot-product">Binary dot product</h3>
<p>I thought <code>popcnt</code> already solved the binary dot product, but the <a href="https://share.firefox.dev/3Yk3Ok8">FlameGraph</a> shows that <code>count_ones()</code> only takes 7% of the <code>binary_dot_product</code>. Although the AVX512 has the <code>vpopcntq</code> instruction, I would prefer to use the AVX2 simulation since it's more common.</p>
<p><a href="https://github.com/komrad36/popcount/blob/master/popcnt.h">This</a> is a good reference for the <code>popcnt</code> implementation with AVX2. The <a href="https://github.com/kemingy/rabitq/commit/edabd4a64c5b8ea2637b5332105638edf16afa7c">commit edabd4a</a> re-implement this in Rust which improves the QPS by <strong>11%</strong> for GIST. This trick only works when the vector has more than 256 dimensions, which means 256 bits for the binary representation.</p>
<h3 id="inline">Inline</h3>
<p>The <a href="https://doc.rust-lang.org/reference/attributes/codegen.html#the-inline-attribute">#[inline]</a> attribute should be used with caution. Adding this attribute to all the SIMD functions improves the QPS by <strong>5%</strong> for GIST.</p>
<h3 id="io">IO</h3>
<p>I need to add some background information here.</p>
<p>The current implementation is based on the IVF algorithm, which will uses <a href="https://en.wikipedia.org/wiki/K-means_clustering"><em>k</em>-means</a> to cluster the vectors and stores the centroids in memory. The query vector is only compared to the clusters with smaller <code>l2_squared_distance(query, centroid)</code>.</p>
<p>There is a parameter called <code>n_probe</code> that controls how many nearest clusters will be probed. A large <code>n_probe</code> will increase the recall but decrease the QPS.</p>
<p>RaBitQ uses the binary dot product to estimate the approximate distance. If it's smaller than the threshold, it will re-rank with the original L2 squared distance and update the threshold accordingly.</p>
<p>Previously, I used <a href="https://doc.rust-lang.org/std/primitive.slice.html#method.select_nth_unstable"><code>slice::select_nth_unstable</code></a> which only selects the n-nearest but doesn't sort them in order. Going through the clusters that are far away from the query will increase the re-ranking ratio, which requires more L2 squared distance computation. Re-sorting the selected n-th clusters improved the QPS by <strong>4%</strong> for GIST.</p>
<p>Another trick is to sort the vectors in each cluster by their distance to the centroids, this <a href="https://github.com/kemingy/rabitq/commit/ea13ebca46257d7c2e22250fe02a481e7681f0a9">commit ea13ebc</a> also improved the QPS by <strong>4%</strong> for GIST.</p>
<p>There are some metadata used to estimate the approximate distance for each vector:</p>
<ul>
<li>factor_ip: f32</li>
<li>factor_ppc: f32</li>
<li>error: f32</li>
<li>x_c_distance_square: f32</li>
</ul>
<p>Previously I use 4 <code>Vec&lt;f32&gt;</code> to store them, which is not IO friendly, since the calculation requires <code>vector[i]</code> for each of them. By combining them into one <code>struct</code> in <a href="https://github.com/kemingy/rabitq/commit/bb440e3e8b150f590523eaa77e7c62165a5ee764">commit bb440e3</a>, the QPS improved by <strong>2.5%</strong> for GIST. This works well because it's 4xf32, so I can use the C representation directly:</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[</span><span>derive</span><span>(Debug, Clone, Copy, Default, Serialize, Deserialize)]
</span><span>#[</span><span>repr</span><span>(C)]
</span><span>struct </span><span>Factor {
</span><span>    </span><span>factor_ip</span><span>: </span><span>f32</span><span>,
</span><span>    </span><span>factor_ppc</span><span>: </span><span>f32</span><span>,
</span><span>    </span><span>error_bound</span><span>: </span><span>f32</span><span>,
</span><span>    </span><span>center_distance_square</span><span>: </span><span>f32</span><span>,
</span><span>}
</span></code></pre>
<p>Unfortunately, <code>faer</code> doesn't support u64 vectors. So I have to store the vector binary representation in <code>Vec&lt;Vec&lt;u64&gt;&gt;</code>. By changing it to <code>Vec&lt;u64&gt;</code> in <a href="https://github.com/kemingy/rabitq/commit/48236b23069db92bdb741fc6693e126b52c397ce">commit 48236b2</a>, the QPS improved by <strong>2%</strong> for GIST.</p>
<h3 id="const-generics">Const generics</h3>
<p>The C++ version uses the template to generate the code for different dimensions. This feature is also available in Rust. I didn't try it because re-compiling the code for different dimensions might only be possible for specific use cases, like inside a company that only has a few fixed dimensions. For the public library, it's better to provide a general solution so users don't have to re-compile it by themselves.</p>
<h3 id="other-tools">Other tools</h3>
<p>There is a <a href="https://github.com/Shnatsel/bounds-check-cookbook/">bounds-check-cookbook</a> which provides several examples of how to eliminate the boundary checking in safe Rust.</p>
<p>I tried <a href="https://doc.rust-lang.org/rustc/profile-guided-optimization.html">PGO</a> and <a href="https://github.com/llvm/llvm-project/tree/main/bolt">BOLT</a> but didn't get any improvement.</p>
<p>Switching to <a href="https://github.com/tikv/jemallocator">jemalloc</a> or <a href="https://github.com/microsoft/mimalloc">mimalloc</a> doesn't improve the performance either.</p>
<h2 id="conclusion">Conclusion</h2>
<ul>
<li>SIMD is awesome when it's used properly</li>
<li>IO is also important, especially for the large datasets</li>
</ul>
<p>The current performance is the same as the C++ version for dataset GIST. While I use more SIMD, the C++ version uses const generics.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://en.algorithmica.org/hpc/algorithms/matmul/">Algorithmica / HPC</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Quiet Art of Attention (384 pts)]]></title>
            <link>https://billwear.github.io/art-of-attention.html</link>
            <guid>41828601</guid>
            <pubDate>Sun, 13 Oct 2024 15:01:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://billwear.github.io/art-of-attention.html">https://billwear.github.io/art-of-attention.html</a>, See on <a href="https://news.ycombinator.com/item?id=41828601">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>There comes a moment in life, often in
      the quietest of hours, when one realizes
      that the world will continue on its
      wayward course, indifferent to our              desires or frustrations. And it is then,
      perhaps, that a subtle truth begins to
      emerge: the only thing we truly possess,
      the only thing we might, with enough
      care, exert some mastery over, is our
      mind. It is not a realization of
      resignation, but rather of liberation.
      For if the mind can be ordered, if it           can be made still in the midst of this
      restless life, then we have already
      discovered the key to a deeper kind of
      freedom.</p>

<p>But how does one begin? It is not with
      grand declarations or bold, sweeping
      changes. That would miss the point
      entirely. Rather, it is with a gentle
      attention to the present, a deliberate
      shift in the way we move through the
      world. We begin by paying attention to
      what our mind does—its wanderings, its
      anxieties, its compulsions. It is a
      garden untended, overgrown with concerns
      that may not even be our own. And the
      first step is simply to watch, to
      observe how the mind moves, without
      judgment, without rush.</p>

<p>In this quiet observation, we begin to
      see patterns. The mind leaps from one
      thing to another, rarely resting. It is
      caught in a web of habits, most of which
      we never consciously chose. But, once we
      notice this, a door opens. There is
      space, however small, between the
      thoughts. And in that space, if we are
      patient, we can decide how to respond
      rather than being dragged along by every
      impulse or fear. This is not about              control in the traditional sense, but
      about clarity. To act, not from reflex,
      but from intent.</p>

<p>It is a simple beginning, but one of
      great consequence. For when we reclaim
      our attention, even in this small way,
      we are no longer mere passengers on the
      journey. We become, in a sense, our own
      guides.</p>

<p>As we grow in this practice of
      attention, something else becomes clear:
      much of what occupies our thoughts is
      unnecessary. The mind is cluttered,
      filled with concerns that seem urgent
      but, on closer inspection, do little to
      serve our deeper well-being.
      Simplification is not just a matter of
      decluttering our physical
      surroundings—it is a way of thinking, of
      living. As we quiet the noise within, we
      see more clearly what truly matters. We
      focus, not on everything, but on the
      essentials. We pare down, not by force,
      but by choice.</p>

<p>This process of simplification is not an
      escape from complexity. It is, in fact,
      a way of engaging with it more
      meaningfully. There are things in life
      that are intricate, yes, but not
      everything needs our attention at once.
      What truly requires our effort can be
      approached in small steps, in manageable
      pieces. The mind works best when it is
      focused on one thing at a time, when it
      is allowed to give itself fully to the          task at hand. In this way, the most
      complex of undertakings becomes simple,
      not because it is easy, but because we
      have allowed it to unfold naturally, one
      step after the other.</p>

<p>It is tempting, in moments of ambition,
      to think that we must change everything
      all at once, that the path to mastery or
      peace requires a sudden, dramatic shift.
      But this is rarely the case. In truth,
      most lasting changes come from small,
      deliberate actions. It is in the
      repetition of these small actions, over
      time, that we build strength, that we
      build the habits of mind that lead to
      deeper clarity. Just as a mountain is
      climbed not in great leaps but in
      steady, measured steps, so too is the
      mind brought into alignment by daily,
      patient attention to the way we think.</p>

<p>But in this process, we must remember
      something important: life is not meant
      to be rushed through. It is not a race,
      nor is it a problem to be solved. It is
      an experience to be lived, and living
      well requires presence. To focus on one
      thing deeply, to give it your full
      attention, is to experience it fully.
      And when we do this, something
      remarkable happens. Time, which so often
      feels like it is slipping through our
      fingers, begins to slow. Moments become
      rich, textured. Even the simplest of
      tasks takes on a new significance when
      approached with care, with attention.</p>

<p>This is the quiet art of living well. It
      does not demand that we abandon the
      world, but that we engage with it more
      mindfully. It asks that we slow down,
      that we look more closely, that we
      listen more carefully. For in doing so,
      we discover that much of what we
      seek—clarity, peace, even strength—was
      always within reach. It was simply
      waiting for us to stop, to pay
      attention, and to begin again with
      intention.</p>

<p>The mind, like a garden, requires
      tending. It needs patience, a steady
      hand, and, above all, consistency. There
      will be days when it seems unruly, when
      old habits return, and when focus feels
      elusive. But these days, too, are part
      of the process. Each small effort, each
      moment of renewed attention, builds upon
      the last. Over time, these moments
      accumulate, and what was once difficult
      becomes second nature.</p>

<p>And so, the journey to mastery of the
      mind begins not with grand gestures but
      with the simplest of practices: the
      practice of paying attention. Attention
      to the present, attention to what truly
      matters, and attention to the quiet
      spaces in between. In this way, step by
      step, thought by thought, we move closer
      to that elusive state of clarity, of
      peace, and of freedom.</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Common Lisp implementation of the Forth 2012 Standard (108 pts)]]></title>
            <link>https://github.com/gmpalter/cl-forth</link>
            <guid>41827461</guid>
            <pubDate>Sun, 13 Oct 2024 12:38:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gmpalter/cl-forth">https://github.com/gmpalter/cl-forth</a>, See on <a href="https://news.ycombinator.com/item?id=41827461">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">CL-Forth</h2><a id="user-content-cl-forth" aria-label="Permalink: CL-Forth" href="#cl-forth"></a></p>
<p dir="auto">Common Lisp implementation of the <a href="https://forth-standard.org/" rel="nofollow">Forth 2012 Standard</a>, CL-Forth</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Platforms</h2><a id="user-content-supported-platforms" aria-label="Permalink: Supported Platforms" href="#supported-platforms"></a></p>
<p dir="auto">CL-Forth is fully supported by CCL v1.12.2-82 or later.</p>
<p dir="auto">CL-Forth also supports SBCL 2.1.0 or later. However, at present, the word <code>RESIZE-FILE</code> will always return an error indication,
resulting in 7 failures in the File-Access word set tests.</p>
<p dir="auto">CL-Forth compiles with LispWorks but crashes running the Forth test suite.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Operating Systems</h2><a id="user-content-supported-operating-systems" aria-label="Permalink: Supported Operating Systems" href="#supported-operating-systems"></a></p>
<p dir="auto">CL-Forth is supported on macOS, Linux, and Windows.</p>
<p dir="auto">On macOS, it has been verified to run on macOS Ventura or later.</p>
<p dir="auto">On Linux, it has been verified to run on distributions with 5.10.162 kernels or later.</p>
<p dir="auto">On Windows, it has been verified to run on Windows 10 or later.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">CL-Forth is made available under the terms of the <a href="https://github.com/gmpalter/cl-forth/blob/main/LICENSE">MIT License</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">CL-Forth is defined using ASDF and is dependent on the <a href="https://github.com/cffi/cffi">CFFI</a> and
<a href="https://github.com/trivial-gray-streams/trivial-gray-streams">trivial-gray-streams</a> libraries.</p>
<p dir="auto">To fetch a copy of CL-Forth and the <a href="https://github.com/gerryjackson/forth2012-test-suite.git">Forth 2012 Test Suite</a> configured
to only run tests for those word sets implemented by CL-Forth.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/gmpalter/cl-forth.git --recurse-submodules"><pre>git clone https://github.com/gmpalter/cl-forth.git --recurse-submodules</pre></div>
<p dir="auto">To load CL-Forth into Lisp</p>
<div dir="auto" data-snippet-clipboard-copy-content="(require '#:asdf)
(load &quot;cl-forth.asd&quot;)
(asdf:load-system '#:cl-forth)"><pre>(<span>require</span> <span>'</span>#:asdf)
(<span>load</span> <span><span>"</span>cl-forth.asd<span>"</span></span>)
(<span>asdf</span>:load-system <span>'</span>#:cl-forth)</pre></div>
<p dir="auto">You can run the <a href="https://github.com/gerryjackson/forth2012-test-suite.git">Forth 2012 Test Suite</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="(asdf:test-system '#:cl-forth)"><pre>(<span>asdf</span>:test-system <span>'</span>#:cl-forth)</pre></div>
<p dir="auto">To start the CL-Forth interpreter loop</p>

<p dir="auto">CL-Forth is case-insensitive.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building a Standalone CL-Forth</h3><a id="user-content-building-a-standalone-cl-forth" aria-label="Permalink: Building a Standalone CL-Forth" href="#building-a-standalone-cl-forth"></a></p>
<p dir="auto">You can build a standalone CL-Forth application.</p>
<p dir="auto">Launch Lisp and evaluate the forms</p>
<div dir="auto" data-snippet-clipboard-copy-content="(require '#:asdf)
(load &quot;cl-forth.asd&quot;)
(asdf:load-system '#:cl-forth/application)
(forth-app:save-application &quot;cl-forth&quot;)"><pre>(<span>require</span> <span>'</span>#:asdf)
(<span>load</span> <span><span>"</span>cl-forth.asd<span>"</span></span>)
(<span>asdf</span>:load-system <span>'</span>#:cl-forth/application)
(<span>forth-app</span>:save-application <span><span>"</span>cl-forth<span>"</span></span>)</pre></div>
<p dir="auto">This will create an executable named <code>cl-forth</code>. When you run <code>cl-forth</code>, it will startup directly into the Forth interpreter
loop.</p>
<div dir="auto" data-snippet-clipboard-copy-content="./cl-forth
CL-Forth Version 1.3
Running under Clozure Common Lisp Version 1.13 (v1.13) DarwinX8664
1 1 + .
2 OK.
: hello-world .&quot; Hello World!&quot; cr ;
OK.
hello-world
Hello World!
OK.
see hello-world
Source code for hello-world:
(DEFUN FORTH-WORDS::HELLO-WORLD (FS &amp;REST PARAMETERS)
  (DECLARE (IGNORABLE PARAMETERS))
  (WITH-FORTH-SYSTEM (FS)
    (TAGBODY (WRITE-STRING &quot;Hello World!&quot;)
             (TERPRI)
     :EXIT)))
OK.
bye
In this session:
  1 definition created
  240 bytes of object code generated"><pre>./cl-forth
CL-Forth Version <span>1.3</span>
Running under Clozure Common Lisp Version <span>1.13</span> (v1.13) DarwinX8664
<span>1</span> <span>1</span> + .
<span>2</span> OK.
: hello-world ." <span>Hello World!"</span> cr ;
OK.
hello-world
Hello World!
OK.
<span>see</span> hello-world
Source code <span>for</span> hello-world:
(DEFUN FORTH-WORDS::HELLO-WORLD (FS &amp;REST PARAMETERS)
  (DECLARE (IGNORABLE PARAMETERS))
  (WITH-FORTH-SYSTEM (FS)
    (TAGBODY (WRITE-STRING "Hello World!")
             (TERPRI)
     :EXIT)))
OK.
bye
In this session:
  <span>1</span> definition created
  <span>240</span> bytes <span>of</span> object code generated</pre></div>
<p dir="auto">The application  recognizes these command line arguments</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>‑‑interpret&nbsp;EXPR</code>, <code>‑i&nbsp;EXPR</code></td>
<td>Evaluate <code>EXPR</code> before entering the Forth interpreter loop. <code>EXPR</code> may need to be quoted to avoid interpretation by the shell.  This argument may be used multiple times.</td>
</tr>
<tr>
<td><code>‑‑transcript&nbsp;PATH</code></td>
<td>Record a timestamped transcript of this session in the file <code>PATH</code></td>
</tr>
<tr>
<td><code>‑‑help</code>, <code>‑h</code></td>
<td>Display the available command line arguments and exit</td>
</tr>
<tr>
<td><code>‑‑version</code>, <code>‑V</code></td>
<td>Display the version of CL-Forth and exit</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Missing Words</h2><a id="user-content-missing-words" aria-label="Permalink: Missing Words" href="#missing-words"></a></p>
<p dir="auto">CL-Forth does not implement the optional Block word set.</p>
<p dir="auto">CL-Forth does not implement the optional Extended-Character word set.</p>
<p dir="auto">CL-Forth does not implement <code>KEY</code> which is part of the Core word set.</p>
<p dir="auto">The following words that are part of the optional Facility and Facility extensions word set are not implemented.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>AT-XY</code></td>
<td><code>KEY?</code></td>
<td><code>PAGE</code></td>
<td><code>EKEY</code></td>
<td><code>EKEY&gt;CHAR</code></td>
</tr>
<tr>
<td><code>EKEY&gt;FKEY</code></td>
<td><code>EKEY?</code></td>
<td><code>EMIT?</code></td>
<td><code>K-ALT-MASK</code></td>
<td><code>K-CTRL-MASK</code></td>
</tr>
<tr>
<td><code>K-DELETE</code></td>
<td><code>K-DOWN</code></td>
<td><code>K-END</code></td>
<td><code>K-F1</code></td>
<td><code>K-F10</code></td>
</tr>
<tr>
<td><code>K-F11</code></td>
<td><code>K-F12</code></td>
<td><code>K-F2</code></td>
<td><code>K-F3</code></td>
<td><code>K-F4</code></td>
</tr>
<tr>
<td><code>K-F5 </code></td>
<td><code>K-F6</code></td>
<td><code>K-F7</code></td>
<td><code>K-K8</code></td>
<td><code>K-F9</code></td>
</tr>
<tr>
<td><code>K-HOME</code></td>
<td><code>K-INSERT</code></td>
<td><code>K-LEFT</code></td>
<td><code>K-NEXT</code></td>
<td><code>K-PRIOR</code></td>
</tr>
<tr>
<td><code>K-RIGHT</code></td>
<td><code>K-SHIFT-MASK</code></td>
<td><code>K-UP</code></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Foreign Function Interface</h2><a id="user-content-foreign-function-interface" aria-label="Permalink: Foreign Function Interface" href="#foreign-function-interface"></a></p>
<p dir="auto">CL-Forth includes a foreign function interface (FFI) loosely based on the External Library Interface in
<a href="https://www.forth.com/swiftforth/" rel="nofollow">SwiftForth</a>. See <a href="https://github.com/gmpalter/cl-forth/blob/main/FFI.md">FFI.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Additional Words</h2><a id="user-content-additional-words" aria-label="Permalink: Additional Words" href="#additional-words"></a></p>
<p dir="auto">CL-Forth includes a number of words defined by other implementation that are not part of the Forth 2012 Standard.</p>
<p dir="auto">These words are specific to CL-Forth.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>.SF</code></td>
<td>Display the contents of the floating-point stack</td>
</tr>
<tr>
<td><code>.SR</code></td>
<td>Display the contents of the return stack</td>
</tr>
<tr>
<td><code>ALL-WORDS</code></td>
<td>Display all words in all word lists in the search order</td>
</tr>
<tr>
<td><code>BREAK</code></td>
<td>Enter a Lisp break loop</td>
</tr>
<tr>
<td><code>INLINEABLE</code></td>
<td>Mark that the most recent definition's code may be inlined</td>
</tr>
<tr>
<td><code>NOTINTERPRETED</code></td>
<td>Mark that the most recent definition must only appear in definitions</td>
</tr>
<tr>
<td><code>RELOAD</code></td>
<td>Reload a predefined definition (i.e., created by <code>define-word</code>)</td>
</tr>
<tr>
<td><code>REMOVE</code></td>
<td>Erase a single word</td>
</tr>
<tr>
<td><code>SHOW-BACKTRACES</code></td>
<td>Controls whether exceptions display the return and data stacks</td>
</tr>
<tr>
<td><code>SHOW-CODE</code></td>
<td>Controls whether completing a definition shows the generated code</td>
</tr>
<tr>
<td><code>STATISTICS</code></td>
<td>Report some useful statistics about this CL-Forth session</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">These words are defined as "Common Usage" in the <a href="https://www.forth.com/forth-books/" rel="nofollow">Forth Programmer's Manual, 3rd Edition</a>.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>,"</code></td>
<td><code>2+</code></td>
<td><code>2-</code></td>
<td><code>C+!</code></td>
<td><code>CONTEXT</code></td>
</tr>
<tr>
<td><code>CURRENT</code></td>
<td><code>CVARIABLE</code></td>
<td><code>M-</code></td>
<td><code>M/</code></td>
<td><code>NOT</code></td>
</tr>
<tr>
<td><code>NUMBER</code></td>
<td><code>NUMBER?</code></td>
<td><code>VOCABULARY</code></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">These words are defined by <a href="https://www.forth.com/swiftforth/" rel="nofollow">SwiftForth</a>.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-?</code></td>
<td><code>EMPTY</code></td>
<td><code>GILD</code></td>
<td><code>OFF</code></td>
<td><code>ON</code></td>
<td><code>OPTIONAL</code></td>
</tr>
<tr>
<td><code>SILENT</code></td>
<td><code>VERBOSE</code></td>
<td><code>WARNING</code></td>
<td><code>\\</code></td>
<td><code>{</code></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Implementation</h2><a id="user-content-implementation" aria-label="Permalink: Implementation" href="#implementation"></a></p>
<p dir="auto"><em>TO BE SUPPLIED</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Native Code Support</h2><a id="user-content-native-code-support" aria-label="Permalink: Native Code Support" href="#native-code-support"></a></p>
<p dir="auto">CL-Forth implements <code>CODE</code> and <code>;CODE</code> to allow the definition of words written in Lisp rather than Forth. The terminator for
the Lisp code block is <code>;ENDCODE</code>.</p>
<p dir="auto">Here is an example of using native code.</p>
<div dir="auto" data-snippet-clipboard-copy-content="\ ( c-addr1 u - c-addr2 u)
\ Converts the string at C-ADDR1 U to uppercase and leaves the result in transient space at C-ADDR2 U.
CODE UPCASE
  (let ((count (cell-signed (stack-pop data-stack)))
        (address (stack-pop data-stack)))
    (unless (plusp count)
      (forth-exception :invalid-numeric-argument &quot;Count to UPCASE must be positive&quot;))
    (multiple-value-bind (data offset)
        (memory-decode-address memory address)
      (let* ((original (forth-string-to-native data offset count))
             (upcased (string-upcase original))
             (string-space (reserve-string-space memory))
             (address (transient-space-base-address memory string-space)))
        (ensure-transient-space-holds memory string-space count)
        (multiple-value-bind (data offset)
            (memory-decode-address memory address)
          (native-into-forth-string upcased data offset)
          (seal-transient-space memory string-space)
          (stack-push data-stack address)
          (stack-push data-stack count)))))
;ENDCODE"><pre>\ ( <span>c</span><span>-</span><span>addr1</span> <span>u</span> <span>-</span> <span>c</span><span>-</span><span>addr2</span> <span>u</span>)
\ <span>Converts</span> <span>the</span> <span>string</span> <span>at</span> <span>C</span><span>-</span><span>ADDR1</span> <span>U</span> <span>to</span> <span>uppercase</span> <span>and</span> <span>leaves</span> <span>the</span> <span>result</span> <span>in</span> <span>transient</span> <span>space</span> <span>at</span> <span>C</span><span>-</span><span>ADDR2</span> <span>U</span>.
<span>CODE</span> <span>UPCASE</span>
  (<span>let</span> ((<span>count</span> (<span>cell</span><span>-</span><span>signed</span> (<span>stack</span><span>-</span><span>pop</span> <span>data</span><span>-</span><span>stack</span>)))
        (<span>address</span> (<span>stack</span><span>-</span><span>pop</span> <span>data</span><span>-</span><span>stack</span>)))
    (<span>unless</span> (<span>plusp</span> <span>count</span>)
      (<span>forth</span><span>-</span><span>exception</span> :<span>invalid</span><span>-</span><span>numeric</span><span>-</span><span>argument</span> <span>"Count to UPCASE must be positive"</span>))
    (<span>multiple</span><span>-</span><span>value</span><span>-</span><span>bind</span> (<span>data</span> <span>offset</span>)
        (<span>memory</span><span>-</span><span>decode</span><span>-</span><span>address</span> <span>memory</span> <span>address</span>)
      (<span>let</span><span>*</span> ((<span>original</span> (<span>forth</span><span>-</span><span>string</span><span>-</span><span>to</span><span>-</span><span>native</span> <span>data</span> <span>offset</span> <span>count</span>))
             (<span>upcased</span> (<span>string</span><span>-</span><span>upcase</span> <span>original</span>))
             (<span>string</span><span>-</span><span>space</span> (<span>reserve</span><span>-</span><span>string</span><span>-</span><span>space</span> <span>memory</span>))
             (<span>address</span> (<span>transient</span><span>-</span><span>space</span><span>-</span><span>base</span><span>-</span><span>address</span> <span>memory</span> <span>string</span><span>-</span><span>space</span>)))
        (<span>ensure</span><span>-</span><span>transient</span><span>-</span><span>space</span><span>-</span><span>holds</span> <span>memory</span> <span>string</span><span>-</span><span>space</span> <span>count</span>)
        (<span>multiple</span><span>-</span><span>value</span><span>-</span><span>bind</span> (<span>data</span> <span>offset</span>)
            (<span>memory</span><span>-</span><span>decode</span><span>-</span><span>address</span> <span>memory</span> <span>address</span>)
          (<span>native</span><span>-</span><span>into</span><span>-</span><span>forth</span><span>-</span><span>string</span> <span>upcased</span> <span>data</span> <span>offset</span>)
          (<span>seal</span><span>-</span><span>transient</span><span>-</span><span>space</span> <span>memory</span> <span>string</span><span>-</span><span>space</span>)
          (<span>stack</span><span>-</span><span>push</span> <span>data</span><span>-</span><span>stack</span> <span>address</span>)
          (<span>stack</span><span>-</span><span>push</span> <span>data</span><span>-</span><span>stack</span> <span>count</span>)))))
;<span>ENDCODE</span></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Starship Flight 5 Stream (1706 pts)]]></title>
            <link>https://twitter.com/SpaceX/status/1845152255944819015</link>
            <guid>41827362</guid>
            <pubDate>Sun, 13 Oct 2024 12:23:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/SpaceX/status/1845152255944819015">https://twitter.com/SpaceX/status/1845152255944819015</a>, See on <a href="https://news.ycombinator.com/item?id=41827362">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Large language models reduce public knowledge sharing on online Q&A platforms (342 pts)]]></title>
            <link>https://academic.oup.com/pnasnexus/article/3/9/pgae400/7754871</link>
            <guid>41827043</guid>
            <pubDate>Sun, 13 Oct 2024 11:26:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://academic.oup.com/pnasnexus/article/3/9/pgae400/7754871">https://academic.oup.com/pnasnexus/article/3/9/pgae400/7754871</a>, See on <a href="https://news.ycombinator.com/item?id=41827043">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widgetname="ArticleFulltext">





                    <h2 scrollto-destination="483096323" id="483096323">Abstract</h2>
<section><p>Large language models (LLMs) are a potential substitute for human-generated data and knowledge resources. This substitution, however, can present a significant problem for the training data needed to develop future models if it leads to a reduction of human-generated content. In this work, we document a reduction in activity on Stack Overflow coinciding with the release of ChatGPT, a popular LLM. To test whether this reduction in activity is specific to the introduction of this LLM, we use counterfactuals involving similar human-generated knowledge resources that should not be affected by the introduction of ChatGPT to such extent. Within 6 months of ChatGPT’s release, activity on Stack Overflow decreased by 25% relative to its Russian and Chinese counterparts, where access to ChatGPT is limited, and to similar forums for mathematics, where ChatGPT is less capable. We interpret this estimate as a lower bound of the true impact of ChatGPT on Stack Overflow. The decline is larger for posts related to the most widely used programming languages. We find no significant change in post quality, measured by peer feedback, and observe similar decreases in content creation by more and less experienced users alike. Thus, LLMs are not only displacing duplicate, low-quality, or beginner-level content. Our findings suggest that the rapid adoption of LLMs reduces the production of public data needed to train them, with significant consequences.</p></section>                    
<div id="pgae400-box1"><p>This study examines the impact of ChatGPT, a large language model, on online communities that contribute to public knowledge shared on the Internet. We found that ChatGPT has led to a 25% drop in activity on Stack Overflow, a key reference website where programmers share knowledge and solve problems. This substitution threatens the future of the open web, as interactions with AI models are not added to the shared pool of online knowledge. Moreover, this phenomenon could weaken the quality of training data for future models, as machine-generated content likely cannot fully replace human creativity and insight. This shift could have significant consequences for both the public Internet and the future of AI.</p></div>                    <h2 scrollto-destination="483096327" id="483096327" data-legacy-id="pgae400-s1">Introduction</h2>
<p>Over the last 30 years, humans have constructed a vast and open library of information on the web. Using powerful search engines, anyone with an internet connection can access valuable information from online knowledge repositories like Wikipedia, Stack Overflow, and Reddit. New content and discussions posted online are quickly integrated into this ever-growing ecosystem, becoming digital public goods used by people all around the world to learn new technologies and solve their problems (<span id="jumplink-pgae400-B1 pgae400-B2 pgae400-B3 pgae400-B4"></span>1–4).</p><p>These public goods are essential for training AI systems, in particular, large language models (LLMs) (<span id="jumplink-pgae400-B5"></span>5). For example, the LLM in ChatGPT (<span id="jumplink-pgae400-B6"></span>6) is trained to recognize patterns, facts, and information from vast repositories of online public text by predicting the next words in sequences. It answers users’ questions by generating responses that not only integrate and contextualize this information but also infer underlying meanings and connections. The remarkable effectiveness of ChatGPT is reflected in its quick adoption (<span id="jumplink-pgae400-B7"></span>7) and application across diverse fields, including auditing (<span id="jumplink-pgae400-B8"></span>8), astronomy (<span id="jumplink-pgae400-B9"></span>9), medicine (<span id="jumplink-pgae400-B10"></span>10), and chemistry (<span id="jumplink-pgae400-B11"></span>11). Randomized control trials show that using LLMs significantly boosts productivity and quality in computer programming, professional writing, customer support tasks, consulting, and writing job applications (<span id="jumplink-pgae400-B12 pgae400-B13 pgae400-B14 pgae400-B15 pgae400-B16"></span>12–16). Indeed, the widely reported successes of LLMs, like ChatGPT, suggest that we will observe a significant change in how people search for, create and share information online.</p><p>Ironically, if LLMs like ChatGPT, substitute for traditional methods of searching and interrogating the web, they could displace the very human behavior that generated their original training data. As people begin to use ChatGPT or similar LLMs instead of online knowledge repositories to find information, traffic and contributions to these repositories will likely decrease, diminishing the quantity and quality of these digital public goods. Previous work refers to this sort of displacement as the “paradox of re-use”: for example, the information on platforms like Wikipedia powers Google search (via information boxes and summaries) while reducing the need to visit Wikipedia (<span id="jumplink-pgae400-B17"></span>17, <span id="jumplink-pgae400-B18"></span>18). While such a shift could have significant social and economic implications, we have little evidence on whether people are indeed reducing their consumption and creation of valuable digital public goods as LLMs’ popularity grows.</p><p>The aim of this article is to evaluate the impact of LLMs on the generation of open data on popular question-and-answer (Q&amp;A) platforms. We focus on the effects of the most widely adopted LLM as of now—ChatGPT. Because ChatGPT performs relatively well on software programming tasks (<span id="jumplink-pgae400-B15"></span>15), we study Stack Overflow, the largest online Q&amp;A platform for software development and programming. Preliminary studies have shown that ChatGPT’s quality is competitive with answers from Stack Overflow in specific fields (<span id="jumplink-pgae400-B19"></span>19, <span id="jumplink-pgae400-B20"></span>20).</p><p>We present three results. First, we examine whether the release of ChatGPT has decreased the volume of posts, i.e. questions and answers, published on the platform. We estimate the causal effect of ChatGPT’s release on Stack Overflow activity using a difference-in-differences model. We compare the weekly posting activity on Stack Overflow against that of four comparable Q&amp;A platforms. These counterfactual platforms are less likely to be affected by ChatGPT either because their users experience difficulties with accessing ChatGPT or because ChatGPT performs poorly in questions discussed on those platforms.</p><p>We find that posting activity on Stack Overflow decreased by about <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mn xmlns="">25</mn><mi mathvariant="normal" xmlns="">%</mi></math></span> relative to the counterfactual platforms 6 months after the release of ChatGPT. We estimate the average effect across the 6 months to be <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mn xmlns="">15</mn><mi mathvariant="normal" xmlns="">%</mi></math>⁠</span>, reflecting a lagged kick-in and gradual adoption of ChatGPT. We interpret the <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mn xmlns="">25</mn><mi mathvariant="normal" xmlns="">%</mi></math></span> figure as a lower bound of the total impact of ChatGPT on Stack Overflow, as LLMs likely had some impact on even the counterfactual platforms. Additional evidence from the 2023 Stack Overflow Developer Survey supports the hypothesis that ChatGPT users are less likely to post on Stack Overflow and to visit the platform regularly.</p><p>Second, we investigate whether ChatGPT is simply displacing lower-quality posts on Stack Overflow. To do so, we use data on up- and downvotes, simple forms of social feedback provided by other users to rate posts. We observe little change in the votes posts received on Stack Overflow since the release of ChatGPT. In addition, we find significant declines in posting by users of all experience levels, from novice to expert. These results suggest that ChatGPT is displacing various Stack Overflow posts, including high-quality content.</p><p>Third, we study the heterogeneity of ChatGPT’s impact across different programming languages discussed on Stack Overflow. We test for these heterogeneities using an event study design. We observe that posting activity in some languages, like Python and Javascript, has decreased significantly more than the platform’s average. Using data on programming language popularity on GitHub, we find that the most widely used languages (and, hence, languages with richer data for training ChatGPT) tend to have larger relative declines in posting activity.</p><p>Our analysis points to several significant implications for the sustainability of the current AI ecosystem. The first is that the decreased production of open data will limit the training of future models (<span id="jumplink-pgae400-B21"></span>21). LLM-generated content itself is likely an ineffective substitute for training data generated by humans for the purpose of training new models (<span id="jumplink-pgae400-B22 pgae400-B23 pgae400-B24"></span>22–24). One analogy is that training an LLM on LLM-generated content is like making a photocopy of a photocopy, providing successively less satisfying results (<span id="jumplink-pgae400-B25"></span>25). While human feedback to LLMs may facilitate continued learning, data generated by interactions with privately owned LLMs belong to the owners of these LLMs.</p><p>This leads to the second issue: the initial advantage of the first mover, in this case OpenAI with its ChatGPT, compounds if the LLM effectively learns from interactions with users while crowding out the generation of new open data that competitors could use to improve their models. While it is well-known that increasing returns to users and data in the digital sector can lead to winner-take-all dynamics and technological lock-in (<span id="jumplink-pgae400-B26"></span>26, <span id="jumplink-pgae400-B27"></span>27), the transformation of the online commons into a private database presents a novel risk to consumer welfare. More broadly, a shift from open data to a more closed web will likely have significant second-order impacts on the ever-growing digital economy (<span id="jumplink-pgae400-B28"></span>28) and how we access, share, and evaluate information. These potential consequences have been overlooked in previous risk taxonomies of LLMs (<span id="jumplink-pgae400-B29"></span>29).</p><p>The rest of the article is organized as follows. We introduce our empirical set-up, including the data and models used in our analysis, in Data and methods section. Results section presents our results. In Discussion section, we discuss their implications. We argue that our findings of a significant decline in activity on Stack Overflow following the release of ChatGPT have important implications for the training of future models, competition in the AI sector, the provision of digital public goods, and how humans seek and share information.</p>                    <h2 scrollto-destination="483096339" id="483096339" data-legacy-id="pgae400-s2">Data and methods</h2>
                    <h3 scrollto-destination="483096340" id="483096340" data-legacy-id="pgae400-s2.1">Stack exchange and Segmentfault data</h3>
<p>To measure the effect ChatGPT can have on digital public goods, we compare the change in Stack Overflow’s activity with the activity on a set of similar platforms. These platforms are similar to Stack Overflow in that they are technical Q&amp;A platforms but are less prone to substitution by ChatGPT given their focus or target group. Specifically, we study the Stack Exchange platforms: Mathematics and Math Overflow and the Russian-language version of Stack Overflow. We also examine a Chinese-language Q&amp;A platform on computer programming called Segmentfault.</p><p>Mathematics and Math Overflow focus on university- and research-level mathematics questions, respectively. We consider these sites to be less susceptible to replacement by ChatGPT given that, during our study’s period of observation, the free-tier version of ChatGPT performed poorly (0–20th percentile) on advanced high-school mathematics exams (<span id="jumplink-pgae400-B6"></span>6) and was therefore unlikely to serve as a suitable alternative to these platforms.</p><p>The Russian Stack Overflow and the Chinese Segmentfault have similar scope as Stack Overflow, but target users located in Russia and China, respectively. We consider these platforms to be less affected by ChatGPT given that ChatGPT is officially unavailable in the Russian Federation, Belarus, Russian-occupied Ukrainian territory, and the People’s Republic of China. Although people in these places can and do access ChatGPT via VPNs, such barriers still represent a hurdle to widespread fast adoption (<span id="jumplink-pgae400-B30"></span>30).</p><p>We extract all posts (questions and answers) on Stack Overflow, Mathematics, Math Overflow, and Russian Stack Overflow from their launch to early June 2023 using <a href="https://archive.org/details/stackexchange" target="_blank">https://archive.org/details/stackexchange</a>. We scraped the data from Segmentfault directly. Our initial dataset comprises 58 million posts on Stack Overflow, over 900 thousand posts for the Russian-language version of Stack Overflow, 3.5 million posts on Mathematics Stack Exchange, 300 thousand posts for Math Overflow, and about 300 thousand for Segmentfault. We focus our analysis on data from January 2022 to the end of May 2023, noting that our findings are robust to alternative time windows.</p><p>For each post in the Stack Exchange sites, we additionally extract the post’s type (question or answer), the number of votes (up—positive feedback, or down—negative feedback) the post received, and the tags assigned to the post, where tags are predefined labels that summarize the content of the post, for instance, an associated programming language. In addition, we also extract the experience of the post’s author (i.e. number of previous posts). Using this information, we classify posts into those from “New”, “Inexperienced”, “Experienced”, and “Expert” users depending on whether the author had 0, 1–10, 11–100, or more than 100 posts, respectively at the time the post was published.<sup><span id="jumplink-FN1"></span>a</sup> For more details on the data from Q&amp;A platforms we used, we refer the reader to section.</p><p>Finally, we also investigated data from the 2023 Stack Overflow Developer Survey, conducted in mid-May 2023. It includes 89,184 responses from software developers living in 185 countries. We focus on user responses to the prompt “Which AI-powered tools did you use regularly over the past year?”, for which ChatGPT was an option to tick. We use this information to provide further suggestive evidence for the relationship between the adoption of ChatGPT and Stack Overflow activity at the individual programmer’s level while controlling for a rich set of characteristics, such as professional status, education, experience, and preferred programming language.</p>                    <h3 scrollto-destination="483096347" id="483096347" data-legacy-id="pgae400-s2.2">Models</h3>
                    <h4 scrollto-destination="483096348" id="483096348" data-legacy-id="pgae400-s2.2.1">Difference-in-differences</h4>
<p>We estimate the effect of ChatGPT for posting activity on Stack Overflow using a difference-in-differences method with four counterfactual platforms. We aggregate posting data at platform- and week-level and fit a regression model using ordinary least squares (OLS):<sup><span id="jumplink-FN2"></span>b</sup></p><div><div id="jumplink-M0001" content-id="M0001"><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow xmlns=""><mi mathvariant="normal">Log</mi></mrow><mo stretchy="false" xmlns="">(</mo><msub xmlns=""><mrow><mi mathvariant="normal">Posts</mi></mrow><mrow><mspace width=".1em"></mspace><mi>p</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="false" xmlns="">)</mo><mo xmlns="">=</mo><msub xmlns=""><mi>α</mi><mrow><mspace width=".1em"></mspace><mi>p</mi></mrow></msub><mo xmlns="">+</mo><msub xmlns=""><mi>λ</mi><mi>t</mi></msub><mo xmlns="">+</mo><mi xmlns="">β</mi><mo xmlns="">×</mo><msub xmlns=""><mrow><mi mathvariant="normal">Treated</mi></mrow><mrow><mspace width=".1em"></mspace><mi>p</mi><mo>,</mo><mi>t</mi></mrow></msub><mo xmlns="">+</mo><msub xmlns=""><mi>ϵ</mi><mrow><mspace width=".1em"></mspace><mi>p</mi><mo>,</mo><mi>t</mi></mrow></msub><mo xmlns="">,</mo></math></p></div><p><span>(1)</span></p></div><p>where <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mrow><mi mathvariant="normal">Posts</mi></mrow><mrow><mspace width=".1em"></mspace><mi>p</mi><mo>,</mo><mi>t</mi></mrow></msub></math></span> is the number of posts on platform <em>p</em> in a week <em>t</em>, which we log-transform. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>α</mi><mrow><mspace width=".1em"></mspace><mi>p</mi></mrow></msub></math></span> are platform fixed effects, <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>λ</mi><mi>t</mi></msub></math></span> are time (week) fixed effects, and <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>ϵ</mi><mrow><mspace width=".1em"></mspace><mi>p</mi><mo>,</mo><mi>t</mi></mrow></msub></math></span> is the error term.</p><p>The coefficient of interest is <em>β</em>, which captures the estimated effect of ChatGPT on posting activity on Stack Overflow relative to the less affected platforms: Treated equals one for weeks after the release of ChatGPT (starting with the week of 2022 November 27) when the platform <em>p</em> is Stack Overflow and zero otherwise. We report standard errors clustered at the monthly level to account for month-specific shocks common to all platforms. We note that <em>β</em> defines an estimate of the effect of ChatGPT on Stack Overflow relative to the counterfactuals averaged across the entire 6-month post-treatment period of our data. We focus our difference-in-differences estimations on the period between January 2022 and May 2023, covering 48 weeks before the release of ChatGPT and 25 weeks after it.<sup><span id="jumplink-FN3"></span>c</sup> However, to show that our results are not specific to the selected time window, we also repeat the estimations using a wider time period starting from January 2019.</p><p>The validity of the difference-in-differences approach relies on the assumption of parallel trends. While Fig. <span id="jumplink-pgae400-F1"></span>1b, illustrates that posting activity on Stack Overflow and the counterfactual platforms had developed in a similar way prior to the ChatGPT shock, we conduct several formal checks. First, we add platform-specific time trends that represent an interaction between a linear time trend and the average change in the number of posts on a platform between 2018 and pre-GPT. This allows us to check if the results are robust to the inclusion of differential time trends (<span id="jumplink-pgae400-B32"></span>32). Second, we estimate a generalized difference-in-differences model. Specifically, we employ a similar specification, but instead of <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">β</mi><mo xmlns="">×</mo><msub xmlns=""><mrow><mi mathvariant="normal">Treated</mi></mrow><mrow><mspace width=".1em"></mspace><mi>p</mi><mo>,</mo><mi>t</mi></mrow></msub></math>⁠</span>, we use <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><munder xmlns=""><mo>∑</mo><mi>t</mi></munder><msub xmlns=""><mi>β</mi><mi>t</mi></msub><mo xmlns="">×</mo><mi xmlns="">I</mi><mo stretchy="false" xmlns="">(</mo><mrow xmlns=""><mi mathvariant="normal">week</mi></mrow><mo xmlns="">=</mo><mi xmlns="">t</mi><mo stretchy="false" xmlns="">)</mo><mo xmlns="">×</mo><mi xmlns="">I</mi><mo stretchy="false" xmlns="">(</mo><mrow xmlns=""><mi mathvariant="normal">platform</mi><mspace width=".1em"></mspace></mrow><mo xmlns="">=</mo><mrow xmlns=""><mi mathvariant="normal">StackOverflow</mi></mrow><mo stretchy="false" xmlns="">)</mo></math>⁠</span>. We standardize the effects to 0 in the week before the public release of ChatGPT by dropping the indicator for that week from the regression. This model allows us to examine possible pretrends in our data. By estimating separate coefficients for the weeks <em>before</em> the release, we can check if posts on Stack Overflow had evolved similarly to the activity on counterfactual platforms prior to the release of ChatGPT. This specification also allows us to investigate the dynamics of the ChatGPT effect over time. Separate coefficients for 25 weeks <em>following</em> the release of ChatGPT show how the effects of ChatGPT realized over time as more users adopted the technology.</p>                    <div data-id="pgae400-f1" data-content-id="pgae400-f1" swap-content-for-modal="true"><p><img src="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/m_pgae400f1.jpeg?Expires=1731613731&amp;Signature=YFepQXrCPIxzAqXuYmMWOEBmLrKFL40uYS5Q--hUOQIkVcRDnZ7rd85dyg6PXbt7s7s0ep~2fKwV2NguWJyv5sx6zFx4ilIkwMbqbwlfot9l4FEBhkt8jPuv-L15I~t2tRfAGx~Dh4PbVSUp9psIByP~k25PuHW5k8xA8HSCRYmdXoUDsP5qwQ8PHGArHIf6nJG-ZWSuEAdP3T21YsfmyC1XswjYBuw1T1bOgYZiLw5Tyd4O0NTIIAUf8gU5R1X2Ak8lPFIGzhDqo5J5DwpX3JLx3eCaLG5bxOCQBVV~WtsLdhnFHx2H4bJ0GmXOlCYmcSG9igu22dBvXXOcxxqaSA__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="a) Time series of weekly posts to Stack Overflow since early 2016. The number of weekly posts decreases at a rate of about 7,000 posts each year from 2016 to 2022. In the 6 months after the release of ChatGPT, the weekly posting rate decreases by around 20,000 posts. b) Comparing posts to Stack Overflow, its Russian- and Chinese-language counterparts, and mathematics Q&amp;A platforms since early 2022. Post counts are standardized by the average and standard deviation of post counts within each platform prior to the release of ChatGPT. Posting activity on Stack Overflow falls significantly more relative to activity on other platforms." data-path-from-xml="pgae400f1.jpg"></p><div><p>Fig. 1.</p><p>a) Time series of weekly posts to Stack Overflow since early 2016. The number of weekly posts decreases at a rate of about 7,000 posts each year from 2016 to 2022. In the 6 months after the release of ChatGPT, the weekly posting rate decreases by around 20,000 posts. b) Comparing posts to Stack Overflow, its Russian- and Chinese-language counterparts, and mathematics Q&amp;A platforms since early 2022. Post counts are standardized by the average and standard deviation of post counts within each platform prior to the release of ChatGPT. Posting activity on Stack Overflow falls significantly more relative to activity on other platforms.</p></div></div><p>The advantage of the difference-in-differences method compared to a simple event study with Stack Overflow data only is that we estimate ChatGPT effects net of possible weekly shocks that are common across the technical Q&amp;A platforms. For the interpretation of the coefficient, we note that we estimate <em>relative</em> change in posting activity on Stack Overflow compared to activity on other platforms before vs. after the release of ChatGPT. To the extent that ChatGPT also affected activity on the counterfactual platforms, our estimates will be downward biased in the magnitude of the effect.</p><p>To investigate whether the decrease in posting was driven mainly by a decrease in the number of posts authored by new or inexperienced users, we run the same regression as in <span id="jumplink-M0001"></span><a href="#M0001">Eq. 1</a> separately for weekly posts made by users with different levels of prior experience. We assign each post the number of previous posts the user had made and differentiate between four groups of posts: posts by “new” users who have not posted before, posts by “inexperienced” users who posted between 1 and 10 times, posts by “experienced” users with between 11 and 100 prior posts, and posts by “expert” users who posted more than 100 times previously.</p>                    <h4 scrollto-destination="483096357" id="483096357" data-legacy-id="pgae400-s2.2.2">Event study</h4>
<p>When analyzing the effect of ChatGPT on activity across programming languages, we can no longer compare data from Stack Overflow with the counterfactual platforms. This is because the tags annotating posts are different between Stack Exchange platforms. Therefore, we study ChatGPT’s heterogeneous effects using an event-study specification. For each programming language <em>i</em> (identified by a tag), we model the standardized number of posts in a week <em>t</em> on Stack Overflow by fitting a simple linear time trend with seasonal effects:</p><div><div id="jumplink-M0002" content-id="M0002"><p><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mover><mrow><mi mathvariant="normal">Posts</mi></mrow><mo accent="false">¯</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo xmlns="">=</mo><msub xmlns=""><mi>β</mi><mn>0</mn></msub><mo xmlns="">+</mo><msub xmlns=""><mi>β</mi><mn>1</mn></msub><mi xmlns="">t</mi><mo xmlns="">+</mo><msub xmlns=""><mi>β</mi><mn>2</mn></msub><mrow xmlns=""><mi mathvariant="normal">ChatGPT</mi></mrow><mo xmlns="">+</mo><msub xmlns=""><mi>β</mi><mn>3</mn></msub><mo stretchy="false" xmlns="">(</mo><mi xmlns="">t</mi><mo xmlns="">×</mo><mrow xmlns=""><mi mathvariant="normal">ChatGPT</mi></mrow><mo stretchy="false" xmlns="">)</mo><mo xmlns="">+</mo><mi xmlns="">η</mi><mo xmlns="">+</mo><msub xmlns=""><mi>ϵ</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo xmlns="">,</mo></math></p></div><p><span>(2)</span></p></div><p>where <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mover xmlns=""><msub><mrow><mi mathvariant="normal">Posts</mi></mrow><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo accent="false">¯</mo></mover></math></span> stands for the standardized number of posts associated with a programming language <em>i</em> in a week <em>t</em>. We standardize the dependent variable in order to be better able to compare effects across programming languages with different numbers of posts.<sup><span id="jumplink-FN4"></span>d</sup><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>β</mi><mn>1</mn></msub><mo stretchy="false" xmlns="">(</mo><mi xmlns="">t</mi><mo stretchy="false" xmlns="">)</mo></math></span> captures the linear time trend and <em>η</em> are seasonal (month of year) fixed effects. ChatGPT equals one if the week <em>t</em> is after the release of ChatGPT and zero otherwise. Coefficient <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>β</mi><mn>2</mn></msub></math></span> captures the change in the intercept, while coefficient <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>β</mi><mn>3</mn></msub></math></span> reflects the change in the slope of the time trend following the release of ChatGPT. We report HAC standard errors.</p>                    <h4 scrollto-destination="483096361" id="483096361" data-legacy-id="pgae400-s2.2.3">Additional regression analysis with the Stack Overflow 2023 survey</h4>
<p>We run an additional model using Stack Overflow survey data to corroborate our findings. We compute the association between self-reported individual activity on Stack Overflow and the adoption of ChatGPT by estimating the following logistic regression:</p><div><div id="jumplink-M0003" content-id="M0003"><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace" displaystyle="true" xmlns=""><mtr><mtd><mrow><mi mathvariant="normal">log</mi></mrow><mstyle scriptlevel="0"><mrow><mo maxsize="2.470em" minsize="2.470em">(</mo></mrow></mstyle><mrow><mfrac><msub><mrow><mi mathvariant="normal">Activity</mi></mrow><mrow><mi>d</mi><mo>,</mo><mi>i</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mrow><mi mathvariant="normal">Activity</mi></mrow><mrow><mi>d</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><mstyle scriptlevel="0"><mrow><mo maxsize="2.470em" minsize="2.470em">)</mo></mrow></mstyle></mtd><mtd><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><mo stretchy="false">(</mo><msub><mrow><mi mathvariant="normal">Use</mi><mspace width=".1em"></mspace><mi mathvariant="normal">ChatGPT</mi></mrow><mi>d</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd></mtd><mtd><mspace width="1em"></mspace><mo>+</mo><msub><mi>β</mi><mn>2</mn></msub><mo stretchy="false">(</mo><msub><mi>X</mi><mi>d</mi></msub><mo stretchy="false">)</mo><mo>+</mo><msub><mrow><mi mathvariant="normal">Age</mi></mrow><mi>d</mi></msub><mo>+</mo><msub><mi>C</mi><mi>d</mi></msub><mo>+</mo><msub><mi>I</mi><mi>d</mi></msub><mo>+</mo><msub><mrow><mi mathvariant="normal">Lang</mi></mrow><mi>i</mi></msub><mo>+</mo><msub><mrow><mi mathvariant="normal">Type</mi></mrow><mi>d</mi></msub><mo>+</mo><msub><mi>ϵ</mi><mrow><mi>d</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo></mtd></mtr></mtable></math></p></div><p><span>(3)</span></p></div><p>where <em>d</em> stands for the developer and <em>i</em> denotes a programming language used by the developer. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mrow><mi mathvariant="normal">Activity</mi></mrow><mrow><mi>d</mi><mo>,</mo><mi>i</mi></mrow></msub></math></span> corresponds to the probability of being a frequent Stack Overflow visitor/contributor. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>X</mi><mi>d</mi></msub></math></span> comprises a set of controls at the developer’s level: a dummy of whether the developer is a professional software engineer, education level, employment status, working mode (remote, hybrid, or in-person), and years of coding. We also add age (Age), country (<em>C</em>), industry (<em>I</em>), programming language (Lang), and developer type (Type) (e.g. researcher, front-end, back-end, full-stack, QA, etc.) fixed effects. Because most developers report using more than one programming language, we expand the dataset to the developer × language level. We apply weights (1/number of languages) to avoid double counts.<sup><span id="jumplink-FN5"></span>e</sup> We cluster standard errors at the programming language level to allow for common shocks. While the cross-sectional nature of the survey data does not allow us to interpret the results as causal, we try to reduce the endogeneity by controlling for a rich set of the above individual characteristics that are likely to influence both the adoption of ChatGPT and Stack Overflow contributions. In this way, we are comparing how the contributions to Stack Overflow vary between ChatGPT adopters and nonadopters, who are otherwise very similar to each other.</p>                    <h2 scrollto-destination="483096365" id="483096365" data-legacy-id="pgae400-s3">Results</h2>
                    <h3 scrollto-destination="483096366" id="483096366" data-legacy-id="pgae400-s3.1">Decrease in posting activity</h3>
<p>Figure <span id="jumplink-pgae400-F1"></span>1a shows the evolution of activity on Stack Overflow from January 2016 to June 2023. Up to 2022 there was a gradual decrease in activity from roughly 110,000 to 60,000 posts per week, that is roughly 7,000k posts less per week each year. However, after the release of ChatGPT (2022 November 30) posting activity decreased sharply, with the weekly average falling from around 60,000 posts to 40,000 within 6 months. Compared to the pre-ChatGPT trend, this decrease represents more than 5 years worth of deceleration in just half a year.</p><p>The decrease in activity on Stack Overflow is larger than for similar platforms for which we expect ChatGPT to be a less viable substitute. Figure <span id="jumplink-pgae400-F1"></span>1b shows the standardized posting activity on Stack Overflow, the Russian- and Chinese-language counterparts of Stack Overflow, and two mathematics Q&amp;A platforms. We standardize posting activity by the average and standard deviation of post counts within each platform prior to the release of ChatGPT.</p><p>Figure <span id="jumplink-pgae400-F1"></span>1b highlights that Stack Overflow activity deviates markedly from activity on the other platforms after the release of ChatGPT. The plot visualizes the standardized posting activity within each platform since early 2022. Smoothed weekly activity varies between plus and minus two standard deviations for all platforms for most of 2022. Events, such as the Chinese New Year and other holidays and the start of the Russian invasion of Ukraine, are visible. Following the release of ChatGPT, we observe a significant and persistent decline in activity on Stack Overflow.</p><p>Our difference-in-differences model reveals that Stack Overflow activity significantly declined after the release of ChatGPT, and that this effect became more pronounced over time. Table <span id="jumplink-pgae400-T1"></span>1 reports our estimates, the first column indicates that ChatGPT decreased posting activity on Stack Overflow by 15% (<span>⁠<span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mn xmlns="">1</mn><mo xmlns="">−</mo><msup xmlns=""><mi>e</mi><mrow><mo>−</mo><mn>0.163</mn></mrow></msup></math>⁠</span>). Note that this is a measure of the average effect across the 6 months of post-ChatGPT data we consider. If ChatGPT adoption is gradual, we expect that the effect observed at the end of the data will be larger than at the beginning.</p>                    <div content-id="pgae400-T1"><div id="pgae400-T1" data-id="pgae400-T1"><p><span id="label-16735">Table 1.</span></p><p>Results of a difference-in-differences model, estimating the change in activity observed weekly on stack overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted.</p> </div><div><table role="table" aria-labelledby="
                        label-16735" aria-describedby="
                        caption-16735"><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of questions</td><td>Weekday posts</td></tr><tr><td>Variables</td><td></td><td></td><td></td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.163</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><mn xmlns="">0.105</mn><mo xmlns="">+</mo></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.151</mn><mo>*</mo></msup></math></span></td></tr><tr><td></td><td>(0.0584)</td><td>(0.0597)</td><td>(0.0613)</td></tr><tr><td>Observations</td><td>370</td><td>370</td><td>370</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0458</td><td>0.0189</td><td>0.0294</td></tr></tbody></table></div><div><table><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of questions</td><td>Weekday posts</td></tr><tr><td>Variables</td><td></td><td></td><td></td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.163</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><mn xmlns="">0.105</mn><mo xmlns="">+</mo></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.151</mn><mo>*</mo></msup></math></span></td></tr><tr><td></td><td>(0.0584)</td><td>(0.0597)</td><td>(0.0613)</td></tr><tr><td>Observations</td><td>370</td><td>370</td><td>370</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0458</td><td>0.0189</td><td>0.0294</td></tr></tbody></table></div><div><p><span><p>All regressions comprise platform fixed effects and week fixed effects. The standard error of the estimate clustered on month is reported in parentheses. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span> (within) is derived after differencing out week and platform fixed effects. Significance codes: ***: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.001</mn></math>⁠</span>, <sup>**</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.01</mn></math>⁠</span>, <sup>*</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.05</mn></math>⁠</span>, <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">+</mo></math>⁠</span>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.1</mn></math>⁠</span>.</p></span></p></div></div><div><div id="pgae400-T1" data-id="pgae400-T1"><p><span id="label-16735">Table 1.</span></p><p>Results of a difference-in-differences model, estimating the change in activity observed weekly on stack overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted.</p> </div><div><table role="table" aria-labelledby="
                        label-16735" aria-describedby="
                        caption-16735"><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of questions</td><td>Weekday posts</td></tr><tr><td>Variables</td><td></td><td></td><td></td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.163</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><mn xmlns="">0.105</mn><mo xmlns="">+</mo></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.151</mn><mo>*</mo></msup></math></span></td></tr><tr><td></td><td>(0.0584)</td><td>(0.0597)</td><td>(0.0613)</td></tr><tr><td>Observations</td><td>370</td><td>370</td><td>370</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0458</td><td>0.0189</td><td>0.0294</td></tr></tbody></table></div><div><table><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of questions</td><td>Weekday posts</td></tr><tr><td>Variables</td><td></td><td></td><td></td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.163</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><mn xmlns="">0.105</mn><mo xmlns="">+</mo></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.151</mn><mo>*</mo></msup></math></span></td></tr><tr><td></td><td>(0.0584)</td><td>(0.0597)</td><td>(0.0613)</td></tr><tr><td>Observations</td><td>370</td><td>370</td><td>370</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0458</td><td>0.0189</td><td>0.0294</td></tr></tbody></table></div><div><p><span><p>All regressions comprise platform fixed effects and week fixed effects. The standard error of the estimate clustered on month is reported in parentheses. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span> (within) is derived after differencing out week and platform fixed effects. Significance codes: ***: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.001</mn></math>⁠</span>, <sup>**</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.01</mn></math>⁠</span>, <sup>*</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.05</mn></math>⁠</span>, <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">+</mo></math>⁠</span>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.1</mn></math>⁠</span>.</p></span></p></div></div><p>Indeed, our second specification observes exactly this trend. We visualize the weekly estimates of the relative change in the Stack Overflow activity in Fig. <span id="jumplink-pgae400-F2"></span>2. This figure shows the impact of ChatGPT is increasing over time and is greater in magnitude than the average post-ChatGPT effect estimated in Table <span id="jumplink-pgae400-T1"></span>1 by the end of our study period. By the end of April 2023, coinciding with a peak in traffic to ChatGPT, <sup><span id="jumplink-FN6"></span>f</sup> the estimated decrease in activity stabilizes at around 25%.</p>                    <div data-id="pgae400-f2" data-content-id="pgae400-f2" swap-content-for-modal="true"><p><img src="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/m_pgae400f2.jpeg?Expires=1731613731&amp;Signature=WXvvneTac~xvlDnzRAJ15zn9dlANlB7kPvy4bvTaS0HzOMfo94DYC3Mdk30ToYQJSr~MqFOO7qgOXxhENPecNKXFLUasXo3ZGOJxw02xXlClEPVm7pst-0FEXR8c9HYr~3p1CmvHSicYqHGdAjUlqlXMF2H7vafyxlF~DnVDWBirizRQrhSD7Iw4DqslNoEc5iDKJJoSubgf6EqJFhQ1mA01nMtODZkIRoE6D0m1ukGR-JTSxAfvT1sJyV3mSTpqQIVvF5DTM4S6EL22rFqd0HcL6RxYnvF3W5bJFzKcUp8E1ZUbLkOLEZBfeFEcbBxZAK7r6-5a6Z2kYmXEo-FyVg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Difference-in-differences analysis for posting activities. The dashed line marks the week of 2022 November 27—the release week of ChatGPT. Eight weeks after its introduction, we observe a steady decline in the activity of Stack Overflow. The plotted coefficients correspond to the interaction between a weekly dummy and posting on Stack Overflow. We normalize the effects to 0 in the week before the public release of ChatGPT by dropping the indicator for that week from the regression. The reported CIs are at 95%. The regression comprises platform fixed effects and week fixed effects." data-path-from-xml="pgae400f2.jpg"></p><div><p>Fig. 2.</p><p>Difference-in-differences analysis for posting activities. The dashed line marks the week of 2022 November 27—the release week of ChatGPT. Eight weeks after its introduction, we observe a steady decline in the activity of Stack Overflow. The plotted coefficients correspond to the interaction between a weekly dummy and posting on Stack Overflow. We normalize the effects to 0 in the week before the public release of ChatGPT by dropping the indicator for that week from the regression. The reported CIs are at 95%. The regression comprises platform fixed effects and week fixed effects.</p></div></div><p>We also tested for heterogeneity in subsets of the data, considering only questions (rather than counting both questions and answers) and posts on weekdays. In both subsets, our estimates did not deviate significantly from the main result: we estimate a 10% relative decrease in questions and 14% relative decrease in posts on weekdays (see the second and third column of Table <span id="jumplink-pgae400-T1"></span>1). Our results are robust to using alternative transformations of the outcome (Tables <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S1 and S2</a></span>), adding platform-specific trends (Table <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S3</a></span>), and extending the time window of the analysis (Table <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S4</a></span>).</p><p>The decrease in Stack Overflow activity is consistent with the individual-level evidence from the 2023 Stack Overflow Developer Survey. In particular, we estimate the relationship between self-reported ChatGPT usage and Stack Overflow visit and contribution frequency using specification <span id="jumplink-M0003"></span><a href="#M0003">Eq. 3</a>. We consider several binary variables as the outcomes: <em>Contribute to Stack Overflow ever (weekly or more)</em> is equal to one if a developer has contributed to Stack Overflow at least once (weekly or more often) and zero otherwise, <em>Visit Stack Overflow daily</em> is equal to one if a developer reports visiting the platform once or more times per day. As participants were recruited through Stack Overflow and related platforms, they represent a selected group of engaged Stack Overflow users, and, therefore, their levels of activity are high: about 75% of all respondents have contributed to the platform at least once, and 42% visit it daily. We report the results in Table <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S7</a></span>. The coefficients represent changes in log odds of the outcomes, and we also compute the average marginal effects of ChatGPT adoption on the likelihood of frequent contributions/visits.</p><p>We find that ChatGPT adopters are less likely to contribute to Stack Overflow and to visit the platform frequently compared to nonadopters of the same age, experience, education, employment status, working mode, industry, and programming language used. Moreover, even when we limit the sample to the most active respondents (i.e. those who have contributed at least once to Stack Overflow), we can still detect statistically significant differences in the probability of both contributing weekly and visiting daily between otherwise similar ChatGPT adopters and nonadopters. The magnitude of the effect is not very high. For instance, the average marginal effect of ChatGPT on the likelihood of contributing to Stack Overflow weekly or more is about 0.8 percentage points (or 2.7% lower probability). However, these results are likely to be downward biased because of the selection into survey participation: those who use Stack Overflow less frequently (including those who have reduced their activity because of ChatGPT) were less likely to respond.</p>                    <h4 scrollto-destination="483096377" id="483096377" data-legacy-id="pgae400-s3.1.1">Post and user heterogeneities</h4>
<p>A decrease in overall activity on Stack Overflow is not an issue if it is rather the less interesting questions that are outsourced to ChatGPT. We use a post’s score (i.e. difference between upvotes and downvotes) observed 5 weeks after its creation as a proxy of its value—good (bad) posts have a positive (negative) score, while neutral questions have a score of zero.</p><p>If ChatGPT is displacing bad questions, we would expect that after its release there would be a downward trend in the share of bad questions. However, as Fig. <span id="jumplink-pgae400-F3"></span>3a shows, while there was a slight uptick in the fraction of good questions, these were mostly replacing neutral questions and the trend of bad questions was flat. The short-lived increase in the fraction of good questions may be a result of ChatGPT inducing interest in novel topics, such as large language models, which usually results in good questions (see our Discussion section below on the increase in interest in CUDA). With respect to answers, there was no change in the trends of good, bad, and neutral answers. In general, there is a remarkable stability in the proportions post-ChatGPT. We confirm these results by estimating a difference-in-differences specification where the outcome is the number of up(down) votes that posts published in a given week receive over the first 5 weeks, normalized to the total number of posts from this week (Table <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S5 and Fig. S1</a></span>). Unlike our previous results on posts, we do not detect any effect of ChatGPT.</p>                    <div data-id="pgae400-f3" data-content-id="pgae400-f3" swap-content-for-modal="true"><p><img src="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/m_pgae400f3.jpeg?Expires=1731613731&amp;Signature=Mad4b29cp5iv46g3rARmcVvhYjKqjNU4s4lEEhCn0l1nuykhHBkhVaricVh8UnErSYHkT4Efx9TuJiJkvtNeaJbpPBWqx8camMB~RQr-MUSzrOeXKZW8CQUXboBU8P7y~U-IQxac2XB60ft4tC27JYs8VWr7Wo2BpQ1nAQ5UQCaxDd9mlCpKOETV3xUkzCn5hyroDGJq-KJK3Xj4o5tTKy6GUdaJGqTdaktzTLssbmVZ7h18ctK4ht8zD0JflNIHpVSZOF1zsAaLAvnpUwBvJmhvamlPmXqUuEPCeQnVBZEpg1ovSwWLfxjpPjryH7MLcWVYCjE1WjKhr2XMu7VzKA__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="a) The weekly time series of the fraction of neutral, good, and bad questions and answers. Good (bad) post are those with a positive (negative) score after 5 weeks of its creation, while neutral questions have a zero score. The horizontal axis indicates the week of the post and the dashed line the week of the release of ChatGPT. We observe no major change in trends in bad posts since the release of ChatGPT. b) Weekly counts of questions and answers, respectively, by users, binned by experience level at the time of posting. We observe larger decreases in posting by users with previous experience post-ChatGPT." data-path-from-xml="pgae400f3.jpg"></p><div><p>Fig. 3.</p><p>a) The weekly time series of the fraction of neutral, good, and bad questions and answers. Good (bad) post are those with a positive (negative) score after 5 weeks of its creation, while neutral questions have a zero score. The horizontal axis indicates the week of the post and the dashed line the week of the release of ChatGPT. We observe no major change in trends in bad posts since the release of ChatGPT. b) Weekly counts of questions and answers, respectively, by users, binned by experience level at the time of posting. We observe larger decreases in posting by users with previous experience post-ChatGPT.</p></div></div><p>Votes do not capture all aspects of quality or more generally the ways in which ChatGPT may have influenced content on Stack Overflow. For example, users with different levels of experience contribute different kinds of content to the platform. New users tend to ask more basic questions, which ChatGPT may answer better. In contrast, experienced users may ask more sophisticated questions beyond the abilities of ChatGPT. A heterogeneous effect of ChatGPT on participation on Stack Overflow by users stratified by experience would have significant implications for content.</p><p>Table <span id="jumplink-pgae400-T2"></span>2 reports changes in activity estimated in a difference-in-differences specification, decomposed by prior user experience at the time of posting. Our estimates show that, while posts made by first-time users on Stack Overflow decreased only slightly relative to the control platforms, inexperienced, experienced, and expert users made significantly fewer posts on average after the release of ChatGPT.<sup><span id="jumplink-FN7"></span>g</sup> The point estimates (a reduction of about 21% relative to the counterfactual platforms) for both inexperienced and experienced users are almost identical, suggesting no significant difference in the decrease in activity. Table <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S8</a></span> estimates separate effects of ChatGPT on questions and answers. Interestingly, while inexperienced users reduce the number of their questions and answers to a similar extent, the effects for experienced and expert users are more pronounced for posting answers. We could link the latter result to lower incentives to contribute to Stack Overflow: as fewer developers are using the platform, the visibility “premium” that could be earned by answering questions becomes lower.</p>                    <div content-id="pgae400-T2"><div id="pgae400-T2" data-id="pgae400-T2"><p><span id="label-19892">Table 2.</span></p><p>Results of a difference-in-differences model, estimating the change in activity observed weekly on stack overflow following the release of ChatGPT by user group, relative to activity on three other platforms (we exclude segment fault as we do not have access to user experience data) less likely to have been impacted.</p> </div><div><table role="table" aria-labelledby="
                        label-19892" aria-describedby="
                        caption-19892"><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th><th>(4)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td></tr><tr><td>VARIABLES</td><td>NewUser</td><td>InexperiencedUser</td><td>ExperiencedUser</td><td>ExpertUser</td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.0833</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.245</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.254</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.168</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td></tr><tr><td></td><td>(0.0376)</td><td>(0.0529)</td><td>(0.0424)</td><td>(0.0292)</td></tr><tr><td>Observations</td><td>296</td><td>296</td><td>296</td><td>296</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0259</td><td>0.198</td><td>0.235</td><td>0.104</td></tr></tbody></table></div><div><table><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th><th>(4)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td></tr><tr><td>VARIABLES</td><td>NewUser</td><td>InexperiencedUser</td><td>ExperiencedUser</td><td>ExpertUser</td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.0833</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.245</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.254</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.168</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td></tr><tr><td></td><td>(0.0376)</td><td>(0.0529)</td><td>(0.0424)</td><td>(0.0292)</td></tr><tr><td>Observations</td><td>296</td><td>296</td><td>296</td><td>296</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0259</td><td>0.198</td><td>0.235</td><td>0.104</td></tr></tbody></table></div><div><p><span><p>Posts by new users are posts by users with no previous posts at the time of posting. Inexperienced users have posted 1–10 times before, experienced users 11–100, and experts more than 100 times. All regressions comprise platform fixed effects and week fixed effects. The standard error of the estimate clustered on month is reported in parentheses. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span> (within) is derived after differencing out week and platform fixed effects. Significance codes: ***: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.001</mn></math>⁠</span>, <sup>**</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.01</mn></math>⁠</span>, <sup>*</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.05</mn></math>⁠</span>, <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">+</mo></math>⁠</span>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.1</mn></math>⁠</span>.</p></span></p></div></div><div><div id="pgae400-T2" data-id="pgae400-T2"><p><span id="label-19892">Table 2.</span></p><p>Results of a difference-in-differences model, estimating the change in activity observed weekly on stack overflow following the release of ChatGPT by user group, relative to activity on three other platforms (we exclude segment fault as we do not have access to user experience data) less likely to have been impacted.</p> </div><div><table role="table" aria-labelledby="
                        label-19892" aria-describedby="
                        caption-19892"><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th><th>(4)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td></tr><tr><td>VARIABLES</td><td>NewUser</td><td>InexperiencedUser</td><td>ExperiencedUser</td><td>ExpertUser</td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.0833</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.245</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.254</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.168</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td></tr><tr><td></td><td>(0.0376)</td><td>(0.0529)</td><td>(0.0424)</td><td>(0.0292)</td></tr><tr><td>Observations</td><td>296</td><td>296</td><td>296</td><td>296</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0259</td><td>0.198</td><td>0.235</td><td>0.104</td></tr></tbody></table></div><div><table><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th><th>(4)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td></tr><tr><td>VARIABLES</td><td>NewUser</td><td>InexperiencedUser</td><td>ExperiencedUser</td><td>ExpertUser</td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.0833</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.245</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.254</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.168</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td></tr><tr><td></td><td>(0.0376)</td><td>(0.0529)</td><td>(0.0424)</td><td>(0.0292)</td></tr><tr><td>Observations</td><td>296</td><td>296</td><td>296</td><td>296</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0259</td><td>0.198</td><td>0.235</td><td>0.104</td></tr></tbody></table></div><div><p><span><p>Posts by new users are posts by users with no previous posts at the time of posting. Inexperienced users have posted 1–10 times before, experienced users 11–100, and experts more than 100 times. All regressions comprise platform fixed effects and week fixed effects. The standard error of the estimate clustered on month is reported in parentheses. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span> (within) is derived after differencing out week and platform fixed effects. Significance codes: ***: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.001</mn></math>⁠</span>, <sup>**</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.01</mn></math>⁠</span>, <sup>*</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.05</mn></math>⁠</span>, <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">+</mo></math>⁠</span>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.1</mn></math>⁠</span>.</p></span></p></div></div><p>Overall, our analysis shows little evidence that ChatGPT tends to replace low-quality posts and no evidence that it replaced posts by inexperienced users relative to experts and experienced users.</p>                    <h3 scrollto-destination="483096385" id="483096385" data-legacy-id="pgae400-s3.2">Heterogeneities across programming languages</h3>
<p>Next, we investigated differences in the impact of ChatGPT on posts about different programming languages, finding significant heterogeneities. In Facet A of Fig. <span id="jumplink-pgae400-F4"></span>4, we plot the estimated effects (slope changes in the linear time trend after the introduction of ChatGPT) for those 69 tags that we connected to a programming language on GitHub. We estimate a negative effect of ChatGPT for most tags, but the estimates range between a 0.25 standard deviation decrease in slope (i.e. change per week following the ChatGPT release) to a 0.03 standard deviation <em>increase</em>. We observe that some of the widely used languages like Python and Javascript are the most impacted by ChatGPT. Interestingly, the model estimates that posts about CUDA have increased (though not significantly) after ChatGPT was released. CUDA is an application programming interface created by Nvidia, a graphics card manufacturer, that facilitates the use of graphics cards for computational tasks, in particular for machine learning and AI. This exception again demonstrates the impact of ChatGPT on the world of computer programming: people are increasingly interested in software relating to AI.</p>                    <div data-id="pgae400-f4" data-content-id="pgae400-f4" swap-content-for-modal="true"><p><img src="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/m_pgae400f4.jpeg?Expires=1731613731&amp;Signature=1oZ7w0HBe0ElTKKgDektcwu2~M6scG70I0u3y5lGHZNEgTyPgguOADMKoj~oi5sSSzRF80~xZ8WSrxilZoiTo2XETnWq62GkAkzFY~QObzO3KGA7ETgpBIUeI3YpsPgacQFwWROPtPF92zDtQeAcGdiMz1OBVVvtlpL0qhL90eg49PHwhT6-f9ITgEnX0bZoNWOPznZ5VQHoqxbV~F1TP~pSQbD1GTZ-3LtUGX~ynAkqQHuqWKSTuJD7~u8mJ-4HUw6o-Jpkicmx1NVfPZ0~hj8R6DJiL2zIgXdMoR4A4W43VJl83BTbe8bmY8rQSWwxPCYiXPEpGJG55tiTHbd8qw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="a) The event study estimates of the effect of ChatGPT’s release on activity on a selection of tags on Stack Overflow. We report HAC-corrected 95% CIs. b) The relationship between estimated effects and salary data from the Stack Overflow Developer Survey. We find no significant relationship. c) The relationship between the number of GitHub repositories using a tag and the estimated effect of ChatGPT on that tag. In both b) and c), we plot a linear fit with bootstrapped 95% CIs. The dashed line in b) indicates that the correlation is not significant." data-path-from-xml="pgae400f4.jpg"></p><div><p>Fig. 4.</p><p>a) The event study estimates of the effect of ChatGPT’s release on activity on a selection of tags on Stack Overflow. We report HAC-corrected 95% CIs. b) The relationship between estimated effects and salary data from the Stack Overflow Developer Survey. We find no significant relationship. c) The relationship between the number of GitHub repositories using a tag and the estimated effect of ChatGPT on that tag. In both b) and c), we plot a linear fit with bootstrapped 95% CIs. The dashed line in b) indicates that the correlation is not significant.</p></div></div><p>Given that previous research suggests that high-wage jobs are more exposed to ChatGPT (<span id="jumplink-pgae400-B33"></span>33), we test whether the impact of ChatGPT is more predominant among better-paid languages. We source salary data from the 2022 Stack Overflow Developer Survey, focusing on US-based developers and calculating medians of reported salaries. In Fig. <span id="jumplink-pgae400-F4"></span>4b, we compare the estimated impact of ChatGPT on different languages against the salary data of developers using those languages. We observe no clear relationship between the estimated labor market value of a specific language and changes in posting behavior in that language post-ChatGPT.</p><p>To better understand the relationship between the size of the user base of a programming language and how it is impacted by ChatGPT, we compare our estimates with data from GitHub, the largest online platform for collaborative software development. Among other sources, ChatGPT was trained on data from GitHub. Because training data were collected up to September 2021, we use data on language use on GitHub up to June 2021. In Facet C of Fig. <span id="jumplink-pgae400-F4"></span>4, we visualize the relationship between the number of GitHub repositories (coding projects) in a specific language and the estimated impact of ChatGPT on that language. We observe that languages with more GitHub repositories tend to be more significantly impacted by the release of ChatGPT in terms of associated activity on Stack Overflow (Pearson’s <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">ρ</mi><mo xmlns="">=</mo><mo xmlns="">−</mo><mn xmlns="">0.45</mn><mo xmlns="">,</mo><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.001</mn></math>⁠</span>). This result is confirmed by estimating a difference-in-differences specification that compares the change in posting following the release of ChatGPT between more and less popular programming languages as measured by the number of GitHub commits attributed to a given language as of 2021 (Table <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S6</a></span>).</p>                    <h3 scrollto-destination="483096390" id="483096390" data-legacy-id="pgae400-s3.3">Subsequent dynamics</h3>
<p>Using the Stack Exchange Data Explorer, we extended the timeseries of weekly posts to Stack Overflow to Spring 2024. We visualize this data in Fig. <span id="jumplink-pgae400-F5"></span>5. We observe a continued, if slower, decrease in weekly posting activity after the end of our statistical analyses. In raw terms, the number of weekly posts to Stack Overflow has fallen from 60,000 to 30,000 from May 2022 to May 2024, with much of that change happening in the 6 months following the release of ChatGPT. Again, this suggests that the 25% estimate of the effect of ChatGPT on Stack Overflow should be interpreted as a lower bound effect, which is likely still growing.</p>                    <div data-id="pgae400-f5" data-content-id="pgae400-f5" swap-content-for-modal="true"><p><img src="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/m_pgae400f5.jpeg?Expires=1731613731&amp;Signature=Gopr5dzWjofg9tEiwPD07o5ZfI29VIxgX1q-94F3bTpTluFRJUaoZKZwhW2vXSyx3o8ozAsf7PHzv9dQV1g9yRfY8uJPhJDjH-6~7pjaLD5JCmo60eoq8UckQo052jPDEmiZLNX0d7hpaRTe1YKj4KsFR-LWvkuTFcSYGYaUvTLJi2HcuGJ0JwR7T46OUjTDm4R2AXD0vy3hG~JhQopeaVuf6pIEfZNz2thCDEoAxntOKnJF4BsgL~PcSgYXqxJ3OXgud3zm0X3lIXm4PqXk2-Zr-RiVP9NBjyHQ1Xei2oqJmw4~aLDGIdrOOMnXwv-i29wh3zg4Harr~2WBPqLdRQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="An extended timeseries of the weekly posts to Stack Overflow. We highlight the release of ChatGPT and the conclusion of the data we use in the statistical analyses, respectively. After May 2023, the decline in posting activity continues, albeit at a slower rate." data-path-from-xml="pgae400f5.jpg"></p><div><p>Fig. 5.</p><p>An extended timeseries of the weekly posts to Stack Overflow. We highlight the release of ChatGPT and the conclusion of the data we use in the statistical analyses, respectively. After May 2023, the decline in posting activity continues, albeit at a slower rate.</p></div></div><p>An extension of the difference-in-differences analysis would not yield reliable estimates of the relative impact of LLMs of Stack Overflow for several reasons. First, the subsequent proliferation of ChatGPT or-better quality LLMs, including open source models and models available in Russia and China mean that the reference timeseries are no longer valid counterfactuals. Moreover, advances in LLM capabilities have significantly improved their performance in mathematical tasks. Thus, we do not extend our difference-in-differences analyses.</p>                    <h2 scrollto-destination="483096394" id="483096394" data-legacy-id="pgae400-s4">Discussion</h2>
<p>The rate at which people have adopted ChatGPT is one of the fastest in the history of technology (<span id="jumplink-pgae400-B7"></span>7). It is essential that we better understand what activities this new technology displaces and what second-order effects this substitution may have (<span id="jumplink-pgae400-B34"></span>34, <span id="jumplink-pgae400-B35"></span>35). This article shows that after the introduction of ChatGPT there was a sharp decrease in human content creation on Stack Overflow. We compare the decrease in activity on Stack Overflow with other Stack Exchange platforms where current LLMs are less likely to be used. Using a difference-in-differences model, we estimates a <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mn xmlns="">25</mn><mi mathvariant="normal" xmlns="">%</mi></math></span> decline to posts on Stack Overflow relative to the counterfactual platforms within 6 months of ChatGPT’s release. We interpret this as a lower bound as ChatGPT is likely to have had small but growing impact on the counterfactual platforms as well. The Stack Overflow Developer Survey confirms that people using ChatGPT were less likely to post questions or answers on Stack Overflow.</p><p>We observe no large change in social feedback on posts, measured using votes, nor in the experience composition of posting users following ChatGPT’s release. These results suggest that average post quality has not changed, nor has ChatGPT replaced only the new and inexperienced users. Posting activity related to more popular programming languages decreased more on average than that for more niche languages. Given that LLMs performance depends on the quantity of training data, this finding suggests that users are more likely to substitute Stack Overflow with ChatGPT with respect to languages LLMs are more knowledgeable about. Consequently, the widespread adoption of LLMs will likely decrease the provision of digital public goods including open data previously generated by interactions on the web.</p><p>Two of our results offer some limited reasons for optimism. While posting activity on Stack Overflow decreased among inexperienced, experienced, and expert users relative to the control platforms, content created by new users remained relatively stable. New users are known to be essential to the long-run health of online communities (<span id="jumplink-pgae400-B36"></span>36). However, this optimism should be nuanced given that, if new users start behaving as inexperienced users did, then new users will also be more to likely reduce their activity in Stack Overflow. The second is that the impact of ChatGPT was less on more niche languages used by fewer people, suggesting that online conversations around such languages and the valuable information they generate will continue.</p><p>Recent work by Burtch et al. (<span id="jumplink-pgae400-B37"></span>37) studying the evolution of activity on Stack Overflow and Reddit found similar results to ours. Using a synthetic control method to adjust for seasonality, the authors report a roughly 20% decrease in posting activity on Stack Overflow within 15 weeks of the release of ChatGPT, and find similar heterogeneities among programming languages. These findings complement ours, which are derived from a more conservative analysis using counterfactual platforms. One difference in our findings is that their method finds a sharp decrease in posts by new users, while we observe fewer posts by more experienced users on Stack Overflow compared to the counterfactual platforms. It would be valuable for future work to resolve this ambiguity given the importance of new users to platform health discussed above.</p><p>Our results and data have some shortcomings that point to other open questions about the use and impact of LLMs. First, while we can present strong evidence that ChatGPT decreased the posting activity in Stack Overflow, we can only partially assess quality of posting activity using data on upvotes and downvotes. Users may be posting more challenging questions, ones that LLMs cannot (yet) address, to Stack Overflow. Future work should examine whether continued activity on Stack Overflow is more complex or sophisticated on average than posts from prior to ChatGPT release. Similarly, ChatGPT may have reduced the volume of duplicate questions about simple topics, though this is unlikely to impact our main results as duplicates are estimated to account for only <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mn xmlns="">3</mn><mi mathvariant="normal" xmlns="">%</mi></math></span> of posts (<span id="jumplink-pgae400-B38"></span>38), and we do not observe significant changes in voting outcomes.</p><p>A second limitation of our work is that we cannot observe the extent to which Russian- and Chinese-language users of the corresponding Q&amp;A platforms are actually hindered from accessing ChatGPT; indeed recent work has shown a spike in VPN and Tor activity following the blocking of ChatGPT in Italy (<span id="jumplink-pgae400-B30"></span>30). While our results are robust to excluding the Chinese and the Russian counterfactuals, given the potential economic importance of ChatGPT and similar LLMs, it is essential that we better understand how such bans and blocks impact the accessibility of these tools (<span id="jumplink-pgae400-B39"></span>39, <span id="jumplink-pgae400-B40"></span>40). Finally, we do not address the issue that ChatGPT may be used to generate Stack Overflow content. Stack Overflow policy effectively banned posts authored by ChatGPT within a week of its release. In any case, a significant amount of ChatGPT generated content on Stack Overflow would mean that our measures underestimate the magnitude of the ChatGPT effect.</p><p>Despite these shortcomings, our results have important implications for the future of digital public goods. Before the introduction of ChatGPT, more human-generated content was posted to Stack Overflow, forming a collective digital public good due to their nonrivalrous and nonexclusionary nature—anyone with internet access can view, absorb, and extend this information, without diminishing the value of the knowledge. Now, part of this information is rather fed into privately owned LLMs like ChatGPT. This represents a significant shift of knowledge from public to private domains.</p><p>This observed substitution effect also poses several issues for the future of AI. The first is that if language models crowd out open data creation, they will be limiting their own future training data and effectiveness. The second is that owners of the current leading models have exclusive access to user inputs and feedback, which, with a relatively smaller pool of open data, gives them a significant advantage against new competitors in training future models. Third, the decline of public resources on the web would reverse progress made by the web toward democratizing access to knowledge and information. Finally, the consolidation of humans searching for information around one or a few language models could narrow our explorations and focus our attention on mainstream topics. We briefly elaborate on these points, then conclude with a wider appeal for more research on the political economy of open data and AI, and how we can incentivize continued contributions to digital public goods.</p>                    <h4 scrollto-destination="483096404" id="483096404" data-legacy-id="pgae400-s4.1.1">Training future models</h4>
<p>Our findings suggest that the widespread adoption of ChatGPT may ironically make it difficult to train future models (<span id="jumplink-pgae400-B41"></span>41). Though researchers have already expressed concerns about running out of data for training AI models (<span id="jumplink-pgae400-B21"></span>21), our results show that the use of LLMs can slow down the creation of new (open) data. Given the growing evidence that data generated by LLMs are unlikely to effectively train new LLMs (<span id="jumplink-pgae400-B22"></span>22, <span id="jumplink-pgae400-B23"></span>23), modelers face the real problem of running out of useful data. While research on using synthetic data and mixed data to train LLMs is still ongoing, current results show that use of synthetic training data can degrade performance (<span id="jumplink-pgae400-B24"></span>24) and may even amplify biases in models (<span id="jumplink-pgae400-B42"></span>42). Human input and guidance can mitigate these issues to some extent, but in general it is still unclear if synthetic data can power continued advances in LLM capabilities.</p><p>If ChatGPT truly is a “blurry JPEG” of the web (<span id="jumplink-pgae400-B25"></span>25), then in the long run, it cannot effectively replace its most important input: data derived from human activity. Indeed, OpenAI’s recent strategic partnerships with Stack Overflow and Reddit demonstrate the value of this kind of data for the continued training of LLMs.<sup><span id="jumplink-FN8"></span>h</sup> The proliferation of LLMs has already impacted other forms of data creation: many Amazon Mechanical Turk workers now generate content (i.e. respond to surveys, evaluate texts) using ChatGPT (<span id="jumplink-pgae400-B43"></span>43). And though watermarks may help humans and models identify data creators (<span id="jumplink-pgae400-B44"></span>44), the general problem of determining whether, for example, a text is written by a human or LLM is difficult at scale (<span id="jumplink-pgae400-B45"></span>45).</p>                    <h3 scrollto-destination="483096407" id="483096407" data-legacy-id="pgae400-s4.2">Competition in the AI sector</h3>
<p>A firm’s early advantage in technological innovation often leads to significant market share via various mechanisms of path dependence (<span id="jumplink-pgae400-B46"></span>46). There are increasing returns to using ChatGPT as more people use it, as it can learn from user feedback (<span id="jumplink-pgae400-B26"></span>26). Our results indicate that ChatGPT is simultaneously decreasing the amount of open training data that competitors could use to build competing models while it captures user data for itself, which may lead to technological lock-in (<span id="jumplink-pgae400-B27"></span>27). Unlike synthetic data, data on user interactions with LLMs can be used to significantly improve and tune their performance (<span id="jumplink-pgae400-B47"></span>47). We suggest that besides increasing returns to scale from network effects, the transformation of public data commons into private databases presents another mechanism by which the tech sector can become even more concentrated.</p>                    <h3 scrollto-destination="483096409" id="483096409" data-legacy-id="pgae400-s4.3">Lost economic value</h3>
<p>Digital public goods generate value in many ways besides feeding LLMs and other algorithms. For instance, Wikipedia is an important source of information worldwide, but in developing countries, readers are more often motivated by intrinsic learning goals and tend to read articles in greater detail (<span id="jumplink-pgae400-B3"></span>3). Unequal access to AI may also compound inequalities in growth and innovation between countries (<span id="jumplink-pgae400-B40"></span>40).</p><p>Digital public goods also provide direct value to the many websites that extract data from open data to complement their core services with extra information (<span id="jumplink-pgae400-B4"></span>4). For instance, there is substantial interdependence between sites like Wikipedia, Reddit, and Stack Overflow and the search engines that use them to enrich responses to user queries via infoboxes (<span id="jumplink-pgae400-B17"></span>17, <span id="jumplink-pgae400-B48"></span>48), sometimes referred to as the “paradox of re-use” (<span id="jumplink-pgae400-B18"></span>18). In the case of search engines, putting links to knowledge sources within infoboxes has mitigated the issue to some degree (<span id="jumplink-pgae400-B49"></span>49), but LLMs like ChatGPT are substituting for search engines and are much less likely to link to sources. Their widespread adoption presents a significant threat to the overall sustainability of the web (<span id="jumplink-pgae400-B50"></span>50).</p><p>Creators of digital public goods may also lose out. Contributors to Stack Overflow or Open Source Software (OSS) often enjoy indirect benefits (<span id="jumplink-pgae400-B51"></span>51). For instance, while OSS itself provides significant value in the global economy (<span id="jumplink-pgae400-B52"></span>52), OSS contributions are valuable signals of a firm’s capabilities to investors (<span id="jumplink-pgae400-B53"></span>53). Individual contributions to Stack Overflow are used to signal ability on the labor market (<span id="jumplink-pgae400-B54"></span>54). Any general tendency of ChatGPT to crowd out contributions to digital public goods, may limit these valuable signals that reduce economic frictions. On the other hand, such signaling activity may serve as a powerful incentive to keep people contributing.</p>                    <h4 scrollto-destination="483096413" id="483096413" data-legacy-id="pgae400-s4.3.1">Narrowing of information seeking</h4>
<p>The substitution effect we report likely has important second-order effects on how people search for information and their exposure to new ideas. LLMs likely favor well-established perspectives and due to their efficiency decrease the need for users to forage for information. These features of LLMs may reinforce a trend observed earlier in the context of the web. Specifically, internet search engines are thought to have pushed science toward consensus and narrower topics by improving efficiency of information search and improving the visibility of mainstream information (<span id="jumplink-pgae400-B55"></span>55). LLMs may also disincentivize the use of new or niche tools because they most amplify our productivity with those tools for which it has much training data. For instance, ChatGPT may not be able to help users of a new programming language that is has not seen many examples of. Given that LLMs are poised to change how we do research (<span id="jumplink-pgae400-B56"></span>56), present a strong competitor to search engines (<span id="jumplink-pgae400-B57"></span>57), and will likely influence our news consumption (<span id="jumplink-pgae400-B58"></span>58), we need to understand what LLM efficiency implies for our contact with diverse sources of information and incentives to try new things.</p><p>More generally, models like ChatGPT are going to generate political and economic winners and losers like many previous breakthrough technologies. While early evidence shows that these models enhance productivity especially among new and inexperienced workers (<span id="jumplink-pgae400-B12"></span>12, <span id="jumplink-pgae400-B14"></span>14), there are other ways in which they may contribute to inequality between people and firms (<span id="jumplink-pgae400-B59"></span>59), for instance via potential negative side effects of automation (<span id="jumplink-pgae400-B33"></span>33, <span id="jumplink-pgae400-B60"></span>60). Our results suggest that the economics of data creation and ownership will become more salient: as data become more valuable, there will be growing interest in how creators of data can capture some of that value (<span id="jumplink-pgae400-B61"></span>61). These multifaceted aspects of the impact of LLMs suggest that the political economy of data and AI will be especially important in the next years (<span id="jumplink-pgae400-B58"></span>58, <span id="jumplink-pgae400-B62"></span>62, <span id="jumplink-pgae400-B63"></span>63).</p><p>In this context, our work highlights the specific issue that valuable digital public goods may be under-produced as a result of the proliferation of AI. A natural follow-up question is how we can incentivize the creation of such goods. While unemployment shocks are known to increase the provision of digital public goods (<span id="jumplink-pgae400-B64"></span>64), it would be an unsatisfying solution to suggest that people put out of work by automation will fill this gap. In the case of platforms like Stack Overflow, active users are often motivated by social feedback and gamification (<span id="jumplink-pgae400-B65"></span>65), but the continual onboarding of new users is what keeps these platforms relevant in the long run (<span id="jumplink-pgae400-B36"></span>36). For the sake of a sustainable open web and an AI ecosystem that draws on its data, we should think about how to keep people exchanging information and knowledge online.</p>                    <h2 scrollto-destination="483096417" id="483096417" data-legacy-id="pgae400-s5">Materials</h2>
                    <h4 scrollto-destination="483096419" id="483096419" data-legacy-id="pgae400-s5.1.1">Stack Exchange platform sites</h4>
<p>The raw dataset obtained from <a href="https://archive.org/details/stackexchange" target="_blank">https://archive.org/details/stackexchange</a> contains nearly all posting activity on the question and answer platforms hosted on the Stack Exchange network from its launch in 2008 to early June 2023. These include Stack Overflow, its Russian language version, and Math Overflow and Math Stack Exchange. Stack Overflow is the largest online Q&amp;A platform for topics relating to computer programming and software development. It provides a community-curated discussion of issues programmers face (<span id="jumplink-pgae400-B65"></span>65). Questions have multiple answers, and users debate the relative merits of solutions and alternatives in comments. A track record on Stack Overflow has value on the labor market as a signal of an individual’s skills (<span id="jumplink-pgae400-B54"></span>54).</p><p>The data contain over 58 million posts, including both questions and answers. Posts are linked to their posting users, from which we infer poster previous activity and can identify posts made by new users. Questions are annotated with tags indicating the topic of the post including programming languages used. Users can give posts upvotes or downvotes, providing posting users with social feedback and reputation points. The Russian language version of Stack Overflow (over 900 thousand posts) and the mathematics-oriented platforms Math Stack Exchange (over 3.5 million posts) and Math Overflow (over 300 thousand posts) have identically structured data dumps hosted in the same location.</p><p>Registered users can upvotes and downvote posts made on Stack Exchange platforms. These votes provide a valuable signal of the value of posts (<span id="jumplink-pgae400-B65"></span>65, <span id="jumplink-pgae400-B66"></span>66). They are the primary way users earn reputation points and status on Stack Exchange platforms. Votes also influence the ranking of posts in user feeds and search engine results, facilitating information filtering. Downvotes are used to moderate. The Stack Exchange data dump contains data on every vote cast, including the corresponding post, the date the vote was made, and whether it was an upvote or downvote.</p>                    <h4 scrollto-destination="483096423" id="483096423" data-legacy-id="pgae400-s5.1.2">Segmentfault</h4>
<p>Segmentfault is a Chinese language platform with a Q&amp;A platform for developers that has many similarities with the Stack Exchange sites. Users post questions on programming language topics and other users post answers. Questions are tagged by relevant languages and technologies, and there are similar gamification elements on the platform. We scraped data on all posts as of early June 2023, gathering over 300 thousand in total. We were careful to follow best practices when collecting this data, limiting strain on the host platform’s servers and retaining only anonymized data and metadata rather than content of posts (<span id="jumplink-pgae400-B67"></span>67).</p>                    <h4 scrollto-destination="483096425" id="483096425" data-legacy-id="pgae400-s5.1.3">Selection of tags</h4>
<p>Stack Overflow posts are annotated by tags which describe the concepts and technologies used in the post. For example, many tags indicate programming languages, web frameworks, database technologies, or programming concepts like functions or algorithms. Stack Overflow reconciles tags referring to the same things via a centralized synonym dictionary. We selected the 1,000 most used tags up to early June 2023 and focused on those 69 which could be directly linked to language statistics reported by GitHub, described next.</p>                    <h4 scrollto-destination="483096427" id="483096427" data-legacy-id="pgae400-s5.1.4">GitHub data on programming language use</h4>
<p>We use data from the June 2021 GHTorrent data dump (<span id="jumplink-pgae400-B68"></span>68) as a proxy measure for the amount of open data available for each programming language. The dataset reports which languages are used in each project or repository on GitHub. We simply count the number of repositories mentioning each language. We then link the languages with tags on Stack Overflow. As an alternative, we count the number of commits, elemental code contributions to repositories, to each repository, hence language. In the main article, we visualize the estimated effects of ChatGPT on specific tags that we can link to GitHub languages. We exclude some tags which refer to file formats or plain text, specifically: yaml, json, text, svg, markdown, and xml.</p>                    <h4 scrollto-destination="483096429" id="483096429" data-legacy-id="pgae400-s5.1.5">Stack Overflow Developer Survey</h4>
<p>The 2023 Stack Overflow Developer Survey was conducted from 2023 May 8 to May 19 and captured responses from 89,184 software developers across 185 countries. Respondents were recruited primarily through channels owned by Stack Overflow, therefore users that are highly engaged on Stack Overflow were more likely to notice the prompts to take the survey over the duration of the collection promotion.<sup><span id="jumplink-FN9"></span>i</sup> This survey includes self-disclosed information about respondents professional status, academic qualifications, employment type, remote work status, and years of coding experience. Moreover the survey asked participants ’Which AI powered tools did you use regularly over the past year’ and included ChatGPT as an option to tick.</p>                    <h2 scrollto-destination="483096431" id="483096431">Notes</h2>
<p><span><span><span rel="nofollow" data-fn-id="FN1">a</span></span><p>There is no standard classification of user experience based on the number of posts. We chose a log-binned classification since activity on Stack Overflow is heavy-tailed (<span id="jumplink-pgae400-B31"></span>31), and log base 10 is a commonly used base.</p></span></p><p><span><span><span rel="nofollow" data-fn-id="FN2">b</span></span><p>For robustness, we test an OLS specification with standardized outcomes and a specification with the raw count of posts that we fit using the Poisson pseudo-maximum likelihood method.</p></span></p><p><span><span><span rel="nofollow" data-fn-id="FN3">c</span></span><p>In this way, we do not include Covid-induced positive shock in 2020 and then the reversion to the trend in 2021.</p></span></p><p><span><span><span rel="nofollow" data-fn-id="FN4">d</span></span><p>We standardize the number of posts within each tag by subtracting the mean and dividing by the standard deviation. Both statistics are calculated using data up to the release of ChatGPT.</p></span></p><p><span><span><span rel="nofollow" data-fn-id="FN5">e</span></span><p>For example, if a developer reports using three languages, there will be three entries (one for each language) in our dataset for this developer, each with a weight of 1/3.</p></span></p><p><span><span><span rel="nofollow" data-fn-id="FN7">g</span></span><p>Prior to the release of ChatGPT, new users contributed 9.5 thousand posts per week; inexperienced users—almost 20 thousand; experienced users—about 17.5 thousand, and expert users—16 thousand.</p></span></p>                    <h2 scrollto-destination="483096441" id="483096441" data-legacy-id="ack1">Acknowledgments</h2>
<p>We thank Frank Neffke, Gergő Tóth, Christoffer Koch, Sándor Juhász, Martin Allen, Manran Zhu, Karl Wachs, László Czaller, Todd Davies, Thomas Fackler, César Hidalgo, Florian Englmaier, Helene Strandt, and James Evans for helpful comments and discussions.</p>                    <h2 scrollto-destination="483096443" id="483096443" data-legacy-id="pgae400-s6">Supplementary Material</h2>
<p><span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">Supplementary material</a></span> is available at <em>PNAS Nexus</em> online.</p>                    <h2 scrollto-destination="483096445" id="483096445" data-legacy-id="pgae400-s7">Funding</h2>
<p>R.M.D.R.C. acknowledges funding from James S. McDonnell Foundation. J.W. acknowledges support from the Hungarian National Scientific Fund (OTKA FK 145960) and use of the HUN-REN Cloud (<span id="jumplink-pgae400-B69"></span>69) in the “Geographies of Creation, Learning and Use in Software” project. N.L. acknowledges support from CRC TRR 190 Rationality &amp; Competition.</p>                    <h2 scrollto-destination="483096447" id="483096447" data-legacy-id="pgae400-s8">Author Contributions</h2>
<p>R.M.D.R.C., N.L., and J.W. together conceived the idea, collected the data, created the models, analyzed the results, and wrote the manuscript.</p>                    <h2 scrollto-destination="483096449" id="483096449" data-legacy-id="pgae400-s9">Preprints</h2>
<p>A preprint of this article is published at <a href="https://arxiv.org/abs/2307.07367" target="_blank">https://arxiv.org/abs/2307.07367</a>.</p>                    <h2 scrollto-destination="483096451" id="483096451" data-legacy-id="pgae400-s10">Data Availability</h2>
<p>Data and code to reproduce our analyses are available on Zenodo: <a href="https://zenodo.org/records/12670482" target="_blank">https://zenodo.org/records/12670482</a>. The Stack Overflow data dump is available here: <a href="https://archive.org/details/stackexchange" target="_blank">https://archive.org/details/stackexchange</a>.</p>                    <h2 scrollto-destination="483096453" id="483096453" data-legacy-id="ref1">References</h2>
<div><div id="ref-auto-pgae400-B1" data-id="pgae400-B1" content-id="pgae400-B1" data-legacy-id="pgae400-B1"><p><span>1</span></p><div><p>Henzinger</p>  <p>M</p><p>, <span><p>Lawrence</p>  <p>S</p></span>. </p><p>2004</p><p>. </p><p>Extracting knowledge from the world wide web</p><p>. </p><p>Proc Natl Acad Sci U S A</p><p>. </p><p>101</p><p>:</p><p>5186</p><p>–</p><p>5191</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B2" data-id="pgae400-B2" content-id="pgae400-B2" data-legacy-id="pgae400-B2"><p><span>2</span></p><div><p>Hess</p>  <p>C</p><p>, <span><p>Ostrom</p>  <p>E</p></span>. </p><p>2003</p><p>. </p><p>Ideas, artifacts, and facilities: information as a common-pool resource</p><p>. </p><p>Law Contemp Probl</p><p>. </p><p>66</p><p>(</p><p>1/2</p><p>):</p><p>111</p><p>–</p><p>145</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B3" data-id="pgae400-B3" content-id="pgae400-B3" data-legacy-id="pgae400-B3"><p><span>3</span></p><div><p>Lemmerich</p>  <p>F</p><p>, <span><p>Sáez-Trumper</p>  <p>D</p></span>, <span><p>West</p>  <p>R</p></span>, <span><p>Zia</p>  <p>L</p></span>. </p><p>2019</p><p>. </p><p>Why the world reads Wikipedia: beyond English speakers. In: Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining; Melbourne. ACM. p. 618–626</p><p>.</p></div></div><div id="ref-auto-pgae400-B4" data-id="pgae400-B4" content-id="pgae400-B4" data-legacy-id="pgae400-B4"><p><span>4</span></p><div><p>Piccardi</p>  <p>T</p><p>, <span><p>Redi</p>  <p>M</p></span>, <span><p>Colavizza</p>  <p>G</p></span>, <span><p>West</p>  <p>R</p></span>. </p><p>2021</p><p>. </p><p>On the value of Wikipedia as a gateway to the web. In: Proceedings of the Web Conference 2021; IW3C2, Ljubljana. p. 249–260</p><p>.</p></div></div><div id="ref-auto-pgae400-B5" data-id="pgae400-B5" content-id="pgae400-B5" data-legacy-id="pgae400-B5"><p><span>5</span></p><div><p>Naveed</p>  <p>H</p><p>, <em>et al</em>. </p><p>2023</p><p>. A comprehensive overview of large language models, arXiv, arXiv:2307.06435, preprint: not peer reviewed. </p></div></div><div id="ref-auto-pgae400-B6" data-id="pgae400-B6" content-id="pgae400-B6" data-legacy-id="pgae400-B6"><p><span>6</span></p><div><p>OpenAI</p><p>. </p><p>2023</p><p>. </p><p>GPT-4 Technical Report</p><p>.</p></div></div><div id="ref-auto-pgae400-B7" data-id="pgae400-B7" content-id="pgae400-B7" data-legacy-id="pgae400-B7"><p><span>7</span></p><div><p>Teubner</p>  <p>T</p><p>, <span><p>Flath</p>  <p>CM</p></span>, <span><p>Weinhardt</p>  <p>C</p></span>, <span><p>van der Aalst</p>  <p>W</p></span>, <span><p>Hinz</p>  <p>O</p></span>. </p><p>2023</p><p>. </p><p>Welcome to the era of ChatGPT et al. the prospects of large language models</p><p>. </p><p>Bus Inf Syst Eng</p><p>. </p><p>65</p><p>(</p><p>2</p><p>):</p><p>95</p><p>–</p><p>101</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B8" data-id="pgae400-B8" content-id="pgae400-B8" data-legacy-id="pgae400-B8"><p><span>8</span></p><div><p>Gu</p>  <p>H</p><p>, <span><p>Schreyer</p>  <p>M</p></span>, <span><p>Moffitt</p>  <p>K</p></span>, <span><p>Vasarhelyi</p>  <p>MA</p></span>. </p><p>2023</p><p>. </p><p>Artificial intelligence co-piloted auditing. Available at SSRN 4444763</p><p>.</p></div></div><div id="ref-auto-pgae400-B9" data-id="pgae400-B9" content-id="pgae400-B9" data-legacy-id="pgae400-B9"><p><span>9</span></p><div><p>Smith</p>  <p>MJ</p><p>, <span><p>Geach</p>  <p>JE</p></span>. </p><p>2023</p><p>. </p><p>Astronomia ex machina: a history, primer and outlook on neural networks in astronomy</p><p>. </p><p>R Soc Open Sci</p><p>. </p><p>10</p><p>(</p><p>5</p><p>):</p><p>221454</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B10" data-id="pgae400-B10" content-id="pgae400-B10" data-legacy-id="pgae400-B10"><p><span>10</span></p><div><p>Kanjee</p>  <p>Z</p><p>, <span><p>Crowe</p>  <p>B</p></span>, <span><p>Rodman</p>  <p>A</p></span>. </p><p>2023</p><p>. </p><p>Accuracy of a generative artificial intelligence model in a complex diagnostic challenge</p><p>. </p><p>JAMA</p><p>. </p><p>330</p><p>(</p><p>1</p><p>):</p><p>78</p><p>–</p><p>80</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B11" data-id="pgae400-B11" content-id="pgae400-B11" data-legacy-id="pgae400-B11"><p><span>11</span></p><div><p>Guo</p>  <p>T</p><p>, <em>et al</em>. </p><p>2023</p><p>. What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks, arXiv, arXiv:2305.18365, preprint: not peer reviewed. </p></div></div><div id="ref-auto-pgae400-B12" data-id="pgae400-B12" content-id="pgae400-B12" data-legacy-id="pgae400-B12"><p><span>12</span></p><div><p>Brynjolfsson</p>  <p>E</p><p>, <span><p>Li</p>  <p>D</p></span>, <span><p>Raymond</p>  <p>LR</p></span>. </p><p>2023</p><p>. </p><p>Generative AI at work. Technical Report, National Bureau of Economic Research</p><p>.</p></div></div><div id="ref-auto-pgae400-B13" data-id="pgae400-B13" content-id="pgae400-B13" data-legacy-id="pgae400-B13"><p><span>13</span></p><div><p>Dell’Acqua</p>  <p>F</p><p>, <em>et al</em>. </p><p>2023</p><p>. </p><p>Navigating the jagged technological frontier: field experimental evidence of the effects of ai on knowledge worker productivity and quality. Harvard Business School Technology &amp; Operations Mgt. Unit Working Paper (24-013)</p><p>.</p></div></div><div id="ref-auto-pgae400-B14" data-id="pgae400-B14" content-id="pgae400-B14" data-legacy-id="pgae400-B14"><p><span>14</span></p><div><p>Noy</p>  <p>S</p><p>, <span><p>Zhang</p>  <p>W</p></span>. </p><p>2023</p><p>. </p><p>Experimental evidence on the productivity effects of generative artificial intelligence</p><p>. </p><p>Science</p><p>. </p><p>381</p><p>(</p><p>6654</p><p>):</p><p>187</p><p>–</p><p>192</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B15" data-id="pgae400-B15" content-id="pgae400-B15" data-legacy-id="pgae400-B15"><p><span>15</span></p><div><p>Peng</p>  <p>S</p><p>, <span><p>Kalliamvakou</p>  <p>E</p></span>, <span><p>Cihon</p>  <p>P</p></span>, <span><p>Demirer</p>  <p>M</p></span>. </p><p>2023</p><p>. The impact of AI on developer productivity: evidence from GitHub Copilot, arXiv, arXiv:2302.06590, preprint: not peer reviewed. </p></div></div><div id="ref-auto-pgae400-B16" data-id="pgae400-B16" content-id="pgae400-B16" data-legacy-id="pgae400-B16"><p><span>16</span></p><div><p>Wiles</p>  <p>E</p><p>, <span><p>Munyikwa</p>  <p>ZT</p></span>, <span><p>Horton</p>  <p>JJ</p></span>. </p><p>2023</p><p>. </p><p>Algorithmic writing assistance on Jobseekers’ resumes increases hires. Technical Report. National Bureau of Economic Research</p><p>.</p></div></div><div id="ref-auto-pgae400-B17" data-id="pgae400-B17" content-id="pgae400-B17" data-legacy-id="pgae400-B17"><p><span>17</span></p><div><p>McMahon</p>  <p>C</p><p>, <span><p>Johnson</p>  <p>I</p></span>, <span><p>Hecht</p>  <p>B</p></span>. </p><p>2017</p><p>. </p><p>The substantial interdependence of Wikipedia and Google: a case study on the relationship between peer production communities and information technologies. In: Proceedings of the International AAAI Conference on Web and Social Media; Montreal. AAAI. vol. 11, p. 142–151</p><p>.</p></div></div><div id="ref-auto-pgae400-B18" data-id="pgae400-B18" content-id="pgae400-B18" data-legacy-id="pgae400-B18"><p><span>18</span></p><div><p>Taraborelli</p>  <p>D</p><p>. </p><p>2015</p><p>. The sum of all human knowledge in the age of machines: a new research agenda for Wikimedia. In: ICWSM-15 Workshop on Wikipedia; Oxford. AAAI.</p></div></div><div id="ref-auto-pgae400-B19" data-id="pgae400-B19" content-id="pgae400-B19" data-legacy-id="pgae400-B19"><p><span>19</span></p><div><p>Delile</p>  <p>Z</p><p>, <em>et al</em>. </p><p>2023</p><p>. Evaluating privacy questions from stack overflow: can ChatGPT compete?, arXiv, arXiv:2306.11174, preprint: not peer reviewed. </p></div></div><div id="ref-auto-pgae400-B20" data-id="pgae400-B20" content-id="pgae400-B20" data-legacy-id="pgae400-B20"><p><span>20</span></p><div><p>Widjojo</p>  <p>P</p><p>, <span><p>Treude</p>  <p>C</p></span>. </p><p>2023</p><p>. Addressing compiler errors: stack overflow or large language models?, arXiv, arXiv:2307.10793, preprint: not peer reviewed. </p></div></div><div id="ref-auto-pgae400-B21" data-id="pgae400-B21" content-id="pgae400-B21" data-legacy-id="pgae400-B21"><p><span>21</span></p><div><p>Villalobos</p>  <p>P</p><p>, <em>et al</em>. </p><p>2022</p><p>. Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning, arXiv, arXiv:2211.04325, preprint: not peer reviewed. </p></div></div><div id="ref-auto-pgae400-B22" data-id="pgae400-B22" content-id="pgae400-B22" data-legacy-id="pgae400-B22"><p><span>22</span></p><div><p>Alemohammad</p>  <p>S</p><p>, <em>et al</em>. </p><p>2023</p><p>. </p><p>Self-consuming generative models go mad</p><p>, </p><p>arXiv, arXiv:2307.01850</p><p>, preprint.</p></div></div><div id="ref-auto-pgae400-B23" data-id="pgae400-B23" content-id="pgae400-B23" data-legacy-id="pgae400-B23"><p><span>23</span></p><div><p>Gudibande</p>  <p>A</p><p>, <em>et al</em>. </p><p>2023</p><p>. </p><p>The false promise of imitating proprietary LLMs, arXiv, arXiv:2305.15717, preprint: not peer reviewed</p><p>. </p></div></div><div id="ref-auto-pgae400-B24" data-id="pgae400-B24" content-id="pgae400-B24" data-legacy-id="pgae400-B24"><p><span>24</span></p><div><p>Shumailov</p>  <p>I</p><p>, <em>et al</em>. </p><p>2024</p><p>. </p><p>Ai models collapse when trained on recursively generated data</p><p>. </p><p>Nature</p><p>. </p><p>631</p><p>(</p><p>8022</p><p>):</p><p>755</p><p>–</p><p>759</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B25" data-id="pgae400-B25" content-id="pgae400-B25" data-legacy-id="pgae400-B25"><p><span>25</span></p><div><p>Chiang</p>  <p>T</p><p>. </p><p>2023</p><p>. </p><p>ChatGPT is a blurry JPEG of the web</p><p>. </p><p>The New Yorker</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B26" data-id="pgae400-B26" content-id="pgae400-B26" data-legacy-id="pgae400-B26"><p><span>26</span></p><div><p>Arthur</p>  <p>WB</p><p>. </p><p>1989</p><p>. </p><p>Competing technologies, increasing returns, and lock-in by historical events</p><p>. </p><p>Econ J</p><p>. </p><p>99</p><p>(</p><p>394</p><p>):</p><p>116</p><p>–</p><p>131</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B27" data-id="pgae400-B27" content-id="pgae400-B27" data-legacy-id="pgae400-B27"><p><span>27</span></p><div><p>David</p>  <p>PA</p><p>. </p><p>1985</p><p>. </p><p>Clio and the economics of QWERTY</p><p>. </p><p>Am Econ Rev</p><p>. </p><p>75</p><p>(</p><p>2</p><p>):</p><p>332</p><p>–</p><p>337</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B28" data-id="pgae400-B28" content-id="pgae400-B28" data-legacy-id="pgae400-B28"><p><span>28</span></p><div><p>Stojkoski</p>  <p>V</p><p>, <span><p>Koch</p>  <p>P</p></span>, <span><p>Coll</p>  <p>E</p></span>, <span><p>Hidalgo</p>  <p>CA</p></span>. </p><p>2024</p><p>. </p><p>Estimating digital product trade through corporate revenue data</p><p>. </p><p>Nat Commun</p><p>. </p><p>15</p><p>(</p><p>1</p><p>):</p><p>5262</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B29" data-id="pgae400-B29" content-id="pgae400-B29" data-legacy-id="pgae400-B29"><p><span>29</span></p><div><p>Weidinger</p>  <p>L</p><p>. </p><p>2022</p><p>. </p><p>Taxonomy of risks posed by language models. In: Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency; Seoul. ACM. p. 214–229</p><p>.</p></div></div><div id="ref-auto-pgae400-B30" data-id="pgae400-B30" content-id="pgae400-B30" data-legacy-id="pgae400-B30"><p><span>30</span></p><div><p>Kreitmeir</p>  <p>DH</p><p>, <span><p>Raschky</p>  <p>PA</p></span>. </p><p>2023</p><p>. </p><p>The Unintended Consequences of Censoring Digital Technology–Evidence from Italy’s ChatGPT Ban, arXiv, arXiv:2304.09339, preprint: not peer reviewed</p><p>. </p></div></div><div id="ref-auto-pgae400-B31" data-id="pgae400-B31" content-id="pgae400-B31" data-legacy-id="pgae400-B31"><p><span>31</span></p><div><p>Upadhyay</p>  <p>U</p><p>, <span><p>Valera</p>  <p>I</p></span>, <span><p>Gomez-Rodriguez</p>  <p>M</p></span>. </p><p>2017</p><p>. </p><p>Uncovering the dynamics of crowdlearning and the value of knowledge. In: Proceedings of the Tenth ACM International Conference on Web Search and Data Mining. p. 61–70; Cambridge (UK). ACM.</p></div></div><div id="ref-auto-pgae400-B32" data-id="pgae400-B32" content-id="pgae400-B32" data-legacy-id="pgae400-B32"><p><span>32</span></p><div><p>Bilinski</p>  <p>A</p><p>, <span><p>Hatfield</p>  <p>LA</p></span>. </p><p>2018</p><p>. </p><p>Nothing to see here? Non-inferiority approaches to parallel trends and other model assumptions, arXiv, arXiv:1805.03273, preprint: not peer reviewed</p><p>. </p></div></div><div id="ref-auto-pgae400-B33" data-id="pgae400-B33" content-id="pgae400-B33" data-legacy-id="pgae400-B33"><p><span>33</span></p><div><p>Eloundou</p>  <p>T</p><p>, <span><p>Manning</p>  <p>S</p></span>, <span><p>Mishkin</p>  <p>P</p></span>, <span><p>Rock</p>  <p>D</p></span>. </p><p>2024</p><p>. </p><p>GPTs are GPTs: labor market impact potential of LLMs</p><p>. </p><p>Science</p><p>. </p><p>384</p><p>(</p><p>6702</p><p>):</p><p>1306</p><p>–</p><p>1308</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B34" data-id="pgae400-B34" content-id="pgae400-B34" data-legacy-id="pgae400-B34"><p><span>34</span></p><div><p>Aghion</p>  <p>P</p><p>, <span><p>Howitt</p>  <p>P</p></span>. </p><p>1992</p><p>. </p><p>A model of growth through creative destruction</p><p>. </p><p>Econometrica</p><p>. </p><p>60</p><p>(</p><p>2</p><p>):</p><p>323</p><p>–</p><p>351</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B35" data-id="pgae400-B35" content-id="pgae400-B35" data-legacy-id="pgae400-B35"><p><span>35</span></p><div><p>Schumpeter</p>  <p>JA</p><p>. </p><p>1942</p><p>. </p><p>Capitalism, socialism, and democracy</p><p>. </p><p>New York</p><p>: </p><p>Routledge</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B36" data-id="pgae400-B36" content-id="pgae400-B36" data-legacy-id="pgae400-B36"><p><span>36</span></p><div><p>Danescu-Niculescu-Mizil</p>  <p>C</p><p>, <span><p>West</p>  <p>R</p></span>, <span><p>Jurafsky</p>  <p>D</p></span>, <span><p>Leskovec</p>  <p>J</p></span>, <span><p>Potts</p>  <p>C</p></span>. </p><p>2013</p><p>. </p><p>No country for old members: user lifecycle and linguistic change in online communities. In: Proceedings of the 22nd international conference on World Wide Web; Rio, ACM. p. 307–318</p><p>.</p></div></div><div id="ref-auto-pgae400-B37" data-id="pgae400-B37" content-id="pgae400-B37" data-legacy-id="pgae400-B37"><p><span>37</span></p><div><p>Burtch</p>  <p>G</p><p>, <span><p>Lee</p>  <p>D</p></span>, <span><p>Chen</p>  <p>Z</p></span>. </p><p>2024</p><p>. </p><p>The consequences of generative AI for online knowledge communities</p><p>. </p><p>Sci Rep</p><p>. </p><p>14</p><p>(</p><p>1</p><p>):</p><p>10413</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B38" data-id="pgae400-B38" content-id="pgae400-B38" data-legacy-id="pgae400-B38"><p><span>38</span></p><div><p>Correa</p>  <p>D</p><p>, <span><p>Sureka</p>  <p>A</p></span>. </p><p>2013</p><p>. </p><p>Fit or unfit: analysis and prediction of ’closed questions’ on Stack Overflow. In: Proceedings of the First ACM conference on Online Social Networks; Boston. ACM. p. 201–212</p><p>.</p></div></div><div id="ref-auto-pgae400-B39" data-id="pgae400-B39" content-id="pgae400-B39" data-legacy-id="pgae400-B39"><p><span>39</span></p><div><p>Bao</p>  <p>H</p><p>, <span><p>Sun</p>  <p>M</p></span>, <span><p>Teplitskiy</p>  <p>M</p></span>. </p><p>2024</p><p>. </p><p>Where there’s a will there’s a way: ChatGPT is used more for science in countries where it is prohibited</p><p>.</p></div></div><div id="ref-auto-pgae400-B40" data-id="pgae400-B40" content-id="pgae400-B40" data-legacy-id="pgae400-B40"><p><span>40</span></p><div><p>Gaessler</p>  <p>F</p><p>, <span><p>Piezunka</p>  <p>H</p></span>. </p><p>2023</p><p>. </p><p>Training with AI: evidence from chess computers</p><p>. </p><p>Strat Manag J</p><p>.</p><p>44</p><p>(</p><p>11</p><p>):</p><p>2724</p><p>–</p><p>2750</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B41" data-id="pgae400-B41" content-id="pgae400-B41" data-legacy-id="pgae400-B41"><p><span>41</span></p><div><p>Taleb</p>  <p>NN</p><p>. </p><p>2012</p><p>. </p><p>Antifragile: how to live in a world we don’t understand</p><p>. </p><p>vol. 3</p><p>. </p><p>London</p><p>: </p><p>Allen Lane</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B42" data-id="pgae400-B42" content-id="pgae400-B42" data-legacy-id="pgae400-B42"><p><span>42</span></p><div><p>Wyllie</p>  <p>S</p><p>, <span><p>Shumailov</p>  <p>I</p></span>, <span><p>Papernot</p>  <p>N</p></span>. </p><p>2024</p><p>. </p><p>Fairness feedback loops: training on synthetic data amplifies bias, arXiv, arXiv:2403.07857, preprint: not peer reviewed</p><p>. </p></div></div><div id="ref-auto-pgae400-B43" data-id="pgae400-B43" content-id="pgae400-B43" data-legacy-id="pgae400-B43"><p><span>43</span></p><div><p>Veselovsky</p>  <p>V</p><p>, <span><p>Ribeiro</p>  <p>MH</p></span>, <span><p>West</p>  <p>R</p></span>. </p><p>2023</p><p>. </p><p>Artificial artificial artificial intelligence: crowd workers widely use large language models for text production tasks, arXiv, arXiv:2306.07899, preprint: not peer reviewed</p><p>. </p></div></div><div id="ref-auto-pgae400-B44" data-id="pgae400-B44" content-id="pgae400-B44" data-legacy-id="pgae400-B44"><p><span>44</span></p><div><p>Tian-Zheng Wei</p>  <p>J</p><p>, <span><p>Wang</p>  <p>RY</p></span>, <span><p>Jia</p>  <p>R</p></span>. </p><p>2024</p><p>. </p><p>Proving membership in LLM pretraining data via data watermarks, arXiv, arXiv:2402.10892, preprint: not peer reviewed.</p> </div></div><div id="ref-auto-pgae400-B45" data-id="pgae400-B45" content-id="pgae400-B45" data-legacy-id="pgae400-B45"><p><span>45</span></p><div><p>Tang</p>  <p>R</p><p>, <span><p>Chuang</p>  <p>Y-N</p></span>, <span><p>Hu</p>  <p>X</p></span>. </p><p>2024</p><p>. </p><p>The science of detecting LLM-generated text</p><p>. </p><p>Commun ACM</p><p>. </p><p>67</p><p>(</p><p>4</p><p>):</p><p>50</p><p>–</p><p>59</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B46" data-id="pgae400-B46" content-id="pgae400-B46" data-legacy-id="pgae400-B46"><p><span>46</span></p><div><p>Page</p>  <p>SE</p><p>. </p><p>2006</p><p>. </p><p>Path dependence</p><p>. </p><p>Quart J Polit Sci</p><p>. </p><p>1</p><p>(</p><p>1</p><p>):</p><p>87</p><p>–</p><p>115</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B47" data-id="pgae400-B47" content-id="pgae400-B47" data-legacy-id="pgae400-B47"><p><span>47</span></p><div><p>Köpf</p>  <p>A</p><p>. </p><p>2024</p><p>. </p><p>Openassistant conversations-democratizing large language model alignment</p><p>. </p><p>Adv Neural Inf Process Syst</p><p>. </p><p>36</p><p>:</p><p>47669</p><p>–</p><p>47681</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B48" data-id="pgae400-B48" content-id="pgae400-B48" data-legacy-id="pgae400-B48"><p><span>48</span></p><div><p>Vincent</p>  <p>N</p><p>, <span><p>Johnson</p>  <p>I</p></span>, <span><p>Hecht</p>  <p>B</p></span>. </p><p>2018</p><p>. </p><p>Examining Wikipedia with a broader lens: quantifying the value of Wikipedia’s relationships with other large-scale online communities. In: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems; Quebec. ACM. p. 1–13</p><p>.</p></div></div><div id="ref-auto-pgae400-B49" data-id="pgae400-B49" content-id="pgae400-B49" data-legacy-id="pgae400-B49"><p><span>49</span></p><div><p>Vincent</p>  <p>N</p><p>, <span><p>Hecht</p>  <p>B</p></span>. </p><p>2021</p><p>. </p><p>A deeper investigation of the importance of Wikipedia links to search engine results</p><p>. </p><p>Proc ACM Hum-Comput Inter</p><p>. </p><p>5</p><p>(</p><p>CSCW1</p><p>):</p><p>1</p><p>–</p><p>15</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B50" data-id="pgae400-B50" content-id="pgae400-B50" data-legacy-id="pgae400-B50"><p><span>50</span></p><div><p>Vincent</p>  <p>N</p><p>. </p><p>2022</p><p>. </p><p>The paradox of reuse, language models edition. Data leverage</p><p>.</p></div></div><div id="ref-auto-pgae400-B51" data-id="pgae400-B51" content-id="pgae400-B51" data-legacy-id="pgae400-B51"><p><span>51</span></p><div><p>Lerner</p>  <p>J</p><p>, <span><p>Tirole</p>  <p>J</p></span>. </p><p>2002</p><p>. </p><p>Some simple economics of open source</p><p>. </p><p>J Ind Econ</p><p>. </p><p>50</p><p>(</p><p>2</p><p>):</p><p>197</p><p>–</p><p>234</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B52" data-id="pgae400-B52" content-id="pgae400-B52" data-legacy-id="pgae400-B52"><p><span>52</span></p><div><p>Greenstein</p>  <p>S</p><p>, <span><p>Nagle</p>  <p>F</p></span>. </p><p>2014</p><p>. </p><p>Digital dark matter and the economic contribution of Apache</p><p>. </p><p>Res Policy</p><p>. </p><p>43</p><p>(</p><p>4</p><p>):</p><p>623</p><p>–</p><p>631</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B53" data-id="pgae400-B53" content-id="pgae400-B53" data-legacy-id="pgae400-B53"><p><span>53</span></p><div><p>Conti</p>  <p>A</p><p>, <span><p>Peukert</p>  <p>C</p></span>, <span><p>Roche</p>  <p>MP</p></span>. </p><p>2021</p><p>. </p><p>Beefing IT up for your investor? Open sourcing and startup funding: evidence from GitHub. HBS Working Paper 22-001</p><p>.</p></div></div><div id="ref-auto-pgae400-B54" data-id="pgae400-B54" content-id="pgae400-B54" data-legacy-id="pgae400-B54"><p><span>54</span></p><div><p>Xu</p>  <p>L</p><p>, <span><p>Nian</p>  <p>T</p></span>, <span><p>Cabral</p>  <p>L</p></span>. </p><p>2020</p><p>. </p><p>What makes geeks tick? A study of stack overflow careers</p><p>. </p><p>Manage Sci</p><p>. </p><p>66</p><p>(</p><p>2</p><p>):</p><p>587</p><p>–</p><p>604</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B55" data-id="pgae400-B55" content-id="pgae400-B55" data-legacy-id="pgae400-B55"><p><span>55</span></p><div><p>Evans</p>  <p>JA</p><p>. </p><p>2008</p><p>. </p><p>Electronic publication and the narrowing of science and scholarship</p><p>. </p><p>Science</p><p>. </p><p>321</p><p>(</p><p>5887</p><p>):</p><p>395</p><p>–</p><p>399</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B56" data-id="pgae400-B56" content-id="pgae400-B56" data-legacy-id="pgae400-B56"><p><span>56</span></p><div><p>Grossmann</p>  <p>I</p><p>, <em>et al</em>. </p><p>2023</p><p>. </p><p>AI and the transformation of social science research</p><p>. </p><p>Science</p><p>. </p><p>380</p><p>(</p><p>6650</p><p>):</p><p>1108</p><p>–</p><p>1109</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B57" data-id="pgae400-B57" content-id="pgae400-B57" data-legacy-id="pgae400-B57"><p><span>57</span></p><div><p>Xu</p>  <p>R</p><p>, <span><p>Feng</p>  <p>Y</p></span>, <span><p>Chen</p>  <p>H</p></span>. </p><p>2023</p><p>. </p><p>ChatGPT vs. Google: a comparative study of search performance and user experience, arXiv, arXiv:2307.01135, preprint: not peer reviewed</p><p>. </p></div></div><div id="ref-auto-pgae400-B58" data-id="pgae400-B58" content-id="pgae400-B58" data-legacy-id="pgae400-B58"><p><span>58</span></p><div><p>Sandrini</p>  <p>L</p><p>, <span><p>Somogyi</p>  <p>R</p></span>. </p><p>2023</p><p>. </p><p>Generative ai and deceptive news consumption</p><p>. </p><p>Econ Lett</p><p>. </p><p>232</p><p>:</p><p>111317</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B59" data-id="pgae400-B59" content-id="pgae400-B59" data-legacy-id="pgae400-B59"><p><span>59</span></p><div><p>Rock</p>  <p>D</p><p>. </p><p>2019</p><p>. </p><p>Engineering value: the returns to technological talent and investments in artificial intelligence. Available at SSRN 3427412</p><p>.</p></div></div><div id="ref-auto-pgae400-B60" data-id="pgae400-B60" content-id="pgae400-B60" data-legacy-id="pgae400-B60"><p><span>60</span></p><div><p>Acemoglu</p>  <p>D</p><p>, <span><p>Restrepo</p>  <p>P</p></span>. </p><p>2019</p><p>. </p><p>Automation and new tasks: how technology displaces and reinstates labor</p><p>. </p><p>J Econ Perspect</p><p>. </p><p>33</p><p>(</p><p>2</p><p>):</p><p>3</p><p>–</p><p>30</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B61" data-id="pgae400-B61" content-id="pgae400-B61" data-legacy-id="pgae400-B61"><p><span>61</span></p><div><p>Li</p>  <p>H</p><p>, <span><p>Vincent</p>  <p>N</p></span>, <span><p>Chancellor</p>  <p>S</p></span>, <span><p>Hecht</p>  <p>B</p></span>. </p><p>2023</p><p>. </p><p>The dimensions of data labor: a road map for researchers, activists, and policymakers to empower data producers. In: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency; Chicago. ACM. p. 1151–1161</p><p>.</p></div></div><div id="ref-auto-pgae400-B62" data-id="pgae400-B62" content-id="pgae400-B62" data-legacy-id="pgae400-B62"><p><span>62</span></p><div><p>Johnson</p>  <p>S</p><p>, <span><p>Acemoglu</p>  <p>D</p></span>. </p><p>2023</p><p>. </p><p>Power and progress: our thousand-year struggle over technology and prosperity</p><p>. </p><p>UK</p><p>: </p><p>Hachette</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B63" data-id="pgae400-B63" content-id="pgae400-B63" data-legacy-id="pgae400-B63"><p><span>63</span></p><div><p>Lehdonvirta</p>  <p>V</p><p>. </p><p>2022</p><p>. </p><p>Cloud empires: how digital platforms are overtaking the state and how we can regain control</p><p>. </p><p>Cambridge</p><p>: </p><p>MIT Press</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B64" data-id="pgae400-B64" content-id="pgae400-B64" data-legacy-id="pgae400-B64"><p><span>64</span></p><div><p>Kummer</p>  <p>M</p><p>, <span><p>Slivko</p>  <p>O</p></span>, <span><p>Zhang</p>  <p>X</p></span>. </p><p>2020</p><p>. </p><p>Unemployment and digital public goods contribution</p><p>. </p><p>Inform Syst Res</p><p>. </p><p>31</p><p>(</p><p>3</p><p>):</p><p>801</p><p>–</p><p>819</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B65" data-id="pgae400-B65" content-id="pgae400-B65" data-legacy-id="pgae400-B65"><p><span>65</span></p><div><p>Anderson</p>  <p>A</p><p>, <span><p>Huttenlocher</p>  <p>D</p></span>, <span><p>Kleinberg</p>  <p>J</p></span>, <span><p>Leskovec</p>  <p>J</p></span>. </p><p>2012</p><p>. </p><p>Discovering value from community activity on focused question answering sites: a case study of Stack Overflow. In: Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining; Beijing. ACM. p. 850–858</p><p>.</p></div></div><div id="ref-auto-pgae400-B66" data-id="pgae400-B66" content-id="pgae400-B66" data-legacy-id="pgae400-B66"><p><span>66</span></p><div><p>Mamykina</p>  <p>L</p><p>, <span><p>Manoim</p>  <p>B</p></span>, <span><p>Mittal</p>  <p>M</p></span>, <span><p>Hripcsak</p>  <p>G</p></span>, <span><p>Hartmann</p>  <p>B</p></span>. </p><p>2011</p><p>. </p><p>Design lessons from the fastest Q&amp;A site in the west. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems; Vancouver. ACM. p. 2857–2866</p><p>.</p></div></div><div id="ref-auto-pgae400-B67" data-id="pgae400-B67" content-id="pgae400-B67" data-legacy-id="pgae400-B67"><p><span>67</span></p><div><p>Franzke</p>  <p>AS</p><p>, <span><p>Bechmann</p>  <p>A</p></span>, <span><p>Zimmer</p>  <p>M</p></span>, <span><p>Ess</p>  <p>C</p></span>, </p><p>The Association of Internet Researchers</p><p>. </p><p>2020</p><p>. </p><p>Internet research: ethical guidelines 3.0. Technical Report, Association of Internet Researchers</p><p>.</p></div></div><div id="ref-auto-pgae400-B68" data-id="pgae400-B68" content-id="pgae400-B68" data-legacy-id="pgae400-B68"><p><span>68</span></p><div><p>Gousios</p>  <p>G</p><p>, <span><p>Spinellis</p>  <p>D</p></span>. </p><p>2012</p><p>. </p><p>GHTorrent: GitHub’s data from a firehose. In: 2012 9th IEEE Working Conference on Mining Software Repositories (MSR); Zurich. IEEE. p. 12–21</p><p>.</p></div></div><div id="ref-auto-pgae400-B69" data-id="pgae400-B69" content-id="pgae400-B69" data-legacy-id="pgae400-B69"><p><span>69</span></p><div><p>Héder</p>  <p>M</p><p>, <em>et al</em>. </p><p>2022</p><p>. </p><p>The past, present and future of the ELKH cloud</p><p>. </p><p>Inform Társadalom</p><p>. </p><p>22</p><p>(</p><p>2</p><p>):</p><p>128</p><p>.</p><!--citationLinks: case 1--></div></div></div>    <!-- /foreach in Model.Sections -->
    



        

        
                    <h2 id="authorNotesSectionTitle" scrollto-destination="authorNotesSectionTitle">Author notes</h2>
<p><span><p><strong>Competing Interest:</strong> The authors declare no competing interests.</p></span></p><p>© The Author(s) 2024. Published by Oxford University Press on behalf of National Academy of Sciences.</p><div><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">https://creativecommons.org/licenses/by/4.0/</a>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</p></div><!-- /foreach -->

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Eating less can lead to a longer life: study in mice shows why (205 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-03277-6</link>
            <guid>41826449</guid>
            <pubDate>Sun, 13 Oct 2024 09:25:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-03277-6">https://www.nature.com/articles/d41586-024-03277-6</a>, See on <a href="https://news.ycombinator.com/item?id=41826449">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <header>
        <div>
            <ul data-test="article-identifier">
                <li data-test="article-category"><span>NEWS</span></li>
                <li><time datetime="2024-10-09">09 October 2024</time></li>
                
            </ul>

            

            <div>
                
                <p>
                    Weight loss and metabolic improvements do not explain the longevity benefits of severe dietary restrictions.
                </p>
            </div>
        </div>
        
            <div data-test="author-info">
    <ol>
        
            <li>
                
                    <span>Elie Dolgin</span>
                
                
                    <ol>
                        <li id="Aff0">
                            <p>Elie Dolgin is a science journalist in Somerville, Massachusetts.</p>
                        </li>
                    </ol>
                
                
                    
                
            </li>
        
    </ol>
</div>
        
    </header>
    
</div><div>
                    
                        <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-03277-6/d41586-024-03277-6_27701126.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-03277-6/d41586-024-03277-6_27701126.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Coloured scanning electron micrograph of fat cells shown in various shades of pink" loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-03277-6/d41586-024-03277-6_27701126.jpg">
  <figcaption>
   <p><span>Fat cells (artificially coloured). Restrictive diets cause fat loss and lengthen life, but the two effects are not necessarily linked.</span><span>Credit: Steve Gschmeissner/SPL</span></p>
  </figcaption>
 </picture>
</figure><p>Cutting calorie intake can lead to a leaner body — <a href="https://www.nature.com/articles/nature.2014.14963" data-track="click" data-label="https://www.nature.com/articles/nature.2014.14963" data-track-category="body text link">and a longer life</a>, an effect often chalked up to the weight loss and <a href="https://www.nature.com/articles/d41586-018-03431-x" data-track="click" data-label="https://www.nature.com/articles/d41586-018-03431-x" data-track-category="body text link">metabolic changes caused by consuming less food</a>. Now, one of the biggest studies<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> of dietary restrictions ever conducted in laboratory animals challenges the conventional wisdom about how dietary restriction boosts longevity.</p><p>The study, involving nearly 1,000 mice fed low-calorie diets or subjected to regular bouts of <a href="https://www.nature.com/articles/d41586-024-02700-2" data-track="click" data-label="https://www.nature.com/articles/d41586-024-02700-2" data-track-category="body text link">fasting</a>, found that such regimens do indeed cause weight loss and related metabolic changes. But other factors — including <a href="https://www.nature.com/articles/d41586-024-00871-6" data-track="click" data-label="https://www.nature.com/articles/d41586-024-00871-6" data-track-category="body text link">immune health</a>, genetics and physiological indicators of resiliency — seem to better explain the link between cutting calories and increased lifespan.</p><p>“The metabolic changes are important,” says Gary Churchill, a mouse geneticist at the Jackson Laboratory in Bar Harbor, Maine, who co-led the study. “But they don’t lead to lifespan extension.”</p><p>To outside investigators, the results drive home the intricate and individualized nature of the body’s reaction to caloric restriction. “It’s revelatory about the complexity of this intervention,” says James Nelson, a biogerontologist at the University of Texas Health Science Center in San Antonio.</p><p>The study was published today in <i>Nature</i> by Churchill and his co-authors, including scientists at Calico Life Sciences in South San Francisco, California, the anti-ageing focused biotech company that funded the study.</p><h2>Counting calories</h2><p>Scientists have long known that caloric restriction, a regimen of long-term limits on food intake, lengthens lifespan in laboratory animals<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>. Some studies<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup><sup>,</sup><sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup> have shown that intermittent fasting, which involves <a href="https://www.nature.com/articles/d41586-021-01578-8" data-track="click" data-label="https://www.nature.com/articles/d41586-021-01578-8" data-track-category="body text link">short bouts of food deprivation</a>, can also increase longevity.</p><p>To learn more about how such diets work, the researchers monitored the health and longevity of 960 mice, each a genetically distinct individual drawn from a diverse population that mirrors the genetic variability found in humans. Some mice were placed on calorie-limited diets, another group followed intermittent fasting regimens, and others were allowed to eat freely.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-024-03244-1" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-03277-6/d41586-024-03277-6_27700972.jpg"><p>Life expectancy rise in rich countries slows down: why discovery took 30 years to prove</p></a>
 </article><p>Cutting calories by 40% yielded the longest longevity bump, but intermittent fasting and less severe calorie restriction also increased average lifespan. The dieting mice also displayed favourable metabolic changes, such as reductions in body fat and blood sugar levels.</p><p>However, the effects of dietary restriction on metabolism and lifespan didn’t always change in lockstep. To the authors’ surprise, the mice that lost the most weight on a calorie-limited diet tended to die younger than did animals that lost relatively modest amounts.</p><p>This suggests that processes beyond simple metabolic regulation drive how the body responds to limited-calorie regimes. What mattered most for lengthening lifespan were traits related to immune health and red-blood-cell function. Also key was overall resilience, presumably encoded in the animals’ genes, to the stress of reduced food intake.</p><p>“The intervention is a stressor,” Churchill explains. The most-resilient animals lost the least weight, maintained immune function and lived longer.</p><h2>Leanness for longevity</h2><p>The findings could reshape how scientists think about studies of dietary restriction in humans. In one of the most comprehensive clinical trials of a low-calorie diet in healthy, non-obese individuals, researchers found<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup> that <a href="https://www.nature.com/articles/d41586-018-03431-x" data-track="click" data-label="https://www.nature.com/articles/d41586-018-03431-x" data-track-category="body text link">the intervention helped to dial down metabolic rates</a> — a short-term effect thought to signal longer-term benefits for lifespan.</p><p>But the mouse data from Churchill’s team suggest that metabolic measurements might reflect <a href="https://www.nature.com/articles/529154a" data-track="click" data-label="https://www.nature.com/articles/529154a" data-track-category="body text link">‘healthspan’ — the period of life spent free from chronic disease and disability</a> — but that other metrics are needed to say whether such ‘anti-ageing’ strategies can truly extend life.</p><p>Daniel Belsky, an epidemiologist who studies ageing at the Columbia University Mailman School of Public Health in New York City, cautions against over-extrapolating from mice to humans. But he also acknowledges that the study “adds to the growing understanding we have that healthspan and lifespan are not the same thing”.</p>
                    
                </div><div id="references" aria-labelledby="Bib1"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">Di Francesco, A. <i>et al.</i> <i>Nature</i> https://doi.org/10.1038/s41586-024-08026-3 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41586-024-08026-3" data-track-item_id="10.1038/s41586-024-08026-3" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-024-08026-3" aria-label="Article reference 1" data-doi="10.1038/s41586-024-08026-3">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Nature&amp;doi=10.1038%2Fs41586-024-08026-3&amp;publication_year=2024&amp;author=Di%20Francesco%2CA.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="2."><p id="ref-CR2">Fontana, L., Partridge, L. &amp; Longo, V. D. <i>Science</i> <b>328</b>, 321–326 (2010).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1126/science.1172539" data-track-item_id="10.1126/science.1172539" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1172539" aria-label="Article reference 2" data-doi="10.1126/science.1172539">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20395504" aria-label="PubMed reference 2">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Science&amp;doi=10.1126%2Fscience.1172539&amp;volume=328&amp;pages=321-326&amp;publication_year=2010&amp;author=Fontana%2CL.&amp;author=Partridge%2CL.&amp;author=Longo%2CV.%20D.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="3."><p id="ref-CR3">Mitchell, S. <i>et al.</i> <i>Cell Metab.</i> <b>29</b>, 221–228.e3 (2019).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cmet.2018.08.011" data-track-item_id="10.1016/j.cmet.2018.08.011" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cmet.2018.08.011" aria-label="Article reference 3" data-doi="10.1016/j.cmet.2018.08.011">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30197301" aria-label="PubMed reference 3">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Cell%20Metab.&amp;doi=10.1016%2Fj.cmet.2018.08.011&amp;volume=29&amp;pages=221-228.e3&amp;publication_year=2019&amp;author=Mitchell%2CS.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="4."><p id="ref-CR4">Duregon, E. <i>et al.</i> <i>Cell Metab.</i> <b>35</b>, 1179–1194.e5 (2023).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cmet.2023.05.003" data-track-item_id="10.1016/j.cmet.2023.05.003" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cmet.2023.05.003" aria-label="Article reference 4" data-doi="10.1016/j.cmet.2023.05.003">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=37437544" aria-label="PubMed reference 4">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Cell%20Metab.&amp;doi=10.1016%2Fj.cmet.2023.05.003&amp;volume=35&amp;pages=1179-1194.e5&amp;publication_year=2023&amp;author=Duregon%2CE.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="5."><p id="ref-CR5">Ravussin, E. <i>et al.</i> <i>J. Gerentol. Ser. A</i> <b>70</b>, 1097–1104 (2015).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1093/gerona/glv057" data-track-item_id="10.1093/gerona/glv057" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1093%2Fgerona%2Fglv057" aria-label="Article reference 5" data-doi="10.1093/gerona/glv057">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=J.%20Gerentol.%20Ser.%20A&amp;doi=10.1093%2Fgerona%2Fglv057&amp;volume=70&amp;pages=1097-1104&amp;publication_year=2015&amp;author=Ravussin%2CE.">
                    Google Scholar</a>&nbsp;
                </p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/d41586-024-03277-6?format=refman&amp;flavour=references">Download references</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Playable Counter-Strike Diffusion World Model (trained on 2x4090, 5M frames) (418 pts)]]></title>
            <link>https://diamond-wm.github.io/</link>
            <guid>41826402</guid>
            <pubDate>Sun, 13 Oct 2024 09:18:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diamond-wm.github.io/">https://diamond-wm.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=41826402">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h2>How does it work?</h2>
            <p>
              We train a diffusion model to predict the next frame of the game.
              The diffusion model takes into account the agent’s action and the previous frames
              to simulate the environment response.
            </p>
            <figure>
              <img src="https://diamond-wm.github.io/static/gifs/kungfu.gif" alt="The diffusion world model takes into account the agent's action and previous frames to generate the next frame.">
              <center><blockquote>The diffusion world model takes into account the agent's action and previous frames to generate the next frame.</blockquote></center>
            </figure>
          <p><br>
            The agent repeatedly provides new actions, and the diffusion model updates the game.
          </p>
          <p>
            The diffusion model acts as a world model in which the agent can learn to play.
          </p>
          <center>
            <img src="https://diamond-wm.github.io/static/gifs/imagination.gif" alt="Autoregressive generation with diffusion world model.">
            <figcaption><blockquote>Autoregressive generation enables the diffusion model to act as a world model in which the agent can learn to play.</blockquote></figcaption>
          </center>
          
            <p>
            To make the world model fast, we need to reduce the number of denoising steps.
            We found <a href="https://arxiv.org/abs/2006.11239">DDPM</a> (Ho et al. 2020) to become unstable with low numbers of denoising steps.
            In contrast, we found <a href="https://arxiv.org/abs/2206.00364">EDM</a> (Karras et al., 2022) to produce stable trajectories even for 1 denoising step.
          </p>
          <center>
            <img src="https://diamond-wm.github.io/static/gifs/ddpm.gif" alt="DDPM vs EDM based diffusion world models. The DDPM-based model becomes unstable for low numbers of denoising steps, while the EDM-based model remains stable.">
            <figcaption><blockquote>The DDPM-based model is unstable for low numbers of denoising steps due to accumulating autoregressive error, while the EDM-based model remains stable. Lower denoising steps enables a faster world model.</blockquote></figcaption>
          </center>
          
          <p>
            But in Boxing, 1-step denoising interpolates between possible outcomes and results in blurry predictions for the unpredictable black player.
          </p>
          <p>
            In contrast, using more denoising steps enables better selection of a particular mode, improving consistency over time.
          </p>
          <center>
            <img src="https://diamond-wm.github.io/static/gifs/boxing.gif" alt="Diffusion world model trajectories for the Atari game Boxing for varying numbers of denoising steps.">
            <figcaption><blockquote>Larger numbers of denoising steps n enable better mode selection for transitions with multiple modes. We therefore use n=3 for Diamond's diffusion world model.</blockquote></figcaption>
          </center>
          
          <p>
            Interestingly, the white player's movements are predicted correctly regardless of the number of denoising steps.
            This is because it is controlled by the policy, so its actions are given to the world model. This removes any ambiguity that can cause blurry predictions.
          </p>
          <p>
            We find that diffusion-based DIAMOND provides better modeling of important visual details than the discrete token-based <a href="https://arxiv.org/abs/2209.00588">IRIS</a>.
          </p>
          <center>
            <img src="https://diamond-wm.github.io/static/gifs/iris.gif" alt="Visualisation of IRIS and DIAMOND world's models on Asterix, Breakout and RoadRunner.">
            <figcaption><blockquote>DIAMOND's world model is able to better capture important visual details than the discrete token-based IRIS.</blockquote></figcaption>
          </center>
          <br>
          <div><p>
            Training an agent with reinforcement learning on this diffusion world model, DIAMOND achieves a mean human-normalized score of 1.46 on Atari 100k (46% better than human); a new best for agents trained in a world model on 100k frames.
            </p><p>
            
            Check out our <a href="https://arxiv.org/pdf/2405.12399">paper</a> for more details!
          </p></div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WordPress.org's latest move involves taking control of a WP Engine plugin (260 pts)]]></title>
            <link>https://www.theverge.com/2024/10/12/24268637/wordpress-org-matt-mullenweg-acf-fork-secure-custom-fields-wp-engine</link>
            <guid>41826082</guid>
            <pubDate>Sun, 13 Oct 2024 08:05:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/10/12/24268637/wordpress-org-matt-mullenweg-acf-fork-secure-custom-fields-wp-engine">https://www.theverge.com/2024/10/12/24268637/wordpress-org-matt-mullenweg-acf-fork-secure-custom-fields-wp-engine</a>, See on <a href="https://news.ycombinator.com/item?id=41826082">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>WordPress.org has taken over a popular WP Engine plugin in order “to remove commercial upsells and fix a security problem,” WordPress cofounder and Automattic CEO Matt Mullenweg <a href="https://wordpress.org/news/2024/10/secure-custom-fields/">announced today</a>. This “minimal” update, which he labels a fork of the Advanced Custom Fields (ACF) plugin, is <a href="https://wordpress.org/plugins/advanced-custom-fields/">now called</a> “Secure Custom Fields.”</p><p>It’s not clear what security problem Mullenweg is referring to in the post. He writes that he’s “invoking point 18 of the plugin directory guidelines,” <a href="https://github.com/wordpress/wporg-plugin-guidelines/blob/trunk/guideline-18.md">in which</a> the WordPress team reserves several rights, including removing a plugin, or changing it “without developer consent.” Mullenweg explains that the move has to do with WP Engine’s <a href="https://www.theverge.com/2024/10/3/24261016/wordpress-wp-engine-lawsuit-automattic-matt-mullenweg">recently-filed lawsuit</a> against him and Automattic.</p><div><blockquote><p>Similar situations have happened before, but not at this scale. This is a rare and unusual situation brought on by WP Engine’s legal attacks, we do not anticipate this happening for other plugins.</p></blockquote></div><p>WP Engine’s ACF team <a href="https://x.com/wp_acf/status/1845169499064107049">claimed on X</a> that WordPress has never “unilaterally and forcibly” taken a plugin “from its creator without consent.” It later <a href="https://x.com/wp_acf/status/1845190372764401908?s=46&amp;t=s7yjJ2YTk92nj3NQJLk0ww">wrote</a> that those who aren’t WP Engine, Flywheel, or ACF Pro customers will need to go to the ACF site and follow steps it <a href="https://www.advancedcustomfields.com/blog/installing-and-upgrading-to-the-latest-version-of-acf/#update-acf">published earlier</a> to “perform a 1-time download of the genuine 6.3.8 version” to keep getting updates.</p><p>As its name implies, the ACF plugin allows website creators to use custom fields when existing generic ones won’t do — something ACF’s <a href="https://www.advancedcustomfields.com/resources/getting-started-with-acf/">overview</a> of the plugin says is already a native, but “not very user friendly,” feature of WordPress. </p><p><em>The Verge</em> has reached out to Automattic, <a href="http://wordpress.org/">WordPress.org</a>, and WP Engine for comment.</p><p><em><strong>Update October 12th: </strong>Adjusted to add clarity about Mullenweg’s use of the “fork” label.</em></p></div></div>]]></description>
        </item>
    </channel>
</rss>