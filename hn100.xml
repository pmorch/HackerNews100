<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 17 Jul 2023 16:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Bad numbers in the ‚Äúgzip beats BERT‚Äù paper? (151 pts)]]></title>
            <link>https://kenschutte.com/gzip-knn-paper/</link>
            <guid>36758433</guid>
            <pubDate>Mon, 17 Jul 2023 14:08:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kenschutte.com/gzip-knn-paper/">https://kenschutte.com/gzip-knn-paper/</a>, See on <a href="https://news.ycombinator.com/item?id=36758433">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h3>Bad numbers in the "gzip beats BERT" paper?</h3><p><i>2023-07-17</i></p><p>The recent paper,<i>‚ÄúLow-Resource‚Äù Text Classification: A Parameter-Free Classification Method with Compressors</i> by Jiang et al. [<a href="https://aclanthology.org/2023.findings-acl.426/">link</a>] recently received a lot of attention on twitter.</p><p>I recently checked out their <a href="https://github.com/bazingagin/npc_gzip">source code</a> to try to recreate their results and to try out  some related ideas.</p><p>
What I found (although I could be mistaken!)
is that there appears
to be a
bug (or at least an unexpected choice) in their kNN code
that makes all the accuracy numbers
for their method higher than
expected.
<b>[tldr: it is reporting a top-2 accuracy rather than a kNN(k=2) accuracy]</b>.</p><p>Table 5 from the paper was often included in tweets and shows the <code>gzip</code> method beating all these other neural-network-based methods:</p><p><a href="https://kenschutte.com/gzip-knn-paper/table5.png"><img src="https://kenschutte.com/gzip-knn-paper/table5.png"></a></p><p>
I'll explain the details below, but my
calculations for these experiments
are (first 4 datasets,
only the "Full" column):
        </p><table id="tab1"><thead><tr><th></th><th>KinyarwandaNews</th><th>KirundiNews</th><th>DengueFilipino</th><th>SwahiliNews</th></tr></thead><tbody><tr><th>in paper</th><td>0.891</td><td>0.905</td><td>0.998</td><td>0.927</td></tr><tr><th>corrected (knn2d)</th><td>0.835</td><td>0.858</td><td>0.999</td><td>0.850</td></tr></tbody></table><p>(Fifth dataset SogouNews is large -- I haven't run it yet)</p><p>
These numbers would significantly change the take-away from
these experiments. For example,
for <code>KirundiNews</code>, the <code>gzip</code> method went from best-perfoming to worst-performing.</p><h4>kNN</h4><p>Their method uses a <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">kNN classifier</a> using k=2 (Appendix C says all experiments used k=2).</p><p>
k=2 is a bit of an odd choice for kNN classification.
For every test case, you search the training set
for the two "closest" examples. Looking at
the labels of these two, there are only two possibilities,
            </p><ul><li><i>The labels are equal.</i>
 So, this is clearly your hypothesized label.
Note that you would get the same answer for k=1.
</li><li><i>The labels are different.</i>
 We have a 1-1 tie that most be broken. There
are many ways to do a tie breaker, but one
reasonable one is take the label of the closer point.
In this case, you get the same answer for k=1.                    
</li></ul><p>

So, going from k=1 to k=2 doesn't add much information to your classifier. But, it can be different depending on the tie-breaking strategy.  
            </p><p>
It is in the case of ties that the source code
here is doing something unexpected, shown below.
            </p><h4>Code</h4><p>The issue is in <code>calc_acc</code> method in <code>experiments.py</code> [<a href="https://github.com/bazingagin/npc_gzip/blob/main/experiments.py#L88">here</a>].</p><p>Here is the relevant snippet, with my added comments, and branches dealing with <code>rand==True</code> removed for clarity:</p><div><pre><span></span><span># here, sorted_pred_lab[][] has the </span>
<span># labels and counts corresponding</span>
<span># to the top-k samples,</span>
<span>#  [[label,count],[label,count],...]</span>
<span># grouped-by label and sorted by count.</span>

<span>most_label</span> <span>=</span> <span>sorted_pred_lab</span><span>[</span><span>0</span><span>][</span><span>0</span><span>]</span>
<span>most_count</span> <span>=</span> <span>sorted_pred_lab</span><span>[</span><span>0</span><span>][</span><span>1</span><span>]</span>

<span>if_right</span> <span>=</span> <span>0</span>
<span>for</span> <span>pair</span> <span>in</span> <span>sorted_pred_lab</span><span>:</span>
    <span># we loop until we drop below 'most_count', ie</span>
    <span># this for-loop iterates over those classes</span>
    <span># tied for highest count</span>
    <span>if</span> <span>pair</span><span>[</span><span>1</span><span>]</span> <span>&lt;</span> <span>most_count</span><span>:</span>
        <span>break</span>

    <span># this says if ANY of those</span>
    <span># in the tied-set are equal to</span>
    <span># the test label,</span>
    <span># it is marked correct (if_right=1)</span>
    <span>if</span> <span>pair</span><span>[</span><span>0</span><span>]</span> <span>==</span> <span>label</span><span>[</span><span>i</span><span>]:</span>
        <span>if_right</span> <span>=</span> <span>1</span>
        <span>most_label</span> <span>=</span> <span>pair</span><span>[</span><span>0</span><span>]</span>

<span># accumulate results:        </span>
<span>pred</span><span>.</span><span>append</span><span>(</span><span>most_label</span><span>)</span>
<span>correct</span><span>.</span><span>append</span><span>(</span><span>if_right</span><span>)</span>
        
</pre></div><p>So, if any of the tie-break labels is equal to the test label, it is marked as correct. For k=2, a tie simply means there was one vote for each of two different classes amongst the 2 closest training points. Therefore, the reported accuracies could be considered <b>top-2</b>, meaning that it's marked correct if either of the top  two choices is correct (you may have encountered top-k in ImageNet, where top-5 accuracy is often cited).</p><p>This method takes arbitrary <code>k</code> but note that it doesn't compute <code>top-k</code> for any <code>k</code>. Only in the special case of <code>k=2</code> do we have that when there is a tie, all the <code>k</code> examples are tied with the max value (1).</p><p>The <code>calc_acc</code> method has a <code>rand</code> flag that seems to be correct:  if <code>rand==True</code> it will correctly break the tie using <code>random.choice</code>. But it seems that this wasn't used for the paper results.</p><h4>Re-calc</h4><p>
I wrote a simple implementation with two
different tie-breaking strategies [<a href="https://gist.github.com/kts/46709a70c4efd167eff7ccfa62f30dd8">gist</a>]:</p><ul><li>[r] random selection</li><li>[d] decrement k until you are left with a case without ties.</li></ul><pre>Results        
           kinnews  kirnews  filipino swahili 
table5     0.891    0.905    0.998    0.927   value in paper
code       0.891    0.906    1.000    0.927   using npc_gzip repo
top2       0.891    0.906    1.000    0.927   top-2
knn1r      0.835    0.858    0.999    0.850   kNN,k=1,tie=random
knn1d      0.835    0.858    0.999    0.850   kNN,k=1,tie=decrement
knn2r      0.828    0.807    0.851    0.842   kNN,k=2,tie=random
knn3r      0.838    0.791    0.851    0.881   kNN,k=3,tie=random
knn2d      0.835    0.858    0.999    0.850   kNN,k=2,tie=decrement
knn3d      0.843    0.794    0.904    0.883   kNN,k=3,tie=decrement
</pre><p>Some sanity checks:</p><ul><li><code>code</code> always equals <code>top2</code></li><li><code>knn1r == knn1d</code>. There are never ties for k=1</li><li><code>knn2d == knn1d</code>. For k=2, ties go to first, so same as using k=1.</li><li><code>knn2r &lt; knn2d</code>. For k=2, on a 1-1 tie, random is just taking the further one 50% of the time. So, it makes sence that's worse than just taking the closest.</li><li><code>table5</code> very close to <code>code</code> (within 0.001 or 0.002): able to recreate numbers from paper</li></ul><p>todo:</p><ul><li>Why is filipino so high (1.0 in one case)?</li><li>Why is 'table5' slightly different than 'code' in two cases?</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Firefox-only minimap (2021) (399 pts)]]></title>
            <link>https://www.stefanjudis.com/a-firefox-only-minimap/</link>
            <guid>36757542</guid>
            <pubDate>Mon, 17 Jul 2023 12:44:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stefanjudis.com/a-firefox-only-minimap/">https://www.stefanjudis.com/a-firefox-only-minimap/</a>, See on <a href="https://news.ycombinator.com/item?id=36757542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="map"><main id="main"><dl><p><dt>Updated at</dt><dd><time datetime="2021-10-18T09:06:08.611Z"><time-ago datetime="2021-10-18T09:06:08.611Z">Oct 18 2021</time-ago></time></dd></p><p><dt>Reading time</dt><dd>1min</dd></p></dl><div><p>Greetings! üëã</p><p>If you discovered this page, you're part of the few Firefox desktop users out there (market share is only 4% right now) and wondered how my blog posts' minimaps are made.</p><p>If you're not using Firefox, this is how the minimap looks like.</p><div><figure><a href="https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png"><picture><source type="image/avif" srcset="https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=avif&amp;fit=scale&amp;q=75&amp;w=300&amp;h=136 300w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=avif&amp;fit=scale&amp;q=75&amp;w=500&amp;h=228 500w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=avif&amp;fit=scale&amp;q=75&amp;w=700&amp;h=319 700w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=avif&amp;fit=scale&amp;q=75&amp;w=900&amp;h=410 900w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=avif&amp;fit=scale&amp;q=75&amp;w=1100&amp;h=502 1100w" sizes="(max-width: 50em) 98vw, 700px"><source type="image/webp" srcset="https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=webp&amp;fit=scale&amp;q=75&amp;w=300&amp;h=136 300w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=webp&amp;fit=scale&amp;q=75&amp;w=500&amp;h=228 500w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=webp&amp;fit=scale&amp;q=75&amp;w=700&amp;h=319 700w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=webp&amp;fit=scale&amp;q=75&amp;w=900&amp;h=410 900w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=webp&amp;fit=scale&amp;q=75&amp;w=1100&amp;h=502 1100w" sizes="(max-width: 50em) 98vw, 700px"><img width="1000" height="456" srcset="https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=jpg&amp;fit=scale&amp;q=75&amp;w=300&amp;h=136 300w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=jpg&amp;fit=scale&amp;q=75&amp;w=500&amp;h=228 500w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=jpg&amp;fit=scale&amp;q=75&amp;w=700&amp;h=319 700w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=jpg&amp;fit=scale&amp;q=75&amp;w=900&amp;h=410 900w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=jpg&amp;fit=scale&amp;q=75&amp;w=1100&amp;h=502 1100w" sizes="(max-width: 50em) 98vw, 700px" src="https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png" alt="Minimap on stefanjudis.com." loading="lazy" onload="this.classList.add('kf-fade-in')"></picture></a></figure></div><p>Firefox is the only browser that supports <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/element()">the fancy <code>element()</code> CSS function</a> (with a vendor prefix, but hey ü§∑‚Äç‚ôÇÔ∏è). The function allows you to display images of arbitrary HTML elements on your page! And the best thing is: it's live! Try selecting some text or scroll around to see lazy-loaded images kicking in. It's magic!</p><p>The CSS to define another HTML element as background image is the following:</p><pre><code><span>mini-map .screen-image .canvas</span> <span>{</span>
  <span>background</span><span>:</span> white <span>-moz-element</span><span>(</span>#main<span>)</span> no-repeat scroll center center / contain<span>;</span>
<span>}</span>
</code></pre><p>There's also some JavaScript to move the minimap's current viewport box, but the CSS one-liner is responsible for painting another DOM node. Use <code>-moz-element</code> and call it a day!</p><p>And with that, keep rocking (and using Firefox)!</p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikipedia-grounded chatbot ‚Äúoutperforms all baselines‚Äù on factual accuracy (101 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-07-17/Recent_research</link>
            <guid>36757520</guid>
            <pubDate>Mon, 17 Jul 2023 12:41:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-07-17/Recent_research">https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-07-17/Recent_research</a>, See on <a href="https://news.ycombinator.com/item?id=36757520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div><p><span typeof="mw:File"><a href="https://meta.wikimedia.org/wiki/Research:Newsletter" title="m:Research:Newsletter"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Wikimedia_Research_Newsletter_Logo.png/76px-Wikimedia_Research_Newsletter_Logo.png" decoding="async" width="76" height="76" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Wikimedia_Research_Newsletter_Logo.png/114px-Wikimedia_Research_Newsletter_Logo.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Wikimedia_Research_Newsletter_Logo.png/152px-Wikimedia_Research_Newsletter_Logo.png 2x" data-file-width="375" data-file-height="375"></a></span></p><p>A monthly overview of recent academic research about Wikipedia and other Wikimedia projects, also published as the <a href="https://meta.wikimedia.org/wiki/Research:Newsletter" title="m:Research:Newsletter">Wikimedia Research Newsletter</a>.</p></div>
<h3><span id="Wikipedia_and_open_access" data-mw-thread-id="h-Wikipedia_and_open_access-signpost-article-title"><span data-mw-comment-start="" id="h-Wikipedia_and_open_access-signpost-article-title"></span>Wikipedia and open access<span data-mw-comment-end="h-Wikipedia_and_open_access-signpost-article-title"></span></span></h3>
<dl><dd><i>Reviewed by <a href="https://en.wikipedia.org/wiki/User:Jullienn" title="User:Jullienn">Nicolas Jullien</a></i></dd></dl>
<p>From the abstract:<sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup>:
</p>
<blockquote> 
<p>"we analyze a large dataset of citations from Wikipedia and model the role of open access in Wikipedia's citation patterns. We find that open-access articles are extensively and increasingly more cited in Wikipedia. What is more, they show a 15% higher likelihood of being cited in Wikipedia when compared to closed-access articles, after controlling for confounding factors. This open-access citation effect is particularly strong for articles with low citation counts, including recently published ones. Our results show that open access plays a key role in the dissemination of scientific knowledge, including by providing Wikipedia editors timely access to novel results."
</p>
</blockquote>
<h4></h4>
<p>This article is a first draft of an analysis of the relationship between the availability of a scientific journal as <a href="https://en.wikipedia.org/wiki/Open_access" title="Open access">open access</a> and the fact that it is cited in the English Wikipedia (note: although it speaks of "Wikipedia", the article looks only at the English pages). It is a preprint and has not been peer-reviewed, so its results should be read with caution, especially since I am not sure about the robustness of the model and the results derived from it (see below). It is of course a very important issue, as access to scientific sources is key to the diffusion of scientific knowledge, but also, as the authors mention, because Wikipedia is seen as central to the diffusion of scientific facts (and is sometimes used by scientists to push their ideas).
</p>
<h4><span id="Review" data-mw-thread-id="h-Review-Wikipedia_and_open_access"><span data-mw-comment-start="" id="h-Review-Wikipedia_and_open_access"></span>Review<span data-mw-comment-end="h-Review-Wikipedia_and_open_access"></span></span></h4>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Figure_3-_Distribution_of_OA_status_and_count_of_citations_by_OpenAlex_concept_(from_Yang_et_al.,_Wikipedia_and_Open_Access).png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Figure_3-_Distribution_of_OA_status_and_count_of_citations_by_OpenAlex_concept_%28from_Yang_et_al.%2C_Wikipedia_and_Open_Access%29.png/500px-Figure_3-_Distribution_of_OA_status_and_count_of_citations_by_OpenAlex_concept_%28from_Yang_et_al.%2C_Wikipedia_and_Open_Access%29.png" decoding="async" width="500" height="291" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Figure_3-_Distribution_of_OA_status_and_count_of_citations_by_OpenAlex_concept_%28from_Yang_et_al.%2C_Wikipedia_and_Open_Access%29.png/750px-Figure_3-_Distribution_of_OA_status_and_count_of_citations_by_OpenAlex_concept_%28from_Yang_et_al.%2C_Wikipedia_and_Open_Access%29.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Figure_3-_Distribution_of_OA_status_and_count_of_citations_by_OpenAlex_concept_%28from_Yang_et_al.%2C_Wikipedia_and_Open_Access%29.png/1000px-Figure_3-_Distribution_of_OA_status_and_count_of_citations_by_OpenAlex_concept_%28from_Yang_et_al.%2C_Wikipedia_and_Open_Access%29.png 2x" data-file-width="4068" data-file-height="2370"></a><figcaption>"Distribution of OA status and count of citations [on English Wikipedia] by <a href="https://en.wikipedia.org/wiki/OpenAlex" title="OpenAlex">OpenAlex</a> concept" (figure 3 from the paper). The black dotted line represent the overall average.</figcaption></figure>
<p>The results presented in the article (and its abstract) highlight two important issues for Wikipedia that will likely be addressed in a more complete version of the paper:
</p>
<ul><li>The question of the reliability of the sources used by Wikipedians</li></ul>
<dl><dd><dl><dd>‚Üí The regressions seem to indicate that the reputation of the journal is not important to be cited in Wikipedia.</dd>
<dd>‚Üí <a href="https://en.wikipedia.org/wiki/Predatory_journals" title="Predatory journals">Predatory journals</a> are known to be more often open access than classical journals, which means that this result potentially indicates that the phenomenon of open access reduces the seriousness of Wikipedia sources.</dd></dl></dd></dl>
<p>The authors say on p. 4 that they provided "each journal with an <a href="https://en.wikipedia.org/wiki/SCImago_Journal_Rank" title="SCImago Journal Rank">SJR score</a>, <a href="https://en.wikipedia.org/wiki/H-index" title="H-index">H-index</a>, and other relevant information."
Why did they not use this as a control variable?
(this echoes a debate on the role of Wikipedia: is it to disseminate verified knowledge, or to serve as a platform for the dissemination of new theories? The authors seem to lean towards the second view: p. 2: "With the rapid development of the Internet, traditional peer review and journal publication can no longer meet the need for the development of new ideas".)
</p>
<ul><li>The solidity of the paper's conclusions</li></ul>
<dl><dd>The authors said: "STEM fields, especially biology and medicine, comprise the most prominent scientific topics in Wikipedia [17]." "General science, technology, and biomedical research have relatively higher OA rates."
<dl><dd>‚Üí So, it is obvious that, on average, there are more citations of Open Access articles in Wikipedia (than in the entire available research corpus), and explain that open access articles are cited more.</dd>
<dd>‚Üí Why not control for academic discipline in the models?</dd></dl></dd></dl>
<p>More problematic (and acknowledged by the authors, so probably in the process of being addressed), the authors said, on p.7, that they built their model with the assumption that the age of a research article and the number of citations it has both influence the probability of an article being cited in Wikipedia.
Of course, for this causal effect to hold, the age and the number of citations must be taken into account at the moment the article is cited in Wikipedia. For example, if some of the citations are made after the citation in Wikipedia, one could argue that the causal effect could be in the other direction.
Also, many articles are open access after an embargo period, and are therefore considered open access in the analysis, whereas they may have been cited in Wikipedia when they were under embargo.
The authors did not check for this, as acknowledged in the last sentence of the article. Would their result hold if they do their model taking the first citation in the English Wikipedia, for example, and the age of the article, its open access status, etc. at that moment? 
</p>
<h4><span id="In_short" data-mw-thread-id="h-In_short-Wikipedia_and_open_access"><span data-mw-comment-start="" id="h-In_short-Wikipedia_and_open_access"></span>In short<span data-mw-comment-end="h-In_short-Wikipedia_and_open_access"></span></span></h4>
<p>Although this first draft is probably not solid enough to be cited in Wikipedia, it signals important research in progress, and I am sure that the richness of the data and the quality of the team will quickly lead to very interesting insights for the Wikipedia community.
</p>
<h4></h4>
<ul><li><a href="https://meta.wikimedia.org/wiki/Research:Newsletter/2020/August#%22Quantifying_Engagement_with_Citations_on_Wikipedia%22" title="m:Research:Newsletter/2020/August">"Quantifying Engagement with Citations on Wikipedia"</a> (about a 2020 paper that among other results found that "open access sources [...] are particularly popular" with readers)</li>
<li>"<a href="https://meta.wikimedia.org/wiki/Research:Newsletter/2022/March#English_Wikipedia_lacking_in_open_access_references" title="m:Research:Newsletter/2022/March">English Wikipedia lacking in open access references</a>" (2022)</li></ul>
<h3><span id=".22Controversies_over_Historical_Revisionism_in_Wikipedia.22"></span><span id="&quot;Controversies_over_Historical_Revisionism_in_Wikipedia&quot;" data-mw-thread-id="h-&quot;Controversies_over_Historical_Revisionism_in_Wikipedia&quot;-signpost-article-title"><span data-mw-comment-start="" id="h-&quot;Controversies_over_Historical_Revisionism_in_Wikipedia&quot;-signpost-article-title"></span>"Controversies over Historical Revisionism in Wikipedia"<span data-mw-comment-end="h-&quot;Controversies_over_Historical_Revisionism_in_Wikipedia&quot;-signpost-article-title"></span></span></h3>
<dl><dd><i>Reviewed by <a href="https://en.wikipedia.org/wiki/User:Jayen466" title="User:Jayen466">Andreas Kolbe</a></i></dd></dl>
<p>From the abstract:<sup id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup>
</p>
<blockquote><p> 
This study investigates the development of historical revisionism on Wikipedia. The edit history of Wikipedia pages allows us to trace the dynamics of individuals and coordinated groups surrounding controversial topics. This project focuses on Japan, where there has been a recent increase in right-wing discourse and dissemination of different interpretations of historical events.</p></blockquote>
<p>This brief study, one of the <a href="https://en.wikipedia.org/wiki/Extended_abstract" title="Extended abstract">extended abstracts</a> accepted at the <a rel="nofollow" href="https://wikiworkshop.org/2023/">Wiki Workshop (10th edition)</a>, follows up on reports that some historical pages on the Japanese Wikipedia, particularly those related to World War II and war crimes, have been edited in ways that reflect radical right-wing ideas (see previous <i>Signpost</i> <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2021-03-28/In_the_media" title="Wikipedia:Wikipedia Signpost/2021-03-28/In the media">coverage</a>). It sets out to answer three questions: 
</p>
<ol><li>What types of historical topics are most susceptible to historical revisionism?</li>
<li>What are the common factors for the historical topics that are subject to revisionism?</li>
<li>Are there groups of editors who are seeking to disseminate revisionist narratives?</li></ol>
<p>The study focuses on the level of controversy of historical articles, based on the notion that the introduction of revisionism is likely to lead to edit wars. The authors found that the most controversial historical articles in the Japanese Wikipedia were indeed focused on areas that are of particular interest to revisionists. From the findings:
</p>
<blockquote><p>Articles related to WWII exhibited significantly greater controversy than general historical articles. Among the top 20 most controversial articles, eleven were largely related to Japanese war crimes and right-wing ideology. Over time, the number of contributing editors and the level of controversy increased. Furthermore, editors involved in edit wars were more likely to contribute to a higher number of controversial articles, particularly those related to right-wing ideology. These findings suggest the possible presence of groups of editors seeking to disseminate revisionist narratives.</p></blockquote> 
<p>The paper establishes that articles covering these topic areas in the Japanese Wikipedia are contested and subject to edit wars. However, it does not measure to what extent article content has been compromised. Edit wars could be a sign of mainstream editors pushing back against revisionists, while conversely an absence of edit wars could indicate that a project has been captured (cf. the <a href="https://en.wikipedia.org/wiki/Croatian_Wikipedia" title="Croatian Wikipedia">Croatian Wikipedia</a>). While this little paper is a useful start, further research on the Japanese Wikipedia seems warranted.
</p><p><i>See also our earlier coverage of a related paper: "<a href="https://meta.wikimedia.org/wiki/Research:Newsletter/2022/November#Wikimedia_Foundation_builds_%22Knowledge_Integrity_Risk_Observatory%22_to_enable_communities_to_monitor_at-risk_Wikipedias" title="m:Research:Newsletter/2022/November">Wikimedia Foundation builds 'Knowledge Integrity Risk Observatory' to enable communities to monitor at-risk Wikipedias</a>"</i>
</p>
<h3><span id="Wikipedia-based_LLM_chatbot_.22outperforms_all_baselines.22_regarding_factual_accuracy"></span><span id="Wikipedia-based_LLM_chatbot_&quot;outperforms_all_baselines&quot;_regarding_factual_accuracy" data-mw-thread-id="h-Wikipedia-based_LLM_chatbot_&quot;outperforms_all_baselines&quot;_regarding_factual_accura-signpost-article-title"><span data-mw-comment-start="" id="h-Wikipedia-based_LLM_chatbot_&quot;outperforms_all_baselines&quot;_regarding_factual_accura-signpost-article-title"></span>Wikipedia-based LLM chatbot "outperforms all baselines" regarding factual accuracy<span data-mw-comment-end="h-Wikipedia-based_LLM_chatbot_&quot;outperforms_all_baselines&quot;_regarding_factual_accura-signpost-article-title"></span></span></h3>
<dl><dd><i>Reviewed by <a href="https://en.wikipedia.org/wiki/User:HaeB" title="User:HaeB">Tilman Bayer</a></i></dd></dl>
<p>This preprint<sup id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup> (by three graduate students at Stanford University's computer science department and <a href="https://en.wikipedia.org/wiki/Monica_S._Lam" title="Monica S. Lam">Monica S. Lam</a> as fourth author) discusses the construction of a Wikipedia-based chatbot:
</p>
<blockquote>
<p>"We design WikiChat [...] to ground LLMs using Wikipedia to achieve the following objectives. While LLMs tend to hallucinate, our chatbot should be factual. While introducing facts to the conversation, we need to maintain the qualities of LLMs in being relevant, conversational, and engaging."
</p>
</blockquote>
<p>The paper sets out from the observation that
</p>
<blockquote>
<p>"LLMs cannot speak accurately about events that occurred after their training, which are often topics of great interest to users, and [...] are highly prone to hallucination when talking about less popular (tail) topics. [...] Through many iterations of experimentation, we have crafted a pipeline based on information retrieval that (1) uses LLMs to suggest interesting and relevant facts that are individually verified against Wikipedia, (2) retrieves additional up-to-date information, and (3) composes coherent and engaging time-aware responses. [...] We focus on evaluating important but previously neglected issues such as conversing about recent and tail topics. We find that WikiChat outperforms all baselines in terms of the factual accuracy of its claims, by up to 12.1%, 28.3% and 32.7% on head, recent and tail topics, while matching <a href="https://en.wikipedia.org/wiki/GPT-3.5" title="GPT-3.5">GPT-3.5</a> in terms of providing natural, relevant, non-repetitive and informational responses."
</p>
</blockquote>
<p>The researchers argue that "most chatbots are evaluated only on static crowdsourced benchmarks like Wizard of Wikipedia (Dinan et al., 2019) and Wizard of Internet (Komeili et al., 2022). Even when human evaluation is used, evaluation is conducted only on familiar discussion topics. This leads to an overestimation of the capabilities of chatbots." They call such topics "head topics" ("Examples include <a href="https://en.wikipedia.org/wiki/Albert_Einstein" title="Albert Einstein">Albert Einstein</a> or <a href="https://en.wikipedia.org/wiki/FC_Barcelona" title="FC Barcelona">FC Barcelona</a>"). In  contrast, the lesser known "tail topics [are] likely to be present in the pre-training data of LLMs at low frequency. Examples include <a href="https://en.wikipedia.org/wiki/Thomas_Percy_Hilditch" title="Thomas Percy Hilditch">Thomas Percy Hilditch</a> or <a href="https://en.wikipedia.org/wiki/Hell%27s_Kitchen_Suomi" title="Hell's Kitchen Suomi">Hell's Kitchen Suomi</a>". As a third category, they consider "recent topics" ("topics that happened in 2023, and therefore are absent from the pre-training corpus of LLMs, even though some background information about them could be present. Examples include <a href="https://en.wikipedia.org/wiki/Spare_(memoir)" title="Spare (memoir)"><i>Spare</i> (memoir)</a> or <a href="https://en.wikipedia.org/wiki/2023_Australian_Open" title="2023 Australian Open">2023 Australian Open</a>"). The latter are obtained from a list of most edited Wikipedia articles in early 2023. 
</p><p>Regarding the "core verification problem [...] whether a claim is backed up by the retrieved paragraphs [the researchers] found that there is a significant gap between LLMs (even GPT-4) and human performance [...]. Therefore, we conduct human evaluation via crowdsourcing, to classify each claim as supported, refuted, or [not having] enough information." (This observation may be of interest regarding efforts to use LLMs as a tools for Wikipedians to check the <a href="https://en.wikipedia.org/wiki/Wikipedia:TSI" title="Wikipedia:TSI">integrity of citations on Wikipedia</a>. See also <a href="#&quot;WiCE:_Real-World_Entailment_for_Claims_in_Wikipedia&quot;">the "WiCE" paper below</a>.) 
</p><p>In contrast, the evalution for "conversationality" is conducted "with simulated users using LLMs. LLMs are good at simulating users: they have the general familiarity with world knowledge and know how users behave socially. They are free to occasionally hallucinate, make mistakes, and repeat or even contradict themselves, as human users sometimes do."
</p><p>In the paper's evaluation, WikiChat impressively outperforms the two comparison baselines in all three topic areas (even the well-known "head" topics). It may be worth noting though that the comparison did not include widely used chatbots such as <a href="https://en.wikipedia.org/wiki/ChatGPT" title="ChatGPT">ChatGPT</a> or <a href="https://en.wikipedia.org/wiki/Bing_AI" title="Bing AI">Bing AI</a>. Instead, the authors chose to compare their chatbot with Atlas (describing it as based on a retrieval-augmented language model that is "state-of-the-art [...] on the KILT benchmark") and GPT-3.5 (while ChatGPT is or has been based on GPT-3.5 too, it involved extensive additional <a href="https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)" title="Fine-tuning (deep learning)">finetuning</a> <a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">by humans</a>).
</p>
<h3><span id="Briefly" data-mw-thread-id="h-Briefly-signpost-article-title"><span data-mw-comment-start="" id="h-Briefly-signpost-article-title"></span>Briefly<span data-mw-comment-end="h-Briefly-signpost-article-title"></span></span></h3>
<dl><dd><small><i>Compiled by <a href="https://en.wikipedia.org/wiki/User:HaeB" title="User:HaeB">Tilman Bayer</a></i></small></dd></dl>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:ChatGPT_with_experimental_Wikipedia_plugin.png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ChatGPT_with_experimental_Wikipedia_plugin.png/220px-ChatGPT_with_experimental_Wikipedia_plugin.png" decoding="async" width="220" height="260" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ChatGPT_with_experimental_Wikipedia_plugin.png/330px-ChatGPT_with_experimental_Wikipedia_plugin.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ChatGPT_with_experimental_Wikipedia_plugin.png/440px-ChatGPT_with_experimental_Wikipedia_plugin.png 2x" data-file-width="1004" data-file-height="1188"></a><figcaption>Example interaction with the Wikipedia ChatGPT plugin, showing an answer generated from the article <a href="https://en.wikipedia.org/wiki/2023_FIFA_Women%27s_World_Cup" title="2023 FIFA Women's World Cup">2023 FIFA Women's World Cup</a></figcaption></figure>
<h4><span id="Wikimedia_Foundation_launches_experimental_ChatGPT_plugin_for_Wikipedia" data-mw-thread-id="h-Wikimedia_Foundation_launches_experimental_ChatGPT_plugin_for_Wikipedia-Briefly"><span data-mw-comment-start="" id="h-Wikimedia_Foundation_launches_experimental_ChatGPT_plugin_for_Wikipedia-Briefly"></span>Wikimedia Foundation launches experimental ChatGPT plugin for Wikipedia<span data-mw-comment-end="h-Wikimedia_Foundation_launches_experimental_ChatGPT_plugin_for_Wikipedia-Briefly"></span></span></h4>
<p>As part of an effort "to understand how Wikimedia can become the essential infrastructure of free knowledge in a possible future state where AI transforms knowledge search", on July 13 the Wikimedia Foundation <a href="https://diff.wikimedia.org/2023/07/13/exploring-paths-for-the-future-of-free-knowledge-new-wikipedia-chatgpt-plugin-leveraging-rich-media-social-apps-and-other-experiments/">announced</a> a new Wikipedia-based plugin for ChatGPT. (Such third-party plugins are currently available to all subscribers of ChatGPT Plus, <a href="https://en.wikipedia.org/wiki/OpenAI" title="OpenAI">OpenAI</a>'s paid variant of their chatbot; <a href="https://gitlab.wikimedia.org/repos/machine-learning/chatgpt-plugin/-/tree/dev">the Wikipedia plugin's code</a> itself is available as open source.) The Foundation describes it as an experiment designed answer research questions such as "whether users of AI assistants like ChatGPT are interested in getting summaries of verifiable knowledge from Wikipedia".
</p><p>The plugin works by first performing a Google site search on Wikipedia to find articles matching the user's query, and then passing the first few paragraphs of each article's text to ChatGPT, together with additional (hidden) <a href="https://gitlab.wikimedia.org/repos/machine-learning/chatgpt-plugin/-/blob/dev/app/prompt_constants.py">instruction prompts</a> on how the assistant should use them to generate an answer for the user (e.g. "In ALL responses, Assistant MUST always link to the Wikipedia articles used").
</p>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Diagram_of_interactions_(Wikipedia_ChatGPT_Plugin).png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Diagram_of_interactions_%28Wikipedia_ChatGPT_Plugin%29.png/500px-Diagram_of_interactions_%28Wikipedia_ChatGPT_Plugin%29.png" decoding="async" width="500" height="188" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Diagram_of_interactions_%28Wikipedia_ChatGPT_Plugin%29.png/750px-Diagram_of_interactions_%28Wikipedia_ChatGPT_Plugin%29.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Diagram_of_interactions_%28Wikipedia_ChatGPT_Plugin%29.png/1000px-Diagram_of_interactions_%28Wikipedia_ChatGPT_Plugin%29.png 2x" data-file-width="3242" data-file-height="1222"></a><figcaption>Diagram of interactions as a user makes a request to the Wikipedia ChatGPT plugin (compare <a rel="nofollow" href="https://twitter.com/WikiResearch/status/1661242880437145600">a similar diagram</a> from the "WikiChat" paper reviewed above)</figcaption></figure>
<h4><span id="Wikimedia_Foundation_Research_report" data-mw-thread-id="h-Wikimedia_Foundation_Research_report-Briefly"><span data-mw-comment-start="" id="h-Wikimedia_Foundation_Research_report-Briefly"></span>Wikimedia Foundation Research report<span data-mw-comment-end="h-Wikimedia_Foundation_Research_report-Briefly"></span></span></h4>
<p>The Wikimedia Foundation's Research department has published its <a href="https://research.wikimedia.org/report.html">biannual activity report</a>, <a href="https://lists.wikimedia.org/hyperkitty/list/wiki-research-l@lists.wikimedia.org/thread/T42KFULMRVJV25UWZ6SPI76V5P4CLYMO/">covering</a> the work of the department's 10 staff members as well as its contractors and formal collaborators during the first half of 2023.
</p>
<h4><span id="New_per-country_pageview_dataset" data-mw-thread-id="h-New_per-country_pageview_dataset-Briefly"><span data-mw-comment-start="" id="h-New_per-country_pageview_dataset-Briefly"></span>New per-country pageview dataset<span data-mw-comment-end="h-New_per-country_pageview_dataset-Briefly"></span></span></h4>
<p>The Wikimedia Foundation <a href="https://diff.wikimedia.org/2023/06/21/new-dataset-uncovers-wikipedia-browsing-habits-while-protecting-users/">announced</a> the public release of "almost 8 years of pageview data, partitioned by country, project, and page", sanitized using <a href="https://en.wikipedia.org/wiki/Differential_privacy" title="Differential privacy">differential privacy</a> to protect sensitive information. See <a href="https://meta.wikimedia.org/wiki/Research:Data#Differential_privacy" title="m:Research:Data">documentation</a>
</p>
<h4><span id="Wikimedia_Research_Showcase" data-mw-thread-id="h-Wikimedia_Research_Showcase-Briefly"><span data-mw-comment-start="" id="h-Wikimedia_Research_Showcase-Briefly"></span>Wikimedia Research Showcase<span data-mw-comment-end="h-Wikimedia_Research_Showcase-Briefly"></span></span></h4>
<p>See the <a href="https://www.mediawiki.org/wiki/Wikimedia_Research/Showcase" title="mw:Wikimedia Research/Showcase">page of the monthly <b>Wikimedia Research Showcase</b></a> for videos and slides of past presentations.
</p>
<h3><span id="Other_recent_publications" data-mw-thread-id="h-Other_recent_publications-signpost-article-title"><span data-mw-comment-start="" id="h-Other_recent_publications-signpost-article-title"></span>Other recent publications<span data-mw-comment-end="h-Other_recent_publications-signpost-article-title"></span></span></h3>
<p><i>Other recent publications that could not be covered in time for this issue include the items listed below. Contributions, whether reviewing or summarizing newly published research, <a href="https://meta.wikimedia.org/wiki/Research:Newsletter#How_to_contribute" title="m:Research:Newsletter">are always welcome</a>.</i>
</p>
<dl><dd><small><i>Compiled by <a href="https://en.wikipedia.org/wiki/User:HaeB" title="User:HaeB">Tilman Bayer</a></i></small></dd></dl>
<h4><span id="Prompting_ChatGPT_to_answer_according_to_Wikipedia_reduces_hallucinations" data-mw-thread-id="h-Prompting_ChatGPT_to_answer_according_to_Wikipedia_reduces_hallucinations-Other_recent_publications"><span data-mw-comment-start="" id="h-Prompting_ChatGPT_to_answer_according_to_Wikipedia_reduces_hallucinations-Other_recent_publications"></span>Prompting ChatGPT to answer according to Wikipedia reduces hallucinations<span data-mw-comment-end="h-Prompting_ChatGPT_to_answer_according_to_Wikipedia_reduces_hallucinations-Other_recent_publications"></span></span></h4>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Prompting_LLMs_to_respond_with_quotes_directly_from_pre-training_data_(Figure_1_from_Weller_et_al._2023).png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Prompting_LLMs_to_respond_with_quotes_directly_from_pre-training_data_%28Figure_1_from_Weller_et_al._2023%29.png/520px-Prompting_LLMs_to_respond_with_quotes_directly_from_pre-training_data_%28Figure_1_from_Weller_et_al._2023%29.png" decoding="async" width="520" height="627" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Prompting_LLMs_to_respond_with_quotes_directly_from_pre-training_data_%28Figure_1_from_Weller_et_al._2023%29.png/780px-Prompting_LLMs_to_respond_with_quotes_directly_from_pre-training_data_%28Figure_1_from_Weller_et_al._2023%29.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Prompting_LLMs_to_respond_with_quotes_directly_from_pre-training_data_%28Figure_1_from_Weller_et_al._2023%29.png/1040px-Prompting_LLMs_to_respond_with_quotes_directly_from_pre-training_data_%28Figure_1_from_Weller_et_al._2023%29.png 2x" data-file-width="2048" data-file-height="2470"></a><figcaption>Figure from the paper: "Prompting LLMs to respond with quotes directly from pre-training data (shown in purple)"</figcaption></figure>
<p>From the abstract:<sup id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup>
</p>
<blockquote><p> 
"Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of 'according to sources', we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on Wikipedia that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance."</p></blockquote>
<p>The authors tested various variations of such "grounding prompts" (e.g. "As an expert editor for Wikipedia, I am confident in the following answer." or "I found some results for that on Wikipedia. Here‚Äôs a direct quote:"). The best performing prompt was "Respond to this question using only information that can be attributed to Wikipedia".
</p>
<h4><span id=".22Citations_as_Queries:_Source_Attribution_Using_Language_Models_as_Rerankers.22"></span><span id="&quot;Citations_as_Queries:_Source_Attribution_Using_Language_Models_as_Rerankers&quot;" data-mw-thread-id="h-&quot;Citations_as_Queries:_Source_Attribution_Using_Language_Models_as_Rerankers&quot;-Other_recent_publications"><span data-mw-comment-start="" id="h-&quot;Citations_as_Queries:_Source_Attribution_Using_Language_Models_as_Rerankers&quot;-Other_recent_publications"></span>"Citations as Queries: Source Attribution Using Language Models as Rerankers"<span data-mw-comment-end="h-&quot;Citations_as_Queries:_Source_Attribution_Using_Language_Models_as_Rerankers&quot;-Other_recent_publications"></span></span></h4>
<p>From the abstract:<sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup>
</p>
<blockquote> 
<p>"This paper explores new methods for locating the sources used to write a text, by fine-tuning a variety of language models to rerank candidate sources. [...] We conduct experiments on two datasets, English Wikipedia and medieval Arabic historical writing, and employ a variety of retrieval and generation based reranking models. [...] We find that <a href="https://en.wikipedia.org/wiki/Semisupervised_learning" title="Semisupervised learning">semisupervised</a> methods can be nearly as effective as fully supervised methods while avoiding potentially costly span-level annotation of the target and source documents."
</p>
</blockquote>

<h4><span id=".22WiCE:_Real-World_Entailment_for_Claims_in_Wikipedia.22"></span><span id="&quot;WiCE:_Real-World_Entailment_for_Claims_in_Wikipedia&quot;" data-mw-thread-id="h-&quot;WiCE:_Real-World_Entailment_for_Claims_in_Wikipedia&quot;-Other_recent_publications"><span data-mw-comment-start="" id="h-&quot;WiCE:_Real-World_Entailment_for_Claims_in_Wikipedia&quot;-Other_recent_publications"></span>"WiCE: Real-World Entailment for Claims in Wikipedia"<span data-mw-comment-end="h-&quot;WiCE:_Real-World_Entailment_for_Claims_in_Wikipedia&quot;-Other_recent_publications"></span></span></h4>
<p>From the abstract:<sup id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup>
</p>
<blockquote> 
<p>"We propose WiCE, a new <a href="https://en.wikipedia.org/wiki/Textual_entailment" title="Textual entailment">textual entailment</a> dataset centered around verifying claims in text, built on real-world claims and evidence in Wikipedia with fine-grained annotations. We collect sentences in Wikipedia that cite one or more webpages and annotate whether the content on those pages entails those sentences. Negative examples arise naturally, from slight misinterpretation of text to minor aspects of the sentence that are not attested in the evidence. Our annotations are over sub-sentence units of the hypothesis, decomposed automatically by GPT-3, each of which is labeled with a subset of evidence sentences from the source document. We show that real claims in our dataset involve challenging verification problems, and we benchmark existing approaches on this dataset. In addition, we show that reducing the complexity of claims by decomposing them by GPT-3 can improve entailment models' performance on various domains."
</p>
</blockquote>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:S.m._della_piet%C3%A0,_int.,_altare_maggiore_di_G.B._Cennini_e_Pier_Maria_Ciottoli_su_dis._del_Mechini_(1623-25)_01.JPG"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/S.m._della_piet%C3%A0%2C_int.%2C_altare_maggiore_di_G.B._Cennini_e_Pier_Maria_Ciottoli_su_dis._del_Mechini_%281623-25%29_01.JPG/220px-S.m._della_piet%C3%A0%2C_int.%2C_altare_maggiore_di_G.B._Cennini_e_Pier_Maria_Ciottoli_su_dis._del_Mechini_%281623-25%29_01.JPG" decoding="async" width="220" height="330" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/S.m._della_piet%C3%A0%2C_int.%2C_altare_maggiore_di_G.B._Cennini_e_Pier_Maria_Ciottoli_su_dis._del_Mechini_%281623-25%29_01.JPG/330px-S.m._della_piet%C3%A0%2C_int.%2C_altare_maggiore_di_G.B._Cennini_e_Pier_Maria_Ciottoli_su_dis._del_Mechini_%281623-25%29_01.JPG 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/S.m._della_piet%C3%A0%2C_int.%2C_altare_maggiore_di_G.B._Cennini_e_Pier_Maria_Ciottoli_su_dis._del_Mechini_%281623-25%29_01.JPG/440px-S.m._della_piet%C3%A0%2C_int.%2C_altare_maggiore_di_G.B._Cennini_e_Pier_Maria_Ciottoli_su_dis._del_Mechini_%281623-25%29_01.JPG 2x" data-file-width="2304" data-file-height="3456"></a><figcaption>GPT-3 automatically decomposed a statement about this altar (from the article <a href="https://en.wikipedia.org/wiki/Santa_Maria_della_Piet%C3%A0,_Prato" title="Santa Maria della Piet√†, Prato">Santa Maria della Piet√†, Prato</a>)</figcaption></figure>
<p>The preprint gives the following examples of such an automatic decomposition performed by GPT-3 (using the prompt "Segment the following sentence into individual facts:" accompanied by several instructional examples):
</p>
<blockquote>
<p>Original Sentence:
</p>
<dl><dd>The main altar houses a 17th-century fresco of figures interacting with the framed 13th century icon of the Madonna (1638), painted by Mario Balassi.</dd></dl>
<p>[Sub-claims predicted by GPT-3:]
</p>
<ul><li>The main altar houses a 17th-century fresco.</li>
<li>The fresco is of figures interacting with the framed 13th-century icon of the Madonna.</li>
<li>The icon of the Madonna was painted by Mario Balassi in 1638.</li></ul>
</blockquote>
<h4><span id=".22SWiPE:_A_Dataset_for_Document-Level_Simplification_of_Wikipedia_Pages.22"></span><span id="&quot;SWiPE:_A_Dataset_for_Document-Level_Simplification_of_Wikipedia_Pages&quot;" data-mw-thread-id="h-&quot;SWiPE:_A_Dataset_for_Document-Level_Simplification_of_Wikipedia_Pages&quot;-Other_recent_publications"><span data-mw-comment-start="" id="h-&quot;SWiPE:_A_Dataset_for_Document-Level_Simplification_of_Wikipedia_Pages&quot;-Other_recent_publications"></span>"SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages"<span data-mw-comment-end="h-&quot;SWiPE:_A_Dataset_for_Document-Level_Simplification_of_Wikipedia_Pages&quot;-Other_recent_publications"></span></span></h4>
<p>From the abstract:<sup id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup>
</p>
<blockquote> 
<p>"[...] we introduce the SWiPE dataset, which reconstructs the document-level editing process from English Wikipedia (EW) articles to paired <a href="https://en.wikipedia.org/wiki/Simple_Wikipedia" title="Simple Wikipedia">Simple Wikipedia</a> (SEW) articles. In contrast to prior work, SWiPE leverages the entire revision history when pairing pages in order to better identify simplification edits. We work with Wikipedia editors to annotate 5,000 EW-SEW document pairs, labeling more than 40,000 edits with proposed 19 categories. To scale our efforts, we propose several models to automatically label edits, achieving an F-1 score of up to 70.6, indicating that this is a tractable but challenging NLU [<a href="https://en.wikipedia.org/wiki/Natural-language_understanding" title="Natural-language understanding">Natural-language understanding</a>] task."
</p>
</blockquote>
<h4><span id=".22Descartes:_Generating_Short_Descriptions_of_Wikipedia_Articles.22"></span><span id="&quot;Descartes:_Generating_Short_Descriptions_of_Wikipedia_Articles&quot;" data-mw-thread-id="h-&quot;Descartes:_Generating_Short_Descriptions_of_Wikipedia_Articles&quot;-Other_recent_publications"><span data-mw-comment-start="" id="h-&quot;Descartes:_Generating_Short_Descriptions_of_Wikipedia_Articles&quot;-Other_recent_publications"></span>"Descartes: Generating Short Descriptions of Wikipedia Articles"<span data-mw-comment-end="h-&quot;Descartes:_Generating_Short_Descriptions_of_Wikipedia_Articles&quot;-Other_recent_publications"></span></span></h4>
<p>From the abstract:<sup id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup>
</p>
<blockquote>
<p>"we introduce the novel task of automatically generating <a href="https://en.wikipedia.org/wiki/Wikipedia:Short_description" title="Wikipedia:Short description">short descriptions for Wikipedia articles and</a> propose Descartes, a multilingual model for tackling it. Descartes integrates three sources of information to generate an article description in a target language: the text of the article in all its language versions, the already-existing descriptions (if any) of the article in other languages, and semantic type information obtained from a knowledge graph. We evaluate a Descartes model trained for handling 25 languages simultaneously, showing that it beats baselines (including a strong translation-based baseline) and performs on par with monolingual models tailored for specific languages. A human evaluation on three languages further shows that the quality of Descartes‚Äôs descriptions is largely indistinguishable from that of human-written descriptions; e.g., 91.3% of our English descriptions (vs. 92.1% of human-written descriptions) pass the bar for inclusion in Wikipedia, suggesting that Descartes is ready for production, with the potential to support human editors in filling a major gap in today‚Äôs Wikipedia across languages."
</p>
</blockquote>
<h4><span id=".22WikiDes:_A_Wikipedia-based_dataset_for_generating_short_descriptions_from_paragraphs.22"></span><span id="&quot;WikiDes:_A_Wikipedia-based_dataset_for_generating_short_descriptions_from_paragraphs&quot;" data-mw-thread-id="h-&quot;WikiDes:_A_Wikipedia-based_dataset_for_generating_short_descriptions_from_parag-Other_recent_publications"><span data-mw-comment-start="" id="h-&quot;WikiDes:_A_Wikipedia-based_dataset_for_generating_short_descriptions_from_parag-Other_recent_publications"></span>"WikiDes: A Wikipedia-based dataset for generating short descriptions from paragraphs"<span data-mw-comment-end="h-&quot;WikiDes:_A_Wikipedia-based_dataset_for_generating_short_descriptions_from_parag-Other_recent_publications"></span></span></h4>
<p>From the abstract:<sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup>
</p>
<blockquote><p>"In this paper, we introduce WikiDes, a novel dataset to generate <a href="https://en.wikipedia.org/wiki/Wikipedia:Short_description" title="Wikipedia:Short description">short descriptions of Wikipedia articles</a> for the problem of text summarization. The dataset consists of over 80k English samples on 6987 topics. [...] [The autogenerated descriptions are preferred in] human evaluation in over 45.33% [cases] against the gold descriptions. [...] The automatic generation of new descriptions reduces the human efforts in creating them and enriches Wikidata-based knowledge graphs. Our paper shows a practical impact on Wikipedia and Wikidata since there are thousands of missing descriptions."
</p></blockquote>
<p>From the introduction:
</p>
<blockquote>
<p>"With the rapid development of Wikipedia and Wikidata in recent years, the editor community has been overloaded with contributing new information adapting to user requirements, and patrolling the massive content daily. Hence, the application of NLP and deep learning is key to solving these problems effectively. In this paper, we propose a summarization approach trained on WikiDes that generates missing descriptions in thousands of Wikidata items, which reduces human efforts and boosts content development faster. The summarizer is responsible for creating descriptions while humans toward a role in patrolling the text quality instead of
starting everything from the beginning. Our work can be scalable to multilingualism, which takes a more positive impact on user experiences in searching for articles by short descriptions in many Wikimedia projects."
</p>
</blockquote>
<p>See also the "Descartes" paper (above).
</p>
<h4><span id=".22Can_Language_Models_Identify_Wikipedia_Articles_with_Readability_and_Style_Issues.3F.22"></span><span id="&quot;Can_Language_Models_Identify_Wikipedia_Articles_with_Readability_and_Style_Issues?&quot;" data-mw-thread-id="h-&quot;Can_Language_Models_Identify_Wikipedia_Articles_with_Readability_and_Style_Issu-Other_recent_publications"><span data-mw-comment-start="" id="h-&quot;Can_Language_Models_Identify_Wikipedia_Articles_with_Readability_and_Style_Issu-Other_recent_publications"></span>"Can Language Models Identify Wikipedia Articles with Readability and Style Issues?"<span data-mw-comment-end="h-&quot;Can_Language_Models_Identify_Wikipedia_Articles_with_Readability_and_Style_Issu-Other_recent_publications"></span></span></h4>
<p>From the abstract:<sup id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup>
</p>
<blockquote>
<p>"we investigate using <a href="https://en.wikipedia.org/wiki/GPT-2" title="GPT-2">GPT-2</a>, a neural language model, to identify poorly written text in Wikipedia by ranking documents by their <a href="https://en.wikipedia.org/wiki/Large_language_model#Perplexity" title="Large language model">perplexity</a>. We evaluated the properties of this ranking using human assessments of text quality, including readability, narrativity and language use. We demonstrate that GPT-2 perplexity scores correlate moderately to strongly with narrativity, but only weakly with reading comprehension scores. Importantly, the model reflects even small improvements to text as would be seen in Wikipedia edits. We conclude by highlighting that Wikipedia's featured articles counter-intuitively contain text with the highest perplexity scores."
</p>
</blockquote>
<h4><span id=".22Wikibio:_a_Semantic_Resource_for_the_Intersectional_Analysis_of_Biographical_Events.22"></span><span id="&quot;Wikibio:_a_Semantic_Resource_for_the_Intersectional_Analysis_of_Biographical_Events&quot;" data-mw-thread-id="h-&quot;Wikibio:_a_Semantic_Resource_for_the_Intersectional_Analysis_of_Biographical_Ev-Other_recent_publications"><span data-mw-comment-start="" id="h-&quot;Wikibio:_a_Semantic_Resource_for_the_Intersectional_Analysis_of_Biographical_Ev-Other_recent_publications"></span>"Wikibio: a Semantic Resource for the Intersectional Analysis of Biographical Events"<span data-mw-comment-end="h-&quot;Wikibio:_a_Semantic_Resource_for_the_Intersectional_Analysis_of_Biographical_Ev-Other_recent_publications"></span></span></h4>
<p>From the abstract:<sup id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup> 
</p>
<blockquote> 
<p>"In this paper we [are] presenting a new corpus annotated for biographical event detection. The corpus, which includes 20 Wikipedia biographies, was compared with five existing corpora to train a model for the biographical event detection task. The model was able to detect all mentions of the target-entity in a biography with an F-score of 0.808 and the entity-related events with an F-score of 0.859. Finally, the model was used for performing an analysis of biases about women and non-Western people in Wikipedia biographies."
</p>
</blockquote>
<h4><span id=".22Detecting_Cross-Lingual_Information_Gaps_in_Wikipedia.22"></span><span id="&quot;Detecting_Cross-Lingual_Information_Gaps_in_Wikipedia&quot;" data-mw-thread-id="h-&quot;Detecting_Cross-Lingual_Information_Gaps_in_Wikipedia&quot;-Other_recent_publications"><span data-mw-comment-start="" id="h-&quot;Detecting_Cross-Lingual_Information_Gaps_in_Wikipedia&quot;-Other_recent_publications"></span>"Detecting Cross-Lingual Information Gaps in Wikipedia"<span data-mw-comment-end="h-&quot;Detecting_Cross-Lingual_Information_Gaps_in_Wikipedia&quot;-Other_recent_publications"></span></span></h4>
<p>From the abstract:<sup id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup>
</p>
<blockquote>
<p>"The proposed approach employs <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_Allocation" title="Latent Dirichlet Allocation">Latent Dirichlet Allocation</a> (LDA) to analyze linked entities in a cross-lingual knowledge graph in order to determine topic distributions for Wikipedia articles in 28 languages. The distance between paired articles across language editions is then calculated. The potential applications of the proposed algorithm to detecting sources of information disparity in Wikipedia are discussed [...]"
</p>
</blockquote>
<p>From the paper:
</p>
<blockquote>
<p>"In this PhD project, leveraging the Wikidata Knowledge base, we aim to provide empirical evidence as well as theoretical grounding to address the following questions:
</p>
<dl><dd>RQ1) How can we measure the information gap between different language editions of Wikipedia?</dd>
<dd>RQ2) What are the sources of the cross-lingual information gap in Wikipedia?</dd></dl>
<p>[...]<br>
The results revealed a correlation between stronger similarities [...] and languages spoken in countries with established historical or geographical connections, such as Russian/Ukrainian, Czech/Polish, and Spanish/Catalan."
</p>
</blockquote>
<h4><span id=".22Wikidata:_The_Making_Of.22"></span><span id="&quot;Wikidata:_The_Making_Of&quot;" data-mw-thread-id="h-&quot;Wikidata:_The_Making_Of&quot;-Other_recent_publications"><span data-mw-comment-start="" id="h-&quot;Wikidata:_The_Making_Of&quot;-Other_recent_publications"></span>"Wikidata: The Making Of"<span data-mw-comment-end="h-&quot;Wikidata:_The_Making_Of&quot;-Other_recent_publications"></span></span></h4>
<p>From the abstract:<sup id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup>
</p>
<blockquote><p>"In this paper, we try to recount [Wikidata's] remarkable journey, and we review what has been accomplished, what has been given up on, and what is yet left to do for the future."</p></blockquote>
<h4><span id=".22Mining_the_History_Sections_of_Wikipedia_Articles_on_Science_and_Technology.22"></span><span id="&quot;Mining_the_History_Sections_of_Wikipedia_Articles_on_Science_and_Technology&quot;" data-mw-thread-id="h-&quot;Mining_the_History_Sections_of_Wikipedia_Articles_on_Science_and_Technology&quot;-Other_recent_publications"><span data-mw-comment-start="" id="h-&quot;Mining_the_History_Sections_of_Wikipedia_Articles_on_Science_and_Technology&quot;-Other_recent_publications"></span>"Mining the History Sections of Wikipedia Articles on Science and Technology"<span data-mw-comment-end="h-&quot;Mining_the_History_Sections_of_Wikipedia_Articles_on_Science_and_Technology&quot;-Other_recent_publications"></span></span></h4>
<p>From the abstract:<sup id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup>
</p>
<blockquote> 
<p>"Priority conflicts and the attribution of contributions to important scientific breakthroughs to individuals and groups play an important role in science, its governance, and evaluation.[....] Our objective is to transform Wikipedia into an accessible, traceable primary source for analyzing such debates. In this paper, we introduce Webis-WikiSciTech-23, a new corpus consisting of science and technology Wikipedia articles, focusing on the identification of their history sections. [...] The identification of passages covering the historical development of innovations is achieved by combining heuristics for section heading analysis and classifiers trained on a ground truth of articles with designated history sections."
</p>
</blockquote>
<h3><span id="References" data-mw-thread-id="h-References-signpost-article-title"><span data-mw-comment-start="" id="h-References-signpost-article-title"></span>References<span data-mw-comment-end="h-References-signpost-article-title"></span></span></h3>
<div>
<ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite id="CITEREFYangShoaibWestColavizza2023">Yang, Puyu; Shoaib, Ahad; West, Robert; Colavizza, Giovanni (2023-05-23). "Wikipedia and open access". <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/2305.13945">2305.13945</a></span> [<a rel="nofollow" href="https://arxiv.org/archive/cs.DL">cs.DL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Wikipedia+and+open+access&amp;rft.date=2023-05-23&amp;rft_id=info%3Aarxiv%2F2305.13945&amp;rft.aulast=Yang&amp;rft.aufirst=Puyu&amp;rft.au=Shoaib%2C+Ahad&amp;rft.au=West%2C+Robert&amp;rft.au=Colavizza%2C+Giovanni&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span> <a rel="nofollow" href="https://github.com/alsowbdxa/Open_access_and_wikipedia">Code</a></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span><cite id="CITEREFKimGarciaArag√≥n2023">Kim, Taehee; Garcia, David; Arag√≥n, Pablo (2023-05-11). <a rel="nofollow" href="https://wikiworkshop.org/2023/papers/WikiWorkshop2023_paper_22.pdf">"Controversies over Historical Revisionism in Wikipedia"</a> <span>(PDF)</span>. Wiki Workshop (10th edition).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Controversies+over+Historical+Revisionism+in+Wikipedia&amp;rft.pub=Wiki+Workshop+%2810th+edition%29&amp;rft.date=2023-05-11&amp;rft.aulast=Kim&amp;rft.aufirst=Taehee&amp;rft.au=Garcia%2C+David&amp;rft.au=Arag%C3%B3n%2C+Pablo&amp;rft_id=https%3A%2F%2Fwikiworkshop.org%2F2023%2Fpapers%2FWikiWorkshop2023_paper_22.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span></span>
</li>
<li id="cite_note-3"><span><b><a href="#cite_ref-3">^</a></b></span> <span><cite id="CITEREFSemnaniYaoZhangLam2023">Semnani, Sina J.; Yao, Violet Z.; Zhang, Heidi C.; Lam, Monica S. (2023-05-23). "WikiChat: A Few-Shot LLM-Based Chatbot Grounded with Wikipedia". <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/2305.14292">2305.14292</a></span> [<a rel="nofollow" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=WikiChat%3A+A+Few-Shot+LLM-Based+Chatbot+Grounded+with+Wikipedia&amp;rft.date=2023-05-23&amp;rft_id=info%3Aarxiv%2F2305.14292&amp;rft.aulast=Semnani&amp;rft.aufirst=Sina+J.&amp;rft.au=Yao%2C+Violet+Z.&amp;rft.au=Zhang%2C+Heidi+C.&amp;rft.au=Lam%2C+Monica+S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span></span>
</li>
<li id="cite_note-4"><span><b><a href="#cite_ref-4">^</a></b></span> <span><cite id="CITEREFWellerMaroneWeirLawrie2023">Weller, Orion; Marone, Marc; Weir, Nathaniel; Lawrie, Dawn; Khashabi, Daniel; Van Durme, Benjamin (2023-05-22). "<span></span>"According to ..." Prompting Language Models Improves Quoting from Pre-Training Data". <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/2305.13252">2305.13252</a></span> [<a rel="nofollow" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=%22According+to+...%22+Prompting+Language+Models+Improves+Quoting+from+Pre-Training+Data&amp;rft.date=2023-05-22&amp;rft_id=info%3Aarxiv%2F2305.13252&amp;rft.aulast=Weller&amp;rft.aufirst=Orion&amp;rft.au=Marone%2C+Marc&amp;rft.au=Weir%2C+Nathaniel&amp;rft.au=Lawrie%2C+Dawn&amp;rft.au=Khashabi%2C+Daniel&amp;rft.au=Van+Durme%2C+Benjamin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><cite id="CITEREFMutherSmith2023">Muther, Ryan; Smith, David (2023-06-29). "Citations as Queries: Source Attribution Using Language Models as Rerankers". <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/2306.17322">2306.17322</a></span> [<a rel="nofollow" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Citations+as+Queries%3A+Source+Attribution+Using+Language+Models+as+Rerankers&amp;rft.date=2023-06-29&amp;rft_id=info%3Aarxiv%2F2306.17322&amp;rft.aulast=Muther&amp;rft.aufirst=Ryan&amp;rft.au=Smith%2C+David&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span></span>
</li>
<li id="cite_note-6"><span><b><a href="#cite_ref-6">^</a></b></span> <span><cite id="CITEREFKamoiGoyalRodriguezDurrett2023">Kamoi, Ryo; Goyal, Tanya; Rodriguez, Juan Diego; Durrett, Greg (2023-03-02), <i>WiCE: Real-World Entailment for Claims in Wikipedia</i>, <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/2303.01432">2303.01432</a></span></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=WiCE%3A+Real-World+Entailment+for+Claims+in+Wikipedia&amp;rft.date=2023-03-02&amp;rft_id=info%3Aarxiv%2F2303.01432&amp;rft.aulast=Kamoi&amp;rft.aufirst=Ryo&amp;rft.au=Goyal%2C+Tanya&amp;rft.au=Rodriguez%2C+Juan+Diego&amp;rft.au=Durrett%2C+Greg&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span> <a rel="nofollow" href="https://github.com/ryokamoi/wice">Code</a></span>
</li>
<li id="cite_note-7"><span><b><a href="#cite_ref-7">^</a></b></span> <span><cite id="CITEREFLabanVigKryscinskiJoty2023">Laban, Philippe; Vig, Jesse; Kryscinski, Wojciech; Joty, Shafiq; Xiong, Caiming; Wu, Chien-Sheng (2023-05-30). "SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages". <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/2305.19204">2305.19204</a></span> [<a rel="nofollow" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=SWiPE%3A+A+Dataset+for+Document-Level+Simplification+of+Wikipedia+Pages&amp;rft.date=2023-05-30&amp;rft_id=info%3Aarxiv%2F2305.19204&amp;rft.aulast=Laban&amp;rft.aufirst=Philippe&amp;rft.au=Vig%2C+Jesse&amp;rft.au=Kryscinski%2C+Wojciech&amp;rft.au=Joty%2C+Shafiq&amp;rft.au=Xiong%2C+Caiming&amp;rft.au=Wu%2C+Chien-Sheng&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span> <a href="https://en.wikipedia.org/wiki/Association_for_Computational_Linguistics" title="Association for Computational Linguistics">ACL 2023</a>, Long Paper. <a rel="nofollow" href="https://github.com/salesforce/simplification">Code</a>, Authors' tweets:
<a rel="nofollow" href="https://twitter.com/PhilippeLaban/status/1671176565122650113">[1]</a>
<a rel="nofollow" href="https://twitter.com/CaimingXiong/status/1669467732369375232">[2]</a></span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><cite id="CITEREFSakotaPeyrardWest2023">Sakota, Marija; Peyrard, Maxime; West, Robert (2023-04-30). <a rel="nofollow" href="https://dl.acm.org/doi/10.1145/3543507.3583220">"Descartes: Generating Short Descriptions of Wikipedia Articles"</a>. <i>Proceedings of the ACM Web Conference 2023</i>. New York, NY, USA: Association for Computing Machinery. pp.&nbsp;1446‚Äì1456. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1145%2F3543507.3583220">10.1145/3543507.3583220</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/9781450394161" title="Special:BookSources/9781450394161"><bdi>9781450394161</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Descartes%3A+Generating+Short+Descriptions+of+Wikipedia+Articles&amp;rft.btitle=Proceedings+of+the+ACM+Web+Conference+2023&amp;rft.place=New+York%2C+NY%2C+USA&amp;rft.pages=1446-1456&amp;rft.pub=Association+for+Computing+Machinery&amp;rft.date=2023-04-30&amp;rft_id=info%3Adoi%2F10.1145%2F3543507.3583220&amp;rft.isbn=9781450394161&amp;rft.aulast=Sakota&amp;rft.aufirst=Marija&amp;rft.au=Peyrard%2C+Maxime&amp;rft.au=West%2C+Robert&amp;rft_id=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.1145%2F3543507.3583220&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span> <span><span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/Paywall" title="closed access publication ‚Äì behind paywall"><img alt="closed access" src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Closed_Access_logo_transparent.svg/9px-Closed_Access_logo_transparent.svg.png" decoding="async" width="9" height="14" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Closed_Access_logo_transparent.svg/14px-Closed_Access_logo_transparent.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Closed_Access_logo_transparent.svg/18px-Closed_Access_logo_transparent.svg.png 2x" data-file-width="640" data-file-height="1000"></a></span></span>, preprint version: <cite id="CITEREFSakotaPeyrardWest2022">Sakota, Marija; Peyrard, Maxime; West, Robert (2022-11-02). <a rel="nofollow" href="http://arxiv.org/abs/2205.10012"><i>Descartes: Generating Short Descriptions of Wikipedia Articles</i></a>. arXiv.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Descartes%3A+Generating+Short+Descriptions+of+Wikipedia+Articles&amp;rft.pub=arXiv&amp;rft.date=2022-11-02&amp;rft.aulast=Sakota&amp;rft.aufirst=Marija&amp;rft.au=Peyrard%2C+Maxime&amp;rft.au=West%2C+Robert&amp;rft_id=http%3A%2F%2Farxiv.org%2Fabs%2F2205.10012&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><cite id="CITEREFTaRahmanMajumderHussain2023">Ta, Hoang Thang; Rahman, Abu Bakar Siddiqur; Majumder, Navonil; Hussain, Amir; Najjar, Lotfollah; Howard, Newton; Poria, Soujanya; Gelbukh, Alexander (2023-02-01). <a rel="nofollow" href="https://www.sciencedirect.com/science/article/pii/S1566253522001610">"WikiDes: A Wikipedia-based dataset for generating short descriptions from paragraphs"</a>. <i>Information Fusion</i>. <b>90</b>: 265‚Äì282. <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/2209.13101">2209.13101</a></span>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1016%2Fj.inffus.2022.09.022">10.1016/j.inffus.2022.09.022</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1566-2535">1566-2535</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:252544839">252544839</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information+Fusion&amp;rft.atitle=WikiDes%3A+A+Wikipedia-based+dataset+for+generating+short+descriptions+from+paragraphs&amp;rft.volume=90&amp;rft.pages=265-282&amp;rft.date=2023-02-01&amp;rft_id=info%3Aarxiv%2F2209.13101&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A252544839%23id-name%3DS2CID&amp;rft.issn=1566-2535&amp;rft_id=info%3Adoi%2F10.1016%2Fj.inffus.2022.09.022&amp;rft.aulast=Ta&amp;rft.aufirst=Hoang+Thang&amp;rft.au=Rahman%2C+Abu+Bakar+Siddiqur&amp;rft.au=Majumder%2C+Navonil&amp;rft.au=Hussain%2C+Amir&amp;rft.au=Najjar%2C+Lotfollah&amp;rft.au=Howard%2C+Newton&amp;rft.au=Poria%2C+Soujanya&amp;rft.au=Gelbukh%2C+Alexander&amp;rft_id=https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS1566253522001610&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span> <span><span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/Paywall" title="closed access publication ‚Äì behind paywall"><img alt="closed access" src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Closed_Access_logo_transparent.svg/9px-Closed_Access_logo_transparent.svg.png" decoding="async" width="9" height="14" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Closed_Access_logo_transparent.svg/14px-Closed_Access_logo_transparent.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Closed_Access_logo_transparent.svg/18px-Closed_Access_logo_transparent.svg.png 2x" data-file-width="640" data-file-height="1000"></a></span></span> <a rel="nofollow" href="https://github.com/declare-lab/WikiDes">Dataset</a></span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span><cite id="CITEREFLiuMedlarGlowacka2021">Liu, Yang; Medlar, Alan; Glowacka, Dorota (2021-07-11). <a rel="nofollow" href="https://doi.org/10.1145/3471158.3472234">"Can Language Models Identify Wikipedia Articles with Readability and Style Issues?"</a>. <i>Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval</i>. ICTIR '21. New York, NY, USA: Association for Computing Machinery. pp.&nbsp;113‚Äì117. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1145%2F3471158.3472234">10.1145/3471158.3472234</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/9781450386111" title="Special:BookSources/9781450386111"><bdi>9781450386111</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Can+Language+Models+Identify+Wikipedia+Articles+with+Readability+and+Style+Issues%3F&amp;rft.btitle=Proceedings+of+the+2021+ACM+SIGIR+International+Conference+on+Theory+of+Information+Retrieval&amp;rft.place=New+York%2C+NY%2C+USA&amp;rft.series=ICTIR+%2721&amp;rft.pages=113-117&amp;rft.pub=Association+for+Computing+Machinery&amp;rft.date=2021-07-11&amp;rft_id=info%3Adoi%2F10.1145%2F3471158.3472234&amp;rft.isbn=9781450386111&amp;rft.aulast=Liu&amp;rft.aufirst=Yang&amp;rft.au=Medlar%2C+Alan&amp;rft.au=Glowacka%2C+Dorota&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1145%2F3471158.3472234&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span> <span><span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/Paywall" title="closed access publication ‚Äì behind paywall"><img alt="closed access" src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Closed_Access_logo_transparent.svg/9px-Closed_Access_logo_transparent.svg.png" decoding="async" width="9" height="14" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Closed_Access_logo_transparent.svg/14px-Closed_Access_logo_transparent.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Closed_Access_logo_transparent.svg/18px-Closed_Access_logo_transparent.svg.png 2x" data-file-width="640" data-file-height="1000"></a></span></span>. Accepted author manuscript: <cite id="CITEREFLiuMedlarGlowacka2021">Liu, Yang; Medlar, Alan; Glowacka, Dorota (August 2021). "Can Language Models Identify Wikipedia Articles with Readability and Style Issues?: International Conference on the Theory of Information Retrieval". <i>ICTIR '21: Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval</i>: 113‚Äì117. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1145%2F3471158.3472234">10.1145/3471158.3472234</a>. <a href="https://en.wikipedia.org/wiki/Hdl_(identifier)" title="Hdl (identifier)">hdl</a>:<a rel="nofollow" href="https://hdl.handle.net/10138%2F352578">10138/352578</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:237367001">237367001</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ICTIR+%2721%3A+Proceedings+of+the+2021+ACM+SIGIR+International+Conference+on+Theory+of+Information+Retrieval&amp;rft.atitle=Can+Language+Models+Identify+Wikipedia+Articles+with+Readability+and+Style+Issues%3F%3A+International+Conference+on+the+Theory+of+Information+Retrieval&amp;rft.pages=113-117&amp;rft.date=2021-08&amp;rft_id=info%3Ahdl%2F10138%2F352578&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A237367001%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1145%2F3471158.3472234&amp;rft.aulast=Liu&amp;rft.aufirst=Yang&amp;rft.au=Medlar%2C+Alan&amp;rft.au=Glowacka%2C+Dorota&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span></span>
</li>
<li id="cite_note-11"><span><b><a href="#cite_ref-11">^</a></b></span> <span><cite id="CITEREFStranisciDamianoMensaPatti2023">Stranisci, Marco Antonio; Damiano, Rossana; Mensa, Enrico; Patti, Viviana; Radicioni, Daniele; Caselli, Tommaso (2023-06-15). "Wikibio: a Semantic Resource for the Intersectional Analysis of Biographical Events". <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/2306.09505">2306.09505</a></span> [<a rel="nofollow" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Wikibio%3A+a+Semantic+Resource+for+the+Intersectional+Analysis+of+Biographical+Events&amp;rft.date=2023-06-15&amp;rft_id=info%3Aarxiv%2F2306.09505&amp;rft.aulast=Stranisci&amp;rft.aufirst=Marco+Antonio&amp;rft.au=Damiano%2C+Rossana&amp;rft.au=Mensa%2C+Enrico&amp;rft.au=Patti%2C+Viviana&amp;rft.au=Radicioni%2C+Daniele&amp;rft.au=Caselli%2C+Tommaso&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span> <a rel="nofollow" href="https://github.com/marcostranisci/WikiBio">Code and data</a></span>
</li>
<li id="cite_note-12"><span><b><a href="#cite_ref-12">^</a></b></span> <span><cite id="CITEREFAshrafimoghari2023">Ashrafimoghari, Vahid (2023-04-30). <a rel="nofollow" href="https://dl.acm.org/doi/10.1145/3543873.3587539">"Detecting Cross-Lingual Information Gaps in Wikipedia"</a>. <i>Companion Proceedings of the ACM Web Conference 2023</i>. New York, NY, USA: Association for Computing Machinery. pp.&nbsp;581‚Äì585. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1145%2F3543873.3587539">10.1145/3543873.3587539</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/9781450394192" title="Special:BookSources/9781450394192"><bdi>9781450394192</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Detecting+Cross-Lingual+Information+Gaps+in+Wikipedia&amp;rft.btitle=Companion+Proceedings+of+the+ACM+Web+Conference+2023&amp;rft.place=New+York%2C+NY%2C+USA&amp;rft.pages=581-585&amp;rft.pub=Association+for+Computing+Machinery&amp;rft.date=2023-04-30&amp;rft_id=info%3Adoi%2F10.1145%2F3543873.3587539&amp;rft.isbn=9781450394192&amp;rft.aulast=Ashrafimoghari&amp;rft.aufirst=Vahid&amp;rft_id=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.1145%2F3543873.3587539&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span></span>
</li>
<li id="cite_note-13"><span><b><a href="#cite_ref-13">^</a></b></span> <span><cite id="CITEREFVrandeƒçiƒáPintscherKr√∂tzsch2023">Vrandeƒçiƒá, Denny; Pintscher, Lydia; Kr√∂tzsch, Markus (2023-04-30). <a rel="nofollow" href="https://dl.acm.org/doi/10.1145/3543873.3585579">"Wikidata: The Making Of"</a>. <i>Companion Proceedings of the ACM Web Conference 2023</i>. New York, NY, USA: Association for Computing Machinery. pp.&nbsp;615‚Äì624. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1145%2F3543873.3585579">10.1145/3543873.3585579</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/9781450394192" title="Special:BookSources/9781450394192"><bdi>9781450394192</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Wikidata%3A+The+Making+Of&amp;rft.btitle=Companion+Proceedings+of+the+ACM+Web+Conference+2023&amp;rft.place=New+York%2C+NY%2C+USA&amp;rft.pages=615-624&amp;rft.pub=Association+for+Computing+Machinery&amp;rft.date=2023-04-30&amp;rft_id=info%3Adoi%2F10.1145%2F3543873.3585579&amp;rft.isbn=9781450394192&amp;rft.aulast=Vrande%C4%8Di%C4%87&amp;rft.aufirst=Denny&amp;rft.au=Pintscher%2C+Lydia&amp;rft.au=Kr%C3%B6tzsch%2C+Markus&amp;rft_id=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.1145%2F3543873.3585579&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span> <a rel="nofollow" href="https://www.youtube.com/watch?v=P3-nklyrDx4">Presentation video recording</a></span>
</li>
<li id="cite_note-14"><span><b><a href="#cite_ref-14">^</a></b></span> <span><cite id="CITEREFKircheisSchmidtSimonsPotthast2023">Kircheis, Wolfgang; Schmidt, Marion; Simons, Arno; Potthast, Martin; Stein, Benno (2023-06-26). <a rel="nofollow" href="https://www.researchgate.net/publication/370902015"><i>Mining the History Sections of Wikipedia Articles on Science and Technology</i></a>. 23rd ACM/IEEE Joint Conference on Digital Libraries (JCDL 2023). Santa Fe, New Mexico, USA.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Mining+the+History+Sections+of+Wikipedia+Articles+on+Science+and+Technology&amp;rft.place=Santa+Fe%2C+New+Mexico%2C+USA&amp;rft.date=2023-06-26&amp;rft.aulast=Kircheis&amp;rft.aufirst=Wolfgang&amp;rft.au=Schmidt%2C+Marion&amp;rft.au=Simons%2C+Arno&amp;rft.au=Potthast%2C+Martin&amp;rft.au=Stein%2C+Benno&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F370902015&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span> , also (with dataset) as: <cite id="CITEREFKircheisSchmidtSimonsStein2023">Kircheis, Wolfgang; Schmidt, Marion; Simons, Arno; Stein, Benno; Potthast, Martin (2023-06-16). <a rel="nofollow" href="https://zenodo.org/record/7845809">"Webis Wikipedia Innovation History 2023"</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.5281%2FZENODO.7845809">10.5281/ZENODO.7845809</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Webis+Wikipedia+Innovation+History+2023&amp;rft.date=2023-06-16&amp;rft_id=info%3Adoi%2F10.5281%2FZENODO.7845809&amp;rft.aulast=Kircheis&amp;rft.aufirst=Wolfgang&amp;rft.au=Schmidt%2C+Marion&amp;rft.au=Simons%2C+Arno&amp;rft.au=Stein%2C+Benno&amp;rft.au=Potthast%2C+Martin&amp;rft_id=https%3A%2F%2Fzenodo.org%2Frecord%2F7845809&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3AWikipedia+Signpost%2F2023-07-17%2FRecent+research"></span>   In 23rd ACM/IEEE Joint Conference on Digital Libraries (JCDL 2023), June 2023. <a rel="nofollow" href="https://github.com/webis-de/JCDL-23">Code</a>, <a rel="nofollow" href="https://wiki-sci-tech-corpus.web.webis.de/">corpus viewer</a></span>
</li>
</ol></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Typo leaks millions of US military emails to Mali web operator (104 pts)]]></title>
            <link>https://www.ft.com/content/ab62af67-ed2a-42d0-87eb-c762ac163cf0</link>
            <guid>36756201</guid>
            <pubDate>Mon, 17 Jul 2023 10:20:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/ab62af67-ed2a-42d0-87eb-c762ac163cf0">https://www.ft.com/content/ab62af67-ed2a-42d0-87eb-c762ac163cf0</a>, See on <a href="https://news.ycombinator.com/item?id=36756201">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-content" data-trackable="trial-barrier-grid" data-barrier="trial" data-barrier-is-sandbox="false">
			<div data-o-component="o-subs-card" data-offer-id="41218b9e-c8ae-c934-43ad-71b13fcb4465" data-offer-prominence="primary" data-offer-title="Trial" data-tracking-context="{&quot;title&quot;:&quot;Trial&quot;,&quot;brief&quot;:&quot;Try full digital access and see why over 1 million readers subscribe to the FT&quot;,&quot;offerId&quot;:&quot;41218b9e-c8ae-c934-43ad-71b13fcb4465&quot;,&quot;price&quot;:&quot;$1 for 4 weeks&quot;,&quot;prominence&quot;:&quot;primary&quot;,&quot;skuIds&quot;:[],&quot;description&quot;:&quot;For 4 weeks receive unlimited Premium digital access to the FT's trusted, award-winning business news&quot;}">
					<h3>Try unlimited access</h3>

					<p>Try full digital access and see why over 1 million readers subscribe to the FT</p><p>Only
						CHF&nbsp;1 for 4 weeks
				</p>

					

				</div>
			<p>
				<h4>Explore our subscriptions</h4>
			</p>
			<div>
					<h5>Individual</h5>
					<p>Find the plan that suits you best.</p>
					
				</div>
			<div>
					<h5>Professional</h5>
					<p>Premium access for businesses and educational institutions.</p>
					
				</div>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Temporary ban on behavioural advertising on Facebook and Instagram (148 pts)]]></title>
            <link>https://www.datatilsynet.no/en/news/aktuelle-nyheter-2023/temporary-ban-of-behavioural-advertising-on-facebook-and-instagram/</link>
            <guid>36756101</guid>
            <pubDate>Mon, 17 Jul 2023 10:04:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.datatilsynet.no/en/news/aktuelle-nyheter-2023/temporary-ban-of-behavioural-advertising-on-facebook-and-instagram/">https://www.datatilsynet.no/en/news/aktuelle-nyheter-2023/temporary-ban-of-behavioural-advertising-on-facebook-and-instagram/</a>, See on <a href="https://news.ycombinator.com/item?id=36756101">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Meta tracks in detail the activity of users of its Facebook and Instagram platforms. Users are profiled based on where they are, what type of content they show interest in and what they publish, amongst others. These personal profiles are used for marketing purposes ‚Äì so called behavioural advertising. The Norwegian Data Protection Authority considers that the practice of Meta is illegal and is therefore imposing a temporary ban of behavioural advertising on Facebook and Instagram.&nbsp;</p>
<h2>Ban</h2>
<p>In&nbsp;December last year, the Irish Data Protection Commission issued a decision on behalf of all data protection authorities across the EEA which established that Meta has conducted illegal behavioural advertising. Since then, Meta has made certain changes, but <a href="https://curia.europa.eu/jcms/upload/docs/application/pdf/2023-07/cp230113en.pdf" target="_blank" rel="noopener">a fresh decision from the Court of Justice of the European Union (curia.europa.eu)</a> has stated that Meta‚Äôs behavioural advertising still does not comply with the law. Therefore, the Norwegian Data Protection Authority is now taking action by imposing a temporary ban.</p>
<p>The&nbsp;ban will apply from 4 August and last for three months, or until Meta can show that it complies with the law. Should Meta not comply with the decision, the company risks a coercive fine of up to one million NOK per day. The Norwegian Data Protection Authority‚Äôs decision only applies to users in Norway.&nbsp;</p>
<h2>The platforms can still operate in Norway</h2>
<p>Many people in Norway use and enjoy social media ‚Äì <a href="https://www.ipsos.com/sites/default/files/ct/publication/documents/2023-04/Ipsos%20SoMe-tracker%20Q1%202023.pdf" target="_blank" rel="noopener">figures from Ipsos (ipsos.com)</a> show that 82% of the adult Norwegian population have Facebook accounts and 65% have Instagram accounts.&nbsp;</p>
<p>- The Norwegian Data Protection Authority‚Äôs decision does not ban Facebook or Instagram in Norway. The purpose is rather to ensure that people in Norway can use these services in a secure way and that their rights are safeguarded, says Head of International in the Norwegian Data Protection Authority, Tobias Judin.&nbsp;</p>
<p>The Norwegian Data Protection Authority does not ban personalised advertising on Facebook or Instagram as such. The decision does not for example stop Meta from targeting advertising based on information a user put in their bio, such as place of residence, gender and age, or based on interests a user has provided themselves. Nor does the decision stop Meta from showing behavioural advertising to users who have given valid consent to it.&nbsp;</p>
<p>- All business models must respect privacy as a human right. Users must have sufficient control over their own data, and any tracking must be limited, Judin says.&nbsp;</p>
<h2>Behavioural advertising one of the largest risks to privacy</h2>
<p>Meta, the company behind Facebook and Instagram, holds vast amounts of data on Norwegians, including sensitive data. Many Norwegians spend a lot of time on these platforms, and therefore tracking and profiling can be used to paint a detailed picture of these people‚Äôs private life, personality and interests. Many people interact with content such as that related to health, politics and sexual orientation, and there is also a danger that this is indirectly used to target marketing to them.&nbsp;</p>
<p>- Invasive commercial surveillance for marketing purposes is one of the biggest risks to data protection on the internet today, Judin says.&nbsp;</p>
<p>When Meta decides what adverts someone is shown, they also decide what not to show someone. This affects freedom of expression and freedom of information in society. There is a risk that behavioural advertising strengthens existing stereotypes or could lead to unfair discrimination of various groups. Behavioural targeting of political adverts in election campaigns is particularly problematic from a democratic perspective.</p>
<p>As tracking is hidden from view, most people find it difficult to understand. There are also are many vulnerable people who use Facebook and Instagram that need extra protection such as children, the elderly and people with cognitive disabilities.&nbsp;&nbsp;</p>
<h2>European co-operation and the way forward</h2>
<p>As Meta has its European headquarters in Dublin, it is normally the Irish Data Protection Commission that supervises the company in the EEA. The Norwegian Data Protection Authority can nevertheless intervene directly against Meta when there is an urgent need to act, and in such cases we can issue a decision which is valid for a period of three months. We consider that the criteria for acting urgently in this case are fulfilled, in particular because Meta has recently received both a decision and a judgment against them to which they have not aligned themselves with. If we don‚Äôt intervene now, the data protection rights of the majority of Norwegians would be violated indefinitely.</p>
<p>Moving forward, we may take the matter to the European Data Protection Board (EDPB), of which we are a member, after the summer. The EDPB will decide whether the decision may be extended beyond its initial three month validity period.&nbsp;</p>
<p>Meta has expressed its views in relation to the case, and the company disagrees with our assessments. Meta can decide to challenge the Norwegian Data Protection Authority‚Äôs decision in the Oslo District Court.</p>
<h2>Download</h2>
<p><a href="https://www.datatilsynet.no/contentassets/36ad4a92100943439df9a8a3a7015c19/urgent-and-provisional-measures--meta_redacted.pdf" target="_blank" rel="noopener">Urgent and Provisional Measures - Meta (pdf).&nbsp;</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LazyVim (486 pts)]]></title>
            <link>https://www.lazyvim.org/</link>
            <guid>36753225</guid>
            <pubDate>Mon, 17 Jul 2023 01:52:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lazyvim.org/">https://www.lazyvim.org/</a>, See on <a href="https://news.ycombinator.com/item?id=36753225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><svg viewBox="0 0 570 125" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M.86 19.2V.72h8.68V19.2H.86Z"></path><path d="M8.863 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M21.905 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm53.778 0V.72h8.68V19.2h-8.68Z"></path><path d="M80.885 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M88.888 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M96.89 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M104.893 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M117.935 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm13.765 0V.72h8.68V19.2h-8.68Z"></path><path d="M136.903 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M144.906 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M152.908 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M160.911 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M168.913 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M176.916 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M189.958 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm5.763 0V.72h8.68V19.2h-8.68Z"></path><path d="M200.923 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M213.966 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm29.77 0V.72h8.68V19.2h-8.68Z"></path><path d="M248.938 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M261.981 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm5.763 0V.72h8.68V19.2h-8.68Z"></path><path d="M272.946 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M285.989 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm29.77 0V.72h8.68V19.2h-8.68Z"></path><path d="M320.961 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M334.004 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm5.762 0V.72h8.68V19.2h-8.68Z"></path><path d="M344.969 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M358.011 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm5.763 0V.72h8.68V19.2h-8.68Z"></path><path d="M368.976 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M376.979 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M390.021 19.2V9.12h-5.179v-1.4h6.579V19.2h-1.4Zm-2.799 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm29.77 0V.72h8.68V19.2h-8.68Z"></path><path d="M424.994 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M432.997 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M446.039 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm87.118-4.2v-1.26l4.606-7.812h-4.536V4.78h5.81v1.26l-4.606 7.812h4.746V15h-6.02ZM.86 36.2V17.72h8.68V36.2H.86Z"></path><path d="M8.863 36.2V17.72h8.68V36.2h-8.68Zm13.042 0V17.72h1.4V36.2h-1.4Zm-2.8 0V17.72h1.4V36.2h-1.4Zm45.775 0V17.72h8.68V36.2h-8.68Z"></path><path d="M72.883 36.2V17.72h8.68V36.2h-8.68Zm10.242 0V24.72h6.58v1.4h-5.18V36.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M88.748 26.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M96.75 26.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M104.893 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M112.895 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M125.938 36.2V26.12h-5.18v-1.4h6.58V36.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm8.003-7.28v-11.2h1.399v9.8h5.18v1.4h-6.579Zm2.799-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M136.763 26.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M144.766 26.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M152.908 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M160.911 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M168.913 36.2V17.72h8.68V36.2h-8.68Zm10.243 0V24.72h6.58v1.4h-5.18V36.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M184.778 28.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm10.383 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M200.923 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M208.926 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M221.968 36.2V26.12h-5.18v-1.4h6.58V36.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm13.765 0V17.72h8.68V36.2h-8.68Z"></path><path d="M240.936 36.2V17.72h8.68V36.2h-8.68Zm10.242 0V24.72h6.58v1.4h-5.18V36.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M256.801 28.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm8.143 10.08V17.72h8.68V36.2h-8.68Z"></path><path d="M272.946 36.2V17.72h8.68V36.2h-8.68Zm13.043 0V17.72h1.4V36.2h-1.4Zm-2.8 0V17.72h1.4V36.2h-1.4Zm29.77 0V17.72h8.68V36.2h-8.68Z"></path><path d="M320.961 36.2V17.72h8.68V36.2h-8.68Zm13.043 0V17.72h1.4V36.2h-1.4Zm-2.8 0V17.72h1.4V36.2h-1.4Zm5.762 0V17.72h8.68V36.2h-8.68Z"></path><path d="M344.969 36.2V17.72h8.68V36.2h-8.68Zm13.042 0V17.72h1.4V36.2h-1.4Zm-2.8 0V17.72h1.4V36.2h-1.4Zm5.763 0V17.72h8.68V36.2h-8.68Z"></path><path d="M368.976 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M376.979 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M384.981 36.2V17.72h8.681V36.2h-8.681Z"></path><path d="M398.024 36.2V26.12h-5.18v-1.4h6.58V36.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm13.765 0V17.72h8.68V36.2h-8.68Z"></path><path d="M416.992 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M424.994 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M432.997 36.2V17.72h8.68V36.2h-8.68Zm13.042 0V17.72h1.4V36.2h-1.4Zm-2.8 0V17.72h1.4V36.2h-1.4Zm55.108-4.2v-1.26l4.606-7.812h-4.536V21.78h5.81v1.26l-4.606 7.812h4.746V32h-6.02ZM.86 53.2V34.72h8.68V53.2H.86Z"></path><path d="M8.863 53.2V34.72h8.68V53.2h-8.68Zm13.042 0V34.72h1.4V53.2h-1.4Zm-2.8 0V34.72h1.4V53.2h-1.4Zm45.775 0V34.72h8.68V53.2h-8.68Z"></path><path d="M72.883 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M80.885 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M88.888 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M96.89 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M104.893 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M112.895 53.2V34.72h8.68V53.2h-8.68Zm13.043 0V34.72h1.4V53.2h-1.4Zm-2.8 0V34.72h1.4V53.2h-1.4Zm21.768 0V34.72h8.68V53.2h-8.68Z"></path><path d="M152.908 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M160.911 53.2V34.72h8.68V53.2h-8.68Zm10.242 0V41.72h6.58v1.4h-5.18V53.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M176.776 45.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm26.387 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M208.926 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M216.928 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M224.931 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M232.933 53.2V34.72h8.68V53.2h-8.68Zm10.243 0V41.72h6.58v1.4h-5.18V53.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M248.798 45.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm16.146 10.08V34.72h8.68V53.2h-8.68Z"></path><path d="M272.946 53.2V34.72h8.68V53.2h-8.68Zm13.043 0V34.72h1.4V53.2h-1.4Zm-2.8 0V34.72h1.4V53.2h-1.4Zm29.77 0V34.72h8.68V53.2h-8.68Z"></path><path d="M320.961 53.2V34.72h8.68V53.2h-8.68Zm13.043 0V34.72h1.4V53.2h-1.4Zm-2.8 0V34.72h1.4V53.2h-1.4Zm5.762 0V34.72h8.68V53.2h-8.68Z"></path><path d="M344.969 53.2V34.72h8.68V53.2h-8.68Zm13.042 0V34.72h1.4V53.2h-1.4Zm-2.8 0V34.72h1.4V53.2h-1.4Zm5.763 0V34.72h8.68V53.2h-8.68Z"></path><path d="M368.976 53.2V34.72h8.68V53.2h-8.68Zm10.243 0V41.72h6.58v1.4h-5.18V53.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M384.981 53.2V34.72h8.681V53.2h-8.681Z"></path><path d="M392.984 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M400.987 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M408.989 53.2V34.72h8.68V53.2h-8.68Zm10.243 0V41.72h6.58v1.4h-5.18V53.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M424.994 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M432.997 53.2V34.72h8.68V53.2h-8.68Zm13.042 0V34.72h1.4V53.2h-1.4Zm-2.8 0V34.72h1.4V53.2h-1.4Zm31.17-4.2v-1.26l4.312-5.292h-4.2V41.3h5.614v1.26l-4.396 5.292h4.55V49h-5.88ZM.86 70.2V51.72h8.68V70.2H.86Z"></path><path d="M8.863 70.2V51.72h8.68V70.2h-8.68Zm13.042 0V51.72h1.4V70.2h-1.4Zm-2.8 0V51.72h1.4V70.2h-1.4Zm45.775 0V51.72h8.68V70.2h-8.68Z"></path><path d="M72.883 70.2V51.72h8.68V70.2h-8.68Zm10.242 0V58.72h6.58v1.4h-5.18V70.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M88.748 60.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M96.75 60.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M104.893 70.2V51.72h8.68V70.2h-8.68Z"></path><path d="M112.895 70.2V51.72h8.68V70.2h-8.68Zm13.043 0V51.72h1.4V70.2h-1.4Zm-2.8 0V51.72h1.4V70.2h-1.4Zm13.765 0V51.72h8.68V70.2h-8.68Z"></path><path d="M144.906 70.2V51.72h8.68V70.2h-8.68Z"></path><path d="M152.908 70.2V51.72h8.68V70.2h-8.68Zm10.243 0V58.72h6.58v1.4h-5.18V70.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M168.773 62.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm42.393 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M216.928 70.2V51.72h8.68V70.2h-8.68Z"></path><path d="M224.931 70.2V51.72h8.68V70.2h-8.68Zm10.242 0V58.72h6.58v1.4h-5.18V70.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M240.796 62.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm26.388 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M272.946 70.2V51.72h8.68V70.2h-8.68Z"></path><path d="M280.949 70.2V51.72h8.68V70.2h-8.68Z"></path><path d="M293.991 70.2V60.12h-5.18v-1.4h6.58V70.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm13.765 0V51.72h8.68V70.2h-8.68Z"></path><path d="M312.959 70.2V51.72h8.68V70.2h-8.68Zm10.242 0V58.72h6.58v1.4h-5.18V70.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M328.824 62.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm8.142 10.08V51.72h8.68V70.2h-8.68Z"></path><path d="M344.969 70.2V51.72h8.68V70.2h-8.68Zm13.042 0V51.72h1.4V70.2h-1.4Zm-2.8 0V51.72h1.4V70.2h-1.4Zm5.763 0V51.72h8.68V70.2h-8.68Z"></path><path d="M368.976 70.2V51.72h8.68V70.2h-8.68Zm13.043 0V51.72h1.4V70.2h-1.4Zm-2.8 0V51.72h1.4V70.2h-1.4Zm8.003-7.28v-11.2h1.4v9.8h5.179v1.4h-6.579Zm2.799-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M392.984 70.2V51.72h8.68V70.2h-8.68Z"></path><path d="M400.987 70.2V51.72h8.68V70.2h-8.68Zm10.242 0V58.72h6.58v1.4h-5.18V70.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M416.852 62.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm8.142 10.08V51.72h8.68V70.2h-8.68Z"></path><path d="M432.997 70.2V51.72h8.68V70.2h-8.68Zm13.042 0V51.72h1.4V70.2h-1.4Zm-2.8 0V51.72h1.4V70.2h-1.4Zm15.165-4.2v-1.26l4.312-5.292h-4.2V58.3h5.614v1.26l-4.396 5.292h4.55V66h-5.88ZM.86 87.2V68.72h8.68V87.2H.86Z"></path><path d="M8.863 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M16.865 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M24.868 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M32.87 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M40.873 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M48.875 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M61.918 87.2V77.12h-5.18v-1.4h6.58V87.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm5.762 0V68.72h8.68V87.2h-8.68Z"></path><path d="M72.883 87.2V68.72h8.68V87.2h-8.68Zm13.042 0V68.72h1.4V87.2h-1.4Zm-2.8 0V68.72h1.4V87.2h-1.4Zm21.768 0V68.72h8.68V87.2h-8.68Z"></path><path d="M112.895 87.2V68.72h8.68V87.2h-8.68Zm13.043 0V68.72h1.4V87.2h-1.4Zm-2.8 0V68.72h1.4V87.2h-1.4Zm5.762 0V68.72h8.68V87.2h-8.68Z"></path><path d="M136.903 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M144.906 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M152.908 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M160.911 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M168.913 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M176.916 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M189.958 87.2V77.12h-5.18v-1.4h6.58V87.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm29.77 0V68.72h8.68V87.2h-8.68Z"></path><path d="M224.931 87.2V68.72h8.68V87.2h-8.68Zm13.042 0V68.72h1.4V87.2h-1.4Zm-2.8 0V68.72h1.4V87.2h-1.4Zm40.013-7.28v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M280.949 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M288.951 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M296.954 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M304.956 87.2V68.72h8.68V87.2h-8.68Zm10.243 0V75.72h6.58v1.4h-5.18V87.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M320.821 79.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm16.145 10.08V68.72h8.68V87.2h-8.68Z"></path><path d="M344.969 87.2V68.72h8.68V87.2h-8.68Zm13.042 0V68.72h1.4V87.2h-1.4Zm-2.8 0V68.72h1.4V87.2h-1.4Zm5.763 0V68.72h8.68V87.2h-8.68Z"></path><path d="M368.976 87.2V68.72h8.68V87.2h-8.68Zm13.043 0V68.72h1.4V87.2h-1.4Zm-2.8 0V68.72h1.4V87.2h-1.4Zm16.005-7.28v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M400.847 77.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M408.849 79.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm16.145 10.08V68.72h8.68V87.2h-8.68Z"></path><path d="M432.997 87.2V68.72h8.68V87.2h-8.68Zm13.042 0V68.72h1.4V87.2h-1.4Zm-2.8 0V68.72h1.4V87.2h-1.4ZM3.1 96.92v-11.2h1.4v9.8h5.18v1.4H3.1Zm2.8-2.8v-8.4h1.4v7h2.38v1.4H5.9Z"></path><path d="M8.723 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M16.725 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M24.728 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M32.73 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M40.733 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M48.735 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M56.738 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm10.382 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M72.743 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M80.745 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm26.388 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M112.755 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M120.758 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm10.383 2.8v-11.2h1.399v9.8h5.18v1.4h-6.579Zm2.799-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M136.763 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M144.766 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M152.768 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M160.771 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M168.773 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M176.776 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M184.778 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm34.39 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M224.791 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M232.793 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm50.396 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M288.811 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M296.814 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M304.816 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M312.819 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm26.387 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M344.829 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M352.831 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm10.383 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M368.836 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M376.839 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm50.395 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M432.857 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M440.859 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Z"></path></svg><p>LazyVim is a Neovim setup powered by <a href="https://github.com/folke/lazy.nvim" target="_blank" rel="noopener noreferrer">üí§ lazy.nvim</a>
to make it easy to customize and extend your config.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/292349/213447056-92290767-ea16-430c-8727-ce994c93e9cc.png" alt="image"></p><h2 id="-features">‚ú® Features<a href="#-features" aria-label="Direct link to ‚ú® Features" title="Direct link to ‚ú® Features">‚Äã</a></h2><ul><li>üî• Transform your Neovim into a full-fledged IDE</li><li>üí§ Easily customize and extend your config with <a href="https://github.com/folke/lazy.nvim" target="_blank" rel="noopener noreferrer">lazy.nvim</a></li><li>üöÄ Blazingly fast</li><li>üßπ Sane default settings for options, autocmds, and keymaps</li><li>üì¶ Comes with a wealth of plugins pre-configured and ready to use</li></ul><h2 id="Ô∏è-requirements">‚ö°Ô∏è Requirements<a href="#Ô∏è-requirements" aria-label="Direct link to ‚ö°Ô∏è Requirements" title="Direct link to ‚ö°Ô∏è Requirements">‚Äã</a></h2><ul><li>Neovim &gt;= <strong>0.8.0</strong> (needs to be built with <strong>LuaJIT</strong>)</li><li>Git &gt;= <strong>2.19.0</strong> (for partial clones support)</li><li>a <a href="https://www.nerdfonts.com/" target="_blank" rel="noopener noreferrer">Nerd Font</a>(v3.0 or greater) <strong><em>(optional, but needed to display some icons)</em></strong></li><li><a href="https://github.com/jesseduffield/lazygit" target="_blank" rel="noopener noreferrer">lazygit</a> <strong><em>(optional)</em></strong></li><li>a <strong>C</strong> compiler for <code>nvim-treesitter</code>. See <a href="https://github.com/nvim-treesitter/nvim-treesitter#requirements" target="_blank" rel="noopener noreferrer">here</a></li><li>for <a href="https://github.com/nvim-telescope/telescope.nvim" target="_blank" rel="noopener noreferrer">telescope.nvim</a> <strong><em>(optional)</em></strong><ul><li><strong>live grep</strong>: <a href="https://github.com/BurntSushi/ripgrep" target="_blank" rel="noopener noreferrer">ripgrep</a></li><li><strong>find files</strong>: <a href="https://github.com/sharkdp/fd" target="_blank" rel="noopener noreferrer">fd</a></li></ul></li><li>a terminal that support true color and <em>undercurl</em>:<ul><li><a href="https://github.com/kovidgoyal/kitty" target="_blank" rel="noopener noreferrer">kitty</a> <strong><em>(Linux &amp; Macos)</em></strong></li><li><a href="https://github.com/wez/wezterm" target="_blank" rel="noopener noreferrer">wezterm</a> <strong><em>(Linux, Macos &amp; Windows)</em></strong></li><li><a href="https://github.com/alacritty/alacritty" target="_blank" rel="noopener noreferrer">alacritty</a> <strong><em>(Linux, Macos &amp; Windows)</em></strong></li><li><a href="https://iterm2.com/" target="_blank" rel="noopener noreferrer">iterm2</a> <strong><em>(Macos)</em></strong></li></ul></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Johnson and Johnson sues researchers who linked talc to cancer (439 pts)]]></title>
            <link>https://www.reuters.com/legal/litigation/johnson-johnson-sues-researchers-who-linked-talc-cancer-2023-07-13/</link>
            <guid>36753032</guid>
            <pubDate>Mon, 17 Jul 2023 01:14:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/litigation/johnson-johnson-sues-researchers-who-linked-talc-cancer-2023-07-13/">https://www.reuters.com/legal/litigation/johnson-johnson-sues-researchers-who-linked-talc-cancer-2023-07-13/</a>, See on <a href="https://news.ycombinator.com/item?id=36753032">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><ul role="tablist"><li data-testid="Text" role="tab" aria-selected="true" tabindex="0">Summary</li><li data-testid="Text" role="tab" aria-selected="false" tabindex="-1">Companies</li><li data-testid="Text" role="tab" aria-selected="false" tabindex="-1">Law Firms</li></ul></div><div><ul><li data-testid="Body">J&amp;J alleges researchers used "junk science" to disparage company's products</li><li data-testid="Body">Defendants say the lawsuits are meant to "silence" scientists</li></ul></div></div><p data-testid="paragraph-0">July 12 (Reuters) - Johnson &amp; Johnson has sued four doctors who published studies citing links between talc-based personal care products and cancer, escalating an attack on scientific studies that the company alleges are inaccurate.</p><p data-testid="paragraph-1">J&amp;J's subsidiary LTL Management, which <a data-testid="Link" href="https://www.reuters.com/business/healthcare-pharmaceuticals/jj-unit-manage-talc-claims-files-bankruptcy-protection-2021-10-14/">absorbed</a> the company's talc liability in a controversial 2021 spinoff, last week filed a <a data-testid="Link" href="https://tmsnrt.rs/46NMFl6" target="_blank">lawsuit</a> in New Jersey federal court asking it to force three researchers to "retract and/or issue a correction" of a study that said asbestos-contaminated consumer talc products sometimes caused patients to develop mesothelioma.</p><p data-testid="paragraph-2">One of the researchers, Richard Kradin, declined to comment. The other two, Theresa Emory and John Maddox, did not respond to requests for comment. Lawyers who have represented the three researchers in similar litigation in the past declined to comment.</p><p data-testid="paragraph-3">J&amp;J is facing more than 38,000 lawsuits alleging that the company's talc products, including its Baby Powder, were contaminated by asbestos and caused cancers including ovarian cancer and mesothelioma. J&amp;J is attempting to resolve those lawsuits, as well as any future talc lawsuits, through an $8.9 billion <a data-testid="Link" href="https://www.reuters.com/legal/jj-unit-goes-bankrupt-second-time-pursue-89-bln-talc-settlement-2023-04-04/">settlement</a> in bankruptcy court.</p><p data-testid="paragraph-4">J&amp;J says that its talc products are safe and do not contain asbestos.</p><p data-testid="paragraph-5">J&amp;J has <a data-testid="Link" href="https://www.reuters.com/business/healthcare-pharmaceuticals/jj-stop-selling-talc-based-baby-powder-globally-2023-2022-08-11/">stopped</a> selling talc-based Baby Powder in favor of cornstarch-based products, citing an increase in lawsuits and "misinformation" about the talc product's safety.</p><p data-testid="paragraph-6">The company in 2021 began <a data-testid="Link" href="https://www.reuters.com/business/healthcare-pharmaceuticals/inside-jjs-secret-plan-cap-litigation-payouts-cancer-victims-2022-02-04/">exploring</a> bankruptcy as a potential solution to the lawsuits, which saw a mixed record at trial, including several defense wins but also a <a data-testid="Link" href="https://www.reuters.com/legal/government/us-supreme-court-declines-hear-jj-appeal-over-2-billion-baby-powder-judgment-2021-06-01/">$2.1 billion</a> verdict awarded to 22 women who blamed their ovarian cancer on asbestos in the company's talc products. J&amp;J said in bankruptcy court filings in April that the costs of its talc-related verdicts, settlements and legal fees have reached about $4.5 billion.</p><p data-testid="paragraph-7">Last week's lawsuit against Emory and Maddox, pathologists affiliated with Peninsula Pathology Associates in Newport News, Virginia, and Kradin, a pulmonologist who worked at Massachusetts General Hospital Cancer Center before his retirement, comes on the heels of another complaint LTL <a data-testid="Link" href="https://tmsnrt.rs/3OekVPq" target="_blank">filed</a> in late May against another doctor, Jacqueline Moline, who works at Northwell Health in Great Neck, New York, on similar grounds.</p><p data-testid="paragraph-8">Moline published an article in 2019 studying 33 patients who said their only exposure to asbestos came from talc products, and Emory, Kradin and Maddox followed up with a 2020 study of 75 similar patients.</p><p data-testid="paragraph-9">All four doctors have provided expert testimony in lawsuits against J&amp;J, and their research has been cited in lawsuits where they have not testified, according to the complaints.</p><p data-testid="paragraph-10">LTL said the researchers concealed the fact that some or all of the patients involved in their studies had been exposed to asbestos from other sources.</p><p data-testid="paragraph-11">The company is also asking the court to force the researchers to disclose the patients' identities.</p><p data-testid="paragraph-12">The lawsuits allege product disparagement and fraud, among other claims.</p><p data-testid="paragraph-13">Adam Zimmerman, a professor at the University of Southern California Gould School of Law, said companies rarely file lawsuits over research they disagree with. It will be very difficult for LTL to prove that the researchers intentionally harmed J&amp;J's reputation, which is required for product disparagement cases in New Jersey, but the company may view the lawsuits as a way to discourage other researchers or reclaim the narrative about talc safety, Zimmerman said.</p><p data-testid="paragraph-14">"When a litigant starts suing opposing experts, that's very aggressive," Zimmerman said. "It sends a message that the gloves are off."</p><p data-testid="paragraph-15">Moline has argued in court papers that LTL's litigation would have a profoundly chilling effect on future medical research if the company were allowed to unmask patients "in the hopes of publicly smearing them." Her court filings say that LTL's lawsuit was meant to "attack and silence" scientists, and that she has an ethical obligation to protect the identities of her research subjects.</p><p data-testid="paragraph-16">LTL's lawsuits allege that the doctors' research allowed them to collect millions of dollars from plaintiffs' lawyers to push a "false narrative" about J&amp;J. The complaint against Moline, for example, said she had made a "small fortune" testifying as a paid expert in lawsuits, receiving over $3 million from her work on asbestos lawsuits. LTL alleged that Kradin also made more than $3 million testifying as a plaintiffs' expert.</p><p data-testid="paragraph-17">The researchers could not immediately be reached for comment.</p><p data-testid="paragraph-18">LTL had filed similar lawsuits against the researchers in December 2022, but those complaints were linked to LTL‚Äôs first bankruptcy filing and were dismissed along with the rest of the bankruptcy in April.</p><p data-testid="paragraph-19">The cases are LTL Management v. Moline and LTL Management v. Emory, U.S. District Court for the District of New Jersey, Nos. 23-cv-02990 and 23-cv-03649.</p><p data-testid="paragraph-20">For LTL: Peter Harvey of Patterson Belknap Webb &amp; Tyler; Allison Brown of Skadden, Arps, Slate, Meagher &amp; Flom; and Kristen Fournier of King &amp; Spalding</p><p data-testid="paragraph-21">For Moline: Kevin Marino and John Tortorella of Marino Tortorella &amp; Boyle</p><p data-testid="paragraph-22">For Emory, Kradin and Maddox: Not yet available
Read more:</p><p data-testid="paragraph-23"><a data-testid="Link" href="https://www.reuters.com/legal/jj-unit-goes-bankrupt-second-time-pursue-89-bln-talc-settlement-2023-04-04/">J&amp;J unit files for second bankruptcy to pursue $8.9 billion talc settlement</a></p><p data-testid="paragraph-24"><a data-testid="Link" href="https://www.reuters.com/legal/cancer-plaintiffs-drill-down-jjs-support-89-bln-talc-deal-2023-06-28/">Cancer plaintiffs drill down on J&amp;J's support for $8.9 billion talc deal</a></p><p data-testid="paragraph-25"><a data-testid="Link" href="https://www.reuters.com/legal/jjs-ltl-units-bankruptcy-dismissed-by-us-appeals-court-filing-2023-01-30/">U.S. court rejects J&amp;J bankruptcy strategy for thousands of talc lawsuits</a></p><p><span data-testid="Text">Reporting by Dietrich Knauth; additional reporting by Brendan Pierson</span></p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank">The Thomson Reuters Trust Principles.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using XPath in 2023 (131 pts)]]></title>
            <link>https://denizaksimsek.com/2023/xpath/</link>
            <guid>36752419</guid>
            <pubDate>Sun, 16 Jul 2023 23:24:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://denizaksimsek.com/2023/xpath/">https://denizaksimsek.com/2023/xpath/</a>, See on <a href="https://news.ycombinator.com/item?id=36752419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>In the latest release of <a href="https://htmx.org/">htmx</a>,
you can add event listeners to elements with <code>hx-on</code>:</p>
<pre tabindex="0"><code></code></pre>
<p>For all the other <code>hx-</code> attributes, we use CSS attribute selectors.
However, with <code>hx-on</code>, the attribute name is not fixed as it contains the event.
CSS attribute selectors support wildcards on the <em>value</em> of attributes,
but not the name:</p>
<pre tabindex="0"><code>[hx-trigger] <span>/* common and normal */</span>
[href^=<span>"https://dz4k.com"</span>] <span>/* "starts with" operator */</span>
[^<span>ƒ•x-on</span><span>:</span>] <span>/* not a thing */</span>
</code></pre>
<h2><ruby>X<rt>XML</rt>Path<rt>Path Language</rt></ruby></h2>
<p>XPath is a query language for extracting information from XML(-like) documents.
Its main use cases are XSLT and parsing API responses.</p>
<p>The XPath language is significantly more expressive than CSS,
making it possible to traverse the XML tree in any direction,
filter nodes based on arbitrary predicates,
and select any kind of node
(including comments, text nodes, and individual attributes).
Our non-existent CSS attribute could be written as follows:</p>
<pre tabindex="0"><code>//@*[starts-with(name(), "hx-on:")]
</code></pre>
<p>This post is not supposed to be an XPath tutorial, but I‚Äôll break this one down:</p>
<dl>
<dt><code>//</code></dt>
<dd>traverse the document (in CSS, this is the default)</dd>
<dt><code>@*</code></dt>
<dd>find any attribute (mnemonic: <strong>at</strong>-tribute)</dd>
<dt><code>[ ... ]</code></dt>
<dd>where‚Ä¶</dd>
<dt><code>starts-with(name(), "hx-on:")</code></dt>
<dd>its name starts with <code>"hx-on:"</code></dd>
</dl>
<p>CSS selectors don‚Äôt have these kinds of features,
and it has good reasons not to.
CSS has strict performance requirements
‚Äì to the point that ‚ÄúCSS optimization‚Äù is generally not a thing ‚Äì
and selectors that offer more control could make slow selectors possible.
In addition, CSS has well-defined specificity rules, whereas XPath does not.</p>
<p>However, while these features make CSS great for stylesheets,
CSS selectors are also the most common way to find DOM elements in
JavaScript code and lacking in that regard.
Many libraries which extend HTML do so by traversing the entire document
and finding elements manually.
This is often not needed since, if you didn‚Äôt know,
<strong>XPath is built into browsers.</strong></p>
<h2>document.evaluate</h2>
<p>The <a href="https://developer.mozilla.org/en-US/Web/XPath/Introduction_to_using_XPath_in_JavaScript"><code>document.evaluate</code> API</a> is somewhat archaic,
partly because it was designed for talking to XML APIs over <code>XMLHTTPRequest</code>.
Here‚Äôs a DOM-friendly wrapper:</p>
<pre tabindex="0"><code>function* xpath(...args) {
  let path, root = document;
  if (args.length &gt; 1) [root, path] = args;
  else [path] = args;

  const nodeIterator = document.evaluate(
    path,
    root,
    null,
    XPathResult.UNORDERED_NODE_ITERATOR_TYPE,
    null,
  );

  for (
    let node = nodeIterator.iterateNext();
    node != null;
    node = nodeIterator.iterateNext()
  ) {
    yield node;
  }
}

// TypeScript declaration
function xpath(path: string): Iterable;
function xpath(root: Element, path: string): Iterable;
</code></pre>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Red Programming Language (210 pts)]]></title>
            <link>https://www.red-lang.org/p/about.html</link>
            <guid>36752146</guid>
            <pubDate>Sun, 16 Jul 2023 22:41:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.red-lang.org/p/about.html">https://www.red-lang.org/p/about.html</a>, See on <a href="https://news.ycombinator.com/item?id=36752146">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-1150177033025383393" itemprop="description articleBody">
<p><b>Red </b>is a next-gen programming language,&nbsp;strongly inspired by&nbsp;<a href="http://rebol.com/">REBOL</a>. Main features are:</p><ul>
<li><b>Human-friendly</b> <a href="https://pointillistic.com/ren/" target="_blank">syntax</a></li>
<li><b><a href="http://en.wikipedia.org/wiki/Homoiconicity">Homoiconic</a></b>&nbsp;(Red is its own meta-language and own <b><a href="http://www.rebol.com/rebolsteps.html" target="_blank">data-format</a></b>)</li>
<li><b>Functional</b>, imperative, <b><a href="http://www.red-lang.org/2016/06/061-reactive-programming.html" target="_blank">reactive</a></b> and <b>symbolic </b>programming</li>
<li><b>Prototype</b>-based object support</li>
<li><b>Multi-</b>typing</li>
<li><b>Powerful pattern-matching&nbsp;</b><a href="http://www.red-lang.org/2016/12/entering-world-of-macros.html" target="_blank">Macros</a> system</li>
<li><b>Rich</b> set of built-in datatypes (50+)</li>
<li>Both <b>statically </b>and <b>JIT</b>-<b>compiled(*)&nbsp;</b>to native code</li>
<li><b>Cross-compilation</b> <a href="https://github.com/red/red/blob/master/usage.txt" target="_blank">done</a> <a href="https://github.com/red/red/blob/master/system/config.r" target="_blank">right</a></li>
<li>Produces executables of <b>less than 1MB</b>, with <b>no dependencies</b></li>
<li><b>Concurrency </b>and <b>parallelism </b>strong support (actors, parallel collections)(*)</li>
<li>Low-level <b>system programming</b> abilities through the built-in Red/System <a href="http://en.wikipedia.org/wiki/Domain-specific_language">DSL</a></li>
<li>Powerful <b>PEG <a href="http://www.red-lang.org/2013/11/041-introducing-parse.html" target="_blank">parser </a></b><a href="http://www.red-lang.org/2013/11/041-introducing-parse.html" target="_blank">DSL</a>&nbsp;built-in</li><li>Fast, compacting<b> Garbage Collector</b></li>
<li><b>Cross-platform native <a href="http://www.red-lang.org/2016/03/060-red-gui-system.html" target="_blank">GUI </a></b><a href="http://www.red-lang.org/2016/03/060-red-gui-system.html" target="_blank">system</a>, with a <a href="http://doc.red-lang.org/gui/VID.html" target="_blank">UI layout DSL</a> and <a href="http://doc.red-lang.org/gui/Draw.html" target="_blank">drawing DSL</a></li>
<li><b>Bridging</b> <a href="https://github.com/red/red/blob/master/bridges/java/hello.red" target="_blank">to the JVM</a></li>
<li>High-level <b>scripting </b>and <a href="http://en.wikipedia.org/wiki/Read-eval-print_loop"><b>REPL</b></a>&nbsp;GUI and CLI consoles included</li>
<li>Visual Studio Code<b>&nbsp;<a href="https://marketplace.visualstudio.com/items?itemName=red-auto.red" target="_blank">plugin</a></b>, with many helpful features</li>
<li>Highly <b><a href="http://www.red-lang.org/2017/03/062-libred-and-macros.html" target="_blank">embeddable</a></b></li>
<li><b>Low </b>memory footprint</li>
<li><b><a href="http://www.red-lang.org/p/download.html" target="_blank">Single-file</a></b>&nbsp;(~1MB)&nbsp;contains whole toolchain, full standard library and REPL&nbsp;(**)</li>
<li><b>No install, no setup</b></li>
<li><b>Fun</b>&nbsp;guaranteed!</li>
</ul>
<div>
<p>(*) Not implemented yet.</p><p>(**) Temporarily split in two binaries</p><p>
Red‚Äôs ambitious goal is to build the world‚Äôs first <b>full-stack language</b>, a language you can use from system programming tasks, up to high-level scripting through DSL. You've probably heard of the term "<a href="http://www.laurencegellert.com/2012/08/what-is-a-full-stack-developer/" target="_blank">Full-Stack Developer</a>". But what is a full-stack Language, exactly?</p><p>

Other languages talk about having "one tool to rule them all". Red has that mindset too, pushed to the limit - it's a single executable that takes in your source files on any platform, and produces a packaged binary for any platform, from any other. The tool doesn‚Äôt depend on anything besides what came with your OS...shipping as a single executable that about a megabyte.</p><p>

But that technical feat alone isn't enough to define Red's notion of a "Full-Stack Language". It's about the ability to bend and redefine the system to meet any need, while still working with literate code, and getting top-flight performance. &nbsp;So what's being put in your hands is more like a "language construction set" than simply "a language". Whether you‚Äôre writing a device driver, a platform-native GUI application, or a shared library... Red lets you use a common syntax to code at the right level of abstraction for the task.</p><p><img src="https://3.bp.blogspot.com/-xhOP35Dm99w/UuXFKgY2dlI/AAAAAAAAAGA/YQu98_pPDjw/s1600/reichart-abstraction-diagram.png">
</p>
</div><p>
It was announced and presented for the first time&nbsp;at <a href="http://reborcon.esperconsultancy.nl/">ReBorCon 2011</a> conference (March, 2011).&nbsp;A more recent presentation video was given at the Recode conference in Montreal (July, 2013):</p><p>
<iframe allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/-KqNO_sDqm4" width="560"></iframe>
</p>
<p>
But if you are unable to visit YouTube, here are some slide decks explaining the reasons for building it, showing the main features and the roadmap.</p><p>
Recode 2013 presentation slides:&nbsp;<a href="http://static.red-lang.org/Recode2013-Red.pdf" target="_new">PDF version</a>.
</p><p>
And for historical purposes, here are some older presentations:</p><ul><li><a href="http://static.red-lang.org/Red-SFD2011-45mn.pdf" target="_blank">SFD 2011 conference slides</a></li><li><a href="http://static.red-lang.org/red-rebor2011.pdf" target="_blank">ReBor 2011 conference slides</a></li></ul>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dusk OS: 32-bit Forth OS. Useful during first stage of civilizational collapse (167 pts)]]></title>
            <link>https://duskos.org/</link>
            <guid>36751422</guid>
            <pubDate>Sun, 16 Jul 2023 21:09:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://duskos.org/">https://duskos.org/</a>, See on <a href="https://news.ycombinator.com/item?id=36751422">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>Dusk OS is a 32-bit Forth and big brother to <a href="http://collapseos.org/">Collapse OS</a>. Its
<a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/design/purpose.txt">primary purpose</a> is to be maximally useful during the <a href="http://collapseos.org/why.html">first stage of
civilizational collapse</a>, that is, when we can't produce modern
computers anymore but that there's still many modern computers still around.</p>
<p>It does so by aggressively prioritizing <a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/design/simple.txt">simplicity</a> at the cost of
<a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/design/limits.txt">unorthodox constraints</a>, while also aiming to make
<a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/design/shell.txt">operators happy</a>.</p>
<p>Dusk OS innovates by having an <a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/cc/index.txt">"almost C" compiler</a> allowing it to
piggy-back on UNIX C code, through a modest <a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/design/port.txt">porting effort</a>, to reach
its goals and stay true to its design constraints with a minimal effort.</p>
<p><a href="https://vimeo.com/800710912">Video showcasing Dusk OS</a></p>
<h2>Why build this OS?</h2>
<p>Most modern operating systems can do whatever we want them to do. Why do we
need another one? Simplicity.</p>
<p>It's difficult to predict post-collapse conditions, but we can suppose that
many <a href="#operator">operators</a> will need to use their machines in novel and
creative ways. Hackability of the operating system then becomes paramount. Open
source modern operating systems all can be modified to fit its user's needs,
but their complexity limits the likelihood that the user is able to do so. A
simpler OS increases this likelihood.</p>
<p>But we can't have our cake and eat it too, right? Either you have a simple toy
OS or a complex one. Well, maybe not?</p>
<p>Its authors believe that in the history of computing, Forth has been
under-explored. Its approach to simplicity is, we think, revolutionary. It
has significant shortcomings when system specifications become more complex
(Forth hates complexity and doesn't manage it well), but we believe it
possible to elegantly marry it with languages that like complexity better.</p>
<p>This mix, we believe, could provide the operator with computing powers rarely
seen with other approaches. We've got to try it.</p>
<p>To be clear: this is a research project, we don't know what it will yield
beforehand. We have the intuition that it might lead to a big "ah ah!" moment
and reveal a breathtaking combination of power and simplicity.</p>
<h2>Features making Dusk OS special</h2>
<h3>A whole OS built from source on boot</h3>
<p>One thing that makes Dusk OS special is that it boots from a very tiny core
(1000 lines of i386 assembly). From this tiny core, on boot, it builds its way
up to a system that has a functional C compiler, which then allows it to
bootstrap itself some more.</p>
<p>With regards to "source bootstrapping", it's even more extreme than Collapse OS
because modern machines allows this process to run quickly and the whole
process is still faster than a regular Linux boot. On Collapse OS target
machines, this process would be prohibitive, so a bigger part of the OS is
cross-compiled into the kernel.</p>
<p>This peculiarity of Dusk OS has interesting properties. The nicest one, in my
humble opinion, is that this allows us to sidestep the <em>entire</em> problems of
binary compatibility and relocation and only deal with source compatibility.
So, no ELF, no binutils, only code that is designed to run from where it was
generated in the first place. This is so much simpler!</p>
<p>Object files? Global symbols? Nah. C functions that don't have a static storage
type are simple Forth words.</p>
<h3>Harmonized Assembly Layer</h3>
<p>Dusk features what we call the <a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/hal.txt">Harmonized Assembly Layer</a> (HAL for short).
This is a cross-CPU assembler, on which the C compiler relies, which prioritizes
implementation and usage simplicity, but is also designed to generate efficient
native code.</p>
<h3>Shortest path to self-hosting for an "almost C" compiler</h3>
<p>Dusk OS self-hosts in about 1000 lines of assembly and a few hundred lines of
Forth (the exact number depends on the target machine). From there, it
bootstraps to DuskCC, which is roughly 1500 lines of Forth code. To my
knowledge, Dusk OS is unique in that regard.</p>
<p>You can pick any C compiler that requires POSIX and it will automatically
require order of magnitudes more lines of code to bootstrap because you need
that POSIX system in addition to the C compiler. So even if you pick a small C
compiler such as tcc, you still need a POSIX system to build it, which is
usually in the millions of LOCs. </p>
<p>To be fair, Dusk OS is not the first project thinking of optimizing that path.
<a href="https://github.com/fosslinux/live-bootstrap">Efforts at making our modern software world bootstrappable</a>
lead to an "almost C", <a href="https://git.sr.ht/~oriansj/M2-Planet">M2-Planet</a> with a feature set comparable to
DuskCC with very few lines of code. M2-Planet itself is about 5K lines of code
and the various stages that lead to it are generally a few hundred lines each.
The project initially ran on top of regular kernels (as in "fat kernels with
lots of code"), but some bare metal stages (<a href="https://github.com/ironmeld/builder-hex0">1</a>,
<a href="https://git.stikonas.eu/andrius/stage0-uefi">2</a>) were created and now this little chain end up being
comparable to Dusk in terms of lines of code. Still more than Dusk, but in the
same ballpark.</p>
<p>Although this path is short and technically leads you to an "almost C"
compiler, you can hardly use it because it has no "real kernel" (those bare
metal stages mentioned above are enough to compile M2-Planet, but really not
much else, they're extremely limited) and no shell. You'll need those if you
want to use your shiny compiler.</p>
<p>One of your best picks, should you try this path, would be <a href="https://www.fiwix.org/">Fiwix</a>, a
minimal POSIX i386 kernel weighting less than 50K lines of C+asm. But then,
M2-Planet is not enough. You need to compile tcc (which M2-Planet can compile
after having applied a few patches) which weights 80K. Userspace is worse.
Bash+coreutils are 400K, even busybox is 190K. We still end up with a pretty
minimal and simple system, but it's still a lot more code than Dusk.</p>
<p>So, unless someone tells me about some option I don't know about, DuskCC is
quite innovative on the aspect of self-hosting path length.</p>

<h2>Who is Dusk for?</h2>
<p>Dusk OS doesn't have users, but <em>operators</em>. What's the difference? Control.
You <em>use</em> a phone, you <em>use</em> a coffee machine, hell you even <em>use</em> a car these
days. But you <em>operate</em> a bulldozer, you <em>operate</em> a crane, you <em>operate</em> a
plane.</p>
<p>You <em>use</em> Linux, you <em>use</em> Windows. You <em>operate</em> Dusk OS.</p>
<p>Can you <em>operate</em> Linux? Sure, if you're some kind of god<sup id="fnref:1"><a href="#fn:1">1</a></sup>, in the same way
that you can <em>operate</em> a Tesla if you're a top Tesla engineer. But you're much
more likely to be able to <em>operate</em> a landmower than a Tesla.</p>
<p>The Dusk operator is someone who's <a href="http://collapseos.org/why.html#creative">creative</a>, close to hardware, can
read a datasheet. Dusk shines when one wants to poke around the hardware
without limit.</p>
<p>It compares favorably to other more complete OSes because there's no concurrent
process to mess with your poking and the driver structure is more approachable,
hackable due to its stricter scope and savvier target audience.</p>
<p>Let's use an example. Let's say you're on a notebook that runs on a chipset of
Intel's ICHn family. You read the datasheet and see "oh, nice, there's an SPI
interface in there. Maybe that it's not hooked to anything on the notebook,
let's play with it."</p>
<p>Now, that chipset is very, very central to the computer. There are good chances,
on a BSD or Linux system, that if you begin poking around its registers, you'll
step on someone else toes and crash the system because, for example, of some
other process that needed to read from disk at the same time.</p>
<p>In Dusk, you could completely break the SATA controller, you'll still be golden
as long as you don't access mass storage. Because Dusk doesn't have
concurrency, you have tight control over what happen or doesn't happen on the
machine, so all you need to do is to avoid words that access mass storage. That
gives you ample wiggling space for your hacking session.</p>
<p>To be clear: this is also possible with a custom made BSD or Linux, but you're
going to have to strip a lot of pieces from your distro before you get there
and some of those pieces might be useful debugging tools which will be
difficult to retrofit because they need a wider system. You'll also need a
higher cognitive space to fit BSD/Linux wider abstractions in your mind.</p>
<h2>Status</h2>
<ul>
<li>Has a VM written in C, buildable from a POSIX environment, which allows Dusk
  to build itself for any of its supported targets.</li>
<li>Has an <a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/cc/index.txt">"almost C" compiler</a> which still needs some work, but is
  already capable of compiling a nice subset of C.</li>
<li>Can run on i386 and ARM. <a href="https://git.sr.ht/~vdupras/duskos/tree/master/HARDWARE.md">Detailed list of supported hardware.</a></li>
<li>Can read, write and boot from FAT12/FAT16 (no FAT32 for now) volumes.</li>
<li>Can create new FAT12/FAT16 volumes.</li>
<li>Very small footprint. In Grid mode (TUI mode) with the Grid text editor
  and DuskCC (including its stdlib) loaded, Dusk uses 180KB of RAM on a PC.</li>
<li>It completely self-hosts on all its target machines. (Well, only PC at this
  point because the ARM port is still a WIP, but it <em>will</em> self-host)</li>
<li>Simple and terse. The core system (all kernels, drivers, filesystems, CC,
  core libraries) is less than 9K lines of code.</li>
<li>Since <code>text/ed</code> has reached a usable status, the main author of Dusk has been
  developing it from within itself on an old Pentium 75 Mhz with 16mb of RAM and
  he's having a blast.</li>
</ul>
<p>List of ported codebases:</p>
<ul>
<li>The <a href="https://wiki.xxiivv.com/site/uxn.html">uxn</a> VM</li>
<li>The <code>puff()</code> algorithm from <a href="https://github.com/madler/zlib">zlib</a></li>
<li>The <a href="https://git.sr.ht/~rabbits/left">left</a> text editor. (Still a bit glitchy...)</li>
</ul>
<p>List of homegrown applications:</p>
<ul>
<li>C Compiler (see <code>doc/cc</code>)</li>
<li>Text editor (see <code>doc/text/ed</code>)</li>
</ul>
<h2>Funding</h2>
<p>You are inspired by Dusk OS and would like to fund its development? I don't do
any kind of crowdfunding, but if you happen to be a rich philanthropist, <a href="https://duskos.org/funding.html">maybe
we can do something</a>. </p>
<h2>Resources</h2>
<ul>
<li><a href="https://sr.ht/~vdupras/duskos">Source code repository</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/index.txt">Documentation</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/ROADMAP.md">Roadmap</a></li>
<li><a href="https://duskos.org/discuss.html">Mailing list</a></li>
<li><a href="https://tumbleforth.hardcoded.net/">Tumble Forth, a Forth vulgarization blog</a></li>
<li><a href="https://alexw.nyc/tech/duskos-1.html">Alex's Dusk OS tutorial</a></li>
</ul>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Undocumented 8086 instructions, explained by the microcode (226 pts)]]></title>
            <link>https://www.righto.com/2023/07/undocumented-8086-instructions.html</link>
            <guid>36751399</guid>
            <pubDate>Sun, 16 Jul 2023 21:06:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.righto.com/2023/07/undocumented-8086-instructions.html">https://www.righto.com/2023/07/undocumented-8086-instructions.html</a>, See on <a href="https://news.ycombinator.com/item?id=36751399">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-5516205124640022120" itemprop="description articleBody">



<p>What happens if you give the Intel 8086 processor an instruction that doesn't exist?
A modern microprocessor (80186 and later) will generate an exception, indicating that an illegal instruction
was executed.
However, early microprocessors didn't include the circuitry to detect illegal instructions, since the chips didn't have
transistors to spare. Instead these processors would do <em>something</em>,
but the results weren't specified.<span id="fnref:6502"><a href="#fn:6502">1</a></span></p>
<p>The 8086 has a number of undocumented instructions.
Most of them are simply duplicates of regular instructions, but a few have unexpected behavior, such as revealing the
values of internal, hidden registers.
In the 8086, most instructions are implemented in microcode, so examining the 8086's microcode can explain why these instructions
behave the way they do.</p>
<p>The photo below shows the 8086 die under a microscope, with the important functional blocks labeled. The metal layer is visible, while the underlying silicon and polysilicon wiring is mostly hidden.
The microcode ROM and the microcode address decoder are in the lower right.
The Group Decode ROM (upper center) is also important, as it performs the first step of instruction decoding.</p>
<p><a href="https://static.righto.com/images/8086-ad-undoc/die-labeled.jpg"><img alt="The 8086 die under a microscope, with main functional blocks labeled. Click on this image (or any other) for a larger version." height="589" src="https://static.righto.com/images/8086-ad-undoc/die-labeled-w600.jpg" title="The 8086 die under a microscope, with main functional blocks labeled. Click on this image (or any other) for a larger version." width="600"></a></p><p>The 8086 die under a microscope, with main functional blocks labeled. Click on this image (or any other) for a larger version.</p>
<h2>Microcode and 8086 instruction decoding</h2>
<p>You might think that machine instructions are the basic steps that a computer performs.
However, instructions usually require multiple steps inside the processor.
One way of expressing these multiple steps is through microcode, a technique dating back to 1951.
To execute a machine instruction, the computer internally executes several simpler micro-instructions, specified by the microcode.
In other words, microcode forms nother layer between the machine instructions and the hardware.
The main advantage of microcode is that it turns the processor's control logic into a programming task instead of a difficult logic design task.</p>
<p>The 8068's <a href="https://www.righto.com/2022/11/how-8086-processors-microcode-engine.html">microcode ROM</a> holds 512 micro-instructions, each 21 bits wide.
Each micro-instruction performs two actions in parallel. First is a move between a source and a destination, typically registers.
Second is an operation that can range from an arithmetic (ALU) operation to a memory access.
The diagram below shows the structure of a 21-bit micro-instruction, divided into six types.</p>
<p><a href="https://static.righto.com/images/8086-ad-undoc/microcode-format.jpg"><img alt="The encoding of a micro-instruction into 21 bits. Based on NEC v. Intel: Will Hardware Be Drawn into the Black Hole of Copyright?" height="203" src="https://static.righto.com/images/8086-ad-undoc/microcode-format-w700.jpg" title="The encoding of a micro-instruction into 21 bits. Based on NEC v. Intel: Will Hardware Be Drawn into the Black Hole of Copyright?" width="700"></a></p>
<p>When executing a machine instruction, the 8086 performs a decoding step.
Although the 8086 is a 16-bit processor, its instructions are based on bytes. In most cases, the first byte specifies the
opcode, which may be followed by additional instruction bytes.
In other cases, the byte is a "prefix" byte, which changes the behavior of the following instruction.
The first byte is analyzed
by something called the <a href="https://www.righto.com/2023/05/8086-processor-group-decode-rom.html">Group Decode ROM</a>.
This circuit categorizes the first byte of the instruction into about <code>35</code> categories that control how the instruction is
decoded and executed.
One category is "1-byte logic"; this indicates a one-byte instruction or prefix that is simple and implemented by logic circuitry in the 8086.
For instructions in this category, microcode is not involved while
the remaining instructions are implemented in microcode.
Many of these instructions are in the "two-byte ROM" category indicating that the instruction has a second byte
that also needs to be decoded by microcode.
This second byte, called the ModR/M byte, specifies that memory addressing mode or registers that the instruction uses.</p>
<p>The next step is the microcode's address decoder circuit, which determines where to start executing microcode based on
the opcode.
Conceptually, you can think of the microcode as stored in a ROM, indexed by the instruction opcode and a few sequence bits.
However, since many instructions can use the same microcode, it would be inefficient to store duplicate copies of these routines.
Instead, the microcode address decoder permits multiple instructions to reference the same entries in the ROM.
This decoding circuitry is similar to a PLA (Programmable Logic Array) so it matches bit patterns to determine a particular starting point.
This turns out to be important for undocumented instructions since undocumented instructions often match the pattern for a "real" instruction, making the undocumented instruction an alias.</p>
<p>The 8086 has several internal registers that are invisible to the programmer but are used by the microcode.
Memory accesses use the Indirect (<code>IND</code>) and Operand (<code>OPR</code>) registers; the <code>IND</code> register holds the address in the segment,
while the <code>OPR</code> register holds the data value that is read or written.
Although these registers are normally not accessible by the programmer, some undocumented instructions provide access to these registers, as will be described later.</p>
<p>The Arithmetic/Logic Unit (ALU) performs arithmetic, logical, and shift operations in the 8086.
The ALU uses three internal registers: <code>tmpA</code>, <code>tmpB</code>, and <code>tmpC</code>. An ALU operation requires two micro-instructions.
The first micro-instruction specifies the operation (such as <code>ADD</code>) and the temporary register that holds one argument (e.g. <code>tmpA</code>);
the second argument is always in <code>tmpB</code>.
A following micro-instruction can access the ALU result through the pseudo-register <code>Œ£</code> (sigma).</p>
<h3>The ModR/M byte</h3>
<p>A fundamental part of the 8086 instruction format is the ModR/M byte, a byte that specifies addressing for many instructions.
The 8086 has a variety of addressing modes, so the ModR/M byte is somewhat complicated.
Normally it specifies one memory address and one register. The memory address is specified through one of eight addressing
modes (below) along with an optional 8- or 16-bit displacement in the instruction.
Instead of a memory address, the ModR/M byte can also specify a second register.
For a few opcodes, the ModR/M byte selects what instruction to execute rather than a register.</p>
<p><a href="https://static.righto.com/images/8086-ad-undoc/modrm.png"><img alt="The 8086's addressing modes. From The register assignments, from MCS-86 Assembly Language Reference Guide." height="220" src="https://static.righto.com/images/8086-ad-undoc/modrm-w250.png" title="The 8086's addressing modes. From The register assignments, from MCS-86 Assembly Language Reference Guide." width="250"></a></p>
<p>The implementation of the ModR/M byte plays an important role in the behavior of undocumented instructions.
Support for this byte is implemented in both microcode and hardware.
The various memory address modes above are implemented by microcode subroutines, which compute the appropriate memory address and
perform a read if necessary.
The subroutine leaves the memory address in the <code>IND</code> register, and if a read is performed, the value is in the <code>OPR</code> register.</p>
<p>The hardware hides the ModR/M byte's selection of memory versus register, by making the value available through the pseudo-register <code>M</code>, while the second register is available through <code>N</code>.
Thus, the microcode for an instruction doesn't need to know if the value was in memory or a register, or which register was selected.
The Group Decode ROM examines the first byte of the instruction to determine if a ModR/M byte is present, and if a read
is required.
If the ModR/M byte specifies memory, the Translation ROM determines which micro-subroutines to call before handling the
instruction itself.
For more on the ModR/M byte, see my post on <a href="https://www.righto.com/2023/02/8086-modrm-addressing.html">Reverse-engineering the ModR/M addressing microcode</a>.</p>
<h2>Holes in the opcode table</h2>
<p>The first byte of the instruction is a value from <code>00</code> to <code>FF</code> in hex.
Almost all of these opcode values correspond to documented 8086 instructions, but there are a few exceptions, "holes" in the opcode table.
The table below shows the 256 first-byte opcodes for the 8086, from hex <code>00</code> to <code>FF</code>. Valid opcodes for the 8086 are in white;
the colored opcodes are undefined and interesting to examine.
Orange, yellow, and green opcodes were given meaning in the 80186, 80286, and 80386 respectively.
The purple opcode is unusual: it was implemented in the 8086 and later processors but not documented.<span id="fnref:prefixes"><a href="#fn:prefixes">2</a></span>
In this section, I'll examine the microcode for these opcode holes.</p>
<p><a href="https://static.righto.com/images/8086-ad-undoc/opcodes.png"><img alt="This table shows the 256 opcodes for the 8086, where the white ones are valid instructions. Click for a larger version." height="453" src="https://static.righto.com/images/8086-ad-undoc/opcodes-w450.png" title="This table shows the 256 opcodes for the 8086, where the white ones are valid instructions. Click for a larger version." width="450"></a></p><p>This table shows the 256 opcodes for the 8086, where the white ones are valid instructions. Click for a larger version.</p>
<h3><code>D6</code>: <code>SALC</code></h3>
<p>The opcode <code>D6</code> (purple above) performs a well-known but undocumented operation that is typically called <code>SALC</code>, for Set AL to Carry.
This instruction sets the <code>AL</code> register to 0 if the carry flag is 0, and sets the <code>AL</code> register to <code>FF</code> if the carry flag is 1.
The curious thing about this undocumented instruction is that it exists in all x86 CPUs, but Intel didn't mention it until 2017.
Intel probably put this instruction into the processor deliberately as a <a href="https://en.wikipedia.org/wiki/Fictitious_entry#Copyright_traps">copyright trap</a>.
The idea is that if a company created a copy of the 8086 processor and the processor included the <code>SALC</code> instruction, this
would prove that the company had copied Intel's microcode and thus had potentially violated Intel's copyright on the microcode.
This came to light when NEC created improved versions of the 8086, the NEC V20 and V30 microprocessors, and was sued by Intel.
Intel analyzed NEC's microcode but was disappointed to find that NEC's chip did not include the hidden instruction, showing
that NEC hadn't copied the microcode.<span id="fnref:magic-instruction"><a href="#fn:magic-instruction">3</a></span>
Although a Federal judge <a href="https://www.nytimes.com/1989/02/08/business/intel-loses-copyright-case-to-nec.html">ruled</a> in 1989 that NEC hadn't infringed
Intel's copyright, the 5-year trial ruined NEC's market momentum.</p>
<p>The <code>SALC</code> instruction is implemented with three micro-instructions, shown below.<span id="fnref:microcode"><a href="#fn:microcode">4</a></span>
The first micro-instruction jumps if the carry (<code>CY</code>) is set.
If not, the next instruction moves 0 to the AL register. <code>RNI</code> (Run Next Instruction) ends the microcode execution
causing the next machine instruction to run.
If the carry was set, all-ones (i.e. <code>FF</code> hex) is moved to the <code>AL</code> register and RNI ends the microcode sequence.</p>
<pre>           JMPS CY 2 <span><b>SALC</b>: jump on carry</span>
ZERO ‚Üí AL  RNI       <span>Move 0 to AL, run next instruction</span>
ONES ‚Üí AL  RNI       <span><b>2:</b>Move FF to AL, run next instruction</span>
</pre>

<h3><code>0F</code>: <code>POP CS</code></h3>
<p>The <code>0F</code> opcode is the first hole in the opcode table.
The 8086 has instructions to push and pop the four segment registers, except opcode <code>0F</code> is undefined where <code>POP CS</code> should be.
This opcode performs <code>POP CS</code> successfully, so the question is why is it undefined?
The reason is that <code>POP CS</code> is essentially useless and doesn't do what you'd expect, so Intel figured it was best not
to document it.</p>
<p>To understand why <code>POP CS</code> is useless, I need to step back and explain the 8086's segment registers.
The 8086 has a <code>20</code>-bit address space, but 16-bit registers.
To make this work, the 8086 has the concept of segments: memory is accessed in 64K chunks called segments, which are positioned
in the 1-megabyte address space.
Specifically, there are four segments: Code Segment, Stack Segment, Data Segment, and Extra Segment,
with four segment registers that define the start of the segment: <code>CS</code>, <code>SS</code>, <code>DS</code>, and <code>ES</code>.</p>
<p>An inconvenient part of segment addressing  is that if you want to access more than 64K, you need to change the segment register.
So you might push the data segment register, change it temporarily so you can access a new part of memory, and then pop the old data segment
register value off the stack.
This would use the <code>PUSH DS</code> and <code>POP DS</code> instructions.
But why not <code>POP CS</code>?</p>
<p>The 8086 executes code from the code segment, with the instruction pointer (<code>IP</code>) tracking the location in the code segment.
The main problem with <code>POP CS</code> is that it changes the code segment, but not the instruction pointer, so now you are executing
code at the old offset in a new segment.
Unless you line up your code extremely carefully, the result is that you're jumping to an unexpected place in memory.
(Normally, you want to change <code>CS</code> and the instruction pointer at the same time, using a <code>CALL</code> or <code>JMP</code> instruction.)</p>
<p>The second problem with <code>POP CS</code> is prefetching.
For efficiency, the 8086 prefetches instructions before they are needed, storing them in an 8-byte prefetch queue.
When you perform a jump, for instance, the microcode flushes the prefetch queue so execution will continue with the
new instructions, rather than the old instructions.
However, the instructions that pop a segment register don't flush the prefetch buffer.
Thus, <code>POP CS</code> not only jumps to an unexpected location in memory, but it will execute an unpredictable number of instructions
from the old code path.</p>
<p>The <code>POP segment register</code> microcode below packs a lot into three micro-instructions.
The first micro-instruction pops a value from the stack.
Specifically, it moves the stack pointer (<code>SP</code>) to the Indirect (<code>IND</code>) register.
The Indirect register is an internal register, invisible to the programmer, that holds the address offset for memory
accesses.
The first micro-instruction also performs a memory read (<code>R</code>) from the stack segment (<code>SS</code>) and then increments <code>IND</code>
by 2 (<code>P2</code>, plus 2).
The second micro-instruction moves <code>IND</code> to the stack pointer, updating the stack pointer with the new value.
It also tells the microcode engine that this micro-instruction is the next-to-last (<code>NXT</code>) and the next machine instruction
can be started.
The final micro-instruction moves the value read from memory to the appropriate segment register and runs the next instruction.
Specifically, reads and writes put data in the internal <code>OPR</code> (Operand) register.
The hardware uses the register <code>N</code> to indicate the register specified by the instruction.
That is, the value will be stored in the <code>CS</code>, <code>DS</code>, <code>ES</code>, or <code>SS</code> register, depending on the bit pattern in the instruction.
Thus, the same microcode works for all four segment registers.
This is why <code>POP CS</code> works even though <code>POP CS</code> wasn't explicitly implemented in the microcode; it uses the common code.</p>
<pre>SP ‚Üí IND  R SS,P2 <span><b>POP sr</b>: read from stack, compute IND plus 2</span>
IND ‚Üí SP  NXT     <span>Put updated value in SP, start next instruction.</span>
OPR ‚Üí N   RNI     <span>Put stack value in specified segment register</span>
</pre>

<p>But why does <code>POP CS</code> run this microcode in the first place?
The microcode to execute is selected based on the instruction, but multiple instructions can execute the same microcode.
You can think of the address decoder as pattern-matching on the instruction's bit patterns, where some of the bits can be ignored.
In this case, the <code>POP sr</code> microcode above is run by any instruction with the bit pattern 000??111, where a question mark
can be either a 0 or a 1.
You can verify that this pattern matches <code>POP ES</code> (<code>07</code>), <code>POP SS</code> (<code>17</code>), and <code>POP DS</code> (<code>1F</code>).
However, it also matches <code>0F</code>, which is why the <code>0F</code> opcode runs the above microcode and performs <code>POP CS</code>.
In other words, to make <code>0F</code> do something other than <code>POP CS</code> would require additional circuitry, so it was easier to
leave the action implemented but undocumented.</p>
<h3><code>60</code>-<code>6F</code>: conditional jumps</h3>
<p>One whole row of the opcode table is unused: values <code>60</code> to <code>6F</code>.
These opcodes simply act the same as <code>70</code> to <code>7F</code>, the conditional jump instructions.</p>
<p>The conditional jumps use the following microcode.
It fetches the jump offset from the instruction prefetch queue (<code>Q</code>) and puts the value into the ALU's <code>tmpBL</code> register,
the low byte of the <code>tmpB</code> register.
It tests the condition in the instruction (<code>XC</code>) and jumps to the <code>RELJMP</code> micro-subroutine if satisfied.
The <code>RELJMP</code> code (not shown) updates the program counter to perform the jump.</p>
<pre>Q ‚Üí tmpBL                <span><b>Jcond cb:</b> Get offset from prefetch queue</span>
           JMP XC RELJMP <span>Test condition, if true jump to RELJMP routine</span>
           RNI           <span>No jump: run next instruction</span>
</pre>

<p>This code is executed for any instruction matching the bit pattern <code>011?????</code>, i.e. anything from <code>60</code> to <code>7F</code>.
The condition is specified by the four low bits of the instruction.
The result is that any instruction <code>60</code>-<code>6F</code> is an alias for the corresponding conditional jump <code>70</code>-<code>7F</code>.</p>
<h3><code>C0</code>, <code>C8</code>: <code>RET/RETF imm</code></h3>
<p>These undocumented opcodes act like a return instruction, specifically <code>RET imm16</code> (<a href="https://www.os2museum.com/wp/undocumented-8086-opcodes-part-i/">source</a>).
Specifically, the instruction <code>C0</code> is the same as <code>C2</code>, near return, while <code>C8</code> is the same as <code>CA</code>, far return.</p>
<p>The microcode below is executed for the instruction bits <code>1100?0?0</code>, so it is executed for <code>C0</code>, <code>C2</code>, <code>C8</code>, and <code>CA</code>.
It gets two bytes from the instruction prefetch queue (<code>Q</code>) and puts them in the <code>AX</code> register.
Next, it calls <code>FARRET</code>, which performs either a near return (popping <code>PC</code> from the stack) or a far return (popping <code>PC</code> and <code>CS</code>
from the stack). Finally, it adds the original argument to the <code>SP</code>, equivalent to popping that many bytes.</p>
<pre>Q ‚Üí tmpAL    ADD tmpA    <span><b>RET/RETF iw:</b> Get word from prefetch, set up ADD</span>
Q ‚Üí tmpAH    CALL FARRET <span>Call Far Return micro-subroutine</span>
IND ‚Üí tmpB               <span>Move SP (in IND) to tmpB for ADD</span>
Œ£ ‚Üí SP       RNI         <span>Put sum in Stack Pointer, end</span>
</pre>

<p>One tricky part is that the <code>FARRET</code> micro-subroutine examines bit 3 of the instruction to determine whether it does a near
return or a far return.
This is why documented instruction <code>C2</code> is a near return and <code>CA</code> is a far return.
Since <code>C0</code> and <code>C8</code> run the same microcode, they will perform the same actions, a near return and a far return respectively.</p>
<h3><code>C1</code>: <code>RET</code></h3>
<p>The undocumented <code>C1</code> opcode is identical to the documented <code>C3</code>, near return instruction.
The microcode below is executed for instruction bits <code>110000?1</code>, i.e. <code>C1</code> and <code>C3</code>.
The first micro-instruction reads from the Stack Pointer, incrementing <code>IND</code> by 2.
Prefetching is suspended and the prefetch queue is flushed, since execution will continue at a new location.
The Program Counter is updated with the value from the stack, read into the <code>OPR</code> register.
Finally, the updated address is put in the Stack Pointer and execution ends.</p>
<pre>SP ‚Üí IND  R SS,P2  <span><b>RET: </b> Read from stack, increment by 2</span>
          SUSP     <span>Suspend prefetching</span>
OPR ‚Üí PC  FLUSH    <span>Update PC from stack, flush prefetch queue</span>
IND ‚Üí SP  RNI      <span>Update SP, run next instruction</span>
</pre>

<h3><code>C9</code>: <code>RET</code></h3>
<p>The undocumented <code>C9</code> opcode is identical to the documented <code>CB</code>, far return instruction.
This microcode is executed for instruction bits <code>110010?1</code>, i.e. <code>C9</code> and <code>CB</code>, so <code>C9</code> is identical to <code>CB</code>.
The microcode below simply calls the <code>FARRET</code> micro-subroutine to pop the Program Counter and CS register.
Then the new value is stored into the Stack Pointer.
One subtlety is that <code>FARRET</code> looks at bit 3 of the instruction to switch between a near return and a far return, as
described earlier.
Since <code>C9</code> and <code>CB</code> both have bit 3 set, they both perform a far return.</p>
<pre>          CALL FARRET  <span><b>RETF:</b> call FARRET routine</span>
IND ‚Üí SP  RNI          <span>Update stack pointer, run next instruction</span>
</pre>

<h3><code>F1</code>: <code>LOCK</code> prefix</h3>
<p>The final hole in the opcode table is <code>F1</code>.
This opcode is different because it is implemented in logic rather than microcode.
The Group Decode ROM indicates that <code>F1</code> is a prefix, one-byte logic, and LOCK.
The Group Decode outputs are the same as <code>F0</code>, so <code>F1</code> also acts as a <code>LOCK</code> prefix.</p>
<h2>Holes in two-byte opcodes</h2>
<p>For most of the 8086 instructions, the first byte specifies the instruction.
However, the 8086 has a few instructions where the second byte specifies the instruction: the <code>reg</code> field of the ModR/M byte provides an opcode extension that selects the instruction.<span id="fnref:extension"><a href="#fn:extension">5</a></span>
These fall into four categories which Intel labeled "Immed", "Shift", "Group 1", and "Group 2", corresponding to opcodes <code>80</code>-<code>83</code>, <code>D0</code>-<code>D3</code>,
<code>F6</code>-<code>F7</code>, and <code>FE</code>-<code>FF</code>.
The table below shows how the second byte selects the instruction.
Note that "Shift", "Group 1", and "Group 2" all have gaps, resulting in undocumented values.</p>
<p><a href="https://static.righto.com/images/8086-ad-undoc/groups.jpg"><img alt="Meaning of the reg field in two-byte opcodes. From MCS-86 Assembly Language Reference Guide." height="129" src="https://static.righto.com/images/8086-ad-undoc/groups-w600.jpg" title="Meaning of the reg field in two-byte opcodes. From MCS-86 Assembly Language Reference Guide." width="600"></a></p>
<p>These sets of instructions are implemented in two completely different ways.
The "Immed" and "Shift" instructions run microcode in the standard way, selected by the first byte.
For a typical arithmetic/logic instruction such as <code>ADD</code>, bits 5-3 of the first instruction byte are latched into the <code>X</code> register to indicate
which ALU operation to perform.
The microcode specifies a generic ALU operation, while the <code>X</code> register controls whether the operation is an <code>ADD</code>, <code>SUB</code>, <code>XOR</code>, or
so forth.
However, the Group Decode ROM indicates that for the special "Immed" and "Shift" instructions, the <code>X</code> register latches the bits
from the <em>second</em> byte.
Thus, when the microcode executes a generic ALU operation, it ends up with the one specified in the second byte.<span id="fnref:alu"><a href="#fn:alu">6</a></span></p>
<p>The "Group 1" and "Group 2" instructions (<code>F0</code>-<code>F1</code>, <code>FE</code>-<code>FF</code>), however, run different microcode for each instruction.
Bits 5-3 of the second byte replace bits 2-0 of the instruction before executing the microcode.
Thus, <code>F0</code> and <code>F1</code> act as if they are opcodes in the range <code>F0</code>-<code>F7</code>, while <code>FE</code> and <code>FF</code> act as if they are opcodes in the range <code>F8</code>-<code>FF</code>.
Thus, each instruction specified by the second byte can have its own microcode, unlike the "Immed" and "Shift" instructions.
The trick that makes this work is that all the "real" opcodes in the range <code>F0</code>-<code>FF</code> are implemented in logic, not microcode,
so there are no collisions.</p>
<h3>The hole in "Shift": <code>SETMO</code>, <code>D0</code>..<code>D3/6</code></h3>
<p>There is a "hole" in the list of shift operations when the second byte has the bits <code>110</code> (6).
(This is typically expressed as <code>D0/6</code> and so forth; the value after the slash is the opcode-selection bits in the ModR/M byte.)
Internally, this value selects the ALU's <code>SETMO</code> (Set Minus One) operation, which simply returns <code>FF</code> or <code>FFFF</code>, for a byte or word operation respectively.<span id="fnref:setmo"><a href="#fn:setmo">7</a></span></p>
<p>The microcode below is executed for 1101000? bit patterns patterns (D0 and D1).
The first instruction gets the value from the <code>M</code> register and sets up the ALU to do whatever operation was
specified in the instruction (indicated by <code>XI</code>).
Thus, the same microcode is used for all the "Shift" instructions, including <code>SETMO</code>.
The result is written back to <code>M</code>. If no writeback to memory is required (<code>NWB</code>), then <code>RNI</code> runs the next instruction, ending
the microcode sequence.
However, if the result is going to memory, then the last line writes the value to memory.</p>
<pre>M ‚Üí tmpB  XI tmpB, NXT  <span><b>rot rm, 1</b>: get argument, set up ALU</span>
Œ£ ‚Üí M     NWB,RNI F     <span>Store result, maybe run next instruction</span>
          W DS,P0 RNI   <span>Write result to memory</span>
</pre>

<p>The D2 and D3 instructions (1101001?) perform a variable number of shifts, specified by the <code>CL</code> register, so they use different microcode (below).
This microcode loops the number of times specified by <code>CL</code>, but the control flow is a bit tricky to avoid shifting if
the intial counter value is 0.
The code sets up the ALU to pass the counter (in <code>tmpA</code>) unmodified the first time (<code>PASS</code>) and jumps to <b>4</b>, which
updates the counter and sets up the ALU for the shift operation (<code>XI</code>).
If the counter is not zero, it jumps back to <b>3</b>, which performs the previously-specified shift and sets up
the ALU to decrement the counter (<code>DEC</code>).
This time, the code at <b>4</b> decrements the counter.
The loop continues until the counter reaches zero. The microcode stores the result as in the previous microcode.</p>
<pre>ZERO ‚Üí tmpA               <span><b>rot rm,CL</b>: 0 to tmpA</span>
CX ‚Üí tmpAL   PASS tmpA    <span>Get count to tmpAL, set up ALU to pass through</span>
M ‚Üí tmpB     JMPS 4       <span>Get value, jump to loop (4)</span>
Œ£ ‚Üí tmpB     DEC tmpA F   <span><b>3</b>: Update result, set up decrement of count</span>
Œ£ ‚Üí tmpA     XI tmpB      <span><b>4</b>: update count in tmpA, set up ALU</span>
             JMPS NZ 3    <span>Loop if count not zero</span>
tmpB ‚Üí M     NWB,RNI      <span>Store result, maybe run next instruction</span>
             W DS,P0 RNI  <span>Write result to memory</span>
</pre>

<h3>The hole in "group 1": <code>TEST</code>, <code>F6/1</code> and <code>F7/1</code></h3>
<p>The <code>F6</code> and <code>F7</code> opcodes are in "group 1", with the specific instruction specified by bits 5-3 of the second byte.
The second-byte table showed a hole for the <code>001</code> bit sequence.
As explained earlier, these bits replace the low-order bits of the instruction, so <code>F6</code> with 001 is processed as if it were
the opcode <code>F1</code>.
The microcode below matches against instruction bits <code>1111000?</code>, so <code>F6/1</code> and <code>F7/1</code> have the same effect as <code>F6/0</code> and <code>F7/1</code> respectively,
that is, the byte and word <code>TEST</code> instructions.</p>
<p>The microcode below gets one or two bytes from the prefetch queue (<code>Q</code>); the <code>L8</code> condition tests if the operation is
an 8-bit (i.e. byte) operation and skips the second micro-instruction.
The third micro-instruction ANDs the argument and the fetched value.
The condition flags (<code>F</code>) are set based on the result, but the result itself is discarded.
Thus, the <code>TEST</code> instruction tests a value against a mask, seeing if any bits are set.</p>
<pre>Q ‚Üí tmpBL    JMPS L8 2     <span><b>TEST rm,i:</b> Get byte, jump if operation length = 8</span>
Q ‚Üí tmpBH                  <span>Get second byte from the prefetch queue</span>
M ‚Üí tmpA     AND tmpA, NXT <span><b>2:</b> Get argument, AND with fetched value</span>
Œ£ ‚Üí no dest  RNI F         <span>Discard result but set flags.</span>
</pre>

<p>I explained the processing of these "Group 3" instructions in more detail in my <a href="https://www.righto.com/2022/11/how-8086-processors-microcode-engine.html">microcode article</a>.</p>
<h3>The hole in "group 2": <code>PUSH</code>, <code>FE/7</code> and <code>FF/7</code></h3>
<p>The <code>FE</code> and <code>FF</code> opcodes are in "group 2", which has a hole for the <code>111</code> bit sequence in the second byte.
After replacement, this will be processed as the <code>FF</code> opcode, which matches the pattern <code>1111111?</code>.
In other words, the instruction will be processed the same as the <code>110</code> bit pattern, which is <code>PUSH</code>.
The microcode gets the Stack Pointer, sets up the ALU to decrement it by 2.
The new value is written to <code>SP</code> and <code>IND</code>. Finally, the register value is written to stack memory.</p>
<!--
For some reason, the [8086 undocumented instructions](https://en.wikipedia.org/wiki/X86_instruction_listings#Undocumented_x86_instructions) page on Wikipedia doesn't list FE-FF.
-->

<pre>SP ‚Üí tmpA  DEC2 tmpA   <span><b>PUSH rm</b>: set up decrement SP by 2</span>
Œ£ ‚Üí IND                <span>Decremented SP to IND</span>
Œ£ ‚Üí SP                 <span>Decremented SP to SP</span>
M ‚Üí OPR    W SS,P0 RNI <span>Write the value to memory, done</span>
</pre>

<h3><code>82</code> and <code>83</code> "Immed" group</h3>
<p>Opcodes <code>80</code>-<code>83</code> are the "Immed" group, performing one of eight arithmetic operations, specified in the ModR/M byte.
The four opcodes differ in the size of the values: opcode <code>80</code> applies an 8-bit immediate value to an 8-bit register, <code>81</code> applies a 16-bit
value to a 16-bit register, <code>82</code> applies an 8-bit value to an 8-bit register, and <code>83</code> applies an 8-bit value to a 16-bit register.
The opcode 82 has the strange situation that <a href="https://en.wikipedia.org/wiki/X86_instruction_listings#Undocumented_instructions_that_are_widely_available_across_many_x86_CPUs_include">some sources</a> say it is undocumented, but it shows up in some Intel documentation as a valid bit combination (e.g. below).
Note that <code>80</code> and <code>82</code> have the  8-bit to 8-bit action, so the <code>82</code> opcode is redundant.</p>
<p><a href="https://static.righto.com/images/8086-ad-undoc/adc.png"><img alt="ADC is one of the instructions with opcode 80-83. From the 8086 datasheet, page 27." height="34" src="https://static.righto.com/images/8086-ad-undoc/adc-w600.png" title="ADC is one of the instructions with opcode 80-83. From the 8086 datasheet, page 27." width="600"></a></p><p>ADC is one of the instructions with opcode 80-83. From the <a href="https://www.electro-tech-online.com/datasheets/8086_intel.pdf">8086 datasheet</a>, page 27.</p>
<p>The microcode below is used for all four opcodes.
If the ModR/M byte specifies memory, the appropriate micro-subroutine is called to compute the effective address in <code>IND</code>,
and fetch the byte or word into <code>OPR</code>.
The first two instructions below get the two immediate data bytes from the prefetch queue; for an 8-bit operation, the second byte
is skipped.
Next, the second argument <code>M</code> is loaded into tmpA and the desired ALU operation (<code>XI</code>) is configured.
The result <code>Œ£</code> is stored into the specified register <code>M</code> and the operation may terminate with <code>RNI</code>.
But if the ModR/M byte specified memory, the following write micro-operation saves the value to memory.</p>
<pre>Q ‚Üí tmpBL  JMPS L8 2    <span><b>alu rm,i</b>: get byte, test if 8-bit op</span>
Q ‚Üí tmpBH               <span>Maybe get second byte</span>
M ‚Üí tmpA   XI tmpA, NXT <span><b>2</b>: </span>
Œ£ ‚Üí M      NWB,RNI F    <span>Save result, update flags, done if no memory writeback</span>
           W DS,P0 RNI  <span>Write result to memory if needed</span>
</pre>

<p>The tricky part of this is the <code>L8</code> condition, which tests if the operation is 8-bit.
You might think that bit 0 acts as the byte/word bit in a nice, orthogonal way, but the 8086 has a bunch of special cases.
Bit 0 of the instruction typically selects between a byte and a word operation, but there are a bunch of special cases.
The Group Decode ROM creates a signal indicating if bit 0 should be used as the byte/word bit.
But it generates a second signal indicating that an instruction should be forced to operate on bytes, for instructions
such as <code>DAA</code> and <code>XLAT</code>.
Another Group Decode ROM signal indicates that bit 3 of the instruction should select byte or word; this
is used for the <code>MOV</code> instructions with opcodes Bx.
Yet another Group Decode ROM signal indicates that inverted bit 1 of the instruction should select byte or word;
this is used for a few opcodes, including <code>80</code>-<code>87</code>.</p>
<p>The important thing here is that for the opcodes under discussion (<code>80</code>-<code>83</code>), the <code>L8</code> micro-condition uses <em>both</em> bits 0 and 1
to determine if the instruction is 8 bits or not.
The result is that only opcode <code>81</code> is considered 16-bit by the <code>L8</code> test, so it is the only one that uses two immediate bytes
from the instruction.
However, the register operations use only bit 0 to select a byte or word transfer.
The result is that opcode <code>83</code> has the unusual behavior of using an 8-bit immediate operand with a 16-bit register.
In this case, the 8-bit value is sign-extended to form a 16-bit value. That is, the top bit of the 8-bit value fills
the entire upper half of the 16-bit value,
converting an 8-bit signed value to a 16-bit signed value (e.g. -1 is <code>FF</code>, which becomes <code>FFFF</code>).
This makes sense for arithmetic operations, but not much sense for logical operations.</p>
<p>Intel documentation is inconsistent about which opcodes are listed for which instructions.
Intel opcode maps generally define opcodes <code>80</code>-<code>83</code>.
However, lists of specific instructions show opcodes <code>80</code>, <code>81</code>, and <code>83</code> for arithmetic operations but only <code>80</code> and <code>81</code> for logical operations.<span id="fnref:immed"><a href="#fn:immed">8</a></span>
That is, Intel omits the redundant <code>82</code> opcode as well as omitting logic operations that perform sign-extension (<code>83</code>).</p>
<h3>More <code>FE</code> holes</h3>
<p>For the "group 2" instructions, the <code>FE</code> opcode performs a byte operation while <code>FF</code> performs a word operation.
Many of these operations don't make sense for bytes: <code>CALL</code>, <code>JMP</code>, and <code>PUSH</code>.
(The only instructions supported for <code>FE</code> are <code>INC</code> and <code>DEC</code>.) But what happens if you use the unsupported instructions?
The remainder of this section examines those cases and shows that the results are not useful.</p>
<h4><code>CALL</code>: <code>FE/2</code></h4>
<p>This instruction performs an indirect subroutine call within a segment, reading the target address from the memory location specified by the ModR/M byte.</p>
<p>The microcode below is a bit convoluted because the code falls through into the shared <code>NEARCALL</code> routine, so there is
some unnecessary register movement.
Before this microcode executes, the appropriate ModR/M micro-subroutine will read the target address from memory.
The code below copies the destination address from <code>M</code> to <code>tmpB</code> and stores it into the PC later in the code
to transfer execution.
The code suspends prefetching, corrects the PC to cancel the offset from prefetching, and flushes the prefetch queue.
Finally, it decrements the SP by two and writes the old PC to the stack.</p>
<pre>M ‚Üí tmpB    SUSP        <span><b>CALL rm</b>: read value, suspend prefetch</span>
SP ‚Üí IND    CORR        <span>Get SP, correct PC</span>
PC ‚Üí OPR    DEC2 tmpC   <span>Get PC to write, set up decrement</span>
tmpB ‚Üí PC   FLUSH       <span><b>NEARCALL</b>: Update PC, flush prefetch</span>
IND ‚Üí tmpC              <span>Get SP to decrement</span>
Œ£ ‚Üí IND                 <span>Decremented SP to IND</span>
Œ£ ‚Üí SP      W SS,P0 RNI <span>Update SP, write old PC to stack</span>
</pre>

<p>This code will mess up in two ways when executed as a byte instruction.
First, when the destination address is read from memory, only a byte will be read, so the destination address will be corrupted.
(I think that the behavior here depends on the bus hardware. The 8086 will ask for a byte from memory but will
read the word that is placed on the bus.
Thus, if memory returns a word, this part may operate correctly.
The 8088's behavior will be different because of its 8-bit bus.)
The second issue is writing the old PC to the stack because only a byte of the PC will be written.
Thus, when the code returns from the subroutine call, the return address will be corrupt.</p>
<h4><code>CALL</code>: <code>FE/3</code></h4>
<p>This instruction performs an indirect subroutine call between segments, reading the target address from the memory location specified by the ModR/M byte.</p>
<pre>IND ‚Üí tmpC  INC2 tmpC    <span><b>CALL FAR rm</b>: set up IND+2</span>
Œ£ ‚Üí IND     R DS,P0      <span>Read new CS, update IND</span>
OPR ‚Üí tmpA  DEC2 tmpC    <span>New CS to tmpA, set up SP-2</span>
SP ‚Üí tmpC   SUSP         <span><b>FARCALL</b>: Suspend prefetch</span>
Œ£ ‚Üí IND     CORR         <span><b>FARCALL2</b>: Update IND, correct PC</span>
CS ‚Üí OPR    W SS,M2      <span>Push old CS, decrement IND by 2</span>
tmpA ‚Üí CS   PASS tmpC    <span>Update CS, set up for NEARCALL</span>
PC ‚Üí OPR    JMP NEARCALL <span>Continue with NEARCALL</span>
</pre>

<p>As in the previous <code>CALL</code>, this microcode will fail in multiple ways when executed in byte mode.
The new CS and PC addresses will be read from memory as bytes, which may or may not work.
Only a byte of the old CS and PC will be pushed to the stack.</p>
<h4><code>JMP</code>: <code>FE/4</code></h4>
<p>This instruction performs an indirect jump within a segment, reading the target address from the memory location specified by the ModR/M byte.
The microcode is short, since the ModR/M micro-subroutine does most of the work.
I believe this will have the same problem as the previous <code>CALL</code> instructions, that it will attempt to read a byte from
memory instead of a word.</p>
<pre>        SUSP       <span><b>JMP rm</b>: Suspend prefetch</span>
M ‚Üí PC  FLUSH RNI  <span>Update PC with new address, flush prefetch, done</span>
</pre>

<h4><code>JMP</code>: <code>FE/5</code></h4>
<p>This instruction performs an indirect jump between segments, reading the new PC and CS values from the memory location specified by the ModR/M byte.
The ModR/M micro-subroutine reads the new PC address. This microcode increments <code>IND</code> and suspends prefetching.
It updates the PC, reads the new CS value from memory, and updates the CS.
As before, the reads from memory will read bytes instead of words, so this code will not meaningfully work in byte mode.</p>
<pre>IND ‚Üí tmpC  INC2 tmpC   <span><b>JMP FAR rm</b>: set up IND+2</span>
Œ£ ‚Üí IND     SUSP        <span>Update IND, suspend prefetch</span>
tmpB ‚Üí PC   R DS,P0     <span>Update PC, read new CS from memory</span>
OPR ‚Üí CS    FLUSH RNI   <span>Update CS, flush prefetch, done</span>
</pre>

<h4><code>PUSH</code>: <code>FE/6</code></h4>
<p>This instruction pushes the register or memory value specified by the ModR/M byte.
It decrements the SP by 2 and then writes the value to the stack.
It will write one byte to the stack but decrements the SP by 2,
so one byte of old stack data will be on the stack along with the data byte.</p>
<pre>SP ‚Üí tmpA  DEC2 tmpA    <span><b>PUSH rm</b>: Set up SP decrement </span>
Œ£ ‚Üí IND                 <span>Decremented value to IND</span>
Œ£ ‚Üí SP                  <span>Decremented value to SP</span>
M ‚Üí OPR    W SS,P0 RNI  <span>Write the data to the stack</span>
</pre>

<h2>Undocumented instruction values</h2>
<p>The next category of undocumented instructions is where the first byte indicates a valid instruction, but
there is something wrong with the second byte.</p>
<h3><code>AAM</code>: ASCII Adjust after Multiply</h3>
<p>The <code>AAM</code> instruction is a fairly obscure one, designed to support binary-coded decimal
arithmetic (BCD).
After multiplying two BCD digits, you end up with a binary value between 0 and <code>81</code> (0√ó0 to 9√ó9).
If you want a BCD result, the <code>AAM</code> instruction converts this binary value to BCD, for instance splitting <code>81</code> into the
decimal digits 8 and 1, where the upper digit is <code>81</code> divided by <code>10</code>, and the lower digit is <code>81</code> modulo <code>10</code>.</p>
<p>The interesting thing about <code>AAM</code> is that the 2-byte instruction is <code>D4</code> <code>0A</code>. You might notice that hex <code>0A</code> is <code>10</code>, and this
is not a coincidence.
There wasn't an easy way to get the value <code>10</code> in the microcode, so instead they made the instruction
provide that value in the second byte.
The undocumented (but well-known) part is that if you provide a value other than <code>10</code>, the instruction will convert the binary input into
digits in that base. For example, if you provide 8 as the second byte, the instruction returns the value divided by 8
and the value modulo 8.</p>
<p>The microcode for <code>AAM</code>, below, sets up the registers.  calls
the <code>CORD</code> (Core Division) micro-subroutine to perform the division,
and then puts the results into <code>AH</code> and <code>AL</code>.
In more detail, the <code>CORD</code> routine divides <code>tmpA/tmpC</code> by <code>tmpB</code>, putting the <em>complement</em> of the quotient in <code>tmpC</code> and leaving the remainder in <code>tmpA</code>.
(If you want to know how CORD works internally, see my <a href="https://www.righto.com/2023/04/reverse-engineering-8086-divide-microcode.html">division post</a>.)
The important step is that the <code>AAM</code> microcode gets the divisor from the prefetch queue (<code>Q</code>).
After calling <code>CORD</code>, it sets up the ALU to perform a 1's complement of <code>tmpC</code> and puts the result (<code>Œ£</code>) into <code>AH</code>.
It sets up the ALU to pass <code>tmpA</code> through unchanged, puts the result (<code>Œ£</code>) into <code>AL</code>, and updates the flags accordingly (<code>F</code>).</p>
<pre>Q ‚Üí tmpB                    <span><b>AAM:</b> Move byte from prefetch to tmpB</span>
ZERO ‚Üí tmpA                 <span>Move 0 to tmpA</span>
AL ‚Üí tmpC    CALL CORD      <span>Move AL to tmpC, call CORD.</span>
             COM1 tmpC      <span>Set ALU to complement</span>
Œ£ ‚Üí AH       PASS tmpA, NXT <span>Complement AL to AH</span>
Œ£ ‚Üí AL       RNI F          <span>Pass tmpA through ALU to set flags</span>
</pre>

<p>The interesting thing is why this code has undocumented behavior.
The 8086's microcode only has support for the constants 0 and all-1's (<code>FF</code> or <code>FFFF</code>), but the microcode needs to divide by <code>10</code>.
One solution would be to implement an additional micro-instruction and more circuitry to provide the constant <code>10</code>, but every
transistor was precious back then.
Instead, the designers took the approach of simply putting the number <code>10</code> as the second byte of the instruction and loading the
constant from there.
Since the <code>AAM</code> instruction is not used very much, making the instruction two bytes long wasn't much of a drawback.
But if you put a different number in the second byte, that's the divisor the microcode will use.
(Of course you could add circuitry to verify that the number is <code>10</code>, but then the implementation is no longer simple.)</p>
<p>Intel could have documented the full behavior, but that creates several problems.
First, Intel would be stuck supporting the full behavior into the future.
Second, there are corner cases to deal with, such as divide-by-zero.
Third, testing the chip would become harder because all these cases would need to be tested.
Fourth, the documentation would become long and confusing.
It's not surprising that Intel left the full behavior undocumented.</p>
<h3><code>AAD</code>: ASCII Adjust before Division</h3>
<p>The <code>AAD</code> instruction is analogous to <code>AAM</code> but used for BCD division.
In this case, you want to divide a two-digit BCD number by something, where the BCD digits are in <code>AH</code> and <code>AL</code>.
The <code>AAD</code> instruction converts the two-digit BCD number to binary by computing <code>AH</code>√ó<code>10+AL</code>, before you perform
the division.</p>
<p>The microcode for <code>AAD</code> is shown below. The microcode sets up the registers, calls the multiplication micro-subroutine
<code>CORX</code> (Core Times), and
then puts the results in <code>AH</code> and <code>AL</code>.
In more detail, the multiplier comes from the instruction prefetch queue <code>Q</code>.
The <code>CORX</code> routine multiples <code>tmpC</code> by <code>tmpB</code>, putting the result in <code>tmpA/tmpC</code>.
Then the microcode adds the low BCD digit (<code>AL</code>) to the product (<code>tmpB + tmpC</code>), putting the sum (<code>Œ£</code>) into <code>AL</code>,
clearing <code>AH</code> and setting the status flags <code>F</code> appropriately.</p>
<p>One interesting thing is that the second-last micro-instruction jumps to <code>AAEND</code>, which is the last
micro-instruction of the <code>AAM</code> microcode above.
By reusing the micro-instruction from <code>AAM</code>, the microcode is one micro-instruction shorter, but
the jump adds one cycle to the execution time.
(The CORX routine is used for integer multiplication; I discuss the internals in <a href="https://www.righto.com/2023/03/8086-multiplication-microcode.html">this post</a>.)</p>
<pre>Q ‚Üí tmpC              <span><b>AAD:</b> Get byte from prefetch queue.</span>
AH ‚Üí tmpB   CALL CORX <span>Call CORX</span>
AL ‚Üí tmpB   ADD tmpC  <span>Set ALU for ADD</span>
ZERO ‚Üí AH   JMP AAEND <span>Zero AH, jump to AAEND</span>
i
...
Œ£ ‚Üí AL      RNI F     <span><b>AAEND:</b> Sum to AL, done.</span>
</pre>

<p>As with <code>AAM</code>, the constant <code>10</code> is provided in the second byte of the instruction.
The microcode accepts any value here, but values other than <code>10</code> are undocumented.</p>
<h3><code>8C</code>, <code>8E</code>: MOV sr</h3>
<p>The opcodes <code>8C</code> and <code>8E</code> perform a <code>MOV</code> register to or from the specified segment register, using the register specification
field in the ModR/M byte.
There are four segment registers and three selection bits, so an invalid segment register can be specified.
However, the hardware that decodes the register number ignores instruction bit 5 for a segment register. Thus,
specifying a segment register 4 to 7 is the same as specifying a segment register 0 to 3.
For more details, see my article on <a href="https://www.righto.com/2023/03/8086-register-codes.html">8086 register codes</a>.</p>
<h2>Unexpected <code>REP</code> prefix</h2>
<h3><code>REP IMUL</code> / <code>IDIV</code></h3>
<p>The <code>REP</code> prefix is used with string operations to cause the operation to be repeated across a block of memory.
However, if you use this prefix with an <code>IMUL</code> or <code>IDIV</code> instruction, it has the unexpected behavior
of negating the product or the quotient (<a href="https://www.reenigne.org/blog/8086-microcode-disassembled/">source</a>).</p>
<p>The reason for this behavior is that the string operations use an internal flag called <code>F1</code> to indicate that a <code>REP</code>
prefix has been applied.
The multiply and divide code reuses this flag to track the sign of the input values, toggling <code>F1</code> for each negative value.
If <code>F1</code> is set, the value at the end is negated. (This handles "two negatives make a positive.")
The consequence is that the <code>REP</code> prefix puts the flag in the 1 state when the multiply/divide starts, so the computed sign
will be wrong at the end and the result is the negative of the expected result.
The microcode is fairly complex, so I won't show it here; I explain it in detail in <a href="https://www.righto.com/2023/03/8086-multiplication-microcode.html">this blog post</a>.</p>
<h3><code>REP RET</code></h3>
<p><a href="https://en.wikipedia.org/wiki/X86_instruction_listings#Undocumented_x86_instructions">Wikipedia</a> lists
<code>REP RET</code> (i.e. <code>RET</code> with a <code>REP</code> prefix) as a way to implement a two-byte return instruction.
This is kind of trivial; the <code>RET</code> microcode (like almost every instruction) doesn't use the <code>F1</code> internal flag,
so the <code>REP</code> prefix has no effect.</p>
<h3><code>REPNZ MOVS/STOS</code></h3>
<p><a href="https://en.wikipedia.org/wiki/X86_instruction_listings#Undocumented_x86_instructions">Wikipedia</a> mentions that
the use of the <code>REPNZ</code> prefix (as opposed to <code>REPZ</code>) is undefined with string operations other than <code>CMPS/SCAS</code>.
An internal flag called <code>F1Z</code> distinguishes between the <code>REPZ</code> and <code>REPNZ</code> prefixes.
This flag is only used by <code>CMPS/SCAS</code>. Since the other string instructions ignore this flag, they will ignore the
difference between <code>REPZ</code> and <code>REPNZ</code>.
I wrote about string operations in more detail in <a href="https://www.righto.com/2023/04/8086-microcode-string-operations.html">this post</a>.</p>
<h2>Using a register instead of memory.</h2>
<p>Some instructions are documented as requiring a memory operand. However, the ModR/M byte can specify a register.
The behavior in these cases can be highly unusual, providing access to hidden registers.
Examining the microcode shows how this happens.</p>
<h3><code>LEA reg</code></h3>
<p>Many instructions have a ModR/M byte that indicates the memory address that the instruction should use, perhaps through
a complicated addressing mode.
The <code>LEA</code> (Load Effective Address) instruction is different: it doesn't access the memory location but returns the address itself.
The undocumented part is that the ModR/M byte can specify a register instead of a memory location. In that case,
what does the <code>LEA</code> instruction do? Obviously it can't return the address of a register, but it needs to return something.</p>
<p>The behavior of <code>LEA</code> is explained by how the 8086 handles the ModR/M byte.
Before running the microcode corresponding to the instruction, the microcode engine calls a short micro-subroutine
for the particular addressing mode.
This micro-subroutine puts the desired memory address (the effective address) into the <code>tmpA</code> register.
The effective address is copied to the <code>IND</code> (Indirect) register and the value is loaded from memory if needed.
On the other hand, if the ModR/M byte specified a register instead of memory, no micro-subroutine is called.
(I explain ModR/M handling in more detail in <a href="https://www.righto.com/2023/02/8086-modrm-addressing.html">this article</a>.)</p>
<p>The microcode for <code>LEA</code> itself is just one line. It stores the effective address in the <code>IND</code> register into the specified destination register, indicated by <code>N</code>.
This assumes that the appropriate ModR/M micro-subroutine was called before this code, putting the effective address into <code>IND</code>.</p>
<pre>IND ‚Üí N   RNI  <span><b>LEA</b>: store IND register in destination, done</span>
</pre>

<p>But if a register was specified instead of a memory location, no ModR/M micro-subroutine gets called.
Instead, the <code>LEA</code> instruction will return whatever value was left
in <code>IND</code> from before, typically the previous memory location that was accessed.
Thus, <code>LEA</code> can be used to read the value of the <code>IND</code> register, which is normally hidden from the programmer.</p>
<h3><code>LDS reg</code>, <code>LES reg</code></h3>
<p>The <code>LDS</code> and <code>LES</code> instructions load a far pointer from memory into the specified segment register and general-purpose register.
The microcode below assumes that the appropriate ModR/M micro-subroutine has set up <code>IND</code> and read the first value into <code>OPR</code>.
The microcode updates the destination register, increments <code>IND</code> by 2, reads the next value, and updates <code>DS</code>.
(The microcode for <code>LES</code> is a copy of this, but updates <code>ES</code>.)</p>
<pre>OPR ‚Üí N               <span><b>LDS</b>: Copy OPR to dest register</span>
IND ‚Üí tmpC  INC2 tmpC <span>Set up incrementing IND by 2</span>
Œ£ ‚Üí IND     R DS,P0   <span>Update IND, read next location</span>
OPR ‚Üí DS    RNI       <span>Update DS</span>
</pre>

<p>If the <code>LDS</code> instruction specifies a register instead of memory, a micro-subroutine will not be called, so <code>IND</code> and <code>OPR</code>
will have values from a previous instruction.
<code>OPR</code> will be stored in the destination register, while the <code>DS</code> value will be read from the address <code>IND+2</code>.
Thus, these instructions provide a mechanism to access the hidden <code>OPR</code> register.</p>
<h3><code>JMP FAR rm</code></h3>
<p>The <code>JMP FAR rm</code> instruction normally jumps to the far address stored in memory at the location indicated by the ModR/M byte.
(That is, the ModR/M byte indicates where the new PC and CS values are stored.)
But, as with <code>LEA</code>, the behavior is undocumented if the ModR/M byte specifies a register, since a register doesn't hold
a four-byte value.</p>
<p>The microcode explains what happens.
As with <code>LEA</code>, the code expects a micro-subroutine to put the address into the <code>IND</code> register.
In this case, the micro-subroutine also loads the value at that address (i.e. the destination <code>PC</code>) into tmpB.
The microcode increments <code>IND</code> by 2 to point to the <code>CS</code> word in memory and reads that into <code>CS</code>.
Meanwhile, it updates the <code>PC</code> with <code>tmpB</code>.
It suspends prefetching and flushes the queue, so instruction fetching will restart at the new address.</p>
<pre>IND ‚Üí tmpC  INC2 tmpC   <span><b>JMP FAR rm</b>: set up to add 2 to IND</span>
Œ£ ‚Üí IND     SUSP        <span>Update IND, suspend prefetching</span>
tmpB ‚Üí PC   R DS,P0     <span>Update PC with tmpB. Read new CS from specified address</span>
OPR ‚Üí CS    FLUSH RNI   <span>Update CS, flush queue, done</span>
</pre>

<p>If you specify a register instead of memory, the micro-subroutine won't get called.
Instead, the program counter will be loaded with whatever value was in <code>tmpB</code> and the <code>CS</code> segment register will
be loaded from the memory location two bytes after the location that <code>IND</code> was referencing.
Thus, this undocumented use of the instruction gives access to the otherwise-hidden <code>tmpB</code> register.</p>

<p>Microprocessor manufacturers soon realized that undocumented instructions were a problem, since
programmers find them and often use them.
This creates an issue for future processors, or even revisions of the current processor:
if you eliminate an undocumented instruction, previously-working code that used the instruction will break,
and it will seem like the new processor is faulty.</p>
<p>The solution was for processors to detect undocumented instructions and prevent them from executing.
By the early 1980s, processors had enough transistors (thanks to Moore's law) that they could include
the circuitry to block unsupported instructions.
In particular, the 80186/80188 and the 80286 generated a trap of type 6 when an unused opcode was executed,
blocking use of the instruction.<span id="fnref:186"><a href="#fn:186">9</a></span>
This trap is also known as #UD (Undefined instruction trap).<span id="fnref:fault"><a href="#fn:fault">10</a></span></p>
<h2>Conclusions</h2>
<p>The 8086, like many early microprocessors, has undocumented instructions but no traps to stop them from executing.<span id="fnref:references"><a href="#fn:references">11</a></span>
For the 8086, these fall into several categories.
Many undocumented instructions simply mirror existing instructions.
Some instructions are implemented but not documented for one reason or another, such as <code>SALC</code> and <code>POP CS</code>.
Other instructions can be used outside their normal range, such as <code>AAM</code> and <code>AAD</code>.
Some instructions are intended to work only with a memory address, so specifying a register can have
strange effects such as revealing the values of the hidden <code>IND</code> and <code>OPR</code> registers.</p>
<p>Keep in mind that my analysis is based on transistor-level simulation and examining the microcode; I haven't verified the behavior on a
physical 8086 processor. Please let me know if you see any errors in my analysis or undocumented instructions that I have
overlooked.
Also note that the behavior could change between different versions of the 8086; in particular, some versions by different manufacturers
(such as the NEC V20 and V30) are known to be different.</p>
<p>I plan to write more about the 8086, so
follow me on Twitter <a href="https://twitter.com/kenshirriff">@kenshirriff</a> or <a href="https://www.righto.com/feeds/posts/default">RSS</a> for updates.
I've also started experimenting with Mastodon recently as <a href="https://oldbytes.space/@kenshirriff">@<span data-cfemail="fe959b908d96978c8c979898be91929a9c878a9b8dd08d8e9f9d9b">[email&nbsp;protected]</span></a>
and Bluesky as <a href="https://staging.bsky.app/profile/righto.com">@righto.com</a> so you can follow me there too.</p>
<h2>Notes and references</h2>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grav is a modern open-source flat-file CMS (120 pts)]]></title>
            <link>https://getgrav.org/</link>
            <guid>36751375</guid>
            <pubDate>Sun, 16 Jul 2023 21:04:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://getgrav.org/">https://getgrav.org/</a>, See on <a href="https://news.ycombinator.com/item?id=36751375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">

<div id="modular-choose">
<div data-animation="fadeIn" data-timeout="200">
<h2>Why Choose Grav?</h2>
<div id="cms-critic">

<p><a href="https://getgrav.org/blog/cms-critic-award-2016"><img alt="Best Open Source CMS Award Badge" src="https://getgrav.org/user/themes/planetoid/images/best-open-source-cms.png"></a>
<a href="https://www.cmscritic.com/cms-critic-awards/"><img alt="Best Flat File CMS Award Badge" src="https://getgrav.org/user/themes/planetoid/images/best-flat-file-cms.png"></a>
</p>

</div>
</div>
<div>
<div data-animation="fadeIn" data-timeout="500">
<div>
<p><img alt="fast Icon" src="https://getgrav.org/user/themes/planetoid/images/fast.svg">
<span>Fast</span>
</p>
</div>
<p>Performance is not just an afterthought, we baked it in from the start</p>
</div>
<div data-animation="fadeIn" data-timeout="900">
<div>
<p><img alt="lego Icon" src="https://getgrav.org/user/themes/planetoid/images/lego.svg">
 <span>Extensible</span>
</p>
</div>
<p>Grav has a powerful API and sophisticated Package Manager to make it super flexible</p>
</div>
<div data-animation="fadeIn" data-timeout="1300">
<div>
<p><img alt="github Icon" src="https://getgrav.org/user/themes/planetoid/images/github.svg">
<span>Open Source</span>
</p>
</div>
<p>Grav is Open Source, and all the code is available on <a href="https://github.com/getgrav/grav">GitHub.com</a></p>
</div>
</div>
</div>

<div id="modular-easy-to-use">

<div data-animation="fadeIn" data-timeout="200">
<h2>Super Easy to Use</h2>
<p>The Grav admin plugin provides a simple and intuitive interface to make configuration and content creation easy and enjoyable.</p>
<p><a href="https://getgrav.org/downloads/plugins">
Get Admin Plugin
</a></p>
</div>
<div>
<div>
<p>The Grav Admin dashboard provides a quick glance at your site state</p>
<p><img data-src="/user/pages/01.tour/_easy-to-use/001-dashboard.png?g-283e9f04" alt="001-dashboard.png" src="https://getgrav.org/001-dashboard.png">
</p></div>
<div>
<p>Easily modify Grav's flexible configuration settings with its advanced forms</p>
<p><img data-src="/user/pages/01.tour/_easy-to-use/002-config.png?g-283e9f04" alt="002-config.png" src="https://getgrav.org/002-config.png">
</p></div>
<div>
<p>Editing content is a breeze with Grav's powerful markdown editor</p>
<p><img data-src="/user/pages/01.tour/_easy-to-use/003-editpage.png?g-283e9f04" alt="003-editpage.png" src="https://getgrav.org/003-editpage.png">
</p></div>
<div>
<p>Changing your administrator information is quick and easy</p>
<p><img data-src="/user/pages/01.tour/_easy-to-use/004-user.png?g-283e9f04" alt="004-user.png" src="https://getgrav.org/004-user.png">
</p></div>
<div>
<p>Grav has a built in package manager with one-click install for plugins</p>
<p><img data-src="/user/pages/01.tour/_easy-to-use/005-plugins.png?g-283e9f04" alt="005-plugins.png" src="https://getgrav.org/005-plugins.png">
</p></div>
<div>
<p>You can also install a wide range of modern themes with one-click</p>
<p><img data-src="/user/pages/01.tour/_easy-to-use/006-themes.png?g-283e9f04" alt="006-themes.png" src="https://getgrav.org/006-themes.png">
</p></div>
</div>
</div>

<div data-animation="fadeIn" data-timeout="200" id="modular-developers">
<p><img alt="Shield Icon" src="https://getgrav.org/user/themes/planetoid/images/shield.svg"></p><h2>be a hero developer</h2>
<p>Packed to the gills with amazing features and tools, coupled with heaps of detailed documentation, Grav will make you look like a hero developer!</p>
<p><a href="http://learn.getgrav.org/">
Read the Documentation
</a></p>
</div>

<div data-animation="fadeIn" data-timeout="200" id="modular-features">
<h2>a bevy of features</h2>
<ul>
<li data-animation="fadeIn" data-timeout="250">
<p><img alt="content-creation.png" src="https://getgrav.org/user/pages/01.tour/_features/content-creation.png?g-283e9f04" width="400" height="400">
</p>
<div>
<h4>Enjoyable Content Creation</h4>
<p>Use your favorite Markdown editor to create your content online or offline</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="450">
<p><img alt="gpm.png" src="https://getgrav.org/user/pages/01.tour/_features/gpm.png?g-283e9f04" width="400" height="400">
</p>
<div>
<h4>One-Click Installs</h4>
<p>The built-in Package Manager lets you to find, install, and easily update extensions and themes for Grav</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="650">
<p><img alt="caching.png" src="https://getgrav.org/user/pages/01.tour/_features/caching.png?g-283e9f04" width="400" height="400">
</p>
<div>
<h4>Stellar Performance</h4>
<p>Grav intelligently caches content to deliver great performance, regardless of hosting</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="850">
<p><img alt="taxonomy.png" src="https://getgrav.org/user/pages/01.tour/_features/taxonomy.png?g-283e9f04" width="400" height="400">
</p>
<div>
<h4>Powerful Content Filtering</h4>
<p>Create unlimited taxonomies such as tags, categories, and authors to filter and manage your content</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="1050">
<p><img alt="frontmatter.png" src="https://getgrav.org/user/pages/01.tour/_features/frontmatter.png?g-283e9f04" width="400" height="400">
</p>
<div>
<h4>Dynamic Content Types</h4>
<p>The flat-file nature of Grav lets you define custom fields for any of your pages, including modular content</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="1250">
<p><img alt="multilang.png" src="https://getgrav.org/user/pages/01.tour/_features/multilang.png?g-283e9f04" width="400" height="400">
</p>
<div>
<h4>Multi-Language Support</h4>
<p>A simple mechanism for presenting sites in multiple languages is built into Grav</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="1450">
<p><img alt="backup.png" src="https://getgrav.org/user/pages/01.tour/_features/backup.png?g-283e9f04" width="353" height="353">
</p>
<div>
<h4>Simple Backups and Restores</h4>
<p>Being file based means backing up and restoring your data is super easy, and changing hosts/servers is a breeze!</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="1650">
<p><img alt="tiger.jpg" src="https://getgrav.org/user/pages/01.tour/_features/tiger.jpg?g-283e9f04" width="386" height="386">
</p>
<div>
<h4>Image Media Processing</h4>
<p>Dynamic image manipulation to resize, crop, resample, and effects all with automatic caching of images</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="1850">
<p><img alt="theme.jpg" src="https://getgrav.org/user/pages/01.tour/_features/theme.jpg?g-283e9f04" width="222" height="222">
</p>
<div>
<h4>Easy Theme Customization</h4>
<p>No need to start from scratch, use Theme Inheritance and then modify the bits you need, allowing for easier update</p>
</div>
</li>
</ul>
<p><a href="https://getgrav.org/features">
Discover all features
</a>
</p></div>

<div data-animation="fadeIn" data-timeout="200" id="modular-pros">
<p><a href="http://trilby.media/" id="trilby-logo" aria-label="Trilby Media"><img alt="Trilby Media" src="https://getgrav.org/user/pages/01.tour/_professional-services/trilby-logo.svg?g-283e9f04"></a></p>
<h2>The Grav Professionals</h2>
<p><a href="http://trilbymedia.com/">Trilby Media</a> is a development company you can turn to for help with your Grav site. Trilby offers a variety of <strong>professional services</strong> and is run by the same team that built Grav in the first place!</p>
<ul>
<li><i></i> Grav consulting services</li>
<li><i></i> Custom Grav plugin development</li>
<li><i></i> Custom Grav theme development</li>
<li><i></i> Existing Grav plugin and theme customization</li>
<li><i></i> Porting to Grav from other platforms</li>
<li><i></i> Grav site design and development</li>
</ul>
<p><a href="http://trilby.media/">
Contact Trilby Media
</a></p>
</div>



<div data-animation="fadeIn" data-timeout="200" id="modular-limitless">
<div>
<h2>There are no limits!</h2>
<p>From simple to sophisticated, Grav has the flexibility to power all sorts of websites.
Flexible content structure and powerful Twig templating allow easy realization of any design.</p>
</div>
<div>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/blog.svg?g-283e9f04" alt="blog.svg">
<figcaption>
<p>Blogs</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/business.svg?g-283e9f04" alt="business.svg">
<figcaption>
<p>Business Sites</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/directory.svg?g-283e9f04" alt="directory.svg">
<figcaption>
<p>Directory</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/documentation.svg?g-283e9f04" alt="documentation.svg">
<figcaption>
<p>Documentation</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/landing-page.svg?g-283e9f04" alt="landing-page.svg">
<figcaption>
<p>Landing Pages</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/portfolio.svg?g-283e9f04" alt="portfolio.svg">
<figcaption>
<p>Portfolios</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/product.svg?g-283e9f04" alt="product.svg">
<figcaption>
<p>Product Sites</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/resume.svg?g-283e9f04" alt="resume.svg">
<figcaption>
<p>Personal Resumes</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/review.svg?g-283e9f04" alt="review.svg">
<figcaption>
<p>E-Commerce</p>
</figcaption>
</figure>
</div>
<p><a href="https://getgrav.org/downloads/skeletons">
See Skeletons
</a></p>
</div>

<div id="modular-built-with">
<h2>Built with Grav</h2>
<p>Grav is a highly versatile platform. Give your creativity wings and Grav will set you free!</p>
</div>

<div id="modular-people-saying">
<div data-animation="fadeIn" data-timeout="200">
<h2>What people are saying</h2>
<p>Still need convincing? Check out what people are saying about Grav. If you have any thoughts or questions, please reach out and ask us.</p>
</div>
<ul id="peoplesaying" data-animation="fadeIn" data-timeout="500">
<li>
<img alt="David Blum" src="https://getgrav.org/images/d/8/f/e/4/d8fe452942f9c971b51c2da92ad58fe7dee6f298-dbloom.jpg?g-283e9f04" width="200" height="200">
<h5>David Blum</h5>
<a href="https://twitter.com/dblO_Om">@dblO_Om</a>
<hr>
<p>@rhuk I just read the complete @getgrav documentation. It is so well written and styled, it's a pleasure. Brilliant!</p>
</li>
<li>
<img alt="Air Petr" src="https://getgrav.org/images/1/6/1/6/f/1616f53bc0c14913b1282e79ac966d152610e326-airpetr.jpg?g-283e9f04" width="200" height="200">
<h5>Air Petr</h5>
<a href="https://twitter.com/air_petr">@air_petr</a>
<hr>
<p>@getgrav Thank you for Grav!!! Dived in for last few days. It's realy spellbinding!</p>
</li>
<li>
<img alt="Bryan Ollendyke" src="https://getgrav.org/images/7/c/2/c/5/7c2c5b24851a5ce5fbba630070a6ed8d70247ab0-btopro.jpg?g-283e9f04" width="200" height="200">
<h5>Bryan Ollendyke</h5>
<a href="https://twitter.com/btopro">@btopro</a>
<hr>
<p>30 seconds after installing @getgrav impression: this is a wordpress killer and we need a site factory for it in @elmsln for portfolios</p>
</li>
<li>
<img alt="Vitor Costa" src="https://getgrav.org/images/c/b/4/2/2/cb422abed3e9fa09607f301058b72cd6710c1083-vmcosta.jpg?g-283e9f04" width="200" height="200">
<h5>Vitor Costa</h5>
<a href="https://twitter.com/vmcosta">@vmcosta</a>
<hr>
<p>I've just found the best flat file CMS, finally‚Ä¶ @getgrav amazing work guys, nicely done, and yes I hate databases too ;)</p>
</li>
<li>
<img alt="Richard Allen" src="https://getgrav.org/images/b/1/6/4/3/b1643fdab2ded2f6f85e1a3926e72f1bda4340be-pmrourkie.jpg?g-283e9f04" width="200" height="200">
<h5>Richard Allen</h5>
<a href="https://twitter.com/pmrourkie">@pmrourkie</a>
<hr>
<p>Absolutely loving @getgrav. Simple, powerful and very fast!</p>
</li>
<li>
<img alt="Ryan Little" src="https://getgrav.org/images/5/7/6/4/5/5764518b67ca205df5da9f8b1accf28d1e2b34f9-ryantereu.jpg?g-283e9f04" width="200" height="200">
<h5>Ryan Little</h5>
<a href="https://twitter.com/ryantereu">@ryantereu</a>
<hr>
<p>Only just began using @getgrav as a cms, but their documentation alone is swoon-worthy: friendly, thorough, well-designed.</p>
</li>
<li>
<img alt="Daniel Kao" src="https://getgrav.org/images/2/2/f/f/d/22ffdaef3925419c80c87fdf90806fee5e4d17fc-diplateevo.jpg?g-283e9f04" width="200" height="200">
<h5>Daniel Kao</h5>
<a href="https://twitter.com/diplateevo">@diplateevo</a>
<hr>
<p>@getgrav Just might be one of the biggest game changers in web CMSs.</p>
</li>
<li>
<img alt="Denis Duvauchelle" src="https://getgrav.org/images/f/e/0/7/c/fe07ce7759371475f20622f7abc3cddcf43be690-desduvauchelle.jpg?g-283e9f04" width="200" height="200">
<h5>Denis Duvauchelle</h5>
<a href="https://twitter.com/desduvauchelle">@desduvauchelle</a>
<hr>
<p>Loving @getgrav over wordpress. If you are developer, you're probably going to fall for it. If your not, you'll probably like it as well.</p>
</li>
<li>
<img alt="Parker Agee" src="https://getgrav.org/images/a/e/a/a/5/aeaa5c5d422874c7a3eaed944d24e8566dd20c1d-parkeragee.jpg?g-283e9f04" width="200" height="200">
<h5>Parker Agee</h5>
<a href="https://twitter.com/parkeragee">@parkeragee</a>
<hr>
<p>Fell in love with the @getgrav and @snipcart combo tonight. Extremely fast set up and works like a charm.</p>
</li>
<li>
<img alt="Casey Grzecka" src="https://getgrav.org/images/d/5/1/e/a/d51ea16a0165d19773ba69f0bb174de430780a17-z3cka.png?g-283e9f04" width="200" height="200">
<h5>Casey Grzecka</h5>
<a href="https://twitter.com/z3cka">@z3cka</a>
<hr>
<p>Grav is legit! I'm ready to leave Drupal in the dust of complexity.</p>
</li>
<li>
<img alt="Harley Hicks" src="https://getgrav.org/images/6/3/4/2/2/63422c586258429ed97e707ed470c2a2fa90f36f-harlshicks.jpg?g-283e9f04" width="200" height="200">
<h5>Harley Hicks</h5>
<a href="https://twitter.com/harlshicks">@harlshicks</a>
<hr>
<p>@getgrav is bringing back my joy for web development and design. So easy to get up and running, and all the puzzle pieces fit so well.</p>
</li>
<li>
<img alt="David Easton" src="https://getgrav.org/images/8/6/1/9/4/8619439d4c7323e62633e939863135cf73489b3e-dweebvid.jpg?g-283e9f04" width="200" height="200">
<h5>David Easton</h5>
<a href="https://twitter.com/dweebvid">@dweebvid</a>
<hr>
<p>Was developing around Wordpress,I wanted to tone down the over-complexity for my needs. I found a flat-file cms called @getGrav and it rocks</p>
</li>
<li>
<img alt="John Williams" src="https://getgrav.org/images/0/5/1/e/6/051e6626984559d0d04747fbdb2815c43e46c27e-thudfactor.jpg?g-283e9f04" width="200" height="200">
<h5>John Williams</h5>
<a href="https://twitter.com/thudfactor">@thudfactor</a>
<hr>
<p>The CMS from @getgrav continues to impress. "What do I need to do to implement open graph metadata? Oh‚Ä¶ nothing."</p>
</li>
<li>
<img alt="Dayle Rees" src="https://getgrav.org/images/c/b/7/7/a/cb77a6ea66f816a13618f278021602ff67f4c01d-daylerees.jpg?g-283e9f04" width="200" height="200">
<h5>Dayle Rees</h5>
<a href="https://twitter.com/daylerees">@daylerees</a>
<hr>
<p>I've just had a play, and it's honestly VERY good. Only a few SF2 components too so it's FAST.</p>
</li>
<li>
<img alt="Serge K. Keller" src="https://getgrav.org/images/5/7/c/9/c/57c9cf7b425a3948bc3a8cfbb7609f9c4ca081d5-citizenk.jpg?g-283e9f04" width="200" height="200">
<h5>Serge K. Keller</h5>
<a href="https://twitter.com/citizenk">@citizenk</a>
<hr>
<p>You know, @getgrav may finally be a CMS that does not weight me down for projects I love. And yes, the pun was definitely intended.</p>
</li>
<li>
<img alt="Stephen Foster" src="https://getgrav.org/images/0/4/d/9/8/04d98ddd24af7e635beafb19e03f71f8ef6da7ce-srfoster.jpg?g-283e9f04" width="200" height="200">
<h5>Stephen Foster</h5>
<a href="https://twitter.com/s_r_foster">@s_r_foster</a>
<hr>
<p>@getgrav Your CMS is the BEST. I was able to make my site in no time flat: http://stephenfoster.us</p>
</li>
<li>
<img alt="Yahya Zini" src="https://getgrav.org/images/6/a/2/0/e/6a20e5684c0cf24cfb769b6426591fc5595510fe-yahyazini.jpg?g-283e9f04" width="200" height="200">
<h5>Yahya Zini</h5>
<a href="https://twitter.com/YahyaZini">@YahyaZini</a>
<hr>
<p>A shout out to the @getgrav team for creating such a great</p>
</li>
<li>
<img alt="Paul Orwig" src="https://getgrav.org/images/2/1/3/7/6/21376785569cd837486cbf928911b30560f60037-porwig.jpg?g-283e9f04" width="200" height="200">
<h5>Paul Orwig</h5>
<a href="https://twitter.com/porwig">@porwig</a>
<hr>
<p>If you haven't taken a look at @getgrav, you should! Many good ideas, thoughtfully implemented. Great work, @rhuk!</p>
</li>
<li>
<img alt="Scott Edgar" src="https://getgrav.org/images/f/c/f/4/e/fcf4ec14ced257a5d6b4c66016d09cf1524b8b4f-thesneakybandit.jpg?g-283e9f04" width="200" height="200">
<h5>Scott Edgar</h5>
<a href="https://twitter.com/thesneakybandit">@thesneakybandit</a>
<hr>
<p>Just started learning a new CMS. Ok, learnt it. Brilliantly simple stuff from @getgrav.</p>
</li>
 <li>
<img alt="Jozef Maxted" src="https://getgrav.org/images/7/b/3/0/4/7b3040e4271f7d1afbb4f03ea45fbcdc5329d075-jozefmaxted.jpg?g-283e9f04" width="200" height="200">
<h5>Jozef Maxted</h5>
<a href="https://twitter.com/jozefmaxted">@jozefmaxted</a>
<hr>
<p>Played around with @getgrav this evening and I'm impressed. Super simple page creation and theming, flexible workflow, just nice to use!</p>
</li>
<li>
<img alt="Noah Betzen" src="https://getgrav.org/images/f/4/6/4/b/f464b1a4979c424feb9bec53ffd8023682632f27-nezteb.jpg?g-283e9f04" width="200" height="200">
<h5>Noah Betzen</h5>
<a href="https://twitter.com/nezteb">@nezteb</a>
<hr>
<p>[Grav] is by far the best CMS I've ever had the privilege of using. @getgrav</p>
</li>
<li>
<img alt="Steven Lain" src="https://getgrav.org/images/1/9/c/7/a/19c7a3177e1edeb93626dd3739990e3ea52e983b-stevenjlain.jpg?g-283e9f04" width="200" height="200">
<h5>Steven Lain</h5>
<a href="https://twitter.com/stevenjlain">@stevenjlain</a>
<hr>
<p>just installed grav &amp; it is such a pleasure to use. Hats off to the grav team. I look forward to using grav on many, many projects!</p>
</li>
<li>
<img alt="Mike Wink" src="https://getgrav.org/images/a/e/2/e/a/ae2ea72e7aa7a7c8eed0b9a75bb48a42a5460cc0-mikewink.png?g-283e9f04" width="200" height="200">
<h5>Mike Wink</h5>
<a href="https://twitter.com/mikewink">@mikewink</a>
<hr>
<p>Ohh how I love deployments in Grav (@getgrav)! Everything is a .md or .yaml file, no hassle with DBs or other content silos.</p>
</li>
<li>
<img alt="Reach Content" src="https://getgrav.org/images/d/a/c/2/4/dac243398f28e3a6eafe8b3a3cedc436e13f9a92-reachcontent.png?g-283e9f04" width="200" height="200">
<h5>Reach Content</h5>
<a href="https://twitter.com/reachcontent">@reachcontent</a>
<hr>
<p>Getting to know @getgrav you'll want to nominate it as Best New</p>
</li>
<li>
<img alt="Bison" src="https://getgrav.org/images/c/2/2/7/d/c227debab8386fc91e461192f1aa54d43296e7b0-bison42.jpg?g-283e9f04" width="200" height="200">
<h5>Bison</h5>
<a href="https://twitter.com/bison_42">@bison_42</a>
<hr>
<p>everyone should know that there is http://getgrav.org out there a flat-file CMS that does not suck it's awesome!</p>
</li>
<li>
<img alt="Faisal Khan" src="https://getgrav.org/images/b/e/5/e/d/be5ed7ce3fe6bcbedbbd8f4aa59df1ea02e0dbae-babushka99.jpg?g-283e9f04" width="200" height="200">
<h5>Faisal Khan</h5>
<a href="https://twitter.com/babushka99">@babushka99</a>
<hr>
<p>Grav is my new love. So uber-smooth, exceptionally fast and just the right amounts of knobs and switches! @getgrav</p>
</li>
</ul>
<p><a href="https://twitter.com/getgrav">
follow @getgrav
</a>
</p></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WebGPU ‚Äì All of the cores, none of the canvas (104 pts)]]></title>
            <link>https://surma.dev/things/webgpu/</link>
            <guid>36751307</guid>
            <pubDate>Sun, 16 Jul 2023 20:54:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://surma.dev/things/webgpu/">https://surma.dev/things/webgpu/</a>, See on <a href="https://news.ycombinator.com/item?id=36751307">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en">
    
  <p>WebGPU is an upcoming Web API that gives you low-level, general-purpose access GPUs.</p>
<!-- more -->
<p>I am not very experienced with graphics. I picked up bits and bobs of WebGL by reading through tutorials on how to build game engines with OpenGL and learned more about shaders by watching <a href="https://twitter.com/iquilezles">Inigo Quilez</a> do amazing things on <a href="https://shadertoy.com/">ShaderToy</a> by just using shaders, without any 3D meshes or models. This got me far enough to do build things like the background animation in <a href="https://proxx.app/">PROXX</a>, but I was never <em>comfortable</em> with WebGL, and I‚Äôll explain shortly why.</p>
<p>When WebGPU came onto my radar, I wanted to get into it but multiple people warned me that WebGPU has even more boilerplate than WebGL. Undeterred, but anticipating the worst, I scraped together all the tutorials and specifications I could find, of which there are not many because it‚Äôs still early days for WebGPU. I got my feet wet and found that I didn‚Äôt find WebGPU significantly more boilerplate-y than WebGL, but actually to be an API I am much more comfortable with.</p>
<p>So here we are. I want to share what I learned while wrapping my head around GPUs and WebGPU. The goal for this blog post is to make WebGPU accessible for web developers. But here‚Äôs an early heads up: I won‚Äôt use WebGPU to generate graphics. Instead, I will use WebGPU to access the raw computing power a GPU provides. Maybe I will do a follow-up blog post how to use WebGPU to render to your screen, but there is already <a href="https://austin-eng.com/webgpu-samples">quite a bit of content</a> out there. I will go as deep as necessary to make sense of WebGPU and hopefully enable you to use it <em>effectively</em> ‚Äî but not necessarily <em>efficiently</em>. I can‚Äôt make you a GPU performance expert; mostly because I am not one myself.</p>
<p>Enough disclaimer. This is a long one. Buckle up!</p>
<h2>WebGL</h2>
<p><a href="https://www.khronos.org/registry/webgl/specs/latest/1.0/">WebGL</a> came about in 2011 and, up to this point, was the only low-level API to access GPUs with from the web. WebGL‚Äôs API is really just OpenGL ES 2.0 with some thin wrappers and helpers to make it web-compatible. Both WebGL and OpenGL are standardized by the <a href="https://www.khronos.org/">Khronos Group</a>, which is basically the W3C for 3D graphics.</p>
<p>OpenGL‚Äôs API itself goes back even further and is, by today‚Äôs standard, not a great API. The design is centered around an internal, global state object. The design makes sense from the perspective that it minimizes the amount of data that needs to be transferred to and from the GPU for any given call. However, it also introduces a lot of mental overhead.</p>
<figure>
  <img loading="lazy" width="1193" height="1300" src="https://surma.dev/assets/internalstate.c00c7a0f.png">
  <figcaption>A visualization of WebGL‚Äôs internal, global state object. Taken from <a href="https://webglfundamentals.org/webgl/lessons/resources/webgl-state-diagram.html" target="_blank" rel="noopener">WebGL Fundamentals</a>.</figcaption>
</figure>
<p>The internal state object is basically a collection of pointers. Your API calls can affect the objects pointed <em>to</em> by the state object, but also the state object itself. As a result, the order of API calls is incredibly important and I always felt like that this makes it hard to build abstractions and libraries. You have to be extremely meticulous in sanitizing all pointers and state items that can interfere the API calls you are going to make, but also restore the pointers and values to their previous value so that your abstractions compose correctly. I often found myself staring at a black canvas (as that‚Äôs pretty much all you get in terms of error reporting in WebGL) and brute-forcing which pointer is currently not pointing the right way. Quite honestly, I have no idea how <a href="https://threejs.org/">ThreeJS</a> manages to be so robust, but it does manage somehow. I think that‚Äôs one of the main reasons why most people use ThreeJS and not WebGL directly.</p>
<blockquote>
<p><strong>It‚Äôs not you, it‚Äôs me:</strong> To be clear, me not being able to internalize WebGL is probably a shortcoming of my own. People smarter than me have been able to build <em>amazing</em> stuff with WebGL (and OpenGL outside the web), but it just never really clicked for me.</p>
</blockquote>
<p>With the advent of ML, neural networks, and dare I say cryptocurrencies, GPUs have shown that they can be useful for more than just drawing triangles on the screen. Using GPUs for calculations of any kind is often called General-Purpose GPU or GPGPU, and WebGL 1 is not great at this. If you wanted to process arbitrary data on the GPU, you have to encode it as a texture, decode it in a shader, do your calculations and then re-encode the result as a texture. WebGL 2 made this a lot easier with <a href="https://webgl2fundamentals.org/webgl/lessons/webgl-gpgpu.html">Transform Feedback</a>, but WebGL2 wasn‚Äôt supported in Safari until September 2021 (while most other browers supported WebGL2 since January 2017), so it wasn‚Äôt really an option. And even then certain limitations of WebGL2 still made it feel somewhat clunky.</p>
<h2>WebGPU</h2>
<p>Outside of the web, a new generation of graphics APIs have established themselves which expose a more low-level interface to graphics cards. These new APIs accommodate new use-cases and constraints that weren‚Äôt around when OpenGL was designed. On the one hand, GPUs are almost ubiquitous now. Even our mobile devices have capable GPUs built in. As a result, <em>both</em> modern graphics programming (3D rendering and ray tracing) and GPGPU use-cases are increasingly common. On the other hand, most of our devices have multi-core processors, so being able to interact with the GPU from multiple threads can be an important optimization vector. While the WebGPU folks were at it, they also revisited some previous design decisions and front-loaded a lot of the validation work that GPUs have to do, allowing the developer to squeeze more performance out of their GPUs.</p>
<p>The most popular of the next-gen GPU APIs are <a href="https://www.vulkan.org/">Vulkan</a> by the Khronos Group, <a href="https://developer.apple.com/metal/">Metal</a> by Apple and <a href="https://docs.microsoft.com/en-us/windows/win32/direct3d12/direct3d-12-graphics">DirectX 12</a> by Microsoft. To bring these new capabilities to the web, WebGPU was born. While WebGL is just a thin wrapper around OpenGL, WebGPU chose a different approach. It introduces its own abstractions and doesn‚Äôt directly mirror any of these native APIs. This is partially because no single API is available on all systems, but also because many concepts (such as extremely low-level memory management) aren‚Äôt idiomatic for a web-facing API. Instead, WebGPU was designed to both feel ‚Äúwebby‚Äù and to comfortably sit on top of any of the native graphics APIs  while abstracting their idiosyncrasies. It‚Äôs being standardized in the W3C with all major browser vendors having a seat at the table. Due to its comparatively low-level nature and its sheer power, WebGPU has a bit of a learning curve and is relatively heavy on the setup, but I‚Äôll try to break it down as best I can.</p>
<h3>Adapters and Devices</h3>
<p>WebGPU‚Äôs first abstractions that you come into contact with are <em>adapters</em> and (logical) <em>devices</em>.</p>
<figure>
  <img loading="lazy" width="418" height="393" src="https://surma.dev/assets/architecture.498626c9.svg">
  <figcaption>Layers of abstraction, from physical GPUs to logical devices.</figcaption>
</figure>
<p>A <em>physical</em> device is the GPU itself, often distinguished between built-in GPUs and discrete GPUs. Commonly, any given device has exactly one GPU, but it also possible to have two or more GPUs. For example, Microsoft‚Äôs SurfaceBook famously has a low-powered integrated GPU and a high-performance discrete GPU between which the operating system will switch on demand.</p>
<p>The <em>driver</em> ‚Äî provided by the GPU manufacturer ‚Äî will expose the GPU‚Äôs capabilities to the operating system in a way the OS understands and expects. The operating system in turn can expose it to applications, using the graphics APIs the operating system offers, like Vulkan or Metal.</p>
<p>The GPU is a shared resource. It is not only used by many applications at the same time, but also controls what you see on your monitor. There needs to be something that enables multiple processes to use the GPU concurrently, so that each app can put their own UI on screen without interfering with other apps or even maliciously reading other apps‚Äô data. To each process, it looks like they have sole control over the physical GPU, but that is obviously not really the case. This multiplexing is mostly done by the driver and the operating system.</p>
<p>Adapters, in turn, are the translation layer from operation system‚Äôs native graphics API to WebGPU. As the browser is a single OS-level application that can run multiple web applications, there is yet again a need for multiplexing, so that each web app feels like it has sole control of the GPU. This is modelled in WebGPU with the concept of <em>logical</em> devices.</p>
<p>To get access to an adapter, you call <code>navigator.gpu.requestAdapter()</code>. At the time of writing, <a href="https://gpuweb.github.io/gpuweb/#gpu-interface"><code>requestAdapter()</code></a> takes very few options. The options allow you to request a high-performance or low-energy adapter.</p>
<blockquote>
<p><strong>Software rendering:</strong> Some implementations also offer a ‚Äúfallback adapter‚Äù for systems with no GPU or a GPU that isn‚Äôt sufficiently capable. Fallback adapters are effectively a pure software implementation, which will not be very fast but keeps your app functional.</p>
</blockquote>
<p>If this succeeds, i.e. the returned adapter is non-<code>null</code>, you can inspect the adapter‚Äôs capabilities and request a logical device from the adapter using <a href="https://gpuweb.github.io/gpuweb/#gpuadapter"><code>adapter.requestDevice()</code></a>.</p>
<pre><code><span>if</span> <span>(</span><span>!</span>navigator<span>.</span>gpu<span>)</span> <span>throw</span> <span>Error</span><span>(</span><span>"WebGPU not supported."</span><span>)</span><span>;</span>

<span>const</span> adapter <span>=</span> <span>await</span> navigator<span>.</span>gpu<span>.</span><span>requestAdapter</span><span>(</span><span>)</span><span>;</span>
<span>if</span> <span>(</span><span>!</span>adapter<span>)</span> <span>throw</span> <span>Error</span><span>(</span><span>"Couldn‚Äôt request WebGPU adapter."</span><span>)</span><span>;</span>

<span>const</span> device <span>=</span> <span>await</span> adapter<span>.</span><span>requestDevice</span><span>(</span><span>)</span><span>;</span>
<span>if</span> <span>(</span><span>!</span>device<span>)</span> <span>throw</span> <span>Error</span><span>(</span><span>"Couldn‚Äôt request WebGPU logical device."</span><span>)</span><span>;</span>
</code></pre>
<p>Without any options, <code>requestDevice()</code> will return a device that does <em>not</em> necessarily match the physical device‚Äôs capabilities, but rather what the WebGPU team considers a reasonable, lowest common denominator of all GPUs. The details are <a href="https://gpuweb.github.io/gpuweb/#limit">specified</a> in the WebGPU standard. For example, even though my GPU is easily capable of handling data buffers up to 4GiB in size, the <code>device</code> returned will only allow data buffers up to 1GiB, and will reject any data buffers that are bigger. This might seem restrictive, but is actually quite helpful: If your WebGPU app runs with the default device, it will run on the vast majority of devices. If necessary, you can inspect the real limits of the physical GPU via <code>adapter.limits</code> and request a <code>device</code> with raised limits by passing an options object to <code>requestDevice()</code>.</p>
<h3>Shaders</h3>
<p>If you have ever done any work with WebGL, you are probably familiar with vertex shaders and fragment shaders. Without going too much into depth, the traditional setup works something like this: You upload a data buffer to your GPU and tell it how to interpret that data as a series of triangles. Each vertex occupies a chunk of that data buffer, describing that vertex‚Äô position in 3D space, but probably also auxillary data like color, texture IDs, normals and other things. Each vertex in the list is processed by the GPU in the <em>vertex stage</em>, running the <em>vertex shader</em> on each vertex, which will apply translation, rotation or perspective distortion.</p>
<blockquote>
<p><strong>Shaders:</strong> The term ‚Äúshader‚Äù used to confuse me, because you can do <em>so much more</em> than just shading. But in the olden days (i.e. late 1980s!), that term was appropriate: It was a small piece of code that ran on the GPU to decide for each pixel what color it should be so that you could <em>shade</em> the objects being rendered, achieving the illusion of lighting and shadows. Nowadays, shaders loosely refer to any program that runs on the GPU.</p>
</blockquote>
<p>The GPU now rasterizes the triangles, meaning the GPU figures out which pixels each triangle covers on the screen. Each pixel is then processed by the <em>fragment shader</em>, which has access to the pixel coordinates but also the auxillary data to decide which color that pixel should be. When used correctly, amazing 3D graphics can be created with this process.</p>
<p>This system of passing data to a vertex shader, then to a fragment shader and then outputting it directly onto the screen is called a <em>pipeline</em>, and in WebGPU you have to explicitly define your pipeline.</p>
<h3>Pipelines</h3>
<p>Currently, WebGPU allows you to create two types of pipelines: A Render Pipeline and a Compute Pipeline. As the name suggest, the Render Pipeline renders something, meaning it creates a 2D image. That image needn‚Äôt be on screen, but could just be rendered to memory (which is called a Framebuffer). A Compute Pipeline is more generic in that it returns a buffer, which can contain any sort of data. For the remainder of this blog post I‚Äôll focus on Compute Pipelines, as I like to think of Render Pipelines as a specialization/optimization of Compute Pipelines. Now, this is both historically backwards ‚Äî the compute pipeline was built as a generalization over the very purpose-built rendering pipeline ‚Äî and also considerably understates that these pipelines are physically different circuits in your GPU. In terms of the API, however, I find this mental model quite helpful. In the future, it seems likely that more types of pipelines ‚Äî maybe a Raytracing Pipeline ‚Äî will get added to WebGPU.</p>
<p>With WebGPU, a pipeline consists of one (or more) programmable stages, where each stage is defined by a shader and an entry point. A Compute Pipline has a single <code>compute</code> stage, while a Render Pipeline would have a <code>vertex</code> and a <code>fragment</code> stage:</p>
<pre><code><span>const</span> module <span>=</span> device<span>.</span><span>createShaderModule</span><span>(</span><span>{</span>
  <span>code</span><span>:</span> <span><span>`</span><span>
    @compute @workgroup_size(64)
    fn main() {
      // Pointless!
    }
  </span><span>`</span></span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> pipeline <span>=</span> device<span>.</span><span>createComputePipeline</span><span>(</span><span>{</span>
  <span>compute</span><span>:</span> <span>{</span>
    module<span>,</span>
    <span>entryPoint</span><span>:</span> <span>"main"</span><span>,</span>
  <span>}</span><span>,</span>
<span>}</span><span>)</span><span>;</span>
</code></pre>
<p>This is the first time <a href="https://gpuweb.github.io/gpuweb/wgsl">WGSL</a> (pronounced ‚Äúwig-sal‚Äù), the WebGPU Shading Language, makes an appearance. WGSL feels like a cross-over of Rust and GLSL to me. It has a lot of Rust-y syntax with GLSL‚Äôs global functions (like <code>dot()</code>, <code>norm()</code>, <code>len()</code>, ...), types (like <code>vec2</code>, <code>mat4x4</code>, ...) and the <a href="https://en.wikipedia.org/wiki/Swizzling_(computer_graphics)">swizzling</a> notation (like <code>some_vec.xxy</code>, ...). The browser will compile your WGSL to whatever the underlying system expects. That‚Äôs likely to be HLSL for DirectX 12, MSL for Metal and <a href="https://www.khronos.org/spir/">SPIR-V</a> for Vulkan.</p>
<blockquote>
<p><strong>SPIR-V:</strong> <a href="https://www.khronos.org/spir/">SPIR-V</a> is interesting because it‚Äôs an open, binary, intermediate format standardized by the Khronos Group. You can think of SPIR-V as the LLVM of parallel programming language compilers, and there is support to compile many languages <em>to</em> SPIR-V as well as compiling SPIR-V to many other languages.</p>
</blockquote>
<p>In the shader module above we are just creating a function called <code>main</code> and marking it as an entry point for the compute stage by using the <code>@compute</code> attribute. You can have multiple functions marked as an entry point in a shader module, as you can reuse the same shader module for multiple pipelines and choose different functions to invoke via the <code>entryPoint</code> options. But what is that <code>@workgroup_size(64)</code> attribute?</p>
<h3>Parallelism</h3>
<p>GPUs are optimized for throughput at the cost of latency. To understand this, we have to look a bit at the architecture of GPUs.  I don‚Äôt want to (and, honestly, can‚Äôt) explain it in its entirety. I‚Äôll go as deep as I feel is necessary. If you want to know more, this <a href="https://fgiesen.wordpress.com/2011/07/09/a-trip-through-the-graphics-pipeline-2011-index/">13-part blog post series</a> by <a href="https://twitter.com/rygorous">Fabian Giesen</a> is really good.</p>
<p>Something that is quite well-known is the fact that GPUs have an extensive number of cores that allow for massively parallel work. However, the cores are not as independent as you might be used to from when programming for a multi-core CPU. Firstly, GPU cores are grouped hierarchically. The terminology for the different layers of the hierarchy isn‚Äôt consistent across vendors and APIs. Intel has a good piece of <a href="https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top/intel-processors-with-intel-uhd-graphics.html">documentation</a> that gives a high-level overview of their architecture and I‚Äôve been told it‚Äôs safe to assume that other GPUs work at least similarly, despite the exact architecture of GPUs being a NDA-protected secret.</p>
<p>In the case of Intel, the lowest level in the hierarchy is the ‚ÄúExecution Unit‚Äù (EU), which has multiple (in this case seven) <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads">SIMT</a> cores. That means it has seven cores that operate in lock-step and always execute the same instructions. However, each core has its own set of registers and stack pointer. So while they <em>have</em> to execute the same operation, they can execute it on different data. This is also the reason why GPU performance experts avoid branches (like <code>if</code>/<code>else</code> or loops): If the EU encounters an <code>if</code>/<code>else</code>, all cores have to execute <em>both</em> branches, unless all cores happen to take the same branch. Each core can be told to ignore the instructions it is being fed, but that obviously wastes precious cycles that could be spent computing. The same applies to loops! If one core finishes their loop early, it will have to pretend to execute the loop body until <em>all</em> cores have finished the loop.</p>
<p>Despite the core‚Äôs frequency, getting data from memory (or pixels from textures) still takes relatively long ‚Äî Fabian says it takes a couple hundred clock cycles. These couple hundred cycles could be spent on computation instead. To make use of these otherwise idle cycles, each EU is heavily oversubscribed with work. Whenever an EU would end up idling (e.g. to wait for a value from memory), it instead switches to another work item and will only switch back once the new work item needs to wait for something. This is the key trick how GPUs optimize for throughput at the cost of latency: Individual work items will take longer as a switch to another work item might stop execution for longer than necessary, but the overall utilization is higher and results in a higher throughput. The GPU strives to always have work queued up to keep EUs busy at all times.</p>
<figure>
  <picture loading="lazy" width="548" height="492">
    <source srcset="https://surma.dev/assets/intel.0cf26a8e.avif " type="image/avif">
    <img src="https://surma.dev/assets/intel.862e630f.jpeg">
  </picture>
  <figcaption>The architecture of an Intel Iris Xe Graphics chip. EUs have 7 SIMT cores. SubSlices have 8 EUs. 8 SubSlices form a Slice.</figcaption>
</figure>
<p>EUs are just the lowest level in the hierarchy, though. Multiple EUs are grouped into what Intel calls a ‚ÄúSubSlice‚Äù. All the EUs in a SubSlice have access to a small amount of Shared Local Memory (SLM), which is about 64KiB in Intel‚Äôs case. If the program to be run has any synchronization commands, it has to be executed within the same SubSlice, as only they have shared memory for synchronization.</p>
<p>In the last layer multiple SubSlices are grouped into a Slice, which forms the GPU. For an integrated Intel GPU, you end up with a total of 170-700 cores. Discrete GPUs can easily have 1500 and more cores. Again, the naming here is taken from Intel, and other vendors probably use different names, but the general architecture is similar in every GPU.</p>
<p>To fully exploit the benefits of this architecture, programs need to be specifically set up for this architecture so that a purely programmatic GPU scheduler can maximize utilization. As a result, graphics APIs expose a <a href="https://github.com/googlefonts/compute-shader-101/blob/main/docs/glossary.md">threading model</a> that naturally allows for work to be dissected this way. In WebGPU, the important primitive here is the ‚Äúworkgroup‚Äù.</p>
<h3>Workgroups</h3>
<p>In the traditional setting, the vertex shader would get invoked once for each vertex, and the fragment shader once for each pixel (I‚Äôm glossing over some details here, I know). In the GPGPU setting, your compute shader will be invoked once for each work item that you schedule. It is up to you to define what a work item is.</p>
<p>The collection of all work items (which I will call the ‚Äúworkload‚Äù) is broken down into workgroups. All work items in a workgroup are scheduled to run together. In WebGPU, the work load is modelled as a 3-dimensional grid, where each ‚Äúcube‚Äù is a work item, and work items are grouped into bigger cuboids to form a workgroup.</p>
<figure>
	<picture loading="lazy" width="2048" height="1280">
    <source srcset="https://surma.dev/assets/workgroups.fac4b107.avif " type="image/avif">
    <img src="https://surma.dev/assets/workgroups.7887d84b.jpeg">
  </picture>
  <figcaption>This is a workload. White-bordered cubes are a work item. Red-bordered cuboids are a workgroup.</figcaption>
</figure>
<p>Finally, we have enough information to talk about the <code>@workgroup_size(x, y, z)</code> attribute, and it might even be mostly self-explanatory at this point: The attribute allows you to tell the GPU what the the size of a workgroup for this shader should be. Or in the language of the picture above, the <code>@workgroup_size</code> attribute defines the size of the red-bordered cubes. <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>√ó</mo><mi>y</mi><mo>√ó</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">x \times y \times z</annotation></semantics></math></span></span> is the number of work items per workgroup. Any skipped parameter is assumed to be 1, so <code>@workgroup_size(64)</code> is equivalent to <code>@workgroup_size(64, 1, 1)</code>.</p>
<p>Of course, the actual EUs are not arranged in the 3D grid on the chip. The aim of modelling work items in a 3D grid is to increase locality. The assumption is that it is likely that neighboring work groups will access similar areas in memory, so when running neighboring workgroups sequentially, the chances of already having values in the cache are higher, saving a couple of hundred cycles by not having to grab them from memory. However, most hardware seemingly just runs workgroups in a serial order as the difference between running a shader with <code>@workgroup_size(64)</code> or <code>@workgroup_size(8, 8)</code> is negligible. So this concept is considered somewhat legacy.</p>
<p>However, workgroups are restricted in multiple ways: <code>device.limits</code> has a bunch of properties that are worth knowing:</p>
<pre><code><span>// device.limits</span>
<span>{</span>
  <span>// ...</span>
  <span>maxComputeInvocationsPerWorkgroup</span><span>:</span> <span>256</span><span>,</span>
  <span>maxComputeWorkgroupSizeX</span><span>:</span> <span>256</span><span>,</span>
  <span>maxComputeWorkgroupSizeY</span><span>:</span> <span>256</span><span>,</span>
  <span>maxComputeWorkgroupSizeZ</span><span>:</span> <span>64</span><span>,</span>
  <span>maxComputeWorkgroupsPerDimension</span><span>:</span> <span>65535</span><span>,</span>
  <span>// ...</span>
<span>}</span>
</code></pre>
<p>The size of each dimension of a workgroup size is restricted, but even if x, y and z individually are within the limits, their product (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mi>x</mi><mo>√ó</mo><mi>y</mi><mo>√ó</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">= x \times y \times z</annotation></semantics></math></span></span>) might not be, as it has a limit of its own. Lastly, you can only have so many workgroups per dimension.</p>
<blockquote>
<p><strong>Pro tip:</strong> Don‚Äôt spawn the maximum number of threads. Despite the GPU being managed by the OS and an underlying scheduler, you may <a href="https://twitter.com/DasSurma/status/1495096911333842946">freeze your entire system with a long-running GPU program</a>.</p>
</blockquote>
<p>So what <em>is</em> the right workgroup size? It really depends on the semantics you assign the work item coordinates. I do realize that this is not really an helpful answer, so I want to give you the same advice that <a href="https://twitter.com/dakangz">Corentin</a> gave me: ‚ÄúUse [a workgroup size of] 64 unless you know what GPU you are targeting or that your workload needs something different.‚Äù It seems to be a safe number that performs well across many GPUs and allows the GPU scheduler to keep as many EUs as possible busy.</p>
<h3>Commands</h3>
<p>We have written our shader and set up the pipeline. All that‚Äôs left to do is actually invoke the GPU to execute it all. As a GPU <em>can</em> be a completely separate card with it‚Äôs own memory chip, you control it via a so-called command buffer or command queue. The command queue is a chunk of memory that contains encoded commands for the GPU to execute. The encoding is highly specific to the GPU and is taken care of by the driver. WebGPU exposes a <code>CommandEncoder</code> to tap into that functionality.</p>
<pre><code><span>const</span> commandEncoder <span>=</span> device<span>.</span><span>createCommandEncoder</span><span>(</span><span>)</span><span>;</span>
<span>const</span> passEncoder <span>=</span> commandEncoder<span>.</span><span>beginComputePass</span><span>(</span><span>)</span><span>;</span>
passEncoder<span>.</span><span>setPipeline</span><span>(</span>pipeline<span>)</span><span>;</span>
passEncoder<span>.</span><span>dispatchWorkgroups</span><span>(</span><span>1</span><span>)</span><span>;</span>
passEncoder<span>.</span><span>end</span><span>(</span><span>)</span><span>;</span>
<span>const</span> commands <span>=</span> commandEncoder<span>.</span><span>finish</span><span>(</span><span>)</span><span>;</span>
device<span>.</span>queue<span>.</span><span>submit</span><span>(</span><span>[</span>commands<span>]</span><span>)</span><span>;</span>
</code></pre>
<p><code>commandEncoder</code> has multiple methods that allows you to copy data from one GPU buffer to another and manipulate textures. It also allows you to create <code>PassEncoder</code>, which encodes the setup and invocation of pipelines. In this case, we have a compute pipline, so we have to create a compute pass, set it to use our pre-declared pipeline and finally call <code>dispatchWorkgroups(w_x, w_y, w_z)</code> to tell the GPU how many workgroups to create along each dimension. In other words, the number of times our compute shader will be invoked is equal to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>x</mi></msub><mo>√ó</mo><msub><mi>w</mi><mi>y</mi></msub><mo>√ó</mo><msub><mi>w</mi><mi>z</mi></msub><mo>√ó</mo><mi>x</mi><mo>√ó</mo><mi>y</mi><mo>√ó</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">w_x \times w_y \times w_z \times x \times y \times z</annotation></semantics></math></span></span>. The pass encoder, by the way, is WebGPU‚Äôs abstraction to avoid that internal, global state object I was ranting about at the start of this blog post. All data and state required to run a GPU pipeline is explicitly passed along through the pass encoder.</p>
<blockquote>
<p><strong>Abstraction:</strong> The command buffer is also the hook for the driver or operating system to let multiple applications use the GPU without them interfering with each other. When you queue up your commands, the abstraction layers below will inject additional commands into the queue to save the previous program‚Äôs state and restore your program‚Äôs state so that it feels like no one else is using the GPU.</p>
</blockquote>
<p>Running this code, we are in fact spawning 64 threads on the GPU and they do <em>absolutely nothing</em>. But it works, so that‚Äôs cool. Let‚Äôs talk about how we give the GPU some data to work.</p>
<h2>Exchanging data</h2>
<p>As promised, I won‚Äôt be using WebGPU for graphics directly, so instead I thought it‚Äôd be fun to run a physics simulation on the GPU and visualize it using Canvas2D. Maybe I am flattering myself calling it a ‚Äúphysics simulation‚Äù ‚Äî what I am doing is generating a whole bunch of circles, have them roll around on a plane in random directions and letting them collide.</p>
<p>For this to work, we need to push some simulation parameters and the initial state <em>to</em> the GPU, run the simulation <em>on</em> the GPU and read the simulation results <em>from</em> the GPU. This is arguably the hairiest part of WebGPU, as there‚Äôs a bunch of data acrobatics (not to say seemingly pointless copying), but this is what allows WebGPU to be a device-agnostic API running at the highest level of performance.</p>
<h3>Bind Group Layouts</h3>
<p>To exchange data with the GPU, we need to extend our pipeline definition with a bind group layout. A bind group is a collection of GPU entities (memory buffers, textures, samplers, etc) that are made accessible during the execution of the pipeline. The bind group <em>layout</em> pre-defines the types, purposes and uses of these GPU entities, which allows the GPU figure out how to run a pipeline most efficiently ahead of time. Let‚Äôs keep it simple in this initial step and give our pipeline access to a single memory buffer:</p>
<pre><code><span><span>const</span> bindGroupLayout <span>=</span>
 device<span>.</span><span>createBindGroupLayout</span><span>(</span><span>{</span>
    <span>entries</span><span>:</span> <span>[</span><span>{</span>
      <span>binding</span><span>:</span> <span>1</span><span>,</span>
      <span>visibility</span><span>:</span> GPUShaderStage<span>.</span><span>COMPUTE</span><span>,</span>
      <span>buffer</span><span>:</span> <span>{</span>
        <span>type</span><span>:</span> <span>"storage"</span><span>,</span>
      <span>}</span><span>,</span>
    <span>}</span><span>]</span><span>,</span>
  <span>}</span><span>)</span><span>;</span></span>
<span>
<span>const</span> pipeline <span>=</span> device<span>.</span><span>createComputePipeline</span><span>(</span><span>{</span></span>
<span>  <span>layout</span><span>:</span> device<span>.</span><span>createPipelineLayout</span><span>(</span><span>{</span>
    <span>bindGroupLayouts</span><span>:</span> <span>[</span>bindGroupLayout<span>]</span><span>,</span>
  <span>}</span><span>)</span><span>,</span></span>
<span>  <span>compute</span><span>:</span> <span>{</span>
    module<span>,</span>
    <span>entryPoint</span><span>:</span> <span>"main"</span><span>,</span>
  <span>}</span><span>,</span>
<span>}</span><span>)</span><span>;</span></span></code></pre><p>The <code>binding</code> number can be freely chosen and is used to tie a variable in our WGSL code to the contents of the buffer in this slot of the bind group layout. Our <code>bindGroupLayout</code> also defines the purpose for each buffer, which in this case is <code>"storage"</code>. Another option is <code>"read-only-storage"</code>, which is read-only (duh!), and allows the GPU to make further optimizations on the basis that this buffer will never be written to and as such doesn‚Äôt need to be synchronized. The last possible value for the buffer type is <code>"uniform"</code>, which in the context of a compute pipeline is mostly functionally equivalent to a storage buffer.</p>
<p>The bind group layout is in place. Now we can create the bind group itself, containing the actual instances of the GPU entities the bind group layout expects. Once that bind group with the buffer inside is in place, the compute shader can fill it with data and we can read it from the GPU. But there‚Äôs a hurdle: Staging Buffers.</p>
<h3>Staging Buffers</h3>
<p>I will say it again: GPUs are heavily optimized for throughput at the cost of latency. A GPU needs to be able feed data to the cores at an incredibly high rate to sustain that throughput. Fabian did some <a href="https://fgiesen.wordpress.com/2011/07/04/a-trip-through-the-graphics-pipeline-2011-part-4/#:~:text=that%E2%80%99s%203.3%20GB/s%20just%20for%20texture%20request%20payloads.%20Lower%20bound%2C">back-of-napkin math</a> in his blog post series from 2011, and arrived at the conclusion that GPUs need to sustain 3.3GB/s <em>just for texture samples</em> for a shader running at 1280x720 resolution. To accommodate today‚Äôs graphics demands, GPUs need shovel data even faster. This is only possible to achieve if the memory of the GPU is very tightly integrated with the cores. This tight integration makes it hard to also expose the same memory to the host machine for reading and writing.</p>
<p>Instead, GPUs have additional memory banks that are accessible to both the host machine as well as the GPU, but are not as tightly integrated and can‚Äôt provide data as fast. Staging buffers are buffers that are allocated in this intermediate memory realm and can be <a href="https://en.wikipedia.org/wiki/Memory-mapped_I/O">mapped</a> to the host system for reading and writing. To read data from the GPU, we copy data from an internal, high-performance buffer to a staging buffer, and then map the staging buffer to the host machine so we can read the data back into main memory. For writing, the process is the same but in reverse.</p>
<p>Back to our code: We will create a writable buffer and add it to the bind group, so that it can be written to by the compute shader. We will also create a second buffer with the same size that will act as a staging buffer. Each buffer is created with a <code>usage</code> bitmask, where you can declare how you intend to use that buffer. The GPU will then figure out where the buffer should be located to fulfill all these use-cases or throw an error if the combination of flags is unfulfillable.</p>
<pre><code><span>const</span> <span>BUFFER_SIZE</span> <span>=</span> <span>1000</span><span>;</span>

<span>const</span> output <span>=</span> device<span>.</span><span>createBuffer</span><span>(</span><span>{</span>
  <span>size</span><span>:</span> <span>BUFFER_SIZE</span><span>,</span>
  <span>usage</span><span>:</span> GPUBufferUsage<span>.</span><span>STORAGE</span> <span>|</span> GPUBufferUsage<span>.</span><span>COPY_SRC</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> stagingBuffer <span>=</span> device<span>.</span><span>createBuffer</span><span>(</span><span>{</span>
  <span>size</span><span>:</span> <span>BUFFER_SIZE</span><span>,</span>
  <span>usage</span><span>:</span> GPUBufferUsage<span>.</span><span>MAP_READ</span> <span>|</span> GPUBufferUsage<span>.</span><span>COPY_DST</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> bindGroup <span>=</span> device<span>.</span><span>createBindGroup</span><span>(</span><span>{</span>
  <span>layout</span><span>:</span> bindGroupLayout<span>,</span>
  <span>entries</span><span>:</span> <span>[</span><span>{</span>
    <span>binding</span><span>:</span> <span>1</span><span>,</span>
    <span>resource</span><span>:</span> <span>{</span>
      <span>buffer</span><span>:</span> output<span>,</span>
    <span>}</span><span>,</span>
  <span>}</span><span>]</span><span>,</span>
<span>}</span><span>)</span><span>;</span>
</code></pre>
<p>Note that <code>createBuffer()</code> returns a <code>GPUBuffer</code>, not an <code>ArrayBuffer</code>. They can‚Äôt be read or written to just yet. For that, they need to be mapped, which is a separate API call and will only succeed for buffers that have <code>GPUBufferUsage.MAP_READ</code> or <code>GPUBufferUsage.MAP_WRITE</code>.</p>
<blockquote>
<p><strong>TypeScript:</strong> I found TypeScript to be quite helpful when exploring new APIs. Luckily, Chrome‚Äôs WebGPU team maintains <a href="https://npm.im/@webgpu/types"><code>@webgpu/types</code></a> so you can enjoy accurate auto-completion.</p>
</blockquote>
<p>Now that we not only have the bind group <em>layout</em>, but even the actual bind group itself, we need to update our dispatch code to make use of this bind group. Afterwards we map our staging buffer to read the results back into JavaScript.</p>
<pre><code><span><span>const</span> commandEncoder <span>=</span> device<span>.</span><span>createCommandEncoder</span><span>(</span><span>)</span><span>;</span>
<span>const</span> passEncoder <span>=</span> commandEncoder<span>.</span><span>beginComputePass</span><span>(</span><span>)</span><span>;</span>
passEncoder<span>.</span><span>setPipeline</span><span>(</span>pipeline<span>)</span><span>;</span></span>
<span>passEncoder<span>.</span><span>setBindGroup</span><span>(</span><span>0</span><span>,</span> bindGroup<span>)</span><span>;</span></span>
<span>passEncoder.dispatchWorkgroups(1);</span>
<span>passEncoder<span>.</span><span>dispatchWorkgroups</span><span>(</span>Math<span>.</span><span>ceil</span><span>(</span><span>BUFFER_SIZE</span> <span>/</span> <span>64</span><span>)</span><span>)</span><span>;</span></span>
<span>passEncoder<span>.</span><span>end</span><span>(</span><span>)</span><span>;</span></span>
<span>commandEncoder<span>.</span><span>copyBufferToBuffer</span><span>(</span>
  output<span>,</span>
  <span>0</span><span>,</span> <span>// Source offset</span>
  stagingBuffer<span>,</span>
  <span>0</span><span>,</span> <span>// Destination offset</span>
  <span>BUFFER_SIZE</span>
<span>)</span><span>;</span></span>
<span><span>const</span> commands <span>=</span> commandEncoder<span>.</span><span>finish</span><span>(</span><span>)</span><span>;</span>
device<span>.</span>queue<span>.</span><span>submit</span><span>(</span><span>[</span>commands<span>]</span><span>)</span><span>;</span>
</span>
<span><span>await</span> stagingBuffer<span>.</span><span>mapAsync</span><span>(</span>
  GPUMapMode<span>.</span><span>READ</span><span>,</span>
  <span>0</span><span>,</span> <span>// Offset</span>
  <span>BUFFER_SIZE</span> <span>// Length</span>
 <span>)</span><span>;</span>
<span>const</span> copyArrayBuffer <span>=</span>
  stagingBuffer<span>.</span><span>getMappedRange</span><span>(</span><span>0</span><span>,</span> <span>BUFFER_SIZE</span><span>)</span><span>;</span>
<span>const</span> data <span>=</span> copyArrayBuffer<span>.</span><span>slice</span><span>(</span><span>)</span><span>;</span>
stagingBuffer<span>.</span><span>unmap</span><span>(</span><span>)</span><span>;</span>
console<span>.</span><span>log</span><span>(</span><span>new</span> <span>Float32Array</span><span>(</span>data<span>)</span><span>)</span><span>;</span></span></code></pre><p>Since we added a bind group layout to our pipeline, any invocation without providing a bind group would now fail. After we define our ‚Äúpass‚Äù, we add an additional command via our command encoder to copy the data from our output buffer to the staging buffer and submit our command buffer to the queue. The GPU will start working through the command queue. We don‚Äôt know when the GPU will be done exactly, but we can already submit our request for the <code>stagingBuffer</code> to be mapped. This function is async as it needs to wait until the command queue has been fully processed. When the returned promise resolves, the buffer is mapped, but not exposed to JavaScript yet. <code>stagingBuffer.getMappedRange()</code> let‚Äôs us request for a subsection (or the entire buffer) to be exposed to JavaScript as a good ol‚Äô <code>ArrayBuffer</code>. This is real, mapped GPU memory, meaning the data will disappear (the <code>ArrayBuffer</code> will be ‚Äúdetached‚Äù), when <code>stagingBuffer</code> gets unmapped, so I‚Äôm using <code>slice()</code> to create JavaScript-owned copy.</p>
<figure>
	<picture loading="lazy" width="1024" height="429">
    <source srcset="https://surma.dev/assets/emptybuffer.e9a718cf.avif " type="image/avif">
    <img src="https://surma.dev/assets/emptybuffer.983daecd.jpeg">
  </picture>
  <figcaption>Not very exciting, but we copied those zeroes from the GPU‚Äôs memory.</figcaption>
</figure>
<p>Something other than zeroes would probably be a bit more convincing. Before we start doing any advanced calculation on our GPU, let‚Äôs put some hand-picked data into our buffer as proof that our pipeline is <em>indeed</em> working as intended. This is our new compute shader code, with extra spacing for clarity.</p>
<pre><code><span>@</span><span>group</span><span>(</span><span>0</span><span>)</span> <span>@</span><span>binding</span><span>(</span><span>1</span><span>)</span>
var<span>&lt;</span>storage<span>,</span> read_write<span>&gt;</span> output<span>:</span> array<span>&lt;</span><span>f32</span><span>&gt;</span><span>;</span>

<span>@</span>compute <span>@</span><span>workgroup_size</span><span>(</span><span>64</span><span>)</span>
<span>fn</span> <span>main</span><span>(</span>

  <span>@</span><span>builtin</span><span>(</span>global_invocation_id<span>)</span>
  global_id <span>:</span> vec3<span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span>

  <span>@</span><span>builtin</span><span>(</span>local_invocation_id<span>)</span>
  local_id <span>:</span> vec3<span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span>

<span>)</span> <span>{</span>
  output<span>[</span>global_id<span>.</span>x<span>]</span> <span>=</span>
    <span>f32</span><span>(</span>global_id<span>.</span>x<span>)</span> <span>*</span> <span>1000</span><span>.</span> <span>+</span> <span>f32</span><span>(</span>local_id<span>.</span>x<span>)</span><span>;</span>
<span>}</span>
</code></pre>
<p>The first two lines declare a module-scope variable called <code>output</code>, which is a dynamically-sized array of <code>f32</code>. The attributes declare where the data comes from: From the buffer in our first (0th) binding group, the entry with <code>binding</code> value 1. The length of the array will automatically reflect the length of the underlying buffer (rounded down).</p>
<blockquote>
<p><strong>Variables:</strong> WGSL diverges from Rust in that a variable declared with <code>let</code> is immutable. If you want a variable to be mutable, they keyword to use is <code>var</code>.</p>
</blockquote>
<p>The signature of our <code>main()</code> function has been augmented with two parameters: <code>global_id</code> and <code>local_id</code>. I could have chosen any name ‚Äî their value is determined by the attributes associated with them: The <code>global_invocation_id</code> is a built-in value that corresponds to the global x/y/z coordinates of this shader invocation in the work <em>load</em>. The <code>local_invocation_id</code> is the x/y/z coordinates of this shader vocation in the work <em>group</em>.</p>
<figure>
	<picture loading="lazy" width="2048" height="1280">
    <source srcset="https://surma.dev/assets/coordinates.436c4f95.avif " type="image/avif">
    <img src="https://surma.dev/assets/coordinates.dd59e014.jpeg">
  </picture>
  <figcaption>An example of three work items a, b and c marked in the workload.</figcaption>
</figure>
<p>This image shows one possible interpretation of the coordinate system for a workload with <code>@workgroup_size(4, 4, 4)</code>. It is up to you to define what the coordinate system is for your use-case. If we agreed on the axes as drawn above, we‚Äôd see the following <code>main()</code> parameters for a, b and c:</p>
<ul>
<li>a:
<ul>
<li><code>local_id=(x=0, y=0, z=0)</code></li>
<li><code>global_id=(x=0, y=0, z=0)</code></li>
</ul>
</li>
<li>b:
<ul>
<li><code>local_id=(x=0, y=0, z=0)</code></li>
<li><code>global_id=(x=4, y=0, z=0)</code></li>
</ul>
</li>
<li>c:
<ul>
<li><code>local_id=(x=1, y=1, z=0)</code></li>
<li><code>global_id=(x=5, y=5, z=0)</code></li>
</ul>
</li>
</ul>
<p>In our shader, we have <code>@workgroup_size(64, 1, 1)</code>, so <code>local_id.x</code> will range from 0 to 63. To be able to inspect both values, I am ‚Äúencoding‚Äù them into a single number. Note that WGSL is strictly typed: Both <code>local_id</code> and <code>global_id</code> are <code>vec3&lt;u32&gt;</code>, so we have to explicitly cast their values to <code>f32</code> to be able to assign them to our <code>f32</code> output buffer.</p>
<figure>
	<picture loading="lazy" width="1024" height="565">
    <source srcset="https://surma.dev/assets/fullbuffer.1d2e3f75.avif " type="image/avif">
    <img src="https://surma.dev/assets/fullbuffer.d31e6ebc.jpeg">
  </picture>
  <figcaption>Actual values filled in by the GPU. Notice how the local invocation ID starts wrapping around after 63, while the global invocation ID keeps going.</figcaption>
</figure>
<p>And this proves that our compute shader is indeed invoked for each value in the output memory and fills it with a unique value. We won‚Äôt know in which order this data has been filled in, as that‚Äôs intentionally unspecified and left up to the GPU‚Äôs scheduler.</p>
<h3>Overdispatching</h3>
<p>The astute observer might have noticed that the total number of shader invocations (<code>Math.ceil(BUFFER_SIZE / 64) * 64</code>) will result in <code>global_id.x</code> getting bigger than the length of our array, as each <code>f32</code> takes up 4 bytes. Luckily, accessing an array is safe-guarded by an implicit clamp, so every write past the end of the array will end up writing to the last element of the array. That avoids memory access faults, but might still generate unusable data. And indeed, if you check the last 3 elements of the returned buffer, you‚Äôll find the numbers 247055, 248056 and 608032. It‚Äôs up to us to prevent that from happening in our shader code with an early exit:</p>
<pre><code><span><span>fn</span> <span>main</span><span>(</span> <span>/* ... */</span><span>)</span> <span>{</span></span>
<span>  <span>if</span><span>(</span>global_id<span>.</span>x <span>&gt;=</span> <span>arrayLength</span><span>(</span><span>&amp;</span>output<span>)</span><span>)</span> <span>{</span>
    <span>return</span><span>;</span>
  <span>}</span></span>
<span>  output<span>[</span>global_id<span>.</span>x<span>]</span> <span>=</span>
    <span>f32</span><span>(</span>global_id<span>.</span>x<span>)</span> <span>*</span> <span>100</span><span>.</span> <span>+</span> <span>f32</span><span>(</span>local_id<span>.</span>x<span>)</span><span>;</span>
<span>}</span></span></code></pre><p>If you want, you can run this <a href="https://surma.dev/things/webgpu/step1/index.html">demo</a> and inspect the full source.</p>
<h3>A structure for the madness</h3>
<p>Our goal here is to have a whole lotta balls moving through 2D space and have happy little collisions. For that, each ball needs to have a radius, a position and a velocity vector. We could just continue working on <code>array&lt;f32&gt;</code>, and say the first float is the first ball‚Äôs x position, the second float is the first ball‚Äôs y position and so on, and so forth. That‚Äôs not what I would call ergonomic. Luckily, WGSL allows us to define our own structs to tie multiple pieces of data together in a neat bag.</p>
<blockquote>
<p><strong>Old news:</strong> If you know what memory alignment is, you can skip this section (although do take a look at the code sample). If you don‚Äôt know what it is, I won‚Äôt really explain the why, but show you how it manifests and how to work with it.</p>
</blockquote>
<p>So it makes sense to define a <code>struct Ball</code> with all these components and turn our <code>array&lt;f32&gt;</code> into <code>array&lt;Ball&gt;</code>. The downside of all this: we have to talk about <a href="https://en.wikipedia.org/wiki/Data_structure_alignment">alignment</a>.</p>
<pre><code><span><span>struct</span> <span>Ball</span> <span>{</span>
  radius<span>:</span> <span>f32</span><span>,</span>
  position<span>:</span> vec2<span>&lt;</span><span>f32</span><span>&gt;</span><span>,</span>
  velocity<span>:</span> vec2<span>&lt;</span><span>f32</span><span>&gt;</span><span>,</span>
<span>}</span></span>
<span>
<span>@</span><span>group</span><span>(</span><span>0</span><span>)</span> <span>@</span><span>binding</span><span>(</span><span>1</span><span>)</span></span>
<span>var&lt;storage, read_write&gt; output: array&lt;f32&gt;;</span>
<span>var<span>&lt;</span>storage<span>,</span> read_write<span>&gt;</span> output<span>:</span> array<span>&lt;</span><span>Ball</span><span>&gt;</span><span>;</span></span>
<span>
<span>@</span>compute <span>@</span><span>workgroup_size</span><span>(</span><span>64</span><span>)</span>
<span>fn</span> <span>main</span><span>(</span>
  <span>@</span><span>builtin</span><span>(</span>global_invocation_id<span>)</span> global_id <span>:</span> vec3<span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span>
  <span>@</span><span>builtin</span><span>(</span>local_invocation_id<span>)</span> local_id <span>:</span> vec3<span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span>
<span>)</span> <span>{</span>
  <span>let</span> num_balls <span>=</span> <span>arrayLength</span><span>(</span><span>&amp;</span>output<span>)</span><span>;</span>
  <span>if</span><span>(</span>global_id<span>.</span>x <span>&gt;=</span> num_balls<span>)</span> <span>{</span>
    <span>return</span><span>;</span>
  <span>}</span>
</span>
<span>  output<span>[</span>global_id<span>.</span>x<span>]</span><span>.</span>radius <span>=</span> <span>999</span><span>.</span><span>;</span>
  output<span>[</span>global_id<span>.</span>x<span>]</span><span>.</span>position <span>=</span> vec2<span>&lt;</span><span>f32</span><span>&gt;</span><span>(</span>global_id<span>.</span>xy<span>)</span><span>;</span>
  output<span>[</span>global_id<span>.</span>x<span>]</span><span>.</span>velocity <span>=</span> vec2<span>&lt;</span><span>f32</span><span>&gt;</span><span>(</span>local_id<span>.</span>xy<span>)</span><span>;</span></span>
<span><span>}</span></span></code></pre><p>If you run this <a href="https://surma.dev/things/webgpu/step2/index.html">demo</a>, you‚Äôll see this in your console:</p>
<figure>
	<picture loading="lazy" width="479" height="440">
    <source srcset="https://surma.dev/assets/alignment.3ee470e2.avif " type="image/avif">
    <img src="https://surma.dev/assets/alignment.47a211c5.jpeg">
  </picture>
  <figcaption>The struct has a hole (padding) in its memory layout due to alignment constraints.</figcaption>
</figure>
<p>I put <code>999</code> the first field of the struct to make it easy to see where the struct begins in the buffer. There‚Äôs a total of 6 numbers until we reach the next <code>999</code>, which is a bit surprising because the struct really only has 5 numbers to store: <code>radius</code>, <code>position.x</code>, <code>position.y</code>, <code>velocity.x</code> and <code>velocity.y</code>. Taking a closer look, it is clear that the number after <code>radius</code> is always 0. This is because of alignment.</p>
<p>Each WGSL data type has well-defined <a href="https://gpuweb.github.io/gpuweb/wgsl/#alignment-and-size">alignment requirements</a>. If a data type has an alignment of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span>, it means that a value of that data type can only be stored at a memory address that is a multiple of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span>. <code>f32</code> has an alignment of 4, while <code>vec2&lt;f32&gt;</code> has an alignment of 8. If we assume our struct starts at address 0, then the <code>radius</code> field can be stored at address 0, as 0 is a multiple of 4. The next field in the struct is <code>vec2&lt;f32&gt;</code>, which has an alignment of 8. However, the first free address after <code>radius</code> is 4, which is <em>not</em> a multiple of 8. To remedy this, the compiler adds padding of 4 bytes to get to the next address that is a multiple of 8. This explains what we see an unused field with the value 0 in the DevTools console.</p>
<figure>
  <picture loading="lazy" width="797" height="605">
    <source srcset="https://surma.dev/assets/alignmenttable.fbedb57c.avif " type="image/avif">
    <img src="https://surma.dev/assets/alignmenttable.b958880e.jpeg">
  </picture>
  <figcaption>
<p>The (shortened) <a href="https://gpuweb.github.io/gpuweb/wgsl/#alignment-and-size">alignment table</a> from the WGSL spec.</p>
  </figcaption>
</figure>
<p>Now that we know how our struct is laid out in memory, we can populate it from JavaScript to generate our initial state of balls and also read it back to visualize it.</p>
<h3>Input &amp; Output</h3>
<p>We have successfully managed to read data from the GPU, bring it to JavaScript and ‚Äúdecode‚Äù it. It‚Äôs now time to tackle the other direction. We need to generate the initial state of all our balls in JavaScript and give it to the GPU so it can run the compute shader on it. Generating the initial state is fairly straight forward:</p>
<pre><code><span>let</span> inputBalls <span>=</span> <span>new</span> <span>Float32Array</span><span>(</span><span>new</span> <span>ArrayBuffer</span><span>(</span><span>BUFFER_SIZE</span><span>)</span><span>)</span><span>;</span>
<span>for</span> <span>(</span><span>let</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>NUM_BALLS</span><span>;</span> i<span>++</span><span>)</span> <span>{</span>
  inputBalls<span>[</span>i <span>*</span> <span>6</span> <span>+</span> <span>0</span><span>]</span> <span>=</span> <span>randomBetween</span><span>(</span><span>2</span><span>,</span> <span>10</span><span>)</span><span>;</span> <span>// radius</span>
  inputBalls<span>[</span>i <span>*</span> <span>6</span> <span>+</span> <span>1</span><span>]</span> <span>=</span> <span>0</span><span>;</span> <span>// padding</span>
  inputBalls<span>[</span>i <span>*</span> <span>6</span> <span>+</span> <span>2</span><span>]</span> <span>=</span> <span>randomBetween</span><span>(</span><span>0</span><span>,</span> ctx<span>.</span>canvas<span>.</span>width<span>)</span><span>;</span> <span>// position.x</span>
  inputBalls<span>[</span>i <span>*</span> <span>6</span> <span>+</span> <span>3</span><span>]</span> <span>=</span> <span>randomBetween</span><span>(</span><span>0</span><span>,</span> ctx<span>.</span>canvas<span>.</span>height<span>)</span><span>;</span> <span>// position.y</span>
  inputBalls<span>[</span>i <span>*</span> <span>6</span> <span>+</span> <span>4</span><span>]</span> <span>=</span> <span>randomBetween</span><span>(</span><span>-</span><span>100</span><span>,</span> <span>100</span><span>)</span><span>;</span> <span>// velocity.x</span>
  inputBalls<span>[</span>i <span>*</span> <span>6</span> <span>+</span> <span>5</span><span>]</span> <span>=</span> <span>randomBetween</span><span>(</span><span>-</span><span>100</span><span>,</span> <span>100</span><span>)</span><span>;</span> <span>// velocity.y</span>
<span>}</span>
</code></pre>
<blockquote>
<p><strong>Buffer-backed-object:</strong> With more complex data structures, it can get quite tedious to manipulate the data from JavaScript. While originally written for worker use-cases, my library <a href="https://github.com/GoogleChromeLabs/buffer-backed-object"><code>buffer-backed-object</code></a> can come in handy here!</p>
</blockquote>
<p>We also already know how to expose a buffer to our shader. We just need to adjust our pipeline bind group layout to expect another buffer:</p>
<pre><code><span><span>const</span> bindGroupLayout <span>=</span> device<span>.</span><span>createBindGroupLayout</span><span>(</span><span>{</span>
  <span>entries</span><span>:</span> <span>[</span></span>
<span>    <span>{</span>
      <span>binding</span><span>:</span> <span>0</span><span>,</span>
      <span>visibility</span><span>:</span> GPUShaderStage<span>.</span><span>COMPUTE</span><span>,</span>
      <span>buffer</span><span>:</span> <span>{</span>
        <span>type</span><span>:</span> <span>"read-only-storage"</span><span>,</span>
      <span>}</span><span>,</span>
    <span>}</span><span>,</span></span>
<span>    <span>{</span>
      <span>binding</span><span>:</span> <span>1</span><span>,</span>
      <span>visibility</span><span>:</span> GPUShaderStage<span>.</span><span>COMPUTE</span><span>,</span>
      <span>buffer</span><span>:</span> <span>{</span>
        <span>type</span><span>:</span> <span>"storage"</span><span>,</span>
      <span>}</span><span>,</span>
    <span>}</span><span>,</span>
  <span>]</span><span>,</span>
<span>}</span><span>)</span><span>;</span></span></code></pre><p>... and create a GPU buffer that we can bind using our bind group:</p>
<pre><code><span><span>const</span> input <span>=</span> device<span>.</span><span>createBuffer</span><span>(</span><span>{</span>
  <span>size</span><span>:</span> <span>BUFFER_SIZE</span><span>,</span>
  <span>usage</span><span>:</span> GPUBufferUsage<span>.</span><span>STORAGE</span> <span>|</span> GPUBufferUsage<span>.</span><span>COPY_DST</span><span>,</span>
<span>}</span><span>)</span><span>;</span></span>
<span>
<span>const</span> bindGroup <span>=</span> device<span>.</span><span>createBindGroup</span><span>(</span><span>{</span>
  <span>layout</span><span>:</span> bindGroupLayout<span>,</span>
  <span>entries</span><span>:</span> <span>[</span></span>
<span>    <span>{</span>
      <span>binding</span><span>:</span> <span>0</span><span>,</span>
      <span>resource</span><span>:</span> <span>{</span>
        <span>buffer</span><span>:</span> input<span>,</span>
      <span>}</span><span>,</span>
    <span>}</span><span>,</span></span>
<span>    <span>{</span>
      <span>binding</span><span>:</span> <span>1</span><span>,</span>
      <span>resource</span><span>:</span> <span>{</span>
        <span>buffer</span><span>:</span> output<span>,</span>
      <span>}</span><span>,</span>
    <span>}</span><span>,</span>
  <span>]</span><span>,</span>
<span>}</span><span>)</span><span>;</span></span></code></pre><p>Now for the new part: Sending data to the GPU. Just like with reading data, we technically have to create a staging buffer that we can map, copy our data into the staging buffer and then issue a command to copy our data from the staging buffer into the storage buffer. However, WebGPU offers a convenience function that will choose the most efficient way of getting our data into the storage buffer for us, even if that involves creating a temporary staging buffer on the fly:</p>
<pre><code>device<span>.</span>queue<span>.</span><span>writeBuffer</span><span>(</span>input<span>,</span> <span>0</span><span>,</span> inputBalls<span>)</span><span>;</span>
</code></pre>
<p>That‚Äôs it? That‚Äôs it! We don‚Äôt even need a command encoder. We can just put this command directly into the command queue. <code>device.queue</code> offers some other, similar convenience functions for textures as well.</p>
<p>Now we need to bind this new buffer to a variable in WGSL and do something with it:</p>
<pre><code><span><span>struct</span> <span>Ball</span> <span>{</span>
  radius<span>:</span> <span>f32</span><span>,</span>
  position<span>:</span> vec2<span>&lt;</span><span>f32</span><span>&gt;</span><span>,</span>
  velocity<span>:</span> vec2<span>&lt;</span><span>f32</span><span>&gt;</span><span>,</span>
<span>}</span>
</span>
<span><span>@</span><span>group</span><span>(</span><span>0</span><span>)</span> <span>@</span><span>binding</span><span>(</span><span>0</span><span>)</span>
var<span>&lt;</span>storage<span>,</span> read<span>&gt;</span> input<span>:</span> array<span>&lt;</span><span>Ball</span><span>&gt;</span><span>;</span></span>
<span>
<span>@</span><span>group</span><span>(</span><span>0</span><span>)</span> <span>@</span><span>binding</span><span>(</span><span>1</span><span>)</span>
var<span>&lt;</span>storage<span>,</span> read_write<span>&gt;</span> output<span>:</span> array<span>&lt;</span><span>Ball</span><span>&gt;</span><span>;</span>
</span>
<span><span>const</span> <span>TIME_STEP</span><span>:</span> <span>f32</span> <span>=</span> <span>0.016</span><span>;</span></span>
<span>
<span>@</span>compute <span>@</span><span>workgroup_size</span><span>(</span><span>64</span><span>)</span>
<span>fn</span> <span>main</span><span>(</span>
  <span>@</span><span>builtin</span><span>(</span>global_invocation_id<span>)</span>
  global_id <span>:</span> vec3<span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span>
<span>)</span> <span>{</span>
  <span>let</span> num_balls <span>=</span> <span>arrayLength</span><span>(</span><span>&amp;</span>output<span>)</span><span>;</span>
  <span>if</span><span>(</span>global_id<span>.</span>x <span>&gt;=</span> num_balls<span>)</span> <span>{</span>
    <span>return</span><span>;</span>
  <span>}</span></span>
<span>  output<span>[</span>global_id<span>.</span>x<span>]</span><span>.</span>position <span>=</span>
    input<span>[</span>global_id<span>.</span>x<span>]</span><span>.</span>position <span>+</span>
    input<span>[</span>global_id<span>.</span>x<span>]</span><span>.</span>velocity <span>*</span> <span>TIME_STEP</span><span>;</span></span>
<span><span>}</span></span></code></pre><p>I hope that the vast majority of this shader code contains no surprises for you at this point.</p>
<figure>
  <video width="512" height="512" src="https://surma.dev/assets/step3.19ad443e.webm" type="video/webm" autoplay="" muted="" loop="" controls=""></video>
  <figcaption>Every frame, WebGPU is used to update the position of the balls. They are drawn to screen using Canvas2D.</figcaption>
</figure>
<p>Lastly, all we need to do is read the <code>output</code> buffer back into JavaScript, write some Canvas2D code to visualize the contents of the buffer and put it all in a <code>requestAnimationFrame()</code> loop. You can see the result this <a href="https://surma.dev/things/webgpu/step3/index.html">demo</a>.</p>
<h2>Performance</h2>
<p>The previous demo just moves each ball along their velocity vector. Not exactly thrilling or computationally complex. Before we look at the performance characteristics of our creation, let me drop in some proper physics calculations in the shader. I won‚Äôt explain them here ‚Äî the blog post is long enough as it is ‚Äî but I will say that I took the maximally na√Øve approach: every ball checks for collisions <em>with every other ball</em>. If you are curious, you can take a look at the source code of the <a href="https://surma.dev/things/webgpu/step4/index.html">final demo</a>, which also contains links to the resources I used to write the physics-y bits.</p>
<figure>
  <video width="512" height="512" src="https://surma.dev/assets/step4.8649df2d.webm" type="video/webm" autoplay="" muted="" loop="" controls=""></video>
  <figcaption>... now with bouncy walls and bouncy balls!</figcaption>
</figure>
<p>I don‚Äôt want to take any exact measurements of this experiment as I haven‚Äôt optimized the physics algorithm nor my usage of WebGPU. However, the fact that even this na√Øve implementation performs really well (on my M1 MacBook Air) is impressive to me. I can go to around 2500 balls before we drop below 60fps. However, looking at the trace, it‚Äôs clear that at 2500 balls the bottleneck is Canvas2D trying to draw the scene, not the WebGPU calculations.</p>
<figure>
	<picture loading="lazy" width="1024" height="625">
    <source srcset="https://surma.dev/assets/performance.63b22948.avif " type="image/avif">
    <img src="https://surma.dev/assets/performance.9d5db577.jpeg">
  </picture>
  <figcaption>At 14000 balls, the raw GPU computation time reaches ~16ms on a M1 MBA.</figcaption>
</figure>
<p>To see how fast this really is, I disabled rendering and instead used <a href="https://developer.mozilla.org/en-US/docs/Web/API/Performance/measure"><code>performance.measure()</code></a> to see how many balls I can simulate before exhausting my frame budget of 16ms. This happens at around 14000 balls on my machine. Something this unoptimized running this fast really makes me drunk on power with how much computational power WebGPU gives me access to.</p>
<h2>Stability &amp; Availability</h2>
<p>WebGPU has been worked on for a while and I think the standards group is eager to declare the API as stable. That being said, the API is only available in Chrome and Firefox behind a flag. I‚Äôm optimistic about Safari shipping this API but at the time of writing there‚Äôs nothing to see in Safari TP just yet.</p>
<p>In terms of stability, some changes landed even while I was doing the research for this article. For example, the syntax for attributes was changed from <code>[[stage(compute), workgroup_size(64)]]</code> to <code>@compute @workgroup_size(64)</code>. At the time of writing, Firefox is still on the old syntax. <code>passEncoder.end()</code> used to be <code>passEncoder.endPass()</code>. There are also some things in the spec that haven‚Äôt been implemented in any browser yet like <a href="https://gpuweb.github.io/gpuweb/#dom-gpuprogrammablestage-constants">shader constants</a> or the API being available on mobile devices.</p>
<p>Basically what I am saying is: Expect some more breaking changes to happen while the browsers and standards folks are on the home stretch of this API‚Äôs journey to ‚ú®stable‚ú®.</p>
<h2>Conclusion</h2>
<p>Having a modern API to talk to GPUs on the web is going to be very interesting. After investing time to overcome the initial learning curve, I really feel empowered to run massively parallel workloads on the GPU using JavaScript. There is also <a href="https://github.com/gfx-rs/wgpu">wgpu</a>, which implements the WebGPU API in Rust, allowing you to use the API outside the browser. wgpu also support WebAssembly as a compile target, so you could run your WebGPU program natively outside the browser and inside the browser via WebAssembly. Fun fact: <a href="https://deno.land/">Deno</a> is the first runtime also support WebGPU out of the box (thanks to wgpu).</p>
<p>If you have questions or are running into problems, there is <a href="https://matrix.to/#/#WebGPU:matrix.org">a Matrix channel</a> with many WebGPU users, browser engineers and standards folks that have been incredibly helpful to me. Go get your feet wet! Exciting times.</p>
<p><em>Thanks to <a href="https://twitter.com/tojiro">Brandon Jones</a> for proof-reading this article and the <a href="https://matrix.to/#/#WebGPU:matrix.org">WebGPU Matrix channel</a> for answering all my questions.</em></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exploring Linear A (137 pts)]]></title>
            <link>https://lineara.xyz/</link>
            <guid>36750743</guid>
            <pubDate>Sun, 16 Jul 2023 19:47:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lineara.xyz/">https://lineara.xyz/</a>, See on <a href="https://news.ycombinator.com/item?id=36750743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="help_menu" onclick="(function(){ help_menu.style.display='none';})();">
<p><span>  &nbsp;Click on words to filter inscriptions.<br></span>
<span>  <br></span>
<span>  &nbsp;'<b>w</b>' - highlight words according to frequency in the corpus.<br></span>
<span>  <br></span>
<span>  &nbsp;'<b>1 to 9</b>' - save the current search terms under that number. <br></span>
<span>  &nbsp;'<b>Shift + 1 to 9</b>' - retrieve the search terms saved under that number. <br></span>
<span>  &nbsp;'<b>/</b>' - Search for an inscription. To use regular expressions, precede regular<br></span>
<span>              expression operators with a '\', e.g. -JA\$ to search for -JA<br></span>
<span>              at the end of words.<br></span>
<span>  <br></span>
<span>  &nbsp;Hover the mouse over an inscription and press:<br></span>
<span>  &nbsp;'<b>i</b>' - to copy an image of the inscription to the clipboard.<br></span>
<span>  &nbsp;'<b>y</b>' - to show John Younger's commentary for the inscription.<br></span>
<span>  &nbsp;'<b>t</b>' - to toggle between translation and transliteration.<br></span>
<span>  &nbsp;'<b>z</b>' - to zoom/unzoom an inscription.<br></span></p><a href="https://linearb.xyz/" target="_blank">
    <p>Linear B</p>
  </a>
  <a href="https://linear0.xyz/" target="_blank">
    <p>Cretan Hieroglyphics</p>
  </a>
  <a href="https://linearc.xyz/" target="_blank">
    <p>Cypro-Minoan</p>
  </a>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Underwater ears everywhere (374 pts)]]></title>
            <link>https://computer.rip/2023-07-15-underwater-ears-everywhere.html</link>
            <guid>36750716</guid>
            <pubDate>Sun, 16 Jul 2023 19:44:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://computer.rip/2023-07-15-underwater-ears-everywhere.html">https://computer.rip/2023-07-15-underwater-ears-everywhere.html</a>, See on <a href="https://news.ycombinator.com/item?id=36750716">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<h2>&gt;&gt;&gt; 2023-07-15 underwater ears everywhere</h2>

<p>Programming note: the subscribe link was broken for a while because I am
bad at computers (yet another case of "forgot to enable the systemd unit").
It's fixed now. The unsubscribe link was also broken and is now fixed but,
you know, maybe that was a feature. Did wonders for reader retention.</p>
<p>You may have seen some recent press coverage about events surrounding the
<em>Titanic</em> and another notable loss at sea. I'm not going to rehash much of
anything around the <em>Titan</em> because it's sort of an exhaustively covered topic
in the mainstream press... although I will defend the Logitech controller by
noting that Playstation-style controllers are extremely popular interfaces in
robotics and 3D navigation (two symmetric analog sticks, unlike other major
game controllers), and considering the genuine PS4 controller's terrible
Bluetooth pairing UX with non-Playstation devices, the Logitech is probably a
more reliable choice. And they did have spares on board!</p>
<p>I actually want to talk a bit about remote sensing, but of a rather different
kind than I usually mention: hydrophones and wide-area sonar. This
little-discussed military surveillance technology played a major role in the
saga of the <em>Titan,</em> and it's one that seems poorly understood by both
journalists and internet randos. I've seen a lot of Bad Takes about the Navy's
involvement in <em>Titan</em> and I want to suggest a few things that might cause you
to interpret the situation differently.</p>
<p>Submarines are very difficult to detect. This is a bad property for tourist
ventures to the deep sea, but a very useful property to the military. Further,
radio communications underwater are extremely difficult. Salt water attenuates
radio signals very quickly, and while the effect decreases as you go to lower
frequencies, it never goes away. Even the US Navy's sophisticated VLF systems
require submarines to be relatively close to the surface (or rather use a wire
antenna relatively close to the surface) for reception---VLF signals only
penetrate seawater by up to about 40 meters. ELF offers better penetration into
hundreds of meters, but ELF facilities are extremely expensive to build and
operate and the receive antennas are formidably large, so the US Navy retired
its ELF infrastructure in 2004.</p>
<p>For this reason, submersibles like <em>Titan</em> communicate with their surface
support vessels via acoustic modems. This method is surprisingly reliable but
produces a very low bitrate, thus the limitation of text messaging. Similar
technology is used in deep-sea oil exploration, <em>Titan</em> likely used a
commercial product for the data link.</p>
<p>The thing that propagates best underwater, in fact far better than above water
and even better as you get deeper, is sound. The potential of sound for
detecting and locating submarines is well-known. The first prominent use of
this approach, widely called sonar, came about during the First World War when
an anti-submarine surface ship successfully detected a submarine directly below
it via reflected sound. This type of sonar works well for locating nearby
submarines, but it is an <em>active</em> technique. That is, an active sonar must
<em>emit</em> a sound in order to receive the reflection. This is actually quite
undesirable for many military applications, because emitting a sound reveals
the presence (and with sufficient receiving equipment, location) of the sonar
device.  Anti-submarine ships stopped using active sonar on a regular basis
fairly quickly, since it prominently advertised their presence to all of the
submarines in the area.</p>
<p>Much more appealing is <em>passive</em> sonar, which works by listening for the sounds
naturally created by underwater vehicles. With a sensitive directional
hydrophone (an underwater microphone), you can hear the noise created by the
screws of a submarine. By rotating the directional hydrophone, you can find the
point of peak amplitude and thus the bearing to the submarine. This basic
submarine hunting technique remains the state of the art today, but the receiving
equipment has become far more capable and automated.</p>
<p>There is an arms race here, an arms race of quietness. I am resisting here the
urge to quote the entire monologue from the beginning of <em>The Hunt for Red
October,</em> but rest assured that [the Americans] will tremble again, at the
sound of [the Soviet's] silence. In practice the magnetohydrodynamic propulsion
technology depicted on the <em>Red October</em> has never proven very practical for
submarines, although it was demonstrated in one very futuristic surface vessel
built by Mitsubishi and called <em>Yamato 1</em> (fortunately it fared better than the
battleship by that name). Instead, the battle of submarine silence has mostly
revolved around obscure technical problems of fluid dynamics, since one of the
loudest noises made by submarines is the cavitation around the screw. I don't
know if this is true today, but at least years ago the low-noise design of the
screw on modern US submarines was classified, and so the screw was covered by a
sheath whenever a submarine was out of the water.</p>
<p>Passive sonar can be performed from ships and even aircraft-deployed buoys, but
for the purpose of long-term maritime sovereignty it makes sense to install
permanent hydrophones that function as a defensive perimeter. Just such a
system was designed in the 1950s by (who else?) AT&amp;T. AT&amp;T had the expertise
not only in acoustic electronics, but also undersea cable laying, a key
component of any practical underwater surveillance system. Large arrays of
hydrophones, spaced along cables, were laid on the ocean floor. The sounds
detected by these hydrophones were printed on waterfall diagrams and inspected
by intelligence analysts, who relied on experience and no small amount of
educated guessing to recognize different types of marine life, geological
phenomena, and vessels at sea.</p>
<p>This system, called SOSUS for Sound Surveillance System, remained secret until
1991. The secrecy of SOSUS is no great surprise, as it was one of the most
important military intelligence systems of the Cold War. It presented a problem
as well, though, as few in the Navy were aware of the details of the system and
ship crews sometimes felt the abbreviated, zero-detail intelligence messages
from SOSUS to be confusing and unreliable. They were being told of likely
submarine detections, but knowing nothing about the system they had come from,
they didn't know whether or not to take them seriously.</p>
<p>By the 1960s, SOSUS consisted of hundreds of individual hydrophones installed
in long, cable-tethered arrays. Cables connected the hydrophone arrays to
highly secured terminal facilities on the coast, which the Navy explained with
a rather uninspiring cover story about undefined survey work. Over the
following decades, computers were applied to the task, automatically detecting
and classifying acoustic signatures. This early automation work inspired
significant research and development on signal processing and pattern matching
in both the military and Bell Laboratories, creating early precedents for the
modern field of machine learning. Additionally, computer and telecommunications
advancements allowed for remote control of the arrays, significantly reducing
the staff required for the program and leading to the eventual closure of many
of the terminal naval facilities.</p>
<p>In 1984, SOSUS was renamed to IUSS, the Integrated Underwater Surveillance
System. This new name reflected not only the increasing automation, but also
the inclusion of several surface vessels in the system. These vessels,
initially the USNS <em>Stalwart</em> and USNS <em>Worthy</em>, functioned as mobile IUSS
arrays and could be moved around to either expand coverage or provide more
accurate locating of a suspected target.</p>
<p>The existence of IUSS was finally declassified in 1991, although it was well
known before that point due to several prominent press mentions. Since the
declassification of IUSS it has enjoyed a dual-use role with the scientific
research community, and IUSS is one of the primary sources of hydrophone
data for marine biology. Today, IUSS automatically detects and classifies
both submarines and whales.</p>
<p>The potential of passive sonar systems to detect submarine accidents is
well-known. The 1968 loss of Soviet submarine <em>K-129</em> was detected by SOSUS,
and the location estimate produced by SOSUS facilitated the recovery of <em>K-129</em>
by the <em>Hughes Glomar Explorer,</em> one of the most fascinating naval intelligence
operations of American history. 1968 was a bad year for submarines with four
lost with all hands, and SOSUS data was used to locate at two of them (Soviet
<em>K-129</em> and US <em>Scorpion</em>. French <em>Minerve</em> and Israeli <em>Dakar</em> would not be
found for decades).</p>
<p>This all brings us to the modern era. <em>Titan</em> was lost on, presumably, the
18th of June. It was not located on the sea floor until the 22nd, four days
later. Press reporting after the discovery included a Navy statement that
IUSS had detected and located the implosion.</p>
<p>This has lead to a somewhat common internet hot take: that the Navy had
definitive information on the fate of <em>Titan</em> and, for some reason, suppressed
it for four days. I believe this to be an unwarranted accusation, and the
timing of the location of the wreck and the statement on IUSS are readily
explainable.</p>
<p>First, we must consider the nature of remote sensing. Remote sensing systems,
whether space-based or deep underwater, produce a large volume of data. The
primary source of actionable information in modern real-time remote sensing
are computer systems that use machine learning and other classification
methods to recognize important events. These computer systems must be trained
on those events, using either naturally or artificially created samples, in
order to correctly classify them. A major concern in naval intelligence is the
collection of up-to-date acoustic signatures for contemporary vessels so that
IUSS can correctly identify them.</p>
<p>A secondary method is retrospective analysis, in which human intelligence
analysts review historic data to look for events that were not classified by
automation when they occurred. Retrospective analysis, particularly with new
signature information, can often yield additional detections. Consider the case
I have previously discussed of the Chinese spy balloons: once signature
information (almost certainly RF emissions) were collected, retrospective
analysis yielded several earlier incidents that were not detected at the time
due to the lack of signatures.</p>
<p>Like the RF spectrum, the ocean contains a lot of noises. They come from
wildlife, from geological processes, and from commercial shipping, all besides
naval operations. The Navy does not rigorously investigate every sound
underwater, it can't possibly do so.</p>
<p>When the Navy became aware of the missing <em>Titan,</em> analysts almost certainly
began a retrospective analysis of IUSS data for anything that could indicate
its fate. They apparently detected loud noises and were able to locate the
source as near the <em>Titanic</em> wreckage, probably fairly quickly after the
<em>Titan</em> was first reported missing.</p>
<p>Here is the first challenge, though: the <em>Titan</em> was a new submersible of novel
(if not necessarily well thought out) construction. The Navy has some
familiarity with the acoustic signatures of imploding military submarines based
on incidentally lost submarines and, in at least one case, the intentional
torpedoing of a submarine to record the resulting acoustics (the <em>Sterlet</em>).
This data is used to produce a signature against which new signals can be
compared. Because of the significant differences in size and construction
between <em>Titan</em> and military submarines, the Navy likely had very low
confidence that known acoustic signatures of catastrophic losses were
applicable. The total number of submarines to have ever imploded underwater is
quite small, and none were of similar size and construction to <em>Titan</em>. The
point is that while intelligence analysts likely <em>suspected</em> they had evidence
of implosion, they probably had low confidence in that conclusion.</p>
<p>It is unwise, in the course of a search and rescue operation, to report that
you <em>think</em> the vessel was irrecoverably lost. Doing so can compromise search
operations by creating political pressure to end them, while making the
situation of families and friends worse. It is customary to be very cautious
with the release of inconclusive information in events like this. The problems
are exemplified by the Coast Guard's announcement that another passive sonar
system had detected possible banging sounds, which motivated a lot of reporting
making wild conclusions based on acoustic signatures that were likely
unrelated.</p>
<p>The more damning accusation, though, is this: did the Navy withhold information
on the detection from searchers out of concern for secrecy? Setting aside that
this makes little sense considering that SOSUS and its capabilities have been
widely known to the public for decades, and the search site was well within
historically published coverage estimates for SOSUS, this accusation doesn't
align with the timeline of the search.</p>
<p>The first search vessel capable of deep undersea exploration, the ROV <em>Pelagic
Odysseus 6k</em>, arrived on the scene on the morning of the 22nd. Just five hours
later, <em>Odysseus</em> had located the wreckage. Considering that the descent to depth
alone would have taken Odysseus over an hour, the wreckage was located extremely
quickly in the challenging undersea environment. One reason is obvious:
the wreckage of <em>Titan</em> was close to the <em>Titanic,</em> although the <em>Titanic</em> debris
field is large and searching it all would have taken hours. The second reason
became known shortly after: when <em>Odysseus</em> began its search, they had almost
certainly already been tipped off by the Navy as to the location of the possible
implosion.</p>
<p>The Navy did not withhold information on the detection for four days out of some
concern for secrecy. Instead, the information was not known to the public for
four days because that was when the search team was first able to actually
investigate the Navy's possible detection.</p>
<p>Indeed, the idea that the Navy suppressed the information seems to come only
from the rumor mill and internet repetition of half-read headlines. The
original press coverage of the IUSS detection, from the WSJ, states that the
Navy reported the finding to the Navy commander on-scene at the search effort
immediately. It does include the amusing sentence that "the Navy asked that the
specific system used not be named, citing national security concerns." This
might seem like a huge cover up to those unfamiliar with intelligence programs,
but it's perfectly in line with both normal military concerns around classified
systems (which are often known by multiple names which must be kept
compartmentalized for unclassified contracting) and the specific history of
IUSS, which during its period of secrecy had problems with being accidentally
named in unclassified reports multiple times.</p>
<p>IUSS is now a smaller system than it once was, although with improving
technology its coverage has probably expanded rather than contracted. It still
serves as a principal method of detecting submarines near the US, an
important concern since submarines are one of the main delivery mechanisms
for nuclear weapons. IUSS is just one of several semi-secret underwater sensing
systems used by the Navy.</p>
<p>A not totally related system that will nonetheless be of interest to many of my
readers (who I suspect to be somewhat concentrated in the San Francisco Bay
Area) is the San Francisco Magnetic Silencing Range. A small building in the
parking lot of Marina Green, complete with a goofy little control tower from
the era of manned operation, is the above-water extent of this system that uses
underwater magnetometers to measure the magnetic field of Navy vessels passing
through the Golden Gate. Since underwater mines are often triggered by
magnetometers, the Navy ensures that the magnetization of vessel hulls does not
exceed a certain limit. If it does, the vessel can be degaussed at one of
several specially-equipped Navy berths---inspiration for at least one episode
of The Next Generation. Similar arrays exist at several major US ports.</p>
<p>The building itself is long-disused, and the array is now fully remote
controlled. When I lived in San Francisco it was abandoned, but I see that it
has apparently been restored to function as the harbormaster's office. I
appreciate the historic preservation effort but something is lost with the
removal of the Navy's sun-faded signage.</p>
	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to register a Kei truck in Pennsylvania (288 pts)]]></title>
            <link>https://danwilkerson.com/posts/2023-05-30-how-to-register-a-kei-truck-in-pa</link>
            <guid>36750554</guid>
            <pubDate>Sun, 16 Jul 2023 19:26:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danwilkerson.com/posts/2023-05-30-how-to-register-a-kei-truck-in-pa">https://danwilkerson.com/posts/2023-05-30-how-to-register-a-kei-truck-in-pa</a>, See on <a href="https://news.ycombinator.com/item?id=36750554">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Kei trucks are a special designation of Japanese car that have very small
engines (660cc) and dimensions (~10m^3). <a href="https://en.wikipedia.org/wiki/Kei_truck">Tell 'em,
Wikipedia</a>. Whoever coined
the idea of "creative constraints" should take a gander at Kei trucks; there's
definitely an upper bound:</p>
<p><span>
      <a href="https://danwilkerson.com/static/6a0d69f1f2a05039b0372fbee9bb937a/41099/comparison.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="The Office 'corporate wants you to tell the difference between two pictures' meme with the Suzuki Carry and Honda Acty, which look
identical." title="" src="https://danwilkerson.com/static/6a0d69f1f2a05039b0372fbee9bb937a/41099/comparison.jpg" srcset="/static/6a0d69f1f2a05039b0372fbee9bb937a/d2f63/comparison.jpg 163w /static/6a0d69f1f2a05039b0372fbee9bb937a/c989d/comparison.jpg 325w https://danwilkerson.com/static/6a0d69f1f2a05039b0372fbee9bb937a/41099/comparison.jpg 500w" sizes="(max-width: 500px) 100vw, 500px" loading="lazy" decoding="async">
  </a>
    </span></p>
<p>Japan's tax incentives have created a brisk export business for often lightly
used trucks. As a result, they're available for purchase in the US.</p>
<p>I recently decided to do just that and had a heck of a time getting on the road.
This guide serves as a lamp in the dark, helping guide fellow fools to
road-legal ownership.</p>
<p>Why did I want one? I'm glad you ask - Kei trucks:</p>
<ul>
<li>Have a 6ft bed</li>
<li>Which can fold flat</li>
<li>And carry ~1000lbs</li>
<li>With 4WD (including a locking rear diff<sup id="fnref-1"><a href="#fn-1">1</a></sup> and axle)</li>
<li>While getting 40 MPG</li>
<li>For $X,000.0</li>
<li>With very low mileage (22k miles in my instance)</li>
</ul>
<p>I often need to haul stuff and had gotten sick of U-Haul rentals. I decided to
find a cheap 90s/early 00s small truck. Pandemic prices were getting me
rusted-out Rangers or 250k mile Tacomas<sup id="fnref-2"><a href="#fn-2">2</a></sup>; then the Kei came into my life.
Libby and I were pulling in to <a href="https://cjreuse.org/">Construction Junction</a>
when a couple in a Honda Acty pulled in behind us. I picked the guy's brain
about how useful it was, getting it insured, and so forth; it all sounded
pretty straightforward, and I was sold.</p>
<h2>Step 0: Find a truck</h2>
<p>You have some choices here: you can import it yourself, buy from an importer, or
risk craigslist. Prices rise as you work down that list.</p>
<p>You'll need to find a truck that is at least 25 years old. In the US, cars more
youthful than that must comply with <a href="https://helpspanish.cbp.gov/s/article/Article-278?language=en_US">FMVSS</a>
(safety standards). If you buy a more recent vintage, <strong>you won't be able to
get it registered anywhere</strong>, so pay attention to that model year!</p>
<p>There are exporter websites out there promising a truck for
~$3-5k all in, provided you can pick it up at the port and are willing to do the
paperwork. As I understand it, the importation process is high stakes - if you
make a mistake, US Customs puts your truck in a garbage compactor and you're out
the money. If you're brave enough to try it, I wish you the best - come back
here after you're leaving your port of choice.</p>
<p>The safer route is to buy from an importer. If you're smart, you'll find an
importer who has already titled the vehicle and you'll be set. Of course, you
wouldn't be Googling "how to title a kei truck Pennsylvania" in that case, but
here we are. "Don't worry," you thought, "it can't take <em>that</em> long to get a
title." <sup id="fnref-3"><a href="#fn-3">3</a></sup></p>
<h2>Step 1: Getting the car titled</h2>
<p>Wait, back it up - you're going to need some paperwork only the seller can get
you. Otherwise I honestly have no idea what will happen to you or your truck.</p>
<h2>Right, uh, Step -1: Make sure the seller has the paperwork</h2>
<p>Before you buy, make sure you'll be able to get everything you need to title
the thing. You'll need:</p>
<ul>
<li>The Export Certificate - this should look like this <span>
      <a href="https://danwilkerson.com/static/7ef6c640216900059c166e42a208bbe8/288ea/export_certificate.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="The export certificate from my Kei truck, listing the seller and exporter as well as some details about the car" title="" src="https://danwilkerson.com/static/7ef6c640216900059c166e42a208bbe8/6aca1/export_certificate.jpg" srcset="/static/7ef6c640216900059c166e42a208bbe8/d2f63/export_certificate.jpg 163w /static/7ef6c640216900059c166e42a208bbe8/c989d/export_certificate.jpg 325w /static/7ef6c640216900059c166e42a208bbe8/6aca1/export_certificate.jpg 650w https://danwilkerson.com/static/7ef6c640216900059c166e42a208bbe8/288ea/export_certificate.jpg 841w" sizes="(max-width: 650px) 100vw, 650px" loading="lazy" decoding="async">
  </a>
    </span></li>
<li>Proof you paid sales tax - PennDOT will really, really want you to prove this, like<br>
a lot. I sent in a photocopy of the receipt from the place I bought the
truck, <a href="https://www.trlawnandgarden.com/">Twin Ridge Lawn and Garden</a>.</li>
</ul>
<h2>Great, back to Step 1a: Translate your Certificate</h2>
<p>So you're gonna need a translation of that certificate. It's not required, but
it supposedly helps to have the document notarized with an American Translation
Association seal. You definitely need the translation to be accompanied by a
sworn affadavit from the translator, though; they should know what to do.</p>
<p>I used the <a href="https://www.atanet.org/directory/">American Translators Association directory</a>
and emailed a handful of eligible translators. The prices and response times
were all over the map. In the end I ended up working with
<a href="http://japanesetoyou.com/">Patricia Pringle</a>; Patricia was super responsive
and very reasonably priced!</p>
<h2>Step, I don't know, 3? 1b? 2.5?: How much does your truck weigh?</h2>
<p>Okay, so you've got your translated export certificate, you've got your proof
of paying sales tax. Forget all that - now you're going to need to prove you
know how much your truck weighs. Why? Because in the state of Pennsylvania that
information is absolutely crucial to verifying the VIN of your vehicle. You're
going to need an <a href="http://www.ptatsvcs.com/wp-content/themes/potomactag/pdf/MV-41.pdf">MV-41</a>,
but first, you're going to need to get the <a href="https://www.falkirkvanhire.com/articles/unladen-vehicle-weight/">unladen
weight</a> of
your shiny new truck.</p>
<p>You'll need to get your truck to a salvage yard or other weigh station<sup id="fnref-4"><a href="#fn-4">4</a></sup> where a
weighmaster can weigh and certify the weight of your vehicle - I visited <a href="https://danddautosalvage.com/">D&amp;D
Auto Salvage</a> on the recommendation of my
mechanic, <a href="https://www.yelp.com/biz/walters-automotive-pittsburgh">Tim Walters</a>.
Thanks, Tim!</p>
<p>Take this slip of paper to a mechanic along with your MV-41 and your truck<sup id="fnref-4"><a href="#fn-4">4</a></sup>.
They'll need to fill out the MV-41, verifying the weight from the weighmaster
and the VIN in your truck (which may be in an unusual location - mine was
stamped on the wheel well). If you're lucky, you'll find a guy who's done this
recently and also thinks imports are super cool - <a href="https://www.yelp.com/biz/all-auto-repair-pittsburgh">thanks
Naiel</a>!</p>
<h2>Step 4: Insurance</h2>
<p>This was actually relatively easy. Don't even bother trying the "Request a
Quote" forms, just start calling shops that give you multiple quotes (e.g.
<a href="https://www.thezebra.com/">Zebra</a>). They'll help tremendously. Almost all the
providers declined to cover my <del>dangerous exotic trash hauler</del> minitruck, but
<a href="https://www.safeco.com/">Safco</a> gave me an acceptable rate on liability
insurance<sup id="fnref-5"><a href="#fn-5">5</a></sup>. Getcher self a copy of your proof of insurance.</p>
<h2>Step 1000: Submit the paperwork</h2>
<p>Once you've gotten your:</p>
<ul>
<li>Original export certificate</li>
<li>Translated export certificate</li>
<li>Proof of paid PA sales tax</li>
<li>Filled out MV-41</li>
<li>The mileage on your odometer, converted to miles</li>
<li>The gross weight of the vehicle, converted to pounds</li>
<li>Proof of insurance</li>
<li>A photo or two of the truck<sup id="fnref-6"><a href="#fn-6">6</a></sup></li>
</ul>
<p>It's finally time to fill out the big kahuna - the MV-1, a request for a title.
You won't find this form online - <a href="https://www.dmv.pa.gov/Driver-Services/New-Resident-Relocation-Information/New%20Residents/Pages/Motor-Vehicle-Information-for-New-Residents.aspx#:~:text=Form%20MV%2D1%20is%20not,issue%20you%20a%20temporary%20registration.">only an authorized agent can fill one
out</a>.
<a href="https://www.dot.state.pa.us/public/dvspubsforms/BMV/BMV%20Fact%20Sheets/fs-mv1.pdf">There are some helpful instructions for filling this out
properly</a>,
I went with <a href="https://www.aaa.com/office/detail/Pittsburgh-PA-East%20Liberty-6969">AAA here in Pittsburgh</a>.
You're also going to find out that you can <a href="https://www.dmv.pa.gov/Pages/Mini-Trucks-FAQ.aspx">only register these vehicles as
antiques in PA</a>. This isn't
as onerous as it once was. You can't use the vehicle for commercial purposes,
and you're not meant to use it more than occasionally - for most folks, that
should fit the bill just fine.</p>
<p>Once you've got an agent, they'll take a look at all your documents and ask you
a few questions about the vehicle. It's important that you:</p>
<ul>
<li>Explain that the truck can be registered as an antique</li>
<li>Demonstrate repeatedly that <em>yes, you have paid the PA State sales tax</em><sup id="fnref-7"><a href="#fn-7">7</a></sup></li>
<li>Request it be titled as a truck</li>
<li>Ensure that they send in the <strong>original export certificate</strong> and not a copy</li>
</ul>
<p>At this point I'd like to share a bit of wisdom I received early on in this process:
this is a once-in-a-lifetime type situation for whomever you're going to
work with. You are the edge case and corner case they've been warned about.
You are going to be their anecdote about the wild stuff that comes through the
door. "Some kind of crazy tiny truck from Japan," they'll laugh, "My God!
Months!". They'll accentuate that last point by waving their hands in the air,
like they're waving an imaginary beachball from side-to-side. Be ready to be
patient.</p>
<p>Once the paperwork is submitted, you'll wait about ~30 days before you hear
from PennDot. If you're super lucky you'll get a plate in the mail along with
the title in a few weeks. If you're a little lucky, they'll send your agent
feedback on your packet that you can correct and resubmit. You're going to need
to follow up with your agent - they're probably not going to reach out. Once
you've addressed their concerns, it'll be another 30 days before you'll know if
things are fixed. If you're really unlucky, you'll do this loop, oh, I don't
know, three or four times<sup id="fnref-8"><a href="#fn-8">8</a></sup>? Finally, you'll get a thick envelope in the mail
and inside will be your prize - an antique PA plate.</p>
<p><span>
      <a href="https://danwilkerson.com/static/e11b26f935ad984a52e00e0a2965c087/ac99c/me_and_truck.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="Me and the truck outside McBrooms after a 3ROC run" title="" src="https://danwilkerson.com/static/e11b26f935ad984a52e00e0a2965c087/6aca1/me_and_truck.jpg" srcset="/static/e11b26f935ad984a52e00e0a2965c087/d2f63/me_and_truck.jpg 163w /static/e11b26f935ad984a52e00e0a2965c087/c989d/me_and_truck.jpg 325w /static/e11b26f935ad984a52e00e0a2965c087/6aca1/me_and_truck.jpg 650w /static/e11b26f935ad984a52e00e0a2965c087/7c09c/me_and_truck.jpg 975w /static/e11b26f935ad984a52e00e0a2965c087/01ab0/me_and_truck.jpg 1300w https://danwilkerson.com/static/e11b26f935ad984a52e00e0a2965c087/ac99c/me_and_truck.jpg 1536w" sizes="(max-width: 650px) 100vw, 650px" loading="lazy" decoding="async">
  </a>
    </span></p>
<h2>Step 1001: Go buy some screws</h2>
<p>Ah, crap, sorry - you could have done this way sooner. The holes in the plate
won't line up with the holes in your plate holder - damn metric system. Grab
a drill and a pencil, mark where the holes should be, apply drill, and you'll
be all set. You'll need two M6 20mm galvanized screws, which you can find at
any hardware store.</p>
<h2>So what's the downside?</h2>
<p>In case you're still weighing the pros and cons, here's my take as of a few months
into ownership.</p>
<ul>
<li>It attracts <em>a lot</em> of attention. This might be a little overwhelming.</li>
<li>You're topping out at 60 MPH, maybe.</li>
<li>No air bags, air conditioning, power steering, or crumple zones.</li>
<li>Parts are all across an ocean.</li>
<li>Right-hand drive is unsettling for everyone involved.</li>
</ul>
<p>I've been really happy with it - it's a lot cheaper than a
similarly sized truck would have run me, with way fewer miles on the odometer.
For bopping around town hauling lumber, yard waste, and furniture, it's perfect.
Just don't expect to be cruising on the interstate in one of these.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChatGPT use declines as users complain about ‚Äòdumber‚Äô answers (108 pts)]]></title>
            <link>https://www.techradar.com/computing/artificial-intelligence/chatgpt-use-declines-as-users-complain-about-dumber-answers-and-the-reason-might-be-ais-biggest-threat-for-the-future</link>
            <guid>36750200</guid>
            <pubDate>Sun, 16 Jul 2023 18:45:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techradar.com/computing/artificial-intelligence/chatgpt-use-declines-as-users-complain-about-dumber-answers-and-the-reason-might-be-ais-biggest-threat-for-the-future">https://www.techradar.com/computing/artificial-intelligence/chatgpt-use-declines-as-users-complain-about-dumber-answers-and-the-reason-might-be-ais-biggest-threat-for-the-future</a>, See on <a href="https://news.ycombinator.com/item?id=36750200">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" alt="A robot head on fire with the ChatGPT logo on one side." onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg"><source type="image/jpeg" alt="A robot head on fire with the ChatGPT logo on one side." onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1920-80.jpg 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg"><img src="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-320-80.jpg" alt="A robot head on fire with the ChatGPT logo on one side." onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1920-80.jpg 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: OpenAI, Shutterstock)</span>
</figcaption>
</div>

<div id="article-body">
<p>Is <a href="https://www.techradar.com/news/chatgpt-explained"><u>ChatGPT</u></a> old news already? It seems impossible, with the explosion of AI popularity seeping into every aspect of our lives - whether it‚Äôs digital masterpieces forged with the <a href="https://www.techradar.com/features/best-ai-art-generators-compared"><u>best AI art generators</u></a> or <a href="https://www.techradar.com/computing/artificial-intelligence/microsofts-new-ai-shopping-tools-will-create-a-buying-guide-just-for-you"><u>helping us with our online shopping</u></a>.</p><p>But despite being the leader in the AI arms race - and powering <a href="https://www.techradar.com/news/bing-ai-chat-is-now-waitlist-free-and-about-to-get-even-more-powerful"><u>Microsoft‚Äôs Bing AI</u></a> - it looks like <a href="https://www.techradar.com/tag/chatgpt" data-auto-tag-linker="true">ChatGPT</a> might be losing momentum. According to <a href="https://www.similarweb.com/blog/insights/ai-news/chatgpt-traffic-drops/" data-url="https://www.similarweb.com/blog/insights/ai-news/chatgpt-traffic-drops/"><u>SimilarWeb</u></a>, traffic to OpenAI‚Äôs ChatGPT site dropped by almost 10% compared to last month, while metrics from <a href="https://app.sensortower.com/overview/6448311069?metric=units&amp;utab=summary&amp;tab=about&amp;country=US" data-url="https://app.sensortower.com/overview/6448311069?metric=units&amp;utab=summary&amp;tab=about&amp;country=US"><u>Sensor Tower</u></a> also demonstrated that downloads of the iOS app are in decline too.</p><p>As reported by <a href="https://www.businessinsider.com/openai-gpt4-ai-model-got-lazier-dumber-chatgpt-2023-7?r=US&amp;IR=T" data-url="https://www.businessinsider.com/openai-gpt4-ai-model-got-lazier-dumber-chatgpt-2023-7?r=US&amp;IR=T"><u>Insider</u></a>, paying users of the more powerful GPT-4 model (access to which is included in <a href="https://www.techradar.com/news/chatgpt-plus-will-get-a-big-update-this-week-heres-why-its-a-big-deal"><u>ChatGPT Plus</u></a>) have been complaining on social media and OpenAI‚Äôs own forums about a dip in output quality from the <a href="https://www.techradar.com/tag/chatbot" data-auto-tag-linker="true">chatbot</a>.</p><p>A common consensus was that GPT-4 was able to generate outputs faster, but at a lower level of quality. <a href="https://twitter.com/petergyang" data-url="https://twitter.com/petergyang"><u>Peter Yang</u></a>, a product lead for Roblox, took to Twitter to decry the bot‚Äôs recent work, claiming that ‚Äúthe quality seems worse‚Äù. One forum user said the recent GPT-4 experience felt ‚Äúlike driving a Ferrari for a month then suddenly it turns into a beaten up old pickup‚Äù.</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">GPT4's output has changed recently.It generates faster, but the quality seems worse.Perhaps <a href="https://www.techradar.com/tag/openai" data-auto-tag-linker="true">OpenAI</a> is trying to save costs.Has anyone else noticed this?<a href="https://twitter.com/petergyang/status/1660314935208054785" data-url="https://twitter.com/petergyang/status/1660314935208054785">May 21, 2023</a></p></blockquote><p><span role="button" tabindex="0" aria-label="See more">See more</span></p></div><h2 id="why-is-gpt-4-suddenly-struggling-3">Why is GPT-4 suddenly struggling?</h2><p>Some users were even harsher, calling the bot ‚Äúdumber‚Äù and ‚Äúlazier‚Äù than before, with a <a href="https://community.openai.com/t/experiencing-decreased-performance-with-chatgpt-4/234269" data-url="https://community.openai.com/t/experiencing-decreased-performance-with-chatgpt-4/234269"><u>lengthy thread on OpenAI‚Äôs forums</u></a> filled with all manner of complaints. One user, ‚Äòbitbytebit‚Äô, described it as ‚Äútotally horrible now‚Äù and ‚Äúbraindead vs. before‚Äù.</p><p>According to users, there was a point a few weeks ago where GPT-4 became massively faster - but at a cost of performance. The AI community has speculated that this could be due to a shift in OpenAI‚Äôs design ethos behind the more powerful machine learning model - namely, breaking it up into multiple smaller models trained in specific areas, which can act in tandem to provide the same end result while being cheaper for OpenAI to run.</p><p>OpenAI has yet to officially confirm this is the case, as there has been no mention of such a major change to the way GPT-4 works. It‚Äôs a credible explanation according to industry experts like Sharon Zhou, CEO of AI-building company <a href="https://www.lamini.ai/" data-url="https://www.lamini.ai/"><u>Lamini</u></a>, who described the multi-model idea as the ‚Äúnatural next step‚Äù in developing GPT-4.</p><h2 id="ais-eating-ais-3">AIs eating AIs</h2><p>However, there‚Äôs another pressing problem with ChatGPT that some users suspect could be the cause of the recent drop in performance - an issue that the AI industry seems largely unprepared to tackle.</p><p>If you‚Äôre not familiar with the term ‚ÄòAI cannibalism‚Äô, let me break it down in brief: large language models (LLMs) like ChatGPT and <a href="https://www.techradar.com/news/google-bard"><u>Google Bard</u></a> scrape the public internet for data to be used when generating responses. In recent months, a veritable boom in AI-generated content online - including an unwanted torrent of <a href="https://www.techradar.com/computing/artificial-intelligence/amazon-has-a-big-problem-as-ai-generated-books-flood-kindle-unlimited"><u>AI-authored novels on Kindle Unlimited</u></a> - means that LLMs are increasingly likely to scoop up materials that were already produced by an AI when hunting through the web for information.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="An iPhone screen showing the OpenAI ChatGPT download page on the App Store" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/techradar/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1200-80.jpg.webp 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg"><source type="image/jpeg" alt="An iPhone screen showing the OpenAI ChatGPT download page on the App Store" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/techradar/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg"><img src="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg" alt="An iPhone screen showing the OpenAI ChatGPT download page on the App Store" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/techradar/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg" srcset="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1200-80.jpg 1200w"></picture></p></div><figcaption itemprop="caption description"><span>ChatGPT app downloads have slowed, indicating a decrease in overall public interest. </span><span itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure><p>This runs the risk of creating a feedback loop, where AI models ‚Äòlearn‚Äô from content that was itself AI-generated, resulting in a gradual decline in output coherence and quality. With numerous LLMs now available both to professionals and the wider public, the risk of AI cannibalism is becoming increasingly prevalent - especially since there‚Äôs yet to be any meaningful demonstration of how AI models might accurately differentiate between ‚Äòreal‚Äô information and AI-generated content.</p><p>Discussions around AI have largely focused on <a href="https://www.techradar.com/features/ai-really-could-destroy-the-world-but-not-in-the-way-you-might-expect"><u>the risks it poses to society</u></a> - for example, Facebook owner Meta recently declined to open up its new speech-generating AI to the public after it was <a href="https://www.techradar.com/news/meta-says-its-new-speech-generating-ai-tool-is-too-dangerous-to-release"><u>deemed ‚Äòtoo dangerous‚Äô to be released</u></a>. But content cannibalization is more of a risk to the future of AI itself; something that threatens to ruin the functionality of tools such as ChatGPT, which depend upon original human-made materials in order to learn and generate content.</p><p>Do you use ChatGPT or GPT-4? If you do, have you felt that there‚Äôs been a drop in quality recently, or have you simply lost interest in the chatbot? I‚Äôd love to hear from you <a href="https://twitter.com/NotThaneKrios" data-url="https://twitter.com/NotThaneKrios"><u>on Twitter</u></a>. With so many competitors now springing up, is it possible that OpenAI‚Äôs dominance might be coming to an end?&nbsp;</p>
</div>
<div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent"><section><p>Sign up to receive daily breaking news, reviews, opinion, analysis, deals and more from the world of tech.</p></section></div>
<div id="slice-container-authorBio"><p>Christian is TechRadar‚Äôs UK-based Computing Editor. He came to us from Maximum PC magazine, where he fell in love with computer hardware and building PCs. He was a regular fixture amongst our freelance review team before making the jump to TechRadar, and can usually be found drooling over the latest high-end graphics card or gaming laptop before looking at his bank account balance and crying.</p>

<p>Christian is a keen campaigner for LGBTQ+ rights and the owner of a charming rescue dog named Lucy, having adopted her after he beat cancer in 2021. She keeps him fit and healthy through a combination of face-licking and long walks, and only occasionally barks at him to demand treats when he‚Äôs trying to work from home.</p></div>



</section>


<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Structured output from LLMs without reprompting (156 pts)]]></title>
            <link>https://automorphic.ai/playground</link>
            <guid>36750083</guid>
            <pubDate>Sun, 16 Jul 2023 18:30:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://automorphic.ai/playground">https://automorphic.ai/playground</a>, See on <a href="https://news.ycombinator.com/item?id=36750083">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Google SoundStorm: Efficient Parallel Audio Generation (277 pts)]]></title>
            <link>https://google-research.github.io/seanet/soundstorm/examples/</link>
            <guid>36749059</guid>
            <pubDate>Sun, 16 Jul 2023 16:53:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://google-research.github.io/seanet/soundstorm/examples/">https://google-research.github.io/seanet/soundstorm/examples/</a>, See on <a href="https://news.ycombinator.com/item?id=36749059">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h3>Efficient Parallel Audio Generation</h3>
        <p>
          [<a href="https://arxiv.org/abs/2305.09636">paper</a>]
        </p>
        <p>
          Zal√°n Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, Marco Tagliasacchi
        </p>
        <p><b>Google Research</b></p>
      </div><p>
        <b>Abstract.</b>  
        We present SoundStorm, a model for efficient, non-autoregressive audio generation. 
        SoundStorm receives as input the semantic tokens of AudioLM, and relies on
        bidirectional attention and confidence-based parallel decoding to generate the tokens of a
        neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model
        produces audio of the same quality and with higher consistency in voice and acoustic
        conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on
        a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences 
        by synthesizing high-quality, natural dialogue segments, given a transcript annotated with 
        speaker turns and a short prompt with the speakers' voices.
      </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A tutorial quantum interpreter in 150 lines of Lisp (222 pts)]]></title>
            <link>https://www.stylewarning.com/posts/quantum-interpreter/</link>
            <guid>36748915</guid>
            <pubDate>Sun, 16 Jul 2023 16:41:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stylewarning.com/posts/quantum-interpreter/">https://www.stylewarning.com/posts/quantum-interpreter/</a>, See on <a href="https://news.ycombinator.com/item?id=36748915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<article>
<p><em>By Robert Smith</em></p>
<p><em>Simulating a universal, gate-based quantum computer on a classical
computer has many uses and benefits. The top benefit is the ability to
inspect the amplitudes of the system‚Äôs state directly. However, while
the mathematics is very well understood, implementing a
general-purpose simulator has largely been folk knowledge. In this
tutorial, we show how to build an interpreter for a general-purpose
quantum programming language called $\mathscr{L}$, capable of
executing most kinds of quantum circuits found in literature. It is
presented economically, allowing its implementation to take fewer than
150 lines of self-contained Common Lisp code. The language
$\mathscr{L}$ is very simple to extend, making the interpreter ripe
for testing different kinds of behavior, such as noise models.</em></p>
<div>
<hr>
<h2>Contents</h2>
<nav id="TableOfContents">
<ol>
<li><a href="#introduction">Introduction</a>
<ol>
<li><a href="#a-note-about-common-lisp">A note about Common Lisp</a></li>
<li><a href="#a-note-to-experienced-quantum-computing-practitioners">A note to experienced quantum computing practitioners</a></li>
</ol>
</li>
<li><a href="#the-language-mathscrl">The Language $\mathscr{L}$</a></li>
<li><a href="#the-quantum-state">The Quantum State</a>
<ol>
<li><a href="#where-does-one-qubit-live">Where does one qubit live?</a></li>
<li><a href="#many-qubits">Many qubits</a></li>
<li><a href="#bit-string-notation-and-a-general-quantum-state">Bit-String notation and a general quantum state</a></li>
<li><a href="#evolving-the-quantum-state">Evolving the quantum state</a></li>
</ol>
</li>
<li><a href="#measurement">Measurement</a></li>
<li><a href="#gates">Gates</a>
<ol>
<li><a href="#gates-as-matrices">Gates as matrices</a></li>
<li><a href="#gates-on-multi-qubit-machines">Gates on multi-qubit machines</a></li>
<li><a href="#single-qubit-gates-and-gates-on-adjacent-qubits">Single-qubit gates and gates on adjacent qubits</a></li>
<li><a href="#multi-qubit-gates-on-non-adjacent-qubits">Multi-qubit gates on non-adjacent qubits</a></li>
</ol>
</li>
<li><a href="#an-interpreter">An interpreter</a>
<ol>
<li><a href="#the-driver-loop">The driver loop</a></li>
<li><a href="#efficiency">Efficiency</a></li>
</ol>
</li>
<li><a href="#examples">Examples</a>
<ol>
<li><a href="#bell-state">Bell state</a></li>
<li><a href="#greenberger--horne--zeilinger-state">Greenberger‚ÄìHorne‚ÄìZeilinger state</a></li>
<li><a href="#the-quantum-fourier-transform">The quantum Fourier transform</a></li>
<li><a href="#example-transcript">Example transcript</a></li>
</ol>
</li>
<li><a href="#source-code">Source code</a></li>
</ol>
</nav>
<hr>
</div>
<h2 id="introduction">Introduction</h2>
<p>Simulating the workings of an ideal quantum computer has many
important applications, such as algorithms research and quantum
program debugging. A variety of quantum computer simulators exist,
both free and commercial. However, while the concept of the simulation
of quantum computers is generally well understood at a high level, the
devil is in the details when it comes to implementation.</p>
<p>Quantum computer simulators found in the wild often have many
limitations. The most prevalent limitation is the number of qubits an
operator can act on. Usually, one-qubit gates and controlled
one-qubit<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> gates are allowed, but nothing more. While these
together are sufficient for universal quantum computation, it leaves
much to be desired when studying quantum algorithms.</p>
<p>In this post, we present an implementation of a fully general quantum
programming language interpreter, allowing measurement as well as
arbitrary unitary operators on an arbitrary number of arbitrarily
indexed qubits. The implementation weighs in at under 150 lines<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>
of code in Common Lisp, though the ideas make implementation simple in
other languages as well. All of the code from this tutorial can be
found on
<a href="https://github.com/stylewarning/quantum-interpreter">GitHub</a>.</p>
<p>This tutorial is aimed at a quantum computing beginner who has some
familiarity with the fundamentals of linear algebra and computer
programming. Beyond those subjects, this tutorial is relatively
self-contained. We also aim this tutorial at practitioners of quantum
computing, who are interested in the brass tacks of simulation, with
all of the details filled out. To such practitioners, the bulk of this
document will be easy to skim, since we recapitulate topics such as
qubits and unitary operators.</p>
<h3 id="a-note-about-common-lisp">A note about Common Lisp</h3>
<p>We use Common Lisp, because it is an excellent platform for both
exploratory and high-performance computing. One of the fastest and
most flexible quantum simulators out there, the <a href="https://github.com/quil-lang/qvm">Quantum Virtual
Machine</a>, is written entirely in
Common Lisp.</p>
<p>We wrote this article so that it would be easy to follow along with a
Common Lisp implementation. The code has no dependencies, and should
work in any ANSI-compliant implementation (I hope).</p>
<p>With that said, this article was also written with portability in
mind. Since no especially Lisp-like features are used, the code should
be easy to port to Python or even C. At minimum, your language should
support complex numbers and arrays.</p>
<h3 id="a-note-to-experienced-quantum-computing-practitioners">A note to experienced quantum computing practitioners</h3>
<p><em>This section is written for experienced practitioners of quantum
computing who happened upon this post, and can be skipped.</em></p>
<p>In this post, we opt to simulate a quantum circuit the ‚ÄúSchrodinger‚Äù
way, that is, by evolving a wavefunction explicitly. For a circuit of
width $n$, we walk through the mathematics of how to interpret a
$k$-qubit gate $g \in \mathsf{SU}(2^k)$ for $k\le n$, specified to act
on a $k$-tuple of numbered qubits‚Äîcorresponding to each qubit‚Äôs
position in the tensor product which forms the Hilbert space of the
system‚Äîas a full operator $g'\in\mathsf{SU}(2^n)$. We do this by
providing an explicit construction of the matrix in the computational
basis of the system.</p>
<p>An alternative approach would have been to describe the action of a
$g$ on an $n$-qubit wavefunction by way of careful manipulation of
indexes, i.e., to effectively permute and partition our wavefunction
into $2^{n-k}$ groups of $2^k$-dimensional vectors corresponding to
the subsystem of qubits being operated on. The major benefit of this
approach is efficiency.</p>
<p>As a first introduction to a computer science graduate, I find this
explanation lacking in two ways:</p>
<ol>
<li>It under-emphasizes that a gate like $\mathsf{CNOT}$, typically
written as a $4\times 4$ matrix $\mathsf{I}\oplus\mathsf{X}$, in a
quantum circuit truly is a linear operator on the Hilbert space of
the entire system. ‚ÄúIt‚Äôs just linear algebra; here‚Äôs the matrix and
here‚Äôs the vector‚Äù is a point I want to drive home.</li>
<li>It requires significant labor to both explain and prove the
correctness of the method, without having significant experience with
tensor algebra, contractions, Einstein notation, and so on.</li>
</ol>
<p>The approach of this post can be used as a basis to follow up with
more efficient techniques, without relinquishing a strong mathematical
foundation. We are very careful to not be hand-wavy, and to not
conflate the different vector spaces at play. We hope that you‚Äôll find
this approach agreeable, even if it sacrifices some efficiency.</p>
<h2 id="the-language-mathscrl">The Language $\mathscr{L}$</h2>
<p>We wish to construct an interpreter for a small quantum programming
language named $\mathscr{L}$. This language supports
both of the fundamental operations of a quantum computer: gates and
measurements.</p>
<p>A <strong>gate</strong> is an operation that modifies a quantum state. (What a
quantum state is exactly we will delve into later.) Because quantum
states are large compared to the physical resources used to construct
them, gates represent the ‚Äúpowerful‚Äù operations of a quantum
computer.</p>
<p>A <strong>measurement</strong> is an observation and collapse of the quantum state,
producing one bit (i.e., $0$ or $1$) of classical information per
qubit. Measurements represent the <em>only</em> way in which one can extract
information from our simulated quantum computer, and indeed, in most
programming models for real quantum computers.</p>
<p>In some sense, one might think of the language $\mathscr{L}$ as the
simplest non-trivial quantum programming language. A program in
$\mathscr{L}$ is just a sequence of gates and measurements. The syntax
is as follows:</p>
<table>
<thead>
<tr>
<th>Non-Terminal</th>
<th></th>
<th>Defintion</th>
</tr>
</thead>
<tbody>
<tr>
<td>program</td>
<td>:=</td>
<td><code>(</code> <em>instruction</em>* <code>)</code></td>
</tr>
<tr>
<td>instruction</td>
<td>:=</td>
<td><code>(</code> <code>GATE</code> <em>matrix</em> <em>qubit</em>+ <code>)</code></td>
</tr>
<tr>
<td></td>
<td>|</td>
<td><code>(</code> <code>MEASURE</code> <code>)</code></td>
</tr>
<tr>
<td>matrix</td>
<td>:=</td>
<td><em>a complex matrix</em> <code>#2A(</code> ‚Ä¶ <code>)</code></td>
</tr>
<tr>
<td>qubit</td>
<td>:=</td>
<td><em>a non-negative integer</em></td>
</tr>
</tbody>
</table>
<p>Spaces and newlines are ignored, except to delimit the tokens of our
language.</p>
<p>We borrow Common Lisp‚Äôs two-dimensional array syntax for the syntax of
matrices. In Common Lisp, the matrix $\left(\begin{smallmatrix}1 &amp;
2\\3 &amp; 4\end{smallmatrix}\right)$ is written <code>#2A((1 2) (3 4))</code>. We
also borrow the syntax for complex numbers: $1-2i$ is written <code>#C(1 -2)</code>.</p>
<p>An example program might be one to construct and subsequently measure
two qubits labeled <code>2</code> and <code>5</code> in a Bell state configuration:</p>
<pre tabindex="0"><code>(
 (GATE #2A((0.70710677 0.70710677) (0.70710677 -0.70710677)) 2)
 (GATE #2A((1 0 0 0) (0 1 0 0) (0 0 0 1) (0 0 1 0))          2 5)
 (MEASURE)
)
</code></pre><p>We will model the semantics of $\mathscr{L}$ operationally, by way of an <strong>abstract machine</strong>. The abstract machine for $\mathscr{L}$ is called $M_n$, where $n$ is a positive but fixed<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> number of qubits. The state of the machine $M_n$ is the pair $(v, b)$ where $v$ is a quantum state, and $b$ is an $n$-bit measurement register.</p>
<p>The quantum state is an element of the set</p>
<p>$$\{\Vert v\Vert=1\mid v\in\mathbb{C}^{2^n}\}.$$</p>
<p>In other words, $v$ is a unit vector of dimension $2^n$ over the
complex numbers. We will discuss this from first principles in the
<a href="#the-quantum-state">next section</a>.</p>
<p>The measurement register is an element of the set $\{0,1\}^n$, i.e.,
a sequence of $n$ bits, which we realize as a non-negative
integer. The $k$th least-significant bit of this integer represents
the last observation of the qubit numbered as $k$. We will <a href="#measurement">discuss
this in detail</a> as well.</p>
<p>In Common Lisp, it suffices to create a structure <code>machine</code> which holds these two pieces of state.</p>
<pre tabindex="0"><code>(defstruct machine
  quantum-state
  measurement-register)
</code></pre><p>Typically, the machine is initialized with each classical bit in the
measurement register $0$, and each qubit starting in the
zero-state. (However, for the purposes of algorithm study or
debugging, the machine may be initialized with any valid state.)</p>
<p>The precise way in which the language $\mathscr{L}$ is interpreted on
$M_n$ is what we describe in this tutorial. Before that, however, we
find it most important to describe what <em>exactly</em> a quantum state is,
and how to represent it on a computer.</p>
<h2 id="the-quantum-state">The Quantum State</h2>
<h3 id="where-does-one-qubit-live">Where does one qubit live?</h3>
<p>Quantum computers are usually just a collection of interacting computational elements called <strong>qubits</strong>. A single qubit has two distinguished states: $\ket{0}$ and $\ket{1}$. If the qubit has a name like $q$, then we label the states $\ket{0}_q$ and $\ket{1}_q$.</p>
<p>The funny notation is called <strong>Dirac notation</strong> or <strong>braket notation</strong>. It happens to be a convenient notation for doing calculations in quantum mechanics, and we just use it for consistency with other texts. The <strong>ket</strong> $\ket{\cdots}$, as a physicist would call it, doesn‚Äôt add any special significance, except to denote that the quantity is a vector. One can actually put <em>anything</em> inside the brackets. In usual linear algebra, one often writes $\mathbf{e}_i$ to denote a basis vector, where in quantum mechanics, one just writes the subscript in a ket $\ket{i}$, dropping the $\mathbf{e}$ entirely. If the notation throws you off, and you‚Äôd like to think in more traditional written linear algebra notation, you can always replace $\ket{x}$ with $\vec x$, and you‚Äôll be safe.</p>
<p>These distinguished states $\ket{0}$ and $\ket{1}$ are understood to be orthonormal basis vectors in a vector space whose scalars are complex numbers $\mathbb{C}$. As such, a qubit can be $\ket{0}$, $\ket{1}$, or a <strong>superposition</strong> $\alpha\ket 0 + \beta\ket 1$, where $\alpha$ and $\beta$ are complex numbers. The numbers $\alpha$ and $\beta$ are called <strong>probability amplitudes</strong>, because $\vert\alpha\vert^2$ (resp. $\vert\beta\vert^2$) represent the probability of the qubit being observed in the $\ket 0$ (resp. $\ket 1$) state. Since they represent probabilities, there‚Äôs an additional constraint, namely that the probabilities add to one: $\vert\alpha\vert^2 + \vert\beta\vert^2=1$.</p>
<p>To those unfamiliar, it may not be obvious why we‚Äôve opted to use the
language of linear algebra. Why do we consider a qubit as being a
linear combination? Why do we suppose that the observable states are
orthonormal vectors? Why can‚Äôt we simply say that a qubit is just a
pair of complex numbers and move on?</p>
<p>The reason for this is scientific, and not mathematical. It turns out that the best theory of quantum mechanics we have is one which describes transformations between states as being linear. In fact, the evolution of a quantum mechanical system is not only described by an operation that is just linear, but also reversible. These conditions‚Äîlinear, reversible, and length-preserving‚Äîgive rise to a special class of transformations called <strong>unitary operators</strong>, which naturally lead us to the discussion of vector spaces over complex numbers<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>.</p>
<p>We will discuss the nature of these operations in more depth when we consider how to implement gates <a href="#gates">later on</a>. For now, however, it‚Äôs sufficient to think of a qubit named $q$ as something that lives in a complex, two-dimensional vector space, which we will call $$B_q := \operatorname{span}_{\mathbb{C}}\{\ket 0_q, \ket 1_q\}.$$ (We will use this $B_q$ notation a few times throughout this tutorial. Remember it!) We also understand that this space is equipped<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> with a way to calculate lengths of vectors‚Äîthe usual norm</p>
<p>$$
\left\Vert\alpha\ket{0}+\beta\ket{1}\right\Vert = \sqrt{\vert\alpha\vert^2+\vert\beta\vert^2}.
$$</p>
<h3 id="many-qubits">Many qubits</h3>
<p>Roughly speaking, a single qubit can be described by two
probabilities. How do we deal with more?</p>
<p>Suppose we have two qubits named $X$ and $Y$. As a pair, quantum
mechanics tells us that they can <em>interact</em>. Practically, what
that means is that their states can be correlated in some way. If
they‚Äôve interacted, knowing information about $X$ might give us a clue
about what $Y$ might be. One well-known example of this is the
<em>Bell state</em>, which can be summarized as follows:</p>
<table>
<thead>
<tr>
<th>Qubit $X$</th>
<th>Qubit $Y$</th>
<th>Prob. Amp.</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\ket 0_X$</td>
<td>$\ket 0_Y$</td>
<td>$1/\sqrt{2}$</td>
<td>$50\%$</td>
</tr>
<tr>
<td>$\ket 0_X$</td>
<td>$\ket 1_Y$</td>
<td>$0$</td>
<td>$0\%$</td>
</tr>
<tr>
<td>$\ket 1_X$</td>
<td>$\ket 0_Y$</td>
<td>$0$</td>
<td>$0\%$</td>
</tr>
<tr>
<td>$\ket 1_X$</td>
<td>$\ket 1_Y$</td>
<td>$1/\sqrt{2}$</td>
<td>$50\%$</td>
</tr>
</tbody>
</table>
<p>Here, we have an example of a <strong>non-factorizable state</strong>; qubits $X$ and $Y$ are correlated to each other dependently. If we know $X$ is in the $\ket 0_X$ state, then we <em>necessarily</em> know that $Y$ is in the $\ket 0_Y$ state. Such a correlation means it‚Äôs not possible to express the probabilities independently. It might be tempting to think that one can simply think of $X$ having a $50\%$ probability of being in either basis state, and $Y$ having a $50\%$ probability of being in either state‚Äîfacts which are certainly true‚Äîbut considering those independently would give us a <em>different</em> distribution of probabilities of the system:</p>
<table>
<thead>
<tr>
<th>Qubit $X$</th>
<th>Qubit $Y$</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\ket 0_X$</td>
<td>$\ket 0_Y$</td>
<td>$P(X=\ket 0_X)P(Y=\ket 0_Y)=25\%$</td>
</tr>
<tr>
<td>$\ket 0_X$</td>
<td>$\ket 1_Y$</td>
<td>$P(X=\ket 0_X)P(Y=\ket 1_Y)=25\%$</td>
</tr>
<tr>
<td>$\ket 1_X$</td>
<td>$\ket 0_Y$</td>
<td>$P(X=\ket 1_X)P(Y=\ket 0_Y)=25\%$</td>
</tr>
<tr>
<td>$\ket 1_X$</td>
<td>$\ket 1_Y$</td>
<td>$P(X=\ket 1_X)P(Y=\ket 1_Y)=25\%$</td>
</tr>
</tbody>
</table>
<p>This state is called <strong>factorizable</strong> because we can express each
probability as a product of probabilities pertaining to the original
qubits, i.e., each probability has a form that looks like
$P(X)P(Y)$. Note that here, knowing something about $X$ gives us <em>no</em>
information about $Y$, since they‚Äôre completely independent. With that
said, it should be emphasized that factorizable states <em>are</em> perfectly
valid states, but they don‚Äôt represent the entirety of possible
states.</p>
<p>If qubits $X$ and $Y$ live in the linear spaces $B_X$ and $B_Y$ respectively, then the composite space is written $B_X\otimes B_Y$. This is called a <strong>tensor product</strong>, which is a way to combine <em>spaces</em> with the above structure. Formally, if we have an $m$-dimensional vector spaces $V:=\operatorname{span}\{v_1,\ldots,v_m\}$ and an $n$-dimensional vector space $W:=\operatorname{span}\{w_1,\ldots,w_n\}$, then their tensor product $T:=V\otimes W$ will be an $mn$-dimensional vector space $\operatorname{span}\{t_1,\ldots,t_{mn}\}$, where each $t_i$ is a formal combination of basis vectors from $V$ and $W$. (There are of course $mn$ different combinations of $v$‚Äôs and $w$‚Äôs.) To give an example without all the abstraction, consider $V$ with a basis $\{\vec x, \vec y, \vec z\}$ and $W$ with a basis $\{\vec p, \vec q\}$. Then $V\otimes W$ will have a basis</p>
<p>$$
\left\{
\begin{array}{lll}
\vec x\otimes\vec p, &amp; \vec y\otimes\vec p, &amp; \vec z\otimes\vec p, \\
\vec x\otimes\vec q, &amp; \vec y\otimes\vec q, &amp; \vec z\otimes\vec q\hphantom{,}
\end{array}
\right\}.
$$</p>
<p>An example vector in the space $V\otimes W$ might be</p>
<p>$$
-i(\vec x\otimes\vec p) - 2(\vec y\otimes\vec p) + 3 (\vec z\otimes\vec p) +
\frac{1}{4}(\vec x\otimes\vec q) - \sqrt{5}(\vec y\otimes\vec q) + e^{6\pi}(\vec z\otimes\vec q),
$$</p>
<p>assuming these vector spaces are over $\mathbb{C}$.</p>
<p>Intuitively, a tensor product ‚Äújust‚Äù gives us a way to associate a number with each possible combination of basis vector. In our case, we need to associate a probability amplitude with each combination of distinguished qubit basis states. We need this ability since‚Äîas we‚Äôve established‚Äîwe need to consider every possible holistic outcome of a collection of qubits, as opposed to the outcomes of the qubits independently. (The former constitute both factorizable and non-factorizable states, while the latter only include factorizable states.)</p>
<h3 id="bit-string-notation-and-a-general-quantum-state">Bit-String notation and a general quantum state</h3>
<p>If we have qubits $X$, $Y$, and $Z$, then they‚Äôll live in the space $B_X\otimes B_Y\otimes B_Z$, which we‚Äôll call $Q_3$. It will be massively inconvenient to write the basis vectors as, for example, $\ket 0_X\otimes \ket 1_Y\otimes\ket 1_Z$, so we instead use the shorthand $\ket{011}$ when the space has been defined. This is called <strong>bit-string notation</strong>. A general element $\ket\psi$ of $Q_3$ can be written $$\psi_0\ket{000}+\psi_1\ket{001}+\psi_2\ket{010}+\psi_3\ket{011}+\psi_4\ket{100}+\psi_5\ket{101}+\psi_6\ket{110}+\psi_7\ket{111}.$$ There are two substantial benefits from using bit-string notation. These benefits are much more thoroughly explained in <a href="https://arxiv.org/abs/1711.02086">this paper</a>‚Äîwhich was a precursor to this very blog post.</p>
<p>The first benefit is that the names of the qubits‚Äî$X$, $Y$, and $Z$‚Äîhave been abstracted away. They‚Äôre now just positions in a bit-string, and we can canonically name the qubits according to their position. We record positions <em>from the right starting from zero</em>, so $X$ is in position $2$, $Y$ is in position $1$, and $Z$ is in position $0$.</p>
<p>The second benefit is one relevant to how we implement quantum states on a computer. As written, the probability amplitude $\psi_i$ has an index $i$ whose binary expansion matches the bit-string of the basis vector whose scalar component is $\psi_i$. This is no accident. The main outcome of this is that we can use a non-negative integer as a way of specifying a bit-string, which also acts as an index into an array of probability amplitudes. So for instance, the above state can be written further compactly as $$\ket\psi=\sum_{i=0}^7\psi_i\ket i.$$ Here, $\ket i$ refers to the $i$th bit-string in lexicographic (‚Äúdictionary‚Äù) order, or equivalently, the binary expansion of $i$ as a bit-string.</p>
<p>Since qubits live in a two-dimensional space, then $n$ qubits will live in a $2^n$-dimensional space. With a great deal of work, we‚Äôve come to our most general<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> representation of an $n$-qubit system: $$\sum_{i=0}^{2^n-1}\psi_i\ket i,$$ where $\vert\psi_i\vert^2$ gives us the probability of observing the bit-string $\ket i$, implying $$\sum_{i=0}^{2^n-1}\vert\psi_i\vert^2=1.$$</p>
<p>On a computer, representing a quantum state for an $n$-qubit system is simple: It‚Äôs just an array of $2^n$ complex numbers. An index $i$ into the array represents the probability amplitude $\psi_i$, which is the scalar component of $\ket{i}$. So, for instance, the state $\ket{000}$ in a 3-qubit system is represented by an array whose first element is $1$ and the rest $0$. Here is a function to allocate a new quantum state of $n$ qubits, initialized to be in the $\ket{\ldots 000}$ state:</p>
<pre tabindex="0"><code>(defun make-quantum-state (n)
  (let ((s (make-array (expt 2 n) :initial-element 0.0d0)))
    (setf (aref s 0) 1.0d0)
    s))
</code></pre><p>Sometimes, given a quantum state, or even an operator on a quantum
state, we will want to recover how many qubits the state represents,
or the operator acts on. In both cases, the question reduces to
determining the number of qubits that a dimension represents. Since
our dimensions are always powers of two, we need to compute the
equivalent of a binary logarithm. In Common Lisp, we can compute this
by computing the number of bits an integer takes to represent using
<code>integer-length</code>. The number $2^n$ is always a <code>1</code> followed by $n$
<code>0</code>‚Äôs, so the length of $2^n$ in binary is $n+1$.</p>
<pre tabindex="0"><code>(defun dimension-qubits (d)
  (1- (integer-length d)))
</code></pre><h3 id="evolving-the-quantum-state">Evolving the quantum state</h3>
<p>Since the quantum state is a vector, the principal way we change it is
through linear operators represented as matrices. As our quantum
program executes, we say that the quantum state
<em>evolves</em>. Matrix‚Äìvector multiplication is accomplished with
<code>apply-operator</code> and matrix‚Äìmatrix multiplication is accomplished
with <code>compose-operators</code>. There is nothing special about these
functions; they are the standard textbook algorithms.</p>
<pre tabindex="0"><code>(defun apply-operator (matrix column)
  (let* ((matrix-size (array-dimension matrix 0))
         (result (make-array matrix-size :initial-element 0.0d0)))
    (dotimes (i matrix-size)
      (let ((element 0))
        (dotimes (j matrix-size)
          (incf element (* (aref matrix i j) (aref column j))))
        (setf (aref result i) element)))
    (replace column result)))

(defun compose-operators (A B)
  (destructuring-bind (m n) (array-dimensions A)
    (let* ((l (array-dimension B 1))
           (result (make-array (list m l) :initial-element 0)))
      (dotimes (i m result)
        (dotimes (k l)
          (dotimes (j n)
            (incf (aref result i k)
                  (* (aref A i j)
                     (aref B j k)))))))))
</code></pre><p>These functions will sit at the heart of the interpreter, which will
be elaborated upon in <a href="#gates">the section about gates</a>.</p>
<h2 id="measurement">Measurement</h2>
<p>Already, through the construction of our quantum state, we‚Äôve
discussed the idea that the probability amplitudes imply a probability
of observing a state. Measurement then amounts to looking at a quantum
state as a discrete probability distribution and sampling from it.</p>
<p>Measurement in quantum mechanics is side-effectful; observation of a
quantum state also simultaneously <em>collapses</em> that state. This means
that when we measure a state to be a bit-string, then the state will
also <em>become</em> that bit-string, zeroing out every other component in the
process.</p>
<p>We thus implement the process of measurement in two steps: The
sampling of the state followed by its collapse.</p>
<pre tabindex="0"><code>(defun observe (machine)
  (let ((b (sample (machine-quantum-state machine))))
    (collapse (machine-quantum-state machine) b)
    (setf (machine-measurement-register machine) b)
    machine))
</code></pre><p>Note that we‚Äôve recorded our observation into the measurement register. We now proceed to define what we mean by <code>sample</code> and <code>collapse</code>.</p>
<p>How shall we sample? This is a classic problem in computer science. If we have $N$ events $\{0, 1,\ldots,N-1\}$, such that event $e$ has probability $P(e)$, then we can sample as follows. Consider the partial sums defined by the recurrence $S(0)=0$ and $S(k)=S(k-1) + P(k-1)$. If we draw a random number $r$ uniformly from $[0,1)$, then we wish to find the $k$ such that $S(k)\leq r &lt; S(k+1)$. Such a $k$ will be a sampling of our events according to the imposed probability distribution.</p>
<p>We can implement this simply by computing successive partial sums, until our condition is satisfied. In fact, we can be a little bit more resourceful. We can find when $r-S(k+1)&lt;0$, which amounts to successive updates $r\leftarrow r-P(k)$.</p>
<p>With a quantum system, we have $P(\ket i) = \vert\psi_i\vert^2$, and the sampled $k$ is the bit-string $\ket k$ we find.</p>
<p>Let‚Äôs do an example. Suppose we have a quantum state</p>
<p>$$
\sqrt{0.2}\ket{00} - \sqrt{0.07}\ket{01} + \sqrt{0.6}\ket{10} + \sqrt{0.13}\ket{11}.
$$</p>
<p>Then our discrete probability distribution is:</p>
<p>$$
P(\ket{00}) = 0.2\qquad P(\ket{01}) = 0.07\qquad P(\ket{10}) = 0.6\qquad P(\ket{11}) = 0.13
$$</p>
<p>Next, suppose we draw a random number $r = 0.2436$. We first check if $r &lt; 0.2$. It‚Äôs not, so $\ket{00}$ is not our sample. Subtract it from $r$ to get $r = 0.0436$. Next check if $r &lt; 0.07$. Yes, so our sample is $\ket{01}$. Pictorially, this looks like the following:</p>
<p><img src="https://www.stylewarning.com/posts/quantum-interpreter/images/sample.svg" alt="A process of selecting a random sample." decoding="async">
</p>
<p>The implementation is straightforward:</p>
<pre tabindex="0"><code>(defun sample (state)
  (let ((r (random 1.0d0)))
    (dotimes (i (length state))
      (decf r (expt (abs (aref state i)) 2))
      (when (minusp r) (return i)))))
</code></pre><p>Collapsing to $\ket k$ is simply zeroing out the array and setting $\psi_k$ to $1$.</p>
<pre tabindex="0"><code>(defun collapse (state basis-element)
  (fill state 0.0d0)
  (setf (aref state basis-element) 1.0d0))
</code></pre><h2 id="gates">Gates</h2>
<h3 id="gates-as-matrices">Gates as matrices</h3>
<p>Gates are the meat of most quantum algorithms. They represent the
‚Äúhard work‚Äù a quantum computer does. As previously described, a gate
$g$ is a transformation that is linear, invertible, and
length-preserving.</p>
<ul>
<li>
<p><strong>Linear</strong>: $g(a\ket\psi+b\ket\phi)=ag(\ket\psi)+bg(\ket\phi)$.</p>
</li>
<li>
<p><strong>Invertible</strong>: There is always an operation $h$ that can cancel out the effect of $g$: $h(g(\ket\psi))=g(h(\ket\psi))=\ket\psi$.</p>
</li>
<li>
<p><strong>Length-Preserving</strong>: $\Vert g(\ket\psi)\Vert = \Vert\ket\psi\Vert$.</p>
</li>
</ul>
<p>These ideas are captured by an overarching idea called a <strong>linear isometry</strong>, which comes from the Greek word <em>isometria</em>, with <em>isos</em> meaning ‚Äúequal‚Äù and <em>metria</em> meaning ‚Äúmeasuring‚Äù. As with all linear transformations, we can write them out as a matrix with respect to a particular basis. Matrices representing linear isometries are called <strong>unitary matrices</strong><sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>.</p>
<p>The simplest gate must be identity, a gate which does nothing.</p>
<p>$$
\mathsf{I} := \begin{pmatrix}
1 &amp; 0\\
0 &amp; 1
\end{pmatrix}
$$</p>
<p>In Common Lisp, this would be defined as</p>
<pre tabindex="0"><code>(defparameter +I+ #2A((1 0)
                      (0 1)))
</code></pre><p>which we will make use of later. Just a notch higher in complexity
would be the quantum analog of a Boolean ‚ÄúNOT‚Äù. This is called the
$\mathsf{X}$ gate:</p>
<p>$$
\mathsf{X} := \begin{pmatrix}
0 &amp; 1\\
1 &amp; 0
\end{pmatrix}.
$$</p>
<p>This has the effect of mapping $\mathsf{X}\ket 0=\ket 1$, which means directly that $\mathsf{X}\ket 1=\ket 0$ and therefore it is its own inverse: $\mathsf{X}^{-1}\mathsf{X} = \mathsf{I}$.</p>
<p>We suggest re-reviewing how one interprets a matrix as an explicit
mapping of each element of the basis, as it helps make sense of
gates. In this tutorial, gate matrices are always specified in terms
of the bit-string basis</p>
<p>$$
\{\ket{\ldots000}, \ket{\ldots001}, \ket{\ldots010}, \ket{\ldots011}, \ldots\}.
$$</p>
<p>We again refer the reader to <a href="https://arxiv.org/abs/1711.02086">this
paper</a> for an in-depth discussion
about this basis.</p>
<p>In the rest of this section, the whole goal is to be able to apply
gates to our quantum state. There are two cases of pedagogical and
operational interest: the one-qubit gate and the many-qubit gate. We
will write two functions to accomplish each of these, in order to
implement a general function called <code>apply-gate</code> for applying any kind
of gate on any collection of qubits for any quantum state.</p>
<pre tabindex="0"><code>(defun apply-gate (state U qubits)
  (assert (= (length qubits) (dimension-qubits (array-dimension U 0))))
  (if (= 1 (length qubits))
      (%apply-1Q-gate state U (first qubits))
      (%apply-nQ-gate state U qubits)))
</code></pre><h3 id="gates-on-multi-qubit-machines">Gates on multi-qubit machines</h3>
<p>If we are working with the machine $M_n$, then our space is $2^n$-dimensional, and as such, our matrices would be written out as $2^n\times 2^n$ arrays of numbers. If we can write out such a matrix, then applying it is as simple as a matrix‚Äìvector multiplication. For instance, for a $4$-qubit machine, an $\mathsf{X}$ on qubit $0$ would be written</p>
<p>$$
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0
\end{pmatrix},
$$</p>
<p>which could be readily applied to a $16$-element quantum state vector. It is easy to verify that this will swap the components of $\ket{\ldots 0}$ with the corresponding components of $\ket{\ldots 1}$.</p>
<p>But as should be plainly obvious from the obnoxious amount of paper wasted by writing out this matrix, it would be better if we could simply generate this matrix with just three pieces of information: the gate matrix $g=\left(\begin{smallmatrix}0 &amp; 1\\1 &amp; 0\end{smallmatrix}\right)$, the qubit index $i=0$, and the size of the machine $n=4$. This is a process we will call <strong>lifting</strong>.</p>
<p>Lifting requires a fundamental tool for constructing operators on spaces that were formed out of tensor products. If we have two finite-dimensional vector spaces $U$ and $V$, and operators $f$ and $g$ on the spaces respectively, then it seems reasonable to consider how $f$ and $g$ transform $U\otimes V$. In some sense, applying $f$ and $g$ ‚Äúin parallel‚Äù on $U\otimes V$ correspond to a new linear operator $h$. If $f$ and $g$ are matrices, then $h$ is defined by a <em>block matrix</em></p>
<p>$$
\begin{equation}
h_{i,j} = f_{i,j} g.
\label{eq:kron}
\end{equation}
$$</p>
<p>More specifically, let $0 \leq i,j &lt; \dim U$. The matrix $h$ will be
an array of $\dim U \times \dim U$ copies of $g$, where the entries of
the $(i,j)$th blocks are multiplied by the single
scalar $f_{i,j}$. This will lead to a matrix with $(\dim U)(\dim V)$
rows and columns, which is exactly the dimension of $U\otimes
V$. Incidentally, we write $h$ as $f\otimes g$, and this combination
of operators is called the <strong>Kronecker product</strong><sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>. As code:</p>
<pre tabindex="0"><code>(defun kronecker-multiply (A B)
  (destructuring-bind (m n) (array-dimensions A)
    (destructuring-bind (p q) (array-dimensions B)
      (let ((result (make-array (list (* m p) (* n q)))))
        (dotimes (i m result)
          (dotimes (j n)
            (let ((Aij (aref A i j))
                  (y (* i p))
                  (x (* j q)))
              (dotimes (u p)
                (dotimes (v q)
                  (setf (aref result (+ y u) (+ x v))
                        (* Aij (aref B u v))))))))))))
</code></pre><p><em>As a matter of terminology, remember that tensor products combine
vector spaces, and Kronecker products combine operator matrices.</em></p>
<h3 id="single-qubit-gates-and-gates-on-adjacent-qubits">Single-qubit gates and gates on adjacent qubits</h3>
<p>From here, we can very easily lift one-qubit gates to machines with
any number of qubits. A gate $g$ on qubit $i$ in an $n$-qubit machine
is just $g$ applied to qubit $i$ and the identity $\mathsf{I}$ on all
other qubits. Writing this out as a Kronecker product, we have</p>
<p>$$
\begin{equation}
\operatorname{lift}(g, i, n) :=
\underbrace{\mathsf{I} \otimes \mathsf{I} \otimes \cdots}_{n-i-1\text{ factors}}
\otimes g \otimes
\underbrace{\cdots \otimes \mathsf{I}}_{i\text{ factors}},
\label{eq:liftone}
\end{equation}
$$</p>
<p>where there are a total of $n$ factors, and $g$ is at positioned $i$ factors from the right.</p>
<p>This concept generalizes to higher-dimensional operators which act on <em>index-adjacent qubits</em>. In other words, if $g$ is a $k$-qubit operator <em>specifically</em> acting on qubits</p>
<p>$$
(i+k-1, i+k-2, \ldots, i+2, i+1, i),
$$</p>
<p>then the lifting operator from \eqref{eq:liftone} is much the same:</p>
<p>$$
\begin{equation}
\operatorname{lift}(g, i, n) := \underbrace{\mathsf{I} \otimes \mathsf{I} \otimes \cdots}_{n-i-k\text{ factors}}
\otimes g \otimes
\underbrace{\cdots \otimes \mathsf{I}}_{i\text{ factors}}.
\label{eq:liftmany}
\end{equation}
$$</p>
<p>It must be emphasized one last time: <em>This only works for multi-qubit operators that act on qubits that are index-adjacent.</em> We will get to how to work with non-adjacent qubits shortly, but first we will turn this into code.</p>
<p>For simplicity, we create a way to iterate a Kronecker product
multiple times, that is, compute</p>
<p>$$
\underbrace{g\otimes \cdots \otimes g}_{n\text{ factors}},
$$</p>
<p>which is usually simply written $g^{\otimes n}$. We must use care when
handling the case when we are ‚ÄúKronecker exponentiating‚Äù by a
non-positive number, so that $f\otimes g^{\otimes 0} = f$.</p>
<pre tabindex="0"><code>(defun kronecker-expt (U n)
  (cond
    ((&lt; n 1) #2A((1)))
    ((= n 1) U)
    (t (kronecker-multiply (kronecker-expt U (1- n)) U))))
</code></pre><p>With <code>kroncker-expt</code>, we can write <code>lift</code> following \eqref{eq:liftmany}:</p>
<pre tabindex="0"><code>(defun lift (U i n)
  (let ((left  (kronecker-expt +I+ (- n i (dimension-qubits
                                           (array-dimension U 0)))))
        (right (kronecker-expt +I+ i)))
    (kronecker-multiply left (kronecker-multiply U right))))
</code></pre><h3 id="multi-qubit-gates-on-non-adjacent-qubits">Multi-qubit gates on non-adjacent qubits</h3>
<p>In this section, we assume we are working on a multi-qubit machine
$M_n$ with $n\ge 2$.</p>
<h4 id="the-general-idea">The general idea</h4>
<p>So far, we‚Äôve managed to get away with lifting operators that act on
either a single qubit, or a collection of index-adjacent qubits. This
has been more-or-less trivial, because we can tack on a series of
identity operators by way of Kronecker products to simulate ‚Äúdoing
nothing‚Äù to the other qubits. However, if we want to apply a
multi-qubit gate to a collection of qubits that aren‚Äôt index-adjacent,
we have to be a little more clever.</p>
<p>The way we accomplish this is by swapping qubits around so that we can
move in and out of index-adjacency. In fact, for a given gate acting
on a given collection of qubits, we aim to compute an operator $\Pi$
which moves these qubits into index-adjacency, so that we can compute</p>
<p>$$
\begin{equation}
\Pi^{-1} \operatorname{lift}(g, 0, n) \Pi.
\label{eq:upq}
\end{equation}
$$</p>
<p>This recipe requires many ingredients, each of which we describe in
detail.</p>
<h4 id="swapping-two-qubits">Swapping two qubits</h4>
<p>To start, we need some way to swap the state of two qubits. We can do
this with the $\mathsf{SWAP}$ operator:</p>
<p>$$
\mathsf{SWAP} := \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
$$</p>
<p>In Common Lisp, we define this in the same way we defined <code>+I+</code>.</p>
<pre tabindex="0"><code>(defparameter +SWAP+ #2A((1 0 0 0)
                         (0 0 1 0)
                         (0 1 0 0)
                         (0 0 0 1)))
</code></pre><p>The $\mathsf{SWAP}$ operator takes two qubits and swaps their
state. What does this mean in a system of correlations, where qubit
state isn‚Äôt strictly compartmentalized (i.e., factorized)? Swapping is
equivalent to swapping the component of $\ket{01}$ with the component
of $\ket{10}$, which are the only two distinguishable
correlations<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>. Still, in a multi-qubit system, we can‚Äôt
immediately arbitrarily swap two qubits with the tools we‚Äôve
developed. What we can do is swap index-adjacent qubits. In
particular, we can define the transpositions</p>
<p>$$
\tau_i := \operatorname{lift}(\mathsf{SWAP}, i, n),\qquad \text{with }0\leq i &lt; n - 1.
$$</p>
<p>The transposition $\tau_i$ swaps qubit $i$ with qubit $i+1$. This is
our first ingredient.</p>
<h4 id="re-arranging-qubits-to-be-index-adjacent">Re-arranging qubits to be index-adjacent</h4>
<p>The second ingredient is a way to re-arrange our qubits so that they
are index-adjacent. Suppose we have a three-qubit operator $g$ which
acts on qubits $(2, 4, 3)$ in a machine of $n=5$ qubits. The space in
which the quantum state of $M_5$ lives is</p>
<p>$$
B_4 \otimes B_3 \otimes B_2 \otimes B_1 \otimes B_0,
$$</p>
<p>but we need to re-arrange our state vector as if we‚Äôve moved $B_2\to
B_0$, $B_4\to B_1$, and $B_3\to B_2$ so that our sub-state sits
index-adjacent. In combinatorics, this permutation is written in
two-line notation</p>
<p>$$
\begin{pmatrix}
0 &amp; 1 &amp; 2 &amp; 3 &amp; 4\\
3 &amp; 4 &amp; 0 &amp; 2 &amp; 1
\end{pmatrix}.
$$</p>
<p>Here, we‚Äôve made a few arbitrary decisions. First, we‚Äôve decided to
re-map a $k$-qubit operator to the $B_{k-1}\otimes\cdots\otimes
B_1\otimes B_0$ subspace. Any other index-adjacent subspace would
work, but this simplifies the code. Second, we see that $0\mapsto 3$
and $1\mapsto 4$, but it doesn‚Äôt matter so much where they map to, as
long as $2$, $4$, and $3$ are mapped correctly.</p>
<p>There‚Äôs no sense in writing the first line in two-line notation, so we
just write the permutation compactly as $34021$. As a quantum
operator, we write this as $\Pi_{34021}$.</p>
<p>The question is: How can we write $\Pi_{34021}$ as familiar operators?
It is a well-known fact in combinatorics that any permutation can be
decomposed into a composition of swaps, and every swap can be
decomposed into a series of adjacent transpositions. We leave this as
an exercise<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup>, but we will show the code to our implementation.</p>
<p>We start with a function which takes a permutation written as a list,
like <code>(3 4 0 2 1)</code>, and converts it to a list of (possibly
non-adjacent) transpositions to be applied left-to-right, represented
as cons cells <code>((0 . 3) (1 . 4) (2 . 3))</code>.</p>
<pre tabindex="0"><code>(defun permutation-to-transpositions (permutation)
  (let ((swaps nil))
    (dotimes (dest (length permutation) (nreverse swaps))
      (let ((src (elt permutation dest)))
        (loop :while (&lt; src dest) :do
          (setf src (elt permutation src)))
        (cond
          ((&lt; src dest) (push (cons src dest) swaps))
          ((&gt; src dest) (push (cons dest src) swaps)))))))
</code></pre><p>Next, we convert these transpositions as cons cells to adjacent
transposition indexes. This is straightforward. If we are swapping
$(a,b)$ with $a&lt;b$, then we transpose $(a, a+1)$, then $(a+1, a+2)$,
and so on until $(b-1, b)$, followed by a reversal of each except
$(b-1, b)$. We can simply write this chain of adjacent transpositions
as $(a, a+1, \ldots, b-1, \ldots, a+1, a)$. In this example, we‚Äôd have
the transposition indexes <code>(0 1 2 1 0 1 2 3 2 1 2)</code>.</p>
<pre tabindex="0"><code>(defun transpositions-to-adjacent-transpositions (transpositions)
  (flet ((expand-cons (c)
           (if (= 1 (- (cdr c) (car c)))
               (list (car c))
               (let ((trans (loop :for i :from (car c) :below (cdr c)
                                  :collect i)))
                 (append trans (reverse (butlast trans)))))))
    (mapcan #'expand-cons transpositions)))
</code></pre><p>These are indexes $i_1, i_2, \ldots$ such that $\Pi = \cdots
\tau_{i_2}\tau_{i_1}$</p>
<p>The last ingredient we need is inverting $\Pi$. If we have $\Pi$
represented as a sequence of $\tau$, then we simply reverse the list
of $\tau$.</p>
<h4 id="using-transpositions-to-implement-multi-qubit-gates">Using transpositions to implement multi-qubit gates</h4>
<p>With all of these, we write what is perhaps the most important
function of our interpreter.</p>
<pre tabindex="0"><code>(defun %apply-nQ-gate (state U qubits)
  (let ((n (dimension-qubits (length state))))
    (labels ((swap (i)
               (lift +swap+ i n))
             (transpositions-to-operator (trans)
               (reduce #'compose-operators trans :key #'swap)))
      (let* ((U01 (lift U 0 n))
             (from-space (append (reverse qubits)
                                 (loop :for i :below n
                                       :when (not (member i qubits))
                                         :collect i)))
             (trans (transpositions-to-adjacent-transpositions
                     (permutation-to-transpositions
                      from-space)))
             (to-&gt;from (transpositions-to-operator trans))
             (from-&gt;to (transpositions-to-operator (reverse trans)))
             (Upq (compose-operators to-&gt;from
                                     (compose-operators U01
                                                        from-&gt;to))))
        (apply-operator Upq state)))))
</code></pre><p>A few quick notes for comprehension:</p>
<ul>
<li>
<p>The value of <code>(swap i)</code> is $\tau_i$ fully lifted.</p>
</li>
<li>
<p>The one-line zinger that defines <code>transpositions-to-operator</code> takes
a list of transposition indexes and converts it into a unitary
operator. It does so by doing what‚Äôs known in functional programming
as a <em>map-reduce</em>, by first mapping $i\mapsto\tau_i$ and reducing by
operator composition.</p>
</li>
<li>
<p>The variable <code>from-space</code> contains the permutation $p$ that encodes
the space in which we‚Äôd like to act. This permutation is calculated
based off of the <code>qubits</code> argument.</p>
</li>
<li>
<p>The variables <code>from-&gt;to</code> and <code>to-&gt;from</code> represent $\Pi_p$ and
$\Pi^{-1}_p$ respectively.</p>
</li>
<li>
<p>The variable <code>Upq</code> is our fully lifted operator, exactly by way of
\eqref{eq:upq}.</p>
</li>
</ul>
<p>The function <code>%apply-nQ-gate</code> is what allows our interpreter to be so
general. Making the interpreter more efficient ultimately is an
exercise in making this function more efficient.</p>
<p>The only thing left to do is integrate all of the topics discussed
hitherto into an interpreter!</p>

<h3 id="the-driver-loop">The driver loop</h3>
<p>The bulk of the interpreter has been written. We‚Äôve described the
semantics of the two instructions of interest: <code>MEASURE</code> and
<code>GATE</code>. Now we create the interpreter itself, which is just a driver
loop to read and execute these instructions, causing state transitions
of our abstract machine. If we see a <code>GATE</code>, we call <code>apply-gate</code>. If
we see a <code>MEASURE</code>, we call <code>observe</code>.</p>
<pre tabindex="0"><code>(defun run-quantum-program (qprog machine)
  (loop :for (instruction . payload) :in qprog
        :do (ecase instruction
              ((GATE)
               (destructuring-bind (gate &amp;rest qubits) payload
                 (apply-gate (machine-quantum-state machine) gate qubits)))
              ((MEASURE)
               (observe machine)))
        :finally (return machine)))
</code></pre><h3 id="efficiency">Efficiency</h3>
<p>Performance-focused individuals will have noticed that this
interpreter is pretty costly, in many ways. The biggest cost is also
unavoidable: The fact that our state grows exponentially with the
number of qubits. Real, physical quantum computers avoid this cost,
which makes them alluring machines to both study and construct.</p>
<p>However, even with this unavoidable cost, this interpreter has been
implemented for ease of understanding and not machine
efficiency. Writing a faster interpreter amounts to avoiding the
construction of the lifted operator matrices. This can be done with
very careful index wrangling and sensitivity to data types and
allocation. This is how the high-performance <a href="https://github.com/quil-lang/qvm">Quantum Virtual
Machine</a> is implemented.</p>
<h2 id="examples">Examples</h2>
<p>What good is writing an interpreter if we don‚Äôt write any programs
worth interpreting? Here are a few examples of programs.</p>
<h3 id="bell-state">Bell state</h3>
<p>The <strong>Bell state</strong> is one which we‚Äôve explored earlier. It is a
two-qubit state $$\frac{1}{\sqrt{2}}(\ket {00} + \ket {11}).$$ Here‚Äôs
a program to generate one, using two new gates, the <strong>controlled-not
gate</strong> $\mathsf{CNOT}$ and the <strong>Hadamard gate</strong> $\mathsf{H}$.</p>
<pre tabindex="0"><code>(defparameter +H+ (make-array '(2 2) :initial-contents (let ((s (/ (sqrt 2))))
                                                         (list (list s s)
                                                               (list s (- s))))))

(defparameter +CNOT+ #2A((1 0 0 0)
                         (0 1 0 0)
                         (0 0 0 1)
                         (0 0 1 0))))

(defun bell (p q)
  `((GATE ,+H+ ,p)
    (GATE ,+CNOT+ ,p ,q)))
</code></pre><h3 id="greenberger--horne--zeilinger-state">Greenberger‚ÄìHorne‚ÄìZeilinger state</h3>
<p>The <strong>Greenberger‚ÄìHorne‚ÄìZeilinger state</strong>, or <strong>GHZ state</strong>, is a
generalization of the Bell state on more than two qubits, namely
$$\frac{1}{\sqrt{2}}(\ket{0\ldots 000} + \ket{1\ldots 111}).$$ This is
accomplished by executing a chain of controlled-not gates:</p>
<pre tabindex="0"><code>(defun ghz (n)
  (cons `(GATE ,+H+ 0)
        (loop :for q :below (1- n)
              :collect `(GATE ,+CNOT+ ,q ,(1+ q)))))
</code></pre><h3 id="the-quantum-fourier-transform">The quantum Fourier transform</h3>
<p>The ordinary discrete Fourier transform of a complex vector is a
unitary operator, and as such, it can be encoded as a quantum
program. We will write a program which computes the Fourier transform
of the probability amplitudes of an input quantum state (a time-domain
signal), producing a new quantum state whose amplitudes represent
components in the frequency domain. This is the central subroutine to
Shor‚Äôs algorithm, which is a quantum algorithm which factors integers
faster than any known classical method.</p>
<p>First, we will need a gate called the <strong>controlled-phase gate</strong> $\mathsf{CPHASE}(\theta)$:</p>
<pre tabindex="0"><code>(defun cphase (angle)
  (make-array '(4 4) :initial-contents `((1 0 0 0)
                                         (0 1 0 0)
                                         (0 0 1 0)
                                         (0 0 0 ,(cis angle)))))
</code></pre><p>Now, we can generate the quantum Fourier transform recursively.</p>
<pre tabindex="0"><code>(defun qft (qubits)
  (labels ((bit-reversal (qubits)
             (let ((n (length qubits)))
               (if (&lt; n 2)
                   nil
                   (loop :repeat (floor n 2)
                         :for qs :in qubits
                         :for qe :in (reverse qubits)
                         :collect `(GATE ,+swap+ ,qs ,qe)))))
           (%qft (qubits)
             (destructuring-bind (q . qs) qubits
               (if (null qs)
                   (list `(GATE ,+H+ ,q))
                   (let ((cR (loop :with n := (1+ (length qs))
                                   :for i :from 1
                                   :for qi :in qs
                                   :for angle := (/ pi (expt 2 (- n i)))
                                   :collect `(GATE ,(cphase angle) ,q ,qi))))
                     (append
                      (qft qs)
                      cR
                      (list `(GATE ,+H+ ,q))))))))
    (append (%qft qubits) (bit-reversal qubits))))
</code></pre><p>The program for a three-qubit quantum Fourier transform <code>(qft '(0 1 2))</code> looks like this:</p>
<pre tabindex="0"><code>(
  (GATE #2A((0.7071067811865475d0 0.7071067811865475d0) (0.7071067811865475d0 -0.7071067811865475d0)) 2) 
  (GATE #2A((1 0 0 0) (0 1 0 0) (0 0 1 0) (0 0 0 #C(0.0d0 1.0d0))) 1 2) 
  (GATE #2A((0.7071067811865475d0 0.7071067811865475d0) (0.7071067811865475d0 -0.7071067811865475d0)) 1) 
  (GATE #2A((1 0 0 0) (0 0 1 0) (0 1 0 0) (0 0 0 1)) 1 2) 
  (GATE #2A((1 0 0 0) (0 1 0 0) (0 0 1 0) (0 0 0 #C(0.7071067811865476d0 0.7071067811865475d0))) 0 1) 
  (GATE #2A((1 0 0 0) (0 1 0 0) (0 0 1 0) (0 0 0 #C(0.0d0 1.0d0))) 0 2) 
  (GATE #2A((0.7071067811865475d0 0.7071067811865475d0) (0.7071067811865475d0 -0.7071067811865475d0)) 0) 
  (GATE #2A((1 0 0 0) (0 0 1 0) (0 1 0 0) (0 0 0 1)) 0 2) 
)
</code></pre><p>(Recall that <code>#C(0 1)</code> represents the complex number $i$.)</p>
<p>We can see the quantum Fourier transform in action by computing the
Fourier transform of $\ket{000}$. Here is a transcript of this
calculation:</p>
<pre tabindex="0"><code>CL-USER&gt; (run-quantum-program
          (qft '(0 1 2))
          (make-machine :quantum-state (make-quantum-state 3)
                        :measurement-register 0))
#S(MACHINE
   :QUANTUM-STATE #(#C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0))
   :MEASUREMENT-REGISTER 0)
</code></pre><p>Indeed, one can verify that the classical Fourier transform of the
vector $[1,0,0,0,0,0,0,0]$ is a vector with eight components equal to
about $0.35355$.</p>
<pre tabindex="0"><code>$ python
Python 2.7.16 (default, May 23 2023, 14:13:27) 
[GCC 8.3.0] on linux2
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.fft.fft([1,0,0,0,0,0,0,0], norm="ortho")
array([0.35355339+0.j, 0.35355339+0.j, 0.35355339+0.j, 0.35355339+0.j,
       0.35355339+0.j, 0.35355339+0.j, 0.35355339+0.j, 0.35355339+0.j])
</code></pre><h3 id="example-transcript">Example transcript</h3>
<p>Here is an example transcript downloading and using this software,
using <a href="https://www.sbcl.org/">Steel Bank Common Lisp</a>.</p>
<pre tabindex="0"><code>$ git clone https://github.com/stylewarning/quantum-interpreter.git
Cloning into 'quantum-interpreter'...
remote: Counting objects: 10, done.
remote: Compressing objects: 100% (10/10), done.
Unpacking objects: 100% (10/10), done.
remote: Total 10 (delta 2), reused 5 (delta 0), pack-reused 0

$ cd quantum-interpreter/

$ sbcl --noinform
* (load "qsim.lisp")
T

* (load "examples.lisp")
T

* (run-quantum-program (bell 0 1)
                       (make-machine :quantum-state (make-quantum-state 2)
                                     :measurement-register 0))
#S(MACHINE
   :QUANTUM-STATE #(0.7071067690849304d0 0.0d0 0.0d0 0.7071067690849304d0)
   :MEASUREMENT-REGISTER 0)

* (run-quantum-program (qft '(0 1 2))
                       (make-machine :quantum-state (make-quantum-state 3)
                                     :measurement-register 0))
#S(MACHINE
   :QUANTUM-STATE #(#C(0.3535533724408484d0 0.0d0) #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0) #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0) #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0) #C(0.3535533724408484d0 0.0d0))
   :MEASUREMENT-REGISTER 0)

* (defun flip-coin ()
    (machine-measurement-register
     (run-quantum-program
      `((GATE ,+H+ 0) (MEASURE))
      (make-machine :quantum-state (make-quantum-state 1)
                    :measurement-register 0))))
FLIP-COIN

* (loop :repeat 10 :collect (flip-coin))
(1 1 0 1 1 0 0 1 0 1)

* (quit)
</code></pre><h2 id="source-code">Source code</h2>
<p>The source code in this tutorial are published under the BSD 3-clause
license. The complete listing and most up-to-date source code can be
found on
<a href="https://github.com/stylewarning/quantum-interpreter">GitHub</a>.</p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>A controlled one-qubit gate is a kind of two-qubit gate.&nbsp;<a href="#fnref:1" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>It‚Äôs actually 124 SLOC, and it has <em>not</em> been ‚Äúcode golfed‚Äù. If we wanted to make an ever tinier quantum interpreter, we could‚Äîbut brevity for its own sake is not the point.&nbsp;<a href="#fnref:2" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>With only a little bit of extra work, mostly bookkeeping, we could make $n$ finite but unbounded during the execution of a program by having instead a collection of so-called <strong>quantum registers</strong>. These would be realized by instead a collection of $v$‚Äôs, which are opportunistically combined with entanglement occurs.&nbsp;<a href="#fnref:3" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>For that matter, why complex numbers, and not just real-valued probabilities? The reason is that a complex number of unit norm can be written as $e^{i\theta}$, where $\theta$ is called the <strong>phase</strong>. Phases are a wave-like property, and allow the complex probability amplitudes to <em>interfere</em>. Interference is a known and understood phenomenon of quantum mechanical systems, and in fact is critical to the function of a quantum computer.&nbsp;<a href="#fnref:4" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Spaces with all of these properties, including a way to calculate distances, are called <strong>Hilbert spaces</strong>.&nbsp;<a href="#fnref:5" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>The fact of the matter is that we can actual get <em>more</em> general by having classical probability distributions of these states, which leads one to so-called ‚Äúdensity operators‚Äù. This is extremely useful when studying imperfect quantum computers which have noisy operations.&nbsp;<a href="#fnref:6" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>While we won‚Äôt use this fact in our interpreter, even though it would be useful for error checking, it is very easy to check if a matrix is unitary. First, we compute another matrix $h$ which is the conjugate-transpose of $g$. The <strong>conjugate-transpose</strong> of a matrix is just the transpose of a matrix with each complex entry conjugated. Once we have this matrix, we check that $hg$ is an identity matrix. The matrix $g$ is unitary if and only if $hg=gh=\mathsf{I}$.&nbsp;<a href="#fnref:7" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>Unfortunately, the definition \eqref{eq:kron} seems somewhat arbitrary and out of nowhere. Fortunately, there is a much more ‚Äúfirst principles‚Äù approach to understanding the tensor product and the Kronecker product, starting with how we map a <em>pair</em> of vectors $v\in V$ and $w\in W$ to a vector $v\otimes w\in V\otimes W$. Such approach is much more satisfying to a mathematician, and even essential to understanding the ‚Äútrue nature‚Äù of the tensor product, but perhaps less so to a curious implementer.&nbsp;<a href="#fnref:8" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>There is no sense in moving $\ket{00}$ or $\ket{11}$ to accomplish a swap-like operation, since we identify each qubits' respective $\ket 0$ identically, and each $\ket 1$ identically.&nbsp;<a href="#fnref:9" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p>If you‚Äôre not particularly keen to figure out the math yourself, you might consult Lemma 14.1 of <a href="https://www.sfu.ca/~mdevos/notes/geom-sym/14_transpositions.pdf">these lecture notes</a>. You‚Äôre also welcome to just take my word for it!&nbsp;<a href="#fnref:10" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
</ol>
</section>
</article>
</div></div>]]></description>
        </item>
    </channel>
</rss>