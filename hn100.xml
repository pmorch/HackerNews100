<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 25 Feb 2025 15:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Signal to leave Sweden if backdoor law passes (335 pts)]]></title>
            <link>https://swedenherald.com/article/signals-ceo-then-were-leaving-sweden</link>
            <guid>43171205</guid>
            <pubDate>Tue, 25 Feb 2025 12:50:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://swedenherald.com/article/signals-ceo-then-were-leaving-sweden">https://swedenherald.com/article/signals-ceo-then-were-leaving-sweden</a>, See on <a href="https://news.ycombinator.com/item?id=43171205">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="single-entry-content"><p>According to Whittaker, the bill requires the encrypted messaging app Signal to install so-called backdoors in the software.</p>
<blockquote>
<p>If you create a vulnerability based on Swedish wishes, it would create a way to undermine our entire network. Therefore, we would never introduce these backdoors, she says.</p>
</blockquote>
<p>The purpose of the bill – which may be passed next year – is for the police and Security Service to be able to request message history in retrospect for individuals suspected of crimes.</p>
<p>The Armed Forces, on the other hand, are negative and write in a letter to the government that the proposal cannot be realized "without introducing vulnerabilities and backdoors that can be exploited by third parties", reports SVT.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Y Combinator backing AI company to abuse factory workers (244 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43170850</link>
            <guid>43170850</guid>
            <pubDate>Tue, 25 Feb 2025 12:04:00 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43170850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="43171286"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171286" href="https://news.ycombinator.com/vote?id=43171286&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Tbh this isn’t that crazy. If you hire someone to do their job outputting 10 items per hour and that number is reasonable because a bunch of other workers you hired for the same job are doing it and 1 guy hits 1 per hour then that guys shouldn’t be doing that job.</p><p>The outrage should be focused on the absolute meme of their ad video cuz they were like “lets literally have a convo with an individual but refer to them as a workspace and have them say human painful responses but then just shit on them anyway impersonally”</p><p>The product is not crazy. The video is wild.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171389"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171389" href="https://news.ycombinator.com/vote?id=43171389&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Because this will definitely be used only to innocently tell off people doing 1/10 the work of everyone else, and not micromanage and hound people to increasingly unrealistic standards in already desperate conditions.</p><p>Safe to say you aren't in any position where every move you make will be watched by AI and analysed for faults so that your boss can scream at you more efficiently whenever you don't meet standards for their pitiful wages.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171537"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171537" href="https://news.ycombinator.com/vote?id=43171537&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Your example sounds reasonable but it's not realistic: The actual use of this type of tools is to intimidate those workers who have outputted 9.8 items instead of the average 9.9 over the past week.</p><p>This is who our society ended up making Amazon delivery workers urinating in fucking bottles inside their trucks.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171411"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171411" href="https://news.ycombinator.com/vote?id=43171411&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Shouldn't the manager of the 'bunch of workers' notice the guy is underperforming and understand why ? Maybe that manager is the one that shouldn't be doing that job</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171485"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171485" href="https://news.ycombinator.com/vote?id=43171485&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>And they do that by either looking over your shoulder (1 person at a time) or collecting metrics on the entire team and the output. Both of these have different downsides.</p><p>The biggest issue is leadership or managers always wanting the number to go up from the individual. "We need 12 widgets per hour instead of 10 for just this one quarter bro" but then that becomes the new norm and eventually "We need 14/16/18/20 widgets per hour..."</p><p>It's boiling frog management that makes people distrust managers doing any kind of performance monitoring</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171528"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171528" href="https://news.ycombinator.com/vote?id=43171528&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>yah this is exactly what labor law says in some countries:  a manager standing behind your desk?  ok.  a machine surveilling you?  not ok.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43171304"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171304" href="https://news.ycombinator.com/vote?id=43171304&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Continued rant :</p><p>It’s kinda like a ruler. If you measure workers so that one’s doing 10x less/worse output than the average that’s good.</p><p>If you compare workers down to the .01% difference in output that’s stupid and inhumane.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171550"><td></td></tr>
                  <tr id="43171524"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171524" href="https://news.ycombinator.com/vote?id=43171524&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>It's clear you never worked in a factory and you have just as much empathy as these CS grads.
This kind of thinking why I hate capitalism so much.</p><p>I worked in a factory multiple times and I can tell from experience nobody needs a stupid performance measurement like this. Your manager will make sure you work you ass off. Or you work with a big dangerous machine so you have to pay very much attention all day.
Of course not every factory is the same, but putting even more pressure to factory workers like this is just inhumane and the most capitalist move I can imagine. Next step is to put robotic whips next to the lines and when their productivity goes below a specific value hit them automatically... Literal slavery.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171536"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171536" href="https://news.ycombinator.com/vote?id=43171536&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Looking for 10x discrepancies is not how this will be used, and you know it. Adoption of this sort of tech is going to lead to Amazon "peeing in bottles" situations. It's wild how much faith people have in the ethics of business owners, especially the ultra-wealthy.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171313"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171313" href="https://news.ycombinator.com/vote?id=43171313&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>&gt; <i>Tbh this isn’t that crazy.</i></p><p>Yep, seems like a bog standard accountability / performance management.</p><p>&gt; <i>The product is not crazy. The video is wild.</i></p><p>This is how it all starts. Sane solutions wielded by madmen.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43171360"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171360" href="https://news.ycombinator.com/vote?id=43171360&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>This sort of performance management is unfortunately necessary. The problem is that we need tools for it to be built by people who can empathise with those subjected to them, and who want to do the right thing, and not these sorts of folks who are too immature and inexperienced to get it right.</p><p>My previous company ran a warehouse and there was a clear bell curve of productivity. Most people were fine, some were excellent, but some were below the level that was realistically achievable. We did careful and considerate analysis and it helped improve productivity.</p><p>When done badly however you end up with management using productivity tracking as a lever to increase productivity across the curve. Amazon driver delivery quotas are a great example – people urinating in bottles is clearly a symptom of the quota being too high. Unfortunately software built naively to help bring up the bottom 10% can too easily be used to force up the productivity of the other 90%.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171508"><td></td></tr>
                  <tr id="43171378"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171378" href="https://news.ycombinator.com/vote?id=43171378&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>I used to run a small factory (two assembly lines 10 people) and something like this would have been useful, not to force people to work harder but the optimise movements and points of friction. I would actively encourage and reward people for making suggestion and we had a process in place to test if changes made thing better (and not just faster - we included easier, simpler, more enjoyable etc in the test)</p><p>Sadly it’s not about the tool in this case, it’s how it’s being promoted and positioned. The line “know who’s working and who’s not” on their website says it all sadly.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171394"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171394" href="https://news.ycombinator.com/vote?id=43171394&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>As someone who grew up in a 3rd world country and whose mother owned a clothing factory, this product seems...fine? The response is an indication of how little people know about how their t-shirts and shoes are made.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171478"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171478" href="https://news.ycombinator.com/vote?id=43171478&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>It's nuanced. If it allows you to find outliers (low performers to manage and high performers to praise), that's fine. If you try to push everyone further and further to their breaking point and make them trade the same amount of money for more of their time and more importantly health, <i>it's certainly not fine</i>.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43171419"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171419" href="https://news.ycombinator.com/vote?id=43171419&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Is it dystopian, or is it just real-time performance monitoring poorly marketed by inexperienced founders?</p><p>There are tools like this for tracking git commits and velocity (that I’ve been on the receiving end of). It probably makes less sense in that context, but if your job is a repetitive task, I don’t think it’s necessarily abuse or dystopian to track it.</p><p>Monitoring bottlenecks isn’t a bad thing. They probably could have chosen an example where the solution to the bottleneck didn’t involve berating a low performer (e.g. adjusting the line to add another station or similar)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171484"><td></td></tr>
                  <tr id="43171321"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171321" href="https://news.ycombinator.com/vote?id=43171321&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Brought to you by the VC famous for InstallMonetizer? Make no mistake, it’ll basically back anything that makes money, there’s no moral high ground. And like it or not, this kind of AI (or should I say A-eye) is here to stay.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171431"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171431" href="https://news.ycombinator.com/vote?id=43171431&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>&gt; Boost your assembly line efficiency by up to 30%</p><p>Ethics of this aside the above claim must be dubious I would think the majority of manufacturing inefficiencies are due to down time as a result of raw material shipping delays or machine break down… of course I’m in no position to offer an informed opinion but just based on the product website I have a hard time taking this stuff seriously.</p><p>Monitoring of factory workers isn’t hard to do with current surveillance and 1 or 2 humans in the loop</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171209"><td></td></tr>
            <tr id="43171369"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171369" href="https://news.ycombinator.com/vote?id=43171369&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Software can not abuse workers. Managers can, with our without software.</p><p>We've had automated KPI measuring tools since punch clocks. Nowadays it's OK in some companies to install remote access software to monitor employees' screens. It's nothing new. It's just collecting data. Question is, what will bosses do with this data, will they abuse or develop.</p><p>I have no hate towards those guys. No love also. It's just business.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171368"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171368" href="https://news.ycombinator.com/vote?id=43171368&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>The secondary school the founder went to is a dead giveaway. Ofcourse yc would fund them. Anyone would fund them infact.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171056"><td></td></tr>
                <tr id="43171434"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171434" href="https://news.ycombinator.com/vote?id=43171434&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Why not deploy it across every moment of everyone's life, with algorithmic prediction of economically unproductive deviance and BadThink?</p><p>Maybe I'll pitch that to someone with money.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43170866"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43170866" href="https://news.ycombinator.com/vote?id=43170866&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Thank you, I’ve been seeing the reaction to the announcement but hadn’t yet found the announcement itself.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171452"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171452" href="https://news.ycombinator.com/vote?id=43171452&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>That post was pulled partly because of my comment. I commented this:</p><p>"While I see the economical usefullness, this sounds like the worst possible application of AI.</p><p>Using AI to surveil is building hell on earth. AI should be used to help people work less/easier, not whip them into working more."</p><p>Which ended up on the top of the thread. Was surprised to wake up this morning and see it gone.</p><p>LinkedIn post I made about this:</p><p><a href="https://www.linkedin.com/posts/crufter_today-y-combinator-deleted-this-announcement-activity-7300050840852086786-x6WH?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAABcNA2MB_d5j-ofPyG8XDq4OUyAsV3UKRKw" rel="nofollow">https://www.linkedin.com/posts/crufter_today-y-combinator-de...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171276"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171276" href="https://news.ycombinator.com/vote?id=43171276&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Imagine the features you could add to this. Like a robot that walks around behind the workers and gives well-timed corrective communications with a whip.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171339"><td></td></tr>
                  <tr id="43171022"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171022" href="https://news.ycombinator.com/vote?id=43171022&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>This kind of product is really shameful, and peak capitalism... looking at people as mere robots to serve your, disgusting</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171316"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171316" href="https://news.ycombinator.com/vote?id=43171316&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Nobody looks at people as robots. Robots are cheaper, do not require food or sleep, and do not have to be murdered when they attempt to unionize.</p><p>Robots are far superior.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171370"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171370" href="https://news.ycombinator.com/vote?id=43171370&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Not yet for many tasks. Humans are more flexible, easier to replace (nothing to install etc) and one fte in this type of work is 10 years of robot. Hope it will change soon, but it's not there yet.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171348"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171348" href="https://news.ycombinator.com/vote?id=43171348&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>But then if everyone is out of their job and unemployed ? who will buy the stuff if noone has the money.</p><p>I think there is a balance.</p><p>Otherwise its going to be 1984 in more than one way (the spying part is already there) (it would also do of that the countries are ready to produce things as much but they won't and limit it to create that constant mood of war to make people not question them / make them weak.)</p><p>I think capitalism has fallen. Capitalism is a good system but to its degrees. If you push the accelerator too hard , you get fuedalism.</p><p>and we are at feudalism. I am not sure if we can undo this. Let this sink in, the american dream , all our thinkng that capitalism being good and communism being bad fundamentally doesn't matter because we have entered a system where the lines of division are so blurry that they are practically nonexistent.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171501"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43171501" href="https://news.ycombinator.com/vote?id=43171501&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>&gt; But then if everyone is out of their job and unemployed ? who will buy the stuff if noone has the money.</p><p>Last time I tried to say something like that I got plenty comments calling me for reading too many sci-fi books... I guess some people just lack imagination and experience with exponentials.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171393"><td></td></tr>
                        <tr id="43171243"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171243" href="https://news.ycombinator.com/vote?id=43171243&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Peak capitalism would have the employees holding their own value and not being crushed in trash power dynamics to allow this kind of stuff. Capitalism is about nonviolent voluntary exchanges between two parties, when one party has a power dynamic skewed in such a way they can use tools like this that employees hate, then that’s not capitalism anymore.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171503"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171503" href="https://news.ycombinator.com/vote?id=43171503&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Funny that what you describe will never exist without heavy (extreme?) regulation and gradual taxation, which are anathema to most advocates of capitalism. Have you considered that maybe your definition of capitalism doesn't agree with the definition society has agreed upon?</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171322"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171322" href="https://news.ycombinator.com/vote?id=43171322&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>so this is more like peak feudalism huh?</p><p>back to 1800's I suppose.</p><p>There is no seperation b/w private entities , the state and the church , all trying to exploit the middle class / lower class was one of the gists that I think when I recall feudalism
sounds familiar ?
Guess what ?
We are living at one right now.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171301"><td></td></tr>
                <tr id="43171379"><td></td></tr>
                        <tr id="43171053"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171053" href="https://news.ycombinator.com/vote?id=43171053&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Apparently on their website they're asking for comments/feedback, so... you can tell them yourself what you think of their disgusting tech</p><p>&gt; Let us know at founders@optifye.ai, and we’ll help them drop their cortisol levels :)</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43171315"><td></td></tr>
                <tr id="43171375"><td></td></tr>
                  <tr id="43171257"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171257" href="https://news.ycombinator.com/vote?id=43171257&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Yeah it has terrible optics, yet it's clearly going to be normalized and come.  The question is who does it and what is the organization of it.  If this company doesn't do it, the next will.</p><p>In certain roles, AI micromanagement clearly will create higher performance.  Add the marketplace of capitalism and it'll all compete away.</p><p>There are certain roles, like artists, where this is the wrong solution wholly: monitoring whether an artist is at her desk will create badly performing artists, and this will show.  In these roles, these tools won't apply.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171324"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171324" href="https://news.ycombinator.com/vote?id=43171324&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p><i>In these roles, these tools won't apply.</i></p><p>There will be companies that will apply them regardless, even in roles where they'll make things worse. The incentive for managers to show 'a bias for action' often results in managers doing any action that they can think of rather than the right action backed by data.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171335"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171335" href="https://news.ycombinator.com/vote?id=43171335&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>In the US? Sure. In more developed parts of the world? Doubtful. European labor laws are already much, much stronger than their US counterparts, and most countries outright ban using cameras to monitor employees.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171303"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171303" href="https://news.ycombinator.com/vote?id=43171303&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>&gt; Yeah it has terrible optics, yet it's clearly going to be normalized and come. The question is who does it and what is the organization of it. If this company doesn't do it, the next will.</p><p>Where have we seen this before..</p></div></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla sales in Europe down 45% in January (223 pts)]]></title>
            <link>https://www.ft.com/content/cdd0b5c8-2703-4fd4-9ebf-26087cac8523</link>
            <guid>43170090</guid>
            <pubDate>Tue, 25 Feb 2025 10:11:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/cdd0b5c8-2703-4fd4-9ebf-26087cac8523">https://www.ft.com/content/cdd0b5c8-2703-4fd4-9ebf-26087cac8523</a>, See on <a href="https://news.ycombinator.com/item?id=43170090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a data-trackable="a11y-skip-to-help" href="https://www.ft.com/accessibility">Accessibility help</a><a data-trackable="a11y-skip-to-navigation" href="#site-navigation">Skip to navigation</a><a data-trackable="a11y-skip-to-content" href="#site-content">Skip to content</a><a data-trackable="a11y-skip-to-footer" href="#site-footer">Skip to footer</a></p><div id="barrier-page"><div id="heroOffer-Hero offer-6dcd8564-bf41-453c-93b5-7b00c6676b60" data-component="heroOffer" data-component-unique-name="Hero offer"><div data-o-grid-colspan="12 L6"><p><span></span><span></span><span></span><span>Subscribe to unlock this article</span><span></span></p></div><div data-o-grid-colspan="12 L6"><p><h2><span>Limited time offer</span></h2><h2><strong><span>Save 40% on Standard Digital</span></strong></h2></p><p><span>was </span><span>CHF660</span><span> </span><span>now </span><span>CHF395</span><span> for your first year
Make up your own mind. Build robust opinions on the FT's trusted journalism.
Offer available until 27 February 2025.</span></p></div></div><div id="recommendedOffers-Recommended offers" data-component="recommendedOffers" data-component-unique-name="Recommended offers"><p><h2 data-o-grid-colspan="12">Explore more offers.</h2></p><div data-o-grid-colspan="12"><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_trial.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>CHF1</span><span> for 4 weeks</span></p><p><span>Then </span><span>CHF85</span><span> per month. Complete digital access to quality FT journalism. Cancel anytime during your trial.</span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_weekend_premium.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>was </span><span>CHF949</span><span> </span><span>now </span><span>CHF815</span><span> per year</span></p><p><span>Get Premium &amp; FT Weekend Print edition for the price of Premium. Complete digital access to quality analysis and expert insights, complemented with our award-winning Weekend Print edition.</span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_print.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>CHF345</span><span> for your first year</span></p><p><span>FT newspaper delivered Monday-Saturday, plus FT Digital Edition delivered to your device Monday-Saturday.</span></p></div></div></div><div data-component="subscriptionOptions" data-component-unique-name="Subscription options"><h2>Explore our full range of subscriptions.</h2><div><div><p>Discover all the plans currently available in your country</p></div><div><p>Digital access for organisations. Includes exclusive features and content.</p></div></div></div><div data-component="whyFT" data-component-unique-name="Why FT"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft?ft-content-uuid=cdd0b5c8-2703-4fd4-9ebf-26087cac8523">Find out why</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Awesome DeepSeek Integrations (106 pts)]]></title>
            <link>https://github.com/deepseek-ai/awesome-deepseek-integration</link>
            <guid>43169827</guid>
            <pubDate>Tue, 25 Feb 2025 09:23:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/awesome-deepseek-integration">https://github.com/deepseek-ai/awesome-deepseek-integration</a>, See on <a href="https://news.ycombinator.com/item?id=43169827">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><table>
    <tbody><tr>    
        <td><a target="_blank" rel="noopener noreferrer" href="https://github.com/ThinkInAIXYZ/deepchat/blob/main/build/icon.png?raw=true"><img src="https://github.com/ThinkInAIXYZ/deepchat/raw/main/build/icon.png?raw=true" alt="Icon" width="64" height="auto"></a></td>
        <td><a href="https://github.com/ThinkInAIXYZ/deepchat/blob/main/README.md">DeepChat</a></td>
        <td>DeepChat is a fully free desktop smart assistant, with a powerful DeepSeek large model, supporting multi-round conversations, internet search, file uploads, knowledge bases, and more.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/171659527?s=400&amp;u=39906ab3b6e2066f83046096a66a77fb3f8bb836&amp;v=4"><img src="https://avatars.githubusercontent.com/u/171659527?s=400&amp;u=39906ab3b6e2066f83046096a66a77fb3f8bb836&amp;v=4" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/quantalogic/quantalogic">Quantalogic</a> </td>
        <td> QuantaLogic is a ReAct (Reasoning &amp; Action) framework for building advanced AI agents. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/13600976/295807353-224d547a-6fbc-47c8-859f-aa14813e2b0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8xMzYwMDk3Ni8yOTU4MDczNTMtMjI0ZDU0N2EtNmZiYy00N2M4LTg1OWYtYWExNDgxM2UyYjBmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZkM2Q2NTI3ZmRjNWY4MDdhMDYxYzMyMWFiZDZhZTA3NGI5NWM0NjNkODIxN2Q4NzA0YWUzODg3NDg4MDJjZjcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.s8qVrORMqA5dEZLpjkrDFrv8Rxifqq3fZxYSP5VLboI"><img src="https://private-user-images.githubusercontent.com/13600976/295807353-224d547a-6fbc-47c8-859f-aa14813e2b0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8xMzYwMDk3Ni8yOTU4MDczNTMtMjI0ZDU0N2EtNmZiYy00N2M4LTg1OWYtYWExNDgxM2UyYjBmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZkM2Q2NTI3ZmRjNWY4MDdhMDYxYzMyMWFiZDZhZTA3NGI5NWM0NjNkODIxN2Q4NzA0YWUzODg3NDg4MDJjZjcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.s8qVrORMqA5dEZLpjkrDFrv8Rxifqq3fZxYSP5VLboI" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/chatbox/README.md">Chatbox</a> </td>
        <td> Chatbox is a desktop client for multiple cutting-edge LLM models, available on Windows, Mac and Linux. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/59196087/295846436-bb65404c-f867-42d8-ae2b-281fe953ab54.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii81OTE5NjA4Ny8yOTU4NDY0MzYtYmI2NTQwNGMtZjg2Ny00MmQ4LWFlMmItMjgxZmU5NTNhYjU0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExNmIxYWRiNzQyMDU5NDBjYWUwYjdkZjEzZWFjMTUyYTdhYzE2ZGQ0ZDM0Zjc5NjZkMTBiN2FmNGY5M2QxNjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.YcFUqt7RuqpUhSaeADZlZ27zCUVVY-RAppj8UVlsCCU"><img src="https://private-user-images.githubusercontent.com/59196087/295846436-bb65404c-f867-42d8-ae2b-281fe953ab54.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii81OTE5NjA4Ny8yOTU4NDY0MzYtYmI2NTQwNGMtZjg2Ny00MmQ4LWFlMmItMjgxZmU5NTNhYjU0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExNmIxYWRiNzQyMDU5NDBjYWUwYjdkZjEzZWFjMTUyYTdhYzE2ZGQ0ZDM0Zjc5NjZkMTBiN2FmNGY5M2QxNjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.YcFUqt7RuqpUhSaeADZlZ27zCUVVY-RAppj8UVlsCCU" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/chatgpt_next_web/README.md"> ChatGPT-Next-Web </a> </td>
        <td> ChatGPT Next Web is a cross-platform ChatGPT web UI, with GPT3, GPT4 &amp; Gemini Pro support. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/Coco%20AI/assets/favicon.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/Coco%20AI/assets/favicon.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/Coco%20AI/README.md">Coco AI</a></td>
        <td> <a href="https://coco.rs/" rel="nofollow">Coco AI</a> is a fully open-source, cross-platform unified search and productivity tool that connects and searches across various data sources, including applications, files, Google Drive, Notion, Yuque, Hugo, and more, both local and cloud-based. By integrating with large models like DeepSeek, Coco AI enables intelligent personal knowledge management, emphasizing privacy and supporting private deployment, helping users quickly and intelligently access their information. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/liubai/assets/liubai-logo.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/liubai/assets/liubai-logo.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/liubai/README.md">Liubai</a> </td>
        <td> Liubai allows DeepSeek to have arms and legs to manipulate your notes, tasks, calendars, and to-do lists just on WeChat! </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/59196087/295849194-1ac9791b-87f7-41d9-9282-a70698344e1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii81OTE5NjA4Ny8yOTU4NDkxOTQtMWFjOTc5MWItODdmNy00MWQ5LTkyODItYTcwNjk4MzQ0ZTFkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQzMjRiMmUxMjRkNWI4MGEzNTI0M2IwNDUwZTMxY2ZhYzM5ZjhiMTlhNTNiOWFkNWY4MWFkMWExMTQzNmZjYzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.wmTb4DZoPsZgMv1my1lUq5PKOo8Aj4aArlZW21eLQEc"><img src="https://private-user-images.githubusercontent.com/59196087/295849194-1ac9791b-87f7-41d9-9282-a70698344e1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii81OTE5NjA4Ny8yOTU4NDkxOTQtMWFjOTc5MWItODdmNy00MWQ5LTkyODItYTcwNjk4MzQ0ZTFkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQzMjRiMmUxMjRkNWI4MGEzNTI0M2IwNDUwZTMxY2ZhYzM5ZjhiMTlhNTNiOWFkNWY4MWFkMWExMTQzNmZjYzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.wmTb4DZoPsZgMv1my1lUq5PKOo8Aj4aArlZW21eLQEc" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/pal/README.md"> Pal - AI Chat Client<br>(iOS, ipadOS) </a> </td>
        <td> Pal is a customized chat playground on iOS. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/099fcbeaa5d07a52e261dffe575cae7fe0fcfffbaaeff5b1eed55f9956a0b2b9/68747470733a2f2f7777772e6c69627265636861742e61692f6c69627265636861742e737667"><img src="https://camo.githubusercontent.com/099fcbeaa5d07a52e261dffe575cae7fe0fcfffbaaeff5b1eed55f9956a0b2b9/68747470733a2f2f7777772e6c69627265636861742e61692f6c69627265636861742e737667" alt="LibreChat" width="64" height="auto" data-canonical-src="https://www.librechat.ai/librechat.svg"></a> </td>
        <td> <a href="https://www.librechat.ai/docs/configuration/librechat_yaml/ai_endpoints/deepseek" rel="nofollow">LibreChat</a> </td>
        <td> LibreChat is a customizable open-source app that seamlessly integrates DeepSeek for enhanced AI interactions. </td>
    </tr>
     <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/longevity-genie/chat-ui/11c6647c83f9d2de21180b552474ac5ffcf53980/static/geneticsgenie/icon-128x128.png"><img src="https://raw.githubusercontent.com/longevity-genie/chat-ui/11c6647c83f9d2de21180b552474ac5ffcf53980/static/geneticsgenie/icon-128x128.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/longevity-genie/just-chat">Just-Chat</a> </td>
        <td> Make your LLM agent and chat with it simple and fast!</td>
     </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3856d7880b3489a97086a10235267f303677b64de726b694510e2c2b946c8e4b/68747470733a2f2f7777772e7061706572736770742e636f6d2f696d616765732f6c6f676f2f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/3856d7880b3489a97086a10235267f303677b64de726b694510e2c2b946c8e4b/68747470733a2f2f7777772e7061706572736770742e636f6d2f696d616765732f6c6f676f2f66617669636f6e2e69636f" alt="PapersGPT" width="64" height="auto" data-canonical-src="https://www.papersgpt.com/images/logo/favicon.ico"></a> </td>
        <td> <a href="https://github.com/papersgpt/papersgpt-for-zotero">PapersGPT</a> </td>
        <td> PapersGPT is a Zotero plugin that seamlessly with DeepSeek and other multiple AI models for quickly reading papers in Zotero. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/rss-translator/RSS-Translator/main/core/static/favicon.ico"><img src="https://raw.githubusercontent.com/rss-translator/RSS-Translator/main/core/static/favicon.ico" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/rss_translator/README.md"> RSS Translator </a> </td>
        <td> Translate RSS feeds into your language! </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/ysnows/enconvo_media/main/logo.png"><img src="https://raw.githubusercontent.com/ysnows/enconvo_media/main/logo.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/enconvo/README.md"> Enconvo </a> </td>
        <td> Enconvo is the Launcher of the AI era, the entry point for all AI functions, and a thoughtful intelligent assistant.</td>
    </tr>
    <tr>
        <td><a target="_blank" rel="noopener noreferrer" href="https://github.com/kangfenmao/cherry-studio/blob/main/src/renderer/src/assets/images/logo.png?raw=true"><img src="https://github.com/kangfenmao/cherry-studio/raw/main/src/renderer/src/assets/images/logo.png?raw=true" alt="Icon" width="64" height="auto"></a></td>
        <td><a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/cherrystudio/README.md">Cherry Studio</a></td>
        <td>A powerful desktop AI assistant for producer</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bf93b3f6e5e8507f37f363ade24096d52f638646f083e0b46828ec9a0c32c6f7/68747470733a2f2f746f6d656d6f2e746f702f696d616765732f6c6f676f2e706e67"><img src="https://camo.githubusercontent.com/bf93b3f6e5e8507f37f363ade24096d52f638646f083e0b46828ec9a0c32c6f7/68747470733a2f2f746f6d656d6f2e746f702f696d616765732f6c6f676f2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://tomemo.top/images/logo.png"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/tomemo/README.md"> ToMemo (iOS, ipadOS) </a> </td>
        <td> A phrasebook + clipboard history + keyboard iOS app with integrated AI macromodeling for quick output use in the keyboard.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/buxuku/video-subtitle-master/refs/heads/main/resources/icon.png"><img src="https://raw.githubusercontent.com/buxuku/video-subtitle-master/refs/heads/main/resources/icon.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/buxuku/video-subtitle-master">Video Subtitle Master</a></td>
        <td> Batch generate subtitles for videos, with the ability to translate subtitles into other languages. This is a client-side tool that supports both Mac and Windows platforms and integrates with multiple translation services such as Baidu, Volcengine, DeepLx, OpenAI, DeepSeek, and Ollama.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/UnknownEnergy/chatgpt-api/blob/master/dist/assets/chatworm-72x72.png"><img src="https://github.com/UnknownEnergy/chatgpt-api/raw/master/dist/assets/chatworm-72x72.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/UnknownEnergy/chatgpt-api/blob/master/README.md">Chatworm</a> </td>
        <td> Chatworm is a webapp for multiple cutting-edge LLM models, open-source and also available on Android. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/icon_512x512@2x.png"><img src="https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/icon_512x512@2x.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/tisfeng/Easydict">Easydict</a></td>
        <td> Easydict is a concise and easy-to-use translation dictionary macOS App that allows you to easily and elegantly look up words or translate text. Supports calling large language model APIs for translation.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6f1817b6a7cec2fcf91243e7b695f87de7da88269d9ba27287301dd4085ea803/68747470733a2f2f7777772e726179636173742e636f6d2f66617669636f6e2d70726f64756374696f6e2e706e67"><img src="https://camo.githubusercontent.com/6f1817b6a7cec2fcf91243e7b695f87de7da88269d9ba27287301dd4085ea803/68747470733a2f2f7777772e726179636173742e636f6d2f66617669636f6e2d70726f64756374696f6e2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://www.raycast.com/favicon-production.png"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/raycast/README.md">Raycast</a></td>
        <td> <a href="https://raycast.com/?via=ViGeng" rel="nofollow">Raycast</a> is a productivity tool for macOS that lets you control your tools with a few keystrokes. It supports various extensions including DeepSeek AI.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7b296d5a5eb2fe1fdef84d531050039229fa75eb1939ae069cd99b8461684bdb/68747470733a2f2f6e69636570726f6d70742e6170702f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/7b296d5a5eb2fe1fdef84d531050039229fa75eb1939ae069cd99b8461684bdb/68747470733a2f2f6e69636570726f6d70742e6170702f66617669636f6e2e69636f" alt="Icon" width="64" height="auto" data-canonical-src="https://niceprompt.app/favicon.ico"></a> </td> <td> <a href="https://niceprompt.app/" rel="nofollow">Nice Prompt</a></td> <td> <a href="https://niceprompt.app/" rel="nofollow">Nice Prompt</a> Organize, share and use your prompts in your code editor, with Cursor and VSCode。</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/193405629?s=200&amp;v=4"><img src="https://avatars.githubusercontent.com/u/193405629?s=200&amp;v=4" alt="PHP Client" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-php/deepseek-php-client/blob/master/README.md">PHP Client</a> </td>
        <td> Deepseek PHP Client is a robust and community-driven PHP client library for seamless integration with the Deepseek API. </td>
    </tr>
        <tr>
  <td>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/tornikegomareli/DeepSwiftSeek/blob/main/logo.webp"><img src="https://github.com/tornikegomareli/DeepSwiftSeek/raw/main/logo.webp" alt="DeepSwiftSeek Logo" width="64" height="auto"></a>
  </td>
  <td>
    <a href="https://github.com/tornikegomareli/DeepSwiftSeek/blob/main/README.md">DeepSwiftSeek</a>
  </td>
  <td>
    DeepSwiftSeek is a lightweight yet powerful Swift client library, pretty good integration with the DeepSeek API. 
    It provides easy-to-use Swift concurrency for chat, streaming, FIM (Fill-in-the-Middle) completions, and more.
  </td>
</tr>
        <tr><td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/958072?s=200&amp;v=4"><img src="https://avatars.githubusercontent.com/u/958072?s=200&amp;v=4" alt="Laravel Integration" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-php/deepseek-laravel/blob/master/README.md">Laravel Integration</a> </td>
        <td> Laravel wrapper for Deepseek PHP client, to seamless deepseek API integration with laravel applications.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/zotero/assets/zotero-icon.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/zotero/assets/zotero-icon.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/zotero/README_cn.md">Zotero</a></td>
        <td> <a href="https://www.zotero.org/" rel="nofollow">Zotero</a> is a free, easy-to-use tool to help you collect, organize, annotate, cite, and share research.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f6cdd8c1aadf66cbf471860750d26bfaab8f49b153e2ca3bd5afb12d3628f2e4/68747470733a2f2f62336c6f672e6f72672f696d616765732f6272616e642f73697975616e2d3132382e706e67"><img src="https://camo.githubusercontent.com/f6cdd8c1aadf66cbf471860750d26bfaab8f49b153e2ca3bd5afb12d3628f2e4/68747470733a2f2f62336c6f672e6f72672f696d616765732f6272616e642f73697975616e2d3132382e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://b3log.org/images/brand/siyuan-128.png"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/SiYuan/README.md">SiYuan</a> </td>
        <td> SiYuan is a privacy-first personal knowledge management system that supports complete offline usage, as well as end-to-end encrypted data sync.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/ArvinLovegood/go-stock/raw/master/build/appicon.png"><img src="https://github.com/ArvinLovegood/go-stock/raw/master/build/appicon.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/ArvinLovegood/go-stock/blob/master/README.md">go-stock</a> </td>
        <td>go-stock is a Chinese stock data viewer built by Wails with NativeUI and powered by LLM.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/102771702?s=200&amp;v=4"><img src="https://avatars.githubusercontent.com/u/102771702?s=200&amp;v=4" alt="Wordware" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/wordware/README.md">Wordware</a> </td>
        <td><a href="https://www.wordware.ai/" rel="nofollow">Wordware</a> is a toolkit that enables anyone to build, iterate, and deploy their AI stack with just natural language.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8b5c668210552bd34e3310b85b1c0e38fdf3c843973b741e355f8e269c370128/68747470733a2f2f6672616d657275736572636f6e74656e742e636f6d2f696d616765732f78524a36764e6f396d555965564e7874304b49545843584575536b2e706e67"><img src="https://camo.githubusercontent.com/8b5c668210552bd34e3310b85b1c0e38fdf3c843973b741e355f8e269c370128/68747470733a2f2f6672616d657275736572636f6e74656e742e636f6d2f696d616765732f78524a36764e6f396d555965564e7874304b49545843584575536b2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://framerusercontent.com/images/xRJ6vNo9mUYeVNxt0KITXCXEuSk.png"></a> </td>
        <td> <a href="https://github.com/langgenius/dify/">Dify</a> </td>
        <td> <a href="https://dify.ai/" rel="nofollow">Dify</a> is an LLM application development platform that supports DeepSeek models for creating assistants, workflows, text generators, and more. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/enricoros/big-AGI/refs/heads/v2-dev/public/favicon.ico"><img src="https://raw.githubusercontent.com/enricoros/big-AGI/refs/heads/v2-dev/public/favicon.ico" alt="Big-AGI" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/enricoros/big-AGI/blob/v2-dev/README.md">Big-AGI</a> </td>
        <td><a href="https://big-agi.com/" rel="nofollow">Big-AGI</a> is a groundbreaking AI suite designed to democratize access to advanced artificial intelligence for everyone.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/LiberSonora/LiberSonora/blob/main/assets/avatar.jpeg?raw=true"><img src="https://github.com/LiberSonora/LiberSonora/raw/main/assets/avatar.jpeg?raw=true" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/LiberSonora/LiberSonora/blob/main/README_en.md">LiberSonora</a> </td>
        <td> LiberSonora, meaning "Voice of Freedom", is an AI-powered, robust, open-source audiobook toolkit that includes features like intelligent subtitle extraction, AI title generation, multilingual translation, with support for GPU acceleration and batch offline processing.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/ripperhe/Bob/master/docs/_media/icon_128.png"><img src="https://raw.githubusercontent.com/ripperhe/Bob/master/docs/_media/icon_128.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://bobtranslate.com/" rel="nofollow">Bob</a></td>
        <td> <a href="https://bobtranslate.com/" rel="nofollow">Bob</a> is a macOS translation &amp; OCR tool ready to use in any app — right out of the box!</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/902ed4848e827dcbea0f8d3300782f922664846019be600557f4d8b77b857035/68747470733a2f2f6167656e746963666c6f772e61692f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/902ed4848e827dcbea0f8d3300782f922664846019be600557f4d8b77b857035/68747470733a2f2f6167656e746963666c6f772e61692f66617669636f6e2e69636f" alt="Icon" width="64" height="auto" data-canonical-src="https://agenticflow.ai/favicon.ico"></a> </td>
        <td> <a href="https://agenticflow.ai/" rel="nofollow">AgenticFlow</a> </td>
        <td> <a href="https://agenticflow.ai/" rel="nofollow">AgenticFlow</a> is a no-code platform where marketers build agentic AI workflows for go-to-market automation, powered by hundreds of everyday apps as tools for your AI agents.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZGGSONG/STranslate/raw/main/img/favicon.svg"><img src="https://github.com/ZGGSONG/STranslate/raw/main/img/favicon.svg" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://stranslate.zggsong.com/en/" rel="nofollow">STranslate</a></td>
        <td> <a href="https://stranslate.zggsong.com/en/" rel="nofollow">STranslate</a>（Windows） is a ready-to-go translation ocr tool developed by WPF </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/32067919/408034405-5e16beb0-993e-47bf-807e-7c8804b313a2.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8zMjA2NzkxOS80MDgwMzQ0MDUtNWUxNmJlYjAtOTkzZS00N2JmLTgwN2UtN2M4ODA0YjMxM2EyLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTMyOTE4ZWUyZjlhMGZhNTY3MzE2OGQxYjcxZmZjYTNmNzUwZTcyMjdlYmJkM2VmNDVkZGIwMzQ0N2U3NzI0YTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Icv1_i2hpH31uR1lFrsozznE55XOb2Sun-5ubFNbtBA"><img src="https://private-user-images.githubusercontent.com/32067919/408034405-5e16beb0-993e-47bf-807e-7c8804b313a2.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8zMjA2NzkxOS80MDgwMzQ0MDUtNWUxNmJlYjAtOTkzZS00N2JmLTgwN2UtN2M4ODA0YjMxM2EyLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTMyOTE4ZWUyZjlhMGZhNTY3MzE2OGQxYjcxZmZjYTNmNzUwZTcyMjdlYmJkM2VmNDVkZGIwMzQ0N2U3NzI0YTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Icv1_i2hpH31uR1lFrsozznE55XOb2Sun-5ubFNbtBA" alt="Asp Client" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/Anwar-alhitar/Deepseek.Asp.Client/blob/master/README.md">ASP Client</a> </td>
        <td><a href="https://github.com/Anwar-alhitar/Deepseek.Asp.Client/blob/master/README.md">Deepseek.ASPClient</a>  is a lightweight ASP.NET wrapper for the Deepseek AI API, designed to simplify AI-driven text processing in .NET applications.. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/64c8ae493f81ae0abfb66e0e21ec886a6dafc733ea6a53afd31cfec1893eddc6/68747470733a2f2f7777772e6770746169666c6f772e746563682f6c6f676f2e706e67"><img src="https://camo.githubusercontent.com/64c8ae493f81ae0abfb66e0e21ec886a6dafc733ea6a53afd31cfec1893eddc6/68747470733a2f2f7777772e6770746169666c6f772e746563682f6c6f676f2e706e67" alt="gpt-ai-flow-logo" width="64" height="auto" data-canonical-src="https://www.gptaiflow.tech/logo.png"></a> </td>
        <td> <a href="https://www.gptaiflow.tech/docs/product/api-keys-setup#setup-deepseek-api-keys" rel="nofollow">GPT AI Flow</a></td>
        <td>
            The ultimate productivity weapon built by engineers for efficiency enthusiasts (themselves): <a href="https://www.gptaiflow.tech/" rel="nofollow">GPT AI Flow</a>
            <ul dir="auto">
                <li>`Shift+Alt+Space` Wake up desktop intelligent hub</li>
                <li>Local encrypted storage</li>
                <li>Custom instruction engine</li>
                <li>On-demand calling without subscription bundling</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/14835226/409142715-b09f17a8-936d-4dac-8b24-1682d52c9a3c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8xNDgzNTIyNi80MDkxNDI3MTUtYjA5ZjE3YTgtOTM2ZC00ZGFjLThiMjQtMTY4MmQ1MmM5YTNjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzNzBlNTliNTY4M2M2Y2IwMWU0NTRjN2QxOWIzMWYxY2YxOWIyNzBhZmVlODY1NmNjMTk5N2EwNGI4MDFhNTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.26iZD75OPiqfIie5FHmrdlrNawBOWwLcdDt0oRXetAg"><img src="https://private-user-images.githubusercontent.com/14835226/409142715-b09f17a8-936d-4dac-8b24-1682d52c9a3c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8xNDgzNTIyNi80MDkxNDI3MTUtYjA5ZjE3YTgtOTM2ZC00ZGFjLThiMjQtMTY4MmQ1MmM5YTNjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzNzBlNTliNTY4M2M2Y2IwMWU0NTRjN2QxOWIzMWYxY2YxOWIyNzBhZmVlODY1NmNjMTk5N2EwNGI4MDFhNTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.26iZD75OPiqfIie5FHmrdlrNawBOWwLcdDt0oRXetAg" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/alecm20/story-flicks">Story-Flicks</a></td>
        <td>With just one sentence, you can quickly generate high-definition story short videos, supporting models such as DeepSeek.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/901a787a65901d2f6d8daa5988635d8f914336711faf00634d3e957c6669e364/68747470733a2f2f70726f6d70742e3136782e656e67696e6565722f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/901a787a65901d2f6d8daa5988635d8f914336711faf00634d3e957c6669e364/68747470733a2f2f70726f6d70742e3136782e656e67696e6565722f66617669636f6e2e69636f" alt="Icon" width="64" height="auto" data-canonical-src="https://prompt.16x.engineer/favicon.ico"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/16x_prompt/README.md">16x Prompt</a> </td>
        <td> <a href="https://prompt.16x.engineer/" rel="nofollow">16x Prompt</a> is an AI coding tool with context management. It helps developers manage source code context and craft prompts for complex coding tasks on existing codebases.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/Alpha%E6%B4%BE/assets/favicon1.png?raw=true"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/Alpha%E6%B4%BE/assets/favicon1.png?raw=true" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/Alpha%E6%B4%BE/README.md"> Alpha Pai </a> </td>
        <td> AI Research Assistant / The Next-Generation Financial Information Portal Driven by AI.<br>Proxy for investors to attend meetings and take notes, as well as providing search and Q&amp;A services for financial information and quantitative analysis for investment research.</td>
    </tr>
        <tr><td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ddb4b0af3d5d1e4276301599439fd72d0cf19471dd70ac0a70046103d30d451a/68747470733a2f2f646f63732e7861726b2d6172676f2e636f6d2f696d672f6c6f676f2e706e67"><img src="https://camo.githubusercontent.com/ddb4b0af3d5d1e4276301599439fd72d0cf19471dd70ac0a70046103d30d451a/68747470733a2f2f646f63732e7861726b2d6172676f2e636f6d2f696d672f6c6f676f2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://docs.xark-argo.com/img/logo.png"></a> </td> 
        <td> <a href="https://www.xark-argo.com/" rel="nofollow">argo</a> </td>
        <td>Locally download and run Ollama and Huggingface models with RAG on Mac/Windows/Linux. Support LLM API too.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f33d267a05cee432e7e3e5bb88f47d0d8b9a0c93fa985daa535a48c351c597fc/68747470733a2f2f7777772e70657465726361742e61692f696d616765732f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/f33d267a05cee432e7e3e5bb88f47d0d8b9a0c93fa985daa535a48c351c597fc/68747470733a2f2f7777772e70657465726361742e61692f696d616765732f66617669636f6e2e69636f" alt="Icon" width="64" height="auto" data-canonical-src="https://www.petercat.ai/images/favicon.ico"></a> </td>
        <td> <a href="https://www.petercat.ai/" rel="nofollow">PeterCat</a> </td>
        <td> A conversational Q&amp;A agent configuration system, self-hosted deployment solutions, and a convenient all-in-one application SDK, allowing you to create intelligent Q&amp;A bots for your GitHub repositories.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/labring/FastGPT/refs/heads/main/.github/imgs/logo.svg"><img src="https://raw.githubusercontent.com/labring/FastGPT/refs/heads/main/.github/imgs/logo.svg" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://fastgpt.cn/en" rel="nofollow">FastGPT</a> </td>
        <td> 
            FastGPT is an open-source AI knowledge base platform built on large language models (LLMs), supporting various models including DeepSeek and OpenAI. We provide out-of-the-box capabilities for data processing, model invocation, RAG retrieval, and visual AI workflow orchestration, enabling you to effortlessly build sophisticated AI applications.
        </td>
   </tr>
   <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/ruzhiai_note/assets/play_store_512.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/ruzhiai_note/assets/play_store_512.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/ruzhiai_note/README.md">RuZhi AI Notes</a> </td>
        <td>RuZhi AI Notes is an intelligent knowledge management tool powered by AI, providing one-stop knowledge management and application services including AI search &amp; exploration, AI results to notes conversion, note management &amp; organization, knowledge presentation &amp; sharing. Integrated with DeepSeek model to provide more stable and higher quality outputs.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7d03f3a5a1f9316d2fc9dd60b466b3e63fdd8d0026ec8f2ab322152199ac5b42/68747470733a2f2f63646e2e6c696e6b2d61692e746563682f646f632f436f572532306c6f676f2e706e67"><img src="https://camo.githubusercontent.com/7d03f3a5a1f9316d2fc9dd60b466b3e63fdd8d0026ec8f2ab322152199ac5b42/68747470733a2f2f63646e2e6c696e6b2d61692e746563682f646f632f436f572532306c6f676f2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://cdn.link-ai.tech/doc/CoW%20logo.png"></a> </td>
        <td> <a href="https://github.com/zhayujie/chatgpt-on-wechat">Chatgpt-on-Wechat</a> </td>
        <td> Chatgpt-on-Wechat(CoW) is a flexible chatbot framework that supports seamless integration of multiple LLMs, including DeepSeek, OpenAI, Claude, Qwen, and others, into commonly used platforms or office software such as WeChat Official Accounts, WeCom, Feishu, DingTalk, and websites. It also supports a wide range of custom plugins. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0feb9a6a193c29d48bc5a74ffc68a9af3d0aee605f5c707e324f27c3df09d97a/68747470733a2f2f617468656e616c61622e61692f6173736574732f66617669636f6e2f66617669636f6e2e737667"><img src="https://camo.githubusercontent.com/0feb9a6a193c29d48bc5a74ffc68a9af3d0aee605f5c707e324f27c3df09d97a/68747470733a2f2f617468656e616c61622e61692f6173736574732f66617669636f6e2f66617669636f6e2e737667" alt="Icon" width="64" height="auto" data-canonical-src="https://athenalab.ai/assets/favicon/favicon.svg"></a> </td> 
        <td> <a href="https://athenalab.ai/" rel="nofollow">Athena</a> </td>
        <td>The world's first autonomous general AI with advanced cognitive architecture and human-like reasoning capabilities, designed to tackle complex real-world challenges.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d4a5745a9a6978748eb1c432efe9c5e8bd8dbc9af981a746f68316713c2411ed/68747470733a2f2f6d61786b622e636e2f696d616765732f66617669636f6e2e706e67"><img src="https://camo.githubusercontent.com/d4a5745a9a6978748eb1c432efe9c5e8bd8dbc9af981a746f68316713c2411ed/68747470733a2f2f6d61786b622e636e2f696d616765732f66617669636f6e2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://maxkb.cn/images/favicon.png"></a> </td>
        <td> <a href="https://github.com/1Panel-dev/MaxKB">MaxKB</a> </td>
        <td> <a href="https://maxkb.cn/" rel="nofollow">MaxKB</a> is a ready-to-use, flexible RAG Chatbot. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/TigerGPT/assets/logo.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/TigerGPT/assets/logo.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://ttm.financial/gpt" rel="nofollow">TigerGPT</a> </td>
        <td>TigerGPT is the first financial AI investment assistant of its kind based on OpenAI, developed by Tiger Group. TigerGPT aims to provide intelligent investment decision-making support for investors. On February 18, 2025, TigerGPT officially integrated the DeepSeek-R1 model to provide users with online Q&amp;A services that support deep reasoning. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/HIX.AI/assets/logo.svg"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/HIX.AI/assets/logo.svg" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://hix.ai/" rel="nofollow">HIX.AI</a> </td>
        <td>Try DeepSeek for free and enjoy unlimited AI chat on HIX.AI. Use DeepSeek R1 for AI chat, writing, coding &amp; more. Experience next-gen AI chat now!</td>
    </tr>
</tbody></table></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Core Git Developers Configure Git (453 pts)]]></title>
            <link>https://blog.gitbutler.com/how-git-core-devs-configure-git/</link>
            <guid>43169435</guid>
            <pubDate>Tue, 25 Feb 2025 08:17:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.gitbutler.com/how-git-core-devs-configure-git/">https://blog.gitbutler.com/how-git-core-devs-configure-git/</a>, See on <a href="https://news.ycombinator.com/item?id=43169435">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
	
	<div>
		<p>A few weeks ago I <a href="https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/" rel="noreferrer">wrote about</a> Git’s <code>help.autocorrect</code> setting and the strange tale of the origin of it’s deciseconds value.</p><p>It got me to thinking about other <code>git config</code> settings that most people likely don’t know about and which should probably be defaulted differently. </p><p>In this post, I’ll go through some of the perhaps obscure Git config settings that I have personally globally enabled and go into them to explain what they do and why they should <em>probably</em> be the default settings. </p><p>Also, it turns out that I learned most of these from the people who actually work on the core Git codebase every day.</p><h2 id="tldr">TLDR</h2><p>First, though, some of you may not particularly care about the wonderful and sordid history of the <code>rerere</code> values or whatever. You may just be thinking “just give me the settings so I can blindly throw them into my <code>~/.gitconfig</code> file." </p><p>Well, fair enough. Here is the fun stuff:</p><pre><code># clearly makes git better

[column]
        ui = auto
[branch]
        sort = -committerdate
[tag]
        sort = version:refname
[init]
        defaultBranch = main
[diff]
        algorithm = histogram
        colorMoved = plain
        mnemonicPrefix = true
        renames = true
[push]
        default = simple
        autoSetupRemote = true
        followTags = true
[fetch]
        prune = true
        pruneTags = true
        all = true

# why the hell not?

[help]
        autocorrect = prompt
[commit]
        verbose = true
[rerere]
        enabled = true
        autoupdate = true
[core]
        excludesfile = ~/.gitignore
[rebase]
        autoSquash = true
        autoStash = true
        updateRefs = true

# a matter of taste (uncomment if you dare)

[core]
        # fsmonitor = true
        # untrackedCache = true
[merge]
        # (just 'diff3' if git version &lt; 2.3)
        # conflictstyle = zdiff3 
[pull]
        # rebase = true</code></pre><p>Copypasta, my friends.</p><h2 id="how-do-git-core-devs-configure-their-gits">How do Git core devs configure their Gits?</h2><p>Before I dig into these one by one, there is an interesting question about if even the core Git developers think that some of these default values should be changed.</p><p>This came up not too long ago on the Git mailing list, and honestly, a few of these settings I personally learned from <a href="https://lore.kernel.org/git/60b5d281552d6_e359f20828@natae.notmuch/?ref=blog.gitbutler.com">this thread</a> called "Spring Cleaning" where Felipe Contreras challenged the Git core team to remove all their built up config options and aliases and see what it’s like to use Git stock, out of the box.</p><p>He challenged the list to pay attention to what settings they really wanted to change and share the top settings changes that seemed the most important with the list.</p><p>The <a href="https://lore.kernel.org/git/60df97ed24687_34a92088a@natae.notmuch/?ref=blog.gitbutler.com">results</a> were very interesting, a rather concise list of 9 config settings and 3 aliases that the experiment participants more or less agreed should arguably be new defaults. Let's just take a look at the proposed config setting changes.</p><pre><code>merge.conflictstyle = zdiff3
rebase.autosquash = true
rebase.autostash = true 
commit.verbose = true
diff.colorMoved = true
diff.algorithm = histogram
grep.patternType = perl
feature.experimental = true
branch.sort = committerdate</code></pre><p>Now, <em>none</em> of these have become the new defaults in the 3 or 4 years since this experiment, but it’s interesting that a lot of the Git developers themselves have a hard time using Git without several of these turned on.</p><p>Even more interesting is that <em>most of you </em>probably don’t know what <em>any</em> of these do.</p><p>So, let’s dig into them. What do these do and why should you almost certainly blindly trust me and go ahead and enable them?</p><p>I'm going to group these settings into three categories:</p><ul><li><a href="#clearly-makes-git-better" rel="noreferrer">Clearly Makes Git Better</a></li><li><a href="#why-the-hell-not" rel="noreferrer">Why the Hell Not?</a></li><li><a href="#a-matter-of-taste" rel="noreferrer">A Matter of Taste</a></li></ul><p>Let's get started.</p><h2 id="clearly-makes-git-better">Clearly Makes Git Better</h2><p>This first group of settings <em>clearly</em> makes Git better by default. There are generally zero downsides to enabling any of them.</p><h2 id="listing-branches">Listing branches</h2><p>I noted this in a previous blog post here about Git Tips under “<a href="https://blog.gitbutler.com/git-tips-2-new-stuff-in-git/#some-git-branch-stuff">Branch Stuff</a>” but as this was also in the Spring Cleaning list, I think everyone agrees that listing out Git branches should probably not be alpha-ordered by default.</p><p>The two settings which help improve this are <code>branch.sort</code> and <code>column.ui</code>. The first of which sorts the list by the most recent commit date (so probably more interesting at the top) rather than by alpha order. The second will put the branch names in a column format so you can see more per screen.</p><pre><code>git config --global column.ui auto
git config --global branch.sort -committerdate
</code></pre><p>The <code>column.ui</code> setting also affects the output of other listing commands (clean, status, tag), but generally I think it’s better than the default.</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-08.04.03@2x.png" alt="" loading="lazy" width="1598" height="984" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-21-at-08.04.03@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-21-at-08.04.03@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-08.04.03@2x.png 1598w" sizes="(min-width: 720px) 720px"></figure><p>You can also sort by other things than committer date, but I think it’s pretty clearly the most useful one.</p><p>Speaking of listing things, it’s also pretty ridiculous that this isn’t the default for listing tags, since it’s what nearly everyone actually wants.</p><p>Normally, if you list tags by alpha order, you’ll get something like this:</p><pre><code>$ git tag
nightly/0.5.100
nightly/0.5.1000
nightly/0.5.1001
nightly/0.5.101
nightly/0.5.1010
</code></pre><p>Nobody wants <code>0.5.101</code> to come after <code>0.5.1000</code>, but that’s alpha order. You can fix this by setting this:</p><pre><code>git config --global tag.sort version:refname
</code></pre><p>Which will generally do what you expect, treating dotted version numbers as a series of integer values for sorting purposes. Trust me, just enable this.</p><h2 id="default-branch">Default branch</h2><p>This one may be a little more controversial, since it can be argued to be somewhat political, but there should be a default branch name in Git where it doesn’t complain every time you <code>init</code> a new repo.</p><pre><code>git config --global init.defaultBranch main
</code></pre><p>Personally, I don’t have a problem with <code>master</code> and most of my repositories use that since that used to be the default, but I’m also fine with <code>main</code>, so whatever it is you want to use, just go ahead and set it.</p><p>Mostly what I find stupid is that now Git is annoying about this rather than just updating the default value. I wish Git had some taste here, but they don't, so you should just set it to something you find reasonable. But whatever.</p><h2 id="better-diff">Better diff</h2><p>There is actually a whole blog post that could be written about <code>git diff</code> algorithms, but the short story is that by default Git will use an old, fast, pretty reliable diff algorithm called "myers diff".</p><p>To give you a sense of what ‘old’ means, it was first published in a paper in 1986, so it’s almost 40 years old now. If you’re as old as I am, perhaps I can give you some childhood perspective as to what that means. The movies ‘The Three Amigos’, ‘An American Tail’ and the first ‘Highlander’ came out in theaters that year.</p><p>In any case, some advances have been made since then (with some tradeoffs too) and it may surprise you to know that Git actually ships with 4 built in diff algorithms it can use: <a><code>myers</code></a>, <code>minimal</code>, <a href="https://blog.jcoglan.com/2017/09/19/the-patience-diff-algorithm/?utm_source=chatgpt.com"><code>patience</code></a> and <code>histogram</code>.</p><p>Almost certainly what you want to be using is the <code>histogram</code> algorithm (an incremental improvement on ‘patience’), rather than the default of 'myers'. You can globally change it like this:</p><pre><code>git config --global diff.algorithm histogram
</code></pre><p>Here is an example of simple code movement diffed in <code>myers</code> vs <code>histogram</code>, to give a short taste of how it can be a bit smarter:</p><p>Let's say we move a css class below a similar one, change it a little, and then run <code>git diff</code> with the default <code>myers</code> algorithm. We may get something like this:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png" alt="" loading="lazy" width="1626" height="1156" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png 1000w, https://blog.gitbutler.com/content/images/size/w1600/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png 1600w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png 1626w" sizes="(min-width: 720px) 720px"></figure><p>Ok, a little confusing. Here is what <code>histogram</code> would give us in the same scenario:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png" alt="" loading="lazy" width="1628" height="1266" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png 1000w, https://blog.gitbutler.com/content/images/size/w1600/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png 1600w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png 1628w" sizes="(min-width: 720px) 720px"></figure><p>It's a bit more clear here what's actually happened.</p><p>As recently as last year, Elijah (of our <a href="https://www.youtube.com/watch?v=KXPmiKfNlZE&amp;ref=blog.gitbutler.com">Git Merge fame</a>) suggested that<br><a href="https://lore.kernel.org/git/CABPp-BEmgOAj17DozyXNaf-9CawDic4uTpMbckef3+zHf7URqQ@mail.gmail.com/?ref=blog.gitbutler.com">histogram or patience</a> might make better defaults, in addition to Felipe's Spring Cleaning suggestion of the same thing, but in reality it’s unlikely to get through the gauntlet anytime soon.</p><p>That’s a big one, but there are also a few more smaller tweaks you can make to <code>git diff</code>:</p><pre><code>git config --global diff.colorMoved plain
git config --global diff.mnemonicPrefix true
git config --global diff.renames true
</code></pre><p>The <code>colorMoved</code> was also in the Spring Cleaning suggestion list, so it also should probably be a default change.</p><p>Here is an example of the previous code movement with the <code>colorMoved</code> turned on:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.14.15@2x.png" alt="" loading="lazy" width="1254" height="778" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-16.14.15@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-16.14.15@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.14.15@2x.png 1254w" sizes="(min-width: 720px) 720px"></figure><p>You can see actually the difference between the moved code and the added line. With <code>colorMoved</code> it will show code movement in different colors then added and removed lines.</p><p>The <code>diff.renames</code> option will detect if a file has been renamed, which is generally good (if slightly more expensive) and <code>diff.mnemonicPrefix</code> will replace the <code>a/</code> and <code>b/</code> in your diff header output with where the diff is coming from, so <code>i/</code> (index), <code>w/</code> (working directory) or <code>c/</code> commit. </p><p>So if I diff a change in my index to my working directory I get this as my diff header instead:</p><pre><code>❯ git diff
diff --git i/apps/web/page.js w/apps/web/page.js
index 7568be2ef..b9e9a00d7 100644
--- i/apps/web/page.js
+++ w/apps/web/page.js
</code></pre><p>A little difficult to see in this example perhaps, but you can tell which side is from the index and which is from the working directory by the leading path names. It’s really subtle, but I like it.</p><h2 id="better-pushing">Better pushing</h2><p>One of the things that has continued to confuse and frustrate me since the very early days of Git is setting up tracking branches properly. When I push, where does it push, or does it push at all?</p><p>There are three updated push settings that I think make for a much nicer default experience. The first (<code>push.default simple</code>) has been the new default since Git 2.0, but the others still need to be set explicitly.</p><pre><code>git config --global push.default simple # (default since 2.0)
git config --global push.autoSetupRemote true
git config --global push.followTags true
</code></pre><p>This has always been a bit of a pain in Git. The new <code>simple</code> default is built more or less for centralized workflows and by default pushes the current branch to the same name on the remote. I think this is a pretty sensible default.</p><p>However, if that branch does not exist and there is no tracking branch setup, you’ll still get this error:</p><pre><code>$ git push
fatal: The current branch my-branch-name has no upstream branch.
To push the current branch and set the remote as upstream, use

    git push --set-upstream origin my-branch-name
</code></pre><p>I have to imagine that you have all seen this roughly <em>one million</em> times.</p><p>If you set <code>push.autoSetupRemote</code> to true, then you won’t get this error anymore. If the upstream is not set, it will automatically set it. I cannot tell you how much I love this setting.</p><p>Finally, the <code>push.followTags</code> setting will push all tags that you have locally that aren’t on the server, every time you push anything. I’ve been bitten by this a few times - if you ever create tags locally, set this up so you don’t have to worry about other people not seeing them.</p><h2 id="better-fetching">Better fetching</h2><p>It can be argued that it’s nice to keep some historical local copies of branches and tags that used to be on the server but are not any longer, but I don’t really buy that.</p><p>Personally, I think the default behavior of Git should be to make your remote references as close to what is on the remote as possible. Prune stuff that’s gone, etc. </p><p>So, I think these fetch settings should be the default:</p><pre><code>git config --global fetch.prune true
git config --global fetch.pruneTags true
git config --global fetch.all true
</code></pre><p>Really all that this does is make sure we delete <code>origin/blah</code> if <code>blah</code> is deleted on the server, and also do it automatically for all the remotes that we have configured. Seems pretty reasonable to me.</p><h2 id="why-the-hell-not">Why the Hell Not?</h2><p>This next batch of settings are generally harmless and occasionally helpful.</p><p>I’m not sure I would necessarily change the defaults, but I also don’t think it would hurt anyone and in many cases would be more helpful, so I’m including them in my list.</p><h2 id="autocorrect-prompting">Autocorrect prompting</h2><p>As I explained in great length in my <a href="https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/">previous post</a>, there is a rather nice feature in Git where if your fingers trip up while typing a command, it will guess what you meant and try to run it.</p><p>The default is to not do this at all. What I rather prefer is to guess and prompt you.</p><pre><code>git config --global help.autocorrect prompt
</code></pre><p>If you want to read about this setting, it’s reasoning and it’s history ad nauseam, I have <a href="https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/">just the post for you</a>.</p><h2 id="commit-with-diffs">Commit with diffs</h2><p>This was also one of the suggestions in the Spring Cleaning list, I think mostly because it just adds more information to the context you can reference when you write your commit message in your editor.</p><p>By default, a <code>git commit</code> will give you a message that looks something like this:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-17.03.26@2x.png" alt="" loading="lazy" width="1494" height="462" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-17.03.26@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-17.03.26@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-17.03.26@2x.png 1494w" sizes="(min-width: 720px) 720px"></figure><p>Where there is just a list of files that were changed. If you set <code>commit.verbose</code> to be true, it will put the whole <code>diff</code> output in there for you to reference as you write your message.</p><pre><code>git config --global commit.verbose true
</code></pre><p>Here’s what it looks like now when you go to commit:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-17.01.54@2x.png" alt="" loading="lazy" width="1488" height="1276" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-17.01.54@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-17.01.54@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-17.01.54@2x.png 1488w" sizes="(min-width: 720px) 720px"></figure><p>All of this will be removed from the commit message (everything under the hilarious <code>-- &gt;8 --</code> "scissors" line), but it can give you much more context in writing your message.</p><h2 id="reuse-recorded-resolutions">Reuse recorded resolutions</h2><p>This setting is only useful if you’re doing rebases with conflicts over and over again. It’s not the most common situation, but there is not really an issue if it’s turned on and never used.</p><pre><code>git config --global rerere.enabled true
git config --global rerere.autoupdate true</code></pre><p>The <code>enabled</code> option will make sure it records the before and after states of rebase conflicts and the <code>autoupdate</code> will automatically re-apply the resolutions if it sees them again. I wrote about this at some length <a href="https://blog.gitbutler.com/git-tips-1-theres-a-git-config-for-that/#reuse-recorded-resolution">over here</a>, so I won’t bore you with the recap any further.</p><h2 id="global-ignore-file">Global ignore file</h2><p>This is pretty dumb, but as there is a <code>~/.gitconfig</code> file with global values, it would be cool if there were a <code>~/.gitignore</code> file with global values. This setting accomplishes that:</p><pre><code>git config --global core.excludesfile ~/.gitignore
</code></pre><p>In reality, this is sort of unnecessary, since Git will already look for global ignore values in the following two places: <code>~/git/ignore</code> and <code>~/.config/git/ignore</code> but since those are a little obscure, I feel like it’s nice to have this more guessable path.</p><h2 id="slightly-nicer-rebase">Slightly nicer rebase</h2><p>This section mostly has to do with the use case where you're fixing up and squashing your commits. If you don't know what that is, please check out our previous blog post on <a href="https://blog.gitbutler.com/git-autosquash/" rel="noreferrer">autosquashing</a>.</p><p>However, if you are squashing and rebasing a lot (or even occasionally), these settings could help and certainly won't hurt things.</p><pre><code>git config --global rebase.autoSquash true
git config --global rebase.autoStash true
git config --global rebase.updateRefs true</code></pre><p>The <code>updateRefs</code> setting should almost certainly be a default, honestly. It just takes stacked refs in a branch and makes sure they're also moved when a branch is rebased.</p><p>If you want to learn a tiny bit more about how to use fixup, autosquash and updateRefs, it's probably easiest to watch a few minutes of a talk where I go over it here:</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/Md44rcw13k4?start=810&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="So You Think You Know Git Part 2 - DevWorld 2024"></iframe></figure><h2 id="a-matter-of-taste">A Matter of Taste</h2><p>The next group is based on your personal taste, but most people don’t know they exist and a lot of people may find them useful. They are commented out in my TLDR settings.</p><h2 id="better-merge-conflicts">Better merge conflicts</h2><p>So, while this is brought up in the Spring Cleaning thread as something that might want to be the new default, I'm not sure that all of you would agree.</p><p>When you have a merge conflict in Git, instead of inserting the conflict markers from left and right, you can ask it to insert what the base of it looked like too. Sometimes this can be really useful, but some people can find it pretty annoying.</p><pre><code>git config --global merge.conflictstyle zdiff3</code></pre><p>There have been discussions on the Git mailing list to make this the default and actually GitButler uses the <code>diff3</code> strategy when dealing with merge conflict markers and to be totally honest, not all of us love it.</p><p>Here is an example of a simple merge conflict marker you might get in a file when doing a merge or rebase:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-11.47.34@2x.png" alt="" loading="lazy" width="1326" height="480" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-21-at-11.47.34@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-21-at-11.47.34@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-11.47.34@2x.png 1326w" sizes="(min-width: 720px) 720px"></figure><p>With the <code>merge.conflictStyle zdiff3</code> setting, it would look like this:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-11.48.36@2x.png" alt="" loading="lazy" width="1320" height="558" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-21-at-11.48.36@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-21-at-11.48.36@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-11.48.36@2x.png 1320w" sizes="(min-width: 720px) 720px"></figure><p>Essentially, in addition to the <code>&lt;&lt;&lt;&lt;&lt;&lt;</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;</code> sections that show you how you changed the block and how the other person changed it, it adds a <code>|||||||</code> block that shows you what the block looked like before either of you changed it.</p><p>That extra context (what that section looked like before either side modified it) can sometimes be super useful, but often it's just more data and somewhat confusing.</p><p>Really, it's up to you if you prefer more data there.</p><div><p>⚠️</p><p>Git has nearly always had <code spellcheck="false">diff3</code> as a strategy. I'm recommending <code spellcheck="false">zdiff3</code> here, which stands for "<i><em>zealous diff3</em></i>" and is slightly better, but only available since Git 2.35 (Jan 2022). If you have an older Git version, just remove the "z".</p></div><h2 id="better-pulling">Better pulling</h2><p>The merge versus rebase debate is of course one that may never be agreed upon, but most of us have a preference. However, you may not know that you can set the <code>git pull</code> default so that it will only do one or the other. No need for <code>git pull --rebase</code>, you can make it the default:</p><pre><code>git config --global pull.rebase true
</code></pre><p>This is a personal decision, but as I’ve migrated to the rebase only camp recently, it is in fact in my config.</p><h2 id="run-the-fsmonitor-processes">Run the fsmonitor processes</h2><p>Again, this is really only a thing for larger repositories, and maybe you don’t want filesystem monitors running all over the place, but it can make things like <code>git status</code> much faster if you have big working directories.</p><p>Maybe it shouldn’t be a default, but it’s not very bad and can make a big difference. Maybe <code>git clone</code> should ask you if you want to set it or not. Whatever, it’s an option for you.</p><pre><code>git config --global core.fsmonitor true
git config --global core.untrackedCache true</code></pre><p>This will run a filesystem monitor (per repository) that notices file changes and updates a cache so that <code>git status</code> doesn’t have to crawl every file and see if anything changed via a thousand <code>mtime</code> stat calls, it can just look at a simple log of file changes.</p><div><p>⚠️</p><p>Be aware that this will run a single process <i><em>per repository</em></i> that you are active in, which can be a lot. They mostly don't do much as they're event based, so it shouldn't affect memory or CPU noticeably, even with hundreds of them, but it's something to keep in mind. You can also leave out the <code spellcheck="false">--global</code> and just enable it for your larger repos.</p></div><h2 id="final-thoughts">Final thoughts</h2><p>Hopefully this has been a useful reference and maybe you learned some new Git config things, some of which should almost certainly already be the defaults, which isn’t even a controversial option in the Git mailing list community.</p><p>There are lots of other ways to pimp your Git ride (aliases, cool external <a href="https://github.com/dandavison/delta?ref=blog.gitbutler.com">pager</a> and <a href="https://github.com/so-fancy/diff-so-fancy?ref=blog.gitbutler.com">diff</a> tools, things like that) but I thought it would be best to just stick to globally useful and relatively simple vanilla Git settings.</p><p>Hope you enjoyed this and see you next time!</p>
			</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What would happen if we didn't use TCP or UDP? (117 pts)]]></title>
            <link>https://github.com/Hawzen/hdp</link>
            <guid>43169103</guid>
            <pubDate>Tue, 25 Feb 2025 07:13:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Hawzen/hdp">https://github.com/Hawzen/hdp</a>, See on <a href="https://news.ycombinator.com/item?id=43169103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">What would happen if we didn't use TCP or UDP?</h2><a id="user-content-what-would-happen-if-we-didnt-use-tcp-or-udp" aria-label="Permalink: What would happen if we didn't use TCP or UDP?" href="#what-would-happen-if-we-didnt-use-tcp-or-udp"></a></p>
<p dir="auto">Switches, bridges, routers, load balancers, firewalls—these network boxes keep the internet running. Routing, blocking, mirroring, duplicating and deduplicating traffic in ways most people never think about. Without them, this document wouldn’t have reached you</p>
<p dir="auto">But the network is just one layer. The OS has its own way of handling packets—classifying, queuing, enforcing firewall rules, translating addresses, deciding what gets through and what gets dropped without a trace. Every part plays by its own rules, shaping what’s “allowed” and what's not</p>
<p dir="auto">At some point, I wondered—<em>what if I sent a packet using a transport protocol that didn’t exist?</em> Not TCP, not UDP, not even ICMP—something completely made up. Would the OS let it through? Would it get stopped before it even left my machine? Would routers ignore it, or would some middlebox kill it on sight? Could it actually move faster by slipping past common firewall rules?</p>
<p dir="auto">No idea.</p>
<p dir="auto">So I had to try.</p>
<p dir="auto">First, I sent the packets to myself, just to see how my own machine handled the poison I made up. Then, I sent them across continents to a remote Linux machine to see if they’d actually make it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Some background first</h2><a id="user-content-some-background-first" aria-label="Permalink: Some background first" href="#some-background-first"></a></p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Feel free to skip this section if you already know how the internet works. Otherwise, continue reading on</p>
</div>
<p dir="auto">But wait—what exactly is a transport layer protocol?</p>
<p dir="auto">The internet isn’t magic. It just looks that way. Underneath, it’s a stack of protocols, each one shoving data to the next until it reaches its destination. At the application level, you send a request—loading a website, streaming a video, or whatever you do. That request gets wrapped by the OS in multiple layers of metadata, addresses, and headers, until it’s nothing but raw bits flying through the network</p>
<p dir="auto">It kinda works like this:</p>
<p dir="auto">  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Hawzen/hdp/blob/master/readme_assets/internet_protocols.png"><img src="https://github.com/Hawzen/hdp/raw/master/readme_assets/internet_protocols.png" alt="a visual guide to how the internet works, it kinda sucks but that why i like it."></a> </p>
<p dir="auto"><sub>The diagram is 100% correct and should be included in all networking textbooks.</sub></p>
<p dir="auto">At the top, apps—browsers, games, whatever—generate requests (Load this page, Send this message, Connect to this game server). Then the requests start their descent through the network stack, getting wrapped, encoded, and addressed at each layer, until all that’s left is a stream of bits flying into the void</p>
<p dir="auto">Each layer plays a role. IP assigns addresses and makes sure packets know where they’re going. The link layer handles the actual transmission—Wi-Fi, Ethernet, fiber optics, whatever. There’s more to it, but we’re not going down that rabbit hole right now. What matters is the layer that makes network communication actually usable</p>
<p dir="auto">The <strong>transport layer</strong> is where networking personally starts to get interesting. It’s the first truly complex protocol layer. It doesn’t just move packets—it manages connections, makes sure multiple applications can share the same machine, and decides how data should flow.</p>
<p dir="auto">This is where <strong>TCP</strong>, <strong>UDP</strong>, and their weird cousins live. The <strong>IP Protocol</strong> defines a field called <code>Protocol</code>. Setting this field to 6 means the encapsulated packet is TCP, 17 is UDP, and <a href="https://en.wikipedia.org/wiki/List_of_IP_protocol_numbers" rel="nofollow">there are others defined</a> but some numbers are deliberately left out for future use</p>
<p dir="auto">But what if we used those <em>unused</em> numbers?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Experiment #1: Sending traffic.. to me!</h2><a id="user-content-experiment-1-sending-traffic-to-me" aria-label="Permalink: Experiment #1: Sending traffic.. to me!" href="#experiment-1-sending-traffic-to-me"></a></p>
<p dir="auto">There are simply too many variables to this experiment. My OS, my router, the receiver's OS, and god knows how many middle boxes are littered on the open internet. It's hard to extrapolate conclusions from experimentation with all these moving parts—so I thought of the following: To begin, I'll send the packets to <em>my own machine</em>, this guarantees that any results are solely due to my OS's behaviour</p>
<p dir="auto">First, I designed a <a href="https://github.com/Hawzen/hdp/blob/master/hdp_specification.md">simple protocol</a>: <strong>HDP</strong>. The specifics don’t matter—what matters is that it doesn’t resemble any known protocol. It’s an outsider, something the OS and network stack weren’t expecting</p>
<p dir="auto">Next, I built a <a href="https://github.com/Hawzen/hdp/blob/master/src/server/main.rs">server, or a listener</a>, whatever you call it. The machine running this code will be patiently waiting for any packets. Then I wrote a <a href="https://github.com/Hawzen/hdp/blob/master/src/client/main.rs">client</a>, the machine running this code will send HDP packets to the server</p>
<p dir="auto">Finally, here are the steps I'll attempt</p>
<ol dir="auto">
<li>Startup an HDP server
<ul dir="auto">
<li>Which will ask the OS to forward any packets with the protocol 255 to a socket it controls</li>
</ul>
</li>
<li>Run the HDP client, sending packets to my local machine
<ul dir="auto">
<li>The client will ask the OS to nicely deliver the packets to 127.0.01
<ul dir="auto">
<li>The OS is configured to hand packets with that target address to the loopback <a href="https://en.wikipedia.org/wiki/Network_interface_controller" rel="nofollow">network interface</a>
<ul dir="auto">
<li>The loopback interface should realize: "uhhh.. this packet should go right back in?", and send it back to my own machine</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>The OS delivers them to the HDP server unmodified..?? 🤞</li>
</ol>
<p dir="auto">Let's do it</p>
<p dir="auto">I opened two shells—one was the server:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ sudo cargo run --bin server"><pre><span><span>$</span></span> sudo cargo run <span><span>--</span>bin server</span></pre></div>
<p dir="auto">And in another shell I opened the client</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ fortune | cowsay | sudo cargo run --bin client 127.0.0.1"><pre><span><span>$</span></span> fortune <span>|</span> cowsay <span>|</span> sudo cargo run <span><span>--</span>bin client 127.0.0.1</span></pre></div>
<p dir="auto">Alright, let's send the packet via the client. 3, 2, 1, and..</p>
<p dir="auto">The server got the message!</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ sudo cargo run --bin server
~~~ IP Header ~~~
Version: 4
IHL: 5
DSCP: 0
ECN: 0
Total Length: 58625
Identification: 36455
Flags: 0
Fragment Offset: 0
TTL: 64
Protocol: 255
Header Checksum: 0
Source IP: [127, 0, 0, 1]
Destination IP: [127, 0, 0, 1]


~~~ HDP Header &amp; Data ~~~
Source Port: 420
Destination Port: 420
Timestamp: 1739640243546134000
Data:  _________________________________________
/ Marriage is not merely sharing the      \
| fettucine, but sharing the burden of    |
| finding the fettucine restaurant in the |
| first place.                            |
|                                         |
\ -- Calvin Trillin                       /
 -----------------------------------------
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||"><pre><span><span>$</span></span> sudo cargo run <span><span>--</span>bin server</span>
<span>~~~</span> <span>IP</span> <span>Header</span> <span>~~~</span>
<span>Version</span><span>:</span> <span>4</span>
<span>IHL</span><span>:</span> <span>5</span>
<span>DSCP</span><span>:</span> <span>0</span>
<span>ECN</span><span>:</span> <span>0</span>
<span>Total</span> <span>Length</span><span>:</span> <span>58625</span>
<span>Identification</span><span>:</span> <span>36455</span>
<span>Flags</span><span>:</span> <span>0</span>
<span>Fragment</span> <span>Offset</span><span>:</span> <span>0</span>
<span>TTL</span><span>:</span> <span>64</span>
<span>Protocol</span><span>:</span> <span>255</span>
<span>Header</span> <span>Checksum</span><span>:</span> <span>0</span>
<span>Source</span> <span>IP</span><span>:</span> [<span>127</span>, <span>0</span>, <span>0</span>, <span>1</span>]
<span>Destination</span> <span>IP</span><span>:</span> [<span>127</span>, <span>0</span>, <span>0</span>, <span>1</span>]


<span>~~~</span> <span>HDP</span> <span>Header</span> <span>&amp;</span> <span>Data</span> <span>~~~</span>
<span>Source</span> <span>Port</span><span>:</span> <span>420</span>
<span>Destination</span> <span>Port</span><span>:</span> <span>420</span>
<span>Timestamp</span><span>:</span> <span>1739640243546134000</span>
<span>Data</span><span>:</span>  _________________________________________
<span><span>/</span></span> <span>Marriage</span> is <span>not</span> merely sharing the      \
<span>|</span> fettucine, but sharing the burden <span>of</span>    <span>|</span>
<span>|</span> finding the fettucine restaurant <span>in</span> the <span>|</span>
<span>|</span> first place<span>.</span>                            <span>|</span>
<span>|</span>                                         <span>|</span>
<span>\</span> <span><span>--</span> Calvin Trillin                       /</span>
 <span><span>-----------------------------------------</span></span>
        <span>\</span>   <span>^</span>__<span>^</span>
         <span>\</span>  (oo)<span>\</span>_______
            (__)<span>\</span>       )<span>\/\</span>
                <span>||----</span>w <span>|</span>
                <span>||</span>     <span>||</span></pre></div>
<p dir="auto">Success! The OS accepted my protocol, looped it back, and delivered it to the server with no shenanigans happening, unexpected!. But before calling it a day, I had another question:</p>
<p dir="auto">What would happen if we repeated this experiment, whilst changing the protocol number defined in the IP packet?</p>
<p dir="auto">My initial choice of <strong>255</strong> was arbitrary—it was an unused protocol number. But what if I tried something more… unconventional? I decided to test different protocol numbers, including:</p>
<ul dir="auto">
<li>6, the number assigned to <strong>TCP</strong> packets</li>
<li>Or 2, which is the protocol number used for <strong>ICMP</strong> (i.e., the thing powering <code>ping</code>)</li>
<li>Or even 256, an index beyond the defined boundaries of the IP Protocol
Would they make it? Would the OS freak out?</li>
</ul>
<p dir="auto">Let's see:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fortune | cowsay | sudo cargo run --bin client 127.0.0.1 # This time looping over protocol numbers"><pre>fortune <span>|</span> cowsay <span>|</span> sudo cargo run <span><span>--</span>bin client 127.0.0.1 # This time looping over protocol numbers</span></pre></div>
<details>
<summary><p dir="auto"><h2 tabindex="-1" dir="auto">Results</h2><a id="user-content-results" aria-label="Permalink: Results" href="#results"></a></p></summary>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Protocol Number</th>
<th>Source IP (Server)</th>
<th>Byte Sum (Server)</th>
<th>Received (Server)</th>
<th>Succeeded (Client)</th>
<th>Byte sum (Client)</th>
<th>Failure reason (Client)</th>
<th>Time difference (μs)</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>70</td>
</tr>
<tr>
<td>1</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>2</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>3</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>61</td>
</tr>
<tr>
<td>4</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>5</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>6</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>7</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>8</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>63</td>
</tr>
<tr>
<td>9</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>66</td>
</tr>
<tr>
<td>10</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>11</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>12</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>63</td>
</tr>
<tr>
<td>13</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>63</td>
</tr>
<tr>
<td>14</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>15</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>16</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>64</td>
</tr>
<tr>
<td>17</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>18</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>42</td>
</tr>
<tr>
<td>19</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>82</td>
</tr>
<tr>
<td>20</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>21</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>22</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>23</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>51</td>
</tr>
<tr>
<td>24</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>25</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>26</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>48</td>
</tr>
<tr>
<td>27</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>43</td>
</tr>
<tr>
<td>28</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>29</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>66</td>
</tr>
<tr>
<td>30</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>56</td>
</tr>
<tr>
<td>31</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>65</td>
</tr>
<tr>
<td>32</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>56</td>
</tr>
<tr>
<td>33</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>49</td>
</tr>
<tr>
<td>34</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>47</td>
</tr>
<tr>
<td>35</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>48</td>
</tr>
<tr>
<td>36</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>37</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>47</td>
</tr>
<tr>
<td>38</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>45</td>
</tr>
<tr>
<td>39</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>40</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>57</td>
</tr>
<tr>
<td>41</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>56</td>
</tr>
<tr>
<td>42</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>51</td>
</tr>
<tr>
<td>43</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>45</td>
</tr>
<tr>
<td>44</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>58</td>
</tr>
<tr>
<td>45</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>46</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>47</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>48</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>51</td>
</tr>
<tr>
<td>49</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>50</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🤯</td>
<td>-</td>
<td>Operation not supported on socket (os error 102)</td>
<td>nan</td>
</tr>
<tr>
<td>51</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🤯</td>
<td>-</td>
<td>Operation not supported on socket (os error 102)</td>
<td>nan</td>
</tr>
<tr>
<td>52</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>53</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>115</td>
</tr>
<tr>
<td>54</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>81</td>
</tr>
<tr>
<td>55</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>83</td>
</tr>
<tr>
<td>56</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>57</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>58</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>69</td>
</tr>
<tr>
<td>59</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>60</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>61</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>105</td>
</tr>
<tr>
<td>62</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>109</td>
</tr>
<tr>
<td>63</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>64</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>65</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>66</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>124</td>
</tr>
<tr>
<td>67</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>101</td>
</tr>
<tr>
<td>68</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>69</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>87</td>
</tr>
<tr>
<td>70</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>71</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>101</td>
</tr>
<tr>
<td>72</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>73</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>111</td>
</tr>
<tr>
<td>74</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>104</td>
</tr>
<tr>
<td>75</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>115</td>
</tr>
<tr>
<td>76</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>77</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>78</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>65</td>
</tr>
<tr>
<td>79</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>80</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>150</td>
</tr>
<tr>
<td>81</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>82</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>83</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>84</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>93</td>
</tr>
<tr>
<td>85</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>86</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>87</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>70</td>
</tr>
<tr>
<td>88</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>49</td>
</tr>
<tr>
<td>89</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>90</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>91</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>92</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>61</td>
</tr>
<tr>
<td>93</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>94</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>55</td>
</tr>
<tr>
<td>95</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>96</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>97</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>98</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>66</td>
</tr>
<tr>
<td>99</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>100</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>53</td>
</tr>
<tr>
<td>101</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>102</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>148</td>
</tr>
<tr>
<td>103</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>111</td>
</tr>
<tr>
<td>104</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>119</td>
</tr>
<tr>
<td>105</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>106</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>107</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>53</td>
</tr>
<tr>
<td>108</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>109</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>44</td>
</tr>
<tr>
<td>110</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>111</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>51</td>
</tr>
<tr>
<td>112</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>45</td>
</tr>
<tr>
<td>113</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>114</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>115</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>85</td>
</tr>
<tr>
<td>116</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>117</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>64</td>
</tr>
<tr>
<td>118</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>24</td>
</tr>
<tr>
<td>119</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>120</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>62</td>
</tr>
<tr>
<td>121</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>48</td>
</tr>
<tr>
<td>122</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>123</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>124</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>49</td>
</tr>
<tr>
<td>125</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>126</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>127</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>128</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>103</td>
</tr>
<tr>
<td>129</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>73</td>
</tr>
<tr>
<td>130</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>57</td>
</tr>
<tr>
<td>131</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>49</td>
</tr>
<tr>
<td>132</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>62</td>
</tr>
<tr>
<td>133</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>43</td>
</tr>
<tr>
<td>134</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>47</td>
</tr>
<tr>
<td>135</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>136</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>112</td>
</tr>
<tr>
<td>137</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>138</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>53</td>
</tr>
<tr>
<td>139</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>57</td>
</tr>
<tr>
<td>140</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>141</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>64</td>
</tr>
<tr>
<td>142</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>143</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>144</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>145</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>146</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>88</td>
</tr>
<tr>
<td>147</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>148</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>106</td>
</tr>
<tr>
<td>149</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>72</td>
</tr>
<tr>
<td>150</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>151</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>152</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>153</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>154</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>155</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>156</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>157</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>110</td>
</tr>
<tr>
<td>158</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>105</td>
</tr>
<tr>
<td>159</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>83</td>
</tr>
<tr>
<td>160</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>89</td>
</tr>
<tr>
<td>161</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>162</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>111</td>
</tr>
<tr>
<td>163</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>103</td>
</tr>
<tr>
<td>164</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>165</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>166</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>167</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>168</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>57</td>
</tr>
<tr>
<td>169</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>170</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>65</td>
</tr>
<tr>
<td>171</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>172</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>173</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>174</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>67</td>
</tr>
<tr>
<td>175</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>55</td>
</tr>
<tr>
<td>176</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>60</td>
</tr>
<tr>
<td>177</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>85</td>
</tr>
<tr>
<td>178</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>179</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>73</td>
</tr>
<tr>
<td>180</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>79</td>
</tr>
<tr>
<td>181</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>182</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>183</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>88</td>
</tr>
<tr>
<td>184</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>185</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>186</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>187</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>188</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>79</td>
</tr>
<tr>
<td>189</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>190</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>81</td>
</tr>
<tr>
<td>191</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>192</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>193</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>194</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>88</td>
</tr>
<tr>
<td>195</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>196</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>99</td>
</tr>
<tr>
<td>197</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>198</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>199</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>200</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>201</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>89</td>
</tr>
<tr>
<td>202</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>203</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>204</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>109</td>
</tr>
<tr>
<td>205</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>104</td>
</tr>
<tr>
<td>206</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>108</td>
</tr>
<tr>
<td>207</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>208</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>209</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>210</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>76</td>
</tr>
<tr>
<td>211</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>212</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>213</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>214</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>215</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>216</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>93</td>
</tr>
<tr>
<td>217</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>105</td>
</tr>
<tr>
<td>218</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>219</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>220</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>98</td>
</tr>
<tr>
<td>221</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>222</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>108</td>
</tr>
<tr>
<td>223</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>224</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>104</td>
</tr>
<tr>
<td>225</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>109</td>
</tr>
<tr>
<td>226</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>227</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>99</td>
</tr>
<tr>
<td>228</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>229</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>79</td>
</tr>
<tr>
<td>230</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>231</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>79</td>
</tr>
<tr>
<td>232</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>102</td>
</tr>
<tr>
<td>233</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>101</td>
</tr>
<tr>
<td>234</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>113</td>
</tr>
<tr>
<td>235</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>236</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>237</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>238</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>106</td>
</tr>
<tr>
<td>239</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>240</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>241</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>89</td>
</tr>
<tr>
<td>242</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>99</td>
</tr>
<tr>
<td>243</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>244</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>98</td>
</tr>
<tr>
<td>245</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>93</td>
</tr>
<tr>
<td>246</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>247</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>248</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>249</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>250</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>251</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>88</td>
</tr>
<tr>
<td>252</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>253</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>254</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>76</td>
</tr>
<tr>
<td>255</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>255</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>256</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🤯</td>
<td>-</td>
<td>Invalid argument (os error 22)</td>
<td>nan</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">What’s up with these failures?</h2><a id="user-content-whats-up-with-these-failures" aria-label="Permalink: What’s up with these failures?" href="#whats-up-with-these-failures"></a></p>
<p dir="auto">Most protocol numbers worked fine—the OS saw the packet, looped it back, and my server received it without an issue. But a few of them outright&nbsp;<em>failed</em>&nbsp;at different points in the stack</p>
<ul dir="auto">
<li><strong>Protocols 1, 2, and 6 failed at the server side</strong>.&nbsp;Meaning: the client successfully sent them, but the server never saw them</li>
<li><strong>Protocols 50 and 51 failed at the client side</strong>.&nbsp;The OS refused to even send them</li>
<li><strong>Protocol 256 didn't even make it past the&nbsp;<code>socket()</code>&nbsp;call</strong></li>
</ul>
<p dir="auto">But&nbsp;<em>why?</em>&nbsp;What’s making the OS treat these packets differently?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Syscalls: What actually matters</h2><a id="user-content-syscalls-what-actually-matters" aria-label="Permalink: Syscalls: What actually matters" href="#syscalls-what-actually-matters"></a></p>
<p dir="auto">One of the most useful debugging techniques I learnt debugging this stuff is, when dealing with low-level code, trace the <em>system calls</em> a process is making</p>
<p dir="auto">A <a href="https://en.wikipedia.org/wiki/System_call" rel="nofollow">system call</a> for the uninitiated is just a function that allows applications to request privileged resources from the OS—whether that’s opening a file, allocating memory, or, in our case,&nbsp;sending a packet over the network</p>
<p dir="auto">In my Rust code I use a library called <a href="https://docs.rs/socket2/latest/socket2/index.html" rel="nofollow"><code>socket2</code></a> which implements a pretty wrapper over the system calls provided by my OS. And to send a packet, I request a socket—which you can think of as just a special file my code can write in to communicate over the network</p>
<p dir="auto">Here's what the client would do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int sockfd = socket(
    AF_INET,    // Domain: ARPA Internet protocols. This tells the OS that we're interested in the IP protocols
    SOCK_RAW,   // Type: Raw socket. The OS normally handles the transport layer, but this gives us full control.
    255         // Protocol: We looped over this field.
);"><pre><span>int</span> <span>sockfd</span> <span>=</span> <span>socket</span>(
    <span>AF_INET</span>,    <span>// Domain: ARPA Internet protocols. This tells the OS that we're interested in the IP protocols</span>
    <span>SOCK_RAW</span>,   <span>// Type: Raw socket. The OS normally handles the transport layer, but this gives us full control.</span>
    <span>255</span>         <span>// Protocol: We looped over this field.</span>
);</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Revisiting the failures</h2><a id="user-content-revisiting-the-failures" aria-label="Permalink: Revisiting the failures" href="#revisiting-the-failures"></a></p>
<p dir="auto"><strong>1, 2, and 6: The Server Never Sees Them</strong><br>
These packets were successfully transmitted from the client, but they were intercepted before my server had a chance to look at them. That suggests something inside the OS intercepted them</p>
<p dir="auto">Originally, I assumed my server would capture any raw IP packet it received. The socket looked like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int sockfd = socket(
    AF_INET,    // Internet domain
    SOCK_RAW,   // Raw socket: should give us full control
    0           // Let the OS decide the protocol
);"><pre><span>int</span> <span>sockfd</span> <span>=</span> <span>socket</span>(
    <span>AF_INET</span>,    <span>// Internet domain</span>
    <span>SOCK_RAW</span>,   <span>// Raw socket: should give us full control</span>
    <span>0</span>           <span>// Let the OS decide the protocol</span>
);</pre></div>
<p dir="auto">I expected 0 to mean:
<em>"Give me everything—TCP, UDP, whatever it is, forward it"</em></p>
<p dir="auto">For context, I ran these experiments on my Mac, which runs Darwin. Looking at the <a href="https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man2/socket.2.html" rel="nofollow">documentation</a>, there is really nothing mentioning the Protocol Number = 0 trick</p>
<p dir="auto">Under the hood, Darwin is just like BSD but with a ton of makeup, meaning it inherits BSD’s socket behaviour and network stack quirks. And on a whim I checked the <strong><a href="https://man.openbsd.org/socket.2" rel="nofollow">BSD socket documentation</a></strong>, and I found this frustratingly vague line:</p>
<blockquote>
<p dir="auto">"A value of 0 for <code>protocol</code> will let the system select an appropriate protocol for the requested socket type."</p>
</blockquote>
<p dir="auto">So instead of delivering <strong>all</strong> raw packets, my OS was silently (and haphazardly) filtering them. My server never even saw the ICMP (1), IGMP (2), or TCP (6) packets—because Darwin likely deemed my socket not appropriate to receive those protocols.. or something?</p>
<p dir="auto"><strong>50 and 51: The Client Can’t Even Send Them</strong><br>
Here, the OS flat-out refused to send the packets. These aren’t just arbitrary numbers—they’re part of <strong>IPSec (ESP and AH)</strong>, which is used for encrypted VPN traffic. I'm not sure <em>why</em> the OS blocked them, but I imagine it's a security feature of sorts in Darwin</p>
<p dir="auto"><strong>256: The <code>socket()</code> Call Fails Immediately</strong><br>
This one is simple:</p>
<ul dir="auto">
<li>The IPv4 protocol field is 8 bits meaning valid values range from 0 to 255</li>
<li>256 is simply too large—the OS rejects it outright as an invalid argument</li>
</ul>
<p dir="auto">No surprises here. But what <em>was</em> surprising is what happened when I tried the same experiment on Linux..</p>
<p dir="auto">After seeing these inconsistencies, I was curious as to how Linux would behave. So I spun up a Linux VM and re-ran the experiment. Right away, the behaviour was very different</p>
<p dir="auto">Running the server I quickly noticed that Linux does not allow binding a raw socket to protocol <code>0</code>—Some invalid protocol numbers like 256 <em>worked</em>. For reference, I logged the results in <a href="https://github.com/Hawzen/hdp/blob/master/samples/results_no_server_linux_client_loopback.md"><code>results_no_server_linux_client_loopback</code></a>. I was satisfied that at least <em>some</em> of the protocol numbers were working as expected</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Lessons learned</h2><a id="user-content-lessons-learned" aria-label="Permalink: Lessons learned" href="#lessons-learned"></a></p>
<p dir="auto">Custom transport-layer protocols are doable, buuuuut the OS isn’t exactly welcoming. The networking stack has so many assumptions baked in, and raw sockets aren’t as raw as you’d expect</p>
<p dir="auto">I imagine this is why most new protocols live at the application layer instead. Instead of fighting the OS, engineers just build on top of existing transport protocols. QUIC, for example, runs over UDP and avoids these issues entirely</p>
<p dir="auto">And if you're ever working with raw sockets, <em>please</em> test across multiple OSes. If Darwin lets you do something, Linux might shut it down. If Linux is fine with it, Windows might pretend it doesn’t exist. There’s really no universal behaviour, even if they claim to <em>implement the POSIX standard</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Next step: What happens outside loopback?</h2><a id="user-content-next-step-what-happens-outside-loopback" aria-label="Permalink: Next step: What happens outside loopback?" href="#next-step-what-happens-outside-loopback"></a></p>
<p dir="auto">So far, these packets never left my machine. Now, I want to send HDP over the public internet:</p>
<ul dir="auto">
<li>Will routers forward it, or will they drop it?</li>
<li>Will firewalls let it through, or flag it as an attack?</li>
<li>Will it have different latency compared to TCP?</li>
<li>Will I accidentally brick DigitalOcean’s network? :D
Time to find out</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Experiment #2:</h2><a id="user-content-experiment-2" aria-label="Permalink: Experiment #2:" href="#experiment-2"></a></p>
<p dir="auto">At first I expected this experiment to be straight-forward (spoilers: it was NOT). How could it not..?</p>
<p dir="auto">I planned to deploy my server on a machine using a cheap cloud provider like Digital Ocean—then I'd send all sorts of packets to it, TCP, UDP, my own protocol, you name it. Gathering statistics about packet drop, latency, whatever, then I'd make conclusions about the feasibility of not using TCP/UDP</p>
<p dir="auto">Simple!</p>
<p dir="auto">But oh it was not, not at all. It wasn't that the experiment was difficult to setup—but what weirded me out was the results.. they weren't anything I expected or was prepared to deal with. Keep reading to see why</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setting up the server</h2><a id="user-content-setting-up-the-server" aria-label="Permalink: Setting up the server" href="#setting-up-the-server"></a></p>
<p dir="auto">I rented the the cheapest VPS on Digital Ocean I could find, then set up my server and all the tooling I needed. Nice!</p>
<p dir="auto">Let's see where the server is..</p>
<div dir="auto" data-snippet-clipboard-copy-content="root@debian-s-1vcpu-512mb-10gb-fra1-01:~# curl myip.wtf
161.35.222.56
root@debian-s-1vcpu-512mb-10gb-fra1-01:~# curl ipinfo.io/161.35.222.56
{
  &quot;ip&quot;: &quot;161.35.222.56&quot;,
  &quot;city&quot;: &quot;Frankfurt am Main&quot;,
  &quot;region&quot;: &quot;Hesse&quot;,
  &quot;country&quot;: &quot;DE&quot;,
  &quot;loc&quot;: &quot;50.1155,8.6842&quot;,
  &quot;org&quot;: &quot;AS14061 DigitalOcean, LLC&quot;,
  &quot;postal&quot;: &quot;60306&quot;,
  &quot;timezone&quot;: &quot;Europe/Berlin&quot;,
  &quot;readme&quot;: &quot;https://ipinfo.io/missingauth&quot;
}"><pre>root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~#</span> curl myip<span>.</span>wtf
<span>161.35</span><span>.</span><span>222.56</span>
root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~#</span> curl ipinfo<span>.</span>io<span>/</span><span>161.35</span><span>.</span><span>222.56</span>
{
  <span><span>"</span>ip<span>"</span></span><span>:</span> <span><span>"</span>161.35.222.56<span>"</span></span>,
  <span><span>"</span>city<span>"</span></span><span>:</span> <span><span>"</span>Frankfurt am Main<span>"</span></span>,
  <span><span>"</span>region<span>"</span></span><span>:</span> <span><span>"</span>Hesse<span>"</span></span>,
  <span><span>"</span>country<span>"</span></span><span>:</span> <span><span>"</span>DE<span>"</span></span>,
  <span><span>"</span>loc<span>"</span></span><span>:</span> <span><span>"</span>50.1155,8.6842<span>"</span></span>,
  <span><span>"</span>org<span>"</span></span><span>:</span> <span><span>"</span>AS14061 DigitalOcean, LLC<span>"</span></span>,
  <span><span>"</span>postal<span>"</span></span><span>:</span> <span><span>"</span>60306<span>"</span></span>,
  <span><span>"</span>timezone<span>"</span></span><span>:</span> <span><span>"</span>Europe/Berlin<span>"</span></span>,
  <span><span>"</span>readme<span>"</span></span><span>:</span> <span><span>"</span>https://ipinfo.io/missingauth<span>"</span></span>
}</pre></div>
<p dir="auto">Alright, looks like the experiment will span continents given that I'm running my client on Saudi Arabia, and the server is hosted in Frankfurt</p>
<p dir="auto">Before running any deep analysis, I wanted to check that there is a network path between my Mac and the server, so I <code>ping</code>'ed the server from my Mac</p>
<div dir="auto" data-snippet-clipboard-copy-content="❯ ping 161.35.222.56
PING 161.35.222.56 (161.35.222.56): 56 data bytes
64 bytes from 161.35.222.56: icmp_seq=0 ttl=47 time=125.364 ms
64 bytes from 161.35.222.56: icmp_seq=1 ttl=47 time=128.061 ms
64 bytes from 161.35.222.56: icmp_seq=2 ttl=47 time=177.931 ms
64 bytes from 161.35.222.56: icmp_seq=3 ttl=47 time=225.798 ms
64 bytes from 161.35.222.56: icmp_seq=4 ttl=47 time=130.101 ms
64 bytes from 161.35.222.56: icmp_seq=5 ttl=47 time=194.563 ms
64 bytes from 161.35.222.56: icmp_seq=6 ttl=47 time=159.518 ms
64 bytes from 161.35.222.56: icmp_seq=7 ttl=47 time=134.343 ms
64 bytes from 161.35.222.56: icmp_seq=8 ttl=47 time=501.139 ms
64 bytes from 161.35.222.56: icmp_seq=9 ttl=47 time=153.672 ms
64 bytes from 161.35.222.56: icmp_seq=10 ttl=47 time=137.927 ms
64 bytes from 161.35.222.56: icmp_seq=11 ttl=47 time=355.672 ms
64 bytes from 161.35.222.56: icmp_seq=12 ttl=47 time=138.777 ms
64 bytes from 161.35.222.56: icmp_seq=13 ttl=47 time=166.116 ms
64 bytes from 161.35.222.56: icmp_seq=14 ttl=47 time=288.758 ms
64 bytes from 161.35.222.56: icmp_seq=15 ttl=47 time=151.458 ms
64 bytes from 161.35.222.56: icmp_seq=16 ttl=47 time=164.025 ms
64 bytes from 161.35.222.56: icmp_seq=17 ttl=47 time=170.132 ms
64 bytes from 161.35.222.56: icmp_seq=18 ttl=47 time=279.034 ms
^C
--- 161.35.222.56 ping statistics ---
19 packets transmitted, 19 packets received, 0.0% packet loss"><pre><span>❯</span> ping <span>161.35</span><span>.</span><span>222.56</span>
<span>PING</span> <span>161.35</span><span>.</span><span>222.56</span> (<span>161.35</span><span>.</span><span>222.56</span>)<span>:</span> <span>56</span> <span>data</span> bytes
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>0</span> ttl<span>=</span><span>47</span> time<span>=</span><span>125.364</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>1</span> ttl<span>=</span><span>47</span> time<span>=</span><span>128.061</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>2</span> ttl<span>=</span><span>47</span> time<span>=</span><span>177.931</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>3</span> ttl<span>=</span><span>47</span> time<span>=</span><span>225.798</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>4</span> ttl<span>=</span><span>47</span> time<span>=</span><span>130.101</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>5</span> ttl<span>=</span><span>47</span> time<span>=</span><span>194.563</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>6</span> ttl<span>=</span><span>47</span> time<span>=</span><span>159.518</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>7</span> ttl<span>=</span><span>47</span> time<span>=</span><span>134.343</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>8</span> ttl<span>=</span><span>47</span> time<span>=</span><span>501.139</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>9</span> ttl<span>=</span><span>47</span> time<span>=</span><span>153.672</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>10</span> ttl<span>=</span><span>47</span> time<span>=</span><span>137.927</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>11</span> ttl<span>=</span><span>47</span> time<span>=</span><span>355.672</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>12</span> ttl<span>=</span><span>47</span> time<span>=</span><span>138.777</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>13</span> ttl<span>=</span><span>47</span> time<span>=</span><span>166.116</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>14</span> ttl<span>=</span><span>47</span> time<span>=</span><span>288.758</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>15</span> ttl<span>=</span><span>47</span> time<span>=</span><span>151.458</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>16</span> ttl<span>=</span><span>47</span> time<span>=</span><span>164.025</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>17</span> ttl<span>=</span><span>47</span> time<span>=</span><span>170.132</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>18</span> ttl<span>=</span><span>47</span> time<span>=</span><span>279.034</span> ms
<span><span>^</span></span><span>C</span>
<span><span>---</span> 161.35.222.56 ping statistics ---</span>
<span>19</span> packets transmitted, <span>19</span> packets received, <span>0.0</span><span>%</span> packet loss</pre></div>
<p dir="auto">It seems it's quite far, but looks fine to me, let's send some packets using our new protocol!</p>
<p dir="auto">First let's start the server in our Digital Ocean machine</p>
<div dir="auto" data-snippet-clipboard-copy-content="root@debian-s-1vcpu-512mb-10gb-fra1-01:~/hdp/hdp# sudo cargo run --bin server
Listening on protocol 255"><pre>root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~/</span>hdp<span>/</span>hdp<span>#</span> sudo cargo run <span><span>--</span>bin server</span>
<span>Listening</span> on protocol <span>255</span></pre></div>
<p dir="auto">And now we can send a packet from my Mac</p>
<div dir="auto" data-snippet-clipboard-copy-content="❯ fortune | cowsay | sudo cargo run --bin client 161.35.222.56
| Protocol Number | Succeeded (Client) | Time (μs) (Client) | Byte sum (Client) | Failure reason (Client) |
| 255 | 🫡 | timestamp | 563 | - |"><pre><span>❯</span> fortune <span>|</span> cowsay <span>|</span> sudo cargo run <span><span>--</span>bin client 161.35.222.56</span>
<span>|</span> <span>Protocol</span> <span>Number</span> <span>|</span> <span>Succeeded</span> (<span>Client</span>) <span>|</span> <span>Time</span> (μs) (<span>Client</span>) <span>|</span> <span>Byte</span> <span>sum</span> (<span>Client</span>) <span>|</span> <span>Failure</span> reason (<span>Client</span>) <span>|</span>
<span>|</span> <span>255</span> <span>|</span> 🫡 <span>|</span> timestamp <span>|</span> <span>563</span> <span>|</span> <span>-</span> <span>|</span></pre></div>
<p dir="auto">Packet sent. Let's check the server again</p>
<div dir="auto" data-snippet-clipboard-copy-content="root@debian-s-1vcpu-512mb-10gb-fra1-01:~/hdp/hdp# sudo cargo run --bin server
Listening on protocol 255
| Protocol Number | Time (μs) (Server) | Source IP (Server) | Byte Sum (Server) |
| --- | --- | --- |
| 255 | timestamp | my_ip | 563 |"><pre>root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~/</span>hdp<span>/</span>hdp<span>#</span> sudo cargo run <span><span>--</span>bin server</span>
<span>Listening</span> on protocol <span>255</span>
<span>|</span> <span>Protocol</span> <span>Number</span> <span>|</span> <span>Time</span> (μs) (<span>Server</span>) <span>|</span> <span>Source</span> <span>IP</span> (<span>Server</span>) <span>|</span> <span>Byte</span> <span>Sum</span> (<span>Server</span>) <span>|</span>
<span>|</span> <span>---</span> <span>|</span><span> --- | --- |</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> my_ip <span>|</span> <span>563</span> <span>|</span></pre></div>
<p dir="auto">Excellent. It seems that all went well, or so I thought. In-fact, all went downhill starting here. I took a quick break then came back. Let's try sending the packet again..</p>
<div dir="auto" data-snippet-clipboard-copy-content="| Protocol Number | Time (μs) (Server) | Source IP (Server) | Byte Sum (Server) |
| --- | --- | --- |
| 255 | timestamp | my_ip | 563 |"><pre><span>|</span> <span>Protocol</span> <span>Number</span> <span>|</span> <span>Time</span> (μs) (<span>Server</span>) <span>|</span> <span>Source</span> <span>IP</span> (<span>Server</span>) <span>|</span> <span>Byte</span> <span>Sum</span> (<span>Server</span>) <span>|</span>
<span>|</span> <span>---</span> <span>|</span><span> --- | --- |</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> my_ip <span>|</span> <span>563</span> <span>|</span></pre></div>
<p dir="auto">It's stuck? I can't see the second packet</p>
<p dir="auto">I <code>Ctrl+C</code> and attempt doing it again. No results..? That can't be right, could it be a client side bug? Let's use <code>tcpdump</code> to see all outgoing packets from my device</p>
<div dir="auto" data-snippet-clipboard-copy-content="❯ sudo tcpdump -i any 'ip[9] == 255'
tcpdump: data link type PKTAP
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on any, link-type PKTAP (Apple DLT_PKTAP), snapshot length 524288 bytes
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427"><pre><span>❯</span> sudo tcpdump <span>-</span>i <span>any</span> 'ip[<span>9</span>] <span>==</span> <span>255</span>'
tcpdump<span>:</span> <span>data</span> link <span>type</span> <span>PKTAP</span>
tcpdump<span>:</span> verbose output suppressed, use <span>-</span>v[v]<span>...</span> for full protocol decode
listening on <span>any</span>, link<span>-</span><span>type</span> <span>PKTAP</span> (<span>Apple</span> <span>DLT_PKTAP</span>), snapshot <span>length</span> <span>524288</span> bytes
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span></pre></div>
<p dir="auto">They're definitely leaving my Mac. What about doing the same thing on the receiving end?</p>
<div dir="auto" data-snippet-clipboard-copy-content="root@debian-s-1vcpu-512mb-10gb-fra1-01:~/hdp# tcpdump -i any 'ip[9] > 17'
tcpdump: data link type LINUX_SLL2
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes
"><pre>root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~/</span>hdp<span>#</span> tcpdump <span>-</span>i <span>any</span> 'ip[<span>9</span>] <span>&gt;</span> <span>17</span>'
tcpdump<span>:</span> <span>data</span> link <span>type</span> <span>LINUX_SLL2</span>
tcpdump<span>:</span> verbose output suppressed, use <span>-</span>v[v]<span>...</span> for full protocol decode
listening on <span>any</span>, link<span>-</span><span>type</span> <span>LINUX_SLL2</span> (<span>Linux</span> cooked v2), snapshot <span>length</span> <span>262144</span> bytes
</pre></div>
<p dir="auto">Nothing appeared</p>
<p dir="auto">I began doubting my earlier results, there they are in my shell. The timestamps and byte sums match. Was I imagining them? Is Linus Torvalds himself gaslighting me??</p>
<p dir="auto">Wait..? How did my ISP's <a href="https://simple.wikipedia.org/wiki/Network_address_translation" rel="nofollow">NATing box</a> forward the packet? NAT'ing relies on ports—but my protocol is just black magic to them</p>
<p dir="auto">I'm confused</p>
<p dir="auto">Very confused</p>
<p dir="auto">After digging a bit in, I found that Digital Ocean doesn't support non-standard IP Protocols</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Hawzen/hdp/blob/master/readme_assets/ihatedigitalocean.png"><img src="https://github.com/Hawzen/hdp/raw/master/readme_assets/ihatedigitalocean.png" alt="digital_ocean_sucks"></a></p>
<p dir="auto">This still doesn't explain it. How did one packet survive? There really is no way to know, and I was banging my head against the wall trying to figure it out</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">One. Last. Try</h3><a id="user-content-one-last-try" aria-label="Permalink: One. Last. Try" href="#one-last-try"></a></p>
<p dir="auto">if any cloud provider would support non-standard IP Protocols, it'd be AWS</p>
<p dir="auto">I provisioned two machines. Set them up. Server. Client. It works.. !</p>
<div dir="auto" data-snippet-clipboard-copy-content="admin@ip-172-31-13-218:~/hdp$ sudo cargo run --bin server 255
Server is listening on SockAddr { ss_family: 2, len: 16 }, protocol: 255
| Protocol Number | Time (μs) (Server) | Source IP (Server) | Byte Sum (Server) |
| --- | --- | --- |
| 255 | timestamp | 54.153.13.186 | 33 |
| 255 | timestamp | 54.153.13.186 | 34 |
| 255 | timestamp | 54.153.13.186 | 35 |
| 255 | timestamp | 54.153.13.186 | 36 |
"><pre>admin<span>@</span>ip<span>-</span><span>172</span><span>-</span><span>31</span><span>-</span><span>13</span><span>-</span><span>218</span><span>:~/</span>hdp<span>$</span> sudo cargo run <span><span>--</span>bin server 255</span>
<span>Server</span> is listening on <span>SockAddr</span> { ss_family<span>:</span> <span>2</span>, len<span>:</span> <span>16</span> }, protocol<span>:</span> <span>255</span>
<span>|</span> <span>Protocol</span> <span>Number</span> <span>|</span> <span>Time</span> (μs) (<span>Server</span>) <span>|</span> <span>Source</span> <span>IP</span> (<span>Server</span>) <span>|</span> <span>Byte</span> <span>Sum</span> (<span>Server</span>) <span>|</span>
<span>|</span> <span>---</span> <span>|</span><span> --- | --- |</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> <span>54.153</span><span>.</span><span>13.186</span> <span>|</span> <span>33</span> <span>|</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> <span>54.153</span><span>.</span><span>13.186</span> <span>|</span> <span>34</span> <span>|</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> <span>54.153</span><span>.</span><span>13.186</span> <span>|</span> <span>35</span> <span>|</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> <span>54.153</span><span>.</span><span>13.186</span> <span>|</span> <span>36</span> <span>|</span>
</pre></div>
<p dir="auto">Granted, the server was just two hops away from the client, and it didn't have to pass through the scary sea of the internet</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Hawzen/hdp/blob/master/readme_assets/latency_difference_between_hdp_and_udp.png"><img src="https://github.com/Hawzen/hdp/raw/master/readme_assets/latency_difference_between_hdp_and_udp.png" alt="Description"></a></p>
<p dir="auto"><sub>The latency is in the microseconds due to both machines being in the same datacenter.</sub></p>
<p dir="auto">The latency difference between the HDP &amp; UDP was a consistent, but negligible 20μs across various benchmarks</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">But what about the internet?</h4><a id="user-content-but-what-about-the-internet" aria-label="Permalink: But what about the internet?" href="#but-what-about-the-internet"></a></p>
<p dir="auto">I tried sending packets from my Mac to the AWS server, and I reproduced the same one packet behaviour above. I left a sample of the results in <a href="https://github.com/Hawzen/hdp/blob/master/samples/tcpdump_tokyo_sever_mac_client.md"><code>tcpdump_tokyo_server_mac_client.md</code></a>. I sent 1 packet for all protocols, and all of them stopped working after the first packet except TCP/UDP/ICMP</p>
<p dir="auto">And as expected, sending or recieving packets from the Digital Ocean machine to the AWS machine didn't work</p>
<p dir="auto">There's no way to know for sure.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Lessons learned</h2><a id="user-content-lessons-learned-1" aria-label="Permalink: Lessons learned" href="#lessons-learned-1"></a></p>
<p dir="auto">Technically <em>yes</em>, you could use your own IP protocol. But unless you're a masochist, I do not suggest it</p>
<ul dir="auto">
<li>Your code won't be portable, and you'll need to support various operating systems</li>
<li>Your protocol will be randomly dropped at NAT gateways &amp; firewalls. It might work on your own network, but I gaurentee it won't work on the internet</li>
<li>From my testing, there's no latency improvements from using a non-standard IP protocol</li>
</ul>
<p dir="auto">TL;DR: <em><strong>Use TCP or UDP</strong></em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Resources</h2><a id="user-content-resources" aria-label="Permalink: Resources" href="#resources"></a></p>
<ul dir="auto">
<li>The <a href="https://datatracker.ietf.org/doc/html/rfc768" rel="nofollow">UDP protocol specification</a> is so minimal it is almost funny</li>
<li><a href="https://datatracker.ietf.org/doc/html/rfc3692#section-2.1" rel="nofollow">IP Protocol numbers that are assigned for testing</a></li>
<li><a href="https://en.wikipedia.org/wiki/List_of_IP_protocol_numbers" rel="nofollow">The list of protocols</a> supported under the IP protocol is pretty interesting</li>
<li><a href="https://hackaday.com/2024/09/21/when-raw-network-sockets-arent-raw-raw-sockets-in-macos-and-linux/" rel="nofollow">This</a> article speaks about some differences between raw sockets in Linux &amp; FreeBSD</li>
<li>How would you implement NAT on something other than TCP or UDP? <a href="https://superuser.com/a/1108226" rel="nofollow">This</a> answer is pretty insightful</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Xcode Constantly Phones Home (122 pts)]]></title>
            <link>https://lapcatsoftware.com/articles/2025/2/5.html</link>
            <guid>43168589</guid>
            <pubDate>Tue, 25 Feb 2025 05:43:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lapcatsoftware.com/articles/2025/2/5.html">https://lapcatsoftware.com/articles/2025/2/5.html</a>, See on <a href="https://news.ycombinator.com/item?id=43168589">Hacker News</a></p>
<div id="readability-page-1" class="page">
<nav>
Previous: <a href="https://lapcatsoftware.com/articles/2025/2/4.html">Inaccessible .bnnsir files on macOS Sequoia</a>
<br><a href="https://lapcatsoftware.com/articles/index.html" title="The Desolation of Blog">Articles index</a></nav>
<header><a href="https://lapcatsoftware.com/">Jeff Johnson</a> (<a href="https://underpassapp.com/">My apps</a>, <a href="https://www.paypal.me/JeffJohnsonWI">PayPal.Me</a>, <a href="https://mastodon.social/@lapcatsoftware" title="@lapcatsoftware@mastodon.social">Mastodon</a>)</header>

<h3>February 24 2025</h3>

<p>Building <a href="https://underpassapp.com/StopTheMadness/">StopTheMadness Pro</a> in Xcode is usually very fast, because my project doesn't use any Swift. It's a combination of Objective-C, which compiles much more quickly than Swift, and JavaScript, which doesn't need to be compiled. However, sometimes the builds were very slow for some strange reason. Checking the Xcode build transcripts, I found that the delay was in the "Gather provisioning inputs" build phase.</p>
<p><img src="https://lapcatsoftware.com/articles/2025/2/5.png" width="425" height="145" alt="Xcode build messages"></p>
<p>This one phase took 50.6 seconds when the entire build was 56.8 seconds!</p>
<p>I tested with my internet disabled, and the slow builds did not occur. Obviously, though, it's impractical to disable my internet every time I want to build and run. After all, my project is a Safari extension! I do use <a href="https://www.obdev.at/products/littlesnitch/">Little Snitch</a>, but I had previously allowed all connections from Xcode to <code>apple.com</code>, because that's required to upload builds to App Store Connect. When I scrutinized the individual Xcode connections with Little Snitch, I saw that <code>developerservices2.apple.com</code> was responsible for the slow "Gathering provisioning inputs" build phase. When I denied those connections with Little Snitch, my builds were always fast. And successful. The build phase is <em>mostly</em> unnecessary.</p>
<p>I found a <a href="https://developer.apple.com/forums/thread/756120">thread in the Apple Developer Forums</a> that discusses the problem, mentioning the <code>-allowProvisioningUpdates</code> option of the command-line <code>xcodebuild</code> tool. From the <code>man</code> page:</p>
<blockquote>Allow xcodebuild to communicate with the Apple Developer website.
           For automatically signed targets, xcodebuild will create and update
           profiles, app IDs, and certificates. For manually signed targets,
           xcodebuild will download missing or updated provisioning profiles.
           Requires a developer account to have been added in Xcode's Accounts
           preference pane.</blockquote>
<p>Connecting to <code>developerservices2.apple.com</code>, and to some other domains, is required in order to upload a build to App Store Connect. For most local builds, on the other hand, the "Gathering provisioning inputs" build phase is unnecessary and can slow down the build considerably. Thus, I've now denied Xcode connections to <code>developerservices2.apple.com</code> by default in Little Snitch and disable the rule only when uploading to App Store Connect.</p>
<p>During my investigation of slow builds, I noticed some other frequent Xcode connections. For example, Xcode connects to <code>devimages-cdn.apple.com</code> every time it launches. According to Apple's support document <a href="https://support.apple.com/101555">Use Apple products on enterprise networks</a>, that domain is used for "Xcode downloadable components". I assume this refers to platform support in the Components pane of Xcode Settings. (Note that the document doesn't mention <code>developerservices2.apple.com</code>.) Again, though, it's unnecessary to check for updates on every launch. I'd rather not tell Apple whenever I launch Xcode, or whenever I make a local build of my app. It certainly doesn't align with Apple's claim that they believe privacy is a fundamental human right. Or perhaps Apple believes that developers are subhuman…</p>
<p>I've saved the worst for last. For some reason, Xcode phones home to <code>appstoreconnect.apple.com</code> every time I open an Xcode project. This also appears to be unnecessary, and I experience no problems after denying the connections in Little Snitch, so I do! I assume that the connections send identifying information about the Xcode project to Apple, otherwise why even make the connections when opening a project? And all of these connections from Xcode, to every domain, require login to your Apple Developer account, so Apple is definitely receiving identifying information about you in any case.</p>
<p>In effect, Xcode is a developer analytics collection mechanism, whether you like it or not, which I don't.</p>

<header><a href="https://lapcatsoftware.com/">Jeff Johnson</a> (<a href="https://underpassapp.com/">My apps</a>, <a href="https://www.paypal.me/JeffJohnsonWI">PayPal.Me</a>, <a href="https://mastodon.social/@lapcatsoftware" title="@lapcatsoftware@mastodon.social">Mastodon</a>)</header>
<nav><a href="https://lapcatsoftware.com/articles/index.html" title="The Desolation of Blog">Articles index</a><br>
Previous: <a href="https://lapcatsoftware.com/articles/2025/2/4.html">Inaccessible .bnnsir files on macOS Sequoia</a>
</nav>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to change your settings to make yourself less valuable to Meta (262 pts)]]></title>
            <link>https://johnoliverwantsyourraterotica.com/</link>
            <guid>43167936</guid>
            <pubDate>Tue, 25 Feb 2025 03:47:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://johnoliverwantsyourraterotica.com/">https://johnoliverwantsyourraterotica.com/</a>, See on <a href="https://news.ycombinator.com/item?id=43167936">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">


			
				<article id="post-7" class="page">

				
					<div>
		<div id="home">
				
				
				
				
				
				
				<div>
				
				
				
				
				<p><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Gray-logo.png" alt="" title="Gray logo"></span>
			</p>
			</div><div>
				
				
				
				
				<p><strong>How to change your settings</strong></p>
			</div><div>
<p><span>to make yourself less valuable to Meta</span></p></div><div>
				
				
				
				
				<p><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Both-logos.png" alt="" title="Both logos"></span>
			</p>
			</div><div>
				<div>
				
				
				
				
				<p><a href="#fb"><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Facebook_Logo_Primary.png" alt="" title="Facebook_Logo_Primary"></span></a>
			</p>
			</div><div>
				
				
				
				
				<p><a href="#ig"><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Instagram_Glyph_Gradient-1.png" alt="" title="Instagram_Glyph_resize"></span></a>
			</p>
			</div>
				
				
				
				
			</div>
				
				
			</div><div id="fb">
				<div>
				
				
				
				
				<p><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Facebook_Logo_Primary.png" alt="" title="Facebook_Logo_Primary"></span>
			</p>
			</div><div><p><strong>TO STOP META FROM FEEDING YOU ADS BASED ON DATA COLLECTED ABOUT YOU FROM OTHER APPS AND WEBSITES</strong></p>
<ul>
<li><span>Click “Ad preferences.”</span></li>
<li><span>Click “Manage info.”</span></li>
<li><span>Click “Activity information from ad partners.”</span></li>
<li><span>Click “Review setting.”</span></li>
<li><span>Select “No, don’t make my ads more relevant by using this information.”</span></li>
<li><span>Click “Confirm.”</span></li>
</ul>

<p><strong>TO STOP META FROM USING YOUR DATA TO HELP ADVERTISERS TARGET YOU ON OTHER APPS</strong></p>
<ul>
<li><span>Click “Ad preferences.”</span></li>
<li><span>Click “Manage info.”</span></li>
<li><span>Click “Ads from ad partners.”</span></li>
<li><span>Select “Don’t show me ads from ad partners.”</span></li>
<li><span>Click the “X” button to close out.</span></li>
</ul>

<p><strong>TO UNLINK YOUR ACCOUNT FROM THE DATA ABOUT YOU THAT OTHER COMPANIES GIVE TO META</strong></p>
<ul>
<li><span>Click “Your information and permissions.”</span></li>
<li><span>Click “Your activity off Meta technologies.”</span></li>
<li><span>Click “Manage future activity.”</span></li>
<li><span>Select </span><b>“</b><span>Disconnect future activity.</span><b>”</b></li>
<li><span>Click “Continue.”</span></li>
<li><span>Click “Disconnect future activity.”</span></li>
</ul>
</div>
				
				
				
				
			</div><div id="ig">
				<div>
				
				
				
				
				<p><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Instagram_Glyph_Gradient-1.png" alt="" title="Instagram_Glyph_resize"></span>
			</p>
			</div><div><p><strong>If your Facebook and Instagram accounts are linked, you’re good to go! </strong></p>

<p><strong>If your Facebook and Instagram accounts are NOT linked, go to<a href="http://accountscenter.instagram.com/" target="_blank" rel="noopener"> accountscenter.instagram.com</a> and repeat the steps above.</strong></p></div>
				
				
				
				
			</div><div><p><strong>OTHER STEPS YOU CAN TAKE:</strong></p>
<ul>
<li><span>Use a privacy-focused web browser like </span><a href="https://www.mozilla.org/en-US/firefox/" target="_blank" rel="noopener"><span>Firefox</span></a><span>.</span></li>
<li><span>Add a browser extension like </span><a href="https://privacybadger.org/" target="_blank" rel="noopener"><span>Privacy Badger</span></a><span> to block advertisers and other third parties from tracking you.&nbsp;</span></li>
<li><span>Disable your phone’s advertising identifier (see instructions for </span><a href="https://ssd.eff.org/module/how-to-get-to-know-iphone-privacy-and-security-settings#disable-ad-tracking" target="_blank" rel="noopener"><span>iOS</span></a><span> and </span><a href="https://ssd.eff.org/module/how-to-get-to-know-android-privacy-and-security-settings#disable-ad-tracking" target="_blank" rel="noopener"><span>Android</span></a><span> devices).&nbsp;</span></li>
</ul>

<p><span>SPECIAL THANKS TO THE </span><a href="https://www.eff.org/deeplinks/2025/01/mad-meta-dont-let-them-collect-and-monetize-your-personal-data" target="_blank" rel="noopener"><span>ELECTRONIC FRONTIER FOUNDATION</span></a></p></div>		</div>

				
				</article>

			

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disclosure of personal information to DOGE “is irreparable harm,” judge rules (237 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/02/judges-block-doge-access-to-personal-data-in-loss-for-trump-administration/</link>
            <guid>43167579</guid>
            <pubDate>Tue, 25 Feb 2025 02:59:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/02/judges-block-doge-access-to-personal-data-in-loss-for-trump-administration/">https://arstechnica.com/tech-policy/2025/02/judges-block-doge-access-to-personal-data-in-loss-for-trump-administration/</a>, See on <a href="https://news.ycombinator.com/item?id=43167579">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>"The plaintiffs have made a clear showing that they are likely to suffer irreparable harm without injunctive relief," the order said. "DOGE affiliates have been granted access to systems of record that contain some of the plaintiffs' most sensitive data—Social Security numbers, dates of birth, home addresses, income and assets, citizenship status, and disability status—and their access to this trove of personal information is ongoing. There is no reason to believe their access to this information will end anytime soon because the government believes their access is appropriate."</p>
<p>The American Federation of Teachers, which represents 1.8 million teachers and nurses, was joined in the lawsuit by the International Association of Machinists and Aerospace Workers, International Federation of Professional and Technical Engineers, National Active and Retired Federal Employees Association, and National Federation of Federal Employees.</p>

<h2>No need to know</h2>
<p>The government insisted that the DOGE affiliates are employees of Education and OPM, and the judge assumed that is true for purposes of evaluating the motion for a restraining order. Even with that allowance, Boardman decided the data access is not permissible under the "need-to-know" exception to the law prohibiting unnecessary disclosure.</p>
<p>The Trump administration did not explain why "the DOGE affiliates at Education <em>need</em> such comprehensive, sweeping access to the plaintiffs' records to audit student loan programs for waste, fraud, and abuse or to conduct cost-estimate analyses," Boardman wrote, adding that "there appears to be no precedent with similar facts."</p>
<p>There are six DOGE affiliates working at Education. They include Adam Ramada, a United States DOGE Service employee, and five "DOGE-affiliated individuals" who have not been identified by name.</p>
<p>"It may be that, with additional time, the government can explain why granting such broad access to the plaintiffs' personal information is necessary for DOGE affiliates at Education to do their jobs, but for now, the record before the Court indicates they do not have a <em>need</em> for these records in the performance of their duties," Boardman wrote.</p>

          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek open source DeepEP – library for MoE training and Inference (410 pts)]]></title>
            <link>https://github.com/deepseek-ai/DeepEP</link>
            <guid>43167373</guid>
            <pubDate>Tue, 25 Feb 2025 02:27:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/DeepEP">https://github.com/deepseek-ai/DeepEP</a>, See on <a href="https://news.ycombinator.com/item?id=43167373">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DeepEP</h2><a id="user-content-deepep" aria-label="Permalink: DeepEP" href="#deepep"></a></p>
<p dir="auto">DeepEP is a communication library tailored for Mixture-of-Experts (MoE) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also as known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.</p>
<p dir="auto">To align with the group-limited gating algorithm proposed in the <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> paper, DeepEP offers a set of kernels optimized for asymmetric-domain bandwidth forwarding, such as forwarding data from NVLink domain to RDMA domain. These kernels deliver high throughput, making them suitable for both training and inference prefilling tasks. Additionally, they support SM (Streaming Multiprocessors) number control.</p>
<p dir="auto">For latency-sensitive inference decoding, DeepEP includes a set of low-latency kernels with pure RDMA to minimize delays. The library also introduces a hook-based communication-computation overlapping method that does not occupy any SM resource.</p>
<p dir="auto">Notice: the implementation in this library may have some slight differences from the <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> paper.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Normal kernels with NVLink and RDMA forwarding</h3><a id="user-content-normal-kernels-with-nvlink-and-rdma-forwarding" aria-label="Permalink: Normal kernels with NVLink and RDMA forwarding" href="#normal-kernels-with-nvlink-and-rdma-forwarding"></a></p>
<p dir="auto">We test normal kernels on H800 (~160 GB/s NVLink maximum bandwidth), with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow the DeepSeek-V3/R1 pretraining setting (4096 tokens per batch, 7168 hidden, top-4 groups, top-8 experts, FP8 dispatching and BF16 combining).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Type</th>
<th>Dispatch #EP</th>
<th>Bottleneck bandwidth</th>
<th>Combine #EP</th>
<th>Bottleneck bandwidth</th>
</tr>
</thead>
<tbody>
<tr>
<td>Intranode</td>
<td>8</td>
<td>153 GB/s (NVLink)</td>
<td>8</td>
<td>158 GB/s (NVLink)</td>
</tr>
<tr>
<td>Internode</td>
<td>16</td>
<td>43 GB/s (RDMA)</td>
<td>16</td>
<td>43 GB/s (RDMA)</td>
</tr>
<tr>
<td>Internode</td>
<td>32</td>
<td>44 GB/s (RDMA)</td>
<td>32</td>
<td>47 GB/s (RDMA)</td>
</tr>
<tr>
<td>Internode</td>
<td>64</td>
<td>46 GB/s (RDMA)</td>
<td>64</td>
<td>45 GB/s (RDMA)</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Low-latency kernels with pure RDMA</h3><a id="user-content-low-latency-kernels-with-pure-rdma" aria-label="Permalink: Low-latency kernels with pure RDMA" href="#low-latency-kernels-with-pure-rdma"></a></p>
<p dir="auto">We test low-latency kernels on H800 with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow a typical DeepSeek-V3/R1 production setting (128 tokens per batch, 7168 hidden, top-8 experts, FP8 dispatching and BF16 combining).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Dispatch #EP</th>
<th>Latency</th>
<th>RDMA bandwidth</th>
<th>Combine #EP</th>
<th>Latency</th>
<th>RDMA bandwidth</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>163 us</td>
<td>46 GB/s</td>
<td>8</td>
<td>318 us</td>
<td>46 GB/s</td>
</tr>
<tr>
<td>16</td>
<td>173 us</td>
<td>43 GB/s</td>
<td>16</td>
<td>329 us</td>
<td>44 GB/s</td>
</tr>
<tr>
<td>32</td>
<td>182 us</td>
<td>41 GB/s</td>
<td>32</td>
<td>350 us</td>
<td>41 GB/s</td>
</tr>
<tr>
<td>64</td>
<td>186 us</td>
<td>40 GB/s</td>
<td>64</td>
<td>353 us</td>
<td>41 GB/s</td>
</tr>
<tr>
<td>128</td>
<td>192 us</td>
<td>39 GB/s</td>
<td>128</td>
<td>369 us</td>
<td>39 GB/s</td>
</tr>
<tr>
<td>256</td>
<td>194 us</td>
<td>39 GB/s</td>
<td>256</td>
<td>360 us</td>
<td>40 GB/s</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirements</h3><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Hopper GPUs (may support more architectures or devices later)</li>
<li>Python 3.8 and above</li>
<li>CUDA 12.3 and above</li>
<li>PyTorch 2.1 and above</li>
<li>NVLink for intranode communication</li>
<li>RDMA network for internode communication</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download and install NVSHMEM dependency</h3><a id="user-content-download-and-install-nvshmem-dependency" aria-label="Permalink: Download and install NVSHMEM dependency" href="#download-and-install-nvshmem-dependency"></a></p>
<p dir="auto">DeepEP also depends on our modified NVSHMEM. Please refer to our <a href="https://github.com/deepseek-ai/DeepEP/blob/main/third-party/README.md">NVSHMEM Installation Guide</a> for instructions.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development</h3><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build and make symbolic links for SO files
NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build
# You may modify the specific SO names according to your own platform
ln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so

# Run test cases
# NOTES: you may modify the `init_dist` function in `tests/utils.py`
# according to your own cluster settings, and launch into multiple nodes 
python tests/test_intranode.py
python tests/test_internode.py
python tests/test_low_latency.py"><pre><span><span>#</span> Build and make symbolic links for SO files</span>
NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build
<span><span>#</span> You may modify the specific SO names according to your own platform</span>
ln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so

<span><span>#</span> Run test cases</span>
<span><span>#</span> NOTES: you may modify the `init_dist` function in `tests/utils.py`</span>
<span><span>#</span> according to your own cluster settings, and launch into multiple nodes </span>
python tests/test_intranode.py
python tests/test_internode.py
python tests/test_low_latency.py</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install"><pre>NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install</pre></div>
<p dir="auto">Then, import <code>deep_ep</code> in your Python project, and enjoy!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Network configurations</h2><a id="user-content-network-configurations" aria-label="Permalink: Network configurations" href="#network-configurations"></a></p>
<p dir="auto">DeepEP is fully tested with InfiniBand networks. However, it is theoretically compatible with RDMA over Converged Ethernet (RoCE) as well.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Traffic isolation</h3><a id="user-content-traffic-isolation" aria-label="Permalink: Traffic isolation" href="#traffic-isolation"></a></p>
<p dir="auto">Traffic isolation is supported by InfiniBand through Virtual Lanes (VL).</p>
<p dir="auto">To prevent interference between different types of traffic, we recommend segregating workloads across different virtual lanes as follows:</p>
<ul dir="auto">
<li>workloads using normal kernels</li>
<li>workloads using low-latency kernels</li>
<li>other workloads</li>
</ul>
<p dir="auto">For DeepEP, you can control the virtual lane assignment by setting the <code>NVSHMEM_IB_SL</code> environment variable.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adaptive routing</h3><a id="user-content-adaptive-routing" aria-label="Permalink: Adaptive routing" href="#adaptive-routing"></a></p>
<p dir="auto">Adaptive routing is an advanced routing feature provided by InfiniBand switches that can evenly distribute traffic across multiple paths. Currently, low-latency kernels support adaptive routing, while normal kernels do not (support may be added soon). <strong>Enabling adaptive routing for normal internode kernels may lead to deadlocks or data corruption issues</strong>.</p>
<p dir="auto">For low-latency kernels, enabling adaptive routing can completely eliminate network congestion caused by routing conflicts, but it also introduces additional latency. We recommend the following configuration for optimal performance:</p>
<ul dir="auto">
<li>enable adaptive routing in environments with heavy network loads</li>
<li>use static routing in environments with light network loads</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Congestion control</h3><a id="user-content-congestion-control" aria-label="Permalink: Congestion control" href="#congestion-control"></a></p>
<p dir="auto">Congestion control is disabled as we have not observed significant congestion in our production environment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interfaces and examples</h2><a id="user-content-interfaces-and-examples" aria-label="Permalink: Interfaces and examples" href="#interfaces-and-examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example use in model training or inference prefilling</h3><a id="user-content-example-use-in-model-training-or-inference-prefilling" aria-label="Permalink: Example use in model training or inference prefilling" href="#example-use-in-model-training-or-inference-prefilling"></a></p>
<p dir="auto">The normal kernels can be used in model training or the inference prefilling phase (without the backward part) as the below example code shows.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
import torch.distributed as dist
from typing import List, Tuple, Optional, Union

from deep_ep import Buffer, EventOverlap

# Communication buffer (will allocate at runtime)
_buffer: Optional[Buffer] = None

# Set the number of SMs to use
# NOTES: this is a static variable
Buffer.set_num_sms(24)


# You may call this function at the framework initialization
def get_buffer(group: dist.ProcessGroup, hidden_bytes: int) -> Buffer:
    global _buffer
    
    # NOTES: you may also replace `get_*_config` with your auto-tuned results via all the tests
    num_nvl_bytes, num_rdma_bytes = 0, 0
    for config in (Buffer.get_dispatch_config(group.size()), Buffer.get_combine_config(group.size())):
        num_nvl_bytes = max(config.get_nvl_buffer_size_hint(hidden_bytes, group.size()), num_nvl_bytes)
        num_rdma_bytes = max(config.get_rdma_buffer_size_hint(hidden_bytes, group.size()), num_rdma_bytes)

    # Allocate a buffer if not existed or not enough buffer size
    # NOTES: the adaptive routing configuration of the network **must be off**
    if _buffer is None or _buffer.group != group or _buffer.num_nvl_bytes < num_nvl_bytes or _buffer.num_rdma_bytes < num_rdma_bytes:
        _buffer = Buffer(group, num_nvl_bytes, num_rdma_bytes)
    return _buffer


def get_hidden_bytes(x: torch.Tensor) -> int:
    t = x[0] if isinstance(x, tuple) else x
    return t.size(1) * max(t.element_size(), 2)


def dispatch_forward(x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
                     topk_idx: torch.Tensor, topk_weights: torch.Tensor,
                     num_experts: int, previous_event: Optional[EventOverlap] = None) -> \
        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], torch.Tensor, torch.Tensor, List, Tuple, EventOverlap]:
    # NOTES: an optional `previous_event` means a CUDA event captured that you want to make it as a dependency 
    # of the dispatch kernel, it may be useful with communication-computation overlap. For more information, please
    # refer to the docs of `Buffer.dispatch`
    global _buffer

    # Calculate layout before actual dispatch
    num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert, is_token_in_rank, previous_event = \
        _buffer.get_dispatch_layout(topk_idx, num_experts,
                                    previous_event=previous_event, async_finish=True,
                                    allocate_on_comm_stream=previous_event is not None)
    # Do MoE dispatch
    # NOTES: the CPU will wait for GPU's signal to arrive, so this is not compatible with CUDA graph
    # For more advanced usages, please refer to the docs of the `dispatch` function
    recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event = \
        _buffer.dispatch(x, topk_idx=topk_idx, topk_weights=topk_weights,
                         num_tokens_per_rank=num_tokens_per_rank, num_tokens_per_rdma_rank=num_tokens_per_rdma_rank,
                         is_token_in_rank=is_token_in_rank, num_tokens_per_expert=num_tokens_per_expert,
                         previous_event=previous_event, async_finish=True,
                         allocate_on_comm_stream=True)
    # For event management, please refer to the docs of the `EventOverlap` class
    return recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event


def dispatch_backward(grad_recv_x: torch.Tensor, grad_recv_topk_weights: torch.Tensor, handle: Tuple) -> \
        Tuple[torch.Tensor, torch.Tensor, EventOverlap]:
    global _buffer

    # The backward process of MoE dispatch is actually a combine
    # For more advanced usages, please refer to the docs of the `combine` function
    combined_grad_x, combined_grad_recv_topk_weights, event = \
        _buffer.combine(grad_recv_x, handle, topk_weights=grad_recv_topk_weights, async_finish=True)

    # For event management, please refer to the docs of the `EventOverlap` class
    return combined_grad_x, combined_grad_recv_topk_weights, event


def combine_forward(x: torch.Tensor, handle: Tuple, previous_event: Optional[EventOverlap] = None) -> \
        Tuple[torch.Tensor, EventOverlap]:
    global _buffer

    # Do MoE combine
    # For more advanced usages, please refer to the docs of the `combine` function
    combined_x, _, event = _buffer.combine(x, handle, async_finish=True, previous_event=previous_event,
                                           allocate_on_comm_stream=previous_event is not None)

    # For event management, please refer to the docs of the `EventOverlap` class
    return combined_x, event


def combine_backward(grad_combined_x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
                     handle: Tuple, previous_event: Optional[EventOverlap] = None) -> \
        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], EventOverlap]:
    global _buffer

    # The backward process of MoE combine is actually a dispatch
    # For more advanced usages, please refer to the docs of the `combine` function
    grad_x, _, _, _, _, event = _buffer.dispatch(grad_combined_x, handle=handle, async_finish=True,
                                                 previous_event=previous_event,
                                                 allocate_on_comm_stream=previous_event is not None)

    # For event management, please refer to the docs of the `EventOverlap` class
    return grad_x, event"><pre><span>import</span> <span>torch</span>
<span>import</span> <span>torch</span>.<span>distributed</span> <span>as</span> <span>dist</span>
<span>from</span> <span>typing</span> <span>import</span> <span>List</span>, <span>Tuple</span>, <span>Optional</span>, <span>Union</span>

<span>from</span> <span>deep_ep</span> <span>import</span> <span>Buffer</span>, <span>EventOverlap</span>

<span># Communication buffer (will allocate at runtime)</span>
<span>_buffer</span>: <span>Optional</span>[<span>Buffer</span>] <span>=</span> <span>None</span>

<span># Set the number of SMs to use</span>
<span># NOTES: this is a static variable</span>
<span>Buffer</span>.<span>set_num_sms</span>(<span>24</span>)


<span># You may call this function at the framework initialization</span>
<span>def</span> <span>get_buffer</span>(<span>group</span>: <span>dist</span>.<span>ProcessGroup</span>, <span>hidden_bytes</span>: <span>int</span>) <span>-&gt;</span> <span>Buffer</span>:
    <span>global</span> <span>_buffer</span>
    
    <span># NOTES: you may also replace `get_*_config` with your auto-tuned results via all the tests</span>
    <span>num_nvl_bytes</span>, <span>num_rdma_bytes</span> <span>=</span> <span>0</span>, <span>0</span>
    <span>for</span> <span>config</span> <span>in</span> (<span>Buffer</span>.<span>get_dispatch_config</span>(<span>group</span>.<span>size</span>()), <span>Buffer</span>.<span>get_combine_config</span>(<span>group</span>.<span>size</span>())):
        <span>num_nvl_bytes</span> <span>=</span> <span>max</span>(<span>config</span>.<span>get_nvl_buffer_size_hint</span>(<span>hidden_bytes</span>, <span>group</span>.<span>size</span>()), <span>num_nvl_bytes</span>)
        <span>num_rdma_bytes</span> <span>=</span> <span>max</span>(<span>config</span>.<span>get_rdma_buffer_size_hint</span>(<span>hidden_bytes</span>, <span>group</span>.<span>size</span>()), <span>num_rdma_bytes</span>)

    <span># Allocate a buffer if not existed or not enough buffer size</span>
    <span># NOTES: the adaptive routing configuration of the network **must be off**</span>
    <span>if</span> <span>_buffer</span> <span>is</span> <span>None</span> <span>or</span> <span>_buffer</span>.<span>group</span> <span>!=</span> <span>group</span> <span>or</span> <span>_buffer</span>.<span>num_nvl_bytes</span> <span>&lt;</span> <span>num_nvl_bytes</span> <span>or</span> <span>_buffer</span>.<span>num_rdma_bytes</span> <span>&lt;</span> <span>num_rdma_bytes</span>:
        <span>_buffer</span> <span>=</span> <span>Buffer</span>(<span>group</span>, <span>num_nvl_bytes</span>, <span>num_rdma_bytes</span>)
    <span>return</span> <span>_buffer</span>


<span>def</span> <span>get_hidden_bytes</span>(<span>x</span>: <span>torch</span>.<span>Tensor</span>) <span>-&gt;</span> <span>int</span>:
    <span>t</span> <span>=</span> <span>x</span>[<span>0</span>] <span>if</span> <span>isinstance</span>(<span>x</span>, <span>tuple</span>) <span>else</span> <span>x</span>
    <span>return</span> <span>t</span>.<span>size</span>(<span>1</span>) <span>*</span> <span>max</span>(<span>t</span>.<span>element_size</span>(), <span>2</span>)


<span>def</span> <span>dispatch_forward</span>(<span>x</span>: <span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]],
                     <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_weights</span>: <span>torch</span>.<span>Tensor</span>,
                     <span>num_experts</span>: <span>int</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]], <span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>, <span>List</span>, <span>Tuple</span>, <span>EventOverlap</span>]:
    <span># NOTES: an optional `previous_event` means a CUDA event captured that you want to make it as a dependency </span>
    <span># of the dispatch kernel, it may be useful with communication-computation overlap. For more information, please</span>
    <span># refer to the docs of `Buffer.dispatch`</span>
    <span>global</span> <span>_buffer</span>

    <span># Calculate layout before actual dispatch</span>
    <span>num_tokens_per_rank</span>, <span>num_tokens_per_rdma_rank</span>, <span>num_tokens_per_expert</span>, <span>is_token_in_rank</span>, <span>previous_event</span> <span>=</span> \
        <span>_buffer</span>.<span>get_dispatch_layout</span>(<span>topk_idx</span>, <span>num_experts</span>,
                                    <span>previous_event</span><span>=</span><span>previous_event</span>, <span>async_finish</span><span>=</span><span>True</span>,
                                    <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)
    <span># Do MoE dispatch</span>
    <span># NOTES: the CPU will wait for GPU's signal to arrive, so this is not compatible with CUDA graph</span>
    <span># For more advanced usages, please refer to the docs of the `dispatch` function</span>
    <span>recv_x</span>, <span>recv_topk_idx</span>, <span>recv_topk_weights</span>, <span>num_recv_tokens_per_expert_list</span>, <span>handle</span>, <span>event</span> <span>=</span> \
        <span>_buffer</span>.<span>dispatch</span>(<span>x</span>, <span>topk_idx</span><span>=</span><span>topk_idx</span>, <span>topk_weights</span><span>=</span><span>topk_weights</span>,
                         <span>num_tokens_per_rank</span><span>=</span><span>num_tokens_per_rank</span>, <span>num_tokens_per_rdma_rank</span><span>=</span><span>num_tokens_per_rdma_rank</span>,
                         <span>is_token_in_rank</span><span>=</span><span>is_token_in_rank</span>, <span>num_tokens_per_expert</span><span>=</span><span>num_tokens_per_expert</span>,
                         <span>previous_event</span><span>=</span><span>previous_event</span>, <span>async_finish</span><span>=</span><span>True</span>,
                         <span>allocate_on_comm_stream</span><span>=</span><span>True</span>)
    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>recv_x</span>, <span>recv_topk_idx</span>, <span>recv_topk_weights</span>, <span>num_recv_tokens_per_expert_list</span>, <span>handle</span>, <span>event</span>


<span>def</span> <span>dispatch_backward</span>(<span>grad_recv_x</span>: <span>torch</span>.<span>Tensor</span>, <span>grad_recv_topk_weights</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>, <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># The backward process of MoE dispatch is actually a combine</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>combined_grad_x</span>, <span>combined_grad_recv_topk_weights</span>, <span>event</span> <span>=</span> \
        <span>_buffer</span>.<span>combine</span>(<span>grad_recv_x</span>, <span>handle</span>, <span>topk_weights</span><span>=</span><span>grad_recv_topk_weights</span>, <span>async_finish</span><span>=</span><span>True</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>combined_grad_x</span>, <span>combined_grad_recv_topk_weights</span>, <span>event</span>


<span>def</span> <span>combine_forward</span>(<span>x</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># Do MoE combine</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>combined_x</span>, <span>_</span>, <span>event</span> <span>=</span> <span>_buffer</span>.<span>combine</span>(<span>x</span>, <span>handle</span>, <span>async_finish</span><span>=</span><span>True</span>, <span>previous_event</span><span>=</span><span>previous_event</span>,
                                           <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>combined_x</span>, <span>event</span>


<span>def</span> <span>combine_backward</span>(<span>grad_combined_x</span>: <span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]],
                     <span>handle</span>: <span>Tuple</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]], <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># The backward process of MoE combine is actually a dispatch</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>grad_x</span>, <span>_</span>, <span>_</span>, <span>_</span>, <span>_</span>, <span>event</span> <span>=</span> <span>_buffer</span>.<span>dispatch</span>(<span>grad_combined_x</span>, <span>handle</span><span>=</span><span>handle</span>, <span>async_finish</span><span>=</span><span>True</span>,
                                                 <span>previous_event</span><span>=</span><span>previous_event</span>,
                                                 <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>grad_x</span>, <span>event</span></pre></div>
<p dir="auto">Moreover, inside the dispatch function, we may not know how many tokens to receive for the current rank. So an implicit CPU wait for GPU received count signal will be involved, as the following figure shows.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepEP/blob/main/figures/normal.png"><img src="https://github.com/deepseek-ai/DeepEP/raw/main/figures/normal.png" alt="normal"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example use in inference decoding</h3><a id="user-content-example-use-in-inference-decoding" aria-label="Permalink: Example use in inference decoding" href="#example-use-in-inference-decoding"></a></p>
<p dir="auto">The low latency kernels can be used in the inference decoding phase as the below example code shows.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
import torch.distributed as dist
from typing import Tuple, Optional

from deep_ep import Buffer

# Communication buffer (will allocate at runtime)
# NOTES: there is no SM control API for the low-latency kernels
_buffer: Optional[Buffer] = None


# You may call this function at the framework initialization
def get_buffer(group: dist.ProcessGroup, num_max_dispatch_tokens_per_rank: int, hidden: int, num_experts: int) -> Buffer:
    # NOTES: the low-latency mode will consume much more space than the normal mode
    # So we recommend that `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256
    global _buffer
    num_rdma_bytes = Buffer.get_low_latency_rdma_size_hint(num_max_dispatch_tokens_per_rank, hidden, group.size(), num_experts)

    # Allocate a buffer if not existed or not enough buffer size
    if _buffer is None or _buffer.group != group or not _buffer.low_latency_mode or _buffer.num_rdma_bytes < num_rdma_bytes:
        # NOTES: for best performance, the QP number **must** be equal to the number of the local experts
        assert num_experts % group.size() == 0
        _buffer = Buffer(group, 0, num_rdma_bytes, low_latency_mode=True, num_qps_per_rank=num_experts // group.size())
    return _buffer


def low_latency_dispatch(hidden_states: torch.Tensor, topk_idx: torch.Tensor, num_max_dispatch_tokens_per_rank: int, num_experts: int):
    global _buffer

    # Do MoE dispatch, compatible with CUDA graph (but you may restore some buffer status once you replay)
    recv_hidden_states, recv_expert_count, handle, event, hook = \
        _buffer.low_latency_dispatch(hidden_states, topk_idx, num_max_dispatch_tokens_per_rank, num_experts,
                                     async_finish=False, return_recv_hook=True)

    # NOTES: the actual tensor will not be received only if you call `hook()`,
    # it is useful for double-batch overlapping, but **without any SM occupation**
    # If you don't want to overlap, please set `return_recv_hook=False`
    # Later, you can use our GEMM library to do the computation with this specific format
    return recv_hidden_states, recv_expert_count, handle, event, hook


def low_latency_combine(hidden_states: torch.Tensor,
                        topk_idx: torch.Tensor, topk_weights: torch.Tensor, handle: Tuple):
    global _buffer

    # Do MoE combine, compatible with CUDA graph (but you may restore some buffer status once you replay)
    combined_hidden_states, event_overlap, hook = \
        _buffer.low_latency_combine(hidden_states, topk_idx, topk_weights, handle,
                                    async_finish=False, return_recv_hook=True)

    # NOTES: the same behavior as described in the dispatch kernel
    return combined_hidden_states, event_overlap, hook"><pre><span>import</span> <span>torch</span>
<span>import</span> <span>torch</span>.<span>distributed</span> <span>as</span> <span>dist</span>
<span>from</span> <span>typing</span> <span>import</span> <span>Tuple</span>, <span>Optional</span>

<span>from</span> <span>deep_ep</span> <span>import</span> <span>Buffer</span>

<span># Communication buffer (will allocate at runtime)</span>
<span># NOTES: there is no SM control API for the low-latency kernels</span>
<span>_buffer</span>: <span>Optional</span>[<span>Buffer</span>] <span>=</span> <span>None</span>


<span># You may call this function at the framework initialization</span>
<span>def</span> <span>get_buffer</span>(<span>group</span>: <span>dist</span>.<span>ProcessGroup</span>, <span>num_max_dispatch_tokens_per_rank</span>: <span>int</span>, <span>hidden</span>: <span>int</span>, <span>num_experts</span>: <span>int</span>) <span>-&gt;</span> <span>Buffer</span>:
    <span># NOTES: the low-latency mode will consume much more space than the normal mode</span>
    <span># So we recommend that `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256</span>
    <span>global</span> <span>_buffer</span>
    <span>num_rdma_bytes</span> <span>=</span> <span>Buffer</span>.<span>get_low_latency_rdma_size_hint</span>(<span>num_max_dispatch_tokens_per_rank</span>, <span>hidden</span>, <span>group</span>.<span>size</span>(), <span>num_experts</span>)

    <span># Allocate a buffer if not existed or not enough buffer size</span>
    <span>if</span> <span>_buffer</span> <span>is</span> <span>None</span> <span>or</span> <span>_buffer</span>.<span>group</span> <span>!=</span> <span>group</span> <span>or</span> <span>not</span> <span>_buffer</span>.<span>low_latency_mode</span> <span>or</span> <span>_buffer</span>.<span>num_rdma_bytes</span> <span>&lt;</span> <span>num_rdma_bytes</span>:
        <span># NOTES: for best performance, the QP number **must** be equal to the number of the local experts</span>
        <span>assert</span> <span>num_experts</span> <span>%</span> <span>group</span>.<span>size</span>() <span>==</span> <span>0</span>
        <span>_buffer</span> <span>=</span> <span>Buffer</span>(<span>group</span>, <span>0</span>, <span>num_rdma_bytes</span>, <span>low_latency_mode</span><span>=</span><span>True</span>, <span>num_qps_per_rank</span><span>=</span><span>num_experts</span> <span>//</span> <span>group</span>.<span>size</span>())
    <span>return</span> <span>_buffer</span>


<span>def</span> <span>low_latency_dispatch</span>(<span>hidden_states</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>num_max_dispatch_tokens_per_rank</span>: <span>int</span>, <span>num_experts</span>: <span>int</span>):
    <span>global</span> <span>_buffer</span>

    <span># Do MoE dispatch, compatible with CUDA graph (but you may restore some buffer status once you replay)</span>
    <span>recv_hidden_states</span>, <span>recv_expert_count</span>, <span>handle</span>, <span>event</span>, <span>hook</span> <span>=</span> \
        <span>_buffer</span>.<span>low_latency_dispatch</span>(<span>hidden_states</span>, <span>topk_idx</span>, <span>num_max_dispatch_tokens_per_rank</span>, <span>num_experts</span>,
                                     <span>async_finish</span><span>=</span><span>False</span>, <span>return_recv_hook</span><span>=</span><span>True</span>)

    <span># NOTES: the actual tensor will not be received only if you call `hook()`,</span>
    <span># it is useful for double-batch overlapping, but **without any SM occupation**</span>
    <span># If you don't want to overlap, please set `return_recv_hook=False`</span>
    <span># Later, you can use our GEMM library to do the computation with this specific format</span>
    <span>return</span> <span>recv_hidden_states</span>, <span>recv_expert_count</span>, <span>handle</span>, <span>event</span>, <span>hook</span>


<span>def</span> <span>low_latency_combine</span>(<span>hidden_states</span>: <span>torch</span>.<span>Tensor</span>,
                        <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_weights</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>):
    <span>global</span> <span>_buffer</span>

    <span># Do MoE combine, compatible with CUDA graph (but you may restore some buffer status once you replay)</span>
    <span>combined_hidden_states</span>, <span>event_overlap</span>, <span>hook</span> <span>=</span> \
        <span>_buffer</span>.<span>low_latency_combine</span>(<span>hidden_states</span>, <span>topk_idx</span>, <span>topk_weights</span>, <span>handle</span>,
                                    <span>async_finish</span><span>=</span><span>False</span>, <span>return_recv_hook</span><span>=</span><span>True</span>)

    <span># NOTES: the same behavior as described in the dispatch kernel</span>
    <span>return</span> <span>combined_hidden_states</span>, <span>event_overlap</span>, <span>hook</span></pre></div>
<p dir="auto">For two micro-batch overlapping, you can refer to the following figure. With our receiving hook interface, the RDMA network traffics are happening in the background, without costing any GPU SMs from the computation part. But notice, the overlapped parts can be adjusted, i.e. the 4 parts of attention/dispatch/MoE/combine may not have the exact same execution time. You may adjust the stage settings according to your workload.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepEP/blob/main/figures/low-latency.png"><img src="https://github.com/deepseek-ai/DeepEP/raw/main/figures/low-latency.png" alt="low-latency"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notices</h2><a id="user-content-notices" aria-label="Permalink: Notices" href="#notices"></a></p>
<ul dir="auto">
<li>For extreme performance, we discover and use a behavior-out-of-doc PTX instruction: <code>ld.global.nc.L1::no_allocate.L2::256B</code>. This instruction will lead to an undefined behavior: accessing volatile GPU memory with non-coherent read-only PTX modifiers <code>.nc</code>. But the correctness is tested to be guaranteed with <code>.L1::no_allocate</code> on Hopper architectures, and performance will be much better. If you find kernels not working on some other platforms, you may add <code>DISABLE_AGGRESSIVE_PTX_INSTRS=1</code> to <code>setup.py</code> and disable this, or file an issue.</li>
<li>For better performance on your cluster, we recommend to run all the tests and use the best auto-tuned configuration. The default configurations are optimized on the DeepSeek's internal cluster.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This code repository is released under <a href="https://github.com/deepseek-ai/DeepEP/blob/main/LICENSE">the MIT License</a>, except for codes that reference NVSHMEM (including <code>csrc/kernels/ibgda_device.cuh</code> and <code>third-party/nvshmem.patch</code>), which are subject to <a href="https://docs.nvidia.com/nvshmem/api/sla.html" rel="nofollow">NVSHMEM SLA</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use this codebase, or otherwise found our work valuable, please cite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{deepep2025,
      title={DeepEP: an efficient expert-parallel communication library},
      author={Chenggang Zhao and Shangyan Zhou and Liyue Zhang and Chengqi Deng and Zhean Xu and Yuxuan Liu and Kuai Yu and Jiashi Li and Liang Zhao},
      year={2025},
      publisher = {GitHub},
      howpublished = {\url{https://github.com/deepseek-ai/DeepEP}},
}"><pre><span>@misc</span>{<span>deepep2025</span>,
      <span>title</span>=<span><span>{</span>DeepEP: an efficient expert-parallel communication library<span>}</span></span>,
      <span>author</span>=<span><span>{</span>Chenggang Zhao and Shangyan Zhou and Liyue Zhang and Chengqi Deng and Zhean Xu and Yuxuan Liu and Kuai Yu and Jiashi Li and Liang Zhao<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
      <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
      <span>howpublished</span> = <span><span>{</span>\url{https://github.com/deepseek-ai/DeepEP}<span>}</span></span>,
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DigiCert: Threat of legal action to stifle Bugzilla discourse (453 pts)]]></title>
            <link>https://bugzilla.mozilla.org/show_bug.cgi?id=1950144</link>
            <guid>43167087</guid>
            <pubDate>Tue, 25 Feb 2025 01:40:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1950144">https://bugzilla.mozilla.org/show_bug.cgi?id=1950144</a>, See on <a href="https://news.ycombinator.com/item?id=43167087">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">

 


<main id="bugzilla-body" tabindex="-1">



<div id="main-inner">










<div id="summary-container">



  
    <p><span id="field-value-status_summary">
      <span data-status="open">Open</span>
      <span id="field-value-bug_id">
        <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1950144">Bug 1950144</a>
      </span>
      <span>
        <span>Opened <span title="2025-02-24 08:36 PST" data-time="1740414973">13 hours ago</span></span>
          <span>Updated <span title="2025-02-24 20:23 PST" data-time="1740457405">1 hour ago</span></span>
      </span>
        </span>
    </p>

  
</div>


<div id="module-categories">
        <p><span id="field-value-component">
      <div>
        <p><span id="component-name" tabindex="0" role="button" aria-haspopup="menu" aria-controls="component-info">CA Certificate Root Program
          
        </span></p>
      </div>
        </span>
    </p></div>






































<meta name="firefox-versions" content="{&quot;FIREFOX_AURORA&quot;:&quot;&quot;,&quot;FIREFOX_DEVEDITION&quot;:&quot;136.0b9&quot;,&quot;FIREFOX_ESR&quot;:&quot;128.7.0esr&quot;,&quot;FIREFOX_ESR115&quot;:&quot;115.20.0esr&quot;,&quot;FIREFOX_ESR_NEXT&quot;:&quot;&quot;,&quot;FIREFOX_NIGHTLY&quot;:&quot;137.0a1&quot;,&quot;LAST_MERGE_DATE&quot;:&quot;2025-02-03&quot;,&quot;LAST_RELEASE_DATE&quot;:&quot;2025-02-04&quot;,&quot;LAST_SOFTFREEZE_DATE&quot;:&quot;2025-01-30&quot;,&quot;LAST_STRINGFREEZE_DATE&quot;:&quot;2025-01-31&quot;,&quot;LATEST_FIREFOX_DEVEL_VERSION&quot;:&quot;136.0b9&quot;,&quot;LATEST_FIREFOX_OLDER_VERSION&quot;:&quot;3.6.28&quot;,&quot;LATEST_FIREFOX_RELEASED_DEVEL_VERSION&quot;:&quot;136.0b9&quot;,&quot;LATEST_FIREFOX_VERSION&quot;:&quot;135.0.1&quot;,&quot;NEXT_MERGE_DATE&quot;:&quot;2025-03-03&quot;,&quot;NEXT_RELEASE_DATE&quot;:&quot;2025-03-04&quot;,&quot;NEXT_SOFTFREEZE_DATE&quot;:&quot;2025-02-27&quot;,&quot;NEXT_STRINGFREEZE_DATE&quot;:&quot;2025-02-28&quot;}">



<div id="c0" data-comment-id="17364853"><p>In <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1910322#c74" title="RESOLVED FIXED - DigiCert: Random value in CNAME without underscore prefix">bug 1910322 comment 74</a> DigiCert wrote,</p>
<blockquote>
<p>“We have not used a legal team as a shield against accountability.”</p>
</blockquote>
<p>Contrary to this statement, I received a letter from DigiCert’s lawyers, Wilson Sonsini, regarding posts made by Sectigo’s Chief Compliance Officer in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1910322" title="RESOLVED FIXED - DigiCert: Random value in CNAME without underscore prefix">bug 1910322</a>.  The upshot of the letter was that DigiCert expected Sectigo to “ensure that Mr. Callan’s statements do not continue and will not be repeated by any other member of Sectigo’s organization.”</p>
<p>I’m Brian Holland, General Counsel for Sectigo, and this is my first time posting on Bugzilla.  I’m posting because at Sectigo we believe that the WebPKI is best served by open, transparent, and honest debate about issues that impact our community.  Attempts to shut down these conversations, through lawyers or otherwise, are harmful to our collective core mission.</p>
<p>In its opening passages, this letter reads (emphasis mine),</p>
<blockquote>
<p>We ask for your prompt cooperation and assistance in taking corrective action and <strong>forcing Mr. Callan to cease his disparaging public statements. We hope your assistance in this matter will render unnecessary legal action by DigiCert against Sectigo.</strong></p>
</blockquote>
<p>After three pages of detail about specific Bugzilla posts and references to the Lanham Act, deceptive trade practices, corporate disparagement, and tortious interference, the letter (the full letter is included as an attachment to this bug) goes on to say (emphasis mine):</p>
<blockquote>
<p>At this point, we are bringing this situation to your attention on behalf of DigiCert because we are hopeful that Mr. Callan’s actions were the actions of one individual and were not part of an organized plan or institutional practice. We also hope that, upon receiving this information, Sectigo will recognize the impropriety of Mr. Callan’s statements and the substantial public, industry, and browser scrutiny and legal risk such statements would prompt if they were to continue. To that end, we expect that Sectigo will investigate this incident promptly and take the appropriate corrective actions, confirm that this situation was not part of an institutional practice, and <strong>ensure that Mr. Callan’s statements do not continue and will not be repeated by any other member of Sectigo’s organization. We hope we can resolve this situation as soon as possible before DigiCert is compelled to seek legal action.</strong></p>
</blockquote>
<p>On December 10, 2024 I sent this response in email to my contact at Wilson Sonsini:</p>
<blockquote>
<p>I have reviewed your letter and the Bugzilla thread referenced therein.  In that letter, you suggest that DigiCert has various legal claims against Sectigo and/or its COO [sic], Tim Callan, for what you call “false and misleading statements about DigiCert” made on the Bugzilla forum.  We strongly disagree.  The statements you point to are questions and/or statements of opinion that are not actionable statements of fact.  Moreover, those comments were made with the intent of facilitating discussion and debate about important questions of first impression for our industry.  They were made by Tim Callan in good faith, are fully protected by the First Amendment, and cannot, as a matter of law, form the basis for any of the causes of action mentioned in your letter.</p>
</blockquote>
<blockquote>
<p>As you are aware, the PKI community is a self-regulating group that, as set out in the bylaws of the Certificate Authority Browser Forum, works “closely together in defining the guidelines and means of implementation for best practices as a way of providing a heightened security for Internet transactions and creating a more intuitive method of displaying secure sites to Internet users.”  For the community to self-regulate, there needs to be open, uninhibited, and robust discussion and debate about best practices in the industry.  Any litigation threats that chill or stifle such debate undermine the self-regulatory system that has worked so well for the industry.</p>
</blockquote>
<blockquote>
<p>Certificate Authorities post incident reports on Bugzilla to “provide lessons learned and transparency about the steps the CA Owner takes to address the immediate issue and prevent future issues.” As the Common CA Database goes on to state “incident reports help the Web PKI ecosystem as a whole because they promote continuous improvement, information sharing, and highlight opportunities to define and adopt improved practices, policies, and controls” of all parties.</p>
</blockquote>
<blockquote>
<p>The TRO involved in this incident report, as one Bugzilla commenter noted, is “an unprecedented event in the WebPKI, and . . . if allowed to proliferate, it would potentially be used by subscribers en masse to do an end-run around important technical security controls.”</p>
</blockquote>
<blockquote>
<p>The PKI Community has never considered how it should respond to TROs and now needs to do so. Understanding the situation faced by your client and why it made certain decisions is important to improving the WebPKI ecosystem. This is why Mr. Callan, and many others, have been asking questions – some of which have been critical questions designed to achieve a consensus as to how best handle situations like this in the future.  In any such discussion, there will be differences of opinion, but open, uninhibited, robust, and transparent discussion is essential for the industry to learn how to best move forward.</p>
</blockquote>
<blockquote>
<p>I hope that your client will, on deeper reflection, realize that as a leader in the PKI Community, it should be driving, rather than stifling, discussion of this topic.  Your client’s threat of litigation is, in our view, both misguided and without merit.  We will strive to be respectful in our tone, but neither Mr. Callan nor Sectigo will be silenced or prevented from asking critical questions and/or engaging in critical discussion about issues of substantial concern to the public and the industry.</p>
</blockquote>
<p>We find the threat of legal action to stifle scrutiny and discussion of public CA practices to be deeply troubling and entirely at odds with the transparent, blameless post-mortem culture that the CCADB incident report guidelines expect CAs to embrace.  Even for a company like Sectigo, the threat of a lawsuit from a well-resourced organization like DigiCert is worrisome, regardless of our confidence that Mr. Callan’s speech was proper, legally protected, and in the best interest of the WebPKI.  Another party challenging DigiCert’s behavior, faced with this same threat, might choose simply to stop asking uncomfortable questions.</p>
<p>No CA should be allowed to intimidate its critics into silence. This would irreparably damage the integrity and quality of the WebPKI.</p>
<p>I am sharing this incident to bring attention to DigiCert’s actions and allow the community to evaluate this approach. What began as a discussion of the threat posed by certificate subscribers using the legal system to circumvent WebPKI security controls needs, in my opinion, to be broadened.</p>
</div><div id="a3699_1689"><p>Component: CA Certificate Compliance → CA Certificate Root Program</p></div>







<dialog id="att-overlay" aria-labelledby="att-overlay-title" data-attachment-count="1">
  
</dialog>

</div> 
</main> 
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everyone at NSF overseeing the Platforms for Wireless Experimentation is gone (382 pts)]]></title>
            <link>https://discuss.systems/@ricci/114059690609284323</link>
            <guid>43166830</guid>
            <pubDate>Tue, 25 Feb 2025 00:59:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discuss.systems/@ricci/114059690609284323">https://discuss.systems/@ricci/114059690609284323</a>, See on <a href="https://news.ycombinator.com/item?id=43166830">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[It's still worth blogging in the age of AI (267 pts)]]></title>
            <link>https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai</link>
            <guid>43166761</guid>
            <pubDate>Tue, 25 Feb 2025 00:46:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai">https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai</a>, See on <a href="https://news.ycombinator.com/item?id=43166761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            

            <div data-current-dropdown="" hx-on="click:
                    if (event.target.closest('.dropdown')) {
                        let targetId = event.target.closest('.dropdown').dataset.target;
                        this.dataset.currentDropdown = (this.dataset.currentDropdown === targetId) ? '' : targetId;
                        event.stopPropagation();
                    }">
                    
                        <p>
                            Archives <span></span>
                        </p>
                    
                    
                        <p>
                            Categories <span></span>
                        </p>
                    
                    <p>
                        Blogroll <span></span>
                    </p>
                </div>

            

    

    

    <p>My post about blogging as
<a href="https://www.gilesthomas.com/2025/02/20250223-til-deep-dive-posts">writing the tutorial that you wished you'd found</a>
really took off
<a href="https://news.ycombinator.com/item?id=43154666">on Hacker News</a>.  There were
a lot of excellent comments, but one thing kept coming up: what's the point
in blogging if people are using ChatGPT, Claude and DeepSeek to spoon-feed them
answers?  Who, apart from the AIs, will read what you write?</p>

<p>I was asking myself the same question when I started blogging semi-regularly again
last year, and this post is an attempt to summarise why I decided that it was worthwhile.
The TL;DR: <strong>blogging isn't just about being read -- it's about learning and thinking, and
having a durable proof that you can do both.</strong></p>


    
        <p>Let's start off by summarising the two big reasons to blog about what you've learned,
as you learn:</p>

<ul>
<li>It helps you make your newly-gained knowledge concrete.</li>
<li>It will help other people in the future -- they might be looking for the information
you blogged about, and find it on your blog.</li>
</ul>

<p>When we're thinking about AI, it's only the second one that matters; you'll learn better
by writing whether or not other people or LLMs read it.  But in terms of
helping other people, these days
you might publish your hard-earned learnings on <a href="https://www.gilesthomas.com/2021/03/fun-with-network-namespaces">Linux Network Namespaces</a>,
but when, the next day, someone wants to find out how to use them, they ask
ChatGPT, it does a search, finds your page, ingests it, and presents the results
as its own, perhaps mashed up with some scraps from elsewhere.  Sure, your site is probably
linked in the "references" section in the response, but frankly, no-one ever looks at that.
What's worse, within the next six months your site is likely to be sucked into the
AIs' next training run, and after that you won't even get a reference.</p>

<p>Now if the "solving other people's problems" aspect of blogging was purely altruistic,
that wouldn't matter a jot.  But of course it's not, there are a bunch of other reasons.
Three that come to mind:</p>

<ol>
<li>Making a name for yourself.</li>
<li>The sheer dopamine hit of knowing that other people like what you've done --
a higher-effort version of getting an upvote or like on social media.</li>
<li>Building a portfolio of writing you can point to.</li>
</ol>

<p>Let's take those in turn.</p>

<p>If you want to blog to make a name for yourself, then you're going to have a hard
time.  Here's an example: if you're not a regular reader of this blog, where do
I (as in, the author of this post) live?  What is my day job?  No cheating and
clicking on the "About" link above, please.</p>

<p>If you knew the answer, you're one of a rare few.  Yesterday there
were about 35,000 visits to this site thanks to that HN link, and fewer than 300 hits
on the "About" page.  This is normal!  If you write a blog post, then even if people
find it interesting, they'll come, read it, hopefully think that it was worth their
time, and then move on.  That is how it should be, there's no need for someone to
become fascinated with your life just because you said something useful once -- and
that's a good thing, no-one wants a stalker.</p>

<p>Even if you churn out banger posts again and again, as a pure blogger you're not going
to build up a "personal brand" that's worth much.</p>

<p>Think about the well-known bloggers you read: they’re famous because
they did something else
important.  They started a major open-source project, or a company, or invented something.
They give regular talks at conferences.  They write successful science fiction.
Or something else.</p>

<p>So, I don't think you can make a name for yourself by blogging alone, and if you
are blogging with that as your goal, I fear you're going to be disappointed.</p>

<p>The dopamine hit is definitely more of a thing.  When people comment on my
posts, I get a nice warm glow.  And when last night, just before I went to bed, I saw that
my previous post was #1 on the front page of HN, I took a screenshot and posted it
to my "Fellow Geeks" WhatsApp group with the caption "w00t!".</p>

<p>But those moments
are rare, and I don't really think AI will make them rarer.
Blogging can sometimes feel like you're
shouting into the void -- most posts get no engagement, and that has been true since
I started back in 2006.  You might have 500 loyal
readers, or none -- there's no way to tell.</p>

<p>I think that all I can say regarding that is to echo what serviceberry
<a href="https://news.ycombinator.com/item?id=43155587">said on HN</a> (bold mine):</p>

<blockquote>
  <p>The corollary is that if you find that post, <strong>say something</strong>. Drop the author a
  note, leave a comment. No one else does. For every YT celebrity, there are
  thousands of people posting good content on the internet and not knowing if
  it's being seen or appreciated by anyone.</p>
</blockquote>

<p>...and maybe suggest that we all occasionally check the references in our helpful AI-generated
responses and drop a line to the authors to say "thanks"!</p>

<p>But let's finish with the last one, which is more positive.  I said that you will
be vanishingly unlikely to make a name for yourself with blogging on its own.  But that
doesn't mean it's pointless from a career perspective.  You're building up a portfolio
of writing about topics that interest you.  Imagine you're in a job interview and
are asked about X.  You reply with the details you know, and add
"but I blogged about that in detail a while back, shall I send you a link later?"
Or if you're aiming to close a contract with a potential consulting client in a
particular area -- wouldn't it be useful to send them a list of links showing your
thoughts on aspects of exactly that topic?</p>

<p>Your GitHub profile shows your contributions to open source and lets people know
how well you can code.  But your blog shows your contributions to knowledge, and
shows how well you can think.  That's valuable!</p>

<p>It's time to wrap this up.  Blogging is valuable because it helps you learn, because it
helps others solve problems, because you get a rare buzz when you realise that yes,
people are reading this stuff, and because you're building a portfolio of writing to show
your skills.  The only one of those that I believe AI might harm is the buzz of
engagement, and that's so rare for most blogs that I don't think it's worth worrying about.</p>

<p>And after all -- if the AI doom scenario does come true, at least as someone whose
thoughts have been regularly published on the Internet, you'll be part of the
paperclip maximisers' training set, so they'll remember you in a sense.  So
there's that.</p>

    

    
        
    

    



            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Clean Code vs. A Philosophy Of Software Design (321 pts)]]></title>
            <link>https://github.com/johnousterhout/aposd-vs-clean-code/blob/main/README.md</link>
            <guid>43166362</guid>
            <pubDate>Mon, 24 Feb 2025 23:52:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/johnousterhout/aposd-vs-clean-code/blob/main/README.md">https://github.com/johnousterhout/aposd-vs-clean-code/blob/main/README.md</a>, See on <a href="https://news.ycombinator.com/item?id=43166362">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true" aria-labelledby="file-name-id-wide file-name-id-mobile"><article itemprop="text"><p dir="auto"><em>(This document is the result of a series of discussions, some online and
some in person, held between Robert "Uncle Bob" Martin and John Ousterhout between
September, 2024 and February, 2025. If you would like to comment on anything
in this discussion, we recommend that you do so on the <a href="https://groups.google.com/g/software-design-book" rel="nofollow">Google group
associated with APOSD</a>)</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introductions</h2><a id="user-content-introductions" aria-label="Permalink: Introductions" href="#introductions"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Hi (Uncle) Bob! You and I have each written books on software design.
We agree on some things, but there are some pretty big differences of
opinion between my recent book <em>A Philosophy of Software Design</em>
(hereafter "APOSD") and your classic book <em>Clean Code</em>. Thanks for
agreeing to discuss those differences here.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">My pleasure John.  Before we begin let me say that I've carefully read through your book and I found it very enjoyable, and full of valuable insights.  There are some things I disagree with you on, such as TDD, and Abstraction-First incrementalism, but overall I enjoyed it a lot.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I'd like to discuss three topics with you: method length, comments,
and test-driven development. But before getting into these,
let's start by comparing overall philosophies. When you hear about a
new idea related to software design, how do you decide whether or not
to endorse that idea?</p>
<p dir="auto">I'll go first. For me, the fundamental goal of software design is
to make it easy to understand and modify the system. I use the term
"complexity" to refer to things that make it hard to understand and
modify a system. The most important contributors
to complexity relate to information:</p>
<ul dir="auto">
<li>How much information must a developer have in their head in order to carry out a task?</li>
<li>How accessible and obvious is the information that the developer needs?</li>
</ul>
<p dir="auto">The more information a developer needs to have, the harder it will be
for them to work on the system. Things get even worse if the required
information isn't obvious. The worst case is when there is a crucial
piece of information hidden in some far-away piece of code
that the developer has never heard of.</p>
<p dir="auto">When I'm evaluating an idea related to software design, I ask whether
it will reduce complexity. This usually means either reducing the amount
of information a developer has to know, or making the required information
more obvious.</p>
<p dir="auto">Now over to you: are there general principles that you use when deciding
which ideas to endorse?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I agree with your approach. A discipline or technique should make the job of programmers easier. I would add that the programmer we want to help most is not the author.  The programmer whose job we want to make easier is the programmer who must read and understand the code written by others (or by themself a week later).  Programmers spend far more hours reading code than writing code, so the activity we want to ease is that of reading.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Method Length</h2><a id="user-content-method-length" aria-label="Permalink: Method Length" href="#method-length"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Our first area of disagreement is method length.
On page 34 of <em>Clean Code</em> you say "The first rule of functions is that
they should be small. The second rule of functions is that
<em>they should be smaller than that</em>." Later on, you say "Functions
should hardly ever be 20 lines long" and suggest that functions
should be "just two, three, or four lines long". On page 35, you
say "Blocks within <code>if</code> statements, <code>else</code> statements, <code>while</code> statements,
and so on should be one line long. Probably that line should be a function
call." I couldn't find anything in <em>Clean Code</em> to suggest that a function
could ever be too short.</p>
<p dir="auto">I agree that dividing up code into relatively small units ("modular design")
is one of the most important ways to reduce the amount of information a
programmer has to keep in their mind at once. The idea, of course, is to take a
complex chunk of functionality and encapsulate it in a separate method
with a simple interface. Developers can then harness the functionality
of the method (or read code that invokes the method) without learning
the details of how the method is implemented; they only need to learn its
interface. The best methods are those that provide a lot of functionality
but have a very simple interface: they replace a large cognitive load
(reading the detailed implementation) with a much smaller
cognitive load (learning the interface). I call these methods "deep".</p>
<p dir="auto">However, like most ideas in software design, decomposition can be taken too far.
As methods get smaller and smaller there is less and less
benefit to further subdivision.
The amount of functionality hidden behind each interface
drops, while the interfaces often become more complex.
I call these interfaces "shallow": they don't help much in terms of
reducing what the programmer needs to know. Eventually, the point is
reached where someone using the method needs
to understand every aspect of its implementation. Such methods
are usually pointless.</p>
<p dir="auto">Another problem with decomposing too far is that it tends to
result in <em>entanglement</em>. Two methods
are entangled (or "conjoined" in APOSD terminology) if, in order to
understand how one of them works internally, you also need to read the
code of the other. If you've ever found yourself flipping back and forth
between the implementations of two methods as you read code, that's a
red flag that the methods might be entangled. Entangled methods
are hard to read because the information you need to have in your head
at once isn't all in the same place. Entangled methods can usually
be improved by combining them so that all the code is in one place.</p>
<p dir="auto">The advice in <em>Clean Code</em> on method length is so extreme that it encourages
programmers to create teeny-tiny methods that suffer from both shallow
interfaces and entanglement.  Setting arbitrary numerical limits such
as 2-4 lines in a method and a single line in the body of an
<code>if</code> or <code>while</code> statement exacerbates this problem.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">While I do strongly recommend very short functions, I don't think it's fair to say that the book sets arbitrary numerical limits. The 2-4 line functions that you referred to on page 34 were part of the <em>Sparkle</em> applet that Kent Beck and I wrote together in 1999 as an exercise for learning TDD. I thought it was remarkable that most of the functions in that applet were 2-4 lines long because it was a Swing program; and Swing programs tend to have very long methods.</p>
<p dir="auto">As for setting limits, on page 13 I make clear that although the recommendations in the book have worked well for me and the other authors, they might not work for everyone.  I claimed no final authority, nor even any absolute "rightness". They are offered for consideration.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I think these problems will be easiest to understand if we look at
specific code examples. But before we do that, let me ask you, Bob:
do you believe that it's possible for code to be over-decomposed, or
is smaller always better? And, if you believe that over-decomposition
is possible, how do you recognize when it has occurred?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">It is certainly possible to over-decompose code.  Here's an example:</p>
<div data-snippet-clipboard-copy-content="void doSomething() {doTheThing()} // over-decomposed."><pre><code>void doSomething() {doTheThing()} // over-decomposed.
</code></pre></div>
<p dir="auto">The strategy that I use for deciding how far to take decomposition is the old rule that a method should do "<em>One Thing</em>".  If I can <em>meaningfully</em> extract one method from another, then the original method did more than one thing.  "Meaningfully" means that the extracted functionality can be given a descriptive name; and that it does less than the original method.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Unfortunately the One Thing approach will lead to over-decompositon:</p>
<ol dir="auto">
<li>
<p dir="auto">The term "one thing" is vague and easy to abuse. For example, if a method has two lines of code, isn't it doing two things?</p>
</li>
<li>
<p dir="auto">You haven't provided any useful guardrails to prevent over-decomposition. The example you gave is too extreme to be useful, and the "can it be named" qualification doesn't help: anything can be named.</p>
</li>
<li>
<p dir="auto">The One Thing approach is simply wrong in many cases. If two things are closely related, it might well make sense to implement them in a single method. For example, any thread-safe method will first have to acquire a lock, then carry out its function. These are two "things", but they belong in the same method.</p>
</li>
</ol>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Let me tackle the last thing first.  You suggested that locking the thread, and preforming a critical section should be together in the same method.  However, I would be tempted to separate the locking from the critical section.</p>
<div data-snippet-clipboard-copy-content="void concurrentOperation() {
	lock()
	criticalSection();
	unlock()
}"><pre><code>void concurrentOperation() {
	lock()
	criticalSection();
	unlock()
}
</code></pre></div>
<p dir="auto">This decouples the critical section from the lock and allows it to be called at times when locking isn't necessary (e.g. in single thread mode) or when the a lock has already been set by someone else.</p>
<p dir="auto">Now, on to the "ease of abuse" argument.  I don't consider that to be a significant concern. <code>If</code> statements are easy to abuse.  <code>Switch</code> statements are easy to abuse.  Assignment statements are easy to abuse.  The fact that something is easy to abuse does not mean that it should be avoided or suppressed.  It simply means people should take appropriate care. There will always be this thing called: <em>judgment</em>.</p>
<p dir="auto">So when faced with this snippet of code in a larger method:</p>
<div data-snippet-clipboard-copy-content="...
amountOwed=0;
totalPoints=0;
..."><pre><code>...
amountOwed=0;
totalPoints=0;
...
</code></pre></div>
<p dir="auto">It would be poor judgement to extract them as follows, because the extraction is not meaningful.  The implementation is not more deeply detailed than the interface.</p>
<div data-snippet-clipboard-copy-content="void clearAmountOwed() {
  amountOwed=0;
}

void clearTotalPoints() {
  totalPoints=0;
}"><pre><code>void clearAmountOwed() {
  amountOwed=0;
}

void clearTotalPoints() {
  totalPoints=0;
}
</code></pre></div>
<p dir="auto">However it may be good judgement to extract them as follows because the interface is abstract, and the implemention has deeper detail.</p>
<div data-snippet-clipboard-copy-content="void clearTotals() {
	amountOwed=0;
	totalPoints=0;
}"><pre><code>void clearTotals() {
	amountOwed=0;
	totalPoints=0;
}
</code></pre></div>
<p dir="auto">The latter has a nice descriptive name that is abstract enough to be meaningful without being redundant.  And the two lines together are strongly related so as to qualify for doing <em>one thing</em>: initialization.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Of course anything can be abused. But the best approaches to design
encourage people to do things the right way and discourage abuse.
Unfortunately, the One Thing Rule encourages abuse for the reasons I
gave above.</p>
<p dir="auto">And of course software designers will need to use judgment: it isn't
possible to provide precise recipes for software design.
But good judgment requires principles and guidance. The
<em>Clean Code</em> arguments about decomposition, including the One Thing
Rule, are one-sided. They give strong, concrete, quantitative
advice about when to chop things up, with virtually no guidance for
how to tell you've gone too far. All I could find is a 2-sentence
example on page 36 about Listing 3-3 (which is pretty trivial),
buried in the middle of exhortations to "chop, chop, chop".</p>
<p dir="auto">One of the reasons I use the deep/shallow characterization is that it
captures both sides of the tradeoff; it will tell you when a decomposition
is good and also when decomposition makes things worse.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">You make a good point that I don't talk much, in the book, about how to make the judgement call.  Back in 2008 my concern was breaking the habit of the very large functions that were common in those early days of the web.  I have been more balanced in the 2d ed.</p>
<p dir="auto">Still, if I must err, I'd rather err on the side of decomposition.  There is value in considering, and visualizing decompositions.  They can always be inlined if we judge them to have gone too far.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Coming back to your <code>clearTotals</code> example:</p>
<ul dir="auto">
<li>The <code>clearTotals</code> method seems to contradict the One Thing Rule: the
variables <code>amountOwed</code> and <code>totalPoints</code> don't seem particularly related, so
initializing them both is doing two things, no? You say that both
statements are performing initialization, which makes it just one thing
(initialization). Does that mean it would also be okay to have a single
method that initializes two completely independent objects with nothing in
common? I suspect not. It feels like you are struggling to create a clean
framework for applying the One Thing Rule; that makes me think it isn't
a good rule.</li>
<li>Without seeing more context I'm skeptical that the <code>clearTotals</code>
method makes sense.</li>
</ul>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I hope you agree that between these two examples, the former is a bit better.</p>
<div data-snippet-clipboard-copy-content="public String makeStatement() {
  clearTotals();
  return makeHeader() + makeRentalDetails() + makeFooter();
}"><pre><code>public String makeStatement() {
  clearTotals();
  return makeHeader() + makeRentalDetails() + makeFooter();
}
</code></pre></div>
<hr>
<div data-snippet-clipboard-copy-content="public String makeStatement() {
  amountOwed=0;
  totalPoints=0;
  return makeHeader() + makeRentalDetails() + makeFooter();
}"><pre><code>public String makeStatement() {
  amountOwed=0;
  totalPoints=0;
  return makeHeader() + makeRentalDetails() + makeFooter();
}
</code></pre></div>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Well, actually, no. The second example is completely clear and obvious:
I don't see anything to be gained by splitting it up.</p>
<p dir="auto"><strong>SPOCK (a.k.a UB):</strong></p>
<p dir="auto">Fascinating.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I think it will be easier to clarify our differences if we consider
a nontrivial code example. Let's look at the <code>PrimeGenerator</code> class from
<em>Clean Code</em>, which is Listing 10-8 on pages 145-146. This Java class
generates the first N prime numbers:</p>
<div data-snippet-clipboard-copy-content="package literatePrimes;

import java.util.ArrayList;

public class PrimeGenerator {
  private static int[] primes;
  private static ArrayList<Integer> multiplesOfPrimeFactors;

  protected static int[] generate(int n) {
    primes = new int[n];
    multiplesOfPrimeFactors = new ArrayList<Integer>();
    set2AsFirstPrime();
    checkOddNumbersForSubsequentPrimes();
    return primes;
  }

  private static void set2AsFirstPrime() {
    primes[0] = 2;
    multiplesOfPrimeFactors.add(2);
  }

  private static void checkOddNumbersForSubsequentPrimes() {
    int primeIndex = 1;
    for (int candidate = 3;
         primeIndex < primes.length;
         candidate += 2) {
      if (isPrime(candidate))
        primes[primeIndex++] = candidate;
    }
  }

  private static boolean isPrime(int candidate) {
    if (isLeastRelevantMultipleOfLargerPrimeFactor(candidate)) {
      multiplesOfPrimeFactors.add(candidate);
      return false;
    }
    return isNotMultipleOfAnyPreviousPrimeFactor(candidate);
  }

  private static boolean
  isLeastRelevantMultipleOfLargerPrimeFactor(int candidate) {
    int nextLargerPrimeFactor = primes[multiplesOfPrimeFactors.size()];
    int leastRelevantMultiple = nextLargerPrimeFactor * nextLargerPrimeFactor;
    return candidate == leastRelevantMultiple;
  }

  private static boolean
  isNotMultipleOfAnyPreviousPrimeFactor(int candidate) {
    for (int n = 1; n < multiplesOfPrimeFactors.size(); n++) {
      if (isMultipleOfNthPrimeFactor(candidate, n))
        return false;
    }
    return true;
  }

  private static boolean
  isMultipleOfNthPrimeFactor(int candidate, int n) {
    return candidate ==
      smallestOddNthMultipleNotLessThanCandidate(candidate, n);
  }

  private static int
  smallestOddNthMultipleNotLessThanCandidate(int candidate, int n) {
    int multiple = multiplesOfPrimeFactors.get(n);
    while (multiple < candidate)
      multiple += 2 * primes[n];
    multiplesOfPrimeFactors.set(n, multiple);
    return multiple;
  }
}"><pre><code>package literatePrimes;

import java.util.ArrayList;

public class PrimeGenerator {
  private static int[] primes;
  private static ArrayList&lt;Integer&gt; multiplesOfPrimeFactors;

  protected static int[] generate(int n) {
    primes = new int[n];
    multiplesOfPrimeFactors = new ArrayList&lt;Integer&gt;();
    set2AsFirstPrime();
    checkOddNumbersForSubsequentPrimes();
    return primes;
  }

  private static void set2AsFirstPrime() {
    primes[0] = 2;
    multiplesOfPrimeFactors.add(2);
  }

  private static void checkOddNumbersForSubsequentPrimes() {
    int primeIndex = 1;
    for (int candidate = 3;
         primeIndex &lt; primes.length;
         candidate += 2) {
      if (isPrime(candidate))
        primes[primeIndex++] = candidate;
    }
  }

  private static boolean isPrime(int candidate) {
    if (isLeastRelevantMultipleOfLargerPrimeFactor(candidate)) {
      multiplesOfPrimeFactors.add(candidate);
      return false;
    }
    return isNotMultipleOfAnyPreviousPrimeFactor(candidate);
  }

  private static boolean
  isLeastRelevantMultipleOfLargerPrimeFactor(int candidate) {
    int nextLargerPrimeFactor = primes[multiplesOfPrimeFactors.size()];
    int leastRelevantMultiple = nextLargerPrimeFactor * nextLargerPrimeFactor;
    return candidate == leastRelevantMultiple;
  }

  private static boolean
  isNotMultipleOfAnyPreviousPrimeFactor(int candidate) {
    for (int n = 1; n &lt; multiplesOfPrimeFactors.size(); n++) {
      if (isMultipleOfNthPrimeFactor(candidate, n))
        return false;
    }
    return true;
  }

  private static boolean
  isMultipleOfNthPrimeFactor(int candidate, int n) {
    return candidate ==
      smallestOddNthMultipleNotLessThanCandidate(candidate, n);
  }

  private static int
  smallestOddNthMultipleNotLessThanCandidate(int candidate, int n) {
    int multiple = multiplesOfPrimeFactors.get(n);
    while (multiple &lt; candidate)
      multiple += 2 * primes[n];
    multiplesOfPrimeFactors.set(n, multiple);
    return multiple;
  }
}
</code></pre></div>
<p dir="auto">Before we dive into this code, I'd encourage everyone reading
this article to take time to read over the code and draw your own conclusions
about it. Did you find the code easy to understand? If so, why? If not, what
makes it complex?</p>
<p dir="auto">Also, Bob, can you confirm that you stand by this code (i.e. the code
properly exemplifies the design philosophy of <em>Clean Code</em> and this
is the way you believe the code should appear if it were used in
production)?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Ah, yes.  The <code>PrimeGenerator</code>.  This code comes from the 1982 paper on <a href="https://www.cs.tufts.edu/~nr/cs257/archive/literate-programming/01-knuth-lp.pdf" rel="nofollow"><em>Literate Programming</em></a> written by Donald Knuth.  The program was originally written in Pascal, and was automatically generated by Knuth's WEB system into a single very large method which I translated into Java.</p>
<p dir="auto">Of course this code was never meant for production.  Both Knuth and I used it as a pedagogical example.  In <em>Clean Code</em> it appears in a chapter named <em>Classes</em>.  The lesson of the chapter is that a very large method will often contain many different sections of code that are better decomposed into independent classes.</p>
<p dir="auto">In the chapter I extracted three classes from that function: <code>PrimePrinter</code>, <code>RowColumnPagePrinter</code> and <code>PrimeGenerator</code>.</p>
<p dir="auto">One of those extracted classes was the <code>PrimeGenerator</code>. It had the following code (which I did not publish in the book.)  The variable names and the overall structure are Knuth's.</p>
<div data-snippet-clipboard-copy-content="public class PrimeGenerator {
  protected static int[] generate(int n) {
    int[] p = new int[n];
    ArrayList<Integer> mult = new ArrayList<Integer>();
    p[0] = 2;
    mult.add(2);
    int k = 1;
    for (int j = 3; k < p.length; j += 2) {
      boolean jprime = false;
      int ord = mult.size();
      int square = p[ord] * p[ord];
      if (j == square) {
        mult.add(j);
      } else {
        jprime=true;
        for (int mi = 1; mi < ord; mi++) {
          int m = mult.get(mi);
          while (m < j)
            m += 2 * p[mi];
          mult.set(mi, m);
          if (j == m) {
            jprime = false;
            break;
          }
        }
      }
      if (jprime)
        p[k++] = j;
    }
    return p;
  }
}"><pre><code>public class PrimeGenerator {
  protected static int[] generate(int n) {
    int[] p = new int[n];
    ArrayList&lt;Integer&gt; mult = new ArrayList&lt;Integer&gt;();
    p[0] = 2;
    mult.add(2);
    int k = 1;
    for (int j = 3; k &lt; p.length; j += 2) {
      boolean jprime = false;
      int ord = mult.size();
      int square = p[ord] * p[ord];
      if (j == square) {
        mult.add(j);
      } else {
        jprime=true;
        for (int mi = 1; mi &lt; ord; mi++) {
          int m = mult.get(mi);
          while (m &lt; j)
            m += 2 * p[mi];
          mult.set(mi, m);
          if (j == m) {
            jprime = false;
            break;
          }
        }
      }
      if (jprime)
        p[k++] = j;
    }
    return p;
  }
}
</code></pre></div>
<p dir="auto">Even though I was done with the lesson of the chapter, I didn't want to leave that method looking so outdated.  So I cleaned it up a bit as an afterthought.  My goal was not to describe how to generate prime numbers.  I wanted my readers to see how large methods, that violate the Single Responsibility Principle, can be broken down into a few smaller well-named classes containing a few smaller well-named methods.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Thanks for the background. Even though the details of that code weren't
the main point of the chapter, presumably the code represents what you think
is the "right" and "cleanest" way to do things, given the algorithm at hand.
And that's where I disagree.</p>
<p dir="auto">There are many design problems with <code>PrimeGenerator</code>, but for now I'll
focus on method length. The code is chopped up so much (8 teeny-tiny methods)
that it's difficult to read. For starters, consider the
<code>isNotMultipleOfAnyPreviousPrimeFactor</code> method. This method invokes
<code>isMultipleOfNthPrimeFactor</code>, which invokes
<code>smallestOddNthMultipleNotLessThanCandidate</code>. These methods are shallow
and entangled:
in order to understand
<code>isNot...</code> you have to read the other two
methods and load all of that code into your mind at once. For example,
<code>isNot...</code> has side effects (it modifies <code>multiplesOfPrimeFactors</code>) but
you can't see that unless you read all three methods.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think you have a point.  Eighteen years ago, when I was in the throes of this refactoring, the names and structure made perfect sense to me.  They make sense to me now, too -- but that's because I once again understand the algorithm.  When I returned to the algorithm for the first time a few days ago, I  struggled with the names and structure.  Once I understood the algorithm the names and structure made perfect sense.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Those names are problematic even for someone who understands the algorithm;
we'll talk about them a bit later, when discussing comments. And, if code
no longer makes sense to the writer when the writer returns to the code later,
that means the code is problematic. The fact that code can eventually
be understood (with great pain and suffering) does not excuse its entanglement.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Would that we had such a crystal ball that we could help our future selves avoid such "<em>great pain and suffering</em>".  ;-)</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">There is no need for a crystal ball. The problems with <code>PrimeGenerator</code> are
pretty obvious, such as the entanglement and interface complexity; maybe you
were surprised that it is hard to understand, but I am not. Said another
way, if you are unable to predict whether your code will be easy to
understand, there are problems with your design methodology.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Fair enough.  I will say, however, that I had equal "<em>pain and suffering</em>" interpreting your rewrite (below).  So, apparently, neither of our methodologies were sufficient to rescue our readers from such struggles.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Going back to my introductory remarks about complexity, splitting up
<code>isNot...</code> into three methods doesn't reduce the amount of information
you have to keep in your mind. It just spreads it out, so it isn't as
obvious that you need to read all three methods together. And, it's harder
to see the overall structure of the code because it's split up: readers have
to flip back and forth between the methods, effectively reconstructing a
monolithic version in their minds. Because the pieces are all related,
this code will be easiest to understand if it's all together in one place.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I disagree.  Here is <code>isNotMultipleOfAnyPreviousPrimeFactor</code>.</p>
<div data-snippet-clipboard-copy-content="  private static boolean
  isNotMultipleOfAnyPreviousPrimeFactor(int candidate) {
    for (int n = 1; n < multiplesOfPrimeFactors.size(); n++) {
      if (isMultipleOfNthPrimeFactor(candidate, n))
        return false;
    }
    return true;
  }"><pre><code>  private static boolean
  isNotMultipleOfAnyPreviousPrimeFactor(int candidate) {
    for (int n = 1; n &lt; multiplesOfPrimeFactors.size(); n++) {
      if (isMultipleOfNthPrimeFactor(candidate, n))
        return false;
    }
    return true;
  }
</code></pre></div>
<p dir="auto">If you trust the <code>isMultipleOfNthPrimeFactor</code> method, then this method stands alone quite nicely.  I mean we loop through all n previous primes and see if the candidate is a multiple.  That's pretty straight forward.</p>
<p dir="auto">Now it would be fair to ask the question how we determine whether the candidate is a multiple, and in that case you'd want to inspect the <code>isMultiple...</code> method.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">This code does appear to be simple and obvious.
Unfortunately, this appearance is deceiving.
If a reader trusts the name <code>isMultipleOfNthPrimeFactor</code> (which suggests
a predicate with no side effects) and doesn't bother to read its code, they
will not realize that it has side effects, and that the side effects
create a constraint on the <code>candidate</code> argument to <code>isNot...</code>
(it must be monotonically non-decreasing from invocation
to invocation). To understand these behaviors, you have to
read both <code>isMultiple...</code> and <code>smallestOdd...</code>. The current decomposition
hides this important information from the reader.</p>
<p dir="auto">If there is one thing more likely to result in bugs than not understanding code,
it's thinking you understand it when you don't.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">That's a valid concern.  However, it is tempered by the fact that the functions are presented in the order they are called.  Thus we can expect that the reader has already seen the main loop and understands that <code>candidate</code> increases by two each iteration.</p>
<p dir="auto">The side effect buried down in <code>smallestOddNth...</code> is a bit more problematic. Now that you've pointed it out I don't like it much.  Still, that side effect should not confound the basic understanding of <code>isNot...</code>.</p>
<p dir="auto">In general, if you trust the names of the methods being called then understanding the caller does not require understanding the callee.  For example:</p>
<div data-snippet-clipboard-copy-content="for (Employee e : employees)
  if (e.shouldPayToday())
	  e.pay();"><pre><code>for (Employee e : employees)
  if (e.shouldPayToday())
	  e.pay();
</code></pre></div>
<p dir="auto">This would not be made more understandable if we replaced those two method calls with the their implementations.  Such a replacement would simply obscure the intent.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">This example works because the called methods are relatively independent of
the parent. Unfortunately that is not the case for <code>isNot...</code>.</p>
<p dir="auto">In fact, <code>isNot...</code> is not only entangled with the methods it calls, it's also
entangled with its callers. <code>isNot...</code> only works if it is invoked in
a loop where <code>candidate</code> increases monotonically. To convince yourself
that it works, you have to find the code that invokes <code>isNot...</code> and
make sure that <code>candidate</code> never decreases from one call to the next.
Separating <code>isNot...</code> from the loop that invokes it makes it harder
for readers to convince themselves that it works.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Which, as I said before, is why the methods are ordered the way they are.  I expect that by the time you get to <code>isNot...</code> you've already read <code>checkOddNumbersForSubsequentPrimes</code> and know that <code>candidate</code> increases by twos.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's discuss this briefly, because it's another area where I
disagree with <em>Clean Code</em>. If methods are entangled, there is no
clever ordering of the method definitions that will fix the problem.</p>
<p dir="auto">In this particular situation two other methods intervene between the
loop in <code>checkOdd...</code> and <code>isNot...</code>, so readers will have forgotten
the loop context before they get to <code>isNot...</code>. Furthermore, the actual
code that creates a dependency on the loop isn't in <code>isNot...</code>: it's in
<code>smallestOdd...</code>, which is even farther away from <code>checkOdd...</code>.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I sincerely doubt anyone is going to forget that <code>candidate</code> is being increased by twos.  It's a pretty obvious way to avoid waste.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">In my opening remarks I talked about how it's important to reduce the
amount of information people have to keep in their minds at once.
In this situation, readers have to remember that loop while they read
four intervening methods that are mostly unrelated to the loop. You apparently think
this will be easy and natural (I disagree). But it's even worse than
that. There is no indication which parts of <code>checkOdd...</code> will be important
later on, so the only safe approach is to remember <em>everything</em>, from <em>every</em>
method, until you have encountered every other method that could possibly
descend from it. And, to make the connection between the pieces, readers
must also reconstruct the call graph to notice that, even through
4 layers of method call, the code in <code>smallestOdd...</code> places constraints
on the loop in <code>checkOdd...</code>. This is an unreasonable cognitive burden to
place on readers.</p>
<p dir="auto">If two pieces of code are tightly related, the solution is to bring
them together. Separating the pieces, even in physically adjacent methods,
makes the code harder to understand.</p>
<p dir="auto">To me, all of the methods in <code>PrimeGenerator</code> are entangled: in order to
understand the class I had to load all of them into my mind
at once. I was constantly flipping back and forth between the methods
as I read the code. This is a red flag indicating
that the code has been over-decomposed.</p>
<p dir="auto">Bob, can you help me understand why you divided the code into such
tiny methods?
Is there some benefit to having so many methods that I have missed?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think you and I are just going to disagree on this.  In general I believe in the principle of small well-named methods and the separation of concerns. Generally speaking if you can break a large method into several well-named smaller methods with different concerns, and by doing so expose their interfaces, and the high level functional decomposition, then that's a good thing.</p>
<ul dir="auto">
<li>Looping over the odd numbers is one concern.</li>
<li>Determining primality is another.</li>
<li>Marking off the multiples of primes is yet another.</li>
</ul>
<p dir="auto">It seems to me that separating and naming those concerns helps to expose the way the algorithm works -- even at the expense of some entaglement.</p>
<p dir="auto">In your solution, which we are soon to see below, you break the algorithm up in a similar way.  However, instead of separating the concerns into functions, you separate them into sections with comments above them.</p>
<p dir="auto">You mentioned that in my solution readers will have to keep the loop context in mind while reading the other functions.  I suggest that in your solution, readers will have to keep the loop context in mind while reading your explanatory comments.  They may have to "flip back and forth" between the sections in order to establish their understanding.</p>
<p dir="auto">Now perhaps you are concerned that in my solution the "flipping" is a longer distance (in lines) than in yours.  I'm not sure that's a significant point since they all fit on the same screen (at least they do on my screen) and the landmarks are pretty obvious.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Method Length Summary</h3><a id="user-content-method-length-summary" aria-label="Permalink: Method Length Summary" href="#method-length-summary"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">It sounds like it's time to wrap up this section. Is this a reasonable
summary of where we agree and disagree?</p>
<ul dir="auto">
<li>
<p dir="auto">We agree that modular design is a good thing.</p>
</li>
<li>
<p dir="auto">We agree that it is possible to over-decompose, and that <em>Clean Code 1st ed.</em>
doesn't provide much guidance on how to recognize over-decomposition.</p>
</li>
<li>
<p dir="auto">We disagree on how far to decompose: you recommend decomposing
code into much smaller units than I do. You believe that
the additional decomposition you recommend makes code easier to
understand; I believe that it goes too far and actually makes code
more difficult to understand.</p>
</li>
<li>
<p dir="auto">You believe that the One Thing Rule, applied with judgment, will
lead to appropriate decompositions. I believe it lacks guardrails
and will lead to over-decomposition.</p>
</li>
<li>
<p dir="auto">We agree that the internal decomposition of <code>PrimeGenerator</code> into
methods is problematic. You point out that your main goal in writing
<code>PrimeGenerator</code> was to show how to decompose into classes, not
so much how to decompose a class internally into methods.</p>
</li>
<li>
<p dir="auto">Entanglement between methods in a class doesn't bother you
as much as it bothers me. You believe that the benefits of decomposing
methods can compensate for problems caused by entanglement.
I believe they can't: when decomposed methods are entangled,
they are harder to read than if they were not decomposed, and this
defeats the whole purpose of decomposition.</p>
</li>
<li>
<p dir="auto">You believe that ordering the methods in a class can help to
compensate for entanglement between methods; I don't.</p>
</li>
</ul>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think this is a fair assessment of our agreements and disagreements.  We both value decomposition,
and we both avoid entanglement; but we disagree on the relative weighting of those two values.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comments</h2><a id="user-content-comments" aria-label="Permalink: Comments" href="#comments"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's move on to the second area of disagreement: comments. In my opinion,
the <em>Clean Code</em> approach to commenting results in code with
inadequate documentation, which increases the cost of software development.
I'm sure you disagree, so let's discuss.</p>
<p dir="auto">Here is what <em>Clean Code</em> says about comments (page 54):</p>
<blockquote>
<p dir="auto">The proper use of comments is to compensate for our failure to express
ourselves in code. Note that I use the word failure. I meant it.
Comments are always failures. We must have them because we cannot always
figure out how to express ourselves without them, but their use is not
a cause for celebration... Every time you write a comment, you should
grimace and feel the failure of your ability of expression.</p>
</blockquote>
<p dir="auto">I have to be honest: I was horrified when I first read this text, and it
still makes me cringe. This stigmatizes writing comments. Junior developers
will think "if I write comments, people may think I've failed, so the
safest thing is to write no comments."</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">That chapter begins with these words:</p>
<blockquote>
<p dir="auto"><em>Nothing can be quite so helpful as a well placed comment.</em></p>
</blockquote>
<p dir="auto">It goes on to say that comments are a <em>necessary</em> evil.</p>
<p dir="auto">The only way a reader could infer that they should write no comments is if they hadn't actually read the chapter.  The chapter walks through a series of comments, some bad, some good.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto"><em>Clean Code</em> focuses a lot more on the "evil" aspects of comments than the
"necessary" aspects. The sentence you quoted above is followed by two
sentences criticizing comments. Chapter 4 spends 4 pages talking about good
comments, followed by 15 pages talking about bad comments. There are snubs
like "the only truly good comment is the comment you found a way
not to write". And "Comments are always failures" is so catchy
that it's the one thing readers are most likely to remember from the
chapter.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">The difference in page count is because there are just a few ways to write good comments, and so many more ways to write bad ones.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I disagree; this illustrates your bias against comments. If you look at
Chapter 13 of APOSD, it finds a lot more
constructive ways to use comments than <em>Clean Code</em>. And if you compare
the tone of Chapter 13 of APOSD with Chapter 4 of <em>Clean Code</em>, the hostility
of <em>Clean Code</em> towards comments becomes pretty clear.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I'll leave you to balance that last comment with the initial statement, and the final example, in the <em>Comments</em> chapter. They do not communicate "hostility".</p>
<p dir="auto">I'm not hostile to comments in general.  I <em>am</em> very hostile to gratuitous comments.</p>
<p dir="auto">You and I likely both survived through a time when comments were absolutely necessary.  In the '70s and '80s I was an assembly language programmer.  I also wrote a bit of FORTRAN. Programs in those languages that had no comments were impenetrable.</p>
<p dir="auto">As a result it became conventional wisdom to write comments by default.  And, indeed, computer science students were taught to write comments uncritically.  Comments became <em>pure good</em>.</p>
<p dir="auto">In <em>Clean Code</em> I decided to fight that mindset.  Comments can be <em>really bad</em> as well as good.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't agree that comments are less necessary today than they were
40 years ago.</p>
<p dir="auto">Comments are crucially important and add enormous value to software.
The problem is that there is a lot of important information that simply
cannot be expressed in code. By adding comments to fill in this missing
information, developers can make code dramatically easier to read.
This is not a "failure of their ability to express themselves", as you
put it.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">It's very true that there is important information that is not, or cannot be, expresssed in code.  That's a failure.  A failure of our languages, or of our ability to use them to express ourselves.  In every case a comment is a failure of our ability to use our languages to express our intent.</p>
<p dir="auto">And we fail at that very frequently, and so comments are a necessary evil -- or, if you prefer, <em>an unfortunate necessity</em>.  If we had the perfect programming language (TM) we would never write another comment.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't agree that a perfect programming language would
eliminate the need for comments. Comments and code serve very different
purposes, so it's not obvious to me that we should use the same
language for both. In my experience, English works quite well
as a language for comments.
Why do you feel that information about a program should
be expressed entirely in code, rather than using a combination of code
and English?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I bemoan the fact that we must sometimes use a human language instead of a programming language.  Human languages are imprecise and full of ambiguities. Using a human language to describe something as precise as a program is very hard, and fraught with many opportunities for error and inadvertent misinformation.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I agree that English isn't always as precise as code, but it can still be
used in precise ways and comments typically don't need the same
degree of precision as code.
Comments often contain qualitative information such
as <em>why</em> something is being done, or the overall idea of something.
English works better for these than code because it is a more
expressive language.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I have no argument with that statement.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Are you concerned that comments will be incorrect or
misleading and that this will slow down software development?
I often hear people complain about stale comments (usually as an excuse
for writing no comments at all) but
I have not found them be a significant problem
over my career. Incorrect comments do happen, but I don't encounter them
very often and when I do, they rarely cost me much time. In contrast, I waste
<em>enormous</em> amounts of time because of inadequate documentation; it's not
unusual for me to spend 50-80% of my development time wading through
code to figure out things that would be obvious if the code was properly
commented.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">You and I have had some very different experiences.</p>
<p dir="auto">I have certainly been helped by well placed comments.  I have also, just as certainly, (and within this very document) been distracted and confused by a comment that was incorrect, misplaced, gratuitous, or otherwise just plain bad.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I invite everyone reading this article to ask yourself the following questions:</p>
<ul dir="auto">
<li>How much does your software development speed suffer because of
incorrect comments?</li>
<li>How much does your software development speed suffer because of
missing comments?</li>
</ul>
<p dir="auto">For me the cost of missing comments is easily 10-100x the cost of incorrect
comments. That is why I cringe when I see things in <em>Clean Code</em> that
discourage people from writing comments.</p>
<p dir="auto">Let's consider the <code>PrimeGenerator</code> class. There is not a single comment
in that code; does this seem appropriate to you?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think it was appropriate for the purpose for which I wrote it. It was an adjunct to the lesson that very large methods can be broken down into smaller classes containing smaller methods. Adding lots of explanatory comments would have detracted from that point.</p>
<p dir="auto">In general, however, the commenting style I used in Listing 4-8 is more appropriate.  That listing, at the very end of the <em>Comments</em> chapter, describes yet another <code>PrimeGenertor</code> with a slightly different algorithm, and a better set of comments.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I disagree that adding comments would have distracted from your point,
and I think Listing 4-8 is also woefully undercommented.
But let's not argue about either of those issues. Instead, let's discuss
what comments the PrimeGenerator code <em>should</em> have if it were used in production.
I will make some suggestions and you can agree or disagree.</p>
<p dir="auto">For starters, let's discuss your use of megasyllabic names like
<code>isLeastRelevantMultipleOfLargerPrimeFactor</code>.  My understanding is that
you advocate using names like this instead of using shorter names
augmented with descriptive comments: you're effectively moving the
comments into code. To me, this approach is problematic:</p>
<ul dir="auto">
<li>Long names are awkward. Developers effectively have to retype
the documentation for a method every time they invoke it, and the long
names waste horizontal space and trigger line wraps in the code. The names are
also awkward to read: my mind wants to parse every syllable every time
I read it, which slows me down. Notice that both you and I resorted to
abbreviating names in this discussion: that's an indication that
the long names are awkward and unhepful.</li>
<li>The names are hard to parse and don't convey information as effectively
as a comment.
When students read <code>PrimeGenerator</code> one of the first things they
complain about is the long names (students can't make sense of them).
For example, the name above is
vague and cryptic: what does "least relevant" mean, and what is a
"larger prime factor"? Even with a complete understanding of the code in
the method, it's hard for me to make sense of the name.  If this name
is going to eliminate the need for a comment, it needs to be even longer.</li>
</ul>
<p dir="auto">In my opinion, the traditional approach of using shorter names with
descriptive comments is more convenient and conveys the required information
more effectively. What advantage is there in the approach you advocate?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">"<em>Megasyllabic</em>": Great word!</p>
<p dir="auto">I like my method names to be sentence fragments that fit nicely with keywords and assignment statements.  It makes the code a bit more natural to read.</p>
<div data-snippet-clipboard-copy-content="if (isTooHot)
  cooler.turnOn();"><pre><code>if (isTooHot)
  cooler.turnOn();
</code></pre></div>
<p dir="auto">I also follow a simple rule about the length of names.  The larger the scope of a method, the shorter its name should be and vice-versa -- the shorter the scope the longer the  name.  The private methods I extracted in this case live in very small scopes, and so have longish names.  Methods like this are typically called from only one place, so there is no burden on the programmer to remember a long name for another call.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Names like <code>isTooHot</code> are totally fine by me.
My concern is about names like <code>isLeastRelevantMultipleOfLargerPrimeFactor</code>.</p>
<p dir="auto">It's interesting that as methods get smaller and narrower, you recommend
longer names.
What this says to me is that the interfaces for those functions are
more complex, so it takes more words to describe them. This provides
supporting evidence for
my assertion a while back that the more you split up a method,
the shallower the resulting methods will be.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">It's not the functions that get smaller, it's the scope that gets smaller.  A private function has a smaller scope than the public function that calls it.  A function called by that private function has an even smaller scope.  As we descend in scope, we also descend in situational detail.  Describing such detail often requires a long name, or a long comment.  I prefer to use a name.</p>
<p dir="auto">As for long names being hard to parse, that's a matter of practice.  Code is full of things that take practice to get used to.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't accept this. Code may be full of things that take practice to get used
to, but that doesn't excuse it.
Approaches that require more practice are worse than
those that require less.
If it's going to take a lot of work to get comfortable with the long names
then there had better be some compensating benefit; so far I'm not seeing any.
And I don't see any reason to believe that practice will make those names
easier to digest.</p>
<p dir="auto">In addition, your comment above violates one of my fundamental rules, which
is "complexity is in the eye of the reader". If you write code that someone
else thinks is complicated, then you must accept that the code is probably
complicated (unless you think the reader is completely incompetent). It
is not OK to make excuses or suggest that it is really the reader's problem
("you just don't have enough practice"). I'm going to have to live by this
same rule a bit later in our discussion.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Fair enough.  As for the meaning of "leastRelevant", that's a much larger problem that you and I will encounter shortly.  It has to do with the intimacy that the author has with the solution, and the reader's lack of that intimacy.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You still haven't answererd my question: why is it better to use super-long names
rather than shorter names augmented with descriptive comments?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">It's a matter of preference for me.  I prefer long names to comments.  I don't trust comments to be maintained, nor do I trust that they will be read.  Have you ever noticed that many IDEs paint comments in light grey so that they can be easily ignored?  It's harder to ignore a name than a comment.</p>
<p dir="auto">(BTW, I have my IDE paint comments in bright fire-engine red)</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't see why a monster name is more likely to be "maintained" than
a comment, and I don't agree that IDEs encourage people to ignore
comments (this is your bias coming out again). My current IDE (VSCode)
doesn't use a lighter color for comments.
My previous one (NetBeans) did, but the color scheme didn't hide the comments; it
distinguished them from the code in a way that made both code and comments
easier to read.</p>
<p dir="auto">Now that we've discussed the specific issue of comments vs. long method
names, let's talk about comments in general. I think there are two major reasons
why comments are needed. The first reason for comments is abstraction.
Simply put, without comments there is no way to have abstraction or modularity.</p>
<p dir="auto">Abstraction is one of the most important components of good software design.
I define an abstraction as "a simplified way of thinking about something
that omits unimportant details." The most obvious example of an abstraction
is a method. It should be possible to use a method without reading its code.
The way we achieve this is by writing a header comment that describes
the method's <em>interface</em> (all the information someone needs in order
to invoke the method). If the method is well designed, the interface will be
much simpler than the code of the method (it omits implementation details),
so the comments reduce the amount of information people must have in
their heads.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Long ago, in a 1995 book, I defined abstraction as:</p>
<blockquote>
<p dir="auto"><em>The amplification of the essential and the elimination of the irrelevant.</em></p>
</blockquote>
<p dir="auto">I certainly agree that abstraction is of importance to good software design.  I also agree that well placed comments can enhance the ability of readers to understand the abstractions we are attempting to employ.  I disagree that comments are the <em>only</em>, or even the <em>best</em>, way to understand those abstractions.  But sometimes they are the only option.</p>
<p dir="auto">But consider:</p>
<div data-snippet-clipboard-copy-content="addSongToLibrary(String title, String[] authors, int durationInSeconds);"><pre><code>addSongToLibrary(String title, String[] authors, int durationInSeconds);
</code></pre></div>
<p dir="auto">This seems like a very nice abstraction to me, and I cannot imagine how a comment might improve it.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Our definitions of abstraction are very similar; that's good to see.
However, the <code>addSongToLibrary</code> declaration is not (yet) a good abstraction
because it omits information
that is essential. In order to use <code>addSongToLibrary</code>, developers
need answers to the following questions:</p>
<ul dir="auto">
<li>Is there any expected format for an author string, such as "LastName, FirstName"?</li>
<li>Are the authors expected to be in alphabetical order? If not, is the order
significant in some other way?</li>
<li>What happens if there is already a song in the library with the given title
but different authors? Is it replaced with the new one, or will the library
keep multiple songs with the same title?</li>
<li>How is the library stored (e.g. is it entirely in memory? saved on disk?)?
If this information is documented somewhere else, such as the
overall class documentation, then it need not be repeated here.</li>
</ul>
<p dir="auto">Thus <code>addSongToLibrary</code> needs quite a few comments.
Sometimes the signature of a method (names and types of the method, its
arguments, and its return value) contains all the information
needed to use it, but this is pretty rare. Just skim through the documentation
for your favorite library package: in how many cases could you understand how
to use a method with only its signature?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Yes, there are times when the signature of a method is an incomplete abstraction and a comment
is required.  This is especially true when the interface is part of a public API, or an API intended
for use by a separate team of developers.  Within a single development team, however, long descriptive
comments on interfaces are often more of an impediment than a help.  The team has intimate knowledge of the
internals of the system, and will generally be able to understand an interface simply from its
signature.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">In one of our in-person discussions you argued that interface comments
are unnecessary because when a group of developers is working on a body
of code they can collectively keep the entire code "loaded" in their
minds, so comments are unnecessary: if you have a question, just ask the
person who is familiar with that code. This creates a huge cognitive load
to keep all that code mentally loaded, and it's hard for me to imagine
that it would actually work. Maybe your memory is better than mine, but I
find that I quickly forget code that I wrote just a few weeks ago. In
a project of any size, I think your approach would result in developers
spending large amounts of time reading code to re-derive the interfaces,
and probably making mistakes along the way. Spending a few minutes to
document the interfaces would save time, reduce cognitive load, and
reduce bugs.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think that certain interfaces need comments, even if they are private to the team.  But I think it is more often the case that the team is familiar enough with the system that well named methods and arguments are sufficient.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's consider a specific example from <code>PrimeGenerator</code>: the <code>isMultipleOfNthPrimeFactor</code>
method. When someone reading the code encounters the call to <code>isMultiple...</code>
in <code>isNot...</code> they need to understand enough about how <code>isMultiple...</code> works
in order to see how it fits into the code of <code>isNot...</code>.
The method name does not fully document the interface, so if there
is no header comment then readers will have to read the code of <code>isMultiple</code>.
This will force readers to load more information into their
heads, which makes it harder to work in the code.</p>
<p dir="auto">Here is my first attempt at a header comment for <code>isMultiple</code>:</p>
<div data-snippet-clipboard-copy-content="    /**
     * Returns true if candidate is a multiple of primes[n], false otherwise.
     * May modify multiplesOfPrimeFactors[n].
     * @param candidate
     *      Number being tested for primality; must be at least as
     *      large as any value passed to this method in the past.
     * @param n
     *      Selects a prime number to test against; must be
     *      <= multiplesOfPrimeFactors.size().
     */"><pre><code>    /**
     * Returns true if candidate is a multiple of primes[n], false otherwise.
     * May modify multiplesOfPrimeFactors[n].
     * @param candidate
     *      Number being tested for primality; must be at least as
     *      large as any value passed to this method in the past.
     * @param n
     *      Selects a prime number to test against; must be
     *      &lt;= multiplesOfPrimeFactors.size().
     */
</code></pre></div>
<p dir="auto">What do you think of this?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think it's accurate.  I wouldn't delete it if I encountered it.  I don't think it should be a javadoc.</p>
<p dir="auto">The first sentence is redundant with the name <code>isMultipleOfNthPrimeFactor</code> and so could be deleted.  The warning of the side effect is useful.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I agree that the first sentence is largely redundant with the name,
and I debated with myself about whether to keep it. I decided to keep it
because I think it is a bit more precise than the name; it's also easier
to read. You propose to eliminate the redundancy between the comment and
the method name by dropping the comment; I would eliminate the redundancy by
shortening the method name.</p>
<p dir="auto">By the way, you complained earlier about comments being less precise than
code, but in this case the comment is <em>more</em> precise (the method
name can't include text like <code>primes[n]</code>).</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Fair enough.  There are times when precision is better expressed in a comment.</p>
<p dir="auto">Continuing with my critique of your comment above: The name <code>candidate</code> is synonymous with "Number being tested for primality".</p>
<p dir="auto">In the end, however, all the words in a comment are just going to have to sit in my brain
until I understand why they are there.  I'm also going to have to worry if
they are accurate.  So I'm going to have to read the code to understand and
validate the comment.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Whoah. That loud sound you just heard was my jaw hitting the floor.
Help me understand this a bit better: approximately what
fraction of comments that you encounter in practice are you willing to
trust without reading the code to verify them?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I look at every comment as potential misinformation.  At best they are a way to crosscheck the author's intent against the code. The amount of credence I give to a comment depends a lot on how easy they make that crosscheck.  When I read a comment that does not cause me to crosscheck, then I consider it to be of no value.  When I see a comment that causes me to crosscheck, and when that crosscheck turns out to be valuable, then that's a really good comment.</p>
<p dir="auto">Another way to say this is that the best comments tell me something surprising and verifiable about the code.  The worst are those that waste my time telling me something obvious, or incorrect.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">It sounds like your answer is 0%: you don't trust any comment unless it has
been verified against the code. This makes no sense to me. As I said above, the vast
majority of comments are correct. It's not hard to write comments; the students
in my software design class are doing this pretty well within a few weeks.
It's also not hard to keep comments up to date as code evolves. Your refusal
to trust comments is another sign of your irrational bias against comments.</p>
<p dir="auto">Refusing to trust comments incurs a very high cost. In order to understand
how to invoke a method, you will have to read all of the code of that method;
if the method invokes other methods, you will
also have to read them, and the methods they invoke, recursively. This is
an enormous amount of work in comparison to reading (and trusting) a
simple interface comment like the one I wrote above.</p>
<p dir="auto">If you choose not to write an interface comment for methods, then you
leave the interface of that method undefined. Even if someone reads the
code of the method, they won't be able to tell which parts of the
implementation are expected to remain the same and which parts may
change (there is no way to specify this "contract" in code). This will
result in misunderstanding and more bugs.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Well, I guess I've just been burned more than you have.  I've gone down too many false comment induced rabbit holes, and wasted too much time on worthless word salads.</p>
<p dir="auto">Of course my trust in comments is not a binary thing.  I read them if they are there; but
I don't implicitly trust then.  The more gratuitous I feel the author was, or the less adept at english the author is, the less I trust the comments.</p>
<p dir="auto">As I said above, our IDEs tend to paint comments in an ignorable color.  I have my IDE paint comments in bright fire engine red because when I write a comment I intend for it to be read.</p>
<p dir="auto">By the same token I use long names as a subsitute for comments because I intend for those long names to be read; and it is very hard for a programmer to ignore names.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I mentioned earlier that there are two general reasons why comments are
needed. So far we've been discussing the first reason (abstraction).
The second general reason for comments is for important information
that is not obvious from the code. The algorithm in <code>PrimeGenerator</code>
is very non-obvious, so quite a few comments are needed to help readers
understand what is going on and why. Most of the algorithm's complexity
arises because it is designed to compute primes efficiently:</p>
<ul dir="auto">
<li>
<p dir="auto">The algorithm goes out of its way to avoid divisions, which were quite
expensive when Knuth wrote his original version (they aren't that expensive
nowadays).</p>
</li>
<li>
<p dir="auto">The first multiple for each new prime number is computed by squaring the
prime, rather than multiplying it by 3. This is mysterious: why is it safe
to skip the intervening odd multiples? Furthermore, it might seem that this
optimization only has a small impact on performance, but in fact it makes an
<em>enormous</em> difference (orders of magnitude). Using the square has the
side-effect that when
testing a candidate, only primes up to the square root of the
candidate are tested. If 3x were used as the initial multiple, primes
within a factor of 3 of the candidate would be tested; that's a <em>lot</em>
more tests.
This implication of using the square is so non-obvious that I only realized
it while preparing material for this discussion; it never occurred to me in
the many times I have discussed the code with students.</p>
</li>
</ul>
<p dir="auto">Neither of these issues is obvious from the code; without
comments, readers are left to figure them out on their own. The students
in my class are generally unable to figure out either of them in the
30 minutes I give them, but I think that comments would have
allowed them to understand in a few minutes. Going back to my
introductory remarks, this is an example where information is important,
so it needs to be made available.</p>
<p dir="auto">Do you agree that there should be comments to explain each of these
two issues?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I agree that the algorithm is subtle.  Setting the first prime multiple as the square of the prime was deeply mysterious at first.  I had to go on an hour long bike ride to understand it.</p>
<p dir="auto">Would a comment help?  Perhaps.  However, my guess is that no one who has been reading our conversation has been helped by it, because you and I are now too intimate with the solution.  You and I can talk about that solution using words that fit into that intimacy; but our readers likely do not yet enjoy that fit.</p>
<p dir="auto">One solution is to paint a picture -- being worth a thousand words.  Here's my attempt.</p>
<div data-snippet-clipboard-copy-content="                                                                X
                                                    1111111111111111111111111
       1111122222333334444455555666667777788888999990000011111222223333344444
   35791357913579135791357913579135791357913579135791357913579135791357913579
   !!! !! !! !  !!  ! !! !  !  !!  ! !!  ! !  !   ! !! !! !
 3 |||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-
 5 |||||||||||-||||-||||-||||-||||-||||-||||-||||-||||-||||-||||-
 7 |||||||||||||||||||||||-||||||-||||||-||||||-||||||-||||||-||||||-
11 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||-||||||||||-
13 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
...
113||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"><pre><code>                                                                X
                                                    1111111111111111111111111
       1111122222333334444455555666667777788888999990000011111222223333344444
   35791357913579135791357913579135791357913579135791357913579135791357913579
   !!! !! !! !  !!  ! !! !  !  !!  ! !!  ! !  !   ! !! !! !
 3 |||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-
 5 |||||||||||-||||-||||-||||-||||-||||-||||-||||-||||-||||-||||-
 7 |||||||||||||||||||||||-||||||-||||||-||||||-||||||-||||||-||||||-
11 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||-||||||||||-
13 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
...
113||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
</code></pre></div>
<p dir="auto">I expect that our readers will have to stare at this for some time, and also look at the code.  But then there will be a <em>click</em> in their brains and they'll say "Ohhh!  Yes!  I see it now!"</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I found this diagram very hard to understand.
It begs for supplemental English text to explain the ideas being
presented. Even the syntax is non-obvious: what does
<code>1111111111111111111111111</code> mean?</p>
<p dir="auto">Maybe we have a fundamental difference of philosophy here. I get the sense
that you are happy to give readers a few clues and leave it to them to put
the clues together. Perhaps you don't mind if people have to stare at something
for a while to figure it out? I don't agree with this approach: it results
in wasted time, misunderstandings, and bugs.
I think software should be totally <em>obvious</em>, where readers don't need to
be clever or "stare at this for some time" to figure things out.
Suffering followed by catharsis is great for Greek tragedies, but not
for reading code. Every question
a reader might have should be naturally answered, either in the code or
in comments. Key ideas and important conclusions should be stated explicitly,
not left for the reader to deduce. Ideally, even if a reader is in a hurry
and doesn't read the code very carefully, their first guesses about how
things work (and why) should be correct. To me, that's clean code.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I don't disagree with your sentiment.  Good clean code should be as easy as possible to understand.  I want to give my readers as many clues as possible so that the code is intuitive to read.</p>
<p dir="auto">That's the goal.  As we are about to see, that can be a tough goal to achieve.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">In that case, do you still stand by the "picture" you painted above? It doesn't
seem consistent with what you just said. And if you really wanted to give
your readers as many clues as possible, you'd include a lot more comments.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I stand by the picture as far as it's accuracy is concerned.  And I think it
makes a good crosscheck.  I have no illusions that it is easy to understand.</p>
<p dir="auto">This algorithm is challenging and will require work to comprehend.  I finally
understood it when I drew this picture in my mind while on that bike ride.  When I got home I drew it for real and presented it in hopes that it might help
someone willing to do the work to understand it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Comments Summary</h3><a id="user-content-comments-summary" aria-label="Permalink: Comments Summary" href="#comments-summary"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's wrap up this section of the discussion. Here is my summary of
where we agree and disgree.</p>
<ul dir="auto">
<li>
<p dir="auto">Our overall views of comments are fundamentally different. I see more
value in comments than you do, and I believe that they play a fundamental
and irreplaceable role in system design. You agree that there are places
where comments are necessary, but that comments don't always make it
easier to understand code, so you see far fewer places where comments are
needed.</p>
</li>
<li>
<p dir="auto">I would probably write 5-10x more lines of comments for a given piece of
code than you would.</p>
</li>
<li>
<p dir="auto">I believe that missing comments are a much greater cause of lost
productivity than erroneous or unhelpful comments;
you believe that comments are a net negative, as generally practiced:
bad comments cost more time than good comments save.</p>
</li>
<li>
<p dir="auto">You view it as problematic that comments are written in English
rather than a programming language. I don't see this as particularly
problematic and think that in many cases English works better.</p>
</li>
<li>
<p dir="auto">You recommend that developers should take information that I would
represent as comments and recast it into code if at all possible. One
example of this is super-long method names. I believe that super-long names
are awkward and hard to understand, and that it would be better to use
shorter names supplemented with comments.</p>
</li>
<li>
<p dir="auto">I believe that it is not possible to define interfaces and create
abstractions without a lot of comments. You agree for public APIs, but see little need to comment
interfaces that are internal to the team.</p>
</li>
<li>
<p dir="auto">You are unwilling to trust comments until you have read code to
verify them. I generally trust comments; by doing so, I don't need to read
as much code as you do. You think this exposes me to too much risk.</p>
</li>
<li>
<p dir="auto">We agree that implementation code only needs comments when the code is
nonobvious. Although neither of us argues for a large number of implementation
comments, I'm more likely to see value in them than you do.</p>
</li>
</ul>
<p dir="auto">Overall, we struggled to find areas of agreement on this topic.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">This is a fair assessment of our individual positions; which I assume are based on our
different individual experiences.  Over the years I have found the vast majority
of comments, as generally practiced in the industry, to be unhelpful. You seem to have found more
help in the comments you have encountered.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">John's Rewrite of PrimeGenerator</h2><a id="user-content-johns-rewrite-of-primegenerator" aria-label="Permalink: John's Rewrite of PrimeGenerator" href="#johns-rewrite-of-primegenerator"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I mentioned that I ask the students in my software design class to rewrite
<code>PrimeGenerator</code> to fix all of its design problems. Here is my rewrite
(note: this was written before we began our discussion; given what I
have learned during the discussion, I would now change several of the
comments, but I have left this in its original form):</p>
<div data-snippet-clipboard-copy-content="package literatePrimes;

import java.util.ArrayList;

public class PrimeGenerator2 {

    /**
     * Computes the first prime numbers; the return value contains the
     * computed primes, in increasing order of size.
     * @param n
     *      How many prime numbers to compute.
     */
    public static int[] generate(int n) {
        int[] primes = new int[n];

        // Used to test efficiently (without division) whether a candidate
        // is a multiple of a previously-encountered prime number. Each entry
        // here contains an odd multiple of the corresponding entry in
        // primes. Entries increase monotonically.
        int[] multiples = new int[n];

        // Index of the last value in multiples that we need to consider
        // when testing candidates (all elements after this are greater
        // than our current candidate, so they don't need to be considered).
        int lastMultiple = 0;

        // Number of valid entries in primes.
        int primesFound = 1;

        primes[0] = 2;
        multiples[0] = 4;

        // Each iteration through this loop considers one candidate; skip
        // the even numbers, since they can't be prime.
        candidates: for (int candidate = 3; primesFound < n; candidate += 2) {
            if (candidate >= multiples[lastMultiple]) {
                lastMultiple++;
            }

            // Each iteration of this loop tests the candidate against one
            // potential prime factor. Skip the first factor (2) since we
            // only consider odd candidates.
            for (int i = 1; i <= lastMultiple; i++) {
                while (multiples[i] < candidate) {
                    multiples[i] += 2*primes[i];
                }
                if (multiples[i] == candidate) {
                    continue candidates;
                }
            }
            primes[primesFound] = candidate;

            // Start with the prime's square here, rather than 3x the prime.
            // This saves time and is safe because all of the intervening
            // multiples will be detected by smaller prime numbers. As an
            // example, consider the prime 7: the value in multiples will
            // start at 49; 21 will be ruled out as a multiple of 3, and
            // 35 will be ruled out as a multiple of 5, so 49 is the first
            // multiple that won't be ruled out by a smaller prime.
            multiples[primesFound] = candidate*candidate;
            primesFound++;
        }
        return primes;
    }
}"><pre><code>package literatePrimes;

import java.util.ArrayList;

public class PrimeGenerator2 {

    /**
     * Computes the first prime numbers; the return value contains the
     * computed primes, in increasing order of size.
     * @param n
     *      How many prime numbers to compute.
     */
    public static int[] generate(int n) {
        int[] primes = new int[n];

        // Used to test efficiently (without division) whether a candidate
        // is a multiple of a previously-encountered prime number. Each entry
        // here contains an odd multiple of the corresponding entry in
        // primes. Entries increase monotonically.
        int[] multiples = new int[n];

        // Index of the last value in multiples that we need to consider
        // when testing candidates (all elements after this are greater
        // than our current candidate, so they don't need to be considered).
        int lastMultiple = 0;

        // Number of valid entries in primes.
        int primesFound = 1;

        primes[0] = 2;
        multiples[0] = 4;

        // Each iteration through this loop considers one candidate; skip
        // the even numbers, since they can't be prime.
        candidates: for (int candidate = 3; primesFound &lt; n; candidate += 2) {
            if (candidate &gt;= multiples[lastMultiple]) {
                lastMultiple++;
            }

            // Each iteration of this loop tests the candidate against one
            // potential prime factor. Skip the first factor (2) since we
            // only consider odd candidates.
            for (int i = 1; i &lt;= lastMultiple; i++) {
                while (multiples[i] &lt; candidate) {
                    multiples[i] += 2*primes[i];
                }
                if (multiples[i] == candidate) {
                    continue candidates;
                }
            }
            primes[primesFound] = candidate;

            // Start with the prime's square here, rather than 3x the prime.
            // This saves time and is safe because all of the intervening
            // multiples will be detected by smaller prime numbers. As an
            // example, consider the prime 7: the value in multiples will
            // start at 49; 21 will be ruled out as a multiple of 3, and
            // 35 will be ruled out as a multiple of 5, so 49 is the first
            // multiple that won't be ruled out by a smaller prime.
            multiples[primesFound] = candidate*candidate;
            primesFound++;
        }
        return primes;
    }
}
</code></pre></div>
<p dir="auto">Everyone can read this and decide for themselves whether they think
it is easier to understand than the original. I'd like to mention a
couple of overall things:</p>
<ul dir="auto">
<li>There is only one method. I didn't subdivide it because I felt the method already divides naturally into pieces that are distinct and understandable. It didn't seem to me that pulling out methods would improve readability significantly. When students rewrite the code, they typically have 2 or 3 methods, and those are usually OK too.</li>
<li>There are a <em>lot</em> of comments. It's extremely rare for me to write code with this density of comments. Most methods I write have no comments in the body, just a header comment describing the interface. But this code is subtle and tricky, so it needs a lot of comments to make the subtleties clear to readers. The long length of some of the comments is a red flag indicating that I struggled to find a clear and simple explanation for the code. Even with all the additional explanatory material this version is a bit shorter than the original (65 lines vs. 70).</li>
</ul>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I presume this is a complete rewrite.  My guess is that you worked to understand the algorithm from <em>Clean Code</em> and then wrote this from scratch.  If that's so, then fair enough.</p>
<p dir="auto">In <em>Clean Code</em> I <em>refactored</em> Knuth's algorithm in order to give it a little structure.  That's not the same as a complete rewrite.</p>
<p dir="auto">Having said that, your version is much better than either Knuth's or mine.</p>
<p dir="auto">I wrote that chapter 18 years ago, so it's been a long time since I saw and understood this algorithm.  When I first saw your challenge I thought: "Oh, I can figure out my own code!"  But, no.  I could see all the moving parts, but I could not figure out why those moving parts generated a list of prime numbers.</p>
<p dir="auto">So then I looked at your code.  I had the same problem.  I could see all the moving parts, all with comments, but I still could not figure out why those moving parts generated a list of prime numbers.</p>
<p dir="auto">Figuring that out required a lot of staring at the ceiling, closing my eyes, visualizing, and riding my bike.</p>
<p dir="auto">Among the problems I had were the comments you wrote.  Let's take them one at a time.</p>
<div data-snippet-clipboard-copy-content="    /**
     * Computes the first prime numbers; the return value contains the
     * computed primes, in increasing order of size.
     * @param n
     *      How many prime numbers to compute.
     */
    public static int[] generate(int n) {"><pre><code>    /**
     * Computes the first prime numbers; the return value contains the
     * computed primes, in increasing order of size.
     * @param n
     *      How many prime numbers to compute.
     */
    public static int[] generate(int n) {
</code></pre></div>
<p dir="auto">It seems to me that this would be better as:</p>
<div data-snippet-clipboard-copy-content="public static int[] generateNPrimeNumbers(int n) {"><pre><code>public static int[] generateNPrimeNumbers(int n) {
</code></pre></div>
<p dir="auto">or if you must:</p>
<div data-snippet-clipboard-copy-content="//Return the first n prime numbers
public static int[] generate(int n) {"><pre><code>//Return the first n prime numbers
public static int[] generate(int n) {
</code></pre></div>
<p dir="auto">I'm not opposed to Javadocs as a rule; but I write them only when absolutely necessary. I also have an aversion for descriptions and <code>@param</code> statements that are perfectly obvious from the method signature.</p>
<p dir="auto">The next comment cost me a good 20 minutes of puzzling things out.</p>
<div data-snippet-clipboard-copy-content="// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// here contains an odd multiple of the corresponding entry in
// primes. Entries increase monotonically."><pre><code>// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// here contains an odd multiple of the corresponding entry in
// primes. Entries increase monotonically.
</code></pre></div>
<p dir="auto">First of all I'm not sure why the "division" statement is necessary.  I'm old school so I expect that everyone knows to avoid division in inner loops if it can be avoided.  But maybe I'm wrong about that...</p>
<p dir="auto">Also, the <em>Sieve of Eratosthenes</em> does not do division, and is a lot easier to understand <em>and explain</em> than this algorithm.  So why this particular algorithm?  I think Knuth was trying to save <em>memory</em> -- and in 1982 saving memory was important.  This algorithm uses a lot less memory than the sieve.</p>
<p dir="auto">Then came the phrase: <code>Each entry here contains an odd multiple...</code>.  I looked at that, and then at the code, and I saw: <code>multiples[0] = 4;</code>.</p>
<p dir="auto">"That's not odd" I said to myself.  "So maybe he meant even."</p>
<p dir="auto">So then I looked down and saw: <code>multiples[i] += 2*primes[i];</code></p>
<p dir="auto">"That's adding an even number!" I said to myself.  "I'm pretty sure he meant to say 'even' instead of 'odd'."</p>
<p dir="auto">I hadn't yet worked out what the <code>multiples</code> array was.  So I thought it was perfectly reasonable that it would have even numbers in it, and that your comment was simply an understandable word transposition.  After all, there's no compiler for comments so they suffer from the kinds of mistakes that humans often make with words.</p>
<p dir="auto">It was only when I got to <code>multiples[primesFound] = candidate*candidate;</code> that I started to question things.  If the <code>candidate</code> is prime, shouldn't <code>prime*prime</code> be odd in every case beyond 2?  I had to do the math in my head to prove that.  (2n+1)(2n+1) = 4n^2+4n+1 ... Yeah, that's odd.</p>
<p dir="auto">OK, so the <code>multiples</code> array is full of odd multiples, except for the first element, since it will be muliples of 2.</p>
<p dir="auto">So perhaps that comment should be:</p>
<div data-snippet-clipboard-copy-content=" // multiples of corresponding prime."><pre><code> // multiples of corresponding prime.
</code></pre></div>
<p dir="auto">Or perhaps we should change the name of the array to something like <code>primeMultiples</code> and drop the comment altogether.</p>
<p dir="auto">Moving on to the next comment:</p>
<div data-snippet-clipboard-copy-content="// Each iteration of this loop tests the candidate against one
// potential prime factor. Skip the first factor (2) since we
// only consider odd candidates."><pre><code>// Each iteration of this loop tests the candidate against one
// potential prime factor. Skip the first factor (2) since we
// only consider odd candidates.
</code></pre></div>
<p dir="auto">That doesn't make a lot of sense.  The code it's talking about is:</p>
<div data-snippet-clipboard-copy-content="for (int i = 1; i <= lastMultiple; i++) {
    while (multiples[i] < candidate) {"><pre><code>for (int i = 1; i &lt;= lastMultiple; i++) {
    while (multiples[i] &lt; candidate) {
</code></pre></div>
<p dir="auto">The <code>multiples</code> array, as we have now learned, is an array of <em>multiples</em> of prime numbers.  This loop is not testing the candidate against prime <em>factors</em>, it's testing it against the current prime <em>multiples</em>.</p>
<p dir="auto">Fortunately for me the third of fourth time I read this comment I realized that you really meant to use the word "multiples".  But the only way for me to know that was to understand the algorithm.  And when I understand the algorithm, why do I need the comment?</p>
<p dir="auto">That left me with one final question.  What the deuce was the reason behind:</p>
<div data-snippet-clipboard-copy-content="multiples[primesFound] = candidate*candidate;"><pre><code>multiples[primesFound] = candidate*candidate;
</code></pre></div>
<p dir="auto">Why the square?  That makes no sense.  So I changed it to:</p>
<div data-snippet-clipboard-copy-content="multiples[primesFound] = candidate;"><pre><code>multiples[primesFound] = candidate;
</code></pre></div>
<p dir="auto">And it worked just fine.  So this must be an optimization of some kind.</p>
<p dir="auto">Your comment to explain this is:</p>
<div data-snippet-clipboard-copy-content="// Start with the prime's square here, rather than 3x the prime.
// This saves time and is safe because all of the intervening
// multiples will be detected by smaller prime numbers. As an
// example, consider the prime 7: the value in multiples will
// start at 49; 21 will be ruled out as a multiple of 3, and
// 35 will be ruled out as a multiple of 5, so 49 is the first
// multiple that won't be ruled out by a smaller prime."><pre><code>// Start with the prime's square here, rather than 3x the prime.
// This saves time and is safe because all of the intervening
// multiples will be detected by smaller prime numbers. As an
// example, consider the prime 7: the value in multiples will
// start at 49; 21 will be ruled out as a multiple of 3, and
// 35 will be ruled out as a multiple of 5, so 49 is the first
// multiple that won't be ruled out by a smaller prime.
</code></pre></div>
<p dir="auto">The first few times I read this it made no sense to me at all.  It was just a jumble of numbers.</p>
<p dir="auto">I stared at the ceiling, and closed my eyes to visualize. I couldn't see it.  So I went on a long contemplative bike ride during which I realized that the prime multiples of 2 will at one point contain 2*3 and then 2*5.  So the <code>multiples</code> array will at some point contain multiples of primes <em>larger</em> than the prime they represent.  <em>And it clicked!</em></p>
<p dir="auto">Suddenly it all made sense. I realized that the <code>multiples</code> array was the equivalent of the array of booleans we use in the <em>Sieve of Eratosthenes</em> -- but with a really interesting twist.  If you were to do the sieve on a whiteboard, you <em>could</em> erase every number less than the candidate, and only cross out the numbers that were the next multiples of all the previous primes.</p>
<p dir="auto">That explanation makes perfect sense to me -- now, but I'd be willing to bet that those who are reading it are puzzling over it.  The idea is just hard to explain.</p>
<p dir="auto">Finally I went back to your comment and could see what you were saying.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">A Tale of Two Programmers</h3><a id="user-content-a-tale-of-two-programmers" aria-label="Permalink: A Tale of Two Programmers" href="#a-tale-of-two-programmers"></a></p>
<p dir="auto">The bottom line here is that you and I both fell into the same trap.  I refactored that old algorithm 18 years ago, and I thought all those method and variable names would make my intent clear -- <em>because I understood that algorithm</em>.</p>
<p dir="auto">You wrote that code awhile back and decorated it with comments that you thought would explain your intent -- <em>because you understood that algorithm</em>.</p>
<p dir="auto">But my names didn't help me 18 years later.  They didn't help you, or your students either.  And your comments didn't help me.</p>
<p dir="auto">We were inside the box trying to communicate to those who stood outside and could not see what we saw.</p>
<p dir="auto">The bottom line is that it is very difficult to explain something to someone who is not intimate with the details you are trying to explain. Often our explanations make sense only after the reader has worked out the details for themself.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">There's a lot of stuff in your discussion above, but I think it all boils down
to one thing: you don't like the comments that I wrote. As I mentioned earlier,
complexity is in the eye of the reader: if you say that my comments were
confusing or didn't help you to understand the code, then I have to take that
seriously.</p>
<p dir="auto">At the same time, you have made it clear that you don't see much value in
comments in general. Your preference is to have essentially no
comments for this code (or any code). You argue above that there is simply nothing that
comments can do to make the code easier to understand; the only way to
understand the code is to read the code. That is a cop-out.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Sorry to interrupt you; but I think you are overstating my position.  I certainly never said that comments can never be helpful.  Sometimes, of course, they are.  What I said was that I only trust them if the code validates them.  Sometimes a comment will make that validation a lot easier.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You keep saying that you sometimes find use for comments, but the reality
is that "sometimes" almost never occurs in your code. We'll see this when
we look at your revision of my code.</p>
<p dir="auto">Now back to my point. In order to
write our various versions of the code, you and I had to accumulate a lot of
knowledge about the algorithm, such as why it's OK for the first multiple
of a prime to be its square. Unfortunately, not all of that knowledge can
be represented in the code. It is our professional responsibility to do
the best we can to convey
that knowledge in comments, so that readers do not
have to reconstruct it over and over. Even if the resulting comments are
imperfect, they will make the code easier to understand.</p>
<p dir="auto">If a situation like this occurred in real life I would work with
you and others to improve my comments. For example, I would ask you
questions to get a better sense of
why the "squared prime" comment didn't seem to help you:</p>
<ul dir="auto">
<li>Are there things in the comment that are misleading or confusing?</li>
<li>Is there some important piece of information you acquired on your
bike ride that suddenly made things clear?</li>
</ul>
<p dir="auto">I would also show the comment to a few other people to get their takes
on it. Then I would rework the comment to improve it.</p>
<p dir="auto">Given your fundamental disbelief in comments, I think it's likely that
you would still see no value in the comment, even after my reworking.
In this case I would show the comment to other people, particularly those
who have a more positive view of comments in general, and get
their input. As long as the comment is not misleading and at least a few
people found it helpful, I would retain it.</p>
<p dir="auto">Now let me me discuss two few specific comments that you objected to. The
first comment was the one for the <code>multiples</code> variable:</p>
<div data-snippet-clipboard-copy-content="// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// here contains an odd multiple of the corresponding entry in
// primes. Entries increase monotonically."><pre><code>// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// here contains an odd multiple of the corresponding entry in
// primes. Entries increase monotonically.
</code></pre></div>
<p dir="auto">There is a bug in this comment that you exposed (the first entry is not odd);
good catch! You then argued that most of the information in the comment
is unnecessary and proposed this as an alternative:</p>
<div data-snippet-clipboard-copy-content=" // multiples of corresponding prime."><pre><code> // multiples of corresponding prime.
</code></pre></div>
<p dir="auto">You have left out too much useful information here. For example, I don't think
it is safe to assume that readers will figure out that the motivation is
avoiding divisions. It's always better to state these assumptions and
motivations clearly so that there will be no confusion. And I think it's
helpful for readers to know that these entries never decrease.
I would simply fix the bug, leaving all of the information intact:</p>
<div data-snippet-clipboard-copy-content="// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// (except the first, which is never used) contains an odd multiple of
// the corresponding entry in primes. Entries increase monotonically."><pre><code>// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// (except the first, which is never used) contains an odd multiple of
// the corresponding entry in primes. Entries increase monotonically.
</code></pre></div>
<p dir="auto">The second comment was this one, for the <code>for</code> loop:</p>
<div data-snippet-clipboard-copy-content="// Each iteration of this loop tests the candidate against one
// potential prime factor. Skip the first factor (2) since we
// only consider odd candidates."><pre><code>// Each iteration of this loop tests the candidate against one
// potential prime factor. Skip the first factor (2) since we
// only consider odd candidates.
</code></pre></div>
<p dir="auto">You objected to this comment because the code of the loop doesn't actually
test the candidate against the prime factor; it tests it against a multiple.
When I write implementation comments like this, my goal is not to restate
the code; comments like that don't usually provide much value. The goal here was
to say <em>what</em> the code is doing in a logical sense, not <em>how</em> it does it.
In that sense, the comment is correct.</p>
<p dir="auto">However, if a comment causes confusion in the reader, then it is not a
good comment. Thus I would rewrite this comment to make it clear that
it describes the abstract function of the code, not its
precise behavior:</p>
<div data-snippet-clipboard-copy-content="// Each iteration of this loop considers one existing prime, ruling
// out the candidate if it is a multiple of that prime. Skip the
// first prime (2) since we only consider odd candidates."><pre><code>// Each iteration of this loop considers one existing prime, ruling
// out the candidate if it is a multiple of that prime. Skip the
// first prime (2) since we only consider odd candidates.
</code></pre></div>
<p dir="auto">To conclude, I agree with your assertion "it is very difficult to explain
something to someone who is not intimate with the details you are trying
to explain." And yet, it is our responsibility as programmers to do exactly
that.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I'm glad we agree.  We also agree about getting others to review the code and make recommendations on the code <em>and</em> the comments.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Bob's Rewrite of PrimeGenerator2</h2><a id="user-content-bobs-rewrite-of-primegenerator2" aria-label="Permalink: Bob's Rewrite of PrimeGenerator2" href="#bobs-rewrite-of-primegenerator2"></a></p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">When I saw your solution, and after I gained a good understanding of it.  I refactored it just a bit.  I loaded it into my IDE, wrote some simple tests, and extracted a few simple methods.</p>
<p dir="auto">I also got rid of that <em>awful</em> labeled <code>continue</code> statement.  And I added 3 to the primes list so that I could mark the first element as <em>irrelevant</em> and give it a value of -1.  (I think I was still reeling from the even/odd confusion.)</p>
<p dir="auto">I like this because the implementation of the <code>generateFirstNPrimes</code> method describes the moving parts in a way that hints at what is going on.  It's easy to read that implementation and get a glimpse of the mechanism.  I'm not at all sure that the comment helps.</p>
<p dir="auto">I think it is just the reality of this algorithm that the effort required to properly explain it, and the effort required for anyone else to read and understand that explanation is roughly equivalent to the effort needed to read the code and go on a bike ride.</p>
<div data-snippet-clipboard-copy-content="package literatePrimes;

public class PrimeGenerator3 {
    private static int[] primes;
    private static int[] primeMultiples;
    private static int lastRelevantMultiple;
    private static int primesFound;
    private static int candidate;

    // Lovely little algorithm that finds primes by predicting
    // the next composite number and skipping over it. That prediction
    // consists of a set of prime multiples that are continuously
    // increased to keep pace with the candidate.

    public static int[] generateFirstNPrimes(int n) {
        initializeTheGenerator(n);

        for (candidate = 5; primesFound < n; candidate += 2) {
            increaseEachPrimeMultipleToOrBeyondCandidate();
            if (candidateIsNotOneOfThePrimeMultiples()) {
                registerTheCandidateAsPrime();
            }
        }
        return primes;
    }

    private static void initializeTheGenerator(int n) {
        primes = new int[n];
        primeMultiples = new int[n];
        lastRelevantMultiple = 1;

        // prime the pump. (Sorry, couldn't resist.)
        primesFound = 2;
        primes[0] = 2;
        primes[1] = 3;

        primeMultiples[0] = -1;// irrelevant
        primeMultiples[1] = 9;
    }

    private static void increaseEachPrimeMultipleToOrBeyondCandidate() {
        if (candidate >= primeMultiples[lastRelevantMultiple])
            lastRelevantMultiple++;

        for (int i = 1; i <= lastRelevantMultiple; i++)
            while (primeMultiples[i] < candidate)
                primeMultiples[i] += 2 * primes[i];
    }

    private static boolean candidateIsNotOneOfThePrimeMultiples() {
        for (int i = 1; i <= lastRelevantMultiple; i++)
            if (primeMultiples[i] == candidate)
                return false;
        return true;
    }

    private static void registerTheCandidateAsPrime() {
        primes[primesFound] = candidate;
        primeMultiples[primesFound] = candidate * candidate;
        primesFound++;
    }
}"><pre><code>package literatePrimes;

public class PrimeGenerator3 {
    private static int[] primes;
    private static int[] primeMultiples;
    private static int lastRelevantMultiple;
    private static int primesFound;
    private static int candidate;

    // Lovely little algorithm that finds primes by predicting
    // the next composite number and skipping over it. That prediction
    // consists of a set of prime multiples that are continuously
    // increased to keep pace with the candidate.

    public static int[] generateFirstNPrimes(int n) {
        initializeTheGenerator(n);

        for (candidate = 5; primesFound &lt; n; candidate += 2) {
            increaseEachPrimeMultipleToOrBeyondCandidate();
            if (candidateIsNotOneOfThePrimeMultiples()) {
                registerTheCandidateAsPrime();
            }
        }
        return primes;
    }

    private static void initializeTheGenerator(int n) {
        primes = new int[n];
        primeMultiples = new int[n];
        lastRelevantMultiple = 1;

        // prime the pump. (Sorry, couldn't resist.)
        primesFound = 2;
        primes[0] = 2;
        primes[1] = 3;

        primeMultiples[0] = -1;// irrelevant
        primeMultiples[1] = 9;
    }

    private static void increaseEachPrimeMultipleToOrBeyondCandidate() {
        if (candidate &gt;= primeMultiples[lastRelevantMultiple])
            lastRelevantMultiple++;

        for (int i = 1; i &lt;= lastRelevantMultiple; i++)
            while (primeMultiples[i] &lt; candidate)
                primeMultiples[i] += 2 * primes[i];
    }

    private static boolean candidateIsNotOneOfThePrimeMultiples() {
        for (int i = 1; i &lt;= lastRelevantMultiple; i++)
            if (primeMultiples[i] == candidate)
                return false;
        return true;
    }

    private static void registerTheCandidateAsPrime() {
        primes[primesFound] = candidate;
        primeMultiples[primesFound] = candidate * candidate;
        primesFound++;
    }
}
</code></pre></div>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">This version is a considerable improvement over the version in <em>Clean Code</em>.
Reducing the number of methods made the code easier to read and resulted
in cleaner interfaces. If it were properly commented, I think this version
would be about as easy to read as my version (the additional methods you
created didn't particularly help, but they didn't hurt either). I suspect
that if we polled readers, some would like your version better and some
would prefer mine.</p>
<p dir="auto">Unfortunately, this revision of the code creates a serious performance
regression: I measured a factor of 3-4x slowdown compared to either
of the earlier revisions. The problem is that you changed the processing of a
particular candidate from a single loop to two loops (the <code>increaseEach...</code> and
<code>candidateIsNot...</code> methods). In the loop from earlier revisions, and in
the <code>candidateIsNot</code>
method, the loop aborts once the candidate is disqualified (and
most candidates are quickly eliminated). However,
<code>increaseEach...</code> must examine every entry in <code>primeMultiples</code>.
This results in 5-10x as many loop iterations and a 3-4x overall slowdown.</p>
<p dir="auto">Given that the whole reason for the current algorithm (and its complexity)
is to maximize performance, this slowdown is unacceptable. The two
methods must be combined.</p>
<p dir="auto">I think what happened here is that you were so focused on something
that isn't actually all that important (creating the tiniest possible methods)
that you dropped the ball on other issues that really are important.
We have now seen this twice. In the original version of <code>PrimeGenerator</code>
you were so determined to make tiny methods that you didn't notice that the
code was becoming incomprehensible. In this version you were so eager to
chop up my single method that you didn't notice that you were blowing up the
performance.</p>
<p dir="auto">I don't think this was just an unfortunate combination of oversights.
One of the most important things
in software design is to identify what is important and focus on that;
if you focus on things that are unimportant, you're likely to mess up the
things that are important.</p>
<p dir="auto">The code in your revision is still under-commented. You believe
that there is no meaningful way for comments to assist the reader in
understanding the code. I think this stems from your general disbelief in
the value of comments; you are quick to throw in the towel.
This algorithm is unusually difficult to explain,
but I still believe that comments can help. For example, I believe you
must make some attempt to help readers understand why the first multiple
for a prime is the square of the prime. You have taken a lot of time to
develop your understanding of this; surely there must be some way to convey
that understanding to others? If you had included that information in
your original version of the code you could have saved yourself that long
bike ride.
Giving up on this is an abdication of professional responsibility.</p>
<p dir="auto">The few comments that you included in your revision are of little value.
The first comment is too cryptic to provide much help: I can't
make any sense of the phrase "predicting the next composite number and
skipping over it" even though I completely understand the code it purports
to explain. One of the comments is just a joke; I was surprised to see
this, given your opposition to extraneous comments.</p>
<p dir="auto">Clearly you and I live in different universes when it comes to comments.</p>
<p dir="auto">Finally, I don't understand why you are offended by the labeled <code>continue</code>
statement in my code. This is a clean and elegant solution to the problem
of escaping from nested loops. I wish more languages
had this feature; the alternative is awkward code where you set a variable,
then exit one level of loop, then check the variable and exit the next
level.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Good catch!  I would have caught that too had I thought to profile the solution.  You are right that separating the two loops added some unecessary iteration.  I found a nice way to solve that problem without using the horrible <code>continue</code>.  My updated version is now faster than yours!  A million primes in 440ms as opposed to yours which takes 561ms.  ;-) Below are just the changes.</p>
<div data-snippet-clipboard-copy-content="  public static int[] generateFirstNPrimes(int n) {
    initializeTheGenerator(n);

    for (candidate = 5; primesFound < n; candidate += 2)
      if (candidateIsPrime())
        registerTheCandidateAsPrime();

    return primes;
  }

  private static boolean candidateIsPrime() {
    if (candidate >= primeMultiples[lastRelevantMultiple])
      lastRelevantMultiple++;

    for (int i = 1; i <= lastRelevantMultiple; i++) {
      while (primeMultiples[i] < candidate)
        primeMultiples[i] += 2 * primes[i];
      if (primeMultiples[i] == candidate)
        return false;
    }
    return true;
  }"><pre><code>  public static int[] generateFirstNPrimes(int n) {
    initializeTheGenerator(n);

    for (candidate = 5; primesFound &lt; n; candidate += 2)
      if (candidateIsPrime())
        registerTheCandidateAsPrime();

    return primes;
  }

  private static boolean candidateIsPrime() {
    if (candidate &gt;= primeMultiples[lastRelevantMultiple])
      lastRelevantMultiple++;

    for (int i = 1; i &lt;= lastRelevantMultiple; i++) {
      while (primeMultiples[i] &lt; candidate)
        primeMultiples[i] += 2 * primes[i];
      if (primeMultiples[i] == candidate)
        return false;
    }
    return true;
  }
</code></pre></div>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Yep, that fixes the problem. I note that you are now down to 4 methods,
from 8 in the <em>Clean Code</em> version.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Test-Driven Development</h2><a id="user-content-test-driven-development" aria-label="Permalink: Test-Driven Development" href="#test-driven-development"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's move on to our third area of disagreement, which is Test-Driven
Development. I am a huge fan of unit testing. I believe that unit tests are
an indispensable part of the software development process and pay for
themselves over and over. I think we agree on this.</p>
<p dir="auto">However, I am not fan of Test-Driven Development (TDD), which dictates
that tests must be written before code and that code must be written
and tested in tiny increments. This approach has serious problems
without any compensating advantages that I have been able to identify.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">As I said at the start I have carefully read <em>A Philosophy of Software Design</em>. I found it to be full of worthwhile insights, and I strongly agree with most of the points you make.</p>
<p dir="auto">So I was surprised to find, on page 157, that you wrote a very short, dismissive, pejorative, and inaccurate section on <em>Test Driven Development</em>.  Sorry for all the adjectives, but I think that's a fair characterization.  So my goal, here, is to correct the misconceptions that led you to write the following:</p>
<blockquote>
<p dir="auto">"Test-driven development is an approach to software development where programmers write unit tests before they write code.  When creating a new class, the develper first writes unit tests for the class, based on its expected behavior.  None of these tests pass, since there is no code for the class.  Then the developer works through the tests one at a time, writing enough code for that test to pass.  When all of the tests pass, the class is finished."</p>
</blockquote>
<p dir="auto">This is just wrong.  TDD is quite considerably different from what you describe.  I describe it using three laws.</p>
<ol dir="auto">
<li>
<p dir="auto">You are not allowed to write any production code until you have first written a unit test that fails because that code does not exist.</p>
</li>
<li>
<p dir="auto">You are not allowed to write more of a unit test than is sufficient to fail, and failing to compile is failing.</p>
</li>
<li>
<p dir="auto">You are not allowed to write more production code than is sufficient to make the currently failing test pass.</p>
</li>
</ol>
<p dir="auto">A little thought will convince you that these three laws will lock you into a cycle that is just a few seconds long.  You'll write a line or two of a test that will fail, you'll write a line or two of production code that will pass, around and around every few seconds.</p>
<p dir="auto">A second layer of TDD is the Red-Green-Refactor loop.  This loop is several minutes long.  It is comprised of a few cycles of the three laws, followed by a period of reflection and refactoring.  During that reflection we pull back from the intimacy of the quick cycle and look at the design of the code we've just written.  Is it clean?  Is it well structured?  Is there a better approach?  Does it match the design we are pursuing?  If not, should it?</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Oops! I plead "guilty as charged" to inaccurately describing TDD.
I will fix this in the next revision of APOSD. That said, your definition
of TDD does not change my concerns.</p>
<p dir="auto">Let's discuss the potential advantages and disadvantages
of TDD; then readers can decide for themselves whether they think TDD is a
good idea overall.</p>
<p dir="auto">Before we start that discussion, let me clarify the approach I prefer as an
alternative to TDD. In your online videos you describe the alternative to
TDD as one where a developer writes the code, gets it fully working
(presumably with manual tests), then goes back and writes the unit tests.
You argue that this approach would be terrible: developers
lose interest once they think code is working, so they wouldn't actually
write the tests. I agree with you completely. However, this isn't the only
alternative to TDD.</p>
<p dir="auto">The approach I prefer is one where the developer works in somewhat
larger units than in TDD, perhaps a few methods or a class. The developer
first writes some code (anwywhere from a few tens of lines to a few hundred
lines), then writes unit tests for that code. As with TDD, the
code isn't considered to be "working" until it has comprehensive unit
tests.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">How about if we call this technique "bundling" for purposes of this
document?  This is the term I use in <em>Clean Code 2d ed.</em></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Fine by me.</p>
<p dir="auto">The reason for working in larger units is to encourage design
thinking, so that a developer can think about a collection of related
tasks and do a bit of planning to come up with a good overall design
where the pieces fit together well.
Of course the initial design ideas will have flaws and refactoring
will still be necessary, but the goal is to center the development
process around design, not tests.</p>
<p dir="auto">To start our discussion, can you make a list of the advantages you
think that TDD provides over the approach I just described?</p>
<p dir="auto"><strong>UB:</strong>
The advantages I usually attribute to TDD are:</p>
<ul dir="auto">
<li>
<p dir="auto">Very little need for debugging.  After all, if you just saw everything working a minute or two ago, there's not much to debug.</p>
</li>
<li>
<p dir="auto">A stream of reliable low level documentation, in the form of very small and isolated unit tests.  Those tests describe the low level structure and operation of every facet of the system.  If you want to know how to do something in the system, there are tests that will show you how.</p>
</li>
<li>
<p dir="auto">A less coupled design which results from the fact that every small part of the system must be designed to be testable, and testability requires decoupling.</p>
</li>
<li>
<p dir="auto">A suite of tests that you trust with your life, and therefore supports fearless refactoring.</p>
</li>
</ul>
<p dir="auto">However, you asked me which of these advantages TDD might have over <em>your</em> preferred method.  That depends on how big you make those larger units you described.  The important thing to me is to keep the cycle time short, and to prevent entanglements that block testability.</p>
<p dir="auto">It seems to me that working in small units, and then immediately writing after the fact tests, can give you all the above advantages, so long as you are very careful to test every aspect of the code you just wrote.  I think a disciplined programmer could effectively work that way.  Indeed, I think such a programmer would produce code that I could not distinguish from code written by another programmer following TDD.</p>
<p dir="auto">Above you suggested that bundling is to encourage design.  I think encouraging design is a very good thing.  My question for you is: Why do you think that TDD does not encourage design?  My own experience is that design comes from strategic thought, which is independent of the tactical behavior of either TDD or Bundling.  Design is taking one step back from the code and envisioning structures that address a larger set of constraints and needs.</p>
<p dir="auto">Once you have that vision in your head it seems to me bundling and TDD will yield similar results.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">First, let me address the four advantages you listed for TDD:</p>
<ul dir="auto">
<li>
<p dir="auto">Very little need for debugging? I think any form of unit testing can
reduce debugging work, but not for the reason you
suggested. The benefit comes because unit tests expose bugs earlier
and in an environment where they are easier to track down. A
relatively simple bug to fix in development can be very painful to
track down in production. I'm not convinced by your argument that
there's less debugging because "you just saw everything working a
minute ago": it's easy to make a tiny change that exposes a really
gnarly bug that has existed for a long time but hasn't yet been
triggered. Hard-to-debug problems arise from the accumulated complexity
of the system, not from the size of the code increments.</p>
<blockquote>
<p dir="auto"><strong>UB:</strong> True.  However, when the cycles are very short then the cause
of even the gnarliest of bugs have the best chance of being tracked down.
The shorter the cycles, the better the chances.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>JOHN:</strong> This is only true up to a point. I think you believe
that making units smaller and smaller continues to provide benefits,
with almost no limit to how small they can get. I think that there
is a point of diminishing returns, where making things even smaller
no longer helps and actually starts to hurt. We saw this disagreement
over method length, and I think we're seeing it again here.</p>
</blockquote>
</li>
<li>
<p dir="auto">Low level documentation? I disagree: unit tests are a poor form
of documentation. Comments are a much more
effective form of documentation, and you can put them right next to the
relevant code. Trying to learn a method's
interface by reading a bunch of unit tests seems much more difficult
than just reading a couple of sentences of English text.</p>
<blockquote>
<p dir="auto"><strong>UB:</strong> Nowadays it's very easy to find the tests for
a function by using the "where-used" feature of the IDE.  As for comments
being better, if that were true then no one would publish example code.</p>
</blockquote>
</li>
<li>
<p dir="auto">A less coupled design? Possibly, but I haven't experienced this myself.
It's not clear to me that designing for testability will produce the
best design.</p>
<blockquote>
<p dir="auto"><strong>UB:</strong> Generally the decoupling arises because the test requires a mock
of some kind.  Mocks tend to force abstractions that might otherwise not exist.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>JOHN:</strong> In my experience, mocking virtually never changes interfaces;
it just provides replacements for existing (typically immovable)
interfaces.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>UB:</strong> Our experiences differ.</p>
</blockquote>
</li>
<li>
<p dir="auto">Enabling fearless refactoring? BINGO! This is the where almost all of the
benefits from unit testing come from, and it is a really really big deal.</p>
<blockquote>
<p dir="auto"><strong>UB:</strong> Agreed.</p>
</blockquote>
</li>
</ul>
<p dir="auto">I agree with your conclusion that TDD and bundling are about the
same in terms of providing these benefits.</p>
<p dir="auto">Now let me explain why I think TDD is likely to result in bad designs.
The fundamental problem with TDD is that it forces developers to work
too tactically, in
units of development that are too small; it discourages design
thinking.  With TDD the basic unit of
development is one test: first the test is written, then the code to
make that test pass. However, the natural units for design are larger
than this: a class or method, for example. These units
correspond to multiple test cases. If a developer thinks only about
the next test, they are only considering part of a design problem at
any given time. It's hard to design something well if you don't think
about the whole design problem at once. TDD explicitly
prohibits developers from writing more code than is needed to pass
the current test; this discourages the kind of strategic thinking needed
for good design.</p>
<p dir="auto">TDD does not provide adequate guidance to encourage design. You mentioned
the Red-Green-Refactor loop, which recommends refactoring after each step,
but there's almost no guidance for refactoring. How should developers
decide when and what to refactor? This seems to be left purely to their
own judgment. For example, if I am writing a method that requires
multiple iterations of the TDD loop, should I refactor after every iteration
(which sounds pretty tedious) or wait until after several iterations so that
I can look at a bigger chunk of code when refactoring and hence be more
strategic? Without guidance it will be tempting for developers to keep
putting off refactoring.</p>
<p dir="auto">TDD is similar to the One Thing Rule we discsused earlier in that it is
biased: it provides very strong and clear instructions pushing developers
in one direction (in this case, acting tactically) with only vague
guidance in the other direction (designing more strategically). As a result,
developers are likely to err on the side of being too tactical.</p>
<p dir="auto">TDD guarantees that developers will initially write bad code. If you start
writing code without thinking about the whole design problem, the first code
you write will almost certainly be wrong. Design only
happens after a bunch of bad code has accumulated.
I watched your video on TDD, and
you repeatedly wrote the wrong code, then fixed it later. If the developer
refactors conscientiously (as you did) they can still end up with good
code, but this works against human nature. With TDD, that bad code will
actually work (there are tests to prove it!) and it's human nature not
to want to change something that
works. If the code I'm developing is nontrivial, I will probably have to
accumulate a lot of bad code with TDD before I have enough code in front
of me to understand what the design should have been.
It will be very difficult for me to force myself to throw away
all that work.</p>
<p dir="auto">It's easy for a developer to believe they are doing TDD correctly while
working entirely tactically, layering on hack after hack with an
occasional minor refactor, without ever thinking about the overall design.</p>
<p dir="auto">I believe that the bundling approach is superior to TDD because it focuses
the development process around design: design first, then code, then write
unit tests. Of course, refactoring will still be
required: it's almost never possible to get the design right the first time.
But starting with design will reduce the amount of bad code you write and
get you to a good design sooner. It is possible to produce equally good
designs with TDD; it's just harder and requires a lot more discipline.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I'll address your points one at a time.</p>
<ul dir="auto">
<li>
<p dir="auto">I haven't found that the scale of TDD is so tactical that it discourages thinking.  Every programmer, regardless of their testing discipline, writes code one line at a time.  That's immensely tactical and yet does not discourage design.  So why would one test at a time discourage it?</p>
</li>
<li>
<p dir="auto">The literature on TDD strongly discourages delaying refactoring.  While thinking about design is strongly encouraged.  Both are integral parts of the discipline..</p>
</li>
<li>
<p dir="auto">We all write bad code at the start.  The discipline of TDD gives us the opportunity, and the safety, to continuously clean it.  Design insights arise from those kinds of cleaning activities.  The discipline of refactoring allows bad designs to be transformed, one step at a time, into better designs.</p>
</li>
<li>
<p dir="auto">It's not clear to me why the act of writing tests late is a better design choice.  There's nothing in TDD that prevents me from thinking through a design long before I write the very first tested code.</p>
</li>
</ul>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You say there is nothing about TDD that stops developers from thinking ahead
about design. This is only partly true. Under TDD I can think ahead, but I
can't actually write my ideas down in the form of code, since that would
violate TDD Rule 1. This is a significant discouragement.</p>
<p dir="auto">You claim that "thinking about design is strongly encouraged" in TDD,
but I haven't seen this in your discussions of TDD. I watched your
video example of using TDD
for computing bowling scores, and design is never even mentioned after the
first minute or two (ironically, one of the conclusions of this
example is that the brief initial design turned out to be
useless). There is no suggestion of thinking ahead in the video;
it's all about cleaning up messes after the fact.
In all of the TDD materials you have shown me, I have not seen any
warnings about the dangers of becoming so tactical with TDD that
design never occurs (perhaps you don't even view this as a serious risk?).</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I usually use an abbeviated form of UML to capture my early design decisions.  I have no objection to capturing them in pseudo-code, or even real code.  However, I would not commit any such pre-written code.  I would likely hold it in a text file, and consult it while following the TDD cycle.  I might feel safe enough to copy and paste from the text file into my IDE in order to make a failing test pass.</p>
<p dir="auto">The Bowling game is an example of how wildly our initial design decisions can  deviate from our eventual solutions.  It's true that introductory videos often do not expose the depth of a discipline.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">As I was watching your TDD video for the second time, you said something
that jumped out at me:</p>
<blockquote>
<p dir="auto">Humans consider things that come first to be important and things that
come at the end to be less important and somehow optional; that's
why they are at the end, so we can leave them out if we have to.</p>
</blockquote>
<p dir="auto">This captures perfectly my concern about TDD. TDD insists that tests must
come first, and design, if it happens at all, comes at the end, after
code is working. I believe that good design is the most important
thing, so it must be the top priority. I don't consider tests optional,
but delaying them is safer than delaying design. Writing tests isn't particularly
difficult; the most important thing is having the discipline to do it.
Getting a good design is really hard, even if you are very disciplined;
that's why it needs to be the center of attention.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">TDD is a coding discipline.  Of course design comes before coding -- I don't know anyone who thinks otherwise.  Even the Bowling Game video made that point. But, as we saw in the Bowling Game video, sometimes the code will take you in a very different direction.</p>
<p dir="auto">That difference does't imply that the design shouldn't have been done.  It just implies that designs are speculative and may not aways survive reality.</p>
<p dir="auto">As Eisenhower once said:</p>
<blockquote>
<p dir="auto">“In preparing for battle I have always found that plans are useless, but planning is indispensable.”</p>
</blockquote>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You ask why writing tests later is a better design choice. It isn't.
The benefit of the bundled approach doesn't come from writing tests later;
it comes from doing design sooner. Writing tests (a bit) later is a
consequence of this choice. The tests are still written pretty early-on
with the bundled approach, so I don't think the delay causes significant
problems.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think we simply disagree that TDD discourages design.  The practice of TDD does not discourage me from design; because I value design.  I would suggest that those who do not value design will not design, no matter what discipline they practice.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You claim that the problems I worry about with TDD simply don't happen in
practice. Unfortunately I have heard contrary claims from senior
developers that I trust. They complain about horrible code produced by
TDD-based teams, and they believe that the problems were caused by TDD.
Of course horrible code can be produced with any design approach.
And maybe those teams didn't implement TDD properly, or maybe those
cases were outliers.
But the problems reported to me line up exactly with what I would
expect to happen, given the tactical nature of TDD.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">My experience differs. I've worked on many projects where TDD has been used
effectively and profitably.  I'm sure the senior developers that you trust are telling you the truth about their experience.  Having never seen TDD lead to such bad outcomes myself, I sincerely doubt that the blame can be traced to TDD.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You ask me to trust your extensive experience with
TDD, and I admit that I have no personal experience with TDD.
On the other hand, I have a lot of experience with tactical programming,
and I know that it rarely ends well.
TDD is one of the most extreme forms of tactical programming I've
encountered.
In general, if "making it work" is the #1 prority, instead of
"develop a clean design", code turns to spaghetti.
I don't see enough safeguards in your approach to TDD
to prevent the disaster scenarios; I don't even see a clear
recognition of the risk.</p>
<p dir="auto">Overall, TDD is in a bad place on the risk-reward spectrum. In comparison
to the bundling approch, the downside risks for poor code quality in TDD
are huge, and I don't see enough upside reward (if any) to compensate.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">All I can say to that is that your opinion is based on a number of false impressions and speculations, and not upon direct experience.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Now let me ask you a couple of questions.</p>
<p dir="auto">First, at a microscopic level, why on earth does TDD prohibit developers
from writing more code than needed to pass the current test? How does
enforcing myopia make systems better?</p>
<blockquote>
<p dir="auto"><strong>UB:</strong>
The goal of the discipline is to make sure that everything is tested.
One good way to do that is to refuse to write any code unless it is to make a failing test pass.  Also, working in such short cycles provides insights into
the way the code is working.  Those insights often lead to better design decisions.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>JOHN:</strong>
I agree that seeing code (partially) working can provide insights. But
surely that benefit can be had without such a severe restriction on
how developers think?</p>
</blockquote>
<p dir="auto">Second, at a broader level, do you think TDD is likely to produce better
designs than approaches that are more design-centric, such as the bundling
approach I described? If so, can you explain why?</p>
<blockquote>
<p dir="auto"><strong>UB:</strong>
My guess is that someone adept at bundling, and someone adept at TDD would produce very similar designs, with very similar test coverage.  I would also venture to guess that the TDDer would be somewhat more productive than the bundler if for no reason other than that the TDDer finds and fixes problems earlier than the bundler.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>JOHN:</strong>
I think that the bundling approach will result in a better design because
it actually focuses on design, rather than focusing on tests and hoping
that a good design will magically emerge. I think it's really hard to argue
that the best way to achieve one thing is to focus your attention on
something else. And the bundling approach will
make progress faster because the early thinking about design will reduce the
amount of bad code you end up having to throw away under TDD. Overall, I'd
argue that the best-case outcomes for the two approaches will
be about the same, but average and (especially) worst-case outcomes will
be far worse for TDD.</p>
</blockquote>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't think we're going to resolve our disagreements on TDD.
To do that, we'd need empirical data about the frequency of good and bad
outcomes from TDD. Unfortunately I'm not aware of any such data.
Thus, readers will have to decide for themselves whether the potential
benefits of TDD outweigh the risks.</p>
<p dir="auto">For anyone who chooses to use TDD, I urge you to do so with extreme
caution. Your primary goal must not be just working code, but rather a
clean design that will allow you to develop quickly in the future.
TDD will not lead you naturally to the best design, so you will need
to do significant and continuous refactoring to avoid spaghetti code.
Ask yourself repeatedly "suppose that I knew everything I know now when
I first started on this project; would I have chosen the current
structure for the code?" When the answer is no (which will happen
frequently) stop and refactor. Recognize that TDD will cause you to
write more bad code than you may be used to, so
you must be prepared to throw out and rewrite more than you are used to.
Take time to plan ahead and think about the overall design, rather than
just making the next test work.
If you do all of these things diligently, I think it is possible to
mitigate the risks of TDD and produce well-designed code.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Let's just say that I agree with all that advice, but disagree with your assertion that TDD might be the cause of bad code.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">TDD Summary</h3><a id="user-content-tdd-summary" aria-label="Permalink: TDD Summary" href="#tdd-summary"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Here is my attempt to summarize our thoughts on Test-Driven Development:</p>
<ul dir="auto">
<li>
<p dir="auto">We agree that unit tests are an essential element in software development.
They allow developers to make significant changes to a system without fear
of breaking something.</p>
</li>
<li>
<p dir="auto">We agree that it is possible use TDD to produce systems with good designs.</p>
</li>
<li>
<p dir="auto">I believe that TDD discourages good design and can easily lead to very bad
code. You do not believe that TDD discourages good
design and don't see much of a risk of bad code.</p>
</li>
<li>
<p dir="auto">I believe that there are better approaches than TDD for producing good
unit test suites, such as the "bundling" approach discussed above. You agree
that bundling can produce outcomes just as good as TDD but think it may lead to
somewhat less test coverage.</p>
</li>
<li>
<p dir="auto">I believe that TDD and bundling have similar best-case outcomes, but that
the average and worst-case outcomes will be much worse for TDD. You disagree
and believe that, if anything, TDD may produce marginally better outcomes
than bundling. You also think that preference and personality are larger factors in
making the choice between the two.</p>
</li>
</ul>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">This is a fair summary of our discussion.  We seem to disagree over the best application
of discipline.  I prefer a disciplined approach to keep the code covered by tests
written first in very short cycles.  You prefer a disciplined approach of writing relatively longer
bundles of code and then writing tests for those bundles.  We disagree on the risks and rewards of
these two disciplines.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Closing Remarks</h2><a id="user-content-closing-remarks" aria-label="Permalink: Closing Remarks" href="#closing-remarks"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">First, I'd like to thank you for tolerating (and responding to) the arguments
I have made about some of the key ideas in <em>Clean Code</em>. I hope this
discussion will provide food for thought for readers.</p>
<p dir="auto">We have covered a lot of topics and subtopics in this discussion, but
I think that most of my concerns result from two general errors made
by <em>Clean Code</em>: failure to focus on what is important, and failure to
balance design tradeoffs.</p>
<p dir="auto">In software design (and probably in any design environment) it is essential
to identify the things that really matter and focus on those. If you
focus your attention on things that are unimportant you are
unlikely to achieve the things that really are important.
Unfortunately, <em>Clean Code</em> repeatedly focuses on things that don't really
matter, such as:</p>
<ul dir="auto">
<li>Dividing ten-line methods into five-line methods and dividing five-line methods
into two- or three-line methods.</li>
<li>Eliminating the use of comments written in English.</li>
<li>Writing tests before code and making the basic unit of development a
test rather than an abstraction.</li>
</ul>
<p dir="auto">None of these provides significant value, and we have seen how they
distract from producing the best possible designs.</p>
<p dir="auto">Conversely, <em>Clean Code</em> fundamentally undervalues comments, which are
essential and irreplaceable. This
comes at a huge cost. Without interface comments the specifications for
interfaces are incomplete. This is guaranteed to result in confusion and bugs.
Without implementation comments, readers are forced to rederive knowledge
and intentions that were in the mind of the original developer. This wastes
time and leads to more bugs.</p>
<p dir="auto">In my opening remarks I said that systems become complex when important
information is not accessible and obvious to developers. By refusing to
write comments, you are hiding important information that you have and
that others need.</p>
<p dir="auto">The second general error in <em>Clean Code</em> has to do with balance. Design
represents a balance between competing concerns. Almost any design idea
becomes a bad thing if taken to the extreme. However, <em>Clean Code</em>
repeatedly gives very strong advice in one direction without correspondingly
strong advice in the other direction or any meaningful guidance about how
to recognize when you have gone too far. For example, making methods
shorter is often a good thing, but the <em>Clean Code</em> position is so one-sided
and extreme that readers are likely to chop things up too much. We saw
in the <code>PrimeGenerator</code> example how this resulted in code that was
nearly incomprehensible. Similarly, the <em>Clean Code</em> position on TDD is
one-sided, failing to
recognize any possible weakness and encouraging readers to take this to
a tactical extreme where design is completely squeezed out of the development
process.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">John, I'd like to thank you for participating in this project.  This was a lot of fun for me.  I love disagreement and debate with smart people.  I also think that we share far more values than separate us.</p>
<p dir="auto">For my part I'll just say that I have given due consideration to the points you've made, and while I disagree with your conclusions above, I have integrated several of your better ideas, as well as this entire document, into the second edition of <em>Clean Code</em>.</p>
<p dir="auto">Thanks again, and give my best to your students.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["The closer to the train station, the worse the kebab" – a "study" (480 pts)]]></title>
            <link>https://www.jmspae.se/write-ups/kebabs-train-stations/</link>
            <guid>43165112</guid>
            <pubDate>Mon, 24 Feb 2025 21:25:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jmspae.se/write-ups/kebabs-train-stations/">https://www.jmspae.se/write-ups/kebabs-train-stations/</a>, See on <a href="https://news.ycombinator.com/item?id=43165112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				

<p><strong>2025-02-14</strong></p>

    <ul>
    
        <li>
            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#introduction">Introduction</a>
            
        </li>
    
        <li>
            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#method">Method</a>
            
                <ul>
                    
                        <li>
                            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#network-data">Network Data</a>
                        </li>
                    
                        <li>
                            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#restaurant-data">Restaurant Data</a>
                        </li>
                    
                        <li>
                            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#routing-and-distance">Routing and Distance</a>
                        </li>
                    
                </ul>
            
        </li>
    
        <li>
            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#results">Results</a>
            
        </li>
    
        <li>
            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#discussion">Discussion</a>
            
        </li>
    
    </ul>

<p><em>This write-up was originally posted <a href="https://www.reddit.com/r/gis/comments/1iph0yy/the_closer_to_the_railway_station_the_less_tasty/">on reddit</a>, though I've cleaned things up specifically for this post. Due to reasons discussed towards the end of this post, I'm not entirely happy with the results and intend to take another shot at it in the near future.</em></p>
<h2 id="introduction">Introduction<a href="#introduction" aria-label="Anchor link for: introduction">🔗</a></h2>
<p>I came across <a href="https://www.reddit.com/r/gis/comments/1iopp56/anyone_motivated_to_prove_that_the_closer_from/">this post</a> sharing a hypothesis from a French subreddit;</p>
<blockquote>
<p>The closer to the train station, the worse the kebab.</p>
</blockquote>
<p>The original French post gained a decent amount of traction compared to the subreddit's relatively small size, indicating a certain amount of agreement among its members. There were some detractors in the comments, however, sharing experiences which ran contrary to the stated hypothesis.</p>
<p>Thus, I figured I had nothing better to do, being a burned-out, unemployed drop-out with a newly-obtained autism diagnosis, so I figured I'd sacrifice my time for a worthy cause and perform this informal <em>"study"</em>. I'll be expecting my Nobel peace prize in the postbox and several job offers in my DMs within the next 3 working days.</p>
<h2 id="method">Method<a href="#method" aria-label="Anchor link for: method">🔗</a></h2>
<p>I assumed the best study area to be Paris, France since;</p>
<ol>
<li>The original post was French</li>
</ol>
<p>I haven't personally heard of this hypothesis in my home country (Sweden, also home to many a kebab-serving restaurant) so I figured I'd assume this to be a French phenomenon for the purpose of this informal "Study".</p>
<ol start="2">
<li>Density</li>
</ol>
<p>The inner city is <em><strong>dense</strong></em> with dozens of train/metro stations and god knows how many kebab shops. I knew early on that this would make my life pretty miserable, but at least it'd provide plenty of sample data.</p>
<h2 id="network-data">Network Data<a href="#network-data" aria-label="Anchor link for: network-data">🔗</a></h2>
<p>I used OSMnx to download and save a navigation network. Given the public transit-centric nature of the French subreddit, I though it'd make sense to stick to walking distance (eg. footpaths, side-walks) thus i set the OSMnx <code>network_type</code> to <code>"walk"</code>. Given the location (and that OSMnx used this CRS automatically when none was provided), all data was projected to EPSG:32631 (UTM zone 31N).</p>
<pre data-lang="py"><code data-lang="py"><span>import </span><span>osmnx </span><span>as </span><span>ox
</span><span>from </span><span>geopandas </span><span>import </span><span>GeoDataFrame
</span><span>
</span><span>#EPSG
</span><span>PROJECTION </span><span>= </span><span>32631
</span><span>
</span><span>graph </span><span>= </span><span>ox.</span><span>graph_from_place</span><span>(</span><span>'Paris, FR'</span><span>, </span><span>network_type</span><span>=</span><span>"walk"</span><span>)
</span><span>graph </span><span>= </span><span>ox.</span><span>project_graph</span><span>(graph, </span><span>to_crs</span><span>=</span><span>PROJECTION</span><span>)
</span><span>
</span><span>ox.</span><span>save_graphml</span><span>(graph, </span><span>filepath</span><span>=</span><span>"network.graphml"</span><span>)
</span></code></pre>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-1.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-1.c53f23b9b1fd351c.jpg"></a>

<em>Figure 1: The study area and network</em></p>
<p>Next up is the various train/metro stations. Given the nature of the original French sub, I figured it'd make sense to include both the long-distance central stations along with the countless metro stations. This was also rather trivial with OSMnx, filtering by <code>railway=subway_entrance</code> or <code>railway=train_station_entrance.</code></p>
<pre data-lang="py"><code data-lang="py"><span>stations: GeoDataFrame </span><span>= </span><span>ox.</span><span>features_from_place</span><span>(</span><span>'Paris, FR'</span><span>, </span><span>tags </span><span>= </span><span>{
</span><span>    </span><span>"railway"</span><span>: [</span><span>"subway_entrance"</span><span>, </span><span>"train_station_entrance"</span><span>]
</span><span>})
</span><span>
</span><span># Filter results to points
</span><span>station_nodes: GeoDataFrame </span><span>= </span><span>stations.loc[stations.geom_type</span><span>==</span><span>"Point"</span><span>]
</span><span>station_nodes </span><span>= </span><span>station_nodes.</span><span>to_crs</span><span>(</span><span>epsg</span><span>=</span><span>PROJECTION</span><span>)
</span><span>
</span><span>station_nodes.</span><span>to_file</span><span>(</span><span>"train_station_entrances.gpkg"</span><span>)
</span></code></pre>
<p>I saved outputs religiously so I could easily inspect them in QGIS. I did attempt to get python notebooks working with my NeoVIM setup, but it was all for naught.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-2.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-2.51f0e47dcd63c0a8.jpg"></a>

<em>Figure 2: Rail/metro entrances... Please ignore the airport iconography.</em></p>
<p>... And there we have the first half of the data, now for the restaurants.</p>
<h2 id="restaurant-data">Restaurant Data<a href="#restaurant-data" aria-label="Anchor link for: restaurant-data">🔗</a></h2>
<p>The Google Places API (and their respective reviews) seemed like a reasonable choice. Google reviews are naturally far from perfect and subject to their fair share of botting and the like, but it's the best I could think of at the time. There are alternatives such as Yelp, but their API is horrifically expensive for poor old me, and I was not in the mood to build a web scraper (it has the same soul-sucking effect on me as prompting an LLM). The $200 of free credit was also enticing.</p>
<p>However, as I started exploring the API... I realised that the Places API doesn't seem to have any way to search within a polygon, only within a point radius. Thank you, Mr. publicly owned mega-corporation. How Fun.</p>
<p>It also didn't help that autocomplete for the <code>googlemaps</code> library wasn't working. Python's a fine language, but its tooling does like to test my patience a little too often. And whilst I'm still complaining... The Google Cloud dashboard is likely the slowest "website" I've ever had the displeasure of interacting with.</p>
<p>So... This meant I'd have to perform some sort of grid search of the whole of Paris, crossing my fingers that I wouldn't bust my free usage. This, along with a couple interesting questions;</p>
<ol>
<li>What is... <em>A kebab?</em></li>
</ol>
<p>When I search for "kebab" (no further context necessary)... How does Google decide what restaurant serves kebab?</p>
<p>After some perusing, it didn't seem to be as deep as I thought. Plenty of restaurants simply had "kebab" in the name, some were designated as "Mediterranean" (Kebab has its origins in Turkey, Persia, middle east in general) and others had a fair few reviews simply mentioning "kebab." Good enough for me.</p>
<ol start="2">
<li>Trouble in query-land</li>
</ol>
<p>It turns out that when you query for places within a given radius, it's only a "bias." It's not a hard cut-off that'll help narrow-down our data harvesting and reduce unnecessary requests. It was becoming increasingly clear that google isn't really a fan of people doing this.</p>
<p>Now with all of that preamble out of the way, I needed to prepare my search.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-3.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-3.605c84ffddad8323.jpg"></a>

<em>Figure 3. Original admin boundaries</em></p>
<p>Paris' administrative boundary contains a couple of large green spaces. To the west, a park and to the east, some sort of sports institute.</p>
<p>After perusing these rather large spaces in Google maps, they seemed to contain a distinct lack of kebab-serving establishments. Thus, they were a burden on our API budget and needed to go.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-4.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-4.b360881962350eea.jpg"></a>

<em>Figure 4. Adjusted admin boundaries w/ network</em></p>
<p>I figured keeping the network and stations wouldn't do any harm, so they went unmodified.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-5.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-5.eedb86cb4c85e12c.jpg"></a>

<em>Figure 5. Sampling points, later projected to WGS84 for harvesting purposes</em></p>
<p>To maximise data-harvesting, I decided to go with a hex layout with a vertical spacing of 1 km. This should give us a search radius of 500m * √3 ~= 866 meters. Plenty of overlap, sure, but we shouldn't be getting any holes anywhere. I'm not sure why I was spending this much time ensuring "data integrity" when that might just have flown the window courtesy of Google, but it's the illusion of control that counts.</p>
<p>This give us 99 sample points which... Should be enough?</p>
<p>Regardless, here's how my 3AM python turned out:</p>
<pre data-lang="py"><code data-lang="py"><span># Already projected to WGS84
</span><span>sample_points: GeoDataFrame </span><span>= </span><span>GeoDataFrame.</span><span>from_file</span><span>(</span><span>"samples.gpkg"</span><span>)
</span><span>gmaps: googlemaps.Client </span><span>= </span><span>googlemaps.</span><span>Client</span><span>(</span><span>key</span><span>=</span><span>'get-your-own'</span><span>)
</span><span>
</span><span>output </span><span>= </span><span>{}
</span><span>
</span><span>for </span><span>point </span><span>in </span><span>sample_points.geometry:
</span><span>    lat, lon </span><span>= </span><span>point.y, point.x
</span><span>
</span><span>    next_page_token </span><span>= </span><span>None
</span><span>    num_fetches </span><span>= </span><span>3
</span><span>
</span><span>    </span><span>while </span><span>num_fetches </span><span>&gt; </span><span>0</span><span>:
</span><span>        result </span><span>= </span><span>{}
</span><span>
</span><span>        </span><span>if </span><span>next_page_token </span><span>== </span><span>None</span><span>:
</span><span>            result </span><span>= </span><span>gmaps.</span><span>places</span><span>(
</span><span>                </span><span>"kebab"</span><span>,
</span><span>                </span><span>location</span><span>=</span><span>(lat, lon),
</span><span>                </span><span>radius</span><span>=</span><span>866</span><span>,
</span><span>            )
</span><span>        </span><span>else</span><span>:
</span><span>            result </span><span>= </span><span>gmaps.</span><span>places</span><span>(
</span><span>                </span><span>page_token</span><span>=</span><span>next_page_token
</span><span>            )
</span><span>
</span><span>        next_page_token </span><span>= </span><span>result.</span><span>get</span><span>(</span><span>"next_page_token"</span><span>)
</span><span>        </span><span>print</span><span>(result[</span><span>"status"</span><span>], next_page_token)
</span><span>
</span><span>        </span><span>for </span><span>p </span><span>in </span><span>result[</span><span>"results"</span><span>]:
</span><span>            output[p[</span><span>"place_id"</span><span>]] </span><span>= </span><span>p
</span><span>
</span><span>        </span><span>if </span><span>next_page_token </span><span>== </span><span>None</span><span>:
</span><span>            </span><span>break
</span><span>
</span><span>        num_fetches </span><span>-= </span><span>1
</span><span>
</span><span>        </span><span>sleep</span><span>(</span><span>2</span><span>)
</span><span>
</span><span>json_out </span><span>= </span><span>json.</span><span>dumps</span><span>(output)
</span><span>
</span><span>with </span><span>open</span><span>(</span><span>"output.json"</span><span>, </span><span>"w"</span><span>) </span><span>as </span><span>file:
</span><span>    file.</span><span>write</span><span>(json_out)
</span></code></pre>
<p>This worked quite well. Initially I skipped paging, resulting in 322 results. However, I noticed that a few establishments were missing in the results compared to my explorations in Google Maps.</p>
<p>After implementing paging and re-running, this gave us a grand total of 400 kebab-serving establishments. I was likely over-zealous with the paging considering how few additional results were retrieved. That, and that the API doesn't cap the search radius (again, it's only a bias) likely led to a fair few redundant API calls.</p>
<p>The raw Google Places API-output also needed to be clipped to the study area, projected to the local UTM zone as well as converted to a geospatial format;</p>
<pre data-lang="py"><code data-lang="py"><span>import </span><span>pandas </span><span>as </span><span>pd
</span><span>
</span><span>with </span><span>open</span><span>(</span><span>"output.json"</span><span>, </span><span>"r"</span><span>) </span><span>as </span><span>file:
</span><span>    data </span><span>= </span><span>json.</span><span>load</span><span>(file)
</span><span>    file.</span><span>close</span><span>()
</span><span>    
</span><span>    </span><span>for </span><span>id </span><span>in </span><span>data:
</span><span>        place </span><span>= </span><span>data[</span><span>id</span><span>]
</span><span>        point </span><span>= </span><span>place[</span><span>"geometry"</span><span>][</span><span>"location"</span><span>]
</span><span>        data[</span><span>id</span><span>][</span><span>"lng"</span><span>] </span><span>= </span><span>point[</span><span>"lng"</span><span>]
</span><span>        data[</span><span>id</span><span>][</span><span>"lat"</span><span>] </span><span>= </span><span>point[</span><span>"lat"</span><span>]
</span><span>        </span><span>del </span><span>data[</span><span>id</span><span>][</span><span>"geometry"</span><span>]
</span><span>
</span><span>    data </span><span>= </span><span>pd.DataFrame.</span><span>from_dict</span><span>(data).T
</span><span>    data.rating </span><span>= </span><span>pd.</span><span>to_numeric</span><span>(data.rating)
</span><span>    data.user_ratings_total </span><span>= </span><span>pd.</span><span>to_numeric</span><span>(data.user_ratings_total)
</span><span>    data </span><span>= </span><span>data[data[</span><span>"user_ratings_total"</span><span>] </span><span>&gt; </span><span>0</span><span>]
</span><span>
</span><span>    </span><span># Cleanup was added after the screenshot below was taken
</span><span>    data </span><span>= </span><span>data.</span><span>drop</span><span>(</span><span>columns</span><span>=</span><span>[
</span><span>        </span><span>"icon"</span><span>,
</span><span>        </span><span>"icon_background_color"</span><span>,
</span><span>        </span><span>"icon_mask_base_uri"</span><span>,
</span><span>        </span><span>"plus_code"</span><span>,
</span><span>        </span><span>"reference"</span><span>,
</span><span>        </span><span>"photos"</span><span>,
</span><span>        </span><span>"opening_hours"
</span><span>    ])
</span><span>
</span><span>    gdata </span><span>= </span><span>GeoDataFrame</span><span>(
</span><span>        data, </span><span>geometry</span><span>=</span><span>geopandas.</span><span>points_from_xy</span><span>(data.lng, data.lat),
</span><span>        </span><span>crs</span><span>=</span><span>4326
</span><span>    )
</span><span>
</span><span>    gdata: GeoDataFrame </span><span>= </span><span>gdata.</span><span>to_crs</span><span>(</span><span>PROJECTION</span><span>)
</span><span>
</span><span>    </span><span># Modified boundaries from Figure 4.
</span><span>    paris </span><span>= </span><span>GeoDataFrame.</span><span>from_file</span><span>(</span><span>"mod_bounary.gpkg"</span><span>);
</span><span>
</span><span>    gdata: GeoDataFrame </span><span>= </span><span>gdata.</span><span>clip</span><span>(paris)
</span><span>
</span><span>    gdata.</span><span>to_file</span><span>(</span><span>"establishments.gpkg"</span><span>)
</span></code></pre>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-6.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-6.1eab106a30493ee2.jpg"></a>

<em>Figure 6. We're in f###ing business</em></p>
<h2 id="routing-and-distance">Routing and Distance<a href="#routing-and-distance" aria-label="Anchor link for: routing-and-distance">🔗</a></h2>
<p>Finally, the fun part. I need to get the distance to the nearest station entrance for each establishment.</p>
<p>I could've absolutely just routed to every single entrance for every single restaurant to get the nearest... But that would've taken several decades. I needed to build some sort of spatial index and route to the nearest ~3 or something along those lines. Since Paris is so dense with plenty of routing options, I figured I wouldn't need to perform too many routing operations.</p>
<p>After some googling and dredging through API docs, however, it seemed GeoPandas was nice enough to do that for us with <code>sindex</code>. Although it didn't have the same "return nearest N" like my beloved r-tree rust library I was all too used to, it did allow me to search within a certain radius (1 km was large enough) and go from there. The query results weren't sorted, so I had to sort the indexes by distance and cut it down to size.</p>
<p>The network analysis was relatively straight-forward thanks to NetworkX, and after a couple of hours I managed to cobble together the following;</p>
<pre data-lang="py"><code data-lang="py"><span>import </span><span>networkx </span><span>as </span><span>nx
</span><span>import </span><span>shapely </span><span>as </span><span>shp
</span><span>
</span><span>establishments: GeoDataFrame </span><span>= </span><span>GeoDataFrame.</span><span>from_file</span><span>(</span><span>"establishments.gpkg"</span><span>)
</span><span>entrances: GeoDataFrame </span><span>= </span><span>GeoDataFrame.</span><span>from_file</span><span>(</span><span>"entrances.gpkg"</span><span>)
</span><span>graph </span><span>= </span><span>ox.</span><span>load_graphml</span><span>(</span><span>"network.graphml"</span><span>)
</span><span>graph </span><span>= </span><span>ox.</span><span>project_graph</span><span>(graph, </span><span>to_crs </span><span>= </span><span>PROJECTION</span><span>)
</span><span>
</span><span># Ensure the same CRS
</span><span>if </span><span>(establishments.crs </span><span>!= </span><span>entrances.crs </span><span>!= </span><span>PROJECTION</span><span>):
</span><span>    </span><span>exit</span><span>(</span><span>100</span><span>)
</span><span>
</span><span># Helper function to get the distance between a graph node and establishment geometry
</span><span>def </span><span>node_geom_dist</span><span>(</span><span>node_id</span><span>: int, </span><span>geom</span><span>: shp.Point):
</span><span>    node </span><span>= </span><span>graph.nodes[node_id]
</span><span>    </span><span>return </span><span>math.</span><span>sqrt</span><span>((geom.x </span><span>- </span><span>node[</span><span>'x'</span><span>]) </span><span>** </span><span>2 </span><span>+ </span><span>(geom.y </span><span>- </span><span>node[</span><span>'y'</span><span>]) </span><span>** </span><span>2</span><span>)
</span><span>
</span><span>distances: list[float] </span><span>= </span><span>[]
</span><span>
</span><span>for </span><span>(</span><span>id</span><span>, establishment) </span><span>in </span><span>establishments.</span><span>iterrows</span><span>():
</span><span>    establishment_geom: shp.Point </span><span>= </span><span>establishment.geometry
</span><span>    establishment_node: int </span><span>= </span><span>ox.</span><span>nearest_nodes</span><span>(graph, establishment_geom.x, establishment_geom.y)
</span><span>    establishment_dist_to_node: float </span><span>= </span><span>node_geom_dist</span><span>(establishment_node, establishment_geom)
</span><span>    
</span><span>    </span><span># Spatial index for rail entrances
</span><span>    index: shp.STRtree </span><span>= </span><span>entrances.sindex
</span><span>    nearest_q </span><span>= </span><span>index.</span><span>query</span><span>(establishment_geom, </span><span>predicate</span><span>=</span><span>"dwithin"</span><span>, </span><span>distance </span><span>= </span><span>1000</span><span>)
</span><span>    nearest_entrances: list[tuple[int, float]] </span><span>= </span><span>[]
</span><span>
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>nearest_q:
</span><span>        ent </span><span>= </span><span>entrances.iloc[i]
</span><span>        ent_geom: shp.Point </span><span>= </span><span>ent.geometry
</span><span>
</span><span>        dist </span><span>= </span><span>ent_geom.</span><span>distance</span><span>(establishment.geometry)
</span><span>        
</span><span>        nearest_entrances.</span><span>append</span><span>((i, dist))
</span><span>     
</span><span>    nearest_entrances </span><span>= </span><span>sorted</span><span>(nearest_entrances, </span><span>key </span><span>= lambda </span><span>e</span><span>: e[</span><span>1</span><span>])[:</span><span>3</span><span>]
</span><span>    entrance_geom: list[shp.Point] </span><span>= </span><span>[entrances.iloc[i].geometry </span><span>for </span><span>(i, </span><span>_</span><span>) </span><span>in </span><span>nearest_entrances]
</span><span>    entrance_nodes: list[int] </span><span>= </span><span>[ox.</span><span>nearest_nodes</span><span>(graph, point.x, point.y) </span><span>for </span><span>point </span><span>in </span><span>entrance_geom]
</span><span>    entrance_geom_dist_to_node: list[float] </span><span>= </span><span>[</span><span>node_geom_dist</span><span>(entrance_nodes[i], entrance_geom[i]) </span><span>for </span><span>i </span><span>in </span><span>range</span><span>(</span><span>len</span><span>(nearest_entrances))]
</span><span>
</span><span>    result_paths </span><span>= </span><span>[nx.</span><span>shortest_path</span><span>(graph, establishment_node, dest_node, </span><span>weight</span><span>=</span><span>"length"</span><span>) </span><span>for </span><span>dest_node </span><span>in </span><span>entrance_nodes]
</span><span>    result_lengths: list[float] </span><span>= </span><span>[nx.</span><span>path_weight</span><span>(graph, path, </span><span>"length"</span><span>) </span><span>+ </span><span>entrance_geom_dist_to_node[i] </span><span>+ </span><span>establishment_dist_to_node </span><span>for </span><span>(i, path) </span><span>in </span><span>enumerate</span><span>(result_paths)]
</span><span>
</span><span>    distances.</span><span>append</span><span>(</span><span>min</span><span>(result_lengths))
</span><span>
</span><span>establishments[</span><span>"distance"</span><span>] </span><span>= </span><span>distances 
</span><span>establishments.</span><span>to_file</span><span>(</span><span>"establishment_results.gpkg"</span><span>)
</span></code></pre>
<p>Not exactly my finest work. The sheer amount of list comprehension is perhaps a little terrifying, but it works.</p>
<p>After some prodding around in QGIS with the resulting data and networks (and many print() statements), I was confident in the accuracy of the results.</p>
<h2 id="results">Results<a href="#results" aria-label="Anchor link for: results">🔗</a></h2>
<p>Now with all of this data, it is time to settle the question of whether or not the kebabs are less tasty the closer they are to a train/metro station...</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-7.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-7.1ff7f2b316a90f62.jpg"></a>

<em>Figure 7. Hmmmmm....</em></p>
<p>With a mighty Pearson's correlation of 0.091, the data indicates that this could be true! If you ignore the fact that the correlation is so weak that calling it 'statistically insignificant' would be quite generous.</p>
<p>Outliers can have an outsized impact on a Pearson's correlation, so after ridding the dataset of some outliers via IQR fencing...</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-8.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-8.af2b5c5cacb7b32d.jpg"></a>

<em>Figure 8. Removed outliers</em></p>
<p>... This increased the coefficient to a whopping 0.098.</p>
<p>This was a bit of a bummer (though hardly surprising) and figuring I had nothing to lose from messing around a little, I tried filtering out metro stations in case my original assumption of the metro being included in the original hypothesis was incorrect.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-9.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-9.3427bdea5242e0b1.jpg"></a>

*Figure 9. Not much better, eh? Correction: "... Nearest train station entrance."</p>
<p>With an even worse coefficient of 0.001, I think It's time to hang up the towel.</p>
<p>Whilst there are some minor indications that the hypothesis <em>could</em> be correct (eg. Many of the absolute worst restaurants being some of the closest) the correlation is simply too weak.</p>
<h2 id="discussion">Discussion<a href="#discussion" aria-label="Anchor link for: discussion">🔗</a></h2>
<p><em><strong>- Are Google reviews an objective measurement of how tasty the kebabs are?</strong></em></p>
<p>Absolutely the f### not. This was a rather subjective observation from the very beginning and Google reviews aren't exactly a good measure of "is the food good?" There are many aspects of the dining experience that could hypothetically impact a review score. The staff, cleanliness, the surrounding environment, etc. Not to mention online skulduggery and review manipulation.</p>
<p><em><strong>- Can tourism have an impact?</strong></em></p>
<p>It absolutely could. I don't want to make any definitive assumptions, but I can absolutely imagine the local regulars being harsher than the massive tourist population, or even vice-versa.</p>
<p><em><strong>- Were the Google results accurate?</strong></em></p>
<p>To an extent, yes. From what I could gather, every location from the query seemed to serve kebab in some form. There were a few weird outliers and nuances, such as Pizza Hut which likely only serves kebab pizza rather than the multitude of different forms in which kebab could possibly be consumed.</p>
<p><em><strong>- Why not restaurants in general?</strong></em></p>
<p>Because the initial hypothesis was too comically hyper-specific for me to give up on.</p>
<p><em><strong>- What about review count?</strong></em></p>
<p>This could very well have an effect, though I was not entirely certain how to properly implement this metric into the analysis at the time.</p>
<p><em><strong>- Gib Data</strong></em></p>
<p>I'm not quite comfortable in doing so, mostly due to potential breaches of Google's TOS. I don't think they would care about me harvesting some 400 POIs for this little experiment, I'm not quite willing to gamble sharing the data with others.</p>
<p>Besides, I gave you the code. Go burn some of your own credits.</p>
<p><em><strong>- Are you Ok?</strong></em></p>
<p>... I guess? Are you?</p>
<p>In conclusion, this was actually quite fun. I wrote this as the project went on (otherwise I would likely never have found the motivation) and I would encourage others to do other silly explorations like this, even if the results end up slightly depressing.</p>
<p>... <em>However</em>, after some additional discussion, I decided I wasn't quite done.</p>
<p>As stated earlier, there were a few detracting comments on the original French post. Interestingly, many of the provided examples of good kebab restaurants next to train stations just so happened to be in Paris.</p>
<p>The user who originally posted the French post for the sub in English provided some <a href="https://imgur.com/gallery/kebab-railway-stations-wuYG9D2">examples</a> which seem to strengthen the hypothesis. It could very well be that whatever conditions affect Paris restaurants (whether it be higher rent, wages, tourism, population density...) had a larger impact than I initially suspected.</p>
<p><em><strong>Stay tuned for part 2... Whenever I get around to doing it!</strong></em></p>


			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude 3.7 Sonnet and Claude Code (1871 pts)]]></title>
            <link>https://www.anthropic.com/news/claude-3-7-sonnet</link>
            <guid>43163011</guid>
            <pubDate>Mon, 24 Feb 2025 18:28:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/claude-3-7-sonnet">https://www.anthropic.com/news/claude-3-7-sonnet</a>, See on <a href="https://news.ycombinator.com/item?id=43163011">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Today, we’re announcing Claude 3.7 Sonnet<sup>1</sup>, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made <a href="https://youtu.be/t3nnDXa81Hs">visible to the user</a>. API users also have fine-grained control over <em>how long</em> the model can think for.</p><p>Claude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. Along with the model, we’re also introducing a command line tool for agentic coding, Claude Code. Claude Code is available as a limited research preview, and enables developers to delegate substantial engineering tasks to Claude directly from their terminal.</p><div><figure><img alt="Screen showing Claude Code onboarding" loading="eager" width="1920" height="1080" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4a4df6b6629f9814aec4eb9323028130f43a8d70-1920x1080.png&amp;w=1920&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4a4df6b6629f9814aec4eb9323028130f43a8d70-1920x1080.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4a4df6b6629f9814aec4eb9323028130f43a8d70-1920x1080.png&amp;w=3840&amp;q=75"></figure></div><p>Claude 3.7 Sonnet is now available on all <a href="https://claude.ai/new">Claude</a> plans—including Free, Pro, Team, and Enterprise—as well as the <a href="https://docs.anthropic.com/en/docs/about-claude/models">Anthropic API</a>, <a href="https://aws.amazon.com/bedrock/claude/">Amazon Bedrock</a>, and <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude">Google Cloud’s Vertex AI</a>. Extended thinking mode is available on all surfaces except the free Claude tier.</p><p>In both standard and extended thinking modes, Claude 3.7 Sonnet has the same price as its predecessors: $3 per million input tokens and $15 per million output tokens—which includes thinking tokens.</p><h2 id="claude-37-sonnet-frontier-reasoning-made-practical">Claude 3.7 Sonnet: Frontier reasoning made practical</h2><p>We’ve developed Claude 3.7 Sonnet with a different philosophy from other reasoning models on the market. Just as humans use a single brain for both quick responses and deep reflection, we believe reasoning should be an integrated capability of frontier models rather than a separate model entirely. This unified approach also creates a more seamless experience for users.</p><p>Claude 3.7 Sonnet embodies this philosophy in several ways. First, Claude 3.7 Sonnet is both an ordinary LLM and a reasoning model in one: you can pick when you want the model to answer normally and when you want it to <a href="https://www.anthropic.com/research/visible-extended-thinking">think longer before answering</a>. In the standard mode, Claude 3.7 Sonnet represents an upgraded version of Claude 3.5 Sonnet. In <a href="https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking">extended thinking mode</a>, it self-reflects before answering, which improves its performance on math, physics, instruction-following, coding, and many other tasks. We generally find that prompting for the model works similarly in both modes.</p><p>Second, when using Claude 3.7 Sonnet through the API, users can also control the <em>budget </em>for thinking: you can tell Claude to think for no more than N tokens, for any value of N up to its output limit of 128K tokens. This allows you to trade off speed (and cost) for quality of answer.</p><p>Third, in developing our reasoning models, we’ve optimized somewhat less for math and computer science competition problems, and instead shifted focus towards real-world tasks that better reflect how businesses actually use LLMs.</p><p><a href="https://www.anthropic.com/claude/sonnet">Early testing</a> demonstrated Claude’s leadership in coding capabilities across the board: Cursor noted Claude is once again best-in-class for real-world coding tasks, with significant improvements in areas ranging from handling complex codebases to advanced tool use. Cognition found it far better than any other model at planning code changes and handling full-stack updates. Vercel highlighted Claude’s exceptional precision for complex agent workflows, while Replit has successfully deployed Claude to build sophisticated web apps and dashboards from scratch, where other models stall. In Canva’s evaluations, Claude consistently produced production-ready code with superior design taste and drastically reduced errors.</p><div><figure><img alt="Bar chart showing Claude 3.7 Sonnet as state-of-the-art for SWE-bench Verified" loading="lazy" width="1920" height="1145" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F08bba4487fb5ac1ba52540ee656d7e4da10ca1be-1920x1145.png&amp;w=1920&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F08bba4487fb5ac1ba52540ee656d7e4da10ca1be-1920x1145.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F08bba4487fb5ac1ba52540ee656d7e4da10ca1be-1920x1145.png&amp;w=3840&amp;q=75"><figcaption>Claude 3.7 Sonnet achieves state-of-the-art performance on SWE-bench Verified, which evaluates AI models’ ability to solve real-world software issues. See the appendix for more information on scaffolding.</figcaption></figure></div><div><figure><img alt="Bar chart showing Claude 3.7 Sonnet as state-of-the-art for TAU-bench" loading="lazy" width="1920" height="1114" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F787e59d548c230afd7efaed1bda1fb7f7ca207b8-1920x1114.png&amp;w=1920&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F787e59d548c230afd7efaed1bda1fb7f7ca207b8-1920x1114.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F787e59d548c230afd7efaed1bda1fb7f7ca207b8-1920x1114.png&amp;w=3840&amp;q=75"><figcaption>Claude 3.7 Sonnet achieves state-of-the-art performance on TAU-bench, a framework that tests AI agents on complex real-world tasks with user and tool interactions. See the appendix for more information on scaffolding.</figcaption></figure></div><div><figure><img alt="Benchmark table comparing frontier reasoning models" loading="lazy" width="2600" height="2360" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F654cf6680d32858dfba9af644f8c4a5b04425af1-2600x2360.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F654cf6680d32858dfba9af644f8c4a5b04425af1-2600x2360.png&amp;w=3840&amp;q=75"><figcaption>Claude 3.7 Sonnet excels across instruction-following, general reasoning, multimodal capabilities, and agentic coding, with extended thinking providing a notable boost in math and science. Beyond traditional benchmarks, it even outperformed all previous models in our <a href="https://www.anthropic.com/research/visible-extended-thinking">Pokémon gameplay tests</a>.</figcaption></figure></div><h2 id="claude-code">Claude Code</h2><p>Since June 2024, Sonnet has been the preferred model for developers worldwide. Today, we're empowering developers further by introducing Claude Code—our first agentic coding tool—in a limited research preview.</p><p>Claude Code is an active collaborator that can search and read code, edit files, write and run tests, commit and push code to GitHub, and use command line tools—keeping you in the loop at every step.</p><p>Claude Code is an early product but has already become indispensable for our team, especially for test-driven development, debugging complex issues, and large-scale refactoring. In early testing, Claude Code completed tasks in a single pass that would normally take 45+ minutes of manual work, reducing development time and overhead.</p><p>In the coming weeks, we plan to continually improve it based on our usage: enhancing tool call reliability, adding support for long-running commands, improved in-app rendering, and expanding Claude's own understanding of its capabilities.</p><p>Our goal with Claude Code is to better understand how developers use Claude for coding to inform future model improvements. By <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">joining this preview</a>, you’ll get access to the same powerful tools we use to build and improve Claude, and your feedback will directly shape its future.</p><h2 id="working-with-claude-on-your-codebase">Working with Claude on your codebase</h2><p>We’ve also improved the coding experience on Claude.ai. Our GitHub integration is now available on all Claude plans—enabling developers to connect their code repositories directly to Claude.</p><p>Claude 3.7 Sonnet is our best coding model to date. With a deeper understanding of your personal, work, and open source projects, it becomes a more powerful partner for fixing bugs, developing features, and building documentation across your most important GitHub projects.</p><h2 id="building-responsibly">Building responsibly</h2><p>We’ve conducted extensive testing and evaluation of Claude 3.7 Sonnet, working with external experts to ensure it meets our standards for security, safety, and reliability. Claude 3.7 Sonnet also makes more nuanced distinctions between harmful and benign requests, reducing <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card">unnecessary refusals by 45%</a> compared to its predecessor.</p><p>The <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card">system card</a> for this release covers new safety results in several categories, providing a detailed breakdown of our Responsible Scaling Policy evaluations that other AI labs and researchers can apply to their work. The card also addresses emerging risks that come with computer use, particularly prompt injection attacks, and explains how we evaluate these vulnerabilities and train Claude to resist and mitigate them. Additionally, it examines potential safety benefits from reasoning models: the ability to understand how models make decisions, and whether model reasoning is genuinely trustworthy and reliable. Read the full <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card">system card </a>to learn more.</p><h2 id="looking-ahead">Looking ahead</h2><p>Claude 3.7 Sonnet and Claude Code mark an important step towards AI systems that can truly augment human capabilities. With their ability to reason deeply, work autonomously, and collaborate effectively, they bring us closer to a future where AI enriches and expands what <a href="https://darioamodei.com/machines-of-loving-grace">humans can achieve</a>.</p><div><figure><img alt="Milestone timeline showing Claude progressing from assistant to pioneer" loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3bde9831ea84e3663fe4598589d71eaa531f9912-1920x1080.png&amp;w=1920&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3bde9831ea84e3663fe4598589d71eaa531f9912-1920x1080.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3bde9831ea84e3663fe4598589d71eaa531f9912-1920x1080.png&amp;w=3840&amp;q=75"></figure></div><p>We're excited for you to explore these new capabilities and to see what you’ll create with them. As always, we welcome your <a href="mailto: feedback@anthropic.com">feedback</a> as we continue to improve and evolve our models.</p></div></article><div><h4>Appendix</h4><p><sup>1 </sup>Lesson learned on <a href="https://www.anthropic.com/news/3-5-models-and-computer-use">naming</a>.</p><h3>Eval data sources</h3><ul><li><a href="https://x.ai/blog/grok-3">Grok</a></li><li><a href="https://developers.googleblog.com/en/gemini-2-family-expands/">Gemini 2 Pro</a></li><li><a href="https://openai.com/index/openai-o3-mini/">o1 and o3-mini</a></li><li><a href="https://cdn.openai.com/o1-system-card-20241205.pdf">Supplementary o1</a></li><li><a href="https://web.archive.org/web/20250203044057/https://openai.com/index/o1-and-new-tools-for-developers/">o1 TAU-bench</a></li><li><a href="https://cdn.openai.com/o3-mini-system-card-feb10.pdf">Supplementary o3-mini</a></li><li><a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">Deepseek R1</a></li></ul><h3>TAU-bench</h3><p><strong>Information about the scaffolding</strong></p><p>Scores were achieved with a prompt addendum to the Airline Agent Policy instructing Claude to better utilize a “planning” tool, where the model is encouraged to write down its thoughts as it solves the problem distinct from our usual thinking mode, during the multi-turn trajectories to best leverage its reasoning abilities. To accommodate the additional steps Claude incurs by utilizing more thinking, the maximum number of steps (counted by model completions) was increased from 30 to 100 (most trajectories completed under 30 steps with only one trajectory reaching above 50 steps).</p><p>Additionally, the TAU-bench score for Claude 3.5 Sonnet (new) differs from what we originally reported on release because of small dataset improvements introduced since then. We re-ran on the updated dataset for more accurate comparison with Claude 3.7 Sonnet.</p><h3>SWE-bench Verified</h3><p><strong>Information about the scaffolding</strong></p><p>There are many approaches to solving open ended agentic tasks like SWE-bench. Some approaches offload much of the complexity of deciding which files to investigate or edit and which tests to run to more traditional software, leaving the core language model to generate code in predefined places, or select from a more limited set of actions. Agentless (<a href="https://arxiv.org/abs/2407.01489">Xia et al., 2024</a>) is a popular framework used in the evaluation of Deepseek’s R1 and other models which augments an agent with prompt- and embedding-based file retrieval mechanisms, patch localization, and best-of-40 rejection sampling against regression tests. Other scaffolds (e.g. <a href="https://aide.dev/blog/sota-bitter-lesson">Aide</a>) further supplement models with additional test-time compute in the form of retries, best-of-N, or Monte Carlo Tree Search (MCTS).</p><p>For Claude 3.7 Sonnet and Claude 3.5 Sonnet (new), we use a much simpler approach with minimal scaffolding, where the model decides which commands to run and files to edit in a single session. Our main “no extended thinking” pass@1 result simply equips the model with the <a href="https://www.anthropic.com/research/swe-bench-sonnet">two tools described here</a>—a bash tool, and a file editing tool that operates via string replacements—as well as the “planning tool” mentioned above in our TAU-bench results. Due to infrastructure limitations, only 489/500 problems are actually solvable on our internal infrastructure (i.e., the golden solution passes the tests). For our vanilla pass@1 score we are counting the 11 unsolvable problems as failures to maintain parity with the <a href="https://www.swebench.com/#verified">official leaderboard</a>. For transparency, we separately release the test cases that did not work on our infrastructure.</p><p>For our “high compute” number we adopt additional complexity and parallel test-time compute as follows:</p><ul><li>We sample multiple parallel attempts with the scaffold above</li><li>We discard patches that break the visible regression tests in the repository, similar to the rejection sampling approach adopted by Agentless; note no hidden test information is used.</li><li>We then rank the remaining attempts with a scoring model similar to our results on GPQA and AIME described in our <a href="https://www.anthropic.com/news/visible-extended-thinking">research post</a> and choose the best one for the submission.</li></ul><p>This results in a score of 70.3% on the subset of n=489 verified tasks which work on our infrastructure. Without this scaffold, Claude 3.7 Sonnet achieves 63.7% on SWE-bench Verified using this same subset. The excluded 11 test cases that were incompatible with our internal infrastructure are:</p><ul><li>scikit-learn__scikit-learn-14710</li><li>django__django-10097</li><li>psf__requests-2317</li><li>sphinx-doc__sphinx-10435</li><li>sphinx-doc__sphinx-7985</li><li>sphinx-doc__sphinx-8475</li><li>matplotlib__matplotlib-20488</li><li>astropy__astropy-8707</li><li>astropy__astropy-8872</li><li>sphinx-doc__sphinx-8595</li><li>sphinx-doc__sphinx-9711</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The best way to use text embeddings portably is with Parquet and Polars (206 pts)]]></title>
            <link>https://minimaxir.com/2025/02/embeddings-parquet/</link>
            <guid>43162995</guid>
            <pubDate>Mon, 24 Feb 2025 18:27:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://minimaxir.com/2025/02/embeddings-parquet/">https://minimaxir.com/2025/02/embeddings-parquet/</a>, See on <a href="https://news.ycombinator.com/item?id=43162995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://stackoverflow.blog/2023/11/09/an-intuitive-introduction-to-text-embeddings/">Text embeddings</a>, particularly modern embeddings generated from large language models, are one of the most useful applications coming from the generative AI boom. Embeddings are a list of numbers which represent an object: in the case of text embeddings, they can represent words, sentences, and full paragraphs and documents, and they do so with a surprising amount of distinctiveness.</p><p>Recently, I created text embeddings representing every distinct <a href="https://magic.wizards.com/en">Magic: the Gathering</a> card released as of the February 2025 Aetherdrift expansion: 32,254 in total. With these embeddings, I can find the mathematical similarity between cards through the encoded representation of their card design, including all mechanical attributes such as the card name, card cost, card text, and even card rarity.</p><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/wog_hu17410071956209960561.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/wog_hu17201351259632573234.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/wog.webp 976w" src="https://minimaxir.com/2025/02/embeddings-parquet/wog.webp" alt="The iconic Magic card Wrath of God, along with its top four most similar cards identified using their respective embeddings. The similar cards are valid matches, with similar card text and card types."><figcaption><p>The iconic Magic card <a href="https://gatherer.wizards.com/pages/card/Details.aspx?multiverseid=129808">Wrath of God</a>, along with its top four most similar cards identified using their respective embeddings. The similar cards are valid matches, with similar card text and card types.</p></figcaption></figure><p>Additionally, I can create a fun 2D <a href="https://umap-learn.readthedocs.io/en/latest/">UMAP</a> projection of all those cards, which also identifies interesting patterns:</p><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/mtg_umap_hu1954379011677155299.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/mtg_umap_hu11781744338083152452.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/mtg_umap_hu15618369906233811734.webp 1024w,https://minimaxir.com/2025/02/embeddings-parquet/mtg_umap.webp 1200w" src="https://minimaxir.com/2025/02/embeddings-parquet/mtg_umap.webp" alt="The UMAP dimensionality reduction process also implicitly clusters the Magic cards to logical clusters, such as by card color(s) and card type."><figcaption><p>The UMAP dimensionality reduction process also implicitly clusters the Magic cards to logical clusters, such as by card color(s) and card type.</p></figcaption></figure><p>I generated these Magic card embeddings for <em>something special</em> besides a pretty data visualization, but if you are curious how I generated them, they were made using the new-but-underrated <a href="https://huggingface.co/Alibaba-NLP/gte-modernbert-base">gte-modernbert-base</a> embedding model and the process is detailed <a href="https://github.com/minimaxir/mtg-embeddings">in this GitHub repository</a>. The embeddings themselves (including the coordinate values to reproduce the 2D UMAP visualization) are available as a <a href="https://huggingface.co/datasets/minimaxir/mtg-embeddings">Hugging Face dataset</a>.</p><p>Most tutorials involving embedding generation omit the obvious question: what do you <em>do</em> with the text embeddings after you generate them? The common solution is to use a <a href="https://en.wikipedia.org/wiki/Vector_database">vector database</a>, such as <a href="https://github.com/facebookresearch/faiss">faiss</a> or <a href="https://qdrant.tech/">qdrant</a>, or even a cloud-hosted service such as <a href="https://www.pinecone.io/">Pinecone</a>. But those aren’t easy to use: faiss has <a href="https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index">confusing configuration options</a>, qdrant requires <a href="https://github.com/qdrant/qdrant?tab=readme-ov-file#client-server">using a Docker container</a> to host the storage server, and Pinecone can get <a href="https://www.pinecone.io/pricing/">very expensive</a> very quickly, and its free Starter tier is limited.</p><p>What many don’t know about text embeddings is that you don’t <em>need</em> a vector database to calculate nearest-neighbor similarity if your data isn’t too large. Using <a href="https://numpy.org/doc/stable/index.html">numpy</a> and my Magic card embeddings, a 2D matrix of 32,254 <code>float32</code> embeddings at a dimensionality of 768D (common for “smaller” LLM embedding models) occupies <strong>94.49 MB</strong> of system memory, which is relatively low for modern personal computers and can fit within free usage tiers of cloud VMs. If both the query vector and the embeddings themselves are unit normalized (many embedding generators normalize by default), then the matrix dot product between the query and embeddings results in a cosine similarity between <code>[-1, 1]</code>, where the higher score is better/more similar. Since dot products are such a fundamental aspect of linear algebra, numpy’s implementation is extremely fast: with the help of additional numpy <a href="https://numpy.org/doc/stable/reference/generated/numpy.argpartition.html">sorting</a> <a href="https://numpy.org/doc/2.1/reference/generated/numpy.argsort.html">shenanigans</a>, on my M3 Pro MacBook Pro it takes just <strong>1.08 ms</strong> on average to calculate all 32,254 dot products, find the top 3 most similar embeddings, and return their corresponding <code>idx</code> of the matrix and and cosine similarity <code>score</code>.</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>def</span> <span>fast_dot_product</span><span>(</span><span>query</span><span>,</span> <span>matrix</span><span>,</span> <span>k</span><span>=</span><span>3</span><span>):</span>
</span></span><span><span>    <span>dot_products</span> <span>=</span> <span>query</span> <span>@</span> <span>matrix</span><span>.</span><span>T</span>
</span></span><span><span>
</span></span><span><span>    <span>idx</span> <span>=</span> <span>np</span><span>.</span><span>argpartition</span><span>(</span><span>dot_products</span><span>,</span> <span>-</span><span>k</span><span>)[</span><span>-</span><span>k</span><span>:]</span>
</span></span><span><span>    <span>idx</span> <span>=</span> <span>idx</span><span>[</span><span>np</span><span>.</span><span>argsort</span><span>(</span><span>dot_products</span><span>[</span><span>idx</span><span>])[::</span><span>-</span><span>1</span><span>]]</span>
</span></span><span><span>
</span></span><span><span>    <span>score</span> <span>=</span> <span>dot_products</span><span>[</span><span>idx</span><span>]</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>idx</span><span>,</span> <span>score</span>
</span></span></code></pre></div><p>In most implementations of vector databases, once you insert the embeddings, they’re stuck there in a proprietary serialization format and you are locked into that library and service. If you’re just building a personal pet project or sanity-checking embeddings to make sure the results are good, that’s a huge amount of friction. For example, when I want to experiment with embeddings, I generate them on a cloud server with a GPU since LLM-based embeddings models are often slow to generate without one, and then download them locally to my personal computer. What is the best way to handle embeddings portably such that they can easily be moved between machines and also in a non-proprietary format?</p><p>The answer, after much personal trial-and-error, is Parquet files, which still has a surprising amount of nuance. But before we talk about why Parquet files are good, let’s talk about how <em>not</em> to store embeddings.</p><h2 id="the-worst-ways-to-store-embeddings">The Worst Ways to Store Embeddings</h2><p>The incorrect-but-unfortunately-common way to store embeddings is in a text format such as a CSV file. Text data is substantially larger than <code>float32</code> data: for example, a decimal number with full precision (e.g. <code>2.145829051733016968e-02</code>) as a <code>float32</code> is 32 bits/4 bytes, while as a text representation (in this case 24 ASCII <code>char</code>s) it’s 24 bytes, <strong>6x larger</strong>. When the CSV is saved and loaded, the data has to be serialized between a numpy and a string representation of the array, which adds significant overhead. Despite that, in <a href="https://github.com/openai/openai-cookbook/blob/a3e98ea4dcf866b5e7a3cb7d63dccaa68c7d63aa/examples/Embedding_Wikipedia_articles_for_search.ipynb">one of OpenAI’s official tutorials</a> for their embeddings models, they save the embeddings as a CSV using <a href="https://pandas.pydata.org/">pandas</a> with the admitted caveat of “Because this example only uses a few thousand strings, we’ll store them in a CSV file. (For larger datasets, use a vector database, which will be more performant.)”. In the case of the Magic card embeddings, pandas-to-CSV performs the <em>worst</em> out of any encoding options: more on why later.</p><p>Numpy has native methods to <a href="https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html">save</a> and <a href="https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html">load</a> embeddings as a <code>.txt</code> that’s straightforward:</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>np</span><span>.</span><span>savetxt</span><span>(</span><span>"embeddings_txt.txt"</span><span>,</span> <span>embeddings</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>embeddings_r</span> <span>=</span> <span>np</span><span>.</span><span>loadtxt</span><span>(</span><span>"embeddings_txt.txt"</span><span>,</span> <span>dtype</span><span>=</span><span>np</span><span>.</span><span>float32</span><span>,</span> <span>delimiter</span><span>=</span><span>" "</span><span>)</span>
</span></span></code></pre></div><p>The resulting file not only takes a few seconds to save and load, but it’s also massive: <strong>631.5 MB</strong>!</p><p>As an aside, HTTP APIs such as OpenAI’s <a href="https://platform.openai.com/docs/guides/embeddings">Embeddings API</a> do transmit the embeddings over text which adds needless latency and bandwidth overhead. I wish more embedding providers offered <a href="https://grpc.io/">gRPC</a> APIs which allow transfer of binary <code>float32</code> data instead to gain a performance increase: Pinecone’s <a href="https://docs.pinecone.io/reference/python-sdk">Python SDK</a>, for example, does just that.</p><p>The second incorrect method to save a matrix of embeddings to disk is to save it as a Python <a href="https://docs.python.org/3/library/pickle.html">pickle</a> object, which stores its representation in memory on disk with a few lines of code from the native <code>pickle</code> library. Pickling is unfortunately common in the machine learning industry since many ML frameworks such as <a href="https://scikit-learn.org/stable/">scikit-learn</a> don’t have easy ways to serialize encoders and models. But it comes with two major caveats: pickled files are a massive security risk as they can execute arbitrary code, and the pickled file may not be guaranteed to be able to be opened on other machines or Python versions. It’s 2025, just stop pickling if you can.</p><p>In the case of the Magic card embeddings, it does indeed work with instant save/loads, and the file size on disk is <strong>94.49 MB</strong>: the same as its memory consumption and about 1/6th of the text size as expected:</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>with</span> <span>open</span><span>(</span><span>"embeddings_matrix.pkl"</span><span>,</span> <span>"wb"</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
</span></span><span><span>    <span>pickle</span><span>.</span><span>dump</span><span>(</span><span>embeddings</span><span>,</span> <span>f</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>with</span> <span>open</span><span>(</span><span>"embeddings_matrix.pkl"</span><span>,</span> <span>"rb"</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
</span></span><span><span>    <span>embeddings_r</span> <span>=</span> <span>pickle</span><span>.</span><span>load</span><span>(</span><span>f</span><span>)</span>
</span></span></code></pre></div><p>But there are still better and easier approaches.</p><h2 id="the-intended-but-not-great-way-to-store-embeddings">The Intended-But-Not-Great Way to Store Embeddings</h2><p>Numpy itself has a canonical way to <a href="https://numpy.org/doc/2.1/reference/generated/numpy.save.html">save</a> and <a href="https://numpy.org/doc/2.1/reference/generated/numpy.load.html">load</a> matrixes — which annoyingly saves as a pickle by default for compatability reasons, but that can fortunately be disabled by setting <code>allow_pickle=False</code>:</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>np</span><span>.</span><span>save</span><span>(</span><span>"embeddings_matrix.npy"</span><span>,</span> <span>embeddings</span><span>,</span> <span>allow_pickle</span><span>=</span><span>False</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>embeddings_r</span> <span>=</span> <span>np</span><span>.</span><span>load</span><span>(</span><span>"embeddings_matrix.npy"</span><span>,</span> <span>allow_pickle</span><span>=</span><span>False</span><span>)</span>
</span></span></code></pre></div><p>File size and I/O speed are the same as with the <code>pickle</code> approach.</p><p>This works — and it’s something I had used for awhile — but in the process it exposes another problem: how do we map metadata (the Magic cards in this case) to embeddings? Currently, we use the <code>idx</code> of the most-similar matches to perform an efficient batched lookup to the source data. In this case, the number of rows matches the number of cards exactly, but what happens if the embeddings matrix needs to be changed, such as to add or remove cards and their embeddings? What happens if you want to add a dataset filter? It becomes a mess that inevitably causes technical debt.</p><p>The solution to this is to colocate metadata such as card names, card text, and attributes with their embeddings: that way, if they are later added, removed, or sorted, the results will remain the same. Modern vector databases such as qdrant and Pinecone do just that, with the ability to filter and sort on the metadata at the same time you query the most similar vectors. This is a bad idea to do in numpy itself, as it’s more optimized for numbers and not other data types such as strings, which have <a href="https://numpy.org/devdocs/user/basics.strings.html">limited operations available</a>.</p><p>The solution is to look at another file format that can store metadata and embeddings simultaneously, and the answer to that is Parquet files. But there’s a rabbit hole as to what’s the <em>best</em> way to interact with them.</p><h2 id="what-are-parquet-files">What are Parquet files?</h2><p>Parquet, developed by the open-source <a href="https://parquet.apache.org/">Apache Parquet</a> project, is a file format for handling columnar data, but despite being <a href="https://blog.x.com/engineering/en_us/a/2013/announcing-parquet-10-columnar-storage-for-hadoop">first released in 2013</a> it hasn’t taken off in the data science community until very recently. <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> The most relevant feature of Parquet is that the resulting files are typed for each column, and that this typing includes nested lists, such as an embedding which is just a list of <code>float32</code> values. As a bonus, the columnar format allows downstream libraries to save/load them selectively and very quickly, far faster than CSVs and with rare parsing errors. The file format also allows for efficient compression and decompression, but that’s less effective with embeddings as there’s little redundant data.</p><p>For Parquet file I/O, the standard approach is to use the <a href="https://arrow.apache.org/">Apache Arrow</a> protocol that is columnar in-memory, which complements the Parquet storage medium on disk. But how do you use Arrow?</p><h2 id="how-do-you-use-parquet-files-in-python-for-embeddings">How do you use Parquet files in Python for embeddings?</h2><p>Ideally, we need a library that can handle nested data easily and can interoperate with numpy for serializing to a matrix and can run fast dot products.</p><p>The official Arrow library that <a href="https://arrow.apache.org/docs/python/index.html">interacts with Parquet natively</a> in Python is <a href="https://arrow.apache.org/docs/python/index.html">pyarrow</a>. Here, I have an example Parquet file generated with [SPOILERS] that contains both the card metadata and an <code>embedding</code> column, with the embedding for each row corresponding to that card.</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>df</span> <span>=</span> <span>pa</span><span>.</span><span>parquet</span><span>.</span><span>read_table</span><span>(</span><span>"mtg-embeddings.parquet"</span><span>)</span>
</span></span></code></pre></div><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/parquet_hu17998700735124782486.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/parquet_hu3640072816198911328.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/parquet_hu8958370197007221068.webp 1024w,https://minimaxir.com/2025/02/embeddings-parquet/parquet.png 1352w" src="https://minimaxir.com/2025/02/embeddings-parquet/parquet.png" alt="Pyarrow’s table schema from the input Parquet file of Magic card embeddings. Note the embedding column at the bottom is a list of 768 floats."><figcaption><p>Pyarrow’s table schema from the input Parquet file of Magic card embeddings. Note the <code>embedding</code> column at the bottom is a list of 768 floats.</p></figcaption></figure><p>But pyarrow is not a DataFrame library, and despite the data being in a Table, it’s hard to slice and access: the documentation suggests that you export to pandas if you need more advanced manipulation.</p><p>Other more traditional data science libraries can leverage pyarrow directly. The most popular one is, of course, pandas itself which can <a href="https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html">read/write Parquet</a> doing just that. There are many, many resources for using pandas well, so it’s often the first choice among data science practioners.</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>df</span> <span>=</span> <span>pd</span><span>.</span><span>read_parquet</span><span>(</span><span>"mtg-embeddings.parquet"</span><span>,</span> <span>columns</span><span>=</span><span>[</span><span>"name"</span><span>,</span> <span>"embedding"</span><span>])</span>
</span></span><span><span><span>df</span>
</span></span></code></pre></div><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/pandas_embed_hu6407179862966887367.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/pandas_embed_hu14762325519826550103.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/pandas_embed_hu12287036768330367704.webp 1024w,https://minimaxir.com/2025/02/embeddings-parquet/pandas_embed.png 1224w" src="https://minimaxir.com/2025/02/embeddings-parquet/pandas_embed.png" alt="Pandas HTML table output of the Magic card DataFrame when printed in a Jupyter Notebook."><figcaption><p>Pandas HTML table output of the Magic card DataFrame when printed in a Jupyter Notebook.</p></figcaption></figure><p>There’s one major weakness for the use case of embeddings: pandas is very bad at nested data. From the image above you’ll see that the <code>embedding</code> column <em>appears</em> to be a list of numbers, but it’s actually a list of numpy <code>object</code>s, which is a very inefficent datatype and why I suspect writing it to a CSV is very slow. Simply converting it to numpy with <code>df["embedding"].to_numpy()</code> results in a 1D array, which is definitely wrong, and trying to cast it to <code>float32</code> doesn’t work. I found that the best way to extract the embeddings matrix from a pandas <code>embedding</code> column is to <a href="https://numpy.org/doc/2.1/reference/generated/numpy.vstack.html">np.vstack()</a> the embeddings, e.g. <code>np.vstack(df["embedding"].to_numpy())</code>, which does result in a <code>(32254, 768)</code> <code>float32</code> matrix as expected. That adds a lot of compute and memory overhead in addition to unnecessary numpy array copies. Finally, after computing the dot products between a candidate query and the embedding matrix, row metadata with the most similar values can then be retrieved using <code>df.loc[idx]</code>. <sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p><p>However, there is another, more recent tabular data library that not only is faster than pandas, it has proper support for nested data. That library is polars.</p><h2 id="the-power-of-polars">The Power of polars</h2><p><a href="https://pola.rs/">Polars</a> is a relatively new Python library which is primarily written in <a href="https://www.rust-lang.org/">Rust</a> and <a href="https://docs.pola.rs/#key-features">supports Arrow</a>, which gives it a <a href="https://duckdblabs.github.io/db-benchmark/">massive performance increase</a> over pandas and many other DataFrame libraries. In the case of Magic cards, 32k rows isn’t nearly “big data” and the gains of using a high-performance library are lesser, but there are some unexpected features that coincidentally work <em>perfectly</em> for the embeddings use case.</p><p>As with pandas, you read a parquet file with a <code>read_parquet()</code>:</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>df</span> <span>=</span> <span>pl</span><span>.</span><span>read_parquet</span><span>(</span><span>"mtg-embeddings.parquet"</span><span>,</span> <span>columns</span><span>=</span><span>[</span><span>"name"</span><span>,</span> <span>"embedding"</span><span>])</span>
</span></span><span><span><span>df</span>
</span></span></code></pre></div><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/polars_embed_hu6230264701954762810.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/polars_embed_hu6820488175446530372.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/polars_embed.png 957w" src="https://minimaxir.com/2025/02/embeddings-parquet/polars_embed.png" alt="Polars HTML table output of the Magic card DataFrame when printed in a Jupyter Notebook."><figcaption><p>Polars HTML table output of the Magic card DataFrame when printed in a Jupyter Notebook.</p></figcaption></figure><p>There’s a notable difference in the table output compared to <code>pandas</code>: it also reports the data type of its columns, and more importantly, it shows that the <code>embedding</code> column consists of arrays, all <code>float32</code>s, and all length 768. That’s a great start!</p><p>polars also has a to_numpy() function. Unlike pandas, if you call <code>to_numpy()</code> on a column as a Series, e.g. <code>df['embedding'].to_numpy()</code>, the returned object is a numpy 2D matrix: no <code>np.vstack()</code> needed. If you look at the <a href="https://docs.pola.rs/api/python/stable/reference/series/api/polars.Series.to_numpy.html">documentation</a> for the function, there’s a curious feature:</p><blockquote><p>This operation copies data only when necessary. The conversion is zero copy when all of the following hold: […]</p></blockquote><p>Zero copy! And in the case of columnar-stored embeddings, the conditions will always hold, but you can set <code>allow_copy=False</code> to throw an error just in case.</p><p>Inversely, if you want to add a 2D embeddings matrix to an existing DataFrame and colocate each embedding’s corresponding metadata, such as after you batch-generate thousands of embeddings and want to save and download the resulting Parquet, it’s just as easy as adding a column to the DataFrame.</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>df</span> <span>=</span> <span>pl</span><span>.</span><span>with_columns</span><span>(</span><span>embedding</span><span>=</span><span>embeddings</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>df</span><span>.</span><span>write_parquet</span><span>(</span><span>"mtg-embeddings.parquet"</span><span>)</span>
</span></span></code></pre></div><p>Now, let’s put the speed to the test using all the Magic card metadata. What if we perform embedding similarity on a Magic card, but beforehand dynamically filter the dataset according to user parameters (therefore filtering the candidate embeddings at the same time since they are colocated) and perform the similarity calculations quickly as usual? Let’s try with <a href="https://gatherer.wizards.com/pages/card/details.aspx?multiverseid=87908">Lightning Helix</a>, a card whose effects are self-explanatory even to those who don’t play Magic.</p><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/helix_1_hu9495365185621367508.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/helix_1_hu243742327427369351.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/helix_1.webp 976w" src="https://minimaxir.com/2025/02/embeddings-parquet/helix_1.webp" alt="The most similar cards to Lightning Helix do have similar effects, although “Lightning” cards dealing damage is a common trope in Magic. Warleader’s Helix is a direct reference to Lightning Helix."><figcaption><p>The most similar cards to Lightning Helix do have similar effects, although “Lightning” cards dealing damage is a common trope in Magic. <a href="https://gatherer.wizards.com/pages/card/Details.aspx?multiverseid=456806">Warleader’s Helix</a> is a direct reference to Lightning Helix.</p></figcaption></figure><p>Now we can also find similar cards to Lightning Helix but with filters. In this case, let’s look for a Sorcery (which are analogous to Instants but tend to be stronger since they have play limitations) and has Black as one of its colors. This limits the candidates to ~3% of the original dataset. The resulting code would look like this, given a <code>query_embed</code>:</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>df_filter</span> <span>=</span> <span>df</span><span>.</span><span>filter</span><span>(</span>
</span></span><span><span>    <span>pl</span><span>.</span><span>col</span><span>(</span><span>"type"</span><span>)</span><span>.</span><span>str</span><span>.</span><span>contains</span><span>(</span><span>"Sorcery"</span><span>),</span>
</span></span><span><span>    <span>pl</span><span>.</span><span>col</span><span>(</span><span>"manaCost"</span><span>)</span><span>.</span><span>str</span><span>.</span><span>contains</span><span>(</span><span>"B"</span><span>),</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>embeddings_filter</span> <span>=</span> <span>df_filter</span><span>[</span><span>"embedding"</span><span>]</span><span>.</span><span>to_numpy</span><span>(</span><span>allow_copy</span><span>=</span><span>False</span><span>)</span>
</span></span><span><span><span>idx</span><span>,</span> <span>_</span> <span>=</span> <span>fast_dot_product</span><span>(</span><span>query_embed</span><span>,</span> <span>embeddings_filter</span><span>,</span> <span>k</span><span>=</span><span>4</span><span>)</span>
</span></span><span><span><span>related_cards</span> <span>=</span> <span>df_filter</span><span>[</span><span>idx</span><span>]</span>
</span></span></code></pre></div><p>As an aside, in polars you can call row subsets of a DataFrame with <code>df[idx]</code>, which makes it infinitely better than pandas and its <code>df.iloc[idx]</code>.</p><p>The resulting similar cards:</p><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/helix_2_hu8536479567478311954.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/helix_2_hu10382916257055575322.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/helix_2.webp 976w" src="https://minimaxir.com/2025/02/embeddings-parquet/helix_2.webp" alt="In this case, the similarity focuses on card text similarity, and these cards have near identical text. Smiting Helix is also a direct reference to Lightning Helix."><figcaption><p>In this case, the similarity focuses on card text similarity, and these cards have near identical text. <a href="https://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=464058">Smiting Helix</a> is also a direct reference to Lightning Helix.</p></figcaption></figure><p>Speed-wise, the code runs at about <strong>1.48ms</strong> on average, or about 37% slower than calculating all dot products, so the filtering does still have some overhead, which is not surprising as that the filtered dataframe does copy the embeddings. Overall, it’s still more than fast enough for a hobby project.</p><p>I’ve created an <a href="https://colab.research.google.com/drive/19C_9sBC0Py2PlXYihl2ed378oGyroONZ?usp=sharing">interactive Colab Notebook</a> where you can generate similarities for any Magic card, and apply any filters you want!</p><h2 id="scaling-to-vector-databases">Scaling to Vector Databases</h2><p>Again, all of this assumes that you are using the embeddings for smaller/noncommercial projects. If you scale to hundreds of thousands of embeddings, the parquet and dot product approach for finding similarity should still be fine, but if it’s a business critical application, the marginal costs of querying a vector database are likely lower than the marginal revenue from a snappy similarity lookup. Deciding how to make these tradeoffs is the fun part of MLOps!</p><p>In the case that the amount of vectors is too large to fit into memory but you don’t want to go all-in on vector databases, another option that may be worth considering is using an old-fashioned database that can now support vector embeddings. Notably, <a href="https://www.sqlite.org/">SQLite</a> databases are just a single portable file, however interacting with them has more technical overhead and considerations than the <code>read_parquet()</code> and <code>write_parquet()</code> of polars. One notable implementation of vector databases in SQLite is the <a href="https://alexgarcia.xyz/sqlite-vec/">sqlite-vec extension</a>, which also allows for simultaneous filtering and similarity calculations.</p><p>The next time you’re working with embeddings, consider whether you really need a vector database. For many applications, the combination of Parquet files and polars provides everything you need: efficient storage, fast similarity search, and easy metadata filtering. Sometimes the simplest solution is the best one.</p><p><em>The code used to process the Magic card data, create the embeddings, and plot the UMAP 2D projection, is all available <a href="https://github.com/minimaxir/mtg-embeddings">in this GitHub repository</a>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Co-Scientist AI fed previous paper with the answer in it (193 pts)]]></title>
            <link>https://pivot-to-ai.com/2025/02/22/google-co-scientist-ai-cracks-superbug-problem-in-two-days-because-it-had-been-fed-the-teams-previous-paper-with-the-answer-in-it/</link>
            <guid>43162582</guid>
            <pubDate>Mon, 24 Feb 2025 17:52:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pivot-to-ai.com/2025/02/22/google-co-scientist-ai-cracks-superbug-problem-in-two-days-because-it-had-been-fed-the-teams-previous-paper-with-the-answer-in-it/">https://pivot-to-ai.com/2025/02/22/google-co-scientist-ai-cracks-superbug-problem-in-two-days-because-it-had-been-fed-the-teams-previous-paper-with-the-answer-in-it/</a>, See on <a href="https://news.ycombinator.com/item?id=43162582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>The hype cycle for Google’s fabulous new AI Co-Scientist tool, based on the Gemini LLM, includes a BBC headline about how José Penadés’ team at Imperial College asked the tool about a problem he’d been working on for years — and it solved it in less than 48 hours! [<a href="https://www.bbc.co.uk/news/articles/clyz6e9edy3o"><i>BBC</i></a><i>; </i><a href="https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/"><i>Google</i></a>]</p>
<p>Penadés works on the evolution of drug-resistant bacteria. Co-Scientist suggested the bacteria might be hijacking fragments of DNA from bacteriophages. The team said that if they’d had this hypothesis at the start, it would have saved years of work.</p>
<p>Sounds almost too good to be true! Because it is. It turns out Co-Scientist had been fed a 2023 paper by Penadés’ team that included a version of the hypothesis. The BBC coverage failed to mention this bit. [<a href="https://www.newscientist.com/article/2469072-can-googles-new-research-assistant-ai-give-scientists-superpowers/"><i>New Scientist</i></a><i>, </i><a href="https://archive.is/etd5F"><i>archive</i></a>]</p>
<p>Google’s other claimed successes for Co-Scientist follow this pattern. The system proposed new drugs for liver fibrosis — but the proposed drugs had previously been studied for this use case.</p>
<p>In 2023, Google loudly publicised how DeepMind had synthesized 43 “new materials” — but studies in 2024 showed that none of the materials was actually new, and that only 3 of 58 syntheses were even successful. [<a href="https://journals.aps.org/prxenergy/abstract/10.1103/PRXEnergy.3.011002"><i>APS</i></a><i>; </i><a href="https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/65957d349138d231611ad8f7/original/challenges-in-high-throughput-inorganic-material-prediction-and-autonomous-synthesis.pdf"><i>ChemrXiv</i></a>]</p>
<p>“Everything was already published, but in different bits,” said Penadés about Co-Scientist. “The system was able to put everything together.”</p>
<p>Sure. LLM-based madlibs can work as a suggestion tool.&nbsp; But the headline claim is not so convincing on AI scientific creativity.</p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Student refines 100-year-old math problem, expanding wind energy possibilities (134 pts)]]></title>
            <link>https://www.psu.edu/news/engineering/story/student-refines-100-year-old-math-problem-expanding-wind-energy-possibilities</link>
            <guid>43162544</guid>
            <pubDate>Mon, 24 Feb 2025 17:49:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.psu.edu/news/engineering/story/student-refines-100-year-old-math-problem-expanding-wind-energy-possibilities">https://www.psu.edu/news/engineering/story/student-refines-100-year-old-math-problem-expanding-wind-energy-possibilities</a>, See on <a href="https://news.ycombinator.com/item?id=43162544">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/ImageObject" data-testid="article-hero-image"><figcaption id="caption" data-testid="caption"><div><p><span>Divya Tyagi, right, a Penn State engineering graduate student, shows her work on a century-old math problem to Sven Schmitz, a College of Engineering faculty member and Tyagi's adviser.</span>&nbsp;<span data-testid="credit">Credit: <!-- -->Kevin Sliman<!-- -->. <!-- -->All Rights Reserved<!-- -->.</span></p></div></figcaption></div><div id="text-content-container"><p>UNIVERSITY PARK, Pa. — A Penn State engineering student refined a century-old math problem into a simpler, more elegant form, making it easier to use and explore. <a href="https://www.linkedin.com/in/dft5201/">Divya Tyagi’s</a> work expands research in aerodynamics, unlocking new possibilities in wind turbine design that Hermann Glauert, a British aerodynamicist and the original author, did not consider.&nbsp;&nbsp;&nbsp;</p>
<p>Tyagi, a graduate student pursuing her master’s degree in aerospace engineering, completed this work as a Penn State undergraduate for her Schreyer Honors College thesis. Her research was published in&nbsp;<a href="https://wes.copernicus.org/articles/10/451/2025/">Wind Energy Science</a>.</p>
<p>“I created an addendum to Glauert’s problem which determines the optimal aerodynamic performance of a wind turbine by solving for the ideal flow conditions for a turbine in order to maximize its power output,” said Tyagi, who earned her bachelor’s degree in aerospace engineering.&nbsp;</p>
<p>Her adviser, <a href="https://www.aero.psu.edu/department/directory-detail-g.aspx?q=SUS52">Sven Schmitz</a>, the Boeing/A.D. Welliver Professor in the Department of Aerospace Engineering and co-author on the paper, said Glauert’s original work focused exclusively on the maximum attainable power coefficient, which measures how efficiently a turbine converts wind energy into electricity. However, Glauert did not account for the total force and moment coefficients acting on the rotor — the spinning unit with attached blades — or how turbine blades bend under wind pressure.&nbsp;&nbsp;&nbsp;</p>
<p>“If you have your arms spread out and someone presses on your palm, you have to resist that movement,” said Schmitz, a faculty member in the <a href="https://iee.psu.edu/">Institute of Energy and the Environment</a>. “We call that the downwind thrust force and the root bending moment, and wind turbines must withstand that, too. You need to understand how large the total load is, which Glauert did not do.”&nbsp;&nbsp;</p>
<p>Schmitz said the simplicity of Tyagi’s addendum based on calculus of variations, a mathematical method used for constrained optimization problems, will allow people to explore new facets of wind turbine design.&nbsp;&nbsp;</p>
<p>“The real impact will be on the next generation of wind turbines using the new knowledge that has been unveiled,” Schmitz said. “As for Divya’s elegant solution, I think it will find its way into the classrooms, across the country and around the world.”&nbsp;</p>
<p>Tyagi said she sees her work as a step toward improving wind energy production and reducing costs.&nbsp;&nbsp;&nbsp;</p>
<p>“Improving the power coefficient of a large wind turbine by just 1% has significant impacts on the energy production of a turbine, and that translates towards the other coefficients that we derived relations for,” she said. "A 1% improvement in power coefficient could notably increase a turbine’s energy output, potentially powering an entire neighborhood."&nbsp;</p>
<p>During her senior year, Tyagi won the Anthony E. Wolk Award for her thesis on the addendum to Glauert’s work. The Wolk Award is presented to a senior in aerospace engineering who has developed the best thesis among aerospace engineering students.&nbsp;&nbsp;</p>
<p>Now pursuing her master’s degree, Tyagi is studying computational fluid dynamics simulations, analyzing airflow around a helicopter rotor.&nbsp;&nbsp;&nbsp;</p>
<p>“The goal is to integrate that with the complex flow around a ship to see how the ship airwake interacts with a helicopter trying to land on its deck,” she said.&nbsp;&nbsp;&nbsp;</p>
<p>Her U.S. Navy-supported research aims to improve flight simulation and pilot safety by better understanding these dynamic interactions.&nbsp;&nbsp;&nbsp;</p>
<p>Reflecting on her undergraduate research, Tyagi said proving her solution on paper was challenging.&nbsp;</p>
<p>“I would spend about 10 to 15 hours a week between the problem, writing the thesis and on research. It took a long time because it was so math intensive,” she said. “But I feel really proud now, seeing all the work I’ve done.”&nbsp;&nbsp;&nbsp;</p>
<p>Schmitz, who has contemplated Glauert’s problem for decades, credited Tyagi’s persistence in tackling it.&nbsp;&nbsp;&nbsp;</p>
<p>“When I thought about the Glauert problem, I thought steps were missing and it was very complicated,” Schmitz said. “There had to be an easier way to do it. That’s when Divya came in. She was the fourth student I challenged with looking at it, and she was the only one who took it on. Her work is truly impressive.”</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a site to tell the time in corporate (334 pts)]]></title>
            <link>https://corporate.watch</link>
            <guid>43162340</guid>
            <pubDate>Mon, 24 Feb 2025 17:33:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://corporate.watch">https://corporate.watch</a>, See on <a href="https://news.ycombinator.com/item?id=43162340">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <h2>We are <span color="magenta">7 weeks</span> into <span color="magenta">Q1, 2025</span> (<span color="magenta">week 7 of 13</span>)</h2>
    <h2>The quarter started <span color="magenta">Wednesday, 01 January</span> and will end <span color="magenta">
            Monday, 31 March</span> (each quarter is <span color="magenta">~13</span> weeks)</h2>
    <h2>There are <span color="magenta">35</span> calendar days until the end of the quarter (<span color="magenta">39.33%</span> to go)</h2>
    <h4>Corporate coordinates generated <span color="darkgreen">Mon, 24 Feb 2025 21:30:01 +0000</span> (<span color="darkgreen">
            2025-02-24T21:30:01.584254767+00:00</span>)</h4>

    <hr>
    <center>
        <h4>
            <i>Wouldn't it be better if there was a tool that removed the need for this though?</i>
        </h4>
        <h4>
            <b>Take a look at <a href="https://objectivetrackr.com/?utm_source=corporatewatch&amp;utm_medium=web&amp;utm_campaign=corporatewatchlanding">objectivetrackr.com</a>!</b>
        </h4>
    </center>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Right to Repair laws have now been introduced in all 50 us states (454 pts)]]></title>
            <link>https://www.ifixit.com/News/108371/right-to-repair-laws-have-now-been-introduced-in-all-50-us-states</link>
            <guid>43161777</guid>
            <pubDate>Mon, 24 Feb 2025 16:55:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ifixit.com/News/108371/right-to-repair-laws-have-now-been-introduced-in-all-50-us-states">https://www.ifixit.com/News/108371/right-to-repair-laws-have-now-been-introduced-in-all-50-us-states</a>, See on <a href="https://news.ycombinator.com/item?id=43161777">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
<p>With the introduction of <a href="https://www.midwestfarmreport.com/2025/02/21/sen-pfaff-introduces-right-to-repair-legislation/">a bill in Wisconsin</a>, Right to Repair legislation has now been introduced in <a href="https://pirg.org/media-center/release-all-50-states-now-have-filed-right-to-repair-legislation-over-last-8-years/">every single US state</a>.&nbsp;</p>



<p>We’ve been fighting for the simple right to fix everything we own for the last eleven years—and we’ve been joined in that fight by more and more advocates, tinkerers, farmers, students, and lawmakers. Today, that movement has touched every corner of the country. Lawmakers in every state in the union have <a href="https://www.repair.org/legislation">filed legislation</a> demanding access to the parts, tools, and documentation we need for repair. This year alone, legislation is active in 24 states.&nbsp;</p>



<figure><img fetchpriority="high" decoding="async" width="1220" height="966" src="https://valkyrie.cdn.ifixit.com/media/2025/02/24014255/all-50-states-have-introduced-right-to-repair-legislation.png" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2025/02/24014255/all-50-states-have-introduced-right-to-repair-legislation.png 1220w, https://valkyrie.cdn.ifixit.com/media/2025/02/24014255/all-50-states-have-introduced-right-to-repair-legislation-1137x900.png 1137w" sizes="(max-width: 1220px) 100vw, 1220px"></figure>



<h2>One in Five Americans Is Protected by Right to Repair Legislation</h2>



<p>Some of those laws have passed: Five states (<a href="https://www.ifixit.com/News/70515/new-york-passes-historic-right-to-repair-bill">New York</a>, <a href="https://www.ifixit.com/News/84491/california-right-to-repair-signed-into-law">California</a>, <a href="https://www.ifixit.com/News/75965/minnesotas-new-right-to-repair-law-will-give-the-whole-world-repair-manuals">Minnesota</a>, <a href="https://www.ifixit.com/News/92144/oregon-just-struck-a-blow-to-parts-pairing-and-won-a-decade-of-repair-support">Oregon</a>, and <a href="https://www.ifixit.com/News/96296/colorado-adds-electronics-to-right-to-repair-protections">Colorado</a>) have passed electronics Right to Repair legislation. One in five Americans lives in a state that has passed Right to Repair—and the remaining states are working hard to restore repair competition.</p>



<p>“Here, there and everywhere—people just want to fix their stuff,” said PIRG’s Senior Right to&nbsp;Repair Campaign Director Nathan Proctor. “Americans are fed up with all the ways in which&nbsp;manufacturers of everything from toasters to tractors frustrate or block repairs, and lawmakers&nbsp;are hearing that frustration and taking action.”</p>



<figure><img decoding="async" width="850" height="850" src="https://valkyrie.cdn.ifixit.com/media/2025/02/24014620/image.png" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2025/02/24014620/image.png 850w, https://valkyrie.cdn.ifixit.com/media/2025/02/24014620/image-150x150.png 150w" sizes="(max-width: 850px) 100vw, 850px"><figcaption>Some scenes from iFixit’s eleven years of advocacy: We called on Governor Kathy Hochul to sign the <a href="https://www.ifixit.com/News/70515/new-york-passes-historic-right-to-repair-bill">first bill in New York</a> and brought a tractor to the <a href="https://www.ifixit.com/News/73291/colorado-approves-first-ever-agricultural-right-to-repair-bill">Colorado</a> statehouse.</figcaption></figure>



<h2>iFixit’s Eleven Years of Advocacy</h2>



<p>We’ve been boots-on-the-ground fighting for Right to Repair since <a href="https://www.repair.org/history">the very beginning</a>, working to develop and testifying on behalf of the first electronics bill, introduced in South Dakota in 2014. Since then, we’ve worked closely with our US advocacy partners, Repair.org and PIRG, to advance legislation.&nbsp;</p>



<p>“Now that Wisconsin filed their first Right to Repair legislation, we’ve completed the sweep of&nbsp;getting bills filed in all 50 states. Our legislative map no longer has any blanks,” said Gay&nbsp;Gordon-Byrne, Executive Director at Repair.org. “This proves that Right to Repair is needed&nbsp;everywhere—and we are well on our way towards making that happen.”</p>



<p><a href="https://www.ifixit.com/News/3970/2012-the-year-of-the-fixer">When we first got involved</a> in Right to Repair, it felt like an uphill battle. Manufacturers told legislators there was no problem—so we brought in repair professionals, did surveys, and shared our repairability expertise to prove that things were really becoming increasingly hard to fix. Manufacturers told legislators that sharing repair information would make it impossible for them to protect their trade secrets and would be dangerous to customers—so we brought in experts, shared data, and proved that repair information is not protected and changing batteries isn’t dangerous.</p>



<p>Over time, more and more legislators joined the fight. The message is simple: If you bought it, you should be able to fix it. And soon, companies started to work with us instead of against us. Now, Google is a major supporter of Right to Repair legislation in the US, and even Apple has come on board to support some laws.</p>



<p>“This is more than a legislative landmark—it’s a tipping point. We’ve gone from a handful of&nbsp;passionate advocates to a nationwide call for repair autonomy,” said Kyle Wiens, CEO of&nbsp;iFixit. “People are fed up with disposable products and locked-down devices. Repair is the&nbsp;future, and this moment proves it.”</p>



<figure><img decoding="async" width="1926" height="1084" src="https://valkyrie.cdn.ifixit.com/media/2025/02/24014856/image-1.png" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2025/02/24014856/image-1.png 1926w, https://valkyrie.cdn.ifixit.com/media/2025/02/24014856/image-1-1536x864.png 1536w, https://valkyrie.cdn.ifixit.com/media/2025/02/24014856/image-1-1599x900.png 1599w" sizes="(max-width: 1926px) 100vw, 1926px"><figcaption><a href="https://www.ifixit.com/News/78204/congress-asks-ifixit-if-the-right-to-repair-exists">Kyle testifying before Congress</a> in a Right to Repair hearing in 2023.</figcaption></figure>



<h2>We’re Not Done Fighting</h2>



<p>Having introduced bills in all 50 states is a massive milestone. It means more pressure on lawmakers, more attention from manufacturers, and more opportunities for all of us to demand repair-friendly products. But we’re not done yet. We’ll keep pushing for stronger laws, better standards, and a future where repair autonomy is a given, not a privilege.</p>



<p>Thank you for being part of this journey. Let’s keep fixing what matters.</p>
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: SubImage (YC W25) – See your infra from an attacker's perspective (118 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43161332</link>
            <guid>43161332</guid>
            <pubDate>Mon, 24 Feb 2025 16:22:10 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43161332">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hi HN! I’m Alex, and along with my co-founder Kunaal, we are thrilled to introduce SubImage (<a href="https://subimage.io/">https://subimage.io</a>): a tool that lets your security team fix issues before they’re found by attackers. Teams use SubImage to map their infrastructure and emulate adversary behavior. Here’s a video of how I would use it to hack our own company: <a href="https://www.youtube.com/watch?v=P_meu4_aIVA" rel="nofollow">https://www.youtube.com/watch?v=P_meu4_aIVA</a>.</p><p>SubImage is our hosted offering built on top of Cartography (<a href="https://github.com/cartography-cncf/cartography">https://github.com/cartography-cncf/cartography</a>), the open source security graph that we created at Lyft in 2019, originally shared on HN here: <a href="https://news.ycombinator.com/item?id=19517977">https://news.ycombinator.com/item?id=19517977</a>. You can think of us as an open-core Wiz alternative.</p><p>In 2016, I worked on Microsoft’s Azure Red Team, where we built an infra mapping service to find the shortest paths to exploit our targets. We were so effective that the Blue Team wanted it too. In 2019, I joined Lyft, where we applied the same ideas to AWS and beyond, helping build and open-source Cartography. Over the past six years, it’s been incredible to grow the community and see over 70 companies (that I know of) use it.</p><p>Kunaal and I first worked closely together in 2020 when we helped bootstrap Lyft’s vulnerability management program and used Cartography as its backbone: <a href="https://eng.lyft.com/vulnerability-management-at-lyft-enforcing-the-cascade-part-1-234d1561b994" rel="nofollow">https://eng.lyft.com/vulnerability-management-at-lyft-enforc...</a>. This is actually where the name SubImage comes from: Lyft services are made up of one or more “SubImages”, and modeling this properly was such a memorable engineering challenge that we decided to name our company after it.</p><p>Cartography pulls metadata from multiple sources -- SaaS, cloud service providers, a company’s internal services -- and writes it to a graph database. This simple technique is incredibly powerful in modeling otherwise unseen misconfigurations and attack paths in areas like access permissions, networking, and software vulnerabilities.</p><p>SubImage picks up where Cartography leaves off: it’s a fully-hosted solution that provides specific recommendations for the problems it finds. The fix-action depends on company size: small teams might run AWS CLI commands, while larger orgs require automated infrastructure-as-code pull requests.</p><p>Here’s a video demo showing how we can use SubImage to understand and take action if our Stripe API key is unexpectedly used: <a href="https://www.youtube.com/watch?v=RBCr35hb5Hk" rel="nofollow">https://www.youtube.com/watch?v=RBCr35hb5Hk</a>.</p><p>SubImage also provides a natural language interface to quickly answer questions about our infra: <a href="https://imgur.com/a/subimage-natural-language-interface-query-graph-QL2ico5" rel="nofollow">https://imgur.com/a/subimage-natural-language-interface-quer...</a>.</p><p>Security is a competitive space, but we have a few differentiators:</p><p>First, we allow a very deep level of customization where the security team can enrich their graph with their own internal data, not just data from the major cloud providers. If it can be expressed as structured JSON, you can graph it; here’s a demo: <a href="https://www.youtube.com/watch?v=rvwDJoZaO_w" rel="nofollow">https://www.youtube.com/watch?v=rvwDJoZaO_w</a>. This flexibility is needed to answer questions like: Which storage buckets contain PII? Who owns them? Who’s on-call for <a href="https://example.com/api/payment" rel="nofollow">https://example.com/api/payment</a>? Which company director owns the most risk?</p><p>Since it’s built on Cartography, teams can also just write custom plugins in Python if they’d like: <a href="https://cartography-cncf.github.io/cartography/dev/writing-intel-modules.html" rel="nofollow">https://cartography-cncf.github.io/cartography/dev/writing-i...</a>.</p><p>Second, our core principle is actionability. Security teams drown in alerts. SubImage traces paths from critical assets to the most exploitable misconfigurations, helping teams cut through the noise and prioritize real threats.</p><p>Finally, we’re built on open source. We created Cartography and as it improves, so does SubImage. Cartography is a CNCF project (<a href="https://eng.lyft.com/cartography-joins-the-cncf-6f6b7be099a7" rel="nofollow">https://eng.lyft.com/cartography-joins-the-cncf-6f6b7be099a7</a>), which means that it is full open source and will remain so.</p><p>Going forward, we’re maintaining Cartography while launching SubImage as a fully managed offering. Our roadmap includes Access Management (prune excessive permissions and enforce security invariants, Change Tracking (detect and alert on infra changes that introduce risk), and Cloud &amp; SaaS Misconfigurations (expand visibility, including vulnerability management).</p><p>Thanks for reading! If this sounds interesting, try out <a href="https://github.com/cartography-cncf/cartography">https://github.com/cartography-cncf/cartography</a>.</p><p>It’s an honor to share SubImage with HN, especially having followed projects here for over a decade. We’d love to hear your questions, feedback, and the challenges you face in security and infra!</p></div></td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Larry Ellison's half-billion-dollar quest to change farming (141 pts)]]></title>
            <link>https://www.wsj.com/tech/larry-ellison-hawaii-greenhouse-farm-food-2d260e1f</link>
            <guid>43161188</guid>
            <pubDate>Mon, 24 Feb 2025 16:11:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/larry-ellison-hawaii-greenhouse-farm-food-2d260e1f">https://www.wsj.com/tech/larry-ellison-hawaii-greenhouse-farm-food-2d260e1f</a>, See on <a href="https://news.ycombinator.com/item?id=43161188">Hacker News</a></p>
Couldn't get https://www.wsj.com/tech/larry-ellison-hawaii-greenhouse-farm-food-2d260e1f: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Breaking into apartment buildings in five minutes on my phone (407 pts)]]></title>
            <link>https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/</link>
            <guid>43160884</guid>
            <pubDate>Mon, 24 Feb 2025 15:48:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/">https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/</a>, See on <a href="https://news.ycombinator.com/item?id=43160884">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="title-block-header">
        <p>
          What a place to use default credentials
        </p>
      </div><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">





<section id="background">
<h3 data-anchor-id="background">Background</h3>
<p>A few months ago I was on my way to catch the <a href="https://www.instagram.com/seabusmemes/?hl=en">SeaBus</a> when I walked by an apartment building with an interesting looking access control panel. I wrote down the “MESH by Viscount” brand name and made a note to look into it when I had a chance. I ended up just missing my ferry (the 30 minute Sunday headways are brutal), so I decided to see if I could find anything promising on my phone while waiting at Waterfront for the next boat.</p>
</section>
<section id="part-0-recon">
<h3 data-anchor-id="part-0-recon">Part 0: Recon</h3>
<p>Googling the name of the system brings up a sales page advertising “TCP/IP capability to remotely program and maintain the system.” That sounds promising, so let’s try to find a manual. <code>"mesh by viscount" filetype:pdf</code> gets us an <a href="https://files.identiv.com/products/telephone-entry/common/Enterphone_MESH_Installation_Guide.pdf">installation guide</a>. Page 4 explains how to log in to the system’s web UI:</p>
<div>
<figure>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/manual.png"></p>
</figure>
</div>
<p>Default credentials that “should” be changed, with no requirement or explanation of how to do so. Surely no building managers ever leave the defaults, right? And even if they did, they’d surely have no reason to expose this thing to the Internet, right?</p>
<p>The screenshot from the manual tells us the web UI login page’s title is “FREEDOM Administration Login”, which gives us something to search for.</p>
<div>
<figure>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/google.png"></p>
</figure>
</div>
<p>Oh no.</p>
</section>
<section id="part-1-pii-galore">
<h3 data-anchor-id="part-1-pii-galore">Part 1: PII galore</h3>
<p>Exposing the panel to the Internet is dumb, but fortunately none of these systems were accessible using the def– just kidding. The <em>very first result</em> happily lets me in with the <code>freedom:viscount</code> login. The first interesting thing here is the Users section:</p>
<div>
<figure>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/users_censored.png"></p>
</figure>
</div>
<p>This maps residents’ full names to their unit numbers. The building address is also used as the Site title. That’s already not great, but it’s worse in conjunction with the Events section:</p>
<div>
<figure>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/events_censored.png"></p>
</figure>
</div>
<p>This is a multi-year log of every time a fob associated with a certain suite number accessed an entrance or an elevator. So we can now easily determine that, say, Jon Snow of Unit 999, 123 Bear St Vancouver BC comes home every day at 6pm.</p>
<p>For good measure, there’s also a Users section which exposes every resident’s phone number.</p>
</section>
<section id="part-2-breaking-in">
<h3 data-anchor-id="part-2-breaking-in">Part 2: Breaking in</h3>
<p>The PII leaks are pretty wild, but the most interesting thing we have access to is the Controlled Areas section. In here I can apparently register new access fobs, disable existing ones, and change the floors they’re authorized for. The system for this is somewhat convoluted. Fortunately I don’t need to understand it at all, because I can just unlock any entrance I want through an override function:</p>
<div>
<figure>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/override.png"></p>
</figure>
</div>
<p>So I can break into this building in about 5 minutes without attracting any attention whatsoever. Neat.</p>
</section>
<section id="part-3-how-widespread-is-this">
<h3 data-anchor-id="part-3-how-widespread-is-this">Part 3: How widespread is this?</h3>
<p>Maybe I just got lucky that the default credentials worked on the first result and this is actually really rare. Let’s get back to a desktop and scan more properly with ZoomEye:</p>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/zoomeye.png"></p>
<p>That’s not a good sign. ZoomEye kindly offers to let me download a CSV of the results for 700 ZoomPoints. I have no idea what a ZoomPoint is nor how I ended up with 2000 of them, but this seems as good a use as any. With all the hosts in hand, let’s put together a quick Nuclei template:</p>
<pre><code>id: mesh-default-login
info:
  name: MESH By Viscount
  author: Eric Daigle
  severity: high
  description: |
    MESH By Viscount default credentials were discovered.
http:
  - method: POST
  redirects: false
  path:
    - "{{BaseURL}}/mesh/servlet/mesh.webadmin.MESHAdminServlet?requestedAction=login"
  headers:
    Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
    Accept-Language: fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7
    Cache-Control: max-age=0
    Content-Type: application/x-www-form-urlencoded
    Cookie: MESHWebAdminLanguage=en; MESHWebAdminRefreshInterval=0;
    MESHWebAdminPageSize=100;
    Connection: keep-alive
  body: "formLoginName=freedom&amp;formLoginPassword=viscount&amp;formLanguage=en&amp;formLogRefreshInterval=0&amp;formPageSize=100"
  matchers:
    - type: word
      part: body
      words:
        - 'Login Failed. Invalid username or password.'
      negative: true</code></pre>
<p>The login behaviour is poorly coded (shocking, I know): it returns 200 whether or not the login was successful. To get around this we use a negative matcher that returns true as long as the “Login Failed” string is not present. The web UI also returns a 301 if the default landing page on successful login has been changed, which we handle as well. Time to send it:</p>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/nuclei.png"></p>
<p>In total, Nuclei finds 89 hits, so about 14% of the apartment buildings using this system that have ever exposed it to ZoomEye are vulnerable. But most of those 659 hits were old - of the buildings using this system that have exposed it to ZoomEye in the past year, 43% are vulnerable and have essentially no access control. The large majority (71) of the exposed systems are in Canada, not surprising since 582 out of the 742 ZoomEye hits were Canadian (Nuclei scans fewer targets due to some duplicates).</p>
<p>I’m so glad we have modern IoT technology to keep us safe! It’s crazy to think people used to trust analog locks with physical keys.</p>
</section>
<section id="timeline">
<h3 data-anchor-id="timeline">Timeline</h3>
<ul>
<li>2024-12-20: vulnerability discovered</li>
<li>2024-12-27: Current vendor of MESH identified as Hirsch (subsidiary of Vitaprotech Group) and contacted</li>
<li>2025-01-09: CEO of Identiv, former vendor of MESH, contacted</li>
<li>2025-01-11: Hirsch product security responds requesting details and are asked if they intend to alert clients</li>
<li>2025-01-29: Hirsch replies stating that these vulnerable systems are not following manufacturers’ recommendations to change the default password</li>
<li>2025-01-30: Hirsch asked for an update as to whether clients running vulnerable systems have been alerted (no response as of publication)</li>
<li>2025-02-14: CVE-2025-26793 assigned</li>
<li>2025-02-15: publication</li>
</ul>
</section>
<section id="support">
<h3 data-anchor-id="support">Support</h3>
<p>If you’ve made it this far, consider supporting my work with a small donation on <a href="https://ko-fi.com/edaigle">ko-fi</a>! This site is ad-free, and social-media-free and uses open-source privacy-respecting <a href="https://matomo.org/privacy/">analytics</a>.</p>


</section>

</main> <!-- /main -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introduction to Stochastic Calculus (382 pts)]]></title>
            <link>https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/</link>
            <guid>43160779</guid>
            <pubDate>Mon, 24 Feb 2025 15:40:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/">https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/</a>, See on <a href="https://news.ycombinator.com/item?id=43160779">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="introduction-to-stochastic-calculus"><strong>Introduction to Stochastic Calculus</strong></h2><p>Notation and code for generating visuals are presented in the <a href="#appendix">Appendix</a>.</p><h3 id="0-introduction"><span><strong>0. Introduction</strong></span><a href="#0-introduction"><i></i></a></h3><p>This document is a brief introduction to stochastic calculus. Like, an actual introduction. Not the textbook “introductions” which immediately blast you with graduate-level probability theory axioms and definitions.</p><p>The goal of this blog post is more to focus on the physical intuition and derivation of Brownian motion, which is the foundation of stochastic calculus. I will avoid very technical formalisms such as probability spaces, measure theory, filtrations, etc. in favor of a more informal approach by considering only well-behaved cases. I also try to avoid introducing too many new concepts and vocabulary.</p><p>I hope that a wider audience can feel inspired as to how stochastic calculus emerges naturally from the physical world. Then, hopefully, more people can appreciate the beauty and meaning of the mathematics behind it, and decide to dig deeper into the subject.</p><h4 id="applications"><span>Applications</span><a href="#applications"><i></i></a></h4><p>Brownian motion and Itô calculare a notable example of fairly high-level mathematics that are applied to model the real world. Stock prices jiggle erratically, molecules bounce in fluids, and noise partially corrupts signals. Stochastic calculus gives us tools to predict, optimize, and understand these messy systems in a simpified model.</p><ul><li><strong>Physics</strong>: Einstein used Brownian motion to prove atoms exist—its jittering matched molecular collisions.</li><li><strong>Finance</strong>: Option pricing (e.g., the famous Black-Scholes equation) relies on stochastic differential equations like \(dS = \mu S dt + \sigma S dW\).</li><li><strong>Biology</strong>: Random walks model how species spread or neurons fire.</li></ul><p>This is just the tip of the iceberg. More and more applications are emerging, notably in machine learning, as <a href="https://arxiv.org/abs/2011.13456">Song et al. (2021)</a> have shown in their great paper “Score-Based Generative Modeling through Stochastic Differential Equations”.</p><p>They precisely use a stochastic differential equation using Itô calculus to model the evolution of noise over time, which they can then reverse in time to generate new samples. This framework generalizes previous ones and improves performance, allowing for new paths of innovation to be explored.</p><h3 id="1-motivation"><span><strong>1. Motivation</strong></span><a href="#1-motivation"><i></i></a></h3><p>Pascal’s triangle gives the number of paths that go either left or right at each step, up to a certain point:</p><p>\[\begin{array}{cccccc} &amp; &amp; &amp; 1 &amp; &amp; &amp; \\ &amp; &amp; 1 &amp; &amp; 1 &amp; &amp; \\ &amp; 1 &amp; &amp; 2 &amp; &amp; 1 &amp; \\ 1 &amp; &amp; 3 &amp; &amp; 3 &amp; &amp; 1 \end{array}\]</p><p>Using 0-indexing, the number of ways to reach the \(k\)-th spot in the \(n\)-th row is \(\binom{n}{k} = \frac{n!}{k!(n-k)!}\). For example, in row 3, there are \(\binom{3}{2} = 3\) ways to hit position 2.</p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/binom_3_2_paths_pascal.svg"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/binom_3_2_paths_pascal.svg" alt="Pascal's Triangle Paths for 3 choose 2" loading="lazy"></a> <em><a href="#b5-latex-code-for-tikz-diagram-of-paths-in-pascals-triangle">Code</a> 2D image: All 3 paths for the 2nd position in the 3rd row of Pascal’s triangle</em></p><p>Why care? This setup powers the binomial distribution, which models repeated trials with two outcomes—win or lose, heads or tails. Think of:</p><ul><li>A basketball player shooting free throws with probability \(p\) of success and \(q = 1 - p\) of failure.</li><li>A gambler betting on dice rolls.</li></ul><p>Pascal’s triangle tells us there are \(\binom{n}{k}\) ways to get \(k\) wins in \(n\) trials. If the trials are <strong>independent</strong>, we can use the multiplication rule for probabilities:</p><blockquote><p>Note that the independence assumption is <strong>strong</strong>. Real life isn’t always so clean—winning streaks in games often tie to mentality or momentum, not just chance. Keep in mind that this model can and will be inaccurate, especially visibile for very long streaks in phenomena like stock prices or sports. However, in more common scenarios, it usually approximates reality well.</p></blockquote><p>\[P(A \text{ and } B \text{ and } C \dots) = P(A) P(B) P(C) \dots\]</p><p>For one sequence with \(k\) wins (probability \(p\) each) and \(n - k\) losses (probability \(q\) each), the probability is \(p^k q^{n-k}\). Multiply by the number of ways to arrange those wins, and we get:</p><p>\[P(k \text{ wins in } n \text{ trials}) = \binom{n}{k} p^k q^{n-k}\]</p><p>This is the binomial distribution—great for discrete setups. Now, let’s zoom out. The real world often involves <strong>continuous</strong> processes, like:</p><ul><li>The motion of a falling object,</li><li>Gas diffusing through a room,</li><li>Stock prices jumping around,</li><li>Molecules colliding in a liquid.</li></ul><p>For these, the binomial model gets messy as trials pile up. Calculus, with its focus on continuous change, feels more natural. In the continuous case:</p><blockquote><p>Points and sums (discrete tools) lead to infinities. We need <strong>intervals</strong> and <strong>integrals</strong> instead.</p></blockquote><h3 id="2-from-discrete-steps-to-continuous-limits"><span><strong>2. From Discrete Steps to Continuous Limits</strong></span><a href="#2-from-discrete-steps-to-continuous-limits"><i></i></a></h3><p>It’s actually known what happens to the binomial distribution as it becomes continuous. But what does that conversion mean mathematically? Let’s dig in with examples and then formalize it.</p><p>In calculus, going from discrete to continuous means shrinking step sizes and cranking up the number of steps. For an interval \([a, b]\), we:</p><ol><li>Split it into \(n\) chunks of size \(h = \frac{b - a}{n}\),</li><li>Sum up contributions (like a Riemann sum),</li><li>Let \(n \to \infty\) and \(h \to 0\), landing on an integral.</li></ol><p>Can we adapt this to the binomial distribution? Let’s try.</p><p>Picture the \(n\)-th row of Pascal’s triangle as a random walk: at each of \(n\) steps, we move \(+1\) (a win) or \(-1\) (a loss).</p><p>We’ll set the probabability of winning as \(p = 0.5\) as a first example since it’s symmetric, making each direction equally likely and simpler to work with.</p><p>The number of ways to get \(k\) wins (and \(n - k\) losses) is \(\binom{n}{k}\). Let’s try to plot this for a different values \(n\) over \(k\). (The code can be found in the <a href="#b1-python-code-for-binomial-plots">Appendix</a>.)</p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/random_walk_combined.png"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/random_walk_combined.png" alt="Plots for n=5,10,25,50,100" loading="lazy"></a> <em><a href="#b1-python-code-for-binomial-plots">Code</a> 2D image: Binomial distribution plots for n=5,10,25,50,100</em></p><p>That looks awfully familiar, doesn’t it? It’s a bell curve, so naturally, we might guess that the limit is a <strong>normal distribution</strong> (aka Gaussian distribution).</p><p>Where does such a normal distribution arise from? The answer lies in the <strong>Central Limit Theorem</strong>, which states that the sum of a large number of independent random variables will be approximately normally distributed. So where’s the sum happening here? Let’s proceed to formalizing our intuition.</p><p>To accomplish this, let’s define a random variable for a single step as:</p><p>\[X(t) = \begin{cases} 1 &amp; \text{with probability } \frac{1}{2} \\ -1 &amp; \text{with probability } \frac{1}{2} \\ \end{cases}\]</p><p>Here, \(X(t)\) will encode our displacement at the \(t\)-th step where \(t \in \{1,\dots,n\}\) is an indexing parameter. As before, we assume that \(X(t_1)\) is independent of \(X(t_2)\) for \(t_1 \ne t_2\). At each step \(t\), \(X(t)\) has mean \(0\) and variance \(1\).</p><p>Then, the overall displacement \(S(n)\) is:</p><p>\[S(n) = X(1) + X(2) + \dots + X(n) = \sum_{t=1}^n X(t)\]</p><p>So there it is! The central limit theorem states more precisely that given \(n\) independent and identically distributed random variables \(X_1, X_2, \dots, X_n\) with mean \(\mu\) and variance \(\sigma^2\), we have:</p><p>\[X_1 + \dots + X_n \sim N(n\mu, n\sigma^2) \text{ as } n \to \infty\]</p><p>This is precisely what need. As we take \(n \to \infty\), we have that</p><p>\[S(n) \sim N(0, n)\]</p><p>such that</p><p>\[\lim_{n \to \infty} \frac{1}{\sqrt{n}} \cdot S(n) = N(0, 1)\]</p><p>which is our desired limit. We have shown that a “continuous binomial distribution” is in fact a normal distribution.</p><p>Here are some very nice 3D animations of sample paths with the distribution evolving over the number of steps:</p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/discrete_binomial.gif"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/discrete_binomial.gif" alt="Discrete Random Walk, 15 steps" loading="lazy"></a> <em><a href="#c1-3d-plot-of-discrete-random-walks">Code</a> 3D animation: Discrete Random Walk, 15 steps</em></p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/discrete_binomial_normalizing.gif"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/discrete_binomial_normalizing.gif" alt="Discrete Random Walk, 100 steps" loading="lazy"></a> <em><a href="#c1-3d-plot-of-discrete-random-walks">Code</a> 3D Animation: Discrete Random Walk, 100 steps over 5 seconds</em></p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/discrete_random_walk.gif"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/discrete_random_walk.gif" alt="Normal Distribution Approximation by Random Walks" loading="lazy"></a> <em><a href="#c4-python-code-for-normal-distribution-approximation-by-random-walks">Code</a> 2D animation: Normal distribution approximation by discrete random walks</em></p><h3 id="3-defining-brownian-motion-wiener-process"><span><strong>3. Defining Brownian motion (Wiener process)</strong></span><a href="#3-defining-brownian-motion-wiener-process"><i></i></a></h3><p>Let’s consider a scenario faced by Scottish botanist <strong>Robert Brown</strong> in the 1820s. Imagine a small particle, like dust or pollen, floating on a body of water.</p><p>Brown realized that its movement was surprisingly erratic. It seemed like the small-scale nature of the setup resulted in such sensitivity to fluctuations, so much is that the real movement from external forces would completely overtake the previous one. Hence, in a simplified mathematical model we scale consider the events at different times as <em>independent</em>.</p><p>In addition, there is positional symmetry: the average position of the particle at time \(t\) seemed float approximately around the origin.</p><p>Motivated by these observations as well as our previous intuition on continuous random walks, let’s first think about a simplified model for 1-dimensional discrete case. We’ll list some properties that a continuous random walk should have.</p><ol><li><strong>Starting Point</strong>: As a mathematical convenience, we position our coordinate system to set the starting point of the walk to be zero.</li><li><strong>Positional Symmetry</strong>: The walk has no directional bias. For each step, the expected displacement is zero, such that the overall expected displacement is also zero.</li><li><strong>Independence</strong>: Steps at different times are independent. The displacement between two different intervals of time is independent.</li><li><strong>Continuity</strong>: The walk is continuous, with no jumps or gaps.</li><li><strong>Normality</strong>: As we established by taking discrete random walks in the continuous limit, the distribution of positions at any given time should be normal.</li></ol><p>So let’s write this mathematically. Such a random variable is usually denoted either by \(B_t\) for “Bronian motion”, which is the physical phenomenon, or \(W_t\) for “Wiener process”, in honor of the mathematician <strong>Norbert Wiener</strong> who developed a lot of its early theory.</p><p>I will use \(W(t)\) to emphasize its dependence on \(t\).</p><p>Let \(W(t)\) be the position of the Brownian motion at time \(t\), and let \(\Delta W(t_1,t_2)\) be the displacement of the Brownian motion from time \(t_1\) to time \(t_2\).</p><blockquote><p>Note that, unlike the discrete case, we cannot consider a single increment and have a single index \(t\) for displacements as we did with \(X(t)\). As mentioned, the continuous case requires considering intervals instead of single steps.</p></blockquote><p>Then, we write some properties of Brownian motion:</p><ol><li>\(W(0)=0\) almost surely</li><li>\(W(t)\sim N(0,t)\)<ul><li>With the first condition, this is often written equivalently as \(\Delta W(s,t)\sim N(0,t-s)\) for all \(s \ne t\)</li></ul></li><li>\(\Delta W(t_1,t_2)\) is independent of \(\Delta W(t_2,t_3)\) for arbitrary distinct \(t_1 &lt; t_2 \le t_3\)</li></ol><p>We can straightforwardly use these conditions are enough to find</p><ol><li>\(E[W(t)]=0\) for all \(t\)</li><li>\(Var(W(t))=t\) for all \(t\)</li></ol><p>This is analogous to the discrete case.</p><p>But it also turns out that these conditions are sufficient to prove continuity, although it’s more involved:</p><ol><li>The sample path \(t \mapsto W(t)\) is almost surely uniformly Hölder continuous for each exponent \(\gamma &lt; \frac{1}{2}\), but is nowhere Hölder continuous for \(\gamma &gt;= \frac{1}{2}\). <a href="https://math.nyu.edu/~bourgade/SA2010/StochasticAnalysis.pdf#page30">p.30,33 of source</a><ul><li>In particular, a sample path \(t \mapsto W(t)\) is almost surely nowhere differentiable.</li></ul></li></ol><p>So, \(W(t)\) is our mathematical model for Brownian motion: a continuous, random, zero-mean process with variance proportional to time. It’s wild—it’s globally somewhat predictable yet locally completely unpredictable. A plot of W(t) looks like a jagged mess, but it’s got structure under the hood. (You can generate one yourself with the code in <a href="#b2-python-code-for-brownian-motion-plot">Appendix</a>.)</p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/sample_brownian_motion.png"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/sample_brownian_motion.png" alt="Sample Brownian Motion Path" loading="lazy"></a> <em><a href="#b2-python-code-for-brownian-motion-plot">Code</a> 2D image: Sample Brownian motion path</em></p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/continuous_brownian_3d_smooth.gif"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/continuous_brownian_3d_smooth.gif" alt="3D Animation Continuous Brownian Motion" loading="lazy"></a> <em><a href="#c2-3d-animation-of-brownian-motion">Code</a> 3D animation: Brownian motion with evolving distribution</em></p><p>Now, let’s take this beast and do something useful with it.</p><hr><h3 id="4-itô-calculus"><span><strong>4. Itô Calculus</strong></span><a href="#4-itô-calculus"><i></i></a></h3><p>Brownian motion \(W(t)\) is continuous but so irregular that it’s nowhere differentiable. To see why, consider the rate of change over a small interval \(dt\):</p><p>\[\lim_{dt \to 0} \frac{W(t + dt) - W(t)}{dt} = \lim_{dt \to 0} \frac{\Delta W(t, t + dt)}{dt}\]</p><p>Since \(\Delta W(t, t + dt) \sim N(0, dt) = \sqrt{dt} \, N(0, 1)\):</p><p>\[\frac{\Delta W(t, t + dt)}{dt} = \frac{\sqrt{dt} \, N(0, 1)}{dt} = \frac{1}{\sqrt{dt}} N(0, 1)\]</p><p>As \(dt \to 0\), \(\frac{1}{\sqrt{dt}}\) grows without bound, and the expression becomes dominated by random fluctuations—it doesn’t converge to a finite derivative. This rules out standard calculus for handling Brownian motion, but we still need a way to work with processes driven by it, like stock prices or particle diffusion.</p><p>In the 1940s, Kiyosi Itô developed a framework to address this: <strong>Itô calculus</strong>. Rather than forcing Brownian motion into the rules of regular calculus, Itô built a new system tailored to its random nature, forming the foundation of stochastic calculus.</p><h4 id="the-increment-dw-and-its-properties"><span><strong>The Increment \(dW\) and Its Properties</strong></span><a href="#the-increment-dw-and-its-properties"><i></i></a></h4><p>Define the small change in Brownian motion over an interval \(dt\):</p><p>\[dW := W(t + dt) - W(t) = \Delta W(t, t + dt)\]</p><p>From Section 3, \(W(t + dt) - W(t) \sim N(0, dt)\), so:</p><p>\[dW = \sqrt{dt} \, N(0, 1)\]</p><p>Unlike the deterministic \(dx\) in regular calculus, \(dW\) is random—its magnitude scales with \(\sqrt{dt}\), and its sign depends on a standard normal distribution \(N(0, 1)\). It’s a small but erratic step, with:</p><ul><li>\(E[dW] = 0\),</li><li>\(Var(dW) = E[(dW)^2] = dt\).</li></ul><p>Now consider \((dW)^2\). Its expected value is \(dt\), but what about its variability? The variance is \(Var[(dW)^2] = 2 dt^2\), which becomes negligible as \(dt \to 0\). This stability allows us to treat \((dW)^2 \approx dt\) in Itô calculus (formally, in the mean-square sense—see the <a href="#a1-notation">Appendix</a> for details). In contrast to regular calculus, where \((dx)^2\) is too small to matter, \((dW)^2\) is on the same scale as \(dt\), which changes how we handle calculations.</p><h4 id="the-itô-integral-integrating-against-randomness"><span><strong>The Itô Integral: Integrating Against Randomness</strong></span><a href="#the-itô-integral-integrating-against-randomness"><i></i></a></h4><p>In regular calculus, \(\int_a^b f(x) \, dx\) approximates the area under a curve by summing rectangles, \(\sum f(x_i) \Delta x\), and taking the limit as \(\Delta x \to 0\). For Brownian motion, we want something like \(\int_0^t f(s) \, dW(s)\), where \(dW(s)\) replaces \(dx\). Here, the steps are random: \(\Delta W(s_i, s_{i+1}) \sim \sqrt{\Delta s} \, N(0, 1)\). We approximate:</p><p>\[\int_0^t f(s) \, dW(s) \approx \sum_{i=0}^{n-1} f(s_i) [\Delta W(s_i, s_{i+1})]\]</p><p>over a partition \(s_0, s_1, \dots, s_n\) of \([0, t]\), then let \(n \to \infty\). Unlike a deterministic integral, the result is a random variable, reflecting \(W(t)’s\) randomness. Using \(f(s_i)\) from the left endpoint keeps the integral “non-anticipating”—we only use information up to time \(s_i\), which aligns with the forward-evolving nature of stochastic processes.</p><h4 id="itôs-lemma-a-chain-rule-for-randomness"><span><strong>Itô’s Lemma: A Chain Rule for Randomness</strong></span><a href="#itôs-lemma-a-chain-rule-for-randomness"><i></i></a></h4><p>For a function \(f(t, W(t))\), regular calculus gives:</p><p>\[df = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial W} dW\]</p><p>But Brownian motion’s roughness requires a second-order term. Taylor-expand \(f(t, W(t))\):</p><p>\[df = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial W} dW + \frac{1}{2} \frac{\partial^2 f}{\partial W^2} (dW)^2 + \text{smaller terms}\]</p><p>As \(dt \to 0\):</p><ul><li>\(dt^2\) and \(dt \, dW\) vanish,</li><li>\((dW)^2 \approx dt\) stays significant.</li></ul><p>This leaves:</p><p>\[df = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial W} dW + \frac{1}{2} \frac{\partial^2 f}{\partial W^2} dt\]</p><p>This is <strong>Itô’s Lemma</strong>. The extra \(\frac{1}{2} \frac{\partial^2 f}{\partial W^2} dt\) arises because \((dW)^2\) contributes at the \(dt\) scale, capturing the effect of Brownian motion’s curvature.</p><p>Since we have the algebraic heuristic \(dW^2 = dt\), we could in some define everything in terms of powers \(dW\) to expand things algebraically and implicitly compute derivative rules.</p><p>This is precisely the idea behind my blog post on <a href="https://jiha-kim.github.io/posts/automatic-stochastic-differentiation/index.html">Automatic Stochastic Differentiation</a>, where we use \(\mathbb{R}[\epsilon]/\epsilon^3\) in a similar fashion to dual numbers \(\mathbb{R}[\epsilon]/\epsilon^2\) for automatic differentiation in deterministic calculus.</p><p>If you haven’t already, I highly recommend checking it out.</p><h4 id="example-fw--w2"><span><strong>Example: \(f(W) = W^2\)</strong></span><a href="#example-fw--w2"><i></i></a></h4><p>Take \(f(t, W(t)) = W^2\):</p><ul><li>\(\frac{\partial f}{\partial t} = 0\),</li><li>\(\frac{\partial f}{\partial W} = 2W\),</li><li>\(\frac{\partial^2 f}{\partial W^2} = 2\).</li></ul><p>Then:</p><p>\[d(W^2) = 0 \cdot dt + 2W \, dW + \frac{1}{2} \cdot 2 \cdot dt = 2W \, dW + dt\]</p><p>Integrate from 0 to \(t\) (with \(W(0) = 0\)):</p><p>\[W(t)^2 = \int_0^t 2W(s) \, dW(s) + t\]</p><p>The \(t\) term matches \(E[W(t)^2] = t\), and the integral is a random component with mean 0, consistent with Brownian motion’s properties.</p><hr><h3 id="5-stochastic-differential-equations"><span><strong>5. Stochastic Differential Equations</strong></span><a href="#5-stochastic-differential-equations"><i></i></a></h3><p>Itô calculus gives us tools—integrals and a chain rule—to handle Brownian motion. Now we can model systems where randomness and trends coexist, using <strong>stochastic differential equations (SDEs)</strong>. Unlike regular differential equations (e.g., \(\frac{dx}{dt} = -kx\)) that describe smooth dynamics, SDEs blend deterministic behavior with stochastic noise, fitting phenomena like stock prices or diffusing particles.</p><h4 id="defining-an-sde"><span><strong>Defining an SDE</strong></span><a href="#defining-an-sde"><i></i></a></h4><p>Consider a process influenced by both a predictable trend and random fluctuations:</p><p>\[dX(t) = a(t, X(t)) \, dt + b(t, X(t)) \, dW(t)\]</p><ul><li>\(X(t)\): The evolving quantity (e.g., position or price).</li><li>\(a(t, X(t)) \, dt\): The “drift”—the systematic part, scaled by \(dt\).</li><li>\(b(t, X(t)) \, dW(t)\): The “diffusion”—random perturbations from Brownian motion.</li></ul><p>Here, \(a\) and \(b\) are functions of time and state, and \(dW(t) = \sqrt{dt} \, N(0, 1)\) brings the noise. Solutions to SDEs aren’t fixed curves but random paths, each run producing a different trajectory with statistical patterns we can study.</p><h4 id="itôs-lemma-revisited"><span><strong>Itô’s Lemma Revisited</strong></span><a href="#itôs-lemma-revisited"><i></i></a></h4><p>Itô’s lemma actually applies to a function \(f(t, X(t))\) and its stochastic derivative \(df(t, X(t))\) for a general \(dX(t) = b(t,X(t))dt+\sigma(t,X(t))dW\), and this is done through the linearity of the Itô differential (as seen using the \(\mathbb{R}[\epsilon]/\epsilon^3\) formulation).</p><p>Considering that \(dX=O(dW)\), we consider terms up to \(dX^2=O(dW^2)\):</p><p>\[\begin{aligned} df &amp;= f_t \, dt + f_X \, dX + \frac{1}{2}f_{XX} dX^2 \\ &amp;= f_t \, dt + f_X \, (b \, dt+\sigma \, dW) + \frac{1}{2}f_{XX} (b \, dt+\sigma \, dW)^2 \\ &amp;= (f_t + bf_X+\frac{1}{2}\sigma^2 f_{XX}) \, dt + \sigma f_X \, dW \end{aligned}\]</p><p>which is the general form typically presented.</p><h4 id="drift-and-diffusion"><span><strong>Drift and Diffusion</strong></span><a href="#drift-and-diffusion"><i></i></a></h4><p>The drift \(a(t, X)\) sets the average direction, like a current pushing a particle. The diffusion \(b(t, X)\) determines the random jitter’s strength. If \(b = 0\), we get a standard ODE; if \(a = 0\), it’s just scaled Brownian motion. Together, they model systems with both structure and uncertainty.</p><p>Take a simple case:</p><p>\[dX(t) = \mu \, dt + \sigma \, dW(t)\]</p><ul><li>\(\mu\): Constant drift.</li><li>\(\sigma\): Constant noise amplitude.</li></ul><p>Starting at \(X(0) = 0\), integrate:</p><p>\[X(t) = \int_0^t \mu \, ds + \int_0^t \sigma \, dW(s) = \mu t + \sigma W(t)\]</p><p>Since \(W(t) \sim N(0, t)\), we have \(X(t) \sim N(\mu t, \sigma^2 t)\)—a process drifting linearly with noise spreading over time. It’s a basic model for things like a stock with steady growth and volatility.</p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/sample_SDE.png"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/sample_SDE.png" alt="Sample SDE Path" loading="lazy"></a> <em><a href="#b3-python-code-for-basic-sde-simulation">Code</a> 2D image: Sample SDE path with mu=1.0, sigma=0.5</em></p><h4 id="geometric-brownian-motion"><span><strong>Geometric Brownian Motion</strong></span><a href="#geometric-brownian-motion"><i></i></a></h4><p>For systems where changes scale with size—like stock prices or certain physical processes—consider <strong>geometric Brownian motion (GBM)</strong>:</p><p>\[dS(t) = \mu S(t) \, dt + \sigma S(t) \, dW(t)\]</p><ul><li>\(S(t)\): The state (e.g., stock price).</li><li>\(\mu S(t)\): Proportional drift.</li><li>\(\sigma S(t)\): Proportional noise.</li></ul><p>The percentage change \(\frac{dS}{S} = \mu \, dt + \sigma \, dW\) has a trend and randomness. To solve, let \(f = \ln S\):</p><ul><li>\(\frac{\partial f}{\partial t} = 0\),</li><li>\(\frac{\partial f}{\partial S} = \frac{1}{S}\),</li><li>\(\frac{\partial^2 f}{\partial S^2} = -\frac{1}{S^2}\).</li></ul><p>Using Itô’s lemma:</p><p>\[d(\ln S) = \frac{1}{S} (\mu S \, dt + \sigma S \, dW) + \frac{1}{2} \left( -\frac{1}{S^2} \right) (\sigma^2 S^2 dt)\] \[= \left( \mu - \frac{1}{2} \sigma^2 \right) dt + \sigma \, dW\]</p><p>Integrate from \(0\) to \(t\):</p><p>\[\ln S(t) - \ln S(0) = \left( \mu - \frac{1}{2} \sigma^2 \right) t + \sigma W(t)\] \[S(t) = S(0) \exp\left( \left( \mu - \frac{1}{2} \sigma^2 \right) t + \sigma W(t) \right)\]</p><p>The drift is adjusted by \(-\frac{1}{2} \sigma^2\) due to the second-order effect of noise, and \(\sigma W(t)\) adds random fluctuations. This form underlies the Black-Scholes model in finance.</p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/gbm_path.png"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/gbm_path.png" alt="Sample Geometric Brownian Motion Path" loading="lazy"></a> <em><a href="#b4-python-code-for-geometric-brownian-motion-simulation">Code</a> 2D image: A sample path of a geometric Brownian motion with parameters μ = 0.15 and σ = 0.2</em></p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/geometric_brownian_drifted_3d.gif"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/geometric_brownian_drifted_3d.gif" alt="Geometric Brownian Motion drifting over time" loading="lazy"></a> <em><a href="#c3-3d-animation-of-geometric-brownian-motion">Code</a> 3D animation: Geometric Brownian Motion drifting over time</em></p><h4 id="beyond-analytics"><span><strong>Beyond Analytics</strong></span><a href="#beyond-analytics"><i></i></a></h4><p>Analytical solutions like GBM’s are exceptions. Most SDEs require numerical simulation (e.g., stepping \(X(t + \Delta t) = X(t) + \mu \Delta t + \sigma \sqrt{\Delta t} \, N(0, 1)\)) or statistical analysis via equations like Fokker-Planck. See the <a href="#b3-python-code-for-basic-sde-simulation">appendix</a> for simulation code.</p><hr><h3 id="6-stratonovich-calculus"><span><strong>6. Stratonovich Calculus</strong></span><a href="#6-stratonovich-calculus"><i></i></a></h3><p>Recall Itô’s lemma:</p><p>\[df = \left(\frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial X^2}\right) dt + \frac{\partial f}{\partial X} dX\]</p><p>That second derivative term is pretty annoying to deal with in calculations. Is there a way we can simplify it to the familiar chain rule in regular calculus?</p><p>\[df = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial X} dX\]</p><p>The answer is yes, and it’s called <strong>Stratonovich calculus</strong>. Let’s explore a bit. First, the deterministic part clearly satisfies the regular chain rule, since we can directly apply it using linearity. The trouble arises in the stochastic part, which we need to analyze. This means we only need to consider a function \(f(X(t))\).</p><p>Remember, for the Itô form, we chose to define the integral by choosing the left endpoint of each interval. In other words, it is this stochastic part that will vary. To delete this second order term, we need to somehow absorb it into the stochastic part by defining some Stratonovich differential, typically denoted by \(\circ dW\).</p><p>Going back to our Riemann sum definitions, our degrees of freedom lie in the choice of the evaluation point for each interval:</p><p>\[\int_{0}^{T} f(X(t)) \diamond dW = \lim_{n \to \infty} \sum_{i=0}^{n-1} f(X(t_i) + \lambda \Delta X(t_i,t_{i+1})) \Delta W(t_i, t_{i+1})\]</p><p>where \(\lambda \in [0,1]\) is a constant that linearly interpolates between the left and right endpoints of each interval giving a corresponding differential \(\diamond dW\), and \(\Delta X(t_i,t_{i+1}):=X(t_{i+1})-X(t_i)\).</p><p>In the deterministic case, since we always have \(O(dX^2) \to 0\), it doesn’t matter where we choose the evaluation point. However, in the stochastic case, remember that \(O(dW^2) \to O(dt)\), so we need a more careful choice of evaluation point.</p><p>Mathematically, our goal is to define a new stochastic integral that preserves the standard chain rule:</p><p>\[df = f_X \circ dX\]</p><p>In the limiting discrete form, let’s try setting every term equal to each other:</p><p>\[f(X+\Delta X) - f(X) = f_X(X+\lambda \Delta X) \Delta X\]</p><p>In other words, our newly defined differential should result in the derivative being a linear approximation of the original function instead of quadratic:</p><p>\[\frac{f(X+\Delta X)-f(X)}{\Delta X} = f_X(X+\lambda \Delta X)\]</p><p>But watch what happens as we take the Taylor expansion on both sides about \(X\) (recalling that \(o(\Delta X^2)\to 0\)):</p><p>\[f_X + \frac{1}{2}f_{XX}\Delta X = f_X + \lambda f_{XX}\Delta X\]</p><p>Comparing coefficients, we wish to set \(\lambda = 1/2\) to preserve the chain rule. So Stratonovich integrals are defined by the midpoint evaluation rule:</p><p>\[\begin{aligned} \int_{0}^{T} f(X(t)) \circ dW &amp;= \lim_{n \to \infty} \sum_{i=0}^{n-1} f(X(t_i) + \frac{1}{2} \Delta X(t_i,t_{i+1})) \Delta W(t_i, t_{i+1}) \\ &amp;= \lim_{n \to \infty} \sum_{i=0}^{n-1} f\left(\frac{X(t_i)+X(t_{i+1})}{2}\right) \Delta W(t_i, t_{i+1}) \\ \end{aligned}\]</p><h4 id="conversion-formula-between-itô-and-stratonovich"><span>Conversion Formula between Itô and Stratonovich</span><a href="#conversion-formula-between-itô-and-stratonovich"><i></i></a></h4><p>There is a formula to convert the Stratonovich differential into a corresponding Itô SDE that depends on the Itô differential as well as the volatility function \(\sigma\).</p><p>Recall that Itô’s lemma states that for \(dX = a dt + b dW\):</p><p>\[df = f_t dt + f_X dX + \frac{1}{2}f_{XX} dX^2 = (af_t + \frac{1}{2} b^2 f_{XX}) dt + bf_X dW\]</p><p>In parallel, we defined Stratonovich’s chain rule to satisfy for \(dX = \tilde a dt + \tilde b \circ dW\):</p><p>\[df = f_t dt + f_X \circ dX = (f_t + \tilde a f_X) dt + \tilde b f_X \circ dW\]</p><p>Hence, between Itô and Stratonovich SDEs, we have in both cases that the differential is scaled by the volatility function of \(X\) and \(f_X\), but the drift function changes. Let’s find a conversion formula between the two.</p><p>Suppose we have:</p><p>\[dX = a dt + b dW = \tilde a dt + b \circ dW\]</p><p>Then, our objective is to find \(\tilde a\) in terms of \(a\).</p><p>Recall from the integral definition that \(b(X) \circ dW = b(X+\frac{1}{2}dX) dW\). If we Taylor expand around \(X\), we have:</p><p>\[b(X+\frac{1}{2}dX) dW = b(X)dW + b_X(X)\frac{1}{2}dX dW + o(dt)\]</p><p>Now, if we plug in \(dX=a dt + b dW\), the first term vanishes, leaving \(b_X b \frac{1}{2}dW^2 \sim \frac{1}{2}b_X b dt\) (where the arguments \(X\) are left implicit).</p><p>Hence:</p><p>\[a = \tilde a + \frac{1}{2} b_X b.\]</p><h4 id="applications-of-stratonovich-calculus"><span><strong>Applications of Stratonovich Calculus</strong></span><a href="#applications-of-stratonovich-calculus"><i></i></a></h4><p>Stratonovich calculus, with its midpoint evaluation rule, adjusts how we handle stochastic integrals compared to Itô’s left-endpoint approach. This shift makes it valuable in certain fields where its properties align with physical systems or simplify calculations. Below are some practical applications, each with a concrete mathematical example.</p><ul><li><p><strong>Physics with Multiplicative Noise</strong>: In physical systems, noise often scales with the state—like a particle in a fluid where random kicks depend on its position. Consider a damped oscillator with position \(X(t)\) under state-dependent noise:</p>\[dX = -k X \, dt + \sigma X \circ dW\]<p>Here, \(k &gt; 0\) is the damping constant, \(\sigma\) is the noise strength, and \(\circ dW\) denotes the Stratonovich differential. Using Stratonovich’s chain rule, for \(f(X) = \ln X\):</p>\[d(\ln X) = \frac{1}{X} (-k X \, dt + \sigma X \circ dW) = -k \, dt + \sigma \circ dW\]<p>This integrates to \(X(t) = X(0) e^{-kt + \sigma W(t)}\), matching the expected exponential decay with noise. Stratonovich fits here because it preserves symmetries in continuous physical processes, unlike Itô, which adds a \(\frac{1}{2} \sigma^2 X \, dt\) drift term.</p></li><li><p><strong>Wong-Zakai Theorem and Smooth Noise</strong>: Real-world noise isn’t perfectly white (uncorrelated like \(dW\))—it’s often smoother. The Wong-Zakai theorem shows that approximating smooth noise (e.g., \(\eta(t)\) with correlation time \(\epsilon\)) as \(\epsilon \to 0\) yields a Stratonovich SDE. Take a simple system:</p>\[\dot{x} = a x + b x \eta(t)\]<p>As \(\eta(t) \to\) white noise, this becomes \(dX = a X \, dt + b X \circ dW\). In Stratonovich form, the solution is \(X(t) = X(0) e^{a t + b W(t)}\). This is useful in engineering, like modeling voltage in a circuit with thermal fluctuations, where noise has slight smoothness.</p></li><li><p><strong>Stochastic Control</strong>: In control problems, Stratonovich can simplify dynamics under feedback. Consider a system with control input \(u(t)\) and noise:</p>\[dX = (a X + u) \, dt + \sigma X \circ dW\]<p>For \(f(X) = X^2\), the Stratonovich rule gives:</p>\[d(X^2) = 2X (a X + u) \, dt + 2X \cdot \sigma X \circ dW = (2a X^2 + 2X u) \, dt + 2\sigma X^2 \circ dW\]<p>The lack of a second-derivative term (unlike Itô’s \(+ \sigma^2 X^2 dt\)) aligns with classical control intuition, making it easier to design \(u(t)\) for, say, stabilizing a noisy pendulum or a drone in wind.</p></li><li><p><strong>Biological Diffusion</strong>: In biology, noise can depend on spatial gradients, like protein diffusion across a cell. Model this as:</p>\[dX = \mu \, dt + \sigma(X) \circ dW, \quad \sigma(X) = \sqrt{2D (1 + k X^2)}\]<p>where \(D\) is diffusivity and \(k\) adjusts noise with position. Stratonovich ensures the diffusion term reflects physical conservation laws, matching experimental data in systems like bacterial motility better than Itô, which alters the drift.</p></li><li><p><strong>Numerical Stability</strong>: For simulations, Stratonovich pairs well with midpoint methods. Take \(dX = -a X \, dt + \sigma \circ dW\). A Stratonovich discretization might use:</p>\[X_{n+1} = X_n - a \left(\frac{X_n + X_{n+1}}{2}\right) \Delta t + \sigma \Delta W_n\]<p>This implicit scheme leverages the midpoint rule, reducing numerical artifacts in models like chemical kinetics compared to Itô’s explicit steps.</p></li></ul><p>The choice between Stratonovich and Itô depends on context. Stratonovich suits systems where noise is tied to physical continuity or symmetry, while Itô dominates in finance for its non-anticipating properties. The conversion \(a = \tilde{a} + \frac{1}{2} b b_X\) lets you switch forms as needed.</p><h2 id="appendix"><span>Appendix</span><a href="#appendix"><i></i></a></h2><h3 id="a0-further-reading"><span>A.0. Further Reading</span><a href="#a0-further-reading"><i></i></a></h3><ul><li><a href="https://www.chrisrackauckas.com/assets/Papers/ChrisRackauckas-IntuitiveSDEs.pdf">An Intuitive Introduction For Understanding and Solving Stochastic Differential Equations - Chris Rackauckas (2017)</a></li><li><a href="https://math.nyu.edu/~bourgade/SA2010/StochasticAnalysis.pdf">Stochastic analysis - Paul Bourgade (2010)</a></li><li><a href="https://www.cmor-faculty.rice.edu/~cox/stoch/SDE.course.pdf">AN INTRODUCTION TO STOCHASTIC DIFFERENTIAL EQUATIONS VERSION 1.2 - Lawrence C. Evans (2013)</a></li><li>Stochastic differential equations An introduction with applications - Bernt K. Øksendal (2003)</li><li><a href="https://en.wikipedia.org/wiki/Stochastic_calculus">Wikipedia: Stochastic calculus</a></li><li><a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">Wikipedia: Stochastic differential equation</a></li></ul><h3 id="a1-notation"><span>A.1. Notation</span><a href="#a1-notation"><i></i></a></h3><p>Here is a list of notation used in this document:</p><ul><li>\(\binom{n}{k}=\frac{n!}{k!(n-k)!}\) is the binomial coefficient</li><li>\(X: \Omega \to \mathbb{R}\) is a random variable from a sample space \(\Omega\) to a real number</li><li>\(P(A)\) is the probability of event \(A\)</li><li>\(E[X]=\int_{\omega \in \Omega} X(\omega) dP(\omega)\) is the expected value of \(X\)</li><li>\(N(\mu, \sigma^2)\) is a normal distribution with mean \(\mu\) and variance \(\sigma^2\)</li><li>\(W(t)\) is the position of a Brownian motion at time \(t\)</li><li>\(\Delta W(t_1,t_2)\) is the displacement of a Brownian motion from time \(t_1\) to time \(t_2\)</li><li>\(dt\) is an infinitesimal time increment</li><li>\(dW := \Delta W(t,t+dt)\) is an infinitesimal increment of Brownian motion over time</li><li>\((dW)^2 \sim dt\) denotes that \((dW^2) = dt + o(dt)\) where \(\lim_{t \to 0} \frac{o(dt)}{dt} = 0\), such that \((dW)^2\) is asymptotically equal to \(dt\) in the mean-square limit:</li></ul><p>\(\lim_{dt \to 0} \frac{E[(dW)^2-dt]^2}{dt}=0\)</p><ul><li>\(f_t:=\frac{\partial f}{\partial t}\) is the partial derivative of \(f\) with respect to \(t\)</li><li>\(f_xx:=\frac{\partial^2 f}{\partial x^2}\) is the second order partial derivative of \(f\) with respect to \(x\)</li></ul><h3 id="b1-python-code-for-binomial-plots"><span>B.1. Python code for binomial plots</span><a href="#b1-python-code-for-binomial-plots"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>from</span> <span>scipy.stats</span> <span>import</span> <span>binom</span>

<span>n_values</span> <span>=</span> <span>[</span><span>5</span><span>,</span> <span>10</span><span>,</span> <span>25</span><span>,</span> <span>50</span><span>,</span> <span>100</span><span>]</span>
<span>p</span> <span>=</span> <span>0.5</span>

<span># Individual plots
</span><span>for</span> <span>n</span> <span>in</span> <span>n_values</span><span>:</span>
    <span>k</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>n</span> <span>+</span> <span>1</span><span>)</span>
    <span>positions</span> <span>=</span> <span>2</span> <span>*</span> <span>k</span> <span>-</span> <span>n</span>
    <span>probs</span> <span>=</span> <span>binom</span><span>.</span><span>pmf</span><span>(</span><span>k</span><span>,</span> <span>n</span><span>,</span> <span>p</span><span>)</span>
    
    <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>6</span><span>,</span> <span>4</span><span>))</span>
    <span>plt</span><span>.</span><span>bar</span><span>(</span><span>positions</span><span>,</span> <span>probs</span><span>,</span> <span>width</span><span>=</span><span>1.0</span><span>,</span> <span>color</span><span>=</span><span>'</span><span>skyblue</span><span>'</span><span>,</span> <span>edgecolor</span><span>=</span><span>'</span><span>black</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>title</span><span>(</span><span>f</span><span>'</span><span>n = </span><span>{</span><span>n</span><span>}</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>'</span><span>Position (# wins - # losses)</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>'</span><span>Probability</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>ylim</span><span>(</span><span>0</span><span>,</span> <span>max</span><span>(</span><span>probs</span><span>)</span> <span>*</span> <span>1.2</span><span>)</span>
    <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>f</span><span>'</span><span>random_walk_n_</span><span>{</span><span>n</span><span>}</span><span>.png</span><span>'</span><span>,</span> <span>dpi</span><span>=</span><span>300</span><span>,</span> <span>bbox_inches</span><span>=</span><span>'</span><span>tight</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>close</span><span>()</span>

<span># Combined plot
</span><span>fig</span><span>,</span> <span>axes</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>(</span><span>5</span><span>,</span> <span>1</span><span>,</span> <span>figsize</span><span>=</span><span>(</span><span>8</span><span>,</span> <span>12</span><span>),</span> <span>sharex</span><span>=</span><span>True</span><span>)</span>
<span>for</span> <span>i</span><span>,</span> <span>n</span> <span>in</span> <span>enumerate</span><span>(</span><span>n_values</span><span>):</span>
    <span>k</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>n</span> <span>+</span> <span>1</span><span>)</span>
    <span>positions</span> <span>=</span> <span>2</span> <span>*</span> <span>k</span> <span>-</span> <span>n</span>
    <span>probs</span> <span>=</span> <span>binom</span><span>.</span><span>pmf</span><span>(</span><span>k</span><span>,</span> <span>n</span><span>,</span> <span>p</span><span>)</span>
    <span>axes</span><span>[</span><span>i</span><span>].</span><span>bar</span><span>(</span><span>positions</span><span>,</span> <span>probs</span><span>,</span> <span>width</span><span>=</span><span>1.0</span><span>,</span> <span>color</span><span>=</span><span>'</span><span>skyblue</span><span>'</span><span>,</span> <span>edgecolor</span><span>=</span><span>'</span><span>black</span><span>'</span><span>)</span>
    <span>axes</span><span>[</span><span>i</span><span>].</span><span>set_title</span><span>(</span><span>f</span><span>'</span><span>n = </span><span>{</span><span>n</span><span>}</span><span>'</span><span>)</span>
    <span>axes</span><span>[</span><span>i</span><span>].</span><span>set_ylabel</span><span>(</span><span>'</span><span>Probability</span><span>'</span><span>)</span>
    <span>axes</span><span>[</span><span>i</span><span>].</span><span>set_ylim</span><span>(</span><span>0</span><span>,</span> <span>max</span><span>(</span><span>probs</span><span>)</span> <span>*</span> <span>1.2</span><span>)</span>
<span>axes</span><span>[</span><span>-</span><span>1</span><span>].</span><span>set_xlabel</span><span>(</span><span>'</span><span>Position (# wins - # losses)</span><span>'</span><span>)</span>
<span>plt</span><span>.</span><span>tight_layout</span><span>()</span>
<span>plt</span><span>.</span><span>savefig</span><span>(</span><span>'</span><span>random_walk_combined.png</span><span>'</span><span>,</span> <span>dpi</span><span>=</span><span>300</span><span>,</span> <span>bbox_inches</span><span>=</span><span>'</span><span>tight</span><span>'</span><span>)</span>
<span>plt</span><span>.</span><span>close</span><span>()</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="b2-python-code-for-brownian-motion-plot"><span><strong>B2. Python Code for Brownian Motion Plot</strong></span><a href="#b2-python-code-for-brownian-motion-plot"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>

<span># Simulate Brownian motion
</span><span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>42</span><span>)</span>
<span>t</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>1000</span><span>)</span>  <span># Time from 0 to 1
</span><span>dt</span> <span>=</span> <span>t</span><span>[</span><span>1</span><span>]</span> <span>-</span> <span>t</span><span>[</span><span>0</span><span>]</span>
<span>dW</span> <span>=</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>dt</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>size</span><span>=</span><span>len</span><span>(</span><span>t</span><span>)</span><span>-</span><span>1</span><span>)</span>  <span># Increments
</span><span>W</span> <span>=</span> <span>np</span><span>.</span><span>concatenate</span><span>([[</span><span>0</span><span>],</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>dW</span><span>)])</span>  <span># Cumulative sum starts at 0
</span>
<span># Plot
</span><span>plt</span><span>.</span><span>plot</span><span>(</span><span>t</span><span>,</span> <span>W</span><span>)</span>
<span>plt</span><span>.</span><span>title</span><span>(</span><span>"</span><span>Sample Brownian Motion Path</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>"</span><span>Time t</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>"</span><span>W(t)</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="b3-python-code-for-basic-sde-simulation"><span><strong>B3. Python Code for Basic SDE Simulation</strong></span><a href="#b3-python-code-for-basic-sde-simulation"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>

<span># Simulate simple SDE: dX = mu dt + sigma dW
</span><span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>42</span><span>)</span>
<span>T</span> <span>=</span> <span>1.0</span>
<span>N</span> <span>=</span> <span>1000</span>
<span>dt</span> <span>=</span> <span>T</span> <span>/</span> <span>N</span>
<span>t</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0</span><span>,</span> <span>T</span><span>,</span> <span>N</span><span>+</span><span>1</span><span>)</span>
<span>mu</span><span>,</span> <span>sigma</span> <span>=</span> <span>1.0</span><span>,</span> <span>0.5</span>
<span>X</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>N</span><span>+</span><span>1</span><span>)</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>N</span><span>):</span>
    <span>dW</span> <span>=</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>dt</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>)</span>
    <span>X</span><span>[</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>X</span><span>[</span><span>i</span><span>]</span> <span>+</span> <span>mu</span> <span>*</span> <span>dt</span> <span>+</span> <span>sigma</span> <span>*</span> <span>dW</span>

<span>plt</span><span>.</span><span>plot</span><span>(</span><span>t</span><span>,</span> <span>X</span><span>,</span> <span>label</span><span>=</span><span>f</span><span>"</span><span>μ=</span><span>{</span><span>mu</span><span>}</span><span>, σ=</span><span>{</span><span>sigma</span><span>}</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>title</span><span>(</span><span>"</span><span>Sample Path of dX = μ dt + σ dW</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>"</span><span>Time t</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>"</span><span>X(t)</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>legend</span><span>()</span>
<span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="b4-python-code-for-geometric-brownian-motion-simulation"><span><strong>B4. Python Code for Geometric Brownian Motion Simulation</strong></span><a href="#b4-python-code-for-geometric-brownian-motion-simulation"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>

<span># Simulate simple SDE: dX = mu dt + sigma dW
</span><span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>42</span><span>)</span>

<span># Simulate Geometric Brownian Motion (exact solution)
</span><span>T_gbm</span> <span>=</span> <span>10.0</span>  <span># Longer time to show exponential nature
</span><span>N_gbm</span> <span>=</span> <span>1000</span>
<span>t_gbm</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0</span><span>,</span> <span>T_gbm</span><span>,</span> <span>N_gbm</span><span>+</span><span>1</span><span>)</span>
<span>S0</span> <span>=</span> <span>100.0</span>  <span># Initial stock price
</span><span>mu</span><span>,</span> <span>sigma</span> <span>=</span> <span>0.15</span><span>,</span> <span>0.2</span>  <span># Slightly larger for visibility
</span><span>S</span> <span>=</span> <span>S0</span> <span>*</span> <span>np</span><span>.</span><span>exp</span><span>((</span><span>mu</span> <span>-</span> <span>0.5</span> <span>*</span> <span>sigma</span><span>**</span><span>2</span><span>)</span> <span>*</span> <span>t_gbm</span> <span>+</span> <span>sigma</span> <span>*</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>t_gbm</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>N_gbm</span><span>+</span><span>1</span><span>))</span>

<span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>8</span><span>,</span> <span>4</span><span>))</span>
<span>plt</span><span>.</span><span>plot</span><span>(</span><span>t_gbm</span><span>,</span> <span>S</span><span>,</span> <span>label</span><span>=</span><span>f</span><span>"</span><span>μ=</span><span>{</span><span>mu</span><span>}</span><span>, σ=</span><span>{</span><span>sigma</span><span>}</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>title</span><span>(</span><span>"</span><span>Sample Path: Geometric Brownian Motion</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>"</span><span>Time t</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>"</span><span>S(t)</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>legend</span><span>()</span>
<span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
<span>plt</span><span>.</span><span>savefig</span><span>(</span><span>"</span><span>gbm_path.png</span><span>"</span><span>,</span> <span>dpi</span><span>=</span><span>300</span><span>,</span> <span>bbox_inches</span><span>=</span><span>"</span><span>tight</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="b5-latex-code-for-tikz-diagram-of-paths-in-pascals-triangle"><span>B5. LaTeX Code for Tikz Diagram of Paths in Pascal’s Triangle</span><a href="#b5-latex-code-for-tikz-diagram-of-paths-in-pascals-triangle"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
</pre></td><td><pre><span>\documentclass</span><span>{</span>standalone<span>}</span>
<span>\usepackage</span><span>{</span>tikz<span>}</span>
<span>\begin{document}</span>

<span>\begin{tikzpicture}</span>[scale=0.8]
    <span>% Add a white background rectangle</span>
  <span>\fill</span><span>[white]</span> (-12, 1) rectangle (10, -5);
  
  <span>% Row labels (only once, to the left of the first diagram)</span>
  <span>\node</span><span>[align=right]</span> at (-11, 0) <span>{</span>Row 0<span>}</span>;
  <span>\node</span><span>[align=right]</span> at (-11, -1) <span>{</span>Row 1<span>}</span>;
  <span>\node</span><span>[align=right]</span> at (-11, -2) <span>{</span>Row 2<span>}</span>;
  <span>\node</span><span>[align=right]</span> at (-11, -3) <span>{</span>Row 3<span>}</span>;

  <span>% Diagram 1: Path RRL</span>
  <span>\node</span> at (-6, 0) <span>{</span>1<span>}</span>; <span>% Row 0</span>
  <span>\node</span> at (-7, -1) <span>{</span>1<span>}</span>; <span>% Row 1</span>
  <span>\node</span> at (-5, -1) <span>{</span>1<span>}</span>;
  <span>\node</span> at (-8, -2) <span>{</span>1<span>}</span>; <span>% Row 2</span>
  <span>\node</span> at (-6, -2) <span>{</span>2<span>}</span>;
  <span>\node</span> at (-4, -2) <span>{</span>1<span>}</span>;
  <span>\node</span> at (-9, -3) <span>{</span>1<span>}</span>; <span>% Row 3</span>
  <span>\node</span> at (-7, -3) <span>{</span>3<span>}</span>;
  <span>\node</span> at (-5, -3) <span>{</span>3<span>}</span>;
  <span>\node</span> at (-3, -3) <span>{</span>1<span>}</span>;
  <span>\draw</span><span>[-&gt;, red, thick]</span> (-6, 0) -- (-5, -1) -- (-4, -2) -- (-5, -3); <span>% RRL</span>
  <span>\node</span> at (-6, -4) <span>{</span>Right-Right-Left<span>}</span>;

  <span>% Diagram 2: Path RLR</span>
  <span>\node</span> at (0, 0) <span>{</span>1<span>}</span>; <span>% Row 0</span>
  <span>\node</span> at (-1, -1) <span>{</span>1<span>}</span>; <span>% Row 1</span>
  <span>\node</span> at (1, -1) <span>{</span>1<span>}</span>;
  <span>\node</span> at (-2, -2) <span>{</span>1<span>}</span>; <span>% Row 2</span>
  <span>\node</span> at (0, -2) <span>{</span>2<span>}</span>;
  <span>\node</span> at (2, -2) <span>{</span>1<span>}</span>;
  <span>\node</span> at (-3, -3) <span>{</span>1<span>}</span>; <span>% Row 3</span>
  <span>\node</span> at (-1, -3) <span>{</span>3<span>}</span>;
  <span>\node</span> at (1, -3) <span>{</span>3<span>}</span>;
  <span>\node</span> at (3, -3) <span>{</span>1<span>}</span>;
  <span>\draw</span><span>[-&gt;, blue, thick]</span> (0, 0) -- (1, -1) -- (0, -2) -- (1, -3); <span>% RLR</span>
  <span>\node</span> at (0, -4) <span>{</span>Right-Left-Right<span>}</span>;

  <span>% Diagram 3: Path LRR</span>
  <span>\node</span> at (6, 0) <span>{</span>1<span>}</span>; <span>% Row 0</span>
  <span>\node</span> at (5, -1) <span>{</span>1<span>}</span>; <span>% Row 1</span>
  <span>\node</span> at (7, -1) <span>{</span>1<span>}</span>;
  <span>\node</span> at (4, -2) <span>{</span>1<span>}</span>; <span>% Row 2</span>
  <span>\node</span> at (6, -2) <span>{</span>2<span>}</span>;
  <span>\node</span> at (8, -2) <span>{</span>1<span>}</span>;
  <span>\node</span> at (3, -3) <span>{</span>1<span>}</span>; <span>% Row 3</span>
  <span>\node</span> at (5, -3) <span>{</span>3<span>}</span>;
  <span>\node</span> at (7, -3) <span>{</span>3<span>}</span>;
  <span>\node</span> at (9, -3) <span>{</span>1<span>}</span>;
  <span>\draw</span><span>[-&gt;, green, thick]</span> (6, 0) -- (5, -1) -- (6, -2) -- (7, -3); <span>% LRR</span>
  <span>\node</span> at (6, -4) <span>{</span>Left-Right-Right<span>}</span>;
<span>\end{tikzpicture}</span>

<span>\end{document}</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="3d-visualizations"><span>3D Visualizations</span><a href="#3d-visualizations"><i></i></a></h3><h4 id="c1-3d-plot-of-discrete-random-walks"><span>C1. 3D Plot of Discrete Random Walks</span><a href="#c1-3d-plot-of-discrete-random-walks"><i></i></a></h4><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>from</span> <span>mpl_toolkits.mplot3d</span> <span>import</span> <span>Axes3D</span>  <span># for 3D plotting
</span><span>import</span> <span>imageio.v3</span> <span>as</span> <span>imageio</span>  <span># using modern imageio v3 API
</span><span>import</span> <span>os</span>
<span>from</span> <span>scipy.special</span> <span>import</span> <span>comb</span>
<span>from</span> <span>scipy.stats</span> <span>import</span> <span>norm</span>

<span># Create a directory for frames
</span><span>os</span><span>.</span><span>makedirs</span><span>(</span><span>'</span><span>gif_frames</span><span>'</span><span>,</span> <span>exist_ok</span><span>=</span><span>True</span><span>)</span>

<span>##############################################
# Part 1: Discrete Binomial Random Walk (N = 15)
##############################################
</span>
<span>N</span> <span>=</span> <span>15</span>  <span># total number of steps (kept small for clear discreteness)
</span><span>num_sample_paths</span> <span>=</span> <span>5</span>  <span># number of sample paths to overlay
</span>
<span># Simulate a few discrete random walk sample paths
</span><span>sample_paths</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>num_sample_paths</span><span>):</span>
    <span>steps</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>choice</span><span>([</span><span>-</span><span>1</span><span>,</span> <span>1</span><span>],</span> <span>size</span><span>=</span><span>N</span><span>)</span>
    <span>path</span> <span>=</span> <span>np</span><span>.</span><span>concatenate</span><span>(([</span><span>0</span><span>],</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>steps</span><span>)))</span>
    <span>sample_paths</span><span>.</span><span>append</span><span>(</span><span>path</span><span>)</span>
<span>sample_paths</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>sample_paths</span><span>)</span>  <span># shape: (num_sample_paths, N+1)
</span>
<span>frames</span> <span>=</span> <span>[]</span>
<span>for</span> <span>t_step</span> <span>in</span> <span>range</span><span>(</span><span>N</span> <span>+</span> <span>1</span><span>):</span>
    <span>fig</span> <span>=</span> <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>7</span><span>))</span>
    <span>ax</span> <span>=</span> <span>fig</span><span>.</span><span>add_subplot</span><span>(</span><span>111</span><span>,</span> <span>projection</span><span>=</span><span>'</span><span>3d</span><span>'</span><span>)</span>
    
    <span># For each discrete time slice up to the current time, plot the PMF
</span>    <span>for</span> <span>t</span> <span>in</span> <span>range</span><span>(</span><span>t_step</span> <span>+</span> <span>1</span><span>):</span>
        <span># For a random walk starting at 0, possible positions are -t, -t+2, ..., t
</span>        <span>x_values</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>-</span><span>t</span><span>,</span> <span>t</span> <span>+</span> <span>1</span><span>,</span> <span>2</span><span>)</span>
        <span>if</span> <span>t</span> <span>==</span> <span>0</span><span>:</span>
            <span>p_values</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1.0</span><span>])</span>
        <span>else</span><span>:</span>
            <span># k = (x + t)/2 gives the number of +1 steps
</span>            <span>k</span> <span>=</span> <span>(</span><span>x_values</span> <span>+</span> <span>t</span><span>)</span> <span>//</span> <span>2</span>  
            <span>p_values</span> <span>=</span> <span>comb</span><span>(</span><span>t</span><span>,</span> <span>k</span><span>)</span> <span>*</span> <span>(</span><span>0.5</span> <span>**</span> <span>t</span><span>)</span>
        <span># Plot the discrete PMF as blue markers (and connect them with a line)
</span>        <span>ax</span><span>.</span><span>scatter</span><span>(</span><span>x_values</span><span>,</span> <span>[</span><span>t</span><span>]</span><span>*</span><span>len</span><span>(</span><span>x_values</span><span>),</span> <span>p_values</span><span>,</span> <span>color</span><span>=</span><span>'</span><span>blue</span><span>'</span><span>,</span> <span>s</span><span>=</span><span>50</span><span>)</span>
        <span>ax</span><span>.</span><span>plot</span><span>(</span><span>x_values</span><span>,</span> <span>[</span><span>t</span><span>]</span><span>*</span><span>len</span><span>(</span><span>x_values</span><span>),</span> <span>p_values</span><span>,</span> <span>color</span><span>=</span><span>'</span><span>blue</span><span>'</span><span>,</span> <span>alpha</span><span>=</span><span>0.5</span><span>)</span>
    
    <span># Overlay the sample random walk paths (projected at z=0)
</span>    <span>for</span> <span>sp</span> <span>in</span> <span>sample_paths</span><span>:</span>
        <span>ax</span><span>.</span><span>plot</span><span>(</span><span>sp</span><span>[:</span><span>t_step</span> <span>+</span> <span>1</span><span>],</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>t_step</span> <span>+</span> <span>1</span><span>),</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>t_step</span> <span>+</span> <span>1</span><span>),</span>
                <span>'</span><span>r-o</span><span>'</span><span>,</span> <span>markersize</span><span>=</span><span>5</span><span>,</span> <span>label</span><span>=</span><span>'</span><span>Sample Path</span><span>'</span> <span>if</span> <span>t_step</span> <span>==</span> <span>0</span> <span>else</span> <span>""</span><span>)</span>
    
    <span>ax</span><span>.</span><span>set_xlabel</span><span>(</span><span>'</span><span>Position (x)</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_ylabel</span><span>(</span><span>'</span><span>Time (steps)</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_zlabel</span><span>(</span><span>'</span><span>Probability</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>'</span><span>Discrete Binomial Random Walk: Step </span><span>{</span><span>t_step</span><span>}</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_zlim</span><span>(</span><span>0</span><span>,</span> <span>1.0</span><span>)</span>
    <span>ax</span><span>.</span><span>view_init</span><span>(</span><span>elev</span><span>=</span><span>30</span><span>,</span> <span>azim</span><span>=-</span><span>60</span><span>)</span>
    
    <span>frame_path</span> <span>=</span> <span>f</span><span>'</span><span>gif_frames/discrete_binomial_</span><span>{</span><span>t_step</span><span>:</span><span>02</span><span>d</span><span>}</span><span>.png</span><span>'</span>
    <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>frame_path</span><span>)</span>
    <span>plt</span><span>.</span><span>close</span><span>()</span>
    <span>frames</span><span>.</span><span>append</span><span>(</span><span>imageio</span><span>.</span><span>imread</span><span>(</span><span>frame_path</span><span>))</span>

<span># Compute per-frame durations: 0.25 sec for all frames except the last one (2 sec)
</span><span>durations</span> <span>=</span> <span>[</span><span>0.25</span><span>]</span> <span>*</span> <span>(</span><span>len</span><span>(</span><span>frames</span><span>)</span> <span>-</span> <span>1</span><span>)</span> <span>+</span> <span>[</span><span>2.0</span><span>]</span>

<span># Write the GIF with variable durations and infinite looping
</span><span>imageio</span><span>.</span><span>imwrite</span><span>(</span><span>'</span><span>discrete_binomial.gif</span><span>'</span><span>,</span> <span>frames</span><span>,</span> <span>duration</span><span>=</span><span>durations</span><span>,</span> <span>loop</span><span>=</span><span>0</span><span>)</span>

<span>##############################################
# Part 2: Discrete Random Walk Normalizing (N = 50)
##############################################
</span>
<span>N</span> <span>=</span> <span>50</span>  <span># total number of steps (increased to show gradual convergence)
</span><span>num_sample_paths</span> <span>=</span> <span>5</span>  <span># number of sample paths to overlay
</span>
<span># Simulate a few discrete random walk sample paths
</span><span>sample_paths</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>num_sample_paths</span><span>):</span>
    <span>steps</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>choice</span><span>([</span><span>-</span><span>1</span><span>,</span> <span>1</span><span>],</span> <span>size</span><span>=</span><span>N</span><span>)</span>
    <span>path</span> <span>=</span> <span>np</span><span>.</span><span>concatenate</span><span>(([</span><span>0</span><span>],</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>steps</span><span>)))</span>
    <span>sample_paths</span><span>.</span><span>append</span><span>(</span><span>path</span><span>)</span>
<span>sample_paths</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>sample_paths</span><span>)</span>  <span># shape: (num_sample_paths, N+1)
</span>
<span>frames</span> <span>=</span> <span>[]</span>
<span>for</span> <span>t_step</span> <span>in</span> <span>range</span><span>(</span><span>N</span> <span>+</span> <span>1</span><span>):</span>
    <span>fig</span> <span>=</span> <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>7</span><span>))</span>
    <span>ax</span> <span>=</span> <span>fig</span><span>.</span><span>add_subplot</span><span>(</span><span>111</span><span>,</span> <span>projection</span><span>=</span><span>'</span><span>3d</span><span>'</span><span>)</span>
    
    <span># Plot the PMFs for all time slices from 0 to the current step
</span>    <span>for</span> <span>t</span> <span>in</span> <span>range</span><span>(</span><span>t_step</span> <span>+</span> <span>1</span><span>):</span>
        <span># For a random walk starting at 0, possible positions are -t, -t+2, ..., t
</span>        <span>x_values</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>-</span><span>t</span><span>,</span> <span>t</span> <span>+</span> <span>1</span><span>,</span> <span>2</span><span>)</span>
        <span>if</span> <span>t</span> <span>==</span> <span>0</span><span>:</span>
            <span>p_values</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1.0</span><span>])</span>
        <span>else</span><span>:</span>
            <span># For each x, number of +1 steps is (x+t)/2
</span>            <span>k</span> <span>=</span> <span>(</span><span>x_values</span> <span>+</span> <span>t</span><span>)</span> <span>//</span> <span>2</span>
            <span>p_values</span> <span>=</span> <span>comb</span><span>(</span><span>t</span><span>,</span> <span>k</span><span>)</span> <span>*</span> <span>(</span><span>0.5</span> <span>**</span> <span>t</span><span>)</span>
        
        <span># Plot the discrete PMF as blue markers and lines
</span>        <span>ax</span><span>.</span><span>scatter</span><span>(</span><span>x_values</span><span>,</span> <span>[</span><span>t</span><span>]</span><span>*</span><span>len</span><span>(</span><span>x_values</span><span>),</span> <span>p_values</span><span>,</span> <span>color</span><span>=</span><span>'</span><span>blue</span><span>'</span><span>,</span> <span>s</span><span>=</span><span>50</span><span>)</span>
        <span>ax</span><span>.</span><span>plot</span><span>(</span><span>x_values</span><span>,</span> <span>[</span><span>t</span><span>]</span><span>*</span><span>len</span><span>(</span><span>x_values</span><span>),</span> <span>p_values</span><span>,</span> <span>color</span><span>=</span><span>'</span><span>blue</span><span>'</span><span>,</span> <span>alpha</span><span>=</span><span>0.5</span><span>)</span>
        
        <span># For the current time slice, overlay the normal approximation in red
</span>        <span>if</span> <span>t</span> <span>==</span> <span>t_step</span> <span>and</span> <span>t</span> <span>&gt;</span> <span>0</span><span>:</span>
            <span>x_cont</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>-</span><span>t</span><span>,</span> <span>t</span><span>,</span> <span>200</span><span>)</span>
            <span>normal_pdf</span> <span>=</span> <span>norm</span><span>.</span><span>pdf</span><span>(</span><span>x_cont</span><span>,</span> <span>0</span><span>,</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>t</span><span>))</span>
            <span>ax</span><span>.</span><span>plot</span><span>(</span><span>x_cont</span><span>,</span> <span>[</span><span>t</span><span>]</span><span>*</span><span>len</span><span>(</span><span>x_cont</span><span>),</span> <span>normal_pdf</span><span>,</span> <span>'</span><span>r-</span><span>'</span><span>,</span> <span>linewidth</span><span>=</span><span>2</span><span>,</span> <span>label</span><span>=</span><span>'</span><span>Normal Approx.</span><span>'</span><span>)</span>
    
    <span># Overlay the sample random walk paths (projected along the z=0 plane)
</span>    <span>for</span> <span>sp</span> <span>in</span> <span>sample_paths</span><span>:</span>
        <span>ax</span><span>.</span><span>plot</span><span>(</span><span>sp</span><span>[:</span><span>t_step</span> <span>+</span> <span>1</span><span>],</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>t_step</span> <span>+</span> <span>1</span><span>),</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>t_step</span> <span>+</span> <span>1</span><span>),</span>
                <span>'</span><span>g-o</span><span>'</span><span>,</span> <span>markersize</span><span>=</span><span>5</span><span>,</span> <span>label</span><span>=</span><span>'</span><span>Sample Path</span><span>'</span> <span>if</span> <span>t_step</span> <span>==</span> <span>0</span> <span>else</span> <span>""</span><span>)</span>
    
    <span>ax</span><span>.</span><span>set_xlabel</span><span>(</span><span>'</span><span>Position (x)</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_ylabel</span><span>(</span><span>'</span><span>Time (steps)</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_zlabel</span><span>(</span><span>'</span><span>Probability</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>'</span><span>Discrete Binomial Random Walk at Step </span><span>{</span><span>t_step</span><span>}</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_zlim</span><span>(</span><span>0</span><span>,</span> <span>1.0</span><span>)</span>
    <span>ax</span><span>.</span><span>view_init</span><span>(</span><span>elev</span><span>=</span><span>30</span><span>,</span> <span>azim</span><span>=-</span><span>60</span><span>)</span>
    
    <span>frame_path</span> <span>=</span> <span>f</span><span>'</span><span>gif_frames/discrete_binomial_</span><span>{</span><span>t_step</span><span>:</span><span>02</span><span>d</span><span>}</span><span>.png</span><span>'</span>
    <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>frame_path</span><span>)</span>
    <span>plt</span><span>.</span><span>close</span><span>()</span>
    <span>frames</span><span>.</span><span>append</span><span>(</span><span>imageio</span><span>.</span><span>imread</span><span>(</span><span>frame_path</span><span>))</span>

<span># Compute per-frame durations: 0.25 sec for all frames except the last one (2 sec)
</span><span>durations</span> <span>=</span> <span>[</span><span>0.25</span><span>]</span> <span>*</span> <span>(</span><span>len</span><span>(</span><span>frames</span><span>)</span> <span>-</span> <span>1</span><span>)</span> <span>+</span> <span>[</span><span>2.0</span><span>]</span>

<span># Write the GIF with variable durations and infinite looping
</span><span>imageio</span><span>.</span><span>imwrite</span><span>(</span><span>'</span><span>discrete_binomial_normalizing.gif</span><span>'</span><span>,</span> <span>frames</span><span>,</span> <span>duration</span><span>=</span><span>durations</span><span>,</span> <span>loop</span><span>=</span><span>0</span><span>)</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="c2-3d-animation-of-brownian-motion"><span>C2. 3D Animation of Brownian Motion</span><a href="#c2-3d-animation-of-brownian-motion"><i></i></a></h3><p>Normal distribution sweeping and evolving across time according Brownian motion</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>from</span> <span>mpl_toolkits.mplot3d</span> <span>import</span> <span>Axes3D</span>  <span># for 3D plotting
</span><span>from</span> <span>scipy.stats</span> <span>import</span> <span>norm</span>
<span>import</span> <span>imageio.v3</span> <span>as</span> <span>imageio</span>  <span># using modern API
</span><span>import</span> <span>os</span>

<span>os</span><span>.</span><span>makedirs</span><span>(</span><span>'</span><span>gif_frames</span><span>'</span><span>,</span> <span>exist_ok</span><span>=</span><span>True</span><span>)</span>

<span># Parameters for continuous Brownian motion
</span><span>num_frames</span> <span>=</span> <span>100</span>  <span># more frames for smoother animation
</span><span>t_values</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0.1</span><span>,</span> <span>5</span><span>,</span> <span>num_frames</span><span>)</span>
<span>x</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>-</span><span>5</span><span>,</span> <span>5</span><span>,</span> <span>200</span><span>)</span>  <span># increased resolution
</span>
<span>num_sample_paths</span> <span>=</span> <span>5</span>
<span>sample_paths</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>num_sample_paths</span><span>,</span> <span>len</span><span>(</span><span>t_values</span><span>)))</span>
<span>dt_cont</span> <span>=</span> <span>t_values</span><span>[</span><span>1</span><span>]</span> <span>-</span> <span>t_values</span><span>[</span><span>0</span><span>]</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>num_sample_paths</span><span>):</span>
    <span>increments</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>0</span><span>,</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>dt_cont</span><span>),</span> <span>size</span><span>=</span><span>len</span><span>(</span><span>t_values</span><span>)</span><span>-</span><span>1</span><span>)</span>
    <span>sample_paths</span><span>[</span><span>i</span><span>,</span> <span>1</span><span>:]</span> <span>=</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>increments</span><span>)</span>

<span>frames</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span><span>,</span> <span>t</span> <span>in</span> <span>enumerate</span><span>(</span><span>t_values</span><span>):</span>
    <span>fig</span> <span>=</span> <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>7</span><span>))</span>
    <span>ax</span> <span>=</span> <span>fig</span><span>.</span><span>add_subplot</span><span>(</span><span>111</span><span>,</span> <span>projection</span><span>=</span><span>'</span><span>3d</span><span>'</span><span>)</span>
    
    <span>mask</span> <span>=</span> <span>t_values</span> <span>&lt;=</span> <span>t</span>
    <span>T_sub</span><span>,</span> <span>X_sub</span> <span>=</span> <span>np</span><span>.</span><span>meshgrid</span><span>(</span><span>t_values</span><span>[</span><span>mask</span><span>],</span> <span>x</span><span>)</span>
    <span>P_sub</span> <span>=</span> <span>(</span><span>1</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>2</span> <span>*</span> <span>np</span><span>.</span><span>pi</span> <span>*</span> <span>T_sub</span><span>))</span> <span>*</span> <span>np</span><span>.</span><span>exp</span><span>(</span><span>-</span><span>X_sub</span><span>**</span><span>2</span> <span>/</span> <span>(</span><span>2</span> <span>*</span> <span>T_sub</span><span>))</span>
    <span>ax</span><span>.</span><span>plot_surface</span><span>(</span><span>X_sub</span><span>,</span> <span>T_sub</span><span>,</span> <span>P_sub</span><span>,</span> <span>cmap</span><span>=</span><span>'</span><span>viridis</span><span>'</span><span>,</span> <span>alpha</span><span>=</span><span>0.7</span><span>,</span> <span>edgecolor</span><span>=</span><span>'</span><span>none</span><span>'</span><span>)</span>
    
    <span>for</span> <span>sp</span> <span>in</span> <span>sample_paths</span><span>:</span>
        <span>ax</span><span>.</span><span>plot</span><span>(</span><span>sp</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>t_values</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>i</span><span>+</span><span>1</span><span>),</span> <span>'</span><span>r-</span><span>'</span><span>,</span> <span>marker</span><span>=</span><span>'</span><span>o</span><span>'</span><span>,</span> <span>markersize</span><span>=</span><span>3</span><span>)</span>
    
    <span>ax</span><span>.</span><span>set_xlabel</span><span>(</span><span>'</span><span>Position (x)</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_ylabel</span><span>(</span><span>'</span><span>Time (t)</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_zlabel</span><span>(</span><span>'</span><span>Density</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>'</span><span>Continuous Brownian Motion at t = </span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>view_init</span><span>(</span><span>elev</span><span>=</span><span>30</span><span>,</span> <span>azim</span><span>=-</span><span>60</span><span>)</span>
    
    <span>frame_path</span> <span>=</span> <span>f</span><span>'</span><span>gif_frames/continuous_3d_smooth_t_</span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>.png</span><span>'</span>
    <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>frame_path</span><span>)</span>
    <span>plt</span><span>.</span><span>close</span><span>()</span>
    <span>frames</span><span>.</span><span>append</span><span>(</span><span>imageio</span><span>.</span><span>imread</span><span>(</span><span>frame_path</span><span>))</span>

<span>imageio</span><span>.</span><span>imwrite</span><span>(</span><span>'</span><span>continuous_brownian_3d_smooth.gif</span><span>'</span><span>,</span> <span>frames</span><span>,</span> <span>duration</span><span>=</span><span>0.1</span><span>)</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="c3-3d-animation-of-geometric-brownian-motion"><span>C3. 3D Animation of Geometric Brownian Motion</span><a href="#c3-3d-animation-of-geometric-brownian-motion"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>from</span> <span>mpl_toolkits.mplot3d</span> <span>import</span> <span>Axes3D</span>  <span># for 3D plotting
</span><span>import</span> <span>imageio.v3</span> <span>as</span> <span>imageio</span>  <span># modern API
</span><span>import</span> <span>os</span>

<span>os</span><span>.</span><span>makedirs</span><span>(</span><span>'</span><span>gif_frames</span><span>'</span><span>,</span> <span>exist_ok</span><span>=</span><span>True</span><span>)</span>

<span># Parameters for geometric Brownian motion (GBM)
</span><span>S0</span> <span>=</span> <span>1.0</span>    <span># initial stock price
</span><span>mu</span> <span>=</span> <span>0.2</span>    <span># drift rate (increased for noticeable drift)
</span><span>sigma</span> <span>=</span> <span>0.2</span> <span># volatility
</span>
<span>num_frames</span> <span>=</span> <span>100</span>
<span>t_values</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0.1</span><span>,</span> <span>5</span><span>,</span> <span>num_frames</span><span>)</span>  <span># avoid t=0 to prevent singularity in density
</span><span>S_range</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0.01</span><span>,</span> <span>5</span><span>,</span> <span>200</span><span>)</span>         <span># price range
</span>
<span># Simulate GBM sample paths
</span><span>num_sample_paths</span> <span>=</span> <span>5</span>
<span>sample_paths</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>num_sample_paths</span><span>,</span> <span>len</span><span>(</span><span>t_values</span><span>)))</span>
<span>dt</span> <span>=</span> <span>t_values</span><span>[</span><span>1</span><span>]</span> <span>-</span> <span>t_values</span><span>[</span><span>0</span><span>]</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>num_sample_paths</span><span>):</span>
    <span>increments</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>0</span><span>,</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>dt</span><span>),</span> <span>size</span><span>=</span><span>len</span><span>(</span><span>t_values</span><span>)</span><span>-</span><span>1</span><span>)</span>
    <span>W</span> <span>=</span> <span>np</span><span>.</span><span>concatenate</span><span>(([</span><span>0</span><span>],</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>increments</span><span>)))</span>
    <span>sample_paths</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>S0</span> <span>*</span> <span>np</span><span>.</span><span>exp</span><span>((</span><span>mu</span> <span>-</span> <span>0.5</span> <span>*</span> <span>sigma</span><span>**</span><span>2</span><span>)</span> <span>*</span> <span>t_values</span> <span>+</span> <span>sigma</span> <span>*</span> <span>W</span><span>)</span>

<span>frames</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span><span>,</span> <span>t</span> <span>in</span> <span>enumerate</span><span>(</span><span>t_values</span><span>):</span>
    <span>fig</span> <span>=</span> <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>7</span><span>))</span>
    <span>ax</span> <span>=</span> <span>fig</span><span>.</span><span>add_subplot</span><span>(</span><span>111</span><span>,</span> <span>projection</span><span>=</span><span>'</span><span>3d</span><span>'</span><span>)</span>
    
    <span>mask</span> <span>=</span> <span>t_values</span> <span>&lt;=</span> <span>t</span>
    <span>T_sub</span><span>,</span> <span>S_sub</span> <span>=</span> <span>np</span><span>.</span><span>meshgrid</span><span>(</span><span>t_values</span><span>[</span><span>mask</span><span>],</span> <span>S_range</span><span>)</span>
    <span>P_sub</span> <span>=</span> <span>(</span><span>1</span> <span>/</span> <span>(</span><span>S_sub</span> <span>*</span> <span>sigma</span> <span>*</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>2</span> <span>*</span> <span>np</span><span>.</span><span>pi</span> <span>*</span> <span>T_sub</span><span>)))</span> <span>*</span> \
            <span>np</span><span>.</span><span>exp</span><span>(</span><span>-</span> <span>(</span><span>np</span><span>.</span><span>log</span><span>(</span><span>S_sub</span> <span>/</span> <span>S0</span><span>)</span> <span>-</span> <span>(</span><span>mu</span> <span>-</span> <span>0.5</span> <span>*</span> <span>sigma</span><span>**</span><span>2</span><span>)</span> <span>*</span> <span>T_sub</span><span>)</span><span>**</span><span>2</span> <span>/</span> <span>(</span><span>2</span> <span>*</span> <span>sigma</span><span>**</span><span>2</span> <span>*</span> <span>T_sub</span><span>))</span>
    <span>ax</span><span>.</span><span>plot_surface</span><span>(</span><span>S_sub</span><span>,</span> <span>T_sub</span><span>,</span> <span>P_sub</span><span>,</span> <span>cmap</span><span>=</span><span>'</span><span>viridis</span><span>'</span><span>,</span> <span>alpha</span><span>=</span><span>0.7</span><span>,</span> <span>edgecolor</span><span>=</span><span>'</span><span>none</span><span>'</span><span>)</span>
    
    <span>for</span> <span>sp</span> <span>in</span> <span>sample_paths</span><span>:</span>
        <span>ax</span><span>.</span><span>plot</span><span>(</span><span>sp</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>t_values</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>i</span><span>+</span><span>1</span><span>),</span> <span>'</span><span>r-</span><span>'</span><span>,</span> <span>marker</span><span>=</span><span>'</span><span>o</span><span>'</span><span>,</span> <span>markersize</span><span>=</span><span>3</span><span>)</span>
    
    <span>ax</span><span>.</span><span>set_xlabel</span><span>(</span><span>'</span><span>Stock Price S</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_ylabel</span><span>(</span><span>'</span><span>Time t</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_zlabel</span><span>(</span><span>'</span><span>Density</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>'</span><span>Geometric Brownian Motion at t = </span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>view_init</span><span>(</span><span>elev</span><span>=</span><span>30</span><span>,</span> <span>azim</span><span>=-</span><span>60</span><span>)</span>
    
    <span>frame_path</span> <span>=</span> <span>f</span><span>'</span><span>gif_frames/geometric_brownian_drifted_3d_t_</span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>.png</span><span>'</span>
    <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>frame_path</span><span>)</span>
    <span>plt</span><span>.</span><span>close</span><span>()</span>
    <span>frames</span><span>.</span><span>append</span><span>(</span><span>imageio</span><span>.</span><span>imread</span><span>(</span><span>frame_path</span><span>))</span>

<span>imageio</span><span>.</span><span>imwrite</span><span>(</span><span>'</span><span>geometric_brownian_drifted_3d.gif</span><span>'</span><span>,</span> <span>frames</span><span>,</span> <span>duration</span><span>=</span><span>0.1</span><span>)</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="c4-python-code-for-normal-distribution-approximation-by-random-walks"><span>C4. Python Code for Normal Distribution Approximation by Random Walks</span><a href="#c4-python-code-for-normal-distribution-approximation-by-random-walks"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>from</span> <span>scipy.stats</span> <span>import</span> <span>norm</span>
<span>import</span> <span>imageio.v3</span> <span>as</span> <span>imageio</span>  <span># modern ImageIO v3 API
</span><span>import</span> <span>os</span>
<span>from</span> <span>scipy.special</span> <span>import</span> <span>comb</span>

<span># Create a directory for frames
</span><span>os</span><span>.</span><span>makedirs</span><span>(</span><span>'</span><span>gif_frames</span><span>'</span><span>,</span> <span>exist_ok</span><span>=</span><span>True</span><span>)</span>

<span># 1. Continuous Brownian Motion with Sample Paths
</span>
<span># Define time values and x range for density
</span><span>t_values</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0.1</span><span>,</span> <span>5</span><span>,</span> <span>50</span><span>)</span>  <span># Times from 0.1 to 5
</span><span>x</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>-</span><span>5</span><span>,</span> <span>5</span><span>,</span> <span>100</span><span>)</span>          <span># Range of x values
</span>
<span># Simulate a few sample Brownian motion paths
</span><span>num_sample_paths</span> <span>=</span> <span>5</span>
<span>dt_cont</span> <span>=</span> <span>t_values</span><span>[</span><span>1</span><span>]</span> <span>-</span> <span>t_values</span><span>[</span><span>0</span><span>]</span>  <span># constant time step (~0.1)
</span><span>sample_paths</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>num_sample_paths</span><span>,</span> <span>len</span><span>(</span><span>t_values</span><span>)))</span>
<span>sample_paths</span><span>[:,</span> <span>0</span><span>]</span> <span>=</span> <span>0</span>
<span>increments</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>0</span><span>,</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>dt_cont</span><span>),</span> <span>size</span><span>=</span><span>(</span><span>num_sample_paths</span><span>,</span> <span>len</span><span>(</span><span>t_values</span><span>)</span><span>-</span><span>1</span><span>))</span>
<span>sample_paths</span><span>[:,</span> <span>1</span><span>:]</span> <span>=</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>increments</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>)</span>

<span>frames</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span><span>,</span> <span>t</span> <span>in</span> <span>enumerate</span><span>(</span><span>t_values</span><span>):</span>
    <span>p</span> <span>=</span> <span>(</span><span>1</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>2</span> <span>*</span> <span>np</span><span>.</span><span>pi</span> <span>*</span> <span>t</span><span>))</span> <span>*</span> <span>np</span><span>.</span><span>exp</span><span>(</span><span>-</span><span>x</span><span>**</span><span>2</span> <span>/</span> <span>(</span><span>2</span> <span>*</span> <span>t</span><span>))</span>
    
    <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>12</span><span>,</span> <span>4</span><span>))</span>
    <span>plt</span><span>.</span><span>subplot</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>)</span>
    <span>plt</span><span>.</span><span>plot</span><span>(</span><span>x</span><span>,</span> <span>p</span><span>,</span> <span>'</span><span>b-</span><span>'</span><span>,</span> <span>label</span><span>=</span><span>f</span><span>'</span><span>t = </span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>title</span><span>(</span><span>'</span><span>Brownian Motion Distribution</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>'</span><span>x</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>'</span><span>Density p(x,t)</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>ylim</span><span>(</span><span>0</span><span>,</span> <span>0.8</span><span>)</span>
    <span>plt</span><span>.</span><span>legend</span><span>()</span>
    <span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
    
    <span>plt</span><span>.</span><span>subplot</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>2</span><span>)</span>
    <span>for</span> <span>sp</span> <span>in</span> <span>sample_paths</span><span>:</span>
        <span>plt</span><span>.</span><span>plot</span><span>(</span><span>t_values</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>sp</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>'</span><span>-o</span><span>'</span><span>,</span> <span>markersize</span><span>=</span><span>3</span><span>)</span>
    <span>plt</span><span>.</span><span>title</span><span>(</span><span>'</span><span>Sample Brownian Paths</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>'</span><span>Time</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>'</span><span>Position</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>xlim</span><span>(</span><span>0</span><span>,</span> <span>5</span><span>)</span>
    <span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
    
    <span>frame_path</span> <span>=</span> <span>f</span><span>'</span><span>gif_frames/continuous_t_</span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>.png</span><span>'</span>
    <span>plt</span><span>.</span><span>tight_layout</span><span>()</span>
    <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>frame_path</span><span>)</span>
    <span>plt</span><span>.</span><span>close</span><span>()</span>
    <span>frames</span><span>.</span><span>append</span><span>(</span><span>imageio</span><span>.</span><span>imread</span><span>(</span><span>frame_path</span><span>))</span>

<span># Save the continuous Brownian motion GIF
# (duration in seconds per frame; adjust as desired)
</span><span>imageio</span><span>.</span><span>imwrite</span><span>(</span><span>'</span><span>continuous_brownian.gif</span><span>'</span><span>,</span> <span>frames</span><span>,</span> <span>duration</span><span>=</span><span>0.1</span><span>)</span>

<span># 2. Discrete Random Walk with Sample Paths
</span>
<span>def</span> <span>simulate_random_walk</span><span>(</span><span>dt</span><span>,</span> <span>T</span><span>,</span> <span>num_paths</span><span>):</span>
    <span>"""</span><span>Simulate random walk paths with step size sqrt(dt).</span><span>"""</span>
    <span>n_steps</span> <span>=</span> <span>int</span><span>(</span><span>T</span> <span>/</span> <span>dt</span><span>)</span>
    <span>positions</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>num_paths</span><span>,</span> <span>n_steps</span> <span>+</span> <span>1</span><span>))</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>num_paths</span><span>):</span>
        <span>increments</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>choice</span><span>([</span><span>-</span><span>1</span><span>,</span> <span>1</span><span>],</span> <span>size</span><span>=</span><span>n_steps</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>dt</span><span>)</span>
        <span>positions</span><span>[</span><span>i</span><span>,</span> <span>1</span><span>:]</span> <span>=</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>increments</span><span>)</span>
    <span>return</span> <span>positions</span>

<span>dt</span> <span>=</span> <span>0.01</span>  <span># Step size
</span><span>T</span> <span>=</span> <span>5.0</span>    <span># Total time
</span><span>num_paths</span> <span>=</span> <span>10000</span>  <span># For histogram
</span><span>times</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>T</span> <span>+</span> <span>dt</span><span>,</span> <span>dt</span><span>)</span>
<span>positions</span> <span>=</span> <span>simulate_random_walk</span><span>(</span><span>dt</span><span>,</span> <span>T</span><span>,</span> <span>num_paths</span><span>)</span>
<span>sample_indices</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>5</span><span>)</span>

<span>frames</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span><span>,</span> <span>t</span> <span>in</span> <span>enumerate</span><span>(</span><span>times</span><span>):</span>
    <span>if</span> <span>i</span> <span>%</span> <span>10</span> <span>==</span> <span>0</span><span>:</span>  <span># Use every 10th frame for the GIF
</span>        <span>current_positions</span> <span>=</span> <span>positions</span><span>[:,</span> <span>i</span><span>]</span>
        <span>x_vals</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>-</span><span>5</span><span>,</span> <span>5</span><span>,</span> <span>100</span><span>)</span>
        <span>p_theoretical</span> <span>=</span> <span>norm</span><span>.</span><span>pdf</span><span>(</span><span>x_vals</span><span>,</span> <span>0</span><span>,</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>t</span><span>)</span> <span>if</span> <span>t</span> <span>&gt;</span> <span>0</span> <span>else</span> <span>1e-5</span><span>)</span>
        
        <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>12</span><span>,</span> <span>4</span><span>))</span>
        <span>plt</span><span>.</span><span>subplot</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>)</span>
        <span>plt</span><span>.</span><span>hist</span><span>(</span><span>current_positions</span><span>,</span> <span>bins</span><span>=</span><span>50</span><span>,</span> <span>density</span><span>=</span><span>True</span><span>,</span> <span>alpha</span><span>=</span><span>0.6</span><span>,</span> <span>label</span><span>=</span><span>f</span><span>'</span><span>t = </span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>plot</span><span>(</span><span>x_vals</span><span>,</span> <span>p_theoretical</span><span>,</span> <span>'</span><span>r-</span><span>'</span><span>,</span> <span>label</span><span>=</span><span>'</span><span>N(0,t)</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>title</span><span>(</span><span>'</span><span>Discrete Random Walk Distribution</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>'</span><span>Position</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>'</span><span>Density</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>ylim</span><span>(</span><span>0</span><span>,</span> <span>0.8</span><span>)</span>
        <span>plt</span><span>.</span><span>legend</span><span>()</span>
        <span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
        
        <span>plt</span><span>.</span><span>subplot</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>2</span><span>)</span>
        <span>for</span> <span>idx</span> <span>in</span> <span>sample_indices</span><span>:</span>
            <span>plt</span><span>.</span><span>plot</span><span>(</span><span>times</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>positions</span><span>[</span><span>idx</span><span>,</span> <span>:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>'</span><span>-o</span><span>'</span><span>,</span> <span>markersize</span><span>=</span><span>3</span><span>)</span>
        <span>plt</span><span>.</span><span>title</span><span>(</span><span>'</span><span>Sample Random Walk Paths</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>'</span><span>Time</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>'</span><span>Position</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>xlim</span><span>(</span><span>0</span><span>,</span> <span>T</span><span>)</span>
        <span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
        
        <span>frame_path</span> <span>=</span> <span>f</span><span>'</span><span>gif_frames/discrete_t_</span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>.png</span><span>'</span>
        <span>plt</span><span>.</span><span>tight_layout</span><span>()</span>
        <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>frame_path</span><span>)</span>
        <span>plt</span><span>.</span><span>close</span><span>()</span>
        <span>frames</span><span>.</span><span>append</span><span>(</span><span>imageio</span><span>.</span><span>imread</span><span>(</span><span>frame_path</span><span>))</span>

<span># Save the discrete random walk GIF with infinite looping
</span><span>imageio</span><span>.</span><span>imwrite</span><span>(</span><span>'</span><span>discrete_random_walk.gif</span><span>'</span><span>,</span> <span>frames</span><span>,</span> <span>duration</span><span>=</span><span>0.1</span><span>,</span> <span>loop</span><span>=</span><span>0</span><span>)</span>
</pre></td></tr></tbody></table></code></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MongoDB acquires Voyage AI (107 pts)]]></title>
            <link>https://investors.mongodb.com/news-releases/news-release-details/mongodb-announces-acquisition-voyage-ai-enable-organizations</link>
            <guid>43160731</guid>
            <pubDate>Mon, 24 Feb 2025 15:37:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://investors.mongodb.com/news-releases/news-release-details/mongodb-announces-acquisition-voyage-ai-enable-organizations">https://investors.mongodb.com/news-releases/news-release-details/mongodb-announces-acquisition-voyage-ai-enable-organizations</a>, See on <a href="https://news.ycombinator.com/item?id=43160731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><b><i><org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> to integrate Voyage AI's industry-leading embedding and reranking models, delivering highly accurate and relevant information retrieval to power sophisticated AI use cases</i></b></p>
<p>,  /PRNewswire/ -- <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB, Inc.</org> (NASDAQ: MDB), the leading database for modern applications, today announced it has acquired Voyage AI, a pioneer in state-of-the-art embedding and reranking models that power next-generation AI applications. Integrating Voyage AI's technology with <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> will enable organizations to easily build trustworthy, AI-powered applications by offering highly accurate and relevant information retrieval deeply integrated with operational data.</p>

    <p>
                        <a href="https://mma.prnewswire.com/media/384058/MongoDB_Logo.html" target="_blank" rel="nofollow">
&nbsp;<img src="https://mma.prnewswire.com/media/384058/MongoDB_Logo.jpg" title="MongoDB" alt="MongoDB">
&nbsp;</a>
                </p>
<p>AI-powered applications can address a broad range of complex use cases that traditional software cannot; however, because AI models are probabilistic, they can hallucinate––when a model generates false or misleading information. Inaccurate or low-quality results can create serious risks––especially in cases where the accuracy of information is critical, such as a hospital performing cancer screenings, a financial firm making autonomous investment decisions, or a law firm offering legal advice. Consequently, the risk of hallucinations has limited the use of AI applications for mission-critical use cases. These hallucinations typically occur when the AI model lacks sufficient understanding or context of data within an enterprise.</p>
<p>To address this challenge, companies need high-quality retrieval—a critical AI capability that ensures the most relevant information is extracted from their data with precision. Voyage AI's advanced embedding and reranking models enable applications to extract meaning from highly specialized and domain-specific text and unstructured data—ranging from legal and financial documents to images, code, and enterprise knowledge bases. Their models are trusted by leading AI innovators like Anthropic, LangChain, Harvey, and Replit. Notably, Voyage AI's embedding models are the highest-rated zero-shot models in the Hugging Face community. Voyage AI is a leader in AI-powered search and retrieval, backed by a team of world-class AI researchers with roots at <org>Stanford</org>, <org>MIT</org>, <org value="ACORN:6001201275" idsrc="xmltag.org">UC Berkeley</org>, and Princeton. Their expertise in cutting-edge embedding models and retrieval architectures will enhance <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB's</org> AI capabilities to solve the most challenging problems with building and scaling AI applications.</p>
<p>"AI has the promise to transform every business, but adoption is held back by the risk of hallucinations," said <person>Dev Ittycheria</person>, CEO of <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org>. "By bringing the power of advanced AI-powered search and retrieval to our highly flexible database, the combination of <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> and Voyage AI enables enterprises to easily build trustworthy AI-powered applications that drive meaningful business impact. With this acquisition, <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> is redefining what's required of the database for the AI era."</p>
<p>"For AI applications to reach their full potential, businesses must trust their outputs, so retrieval needs to be deeply integrated with operational data to be accurate and relevant," said Tengyu Ma, Founder of Voyage AI. "Joining MongoDB enables us to bring our cutting-edge AI retrieval technology to a broader audience and integrate it seamlessly into mission-critical applications. By combining our expertise in embeddings and reranking with <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB's</org> best-in-class database, we can help organizations build AI applications that deliver more accurate and reliable results at scale, empowering them to confidently apply AI to high-stakes use cases."</p>
<p>Voyage AI's embedding and reranking models will remain available through <u><a href="https://c212.net/c/link/?t=0&amp;l=en&amp;o=4368243-1&amp;h=744614746&amp;u=http%3A%2F%2Fvoyage.ai%2F&amp;a=voyage.ai" target="_blank" rel="nofollow">voyage.ai</a></u>, <location>AWS Marketplace</location>, and <location>Azure Marketplace</location>, with further <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> integrations launching later this year.</p>
<p>For a deeper look at the technology behind Voyage AI and what this means for AI-powered applications, see the <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> blog <a href="https://c212.net/c/link/?t=0&amp;l=en&amp;o=4368243-1&amp;h=3611869988&amp;u=https%3A%2F%2Fwww.mongodb.com%2Fblog%2Fpost%2Fredefining-database-ai-why-mongodb-acquired-voyage-ai&amp;a=here" target="_blank" rel="nofollow">here</a>.</p>
<p><b>About <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org></b></p>
<p>Headquartered in <location value="LU/us.ny.nyc" idsrc="xmltag.org">New York</location>, <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB's</org> mission is to empower innovators to create, transform, and disrupt industries with software and data. <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB's</org> unified, intelligent data platform was built to power the next generation of applications, and <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> is the most widely available, globally distributed database on the market. With integrated capabilities for operational data, search, real-time analytics, and AI-powered retrieval, <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> helps organizations everywhere move faster, innovate more efficiently, and simplify complex architectures. Millions of developers and more than 50,000 customers across almost every industry—including 70% of the Fortune 100—rely on&nbsp;MongoDB for their most important applications. To learn more, visit <a href="https://c212.net/c/link/?t=0&amp;l=en&amp;o=4368243-1&amp;h=952736343&amp;u=https%3A%2F%2Fwww.mongodb.com%2F&amp;a=mongodb.com" target="_blank" rel="nofollow">mongodb.com</a>.</p>
<p><b>Forward-looking Statements</b></p>
<p>This press release includes certain "forward-looking statements" within the meaning of Section 27A of the Securities Act of 1933, as amended, or the Securities Act, and Section 21E of the Securities Exchange Act of 1934, as amended, including statements concerning <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB's</org> acquisition of Voyage AI. These forward-looking statements include, but are not limited to, plans, objectives, expectations and intentions and other statements contained in this press release that are not historical facts and statements identified by words such as "anticipate," "believe," "continue," "could," "estimate," "expect," "intend," "may," "plan," "project," "will," "would" or the negative or plural of these words or similar expressions or variations. These forward-looking statements reflect our current views about our plans, intentions, expectations, strategies and prospects, which are based on the information currently available to us and on assumptions we have made. Although we believe that our plans, intentions, expectations, strategies and prospects as reflected in or suggested by those forward-looking statements are reasonable, we can give no assurance that the plans, intentions, expectations or strategies will be attained or achieved. Furthermore, actual results may differ materially from those described in the forward-looking statements and are subject to a variety of assumptions, uncertainties, risks and factors that are beyond our control including, without limitation: our customers renewing their subscriptions with us and expanding their usage of software and related services; global political changes; the effects of the ongoing military conflicts between <location value="LC/ru" idsrc="xmltag.org">Russia</location> and <location value="LC/ua" idsrc="xmltag.org">Ukraine</location> and <location value="LC/il" idsrc="xmltag.org">Israel</location> and <org value="ACORN:5020096112" idsrc="xmltag.org">Hamas</org> on our business and future operating results; economic downturns and/or the effects of rising interest rates, inflation and volatility in the global economy and financial markets on our business and future operating results; our potential failure to meet publicly announced guidance or other expectations about our business and future operating results; our limited operating history; our history of losses; failure of our platform to satisfy customer demands; the effects of increased competition; our investments in new products and our ability to introduce new features, services or enhancements; social, ethical and security issues relating to the use of new and evolving technologies, such as artificial intelligence, in our offerings or partnerships; our ability to effectively expand our sales and marketing organization; our ability to continue to build and maintain credibility with the developer community; our ability to add new customers or increase sales to our existing customers; our ability to maintain, protect, enforce and enhance our intellectual property; the effects of social, ethical and regulatory issues relating to the use of new and evolving technologies, such as artificial intelligence, in our offerings or partnerships; the growth and expansion of the market for database products and our ability to penetrate that market; our ability to integrate acquired businesses and technologies successfully or achieve the expected benefits of such acquisitions, including the acquisition of Voyage AI; the risk of any unexpected costs or expenses resulting from the acquisition of Voyage AI; the risk of any litigation relating to such acquisition; the risk that such acquisition and the announcement of it could have an adverse effect on our operating results and business generally; our ability to maintain the security of our software and adequately address privacy concerns; our ability to manage our growth effectively and successfully recruit and retain additional highly-qualified personnel; and the price volatility of our common stock. These and other risks and uncertainties are more fully described in our filings with the <org>Securities and Exchange Commission</org> ("<org>SEC</org>"), including under the caption "Risk Factors" in our Quarterly Report on Form 10-Q for the quarter ended <chron>October 31, 2024</chron>, filed with the <org>SEC</org> on <chron>December 10, 2024</chron>, and other filings and reports that we may file from time to time with the <org>SEC</org>. Except as required by law, we undertake no duty or obligation to update any forward-looking statements contained in this release as a result of new information, future events, changes in expectations or otherwise.</p>
<p><b>Press Contact:<br></b><u><a href="mailto:press@mongodb.com" target="_blank" rel="nofollow">press@mongodb.com</a></u></p>






<p id="PURL"><img title="Cision" width="12" height="12" alt="Cision" src="https://c212.net/c/img/favicon.png?sn=NY25220&amp;sd=2025-02-24" loading="lazy"> View original content to download multimedia:<a id="PRNURL" rel="nofollow" href="https://www.prnewswire.com/news-releases/mongodb-announces-acquisition-of-voyage-ai-to-enable-organizations-to-build-trustworthy-ai-applications-302382979.html" target="_blank">https://www.prnewswire.com/news-releases/mongodb-announces-acquisition-of-voyage-ai-to-enable-organizations-to-build-trustworthy-ai-applications-302382979.html</a></p>
<p>SOURCE  <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB, Inc.</org></p>

</div></div>]]></description>
        </item>
    </channel>
</rss>