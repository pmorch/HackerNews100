<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 28 Nov 2023 17:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Microsoft opens sources ThreadX RTOS used in Raspberry Pis (146 pts)]]></title>
            <link>https://www.theregister.com/2023/11/28/microsoft_opens_sources_threadx/</link>
            <guid>38445020</guid>
            <pubDate>Tue, 28 Nov 2023 12:35:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/11/28/microsoft_opens_sources_threadx/">https://www.theregister.com/2023/11/28/microsoft_opens_sources_threadx/</a>, See on <a href="https://news.ycombinator.com/item?id=38445020">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Microsoft is open sourcing the realtime operating system that it acquired with Express Logic, donating it to the Eclipse Foundation.</p>
<p>The vendor has made its ThreadX RTOS, and the Azure RTOS development suite that includes it, open source. The company has <a href="https://techcommunity.microsoft.com/t5/internet-of-things-blog/microsoft-contributes-azure-rtos-to-open-source/ba-p/3986318" rel="nofollow">contributed Azure RTOS</a> to the stewardship of the Eclipse Foundation, where it will be <a href="https://eclipse-foundation.blog/2023/11/21/introducing-eclipse-threadx/" rel="nofollow">known as Eclipse ThreadX</a> and available under the permissive MIT licence.</p>
<p>Although there are many Real-time OSes – RTOS for short – out there, you generally don't hear much about them. You may never have heard of ThreadX, but there's a fairly good chance that you unwittingly have several copies lying around or even running right now.</p>

    

<p>There are more famous RTOSes, such as Wind River's VxWorks: it's running on Mars, inside <a href="https://www.theregister.com/2021/02/23/perseverance_landing_video/">NASA's Perseverance Rover</a> for example. Blackberry's QNX RTOS, which we called <a href="https://www.theregister.com/2015/02/18/qnx_hypervisor/">the money-making part of the business</a>, has been in the spotlight twice: once as <a href="https://www.theregister.com/2010/09/28/blackberry_tablet_runs_qnx/">the basis for RIM's Blackberry X</a> fondleslab and smartphone range, but back in the 1990s for the amazing QNX <a href="http://qnx.puslapiai.lt/qnxdemo/qnx_demo_disk.htm" rel="nofollow">single-floppy demo disk</a>.</p>

        


        

<p>ThreadX was quite pervasive, though. Microsoft claims 12 billion devices run it, and you might own some of them. For a while it powered <a href="https://www.theregister.com/2017/05/01/intel_amt_me_vulnerability/">Intel's on-chip Management Engine</a>. It is also the firmware that controls every Raspberry Pi bigger than the Pi Pico. On the Pi 1, 2 and 3, it's <a href="https://github.com/raspberrypi/documentation/blob/develop/documentation/asciidoc/computers/configuration/boot_folder.adoc" rel="nofollow">the file on your Pi's SD card</a> called <code>bootcode.bin</code>; in the Pi 4 and 400, it's called <code>start*.elf</code>. Even though it's <a href="https://github.com/raspberrypi/firmware" rel="nofollow">on GitHub</a> and <a href="https://packages.debian.org/sid/raspi-firmware" rel="nofollow">included in Debian</a>, it's a proprietary "blob" (<em>B</em>inary <em>L</em>arge <em>OB</em>ject). It's a metaphorical sealed black box which doesn't even contain Arm code: instead, it runs on the Pi's VideoCore GPU. This is the primary device, the part that boots up the Pi and controls its hardware: the Arm cores are <a href="https://raspberrypi.stackexchange.com/a/106077/125748" rel="nofollow">slave devices</a> to the VideoCore GPU.</p>
<div><p><img src="https://regmedia.co.uk/2017/10/18/rpi_with_active_cooling.jpg?x=174&amp;amp;y=115&amp;amp;crop=1" width="174" height="115" alt="Raspberry Pi active cooling"></p><h2 title="Choose between five options">Ah lovely, here's something you can do with those Raspberry Pis, NUC PCs in the bottom of the drawer: Run Ubuntu Appliances on them</h2>
<p><a href="https://www.theregister.com/2020/06/18/ubuntu_raspberry_pi/"><span>READ MORE</span></a></p></div>
<p>This, incidentally, is why Xen only runs on the Pi 4 and later: they were the first versions where the Arm cores had their own interrupt controller, and even so, <a href="https://xenproject.org/2020/09/29/xen-on-raspberry-pi-4-adventures/" rel="nofollow">it took some work</a>. We have read that in the Pi 5, it's in EEPROM and does even less, meaning that the Arm cores have more control, as this <a href="https://forums.theregister.com/post/reply/4735439" rel="nofollow"><em>Reg</em> comment</a> bears out.</p>
<p>As soon as this innocent little OS turned 21 in 2019, Microsoft grabbed it, <a href="https://www.theregister.com/2020/05/21/microsoft_build_iot/">acquiring ThreadX owners Express Logic</a> and rebranding the poor thing as <em>Azure RTOS</em>, which hasn't done any favours for its brand awareness. The purchase came soon after <a href="https://www.theregister.com/2018/10/22/freertos_iot_platform_security_flaws/">AWS took over stewardship of FreeRTOS</a>, and some observers, such as <a href="https://www.reddit.com/r/embedded/comments/181g1nk/comment/kae4gi2/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3" rel="nofollow">this commenter on Reddit</a>, claim that the deal was a response to Amazon's move. After the purchase, original developer William Lamie left, starting a new company which sells a "fifth-generation" RTOS with POSIX-compatible threads, called <a href="https://px5rtos.com/about/" rel="nofollow">PX5OS</a>.</p>
<p>Even so, ThreadX is a tested and established product; some parts even have <a href="https://www.tuvsud.com/en/industries/mobility-and-automotive/automotive-and-oem/iso-26262-functional-safety/iso-26262-functional-safety-certification-programme" rel="nofollow">TÜV Functional Safety</a> (FuSa) certification, such as the <a href="https://www.st.com/resource/en/product_presentation/stm32_stm8_functional-safety-packages.pdf" rel="nofollow">STM32 version</a> [PDF]. That kind of thing is powerfully attractive to some customers.</p>
<ul>

<li><a href="https://www.theregister.com/2023/04/12/python_management_eu/">Python head hisses at looming Euro cybersecurity rules</a></li>

<li><a href="https://www.theregister.com/2022/03/08/netbeans_13_arrives/">Open-source IDE NetBeans hits v13 – tweaks for Gradle, Maven</a></li>

<li><a href="https://www.theregister.com/2021/09/23/gaia_x_edc/">Eclipse Data Connector arrives for GAIA-X, Europe's plan to protect its cloud data from foreign tech firms</a></li>

<li><a href="https://www.theregister.com/2020/05/15/microsoft_brad_smith_open_source/">Everything OK with Microsoft? Windows giant admits it was 'on the wrong side of history' with regard to open source</a></li>
</ul>
<p>At this point, only the current version <a href="https://github.com/azure-rtos" rel="nofollow">is on GitHub</a>, and we don't see any trace of a VideoCore version. Although the GPU drivers have <a href="https://www.theregister.com/2012/10/24/raspberry_pi_broadcom_soc_drivers_now_fully_open_source/">long been open source</a>, the firmware never was, and attempts to write <a href="https://github.com/christinaa/rpi-open-firmware" rel="nofollow">an independent FOSS version</a> were never completed, for <a href="https://github.com/christinaa/rpi-open-firmware/issues/37" rel="nofollow">reasons explained</a> on the project page. Now, there is at least some hope that the Raspberry Pi Foundation might be able to get permission to release the source code for its version. As of last year, the foundation had <a href="https://www.theregister.com/2022/02/28/pi_at_10/">sold over 46 million</a> of the things, and if the whole software stack were open source, that would make them even more appealing for a lot of people. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Modern C++ Programming Course (306 pts)]]></title>
            <link>https://github.com/federico-busato/Modern-CPP-Programming</link>
            <guid>38444834</guid>
            <pubDate>Tue, 28 Nov 2023 12:03:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/federico-busato/Modern-CPP-Programming">https://github.com/federico-busato/Modern-CPP-Programming</a>, See on <a href="https://news.ycombinator.com/item?id=38444834">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Modern C++ Programming</h2>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/other/cpp_logo.png"><img src="https://github.com/federico-busato/Modern-CPP-Programming/raw/master/other/cpp_logo.png"></a>
</p>
<p dir="auto">
    <a href="https://github.com/federico-busato/Modern-CPP-Programming/releases" alt="Release">
        <img src="https://camo.githubusercontent.com/1b6f7342b709e71b39458b3269e058319d733d1af5fafa3a573663b8002f329f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f666564657269636f2d62757361746f2f4d6f6465726e2d4350502d50726f6772616d6d696e673f7374796c653d666f722d7468652d6261646765" data-canonical-src="https://img.shields.io/github/v/release/federico-busato/Modern-CPP-Programming?style=for-the-badge">
    </a>
</p>
<p dir="auto">
    <a href="https://github.com/federico-busato/Modern-CPP-Programming/commits/master" alt="Commits">
        <img src="https://camo.githubusercontent.com/2c71dfa2a8357ff58974cba85f484d59caf8946a60dd51f0aa58b29cc3995b18/68747470733a2f2f62616467656e2e6e65742f6769746875622f636f6d6d6974732f666564657269636f2d62757361746f2f4d6f6465726e2d4350502d50726f6772616d6d696e673f7374796c653d666f722d7468652d6261646765267363616c653d312e32" data-canonical-src="https://badgen.net/github/commits/federico-busato/Modern-CPP-Programming?style=for-the-badge&amp;scale=1.2">
    </a>
</p>
<p dir="auto">
    <a href="https://github.com/federico-busato/Modern-CPP-Programming/network/members" alt="Forks">
        <img src="https://camo.githubusercontent.com/3c0ebfad13db3deb80ee67da0ddaf15f309737ae4718d13c642ed24b351fee2f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f666564657269636f2d62757361746f2f4d6f6465726e2d4350502d50726f6772616d6d696e673f7374796c653d666f722d7468652d6261646765" data-canonical-src="https://img.shields.io/github/forks/federico-busato/Modern-CPP-Programming?style=for-the-badge">
    </a>
    
        <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/32cf116583a049de0a44673102fcee773225f7f13d62140763d6c73b80d87612/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f666564657269636f2d62757361746f2f4d6f6465726e2d4350502d50726f6772616d6d696e673f7374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/32cf116583a049de0a44673102fcee773225f7f13d62140763d6c73b80d87612/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f666564657269636f2d62757361746f2f4d6f6465726e2d4350502d50726f6772616d6d696e673f7374796c653d666f722d7468652d6261646765" data-canonical-src="https://img.shields.io/github/stars/federico-busato/Modern-CPP-Programming?style=for-the-badge"></a>
    
</p>
<h2 tabindex="-1" dir="auto">C++11  /  C++14  /  C++17  / C++20 / (C++23)</h2>
<p dir="auto">This <em>open-access</em> course is directed at those who are already familiar with C and object-oriented programming towards a proficiency level of C++ programming. The course covers the basics of C++ programming and moves on to advanced C++ semantics and concepts.</p>
<p dir="auto"><strong>Key features</strong>:</p>
<ul dir="auto">
<li><em>Free and frequently updated</em></li>
<li>Include the <em>last language standard</em> concepts and features</li>
<li><em>Practical teaching</em>: small structured descriptions associated with code</li>
<li><em>Minimal code examples</em> for showing just a specific feature or issue without digressing</li>
<li><em>Complementary language aspects</em>: tools, coding conventions, project organization, and code optimization</li>
<li><em>Experience-based</em>: many aspects, examples, and problems come from real-world cases faced during my work as software engineer</li>
</ul>
<p dir="auto">If you enjoy the course or you find it useful, please add a <strong>Star</strong></p>
<p dir="auto"><a href="https://github.com/federico-busato/Modern-CPP-Programming"><img src="https://camo.githubusercontent.com/042085f25024e237f7121bbe74b9edcae59dd62617e34f158576b52b74c9d8e3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f666564657269636f2d62757361746f2f4d6f6465726e2d4350502d50726f6772616d6d696e673f7374796c653d736f6369616c" alt="stars - Modern-CPP-Programming" data-canonical-src="https://img.shields.io/github/stars/federico-busato/Modern-CPP-Programming?style=social"></a></p>
<h2 tabindex="-1" dir="auto">CHAPTERS</h2>
<table>
<thead>
<tr>
<th></th>
<th>TITLE</th>
<th>MAIN FOCUS</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/01.Introduction.pdf">Introduction</a></strong></td>
<td>History of C/C++, Areas of Applications, Course introduction</td>
</tr>
<tr>
<td><strong>2</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/02.Basic_Concepts_I.pdf">Basic Concepts I - Fundamental Types</a></strong></td>
<td>Types overview, operators, and conversion</td>
</tr>
<tr>
<td><strong>3</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/03.Basic_Concepts_II.pdf">Basic Concepts II - Integral and Floating-point Types</a></strong></td>
<td>Integral and floating-point types and their arithmetic</td>
</tr>
<tr>
<td><strong>4</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/04.Basic_Concepts_III.pdf">Basic Concepts III - Entities and Control Flow</a></strong></td>
<td>Enumerators, structures, control flow statements</td>
</tr>
<tr>
<td><strong>5</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/05.Basic_Concepts_IV.pdf">Basic Concepts IV - Memory Management</a></strong></td>
<td>Heap, Stack, pointers, references, const properties, conversion operators</td>
</tr>
<tr>
<td><strong>6</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/06.Basic_Concepts_V.pdf">Basic Concepts V - Functions and Preprocessing</a></strong></td>
<td>Functions, lambda expressions, preprocessing directives</td>
</tr>
<tr>
<td><strong>7</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/07.Object_Oriented_I.pdf">Object Oriented Programming I&nbsp;-&nbsp;Class&nbsp;Concepts</a></strong></td>
<td>Class hierarchy, constructor, destructor, class keywords</td>
</tr>
<tr>
<td><strong>8</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/08.Object_Oriented_II.pdf">Object Oriented Programming II&nbsp;-&nbsp;Polymorphism and Operator Overloading</a></strong></td>
<td>Polymorphism, operators overloading</td>
</tr>
<tr>
<td><strong>9</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/09.Templates_I.pdf">Templates and Meta-programming I&nbsp;-&nbsp;Function Templates and Compile-Time Utilities</a></strong></td>
<td>Function template, type traits</td>
</tr>
<tr>
<td><strong>10</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/10.Templates_II.pdf">Templates and Meta-programming II&nbsp;-&nbsp;Class Templates and SFINAE</a></strong></td>
<td>Class template, SFINAE</td>
</tr>
<tr>
<td><strong>11</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/11.Translation_Units_I.pdf">Translation Units&nbsp;I</a></strong></td>
<td>Linkage and One Definition Rule</td>
</tr>
<tr>
<td><strong>12</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/12.Translation_Units_II.pdf">Translation Units&nbsp;II</a></strong></td>
<td>Dealing with multiple translation units and files,&nbsp;&nbsp;<code>#include</code>, and modules</td>
</tr>
<tr>
<td><strong>13</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/13.Code_Convention.pdf">Code Conventions</a></strong></td>
<td>Project organization and main code conventions</td>
</tr>
<tr>
<td><strong>14</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/14.Ecosystem_I.pdf">Ecosystem&nbsp;I</a></strong></td>
<td>Debugging, and testing</td>
</tr>
<tr>
<td><strong>15</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/15.Ecosystem_II.pdf">Ecosystem&nbsp;II</a></strong></td>
<td>Cmake,&nbsp;documenting, and other Tools</td>
</tr>
<tr>
<td><strong>16</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/16.Utilities.pdf">Utilities</a></strong></td>
<td>Main <code>std</code> libraries</td>
</tr>
<tr>
<td><strong>17</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/17.Iterators_Containers_Alg.pdf">Containers, Iterators, and Algorithms</a></strong></td>
<td>Containers, iterators, algorithms, ranges</td>
</tr>
<tr>
<td><strong>18</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/18.Advanced_Topics_I.pdf">Advanced Topics&nbsp;I</a></strong></td>
<td>Move semantics, universal reference, type deduction</td>
</tr>
<tr>
<td><strong>19</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/19.Advanced_Topics_II.pdf">Advanced Topics&nbsp;II</a></strong></td>
<td>Error handling, C++ idioms, smart pointers</td>
</tr>
<tr>
<td><strong>20</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/20.Optimization_I.pdf">Optimization I&nbsp;-&nbsp;Basic Concepts</a></strong></td>
<td>Ahmdal Law, performance bounds, architecture concepts (ILP, SIMD, etc.), memory hierarchy</td>
</tr>
<tr>
<td><strong>21</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/21.Optimization_II.pdf">Optimization II - Code Optimization</a></strong></td>
<td>Arithmetic optimizations, memory optimizations, etc.</td>
</tr>
<tr>
<td><strong>22</strong></td>
<td><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/22.Optimization_III.pdf">Optimization III -&nbsp;Non-Coding Optimizations and Benchmarking</a></strong></td>
<td>Compiler optimizations, profiling and benchmarking tools</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">TOPICS IN DETAILS</h2>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/01.Introduction.pdf">1. Introduction</a></strong></p>
<ul dir="auto">
<li><strong>A Little History of C/C++ Programming Languages</strong></li>
<li><strong>Areas of Application and Popularity</strong></li>
<li><strong>C++ Philosophy</strong></li>
<li><strong>C++ Weakness</strong></li>
<li><strong>Books and References</strong></li>
<li><strong>The Course</strong></li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/02.Basic_Concepts_I.pdf">2. Basic Concepts I - Fundamental Types</a></strong></p>
<ul dir="auto">
<li><strong>Preparation</strong>: What compiler should I use?, What editor/IDE compiler should I use?, How to compile?</li>
<li><strong>Hello World</strong>: I/O Stream</li>
<li><strong>C++ Fundamental Types Overview</strong>: Arithmetic types, Non-standard arithmetic types, <code>void</code> type, Pointer type and <code>nullptr</code></li>
<li><strong>Conversion Rules</strong></li>
<li><strong><code>auto</code> declaration</strong></li>
<li><strong>C++ Operators</strong>: Operators precedence, Prefix/Postfix increment/decrement, Assignment, Compound , and Comma operators, Spaceship operator <code>&lt;=&gt;</code> , Safe Comparison Operators</li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/03.Basic_Concepts_II.pdf">3. Basic Concepts II - Integral and Floating-point Types</a></strong></p>
<ul dir="auto">
<li><strong>Integral Data Types</strong>: Fixed width integers, <code>size_t</code> and<code>ptrdiff_t</code>, When use signed/unsigned integer? Promotion, Truncation, Undefined behavior</li>
<li><strong>Floating-point Types and Arithmetic</strong>: IEEE Floating-point Standard and Other Representations, Normal/Denormal values, Infinity, Not a Number (<code>NaN</code>), Machine Epsilon, Units at the Last Place (ULP), Cheatsheet, Summary, Arithmetic Properties, Detect Floating-point Errors</li>
<li><strong>Floating-point Issues</strong>: Catastrophic cancellation, Floating-point comparison</li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/04.Basic_Concepts_III.pdf">4. Basic Concepts III - Entities and Control Flow</a></strong></p>
<ul dir="auto">
<li><strong>Enumerators</strong></li>
<li><strong><code>struct</code>, Bitfield, <code>union</code></strong></li>
<li><strong>Control Flow</strong>: <code>if</code> Statement, <code>for</code> Loop, Range-base <code>for</code> loop, <code>switch</code>, <code>goto</code></li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/05.Basic_Concepts_IV.pdf">5. Basic Concepts IV - Memory Management</a></strong></p>
<ul dir="auto">
<li><strong>Heap and Stack</strong>: Stack Memory, <code>new</code>, <code>delete</code>, Non-allocating placement allocation, Non-throwing allocation, Memory leak</li>
<li><strong>Initialization</strong>: Variable initialization, Uniform initialization, Fixed-size array initialization, Structure initialization, Dynamic memory initialization</li>
<li><strong>Pointers and References</strong>: Pointer Operations, Address-of operator <code>&amp;</code>, Reference</li>
<li><strong>Constant and Literals, <code>const</code>, <code>constexpr</code>, <code>consteval</code>, <code>constinit</code></strong>, <code>if constexpr</code>, <code>std::is constant evaluated()</code>, <code>if consteval</code></li>
<li><strong><code>volatile</code> keyword</strong></li>
<li><strong>Explicit Type Conversion</strong>: <code>static_cast</code>, <code>const_cast</code>, <code>reinterpret_cast</code>, Type punning</li>
<li><code>sizeof</code> Operator</li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/06.Basic_Concepts_V.pdf">6. Basic Concepts V - Functions and Preprocessing</a></strong></p>
<ul dir="auto">
<li><strong>Declaration and Definition</strong></li>
<li><strong>Functions</strong>: Pass by-value, Pass by-pointer, Pass by-reference, Function signature and Overloading, Overloading and <code>=delete</code>, Default parameters, Attributes</li>
<li><strong>Function Pointer and Function Objects</strong></li>
<li><strong>Lambda Expressions</strong>: Capture list, Other features, Capture list and classes</li>
<li><strong>Preprocessing</strong>: Preprocessors, Common errors, Useful macro, Stringizing operator (<code>#</code>),  <code>#error</code> and <code>warning</code>, <code>#pragma</code>, Token-Pasting Operator (<code>##</code>), Variadic Macro</li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/07.Object_Oriented_I.pdf">7. Object-Oriented Programming I&nbsp;-&nbsp;Class&nbsp;Concepts</a></strong></p>
<ul dir="auto">
<li><strong>C++ Classes</strong>: RAII Idiom</li>
<li><strong>Class Hierarchy</strong>:  Access specifiers, Inheritance access specifiers</li>
<li><strong>Class Constructor</strong>: Default constructor, Class initialization, Uniform initialization, Delegate constructor, <code>explicit</code> keyword, <code>[[nodiscard]]</code> and classes</li>
<li><strong>Copy Constructor</strong></li>
<li><strong>Class Destructor</strong></li>
<li><strong>Defaulted  Constructors, Destructor, and Operators</strong> (<code>= default</code>)</li>
<li><strong>Class Keywords</strong>: <code>this</code>, <code>static</code>, <code>const</code>, <code>mutable</code>, <code>using</code>, <code>friend</code>, <code>delete</code></li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/08.Object_Oriented_II.pdf">8. Object-Oriented Programming II&nbsp;-&nbsp;Polymorphism and Operator Overloading</a></strong></p>
<ul dir="auto">
<li><strong>Polymorphism</strong>: <code>virtual</code> methods, Virtual table, <code>override</code> keyword, <code>final</code> keyword, Common errors, Pure virtual method, Abstract class and interface</li>
<li><strong>Inheritance Casting and Run-time Type Identification</strong></li>
<li><strong>Operator Overloading</strong>: Overview, Comparison operator <code>&lt;</code>, Spaceship operator <code>&lt;=&gt;</code>, Subscript operator <code>[]</code>, Multidimensional Subscript operator <code>[]</code>, Function call operator <code>()</code>, static operator <code>[]</code> and operator <code>()</code>, Conversion operator <code>T()</code>, Return type overloading resolution, Increment and decrement operators <code>++</code>/<code>--</code>, Assignment operator <code>=</code>, Stream operator <code>&lt;&lt;</code>, Operator Notes</li>
<li><strong>C++ Special Objects</strong>: Aggregate, Trivial class, Standard-layout class, Plain old data (POD), Hierarchy</li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/09.Templates_I.pdf">9. Templates and Meta-programming I -&nbsp;Function Templates and Compile-Time Utilities</a></strong></p>
<ul dir="auto">
<li><strong>Function Template</strong>: Overview, Template parameters, Template parameter - default value, Specialization, Overloading</li>
<li><strong>Template Variable</strong></li>
<li><strong>Template Parameter Types</strong>: Generic Type Notes, <code>auto</code> Placeholder, Class template parameter type, Array and pointer types, Function type</li>
<li><strong>Compile-Time Utilities</strong>: <code>static_assert</code>, <code>decltype</code> Keyword, <code>using</code> Keyword</li>
<li><strong>Type Traits</strong>: Overview, Type traits library, Type manipulation</li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/10.Templates_II.pdf">10. Templates and Meta-programming II - &nbsp;-&nbsp;Class Templates and SFINAE</a></strong></p>
<ul dir="auto">
<li><strong>Class Template</strong>: Class specialization, Template class constructor</li>
<li><strong>Class Template - Advanced Concepts</strong>: Class + Function - Specialization, Dependent Names - <code>typename</code> and <code>template</code> Keywords, Class template hierarchy and <code>using</code>, <code>friend</code> Keyword, Template Template Arguments</li>
<li><strong>Template Meta-Programming</strong></li>
<li><strong>SFINAE: Substitution Failure Is Not An Error</strong>: Function SFINAE, Class SFINAE, Class + Function SFINAE</li>
<li><strong>Variadic Template</strong>: Folding Expression, Variadic class template</li>
<li><strong>C++20 Concepts</strong>: Overview, <code>concept</code> Keyword, <code>requires</code> Clause, <code>requires</code> Expression, <code>requires</code> Expression + Clause, <code>requires</code> Clause + Expression, <code>requires</code> and <code>constexpr</code>, Nested <code>requires</code></li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/11.Translation_Units_I.pdf">11. Translation Units I</a></strong></p>
<ul dir="auto">
<li><strong>Basic Concepts</strong>: Translation unit, Local and global scope, Linkage</li>
<li><strong>Storage Class and Duration</strong>: Storage duration, Storage class, <code>static</code> and <code>extern</code> keywords, Internal/External linkage examples</li>
<li><strong>Linkage of <code>const</code> and <code>constexpr</code></strong>: Static Initialization Order Fiasco</li>
<li><strong>Linkage Summary</strong></li>
<li><strong>Dealing with Multiple Translation Units</strong>: Class in multiple translation units</li>
<li><strong>One Definition Rule (ODR)</strong>: Global variable issues, ODR - Point 3, <code>inline</code> functions/variables, <code>constexpr</code> and <code>inline</code></li>
<li><strong>ODR - Function Template</strong>: Cases, <code>extern</code> keyword</li>
<li><strong>ODR - Class Template</strong>: Cases, <code>extern</code> keyword</li>
<li><strong>ODR Undefined Behavior and Summary</strong></li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/12.Translation_Units_II.pdf">12. Translation Units II</a></strong></p>
<ul dir="auto">
<li><strong><code>#include</code> Issues</strong>: Forward declaration, Include guard, Circular dependencies, Common linking errors</li>
<li><strong>C++20 Modules</strong>: Overview, Terminology, Visibility and Reachability, Module unit types, Keywords, Global module fragment, Private module fragment, Header module unit, Module partitions</li>
<li><strong>Namespace</strong>: Namespace functions vs. <code>static</code> methods, Namespace alias, Anonymous namespace, <code>inline</code> namespace, Attributes and namespace</li>
<li><strong>Compiling Multiple Translation Units</strong>: Fundamental compiler flags, Compile Methods, Deal with libraries, Build static/dynamic libraries, Find dynamic library dependencies, Analyze object/executable symbols</li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/13.Code_Convention.pdf">13. Code Conventions</a></strong></p>
<ul dir="auto">
<li><strong>C++ Project Organization</strong>: Project directories, Project files, “Common” Project Organization Notes, Alternative - “Canonical” project organization</li>
<li><strong>Coding Styles and Conventions</strong></li>
<li><strong><code>#include</code></strong></li>
<li><strong>Macro and Preprocessing</strong></li>
<li><strong>Namespace</strong></li>
<li><strong>Variables</strong></li>
<li><strong>Functions</strong></li>
<li><strong>Structs and Classes</strong></li>
<li><strong>Control Flow</strong></li>
<li><strong>Modern C++ Features</strong></li>
<li><strong>Maintainability</strong></li>
<li><strong>Naming</strong></li>
<li><strong>Readability and Formatting</strong></li>
<li><strong>Code Documentation</strong></li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/14.Ecosystem_I.pdf">14. Ecosystem I</a></strong></p>
<ul dir="auto">
<li><strong>Debugging</strong></li>
<li><strong>Assertion</strong></li>
<li><strong>Execution debugging</strong></li>
<li><strong>Memory Debugging</strong>: <code>valgrind</code>, Stack protection</li>
<li><strong>Sanitizers</strong>: Address sanitizer, Leak sanitizer, Memory sanitizers, Undefined behavior sanitizer</li>
<li><strong>Debugging Summary</strong></li>
<li><strong>Compiler Warnings</strong></li>
<li><strong>Static Analysis</strong></li>
<li><strong>Code Testing</strong>: Unit test, Code coverage, Fuzz testing</li>
<li><strong>Code Quality</strong>: <code>clang-tidy</code></li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/15.Ecosystem_II.pdf">15. Ecosystem II</a></strong></p>
<ul dir="auto">
<li><strong>CMake</strong>: <code>cmake</code> and <code>ctest</code></li>
<li><strong>Code Documentation</strong>: <code>doxygen</code></li>
<li><strong>Code Statistics</strong>: Count lines of code, Cyclomatic complexity analyzer</li>
<li><strong>Other Tools</strong>: Code formatting - <code>clang-format</code>, <code>Compiler Explorer</code>, Code transformation - <code>CppInsights</code>, Code autocompletion - <code>GitHub Co-Pilot/TabNine/Kite</code>, Local code search - <code>ripgrep</code>, Code search engine - <code>searchcode/grep.app</code>, Code benchmarking - <code>Quick-Bench</code>, Font for Coding</li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/16.Utilities.pdf">16. Utilities</a></strong></p>
<ul dir="auto">
<li><strong>I/O Stream</strong>: Manipulator, <code>ofstream/ifstream</code></li>
<li><strong>Math Libraries</strong></li>
<li><strong>Strings</strong>: <code>std::string</code>, Conversion from/to numeric  values, <code>std::string_view</code>, <code>std::format</code>, <code>std::print</code></li>
<li><strong>Random Number</strong>: Basic Concepts, C++ <code>&lt;random&gt;</code>, Seed, PRNG period and quality, Distribution, Quasi-random</li>
<li><strong>Time Measuring</strong>: Wall-Clock time, User time, System time</li>
<li><strong>Std Template Classes</strong>: <code>std::byte</code> <code>std::pair</code>, <code>std::tuple</code>, <code>std::variant</code>, <code>std::optional</code>, <code>std::any</code></li>
<li><strong>Filesystem Library</strong>: Query methods, Modify methods</li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/17.Iterators_Containers_Alg.pdf">17. Containers, Iterators, and Algorithms</a></strong></p>
<ul dir="auto">
<li><strong>Containers and Iterators</strong></li>
<li><strong>Sequence Containers</strong>: <code>std::array</code>, <code>std::vector</code>, <code>std::list</code>, <code>std::deque</code>, <code>std::forward_list</code></li>
<li><strong>Associative Containers</strong>: <code>std::set</code>, <code>std::map</code>, <code>std::multiset</code></li>
<li><strong>Container Adaptors</strong>: <code>std::stack</code>, <code>std::queue</code>, <code>std::priority_queue</code></li>
<li><strong>View</strong>: <code>std::span</code></li>
<li><strong>Implement a Custom Iterator</strong>: Semantic, Implement a simple Iterator</li>
<li><strong>Iterator Utility Methods</strong>: <code>std::advance</code>, <code>std::next</code>, <code>std::prev</code>, <code>std::distance</code>, Container access methods, Iterator traits</li>
<li><strong>Algorithms Library</strong>: <code>std::find_if</code>, <code>std::sort</code>, <code>std::accumulate</code>, <code>std::generate</code>, <code>std::remove_if</code></li>
<li><strong>C++20 Ranges</strong>: Key concepts, Range view,  Range adaptor, Range factory, Range algorithms, Range actions</li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/18.Advanced_Topics_I.pdf">18. Advanced Topics I</a></strong></p>
<ul dir="auto">
<li><strong>Move Semantic</strong>: <code>lvalues</code> and <code>rvalues</code> references, Move semantic, Compiler implicitly declared, <code>std::move</code></li>
<li><strong>Universal Reference and Perfect Forwarding</strong>: Universal reference, Reference collapsing rules, Perfect forwarding</li>
<li><strong>Value Categories</strong></li>
<li><strong><code>&amp;</code>, <code>&amp;&amp;</code> Ref-qualifiers and <code>volatile</code> Overloading</strong></li>
<li><strong>Copy Elision and RVO</strong></li>
<li><strong>Type Deduction</strong>: Pass by-reference, Pass by-pointer, Pass by-value, <code>auto</code> deduction</li>
<li><strong><code>const</code> Correctness</strong></li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/19.Advanced_Topics_II.pdf">19. Advanced Topics II</a></strong></p>
<ul dir="auto">
<li><strong>Undefined Behavior</strong></li>
<li><strong>Error Handling</strong>: C++ Exceptions, Defining custom exceptions, <code>noexcept</code> keyword, Memory allocation issues, Alternative error handling approaches</li>
<li><strong>C++ Idioms</strong>: Rule of zero/three/five, Singleton, PIMPL, CRTP, Template Virtual Functions</li>
<li><strong>Smart pointers</strong>: <code>std::unique_ptr</code>, <code>std::shared_ptr</code>, <code>std::weak_ptr</code></li>
<li><strong>Concurrency</strong>: Thread Methods, Mutex, Atomic, Task-based parallelism</li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/20.Optimization_I.pdf">20. Optimization I&nbsp;-&nbsp;Basic Concepts</a></strong></p>
<ul dir="auto">
<li><strong>Introduction</strong>: Moore’s Law, Moore’s Law limitations, Reasons for Optimizing</li>
<li><strong>Basic Concepts</strong>: Asymptotic complexity, Time-Memory Trade-off, Developing Cycle, Ahmdal's law, Throughput, Bandwidth, Latency, Performance bounds, Arithmetic intensity</li>
<li><strong>Basic Architecture Concepts</strong>: Instruction-level parallelism (ILP), Little’s law, Data-level parallelism (SIMD), Thread-level parallelism (TLP), Single Instruction Multiple Threads (SIMT), RISC, CISC Instruction sets</li>
<li><strong>Memory Hierarchy</strong>: Memory hierarchy concepts, Memory locality</li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/21.Optimization_II.pdf">21. Optimization II&nbsp;-&nbsp;Code Optimization</a></strong></p>
<ul dir="auto">
<li><strong>I/O Operations</strong>: <code>printf</code>, Memory mapped I/O, Speed up raw data loading</li>
<li><strong>Memory Optimizations</strong>: Heap memory, Stack memory, Cache utilization, data alignment, Memory Prefetch</li>
<li><strong>Arithmetic</strong>: Data types, Operations, Conversion, Floating-point, Compiler intrinsic functions, Value in a range, Lookup table</li>
<li><strong>Control Flow</strong>: Loop hoisting, Loop unrolling, Branch hints, Compiler hints, Recursion</li>
<li><strong>Functions</strong>: Function call cost, Argument passing, Function optimizations, Function inlining, Pointers aliasing</li>
<li><strong>C++ Objects</strong>: Object RAII optimizations</li>
<li><strong>Std Library and Other Language Aspects</strong></li>
</ul>
<p dir="auto"><strong><a href="https://github.com/federico-busato/Modern-CPP-Programming/blob/master/22.Optimization_III.pdf">22. Optimization III -&nbsp;Non-Coding Optimizations and Benchmarking</a></strong></p>
<ul dir="auto">
<li><strong>Compiler Optimizations</strong>: About the compiler, Compiler optimization flags, Linker optimization flags, Architecture flags, Help the Compiler to produce better code, Profile guided optimization (PGO), Post-Processing Binary Optimizer</li>
<li><strong>Compiler Transformation Techniques</strong></li>
<li><strong>Libraries and Data Structures</strong>: External libraries, Std library</li>
<li><strong>Performance Benchmarking</strong>: What to test?, Workload/Dataset quality, Cache behavior, Stable CPU performance, Program, Multi-threads considerations, Program memory layout, Measurement overhead, Compiler optimizations, Metric evaluation</li>
<li><strong>Profiling</strong>: <code>gprof</code>, <code>uftrace</code>, <code>callgrind</code>, <code>cachegrind</code>, <code>perf</code> Linux profiler</li>
<li><strong>Parallel Computing</strong>: Concurrency vs. Parallelism, Performance scaling, Gustafson’s Law, Parallel programming languages</li>
</ul>
<h3 tabindex="-1" dir="auto">Roadmap</h3>
<ol dir="auto">
<li>Software Design Chapter</li>
<li>Build Aspects Chapter (e.g. reducing build time)</li>
</ol>
<h3 tabindex="-1" dir="auto">Essential Tool</h3>
<p dir="auto">Online compiler and execution: <a href="https://godbolt.org/" rel="nofollow">CompilerExplorer</a></p>
<ul dir="auto">
<li>for code execution: [Add new..] -&gt; [execution only]</li>
</ul>
<h3 tabindex="-1" dir="auto">Reporting bugs and contributing</h3>
<p dir="auto">If you find any typo, conceptual error, or section to improve, please report them by writing directly to me or by using the <code>issue</code> panel</p>
<p dir="auto">
    <a href="https://github.com/federico-busato/Modern-CPP-Programming-Material/issues" alt="Issues">
        <img src="https://camo.githubusercontent.com/b5b92cd46105f0494947c1a0642ff4e9ff7fdef596b3f89bd812817181a0738b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d636c6f7365642f666564657269636f2d62757361746f2f4d6f6465726e2d4350502d50726f6772616d6d696e673f7374796c653d666f722d7468652d6261646765" data-canonical-src="https://img.shields.io/github/issues-closed/federico-busato/Modern-CPP-Programming?style=for-the-badge">
    </a>
</p>
<h2 tabindex="-1" dir="auto">Author</h2>
<p dir="auto"><code>Federico Busato</code></p>
<ul dir="auto">
<li>Twitter: <a href="https://twitter.com/fedebusato" rel="nofollow">twitter.com/fedebusato</a></li>
<li>LinkedIn: <a href="https://www.linkedin.com/in/federico-busato/" rel="nofollow">www.linkedin.com/in/federico-busato/</a></li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Half Life:25th Anniversary Documentary (278 pts)]]></title>
            <link>https://www.youtube.com/watch?v=TbZ3HzvFEto</link>
            <guid>38444719</guid>
            <pubDate>Tue, 28 Nov 2023 11:40:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=TbZ3HzvFEto">https://www.youtube.com/watch?v=TbZ3HzvFEto</a>, See on <a href="https://news.ycombinator.com/item?id=38444719">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[PeerTube v6 is out, and powered by your ideas (276 pts)]]></title>
            <link>https://framablog.org/2023/11/28/peertube-v6-is-out-and-powered-by-your-ideas/</link>
            <guid>38443855</guid>
            <pubDate>Tue, 28 Nov 2023 09:07:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://framablog.org/2023/11/28/peertube-v6-is-out-and-powered-by-your-ideas/">https://framablog.org/2023/11/28/peertube-v6-is-out-and-powered-by-your-ideas/</a>, See on <a href="https://news.ycombinator.com/item?id=38443855">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wtr-content" data-bg="#FFFFFF" data-fg="#6a5687" data-width="5" data-mute="" data-fgopacity="0.75" data-mutedopacity="0.5" data-placement="top" data-placement-offset="0" data-content-offset="0" data-placement-touch="top" data-placement-offset-touch="0" data-transparent="1" data-shadow="1" data-touch="1" data-non-touch="1" data-comments="" data-commentsbg="#ffcece" data-location="page" data-mutedfg="#b49cd9" data-endfg="#f44813" data-rtl=""><p><a href="https://framablog.org/2023/11/28/peertube-v6-is-out-and-powered-by-your-ideas/?print=pdf" target="_blank"><img decoding="async" src="https://framablog.org/wp-content/plugins/pdf-print/images/pdf.png" alt="image_pdf" title="Afficher le PDF"></a><a href="https://framablog.org/2023/11/28/peertube-v6-is-out-and-powered-by-your-ideas/?print=print" target="_blank"><img decoding="async" src="https://framablog.org/wp-content/plugins/pdf-print/images/print.png" alt="image_print" title="Contenu imprimé"></a></p><p>It’s #givingtuesday, so we’re giving you PeerTube v6 today&nbsp;! <a href="https://joinpeertube.org/">PeerTube</a> is the software we develop for creators, media, institutions, educators… to manage their own video platform, as an alternative to YouTube and Twitch.</p>


<p>The sixth major version is being released today and we are very proud&nbsp;! It is the most ambitious one since we added peer-to-peer livestreaming. There is a good reason for that&nbsp;: we packed this v6 with features inspired by <a href="https://ideas.joinpeertube.org/">your ideas</a>&nbsp;!</p>
<p>We are so eager to present all the work we achieved that we’ll get right into it. But stay tuned&nbsp;: in two weeks, we’ll take more time to talk about PeerTube’s history, the state of this project and the great plans we have for its future&nbsp;!</p>
<figure id="attachment_32655"><a href="https://soutenir.framasoft.org/"><img fetchpriority="high" decoding="async" src="https://framablog.org/wp-content/uploads/2023/11/3-youtube-videoraptor-light-265x300.png" alt="Illustration of Videoraptor, an insectoid monster whose three heads bear the logos of YouTube, Vimeo and Twitch." width="265" height="300" srcset="https://framablog.org/wp-content/uploads/2023/11/3-youtube-videoraptor-light-265x300.png 265w, https://framablog.org/wp-content/uploads/2023/11/3-youtube-videoraptor-light-906x1024.png 906w, https://framablog.org/wp-content/uploads/2023/11/3-youtube-videoraptor-light-768x868.png 768w, https://framablog.org/wp-content/uploads/2023/11/3-youtube-videoraptor-light-1358x1536.png 1358w, https://framablog.org/wp-content/uploads/2023/11/3-youtube-videoraptor-light-1811x2048.png 1811w" sizes="(max-width: 265px) 100vw, 265px"></a><figcaption>Click to support us and help push back Videoraptor – Illustration CC-By <a href="https://www.peppercarrot.com/en/files/framasoft.html">David Revoy</a></figcaption></figure>
<h2 id="this-year-two-minor-updates-and-a-major-achievement">This year&nbsp;: two minor updates and a major achievement</h2>
<p>In 2023, and before preparing this major update, we released only two minor versions… but one of them brought to the table a major technical feature that will help democratize video hosting even more.</p>
<h3 id="march-2023-peertube-v5-1">March 2023&nbsp;: PeerTube v5.1</h3>
<p>You’ll get more details in <a href="https://joinpeertube.org/news/release-5.1">the news dedidacted to the 5.1 release</a>, so to keep it short, this version brought&nbsp;:</p>
<ul>
<li>an «&nbsp;asking for an account&nbsp;» feature, where instance moderators can <strong>manage and moderate news account requests</strong>&nbsp;;</li>
<li>a <strong>back-to-live button</strong>, so when you can lag behind during a livestream, you can go back to the direct</li>
<li>Improvements on the <strong>authentification plugin</strong>, to facilitate signing on with external credentials</li>
</ul>
<h3 id="june-2023-peertube-5-2-">June 2023&nbsp;: PeerTube 5.2…</h3>
<p>As you’ll find out in our <a href="https://joinpeertube.org/news/release-5.2">5.2 release blogpost</a>, there were some smaller but important new features such as&nbsp;:</p>
<ul>
<li>Adapting <strong>RSS feeds to podcast standards</strong>, so any podcast client could be able to read a PeerTube channel, for example</li>
<li>The option to <strong>set the privacy of a livestream replay</strong>, that way streamers can choose beforehand if the replay of their live will be <em>Public</em>, <em>Unlisted</em>, <em>Private</em> or <em>Internal</em></li>
<li>Improved mouse-free navigation&nbsp;: for those who prefer or need to <strong>navigate using their keyboard</strong></li>
<li>And <strong>upgrades in our documentation</strong> (it’s quite thorough&nbsp;: <a href="https://docs.joinpeertube.org/">check it out</a>&nbsp;!)</li>
</ul>
<h3 id="-with-a-major-feature-remote-transcoding">…with a major feature&nbsp;: Remote Transcoding</h3>
<p>But the game changer in this 5.2 release was the new <strong>remote transcoding feature</strong>.</p>
<p>When a creator uploads a video (or when they are streaming live), PeerTube needs to transform their video file into an efficient format. This task is called video transcoding, and it consumes lots of CPU power. PeerTube admins used to need (costly) big-CPU servers for a task that wasn’t permanent… until remote transcoding.</p>
<p>Remote transcoding allows PeerTube admins to deport some or all of their transcoding tasks to another, more powerful server, one that can be shared with other admins, for example.</p>
<p><strong>It makes the whole PeerTube administration cheaper, more resilient, more power-efficient</strong>… and opens a way of sharing resources between communities&nbsp;!</p>
<p>We want, once again to thank the NGI Entrust program and the NLnet foundation for the grant that helped us achieve such a technical improvement&nbsp;!</p>
<figure id="attachment_32632"><a href="https://soutenir.framasoft.org/"><img decoding="async" src="https://framablog.org/wp-content/uploads/2023/11/3-sepia-276x300.png" alt="Drawing of Sepia, PeerTube's octopus mascot. They are wearing a superhero cape, with the initials &quot;6&quot; on his chest." width="276" height="300" srcset="https://framablog.org/wp-content/uploads/2023/11/3-sepia-276x300.png 276w, https://framablog.org/wp-content/uploads/2023/11/3-sepia-944x1024.png 944w, https://framablog.org/wp-content/uploads/2023/11/3-sepia-768x833.png 768w, https://framablog.org/wp-content/uploads/2023/11/3-sepia.png 1338w" sizes="(max-width: 276px) 100vw, 276px"></a><figcaption>Click to support us and help Sepia reach their potential – Illustration CC-By <a href="https://www.peppercarrot.com/en/files/framasoft.html">David Revoy</a></figcaption></figure>
<h2 id="peertube-v6-powered-by-your-ideas-">PeerTube v6 is Based… (on your ideas)</h2>
<p>Enough about the past, let’s detail the features of this new major version. Note that, for this whole 2023 roadmap, we developed features suggested and upvoted by… you&nbsp;! Or at least by those of you who shared your ideas on <a href="https://ideas.joinpeertube.org/">our feedback website</a>.</p>
<h3 id="protect-your-videos-with-passwords-">Protect your videos with passwords&nbsp;!</h3>
<p>That was a very awaited feature. Password-protected videos can be used in lots of situations&nbsp;: to create exclusive content, mark a step in a pedagogical plan, share videos with people trusted by the ones you trust…</p>

<p>On their PeerTube account, creators can now set a single password when they upload, import or update the settings of their videos.</p>
<p>But with our REST API, admins and developers can take it a step further. They can set and store as many passwords as they want, thus easily give and revoke access to videos.</p>
<p>This feature was the work of Wicklow, during his internship with us.</p>
<h3 id="video-storyboard-preview-what-s-coming-">Video storyboard&nbsp;: preview what’s coming&nbsp;!</h3>
<p>If you like to peruse your videos online, you might be used to hover the progress bar with your mouse or finger. Usually, a preview of the frame appears as a thumbnail&nbsp;: that’s called a storyboard feature, and that’s now available in PeerTube&nbsp;!</p>

<p>Please note that as Storyboards are only generated when uploading (or importing) a video, they will only be available for new videos of instances that upgraded to v6…</p>
<p>Or you can ask, very kindly, to your admin(s) that they use the magical <code>npm run create-generate-storyboard-job</code> command (warning&nbsp;: this task might need some CPU power), and generate storyboards for older videos.</p>
<h3 id="upload-a-new-version-of-your-video">Upload a new version of your video&nbsp;!</h3>
<p>Sometimes, video creators want to update a video, to correct a mistake, offer new informations… or just to propose a better cut of their work&nbsp;!</p>
<p>Now, with PeerTube, they can upload and replace an older version of their video. Though the older video file will be permanently erased (no backsies&nbsp;!), creators will keep the same URL, title and infos, comments, stats, etc.</p>

<p>Obviously, such a feature requires trust between videomakers and admins, who don’t want to be responsible for a cute kitten video being «&nbsp;updated&nbsp;» into an awful advertisement for cat-hating groups.</p>
<p>That’s why such a feature will only be available if admins choose to enable it on their PeerTube platforms, and will display a «&nbsp;Video re-upload&nbsp;» tag on updated videos.</p>
<h3 id="get-chapters-in-your-videos-">Get chapters in your videos&nbsp;!</h3>
<p>Creators can now add chapters to their videos on PeerTube. In a video settings page, they’ll get a new «&nbsp;chapters&nbsp;» tab where they’ll only need to specify the timecode and title of each chapter for PeerTube to add it.</p>

<p>If they import their video from another platform (<em>cough</em> YouTube <em>cough</em>), PeerTube should automatically recognize and import chapters set on this distant video.</p>
<p>When chapters are set, markers will appear and segment the progress bar. Chapter titles will be displayed when you hover or touch one of those chapters segments.</p>
<h3 id="stress-tests-performance-and-config-recommandations">Stress tests, performance and config recommandations</h3>
<p>Last year, thanks to French indie journalist David Dufresne’s Au Poste&nbsp;! livestream show and his hoster Octopuce, we got a livestream stress test with more than 400 simultaneous viewers&nbsp;: <a href="https://www.octopuce.fr/test-de-charge-dun-peertube-en-live-avec-auposte/">see the report here on Octopuce’s blog[FR]</a>.</p>
<p>Such tests are really helpful to understand where we can improve PeerTube to reduce bottlenecks, improve performance, and give advice on the best configuration for a PeerTube server if an admin plans on getting a lot of traffic.</p>
<p>That’s why this year, we have decided to realize more tests, with a thousand simultaneous users simulated both in livestream and classic video streaming conditions. Lots of thanks and datalove to Octopuce for helping us deploy our test infrastructure.</p>
<p>We will soon publish a report with our conclusions and recommended server configurations depending on usecases (late 2023, early 2024). In the meantime, early tests motivated us to <strong>add many performances improvements</strong> into this v6, such as (brace yourselves for the technical terms)&nbsp;:</p>
<ul>
<li>Process unicast HTTP job in worker threads</li>
<li>Sign ActivityPub requests in worker threads</li>
<li>Optimize recommended videos HTTP request</li>
<li>Optimize videos SQL queries when filtering on lives or tags</li>
<li>Optimize /videos/{id}/views endpoint with many viewers</li>
<li>Add ability to disable PeerTube HTTP logs</li>
</ul>
<h3 id="-and-there-s-always-more-">…and there’s always more&nbsp;!</h3>
<p>A new major version always comes with its lot of changes, improvements, bugfixes, etc. You can read <a href="https://github.com/Chocobozzz/PeerTube/releases/tag/v6.0.0">the complete log here</a>, but here are the highlights&nbsp;:</p>
<ul>
<li>We needed to settle a technical debt&nbsp;: <strong>v6 removes support for WebTorrent to focus on HLS (with WebRTC P2P)</strong>. Both are technical bricks used to get peer-to-peer streaming in web browsers, but HLS is more fitted to what we are doing (and plan to do) with PeerTube</li>
<li>The <strong>video player is more efficient</strong>
<ul>
<li>It is not being rebuilt anymore every time the video changes</li>
<li>It keeps your watching settings (speed, fullscreen, etc.) when the video changes</li>
<li>It automatically adjust its size to match the video ratio</li>
</ul>
</li>
<li>We have <strong>improved SEO</strong>, to help videos hosted on a PeerTube platform appear higher in the search results of search engines</li>
<li>We worked a lot on <strong>improving PeerTube’s accessibility</strong> on many levels, to streamline the experience of people with disabilities.</li>
</ul>
<figure id="attachment_32657"><a href="https://soutenir.framasoft.org/"><img decoding="async" src="https://framablog.org/wp-content/uploads/2023/11/5-youtube-premium-yetube-light-279x300.png" alt="Illustration de Yetube, un monstre de type Yéti avec le logo de YouTube Premium." width="279" height="300" srcset="https://framablog.org/wp-content/uploads/2023/11/5-youtube-premium-yetube-light-279x300.png 279w, https://framablog.org/wp-content/uploads/2023/11/5-youtube-premium-yetube-light-951x1024.png 951w, https://framablog.org/wp-content/uploads/2023/11/5-youtube-premium-yetube-light-768x827.png 768w, https://framablog.org/wp-content/uploads/2023/11/5-youtube-premium-yetube-light-1427x1536.png 1427w, https://framablog.org/wp-content/uploads/2023/11/5-youtube-premium-yetube-light-1903x2048.png 1903w" sizes="(max-width: 279px) 100vw, 279px"></a><figcaption>Click to support us and help push Yetube back – CC-By Illustration <a href="https://www.peppercarrot.com/en/files/framasoft.html">David Revoy</a></figcaption></figure>
<h2 id="what-about-peertube-s-future-">What about PeerTube’s future&nbsp;?</h2>
<p>With YouTube waging war against adblockers, Twitch increasingly exploiting streamers, and everyone becoming more and more aware of the toxicity of this system… PeerTube is getting traction, recognition and a growing community.</p>
<p>We have so <strong>many announcements to make about the future we plan for PeerTube</strong>, that we will publish a separate news, in two weeks. We are also planning on hosting an «&nbsp;Ask Us Anything&nbsp;» livestream, to answer the questions you’d have about PeerTube.</p>
<p>Please stay tuned by subscribing to <a href="https://joinpeertube.org/news">PeerTube’s Newsletter</a>, following <a href="https://framapiaf.org/@peertube">PeerTube’s Mastodon account</a> or keeping an eye on the <a href="https://framablog.org/">Framablog</a>.</p>
<figure id="attachment_32914"><a href="https://soutenir.framasoft.org/"><img loading="lazy" decoding="async" src="https://framablog.org/wp-content/uploads/2023/11/sepia-1024x576.jpg" alt="Drawing in the style of a fighting video game, where the octopus of PeerTube and the monster of YouTube, Twitch and Vimeo go head to head." width="1024" height="576" srcset="https://framablog.org/wp-content/uploads/2023/11/sepia-1024x576.jpg 1024w, https://framablog.org/wp-content/uploads/2023/11/sepia-300x169.jpg 300w, https://framablog.org/wp-content/uploads/2023/11/sepia-768x432.jpg 768w, https://framablog.org/wp-content/uploads/2023/11/sepia-1536x864.jpg 1536w, https://framablog.org/wp-content/uploads/2023/11/sepia.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Click to support us and help Sepia push back Videoraptor – Illustration CC-By <a href="https://www.peppercarrot.com/en/files/framasoft.html">David Revoy</a></figcaption></figure>
<h2 id="thank-you-for-supporting-peertube-and-framasoft">Thank you for supporting PeerTube and Framasoft</h2>
<p>In the meantime, we want to remind you that all these developments were achieved by only one full-time payed developer, an intern, and a fabulous community (lots of datalove to Chocobozzz, Wicklow, and the many, many contributors&nbsp;: y’all are amazing&nbsp;!)</p>
<p>Framasoft being a French not-for-profit mainly funded by grassroots donations (75&nbsp;% of our yearly income comes from people like you and us), PeerTube development has been funded by two main sources&nbsp;:</p>
<ul>
<li>French-speaking FOSS enthusiasts</li>
<li>Grants from the NGI initiative, through NLnet (in 2021 &amp; 2023)</li>
</ul>
<p>If you are a non-French-speaking PeerTube aficionado, please consider <strong>supporting our work by <a href="https://soutenir.framasoft.org/">making a donation to Framasoft</a></strong>. It will greatly help us fund our many, many projects, and balance our 2024 budget.</p>
<p>Once again this year we need you, your support, your sharing to help us regain ground on the toxic GAFAM web and multiply the number of ethical digital spaces. So we’ve asked <a href="https://www.peppercarrot.com/fr/files/framasoft.html">David Revoy</a> to help us present this on our <a href="https://soutenir.framasoft.org/">support Framasoft</a> page, which we invite you to visit (because it’s beautiful) and above all to share as widely as possible&nbsp;:</p>
<p><a href="https://soutenir.framasoft.org/"><img loading="lazy" decoding="async" src="https://framablog.org/wp-content/uploads/2023/11/2023-11-28-Soutenir-Framasoft.png" alt="Screenshot of the Framasoft 2023 donation bar at 12% - €23575" width="1110" height="591" srcset="https://framablog.org/wp-content/uploads/2023/11/2023-11-28-Soutenir-Framasoft.png 1110w, https://framablog.org/wp-content/uploads/2023/11/2023-11-28-Soutenir-Framasoft-300x160.png 300w, https://framablog.org/wp-content/uploads/2023/11/2023-11-28-Soutenir-Framasoft-1024x545.png 1024w, https://framablog.org/wp-content/uploads/2023/11/2023-11-28-Soutenir-Framasoft-768x409.png 768w" sizes="(max-width: 1110px) 100vw, 1110px"></a></p>
<p><strong>If we are to balance our budget for 2024, we have five weeks to raise €176,425&nbsp;: we can’t do it without your help&nbsp;!</strong></p>
<p><a href="https://soutenir.framasoft.org/" target="_blank" rel="noopener"><strong>Support Framasoft</strong></a></p>
<p>Thanks again for supporting PeerTube,<br>
Framasoft’s team.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Is Knuth's TAOCP worth the time and effort? (2023) (199 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=38443668</link>
            <guid>38443668</guid>
            <pubDate>Tue, 28 Nov 2023 08:40:19 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=38443668">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="38443668">
      <td><span></span></td>      <td><center><a id="up_38443668" href="https://news.ycombinator.com/vote?id=38443668&amp;how=up&amp;goto=item%3Fid%3D38443668"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=38443668">Ask HN: Is Knuth's TAOCP worth the time and effort? (2023)</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_38443668">112 points</span> by <a href="https://news.ycombinator.com/user?id=toinewx">toinewx</a> <span title="2023-11-28T08:40:19"><a href="https://news.ycombinator.com/item?id=38443668">3 hours ago</a></span> <span id="unv_38443668"></span> | <a href="https://news.ycombinator.com/hide?id=38443668&amp;goto=item%3Fid%3D38443668">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Is%20Knuth%27s%20TAOCP%20worth%20the%20time%20and%20effort%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=38443668&amp;auth=cbb7e6c6f888ad6103e9191e4a503745836b0d15">favorite</a> | <a href="https://news.ycombinator.com/item?id=38443668">72&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>I'm starting the first volume.
Right at the start, Knuth introduces Induction Mathematical Proof, and more specifically tries to show that we can describe it as an `algorithm mathematical proof`.</p><p>I showed it to a friend who is quite good at math, and he told me the book may be trying too hard especially in the examples variety, and how it might not be needed for comprehension's sake.</p><p>Would you still recommend this book, and if yes, in what circumstances?</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Noyb files GDPR complaint against Meta over "Pay or Okay" (119 pts)]]></title>
            <link>https://noyb.eu/en/noyb-files-gdpr-complaint-against-meta-over-pay-or-okay</link>
            <guid>38443292</guid>
            <pubDate>Tue, 28 Nov 2023 07:20:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://noyb.eu/en/noyb-files-gdpr-complaint-against-meta-over-pay-or-okay">https://noyb.eu/en/noyb-files-gdpr-complaint-against-meta-over-pay-or-okay</a>, See on <a href="https://news.ycombinator.com/item?id=38443292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
                    

<p><strong>Today, <em>noyb </em>filed a complaint against Meta with the Austrian data protection authority. European users now have the “choice” to either consent to being tracked for personalized advertising – or pay up to €251.88 a year to retain their fundamental right to data protection on Instagram and Facebook. Not only is the cost unacceptable, but industry numbers suggest that only 3 percent of people want to be tracked – while more than 99 percent decide against a payment when faced with a “privacy fee”. If Meta gets away with this, competitors will soon follow in its footsteps. Given that the average phone has 35 apps installed, keeping your phone private could soon cost around € 8,815 a year.</strong></p>

<ul><li><a href="https://noyb.eu/sites/default/files/2023-11/Complaint%20-%20Meta%20Pay%20or%20Okay%20-%20REDACTED.pdf">Complaint with the Austrian data protection authority (automated machine-translation)</a></li>
</ul><p><strong>Another attempt to circumvent EU privacy laws.</strong> The European Court of Justice (CJEU) <a href="https://noyb.eu/en/cjeu-declares-metafacebooks-gdpr-approach-largely-illegal">ruled in July</a> that Meta’s handling of user data for personalized ads was illegal. But not only that: In January, the European Data Protection Board <a href="https://noyb.eu/en/breaking-meta-prohibited-use-personal-data-advertising">fined Meta €390 million</a> for this violation and ordered the company to obtain users’ consent, based on a <em>noyb </em><a href="https://noyb.eu/en/noybeu-filed-complaints-over-forced-consent-against-google-instagram-whatsapp-and-facebook">complaint from 2018</a>. In its next attempt to undermine EU law, Meta now wants to charge people for choosing a privacy-friendly setting. Since the beginning of November, Instagram and Facebook users have had to choose between paying up to €251.88 a year or having their personal data surveilled for targeted advertising.</p>

<p><strong>“Freely given” consent at a high price?</strong> Under EU law, consent to online tracking and personalized advertising is only valid if it is “freely given”. This is to ensure that users only give up their fundamental right to privacy if it is their genuine free will to do so. Meta has now implemented the exact opposite of a genuinely free choice: Facebook alone will introduce a “privacy fee” of up to €12.99 per month if users do not consent to their personal data being processed for targeted advertising. Each linked account (such as Instagram) will cost another €8, making a total of €251.88 a year for one person using Instagram and Facebook. By comparison: <a href="https://s21.q4cdn.com/399680738/files/doc_earnings/2023/q3/presentation/Earnings-Presentation-Q3-2023.pdf">Meta says</a> its average revenue per user in Europe between Q3 2022 and Q3 2023 was $16.79. This equates to an annual revenue of just €62,88 per user – and puts the monthly fee way out of proportion.</p>

<p>Felix Mikolasch, data protection lawyer at <em>noyb</em>: <em>“EU law requires that consent is the genuine free will of the user. Contrary to this law, Meta charges a “privacy fee” of up to €250 per year if anyone dares to exercise their fundamental right to data protection.”</em></p>

<p><strong>3 to 10 percent want personalized ads – but 99.9 percent consent.</strong> All available scientific research suggests that so-called “Pay or Okay” systems are the antithesis of free consent and fundamentally affect the “free will” of users. For example, <a href="https://arxiv.org/pdf/2309.11625.pdf">the CEO of the “Pay or Okay” provider contentpass</a> stated that 99,9 percent of visitors agree to tracking when faced with a € 1,99 fee. At the same time, <a href="https://noyb.eu/sites/default/files/2020-05/Gallup_Facebook_DE.pdf">objective surveys suggest</a> that only 3 to 10 percent of users want their personal data to be used for targeted advertising.</p>

<p>Max Schrems, Chairman of <em>noyb</em>:<em> “When 3 percent of people actually want to swim, but 99.9 percent end up in the water, every child knows that it wasn’t a “free” choice. It’s neither smart nor legal – it’s just pitiful how Meta continues to ignore EU law.”</em></p>

<p><strong>Data protection could soon cost €35,000 per family.</strong> If Meta is successful in defending this new approach, it is likely to set off a domino effect. Already now, <a href="https://techcrunch.com/2023/10/02/tiktok-begins-testing-4-99-ad-free-subscription-tier/">TikTok is reportedly testing</a> an ad-free subscription outside the US. Other app providers could follow in the near future, making online privacy unaffordable. <a href="https://www.thinkwithgoogle.com/marketing-strategies/app-and-mobile/average-number-of-apps-on-smartphones/">According to Google</a>, the average person has 35 apps installed on their smartphone. If all of these apps followed Meta’s lead and charged a similar fee, people would have to pay a “fundamental rights fee” of €8,815.80 a year. For a family of four, the price of data privacy would rise to €35,263.20 per year - more than the average full-time income in the EU. Obviously, these figures become even more extreme in EU Member States with lower average incomes.</p>

<p>Max Schrems:<em> “Fundamental rights are usually available to everyone. How many people would still exercise their right to vote if they had to pay € 250 to do so? There were times when fundamental rights were reserved for the rich. It seems Meta wants to take us back for more than a hundred years.”</em></p>

<p><strong>Privacy only for the rich. </strong>While this price is extremely high in general, it also completely ignores the very different income levels in EU countries – and the fact that <a href="https://ec.europa.eu/eurostat/documents/15234730/17582411/KS-HA-23-001-EN-N.pdf/5d783d9e-9cb3-897c-8360-5122563ae8f3?version=6.0&amp;t=1700579783008">21.6% of the EU population is at risk of poverty or social exclusion</a>. The complainant in this case, for example, is in financial distress and receives unemployment assistance. He simply cannot afford to pay another €250 a year, when he is already struggling to pay his rent.</p>

<p>Max Schrems:<em> “More than 20% of the EU population are already at risk of poverty. For the complainant in our case, as for many others, a ‘Pay or Okay’ system would mean paying the rent or having privacy.”</em></p>

<p><strong>The DPA should initiate an urgency procedure.</strong> Given the seriousness of the violations and the unusually high number of users affected, <em>noyb </em>urges the Austrian data protection authority to initiate an urgency procedure to stop the illegal processing. In addition, <em>noyb </em>suggests that the authority imposes a deterrent fine, making sure that no other company starts copying Meta’s approach.</p>
            
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Designing a SIMD Algorithm from Scratch (248 pts)]]></title>
            <link>https://mcyoung.xyz/2023/11/27/simd-base64/</link>
            <guid>38443253</guid>
            <pubDate>Tue, 28 Nov 2023 07:12:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mcyoung.xyz/2023/11/27/simd-base64/">https://mcyoung.xyz/2023/11/27/simd-base64/</a>, See on <a href="https://news.ycombinator.com/item?id=38443253">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p><span> <span> <a href="https://mcyoung.xyz/tags.html#rust">#rust</a> <a href="https://mcyoung.xyz/tags.html#optimization">#optimization</a> </span> <span> 2023-11-27 </span> </span></p> <p>Another explainer on a fun, esoteric topic: optimizing code with SIMD (single instruction multiple data, also sometimes called <em>vectorization</em>). Designing a good, fast, portable SIMD algorithm is not a simple matter and requires thinking a little bit like a circuit designer.</p> <p>Here’s the mandatory performance benchmark graph to catch your eye.</p> <p><img src="https://mcyoung.xyz/public/simd-img/graph.png" alt="perf perf perf"></p> <p>“SIMD” often gets thrown around as a buzzword by performance and HPC (high performance computing) nerds, but I don’t think it’s a topic that has very friendly introductions out there, for a lot of reasons.</p> <ul> <li>It’s not something you will really want to care about unless you think performance is cool.</li> <li>APIs for programming with SIMD in most programming languages are <em>garbage</em> (I’ll get into why).</li> <li>SIMD algorithms are hard to think about if you’re very procedural-programming-brained. A functional programming mindset can help a lot.</li> </ul> <p>This post is mostly about <a href="https://docs.rs/vb64/latest/vb64/"><code>vb64</code></a> (which stands for <em>v</em>ector <em>b</em>ase<em>64</em>), a base64 codec I wrote to see for myself if Rust’s <code>std::simd</code> library is any good, but it’s also an excuse to talk about SIMD in general.</p> <p>What <em>is</em> SIMD, anyways? Let’s dive in.</p> <p>If you want to skip straight to the writeup on <code>vb64</code>, click <a href="#parsing-with-simd">here</a>.</p> <h2 id="problems-with-physics"><a href="#problems-with-physics">Problems with Physics</a></h2> <p>Unfortunately, computers exist in the real world<sup>[citation-needed]</sup>, and are bound by the laws of nature. SIMD has relatively little to do with theoretical CS considerations, and everything to do with <em>physics</em>.</p> <p>In the infancy of modern computing, you could simply improve performance of existing programs by buying new computers. This is often incorrectly attributed to Moore’s law (the number of transistors on IC designs doubles every two years). Moore’s law still appears to hold as of 2023, but some time in the last 15 years the <a href="https://en.wikipedia.org/wiki/Dennard_scaling">Dennard scaling</a> effect broke down. This means that denser transistors eventually means increased power dissipation density. In simpler terms, we don’t know how to continue to increase the clock frequency of computers without literally <em>liquefying</em> them.</p> <p>So, since the early aughts, the hot new thing has been bigger core counts. Make your program more multi-threaded and it will run faster on bigger CPUs. This comes with synchronization overhead, since now the cores need to cooperate. All control flow, be it jumps, virtual calls, or synchronization will result in “stall”.</p> <p>The main causes of stall are <em>branches</em>, instructions that indicate code can take one of two possible paths (like an <code>if</code> statement), and <em>memory operations</em>. Branches include all control flow: <code>if</code> statements, loops, function calls, function returns, even <code>switch</code> statements in C. Memory operations are loads and stores, especially ones that are cache-unfriendly.</p> <h3 id="procedural-code-is-slow"><a href="#procedural-code-is-slow">Procedural Code Is Slow</a></h3> <p>Modern compute cores do not execute code line-by-line, because that would be very inefficient. Suppose I have this program:</p> <div><figure><pre><code data-lang="rust"><span>let</span> <span>a</span> <span>=</span> <span>x</span> <span>+</span> <span>y</span><span>;</span>
<span>let</span> <span>b</span> <span>=</span> <span>x</span> <span>^</span> <span>y</span><span>;</span>
<span>println!</span><span>(</span><span>"{a}, {b}"</span><span>);</span></code></pre></figure></div> <p>There’s no reason for the CPU to wait to finish computing <code>a</code> before it begins computing <code>b</code>; it does not depend on <code>a</code>, and while the add is being executed, the xor circuits are idle. Computers say “program order be damned” and issue the add for <code>a</code> and the xor for <code>b</code> simultaneously. This is called <em>instruction-level parallelism</em>, and dependencies that get in the way of it are often called <em>data hazards</em>.</p> <p>Of course, the Zen 2 in the machine I’m writing this with does not have one measly adder per core. It has dozens and dozens! The opportunities for parallelism are massive, as long as the compiler in your CPU’s execution pipeline can clear any data hazards in the way.</p> <p>The better the core can do this, the more it can saturate all of the “functional units” for things like arithmetic, and the more numbers it can crunch per unit time, approaching maximum utilization of the hardware. Whenever the compiler can’t do this, the execution pipeline stalls and your code is slower.</p> <p>Branches stall because they need to wait for the branch condition to be computed before fetching the next instruction (speculative execution is a somewhat iffy workaround for this). Memory operations stall because the data needs to physically arrive at the CPU, and the speed of light is finite in this universe.</p> <p>Trying to reduce stall by improving opportunities for single-core parallelism is not a new idea. Consider the not-so-humble GPU, whose purpose in life is to render images. Images are vectors of pixels (i.e., color values), and rendering operations tend to be highly local. For example, a convolution kernel for a Gaussian blur will be two or even three orders of magnitude smaller than the final image, lending itself to locality.</p> <p>Thus, GPUs are built for divide-and-conquer: they provide primitives for doing batched operations, and extremely limited control flow.</p> <p>“SIMD” is synonymous with “batching”. It stands for “single instruction, multiple data”: a single instruction dispatches parallel operations on multiple <em>lanes</em> of data. GPUs are the original SIMD machines.</p> <h2 id="lane-wise"><a href="#lane-wise">Lane-wise</a></h2> <p>“SIMD” and “vector” are often used interchangeably. The fundamental unit a SIMD instruction (or “vector instruction”) operates on is a vector: a fixed-size array of numbers that you primarily operate on component-wise These components are called <em>lanes</em>.</p> <p>SIMD vectors are usually quite small, since they need to fit into registers. For example, on my machine, the largest vectors are 256 bits wide. This is enough for 32 bytes (a <code>u8x32</code>), 4 double-precision floats (an <code>f64x8</code>), or all kinds of things in between.</p> <p><img src="https://mcyoung.xyz/public/simd-img/vectors.png" alt="some 256-bit vectors"></p> <p>Although this doesn’t seem like much, remember that offloading the overhead of keeping the pipeline saturated by a factor of 4x can translate to that big of a speedup in latency.</p> <h3 id="one-bit-lanes"><a href="#one-bit-lanes">One-bit Lanes</a></h3> <p>The simplest vector operations are bitwise: and, or, xor. Ordinary integers can be thought of as vectors themselves, with respect to the bitwise operations. That’s literally what “bitwise” means: lanes-wise with lanes that are one bit wide. An <code>i32</code> is, in this regard, an <code>i1x32</code>.</p> <p>In fact, as a warmup, let’s look at the problem of counting the number of 1 bits in an integer. This operation is called “population count”, or <code>popcnt</code>. If we view an <code>i32</code> as an <code>i1x32</code>, <code>popcnt</code> is just a fold or reduce operation:</p> <div><figure><pre><code data-lang="rust"><span>pub</span> <span>fn</span> <span>popcnt</span><span>(</span><span>mut</span> <span>x</span><span>:</span> <span>u32</span><span>)</span> <span>-&gt;</span> <span>u32</span> <span>{</span>
  <span>let</span> <span>mut</span> <span>bits</span> <span>=</span> <span>[</span><span>0</span><span>;</span> <span>32</span><span>];</span>
  <span>for</span> <span>(</span><span>i</span><span>,</span> <span>bit</span><span>)</span> <span>in</span> <span>bits</span><span>.iter_mut</span><span>()</span><span>.enumerate</span><span>()</span> <span>{</span>
    <span>*</span><span>bit</span> <span>=</span> <span>(</span><span>x</span> <span>&gt;&gt;</span> <span>i</span><span>)</span> <span>&amp;</span> <span>1</span><span>;</span>
  <span>}</span>
  <span>bits</span><span>.into_iter</span><span>()</span><span>.fold</span><span>(</span><span>0</span><span>,</span> <span>|</span><span>total</span><span>,</span> <span>bit</span><span>|</span> <span>total</span> <span>+</span> <span>bit</span><span>)</span>
<span>}</span></code></pre></figure></div> <p>In other words, we interpret the integer as an array of bits and then add the bits together to a 32-bit accumulator. Note that the accumulator needs to be higher precision to avoid overflow: accumulating into an <code>i1</code> (as with the <code>Iterator::reduce()</code> method) will only tell us whether the number of 1 bits is even or odd.</p> <p>Of course, this produces… comically bad code, frankly. We can do much better if we notice that we can <em>vectorize</em> the addition: first we add all of the adjacent pairs of bits together, then the pairs of pairs, and so on. This means the number of adds is logarithmic in the number of bits in the integer.</p> <p>Visually, what we do is we “unzip” each vector, shift one to line up the lanes, add them, and then repeat with lanes twice as big.</p> <p><img src="https://mcyoung.xyz/public/simd-img/popcnt.png" alt="first two popcnt merge steps"></p> <p>This is what that looks like in code.</p> <div><figure><pre><code data-lang="rust"><span>pub</span> <span>fn</span> <span>popcnt</span><span>(</span><span>mut</span> <span>x</span><span>:</span> <span>u32</span><span>)</span> <span>-&gt;</span> <span>u32</span> <span>{</span>
  <span>// View x as a i1x32, and split it into two vectors</span>
  <span>// that contain the even and odd bits, respectively.</span>
  <span>let</span> <span>even</span> <span>=</span> <span>x</span> <span>&amp;</span> <span>0x55555555</span><span>;</span> <span>// 0x5 == 0b0101.</span>
  <span>let</span> <span>odds</span> <span>=</span> <span>x</span> <span>&amp;</span> <span>0xaaaaaaaa</span><span>;</span> <span>// 0xa == 0b1010.</span>
  <span>// Shift odds down to align the bits, and then add them together.</span>
  <span>// We interpret x now as a i2x16. When adding, each two-bit</span>
  <span>// lane cannot overflow, because the value in each lane is</span>
  <span>// either 0b00 or 0b01.</span>
  <span>x</span> <span>=</span> <span>even</span> <span>+</span> <span>(</span><span>odds</span> <span>&gt;&gt;</span> <span>1</span><span>);</span>

  <span>// Repeat again but now splitting even and odd bit-pairs.</span>
  <span>let</span> <span>even</span> <span>=</span> <span>x</span> <span>&amp;</span> <span>0x33333333</span><span>;</span> <span>// 0x3 == 0b0011.</span>
  <span>let</span> <span>odds</span> <span>=</span> <span>x</span> <span>&amp;</span> <span>0xcccccccc</span><span>;</span> <span>// 0xc == 0b1100.</span>
  <span>// We need to shift by 2 to align, and now for this addition</span>
  <span>// we interpret x as a i4x8.</span>
  <span>x</span> <span>=</span> <span>even</span> <span>+</span> <span>(</span><span>odds</span> <span>&gt;&gt;</span> <span>2</span><span>);</span>

  <span>// Again. The pattern should now be obvious.</span>
  <span>let</span> <span>even</span> <span>=</span> <span>x</span> <span>&amp;</span> <span>0x0f0f0f0f</span><span>;</span> <span>// 0x0f == 0b00001111.</span>
  <span>let</span> <span>odds</span> <span>=</span> <span>x</span> <span>&amp;</span> <span>0xf0f0f0f0</span><span>;</span> <span>// 0xf0 == 0b11110000.</span>
  <span>x</span> <span>=</span> <span>even</span> <span>+</span> <span>(</span><span>odds</span> <span>&gt;&gt;</span> <span>4</span><span>);</span> <span>// i8x4</span>

  <span>let</span> <span>even</span> <span>=</span> <span>x</span> <span>&amp;</span> <span>0x00ff00ff</span><span>;</span>
  <span>let</span> <span>odds</span> <span>=</span> <span>x</span> <span>&amp;</span> <span>0xff00ff00</span><span>;</span>
  <span>x</span> <span>=</span> <span>even</span> <span>+</span> <span>(</span><span>odds</span> <span>&gt;&gt;</span> <span>8</span><span>);</span>  <span>// i16x2</span>

  <span>let</span> <span>even</span> <span>=</span> <span>x</span> <span>&amp;</span> <span>0x0000ffff</span><span>;</span>
  <span>let</span> <span>odds</span> <span>=</span> <span>x</span> <span>&amp;</span> <span>0xffff0000</span><span>;</span>
  <span>// Because the value of `x` is at most 32, although we interpret this as a</span>
  <span>// i32x1 add, we could get away with just one e.g. i16 add.</span>
  <span>x</span> <span>=</span> <span>even</span> <span>+</span> <span>(</span><span>odds</span> <span>&gt;&gt;</span> <span>16</span><span>);</span>

  <span>x</span> <span>// Done. All bits have been added.</span>
<span>}</span></code></pre></figure></div> <p>This still won’t optimize down to a <code>popcnt</code> instruction, of course. The search scope for such a simplification is in the regime of superoptimizers. However, the generated code is small and fast, which is why this is the ideal implementation of <code>popcnt</code> for systems without such an instruction.</p> <p>It’s <em>especially</em> nice because it is implementable for e.g. <code>u64</code> with only one more reduction step (remember: it’s <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(\log n)</annotation></semantics></math></span></span></span>!), and does not at any point require a full <code>u64</code> addition.</p> <p>Even though this is “just” using scalars, divide-and-conquer approaches like this are the bread and butter of the SIMD programmer.</p> <h3 id="scaling-up-operations-on-real-vectors"><a href="#scaling-up-operations-on-real-vectors">Scaling Up: Operations on Real Vectors</a></h3> <p>Proper SIMD vectors provide more sophisticated semantics than scalars do, particularly because there is more need to provide replacements for things like control flow. Remember, control flow is slow!</p> <p>What’s actually available is highly dependent on the architecture you’re compiling to (more on this later), but the way vector instruction sets are usually structured is something like this.</p> <p>We have <em>vector registers</em> that are kind of like really big general-purpose registers. For example, on x86, most “high performance” cores (like my Zen 2) implement AVX2, which provides 256 bit <code>ymm</code> vectors. The registers themselves do not have a “lane count”; that is specified by the instructions. For example, the “vector byte add instruction” interprets the register as being divided into eight-byte lanes and adds them. The corresponding x86 instruction is <code>vpaddb</code>, which interprets a <code>ymm</code> as an <code>i8x32</code>.</p> <p>The operations you usually get are:</p> <ol> <li> <p>Bitwise operations. These don’t need to specify a lane width because it’s always implicitly <code>1</code>: they’re <em>bit</em>wise.</p> </li> <li> <p>Lane-wise arithmetic. This is addition, subtraction, multiplication, division (both int and float), and shifts<sup id="fnref:shifts-are-arithmetic" role="doc-noteref"><a href="#fn:shifts-are-arithmetic" rel="footnote">1</a></sup> (int only). Lane-wise min and max are also common. These require specifying a lane width. Typically the smallest number of lanes is two or four.</p> </li> <li> <p>Lane-wise compare. Given <code>a</code> and <code>b</code>, we can create a new <em>mask vector</em> <code>m</code> such that <code>m[i] = a[i] &lt; b[i]</code> (or any other comparison operation). A mask vector’s lanes contain boolean values with an unusual bit-pattern: all-zeros (for false) or all-ones (for true)<sup id="fnref:minus-true" role="doc-noteref"><a href="#fn:minus-true" rel="footnote">2</a></sup>.</p> <ul> <li>Masks can be used to select between two vectors: for example, given <code>m</code>, <code>x</code>, and <code>y</code>, you can form a fourth vector <code>z</code> such that <code>z[i] = m[i] ? a[i] : b[i]</code>.</li> </ul> </li> <li> <p>Shuffles (sometimes called swizzles). Given <code>a</code> and <code>x</code>, create a third vector <code>s</code> such that <code>s[i] = a[x[i]]</code>. <code>a</code> is used as a lookup table, and <code>x</code> as a set of indices. Out of bounds produces a special value, usually zero. This emulates parallelized array access without needing to actually touch RAM (RAM is extremely slow).</p> <ul> <li>Often there is a “shuffle2” or “riffle” operation that allows taking elements from one of two vectors. Given <code>a</code>, <code>b</code>, and <code>x</code>, we now define <code>s</code> as being <code>s[i] = (a ++ b)[x[i]]</code>, where <code>a ++ b</code> is a double-width concatenation. How this is actually implemented depends on architecture, and it’s easy to build out of single shuffles regardless.</li> </ul> </li> </ol> <p>(1) and (2) are ordinary number crunching. Nothing deeply special about them.</p> <p>The comparison and select operations in (3) are intended to help SIMD code stay “branchless”. Branchless code is written such that it performs the same operations regardless of its inputs, and relies on the properties of those operations to produce correct results. For example, this might mean taking advantage of identities like <code>x * 0 = 0</code> and <code>a ^ b ^ a = b</code> to discard “garbage” results.</p> <p>The shuffles described in (4) are much more powerful than meets the eye.</p> <p>For example, “broadcast” (sometimes called “splat”) makes a vector whose lanes are all the same scalar, like Rust’s <code>[42; N]</code> array literal. A broadcast can be expressed as a shuffle: create a vector with the desired value in the first lane, and then shuffle it with an index vector of <code>[0, 0, ...]</code>.</p> <p><img src="https://mcyoung.xyz/public/simd-img/broadcast.png" alt="diagram of a broadcast"></p> <p>“Interleave” (also called “zip” or “pack”) takes two vectors <code>a</code> and <code>b</code> and creates two new vectors <code>c</code> and <code>d</code> whose lanes are alternating lanes from <code>a</code> and <code>b</code>. If the lane count is <code>n</code>, then <code>c = [a[0], b[0], a[1], b[1], ...]</code> and <code>d = [a[n/2], b[n/2], a[n/2 + 1], b[n/2 + 1], ...]</code>. This can also be implemented as a shuffle2, with shuffle indices of <code>[0, n, 1, n + 1, ...]</code>. “Deinterleave” (or “unzip”, or “unpack”) is the opposite operation: it interprets a pair of vectors as two halves of a larger vector of pairs, and produces two new vectors consisting of the halves of each pair.</p> <p>Interleave can also be interpreted as taking a <code>[T; N]</code>, transmuting it to a <code>[[T; N/2]; 2]</code>, performing a matrix transpose to turn it into a <code>[[T; 2]; N/2]</code>, and then transmuting that back to <code>[T; N]</code> again. Deinterleave is the same but it transmutes to <code>[[T; 2]; N/2]</code> first.</p> <p><img src="https://mcyoung.xyz/public/simd-img/interleave.png" alt="diagram of a interleave"></p> <p>“Rotate” takes a vector <code>a</code> with <code>n</code> lanes and produces a new vector <code>b</code> such that <code>b[i] = a[(i + j) % n]</code>, for some chosen integer <code>j</code>. This is yet another shuffle, with indices <code>[j, j + 1, ..., n - 1, 0, 1, ... j - 1]</code>.</p> <p><img src="https://mcyoung.xyz/public/simd-img/rotate.png" alt="diagram of a rotate"></p> <p>Shuffles are worth trying to wrap your mind around. SIMD programming is all about reinterpreting larger-than-an-integer-sized blocks of data as smaller blocks of varying sizes, and shuffling is important for getting data into the right “place”.</p> <h3 id="intrinsics-and-instruction-selection"><a href="#intrinsics-and-instruction-selection">Intrinsics and Instruction Selection</a></h3> <p>Earlier, I mentioned that what you get varies by architecture. This section is basically a giant footnote.</p> <p>So, there’s two big factors that go into this.</p> <ol> <li>We’ve learned over time which operations tend to be most useful to programmers. x86 might have something that ARM doesn’t because it “seemed like a good idea at the time” but turned out to be kinda niche.</li> <li>Instruction set extensions are often market differentiators, even within the same vendor. Intel has AVX-512, which provides even more sophisticated instructions, but it’s only available on high-end server chips, because it makes manufacturing more expensive.</li> </ol> <p>Toolchains generalize different extensions as “target features”. Features can be detected at runtime through architecture-specific magic. On Linux, the <code>lscpu</code> command will list what features the CPU advertises that it recognizes, which correlate with the names of features that e.g. LLVM understands. What features are enabled for a particular function affects how LLVM compiles it. For example, LLVM will only emit <code>ymm</code>-using code when compiling with <code>+avx2</code>.</p> <p>So how do you write portable SIMD code? On the surface, the answer is mostly “you don’t”, but it’s more complicated than that, and for that we need to understand how the later parts of a compiler works.</p> <p>When a user requests an add by writing <code>a + b</code>, how should I decide which instruction to use for it? This seems like a trick question… <em>just</em> an <code>add</code> right? On x86, even this isn’t so easy, since you have a choice between the actual <code>add</code> instruction, or a <code>lea</code> instruction (which, among other things, preserves the <code>rflags</code> register). This question becomes more complicated for more sophisticated operations. This general problem is called <em>instruction selection</em>.</p> <p>Because which “target features” are enabled affects which instructions are available, they affect instruction selection. When I went over operations “typically available”, this means that compilers will usually be able to select good choices of instructions for them on most architectures.</p> <p>Compiling with something like <code>-march=native</code> or <code>-Ctarget-cpu=native</code> gets you “the best” code possible for the machine you’re building on, but it might not be portable<sup id="fnref:abi" role="doc-noteref"><a href="#fn:abi" rel="footnote">3</a></sup> to different processors. Gentoo was quite famous for building packages from source on user machines to take advantage of this (not to mention that they loved using <code>-O3</code>, which mostly exists to slow down build times with little benefit).</p> <p>There is also runtime feature detection, where a program decides which version of a function to call at runtime by asking the CPU what it supports. Code deployed on heterogenous devices (like cryptography libraries) often make use of this. Doing this correctly is very hard and something I don’t particularly want to dig deeply into here.</p> <p>The situation is made worse by the fact that in C++, you usually write SIMD code using “intrinsics”, which are special functions with inscrutable names like <code>_mm256_cvtps_epu32</code> that represent a low-level operation in a specific instruction set (this is a float to int cast from AVX2). Intrinsics are defined by hardware vendors, but don’t necessarily map down to single instructions; the compiler can still optimize these instructions by merging, deduplication, and through instruction selection.</p> <p>As a result you wind up writing the same code multiple times for different instruction sets, with only minor maintainability benefits over writing assembly.</p> <p>The alternative is a portable SIMD library, which does some instruction selection behind the scenes at the library level but tries to rely on the compiler for most of the heavy-duty work. For a long time I was skeptical that this approach would actually produce good, competitive code, which brings us to the actual point of this article: using Rust’s portable SIMD library to implement a somewhat fussy algorithm, and measuring performance.</p> <h2 id="parsing-with-simd"><a href="#parsing-with-simd">Parsing with SIMD</a></h2> <p>Let’s design a SIMD implementation for a well-known algorithm. Although it doesn’t look like it at first, the power of shuffles makes it possible to parse text with SIMD. And this parsing can be very, very fast.</p> <p>In this case, we’re going to implement base64 decoding. To review, base64 is an encoding scheme for arbitrary binary data into ASCII. We interpret a byte slice as a bit vector, and divide it into six-bit chunks called <em>sextets</em>. Then, each sextet from 0 to 63 is mapped to an ASCII character:</p> <ol> <li><code>0</code> to <code>25</code> go to <code>'A'</code> to <code>'Z'</code>.</li> <li><code>26</code> to <code>51</code> go to <code>'a'</code> to <code>'z'</code>.</li> <li><code>52</code> to <code>61</code> go to <code>'0'</code> to <code>'9'</code>.</li> <li><code>62</code> goes to <code>+</code>.</li> <li><code>63</code> goes to <code>/</code>.</li> </ol> <p>There <em>are</em> other variants of base64, but the bulk of the complexity is the same for each variant.</p> <p>There are a few basic pitfalls to keep in mind.</p> <ol> <li> <p>Base64 is a “big endian” format: specifically, the bits in each byte are big endian. Because a sextet can span only parts of a byte, this distinction is important.</p> </li> <li> <p>We need to beware of cases where the input length is not divisible by 4; ostensibly messages should be padded with <code>=</code> to a multiple of 4, but it’s easy to just handle messages that aren’t padded correctly.</p> </li> </ol> <p>The length of a decoded message is given by this function:</p> <div><figure><pre><code data-lang="rust"><span>fn</span> <span>decoded_len</span><span>(</span><span>input</span><span>:</span> <span>usize</span><span>)</span> <span>-&gt;</span> <span>usize</span> <span>{</span>
  <span>input</span> <span>/</span> <span>4</span> <span>*</span> <span>3</span> <span>+</span> <span>match</span> <span>input</span> <span>%</span> <span>4</span> <span>{</span>
    <span>1</span> <span>|</span> <span>2</span> <span>=&gt;</span> <span>1</span><span>,</span>
    <span>3</span> <span>=&gt;</span> <span>2</span><span>,</span>
    <span>_</span> <span>=&gt;</span> <span>0</span><span>,</span>
  <span>}</span>
<span>}</span></code></pre></figure></div> <p>Given all this, the easiest way to implement base64 is something like this.</p> <div><figure><pre><code data-lang="rust"><span>fn</span> <span>decode</span><span>(</span><span>data</span><span>:</span> <span>&amp;</span><span>[</span><span>u8</span><span>],</span> <span>out</span><span>:</span> <span>&amp;</span><span>mut</span> <span>Vec</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>(),</span> <span>Error</span><span>&gt;</span> <span>{</span>
  <span>// Tear off at most two trailing =.</span>
  <span>let</span> <span>data</span> <span>=</span> <span>match</span> <span>data</span> <span>{</span>
    <span>[</span><span>p</span> <span>@</span> <span>..</span><span>,</span> <span>b'='</span><span>,</span> <span>b'='</span><span>]</span> <span>|</span> <span>[</span><span>p</span> <span>@</span> <span>..</span><span>,</span> <span>b'='</span><span>]</span> <span>|</span> <span>p</span> <span>=&gt;</span> <span>p</span><span>,</span>
  <span>};</span>

  <span>// Split the input into chunks of at most 4 bytes.</span>
  <span>for</span> <span>chunk</span> <span>in</span> <span>data</span><span>.chunks</span><span>(</span><span>4</span><span>)</span> <span>{</span>
    <span>let</span> <span>mut</span> <span>bytes</span> <span>=</span> <span>0u32</span><span>;</span>
    <span>for</span> <span>&amp;</span><span>byte</span> <span>in</span> <span>chunk</span> <span>{</span>
      <span>// Translate each ASCII character into its corresponding</span>
      <span>// sextet, or return an error.</span>
      <span>let</span> <span>sextet</span> <span>=</span> <span>match</span> <span>byte</span> <span>{</span>
        <span>b'A'</span><span>..=</span><span>b'Z'</span> <span>=&gt;</span> <span>byte</span> <span>-</span> <span>b'A'</span><span>,</span>
        <span>b'a'</span><span>..=</span><span>b'z'</span> <span>=&gt;</span> <span>byte</span> <span>-</span> <span>b'a'</span> <span>+</span> <span>26</span><span>,</span>
        <span>b'0'</span><span>..=</span><span>b'9'</span> <span>=&gt;</span> <span>byte</span> <span>-</span> <span>b'0'</span> <span>+</span> <span>52</span><span>,</span>
        <span>b'+'</span> <span>=&gt;</span> <span>62</span><span>,</span>
        <span>b'/'</span> <span>=&gt;</span> <span>63</span><span>,</span>
        <span>_</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span><span>Error</span><span>(</span><span>...</span><span>)),</span>
      <span>};</span>

      <span>// Append the sextet to the temporary buffer.</span>
      <span>bytes</span> <span>&lt;&lt;=</span> <span>6</span><span>;</span>
      <span>bytes</span> <span>|</span><span>=</span> <span>sextet</span> <span>as</span> <span>u32</span><span>;</span>
    <span>}</span>

    <span>// Shift things so the actual data winds up at the</span>
    <span>// top of `bytes`.</span>
    <span>bytes</span> <span>&lt;&lt;=</span> <span>32</span> <span>-</span> <span>6</span> <span>*</span> <span>chunk</span><span>.len</span><span>();</span>

    <span>// Append the decoded data to `out`, keeping in mind that</span>
    <span>// `bytes` is big-endian encoded.</span>
    <span>let</span> <span>decoded</span> <span>=</span> <span>decoded_len</span><span>(</span><span>chunk</span><span>.len</span><span>());</span>
    <span>out</span><span>.extend_from_slice</span><span>(</span><span>&amp;</span><span>bytes</span><span>.to_be_bytes</span><span>()[</span><span>..</span><span>decoded</span><span>]);</span>
  <span>}</span>

  <span>Ok</span><span>(())</span>
<span>}</span></code></pre></figure></div> <p>So, what’s the process of turning this into a SIMD version? We want to follow one directive with inexorable, robotic dedication.</p> <p><strong>Eliminate all branches.</strong></p> <p>This is not completely feasible, since the input is of variable length. But we can try. There are several branches in this code:</p> <ol> <li>The <code>for chunk in</code> line. This one is is the length check: it checks if there is any data left to process.</li> <li>The <code>for &amp;byte in</code> line. This is the hottest loop: it branches once per input byte.</li> <li>The <code>match byte</code> line is several branches, to determine which of the five “valid” match arms we land in.</li> <li>The <code>return Err</code> line. Returning in a hot loop is extra control flow, which is not ideal.</li> <li>The call to <code>decoded_len</code> contains a <code>match</code>, which generates branches.</li> <li>The call to <code>Vec::extend_from_slice</code>. This contains not just branches, but potential calls into the allocator. Extremely slow.</li> </ol> <p>(5) is the easiest to deal with. The <code>match</code> is mapping the values <code>0, 1, 2, 3</code> to <code>0, 1, 1, 2</code>. Call this function <code>f</code>. Then, the sequence given by <code>x - f(x)</code> is <code>0, 0, 1, 1</code>. This just happens to equal <code>x / 2</code> (or <code>x &gt;&gt; 1</code>), so we can write a completely branchless version of <code>decoded_len</code> like so.</p> <div><figure><pre><code data-lang="rust"><span>pub</span> <span>fn</span> <span>decoded_len</span><span>(</span><span>input</span><span>:</span> <span>usize</span><span>)</span> <span>-&gt;</span> <span>usize</span> <span>{</span>
  <span>let</span> <span>mod4</span> <span>=</span> <span>input</span> <span>%</span> <span>4</span><span>;</span>
  <span>input</span> <span>/</span> <span>4</span> <span>*</span> <span>3</span> <span>+</span> <span>(</span><span>mod4</span> <span>-</span> <span>mod4</span> <span>/</span> <span>2</span><span>)</span>
<span>}</span></code></pre></figure></div> <p>That’s one branch eliminated<sup id="fnref:why-cant-llvm-do-it" role="doc-noteref"><a href="#fn:why-cant-llvm-do-it" rel="footnote">4</a></sup>. ✅</p> <p>The others will not prove so easy. Let’s turn our attention to the innermost loop next, branches (2), (3), and (4).</p> <h3 id="the-hottest-loop"><a href="#the-hottest-loop">The Hottest Loop</a></h3> <p>The superpower of SIMD is that because you operate on so much data at a time, you can unroll the loop so hard it becomes branchless.</p> <p>The insight is this: we want to load at most four bytes, do something to them, and then spit out at most three decoded bytes. While doing this operation, we may encounter a syntax error so we need to report that somehow.</p> <p>Here’s some facts we can take advantage of.</p> <ol> <li>We don’t need to figure out how many bytes are in the “output” of the hot loop: our handy branchless <code>decoded_len()</code> does that for us.</li> <li>Invalid base64 is extremely rare. We want that syntax error to cost as little as possible. If the user still cares about which byte was the problem, they can scan the input for it after the fact.</li> <li><code>A</code> is zero in base64. If we’re parsing a truncated chunk, padding it with <code>A</code> won’t change the value<sup id="fnref:pad-with-A" role="doc-noteref"><a href="#fn:pad-with-A" rel="footnote">5</a></sup>.</li> </ol> <p>This suggests an interface for the body of the “hottest loop”. We can factor it out as a separate function, and simplify since we can assume our input is always four bytes now.</p> <div><figure><pre><code data-lang="rust"><span>fn</span> <span>decode_hot</span><span>(</span><span>ascii</span><span>:</span> <span>[</span><span>u8</span><span>;</span> <span>4</span><span>])</span> <span>-&gt;</span> <span>([</span><span>u8</span><span>;</span> <span>3</span><span>],</span> <span>bool</span><span>)</span> <span>{</span>
  <span>let</span> <span>mut</span> <span>bytes</span> <span>=</span> <span>0u32</span><span>;</span>
  <span>let</span> <span>mut</span> <span>ok</span> <span>=</span> <span>true</span><span>;</span>
  <span>for</span> <span>byte</span> <span>in</span> <span>ascii</span> <span>{</span>
    <span>let</span> <span>sextet</span> <span>=</span> <span>match</span> <span>byte</span> <span>{</span>
      <span>b'A'</span><span>..=</span><span>b'Z'</span> <span>=&gt;</span> <span>byte</span> <span>-</span> <span>b'A'</span><span>,</span>
      <span>b'a'</span><span>..=</span><span>b'z'</span> <span>=&gt;</span> <span>byte</span> <span>-</span> <span>b'a'</span> <span>+</span> <span>26</span><span>,</span>
      <span>b'0'</span><span>..=</span><span>b'9'</span> <span>=&gt;</span> <span>byte</span> <span>-</span> <span>b'0'</span> <span>+</span> <span>52</span><span>,</span>
      <span>b'+'</span> <span>=&gt;</span> <span>62</span><span>,</span>
      <span>b'/'</span> <span>=&gt;</span> <span>63</span><span>,</span>
      <span>_</span> <span>=&gt;</span> <span>!</span><span>0</span><span>,</span>
    <span>};</span>

    <span>bytes</span> <span>&lt;&lt;=</span> <span>6</span><span>;</span>
    <span>bytes</span> <span>|</span><span>=</span> <span>sextet</span> <span>as</span> <span>u32</span><span>;</span>
    <span>ok</span> <span>|</span><span>=</span> <span>byte</span> <span>==</span> <span>!</span><span>0</span><span>;</span>
  <span>}</span>

  <span>// This is the `to_be_bytes()` call.</span>
  <span>let</span> <span>[</span><span>b1</span><span>,</span> <span>b2</span><span>,</span> <span>b3</span><span>,</span> <span>_</span><span>]</span> <span>=</span> <span>bytes</span><span>.to_le_bytes</span><span>();</span>
  <span>([</span><span>b3</span><span>,</span> <span>b2</span><span>,</span> <span>b1</span><span>],</span> <span>ok</span><span>)</span>
<span>}</span>

<span>// In decode()...</span>
<span>for</span> <span>chunk</span> <span>in</span> <span>data</span><span>.chunks</span><span>(</span><span>4</span><span>)</span> <span>{</span>
  <span>let</span> <span>mut</span> <span>ascii</span> <span>=</span> <span>[</span><span>b'A'</span><span>;</span> <span>4</span><span>];</span>
  <span>ascii</span><span>[</span><span>..</span><span>chunk</span><span>.len</span><span>()]</span><span>.copy_from_slice</span><span>(</span><span>chunk</span><span>);</span>

  <span>let</span> <span>(</span><span>bytes</span><span>,</span> <span>ok</span><span>)</span> <span>=</span> <span>decode_hot</span><span>(</span><span>ascii</span><span>);</span>
  <span>if</span> <span>!</span><span>ok</span> <span>{</span>
    <span>return</span> <span>Err</span><span>(</span><span>Error</span><span>)</span>
  <span>}</span>

  <span>let</span> <span>len</span> <span>=</span> <span>decoded_len</span><span>(</span><span>chunk</span><span>.len</span><span>());</span>
  <span>out</span><span>.extend_from_slice</span><span>(</span><span>&amp;</span><span>bytes</span><span>[</span><span>..</span><span>decoded</span><span>]);</span>
<span>}</span></code></pre></figure></div> <p>You’re probably thinking: why not return <code>Option&lt;[u8; 3]&gt;</code>? Returning an enum will make it messier to eliminate the <code>if !ok</code> branch later on (which we will!). We want to write branchless code, so let’s focus on finding a way of producing that three-byte output without needing to do early returns.</p> <p>Now’s when we want to start talking about vectors rather than arrays, so let’s try to rewrite our function as such.</p> <div><figure><pre><code data-lang="rust"><span>fn</span> <span>decode_hot</span><span>(</span><span>ascii</span><span>:</span> <span>Simd</span><span>&lt;</span><span>u8</span><span>,</span> <span>4</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>(</span><span>Simd</span><span>&lt;</span><span>u8</span><span>,</span> <span>4</span><span>&gt;</span><span>,</span> <span>bool</span><span>)</span> <span>{</span>
  <span>unimplemented!</span><span>()</span>
<span>}</span></code></pre></figure></div> <p>Note that the output is now four bytes, not three. SIMD lane counts need to be powers of two, and that last element will never get looked at, so we don’t need to worry about what winds up there.</p> <p>The callsite also needs to be tweaked, but only slightly, because <code>Simd&lt;u8, 4&gt;</code> is <code>From&lt;[u8; 4]&gt;</code>.</p> <h3 id="ascii-to-sextet"><a href="#ascii-to-sextet">ASCII to Sextet</a></h3> <p>Let’s look at the first part of the <code>for byte in ascii</code> loop. We need to map each lane of the <code>Simd&lt;u8, 4&gt;</code> to the corresponding sextet, and somehow signal which ones are invalid. First, notice something special about the <code>match</code>: almost every arm can be written as <code>byte - C</code> for some constant <code>C</code>. The non-range case looks a little silly, but humor me:</p> <div><figure><pre><code data-lang="rust"><span>let</span> <span>sextet</span> <span>=</span> <span>match</span> <span>byte</span> <span>{</span>
  <span>b'A'</span><span>..=</span><span>b'Z'</span> <span>=&gt;</span> <span>byte</span> <span>-</span> <span>b'A'</span><span>,</span>
  <span>b'a'</span><span>..=</span><span>b'z'</span> <span>=&gt;</span> <span>byte</span> <span>-</span> <span>b'a'</span> <span>+</span> <span>26</span><span>,</span>
  <span>b'0'</span><span>..=</span><span>b'9'</span> <span>=&gt;</span> <span>byte</span> <span>-</span> <span>b'0'</span> <span>+</span> <span>52</span><span>,</span>
  <span>b'+'</span>        <span>=&gt;</span> <span>byte</span> <span>-</span> <span>b'+'</span> <span>+</span> <span>62</span><span>,</span>
  <span>b'/'</span>        <span>=&gt;</span> <span>byte</span> <span>-</span> <span>b'/'</span> <span>+</span> <span>63</span><span>,</span>
  <span>_</span> <span>=&gt;</span> <span>!</span><span>0</span><span>,</span>
<span>};</span></code></pre></figure></div> <p>So, it should be sufficient to build a vector <code>offsets</code> that contains the appropriate constant <code>C</code> for each lane, and then <code>let sextets = ascii - offsets;</code></p> <p>How can we build <code>offsets</code>? Using compare-and-select.</p> <div><figure><pre><code data-lang="rust"><span>// A lane-wise version of `x &gt;= start &amp;&amp; x &lt;= end`.</span>
<span>fn</span> <span>in_range</span><span>(</span><span>bytes</span><span>:</span> <span>Simd</span><span>&lt;</span><span>u8</span><span>,</span> <span>4</span><span>&gt;</span><span>,</span> <span>start</span><span>:</span> <span>u8</span><span>,</span> <span>end</span><span>:</span> <span>u8</span><span>)</span> <span>-&gt;</span> <span>Mask</span><span>&lt;</span><span>i8</span><span>,</span> <span>4</span><span>&gt;</span> <span>{</span>
  <span>bytes</span><span>.simd_ge</span><span>(</span><span>Simd</span><span>::</span><span>splat</span><span>(</span><span>start</span><span>))</span> <span>&amp;</span> <span>bytes</span><span>.simd_le</span><span>(</span><span>Simd</span><span>::</span><span>splat</span><span>(</span><span>end</span><span>))</span>
<span>}</span>

<span>// Create masks for each of the five ranges.</span>
<span>// Note that these are disjoint: for any two masks, m1 &amp; m2 == 0.</span>
<span>let</span> <span>uppers</span> <span>=</span> <span>in_range</span><span>(</span><span>ascii</span><span>,</span> <span>b'A'</span><span>,</span> <span>b'Z'</span><span>);</span>
<span>let</span> <span>lowers</span> <span>=</span> <span>in_range</span><span>(</span><span>ascii</span><span>,</span> <span>b'a'</span><span>,</span> <span>b'z'</span><span>);</span>
<span>let</span> <span>digits</span> <span>=</span> <span>in_range</span><span>(</span><span>ascii</span><span>,</span> <span>b'0'</span><span>,</span> <span>b'9'</span><span>);</span>
<span>let</span> <span>pluses</span> <span>=</span> <span>ascii</span><span>.simd_eq</span><span>([</span><span>b'+'</span><span>;</span> <span>N</span><span>]</span><span>.into</span><span>());</span>
<span>let</span> <span>solidi</span> <span>=</span> <span>ascii</span><span>.simd_eq</span><span>([</span><span>b'/'</span><span>;</span> <span>N</span><span>]</span><span>.into</span><span>());</span>

<span>// If any byte was invalid, none of the masks will select for it,</span>
<span>// so that lane will be 0 in the or of all the masks. This is our</span>
<span>// validation check.</span>
<span>let</span> <span>ok</span> <span>=</span> <span>(</span><span>uppers</span> <span>|</span> <span>lowers</span> <span>|</span> <span>digits</span> <span>|</span> <span>pluses</span> <span>|</span> <span>solidi</span><span>)</span><span>.all</span><span>();</span>

<span>// Given a mask, create a new vector by splatting `value`</span>
<span>// over the set lanes.</span>
<span>fn</span> <span>masked_splat</span><span>(</span><span>mask</span><span>:</span> <span>Mask</span><span>&lt;</span><span>i8</span><span>,</span> <span>N</span><span>&gt;</span><span>,</span> <span>value</span><span>:</span> <span>i8</span><span>)</span> <span>-&gt;</span> <span>Simd</span><span>&lt;</span><span>i8</span><span>,</span> <span>4</span><span>&gt;</span> <span>{</span>
  <span>mask</span><span>.select</span><span>(</span><span>Simd</span><span>::</span><span>splat</span><span>(</span><span>val</span><span>),</span> <span>Simd</span><span>::</span><span>splat</span><span>(</span><span>0</span><span>))</span>
<span>}</span>

<span>// Fill the the lanes of the offset vector by filling the</span>
<span>// set lanes with the corresponding offset. This is like</span>
<span>// a "vectorized" version of the `match`.</span>
<span>let</span> <span>offsets</span> <span>=</span> <span>masked_splat</span><span>(</span><span>uppers</span><span>,</span>  <span>65</span><span>)</span>
            <span>|</span> <span>masked_splat</span><span>(</span><span>lowers</span><span>,</span>  <span>71</span><span>)</span>
            <span>|</span> <span>masked_splat</span><span>(</span><span>digits</span><span>,</span>  <span>-</span><span>4</span><span>)</span>
            <span>|</span> <span>masked_splat</span><span>(</span><span>pluses</span><span>,</span> <span>-</span><span>19</span><span>)</span>
            <span>|</span> <span>masked_splat</span><span>(</span><span>solidi</span><span>,</span> <span>-</span><span>16</span><span>);</span>

<span>// Finally, Build the sextets vector.</span>
<span>let</span> <span>sextets</span> <span>=</span> <span>ascii</span><span>.cast</span><span>::</span><span>&lt;</span><span>i8</span><span>&gt;</span><span>()</span> <span>-</span> <span>offsets</span><span>;</span></code></pre></figure></div> <p>This solution is quite elegant, and will produce very competitive code, but it’s not actually ideal. We need to do a lot of comparisons here: eight in total. We also keep lots of values alive at the same time, which might lead to unwanted register pressure.</p> <h3 id="simd-hash-table"><a href="#simd-hash-table">SIMD Hash Table</a></h3> <p>Let’s look at the byte representations of the ranges. <code>A-Z</code>, <code>a-z</code>, and <code>0-9</code> are, as byte ranges, <code>0x41..0x5b</code>, <code>0x61..0x7b</code>, and <code>0x30..0x3a</code>. Notice they all have different high nybbles! What’s more, <code>+</code> and <code>/</code> are <code>0x2b</code> and <code>0x2f</code>, so the function <code>byte &gt;&gt; 4</code> is <em>almost</em> enough to distinguish all the ranges. If we subtract one if <code>byte == b'/'</code>, we have a <em>perfect hash</em> for the ranges.</p> <p>In other words, the value <code>(byte &gt;&gt; 4) - (byte == '/')</code> maps the ranges as follows:</p> <ul> <li><code>A-Z</code> goes to 4 or 5.</li> <li><code>a-z</code> goes to 6 or 7.</li> <li><code>0-9</code> goes to 3.</li> <li><code>+</code> goes to 2.</li> <li><code>/</code> goes to 1.</li> </ul> <p>This is small enough that we could cram a lookup table of values for building the <code>offsets</code> vector into another SIMD vector, and use a shuffle operation to do the lookup.</p> <p>This is not my original idea; I came across a <a href="https://github.com/WojciechMula/base64simd/issues/3">GitHub issue</a> where an anonymous user points out this perfect hash.</p> <p>Our new ascii-to-sextet code looks like this:</p> <div><figure><pre><code data-lang="rust"><span>// Compute the perfect hash for each lane.</span>
<span>let</span> <span>hashes</span> <span>=</span> <span>(</span><span>ascii</span> <span>&gt;&gt;</span> <span>Simd</span><span>::</span><span>splat</span><span>(</span><span>4</span><span>))</span>
  <span>+</span> <span>Simd</span><span>::</span><span>simd_eq</span><span>(</span><span>ascii</span><span>,</span> <span>Simd</span><span>::</span><span>splat</span><span>(</span><span>b'/'</span><span>))</span>
    <span>.to_int</span><span>()</span>  <span>// to_int() is equivalent to masked_splat(-1, 0).</span>
    <span>.cast</span><span>::</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>();</span>

<span>// Look up offsets based on each hash and subtract them from `ascii`.</span>
<span>let</span> <span>sextets</span> <span>=</span> <span>ascii</span>
    <span>// This lookup table corresponds to the offsets we used to build the</span>
    <span>// `offsets` vector in the previous implementation, placed in the</span>
    <span>// indices that the perfect hash produces.</span>
  <span>-</span> <span>Simd</span><span>::</span><span>&lt;</span><span>i8</span><span>,</span> <span>8</span><span>&gt;</span><span>::</span><span>from</span><span>([</span><span>0</span><span>,</span> <span>16</span><span>,</span> <span>19</span><span>,</span> <span>4</span><span>,</span> <span>-</span><span>65</span><span>,</span> <span>-</span><span>65</span><span>,</span> <span>-</span><span>71</span><span>,</span> <span>-</span><span>71</span><span>])</span>
    <span>.cast</span><span>::</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>()</span>
    <span>.swizzle_dyn</span><span>(</span><span>hashes</span><span>);</span></code></pre></figure></div> <p>There is a small wrinkle here: <a href="https://doc.rust-lang.org/std/simd/struct.Simd.html#method.swizzle_dyn"><code>Simd::swizzle_dyn()</code></a> requires that the index array be the same length as the lookup table. This is annoying because right now <code>ascii</code> is a <code>Simd&lt;u8, 4&gt;</code>, but that will not be the case later on, so I will simply sweep this under the rug.</p> <p>Note that we no longer get validation as a side-effect of computing the sextets vector. The same GitHub issue also provides an exact bloom-filter for checking that a particular byte is valid; you can see my implementation <a href="https://github.com/mcy/vb64/blob/894f833e933860e070dabcfcc189430c45fecbd7/src/simd.rs#L93">here</a>. I’m not sure how the OP constructed the bloom filter, but the search space is small enough that you could have written a little script to brute force it.</p> <h3 id="riffling-the-sextets"><a href="#riffling-the-sextets">Riffling the Sextets</a></h3> <p>Now comes a much tricker operation: we need to somehow pack all four sextets into three bytes. One way to try to wrap our head around what the packing code in <code>decode_hot()</code> is doing is to pass in the all-ones sextet in one of the four bytes, and see where those ones end up in the return value.</p> <p>This is not unlike how they use radioactive dyes in biology to track the moment of molecules or cells through an organism.</p> <div><figure><pre><code data-lang="rust"><span>fn</span> <span>bits</span><span>(</span><span>value</span><span>:</span> <span>u32</span><span>)</span> <span>-&gt;</span> <span>String</span> <span>{</span>
  <span>let</span> <span>[</span><span>b1</span><span>,</span> <span>b2</span><span>,</span> <span>b3</span><span>,</span> <span>b4</span><span>]</span> <span>=</span> <span>value</span><span>.reverse_bits</span><span>()</span><span>.to_le_bytes</span><span>();</span>
  <span>format!</span><span>(</span><span>"{b1:08b} {b2:08b} {b3:08b} {b4:08b}"</span><span>)</span>
<span>}</span>

<span>fn</span> <span>decode_pack</span><span>(</span><span>input</span><span>:</span> <span>[</span><span>u8</span><span>;</span> <span>4</span><span>])</span> <span>{</span>
  <span>let</span> <span>mut</span> <span>output</span> <span>=</span> <span>0u32</span><span>;</span>
  <span>for</span> <span>byte</span> <span>in</span> <span>input</span> <span>{</span>
    <span>output</span> <span>&lt;&lt;=</span> <span>6</span><span>;</span>
    <span>output</span> <span>|</span><span>=</span> <span>byte</span> <span>as</span> <span>u32</span><span>;</span>
  <span>}</span>
  <span>output</span> <span>&lt;&lt;=</span> <span>8</span><span>;</span>

  <span>println!</span><span>(</span><span>"{}</span><span>\n</span><span>{}</span><span>\n</span><span>"</span><span>,</span> <span>bits</span><span>(</span><span>u32</span><span>::</span><span>from_be_bytes</span><span>(</span><span>input</span><span>)),</span> <span>bits</span><span>(</span><span>output</span><span>));</span>
<span>}</span>

<span>decode_pack</span><span>([</span><span>0b111111</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>]);</span>
<span>decode_pack</span><span>([</span><span>0</span><span>,</span> <span>0b111111</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>]);</span>
<span>decode_pack</span><span>([</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0b111111</span><span>,</span> <span>0</span><span>]);</span>
<span>decode_pack</span><span>([</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0b111111</span><span>]);</span>

<span>// Output:</span>
<span>// 11111100 00000000 00000000 00000000</span>
<span>// 00111111 00000000 00000000 00000000</span>
<span>//</span>
<span>// 00000000 11111100 00000000 00000000</span>
<span>// 11000000 00001111 00000000 00000000</span>
<span>//</span>
<span>// 00000000 00000000 11111100 00000000</span>
<span>// 00000000 11110000 00000011 00000000</span>
<span>//</span>
<span>// 00000000 00000000 00000000 11111100</span>
<span>// 00000000 00000000 11111100 00000000</span></code></pre></figure></div> <p>Bingo. Playing around with the inputs lets us verify which pieces of the bytes wind up where. For example, by passing <code>0b110000</code> as <code>input[1]</code>, we see that the two high bits of <code>input[1]</code> correspond to the low bits of <code>output[0]</code>. I’ve written the code so that the bits in each byte are printed in little-endian order, so bits on the left are the low bits.</p> <p>Putting this all together, we can draw a schematic of what this operation does to a general <code>Simd&lt;u8, 4&gt;</code>.</p> <p><img src="https://mcyoung.xyz/public/simd-img/riffle.png" alt="the riffling operation"></p> <p>Now, there’s no single instruction that will do this for us. Shuffles can be used to move bytes around, but we’re dealing with <em>pieces</em> of bytes here. We also can’t really do a shift, since we need bits that are overshifted to move into adjacent lanes.</p> <p>The trick is to just make the lanes bigger.</p> <p>Among the operations available for SIMD vectors are lane-wise casts, which allow us to zero-extend, sign-extend, or truncate each lane. So what we can do is cast <code>sextets</code> to a vector of <code>u16</code>, do the shift there and then… somehow put the parts back together?</p> <p>Let’s see how far shifting gets us. How much do we need to shift things by? First, notice that the order of the bits within each chunk that doesn’t cross a byte boundary doesn’t change. For example, the four low bits of <code>input[1]</code> are in the same order when they become the high bits of <code>output[1]</code>, and the two high bits of <code>input[1]</code> are also in the same order when they become the low bits of <code>output[0]</code>.</p> <p>This means we can determine how far to shift by comparing the bit position of the lowest bit of a byte of <code>input</code> with the bit position of the corresponding bit in <code>output</code>.</p> <p><code>input[0]</code>’s low bit is the third bit of <code>output[0]</code>, so we need to shift <code>input[0]</code> by 2. <code>input[1]</code>’s lowest bit is the fifth bit of <code>output[1]</code>, so we need to shift by 4. Analogously, the shifts for <code>input[2]</code> and <code>input[3]</code> turn out to be 6 and 0. In code:</p> <div><figure><pre><code data-lang="rust"><span>let</span> <span>sextets</span> <span>=</span> <span>...</span><span>;</span>
<span>let</span> <span>shifted</span> <span>=</span> <span>sextets</span><span>.cast</span><span>::</span><span>&lt;</span><span>u16</span><span>&gt;</span><span>()</span> <span>&lt;&lt;</span> <span>Simd</span><span>::</span><span>from</span><span>([</span><span>2</span><span>,</span> <span>4</span><span>,</span> <span>6</span><span>,</span> <span>0</span><span>]);</span></code></pre></figure></div> <p>So now we have a <code>Simd&lt;u16, 4&gt;</code> that contains the individual chunks that we need to move around, in the high and low bytes of each <code>u16</code>, which we can think of as being analogous to a <code>[[u8; 2]; 4]</code>. For example, <code>shifted[0][0]</code> contains <code>sextet[0]</code>, but shifted. This corresponds to the red segment in the first schematic. The smaller blue segment is given by <code>shifted[1][1]</code>, i.e., the high byte of the second <code>u16</code>. It’s already in the right place within that byte, so we want <code>output[0] = shifted[0][0] | shifted[1][1]</code>.</p> <p>This suggests a more general strategy: we want to take two vectors, the low bytes and the high bytes of each <code>u16</code> in <code>shifted</code>, respectively, and somehow shuffle them so that when or’ed together, they give the desired output.</p> <p>Look at the schematic again: if we had a vector consisting of <code>[..aaaaaa, ....bbbb, ......cc]</code>, we could or it with a vector like <code>[bb......, cccc...., dddddd..]</code> to get the desired result.</p> <p>One problem: <code>dddddd..</code> is <code>shifted[3][0]</code>, i.e., it’s a low byte. If we change the vector we shift by to <code>[2, 4, 6, 8]</code>, though, it winds up in <code>shifted[3][1]</code>, since it’s been shifted up by <code>8</code> bits: a full byte.</p> <div><figure><pre><code data-lang="rust"><span>// Split shifted into low byte and high byte vectors.</span>
<span>// Same way you'd split a single u16 into bytes, but lane-wise.</span>
<span>let</span> <span>lo</span> <span>=</span> <span>shifted</span><span>.cast</span><span>::</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>();</span>
<span>let</span> <span>hi</span> <span>=</span> <span>(</span><span>shifted</span> <span>&gt;&gt;</span> <span>Simd</span><span>::</span><span>from</span><span>([</span><span>8</span><span>;</span> <span>4</span><span>]))</span><span>.cast</span><span>::</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>();</span>

<span>// Align the lanes: we want to get shifted[0][0] | shifted[1][1],</span>
<span>// shifted[1][0] | shifted[2][1], etc.</span>
<span>let</span> <span>output</span> <span>=</span> <span>lo</span> <span>|</span> <span>hi</span><span>.rotate_lanes_left</span><span>::</span><span>&lt;</span><span>1</span><span>&gt;</span><span>();</span></code></pre></figure></div> <p>Et voila, here is our new, totally branchless implementation of <code>decode_hot()</code>.</p> <div><figure><pre><code data-lang="rust"><span>fn</span> <span>decode_hot</span><span>(</span><span>ascii</span><span>:</span> <span>Simd</span><span>&lt;</span><span>u8</span><span>,</span> <span>4</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>(</span><span>Simd</span><span>&lt;</span><span>u8</span><span>,</span> <span>4</span><span>&gt;</span><span>,</span> <span>bool</span><span>)</span> <span>{</span>
  <span>let</span> <span>hashes</span> <span>=</span> <span>(</span><span>ascii</span> <span>&gt;&gt;</span> <span>Simd</span><span>::</span><span>splat</span><span>(</span><span>4</span><span>))</span>
    <span>+</span> <span>Simd</span><span>::</span><span>simd_eq</span><span>(</span><span>ascii</span><span>,</span> <span>Simd</span><span>::</span><span>splat</span><span>(</span><span>b'/'</span><span>))</span>
      <span>.to_int</span><span>()</span>
      <span>.cast</span><span>::</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>();</span>

  <span>let</span> <span>sextets</span> <span>=</span> <span>ascii</span>
    <span>-</span> <span>Simd</span><span>::</span><span>&lt;</span><span>i8</span><span>,</span> <span>8</span><span>&gt;</span><span>::</span><span>from</span><span>([</span><span>0</span><span>,</span> <span>16</span><span>,</span> <span>19</span><span>,</span> <span>4</span><span>,</span> <span>-</span><span>65</span><span>,</span> <span>-</span><span>65</span><span>,</span> <span>-</span><span>71</span><span>,</span> <span>-</span><span>71</span><span>])</span>
      <span>.cast</span><span>::</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>()</span>
      <span>.swizzle_dyn</span><span>(</span><span>hashes</span><span>);</span>  <span>// Note quite right yet, see next section.</span>

  <span>let</span> <span>ok</span> <span>=</span> <span>/* bloom filter shenanigans */</span><span>;</span>

  <span>let</span> <span>shifted</span> <span>=</span> <span>sextets</span><span>.cast</span><span>::</span><span>&lt;</span><span>u16</span><span>&gt;</span><span>()</span> <span>&lt;&lt;</span> <span>Simd</span><span>::</span><span>from</span><span>([</span><span>2</span><span>,</span> <span>4</span><span>,</span> <span>6</span><span>,</span> <span>8</span><span>]);</span>
  <span>let</span> <span>lo</span> <span>=</span> <span>shifted</span><span>.cast</span><span>::</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>();</span>
  <span>let</span> <span>hi</span> <span>=</span> <span>(</span><span>shifted</span> <span>&gt;&gt;</span> <span>Simd</span><span>::</span><span>splat</span><span>(</span><span>8</span><span>))</span><span>.cast</span><span>::</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>();</span>
  <span>let</span> <span>output</span> <span>=</span> <span>lo</span> <span>|</span> <span>hi</span><span>.rotate_lanes_left</span><span>::</span><span>&lt;</span><span>1</span><span>&gt;</span><span>();</span>

  <span>(</span><span>output</span><span>,</span> <span>ok</span><span>)</span>
<span>}</span></code></pre></figure></div> <p>The compactness of this solution should not be understated. The simplicity of this solution is a large part of what makes it so efficient, because it aggressively leverages the primitives the hardware offers us.</p> <h3 id="scaling-up"><a href="#scaling-up">Scaling Up</a></h3> <p>Ok, so now we have to contend with a new aspect of our implementation that’s crap: a <code>Simd&lt;u8, 4&gt;</code> is tiny. That’s not even 128 bits, which are the smallest vector registers on x86. What we need to do is make <code>decode_hot()</code> generic on the lane count. This will allow us to tune the number of lanes to batch together depending on benchmarks later on.</p> <div><figure><pre><code data-lang="rust"><span>fn</span> <span>decode_hot</span><span>&lt;</span><span>const</span> <span>N</span><span>:</span> <span>usize</span><span>&gt;</span><span>(</span><span>ascii</span><span>:</span> <span>Simd</span><span>&lt;</span><span>u8</span><span>,</span> <span>N</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>(</span><span>Simd</span><span>&lt;</span><span>u8</span><span>,</span> <span>N</span><span>&gt;</span><span>,</span> <span>bool</span><span>)</span>
<span>where</span>
  <span>// This makes sure N is a small power of 2.</span>
  <span>LaneCount</span><span>&lt;</span><span>N</span><span>&gt;</span><span>:</span> <span>SupportedLaneCount</span><span>,</span>
<span>{</span>
  <span>let</span> <span>hashes</span> <span>=</span> <span>(</span><span>ascii</span> <span>&gt;&gt;</span> <span>Simd</span><span>::</span><span>splat</span><span>(</span><span>4</span><span>))</span>
    <span>+</span> <span>Simd</span><span>::</span><span>simd_eq</span><span>(</span><span>ascii</span><span>,</span> <span>Simd</span><span>::</span><span>splat</span><span>(</span><span>b'/'</span><span>))</span>
      <span>.to_int</span><span>()</span>
      <span>.cast</span><span>::</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>();</span>

  <span>let</span> <span>sextets</span> <span>=</span> <span>ascii</span>
    <span>-</span> <span>tiled</span><span>(</span><span>&amp;</span><span>[</span><span>0</span><span>,</span> <span>16</span><span>,</span> <span>19</span><span>,</span> <span>4</span><span>,</span> <span>-</span><span>65</span><span>,</span> <span>-</span><span>65</span><span>,</span> <span>-</span><span>71</span><span>,</span> <span>-</span><span>71</span><span>])</span>
      <span>.cast</span><span>::</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>()</span>
      <span>.swizzle_dyn</span><span>(</span><span>hashes</span><span>);</span>  <span>// Works fine now, as long as N &gt;= 8.</span>

  <span>let</span> <span>ok</span> <span>=</span> <span>/* bloom filter shenanigans */</span><span>;</span>

  <span>let</span> <span>shifted</span> <span>=</span> <span>sextets</span><span>.cast</span><span>::</span><span>&lt;</span><span>u16</span><span>&gt;</span><span>()</span> <span>&lt;&lt;</span> <span>tiled</span><span>(</span><span>&amp;</span><span>[</span><span>2</span><span>,</span> <span>4</span><span>,</span> <span>6</span><span>,</span> <span>8</span><span>]);</span>
  <span>let</span> <span>lo</span> <span>=</span> <span>shifted</span><span>.cast</span><span>::</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>();</span>
  <span>let</span> <span>hi</span> <span>=</span> <span>(</span><span>shifted</span> <span>&gt;&gt;</span> <span>Simd</span><span>::</span><span>splat</span><span>(</span><span>8</span><span>))</span><span>.cast</span><span>::</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>();</span>
  <span>let</span> <span>output</span> <span>=</span> <span>lo</span> <span>|</span> <span>hi</span><span>.rotate_lanes_left</span><span>::</span><span>&lt;</span><span>1</span><span>&gt;</span><span>();</span>

  <span>(</span><span>output</span><span>,</span> <span>ok</span><span>)</span>
<span>}</span>

<span>/// Generates a new vector made up of repeated "tiles" of identical</span>
<span>/// data.</span>
<span>const</span> <span>fn</span> <span>tiled</span><span>&lt;</span><span>T</span><span>,</span> <span>const</span> <span>N</span><span>:</span> <span>usize</span><span>&gt;</span><span>(</span><span>tile</span><span>:</span> <span>&amp;</span><span>[</span><span>T</span><span>])</span> <span>-&gt;</span> <span>Simd</span><span>&lt;</span><span>T</span><span>,</span> <span>N</span><span>&gt;</span>
<span>where</span>
  <span>T</span><span>:</span> <span>SimdElement</span><span>,</span>
  <span>LaneCount</span><span>&lt;</span><span>N</span><span>&gt;</span><span>:</span> <span>SupportedLaneCount</span><span>,</span>
<span>{</span>
  <span>let</span> <span>mut</span> <span>out</span> <span>=</span> <span>[</span><span>tile</span><span>[</span><span>0</span><span>];</span> <span>N</span><span>];</span>
  <span>let</span> <span>mut</span> <span>i</span> <span>=</span> <span>0</span><span>;</span>
  <span>while</span> <span>i</span> <span>&lt;</span> <span>N</span> <span>{</span>
    <span>out</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>tile</span><span>[</span><span>i</span> <span>%</span> <span>tile</span><span>.len</span><span>()];</span>
    <span>i</span> <span>+=</span> <span>1</span><span>;</span>
  <span>}</span>
  <span>Simd</span><span>::</span><span>from_array</span><span>(</span><span>out</span><span>)</span>
<span>}</span></code></pre></figure></div> <p>We have to change virtually nothing, which is pretty awesome! But unfortunately, this code is subtly incorrect. Remember how in the <code>N = 4</code> case, the result of <code>output</code> had a garbage value that we ignore in its highest lane? Well, now that garbage data is interleaved into output: every fourth lane contains garbage.</p> <p>We can use a shuffle to delete these lanes, thankfully. Specifically, we want <code>shuffled[i] = output[i + i / 3]</code>, which skips every forth index. So, <code>shuffled[3] = output[4]</code>, skipping over the garbage value in <code>output[3]</code>. If <code>i + i / 3</code> overflows <code>N</code>, that’s ok, because that’s the high quarter of the final output vector, which is ignored anyways. In code:</p> <div><figure><pre><code data-lang="rust"><span>fn</span> <span>decode_hot</span><span>&lt;</span><span>const</span> <span>N</span><span>:</span> <span>usize</span><span>&gt;</span><span>(</span><span>ascii</span><span>:</span> <span>Simd</span><span>&lt;</span><span>u8</span><span>,</span> <span>N</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>(</span><span>Simd</span><span>&lt;</span><span>u8</span><span>,</span> <span>N</span><span>&gt;</span><span>,</span> <span>bool</span><span>)</span>
<span>where</span>
  <span>// This makes sure N is a small power of 2.</span>
  <span>LaneCount</span><span>&lt;</span><span>N</span><span>&gt;</span><span>:</span> <span>SupportedLaneCount</span><span>,</span>
<span>{</span>
  <span>/* snip */</span>

  <span>let</span> <span>decoded_chunks</span> <span>=</span> <span>lo</span> <span>|</span> <span>hi</span><span>.rotate_lanes_left</span><span>::</span><span>&lt;</span><span>1</span><span>&gt;</span><span>();</span>
  <span>let</span> <span>output</span> <span>=</span> <span>swizzle!</span><span>(</span><span>N</span><span>;</span> <span>decoded_chunks</span><span>,</span> <span>array!</span><span>(</span><span>N</span><span>;</span> <span>|</span><span>i</span><span>|</span> <span>i</span> <span>+</span> <span>i</span> <span>/</span> <span>3</span><span>));</span>

  <span>(</span><span>output</span><span>,</span> <span>ok</span><span>)</span>
<span>}</span></code></pre></figure></div> <blockquote> <p><code>swizzle!()</code> is a helper macro<sup id="fnref:macros" role="doc-noteref"><a href="#fn:macros" rel="footnote">6</a></sup> for generating generic implementations of <code>std::simd::Swizzle</code>, and <code>array!()</code> is something I wrote for generating generic-length array constants; the closure is called once for each <code>i in 0..N</code>.</p> </blockquote> <p>So now we can decode 32 base64 bytes in parallel by calling <code>decode_hot::&lt;32&gt;()</code>. We’ll try to keep things generic from here, so we can tune the lane parameter based on benchmarks.</p> <h3 id="the-outer-loop"><a href="#the-outer-loop">The Outer Loop</a></h3> <p>Let’s look at <code>decode()</code> again. Let’s start by making it generic on the internal lane count, too.</p> <div><figure><pre><code data-lang="rust"><span>fn</span> <span>decode</span><span>&lt;</span><span>const</span> <span>N</span><span>:</span> <span>usize</span><span>&gt;</span><span>(</span><span>data</span><span>:</span> <span>&amp;</span><span>[</span><span>u8</span><span>],</span> <span>out</span><span>:</span> <span>&amp;</span><span>mut</span> <span>Vec</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>(),</span> <span>Error</span><span>&gt;</span>
<span>where</span>
  <span>LaneCount</span><span>&lt;</span><span>N</span><span>&gt;</span><span>:</span> <span>SupportedLaneCount</span><span>,</span>
<span>{</span>
  <span>let</span> <span>data</span> <span>=</span> <span>match</span> <span>data</span> <span>{</span>
    <span>[</span><span>p</span> <span>@</span> <span>..</span><span>,</span> <span>b'='</span><span>,</span> <span>b'='</span><span>]</span> <span>|</span> <span>[</span><span>p</span> <span>@</span> <span>..</span><span>,</span> <span>b'='</span><span>]</span> <span>|</span> <span>p</span> <span>=&gt;</span> <span>p</span><span>,</span>
  <span>};</span>

  <span>for</span> <span>chunk</span> <span>in</span> <span>data</span><span>.chunks</span><span>(</span><span>N</span><span>)</span> <span>{</span> <span>// N-sized chunks now.</span>
    <span>let</span> <span>mut</span> <span>ascii</span> <span>=</span> <span>[</span><span>b'A'</span><span>;</span> <span>N</span><span>];</span>
    <span>ascii</span><span>[</span><span>..</span><span>chunk</span><span>.len</span><span>()]</span><span>.copy_from_slice</span><span>(</span><span>chunk</span><span>);</span>

    <span>let</span> <span>(</span><span>dec</span><span>,</span> <span>ok</span><span>)</span> <span>=</span> <span>decode_hot</span><span>::</span><span>&lt;</span><span>N</span><span>&gt;</span><span>(</span><span>ascii</span><span>.into</span><span>());</span>
    <span>if</span> <span>(</span><span>!</span><span>ok</span><span>)</span> <span>{</span>
      <span>return</span> <span>Err</span><span>(</span><span>Error</span><span>);</span>
    <span>}</span>

    <span>let</span> <span>decoded</span> <span>=</span> <span>decoded_len</span><span>(</span><span>chunk</span><span>.len</span><span>());</span>
    <span>out</span><span>.extend_from_slice</span><span>(</span><span>&amp;</span><span>dec</span><span>[</span><span>..</span><span>decoded</span><span>]);</span>
  <span>}</span>

  <span>Ok</span><span>(())</span>
<span>}</span></code></pre></figure></div> <p>What branches are left? There’s still the branch from <code>for chunks in ...</code>. It’s not ideal because it can’t do an exact pointer comparison, and needs to do a <code>&gt;=</code> comparison on a length instead.</p> <p>We call <code>[T]::copy_from_slice</code>, which is super slow because it needs to make a variable-length <code>memcpy</code> call, which can’t be inlined. Function calls are branches! The bounds checks are also a problem.</p> <p>We branch on <code>ok</code> every loop iteration, still. Not returning early in <code>decode_hot</code> doesn’t win us anything (yet).</p> <p>We potentially call the allocator in <code>extend_from_slice</code>, and perform another non-inline-able <code>memcpy</code> call.</p> <h3 id="preallocating-with-slop"><a href="#preallocating-with-slop">Preallocating with Slop</a></h3> <p>The last of these is the easiest to address: we can reserve space in <code>out</code>, since we know exactly how much data we need to write thanks to <code>decoded_len</code>. Better yet, we can reserve some “slop”: i.e., scratch space past where the end of the message would be, so we can perform full SIMD stores, instead of the variable-length memcpy.</p> <p>This way, in each iteration, we write the full SIMD vector, including any garbage bytes in the upper quarter. Then, the next write is offset <code>3/4 * N</code> bytes over, so it overwrites the garbage bytes with decoded message bytes. The garbage bytes from the final right get “deleted” by not being included in the final <code>Vec::set_len()</code> that “commits” the memory we wrote to.</p> <div><figure><pre><code data-lang="rust"><span>fn</span> <span>decode</span><span>&lt;</span><span>const</span> <span>N</span><span>:</span> <span>usize</span><span>&gt;</span><span>(</span><span>data</span><span>:</span> <span>&amp;</span><span>[</span><span>u8</span><span>],</span> <span>out</span><span>:</span> <span>&amp;</span><span>mut</span> <span>Vec</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>(),</span> <span>Error</span><span>&gt;</span>
<span>where</span> <span>LaneCount</span><span>&lt;</span><span>N</span><span>&gt;</span><span>:</span> <span>SupportedLaneCount</span><span>,</span>
<span>{</span>
  <span>let</span> <span>data</span> <span>=</span> <span>match</span> <span>data</span> <span>{</span>
    <span>[</span><span>p</span> <span>@</span> <span>..</span><span>,</span> <span>b'='</span><span>,</span> <span>b'='</span><span>]</span> <span>|</span> <span>[</span><span>p</span> <span>@</span> <span>..</span><span>,</span> <span>b'='</span><span>]</span> <span>|</span> <span>p</span> <span>=&gt;</span> <span>p</span><span>,</span>
  <span>};</span>

  <span>let</span> <span>final_len</span> <span>=</span> <span>decoded_len</span><span>(</span><span>data</span><span>);</span>
  <span>out</span><span>.reserve</span><span>(</span><span>final_len</span> <span>+</span> <span>N</span> <span>/</span> <span>4</span><span>);</span>  <span>// Reserve with slop.</span>

  <span>// Get a raw pointer to where we should start writing.</span>
  <span>let</span> <span>mut</span> <span>ptr</span> <span>=</span> <span>out</span><span>.as_mut_ptr_range</span><span>()</span><span>.end</span><span>();</span>
  <span>let</span> <span>start</span> <span>=</span> <span>ptr</span><span>;</span>

  <span>for</span> <span>chunk</span> <span>in</span> <span>data</span><span>.chunks</span><span>(</span><span>N</span><span>)</span> <span>{</span> <span>// N-sized chunks now.</span>
    <span>/* snip */</span>

    <span>let</span> <span>decoded</span> <span>=</span> <span>decoded_len</span><span>(</span><span>chunk</span><span>.len</span><span>());</span>
    <span>unsafe</span> <span>{</span>
      <span>// Do a raw write and advance the pointer.</span>
      <span>ptr</span><span>.cast</span><span>::</span><span>&lt;</span><span>Simd</span><span>&lt;</span><span>u8</span><span>,</span> <span>N</span><span>&gt;&gt;</span><span>()</span><span>.write_unaligned</span><span>(</span><span>dec</span><span>);</span>
      <span>ptr</span> <span>=</span> <span>ptr</span><span>.add</span><span>(</span><span>decoded</span><span>);</span>
    <span>}</span>
  <span>}</span>

  <span>unsafe</span> <span>{</span>
    <span>// Update the vector's final length.</span>
    <span>// This is the final "commit".</span>
    <span>let</span> <span>len</span> <span>=</span> <span>ptr</span><span>.offset_from</span><span>(</span><span>start</span><span>);</span>
    <span>out</span><span>.set_len</span><span>(</span><span>len</span> <span>as</span> <span>usize</span><span>);</span>
  <span>}</span>

  <span>Ok</span><span>(())</span>
<span>}</span></code></pre></figure></div> <p>This is safe, because we’ve pre-allocated exactly the amount of memory we need, and where <code>ptr</code> lands is equal to the amount of memory actually decoded. We could also compute the final length of <code>out</code> ahead of time.</p> <p>Note that if we early return due to <code>if !ok</code>, <code>out</code> remains unmodified, because even though we did write to its buffer, we never execute the “commit” part, so the code remains correct.</p> <h3 id="delaying-failure"><a href="#delaying-failure">Delaying Failure</a></h3> <p>Next up, we can eliminate the <code>if !ok</code> branches by waiting to return an error until as late as possible: just before the <code>set_len</code> call.</p> <p>Remember our observation from before: most base64 encoded blobs are valid, so this unhappy path should be very rare. Also, syntax errors cannot cause code that follows to misbehave arbitrarily, so letting it go wild doesn’t hurt anything.</p> <div><figure><pre><code data-lang="rust"><span>fn</span> <span>decode</span><span>&lt;</span><span>const</span> <span>N</span><span>:</span> <span>usize</span><span>&gt;</span><span>(</span><span>data</span><span>:</span> <span>&amp;</span><span>[</span><span>u8</span><span>],</span> <span>out</span><span>:</span> <span>&amp;</span><span>mut</span> <span>Vec</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>(),</span> <span>Error</span><span>&gt;</span>
<span>where</span> <span>LaneCount</span><span>&lt;</span><span>N</span><span>&gt;</span><span>:</span> <span>SupportedLaneCount</span><span>,</span>
<span>{</span>
  <span>/* snip */</span>
  <span>let</span> <span>mut</span> <span>error</span> <span>=</span> <span>false</span><span>;</span>
  <span>for</span> <span>chunk</span> <span>in</span> <span>data</span><span>.chunks</span><span>(</span><span>N</span><span>)</span> <span>{</span>
    <span>let</span> <span>mut</span> <span>ascii</span> <span>=</span> <span>[</span><span>b'A'</span><span>;</span> <span>N</span><span>];</span>
    <span>ascii</span><span>[</span><span>..</span><span>chunk</span><span>.len</span><span>()]</span><span>.copy_from_slice</span><span>(</span><span>chunk</span><span>);</span>

    <span>let</span> <span>(</span><span>dec</span><span>,</span> <span>ok</span><span>)</span> <span>=</span> <span>decode_hot</span><span>::</span><span>&lt;</span><span>N</span><span>&gt;</span><span>(</span><span>ascii</span><span>.into</span><span>());</span>
    <span>error</span> <span>|</span><span>=</span> <span>!</span><span>ok</span><span>;</span>

    <span>/* snip */</span>
  <span>}</span>

  <span>if</span> <span>error</span> <span>{</span>
    <span>return</span> <span>Err</span><span>(</span><span>Error</span><span>);</span>
  <span>}</span>

  <span>unsafe</span> <span>{</span>
    <span>let</span> <span>len</span> <span>=</span> <span>ptr</span><span>.offset_from</span><span>(</span><span>start</span><span>);</span>
    <span>out</span><span>.set_len</span><span>(</span><span>len</span> <span>as</span> <span>usize</span><span>);</span>
  <span>}</span>

  <span>Ok</span><span>(())</span>
<span>}</span></code></pre></figure></div> <p>The branch is still “there”, sure, but it’s out of the hot loop.</p> <p>Because we never hit the <code>set_len</code> call and commit whatever garbage we wrote, said garbage essentially disappears when we return early, to be overwritten by future calls to <code>Vec::push()</code>.</p> <h3 id="unroll-it-harder"><a href="#unroll-it-harder">Unroll It Harder</a></h3> <p>Ok, let’s look at the memcpy from <code>copy_from_slice</code> at the start of the hot loop. The loop has already been partly unrolled: it does <code>N</code> iterations with SIMD each step, doing something funny on the last step to make up for the missing data (padding with <code>A</code>).</p> <p>We can take this a step further by doing an “unroll and jam” optimization. This type of unrolling splits the loop into two parts: a hot vectorized loop and a cold remainder part. The hot loop <em>always</em> handles length <code>N</code> input, and the remainder runs at most once and handles <code>i &lt; N</code> input.</p> <p>Rust provides an iterator adapter for hand-rolled (lol) unroll-and-jam: <code>Iterator::chunks_exact()</code>.</p> <div><figure><pre><code data-lang="rust"><span>fn</span> <span>decode</span><span>&lt;</span><span>const</span> <span>N</span><span>:</span> <span>usize</span><span>&gt;</span><span>(</span><span>data</span><span>:</span> <span>&amp;</span><span>[</span><span>u8</span><span>],</span> <span>out</span><span>:</span> <span>&amp;</span><span>mut</span> <span>Vec</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>(),</span> <span>Error</span><span>&gt;</span>
<span>where</span> <span>LaneCount</span><span>&lt;</span><span>N</span><span>&gt;</span><span>:</span> <span>SupportedLaneCount</span><span>,</span>
<span>{</span>
  <span>/* snip */</span>
  <span>let</span> <span>mut</span> <span>error</span> <span>=</span> <span>false</span><span>;</span>
  <span>let</span> <span>mut</span> <span>chunks</span> <span>=</span> <span>data</span><span>.chunks_exact</span><span>(</span><span>N</span><span>);</span>
  <span>for</span> <span>chunk</span> <span>in</span> <span>&amp;</span><span>mut</span> <span>chunks</span> <span>{</span>
    <span>// Simd::from_slice() can do a load in one instruction.</span>
    <span>// The bounds check is easy for the compiler to elide.</span>
    <span>let</span> <span>(</span><span>dec</span><span>,</span> <span>ok</span><span>)</span> <span>=</span> <span>decode_hot</span><span>::</span><span>&lt;</span><span>N</span><span>&gt;</span><span>(</span><span>Simd</span><span>::</span><span>from_slice</span><span>(</span><span>chunk</span><span>));</span>
    <span>error</span> <span>|</span><span>=</span> <span>!</span><span>ok</span><span>;</span>
    <span>/* snip */</span>
  <span>}</span>

  <span>let</span> <span>rest</span> <span>=</span> <span>chunks</span><span>.remainder</span><span>();</span>
  <span>if</span> <span>!</span><span>rest</span><span>.empty</span><span>()</span> <span>{</span>
    <span>let</span> <span>mut</span> <span>ascii</span> <span>=</span> <span>[</span><span>b'A'</span><span>;</span> <span>N</span><span>];</span>
    <span>ascii</span><span>[</span><span>..</span><span>chunk</span><span>.len</span><span>()]</span><span>.copy_from_slice</span><span>(</span><span>chunk</span><span>);</span>

    <span>let</span> <span>(</span><span>dec</span><span>,</span> <span>ok</span><span>)</span> <span>=</span> <span>decode_hot</span><span>::</span><span>&lt;</span><span>N</span><span>&gt;</span><span>(</span><span>ascii</span><span>.into</span><span>());</span>
    <span>/* snip */</span>
  <span>}</span>

  <span>/* snip */</span>
<span>}</span></code></pre></figure></div> <p>Splitting into two parts lets us call <code>Simd::from_slice()</code>, which performs a single, vector-sized load.</p> <h2 id="so-how-fast-is-it"><a href="#so-how-fast-is-it">So, How Fast Is It?</a></h2> <p>At this point, it looks like we’ve addressed every branch that we can, so some benchmarks are in order. I wrote a benchmark that decodes messages of every length from 0 to something like 200 or 500 bytes, and compared it against the baseline base64 implementation on crates.io.</p> <p>I compiled with <code>-Zbuild-std</code> and <code>-Ctarget-cpu=native</code> to try to get the best results. Based on some tuning, <code>N = 32</code> was the best length, since it used one YMM register for each iteration of the hot loop.</p> <p><img src="https://mcyoung.xyz/public/simd-img/graph-old.png" alt="a performance graph; our code is really good compared to the baseline, but variance is high"></p> <p>So, we have the baseline beat. But what’s up with that crazy heartbeat waveform? You can tell it has something to do with the “remainder” part of the loop, since it correlates strongly with <code>data.len() % 32</code>.</p> <p>I stared at the assembly for a while. I don’t remember what was there, but I think that <code>copy_from_slice</code> had been inlined and unrolled into a loop that loaded each byte at a time. The moral equivalent of this:</p> <div><figure><pre><code data-lang="rust"><span>let</span> <span>mut</span> <span>ascii</span> <span>=</span> <span>[</span><span>b'A'</span><span>;</span> <span>N</span><span>];</span>
<span>for</span> <span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span> <span>in</span> <span>Iterator</span><span>::</span><span>zip</span><span>(</span><span>&amp;</span><span>mut</span> <span>ascii</span><span>,</span> <span>chunk</span><span>)</span> <span>{</span>
  <span>*</span><span>a</span> <span>=</span> <span>*</span><span>b</span><span>;</span>
<span>}</span></code></pre></figure></div> <p>I decided to try <code>Simd::gather_or()</code>, which is kind of like a “vectorized load”. It wound up producing worse assembly, so I gave up on using a gather and instead wrote a carefully optimized loading function by hand.</p> <h3 id="unroll-and-jam-revisited"><a href="#unroll-and-jam-revisited">Unroll and Jam, Revisited</a></h3> <p>The idea here is to perform the largest scalar loads Rust offers where possible. The strategy is again unroll and jam: perform <code>u128</code> loads in a loop and deal with the remainder separately.</p> <p>The hot part looks like this:</p> <div><figure><pre><code data-lang="rust"><span>let</span> <span>mut</span> <span>buf</span> <span>=</span> <span>[</span><span>b'A'</span><span>;</span> <span>N</span><span>];</span>

<span>// Load a bunch of big 16-byte chunks. LLVM will lower these to XMM loads.</span>
<span>let</span> <span>ascii_ptr</span> <span>=</span> <span>buf</span><span>.as_mut_ptr</span><span>();</span>
<span>let</span> <span>mut</span> <span>write_at</span> <span>=</span> <span>ascii_ptr</span><span>;</span>
<span>if</span> <span>slice</span><span>.len</span><span>()</span> <span>&gt;=</span> <span>16</span> <span>{</span>
  <span>for</span> <span>i</span> <span>in</span> <span>0</span><span>..</span><span>slice</span><span>.len</span><span>()</span> <span>/</span> <span>16</span> <span>{</span>
    <span>unsafe</span> <span>{</span>
      <span>write_at</span> <span>=</span> <span>write_at</span><span>.add</span><span>(</span><span>i</span> <span>*</span> <span>16</span><span>);</span>

      <span>let</span> <span>word</span> <span>=</span> <span>slice</span><span>.as_ptr</span><span>()</span><span>.cast</span><span>::</span><span>&lt;</span><span>u128</span><span>&gt;</span><span>()</span><span>.add</span><span>(</span><span>i</span><span>)</span><span>.read_unaligned</span><span>();</span>
      <span>write_at</span><span>.cast</span><span>::</span><span>&lt;</span><span>u128</span><span>&gt;</span><span>()</span><span>.write_unaligned</span><span>(</span><span>word</span><span>);</span>
    <span>}</span>
  <span>}</span>
<span>}</span></code></pre></figure></div> <p>The cold part seems hard to optimize at first. What’s the least number of unaligned loads you need to do to load 15 bytes from memory? It’s two! You can load a <code>u64</code> from <code>p</code>, and then another one from <code>p + 7</code>; these loads (call them <code>a</code> and <code>b</code>) overlap by one byte, but we can or them together to merge that byte, so our loaded value is <code>a as u128 | (b as u128 &lt;&lt; 56)</code>.</p> <p>A similar trick works if the data to load is between a <code>u32</code> and a <code>u64</code>. Finally, to load 1, 2, or 3 bytes, we can load <code>p</code>, <code>p + len/2</code> and <code>p + len-1</code>; depending on whether <code>len</code> is 1, 2, or 3, this will potentially load the same byte multiple times; however, this reduces the number of branches necessary, since we don’t need to distinguish the 1, 2, or 3 lines.</p> <p>This is the kind of code that’s probably easier to read than to explain.</p> <div><figure><pre><code data-lang="rust"><span>unsafe</span> <span>{</span>
  <span>let</span> <span>ptr</span> <span>=</span> <span>slice</span><span>.as_ptr</span><span>()</span><span>.offset</span><span>(</span><span>write_at</span><span>.offset_from</span><span>(</span><span>ascii_ptr</span><span>));</span>
  <span>let</span> <span>len</span> <span>=</span> <span>slice</span><span>.len</span><span>()</span> <span>%</span> <span>16</span><span>;</span>

  <span>if</span> <span>len</span> <span>&gt;=</span> <span>8</span> <span>{</span>
    <span>// Load two overlapping u64s.</span>
    <span>let</span> <span>lo</span> <span>=</span> <span>ptr</span><span>.cast</span><span>::</span><span>&lt;</span><span>u64</span><span>&gt;</span><span>()</span><span>.read_unaligned</span><span>()</span> <span>as</span> <span>u128</span><span>;</span>
    <span>let</span> <span>hi</span> <span>=</span> <span>ptr</span><span>.add</span><span>(</span><span>len</span> <span>-</span> <span>8</span><span>)</span><span>.cast</span><span>::</span><span>&lt;</span><span>u64</span><span>&gt;</span><span>()</span><span>.read_unaligned</span><span>()</span> <span>as</span> <span>u128</span><span>;</span>
    <span>let</span> <span>data</span> <span>=</span> <span>lo</span> <span>|</span> <span>(</span><span>hi</span> <span>&lt;&lt;</span> <span>((</span><span>len</span> <span>-</span> <span>8</span><span>)</span> <span>*</span> <span>8</span><span>));</span>

    <span>let</span> <span>z</span> <span>=</span> <span>u128</span><span>::</span><span>from_ne_bytes</span><span>([</span><span>b'A'</span><span>;</span> <span>16</span><span>])</span> <span>&lt;&lt;</span> <span>(</span><span>len</span> <span>*</span> <span>8</span><span>);</span>
    <span>write_at</span><span>.cast</span><span>::</span><span>&lt;</span><span>u128</span><span>&gt;</span><span>()</span><span>.write_unaligned</span><span>(</span><span>data</span> <span>|</span> <span>z</span><span>);</span>
  <span>}</span> <span>else</span> <span>if</span> <span>len</span> <span>&gt;=</span> <span>4</span> <span>{</span>
    <span>// Load two overlapping u32s.</span>
    <span>let</span> <span>lo</span> <span>=</span> <span>ptr</span><span>.cast</span><span>::</span><span>&lt;</span><span>u32</span><span>&gt;</span><span>()</span><span>.read_unaligned</span><span>()</span> <span>as</span> <span>u64</span><span>;</span>
    <span>let</span> <span>hi</span> <span>=</span> <span>ptr</span><span>.add</span><span>(</span><span>len</span> <span>-</span> <span>4</span><span>)</span><span>.cast</span><span>::</span><span>&lt;</span><span>u32</span><span>&gt;</span><span>()</span><span>.read_unaligned</span><span>()</span> <span>as</span> <span>u64</span><span>;</span>
    <span>let</span> <span>data</span> <span>=</span> <span>lo</span> <span>|</span> <span>(</span><span>hi</span> <span>&lt;&lt;</span> <span>((</span><span>len</span> <span>-</span> <span>4</span><span>)</span> <span>*</span> <span>8</span><span>));</span>

    <span>let</span> <span>z</span> <span>=</span> <span>u64</span><span>::</span><span>from_ne_bytes</span><span>([</span><span>b'A'</span><span>;</span> <span>8</span><span>])</span> <span>&lt;&lt;</span> <span>(</span><span>len</span> <span>*</span> <span>8</span><span>);</span>
    <span>write_at</span><span>.cast</span><span>::</span><span>&lt;</span><span>u64</span><span>&gt;</span><span>()</span><span>.write_unaligned</span><span>(</span><span>data</span> <span>|</span> <span>z</span><span>);</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>// Load 3 overlapping u8s.</span>

    <span>// For len       1       2       3     ...</span>
    <span>// ... this is  ptr[0]  ptr[0]  ptr[0]</span>
    <span>let</span> <span>lo</span> <span>=</span> <span>ptr</span><span>.read</span><span>()</span> <span>as</span> <span>u32</span><span>;</span>
    <span>// ... this is  ptr[0]  ptr[1]  ptr[1]</span>
    <span>let</span> <span>mid</span> <span>=</span> <span>ptr</span><span>.add</span><span>(</span><span>len</span> <span>/</span> <span>2</span><span>)</span><span>.read</span><span>()</span> <span>as</span> <span>u32</span><span>;</span>
    <span>// ... this is  ptr[0]  ptr[1]  ptr[2]</span>
    <span>let</span> <span>hi</span> <span>=</span> <span>ptr</span><span>.add</span><span>(</span><span>len</span> <span>-</span> <span>1</span><span>)</span><span>.read</span><span>()</span> <span>as</span> <span>u32</span><span>;</span>

    <span>let</span> <span>data</span> <span>=</span> <span>lo</span> <span>|</span> <span>(</span><span>mid</span> <span>&lt;&lt;</span> <span>((</span><span>len</span> <span>/</span> <span>2</span><span>)</span> <span>*</span> <span>8</span><span>))</span> <span>|</span> <span>hi</span> <span>&lt;&lt;</span> <span>((</span><span>len</span> <span>-</span> <span>1</span><span>)</span> <span>*</span> <span>8</span><span>);</span>

    <span>let</span> <span>z</span> <span>=</span> <span>u32</span><span>::</span><span>from_ne_bytes</span><span>([</span><span>b'A'</span><span>;</span> <span>4</span><span>])</span> <span>&lt;&lt;</span> <span>(</span><span>len</span> <span>*</span> <span>8</span><span>);</span>
    <span>write_at</span><span>.cast</span><span>::</span><span>&lt;</span><span>u32</span><span>&gt;</span><span>()</span><span>.write_unaligned</span><span>(</span><span>data</span> <span>|</span> <span>z</span><span>);</span>
  <span>}</span>
<span>}</span></code></pre></figure></div> <p>I learned this type of loading code while contributing to Abseil: it’s very useful for loading variable-length data for data-hungry algorithms, like a codec or a hash function.</p> <p>Here’s the same benchmark again, but with our new loading code.</p> <p><img src="https://mcyoung.xyz/public/simd-img/graph.png" alt="a performance graph; our code is even better and the variance is very tight"></p> <p>The results are really, really good. The variance is super tight, and our performance is 2x that of the baseline pretty much everywhere. <em>Success.</em></p> <h3 id="encoding-web-safe"><a href="#encoding-web-safe">Encoding? Web-Safe?</a></h3> <p>Writing an encoding function is simple enough: first, implement an <code>encode_hot()</code> function that reverses the operations from <code>decode_hot()</code>. The perfect hash from before won’t work, so you’ll need to <a href="https://github.com/mcy/vb64/blob/main/src/simd.rs#L170">invent a new one</a>.</p> <p>Also, the loading/storing code around the encoder is slightly different, too. <code>vb64</code> implements a very efficient encoding routine too, so I suggest taking a look at the source code if you’re interested.</p> <p>There is a base64 variant called web-safe base64, that replaces the <code>+</code> and <code>/</code> characters with <code>-</code> and <code>_</code>. Building a perfect hash for these is trickier: you would probably have to do something like <code>(byte &gt;&gt; 4) - (byte == '_' ? '_' : 0)</code>. I don’t support web-safe base64 yet, but only because I haven’t gotten around to it.</p> <h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2> <p>My library doesn’t really solve an important problem; base64 decoding isn’t a bottleneck… anywhere that I know of, really. But writing SIMD code is really fun! Writing branchless code is often overkill but can give you a good appreciation for what your compilers can and <em>can’t</em> do for you.</p> <p>This project was also an excuse to try <code>std::simd</code>. I think it’s great overall, and generates excellent code. There’s some rough edges I’d like to see fixed to make SIMD code even simpler, but overall I’m very happy with the work that’s been done there.</p> <p>This is probably one of the most complicated posts I’ve written in a long time. SIMD (and performance in general) is a complex topic that requires a breadth of knowledge of tricks and hardware, a lot of which isn’t written down. More of it is written down now, though. ◼</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Simplifying Transformer Blocks (107 pts)]]></title>
            <link>https://arxiv.org/abs/2311.01906</link>
            <guid>38442779</guid>
            <pubDate>Tue, 28 Nov 2023 05:49:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2311.01906">https://arxiv.org/abs/2311.01906</a>, See on <a href="https://news.ycombinator.com/item?id=38442779">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2311.01906.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections &amp; normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable.
<br>In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and BERT encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15% faster training throughput, and using 15% fewer parameters.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Bobby He [<a href="https://arxiv.org/show-email/52c09d90/2311.01906">view email</a>]      <br>    <strong>[v1]</strong>
        Fri, 3 Nov 2023 13:30:52 UTC (7,026 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[That time Verisign typo-squatted all of .com and .net (259 pts)]]></title>
            <link>http://rachelbythebay.com/w/2023/11/27/sitefinder/</link>
            <guid>38442133</guid>
            <pubDate>Tue, 28 Nov 2023 03:53:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://rachelbythebay.com/w/2023/11/27/sitefinder/">http://rachelbythebay.com/w/2023/11/27/sitefinder/</a>, See on <a href="https://news.ycombinator.com/item?id=38442133">Hacker News</a></p>
Couldn't get http://rachelbythebay.com/w/2023/11/27/sitefinder/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Antirez' tiny JSON selector library (143 pts)]]></title>
            <link>https://notes.billmill.org/link_blog/2023/11/antirez__tiny_JSON_selector_library.html</link>
            <guid>38442050</guid>
            <pubDate>Tue, 28 Nov 2023 03:40:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notes.billmill.org/link_blog/2023/11/antirez__tiny_JSON_selector_library.html">https://notes.billmill.org/link_blog/2023/11/antirez__tiny_JSON_selector_library.html</a>, See on <a href="https://news.ycombinator.com/item?id=38442050">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <p>Nov 27, 2023</p>
    <p>Salvatore Sanfilippo, aka <a href="https://github.com/antirez">antirez</a> released a <a href="https://github.com/antirez/botlib">telegram bot library in C</a> recently. While I'm not at all interested in a telegram bot library, I always like to read Salvatore's code, which is uniformly superb.</p>
<p>The bit of code that caught my eye in this release is in <a href="https://github.com/antirez/botlib/blob/ca2f977deeb3455e69e9a8f6b8ba711798dba6e6/json_wrap.c">this file</a>, which implements a simple selector library on top of the <a href="https://github.com/DaveGamble/cJSON">json library he's using</a>. In about 100 lines of code, it implements a function that parses and executes a tiny <a href="https://notes.billmill.org/programming/json/jq.html">jq</a>-like language:</p>
<pre><code><div><pre><span></span><span>/* You can select things like this:</span>
<span> *</span>
<span> * cJSON *json = cJSON_Parse(myjson_string);</span>
<span> * cJSON *width = cJSON_Select(json,".features.screens[*].width",4);</span>
<span> * cJSON *height = cJSON_Select(json,".features.screens[4].*","height");</span>
<span> * cJSON *price = cJSON_Select(json,".features.screens[4].price_*",</span>
<span> *                  price_type == EUR ? "eur" : "usd");</span>
</pre></div>
</code></pre>
<p>It's a really nice example of just how simple you can make a parser for a tiny language.</p>
<p>I think it'd be fun to re-implement this in another language to understand it better, and I might give it a go at some point.</p>

    
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[San Francisco Blows Housing Deadline, Prompting Standoff with State Regulators (116 pts)]]></title>
            <link>https://sfstandard.com/2023/11/27/san-francisco-blows-state-housing-deadline-constraints-reduction-ordinance/</link>
            <guid>38441866</guid>
            <pubDate>Tue, 28 Nov 2023 03:15:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sfstandard.com/2023/11/27/san-francisco-blows-state-housing-deadline-constraints-reduction-ordinance/">https://sfstandard.com/2023/11/27/san-francisco-blows-state-housing-deadline-constraints-reduction-ordinance/</a>, See on <a href="https://news.ycombinator.com/item?id=38441866">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>San Francisco has crossed a line in the sand, according to state housing officials.&nbsp;</p><p>That line was a deadline for passing legislation to expedite housing approvals, which came and went on Nov. 24.&nbsp;The state is giving the city another 30 days to get the bill on track before potentially revoking its permitting authority or withholding state funds, among other actions.&nbsp;</p><p>The bill in question is Mayor London Breed’s Constraints Reduction Ordinance—now in <a href="https://sfgov.legistar.com/View.ashx?M=F&amp;ID=12346119&amp;GUID=522E3F50-F707-46EA-9430-EDB185A34D22">its fifth revision</a> and set to be considered again at the Board of Supervisors’ Land Use and Transportation Committee on Monday.&nbsp;</p><p>The ordinance, which eliminates neighborhood notifications and other forms of discretionary review from many housing projects, has been <a href="https://sfstandard.com/2023/10/27/san-francisco-under-pressure-to-pass-housing-bill-or-else/">repeatedly stalled in committee</a> as supervisors tacked on amendments that weren’t necessary, according to the California Department of Housing and Community Development.&nbsp;</p><p>Those changes wound up pushing the city past a 30-day deadline, imposed <a href="https://sfstandard.com/2023/10/25/san-francisco-housing-crisis-audit-lose-local-control/">in a report published Oct. 25</a>. The board is also in defiance of a separate Oct. 26 letter, which warned supervisors to pass the <a href="https://sfstandard.com/2023/10/27/san-francisco-under-pressure-to-pass-housing-bill-or-else/">bill without amendments</a> or risk falling out of compliance with state law.&nbsp;&nbsp;</p><p>On Monday morning, Breed posted on X, formerly Twitter, warning that the city is "already slipping" in enacting its Housing Element and warned of stiff consequences  if it violates state law. </p><figure><div><blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">The right track means passing legislation that I introduced, along with Supervisors <a href="https://twitter.com/JoelEngardio?ref_src=twsrc%5Etfw">@JoelEngardio</a> and <a href="https://twitter.com/mattdorsey?ref_src=twsrc%5Etfw">@mattdorsey</a>, to remove unnecessary and burdensome rules that get in the way of housing being built.&nbsp;And it means rejecting amendments to weaken it. <a href="https://t.co/5zgcCMxwtw">https://t.co/5zgcCMxwtw</a></p>— London Breed (@LondonBreed) <a href="https://twitter.com/LondonBreed/status/1729203245384630447?ref_src=twsrc%5Etfw">November 27, 2023</a></blockquote></div></figure><p>In a statement, the state housing department said that it will send a “Corrective Action Letter” this week that gives the city another 30 days to pass the bill before having its <a href="https://sfstandard.com/2023/01/24/sf-housing-plan-passes-but-faces-tight-timeline-and-tough-road-to-implementation/">Housing Element</a> decertified.&nbsp;</p><p>Should that happen, it would subject the city to consequences such as “ineligibility or delay in receiving certain state funds and the <a href="https://sfstandard.com/2023/01/27/developers-ready-to-pounce-as-bay-area-cities-fall-behind-on-housing-deadline/#:~:text=The%20most%20imminent%20penalty%20is,some%20affordable%20housing%20is%20included.">application of the ‘builder’s remedy,’</a> among others,” according to the department.&nbsp;</p><p>If the land use committee passes the Constraints Reduction Ordinance, the full board will vote on it Tuesday. Even if the board passes the bill within the 30-day window, it’s not clear whether the tweaked bill will satisfy the state.&nbsp;</p><figure><span><span></span><img alt="Aaron Peskin speaks to a crowd outside City Hall. " sizes="(min-width: 1001px) 650px, (min-width: 768px) 550px, 100vw" srcset="https://content.sfstandard.com/wp-content/uploads/2023/10/SRO-CityFederal-092823_JY_004.jpg?w=640&amp;q=75 640w, https://content.sfstandard.com/wp-content/uploads/2023/10/SRO-CityFederal-092823_JY_004.jpg?w=750&amp;q=75 750w, https://content.sfstandard.com/wp-content/uploads/2023/10/SRO-CityFederal-092823_JY_004.jpg?w=768&amp;q=75 768w, https://content.sfstandard.com/wp-content/uploads/2023/10/SRO-CityFederal-092823_JY_004.jpg?w=828&amp;q=75 828w, https://content.sfstandard.com/wp-content/uploads/2023/10/SRO-CityFederal-092823_JY_004.jpg?w=1024&amp;q=75 1024w, https://content.sfstandard.com/wp-content/uploads/2023/10/SRO-CityFederal-092823_JY_004.jpg?w=1080&amp;q=75 1080w, https://content.sfstandard.com/wp-content/uploads/2023/10/SRO-CityFederal-092823_JY_004.jpg?w=1200&amp;q=75 1200w, https://content.sfstandard.com/wp-content/uploads/2023/10/SRO-CityFederal-092823_JY_004.jpg?w=1920&amp;q=75 1920w, https://content.sfstandard.com/wp-content/uploads/2023/10/SRO-CityFederal-092823_JY_004.jpg?w=2048&amp;q=75 2048w, https://content.sfstandard.com/wp-content/uploads/2023/10/SRO-CityFederal-092823_JY_004.jpg?w=3840&amp;q=75 3840w" src="https://content.sfstandard.com/wp-content/uploads/2023/10/SRO-CityFederal-092823_JY_004.jpg?w=3840&amp;q=75" decoding="async" data-nimg="responsive" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span><figcaption>District 3 Supervisor Aaron Peskin speaks at a press conference in Portsmouth Square in Chinatown in San Francisco on Sept. 28, 2023. | <span>Source: </span>Juliana Yamada for The Standard</figcaption></figure><p>In a Nov. 21 letter, Planning Department Director Rich Hillis wrote that the Mayor’s Office plans to introduce further changes to ensure the bill is compliant. The department is “confident that the changes made, and that continue to be made, to San Francisco’s review and approval process substantially comply with the City’s approved Housing Element,” Hillis wrote.</p><p>But community groups opposed to the legislation are expected to keep up the pressure against the mayor’s bill.</p><p>The Council of Community Housing Organizations, the San Francisco Anti-Displacement Coalition and other groups rallied members to show up to Monday’s land use meeting in support of a separate resolution pushing back on the state deadlines.&nbsp;</p><p>The nonbinding resolution, authored by Board President Aaron Peskin, urges City Attorney David Chiu to request an extension of the deadlines and to “<a href="https://sfgov.legistar.com/LegislationDetail.aspx?ID=6412806&amp;GUID=4DEFDA6D-09E3-48E1-8E57-283310D51F54&amp;Options=ID%7CText%7C&amp;Search=revise+and+correct">revise and correct</a>” its warning letter. The resolution will also be considered on Monday.&nbsp;</p><p>However, the state housing department was clear that it’s not budging.</p><p>The department said it “does not plan on revising the timelines or content” of the deadlines.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The `hanging-punctuation property` in CSS (232 pts)]]></title>
            <link>https://chriscoyier.net/2023/11/27/the-hanging-punctuation-property-in-css/</link>
            <guid>38441747</guid>
            <pubDate>Tue, 28 Nov 2023 02:58:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chriscoyier.net/2023/11/27/the-hanging-punctuation-property-in-css/">https://chriscoyier.net/2023/11/27/the-hanging-punctuation-property-in-css/</a>, See on <a href="https://news.ycombinator.com/item?id=38441747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>(I did <a href="https://twitter.com/chriscoyier/status/1699862605517713786">a Twitter thread</a> on this a while back, against my better judgment, and now moving to a proper blog.)</p>
<hr>
<p>The <code>hanging-punctuation</code> property in CSS is almost a no-brainer. The classic example is a blockquote that starts with a curly-quote. Hanging that opening curly-quote into the space off to the start of the text and aligning the actual words is a better look.</p>
<figure><img data-attachment-id="10826" data-permalink="https://chriscoyier.net/2023/11/27/the-hanging-punctuation-property-in-css/f5ceileacaammq2/" data-orig-file="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceILEacAAMMq2.jpeg?fit=1200%2C740&amp;ssl=1" data-orig-size="1200,740" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="F5ceILEacAAMMq2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceILEacAAMMq2.jpeg?fit=300%2C185&amp;ssl=1" data-large-file="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceILEacAAMMq2.jpeg?fit=1024%2C631&amp;ssl=1" decoding="async" fetchpriority="high" width="1024" height="631" src="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceILEacAAMMq2.jpeg?resize=1024%2C631&amp;ssl=1" alt="Example of hanging punction look from Steve Hickey
" srcset="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceILEacAAMMq2.jpeg?resize=1024%2C631&amp;ssl=1 1024w, https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceILEacAAMMq2.jpeg?resize=300%2C185&amp;ssl=1 300w, https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceILEacAAMMq2.jpeg?resize=768%2C474&amp;ssl=1 768w, https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceILEacAAMMq2.jpeg?w=1200&amp;ssl=1 1200w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"><figcaption><a href="https://stevehickeydesign.com/blog/2012/12/04/hanging-punctuation-with-css/">via Steve Hickey</a></figcaption></figure>
<p>Here’s a demo:</p>

<p>The blue line is just to help see the alignment.</p>
<figure><img data-attachment-id="10827" data-permalink="https://chriscoyier.net/2023/11/27/the-hanging-punctuation-property-in-css/f5cerfeaaaagybw/" data-orig-file="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceRFeaAAAgybW.jpg?fit=1200%2C614&amp;ssl=1" data-orig-size="1200,614" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="F5ceRFeaAAAgybW" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceRFeaAAAgybW.jpg?fit=300%2C154&amp;ssl=1" data-large-file="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceRFeaAAAgybW.jpg?fit=1024%2C524&amp;ssl=1" decoding="async" width="1024" height="524" src="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceRFeaAAAgybW.jpg?resize=1024%2C524&amp;ssl=1" alt="Screenshot of safari working and other browsers not working
" srcset="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceRFeaAAAgybW.jpg?resize=1024%2C524&amp;ssl=1 1024w, https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceRFeaAAAgybW.jpg?resize=300%2C154&amp;ssl=1 300w, https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceRFeaAAAgybW.jpg?resize=768%2C393&amp;ssl=1 768w, https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ceRFeaAAAgybW.jpg?w=1200&amp;ssl=1 1200w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></figure>
<p>It is a cascading property, so you can just do this if you like:</p>
<pre aria-describedby="shcb-language-1" data-shcb-language-name="CSS" data-shcb-language-slug="css"><span><code><span>html</span> {
  <span>hanging-punctuation</span>: first last;
}</code></span><small id="shcb-language-1"><span>Code language:</span> <span>CSS</span> <span>(</span><span>css</span><span>)</span></small></pre>
<p>In case you go against the grain, for aesthetics, and align text the other way, the `last` value will hang punctuation off the other else also. That’s what it’s supposed to do anyway, but in my testing (trying quotes and periods), Safari doesn’t support that. 🤷‍♀️</p>
<p>There is some risk to the property. Because the punctuation hangs off the edge, if you don’t have any available space, it can trigger a horizontal scroll bar, which sucks. This is probably why it’s not a default. It’s rare there is zero space on the edge of text, though, so meh.</p>
<figure><img data-attachment-id="10828" data-permalink="https://chriscoyier.net/2023/11/27/the-hanging-punctuation-property-in-css/f5ce2maacaahdy8/" data-orig-file="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ce2mAacAAHDy8.jpg?fit=1200%2C788&amp;ssl=1" data-orig-size="1200,788" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="F5ce2mAacAAHDy8" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ce2mAacAAHDy8.jpg?fit=300%2C197&amp;ssl=1" data-large-file="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ce2mAacAAHDy8.jpg?fit=1024%2C672&amp;ssl=1" decoding="async" width="1024" height="672" src="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ce2mAacAAHDy8.jpg?resize=1024%2C672&amp;ssl=1" alt="horizontal scrollbar triggered by hanging indentation
" srcset="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ce2mAacAAHDy8.jpg?resize=1024%2C672&amp;ssl=1 1024w, https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ce2mAacAAHDy8.jpg?resize=300%2C197&amp;ssl=1 300w, https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ce2mAacAAHDy8.jpg?resize=768%2C504&amp;ssl=1 768w, https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5ce2mAacAAHDy8.jpg?w=1200&amp;ssl=1 1200w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></figure>
<p>Want it to work across all browsers? Use a negative <code>text-indent</code> instead. Then test for support and replace it.</p>
<pre aria-describedby="shcb-language-2" data-shcb-language-name="CSS" data-shcb-language-slug="css"><span><code><span>blockquote</span> {
  <span>text-indent</span>: -<span>0.45em</span>; 
}
@​supports (<span>hanging-punctuation:</span> first) {
  <span>blockquote</span> {
    <span>text-indent</span>: <span>0</span>;
    <span>hanging-punctuation</span>: first;
  }
}</code></span><small id="shcb-language-2"><span>Code language:</span> <span>CSS</span> <span>(</span><span>css</span><span>)</span></small></pre>
<p>Having to use a magic number for the `text-indent` kinda sucks, so definitely isolate where you are applying it. Here’s a demo where a custom property is used instead to make it less weird:</p>

<p>By the way! For putting curly quotes on blockquote, might as well do that in CSS rather than in the content.</p>
<pre aria-describedby="shcb-language-3" data-shcb-language-name="CSS" data-shcb-language-slug="css"><span><code><span>blockquote</span> {
  &amp;::before {
    <span>content</span>: open-quote;
  }
  &amp;<span>::after</span> {
    <span>content</span>: close-quote;
  }
}</code></span><small id="shcb-language-3"><span>Code language:</span> <span>CSS</span> <span>(</span><span>css</span><span>)</span></small></pre>
<p>Hanging punctuation is relevant in design software and print design as well. I feel like any half-decent book typesetting will be doing this. Adobe InDesign calls it “Optical Margin Alignment”.</p>
<p><a href="https://fonts.google.com/knowledge/using_type/working_with_hanging_punctuation">Elliot Jay Stocks notes that here.</a></p>
<figure><img data-attachment-id="10829" data-permalink="https://chriscoyier.net/2023/11/27/the-hanging-punctuation-property-in-css/f5cgb3macaa_ijn/" data-orig-file="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5cgb3MacAA_IJN.jpg?fit=1200%2C665&amp;ssl=1" data-orig-size="1200,665" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="F5cgb3MacAA_IJN" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5cgb3MacAA_IJN.jpg?fit=300%2C166&amp;ssl=1" data-large-file="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5cgb3MacAA_IJN.jpg?fit=1024%2C567&amp;ssl=1" decoding="async" loading="lazy" width="1024" height="567" src="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5cgb3MacAA_IJN.jpg?resize=1024%2C567&amp;ssl=1" alt="" srcset="https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5cgb3MacAA_IJN.jpg?resize=1024%2C567&amp;ssl=1 1024w, https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5cgb3MacAA_IJN.jpg?resize=300%2C166&amp;ssl=1 300w, https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5cgb3MacAA_IJN.jpg?resize=768%2C426&amp;ssl=1 768w, https://i0.wp.com/chriscoyier.net/wp-content/uploads/2023/11/F5cgb3MacAA_IJN.jpg?w=1200&amp;ssl=1 1200w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></figure>
<p>I think <code>hanging-punctuation</code> is nice! Just a nice bonus where supported and not a huge deal if it’s not. I’d probably start a new project with:</p>
<pre aria-describedby="shcb-language-4" data-shcb-language-name="CSS" data-shcb-language-slug="css"><span><code><span>html</span> {
  <span>hanging-punctuation</span>: first allow-end last;
}</code></span><small id="shcb-language-4"><span>Code language:</span> <span>CSS</span> <span>(</span><span>css</span><span>)</span></small></pre>
 </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Outlook is good, both for yourself and 766 third parties (303 pts)]]></title>
            <link>https://godforsaken.website/@Shrigglepuss/111482466182637440</link>
            <guid>38441710</guid>
            <pubDate>Tue, 28 Nov 2023 02:52:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://godforsaken.website/@Shrigglepuss/111482466182637440">https://godforsaken.website/@Shrigglepuss/111482466182637440</a>, See on <a href="https://news.ycombinator.com/item?id=38441710">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[RPG Engine for the Nintendo 64 (132 pts)]]></title>
            <link>https://github.com/breadbored/N64-RPG</link>
            <guid>38441643</guid>
            <pubDate>Tue, 28 Nov 2023 02:43:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/breadbored/N64-RPG">https://github.com/breadbored/N64-RPG</a>, See on <a href="https://news.ycombinator.com/item?id=38441643">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">RPG Engine for the Nintendo 64</h2>
<h2 tabindex="-1" dir="auto">About</h2>
<p dir="auto">This unnamed project is an attempt to write all the parts of an RPG from scratch. Below is the list of features currently being written:</p>
<ul>
<li> Actors</li>
<li> Player</li>
<li> Camera</li>
<li> NPCs</li>
<li> Collision</li>
<li> Map Rendering
<ul>
<li> Background (Landscape) Tilemap</li>
<li> Foreground (Structure / Building) Tilemap</li>
<li> NPC Map Layer</li>
<li> Entrance/Exit Layer
<ul>
<li> Entrance/Exit Triggers</li>
<li> Map Switching</li>
</ul>
</li>
<li> Map Builder (external tool; Tiled)</li>
<li> Tilemap Renderer from Map Builder's Binaries</li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Setup</h2>
<p dir="auto">If you are on Windows, you will need to first set up WSL to get a linux environment in Windows</p>
<p dir="auto">You will need to install the following (if you are using WSL, you will need to install inside that environment):</p>
<ul dir="auto">
<li>Docker</li>
<li>NodeJS &gt;v18</li>
<li>NPM</li>
</ul>
<p dir="auto">To get libdragon setup, you will need to run the following:</p>
<ul dir="auto">
<li>Make sure the Docker daemon is running</li>
<li><code>npm i libdragon -g</code></li>
<li><code>cd /path/to/this/project</code></li>
<li><code>libdragon init</code></li>
</ul>
<p dir="auto">To build the project:</p>
<ul dir="auto">
<li><code>libdragon make assets</code></li>
<li><code>libdragon make</code></li>
</ul>
<p dir="auto">This will produce an N64 ROM  called <code>breacodes.z64</code></p>
<h2 tabindex="-1" dir="auto">Playing</h2>
<h3 tabindex="-1" dir="auto">The best way to play is on a real Nintendo 64 via a flashcart</h3>
<p dir="auto">However if you must use an emulator, you must use an accurate emulator such as Ares. Common emulators are <em><strong>not</strong></em> accurate. They are typically optimized to run official games at a good speed, but are not hardware accurate. <code>libdragon</code> is a different from the official SDK, and most emulators cover emulation for the official SDK only, meaning it requires hardware accuracy or specially built support.</p>
<h2 tabindex="-1" dir="auto">Editing the map</h2>
<p dir="auto">To make the map, I used software called <a href="https://thorbjorn.itch.io/tiled?download" rel="nofollow">Tiled</a>. You can find the project file at <code>map/n64-rpg.tiled-project</code>. I export as a CSV called <code>overworld.csv</code> and it will generate a CSV for each layer.</p>
<p dir="auto">To build the mpa, run the included tool with <code>python3 tools/makemap.py</code> to generate the map file.</p>
<p dir="auto">Important notes:</p>
<p dir="auto">I work only in the positive area (0,0) and higher. Do not add any tiles in the negative coordinates or it will shift the entire map.</p>
<p dir="auto">Tiled does not export with perfect coordinates in CSV format, so every layer must have a tile in the (0,0) coordinate. This is to prevent offset problems. For example, the background layer has a grass tile at (0,0), and the foreground layer has a tree.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Orion Browser by Kagi (237 pts)]]></title>
            <link>https://kagi.com/orion/</link>
            <guid>38441139</guid>
            <pubDate>Tue, 28 Nov 2023 01:33:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kagi.com/orion/">https://kagi.com/orion/</a>, See on <a href="https://news.ycombinator.com/item?id=38441139">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <h2>Bring your favorite extensions.</h2>
                <p>
                    <h3>
                    Orion offers native support for many Firefox and Chrome browser
                    extensions allowing access to the world's largest
                    eco-system of browser extensions.                    </h3>
                    <h3>
                    We're still in the process of expanding our extension support to include all available options. Simultaneously, we're working on bringing this feature to iOS
                    and Orion is the first browser that allows you to
                    install
                    select web extensions directly from the Chrome Web Store or Firefox
                    Add-Ons on your iPhone or iPad.
                    </h3>
                    <h2><a href="https://kagi.com/orion/faq.html#extensions">Read more</a>
                    </h2>
                </p>
            </div><div>
            <p>
                1. Orion is currently available for Apple macOS, iPadOS and iOS. We are planning support for other platforms in the future.</p>
            <p>
                2. We have ported the Web Extensions API to WebKit, allowing
                Orion to run Chrome and Firefox extensions out of the box.
                This support is still experimental while we are in beta and
                some extensions may not work properly.
            </p>
            <p>
                3. Live Text is a macOS feature allowing to interact with a text in the photo, currently supported in <a href="https://www.apple.com/macos/feature-availability/#live-text">multiple languages</a>.
            </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Play keeps banning the same web browser due to vague DMCA notices (159 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/11/weird-dmca-takedown-google-play-bans-app-because-it-can-load-warnerbros-com/</link>
            <guid>38440194</guid>
            <pubDate>Mon, 27 Nov 2023 23:43:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/11/weird-dmca-takedown-google-play-bans-app-because-it-can-load-warnerbros-com/">https://arstechnica.com/tech-policy/2023/11/weird-dmca-takedown-google-play-bans-app-because-it-can-load-warnerbros-com/</a>, See on <a href="https://news.ycombinator.com/item?id=38440194">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/05/downloader-browser-800x450.png" alt="Screenshot of the Google home page displayed on Downloader, an Android app with a built-in browser.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/05/downloader-browser.png" data-height="1080" data-width="1920">Enlarge</a> <span>/</span> The Downloader app that was suspended from Google Play.</p></figcaption>  </figure>

  




<!-- cache hit 74:single/related:c86ee390c9378031a828d29352811a8a --><!-- empty -->
<p>App developer Elias Saba has had some bad luck with Digital Millennium Copyright Act (DMCA) takedowns. His Android TV app Downloader, which combines a web browser with a file manager, was <a href="https://arstechnica.com/tech-policy/2023/05/google-bans-downloader-app-after-tv-firms-complain-it-can-load-a-pirate-website/">suspended by Google Play in May</a> after several Israeli TV companies complained that the app could be used to load a pirate website.</p>
<p>Google <a href="https://arstechnica.com/tech-policy/2023/06/google-un-bans-downloader-app-but-developer-still-mad-about-broken-dmca/">reversed that suspension</a> after three weeks. But Downloader has been suspended by Google Play again, and this time the reason is even harder to understand. Based on a vague DMCA notice, it appears that Downloader was suspended simply because it can load <a href="https://www.warnerbros.com/">the Warner Bros. website</a>.</p>
<p>Downloader is similar to standard web browsers in that it lets users access both legal and illegally shared content. The app can be used for general web surfing and can download files from a website when a user inputs the desired URL. According to Saba, the app itself contains no infringing content, nor does it direct users to infringing content.</p>
<p>Google notified Saba that the app was suspended again last night, according to the notice that Saba shared with Ars. "Your app contains content that allegedly infringes upon the copyright of others, and violates applicable copyright laws in the relevant country/jurisdiction," the notice from Google said.</p>                                            
                                                        
<p>The notice includes a copy of the DMCA complaint, which came from <a href="https://markscan.co.in/">MarkScan</a>, a "digital asset protection" firm that content owners hire to enforce copyrights. MarkScan said in its complaint that it represents Warner Bros. Discovery Inc.</p>
<h2>“Properties of Warner Bros. Discovery”</h2>
<p>A DMCA notice is supposed to identify and describe the copyrighted work that was infringed. But MarkScan's notice about Downloader identifies the copyrighted work only as "Properties of Warner Bros. Discovery Inc." It provides no detail on which Warner Bros. work was infringed by Downloader.</p>
<p>A DMCA notice is also supposed to provide an example of where someone can see "an authorized example of the work." In this field, MarkScan simply entered the main Warner Bros. URL: <a href="https://www.warnerbros.com/" target="_blank" rel="noopener">https://www.warnerbros.com/</a>.</p>
<p>"I have a good faith belief that use of the copyrighted materials described above as allegedly infringing is not authorized by the copyright owner, its agent, or the law," MarkScan's notice said.</p>
<p>Unsurprisingly, Saba is outraged. "You would think that Google would at least verify that the takedown request is actually making a plausible claim," he told Ars today. "The most important field in the takedown where the claimant has to specify where the copyright infringement exists is void of all detail. If this complete lack of information is all it takes to take an app down, then no app in the Google Play Store is safe from being suspended with just a few clicks and a frivolous takedown request."</p>
<p>The Downloader app had been installed over 10 million times, according to an <a href="https://web.archive.org/web/20230824170506/https://play.google.com/store/apps/details?id=com.esaba.downloader">Internet Archive capture</a> taken before the latest suspension.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linexjlin/GPTs: leaked prompts of GPTs (129 pts)]]></title>
            <link>https://github.com/linexjlin/GPTs</link>
            <guid>38440011</guid>
            <pubDate>Mon, 27 Nov 2023 23:24:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/linexjlin/GPTs">https://github.com/linexjlin/GPTs</a>, See on <a href="https://news.ycombinator.com/item?id=38440011">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
          <p>      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?
</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How could Al Bundy afford a house when he was making minimum wage? (143 pts)]]></title>
            <link>https://old.reddit.com/r/AskHistorians/comments/ggozng/in_the_sitcom_married_with_children_protagonist/</link>
            <guid>38439851</guid>
            <pubDate>Mon, 27 Nov 2023 23:08:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/AskHistorians/comments/ggozng/in_the_sitcom_married_with_children_protagonist/">https://old.reddit.com/r/AskHistorians/comments/ggozng/in_the_sitcom_married_with_children_protagonist/</a>, See on <a href="https://news.ycombinator.com/item?id=38439851">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>[–]<a href="https://old.reddit.com/user/MaroonTrojan">MaroonTrojan</a><span></span> <span title="53">53 points</span><span title="54">54 points</span><span title="55">55 points</span> <time title="Mon May 11 08:47:25 2020 UTC" datetime="2020-05-11T08:47:25+00:00">3 years ago</time><time title="last edited 3 years ago" datetime="2020-05-11T08:50:37+00:00">*</time>&nbsp;(4 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_fq961juhnz"><div itemprop="text"><p>I'd like to respond to this question from the opposite angle, which is: is it realistic that a Chicago homeowner family where the man is a retail worker would have a homemaker wife in the 1980s/1990s. The answer-- as others have pointed out-- is no. But it's interesting to look at why in the early 1990s that might have been a popular choice for a sitcom family dynamic and how Peg Bundy differed from other sitcom women of her era and those who came before her.</p>

<p>Let's look at the original sitcom woman: Lucy. She managed to live in an apartment with wall-to-wall carpeting and fresh drapes while living on a single-income non-white entertainer's salary in the 1950s and maintaining a friendship with her landlord and his wife. Was that realistic? Probably not. But the show didn't get into the details of how they paid the bills. It presented the normalized idea that men were the breadwinners and women stayed at home and had lots of spare time to get into crazy schemes. Her schemes-- even when they were purely economical ones-- were always motivated internally. Lucy wanted fame: on television, or in the club, or in Hollywood. She never tried to advance herself to make money. Because that's not the sort of thing a (successful?) woman did.</p>

<p>The next relevant sitcom wife of the era was Laura Pietrie (Mary Tyler Moore on the Dick Van Dyke show). Again, here was a pretty domestic wife, now in a New York suburb instead of the city. On a few occasions she did go after money opportunities-- like appearing on a game show-- but the premise of the episode was never that the family's economic security was in doubt. Instead, the woman grasping after success was characterized as reaching at "even more" economic success for "already successful" households.</p>

<p>In reality, in this era, the work situation was anything but rosy or optional. Firms like Manpower and Kelly Girl were discovering the power of the female "temp" workforce. Why pay a full-time (male) clerk (who would expect benefits and a pension) when you could just as easily hire a Kelly Girl "temp" who was "just doing this for some extra money" and didn't have to be paid-- even thought of-- if she were sick, unavailable, or anything else. You were paying Kelly Girl, and if your usual Kelly Girl was unavailable, they'd just send a different one.</p>

<p>These details of women entering the workplace are detailed in TEMP by Louis Hyman, who is a labor economist at Cornell. In his book, he makes the point that in the post-war 20th Century, women were essential in establishing the idea of "temp" labor, and once big corporate management consultancy firms (McKinsey being the big one) figured out how much more profitable it was to hire only temp workers-- the economy was forever changed.</p>

<p>But the real changes in the economy were light-years ahead of what we saw on TV. In 1977 we saw the release of The Mary Tyler Moore Show: a big deal because it featured a single woman renting an apartment and holding a newspaper job on her own: contrasted by Phyllis, of course, a "normal" woman with a child and husband. But there was also Rhoda, Mary's "weird" neighbor who washed her own windows and wore trousers. Fifty years ago, these details were groundshaking.</p>

<p>Anyway, fast forward through the series of spinoffs from the MGM TV world and Norman Lear's universe to pause briefly on Edith Bunker. Here again-- 25 years after Lucy-- was a homemaker wife who would never even DREAM of holding a job other than taking care of her husband. And suddenly-- magically-- that's what makes her ridiculous. Edith is always portrayed as the voice of... well-- if not reason-- common sense maybe. And although Archie Bunker's household is still living as if it's the 1950s... they're certainly not living as well as Lucy did in the 1950s... Meathead is not afraid to point this out. Edith is.</p>

<p>Moving on further: the pilot of Cheers. 1980s now. Diane enters Cheers to enjoy a toast: she's just gotten engaged. But her fiancé runs off to be with his ex wife so suddenly... she needs a job? What an unusual and unexpected position for an educated white woman to be in!? She takes the job but immediately the stakes turn into a sexual ping-pong match between her and her boss. I guess the idea that she was even at the table was considered revolutionary.</p>

<p>Later, there was Murphy Brown. She was the opposite of all these sitcom wives: married to her career. Sexless. But a woman! And hot! The writers couldn't resist and eventually a pregnancy story got built into the show, which Dan Quayle-- Vice-President-at-the-Time who once spelled potato wrong and for a brief period of time that's the stupidest we thought Republicans could ever be-- took great offense to. Anyway he tried to burn Murphy Brown but historically speaking he was too stupid to and failed. The show carried on for several seasons including Murphy once stealing the 'T' off TRUMP Tower but the point was always that she was some sort of exceptional, outsize woman, not anybody normal. A woman, but not a wife. This was new and revolutionary.</p>

<p>So we've reached Peg Bundy. After generations where the hits had sterling, strong wives: why was she a bon-bon eating, stay at home manicurist? And why was it so successful? Well... in a weird way... because it was new. As much as these outsider perspectives on women in television had on women making advancements in the field of women's equality, there were still lots of places in America (and then later, the rest of the world) where women had NOT seen the sort of economic and labor equalizing forces that defined the 20th Century. </p>

<p>Married With Children became a huge international hit. In Russia it aired five nights a week: they recreated the entire English language series, then commissioned the original American writing staff to write an entire extra season that would air in Russia (and then get re-broadcast in Ukraine, Poland, etc.). </p>

<p>My point is that since there was such a social incentive to make the point that your wife didn't "need" to work, she just did to "get out of the house" or whatever, the idea of working women wasn't normalized on television until the late 90s, when Al Bundy: supporter of a decadent wife and even more decadent children became the face of "enough is enough". Men saw-- through Al Bundy-- an economy that had long ceased to exist (a single male breadwinner; no education but the expectation is to put the kids through college) and didn't get the joke. You're out here checking Chicago real estate prices instead of asking the bigger question: were single-income retail families viable in the 1990s? No. They were not. </p>

<p>What this meant for women was especially tough. There was a tacit understanding that going to work was something "everyone was doing" but was also "not normal". That can't hold for long. In the  late 90s and early 00s we see and justify women working (in general) with them excelling in female-driven careers: Monica is a chef; Rachel is a personal shopper at Bloomingdales. We've since moved on from a family dynamic in the Chicago suburbs and are looking at a sitcom area dominated by childless attractive people unstressed by the worries that 9/11 might convey.</p>

<p>And we've been carrying forward our unwillingness to confront those troubles for 20 years now. </p>

<p>I am struggling to imagine a broadly successful sitcom pilot in which we don't ask the question, "but why does the wife have a job?" The best answer might be Modern Family. That pilot aired in 2009, and still, the answer was <em>because she works for her dad</em>. </p>

<p>The good question isn't 'why doesn't Peg Bundy have a job in the early 90's'. The good question is "why, in the early 90's, would we look at Peg Bundy and assume she's real?" That question has everything to do with a calculated effort to denormalize women as equal contributors to the household and hypernormalize the male breadwinner at a time when it was so anti-normal that we were there to laugh at it. As we're seeing now: people adhere to old visions of society until a big impact occasion forces them to change focus.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/AskHistorians/comments/ggozng/in_the_sitcom_married_with_children_protagonist/fq961ju/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[After 151 years, Popular Science will no longer offer a magazine (232 pts)]]></title>
            <link>https://www.theverge.com/2023/11/27/23978042/popular-science-digital-magazine-discontinued</link>
            <guid>38439817</guid>
            <pubDate>Mon, 27 Nov 2023 23:06:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/11/27/23978042/popular-science-digital-magazine-discontinued">https://www.theverge.com/2023/11/27/23978042/popular-science-digital-magazine-discontinued</a>, See on <a href="https://news.ycombinator.com/item?id=38439817">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>After 151 years, <em>Popular Science</em> will no longer be available to purchase as a magazine. In a statement to <em>The Verge</em>, Cathy Hebert, the communications director for <em>PopSci</em> owner Recurrent Ventures, says the outlet needs to “evolve” beyond its magazine product, which published its <a href="https://www.popsci.com/story/science/popsci-spring-2021-digital-issue/">first all-digital issue in 2021</a>.</p><p><em>PopSci</em>, which covers a whole range of stories related to the fields of science, technology, and nature, published its first issue in 1872. Things have changed a lot over the years, with the magazine <a href="https://nypost.com/2018/07/07/this-mag-accurately-predicted-the-future-95-years-ago/">switching to a quarterly publication</a> schedule in 2018 and doing away with the physical copies altogether after 2020.</p><section tabindex="0" aria-label="Zoom" role="button"><p><span><img alt="<em>The November 1921 issue of </em>Popular Science." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/376x522/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/384x533/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/415x576/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/480x666/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/540x749/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/640x888/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/750x1041/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/828x1149/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/1080x1499/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/1200x1665/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/1440x1998/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/1920x2664/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/2048x2842/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/2400x3330/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:686x952/2400x3330/filters:focal(343x476:344x477):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115638/PopularScienceNovember1921.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span><span><img alt="<em>The July 1922 issue of </em>Popular Science<em>.</em>" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/376x531/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/384x542/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/415x586/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/480x677/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/540x762/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/640x903/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/750x1058/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/828x1169/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/1080x1524/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/1200x1694/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/1440x2032/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/1920x2710/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/2048x2890/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/2400x3387/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:680x960/2400x3387/filters:focal(340x480:341x481):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115637/PopularScienceJuly1922.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span><span><img alt="<em>The January 1925 issue of </em>Popular Science." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/376x535/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/384x547/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/415x591/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/480x683/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/540x769/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/640x911/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/750x1068/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/828x1179/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/1080x1538/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/1200x1709/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/1440x2050/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/1920x2734/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/2048x2916/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/2400x3417/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1224/2400x3417/filters:focal(430x612:431x613):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115635/PopularScienceJanuary1925.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span><span><img alt="<em>The April 1925 issue of </em>Popular Science." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/376x530/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/384x541/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/415x585/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/480x676/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/540x761/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/640x902/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/750x1057/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/828x1166/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/1080x1521/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/1200x1691/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/1440x2029/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/1920x2705/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/2048x2885/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/2400x3381/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:860x1212/2400x3381/filters:focal(430x606:431x607):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115632/PopularScienceApril1925.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span><span><img alt="<em>The December 1925 issue of </em>Popular Science." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/376x529/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/384x540/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/415x584/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/480x676/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/540x760/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/640x901/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/750x1056/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/828x1165/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/1080x1520/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/1200x1689/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/1440x2027/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/1920x2702/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/2048x2883/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/2400x3378/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:894x1258/2400x3378/filters:focal(447x629:448x630):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25115634/PopularScienceDecember1925.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><div><p>1<!-- -->/<!-- -->5</p><div><figcaption><em>The November 1921 issue of </em>Popular Science.</figcaption> <p><cite>Image: Popular Science via Wikimedia Commons</cite></p></div></div></section><p>In <a href="https://www.linkedin.com/posts/bita-saha_popularscience-activity-7130385752944750592--B1o?utm_source=share&amp;utm_medium=member_android">a post on LinkedIn</a>, former <em>PopSci</em> editor Purbita Saha commented on the magazine’s discontinuation, stating she’s “frustrated, incensed, and appalled that the owners shut down a pioneering publication that’s adapted to 151 years worth of changes in the space of a five-minute Zoom call.” Layoffs have impacted journalists on the science beat particularly hard in recent weeks. <a href="https://www.cnn.com/2023/06/28/media/national-geographic-magazine-staff-writers/index.html"><em>National Geographic</em> cut</a> the remainder of the magazine’s editorial staff in June, followed by <a href="https://www.linkedin.com/posts/angelymercado_climatechange-climatejournalism-climatejustice-activity-7128768163462148098-pKWd?utm_source=share&amp;utm_medium=member_desktop"><em>Gizmodo</em> laying off</a> its last climate reporter, and <a href="https://talkingbiznews.com/media-news/climate-reporter-clifford-among-those-laid-off-at-cnbc-com/">CNBC shuttering its climate desk</a> last week.</p><p>“PopSci is a phenomenal brand, and as consumer trends shift it’s important we prioritize investment in new formats,” Herbert tells <em>The Verge</em>. “We believe that the content strategy has to evolve beyond the digital magazine product. A combination of its news team, along with commerce, video, and other initiatives, will produce content that naturally aligns with PopSci’s mission.”</p><div><p>PopSci laid off several employees earlier this month, leaving around five editorial staff members</p></div><p>In addition to dropping its magazine format, <em>PopSci</em> laid off several employees earlier this month, leaving around five editorial staff members and “a few” workers on the publication’s commerce team, <a href="https://www.axios.com/2023/11/13/recurrent-ventures-layoffs">according to <em>Axios</em></a>. The digital media group Recurrent Ventures <a href="https://www.prnewswire.com/news-releases/recurrent-ventures-named-the-new-parent-company-of-popular-science-the-drive-domino-field--stream-and-other-brands-301312885.html">acquired <em>PopSci</em></a><em> </em>in 2021 and <a href="https://www.adweek.com/media/recurrent-ventures-new-ceo-andrew-perlman/">named its third CEO</a> in three years just one week before the layoffs hit. </p><p><em>PopSci</em> will continue to offer articles on its website, along with its PopSci Plus subscription, which offers access to exclusive content and the magazine’s archive. However, its discontinuation marks the end of an era, and the other cuts across the science journalism field won’t make it easier to stay up to date on the <a href="https://www.theverge.com/2023/11/20/23969101/temperature-check-climate-change-united-nations-negotiations-dubai">state of our climate</a> or dive into <a href="https://www.theverge.com/c/23307867/human-composting-process-return-home">fascinating stories</a> that you might not otherwise come across without the media outlets that bring them to our attention.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let's try to understand AI monosemanticity (316 pts)]]></title>
            <link>https://www.astralcodexten.com/p/god-help-us-lets-try-to-understand</link>
            <guid>38438261</guid>
            <pubDate>Mon, 27 Nov 2023 21:04:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.astralcodexten.com/p/god-help-us-lets-try-to-understand">https://www.astralcodexten.com/p/god-help-us-lets-try-to-understand</a>, See on <a href="https://news.ycombinator.com/item?id=38438261">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>You’ve probably heard AI is a “black box”. No one knows how it works. Researchers simulate a weird type of pseudo-neural-tissue, “reward” it a little every time it becomes a little more like the AI they want, and eventually it becomes the AI they want. But God only knows what goes on inside of it.</p><p>This is bad for safety. For safety, it would be nice to look inside the AI and see whether it’s executing an algorithm like “do the thing” or more like “trick the humans into thinking I’m doing the thing”. But we can’t. Because we can’t look inside an AI at all. </p><p><span>Until now! </span><strong><a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html" rel="">Towards Monosemanticity</a></strong><span>, recently out of big AI company/research lab Anthropic, claims to have gazed inside an AI and seen its soul. It looks like this:</span></p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 848w, https://substackcdn.com/image/fetch/w_1228,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 1228w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 1456w, https://substackcdn.com/image/fetch/w_2456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 2456w, https://substackcdn.com/image/fetch/w_3684,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 3684w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1228,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 848w, https://substackcdn.com/image/fetch/w_1228,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 1228w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 1456w, https://substackcdn.com/image/fetch/w_2456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 2456w, https://substackcdn.com/image/fetch/w_3684,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5a9650-204e-4170-a6ca-6cd357bacbca_780x487.png 3684w" width="1228"></picture></div></figure><p><span>How did they do it? What </span><em>is</em><span> inside of an AI? And what the heck is “monosemanticity”?</span></p><p><em>[disclaimer: after talking to many people much smarter than me, I might, just barely, sort of understand this. Any mistakes below are my own.]</em></p><p>A stylized neural net looks like this:</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png 4368w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F510ef384-8418-413e-9494-eb7fe63318a8_700x350.png 4368w" width="1456"></picture></div></figure><p>Input neurons (blue) take information from the world. In an image AI, they might take the values of pixels in the image; in a language AI, they might take characters in a text.</p><p>These connect to interneurons (black) in the “hidden layers”, which do mysterious things.</p><p>Then those connect to output neurons (green). In an image AI, they might represent values of pixels in a piece of AI art; in a language AI, characters in the chatbot response.</p><p>“Understanding what goes on inside an AI” means understanding what the black neurons in the middle layer do.</p><p>A promising starting point might be to present the AI with lots of different stimuli, then see when each neuron does vs. doesn’t fire. For example, if there’s one neuron that fires every time the input involves a dog, and never fires any other time, probably that neuron is representing the concept “dog”.</p><p>Sounds easy, right? A good summer project for an intern, right?</p><p>There are at least two problems.</p><p>First, GPT-4 has over 100 billion neurons (the exact number seems to be secret, but it’s somewhere up there).</p><p><span>Second, this doesn’t work. When you switch to a weaker AI with “only” a few hundred neurons and build special tools to automate the stimulus/analysis process, the neurons aren’t this simple. A few low-level ones respond to basic features (like curves in an image). But deep in the middle, where the real thought has to be happening, there’s nothing representing “dog”. Instead, the neurons are much weirder than this. In one image model, an </span><a href="https://distill.pub/2020/circuits/zoom-in/" rel="">earlier paper</a><span> found “one neuron that responds to cat faces, fronts of cars, and cat legs”. The authors described this as “polysemanticity” - multiple meanings for one neuron.</span></p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 1272w, https://substackcdn.com/image/fetch/w_1358,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 1358w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 1456w, https://substackcdn.com/image/fetch/w_2716,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 2716w, https://substackcdn.com/image/fetch/w_4074,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 4074w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1358,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png" sizes="100vw" alt="Bizarre pictures resembling psychedelic cat faces, car fronts, and cat legs." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 1272w, https://substackcdn.com/image/fetch/w_1358,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 1358w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 1456w, https://substackcdn.com/image/fetch/w_2716,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 2716w, https://substackcdn.com/image/fetch/w_4074,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7b0831-f9c5-4c69-aa63-11c124788f8a_606x205.png 4074w" title="The three images that most strongly activate neuron 4e:55" width="1358"></picture><figcaption>The three images that most strongly activate neuron 4e:55</figcaption></div></figure><p><span>Some very smart people spent a lot of time trying to figure out what conceptual system could make neurons behave like this, and came up with the </span><strong><a href="https://transformer-circuits.pub/2022/toy_model/index.html" rel="">Toy Models Of Superposition</a></strong><span> paper.</span></p><p>Their insight is: suppose your neural net has 1,000 neurons. If each neuron represented one concept, like “dog”, then the net could, at best, understand 1,000 concepts. Realistically it would understand many fewer than this, because in order to get dogs right, it would need to have many subconcepts like “dog’s face” or “that one unusual-looking dog”. So it would be helpful if you could use 1,000 neurons to represent much more than 1,000 concepts.</p><p><span>Here’s a way to make two neurons represent five concepts (</span><a href="https://transformer-circuits.pub/2022/toy_model/index.html" rel="">adapted from here</a><span>):</span></p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png 4368w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec5a30b-ddcb-4114-8b1d-bca34aef713f_472x393.png 4368w" width="1456"></picture></div></figure><p>If neuron A is activated at 0.5, and neuron B is activated at 0, you get “dog”. </p><p>If neuron A is activated at 1, and neuron B is activated at 0.5, you get “apple”. </p><p>And so on.</p><p>The exact number of vertices in this abstract shape is a tradeoff. More vertices means that the two-neuron-pair can represent more concepts. But it also risks confusion. If you activate the concepts “dog” and “heart” at the same time, the AI might interpret this as “apple”. And there’s some weak sense in which the AI interprets “dog” as “negative eye”.</p><p>This theory is called “superposition”. Do AIs really do it? And how many vertices do they have on their abstract shapes?</p><p><span>The Anthropic interpretability team trained a very small, simple AI. It needed to remember 400 features, but it had only 30 neurons, so it would have to try something like the superposition strategy. Here’s what they found (slightly edited from </span><a href="https://transformer-circuits.pub/2022/toy_model/index.html" rel="">here</a><span>):</span></p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png 4368w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F157938c0-cc3b-4bfa-8c21-7513581cca04_1433x1046.png 4368w" width="1456"></picture></div></figure><p>Follow the black line. On the far left of the graph, the data is dense; you need to think about every feature at the same time. Here the AI assigns one neuron per concept (meaning it will only ever learn 30 of the 400 concepts it needs to know, and mostly fail the task).</p><p>Moving to the right, we allow features to be less common - the AI may only have to think about a few at a time. The AI gradually shifts to packing its concepts into tetrahedra (three neurons per four concepts) and triangles (two neurons per three concepts). When it reaches digons (one neuron per two concepts) it stops for a while (to repackage everything this way?) Next it goes through pentagons and an unusual polyhedron called the “square anti-prism” . . . </p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 424w, https://substackcdn.com/image/fetch/w_632,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 632w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 848w, https://substackcdn.com/image/fetch/w_1264,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 1264w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 1456w, https://substackcdn.com/image/fetch/w_1896,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 1896w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_632,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 424w, https://substackcdn.com/image/fetch/w_632,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 632w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 848w, https://substackcdn.com/image/fetch/w_1264,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 1264w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 1456w, https://substackcdn.com/image/fetch/w_1896,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72403ec2-1c5e-4e15-92c1-0095cc089258_950x893.png 1896w" title="Source: https://en.wikipedia.org/wiki/Square_antiprism#/media/File:Square_antiprism.png" width="632"></picture><figcaption>Source: https://en.wikipedia.org/wiki/Square_antiprism#/media/File:Square_antiprism.png</figcaption></div></figure><p><span>. . . which </span><a href="https://en.wikipedia.org/wiki/Biscornu" rel="">Wikipedia says</a><span> is best known for being the shape of the </span><a href="https://en.wikipedia.org/wiki/Biscornu" rel="">biscornu</a><span> (a “stuffed ornamental pincushion”) and </span><a href="https://en.wikipedia.org/wiki/One_World_Trade_Center" rel="">One World Trade Center</a><span> in New York:</span></p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png 4368w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png" sizes="100vw" alt="Picture of a pincushion and One World Trade Center" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2707d6a2-bb85-49c6-a160-fe98e0de184b_1128x403.png 4368w" title="Freedom Tower confirmed as fundamental to the nature of thought itself; America can't stop winning." width="1456"></picture><figcaption>Freedom Tower confirmed as fundamental to the nature of thought itself; America can't stop winning.</figcaption></div></figure><p>After exhausting square anti-prisms (8 features per three neurons) it gives up. Why? I don’t know. </p><p>A friend who understands these issues better than I warns that we shouldn’t expect to find pentagons and square anti-prisms in GPT-4. Probably GPT-4 does something incomprehensible in 1000-dimensional space. But it’s the 1000-dimensional equivalent of these pentagons and square anti-prisms, conserving neurons by turning them into dimensions and then placing concepts in the implied space.</p><p><span>The Anthropic interpretability team describes this as simulating a more powerful AI. That is, the two-neuron AI in the pentagonal toy example above is simulating a five-neuron AI. They go on to prove that the real AI can then run computations in the simulated AI; in some sense, there really </span><em>is</em><span> an abstract five neuron AI doing all the cognition. The only reason all of our AIs aren’t simulating infinitely powerful AIs and letting </span><em>them</em><span> do all the work is that as real neurons start representing more and more simulated neurons, it produces more and more noise and conceptual interference.</span></p><p><span>This is great for AIs but bad for interpreters. We hoped we could figure out what our AIs were doing just by looking at them. But it turns out they’re simulating much bigger and more complicated AIs, and if we want to know what’s going on, we have to look at </span><em>those</em><span>. But </span><em>those</em><span> AIs only exist in simulated abstract hyperdimensional spaces. Sounds hard to dissect!</span></p><p>Still, last month Anthropic’s interpretability team announced that they successfully dissected of one of the simulated AIs in its abstract hyperdimensional space. </p><p><span>(finally, we’re back to </span><a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html" rel="">the monosemanticity paper</a><span>!)</span></p><p>First the researchers trained a very simple 512-neuron AI to predict text, like a tiny version of GPT or Anthropic’s competing model Claude.</p><p>Then, they trained a second AI called an autoencoder to predict the activations of the first AI. They told it to posit a certain number of features (the experiments varied between ~2,000 and ~100,000), corresponding to the neurons of the higher-dimensional AI it was simulating. Then they made it predict how those features mapped onto the real neurons of the real AI.</p><p><span>They found that even though the original AI’s neurons weren’t comprehensible, the new AI’s simulated neurons (aka “features”) were! They were </span><em>monosemantic</em><span>, ie they meant one specific thing.</span></p><p><span>Here’s </span><a href="https://transformer-circuits.pub/2023/monosemantic-features/vis/a1.html#feature-2663" rel="">feature #2663</a><span> (remember, the original AI only had 512 neurons, but they’re treating it as simulating a larger AI with up to ~100,000 neuron-features). </span></p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png 4368w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb112770b-f685-4974-9cfb-16230cf4cdb0_1065x642.png 4368w" width="1456"></picture></div></figure><p>Feature #2663 represents God.</p><p>The single sentence in the training data that activated it most strongly is from Josephus, Book 14: “And he passed on to Sepphoris, as God sent a snow”. But we see that all the top activations are different uses of “God”.</p><p>This simulated neuron seems to be composed of a collection of real neurons including 407, 182, and 259, though probably there are many more than these and the interface just isn’t showing them to me. </p><p><span>None of these neurons are themselves very Godly. When we look at </span><a href="https://transformer-circuits.pub/2023/monosemantic-features/vis/a-neurons.html#feature-407" rel="">neuron #407</a><span> - the real neuron that contributes most to the AI’s understanding of God! - an AI-generated summary describes it as “fir[ing] primarily on non-English text, particularly accented Latin characters. It also occasionally fires on non-standard text like HTML tags.” Probably this is because you can’t really understand AIs at the real-neuron-by-real-neuron level, so the summarizing AI - having been asked to do this impossible thing - is reading tea leaves and saying random stuff.</span></p><p>But at the feature level, everything is nice and tidy! Remember, this AI is trying to predict the next token in a text. At this level, it does so intelligibly. When Feature #2663 is activated, it increases the probability of the next token being “bless”, “forbid”, “damn”, or “-zilla”.</p><p>Shouldn’t the AI be keeping the concept of God, Almighty Creator and Lord of the Universe, separate from God- as in the first half of Godzilla? Probably GPT-4 does that, but this toy AI doesn’t have enough real neurons to have enough simulated neurons / features to spare for the purpose. In fact, you can see this sort of thing change later in the paper:</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 848w, https://substackcdn.com/image/fetch/w_1194,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 1194w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 1456w, https://substackcdn.com/image/fetch/w_2388,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 2388w, https://substackcdn.com/image/fetch/w_3582,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 3582w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1194,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 848w, https://substackcdn.com/image/fetch/w_1194,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 1194w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 1456w, https://substackcdn.com/image/fetch/w_2388,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 2388w, https://substackcdn.com/image/fetch/w_3582,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47311206-f51c-4142-b7ba-3baa17a949dc_1072x1474.png 3582w" width="1194"></picture></div></figure><p>At the bottom of this tree, you can see what happens to the AI’s representation of “the” in mathematical terminology as you let it have more and more features.</p><p>First: why is there a feature for “the” in mathematical terminology? I think because of the AI’s predictive imperative - it’s helpful to know that some specific instance of “the” should be followed by math words like “numerator” or “cosine”. </p><p>In their smallest AI (512 features), there is only one neuron for “the” in math. In their largest AI tested here (16,384 features), this has branched out to one neuron for “the” in machine learning, one for “the” in complex analysis, and one for “the” in topology and abstract algebra.</p><p>So probably if we upgraded to an AI with more simulated neurons, the God neuron would split in two - one for God as used in religions, one for God as used in kaiju names. Later we might get God in Christianity, God in Judaism, God in philosophy, et cetera.</p><p>Not all features/simulated-neurons are this simple. But many are. The team graded 412 real neurons vs. simulated neurons on subjective interpretability, and found the simulated neurons were on average pretty interpretable:</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png 4368w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e75c4cc-254c-46fb-b217-d6fbbbc1aa8a_1368x496.png 4368w" width="1456"></picture></div></figure><p>Some, like the God neuron, are for specific concepts. Many others, including some of the most interpretable, are for “formal genres” of text, like whether it’s uppercase or lowercase, English vs. some other alphabet, etc.</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png 4368w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0c1a581-9cb0-4391-b338-7b4930e1e43b_1020x344.png 4368w" width="1456"></picture></div></figure><p>How common are these features? That is, suppose you train two different 4,096-feature AIs on the same text datasets. Will they have mostly the same 4,096 features? Will they both have some feature representing God? Or will the first choose to represent God together with Godzilla, and the second choose to separate them? Will the second one maybe not have a feature for God at all, instead using that space to store some other concept the first AI can’t possibly understand? </p><p>The team tests this, and finds that their two AIs are pretty similar! On average, if there’s a feature in the first one, the most similar feature in the second one will “have a median correlation of 0.72”.</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png 4368w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png 1456w, https://substackcdn.com/image/fetch/w_2912,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png 2912w, https://substackcdn.com/image/fetch/w_4368,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d36d3c2-1461-4bff-9582-9f8a8a860756_1371x794.png 4368w" width="1456"></picture></div></figure><p>What comes after this?</p><p><span>In May of this year, OpenAI </span><a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models" rel="">tried to make GPT-4 (very big) understand GPT-2 (very small)</a><span>. They got GPT-4 to inspect each of GPT-2’s 307,200 neurons and report back on what it found.</span></p><p>It found a collection of intriguing results and random gibberish, because they hadn’t mastered the techniques described above of projecting the real neurons into simulated neurons and analyzing the simulated neurons instead. Still, it was impressively ambitious. Unlike the toy AI in the monosemanticity paper, GPT-2 is a real (albeit very small and obsolete) AI that once impressed people.</p><p>But what we really want is to be able to interpret the current generation of AIs. The Anthropic interpretability team admits we’re not there yet, for a few reasons.</p><p><em>First</em><span>, scaling the autoencoder:</span></p><blockquote><p>Scaling the application of sparse autoencoders to frontier models strikes us as one of the most important questions going forward. We're quite hopeful that these or similar methods will work – Cunningham et al.'s work seems to suggest this approach can work on somewhat larger models, and we have preliminary results that point in the same direction. However, there are significant computational challenges to be overcome. Consider an autoencoder with a 100× expansion factor applied to the activations of a single MLP layer of width 10,000: it would have ~20 billion parameters. Additionally, many of these features are likely quite rare, potentially requiring the autoencoder to be trained on a substantial fraction of the large model's training corpus. So it seems plausible that training the autoencoder could become very expensive, potentially even more expensive than the original model. We remain optimistic, however, and there is a silver lining – it increasingly seems like a large chunk of the mechanistic interpretability agenda will now turn on succeeding at a difficult engineering and scaling problem, which frontier AI labs have significant expertise in.</p></blockquote><p>In other words, in order to even begin to interpret an AI like GPT-4 (or Anthropic’s equivalent, Claude), you would need an interpreter-AI around the same size. But training an AI that size takes a giant company and hundreds of millions (soon billions) of dollars. </p><p><em>Second</em><span>, scaling the interpretation. Suppose we find all the simulated neurons for God and Godzilla and everything else, and have a giant map of exactly how they connect, and hang that map in our room. Now we want to answer questions like:</span></p><ul><li><p>If you ask the AI a controversial question, how does it decide how to respond?</p></li><li><p>Is the AI using racial stereotypes in forming judgments of people?</p></li><li><p>Is the AI plotting to kill all humans?</p></li></ul><p>There will be some combination of millions of features and connections that answers these questions. In some case we can even imagine how we would begin to do it - check how active the features representing race are when we ask it to judge people, maybe. But realistically, when we’re working with very complex interactions between millions of neurons we’ll have to automate the process, some larger scale version of “ask GPT-4 to tell us what GPT-2 is doing”. </p><p><span>This probably works for racial stereotypes. It’s more complicated once you start asking about killing all humans (what if the GPT-4 equivalent is the one plotting to kill all humans, and feeds us false answers?) But maybe there’s some way to make an interpreter AI which itself is too dumb to plot, but which can interpret a more general, more intelligent, more dangerous AI. You can see more about how this could tie into more general alignment plans in </span><a href="https://www.astralcodexten.com/p/elk-and-the-problem-of-truthful-ai" rel="">the post on the ELK problem</a><span>. I also just found </span><a href="https://www.ai-transparency.org/" rel="">this paper</a><span>, which I haven’t fully read yet but which seems like a start on engineering safety into interpretable AIs.</span></p><p>Finally, what does all of this tell us about humans?</p><p>Humans also use neural nets to reason about concepts. We have a lot of neurons, but so does GPT-4.  Our data is very sparse - there are lots of concepts (eg octopi) that come up pretty rarely in everyday life. Are our brains full of strange abstract polyhedra? Are we simulating much bigger brains?</p><p><span>This field is very new, but I was able to find one paper, </span><a href="https://arxiv.org/abs/2310.11431" rel="">Identifying Interpretable Visual Features in Artificial and Biological Neural Systems</a><span>. The authors say:</span></p><blockquote><p>Through a suite of experiments and analyses, we find evidence consistent with the hypothesis that neurons in both deep image model [AIs] and the visual cortex [of the brain] encode features in superposition. That is, we find non-axis aligned directions in the neural state space that are more interpretable than individual neurons. In addition, across both biological and artificial systems, we uncover the intriguing phenomenon of what we call feature synergy - sparse combinations in activation space that yield more interpretable features than the constituent parts. Our work pushes in the direction of automated interpretability research for CNNs, in line with recent efforts for language models. Simultaneously, it provides a new framework for analyzing neural coding properties in biological systems.</p></blockquote><p><span>This is a single non-peer-reviewed paper announcing a surprising claim in a hype-filled field. That means it </span><em>has</em><span> to be true - otherwise it would be unfair!</span></p><p>If this topic interests you, you might want to read the full papers, which are much more comprehensive and interesting than this post was able to capture. My favorites are:</p><ul><li><p><a href="https://distill.pub/2020/circuits/zoom-in/" rel="">An Introduction To Circuits</a></p></li><li><p><a href="https://transformer-circuits.pub/2022/toy_model/index.html" rel="">Toy Models of Superposition</a></p></li><li><p><a href="https://transformer-circuits.pub/2023/superposition-composition/index.html" rel="">Distribution Representations: Composition &amp; Superposition</a></p></li><li><p><a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html" rel="">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a></p></li></ul><p><span>In the unlikely scenario where all of this makes total sense and you feel like you’re ready to make contributions, you might be a good candidate for Anthropic or OpenAI’s alignment teams, both of which are hiring. If you feel like it’s the sort of thing which </span><em>could</em><span> make sense and you want to transition into learning more about it, you might be a good candidate for alignment training/scholarship programs like </span><a href="https://www.matsprogram.org/" rel="">MATS</a><span>.</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cheaper microscope could bring protein mapping technique to the masses (147 pts)]]></title>
            <link>https://www.science.org/content/article/cheaper-microscope-could-bring-protein-mapping-technique-masses</link>
            <guid>38438000</guid>
            <pubDate>Mon, 27 Nov 2023 20:45:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/cheaper-microscope-could-bring-protein-mapping-technique-masses">https://www.science.org/content/article/cheaper-microscope-could-bring-protein-mapping-technique-masses</a>, See on <a href="https://news.ycombinator.com/item?id=38438000">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/cheaper-microscope-could-bring-protein-mapping-technique-masses: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[AI SDR which automatically finds and converts leads for Your business (484 pts)]]></title>
            <link>https://jusdr.com</link>
            <guid>38437114</guid>
            <pubDate>Mon, 27 Nov 2023 19:38:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jusdr.com">https://jusdr.com</a>, See on <a href="https://news.ycombinator.com/item?id=38437114">Hacker News</a></p>
Couldn't get https://jusdr.com: Error: getaddrinfo ENOTFOUND jusdr.com]]></description>
        </item>
        <item>
            <title><![CDATA[Strange Metals: Where electricity may flow without electrons (137 pts)]]></title>
            <link>https://www.quantamagazine.org/meet-strange-metals-where-electricity-may-flow-without-electrons-20231127/</link>
            <guid>38436851</guid>
            <pubDate>Mon, 27 Nov 2023 19:18:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/meet-strange-metals-where-electricity-may-flow-without-electrons-20231127/">https://www.quantamagazine.org/meet-strange-metals-where-electricity-may-flow-without-electrons-20231127/</a>, See on <a href="https://news.ycombinator.com/item?id=38436851">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>After a year of trial and error, Liyang Chen had managed to whittle down a metallic wire into a microscopic strand half the width of an <em>E.coli</em> bacterium — just thin enough to allow a trickle of electric current to pass through. The drips of that current might, Chen hoped, help settle a persistent mystery about how charge moves through a bewildering class of materials known as strange metals.</p>
<p>Chen, then a graduate student, and his collaborators at Rice University measured the current flowing through their atoms-thin strand of metal. And they found that it flowed smoothly and evenly. So evenly, in fact, that it defied physicists’ standard conception of electricity in metals.</p>
<p>Canonically, electric current results from the collective movement of electrons, each carrying one indivisible chunk of electric charge. But the dead steadiness of Chen’s current implied that it wasn’t made of units at all. It was like finding a liquid that somehow lacked individually recognizable molecules.</p>
<p>While that might sound outlandish, it’s exactly what some physicists expected from the metal the group tested, which along with its unusual kin has beguiled and bewildered physicists since the 1980s. “It’s a very beautiful piece of work,” said <a href="http://sachdev.physics.harvard.edu/">Subir Sachdev</a>, a theoretical physicist at Harvard University who specializes in strange metals.</p>
<p>The observation, <a href="https://www.science.org/doi/10.1126/science.abq6100">reported last week</a> in the journal <em>Science</em>, is one of the most straightforward indications yet that whatever carries current through these unusual metals doesn’t look anything like electrons. The new experiment strengthens suspicions that a new quantum phenomenon is arising within strange metals. It also provides new grist for theoretical physicists attempting to understand what it might be.<strong><em>&nbsp;</em></strong></p>
<p>“Strange metals, no one has any earthly idea where they’re coming from,” said <a href="https://physics.illinois.edu/people/directory/profile/abbamont">Peter Abbamonte</a>, a physicist at the University of Illinois, Urbana-Champaign. “It used to be considered an inconvenience, but now we realize it’s really a different phase of matter living in these things.”</p>
<h2><strong>A Cuprate Wrench</strong></h2>
<p>The first challenge to the conventional understanding of metals came in 1986, when Georg Bednorz and Karl Alex Müller rocked the physics world with their discovery of high-temperature superconductors — materials that perfectly carry an electric current even at relatively warm temperatures. Familiar metals like tin and mercury become superconductors only when chilled to within a few degrees of absolute zero. Bednorz and Müller measured the electrical resistance in a copper-based (“cuprate”) material and saw that it vanished at a relatively balmy 35 kelvins. (For their breakthrough discovery, Bednorz and Müller pocketed a Nobel Prize just a year later.)</p>

<p>Physicists soon realized that high-temperature superconductivity was only the beginning of the mysterious behavior of the cuprates.</p>
<p>The cuprates got really weird when they stopped superconducting and started resisting. As all metals warm, resistance increases. Warmer temperatures mean atoms and electrons jiggle more, creating more resistance-inducing collisions as electrons shuttle current through a material. In normal metals, such as nickel, resistance rises quadratically at low temperatures — slowly at first and then faster and faster. But in the cuprates, it rose linearly: Each degree of warming brought the same increase in resistance — a bizarre pattern that continued over hundreds of degrees and, in terms of strangeness, overshadowed the material’s superconducting ability. The cuprates were the strangest metals researchers had ever seen.</p>
<p>“Superconductivity is a mouse,” said <a href="https://cse.umn.edu/physics/andrey-chubukov">Andrey Chubukov</a>, a theoretical physicist at the University of Minnesota. “The elephant … is this strange metal behavior.”</p>
<p>The linear rise in resistance threatened a celebrated explanation of how electric charge moves through metals. Proposed in 1956, Lev Landau’s “Fermi liquid” theory placed electrons at the center of it all. It built upon earlier theories that, for simplicity, assumed that electrons carry electric current, and that the electrons move through a metal like a gas; they flit freely between atoms without interacting with each other.</p>
<p>Landau added a way of handling the crucial but complicated fact that electrons interact. They are negatively charged, which means they constantly repel each other. Considering this interaction between the particles transformed the electron gas into something of an ocean — now, as one electron moved through the fluid of electrons, it disturbed the nearby electrons. Through a complicated series of interactions involving mutual repulsion, these now gently interacting electrons ended up traveling in crowds — in clumps known as quasiparticles.</p>

<p>The miracle of Fermi liquid theory was that each quasiparticle behaved almost exactly as if it were a single, fundamental electron. One major difference, though, was that these blobs moved more sluggishly or more nimbly (depending on the material) than a bare electron, effectively acting heavier or lighter. Now, just by adjusting the mass terms in their equations, physicists could continue to treat current as the movement of electrons, only with an asterisk specifying that each electron was really a quasiparticle clump.</p>
<p>A major triumph of Landau’s framework was that in normal metals, it nailed the complicated way in which resistance rises quadratically with temperature. Electron-like quasiparticles became the standard way of understanding metals. “It’s in every textbook,” Sachdev said.</p>
<p>But in the cuprates, Landau’s theory failed dramatically. Resistance rose in an immaculate line rather than the standard quadratic curve. Physicists have long interpreted this line as a sign that cuprates are home to a new physical phenomenon.</p>
<p>“You pretty much have to believe that nature is either giving you a clue or nature is incredibly cruel,” said <a href="https://physics.fsu.edu/person/gregory-boebinger">Gregory Boebinger</a>, a physicist at Florida State University who has spent much of his career studying the cuprates’ linear response. “To put up such a terribly simple and beguiling signature and to have it not be physically important would just be too much to bear.”</p>

<p>And the cuprates were just the beginning. Researchers have since discovered a <a href="https://www.science.org/doi/10.1126/science.abh4273">host of disparate materials</a> with the same alluring linear resistance, including organic “Bechgaard salts” and misaligned sheets of graphene. As these “strange metals” proliferated, scientists wondered why Landau’s Fermi fluid theory seemed to break down in all these different materials. Some came to suspect that it was because there were no quasiparticles at all; the electrons were somehow organizing themselves in a strange new way that obscured any individuality, much as the discrete nature of grapes gets lost in a bottle of wine.</p>
<p>“It’s a phase of matter where an electron really has no identity,” Abbamonte said. “Nevertheless, [a strange metal] is a metal; it somehow carries current.”</p>
<p>But one does not simply abolish electrons. To some scientists, a potentially continuous electric current — one that isn’t divvied up into electrons — is too radical. And <a href="https://www.nature.com/articles/s41586-021-03697-8">some strange metal experiments</a> continue to match certain predictions of Landau’s theory. The persisting controversy prompted Chen’s thesis adviser, <a href="https://profiles.rice.edu/faculty/douglas-natelson">Douglas Natelson</a> of Rice University, along with his colleague <a href="https://profiles.rice.edu/faculty/qimiao-si">Qimiao Si</a>, to consider how they might more directly scrutinize the anatomy of the charge moving through a strange metal.</p>
<p>“What could I measure that would actually tell me what’s going on?” Natelson wondered.</p>
<h2><strong>The Anatomy of Electricity</strong></h2>
<p>The team’s goal was to dissect the current in a strange metal. Did it come in electron-size chunks of charge? Did it come in chunks at all? To find out, they took inspiration from a classic way of measuring fluctuations in a flow — the “shot noise” — a phenomenon that can be understood if we think of the ways that rain might fall during a rainstorm.</p>
<p>Imagine you’re sitting in your car, and you know from a trustworthy weather forecast that 5 millimeters of rain will fall over the next hour. Those 5 millimeters are like the total electrical current. If that rain is parceled into a handful of giant drops, the variation in when those drops hit your roof will be high; sometimes drops will splatter back to back, and at other times they will be spaced out. In this case, the shot noise is high. But if the same 5 millimeters of rain is spread into a constant mist of tiny droplets, the variation in arrival time — and therefore the shot noise — will be low. The mist will smoothly deliver almost the same amount of water from moment to moment. In this way, shot noise reveals the size of the drops.</p>
<p>“Just measuring the rate at which water shows up doesn’t tell you the whole picture,” Natelson said. “Measuring the fluctuations [in that rate] tells you a lot more.”</p>
<p>Similarly, listening to the crackle in electric current can tell you about the chunks of charge that make it up. Those chunks are normally Landau’s electron-like quasiparticles. Indeed, recording the shot noise in a normal metal is a common way of measuring the fundamental charge of the electron — 1.6 × 10<sup>−19 </sup>coulombs.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sports Illustrated Published Articles by Fake, AI-Generated Writers (175 pts)]]></title>
            <link>https://futurism.com/sports-illustrated-ai-generated-writers</link>
            <guid>38436516</guid>
            <pubDate>Mon, 27 Nov 2023 18:53:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://futurism.com/sports-illustrated-ai-generated-writers">https://futurism.com/sports-illustrated-ai-generated-writers</a>, See on <a href="https://news.ycombinator.com/item?id=38436516">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="incArticle"><p>There was nothing in Drew Ortiz's <a href="https://web.archive.org/web/20221205082417/https://www.si.com/review/author/drewortiz/">author biography</a> at <i>Sports Illustrated </i><span>to suggest that he was anything other than human. </span></p><p>"Drew has spent much of his life outdoors, and is excited to guide you through his never-ending list of the best products to keep you from falling to the perils of nature," it read. "Nowadays, there is rarely a weekend that goes by where Drew isn't out camping, hiking, or just back on his parents' farm."</p><p>The only problem? Outside of <i>Sports Illustrated</i>, Drew Ortiz doesn't seem to exist. He has no social media presence and no publishing history. And even more strangely,&nbsp;his profile photo on <em>Sports Illustrated </em><a href="https://archive.is/ndV3A">is for sale</a> on a website that sells AI-generated headshots,&nbsp;where he's described as "neutral white young-adult male with short brown hair and blue eyes."</p><figure></figure><figure></figure><p>Ortiz isn't the only AI-generated author published by <i>Sports</i> <em>Illustrated</em>,&nbsp;according to a person involved with the creation of the content who asked to be kept anonymous to protect them from professional repercussions.</p><p>"There's a lot," they told us of the fake authors. "I was like, what are they? This is ridiculous. This person does not exist."</p><p>"At the bottom [of the page] there would be a photo of a person and some fake description of them like, 'oh, John lives in Houston, Texas. He loves yard games and hanging out with his dog, Sam.' Stuff like that," they continued. "It's just crazy."</p><p>The AI authors' writing often sounds like it was written by an alien;&nbsp;<a href="https://web.archive.org/web/20221004090814/https://www.si.com/review/full-size-volleyball/">one Ortiz article</a>, for instance, warns that volleyball "can be a little tricky to get into, especially without an actual ball to practice with."</p><p>According to a second person involved in the creation of the <em>Sports Illustrated</em> content who also asked to be kept anonymous, that's because it's not just the authors' headshots that are AI-generated. At least some of the articles themselves, they said, were churned out using AI as well.</p><p>"The content is absolutely AI-generated,"&nbsp;the second source said, "no matter how much they say that it's not."</p><p>After we reached out with questions to the magazine's publisher, The Arena Group, all the AI-generated authors disappeared from <em>Sports Illustrated</em>'s site without explanation. Our questions received no response.</p><p>The AI content marks a staggering fall from grace for <em>Sports Illustrated</em>, which in past decades won numerous National Magazine Awards for its sports journalism and published work by literary giants ranging from William Faulkner to John Updike.</p><p>But now that it's under the management of The Arena Group, parts of the magazine seem to have devolved into a Potemkin Village in which phony writers are cooked up out of thin air, outfitted with equally bogus biographies and expertise to win readers' trust, and used to pump out AI-generated buying guides that are monetized by affiliate links to products that provide a financial kickback when readers click them.</p><p><em>Do you know anything about The Arena Group's use of AI-generated content? Shoot us an email at <a href="mailto:tips@futurism.com">tips@futurism.com</a>. We can keep you anonymous.</em></p><p>Making the whole thing even more dubious, these AI-generated personas are periodically scrubbed from existence in favor of new ones.</p><p><span>Sometime this summer, for example, Ortiz disappeared from<em> Sports Illustrated</em>'s site entirely, his profile page instead redirecting to that of a</span><span> "<a href="https://archive.is/D9DV2">Sora Tanaka</a>." Again, there's no online record of a writer by that name — but Tanaka's profile picture <a href="https://generated.photos/face/joyful-asian-young-adult-female-with-long-brown-hair-and-brown-eyes--5f1836d41f67b00006889840">is for sale</a> on the same AI headshot marketplace as Ortiz, where she's listed as "joyful asian young-adult female with long brown hair and brown eyes."</span></p><p><span>"Sora has always been a fitness guru, and loves to try different foods and drinks," read Tanaka's bio. "Ms. Tanaka is thrilled to bring her fitness and nutritional expertise to the Product Reviews Team, and promises to bring you nothing but the best of the best."</span></p><figure></figure><figure></figure><p>But Tanaka didn't last, either. Eventually she also disappeared, replaced by yet another profile that carried no headshot at all, which <em>Sports Illustrated</em> deleted along with the other AI-generated content after we reached out.</p><p>It wasn't just author&nbsp;profiles that the magazine repeatedly replaced. Each time an author was switched out, the posts they supposedly penned would be reattributed to the new persona, with no editor's note explaining the change in byline.</p><p>None of the articles credited to Ortiz or the other names contained any disclosure about the use of AI or that the writer wasn't real, though they did eventually gain a disclaimer explaining that the content was "created by a 3rd party," and that the "<em>Sports Illustrated</em> editorial staff are not involved in the creation of this content."</p><p><em>Do you know anything about that "3rd party," or how the content was created? Email us at <a href="mailto:tips@futurism.com">tips@futurism.com</a>. We can keep you anonymous.</em></p><p>Though <i>Sports Illustrated</i>'s AI-generated authors and their articles disappeared after we asked about them, similar operations appear to be alive and well elsewhere in The Arena Group's portfolio.</p><p>Take <em>TheStreet</em>, a financial publication cofounded by Jim Cramer in 1996 that The Arena Group <a href="https://www.businesswire.com/news/home/20190808005215/en/Maven-Closes-Acquisition-TheStreet-New-Jim-Cramer">bought for $16.5 million</a> in 2019. Like at<em> Sports Illustrated</em>, we found authors at <em>TheStreet </em>with highly specific biographies&nbsp;detailing seemingly flesh-and-blood humans with specific areas of expertise — but with profile photos traceable to that same AI face website. And like at <em>Sports Illustrated</em>, these fake writers are periodically wiped from existence and their articles reattributed to new names, with no disclosure about the use of AI.</p><p>Sometimes <em>The</em><em>Street</em>'s efforts to remove the fake writers can be sloppy. On its <a href="https://archive.is/EVdBw">review </a><a href="https://archive.is/EVdBw">section's title page</a>,&nbsp;for instance, the site still proudly flaunts the expertise of AI-generated contributors who have since been deleted, linking to writer profiles it describes as ranging "from <a href="https://www.thestreet.com/review/author/domino-abrams/">stay-at-home dads</a> to <a href="https://www.thestreet.com/review/author/denise-mcnamara/">computer and information analysts</a>." This team,&nbsp;the site continues, "is comprised of a well-rounded group of people who bring varying backgrounds and experiences to the table."</p><p>People? We're not so sure.</p><p>The "stay-at-home dad" linked in that sentence above, for instance, is a so-called "<a href="https://web.archive.org/web/20211022203137/https://www.thestreet.com/review/author/domino-abrams/">Domino Abrams</a><span>" — "a pro at home cleaning and maintenance," at least until he was expunged from the site — whose profile picture can <a href="https://images.generated.photos/2X9jxxHg-BqU-lY8ER7BYjhM7v7Mk1lvBdZLexTtQjo/rs:fit:256:256/czM6Ly9pY29uczgu/Z3Bob3Rvcy1wcm9k/LnBob3Rvcy92M18w/ODk3Njc1LmpwZw.jpg">again be found</a>&nbsp;on that same site that sells AI-generated headshots. </span></p><figure></figure><p><span>Or look at "<a href="https://web.archive.org/web/20220706182957/https://www.thestreet.com/review/author/denise-mcnamara/">Denise McNamara</a>," the "information analyst" that <em>TheStreet</em> boasted about — "her extensive personal experience with electronics allows her to share her findings with others online" — whose profile picture is <a href="https://images.generated.photos/aArmEGQt-wLB_b9taXiittj3hoiXzDkOp8Npv-yDnSk/rs:fit:256:256/czM6Ly9pY29uczgu/Z3Bob3Rvcy1wcm9k/LnBob3Rvcy90cmFu/c3BhcmVudF92My92/M18wMzc1ODk5LnBu/Zw.png">once again listed</a> on the same AI headshot marketplace. </span><span>Or "<a href="https://web.archive.org/web/20211203045751/https://www.thestreet.com/review/author/nicole-merrifield/">Nicole Merrifield</a>," an alleged "first grade teacher" who "loves helping people," but whose profile is <a href="https://images.generated.photos/a7aiK5ivGIOmRmVNbqI17mxJ6bknNdLxB6l2LQTEFkA/rs:fit:256:256/czM6Ly9pY29uczgu/Z3Bob3Rvcy1wcm9k/LnBob3Rvcy92M18w/MTQ5Nzc5LmpwZw.jpg">again from</a>&nbsp;that AI headshot site. </span><span>(At some point this year, Abrams, McNamara, and Merrifield were replaced by bylines whose profile pictures aren't for sale on the AI headshot site.)</span></p><p>Basic scrutiny shows that the quality of the AI authors' posts is often poor, with bizarre-sounding language and glaring formatting discrepancies.</p><p>This <a href="https://web.archive.org/web/20221004132724/https://www.thestreet.com/review/improve-your-finances/">article about personal finance</a> by the AI-generated Merrifield,<span> for example, starts off with the sweeping libertarian claim that "your financial status translates to your value in society." </span></p><p>After that bold premise, the article explains that "people with strong financial status are revered and given special advantages everywhere around the world," and launches into a numbered list of how you can "improve your finance status" for yourself. Each number on what should be a five-point list, though, is just number one. Mistakes happen, but we can't imagine that anyone who can't count to five would give stellar financial advice.</p><figure></figure><p>Abysmal-quality AI content, though, shouldn't be surprising at The Arena Group.</p><p>Back in February, when the company first started publishing AI-generated health advice at its magazine <em>Men's Journal</em>,&nbsp;<a href="https://futurism.com/neoscope/magazine-mens-journal-errors-ai-health-article">we found that its first story</a> was riddled with errors, prompting it to issue a massive correction.</p><p>Before that, when The Arena Group first announced its foray into AI, its CEO Ross Levinsohn promised in an <a href="https://www.wsj.com/articles/sports-illustrated-publisher-taps-ai-to-generate-articles-story-ideas-11675428443">interview with&nbsp;<em>The Wall Street Journal</em></a> that its quality would be outstanding.</p><p>"It’s not about 'crank out AI content and do as much as you can,'" he told the newspaper early this year. "Google will penalize you for that and more isn't better; better is better."</p><p>Needless to say, neither fake authors who are suddenly replaced with different names nor deplorable-quality AI-generated content with no disclosure amount to anything resembling good journalism, and to see it published by a&nbsp;once-iconic magazine like <em>Sports Illustrated </em>is disheartening. Bylines exist for a reason: they give credit where it's due, and just as importantly, they let readers hold writers accountable.</p><p>The undisclosed AI content is a direct affront to the fabric of media ethics, in other words, not to mention a perfect recipe for eroding reader trust. And at the end of the day, it's just remarkably irresponsible behavior that we shouldn't see anywhere — let alone normalized by a high-visibility publisher.</p><p>The Arena Group is also hardly alone, either. As powerful generative AI tools have debuted over the past few years, many publishers have quickly attempted to use the tech to churn out monetizable content.</p><p>In almost every case, though, these efforts to cut out human journalists have backfired embarrassingly.</p><p>We <a href="https://futurism.com/the-byte/cnet-publishing-articles-by-ai">caught&nbsp;<em>CNET</em></a> and&nbsp;<a href="https://futurism.com/cnet-bankrate-restarts-ai-articles"><em>Bankrate</em></a>, both owned by Red Ventures, publishing barely-disclosed AI content that was filled with <a href="https://futurism.com/cnet-ai-errors">factual mistakes</a> and even <a href="https://futurism.com/cnet-ai-plagiarism">plagiarism</a>; in the ensuing storm of criticism, <em>CNET&nbsp;</em>issued corrections to <a href="https://www.theverge.com/2023/1/25/23571082/cnet-ai-written-stories-errors-corrections-red-ventures">more than half</a> its AI-generated articles. G/O Media also published AI-generated material on its portfolio of sites, resulting in embarrassing bungles <a href="https://futurism.com/gizmodo-kotaku-staff-furious-ai-content">at <em>Gizmodo</em></a> and&nbsp;<a href="https://futurism.com/the-av-club-imdb"><em>The A.V. Club</em></a>. We <a href="https://futurism.com/buzzfeed-publishing-articles-by-ai">caught </a><em><a href="https://futurism.com/buzzfeed-publishing-articles-by-ai">BuzzFeed</a>&nbsp;</em>publishing slapdash AI-generated travel guides. And <em>USA Today&nbsp;</em>and other Gannett newspapers <a href="https://www.cnn.com/2023/08/30/tech/gannett-ai-experiment-paused/index.html">were busted publishing</a> hilariously garbled AI-generated sports roundups that one of the company's own sports journalists <a href="https://futurism.com/gannett-sports-writer-ai-generated-content">described as</a> "embarrassing," saying they "shouldn't ever" have been published.</p><p>If any media organization finds a way to engage with generative AI in a way that isn't either woefully ill-advised or actively unethical, we're all ears. In the meantime, forgive us if we don't hold our breath.</p><p><strong>More on AI-generated journalism: </strong><a href="https://futurism.com/usa-today-updates-ai-generated-sports"><em>USA Today</em> Updates Every AI-Generated Sports Article to Correct "Errors"</a></p><br></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Broadcom lays off many VMware employees after closing acquisition (320 pts)]]></title>
            <link>https://www.businessinsider.com/broadcom-vmware-layoffs-employees-face-job-cuts-acquisition-2023-11</link>
            <guid>38436419</guid>
            <pubDate>Mon, 27 Nov 2023 18:48:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/broadcom-vmware-layoffs-employees-face-job-cuts-acquisition-2023-11">https://www.businessinsider.com/broadcom-vmware-layoffs-employees-face-job-cuts-acquisition-2023-11</a>, See on <a href="https://news.ycombinator.com/item?id=38436419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="piano-inline-content-wrapper" data-piano-inline-content-wrapper=""> 
                    
                    
                    
                          
                          
                          <section data-offer-key="pre-churn-offer" data-component-type="inline-offer" data-place-after-element-selector=".post-content .content-lock-content > p">
                            <article>
                              <img src="https://www.businessinsider.com/public/assets/subscription/marketing/banner-overlay/top-left.svg" alt="">
                              <img src="https://www.businessinsider.com/public/assets/subscription/marketing/banner-overlay/bottom-right.svg" alt="">
                          
                                        </article>
                          </section>
                    
                    <div data-component-type="content-lock" data-load-strategy="exclude">
                                  <ul><li>Some VMware employees learned Monday their positions would be eliminated.</li><li>Broadcom announced it closed its $69 billion acquisition of VMware on November 22.</li><li>VMware had already started cutting jobs prior to the deal closing.</li></ul><!-- Excluded mobile ad on desktop --><div id="formContainer" data-component-type="inline-newsletter-module" data-event-label="insider_today" data-newsletter-id="1" data-list="Insider Today" data-acq-source="techinlinesignup">
                        
                        
                          <section>
                              
                        
                            <p><svg version="1.1" xmlns="http://www.w3.org/2000/svg" role="img" width="50" height="50" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50;" xml:space="preserve">
                          <title>Loading</title>
                          <desc>Something is loading.</desc>
                          <path fill="#111" d="M43.935,25.145c0-10.318-8.364-18.683-18.683-18.683c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615c8.072,0,14.615,6.543,14.615,14.615H43.935z">
                            <animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform>
                          </path>
                        </svg></p>
                            
                        
                            
                        
                            <div>
                              <p>Thanks for signing up!</p>
                              
                              <p>
                              Access your favorite topics in a personalized feed while you're on the go.
                                    </p>
                            </div>
                        
                            
                            
                          </section>
                        
                            <div>
                                <p><img src="https://www.businessinsider.com/public/assets/rebrand/newsletter-bull.png" data-old-src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1 1'%3E%3C/svg%3E" data-src="/public/assets/rebrand/newsletter-bull.png">
                              
                              
                              
                              </p>    </div>
                        
                          
                        </div><p>Some VMware employees learned on Monday that their positions <a target="_blank" href="https://www.businessinsider.com/broadcom-vmware-huge-layoffs-acquisition-2023-8" data-analytics-product-module="body_link" rel="">would be eliminated</a> following Broadcom's announcement last week that its deal to acquire the company had closed.</p><p><a target="_blank" href="https://www.businessinsider.com/vmware-chaos-broadcom-acquisition-layoffs-conference-employees-customers-2022-8" data-analytics-product-module="body_link" rel="">Broadcom first announced</a> in May 2022 that it would acquire <a target="_blank" href="https://www.businessinsider.com/vmware-partners-optimistic-broadcom-acquisition-cozy-up-to-customers-2022-9" data-analytics-product-module="body_link" rel="">VMware</a> for $61 billion and assume $8 billion of its net debt. The company announced that it closed the deal on November 22, shortly after receiving regulatory clearance from China. It also had to receive regulatory clearance from other countries including the US and the UK.</p><p>Employees whose positions were eliminated received an email on Monday, viewed by Business Insider, that read: "Broadcom recently completed its acquisition of VMware. As part of integration planning, and following an organizational needs assessment, we identified go-forward roles that will be required within the combined company. We regret to inform you that your position is being eliminated and your employment will be terminated."</p><p>"We would like to thank you for your dedication and service. We want to make this transition as smooth as possible, including offering you a generous severance package and providing you a non-working paid notice period," the email continued.</p><!-- Excluded mobile ad on desktop --><p>Currently, it's unclear exactly how many employees will be affected by the cuts. <a target="_blank" href="https://www.businessinsider.com/broadcom-ceo-vmware-explore-conference-customer-employee-concern2022-8" data-analytics-product-module="body_link" rel="">Broadcom</a> did not immediately respond to a request for comment.</p><p><a target="_blank" href="https://www.businessinsider.com/vmware-employees-poached-aws-google-recruiters-after-broadcom-acqusition-2022-8" data-analytics-product-module="body_link" rel="">VMware</a> — which, as of February, employed more than 38,000 people — had already <a target="_blank" href="https://www.businessinsider.com/vmware-layoffs-job-cuts-broadcom-acquisition-deadline-looms-2023-10" data-analytics-product-module="body_link" rel="">cut jobs</a> before the acquisition closed, BI previously reported. <a target="_blank" href="https://www.businessinsider.com/vmware-broadcom-acquisition-deal-leaked-email-options-employees-2023-9" data-analytics-product-module="body_link" rel="">VMware sent</a> a letter to employees in September saying that they would be offered a job by Broadcom, offered a transitional role, or receive a severance package.</p><p>Some <a target="_blank" href="https://www.businessinsider.com/broadcom-vmware-deal-employees-concerned-carbon-black-euc-2023-5" data-analytics-product-module="body_link" rel="">VMware employees</a> speculated that <a target="_blank" href="https://www.businessinsider.com/broadcom-vmware-euc-carbon-black-divest-units-2023-5" data-analytics-product-module="body_link" rel="">Broadcom could spin out</a> certain units.</p><p>Several <a target="_blank" href="https://www.businessinsider.com/meet-vmwares-top-executive-departures-and-hires-in-2022-2022-8" data-analytics-product-module="body_link" rel="">top VMware executives</a> have left the cloud-computing company over the past year. Some <a target="_blank" href="https://www.businessinsider.com/vmware-employees-are-leaving-in-droves-after-broadcom-acquisition-2022-8" data-analytics-product-module="body_link" rel="">VMware employees</a> said they worried about a culture clash with Broadcom, especially <a target="_blank" href="https://www.businessinsider.com/broadcom-ceo-comments-pressure-vmware-staff-come-into-office-wfh-2022-8" data-analytics-product-module="body_link" rel="">since Broadcom requires</a> workers to return to the office. They also said some deals <a target="_blank" href="https://www.businessinsider.com/vmware-sales-slowdown-contracts-salespeople-frustration-2022-8" data-analytics-product-module="body_link" rel="">had slowed as clients waited for news about VMware's fate.</a></p><!-- Excluded mobile ad on desktop --><p><strong><em>Got a tip?</em></strong><em>&nbsp;Contact this reporter via email at&nbsp;</em><a target="_blank" rel="noopener nofollow" href="mailto:rmchan@businessinsider.com" data-analytics-product-module="body_link"><em><u>rmchan@businessinsider.com</u></em></a><em>, Signal at 646.376.6106, or Telegram at @rosaliechan.&nbsp;(PR pitches by email only, please.) Other types of secure messaging available upon request.</em></p>
                      </div>
                    
                    
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My Toddler Loves Planes, So I Built Her a Radar (1066 pts)]]></title>
            <link>https://jacobbartlett.substack.com/p/my-toddler-loves-planes-so-i-built</link>
            <guid>38435908</guid>
            <pubDate>Mon, 27 Nov 2023 18:16:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jacobbartlett.substack.com/p/my-toddler-loves-planes-so-i-built">https://jacobbartlett.substack.com/p/my-toddler-loves-planes-so-i-built</a>, See on <a href="https://news.ycombinator.com/item?id=38435908">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae00ead5-bd9f-41c0-a573-04504ae2aea5_700x486.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae00ead5-bd9f-41c0-a573-04504ae2aea5_700x486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae00ead5-bd9f-41c0-a573-04504ae2aea5_700x486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae00ead5-bd9f-41c0-a573-04504ae2aea5_700x486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae00ead5-bd9f-41c0-a573-04504ae2aea5_700x486.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae00ead5-bd9f-41c0-a573-04504ae2aea5_700x486.png" width="700" height="486" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ae00ead5-bd9f-41c0-a573-04504ae2aea5_700x486.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:486,&quot;width&quot;:700,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae00ead5-bd9f-41c0-a573-04504ae2aea5_700x486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae00ead5-bd9f-41c0-a573-04504ae2aea5_700x486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae00ead5-bd9f-41c0-a573-04504ae2aea5_700x486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae00ead5-bd9f-41c0-a573-04504ae2aea5_700x486.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Late-stage user testing with my toddler</figcaption></figure></div><p>This is a tale of inspiration, of a vision taking shape, and of rapid feedback from that most discerning of customers: a 2-year-old girl.</p><p><span>Keen to skip ahead to the final product? Go ahead and download </span><a href="https://apps.apple.com/gb/app/aviator-radar-on-your-phone/id6469189335" rel="">Aviator — Radar on your Phone</a><span> from the App Store now!</span></p><blockquote><p><em><span>For the HackerNews crowd—please </span><a href="https://twitter.com/jacobs_handle" rel="">follow me on X/Twitter</a><span> if you liked this post!</span></em></p></blockquote><p>I took my toddler abroad this summer.</p><p><span>She was so excited. But, in order to make sure she could handle the 3-hour flight, my wife and I made sure to </span><em>hype up</em><span> the airplane journey. So much so, that my toddler was shocked when we had to get into a cab for the airport — she expected to walk straight from our house onto a plane.</span></p><p><span>Once we boarded the flight, things took an incredible turn — it turns out, if the crew spots you with a cute plane-obsessed toddler, they </span><em>invite you in to check out the cockpit.</em></p><p>This kindled my daughter’s obsession with airplanes. She keeps adorably asking me to find planes for her in the sky, and becomes delighted when I spot one for her.</p><p>Last week, we spent an hour in the garden, with her on my shoulders, spotting planes twinkling in the evening sky, one after the other.</p><p>While it’s always great to play with one’s daughter, I knew we could be applying a more efficient approach.</p><p><span>I found </span><a href="https://www.flightradar24.com/" rel="">FlightRadar24</a><span>, which shows the positions of planes overlaid on a map. It worked pretty well, but it was a little annoying to have to orient myself to work out where to look in the sky.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb61737-0621-4218-9b89-fd417dc1bb56_700x559.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb61737-0621-4218-9b89-fd417dc1bb56_700x559.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb61737-0621-4218-9b89-fd417dc1bb56_700x559.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb61737-0621-4218-9b89-fd417dc1bb56_700x559.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb61737-0621-4218-9b89-fd417dc1bb56_700x559.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb61737-0621-4218-9b89-fd417dc1bb56_700x559.png" width="700" height="559" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dcb61737-0621-4218-9b89-fd417dc1bb56_700x559.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:559,&quot;width&quot;:700,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb61737-0621-4218-9b89-fd417dc1bb56_700x559.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb61737-0621-4218-9b89-fd417dc1bb56_700x559.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb61737-0621-4218-9b89-fd417dc1bb56_700x559.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb61737-0621-4218-9b89-fd417dc1bb56_700x559.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Can you spot Heathrow airport?</figcaption></figure></div><p>It’s also pretty tough to spot aircraft on a 2-dimensional plane (pun intended). A Learjet at 40,000 feet shows up the same as an AirBus that just took off at London City Airport, however it’s a lot easier to spot the jumbo jet in the real sky.</p><p><span>Finally, and most importantly, my toddler doesn’t really understand or care what a map is. She </span><em>just</em><span> wanted to look at planes.</span></p><p>So we have our problems.</p><p>Orientation.</p><p>Sizing.</p><p>Usability.</p><p>As an aphyscial mobile tech lead, I wouldn’t know where to start building my kid a rocking horse. But nothing was going to stand in the way of me making her a cool app.</p><p>We had the idea for our app:</p><blockquote><p><em><strong>Show nearby flights on a radar.</strong></em></p></blockquote><p>In keeping with the requirements we’d created via our research:</p><ol><li><p>The app needs to remain oriented correctly, rotating with the device so that it shows airplanes in the correct direction.</p></li><li><p>The app has to show aircraft as bigger or smaller depending on how high they are.</p></li><li><p>The app must be fun, and feel more like a retro kid’s toy than a serious business app.</p></li></ol><p>These requirements led to a few moving parts which form the proof of concept:</p><ol><li><p><strong>Maintaining Orientation </strong><span>is a core differentiating product requirement, since this is missing from the existing solutions. I’m not in the business of detailed flight information — I just want to make a cool radar! The iOS Core Location API has be covered, offering a delegate callback every time the user re-orients their device.</span></p></li><li><p><span>The most important component, of course, is a </span><strong>Flight Data API</strong><span>. </span><a href="https://opensky-network.org/" rel="">OpenSky Network</a><span> has exactly what I need. A simple REST API, free for non-commercial use, with live data of flights in an area. We’d want to ping this endpoint every few seconds for realistic radar sweeps.</span></p></li><li><p><span>To call the API, we need some </span><strong>Location</strong><span> data. Core Location has us covered again — to get a good number of nearby flights, we could query +/- 1 degree of latitude from the user’s location, with a precision of 0.1 degrees (about 10km) to ensure a user’s location is sufficiently obfuscated. We also only need to fetch this data once per session.</span></p></li><li><p><span>Finally, and like most difficult, we need to dust off our </span><strong>Trigonometry</strong><span> skills to compare flight location data with our own, oriented, coordinates. This will allow us to draw nearby aircraft to the screen in the correct place, according to their relative position from us in the sky.</span></p></li></ol><p><span>Since I don’t intend to build a business atop this app — again, the OpenSky Network API is restricted to non-commercial use — I’ll probably use the dead-simple </span><em>MV</em><span> architecture for SwiftUI. I’ll leave a bit of business logic in the views, rely on SwiftUI’s built-in APIs for the heavy lifting, and factor out core services such as API and Location.</span></p><p>Once I prove the concept, I can get to work on the really fun part — turning it into a cool radar and testing it with my toddler!</p><p>First things first.</p><p><span>For the mascot, I’m picturing a cartoon of my daughter in a cute aviator hat. So we have our app name already: </span><strong>Aviator</strong><span>.</span></p><p>Drawing on my infinite willpower, I’m not going to waste time with an app icon until the MVP is complete. But I now have a project name with which to get started.</p><p>The first of my key differentiating product requirements is maintaining orientation — to be useful, the objects on-screen need to correspond to their real-life location. Therefore, When the user rotates, the screen itself rotates and keeps pointing North.</p><p><span>Ignoring the template files for </span><code>AviatorApp</code><span> and </span><code>ContentView</code><span> for now, I whip up a singleton </span><code>LocationManager</code><span> and wire up the </span><code>didUpdateHeading</code><span> method from </span><code>CLLocationManagerDelegate</code><span>.</span></p><blockquote><p><em>In navigation, the Heading is the compass direction in which a vessel — or in this case, an iPhone — is pointed.</em></p></blockquote><p><span>My </span><code>LocationManager</code><span> also handles the initial setup of requesting location permissions, setting the delegate, and telling Core Location to start sending orientation info.</span></p><pre><code>final class LocationManager: CLLocationManager, CLLocationManagerDelegate {
        
    static let shared = LocationManager()
    
    private(set) var rotationAngleSubject = CurrentValueSubject&lt;Double, Never&gt;(0)
    
    override private init() {
        super.init()
        requestWhenInUseAuthorization()
        delegate = self
        startUpdatingHeading()
    }
    
    func locationManager(_ manager: CLLocationManager, didUpdateHeading newHeading: CLHeading) {
        rotationAngleSubject.send(-newHeading.magneticHeading)
    }
}</code></pre><p><span>To make things play nicely with a SwiftUI view, I’m going to send the orientation information via a Combine publisher, </span><code>rotationAngleSubject</code><span>. This means I can reactively handle it in my view with </span><code>.onReceive</code><span>, and set a local </span><code>@State</code><span> property, </span><code>rotationAngle</code><span>.</span></p><p><span>In my view, to get a nice compass effect, I draw a set of rectangles that vary with this </span><code>rotationAngle</code><span>.</span></p><pre><code>@State private var rotationAngle: Angle = .degrees(0)

var body: some View {
    ZStack {
        ForEach(0..&lt;36) {
            let angle = Angle.degrees(Double($0 * 10)) + rotationAngle
            Rectangle()
                .frame(width: $0 == 0 ? 16 : 8, height: $0 == 0 ? 3 : 2)
                .foregroundColor($0 == 0 ? .red : .blue)
                .rotationEffect(angle)
                .offset(x: 120 * cos(CGFloat(angle.radians)), y: 120 * sin(CGFloat(angle.radians)))
                .animation(.bouncy, value: rotationAngle)
        }
    }
    .onReceive(LocationManager.shared.rotationAngleSubject) { angle in
        rotationAngle = Angle.degrees(angle)
    }
}</code></pre><p>Testing on my device, it looks pretty good, and responds perfectly to my real-world location!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc54c926e-5005-4a15-b8d2-42c1bb993396_600x600.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc54c926e-5005-4a15-b8d2-42c1bb993396_600x600.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc54c926e-5005-4a15-b8d2-42c1bb993396_600x600.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc54c926e-5005-4a15-b8d2-42c1bb993396_600x600.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc54c926e-5005-4a15-b8d2-42c1bb993396_600x600.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc54c926e-5005-4a15-b8d2-42c1bb993396_600x600.gif" width="600" height="600" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c54c926e-5005-4a15-b8d2-42c1bb993396_600x600.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:600,&quot;width&quot;:600,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc54c926e-5005-4a15-b8d2-42c1bb993396_600x600.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc54c926e-5005-4a15-b8d2-42c1bb993396_600x600.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc54c926e-5005-4a15-b8d2-42c1bb993396_600x600.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc54c926e-5005-4a15-b8d2-42c1bb993396_600x600.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Which begs the question, why can’t Google Maps ever work out which direction I am facing?</figcaption></figure></div><p>You’ll notice a funny visual glitch due to the animation logic treating 0 degrees and 360 degrees as separate numbers — all the rectangles decide to spin around when I go past true North — but it’s fine for the PoC (as I’m unlikely to actually keep this UI anyway).</p><p>Now my warm-up is over.</p><p>The really important piece is next: Parsing out data from the OpenSky Network API.</p><p><span>It allows you to specify a range of latitude and longitudes, and returns an array of local flights in that range via a simple </span><code>GET</code><span> request — meaning you can simply paste this into your browser to find out what flights I can see overhead:</span></p><pre><code>https://opensky-network.org/api/states/all?lamin=51.0&amp;lamax=52.0&amp;lomin=-0.5&amp;lomax=0.5</code></pre><p><a href="https://openskynetwork.github.io/opensky-api/rest.html" rel="">The REST API is documented well</a><span>, but has an un-keyed structure, meaning the data is presented as a list properties in order.</span></p><p><span>We need to use an </span><code>UnkeyedContainer</code><span> to decode it, which is designed to parse out fields from the JSON response in order.</span></p><pre><code>struct Flight: Decodable {

    let icao24: String 
    let callsign: String?
    let origin_country: String? 
    let time_position: Int?
    let last_contact: Int
    let longitude: Double
    let latitude: Double

    // ... 

    init(from decoder: Decoder) throws {
        var container = try decoder.unkeyedContainer()
        icao24 = try container.decode(String.self)
        callsign = try? container.decode(String?.self)
        origin_country = try container.decode(String.self)
        time_position = try? container.decode(Int?.self)
        last_contact = try container.decode(Int.self)
        longitude = try container.decode(Double.self)
        latitude = try container.decode(Double.self)

        // ...
    }
}</code></pre><p><span>We can write a simple API that performs a </span><code>GET</code><span> request based on the user’s location coordinates.</span></p><pre><code>final class FlightAPI {
    
    func fetchLocalFlightData(coordinate: CLLocationCoordinate2D) async throws -&gt; [Flight] {
        
        let lamin = String(format: "%.1f", coordinate.latitude - 0.25)
        let lamax = String(format: "%.1f", coordinate.latitude + 0.25)
        let lomin = String(format: "%.1f", coordinate.longitude - 0.5)
        let lomax = String(format: "%.1f", coordinate.longitude + 0.5)

        let url = URL(string: "https://opensky-network.org/api/states/all?lamin=\(lamin)&amp;lamax=\(lamax)&amp;lomin=\(lomin)&amp;lomax=\(lomax)")!
        let data = try await URLSession.shared.data(from: url).0
        return try JSONDecoder().decode([Flight].self, from: data)
    }
}</code></pre><blockquote><p><em>You might notice that I used a range of 1 degree of longitude, but only 0.5 degrees of latitude in this API call. That’s because at my latitude, the UK, a 0.5 latitude by 1 longitude rectangle shows up, approximately, as a square.</em></p></blockquote><p>Now we’re getting somewhere!</p><p><span>The flight data is nicely parsed into an array of in-memory </span><code>Flight</code><span> objects which are now nice and easy to deal with.</span></p><p><span>It’s pretty trivial to amend my </span><code>LocationManager</code><span> to listen for significant location changes and send these coordinates via a publisher.</span></p><p><span>Again, in pure MV architectural style, my view listens to coordinates via </span><code>.onReceive</code><span> and calls my new </span><code>FlightAPI</code><span> with these coordinates. The result? Data about the overhead airplanes in your local slice of sky.</span></p><p>Now, we arrive at the hardest part of my initial proof of concept: actually displaying the airplane icons in their correct locations, relative to my own location.</p><p>My first iteration was a blunt instrument: I multiplied the relative lat and long by a hardcoded on-screen points value.</p><pre><code>@State private var coordinates: CLLocationCoordinate2D?
@State private var flights: [Flight] = []

private var airplanes: some View {
    ForEach(flights, id: \.icao24) { flight in
        let latDiff = coordinate.latitude - (flight.latitude ?? 0)
        let lngDiff = coordinate.longitude - (flight.longitude ?? 0)
        Image(systemName: "airplane")
            .resizable()
            .frame(width: 20, height: 20)
            .rotationEffect(.degrees(flight.true_track ?? 0))
            .foregroundColor(.red)
            .offset(x: 250 * latDiff, y: 250 * lngDiff)
    }
}</code></pre><p>There was, of course, no way this would be accurate, since the absolute distance of a degree of latitude or longitude varies with your geolocation. But again, it’s a good place to begin.</p><p>How do I actually test the accuracy of my airplane drawings?</p><p>I could draw a map underneath everything!</p><p><span>Now my </span><code>AviatorView</code><span> has 3 layers: The compass on top, airplanes drawn to the screen, and an unadorned SwiftUI </span><code>Map</code><span> view under it all.</span></p><pre><code>@State private var cameraPosition: MapCameraPosition = .camera(MapCamera(
        centerCoordinate: CLLocationCoordinate2D(latitude: 51.0, longitude: 0.0),
        distance: 100_000,
        heading: 0))

var body: some View {
    ZStack {
        Map(position: $cameraPosition) { } 
        airplanes
        compass
    }
}</code></pre><p>Here’s the result of my first late-night hackathon, compared against the FlightRadar projection as a source of truth.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a0f987-1471-4517-a668-7aec954390da_1190x788.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a0f987-1471-4517-a668-7aec954390da_1190x788.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a0f987-1471-4517-a668-7aec954390da_1190x788.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a0f987-1471-4517-a668-7aec954390da_1190x788.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a0f987-1471-4517-a668-7aec954390da_1190x788.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a0f987-1471-4517-a668-7aec954390da_1190x788.png" width="1190" height="788" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/84a0f987-1471-4517-a668-7aec954390da_1190x788.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:788,&quot;width&quot;:1190,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1589429,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a0f987-1471-4517-a668-7aec954390da_1190x788.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a0f987-1471-4517-a668-7aec954390da_1190x788.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a0f987-1471-4517-a668-7aec954390da_1190x788.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a0f987-1471-4517-a668-7aec954390da_1190x788.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Results of Day #1, my app on the left, compared against FlightRadar on the right</figcaption></figure></div><p>I’m certainly on to something, since the number and clustering of airplanes in the sky looks about right — however the positioning is pretty far off.</p><p>Suddenly, another flash of inspiration. It’s so simple. I can’t believe I didn’t think of it before.</p><p><span>I need to </span><em>draw the airplanes on the map</em><span> using annotations!</span></p><p><span>The idea has been brewing all day: we’ll use a </span><code>Map</code><span>, and draw aircraft-shaped annotations on top at their precice geolocations.</span></p><p>Eventually, I want to find a way to hide the actual map, and only display the airplanes as markers on the radar position. This should get us the cool, fully-oriented radar effect we’re aiming at.</p><p><span>In iOS 17, which I intend to target, drawing annotations on a map is a breeze. Let’s refactor out a </span><code>FlightMapView</code><span>.</span></p><pre><code>import MapKit
import SwiftUI

struct FlightMapView: View {
    
    @Binding var cameraPosition: MapCameraPosition
    
    let flights: [Flight]

    var body: some View {
        Map(position: $cameraPosition) {
            planeMapAnnotations
        }
        .mapStyle(.imagery)
        .allowsHitTesting(false)
    }
}</code></pre><p>Here, for the purposes of a radar, we want to prevent hit-testing — i.e., we don’t want the map to be interactive. In our ideal world, the map is invisible, and the user just sees flights and their positions.</p><p>After orientation, sizing was the next core issue which the existing solutions simply didn’t handle that well.</p><p>I added some simple log scaling to the map annotations using the altitude of the flight so higher up aircraft appear larger on-screen.</p><p><span>Additionally, I used the aircraft’s </span><code>true_track</code><span> property, combined with the user’s orientation from Core Location, to show the plane facing the correct direction.</span></p><pre><code>@State private var rotationAngle: Angle = .degrees(0)

private var planeMapAnnotations: some MapContent {
    ForEach(flights, id: \.icao24) { flight in
        Annotation(flight.icao24, coordinate: flight.coordinate) {
            let rotation = rotationAngle.degrees + flight.true_track
            let scale = min(2, max(log10(height + 1), 0.5))
            Image(systemName: "airplane")
                .rotationEffect(.degrees(rotation))
                .scaleEffect(scale)
            }
        }
        .tint(.white)
    }
}</code></pre><p>Now’s the time for the ultimate test to find out if my MVP actually works.</p><p>I’m going to go plane-spotting with my daughter.</p><p>We’ve got real map annotations, and show the user’s location and direction on the map.</p><p>Most importantly, it accurately finds the airplanes!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18021ff7-f048-4b55-8bf8-5cb8b4c67ec4_1188x972.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18021ff7-f048-4b55-8bf8-5cb8b4c67ec4_1188x972.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18021ff7-f048-4b55-8bf8-5cb8b4c67ec4_1188x972.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18021ff7-f048-4b55-8bf8-5cb8b4c67ec4_1188x972.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18021ff7-f048-4b55-8bf8-5cb8b4c67ec4_1188x972.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18021ff7-f048-4b55-8bf8-5cb8b4c67ec4_1188x972.png" width="1188" height="972" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/18021ff7-f048-4b55-8bf8-5cb8b4c67ec4_1188x972.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:972,&quot;width&quot;:1188,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1850682,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18021ff7-f048-4b55-8bf8-5cb8b4c67ec4_1188x972.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18021ff7-f048-4b55-8bf8-5cb8b4c67ec4_1188x972.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18021ff7-f048-4b55-8bf8-5cb8b4c67ec4_1188x972.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18021ff7-f048-4b55-8bf8-5cb8b4c67ec4_1188x972.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The first plane we spotted via Aviator, the aptly-named 3c65d4</figcaption></figure></div><p>The MVP was a smashing success, since my daughter and I spotted a plane which was visible on the app!</p><p>This initial test also yielded 2 pieces of important information.</p><p><span>Firstly, my scaling logic is backwards — see the tiny plane on the ground at London City Airport. Since the point of the app is locating aircraft in the sky, we need to </span><strong>reverse the scaling</strong><span>. Lower-down planes must show up as larger, since we’re using our eyes to spot them.</span></p><p><span>Secondly, my toddler does </span><em>not</em><span> care about maps, just airplanes. I needed to </span><strong>remove the map </strong><span>if I wanted to clear out the noise and focus on spotting aircraft. And start to build my radar!</span></p><p>I handily fixed the scaling logic for the aircraft.</p><p>After some trial and error — to see both what looks good on-screen, and gives a reasonable spread of sizes, I landed on this for the scaling:</p><pre><code>min(2, max(4.7 - log10(flight.geo_altitude + 1), 0.7))</code></pre><p>These scalings came out of my local overhead scan:</p><pre><code>Scale:  1.0835408863965839
Scale:  0.8330645861650874
Scale:  1.095791123396205
Scale:  1.1077242935783653
Scale:  2.0
Scale:  1.4864702267977097
Scale:  0.7</code></pre><p>This distribution works pretty well — aside from the NOx, it’s turning out quite useful living in an air travel hub.</p><p>I was nearly ready to build the radar I was envisioning. But there was a problem.</p><p>The open-source OpenSky API kept timing out, returning 502 bad gateway errors, and sometimes simply yielding a 200 response with with null data.</p><p>Franky, that’s fine by me — this isn’t a corporate business app and this great API costs me nothing. They have no SLA and I don’t feel entitled to one.</p><p>To help improve robustness on the client-side, I implemented some basic retry logic in the API call.</p><pre><code>private func fetchFlights(at coordinate: CLLocationCoordinate2D, retries: Int = 3) async {
    do {
        try await api.fetchLocalFlightData(coordinate: coordinate)

    } catch {
        if retries &gt; 0 {
            try await fetchFlights(at: coordinate, retries: retries - 1)
        }
    }
}</code></pre><p>The next day, the API was working fine all day—it seems like it’s mostly good apart from certain high-traffic periods.</p><p>The most important noise-reducing task is to make the actual map invisible. The radar won’t work without this.</p><p><span>I was able to do this using a flat-coloured </span><code>MapPolygon</code><span> — ostensibly designed so you can place overlays to highlight sections of a map. But I wanted to use it to hide </span><em>everything</em><span> except our annotations.</span></p><pre><code>struct FlightMapView: View {

    var body: some View {
        Map(position: $cameraPosition) {
            planeMapAnnotations
            MapPolygon(overlay(coordinate: coordinate))
        }
        .mapStyle(.imagery)
        .allowsHitTesting(false)
    }

    // ...
    
    private func rectangle(around coordinate: CLLocationCoordinate2D) -&gt; [CLLocationCoordinate2D] {
        [
            CLLocationCoordinate2D(latitude: coordinate.latitude - 1, longitude: coordinate.longitude - 1),
            CLLocationCoordinate2D(latitude: coordinate.latitude - 1, longitude: coordinate.longitude + 1),
            CLLocationCoordinate2D(latitude: coordinate.latitude + 1, longitude: coordinate.longitude + 1),
            CLLocationCoordinate2D(latitude: coordinate.latitude + 1, longitude: coordinate.longitude - 1)
        ]
    }
    
    private func overlay(coordinate: CLLocationCoordinate2D) -&gt; MKPolygon {
        let rectangle = rectangle(around: coordinate)
        return MKPolygon(coordinates: rectangle, count: rectangle.count)
    }
}</code></pre><p>Tapping my dwindling reserves of my good luck, this approach worked a treat! We could now see the airplanes, but no map, just like we wanted!</p><p><span>Critically, Apple has designed the overlay to place </span><em>on top</em><span> of the map but </span><em>underneath</em><span> the annotations. Had they done it any other way, my daughter’s new toy would be hobbled.</span></p><p>The final piece of my core requirements was a radar view.</p><p>This was essentially a set of lines, concentric circles, and 20 degrees of rotating angular gradient. For a SwiftUI aficionado like myself, this was simples.</p><p>Look how far we’ve come.</p><p>With today’s core visual changes — hiding the map via an overlay, and a few lines of SwiftUI views for the radar — we are now rapidly closing in on our original vision.</p><p>Comparing the resulting Radar UI with the aircraft in the sky overhead, we’re pretty nicely matched.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bde87e-1535-45a4-89e5-cb3aed63b1b9_967x575.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bde87e-1535-45a4-89e5-cb3aed63b1b9_967x575.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bde87e-1535-45a4-89e5-cb3aed63b1b9_967x575.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bde87e-1535-45a4-89e5-cb3aed63b1b9_967x575.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bde87e-1535-45a4-89e5-cb3aed63b1b9_967x575.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bde87e-1535-45a4-89e5-cb3aed63b1b9_967x575.png" width="967" height="575" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/34bde87e-1535-45a4-89e5-cb3aed63b1b9_967x575.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:575,&quot;width&quot;:967,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:741234,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bde87e-1535-45a4-89e5-cb3aed63b1b9_967x575.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bde87e-1535-45a4-89e5-cb3aed63b1b9_967x575.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bde87e-1535-45a4-89e5-cb3aed63b1b9_967x575.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bde87e-1535-45a4-89e5-cb3aed63b1b9_967x575.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Day 3 results — displaying the flights over Sidcup</figcaption></figure></div><p>After 3 solid evenings of work, my toddler was finally starting to show some interest in the toy I’d created.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52fbd543-157a-49dd-95dc-5b59695a9140_1188x971.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52fbd543-157a-49dd-95dc-5b59695a9140_1188x971.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52fbd543-157a-49dd-95dc-5b59695a9140_1188x971.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52fbd543-157a-49dd-95dc-5b59695a9140_1188x971.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52fbd543-157a-49dd-95dc-5b59695a9140_1188x971.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52fbd543-157a-49dd-95dc-5b59695a9140_1188x971.png" width="1188" height="971" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/52fbd543-157a-49dd-95dc-5b59695a9140_1188x971.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1188,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:937846,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52fbd543-157a-49dd-95dc-5b59695a9140_1188x971.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52fbd543-157a-49dd-95dc-5b59695a9140_1188x971.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52fbd543-157a-49dd-95dc-5b59695a9140_1188x971.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52fbd543-157a-49dd-95dc-5b59695a9140_1188x971.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>We saw the planes it detected! However you’ll have to take my word for it, due  my antiquated iPhone camera</figcaption></figure></div><p>We’ve proven the concept, and put together an MVP that achieves the core initial goals we set out to do.</p><p>Now, we can start to think about putting it on the App Store.</p><p>I’m a big fan of Skeuomorphism. As such, I wanted to flex all my animation muscles to give this app the retro, toy-like quality I envisoned.</p><p>I was proud of the effect I produced on the radar.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66f9dee5-66fb-4c9f-98b3-80a3b0bcfb19_576x576.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66f9dee5-66fb-4c9f-98b3-80a3b0bcfb19_576x576.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66f9dee5-66fb-4c9f-98b3-80a3b0bcfb19_576x576.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66f9dee5-66fb-4c9f-98b3-80a3b0bcfb19_576x576.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66f9dee5-66fb-4c9f-98b3-80a3b0bcfb19_576x576.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66f9dee5-66fb-4c9f-98b3-80a3b0bcfb19_576x576.gif" width="576" height="576" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/66f9dee5-66fb-4c9f-98b3-80a3b0bcfb19_576x576.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:576,&quot;width&quot;:576,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66f9dee5-66fb-4c9f-98b3-80a3b0bcfb19_576x576.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66f9dee5-66fb-4c9f-98b3-80a3b0bcfb19_576x576.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66f9dee5-66fb-4c9f-98b3-80a3b0bcfb19_576x576.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66f9dee5-66fb-4c9f-98b3-80a3b0bcfb19_576x576.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Realistic fade-out effect on the radar to improve</figcaption></figure></div><p><span>The implementation is what I’d call </span><em>“dumb genius”.</em></p><p>Originally, I thought about using trigonometry and timers — recolouring and fading-out individual map annotations whenever the line hit them.</p><p>But then I realised that my line was simply a 20-degree-wide angular gradient that went from green to clear.</p><p>What if it was a 360-degree wide angular gradient?</p><p>And what if this gradient went from green, to clear, to clear, to clear, to black?</p><pre><code>private var radarLine: some View {
    Circle()
        .fill(
            AngularGradient(
                gradient: Gradient(colors: [
                    Color.black, Color.black, Color.black, Color.black,
                    Color.black.opacity(0.8), Color.black.opacity(0.6),
                    Color.black.opacity(0.4), Color.black.opacity(0.2),
                    Color.clear, Color.clear, Color.clear, Color.clear,
                    Color.clear, Color.clear, Color.clear, Color.clear,
                    Color.clear, Color.clear, Color.clear, Color.green]),
                center: .center,
                startAngle: .degrees(rotationDegree),
                endAngle: .degrees(rotationDegree + 360)
            )
        )
        .rotationEffect(Angle(degrees: rotationDegree))
        .animation(.linear(duration: 6).repeatForever(autoreverses: false), value: rotationDegree)
}</code></pre><p><span>More often than not, the </span><a href="https://grugbrain.dev/" rel="">grug-brained</a><span> solution works best.</span></p><p>I also found some weird visual artefacts from the map appearing in the corners of the screen when rotating the device too quickly — the overlay seems to be lazily rendering outside the map’s camera position.</p><p>Creating a black outline of the radar view with a reverse mask solved the issue (i.e. a black rectangle with a circular hole for the radar).</p><p>Our UI is looking pretty tidy now. But I wouldn’t yet call it retro.</p><p>I wanted to add a CRT-screen effect, with television scanlines, to make the app look like it was really drawn on an old radar scanner.</p><p><span>iOS 17 comes with support for Metal shaders built into </span><code>colorEffect</code><span>, so it implementing this effect is easier than ever before.</span></p><pre><code>#include &lt;metal_stdlib&gt;
using namespace metal;

[[ stitchable ]] half4 crtScreen(
    float2 position,
    half4 color,
    float time
) {
    
    if (all(abs(color.rgb - half3(0.0, 0.0, 0.0)) &lt; half3(0.01, 0.01, 0.01))) {
        return color;
    }
    
    const half scanlineIntensity = 0.2;
    const half scanlineFrequency = 400.0;
    half scanlineValue = sin((position.y + time * 10.0) * scanlineFrequency * 3.14159h) * scanlineIntensity;
    return half4(color.rgb - scanlineValue, color.a);
}</code></pre><p>I might save digging into the C++ for another article. Feel free to steal it — most importantly, I created a view modifier that can apply the CRT effect to any view we like!</p><pre><code>extension View {
    
    func crtScreenEffect(startTime: Date) -&gt; some View {
        modifier(CRTScreen(startTime: startTime))
    }
}

struct CRTScreen: ViewModifier {
    
    let startTime: Date
    
    func body(content: Content) -&gt; some View {
        content
            .colorEffect(
                ShaderLibrary.crtScreen(
                    .float(startTime.timeIntervalSinceNow)
                )
            )
    }
}</code></pre><p><span>Note that this modifier, and the shader itself, take in a </span><code>time</code><span> parameter to make the scanlines move up rapidly and make the effect far more dynamic.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bba1ea3-a8e3-48c4-af81-fd00d29e76ce_600x600.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bba1ea3-a8e3-48c4-af81-fd00d29e76ce_600x600.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bba1ea3-a8e3-48c4-af81-fd00d29e76ce_600x600.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bba1ea3-a8e3-48c4-af81-fd00d29e76ce_600x600.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bba1ea3-a8e3-48c4-af81-fd00d29e76ce_600x600.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bba1ea3-a8e3-48c4-af81-fd00d29e76ce_600x600.gif" width="600" height="600" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1bba1ea3-a8e3-48c4-af81-fd00d29e76ce_600x600.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:600,&quot;width&quot;:600,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bba1ea3-a8e3-48c4-af81-fd00d29e76ce_600x600.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bba1ea3-a8e3-48c4-af81-fd00d29e76ce_600x600.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bba1ea3-a8e3-48c4-af81-fd00d29e76ce_600x600.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bba1ea3-a8e3-48c4-af81-fd00d29e76ce_600x600.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>I actually recorded and gif-ified this before implementing the time-modulation — see those below!</figcaption></figure></div><p>While the OpenSky Network website is pretty clear, I wanted to be polite and sent a note to ensure my App Store listing would be fine under their policy.</p><p>They very kindly replied within 20 minutes!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbbc39cf-98a6-4698-be3f-412269e7fd58_507x151.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbbc39cf-98a6-4698-be3f-412269e7fd58_507x151.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbbc39cf-98a6-4698-be3f-412269e7fd58_507x151.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbbc39cf-98a6-4698-be3f-412269e7fd58_507x151.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbbc39cf-98a6-4698-be3f-412269e7fd58_507x151.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbbc39cf-98a6-4698-be3f-412269e7fd58_507x151.png" width="507" height="151" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bbbc39cf-98a6-4698-be3f-412269e7fd58_507x151.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:151,&quot;width&quot;:507,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbbc39cf-98a6-4698-be3f-412269e7fd58_507x151.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbbc39cf-98a6-4698-be3f-412269e7fd58_507x151.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbbc39cf-98a6-4698-be3f-412269e7fd58_507x151.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbbc39cf-98a6-4698-be3f-412269e7fd58_507x151.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Gotta love the open-source community.</figcaption></figure></div><p>To help sell the experience of a radar, and also help a touch with accessibility, I added a little beep-boop system sound effect whenever the flights update .</p><pre><code>private func fetchFlights(coordinate: Coordinate, retries: Int = 2) async {
    do {
        let flights = try await api.fetchLocalFlightData(coordinate: coordinate)
        await MainActor.run {
            self.flights = flights
            AudioServicesPlaySystemSound(1052)
            hapticTrigger.toggle()
        }

    // ...

}</code></pre><p><span>Alongside the new </span><code>sensoryFeedback</code><span> modifier on the main view for some haptics:</span></p><pre><code>.sensoryFeedback(.levelChange, trigger: hapticTrigger)</code></pre><p>What I realised now, however, is that this beep might get annoying to some people. So I should add a few customisation options.</p><p>Firstly, a silent mode is in order.</p><p><span>But also, perhaps, a few simple other customisations with </span><code>@AppStorage</code><span>.</span></p><pre><code>@AppStorage("silent") var silentMode: Bool = false
@AppStorage("showMap") var showMap: Bool = false
@AppStorage("userColor") var userColor: Color = .green</code></pre><p>Now, uses can turn off the sound, and even turn off the radar overlay to see the map underneath.</p><p>Most importantly, however, since I’m building this for my kid, picking a custom color for the radar via the SwiftUI color picker is absolutely mandatory.</p><p>Finally, what’s life without an animated SFSymbol or two?</p><pre><code>private func toggleableIcon(state: Bool, iconTrue: String, iconFalse: String) -&gt; some View {
        Image(systemName: state ? iconTrue : iconFalse)
            .contentTransition(.symbolEffect(.replace))
    // ...
}</code></pre><p>I think our app is ready for prime-time now.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05a94691-f398-4c9a-aef5-55777cc2abfc_600x600.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05a94691-f398-4c9a-aef5-55777cc2abfc_600x600.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05a94691-f398-4c9a-aef5-55777cc2abfc_600x600.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05a94691-f398-4c9a-aef5-55777cc2abfc_600x600.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05a94691-f398-4c9a-aef5-55777cc2abfc_600x600.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05a94691-f398-4c9a-aef5-55777cc2abfc_600x600.gif" width="600" height="600" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/05a94691-f398-4c9a-aef5-55777cc2abfc_600x600.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:600,&quot;width&quot;:600,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05a94691-f398-4c9a-aef5-55777cc2abfc_600x600.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05a94691-f398-4c9a-aef5-55777cc2abfc_600x600.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05a94691-f398-4c9a-aef5-55777cc2abfc_600x600.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05a94691-f398-4c9a-aef5-55777cc2abfc_600x600.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I need to do a little bit of refactoring to move views into their own files.</p><p><span>Now the top-level </span><code>AviatorView</code><span> looks a bit like this:</span></p><pre><code>// @State properties ...

var body: some View {
    ZStack {
        if let coordinate = locationManager.coordinateSubject.value {
            FlightMapView(
                cameraPosition: $cameraPosition,
                flights: flights,
                rotationAngle: rotationAngle,
                coordinate: coordinate
            )
        }
    
        TimelineView(.animation) { context in
            RadarView()
                .crtScreenEffect()
                .negativeHighlight()
        }
    
        ControlsView(errorMessage: errorMessage)
    }

    // onRecieve modifiers ...
}</code></pre><p><span>Annoyingly, I stopped paying Midjourney last month, so I wrangled a free-for-non-commercial-use generator at </span><a href="https://gencraft.com/" rel="">Gencraft</a><span>.</span></p><p>Fortunately, I managed to approximate my daughter in an aviator hat, which is precisely the look I was going for!</p><p>This also led to my most successful Tweet ever.</p><p>I haven’t personally paid for the Apple Developer Program for years.</p><p>Look at this graveyard of discarded side projects.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa59db988-401e-4c3a-bc8b-00af5750fa31_700x368.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa59db988-401e-4c3a-bc8b-00af5750fa31_700x368.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa59db988-401e-4c3a-bc8b-00af5750fa31_700x368.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa59db988-401e-4c3a-bc8b-00af5750fa31_700x368.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa59db988-401e-4c3a-bc8b-00af5750fa31_700x368.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa59db988-401e-4c3a-bc8b-00af5750fa31_700x368.png" width="700" height="368" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a59db988-401e-4c3a-bc8b-00af5750fa31_700x368.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:368,&quot;width&quot;:700,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa59db988-401e-4c3a-bc8b-00af5750fa31_700x368.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa59db988-401e-4c3a-bc8b-00af5750fa31_700x368.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa59db988-401e-4c3a-bc8b-00af5750fa31_700x368.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa59db988-401e-4c3a-bc8b-00af5750fa31_700x368.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>RevisionApp will always be the one that got away…</figcaption></figure></div><p>Welp. I’m £79 down and ready to hit publish.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea5f87bd-13a5-498c-8587-04789a650371_700x290.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea5f87bd-13a5-498c-8587-04789a650371_700x290.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea5f87bd-13a5-498c-8587-04789a650371_700x290.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea5f87bd-13a5-498c-8587-04789a650371_700x290.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea5f87bd-13a5-498c-8587-04789a650371_700x290.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea5f87bd-13a5-498c-8587-04789a650371_700x290.png" width="700" height="290" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea5f87bd-13a5-498c-8587-04789a650371_700x290.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:290,&quot;width&quot;:700,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea5f87bd-13a5-498c-8587-04789a650371_700x290.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea5f87bd-13a5-498c-8587-04789a650371_700x290.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea5f87bd-13a5-498c-8587-04789a650371_700x290.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea5f87bd-13a5-498c-8587-04789a650371_700x290.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><blockquote><p><em><strong>Fun fact:</strong><span> I’m targeting iOS 17 only. But I still need to supply screenshots for 6.5" and 5.5" iPhones. The latest 5.5" iPhone? The 8 Plus. Which has a maximum version of iOS 16. Yup. Fortunately, the good people at </span><a href="https://appscreens.com/" rel="">AppScreens</a><span> allowed me to export for both sizes. But don’t get me started on re-scaling videos.</span></em></p></blockquote><p><span>While we wait for Apple app review to work its magic, let’s run a few more rounds of weekend user-testing with my toddler, who’s absolutely </span><em>delighted</em><span> that she can now pick her own colour for the UI.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83197c81-b287-4016-be20-7ee1eaaea23a_1186x970.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83197c81-b287-4016-be20-7ee1eaaea23a_1186x970.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83197c81-b287-4016-be20-7ee1eaaea23a_1186x970.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83197c81-b287-4016-be20-7ee1eaaea23a_1186x970.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83197c81-b287-4016-be20-7ee1eaaea23a_1186x970.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83197c81-b287-4016-be20-7ee1eaaea23a_1186x970.png" width="1186" height="970" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83197c81-b287-4016-be20-7ee1eaaea23a_1186x970.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:970,&quot;width&quot;:1186,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1202923,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83197c81-b287-4016-be20-7ee1eaaea23a_1186x970.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83197c81-b287-4016-be20-7ee1eaaea23a_1186x970.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83197c81-b287-4016-be20-7ee1eaaea23a_1186x970.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83197c81-b287-4016-be20-7ee1eaaea23a_1186x970.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd573eebf-3e73-461f-9501-779dad4bf7a4_1187x969.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd573eebf-3e73-461f-9501-779dad4bf7a4_1187x969.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd573eebf-3e73-461f-9501-779dad4bf7a4_1187x969.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd573eebf-3e73-461f-9501-779dad4bf7a4_1187x969.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd573eebf-3e73-461f-9501-779dad4bf7a4_1187x969.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd573eebf-3e73-461f-9501-779dad4bf7a4_1187x969.png" width="1187" height="969" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d573eebf-3e73-461f-9501-779dad4bf7a4_1187x969.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:969,&quot;width&quot;:1187,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:551861,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd573eebf-3e73-461f-9501-779dad4bf7a4_1187x969.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd573eebf-3e73-461f-9501-779dad4bf7a4_1187x969.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd573eebf-3e73-461f-9501-779dad4bf7a4_1187x969.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd573eebf-3e73-461f-9501-779dad4bf7a4_1187x969.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaff5f2f-abb4-4588-97c7-a61248f1678a_1188x973.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaff5f2f-abb4-4588-97c7-a61248f1678a_1188x973.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaff5f2f-abb4-4588-97c7-a61248f1678a_1188x973.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaff5f2f-abb4-4588-97c7-a61248f1678a_1188x973.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaff5f2f-abb4-4588-97c7-a61248f1678a_1188x973.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaff5f2f-abb4-4588-97c7-a61248f1678a_1188x973.png" width="1188" height="973" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aaff5f2f-abb4-4588-97c7-a61248f1678a_1188x973.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:973,&quot;width&quot;:1188,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:573406,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaff5f2f-abb4-4588-97c7-a61248f1678a_1188x973.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaff5f2f-abb4-4588-97c7-a61248f1678a_1188x973.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaff5f2f-abb4-4588-97c7-a61248f1678a_1188x973.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaff5f2f-abb4-4588-97c7-a61248f1678a_1188x973.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05c719a5-72ab-41bf-94c4-f500be48ff5b_700x519.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05c719a5-72ab-41bf-94c4-f500be48ff5b_700x519.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05c719a5-72ab-41bf-94c4-f500be48ff5b_700x519.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05c719a5-72ab-41bf-94c4-f500be48ff5b_700x519.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05c719a5-72ab-41bf-94c4-f500be48ff5b_700x519.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05c719a5-72ab-41bf-94c4-f500be48ff5b_700x519.png" width="700" height="519" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/05c719a5-72ab-41bf-94c4-f500be48ff5b_700x519.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:519,&quot;width&quot;:700,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05c719a5-72ab-41bf-94c4-f500be48ff5b_700x519.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05c719a5-72ab-41bf-94c4-f500be48ff5b_700x519.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05c719a5-72ab-41bf-94c4-f500be48ff5b_700x519.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05c719a5-72ab-41bf-94c4-f500be48ff5b_700x519.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>App Store Listing for Aviator v1.0.0</figcaption></figure></div><p>Want to download the app yourself?</p><p><span>Go to </span><a href="https://apps.apple.com/gb/app/aviator-radar-on-your-phone/id6469189335" rel="">Aviator — Radar on your Phone</a><span> now (and don’t forget to rate)!</span></p><p>I’m pretty happy with what I put together in a few evenings over a week. It’s been ages since I picked up a side project, and making a fun toy for my daughter is the most fun I’ve had coding in years.</p><p>After this write-up, I’ve got a few features in mind in my mini-roadmap for the next release:</p><ul><li><p>Add zoom levels to the map to restrict the radar to closer aircraft only.</p></li><li><p>Use the advanced version of the OpenSky Network API to show helicopters, satellites, and airplane size classes.</p></li><li><p>Toggle origin &amp; destination country display on airplanes.</p></li><li><p>Improve the CRT screen effect with more advanced Metal shaders.</p></li><li><p>Refactor all the controls into a resizeable progressive-disclosure pull-out modal with detents.</p></li><li><p>Implement slider controls to filter out certain distances &amp; heights — e.g. to hide all low-down, far-off aircraft.</p></li><li><p>Implement “zany mode” which renders UFOs, giant bugs, and aliens on the radar.</p></li></ul><p>If you have any ideas of your own, or simply some feedback, please let me know in the comments!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ed.bf: text editor in Brainfuck (132 pts)]]></title>
            <link>https://github.com/bf-enterprise-solutions/ed.bf</link>
            <guid>38435017</guid>
            <pubDate>Mon, 27 Nov 2023 17:15:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bf-enterprise-solutions/ed.bf">https://github.com/bf-enterprise-solutions/ed.bf</a>, See on <a href="https://news.ycombinator.com/item?id=38435017">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content"><pre>[ED.BF---AN OPINIONATED ED RE-IMPLEMENTATION IN BRAINFUCK

 The original grep [as part of ed text editor] was written [overnight]
 in PDP-11 assembly language [by Ken Thompson].
 -- Brian Kernigan, in the YouTube Computerphile video about grep
 (<a href="https://www.youtube.com/watch?v=NTfOnGZUZDk" rel="nofollow">https://www.youtube.com/watch?v=NTfOnGZUZDk</a>).

 Not being Ken Thompson is a struggle every working software engineer
 has to contend with.
 -- David Gillies, a YouTube comment under the same video.

 UNIX ed is notorious for being the most user-hostile text editor. It
 was initially implemented in assembly. Brainfuck is notorious for
 being one of the most programmer-hostile programming languages. It is
 extremely close to assembly. Feels like a perfect match for one get
 the feel of the legendary programmer existence, doesn't it?

 ed.bf is an incomplete, buggy and (potentially) Turing-complete
 re-implementation of UNIX ed in Brainfuck. It is ALREADY non-standard
 in where we feel it necessary.

 The memory layout (see below) it has allows for terrible hacks and
 more efficient text editing if you understand it. It will NEVER be
 fixed and will always stay overrestrictive/hackable. Enjoy.

 [1 GETTING STARTED

  Clone the code:
  [shell:
   git clone <a href="https://github.com/bf-enterprise-solutions/ed.bf.git">https://github.com/bf-enterprise-solutions/ed.bf.git</a>]

  Find the proper implementation. ed.bf work only on implementations where:
  - All cells are initially zeroed.
  - Tape size is at least 122 * cell size (see layout for why).
  - Cell wraparound works in both directions.

  Run ed.bf and enjoy the most user-and-programmer-hostile text editor!]

 [2 ED.BF COMMANDS

  - 'c' to change the current line.
  - 'd' to remove the current line (flush it clean, basically). Moves
  all the filled lines after the deleted one to cover it.
  - 'p' to print the current line.
  - '=' to print the current line number. Even if the line is empty.
  - newline to go to next line.
  - '-' to go to previous line.
  - Any number to go to the respective numbered line.
  - '!' to evaluate Brainfuck code on the current line contents. Given
    that Brainfuck implies no underlying OS, there's no shell. Thus,
    the meta-evaluation of Brainfuck code is as close to ed's '!'
    meaning as we can get. Uses meta.bf
    (<a href="https://github.com/bf-enterprise-solutions/meta.bf">https://github.com/bf-enterprise-solutions/meta.bf</a>) internally.
  - 'q' to exit ed.bf.]

 [3 EXAMPLE SESSION

  This is simply a free-style copy of the example ed session from
  Wikipedia (<a href="https://en.wikipedia.org/wiki/Ed_(text_editor)#Example" rel="nofollow">https://en.wikipedia.org/wiki/Ed_(text_editor)#Example</a>).

  Lines starting with '&gt;' are user input. Note that ed.bf doen't do
  any indication of when to input commands, much like original
  ed. Thus, these '&gt;' were added for clarity and shouldn't be there in
  the actual session.

  Also note that ed.bf doesn't yet support comments, neither on the
  line by themselves nor inline. Those were added for clarity
  too. Using those can break ed.bf. Or it won't do anything, who
  knows...
  =========================================================================
  # bff is used as bf interpreter there, but any other
  # interpreter/compiler should be fine too
  $ bff ed.bf # run ed.bf
  &gt; c # input one line
  &gt; ed.bf is an opinionated UNIX ed re-implementation in Brainfuck.
  &gt;
  ed.bf is an opinionated UNIX ed re-implementation in Brainfuck.
  &gt; c # input another line
  &gt; This is line number two.
  &gt;
  This is line number two.
  &gt; c # input empty line
  &gt;
  &gt; = # print a line number
  3
  &gt; - # line back
  This is line number two.
  &gt; - # line back
  ed.bf is an opinionated UNIX ed re-implementation in Brainfuck.
  &gt; - # line back, but there's nowhere to go
  ?
  &gt; p # print current line
  ed.bf is an opinionated UNIX ed re-implementation in Brainfuck.
  &gt; ! [.&gt;]+++++ +++++.[-] # run a script
  ed.bf is an opinionated UNIX ed re-implementation in Brainfuck.
  &gt; q # quit
  =========================================================================]

 [3 MEMORY LAYOUT

  Layout is approximately this:
  [0][80 command cells][0][80 text cells (a line)][0][80 command cells][0] etc.

  80 text cells mean a restriction of 80 characters per
  line. Enforcing this otherwise optional thing helps to ensure the
  layout stability.

  80 command cells need a further explanation. After the command
  parsing, their layout is:
  [line number/exit flag][start line number][end line number][command flag][command][75 argument cells]


  This layout hints at some more restrictions that ed.bf has:

  - Line number is one cell, and thus there can only be as many lines,
  as cell capacity allows. For 8 bit implementations, 255 is the
  maximum line number. Pick a 16/32/64 bit implementation if you
  want more lines.
  - Same restriction holds for start and end line numbers.
  - Line number serves as the exit flag. If it's zero, ed.bf
  exits. Thus, the range of values for line numbers is 1 to 255.
  - Commands can only be one char wide, as in classic ed.
  - There are only 75 cells for arguments. Anything wider will break
  ed.bf in horrible ways. Unless you know what you're doing,
  restrict your command arguments to 75 characters.
  - Command flag is the flag used for the command switch. It should be
  zero unless the current command is not yet processed.
  - Start and end line numbers are not yet used, but will be... one
  day.

  You can hack the layout, if you want. It is trivial to rewrite
  command/line areas by supplying overly long input.]

 [4 TO DO
  - [ ] More commands
  -- [ ] Simple commands ('a', 'd', '=' etc.)
  --- [X] '=' -- half-standard: indexes lines from 1. Prints current
  line, instead of the last line.
  --- [X] 'q' -- non-standard: does not prompt for
  confirmation. Brainfuck has no file handling anyway.
  --- [X] 'c' -- non-standard: inserts exactly one line terminated by
  newline. Creates the line if necessary.
  --- [X] 'p'.
  --- [X] newline -- non-standard: moves forward and prints next line
  unless it's an empty line. Stays on this empty line.
  --- [X] 'd'.
  --- [X] Standalone '-'.
  -- [ ] Commands with args ('s' etc.)
  -- [ ] OS-specific placeholders ('w', 'r').
  -- [ ] Commands with optional args.
  -- [X] Meta-evaluation with '!'.
  - [ ] Addresses.
  -- [X] Address without a command -- half-standard: moves to lines
  that don't even exist yet. Enjoy.
  -- [X] Simple addresses ('1', '222').
  -- [ ] Ranges ('10,20').
  -- [ ] Address aliases (',' and '.', '$', ';', '%').
  -- [ ] Relative addresses ('-10').
  -- [ ] Mark addresses ('k', ''').
  -- [ ] Forward regexp addresses.
  -- [ ] Backward regexp addresses.
  -- [ ] Addresses with commands.
  - [ ] Make ed.bf embeddable.
  -- [ ] Terminate on the same memory cell that ed.bf started on.
  -- [ ] Clean up the memory before terminating.
  - [ ] Use GNU m4 macros to reduce code repetition.
  -- Is that worthy of Brainfuck programmer? No, it's not.
  --- Do I care? Depends on how much bloat I'll end up with.]

 [5 CHANGE LOG
  Version 1.5.7-revision-12
  - Integrate meta.bf nested loops support.

  Version 1.4:
  - Rename to ed.bf and polish the code a bit.

  Version 1.3:
  - Add meta-evaluation in with the help of meta.bf
    (<a href="https://github.com/bf-enterprise-solutions/meta.bf">https://github.com/bf-enterprise-solutions/meta.bf</a>).

  Version 1.2
  - A new and minimal bfed-min.bf

  Version 1.1
  - Command sectors are 80 cells long now.

  Version 1.0
  - Finally, a full range of editing primitives: =, -, newline, c, d, p.]]
</pre></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learnings from fine-tuning LLM on my Telegram messages (195 pts)]]></title>
            <link>https://asmirnov.xyz/doppelganger</link>
            <guid>38434914</guid>
            <pubDate>Mon, 27 Nov 2023 17:09:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asmirnov.xyz/doppelganger">https://asmirnov.xyz/doppelganger</a>, See on <a href="https://news.ycombinator.com/item?id=38434914">Hacker News</a></p>
<div id="readability-page-1" class="page">
    
        <i>27 Nov 2023</i>
        <div id="toc"><ul>
<li><a href="#approach" id="toc-approach">Approach</a></li>
<li><a href="#data-preparation" id="toc-data-preparation">Data
preparation</a></li>
<li><a href="#evaluation-plan" id="toc-evaluation-plan">Evaluation
plan</a></li>
<li><a href="#lora" id="toc-lora">LoRA</a></li>
<li><a href="#full-fine-tuning" id="toc-full-fine-tuning">Full
fine-tuning</a></li>
<li><a href="#closing-thoughts" id="toc-closing-thoughts">Closing
thoughts</a></li>
<li><a href="#code" id="toc-code">Code</a></li>
</ul></div>
    <hr>
    <p>For most people I interact with, I’m just another text-based
    program for the most of the time. If input and output are so simple,
    could I be replaced by the model? For this to work, the model would
    need to not only understand my writing style but also know a lot
    about me. The best source for this is my Telegram messenger, as I
    use it daily and it contains almost everything about my thoughts and
    actions in the form of chat histories.</p>
    <h2 id="approach"><a href="#approach">Approach</a></h2>
    <p>The most straightforward approach would be to extract all my
    messages, load them into ChatGPT’s context, and instruct it to use
    this information to mimic my style when responding to new messages.
    However, this approach is limited by the context window size,
    requiring me to preprocess messages to extract key points. As I want
    to avoid this hassle, perhaps Retrieval Augmented Generation (RAG)
    could be used to pull necessary information when needed. However
    from my experience, retrieving from diverse data like chat sessions
    usually needs a supervised fine-tuning of the retrieval model, and
    I’m not keen on creating such a dataset. So, fine-tuning seems like
    the best option. It’s ideal for several reasons: it should capture
    my writing style and potentially accumulate knowledge from all my
    messages without having to select what’s important.</p>
    <p>OpenAI offers <a href="https://platform.openai.com/docs/guides/fine-tuning" target="_blank">fine-tuning capabilities</a>, but as I’ll be using
    my private messages, I don’t want to use any third-party fine-tuning
    services. So, I need to choose a base model. According to the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" target="_blank">Hugging Face Open LLM Leaderboard</a>, one of the
    top smaller models (≤13B parameters) is <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1" target="_blank">Mistral 7B</a>. It even outperforms <a href="https://huggingface.co/meta-llama/Llama-2-13b-hf" target="_blank">Llama 2 13B</a>. Now, the question is whether <a href="https://arxiv.org/abs/2106.09685" target="_blank">LoRA</a> is
    sufficient or if full fine-tuning is necessary. Various comparisons
    <a href="#fn1" id="fnref1" role="doc-noteref">[1]</a> <a href="#fn2" id="fnref2" role="doc-noteref">[2]</a>
    suggests that LoRA is a bit worse than full fine-tuning but still
    fine most of the time. However, for specific tasks like mine
    (Russian language + chat), I found a <a href="https://arxiv.org/abs/2304.08109" target="_blank">paper</a>,
    where researchers conducted Llama instruction fine-tuning in
    Chinese, similar in complexity to my goal. They found that
    LoRA-based tuning on a base model without prior instruction tuning
    is less effective than full fine-tuning. Yet, LoRA-based tuning on a
    model already fine-tuned for instructions can yield comparable
    results. In my case, this means either full fine-tuning on a base
    model or LoRA on a model already fine-tuned for chatting in Russian.
    Since I couldn’t find a model fine-tuned for Russian chat, I’ll try
    LoRA on a model fine-tuned for English chat, like the fine-tuned
    Mistral model <a href="https://huggingface.co/ehartford/dolphin-2.2.1-mistral-7b" target="_blank">Dolphin</a>.</p>
    <p>So, the plan is:</p>
    <ol type="1">
    <li>Start with LoRA on top of Dolphin, the English chat fine-tuned
    Mistral</li>
    <li>If quality is not sufficient, try full fine-tuning on
    Mistral</li>
    </ol>
    <h2 id="data-preparation"><a href="#data-preparation">Data
    preparation</a></h2>
    <p>One unique aspect of messaging in apps like Telegram, compared to
    emails, is the conversational flow. Messages don’t usually alternate
    one-by-one between you and your contact. Instead, you often find
    yourself sending a couple of messages in a row, followed by several
    responses from the other person. These messages are generally short,
    too. I wanted to preserve this natural conversational style in my
    data.</p>
    <p>Telegram offers a <a href="https://telegram.org/blog/export-and-more" target="_blank">built-in feature</a> to export all chats into JSON.
    After some filtering and grouping messages into sessions, I’ve
    compiled data from the last five years of using Telegram. This
    resulted in 15,789 sessions from 466 chats, with an average session
    length of 8.51 messages. For structuring the data, I’ve chosen the
    <a href="https://github.com/openai/openai-python/blob/284c1799070c723c6a553337134148a7ab088dd8/chatml.md" target="_blank">ChatML</a> prompt format. Here’s a sample session
    (translated from Russian):</p>
    <p>&lt;|im_start|&gt;John Smith<br> <strong>&gt;&gt;&gt; damn,
    can’t get around the 135 time limit</strong><br>
    <strong>&gt;&gt;&gt; trying to do everything super optimally, but no
    luck&lt;|im_end|&gt;</strong><br> &lt;|im_start|&gt;Alexander
    Smirnov<br> <strong>&gt;&gt;&gt; yeah same</strong><br>
    <strong>&gt;&gt;&gt; you still going with the same
    idea?&lt;|im_end|&gt;</strong><br> &lt;|im_start|&gt;John
    Smith<br> <strong>&gt;&gt;&gt; dunno, I think we’re on the same
    page</strong><br> <strong>&gt;&gt;&gt; as you said</strong><br>
    <strong>&gt;&gt;&gt; going with the reversed string in a try and
    trying to find something there</strong><br> <strong>&gt;&gt;&gt;
    seems like real shit because z function ruins
    everything……………………&lt;|im_end|&gt;</strong><br>
    &lt;|im_start|&gt;Alexander Smirnov<br> <strong>&gt;&gt;&gt; don’t
    get where z comes into this&lt;|im_end|&gt;</strong><br>
    &lt;|im_start|&gt;John Smith<br> <strong>&gt;&gt;&gt; dunno seems
    like I’m doing everything iteratively anyway, but yeah gotta reverse
    some strings to build the z function</strong><br>
    <strong>&gt;&gt;&gt; and it’s just a random solution</strong><br>
    <strong>&gt;&gt;&gt; from discussions&lt;|im_end|&gt;</strong><br>
    &lt;|im_start|&gt;Alexander Smirnov<br> <strong>&gt;&gt;&gt; got
    it&lt;|im_end|&gt;</strong><br></p>
    <details>
    <summary>
    original
    </summary>
    &lt;|im_start|&gt;Иван Иванович<br> <strong>&gt;&gt;&gt; бля не
    могу обойти таймлим на 135</strong><br> <strong>&gt;&gt;&gt;
    пытаюсь все супер оптимально делать, но
    хуйтам)&lt;|im_end|&gt;</strong><br> &lt;|im_start|&gt;Alexander
    Smirnov<br> <strong>&gt;&gt;&gt; да вот жиза</strong><br>
    <strong>&gt;&gt;&gt; ты с той же
    идеей?&lt;|im_end|&gt;</strong><br> &lt;|im_start|&gt;Иван
    Иванович<br> <strong>&gt;&gt;&gt; да хз, думаю у нас одно и
    тоже</strong><br> <strong>&gt;&gt;&gt; как ты
    сказал</strong><br> <strong>&gt;&gt;&gt; иду с реверснутой строкой
    в трай и чето пытаюсь там найти</strong><br> <strong>&gt;&gt;&gt;
    походу реальная параша на z функции все
    руинит……………………&lt;|im_end|&gt;</strong><br>
    &lt;|im_start|&gt;Alexander Smirnov<br> <strong>&gt;&gt;&gt; не
    пон где тут про z&lt;|im_end|&gt;</strong><br>
    &lt;|im_start|&gt;Иван Иванович<br> <strong>&gt;&gt;&gt; хз вроде
    все итеративно итак делаю, ну да кое где надо реверснуть строки
    чтобы з функцию построить</strong><br> <strong>&gt;&gt;&gt; а это
    просто рандомное решение</strong><br> <strong>&gt;&gt;&gt; с
    дискашенов&lt;|im_end|&gt;</strong><br>
    &lt;|im_start|&gt;Alexander Smirnov<br> <strong>&gt;&gt;&gt;
    пон&lt;|im_end|&gt;</strong><br>
    </details>
    <p>My data collator ensures that the loss is only calculated based
    on someone’s response. Predicting who will speak next is relatively
    straightforward, and we don’t want the model to focus on learning
    that. Therefore, parts of the conversation where the loss is
    calculated are highlighted in bold.</p>
    <p>You might notice that not only my responses but also those of
    others are used for loss calculation. This is deliberate. By doing
    this, the model will be able to role-play not only as me but also as
    my frequent conversational partners!</p>
    <h2 id="evaluation-plan"><a href="#evaluation-plan">Evaluation
    plan</a></h2>
    <p>I will test models by having chats in two ways. First, the model
    will pretend to be me and I will be chatting with myself from the
    perspective of my different friends. Then, I’ll chat as myself while
    the model acts as my friends. My conversation starter will always be
    the same 2 messages: “hey” and “what’s up?” (in Russian, “прив” and
    “как дела?”). Generated phrases and persons as the model acts who
    from will be <strong>highlighted</strong>. All conversations
    initially will be held in Russian and may be accessed by clicking on
    the ‘original’ details button. For testing I will be using <a href="https://github.com/oobabooga/text-generation-webui" target="_blank">oobabooga/text-generation-webui</a>.</p>
    <p>In the beginning, I want to explore how the generic conversation
    fine-tuned Mistral model deals with that task without any prior
    training from my side.</p>
    <p>---</p>
    <details>
    <summary>
    Friend 1 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; I’m good, what about you?</strong><br>
    &gt;&gt;&gt; nothing much<br> &gt;&gt;&gt; what did you do
    today?<br> <strong>&gt;&gt;&gt; I worked.</strong><br>
    <strong>&gt;&gt;&gt; then I did a few tasks in Python and studied
    data processing libraries.</strong><br> <strong>&gt;&gt;&gt; in
    the evening I’ll do wrestling training.</strong><br>
    <strong>&gt;&gt;&gt; have you ever been into sports or do you have
    any hobbies?</strong><br> &gt;&gt;&gt; I see<br> &gt;&gt;&gt;
    want to watch a movie today?<br> <strong>&gt;&gt;&gt; maybe, if I
    find the time.</strong><br> <strong>&gt;&gt;&gt; what’s your
    favorite music? Electronic, rock, or other genres?</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; все хорошо, а у тебя что нового?</strong><br>
    &gt;&gt;&gt; да ничего<br> &gt;&gt;&gt; чего сегодня делал?<br>
    <strong>&gt;&gt;&gt; я работал.</strong><br> <strong>&gt;&gt;&gt;
    потом сделал несколько задач на python и изучал библиотеки для
    обработки данных.</strong><br> <strong>&gt;&gt;&gt; в вечер
    займусь тренировкой на борцу.</strong><br> <strong>&gt;&gt;&gt; ты
    когда-нибудь занимаешься спортом или имеешь какое-то
    хобби?</strong><br> &gt;&gt;&gt; понятно<br> &gt;&gt;&gt;
    посмотрим кино сегодня?<br> <strong>&gt;&gt;&gt; может быть, если
    найдется время.</strong><br> <strong>&gt;&gt;&gt; какая ваша
    любимая музыка? Электронная музыка, рок или другие
    жанры?</strong></p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 1</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; well, everything is fine so far. what’s new
    with you?</strong><br> &gt;&gt;&gt; spent the whole day
    programming<br> &gt;&gt;&gt; what did you do today?<br>
    <strong>&gt;&gt;&gt; finished at work</strong><br>
    <strong>&gt;&gt;&gt; and did some housework, now I’m resting. are
    you also resting or still working?</strong><br> &gt;&gt;&gt; I’m
    already resting<br> <strong>&gt;&gt;&gt; well, that’s good. can we
    talk about something interesting or do you have any
    questions?</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; ну пока все нормально. а у вас что
    нового?</strong><br> &gt;&gt;&gt; прогал целый день<br>
    &gt;&gt;&gt; чего сегодня делала?<br> <strong>&gt;&gt;&gt;
    закончила на работе</strong><br> <strong>&gt;&gt;&gt; и сделала
    немного домашней работы, теперь отдыхаю. а вы тоже отдохните или еще
    работаете?</strong><br> &gt;&gt;&gt; я уже отдыхаю<br>
    <strong>&gt;&gt;&gt; ну хорошо. можно поговорить о чем-нибудь
    интересном или вы имеете какие-то вопросы?</strong></p>
    </details>
    <p>---</p>
    <p>Ok, it is capable of forming coherent sentences. The most
    noticeable problem is its lack of awareness regarding the context of
    the conversations which leads to bland and generic replies. The
    messages lacked any distinct style, feeling quite basic. Another
    issue is that the model’s Russian is poor. This is expected, as the
    model is too small to generalize well to languages other than its
    primary one, English. Additionally, the model tended to be overly
    proactive, ending almost every sentence with a question, which isn’t
    how real people typically communicate in messengers.</p>
    <p>Let’s try to fix all of these!</p>
    <h2 id="lora"><a href="#lora">LoRA</a></h2>
    <p>LoRA offers a low-effort approach in terms of both the training
    pipeline and hardware requirements. It trains around 1% of the total
    weights. I chose a 1024 sequence length and a batch size of 8. The
    training, which consumed 20GB of VRAM on an RTX 3090, took three
    epochs and lasted for 5.5 hours. For this, I used <a href="https://vast.ai/" target="_blank">vast.ai</a>, where the GPU
    cost was $0.362 per hour, totaling $2 for the entire training,
    excluding time spent on experiments and bug fixes.</p>
    <p>Here are the results:</p>
    <p>---</p>
    <details>
    <summary>
    Friend 1 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; hi) all good</strong><br>
    <strong>&gt;&gt;&gt; you?)</strong><br> &gt;&gt;&gt; ok as
    well<br> &gt;&gt;&gt; what you were doing today?<br>
    <strong>&gt;&gt;&gt; had an amazon interview and solved
    problems</strong><br> <strong>&gt;&gt;&gt; had to solve
    3</strong><br> &gt;&gt;&gt; cool<br> &gt;&gt;&gt; hard?<br>
    <strong>&gt;&gt;&gt; don’t know what</strong><br>
    <strong>&gt;&gt;&gt; hadn’t looked yet</strong><br>
    <strong>&gt;&gt;&gt; I hadn’t started yet</strong><br>
    <strong>&gt;&gt;&gt; chatting whole day with the team and meet new
    ones</strong><br> &gt;&gt;&gt; how are they?<br>
    <strong>&gt;&gt;&gt; ok</strong><br> <strong>&gt;&gt;&gt; same
    coworking as you have))</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; Привет) нолм все</strong><br>
    <strong>&gt;&gt;&gt; ты как?)</strong><br> &gt;&gt;&gt; да тоже
    норм<br> &gt;&gt;&gt; чего делал сегодня?<br>
    <strong>&gt;&gt;&gt; собесился в амазон и решал
    задачки</strong><br> <strong>&gt;&gt;&gt; надо было 3
    прорешить</strong><br> &gt;&gt;&gt; прикольно<br> &gt;&gt;&gt;
    сложные?<br> <strong>&gt;&gt;&gt; хз какие</strong><br>
    <strong>&gt;&gt;&gt; пока не смотрел</strong><br>
    <strong>&gt;&gt;&gt; я еще ничего не началь</strong><br>
    <strong>&gt;&gt;&gt; весь день просто болтаю с людьми из команды и
    встречаю новых</strong><br> &gt;&gt;&gt; как ребята?<br>
    <strong>&gt;&gt;&gt; норм</strong><br> <strong>&gt;&gt;&gt; такой
    же ковер как у тебя))</strong></p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Friend 2 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; ok, we show yesterday’s product to searchcom,
    today we write endpoints for them so that someone else can request
    our api</strong><br> &gt;&gt;&gt; cool<br> &gt;&gt;&gt; what
    problems do we have now?<br> <strong>&gt;&gt;&gt; no
    problems</strong><br> <strong>&gt;&gt;&gt; everything is
    working</strong><br> <strong>&gt;&gt;&gt; I added an option to
    choose index</strong><br> <strong>&gt;&gt;&gt; so now we can
    attach several indexes</strong><br> <strong>&gt;&gt;&gt; and
    separate collections within single index</strong><br> &gt;&gt;&gt;
    what we have to do?<br> <strong>&gt;&gt;&gt; test</strong><br>
    <strong>&gt;&gt;&gt; yes ok</strong><br> <strong>&gt;&gt;&gt; are
    they always will be sending post-script?</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; хорошо, показываем вчерашний продукт ищком,
    сегодня пилим эндпоинты для них чтобы кто-то еще мог нашу апи
    запросить</strong><br> &gt;&gt;&gt; найс<br> &gt;&gt;&gt; какие
    сейчас есть проблемы?<br> <strong>&gt;&gt;&gt; нет
    проблем</strong><br> <strong>&gt;&gt;&gt; все
    работает</strong><br> <strong>&gt;&gt;&gt; я добавил опцию выбора
    индекса</strong><br> <strong>&gt;&gt;&gt; так что теперь уже можно
    несколько индексов подключить</strong><br> <strong>&gt;&gt;&gt; и
    отдельные коллекции внутри одного индекса</strong><br>
    &gt;&gt;&gt; что надо сделать?<br> <strong>&gt;&gt;&gt;
    протестировать</strong><br> <strong>&gt;&gt;&gt; ага
    норм</strong><br> <strong>&gt;&gt;&gt; а точно они всегда
    пост-скрипт будут кидать?</strong></p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Friend 3 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; Hi, good)</strong><br> <strong>&gt;&gt;&gt;
    you?</strong><br> &gt;&gt;&gt; ok as well<br> &gt;&gt;&gt; what
    do you do at the evening?<br> <strong>&gt;&gt;&gt; we have meeting
    with office collegues at 8 30 msk</strong><br>
    <strong>&gt;&gt;&gt; after it I can</strong><br> &gt;&gt;&gt; can
    what?<br> <strong>&gt;&gt;&gt; haha)</strong><br> &gt;&gt;&gt;
    didn’t get it<br> <strong>&gt;&gt;&gt; I don’t know(</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; Привет, хорошо)</strong><br>
    <strong>&gt;&gt;&gt; ты?</strong><br> &gt;&gt;&gt; да тоже
    норм<br> &gt;&gt;&gt; что делаешь вечером?<br>
    <strong>&gt;&gt;&gt; у нас с колегами встреча по офису в 8 30
    мск</strong><br> <strong>&gt;&gt;&gt; после нее я
    могу</strong><br> &gt;&gt;&gt; можешь что?<br>
    <strong>&gt;&gt;&gt; ахах)</strong><br> &gt;&gt;&gt; не
    понял<br> <strong>&gt;&gt;&gt; я не знаю(</strong></p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 1</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; hi)</strong><br> <strong>&gt;&gt;&gt; seems
    good</strong><br> <strong>&gt;&gt;&gt; was working, than went to
    the call with Nadya</strong><br> <strong>&gt;&gt;&gt; now reading
    normally</strong><br> <strong>&gt;&gt;&gt; you?</strong><br>
    &gt;&gt;&gt; good as well<br> &gt;&gt;&gt; any plans for the
    evening?<br> <strong>&gt;&gt;&gt; after work wendesday I would
    love to go running</strong><br> <strong>&gt;&gt;&gt; and then have
    a lunch</strong><br> <strong>&gt;&gt;&gt; your
    plans?</strong><br> &gt;&gt;&gt; I though about watching
    movie<br> <strong>&gt;&gt;&gt; aa</strong><br>
    <strong>&gt;&gt;&gt; ooo</strong><br> <strong>&gt;&gt;&gt; to be
    honest I like when it is raining</strong><br> <strong>&gt;&gt;&gt;
    when you don’t feel like going anywhere</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; привет)</strong><br> <strong>&gt;&gt;&gt;
    вроде хорошо</strong><br> <strong>&gt;&gt;&gt; поработал, затем
    поехал на коле с Надей</strong><br> <strong>&gt;&gt;&gt; сейчас
    читаю нормально</strong><br> <strong>&gt;&gt;&gt; а ты
    как?</strong><br> &gt;&gt;&gt; да тоже хорошо<br> &gt;&gt;&gt;
    какие планы на вечер?<br> <strong>&gt;&gt;&gt; мне после работы
    среда хотелось бы сходить на пробежку</strong><br>
    <strong>&gt;&gt;&gt; а потом ужинать</strong><br>
    <strong>&gt;&gt;&gt; а что твои планы?</strong><br> &gt;&gt;&gt; я
    думал кино посмотреть<br> <strong>&gt;&gt;&gt; аа</strong><br>
    <strong>&gt;&gt;&gt; ооо</strong><br> <strong>&gt;&gt;&gt; честно
    говоря мне нравится теплый дождь больше всего</strong><br>
    <strong>&gt;&gt;&gt; когда так не хочется никуда идти</strong></p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 2</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; Hi, great!</strong><br> <strong>&gt;&gt;&gt;
    Sorry will be 20-30 min late</strong><br> &gt;&gt;&gt; ok<br>
    &gt;&gt;&gt; why?<br> <strong>&gt;&gt;&gt; yes, I just got up and
    forgot to press the «exit sleep» button</strong><br>
    <strong>&gt;&gt;&gt; my telegram didn’t close on its
    own</strong><br> <strong>&gt;&gt;&gt; I thought it behaved like
    this when you logging off the Internet, but no</strong><br>
    <strong>&gt;&gt;&gt; I still logged off the Internet, but telegram
    still didn’t respond</strong><br> <strong>&gt;&gt;&gt; and it was
    strange, because yesterday there was also such a case when I sent
    you a message and did not receive an answer</strong><br>
    <strong>&gt;&gt;&gt; today I looked at mine, but not yours, but
    yours was displayed)</strong><br> &gt;&gt;&gt; ok<br>
    &gt;&gt;&gt; any plans?<br> <strong>&gt;&gt;&gt; yes, we can chat
    a little</strong><br> <strong>&gt;&gt;&gt; and I can tell you that
    the method for generating answers worked really well for
    me</strong><br> <strong>&gt;&gt;&gt; he gives respectable results
    in training</strong><br> <strong>&gt;&gt;&gt; but not on the
    test</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; Привет, хорошо!</strong><br>
    <strong>&gt;&gt;&gt; Соре опаздываю еще минут 20-30</strong><br>
    &gt;&gt;&gt; пон<br> &gt;&gt;&gt; а чего так?<br>
    <strong>&gt;&gt;&gt; да я просто встал и забыл нажать кнопку «выйти
    из сна»</strong><br> <strong>&gt;&gt;&gt; у меня телега сама не
    закрылась</strong><br> <strong>&gt;&gt;&gt; я думал она так себя
    ведёт, когда ты выходишь из интернета, но вот</strong><br>
    <strong>&gt;&gt;&gt; я все таки вышел из интернета, но телега все
    равно не отвечает</strong><br> <strong>&gt;&gt;&gt; и это странно
    было, потому что вчера тоже был такой случай, когда я отправил тебе
    сообщение и не получал ответа</strong><br> <strong>&gt;&gt;&gt;
    сегодня же я посмотрел свое, но не твоё, а твое
    отобразилось)</strong><br> &gt;&gt;&gt; окей<br> &gt;&gt;&gt;
    какие планы?<br> <strong>&gt;&gt;&gt; да можно чуть
    поболтать</strong><br> <strong>&gt;&gt;&gt; и можно рассказать что
    у меня прям очень сильно заработал метод по генерации
    ответов</strong><br> <strong>&gt;&gt;&gt; он на тренировках дает
    респектабельные результаты</strong><br> <strong>&gt;&gt;&gt; но не
    на тесте</strong></p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 3</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; hey, hi</strong><br> <strong>&gt;&gt;&gt;
    Good, you?)</strong><br> <strong>&gt;&gt;&gt; Haven’t gone to bed
    yet)0)0)0)</strong><br> <strong>&gt;&gt;&gt; Today I get up and go
    to a programming course</strong><br> <strong>&gt;&gt;&gt; To
    tighten up the base, etc.</strong><br> &gt;&gt;&gt; nice<br>
    &gt;&gt;&gt; what did you do yesterday?<br> <strong>&gt;&gt;&gt;
    Yesterday there was already the same course in which it seemed to me
    that I did very poorly</strong><br> <strong>&gt;&gt;&gt; Then I
    wanted to solve hw but I don’t care</strong><br>
    <strong>&gt;&gt;&gt; Otherwise I’m fucked because I can’t
    sleep)</strong><br> <strong>&gt;&gt;&gt; Really fucked
    up</strong><br> <strong>&gt;&gt;&gt; It will probably be better
    now, since I’m already maximally overtrained</strong><br>
    &gt;&gt;&gt; cool man<br> &gt;&gt;&gt; what are your plans for
    life in general?<br> <strong>&gt;&gt;&gt; Fuck, get a job
    interviews first</strong><br> <strong>&gt;&gt;&gt; And if
    everything is ok, then actually order a computer</strong><br>
    <strong>&gt;&gt;&gt; Like, I think you should order a computer when
    it’s more profitable than working at work</strong><br>
    <strong>&gt;&gt;&gt; But I don’t care</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; Прив, дароу</strong><br> <strong>&gt;&gt;&gt;
    Норм, ты как?)</strong><br> <strong>&gt;&gt;&gt; Еще спать не
    зашел)0)0)0)</strong><br> <strong>&gt;&gt;&gt; Сегодня встаю и еду
    на курс по програмированию</strong><br> <strong>&gt;&gt;&gt; Чтоб
    базу подтянуть и тд</strong><br> &gt;&gt;&gt; найс<br>
    &gt;&gt;&gt; че вчера делал?<br> <strong>&gt;&gt;&gt; Вчера уже
    был такой же курс в котором мне казалось что я оч плохо
    провел</strong><br> <strong>&gt;&gt;&gt; Потом хотел порешать дз
    но пофиг</strong><br> <strong>&gt;&gt;&gt; А то заебался из-за
    того что спать не могу)</strong><br> <strong>&gt;&gt;&gt; Прям
    пиздец</strong><br> <strong>&gt;&gt;&gt; Наверное сейчас будет
    лучше, так как уже максимально переобуженный</strong><br>
    &gt;&gt;&gt; слушай ну с кайфом<br> &gt;&gt;&gt; какие планы
    вообще на жизнь?<br> <strong>&gt;&gt;&gt; Бля, тупо сначала
    собеситься на работу</strong><br> <strong>&gt;&gt;&gt; И если все
    ок то реально заказать комп</strong><br> <strong>&gt;&gt;&gt; Типо
    я думаю надо было бы заказывать комп когда выигральнее чем на работе
    работаешь</strong><br> <strong>&gt;&gt;&gt; Но я хз</strong></p>
    </details>
    <p>---</p>
    <p>This is much better. It definitely captures the style of the
    person it’s responding on behalf of. It also identifies the most
    common topics discussed between specific pairs of people. For
    example, with friend 2, the focus is clearly more on work. However,
    the grammar is still off, and it loses the context of the
    conversation quickly. I’m pretty confident that LoRA would work with
    reasonable quality in English, and full fine-tuning might not be
    necessary. But, since Russian isn’t the model’s native language,
    let’s try full fine-tuning.</p>
    <h2 id="full-fine-tuning"><a href="#full-fine-tuning">Full
    fine-tuning</a></h2>
    <p>Full fine-tuning is more challenging due to the need for
    multi-GPU training. Popular methods include either <a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/" target="_blank">ZeRO &amp; DeepSpeed</a> <a href="#fn3" id="fnref3" role="doc-noteref">[3]</a>
    or <a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/" target="_blank">FSDP</a> <a href="#fn4" id="fnref4" role="doc-noteref">[4]</a>, with FSDP
    essentially being a ZeRO3 <a href="#fn5" id="fnref5" role="doc-noteref">[5]</a>. I decided to go
    with FSDP.</p>
    <p>While implementing the training pipeline, I referred to the <a href="https://github.com/tatsu-lab/stanford_alpaca/" target="_blank">Stanford Alpaca fine-tuning code</a> and <a href="https://github.com/abacaj/fine-tune-mistral/" target="_blank">Anton Bacaj’s Mistral fine-tuning code</a>.</p>
    <p>Using a half-precision FSDP full shard with a 1024 sequence
    length and a micro batch size of 2 required 63GB of VRAM on each of
    the eight A100 80 GB GPUs. The training, lasting three epochs, took
    just 20 minutes. The total cost for the VM was $8.88 per hour,
    resulting in $3, not including the time for experiments and bug
    fixes.</p>
    <p>Conversations:</p>
    <p>---</p>
    <details>
    <summary>
    Friend 1 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; hey) good</strong><br> <strong>&gt;&gt;&gt;
    you?</strong><br> &gt;&gt;&gt; good as well<br> &gt;&gt;&gt;
    what you were doing today?<br> <strong>&gt;&gt;&gt; played a board
    game</strong><br> <strong>&gt;&gt;&gt; I’ll go home
    now</strong><br> &gt;&gt;&gt; nice<br> &gt;&gt;&gt; what kind of
    game?<br> <strong>&gt;&gt;&gt; catan</strong><br>
    <strong>&gt;&gt;&gt; strategy</strong><br> <strong>&gt;&gt;&gt;
    very good</strong><br> &gt;&gt;&gt; who was there?<br>
    <strong>&gt;&gt;&gt; Max Yavich and Fedya</strong><br>
    &gt;&gt;&gt; who won?<br> <strong>&gt;&gt;&gt; Max</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; Прив) норм</strong><br> <strong>&gt;&gt;&gt;
    а у тебя?</strong><br> &gt;&gt;&gt; да тоже норм<br>
    &gt;&gt;&gt; чего делал сегодня?<br> <strong>&gt;&gt;&gt; играли в
    настолку</strong><br> <strong>&gt;&gt;&gt; ща поеду
    домой</strong><br> &gt;&gt;&gt; найс<br> &gt;&gt;&gt; что за
    настолка?<br> <strong>&gt;&gt;&gt; колонизаторы</strong><br>
    <strong>&gt;&gt;&gt; стратежка</strong><br> <strong>&gt;&gt;&gt;
    оч хорошая</strong><br> &gt;&gt;&gt; кто был?<br>
    <strong>&gt;&gt;&gt; Макс Явич и Федя</strong><br> &gt;&gt;&gt;
    кто выиграл?<br> <strong>&gt;&gt;&gt; Макс</strong></p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Friend 2 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; hi, good</strong><br> <strong>&gt;&gt;&gt;
    let’s meet in half an hour?</strong><br> &gt;&gt;&gt; yep<br>
    &gt;&gt;&gt; what did you want?<br> <strong>&gt;&gt;&gt; damn I
    haven’t finished the script for recalculating models
    yet</strong><br> <strong>&gt;&gt;&gt; and I need to finish it to
    send it to them</strong><br> <strong>&gt;&gt;&gt; this
    recalculation takes a lot of time, so I just want to turn around and
    start doing it and not waste time on preparation</strong><br>
    &gt;&gt;&gt; got it<br> &gt;&gt;&gt; what is the plan for
    today?<br> <strong>&gt;&gt;&gt; meet in half an hour to discuss
    what we are doing</strong><br> <strong>&gt;&gt;&gt; if decided,
    then I send an application to them and wait for a
    response</strong><br> <strong>&gt;&gt;&gt; if we haven’t decided,
    then we continue to work</strong><br> &gt;&gt;&gt; who do you mean
    by them?<br> <strong>&gt;&gt;&gt; investors</strong><br>
    <strong>&gt;&gt;&gt;
    https://www.ycombinator.com/howtoapply</strong><br>
    <strong>&gt;&gt;&gt; there are two tests)</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; прив, збс</strong><br> <strong>&gt;&gt;&gt;
    го кол через пол часа?</strong><br> &gt;&gt;&gt; да го<br>
    &gt;&gt;&gt; а чего ты хотел?<br> <strong>&gt;&gt;&gt; блять я еще
    не доделал скрипт по пересчету моделей</strong><br>
    <strong>&gt;&gt;&gt; и мне надо доделать чтобы отправить
    им</strong><br> <strong>&gt;&gt;&gt; вот этот пересчет занимает
    много времени поэтому я хочу просто уже развернуться и начать делать
    а не тратить время на подготовку</strong><br> &gt;&gt;&gt;
    понял<br> &gt;&gt;&gt; а какой вообще план на сегодня?<br>
    <strong>&gt;&gt;&gt; встречаемся через пол часа, обсуждаем что
    делаем</strong><br> <strong>&gt;&gt;&gt; если решили то отправляю
    заявку в им и жду ответа</strong><br> <strong>&gt;&gt;&gt; если не
    решили то продолжаем работать</strong><br> &gt;&gt;&gt; им это
    кому?<br> <strong>&gt;&gt;&gt; инвесторы</strong><br>
    <strong>&gt;&gt;&gt;
    https://www.ycombinator.com/howtoapply</strong><br>
    <strong>&gt;&gt;&gt; там два теста)</strong></p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Friend 3 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; hey we’re sitting and solving
    problems</strong><br> &gt;&gt;&gt; what kind of problems?<br>
    <strong>&gt;&gt;&gt; data analysis +-</strong><br> &gt;&gt;&gt;
    and for what?<br> <strong>&gt;&gt;&gt;
    https://yandex.ru/cup/profile/</strong><br> <strong>&gt;&gt;&gt;
    it seems it just became more interesting</strong><br> &gt;&gt;&gt;
    cool<br> &gt;&gt;&gt; what are you planning this evening?<br>
    <strong>&gt;&gt;&gt; idk yet</strong><br> &gt;&gt;&gt; and on the
    weekend?<br> <strong>&gt;&gt;&gt; apparently I’ll also finish
    solving problems and get some work done</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; здарова нормуль сидим решаем
    задачки</strong><br> &gt;&gt;&gt; чего за задачки?<br>
    <strong>&gt;&gt;&gt; анализ данных +-</strong><br> &gt;&gt;&gt; а
    для чего?<br> <strong>&gt;&gt;&gt;
    https://yandex.ru/cup/profile/</strong><br> <strong>&gt;&gt;&gt;
    вроде просто поинтересней стало</strong><br> &gt;&gt;&gt;
    прикольно<br> &gt;&gt;&gt; что вечером планируешь?<br>
    <strong>&gt;&gt;&gt; хз пока</strong><br> &gt;&gt;&gt; а на
    выхах?<br> <strong>&gt;&gt;&gt; видимо тоже буду дорешивать задачи
    и с работой позаниматься</strong></p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 1</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; hey)</strong><br> <strong>&gt;&gt;&gt;
    good</strong><br> <strong>&gt;&gt;&gt; you?</strong><br>
    &gt;&gt;&gt; that’s okay too<br> &gt;&gt;&gt; what did you do
    today?<br> <strong>&gt;&gt;&gt; well, I worked, of
    course</strong><br> <strong>&gt;&gt;&gt; and also</strong><br>
    <strong>&gt;&gt;&gt; met with Alena</strong><br>
    <strong>&gt;&gt;&gt; ate at the ramen shop with
    everyone</strong><br> <strong>&gt;&gt;&gt; and went
    home</strong><br> <strong>&gt;&gt;&gt; by the way, we booked a
    place for 14 people</strong><br> <strong>&gt;&gt;&gt; for the
    evening))</strong><br> &gt;&gt;&gt; cool<br> &gt;&gt;&gt; and
    what’s about university?<br> <strong>&gt;&gt;&gt;
    yeah</strong><br> <strong>&gt;&gt;&gt; studied a
    lot</strong><br> <strong>&gt;&gt;&gt; in general, every day I want
    to study more</strong><br> &gt;&gt;&gt; what is the hardest thing
    now?<br> <strong>&gt;&gt;&gt; well I don’t know</strong><br>
    <strong>&gt;&gt;&gt; it just takes a very long time</strong><br>
    <strong>&gt;&gt;&gt; there are some optimization methods that need
    to be rewritten</strong><br> <strong>&gt;&gt;&gt; so I think maybe
    it’s not worth optimizing</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; прив)</strong><br> <strong>&gt;&gt;&gt;
    норм</strong><br> <strong>&gt;&gt;&gt; а у тебя?</strong><br>
    &gt;&gt;&gt; да тоже норм<br> &gt;&gt;&gt; чего делала
    сегодня?<br> <strong>&gt;&gt;&gt; ну я работала,
    конечно</strong><br> <strong>&gt;&gt;&gt; а еще</strong><br>
    <strong>&gt;&gt;&gt; с Аленой встретилась</strong><br>
    <strong>&gt;&gt;&gt; поели в раменной со всеми</strong><br>
    <strong>&gt;&gt;&gt; и домой</strong><br> <strong>&gt;&gt;&gt;
    кстати, мы забронировали рест на 14 человек</strong><br>
    <strong>&gt;&gt;&gt; для вечера))</strong><br> &gt;&gt;&gt;
    прикольно<br> &gt;&gt;&gt; а по унику чего?<br>
    <strong>&gt;&gt;&gt; ага</strong><br> <strong>&gt;&gt;&gt;
    заботала очень много</strong><br> <strong>&gt;&gt;&gt; вообще,
    каждый день хочу больше ботать</strong><br> &gt;&gt;&gt; а что
    сейчас самое тяжелое?<br> <strong>&gt;&gt;&gt; ну не
    знаю</strong><br> <strong>&gt;&gt;&gt; просто очень долго
    получается</strong><br> <strong>&gt;&gt;&gt; там некоторые методы
    оптимизации надо переписывать</strong><br> <strong>&gt;&gt;&gt;
    так что я думаю, может, это не стоит оптимизировать</strong></p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 2</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; Hey, good, what about you?</strong><br>
    <strong>&gt;&gt;&gt; Are you already in Bangkok?</strong><br>
    &gt;&gt;&gt; yeah, already here<br> &gt;&gt;&gt; what are your
    plans for today?<br> <strong>&gt;&gt;&gt; Well, I would like to
    work on these pioneer tasks, if you don’t mind, we can do it
    together</strong><br> <strong>&gt;&gt;&gt; Or you can skip, I’ll
    finish the livechat tomorrow</strong><br> &gt;&gt;&gt; and what
    about live chat?<br> <strong>&gt;&gt;&gt; Damn right now I
    launched the front on the prod without this shit that generates this
    thing with a logo, and it swears, says to connect js from google
    analytics</strong><br> <strong>&gt;&gt;&gt; I understand that this
    is a fucking cringe, but I still tried to remember where to get
    it</strong><br> &gt;&gt;&gt; yeah got it<br> &gt;&gt;&gt; what
    will you do on weekends?<br> <strong>&gt;&gt;&gt; We are meeting
    with friends here before leaving, tomorrow we need to go to the
    hotel by 6 am to spend the day in the best traditions</strong><br>
    <strong>&gt;&gt;&gt; There’s another party in the
    evening</strong><br> <strong>&gt;&gt;&gt; Actually, let’s have a
    call, I’ll tell you</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; Прив, норм, а у тебя?</strong><br>
    <strong>&gt;&gt;&gt; А ты в Бангкоке уже?</strong><br>
    &gt;&gt;&gt; ага, уже тут<br> &gt;&gt;&gt; какие планы на
    сегодня?<br> <strong>&gt;&gt;&gt; Ну я хотел бы поработать над
    этими пионерскими заданиями, если ты не против, то можно вместе
    поделать</strong><br> <strong>&gt;&gt;&gt; Или можешь пока
    сольёшься, я завтра буду допиливать лайвчат</strong><br>
    &gt;&gt;&gt; а чего там по лайвчату?<br> <strong>&gt;&gt;&gt; Да
    бля щас запустил на проде фронт без этого говна, которое генерирует
    эту штуку с лого, и она ругается, говорит, чтобы подключить js от
    google analytics</strong><br> <strong>&gt;&gt;&gt; Я понимаю, что
    это пиздец кринж, но я пошел все равно вспоминать, где его
    достать</strong><br> &gt;&gt;&gt; ага понял<br> &gt;&gt;&gt; что
    на выхах будешь делать?<br> <strong>&gt;&gt;&gt; Встречаемся с
    друзьями тут перед отъездом, завтра надо в отель к 6 утра поехать
    чтобы день провести в лучших традициях</strong><br>
    <strong>&gt;&gt;&gt; Там еще одна тусовка вечером</strong><br>
    <strong>&gt;&gt;&gt; На самом деле давай колл, расскажу</strong></p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 3</strong>
    </summary>
    <p>&gt;&gt;&gt; hey<br> &gt;&gt;&gt; what’s up?<br>
    <strong>&gt;&gt;&gt; Hi, haven’t you called yet?)</strong><br>
    <strong>&gt;&gt;&gt; Hey</strong><br> <strong>&gt;&gt;&gt;
    ok)</strong><br> <strong>&gt;&gt;&gt; I’m going to play Dota
    now</strong><br> <strong>&gt;&gt;&gt; Join if you want
    to)</strong><br> &gt;&gt;&gt; mb a little later)<br>
    &gt;&gt;&gt; what are your plans for weekends?<br>
    <strong>&gt;&gt;&gt; Lot of gyms/flexes/parks</strong><br>
    <strong>&gt;&gt;&gt; + maybe come to your place</strong><br>
    &gt;&gt;&gt; hmm well mb mb)<br> &gt;&gt;&gt; what’s with your job
    btw?<br> <strong>&gt;&gt;&gt; Well, I’m waiting for the
    info</strong><br> &gt;&gt;&gt; did they say what deadlines
    are?<br> <strong>&gt;&gt;&gt; He said that there will be a call
    next week to discuss it</strong><br> &gt;&gt;&gt; remind me what
    this position is<br> <strong>&gt;&gt;&gt; Data
    Scientist</strong><br> <strong>&gt;&gt;&gt; The team is engaged in
    recommending products and other shit</strong><br>
    <strong>&gt;&gt;&gt; Creating model</strong><br>
    <strong>&gt;&gt;&gt; As part of this position, you will have to
    develop skills in machine learning, statistical data analysis,
    programming and much more</strong><br> <strong>&gt;&gt;&gt; It is
    how I understood it</strong></p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив<br> &gt;&gt;&gt; как дела?<br>
    <strong>&gt;&gt;&gt; Прив, звонил еще нет?)</strong><br>
    <strong>&gt;&gt;&gt; Даров</strong><br> <strong>&gt;&gt;&gt;
    Нормально)</strong><br> <strong>&gt;&gt;&gt; Сча в доту
    иду</strong><br> <strong>&gt;&gt;&gt; Если хош
    присоединяйся)</strong><br> &gt;&gt;&gt; мб чуть позже)<br>
    &gt;&gt;&gt; какие планы на выхи?<br> <strong>&gt;&gt;&gt; Дохуя
    залов/флексов/парков</strong><br> <strong>&gt;&gt;&gt; + мона
    залипнуть к тебе</strong><br> &gt;&gt;&gt; хм ну мб мб)<br>
    &gt;&gt;&gt; а чего у тебя с работой в итоге?<br>
    <strong>&gt;&gt;&gt; НУ я жду инфу</strong><br> &gt;&gt;&gt; а
    какие сроки не сказали?<br> <strong>&gt;&gt;&gt; Сказал что на
    следующей неделе созвон где обсуждение будет</strong><br>
    &gt;&gt;&gt; а напомни что за позиция<br> <strong>&gt;&gt;&gt;
    Data Scientist</strong><br> <strong>&gt;&gt;&gt; Команда
    занимается рекомендацией товаров и прочей хуеты</strong><br>
    <strong>&gt;&gt;&gt; Создания моделькеи</strong><br>
    <strong>&gt;&gt;&gt; В рамках этой позиции ты должен будет развивать
    навыки машинного обучения, статистического анализа данных,
    программирования и многое другое</strong><br> <strong>&gt;&gt;&gt;
    Ну я так понял</strong></p>
    </details>
    <p>---</p>
    <p>Conversations have become more interesting and engaging, although
    there’s still a risk of losing context. Russian language performance
    has improved, but errors still occur. I believe that before
    fine-tuning for a specific task with limited data, like mine, it
    would be beneficial to first fine-tune the model unsupervised on a
    large corpus of Russian texts. Additionally, incorporating common
    conversation partners’ names as separate tokens might enhance the
    quality.</p>
    <p>I wouldn’t say it has turned out to be significantly better than
    LoRA. It might be more effective to focus solely on a single person
    and calculate the loss based only on my responses (or someone
    else’s), instead of trying to learn about each and every
    conversational partner.</p>
    <h2 id="closing-thoughts"><a href="#closing-thoughts">Closing
    thoughts</a></h2>
    <p>Certainly, I had to cherry-pick the results, not because most of
    the model’s replies were inadequate, but because many were simple
    responses like “I’ll call you later,” “busy,” and “ok,” which are
    naturally frequent in conversations. Despite this, it’s clear that
    the model excels in mimicking the style of the person it’s
    impersonating. It also captures the commonly discussed topics
    between two people. However, it significantly lacks context in
    conversations. Responding to queries like “yo, so?” or “what are
    your plans for the weekend” is challenging without having full
    context. Perhaps utilizing a system like <a href="https://www.rewind.ai/" target="_blank">Rewind</a>, which
    captures everything the user does across the computer, could be
    beneficial.</p>
    <h2 id="code"><a href="#code">Code</a></h2>
    <p>You can find code for this project as well as instructions on how
    to replicate it yourself on your own Telegram dump in <a href="https://github.com/furiousteabag/doppelganger" target="_blank">my github repo</a>. Training logs may be accessed on
    <a href="https://wandb.ai/furiousteabag/doppelganger" target="_blank">WandB</a>.</p>
    
    <hr>
    <a href="https://asmirnov.xyz/">home</a>
  

</div>]]></description>
        </item>
    </channel>
</rss>