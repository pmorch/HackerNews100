<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 12 Jul 2024 19:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Hackers Steal Phone, SMS Records for Nearly All AT&T Customers (139 pts)]]></title>
            <link>https://krebsonsecurity.com/2024/07/hackers-steal-phone-sms-records-for-nearly-all-att-customers/</link>
            <guid>40948035</guid>
            <pubDate>Fri, 12 Jul 2024 18:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2024/07/hackers-steal-phone-sms-records-for-nearly-all-att-customers/">https://krebsonsecurity.com/2024/07/hackers-steal-phone-sms-records-for-nearly-all-att-customers/</a>, See on <a href="https://news.ycombinator.com/item?id=40948035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p><strong>AT&amp;T Corp.</strong> disclosed today that a new data breach has exposed phone call and text message records for roughly 110 million people — nearly all of its customers. AT&amp;T said it delayed disclosing the incident in response to “national security and public safety concerns,” noting that some of the records included data that could be used to determine where a call was made or text message sent. AT&amp;T also acknowledged the customer records were exposed in a cloud database that was protected only by a username and password (no multi-factor authentication needed).</p>
<p><img decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2023/03/attbldg.png" alt="" width="749" height="497"></p>
<p>In <a href="https://www.sec.gov/Archives/edgar/data/732717/000073271724000046/t-20240506.htm" target="_blank" rel="noopener">a regulatory filing</a> with the <strong>U.S. Securities and Exchange Commission</strong> today, AT&amp;T said cyber intruders accessed an AT&amp;T workspace on a third-party cloud platform in April, downloading files containing customer call and text interactions between May 1 and October 31, 2022, as well as on January 2, 2023.</p>
<p>The company said the stolen data includes records of calls and texts for mobile providers that resell AT&amp;T’s service, but that it does not include the content of calls or texts, Social Security numbers, dates of birth, or any other personally identifiable information.</p>
<p>However, the company said a subset of stolen records included information about the location of cellular communications towers closest to the subscriber, data that could be used to determine the approximate location of the customer device initiating or receiving those text messages or phone calls.</p>
<p>“While the data does not include customer names, there are often ways, using publicly available online tools, to find the name associated with a specific telephone number,” AT&amp;T allowed.</p>
<p>AT&amp;T’s said it learned of the breach on April 19, but delayed disclosing it at the request of federal investigators. The company’s SEC disclosure says at least one individual has been detained by the authorities in connection with the breach.</p>
<p>In a written statement shared with KrebsOnSecurity, the FBI confirmed that it asked AT&amp;T to delay notifying affected customers.</p>
<p>“Shortly after identifying a potential breach to customer data and before making its materiality decision, AT&amp;T contacted the FBI to report the incident,” the FBI statement reads. “In assessing the nature of the breach, all parties discussed a potential delay to public reporting under Item 1.05(c) of the SEC Rule, due to potential risks to national security and/or public safety. AT&amp;T, FBI, and DOJ worked collaboratively through the first and second delay process, all while sharing key threat intelligence to bolster FBI investigative equities and to assist AT&amp;T’s incident response work.”</p>
<p><a href="https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/?guccounter=1" target="_blank" rel="noopener">Techcrunch</a> quoted an AT&amp;T spokesperson saying the customer data was stolen as a result of a still-unfolding data breach involving more than 160 customers of the cloud data provider <strong>Snowflake</strong>.</p>
<p>Earlier this year, malicious hackers figured out that many major companies have uploaded massive amounts of valuable and sensitive customer data to Snowflake servers, all the while protecting those Snowflake accounts with little more than a username and password.<span id="more-68041"></span></p>
<p><a href="https://www.wired.com/story/epam-snowflake-ticketmaster-breach-shinyhunters/" target="_blank" rel="noopener">Wired reported</a> last month how the hackers behind the Snowflake data thefts purchased stolen Snowflake credentials from dark web services that sell access to usernames, passwords and authentication tokens that are siphoned by information-stealing malware. For its part, Snowflake says it now requires all new customers to use multi-factor authentication.</p>
<p>Other companies with millions of customer records stolen from Snowflake servers include <strong>Advance Auto Parts</strong>, <strong>Allstate</strong>, <strong>Anheuser-Busch</strong>, <strong>Los Angeles Unified</strong>, <strong>Mitsubishi</strong>, <strong>Neiman Marcus</strong>, <strong>Progressive</strong>, <strong>Pure Storage</strong>, <strong>Santander Bank</strong>, <strong>State Farm</strong>, and <strong>Ticketmaster</strong>.</p>
<p>Earlier this year, AT&amp;T <a href="https://techcrunch.com/2024/03/30/att-reset-account-passcodes-customer-data/" target="_blank" rel="noopener">reset passwords for millions of customers</a> after the company <a href="https://krebsonsecurity.com/2022/08/it-might-be-our-data-but-its-not-our-breach/" target="_blank" rel="noopener">finally acknowledged a data breach from 2018</a> involving approximately 7.6 million current AT&amp;T account holders and roughly 65.4 million former account holders.</p>
<p><strong>Mark Burnett</strong> is an application security architect, consultant and author. Burnett said the only real use for the data stolen in the most recent AT&amp;T breach is to know who is contacting whom and how many times.</p>
<p>“The most concerning thing to me about this AT&amp;T breach of ALL customer call and text records is that this isn’t one of their main databases; it is metadata on who is contacting who,” Burnett <a href="https://infosec.exchange/@zcutlip@hachyderm.io/112774443764622821" target="_blank" rel="noopener">wrote</a> on Mastodon. “Which makes me wonder what would call logs without timestamps or names have been used for.”</p>
<p>It remains unclear why so many major corporations persist in the belief that it is somehow acceptable to store so much sensitive customer data with so few security protections. For example, Advance Auto Parts said the data exposed included full names, Social Security numbers, drivers licenses and government issued ID numbers on <a href="https://www.bleepingcomputer.com/news/security/advance-auto-parts-data-breach-impacts-23-million-people/" target="_blank" rel="noopener">2.3 million people</a> who were former employees or job applicants.</p>
<p>That may be because, apart from the class-action lawsuits that invariably ensue after these breaches, there is little holding companies accountable for sloppy security practices. AT&amp;T told the SEC it does not believe this incident is likely to materially impact AT&amp;T’s financial condition or results of operations. AT&amp;T reported revenues of more than $30 billion in its most recent quarter.</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel is selling defective 13-14th Gen CPUs (116 pts)]]></title>
            <link>https://alderongames.com/intel-crashes</link>
            <guid>40946644</guid>
            <pubDate>Fri, 12 Jul 2024 15:46:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alderongames.com/intel-crashes">https://alderongames.com/intel-crashes</a>, See on <a href="https://news.ycombinator.com/item?id=40946644">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>My team at Alderon Games, working on the multiplayer dinosaur survival game <a href="https://pathoftitans.com/">Path of Titans</a>, has been encountering significant problems with Intel CPU stability. These issues, including crashes, instability, and memory corruption, are confined to the 13th and 14th generation processors. Despite all released microcode, BIOS, and firmware updates, the problem remains unresolved.</p>
<p>We have identified failures in five main areas:</p>
<ul>
<li>
<strong>End Customers:</strong> Thousands of crashes on Intel CPUs on 13th and 14th Gen CPUs in our crash reporting tools.</li>
<li>
<strong>Official Dedicated Game Servers:</strong> Experiencing constant crashes, taking entire servers down.</li>
<li>
<strong>Development Team:</strong> Developers using these CPUs face frequent instability while building and working on the game. It can also cause SSD and memory corruption.</li>
<li>
<strong>Game Server Providers:</strong> Hosting community servers with persistent crashing issues.</li>
<li>
<strong>Benchmarking Tools:</strong> Decompression and memory tests unrelated to Path of Titans also fail.</li>
</ul>
<p>Over the last 3–4 months, we have observed that CPUs initially working well deteriorate over time, eventually failing. The failure rate we have observed from our own testing is nearly 100%, indicating it's only a matter of time before affected CPUs fail. This issue is gaining attention from news outlets and has been noted by Fortnite and RAD Game Tools, which powers decompression behind Unreal Engine.</p>
<p>Users are also receiving misleading error messages about running out of video driver memory, despite having sufficient memory.</p>
<h2>Actions We Are Taking</h2>
<p>To prevent further harm to our game, we are implementing the following measures:</p>
<ul>
<li>
<strong>Server Migration:</strong> We are swapping all our servers to AMD, which experience 100 times fewer crashes compared to Intel CPUs that were found to be defective.</li>
<li>
<strong>Hosting Recommendations:</strong> We advise anyone hosting Path of Titans servers or selling game servers to avoid purchasing or using 13th and 14th gen Intel CPUs.</li>
<li>
<strong>In-Game Notifications:</strong> We are adding a popup message in-game to inform users with these processors about the issue. Many users are currently unaware of why their game is crashing and what they can do about it.</li>
</ul>
<h2>Resources</h2>
<ul>
<li>
<a href="https://www.epicgames.com/help/en-US/c-Category_Fortnite/c-Fortnite_TechnicalSupport/frequent-crashes-in-fortnite-on-i9-13900k-kf-ks-or-i9-14900k-kf-ks-cpus-a000086852?sessionInvalidated=true">Frequent Crashes in Fortnite on i9-13900K/KF/KS or i9-14900K/KF/KS CPUs</a>
</li>
<li>
<a href="https://www.radgametools.com/oodleintel.htm">RAD Game Tools Intel CPU Issues</a>
</li>
<li>
<a href="https://hardwaretimes.com/pc-gamers-amd-ryzen-intel-13900k-14900k-crash-fail/">PC Gamers are Switching to AMD Ryzen as Intel 13900K/14900K Chips Continue to Crash &amp; Fail</a>
</li>
<li>
<a href="https://www.techspot.com/review/2836-intel-cpu-crash-baseline-spec/">Intel CPUs Are Crashing and It's Intel's Fault: Intel Baseline Profile Benchmark</a>
</li>
</ul>
<p>We look forward to more information becoming available about these problems.</p>
<p>For Intel's sake, we hope they recall these CPUs and refund consumers. This post isn't a endorsement of AMD CPUs or any other PC company. Keep in mind any product can have defects and issues, we just want to let you know where these crashes are coming from and what is going on.</p>
<p>By Matthew Cassells</p>
<p>Founder of Alderon Games</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tau: Open-source PaaS – A self-hosted Vercel / Netlify / Cloudflare alternative (195 pts)]]></title>
            <link>https://github.com/taubyte/tau</link>
            <guid>40946033</guid>
            <pubDate>Fri, 12 Jul 2024 14:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/taubyte/tau">https://github.com/taubyte/tau</a>, See on <a href="https://news.ycombinator.com/item?id=40946033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p><a href="https://discord.gg/KbN3KN7kpQ" rel="nofollow">
    <img src="https://github.com/taubyte/tau/raw/main/images/discord-btn.png" alt="Join our Discord" height="30">
  </a>
  <a href="https://tau.how/" rel="nofollow">
    <img src="https://github.com/taubyte/tau/raw/main/images/docs-btn.png" alt="Read the Docs" height="30">
  </a>
  <a href="https://console.taubyte.com/" rel="nofollow">
    <img src="https://github.com/taubyte/tau/raw/main/images/sandbox-btn.png" alt="Try our Sandbox" height="30">
  </a>
</p>
<br>
<div dir="auto">
  <a href="https://taubyte.com/" rel="nofollow">
    <themed-picture data-catalyst-inline="true"><picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/taubyte/tau/raw/main/images/logo-with-text-tau-white.png">
      <img width="160" src="https://github.com/taubyte/tau/raw/main/images/logo-with-text-tau-black.png" alt="Tau Logo">
    </picture></themed-picture>
  </a>
  
  <p dir="auto"><a href="https://github.com/taubyte/tau/releases"><img src="https://camo.githubusercontent.com/ebb1a5c3e4b55473746c5de17b99da02cab50ea1679c9d7d76e56d4d03904a1f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f746175627974652f7461752e737667" alt="Release" data-canonical-src="https://img.shields.io/github/release/taubyte/tau.svg"></a>
<a href="https://github.com/taubyte/tau/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/9a7bccf3d134b27ddedef3543cff4930fdd5744a7249f17e6550aa9f111ea310/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f746175627974652f746175" alt="License" data-canonical-src="https://img.shields.io/github/license/taubyte/tau"></a></p>
  <p><strong>
  <p dir="auto"><h2 tabindex="-1" dir="auto">Open Source Git-Native CDN PaaS</h2><a id="user-content-open-source-git-native-cdn-paas" aria-label="Permalink: Open Source Git-Native CDN PaaS" href="#open-source-git-native-cdn-paas"></a></p>
  </strong>
</p></div>

<hr>

<p dir="auto">Tau is a framework for building low maintenance &amp; highly scalable cloud computing platforms that software developers will love!</p>
<p dir="auto"><code>tau</code> is a single binary with no external dependencies except standard system libraries. On top of that, it requires minimal configuration. These are the main steps:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Install Tau</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="curl https://get.tau.link/tau | sh"><pre>curl https://get.tau.link/tau <span>|</span> sh</pre></div>
</li>
<li>
<p dir="auto"><strong>Configure</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="tau config generate -n yourdomain.com -s compute --services all --ip your_public_ip"><pre>tau config generate -n yourdomain.com -s compute --services all --ip your_public_ip</pre></div>
</li>
<li>
<p dir="auto"><strong>Launch</strong></p>

</li>
</ol>
<p dir="auto">For a complete step-by-step guide, refer to <a href="https://tau.how/01-getting-started/04-deploy-a-cloud/" rel="nofollow">Deploy tau</a>.</p>
<p dir="auto">Building <code>tau</code> youself is a straightforward <code>go build</code> given you have Go installed.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Background</h2><a id="user-content-background" aria-label="Permalink: Background" href="#background"></a></p>
<p dir="auto">The cost and time required to build software, take it from the development environment to production, and then scale it effectively to meet end-user demand are extremely high.</p>
<p dir="auto">Developer-friendly platforms, like the major cloud computing providers, are expensive, lock users in, and overlook local development and E2E testing.</p>
<p dir="auto">This is really a two-sided problem. Do you save on infrastructure cost, or do you lower development time?</p>
<p dir="auto">If you invest in your own platform, it's a rocky road that impedes the speed of development and generally ends up costing more. We all know the Kubernetes fairy tale does not end well!</p>
<p dir="auto">If you invest in development speed, you're limited by your provider's features and cost.</p>
<p dir="auto">To us, solving this problem means:</p>
<ul dir="auto">
<li>Giving you, or your very small team, the ability to build and maintain a cloud computing platform that will go head-to-head with the ones backed by thousands of engineers.</li>
<li>Setting software developers free from infrastructure and operational constraints. We refer to this as "Local Coding Equals Global Production."</li>
</ul>
<p dir="auto"><code>tau</code> solves for building and maintaining a cloud computing platform, and also provides the foundations for an amazing developer experience.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Minimal Configuration</h2><a id="user-content-minimal-configuration" aria-label="Permalink: Minimal Configuration" href="#minimal-configuration"></a></p>
<p dir="auto">One of the reasons <code>tau</code> requires minimal configuration is because it has built-in auto-discovery.
Just like a self-driving car gathering information through sensors, <code>tau</code> will gather information and try to find the best ways to be reachable, available, etc.</p>
<p dir="auto">That said, some configuration like bootstrap peers is necessary. Unless you're running a single-node cloud, each node will need to know at least one other peer.</p>
<p dir="auto">A Cloud built with <code>tau</code> is very dynamic; at a low level, nodes communicate assets, routes, and services, and they also exchange information about other peers. Enriched by distributed services like <code>seer</code> and <code>gateway</code>, the cloud can load-balance incoming requests to ensure optimal performance and reliability.</p>
<p dir="auto">This behavior is built into cloud resources as well. For example, a protocol we call <code>hoarder</code> ensures object storages and databases are replicated; all you need to do is enable it on a few nodes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Local Coding Equals Global Production</h2><a id="user-content-local-coding-equals-global-production" aria-label="Permalink: Local Coding Equals Global Production" href="#local-coding-equals-global-production"></a></p>
<p dir="auto">In your traditional setup, the platform is a complex set of templates, pipelines, and integrations that ultimately help turn configuration into API calls and code into assets. Because of that complexity, and also the fact that many components need to run inside a very complex environment of their own, it's impossible to satisfy the 'local == production' equation.</p>
<p dir="auto">Granted, there are some solutions that either mock or reroute to dev/prod resources, enabling developers to build or debug locally. However, it's still a 3rd party service you need to integrate and manage.</p>
<p dir="auto">In order to satisfy the equation, we decided to build <code>tau</code> so it simplifies, ports, and/or sandboxes every aspect of the cloud.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Git-Native</h2><a id="user-content-git-native" aria-label="Permalink: Git-Native" href="#git-native"></a></p>
<p dir="auto">Traditionally, you interface with infrastructure through API calls. This is the case for every cloud computing provider alongside orchestration solutions like Kubernetes.</p>
<p dir="auto">A few years back, the concept of GitOps started to make waves, and that was around the time we started building, so we decided to cut the unnecessary garbage between the definition of a cloud resource, which should be stored in Git, and its instantiation.</p>
<p dir="auto">As a result, <code>tau</code> has no API calls to create a serverless function, for example. Instead, it adopts Git as the only way to alter infrastructure.</p>
<p dir="auto">Also, git being core to <code>tau</code> means that nodes in the cloud do tune to a specific branch, by default main or master. Among what it enables is an easy way to set up development environments, for example.</p>
<p dir="auto">A specific use case is local development in which case <a href="https://github.com/taubyte/tau/tree/main/tools/dream">dream-cli</a> nodes can also be tuned to the current branch.</p>
<p dir="auto">In addition to the nodes being on a branch, the application registry, managed by the 'tns' protocol, uses commit ids to version entries, allowing nodes serving the assets to detect new versions, or a roll-back for that matter.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Networking</h2><a id="user-content-networking" aria-label="Permalink: Networking" href="#networking"></a></p>
<p dir="auto">Internally, <code>tau</code>, using <a href="https://github.com/libp2p/go-libp2p">libp2p</a>, builds an overlay peer-to-peer network between the nodes, enabling some pretty cool features like:</p>
<ul dir="auto">
<li>Automatic node and protocol discovery &amp; routing. If, for example, a node is down, changes its IP address/port, or the services it supports, other nodes will update the info automatically.</li>
<li>Transport independent. Nodes can use any combination of TCP/IP, WebSocket, QUIC, and more.</li>
<li>NAT Traversal &amp; Circuit Relay, which allow nodes that are not public to be part of the cloud.</li>
</ul>
<p dir="auto">Unless absolutely required, which is extremely rare, no well-designed software should rely on IP addresses and ports. This is why every <code>tau</code> cloud is identified with an FQDN (i.e., enterprise.starships.ws) so no absolute network reference is used in an application. Under the hood, the Cloud will transparently take care of DNS resolution and HTTP load balancing, eliminating the need to set these up.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Storage</h2><a id="user-content-storage" aria-label="Permalink: Storage" href="#storage"></a></p>
<p dir="auto">In every other cloud computing implementation, storage means a location and generally a path. For example, <code>https://tau.how/assets/logo-w.svg</code> has two main components <code>tau.how</code>, which translates to an IP address and a location, and <code>/assets/logo-w.svg</code>, which is a path relative to the location. This way of addressing, called "location-based addressing," is simply not portable. Why? you might ask. Well, for starters, nothing guarantees the data returned is an SVG logo in this case. The other issue is the <code>tau.how</code> host we connected to might not have it.</p>
<p dir="auto">To solve this issue, <code>tau</code> uses content-addressing, a concept introduced by torrent networks and popularized by <a href="https://github.com/taubyte/tau/blob/main">IPFS</a>.</p>
<p dir="auto">So when you request <code>https://tau.how/assets/logo-w.svg</code>, which is actually hosted by a <code>tau</code> Cloud, the host that handles the request will resolve (<code>host=tau.how, path=/assets/logo-w.svg</code>) to a content address, or CID, then retrieve the content reader and then forward it through an HTTP writer to you.</p>
<p dir="auto">A few cool facts about this approach:</p>
<ul dir="auto">
<li>Content is chunked and then stored in a DAG, which means it's deduplicated.</li>
<li>Content can be downloaded from multiple peers in parallel.</li>
<li>Content can be verified as the CID is its hash.</li>
<li>When content is in demand, the cloud automatically dedicates more peers to its distribution.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Computing</h2><a id="user-content-computing" aria-label="Permalink: Computing" href="#computing"></a></p>
<p dir="auto">As of today, <code>tau</code> supports <a href="https://webassembly.org/" rel="nofollow">WebAssembly</a> for computing. The reason we started with it is that it's highly portable and sandboxed. We support containers for CI/CD but not for computing yet. We're working on a way to implement containers and virtual machines while abiding by our principles of portability and sandboxing.</p>
<p dir="auto">Code, binary, images, along with any attached assets, are stored and retrieved using the same principles described in <a href="#storage">Storage</a>, which considerably reduces provisioning time and brings computing close to data (data gravity) and/or user (edge computing).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">E2E Testing</h2><a id="user-content-e2e-testing" aria-label="Permalink: E2E Testing" href="#e2e-testing"></a></p>
<p dir="auto">If you're looking to create E2E tests for projects hosted on <code>tau</code>, you can use <code>dream</code>, a sub-package within <code>tau</code>. We don't have documentation for it yet, but you can quickly learn from tests like <a href="https://github.com/taubyte/tau/blob/main/services/seer/tests/dns_test.go#L35">services/seer/tests/dns_test.go</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running a Local Cloud</h2><a id="user-content-running-a-local-cloud" aria-label="Permalink: Running a Local Cloud" href="#running-a-local-cloud"></a></p>
<p dir="auto">While you can't practically run <code>tau</code> on your local machine, you can do so using <a href="https://github.com/taubyte/tau/tree/main/tools/dream">dream-cli</a>, which is a CLI wrapper around <code>dream</code>. It creates local cloud environments mirroring production settings. Unlike <code>tau</code>, it offers an API for real-time configuration and testing.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Extending Tau</h2><a id="user-content-extending-tau" aria-label="Permalink: Extending Tau" href="#extending-tau"></a></p>
<p dir="auto"><code>tau</code> can be extended using a plugin system we call <a href="https://github.com/taubyte/tau/tree/main/pkg/vm-orbit">orbit</a>. An open-source example is <a href="https://github.com/ollama-cloud">ollama-cloud</a>, which demonstrates how to add LLM capabilities to your cloud.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">To learn more, check:</p>
<ul dir="auto">
<li><a href="https://taubyte.com/blog/introduction-to-taubyte/" rel="nofollow">Introduction to Taubyte</a></li>
<li><a href="https://taubyte.com/blog/be-competitive-in-few-minutes/" rel="nofollow">Be Competitive in a Few Minutes: Deployment Guide</a></li>
</ul>
<p dir="auto">For comprehensive documentation, visit our <a href="https://tau.how/" rel="nofollow">documentation</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support</h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<p dir="auto">Questions or need assistance? Ping us on <a href="https://discord.com/invite/KbN3KN7kpQ" rel="nofollow">Discord</a>!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Windows NT for Power Macintosh (218 pts)]]></title>
            <link>https://github.com/Wack0/maciNTosh</link>
            <guid>40945076</guid>
            <pubDate>Fri, 12 Jul 2024 12:51:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Wack0/maciNTosh">https://github.com/Wack0/maciNTosh</a>, See on <a href="https://news.ycombinator.com/item?id=40945076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Windows NT for Power Macintosh</h2><a id="user-content-windows-nt-for-power-macintosh" aria-label="Permalink: Windows NT for Power Macintosh" href="#windows-nt-for-power-macintosh"></a></p>
<p dir="auto">This repository currently contains the source code for the ARC firmware and its loader, targeting New World Power Macintosh systems using the <em>Gossamer</em> architecture (that is, MPC106 "Grackle" memory controller and PCI host, and "Heathrow" or "Paddington" super-I/O chip on the PCI bus). That is, the following systems:</p>
<ul dir="auto">
<li>iMac G3 (tray-loading)</li>
<li>Power Macintosh G3 (Blue &amp; White) <em>"Yosemite"</em></li>
<li>Macintosh PowerBook G3 Bronze Keyboard <em>"Lombard"</em></li>
<li>Power Macintosh G4 PCI <em>"Yikes!"</em></li>
</ul>
<p dir="auto">The ARC firmware itself runs at a low enough level that it should be compatible with Old World systems using the same chipset too, but there is currently no loader for these systems; these are the following:</p>
<ul dir="auto">
<li>Power Macintosh G3 (beige)</li>
<li>Macintosh PowerBook G3 Series <em>"Wallstreet"</em>, <em>"PDQ"</em></li>
</ul>
<p dir="auto">There may be issues on your hardware; with real hardware, this has only been tested on a Lombard.</p>
<p dir="auto">NT HAL and drivers have no source present for now.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Drivers present in ARC firmware</h2><a id="user-content-drivers-present-in-arc-firmware" aria-label="Permalink: Drivers present in ARC firmware" href="#drivers-present-in-arc-firmware"></a></p>
<ul dir="auto">
<li>Cuda and PMU (albeit Cuda is untested on real hardware)
<ul dir="auto">
<li>ADB keyboard</li>
</ul>
</li>
<li>Flat 32bpp video framebuffer, set up by the loader. Currently the loader only supports ATI hardware (there may be issues with any ATI hardware with fcode version prior to 1.69, only the ATI Rage Pro LT (as present in Lombard) has been tested)</li>
<li>Mac I/O internal IDE controllers, forked from OpenBIOS (<strong>there are no drivers for PCI IDE controllers!</strong>)</li>
<li>USB OHCI forked from OpenBIOS (<strong>broken, nonworking, and initialisation code commented out</strong>)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Drivers currently done for NT</h2><a id="user-content-drivers-currently-done-for-nt" aria-label="Permalink: Drivers currently done for NT" href="#drivers-currently-done-for-nt"></a></p>
<ul dir="auto">
<li>HAL, including: NT boot time framebuffer, super I/O interrupt controller, Grackle PCI bus support, Cuda and PMU (including low level ADB), serial port for kernel debugging only
<ul dir="auto">
<li>(please note Cuda support is currently untested on real hardware)</li>
</ul>
</li>
<li>Mac I/O internal IDE controller, forked from <code>atapi.sys</code> from NT4 DDK</li>
<li>General HID/storage driver, intended to also contain a USB stack in future but currently only implements ADB keyboard/mouse and ramdisk as floppy drive for installing drivers at text setup time</li>
<li>Flat 32bpp video framebuffer miniport driver</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Software compatibility</h2><a id="user-content-software-compatibility" aria-label="Permalink: Software compatibility" href="#software-compatibility"></a></p>
<p dir="auto">NT4 only, currently. NT 3.51 may become compatible if HAL and drivers get ported to it. NT 3.5 will never be compatible, as it only supports PowerPC 601.
(The additional suspend/hibernation features in NT 3.51 PMZ could be made compatible in theory but in practise would require all of the additional drivers for that to be reimplemented.)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing</h2><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Preliminary</h3><a id="user-content-preliminary" aria-label="Permalink: Preliminary" href="#preliminary"></a></p>
<ul dir="auto">
<li>Grab binaries from the releases page. Burn the image to optical media.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Partitioning Disk</h3><a id="user-content-partitioning-disk" aria-label="Permalink: Partitioning Disk" href="#partitioning-disk"></a></p>
<ul dir="auto">
<li>Boot your PowerMac from the burned optical media. When you get to ARC firmware menu, go to <code>Run firmware setup</code>, then <code>Repartition disk for NT installation</code>.</li>
<li>The disk partitioner will first let you enter partition size of the NT partition (up to the 16383x16x63 CHS limit, minus 32 MB ARC system partition + 1 MB for partition tables / MBR backup / OS 9 drivers / ARC environment variable storage, giving a maximum possible size of 8030 MB), then will drop to a menu allowing the creation of additional Mac partitions.
<ul dir="auto">
<li>After adding a partition to the list, the only way to remove from the list is by cancelling the operation and starting the partitioner again.</li>
</ul>
</li>
<li>After you have created all Mac partitions you want, choose <code>Finish partitioning and install</code>, and confirm the operation.</li>
<li>When finished, the partitioner will ask to <code>Press any key to restart</code>. Do so, and boot your PowerMac from the CD again.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing NT</h3><a id="user-content-installing-nt" aria-label="Permalink: Installing NT" href="#installing-nt"></a></p>
<ul dir="auto">
<li>Eject CD and insert your NT4 CD.</li>
<li>Go to <code>Run a program</code> and enter the path <code>cd:\ppc\setupldr</code> - this may be <code>cd01:</code> or <code>cd02:</code> (...) if you have multiple optical drives present on your system.
<ul dir="auto">
<li>This may error with <code>The file or device does not exist</code>, just go back to <code>Run a program</code> and try again if so.</li>
</ul>
</li>
<li>NT4 setupldr will start.
<ul dir="auto">
<li>You will receive the message <code>Setup could not determine the type of computer you have</code>.</li>
<li>Choose <code>Other</code> (default selected option), just press <code>Enter</code> when asked for hardware support disk.</li>
<li>Pick your system from the list - all are equivalent and will load the Gossamer chipset HAL <code>halgoss</code>.</li>
</ul>
</li>
<li>Next you will receive the message <code>Setup could not determine the type of one or more mass storage drivers installed in your system</code>. Two drivers need to be loaded at this point:
<ul dir="auto">
<li>press <code>S</code> to pick a driver, choose <code>Other</code> from the list, press <code>Enter</code> when asked for hardware support disk</li>
<li>Choose the first driver <code>Mac I/O IDE Controller</code></li>
<li>follow the previous steps again, but this time choose the second driver <code>PowerMac General HID &amp; Storage</code></li>
<li>finally, press Enter to continue</li>
</ul>
</li>
<li>You will receive the message <code>Setup could not determine the type of video adapter installed in the system</code>. Choose <code>Other</code> from the list, press <code>Enter</code> when asked for hardware support disk, and choose the only option <code>Open Firmware Frame Buffer</code>.</li>
<li>NT will boot and text setup will start. Go through the text setup.</li>
<li>Under <code>Setup has determined that your computer contains the following hardware and software components</code>, change <code>Keyboard</code> from <code>Unknown</code> to <code>XT, AT or Enhanced Keyboard (83-104 keys)</code> and <code>Pointing Device</code> from <code>Unknown</code> to <code>No Mouse or Other Pointing Device</code>.</li>
<li>Choose the <code>C:</code> drive from the partition list. If you chose to create an NT partition of size 2GB or less, it must be formatted.</li>
<li>If you chose to create an NT partition of over 2GB in size, <code>chkdsk</code> will find errors and require a reboot. Boot your PowerMac from the ARC firmware CD again and follow the steps to boot the NT4 text setup again.</li>
<li>Proceed through the rest of NT text and graphical setup as normal.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Known issues</h2><a id="user-content-known-issues" aria-label="Permalink: Known issues" href="#known-issues"></a></p>
<ul dir="auto">
<li>If you are looking for a stable operating system, this is not it. Expect bugchecks, expect graphical setup to fail and restart because of bugchecks, etc.
<ul dir="auto">
<li>On a laptop system you may wish to remove the battery. At least on Lombard, the only way to power off the system when it bugchecks is via PMU reset or via total power removal.</li>
</ul>
</li>
<li>Currently the implemented drivers are the bare minimum to run and use NT.</li>
<li>I have observed PMU hard shutdowns on NT boot, fixed only by a PMU reset. No idea what caused this.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dualboot quirks</h2><a id="user-content-dualboot-quirks" aria-label="Permalink: Dualboot quirks" href="#dualboot-quirks"></a></p>
<p dir="auto">If you create additional Mac partitions, please make note of the following:</p>
<ul dir="auto">
<li>The Mac partitions are listed in the partition table as HFS partitions but are not formatted. Use Disk Utility from OS X 10.1 or above to format the partitions. (Erase the <strong>volumes</strong>, not the <strong>drive</strong>!)</li>
<li>The OS X installer, and just booting OS 8/OS 9, will error if a valid MBR is present on the disk at all, which is required for NT. In ARC firmware, go to <code>Run firmware setup</code> then <code>Reboot to OSX install or OS8/OS9</code> if you wish to boot to those listed operating systems.
<ul dir="auto">
<li>Booting back to the ARC firmware will fix the MBR, so be sure to always use this option when unsure.</li>
<li>In particular, formatting the created HFS partitions in OS X 10.2 and 10.3 will not work when a valid MBR is present!</li>
</ul>
</li>
<li>To allow OS 9 to mount the hard disk, boot from an OS 9 CD, run Drive Setup, select the drive and use the <code>Update Driver</code> option from the <code>Functions</code> menu.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building ARC firmware</h2><a id="user-content-building-arc-firmware" aria-label="Permalink: Building ARC firmware" href="#building-arc-firmware"></a></p>
<p dir="auto">You need devkitPPC. Additionally, a <code>libgcc.a</code> compiled for <code>powerpcle</code> must be present in <code>arcgrackle/gccle</code>. If you need to find one, it should be present on any Void Linux mirror, the current filename to search for as of 2024-07-12 is <code>cross-powerpcle-linux-gnu-0.34_1.x86_64.xbps</code> - decompress it by <code>zstdcat cross-powerpcle-linux-gnu-0.34_1.x86_64.xbps -o cross-powerpcle-linux-gnu-0.34_1.x86_64.tar</code>, then pull the file out of the tarball: <code>usr/lib/gcc/powerpcle-linux-gnu/10.2/libgcc.a</code>.</p>
<ul dir="auto">
<li>Ensure <code>DEVKITPPC</code> environment variable is set to your devkitPPC directory, usually <code>/opt/devkitpro/devkitPPC</code></li>
<li>Build the big endian libc: <code>cd baselibc ; make ; cd ..</code></li>
<li>Build the ARC firmware loader: <code>cd arcloader_grackle ; make ; cd ..</code></li>
<li>Build the little endian libc: <code>cd arcgrackle/baselibc ; make ; cd ../..</code></li>
<li>Build the ARC firmware itself: <code>cd arcgrackle ; make ; cd ..</code></li>
</ul>
<p dir="auto">Replace <code>stage1.elf</code> and <code>stage2.elf</code> inside the release image. For recreating the image from a folder dump, use your preferred tool to create a hybrid HFS+ISO image, make sure <code>System</code> folder is blessed and <code>BootX</code> file is of type <code>tbxi</code>.</p>
<p dir="auto">Please note that <code>stage1.elf</code> must not be larger than 16KB and <code>stage2.elf</code> must not be larger than 224KB.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<ul dir="auto">
<li>libc used is <a href="https://github.com/PetteriAimonen/Baselibc">baselibc</a></li>
<li>ELF loader and makefiles adapted from <a href="https://github.com/fail0verflow/hbc">The Homebrew Channel</a></li>
<li>Some lowlevel powerpc stuff, and ARC firmware framebuffer console implementation and font, adapted from <a href="https://github.com/devkitPro/libogc">libogc</a></li>
<li>Some ARC firmware drivers (IDE, USB) adapted from <a href="https://github.com/openbios/openbios">OpenBIOS</a>
<ul dir="auto">
<li>USB drivers in OpenBIOS were themselves adapted from <a href="https://github.com/coreboot/coreboot">coreboot</a></li>
</ul>
</li>
<li>ISO9660 FS implementation inside ARC firmware is <a href="https://github.com/erincandescent/lib9660">lib9660</a> with some modifications.</li>
<li>FAT FS implementation inside ARC firmware is <a href="http://elm-chan.org/fsw/ff/00index_p.html" rel="nofollow">Petit FatFs</a> with some modifications.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AT&T says criminals stole phone records of 'nearly all' customers in data breach (290 pts)]]></title>
            <link>https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/</link>
            <guid>40944505</guid>
            <pubDate>Fri, 12 Jul 2024 11:17:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/">https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/</a>, See on <a href="https://news.ycombinator.com/item?id=40944505">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">U.S. phone giant AT&amp;T confirmed Friday it will begin notifying millions of consumers about a fresh data breach that allowed cybercriminals to steal the phone records of “nearly all” of its customers, a company spokesperson told TechCrunch.</p>

<p>In a statement, AT&amp;T said that the stolen data contains phone numbers of both cellular and landline customers, as well as AT&amp;T records of calls and text messages — such as who contacted who by phone or text — during a six-month period between May 1, 2022 and October 31, 2022.&nbsp;</p>

	
	


<p>AT&amp;T said some of the stolen data includes more recent records from January 2, 2023 for a smaller but unspecified number of customers.</p>

	
	


<p>The stolen data also includes call records of customers with phone service from other cell carriers that rely on AT&amp;T’s network, the company said.&nbsp;</p>

<p>AT&amp;T said the stolen data “does not contain the content of calls or texts,” but does include calling and texting records that an AT&amp;T phone number interacted with during the six-month period, as well as the total count of a customer’s calls and texts, and call durations — information that is often referred to as metadata. The stolen data does not include the time or date of calls or texts, AT&amp;T said.</p>

<p>Some of the stolen records include cell site identification numbers associated with phone calls and text messages, information that can be used to determine the approximate location of where a call was made or text message sent.</p>

<p>In all, the phone giant said it will notify around 110 million AT&amp;T customers of the data breach, company spokesperson Andrea Huguely told TechCrunch.&nbsp;</p>

	
	


	
	


<p>AT&amp;T published <a href="https://www.att.com/DataIncident" target="_blank" rel="noreferrer noopener nofollow">a website with information for customers</a> about the data incident.<strong> </strong>AT&amp;T also disclosed the data breach in <a rel="nofollow" href="https://www.sec.gov/ix?doc=/Archives/edgar/data/0000732717/000073271724000046/t-20240506.htm">a filing with regulators</a> before the market opened on Friday.</p>

<h2 id="h-breach-linked-to-snowflake">Breach linked to Snowflake</h2>

<p>AT&amp;T said it learned of the data breach on April 19, and that it was <a href="https://techcrunch.com/2024/03/30/att-reset-account-passcodes-customer-data/" target="_blank" rel="noreferrer noopener">unrelated to its earlier security incident</a> in March.&nbsp;</p>

<p>AT&amp;T’s Huguely told TechCrunch that the most recent compromise of customer records were stolen from the cloud data giant Snowflake <a href="https://techcrunch.com/2024/06/10/mandiant-hackers-snowflake-stole-significant-volume-data-customers/" target="_blank" rel="noreferrer noopener">during a recent spate of data thefts</a> targeting Snowflake’s customers.</p>

	
	


<p>Snowflake allows its corporate customers, like tech companies and telcos, to analyze huge amounts of customer data in the cloud. It’s not clear for what reason AT&amp;T was storing customer data in Snowflake, and the spokesperson would not say.</p>

<p>AT&amp;T is the latest company in recent weeks to confirm it had data stolen from Snowflake, <a href="https://techcrunch.com/2024/05/31/live-nation-confirms-ticketmaster-was-hacked-says-personal-information-stolen-in-data-breach/" target="_blank" rel="noreferrer noopener">following Ticketmaster</a> and <a href="https://techcrunch.com/2024/06/07/snowflake-ticketmaster-lendingtree-customer-data-breach/" target="_blank" rel="noreferrer noopener">LendingTree subsidiary QuoteWizard</a>, and others.</p>

	
	


<p>Snowflake blamed the data thefts on its customers for not using multi-factor authentication to secure their Snowflake accounts, a security feature that the cloud data giant did not enforce or require its customers to use.&nbsp;</p>

<p>Cybersecurity incident response firm Mandiant, which Snowflake called in to help with notifying customers, later said <a href="https://techcrunch.com/2024/06/10/mandiant-hackers-snowflake-stole-significant-volume-data-customers/" target="_blank" rel="noreferrer noopener">about 165 Snowflake customers had a “significant volume of data” stolen from their customer accounts</a>.&nbsp;</p>

	
	


<p>Mandiant attributed the breach to an as-yet-uncategorized cybercriminal group tracked only as UNC5537. Mandiant’s researchers say the hackers are financially motivated and have members in North America and at least one member in Turkey.&nbsp;</p>

<p>Some of the other corporate victims of the Snowflake account thefts had data subsequently published on known cybercrime forums. For AT&amp;T’s part, the company said that it does not believe that the data is publicly available at this time.</p>

<p>AT&amp;T’s statement said it was working with law enforcement to arrest the cybercriminals involved in the breach. AT&amp;T said that “at least one person has been apprehended.” AT&amp;T’s spokesperson said that the arrested individual was not an AT&amp;T employee, but deferred questions about the alleged criminals to the FBI. </p>

	
	


<p>An FBI spokesperson confirmed to TechCrunch on Friday that that after the phone giant contacted the agency to report the breach, AT&amp;T, the FBI and the Department of Justice agreed to delay notifying the public and customers on two occasions, citing “potential risks to national security and/or public safety.” </p>

	
	


<p>“AT&amp;T, FBI, and DOJ worked collaboratively through the first and second delay process, all while sharing key threat intelligence to bolster FBI investigative equities and to assist AT&amp;T’s incident response work,” the FBI spokesperson said.</p>

<p>The FBI did not comment on the arrest of one of the alleged cybercriminals.</p>

<p>This is <a href="https://techcrunch.com/2024/06/29/2024-in-data-breaches-1-billion-stolen-records-and-rising/" target="_blank" rel="noreferrer noopener">the second security incident AT&amp;T has disclosed this year</a>. AT&amp;T was forced to reset the account passcodes of millions of its customers after a cache of customer account information — including encrypted passcodes for accessing AT&amp;T customer accounts — was published on a cybercrime forum. A security researcher told TechCrunch at the time that the encrypted passcodes could be easily decrypted, prompting AT&amp;T to <a href="https://techcrunch.com/2024/04/10/att-notifies-regulators-after-customer-data-breach/" target="_blank" rel="noreferrer noopener">take precautionary action to protect customer accounts</a>.</p>

<p><strong>Read more on TechCrunch:</strong></p>

	
	


<ul>
<li><a href="https://techcrunch.com/2024/07/11/mspy-spyware-millions-customers-data-breach/" target="_blank" rel="noreferrer noopener">Data breach exposes millions of mSpy spyware customers</a></li>



<li><a href="https://techcrunch.com/2024/07/10/apple-alerts-iphone-users-in-98-countries-to-mercenary-spyware-attacks/" target="_blank" rel="noreferrer noopener">Apple warns iPhone users in 98 countries of spyware attacks</a></li>



<li><a href="https://techcrunch.com/2024/07/09/evolve-bank-says-ransomware-gang-stole-personal-data-on-millions-of-customers/" target="_blank" rel="noreferrer noopener">Evolve Bank says ransomware gang stole personal data on millions of customers</a></li>



<li><a href="https://techcrunch.com/2024/07/05/openai-breach-is-a-reminder-that-ai-companies-are-treasure-troves-for-hackers/" target="_blank" rel="noreferrer noopener">OpenAI breach is a reminder that AI companies are treasure troves for hackers</a></li>
</ul>

<p><em>Updated with comment from the FBI.</em></p>

<figure></figure>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[As an Employee, You Are Disposable (2023) (240 pts)]]></title>
            <link>https://nelson.cloud/as-an-employee-you-are-disposable/</link>
            <guid>40943436</guid>
            <pubDate>Fri, 12 Jul 2024 07:41:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nelson.cloud/as-an-employee-you-are-disposable/">https://nelson.cloud/as-an-employee-you-are-disposable/</a>, See on <a href="https://news.ycombinator.com/item?id=40943436">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>The recent tech layoffs have shown that employees are disposable in the eyes of executives. This isn’t surprising though and I’m definitely not the first person that has written about this. I just want to highlight the current situation.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/tech-layoff-tracker.webp" alt="TrueUp tech layoff tracker"><figcaption><p><em>Source:
<a href="https://www.trueup.io/layoffs?ref=nelson.cloud" target="_blank">TrueUp: Tech Layoff Tracker</a></em></p></figcaption></figure><p>It doesn’t matter if investor expectations are surpassed, layoffs can still take place.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/cnbc-shopify.webp" alt="CNBC Shopfiy headline"><figcaption><p><em>Source:
<a href="https://www.cnbc.com/2023/05/04/shopify-cuts-20percent-of-its-workforce-shares-surge-on-earnings-beat.html?ref=nelson.cloud" target="_blank">CNBC: Shopify cuts 20% of its workforce; shares surge on earnings beat</a></em></p></figcaption></figure><p>It’s somewhat understandable if a company is struggling financially and resorts to layoffs. However, there’s plenty of companies that are profitable and still lay off the people that earned the company those profits.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/microsoft.webp" alt="Polygon Microsoft headline"><figcaption><p><em>Source:
<a href="https://www.polygon.com/23561210/microsoft-layoffs-xbox-bethesda-halo-infinite-343-industries?ref=nelson.cloud" target="_blank">Polygon: Microsoft mass layoffs reportedly impact Bethesda, Halo Infinite teams</a></em></p></figcaption></figure><p>Many companies are not only profitable, but their executives continue to earn huge sums of money amidst layoffs.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/ars-google.webp" alt="Ars Technica headline"><figcaption><p><em>Source:
<a href="https://arstechnica.com/tech-policy/2023/05/googlers-angry-about-ceos-226m-pay-after-cuts-in-perks-and-12000-layoffs/?ref=nelson.cloud" target="_blank">Ars Technica: Googlers angry about CEO’s $226M pay after cuts in perks and 12,000 layoffs</a></em></p></figcaption></figure><p>Aside from layoffs, employees may have their pay frozen even though company revenues are up. That’s what happened at Microsoft. Let’s not forget that Microsoft is a $2.5 trillion dollar company (at the time of this writing).</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/microsoft-pay-freeze.webp" alt="Techradar headline"><figcaption><p><em>Source:
<a href="https://www.techradar.com/pro/microsoft-workers-protest-landmark-year-ceo-memo-following-pay-freeze?ref=nelson.cloud" target="_blank">Techradar: Microsoft workers protest ’landmark year’ CEO memo following pay freeze</a></em></p></figcaption></figure><p>It doesn’t matter how much value you’ve delivered. It doesn’t matter how much impact you’ve had in a company. It doesn’t matter how long you’ve been at a company. You are still disposable.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/jeremy-joslin.webp" alt="Tweet from @jcj"><figcaption><p><em>Source:
<a href="https://twitter.com/jcj/status/1616482322278420481?ref=nelson.cloud" target="_blank">Jeremy Joslin (@jcj) on Twitter</a></em></p></figcaption></figure><p>This article shows the mindset some very wealthy executives have about the average worker/employee.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/tim-gurner.webp" alt="BBC Tim Gurner headline"><figcaption><p><em>Source:
<a href="https://www.bbc.com/news/business-66803279?ref=nelson.cloud" target="_blank">BBC: Tim Gurner apologises over call for more unemployment to fix worker attitudes</a></em></p></figcaption></figure><p>There are some bits I want to highlight:</p><blockquote><p>“There’s been a systematic change where employees feel the employer is extremely lucky to have them,” Mr Gurner said. “We need to remind people they work for the employer, not the other way around.”</p></blockquote><blockquote><p>[Mr Gurner] has previously made headlines by suggesting young people cannot afford homes because they spend too much on avocado toast.</p></blockquote><h2 id="in-conclusion">In Conclusion…</h2><p>It’s okay to like your job and employer. Just understand that, <strong>as an employee, you are disposable</strong>.</p><h2 id="further-reading">Further Reading</h2><p>Here are some articles I’ve come across that share similar sentiments or are very relevant. I highly recommend giving them a read.</p><ul><li><a href="https://www.qword.net/2023/04/30/maybe-you-should-store-passwords-in-plaintext?ref=nelson.cloud" target="_blank">Maybe you should store passwords in plaintext</a></li><li><a href="https://www.mcsweeneys.net/articles/our-company-is-doing-so-well-that-youre-all-fired?ref=nelson.cloud" target="_blank">Our Company Is Doing So Well That You’re All Fired</a></li><li><a href="https://hbr.org/2022/12/what-companies-still-get-wrong-about-layoffs?ref=nelson.cloud" target="_blank">What Companies Still Get Wrong About Layoffs</a></li><li><a href="https://ludic.mataroa.blog/blog/i-accidentally-saved-half-a-million-dollars/?ref=nelson.cloud" target="_blank">I Accidentally Saved Half A Million Dollars</a></li></ul></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GE Aerospace Successfully Develops and Tests New Hypersonic Dual-Mode Ramjet (117 pts)]]></title>
            <link>https://www.geaerospace.com/news/press-releases/ge-aerospace-successfully-develops-and-tests-new-hypersonic-dual-mode-ramjet</link>
            <guid>40943253</guid>
            <pubDate>Fri, 12 Jul 2024 06:54:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.geaerospace.com/news/press-releases/ge-aerospace-successfully-develops-and-tests-new-hypersonic-dual-mode-ramjet">https://www.geaerospace.com/news/press-releases/ge-aerospace-successfully-develops-and-tests-new-hypersonic-dual-mode-ramjet</a>, See on <a href="https://news.ycombinator.com/item?id=40943253">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><ul><li><em>A dual-mode ramjet was developed and tested in less than 11 months</em></li><li><em>Testing demonstrates a threefold increase in airflow compared to previously flight-tested hypersonic technology demonstrators</em></li></ul><p><strong>CINCINNATI – July 10, 2024</strong> – GE Aerospace announced today the successful demonstration of a new, cutting-edge hypersonic dual-mode ramjet. This achievement – which could enable high-speed flight and longer range across numerous multi-mission aircraft – represents the most&nbsp;recent milestone in a diverse portfolio of hypersonic programs.</p><p>The dual-mode ramjet began testing in March of this year in the clean air, continuous flow, high-speed propulsion testing facility in Evendale, OH, just 11 months after the launch of the design effort. The testing delivered promising results, exceeding performance expectations and demonstrating robust operation of a dual-mode ramjet with a threefold (3X) increase in airflow compared to previously flight-tested hypersonic technology demonstrators.</p><p>"The rapid progression from design to testing underscores our commitment to driving innovation in hypersonic technologies,” said Amy Gowder, president and CEO of Defense &amp; Systems at GE Aerospace. “This milestone not only shows the exceptional talent and dedication of our team but also reaffirms our position as a leader in the pursuit of hypersonic flight."</p><p>The successful development and testing of the dual-mode ramjet in such a short period of time was made possible through the collaboration of GE Aerospace’s team of engineers, Innoveering – a company acquired by GE Aerospace in 2022 that specializes in hypersonic propulsion – and GE Aerospace’s Research Center.</p><p>“The technology’s robust performance paves the way for the next phase of development, which will focus on continued testing and technology demonstration in alignment with our roadmap for integrated high-speed propulsion solutions,” said Mark Rettig, vice president &amp; general manager of Edison Works Business &amp; Technology Development at GE Aerospace.</p><p><strong>About&nbsp;GE Aerospace</strong><br>GE Aerospace (NYSE: GE) is a global aerospace propulsion, services, and systems leader with an installed base of approximately 44,000 commercial and 26,000 military aircraft engines. With a global team of 52,000 employees building on more than a century of innovation and learning, GE Aerospace is committed to inventing the future of flight, lifting people up, and bringing them home safely. Learn more about how GE Aerospace and its partners are defining flight for today, tomorrow and the future at&nbsp;<a href="https://www.geaerospace.com/">www.geaerospace.com</a>.&nbsp;</p><p><strong>Media Contact:</strong></p><p>Amanda Mayfield<br><a href="https://www.geaerospace.com/cdn-cgi/l/email-protection#e6a78b87888287c8ab879f808f838a82a68183c885898b"><span data-cfemail="89c8e4e8e7ede8a7c4e8f0efe0ece5edc9eeeca7eae6e4">[email&nbsp;protected]</span></a><br>321-442-1259</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using S3 as a Container Registry (257 pts)]]></title>
            <link>https://ochagavia.nl/blog/using-s3-as-a-container-registry/</link>
            <guid>40942732</guid>
            <pubDate>Fri, 12 Jul 2024 04:26:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ochagavia.nl/blog/using-s3-as-a-container-registry/">https://ochagavia.nl/blog/using-s3-as-a-container-registry/</a>, See on <a href="https://news.ycombinator.com/item?id=40942732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p>For the last four months I’ve been developing a custom container image builder, collaborating with Outerbounds<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. The technical details of the builder itself might be the topic of a future article, but there’s something surprising I wanted to share already: you can use <a href="https://en.wikipedia.org/wiki/Amazon_S3">S3</a> as a container registry! You heard it right. All it takes is to expose an S3 bucket through HTTP and to upload the image’s files to specific paths. With that in place, you can actually <code>docker pull</code> from it. Isn’t that neat?</p>
<p>In the rest of this post I’ll explain how it all works, but let’s start with a demo for the impatient among us. I created a container image that runs <a href="https://en.wikipedia.org/wiki/Cowsay">cowsay</a> and mirrored it to a bucket. Here’s what happens when you pull and run it from the bucket’s url:</p>
<pre tabindex="0"><code>$ docker run --rm pub-40af5d7df1e0402d9a92b982a6599860.r2.dev/cowsay

 _________________________
&lt; This is seriously cool! &gt;
 -------------------------
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||
</code></pre><p>Don’t you agree with the cow? Note that, for this demo, I’m using <a href="https://www.cloudflare.com/developer-platform/r2/">R2</a> instead of S3 (because it has free egress 😎). Fortunately, it doesn’t matter whether you use R2 or S3, since they are API-compatible. As a matter of fact, I used the AWS SDK to push my image to R2.</p>
<h3 id="but-why">But why?</h3>
<p>Using S3 is not the traditional approach for hosting container images. You’d normally use a container registry, such as <a href="https://hub.docker.com/">DockerHub</a>, <a href="https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry">GitHub Container Registry</a>, <a href="https://aws.amazon.com/ecr/">ECR</a>, etc. What benefits does S3 bring, then, to make deviating from the trodden, <a href="https://boringtechnology.club/">boring</a>, path worthwhile?</p>
<p>Let’s take a step back. We are developing a custom image builder (or bakery, as we affectionately call it) because of speed. We want to go from requirements to a ready-to-pull image in a few seconds. The easiest container registry to use in our case is ECR, because we are on AWS. However, it turns out there’s a substantial performance difference between S3 and ECR when it comes to upload speed!</p>
<p>I discovered the performance gap somewhat by accident. Since speed is important for us, and the first rule of performance optimization is to measure, I instrumented the code to generate <a href="https://medium.com/jaegertracing/jaeger-tracing-a-friendly-guide-for-beginners-7b53a4a568ca">traces</a>. Having that, I went hunting for optimization opportunities and came across something unexpected: the traces showed that pushing layers to the container registry accounted for a significant amount of time! That felt off, so I decided to run a small benchmark: to upload a 198 MiB layer to ECR and to S3, and observe the difference in duration.</p>
<p>Here’s the outcome:</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>Minimum observed speed</strong></th>
<th><strong>Maximum observed speed</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ECR</strong></td>
<td>24 MiB/s (8.2 s)</td>
<td>28 MiB/s (7.0 s)</td>
</tr>
<tr>
<td><strong>S3</strong></td>
<td>115 MiB/s (1.7 s)</td>
<td>190 MiB/s (1.0 s)</td>
</tr>
</tbody>
</table>
<p>The table shows that S3 is up to 8x faster than ECR, which is almost an order of magnitude<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>! Of course, there are <a href="#caveats">caveats</a>, but “raw” S3 container registries are nevertheless a promising avenue of optimization.</p>
<h3 id="what-makes-s3-faster-than-ecr">What makes S3 faster than ECR?</h3>
<p>The big difference between pushing to ECR and uploading objects to S3 is that the latter allows uploading a single layer’s chunks in parallel. Given enough bandwidth, this yields a massive increase in throughput. In fact, parallel chunked uploads are recommended in the <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-guidelines.html#optimizing-performance-guidelines-scale">AWS docs</a> to maximize bandwidth usage<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</p>
<p>Why can’t ECR support this kind of parallel uploads? The “problem” is that it implements the <a href="https://github.com/opencontainers/distribution-spec/blob/2291163927cae6f5105a07d32c675c00ff39244c/spec.md">OCI Distribution Spec</a>, which is the standard for container registries (i.e. the reason why you can <code>docker pull</code> and <code>docker push</code> to different registry implementations). According to the specification, a layer push must happen sequentially: even if you upload the layer in chunks, each chunk needs to finish uploading before you can move on to the next one. Needless to say, having a single active connection per layer leaves a significant amount of bandwidth unused!</p>
<p><em>Aside: we also tested the performance of sequential uploads to S3. The result? Throughput went down to ECR-like levels!</em></p>
<h3 id="but-s3-is-not-a-container-registry">But S3 is not a container registry!</h3>
<p>Indeed, S3 is not a container registry in the strict sense of the word. You can’t <code>docker push</code> to it, and the fact that you can <code>docker pull</code> is mostly a happy coincidence. So how does it work?</p>
<p>The answer to our question is revealed by looking at the inner workings of <code>docker pull</code>. Spoiler: it’s HTTP requests all the way down. More specifically, I logged<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> the requests issued by <code>docker pull</code> and saw that they are “just” a bunch of <code>HEAD</code> and <code>GET</code> requests. As an example, see the log of a <code>docker pull my-image:latest</code> at my self-hosted registry (lines starting with <code>#</code> are comments):</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span># Check whether the image's manifest is present in the registry</span>
</span></span><span><span>HEAD /v2/my-image/manifests/latest
</span></span><span><span><span># Download the image's manifest</span>
</span></span><span><span>GET /v2/my-image/manifests/latest
</span></span><span><span><span># Re-download the image's manifest, now addressed using the manifest's hash</span>
</span></span><span><span><span># (I think this is a sanity check by docker)</span>
</span></span><span><span>GET /v2/my-image/manifests/sha256:dabf91b69c191a1a0a1628fd6bdd029c0c4018041c7f052870bb13c5a222ae76
</span></span><span><span><span># Download one of the image's blobs (which happens to be the image's metadata)</span>
</span></span><span><span>GET /v2/my-image/blobs/sha256:a606584aa9aa875552092ec9e1d62cb98d486f51f389609914039aabd9414687
</span></span><span><span><span># Download the remaining image's blob (which happens to be its only layer)</span>
</span></span><span><span>GET /v2/my-image/blobs/sha256:ec99f8b99825a742d50fb3ce173d291378a46ab54b8ef7dd75e5654e2a296e99
</span></span></code></pre></div><p>That’s it! A <code>docker pull</code> is merely downloading files through HTTP! Which means… You can pull containers from <em>any</em> static file server, as long as it has the necessary files at the expected paths and sets the right <code>Content-Type</code> header for each request. Since a S3 bucket is capable of both, a carefully crafted bucket can become a container registry!</p>
<p><em>Aside: if you want to know more about “manifests”, “blobs”, and such, check out my article on <a href="https://ochagavia.nl/blog/crafting-container-images-without-dockerfiles/">Crafting container images without dockerfiles</a> and the <a href="https://github.com/opencontainers/image-spec/blob/036563a4a268d7c08b51a08f05a02a0fe74c7268/spec.md">OCI Image Format Specification</a>.</em></p>
<h3 id="caveats">Caveats</h3>
<p>In case it’s not already clear: this is all very experimental. I’m waiting to do more research before making any serious claims. Will it end up in production? Or will you, my dear reader, send me an email explaining how my approach is utterly flawed?</p>
<p>Note that, while I haven’t made a survey of the container registry offerings out there, it’s obvious they come with features that make them more attractive than dumping files on a bucket. For instance: you can trust the images you upload are actually valid (because the registry uses the standard push method); you can run automated security scans against your layers and receive warnings if there’s anything fishy; you can natively specify who has access to private repositories; etc.</p>
<p>Don’t let these caveats discourage you, though. If it all works as well as I’m hoping, maybe we’ll see a new trend of hosting public container images in Cloudflare’s R2! What would you say to free egress?</p>
<h5 id="ps-what-about-the-whale">PS. What about the whale?</h5>
<p>It’s a pun… <a href="https://www.google.com/search?q=docker+logo&amp;hl=en">go have a look</a> at the docker logo 😉</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Floppy8 – A Tiny Computer, in a Floppy Drive (2023) (159 pts)]]></title>
            <link>https://abe.today/blogs/main/floppy8-a-tiny-computer-in-a-floppy-drive</link>
            <guid>40942141</guid>
            <pubDate>Fri, 12 Jul 2024 01:45:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abe.today/blogs/main/floppy8-a-tiny-computer-in-a-floppy-drive">https://abe.today/blogs/main/floppy8-a-tiny-computer-in-a-floppy-drive</a>, See on <a href="https://news.ycombinator.com/item?id=40942141">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
          <p>This post covers the design and creation of the Floppy8, a microcomputer and cartridge system which fits inside a floppy drive.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_15-07-54_600x600.png?v=1677186494" alt=""></p>
<p>To see it in action, checkout this video!</p>
<p><iframe src="https://www.youtube.com/embed/zvp-eqWCjGU" title="Floppy8 Demo" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="337" height="600" frameborder="0"></iframe></p>
<p>The Floppy8 plays 4K movies and games on custom cartridges, features wireless controllers, status lights, motorized cartridge ejection and more!</p>
<p>Similar to a Famicon or Super Nintendo, the cartridge sticks out, allowing the label to still be partially visible. The front button safely unmounts and ejects the cart when pressed. The RGB indicator LED flashes different colors for waiting, mounting, running and error states.</p>
<p>The wireless controllers are modified off-the-shelf NES clones which have a beige color and replaced d-pads along with extra added weights for that premium feel.</p>
<h2>How (and why) I Made It</h2>
<p><span>Please note: This post contains affiliate links to many of the parts used in this build!</span></p>
<p>If you're like me, you often find yourself browsing Ebay (the online auction site) for weird items in the middle of the night (the time that isn't day) . One evening, while searching for floppy drives, I came across this little gorgeous little guy</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/s-l1600_240x240.jpg?v=1676999242" alt=""><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/s-l16002_240x240.jpg?v=1676999255" alt=""></p>
<div><p>Strangely, the listing identified it as an <a href="https://www.google.com/search?tbm=isch&amp;q=amiga%201010&amp;tbs=imgo:1" target="_blank" rel="noopener noreferrer">Amiga 1010 disk drive</a>. Maybe it is? But it doesn't look like one and I can not find anything that looks similar to it. The mystery intrigued me slightly, but the beautifully simple industrial design intrigued me more. </p><p> I was led to this disk drive that night by a growing fixation on physical media which had been bubbling inside me for the previous months. I began to realize that when you have the ability to watch any film or play any game in seconds... it removes some of the joy.</p><p> Not to mention, physical media which become tarnished or worn over years of love come to tell a story beyond the media contained within. There's a warmth to holding your favorite movie in your hands and that idea had led me to this drive.</p><p> The longer I laid there, basking in the bright white light of the photos of this mysterious Ebay floppy drive, the more I became obsessed. The texture of the front cover, the smooth shiny beige metal outside, the playful and complex rainbow ribbon cable. Before I knew it, I had spent much too much for a disk drive that had to travel much too far to arrive at my door. </p><p> Luckily, I was on a three week work trip while I waited the three weeks of shipping, so the time passed quickly and the question rattled in my brain "what do I do with such a satisfying device?" After all I don't own an Amiga and I don't intend to start, so what's a lad to do.</p><p> I came to the conclusion that the best use of this device was to pull the innards (as carefully as possible) and replace them with a framework of my own design which allowed the drive to be useful and fun for years to come.</p><p> But in 2022 we don't use floppy disks - they are much too impractical, so we'll need a new media, which means we'll need a new drive mechanism, which means we'll need the spirit of adventure because I have no idea how to do that.</p></div>
<h2>Week 0: The Plan</h2>
<p>I had a simple dream for this project...</p>
<ol>
<li>Design a new physical media format that hit a mix of nostalgia and practicality</li>
<li>Mount some computer inside the case to read that media</li>
<li>Design a drive that had a satisfying tactical feel when inserting a cartridge / floppy</li>
<li>Replace the status LED</li>
<li>Replace the mechanical front-eject button with one to eject the new cart type</li>
<li>Leave all original pieces unmodified so they could be put back if needed</li>
<li>Make it actually practical to use and enjoy!</li>
</ol>
<p>On paper a simple task, in practice this project tested my patience and was one of the hardest things I've ever done. As someone who has never done any really practical 3D printing... or electrical engineering... or anything - we had a lot to tackle.</p>
<h2>Week 1: It's real small</h2>
<p>Once the drive finally arrived, I realized how small it really was (which implies that if it <i>is</i> some Amiga 1010 variant that it's probably a late model due to its scale).</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/tiny_600x600.png?v=1677002508" alt=""></p>
<p>The device shipped from Australia and still has a "Bruining Headlam Computers" sticker on it, I assume this is from when it was last serviced many decades ago. This did not help in solving the mystery of what this drive actually was, but I kept the sticker for nostalgia and to keep with my goal of being able to restore the drive to it's original state if needed.</p>
<p>Although it was about 105mm by 43mm and around 5" deep the real concern for me was the height of the floppy disk hole in the front cover. I knew I needed to retain this part so whatever new disk /&nbsp;cartridge format I invented would need to fit within this narrow slow. Luckily, this drive has a slightly taller than average hole, which allows our new cartridges about 4mm of height to play with.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/floppy8_240x240.png?v=1677002638" alt=""> <img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/floppy8-2_240x240.png?v=1677002653" alt="" width="224" height="180"></p>
<p>The other mind-blowing thing was the amazing condition of the device. It looked almost new, with just a few hints of wear. I was as careful as possible to avoid new scratches, dings or other tarnish as I began work on the device.</p>
<p>Sadly over the weeks of working with this device, some of the paint around the front has some very small chips. This saddens me, but I'll be even more careful in the future.</p>
<h2>Week 2: We need Cartridges</h2>
<div><p>Before I could begin laying out any internals, I needed a sense of how the new physical media would work so designing the new media / floppy / cartridge (whatever you'd like to call it) was the top concern in the first few days. I needed to validate that I could manufacture something which was only 4mm tall and held some form of flash media <i>and</i> fit my aesthetic goals. </p><p> Initially, I thought I might retain the visual elements of a 3.5" floppy. I enlisted my brother to help develop our first idea - SD cards attached to custom circuit boards which looked like floppies but extended the SD card wires to an edge-connected pin set.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/floppy-circuit_240x240.png?v=1677002986" alt=""><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/floppy-circuit-2_240x240.png?v=1677003001" alt="" width="233" height="237"></p>
<div><p>Wow! My brother's designs, visually, looked fantastic. They replicted the original disks very closely and they were easy to manufacture. </p><p> The simplicity was great, but I began to realize I needed something which lended itself well to a high-quality glossy label (more like a video game cart than a floppy) and more importantly, it needed to be made of a fairly soft material. I was worried that the sharp edges of a circuit board could damage the front cover of the drive. This concern, of doing this project non-destructively, held for the whole build. With the exception of the back sticker which was cut to reach a screw, this drive was never harmed and could be restored for historic value.</p><p> I investigated a few ideas to encase these circuits, but with only 4mm of space, I could never conquer an idea which fit all the requirements, so we moved to fully 3D printed carts.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/cart-0_480x480.png?v=1677003044" alt=""></p>
<div><p>The initial prototype was compelling. I used the free tool, <a href="https://penpot.app/">Penpot</a> to modify existing art for a game and printed a label on a cart I designed. Initially, I intended to use small USB flash memory sticks embedded in the cart, but I found that the square edges of a USB make it hard to align into a socket (more on this later). They were also slow when benchmarked and just kinda ugly. I would soon pivot to embedding micro SD cards in the 3D printed carts - which works well. </p><p> This was the first tangible element of the device I sometimes called the Floppy-puter or occasionally Floppy8. I designed the carts in the free browser-based, grade-school focused 3D modeling tool <a href="https://tinkercad.com/">Tinkercad</a>. I used this tool for all 3D models for this project. </p><p> While designing the carts, I purchased a small dye-sub printer, called the <a href="https://amzn.to/3kF3Wcn">Polaroid Hi-print</a> and fit the label indent on the cart to the photo size the printer produced. I also bought a <a href="https://amzn.to/3ZVicy1">small tool to round paper corners</a>. Together along with my existing straight cutter, I was able to produce high-quality labels. Here is an overview of the whole cart creation process.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/cart_480x480.gif?v=1677003075" alt="">&nbsp;</p>
<div><p>The cart design was finalized pretty quickly and aside from small fit tweaks the design stayed the same from early on. It was a nice constant to have and a good motivator to hold and look at during frustrating days. </p><p> Eventually, I would make a few carts of games or films I liked. I wanted to make them out of different filament as I loved the mismatched look of different colored 3.5" floppies and wanted to retain that.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/carts_480x480.png?v=1677003603" alt=""></p>
<p>Around this same time, I also began work on the software which would power the computer. I initially began with a Raspbian base (when I thought I would use a Raspberry Pi) and began customizing it to my liking, reducing the desktop UI, changing splash screens, etc.</p>
<p>I knew I would need a fair bit of logic to recognize the insertion of the carts, mount them, run an autostart, then once the program exited or the eject button was pressed it needed to attempt to safely kill the program (if that failed, non-safely kill it) then unmount the drive and trigger a servo to eject the cart. While this was happening it would also be updating the front LED to indicate the device's status.</p>
<p>Originally, I wrote the code as <a href="https://gist.github.com/abeisgoat/b0567184214297cbfe2b2912d132f848" title="a bash script">a bash script</a> and this worked fine until I had many signals coming from different places (e.g. a program exit or eject button press may trigger the eject process). I eventually gave up and wrote the whole software stack in Node and used <a href="https://www.npmjs.com/package/firmata">firmata</a> to talk to the Arduino hardware.&nbsp;</p>
<p>In order to get the LED status to indicate an inserted, but unmounted cart, I used dmesg to read the kernel logs early and reflect insertion before the cart was mounted. I then used fairly straight forward logic to handle cart, LED, and program status.</p>
<p>The final setup requires each cart to have <strong></strong><em>autostart.sh</em> and an <em>autokill.sh</em> along with any binaries / media it needs. Ideally those scripts will boot the program then safely kill it once the eject button is pressed, but it will also force quit if the program hangs.</p>
<p>The software never changed much after this point, just tweaks to make it more reliable.</p>
<p>However, what did change a lot after this point was my approach to getting a Micro SD card reader to fit in the small gap inside the carts where the edge connector of the Micro SD card sticks out.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_16-42-43_480x480.png?v=1677192177" alt=""></p>
<p>After buying and ripping apart a dozen or so SD card readers, I finally found this incredibly tiny USB-C reader which fits perfectly in the gap in the cart. Designing the rest of the drive bay would mean figuring out how to mount this card reader while also aligning the rest of the cart properly so it would smoothly insert when pushed in.</p>
<p>This wasn't <em>that</em> hard but did involve a lot of prototyping as tolerances on 3D printers are not great, so even though something was modeled to fit, that was no guarantee.</p>
<h2>Week 3-6: So many unknowns...</h2>
<p>Once I had a mostly-finished cart design I had to begin prototyping the computer, I knew I had a few big unknowns I had to explore. Some easier than others - but all challenges none-the-less.</p>
<ol>
<li>Is a Raspberry Pi powerful enough to do everything I want to do?</li>
<li>How can I create an eject mechanism for the cartridges?</li>
<li>How can I control the front-facing LED?</li>
<li>Do I need fans?</li>
<li>Will the metal case block wifi?</li>
<li>How will I fit all the wires?</li>
</ol>
<p>Let's start at the beginning...</p>
<h3>Is a Raspberry Pi powerful enough to do everything I want to do?</h3>
<div><p>No - they are not. My initial tests showed the Pi of <i>barely</i> being capable of playing 4K video and definitely not at an acceptable framerate. In my tests, emulator and game performance was terrible for anything but the oldest consoles. This made me question all the Raspberry Pi emulator devices around - either my hardware / software was just terrible or folks aren't really playing those devices much.</p><p> I ended up purchasing a <a href="https://www.lattepanda.com/">Latte Panda 3 Delta</a> - an obscure but really wonderful device. Aside from having a built-in Arduino, it also has headers for USB, power, and more. This made running wires for buttons, ports, lights, etc trivial and despite my constant wiring mistakes the board held up and worked really well.</p><p> In my tests the performance was really reasonable for a small single board computer and in many ways, I don't think the Floppy8 would exist without this device. Sure other single board options exist, but the Latte Panda was really perfect. The only complaint I have is that documentation is minimal, but because the hardware is basically an x86 machine with an embedded Arduino in a tiny package, I did not feel the need for complex docs.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/latte-panda_480x480.png?v=1677003630" alt=""></p>
<p><br> Perhaps the luckiest element of this build was the fact that the Latte Panda just barely fit into the floppy drive. I printed this rose-gold plastic piece with the internal dimensions of the case to see the fit and as you can see, it is snug length-wise, but it sure fits.</p>
<h3>How can I create an eject mechanism for the cartridges?</h3>
<p>I knew early on that I wanted the device to be able to "eject" carts. I experimented with many types of spring-loaded mechanisms, latches, etc but in the end I ended up simply placing a servo with a cog which rotates and pushes the cart out. The actual implementation was simple, but getting power to the servo was tricky. I tried various electircal-engineery ways to get a high amperage 5v line for the servo, but nothing worked. At one point I thought I'd put a battery pack inside the device which would be kept charged and used only for the high-amp spikes for the servo, but somehow in testing I managed to miswire that and burn up the servo. So my final decision was to pull from the 5v line on the LattePanda but put a 0.4amp fuse to protect the board in the case of a jammed servo causing an amperage spike and harming the board. In practice this has worked perfectly fine and despite jams / etc the fuse has never tripped and the board has never shown any issue.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/servo_480x480.png?v=1677003666" alt=""></p>
<h3>How can I control the front-facing LED?</h3>
<p>Controlling an RGB LED is, apparently, trivial.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/led_480x480.png?v=1677003693" alt=""></p>
<p>After soldering up some resistors (for some reason, idk it's what the internet said) you can control the colors straight from a couple PWM pins on an Arduino. This worked quickly and reliably. The terrible mess pictured above was replaced by a nicer heat-shrunk LED later, but the first attempt also worked, it was just ugly.</p>
<h3>Do I need fans?</h3>
<p>Early on, I did some heat stress testing by placing the Latte Panda inside the case and completely blocking the front and back with scrap plastic.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/heat_480x480.png?v=1677003722" alt=""></p>
<div><p>I let the device sit playing 4K video for several hours. The Latte Panda kept a steady 69c internal temp. Warm, but reasonable. The metal case works as a decent radiator and helps expell the warmth into the surrounding air. This was not completely similar to the final build, but it was close enough that I was confident the onboard fan would keep the device cool enough. </p><p> Eventually I printed a heat sensitive cartridge, placed it in the front slot and let the device run for a while.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/heat2_480x480.png?v=1677003743" alt=""></p>
<p>This was not informative, but it was cool to look at.</p>
<h3>Will the metal case block wifi?</h3>
<p>This is a quick one - no. Nice.</p>
<h3>How will I fit all the wires?</h3>
<p>I ended up crimping many custom wires for this project. I invested in some <a href="https://amzn.to/3kA3n3G">nice wire</a> for another project and used it all over this one. I also bought a <a href="https://amzn.to/3Dcs2lh">wire-stripper</a> and a <a href="https://amzn.to/3H1vr7M">crimp kit</a>. The most critical and difficult wire to create was the short USB-C wire which attached the Micro SD reader for the carts to the USB headers on the Latte Panda.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/wire_480x480.png?v=1677003772" alt=""></p>
<p>This required soldering to a small <a href="https://amzn.to/3wrefUm">USB-C female port</a>.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/usbc_240x240.png?v=1677003792" alt=""></p>
<p>Most of the other wires were simple - just short :)</p>
<h2>Week 7-10: 3D Printing!</h2>
<p>As the prototyping of core elements began to subside it became time to begin iterating on the most complex element of the Floppy8, the designs for the internal brackets / drive which would actually hold all the components securely.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/3d-breakout_600x600.png?v=1677003823" alt=""></p>
<div><p>As I mentioned before, I used Tinkercad for all the CAD work. This screenshot shows a small percentage of all the digital artifacts of this build. In total I would design and print 201 STL files for this project. Slowly tweaking and iterating fits, tolerance, feels - not to mention iterating on the overall designs as needs came up. </p><p> I've never 3D printed anything which required multiple parts so figuring out how to make parts which could be screwed together and hold strongly took time. The final internal design uses 11 parts...</p></div>
<ol>
<li>
<b>Button (Dark Grey)</b> - This is used to replace the stock front button with one which has a short throw and flairs at the bottom.</li>
<li>
<b>Button Mount (Orange)</b> - This bracket holds the small circuit board which the front button is mounted to.</li>
<li>
<b>Cover Mount (Red)</b> - The original front cover and the button mount screw to this bracket. It has additional holes to attach it to the cart shifter.</li>
<li>
<b>Cart Shifter (Light Blue)</b> - This L shaped piece has two feet on the backside which the Latte Panda rests on and a slight slope on the front to help shift the cart upward and into the actual bay.</li>
<li>
<b>Drive Bay (Brown)</b> - This holds the actual cart when inserted along with the Micro SD card reader. It also serves as a mount for the Servo bracket (Pink) and LED bracket (Light Purple / Beige).</li>
<li>
<b>Servo Bracket (Pink)</b> - This bracket holds the micro-sized servo and screws to the Drive Bay and Cover Mount to add rigidity.</li>
<li>
<b>LED Bracket (Beige and Light Purple)</b> - This wraps about the LED assembly and holds it against the Cover Mount while being screwed to the Drive Bay.</li>
<li>
<b>Rear Bracket (Yellow)</b> - This bracket has the feet for the rear Latte Panda screw holes and also has a mount point for the power switch in the top left. It also has screw holes for the back cover.</li>
<li>
<b>Back Cover (Green)</b> - This cover / vent serve as the back plate for the device.</li>
<li>
<b>Port Cover (Orange)</b> - This removable port cover has carefully cut holes for the USB-C and HDMI ports on the Latte panda, but because the ports are a bit sunk into the device, the piece pops out in case the cables need to go deeper than the plate allows.</li>
<li>
<b>Metal Rods (Dark Grey)</b> - These pieces are made from 3mm risers which have been screwed together to the proper length, then sealed in heat-shrink tubing. They are included in the model for context.</li>
</ol>
<p>It's worth nothing all the parts are printed without supports except the Cover Mount which has those small corners cut out to allow for the heads of screws to attach to the metal riser rods. As that corner is an unsupported overhand on two sides, it needed supports.</p>
<h2>Week 11: Assembly</h2>
<p>As I printed I would attempt to put pieces together, iterate, print again. I learned to design pieces to be small to save print time and slowly over weeks the device started to emerge.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/near-final-assembly_480x480.png?v=1677003861" alt=""></p>
<p>With some assembly working correctly, I was able to check how the Floppy8 would feel with a cart inserted. I wanted it to resemble a Super Nintendo or a Famicom, with the cart label still visible when inserted.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/fit_480x480.png?v=1677003887" alt=""></p>
<p>One of the last elements to sort out was how to mount the internal switch for the front button (and how to replace the front button with one which matched the plastic of the front cover). I tried various options to recreate the visible element of the button, I tried making a mold and casting it in resin, etc. I eventually just printed it and resigned to sanding it smooth. I also found <a href="https://amzn.to/3EDmlOa">these adorable PCBs</a> on Amazon which are perfect to mount a single button.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_15-49-17_480x480.png?v=1677188971" alt=""></p>
<p>I like this close up because it helps to show how fucked up projects are when you look at them up close. In real life the Floppy8 looks fantastic because it fits in the original shell, but up close the internals have lots of bits which were printed bad or screw holes I had to drill out or whatever. These things aren't as perfect as photos make them look, so don't get down on yourself if your project isn't perfect.</p>
<p>Anyway, after getting these last elements sorted the whole thing felt fantastic to me - it felt real. I kept iterating then one day, like the flip of a switch, I realized we were almost done. I reprinted all the existing parts in black to help reduce unintended internal LED glow and began putting together the final bits.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/assembly_600x600.png?v=1677003908" alt=""></p>
<p>Putting all the pieces together, including a lot of wire crimping, we ended up with a functional but unhoused Floppy8.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/assembled_600x600.png?v=1677003936" alt=""></p>
<p>Testing at this point confirmed that our cartridges were read, the servo ejects them, the LED reflects the status, it all worked. Perhaps amazingly it also all fit in the case.</p>
<h2>Week 12: Final Polish and Controllers</h2>
<p>As the final bits and bobs began to line up I was finally able to do some real testing for heat, cartridge seating, and ~vibe~.</p>
<p>Heat wise, amazingly, even with all the junk in the case the Latte Panda stayed cool. However, there are some air vents directly below the cartridge slot which pump a lot of heat up around the cart. Originally the SD cart reader was mounted in the place with hot glue... you can see where this is going.</p>
<p>Aside from dripping glue down onto the Panda (which luckily missed the fan by a few mil) the glue was also prone to shifting over the course of a long gaming session or film. I redesigned the cartridge slot as two parts, one which aligns the carts and one which firmly holds the SD reader.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_15-44-05_480x480.png?v=1677188782" alt=""></p>
<p>This redesign made the feel of inserting a cart <em>way</em> better.</p>
<p>I also took this opportunity to work on input devices! I picked up this pair of <a href="https://amzn.to/3Kxryur">clone NES controllers</a>. I also purchase the official Nintendo Switch Bluetooth NES controllers, but the after-market clones are actually beige, not grey like the official ones - so I opted to go with those.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-25-46_480x480.png?v=1677266875" alt=""></p>
<p>I made a few small modifications to the controllers. I replaced the cheap looking matte d-pad with the one from the official controllers so it had the imprinted arrows and felt more premium. Then I took a note out of the book of toy-makers I added a bunch of weight to the controller to make it feel more real.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-27-09_480x480.png?v=1677266988" alt=""></p>
<p>I glued in a ton of ball bearings and tbh this makes way more difference than you'd think! The other very small detail was the light bleed from the connection LED.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-26-48_480x480.png?v=1677266981" alt=""></p>
<p>This small detail highlighted the cheapness / thinness of the plastic, so I used a small piece of heat-shrink tubing around the LED to help focus the light.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-26-57_480x480.png?v=1677267126" alt=""></p>
<p>It's another super tiny detail, but it makes everything feel more polished.</p>
<p>I also purchased a replacement <a href="https://amzn.to/3XTj6Ja">Fire TV remote</a> (which is just bluetooth and works great with Linux) along with an after-market beige shell for it.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-34-44_240x240.png?v=1677267398" alt=""></p>
<p>Sadly the shell is shipping from China and has a ~2mo wide delivery window. So it hasn't arrived yet. When it does, I'll look into removing branding from the remote's buttons and replacing the shell so we have a proper remote! I considered buying an actual old beige remote, but they realistically didn't have all the buttons I wanted, but if I find the right one, I may migrate the insides of the Fire TV remote to a whole new home in something truly retro!</p>
<h2>Anyway, we're done...</h2>
<p>Finally after weeks of tweaking, ups and down, frustrations and almost giving up at least once a day for weeks, it was done.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_15-10-03_600x600.png?v=1677186628" alt=""></p>
<p>I was able to sit, watch a film, play a game, etc. I made a lot of mistakes, wasted a lot of filament, but at the end of the day I'm very proud that I kept pushing and didn't give up. I'll probably never use the Floppy8 much, but I'm proud to have it on my desk.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/IMG_4722_600x600.jpg?v=1677189226" alt=""></p>
<p>Thanks for reading and happy hacking!</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AuraFlow v0.1: a open source alternative to Stable Diffusion 3 (139 pts)]]></title>
            <link>https://blog.fal.ai/auraflow/</link>
            <guid>40941853</guid>
            <pubDate>Fri, 12 Jul 2024 00:42:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.fal.ai/auraflow/">https://blog.fal.ai/auraflow/</a>, See on <a href="https://news.ycombinator.com/item?id=40941853">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
        <p>Open-source AI is in jeopardy. As community interest in AI models skyrocketed over the past year, we noticed that development of new open-source foundational models came to a halt. Some even boldly announced that open-source AI is dead. Not so fast!</p><p>We are excited to present you the first release of our AuraFlow model series, the largest yet completely open sourced flow-based generation model that is capable of text-to-image generation. AuraFlow is a reaffirmation of the open-source community's resilience and relentless determination.</p><h2 id="how-do-i-use-it">How do I use it?</h2><p>If you want to try out a few quick prompts, go to <a href="https://fal.ai/models/fal-ai/aura-flow/playground?ref=blog.fal.ai">fal’s model gallery</a> to start playing around.</p><p>If you want to build some cool Comfy workflows with the model, get the latest version of <a href="https://github.com/comfyanonymous/ComfyUI?ref=blog.fal.ai" rel="noreferrer">Comfy</a> and download the model weights from our <a href="https://huggingface.co/fal/AuraFlow?ref=blog.fal.ai" rel="noreferrer">HuggingFace page</a>.</p><p>We would love to give a huge shout out to ComfyUI and the HuggingFace 🤗 diffusers 🧨 teams for supporting AuraFlow natively on Comfy and <code>diffusers</code> on day 0!</p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-1.png" alt="" loading="lazy" width="1257" height="593" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-1.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-1.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-1.png 1257w" sizes="(min-width: 720px) 720px"><figcaption><span>A fine selection of AuraFlow v0.1 generations</span></figcaption></figure><h2 id="how-this-collaboration-happened">How this collaboration happened</h2><p><a href="https://x.com/cloneofsimo?ref=blog.fal.ai" rel="noreferrer">Simo</a> is one of our favorite researchers in the wild world of generative media models. You may know him from the amazing <a href="https://github.com/cloneofsimo/lora?ref=blog.fal.ai" rel="noreferrer">adaptation of the LoRA paper</a> for text-to-image models. Few months ago, Simo wanted to implement MMDiT from scratch, and see if he would be able to reproduce it. His initial attempts at <a href="https://github.com/cloneofsimo/minRF?ref=blog.fal.ai">https://github.com/cloneofsimo/minRF</a> and its initial result <a href="https://huggingface.co/cloneofsimo/lavenderflow-5.6B?ref=blog.fal.ai">Lavenderflow-v0</a> came out to be promising. Soon, he found various aspects that could be optimized to train the model on a larger scale more efficiently.</p><p>Timing couldn’t have worked out better. Right around this time, we were convinced that a SOTA open-sourced model is the way forward for this space to move forward. We wanted to bring serious resources and compute to scale up the model. We were aligned very well, and thus begun the collaboration.</p><p>AuraFlow demonstrates that collaborative, transparent AI development is not only alive but thriving, ready to tackle the challenges and opportunities of tomorrow's AI landscape.</p><h2 id="technical-details">Technical Details</h2><p>Here, we wanted to share some initial technical details that stand out. We are planning on following up with a more detailed report and possibly a paper as well.</p><p><strong>1. MFU as a first-class citizen</strong></p><p><strong>Most layers don’t need MMDiT Blocks</strong>: While MMDiT achieved good performance, we found that removing many layers to just be single DiT block were much more scalable and compute efficient way to train these models. With careful search in the small-scale proxy, we’ve removed most of the MMDiT blocks and replaced them with large DiT Encoder blocks. These improved the model flops utilization at 6.8B scale by 15%. </p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-2.png" alt="" loading="lazy" width="1110" height="710" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-2.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-2.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-2.png 1110w" sizes="(min-width: 720px) 720px"><figcaption><span>Number of Double Layers and Optimal Learning Rate</span></figcaption></figure><p><strong>Improved training with torch.compile:</strong> At fal, we are already big fans of Torch Dynamo + Inductor, and build on top of this tooling (with a custom dynamo backend) to run our inference workloads super fast (and efficiently utilizing the underlying hardware). Since PT2’s torch.compile is able to handle both forward and backwards passes, AuraFlow’s training was further optimized with its primitives on each layers forward method, and further able to improve MFU by extra 10% ~ 15% depending on the stage.</p><p>2.<strong> Unlock zero-shot learning rate transfer</strong>It is clear that we are not Meta, and would like to have very good hyperparameters even without sweeping them. Fortunately, we noticed MMDiT architectures were also zero-shot LR transferred with maximal-update-parameterization was utilized.Compared to SP, muP was clearly the winner in terms of predictability of learning rate at scale.</p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-3.png" alt="" loading="lazy" width="1042" height="668" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-3.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-3.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-3.png 1042w" sizes="(min-width: 720px) 720px"><figcaption><span>Standard Parametrization</span></figcaption></figure><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-4.png" alt="" loading="lazy" width="1196" height="774" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-4.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-4.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-4.png 1196w" sizes="(min-width: 720px) 720px"><figcaption><span>Maximal Update Parametrization</span></figcaption></figure><p><strong>3. Re-captioned, everything.</strong>It is common trick to recaption everything to make sure there are no faulty text conditions in the dataset. We used our in-house captioner &amp; external captioned dataset to train these models, which improves the quality of the instruction-following significantly. We followed the DALL·E 3 approach to the extreme, and we had no captions that were alt-texts.</p><p><strong>4. Wider, shorter, better!</strong>To further investigate the optimal architecture, we were interested into making a fatter model, i.e., making the architecture overall utilize largest matmul divisible by 256. This lead us into searching for optimal aspect ratio under optimal learning rate found by muP.With these findings, we were confident that aspect ratio of 20 ~ 100 is indeed suitable at larger scale, which was similar with findings from <a href="https://arxiv.org/abs/2010.14701?ref=blog.fal.ai">Scaling Laws for Autoregressive Generative Modeling</a>. We ended up using 3072 / 36, which resulted in model size of 6.8B parameters.</p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-6.png" alt="" loading="lazy" width="1945" height="1409" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-6.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-6.png 1000w, https://blog.fal.ai/content/images/size/w1600/2024/07/Untitled-6.png 1600w, https://blog.fal.ai/content/images/2024/07/Untitled-6.png 1945w" sizes="(min-width: 720px) 720px"><figcaption><span>Number of Parameters / Loss</span></figcaption></figure><p>In the end, we did the best of our ability to improve and effectively find the optimal configurations for large scale training. Utilizing the findings from above, we were able to train a text-to-image model from scratch in our largest possible settings for 4 week of compute time, including 256x256, 512x512, 1024x1024 pre-training and aspect ratio fine-tuning. Final model achieves a GenEval score of 0.63~0.67 during pretraining, and similarly 0.64 after 1024x1024 pretraining. But with prompt-enhancement pipeline similar to DALL·E 3, we were able to achieve 0.703!</p><div data-kg-toggle-state="close">
            <div>
                <h4><span>Prompt for prompt-enhancement</span></h4>
                </div>
            <p><span>A caption is a way that a person would describe an image separated by commas when necessary. All in lower case. Expand the input below into a more detailed caption without changing the original relative positions or interactions between objects, colors or any other specific attributes if they are disclosed in the original prompt. Clarify positional information, colors, counts of objects, other visual aspects and features. Make sure to include as much detail as possible. Make sure to describe the spatial relationships seen in the image. You can use words like left/right, above/below, front/behind, far/near/adjacent, inside/outside. Make sure to include object interactions like "a table is in front of the kitchen pot" and "there are baskets on the table". Also describe relative sizes of objects seen in the image. Make sure to include counts of prominent objects in the image, especially when there is humans in the image. When its a photograph, include photographic details like bokeh, large field of view etc but dont just say it to say something, do it only when it makes sense. When its art, include details about the style like minimalist, impressionist, oil painting etc. Include world and period knowledge if it makes sense to, like 1950s chevrolet etc.</span></p>
        </div><h3 id="challenges-of-distributed-training-on-multi-modal-data">Challenges of distributed training on multi-modal data</h3><p>One of the harshest realities of training image models is that, unlike LLMs, the modality of the data itself can be a real pain to deal with. During AuraFlow’s training, we leveraged our expertise from dealing with distributed storage as well as managing a large fleet of thousands of GPUs.</p><p>Some of this expertise was directly transferable from production grade inference/fine-tuning systems, where we were able to use open source projects like <a href="https://github.com/juicedata/juicefs?ref=blog.fal.ai">JuiceFS</a> and some were more novel challenges like how do you stream massive amounts of data in and out of multiple nodes while leveraging local NVME space as a staging ground to not to reduce the MFU.</p><p>Be on the lookout for a detailed post on how we choose our storage mediums, where we trained this model, how we evaluated GPU performance and managed large clusters!</p><h2 id="what-is-next">What is next?</h2><p>We are not done training! This model is an initial release to kickstart some community engagement. We will continue training the model and apply our learnings from this first attempt. We also noticed that smaller models or MoE’s might be more efficient for consumer GPU cards which have a limiter amount of compute power, so follow closely for a mini version of model that is still as powerful yet much much faster to run. In the meantime, we encourage the community to experiment with what we are releasing today.</p><p>Our goal is to make this model a standard backbone that other innovative work can be built on top of. We look forward to community contributions. If you want to train finetunes, IP-Adapters, or quantizations of the current model, we are happy to support you in any way we can. There is already a vibrant community around fal and Aura models in our Discord. We <a href="https://discord.gg/fal-ai?ref=blog.fal.ai">invite</a> you to join if you want to get involved.</p><p>For business inquiries, please email us at <a href="mailto:hello@fal.ai">hello@fal.ai</a> 😄</p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Third Places and Neighborhood Entrepreneurship: Evidence from Starbucks Cafés (103 pts)]]></title>
            <link>https://www.nber.org/papers/w32604</link>
            <guid>40941645</guid>
            <pubDate>Thu, 11 Jul 2024 23:50:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nber.org/papers/w32604">https://www.nber.org/papers/w32604</a>, See on <a href="https://news.ycombinator.com/item?id=40941645">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><span>Working Paper</span> 32604
  </p>

        <p><span>DOI</span> 10.3386/w32604
  </p>

        <p><span>Issue Date</span> <time datetime="2024-06-21T12:00:00Z">June 2024</time>

  </p>

          </div><div>
    <p>
Sociologists have shown that “third places” such as neighborhood cafés help people maintain and use their network ties. Do they help local entrepreneurs, for whom networks are important? We examine whether the introduction of Starbucks cafés into U.S. neighborhoods with no coffee shops increased entrepreneurship. We find that, when compared to census tracts that were scheduled to receive a Starbucks but did not do so, tracts that received a Starbucks saw an increase in the number of startups of 5.0% to 11.8% (or 1.1 to 3.5 firms) per year, over the subsequent 7 years. There was no effect on neighborhoods with prior cafés. A partnership between Starbucks and Magic Johnson focused on underprivileged neighborhoods produced larger effects. Starbucks locations with more square footage and those with a higher number of visits also produced larger effects.
</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Physics-Based Deep Learning Book (287 pts)]]></title>
            <link>https://physicsbaseddeeplearning.org/intro.html</link>
            <guid>40941056</guid>
            <pubDate>Thu, 11 Jul 2024 22:10:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://physicsbaseddeeplearning.org/intro.html">https://physicsbaseddeeplearning.org/intro.html</a>, See on <a href="https://news.ycombinator.com/item?id=40941056">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main-content" role="main">
<h2>Welcome …<a href="#welcome" title="Permalink to this headline">#</a></h2>
<figure id="pbdl-logo-large">
<img alt="_images/logo-xl.jpg" src="https://physicsbaseddeeplearning.org/_images/logo-xl.jpg">
</figure>
<p>Welcome to the <em>Physics-based Deep Learning Book</em> (v0.2) 👋</p>
<p><strong>TL;DR</strong>:
This document contains a practical and comprehensive introduction of everything
related to deep learning in the context of physical simulations.
As much as possible, all topics come with hands-on code examples in the
form of Jupyter notebooks to quickly get started.
Beyond standard <em>supervised</em> learning from data, we’ll look at <em>physical loss</em> constraints,
more tightly coupled learning algorithms with <em>differentiable simulations</em>,
training algorithms tailored to physics problems,
as well as
reinforcement learning and uncertainty modeling.
We live in exciting times: these methods have a huge potential to fundamentally
change what computer simulations can achieve.</p>

<hr>
<section id="coming-up">
<h2>Coming up<a href="#coming-up" title="Permalink to this headline">#</a></h2>
<p>As a <em>sneak preview</em>, the next chapters will show:</p>
<ul>
<li><p>How to train networks to infer a fluid flow around shapes like airfoils, and estimate the uncertainty of the prediction. This gives a <em>surrogate model</em> that replaces a traditional numerical simulation.</p></li>
<li><p>How to use model equations as residuals to train networks that represent solutions, and how to improve upon these residual constraints by using <em>differentiable simulations</em>.</p></li>
<li><p>How to more tightly interact with a full simulator for <em>inverse problems</em>. E.g., we’ll demonstrate how to circumvent the convergence problems of standard reinforcement learning techniques by leveraging simulators in the training loop.</p></li>
<li><p>We’ll also discuss the importance of <em>inversion</em> for the update steps, and how higher-order information can be used to speed up convergence, and obtain more accurate neural networks.</p></li>
</ul>
<p>Throughout this text,
we will introduce different approaches for introducing physical models
into deep learning, i.e., <em>physics-based deep learning</em> (PBDL) approaches.
These algorithmic variants will be introduced in order of increasing
tightness of the integration, and the pros and cons of the different approaches
will be discussed. It’s important to know in which scenarios each of the
different techniques is particularly useful.</p>
<div>
<p>Executable code, right here, right now</p>
<p>We focus on Jupyter notebooks, a key advantage of which is that all code examples
can be executed <em>on the spot</em>, from your browser. You can modify things and
immediately see what happens – give it a try by
<a href="https://colab.research.google.com/github/tum-pbs/pbdl-book/blob/main/intro-teaser.ipynb">[running this teaser example in your browser]</a>.</p>
<p>Plus, Jupyter notebooks are great because they’re a form of <a href="https://en.wikipedia.org/wiki/Literate_programming">literate programming</a>.</p>
</div>
</section>
<section id="comments-and-suggestions">
<h2>Comments and suggestions<a href="#comments-and-suggestions" title="Permalink to this headline">#</a></h2>
<p>This <em>book</em>, where “book” stands for a collection of digital texts and code examples,
is maintained by the
<a href="https://ge.in.tum.de/">Physics-based Simulation Group</a> at <a href="https://www.tum.de/">TUM</a>.
Feel free to contact us if you have any comments, e.g., via <a href="mailto:i15ge%40cs.tum.edu">old fashioned email</a>.
If you find mistakes, please also let us know! We’re aware that this document is far from perfect,
and we’re eager to improve it. Thanks in advance 😀!
Btw., we also maintain a <a href="https://github.com/thunil/Physics-Based-Deep-Learning">link collection</a> with recent research papers.</p>
<figure id="divider-mult">
<a href="https://physicsbaseddeeplearning.org/_images/divider-mult.jpg"><img alt="_images/divider-mult.jpg" src="https://physicsbaseddeeplearning.org/_images/divider-mult.jpg"></a>
<figcaption>
<p><span>Fig. 1 </span><span>Some visual examples of numerically simulated time sequences. In this book, we explain how to realize algorithms that use neural networks alongside numerical solvers.</span><a href="#divider-mult" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="thanks">
<h2>Thanks!<a href="#thanks" title="Permalink to this headline">#</a></h2>
<p>This project would not have been possible without the help of many people who contributed. Thanks to everyone 🙏 Here’s an alphabetical list:</p>
<ul>
<li><p><a href="https://ge.in.tum.de/about/philipp-holl/">Philipp Holl</a></p></li>
<li><p><a href="https://ge.in.tum.de/">Maximilian Mueller</a></p></li>
<li><p><a href="https://ge.in.tum.de/about/patrick-schnell/">Patrick Schnell</a></p></li>
<li><p><a href="https://ge.in.tum.de/">Felix Trost</a></p></li>
<li><p><a href="https://ge.in.tum.de/about/n-thuerey/">Nils Thuerey</a></p></li>
<li><p><a href="https://ge.in.tum.de/about/kiwon/">Kiwon Um</a></p></li>
</ul>
<p>Additional thanks go to
Georg Kohl for the nice divider images (cf. <span id="id1">[<a href="https://physicsbaseddeeplearning.org/references.html#id8" title="Georg Kohl, Kiwon Um, and Nils Thuerey. Learning similarity metrics for numerical simulations. International Conference on Machine Learning, 2020. URL: https://ge.in.tum.de/publications/2020-lsim-kohl/.">KUT20</a>]</span>),
Li-Wei Chen for the airfoil data image,
and to
Chloe Paillard for proofreading parts of the document.</p>
</section>
<section id="citation">
<h2>Citation<a href="#citation" title="Permalink to this headline">#</a></h2>
<p>If you find this book useful, please cite it via:</p>
<div><pre><span></span><span>@book</span><span>{</span><span>thuerey2021pbdl</span><span>,</span>
  <span>title</span><span>=</span><span>{</span><span>Physics</span><span>-</span><span>based</span> <span>Deep</span> <span>Learning</span><span>},</span>
  <span>author</span><span>=</span><span>{</span><span>Nils</span> <span>Thuerey</span> <span>and</span> <span>Philipp</span> <span>Holl</span> <span>and</span> <span>Maximilian</span> <span>Mueller</span> <span>and</span> <span>Patrick</span> <span>Schnell</span> <span>and</span> <span>Felix</span> <span>Trost</span> <span>and</span> <span>Kiwon</span> <span>Um</span><span>},</span>
  <span>url</span><span>=</span><span>{</span><span>https</span><span>:</span><span>//</span><span>physicsbaseddeeplearning</span><span>.</span><span>org</span><span>},</span>
  <span>year</span><span>=</span><span>{</span><span>2021</span><span>},</span>
  <span>publisher</span><span>=</span><span>{</span><span>WWW</span><span>}</span>
<span>}</span>
</pre></div>
</section>









</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beating the L1 cache with value speculation (2021) (129 pts)]]></title>
            <link>https://mazzo.li/posts/value-speculation.html</link>
            <guid>40940241</guid>
            <pubDate>Thu, 11 Jul 2024 20:17:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mazzo.li/posts/value-speculation.html">https://mazzo.li/posts/value-speculation.html</a>, See on <a href="https://news.ycombinator.com/item?id=40940241">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">



<p>If we have a heuristic to guess some value cheaply, we can remove a data dependency in a tight loop using the branch predictor. This allows the CPU to run more instructions in parallel, increasing performance. If this explanation does not make much sense to you, keep reading to learn about some of the magic making your CPU fast!</p>
<hr>
<p><a href="https://twitter.com/pervognsen">Per Vognsen</a>’s twitter feed is full of neat low-level curiosities, usually leveraging CPU features for some performance benefit.</p>
<p><a href="https://twitter.com/pervognsen/status/1412611878140874757">Recently</a> he tweeted about a trick that I had never heard of – value speculation.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> The trick exploits the branch predictor to guess values, enabling more instruction parallelism and therefore removing a bottleneck on the L1 cache. Note that the bottleneck is <em>not</em> due to L1 cache misses, but on L1 cache <em>hits</em> introducing unwanted data dependencies.</p>
<section id="footnotes" role="doc-endnotes">
<ol>
<li id="fn1"><p>Per, in turn, referenced <a href="https://pvk.ca/Blog/2020/07/07/flatter-wait-free-hazard-pointers/">a blog post by Paul Khuong</a> with a real-world example deploying this trick. Paul, in turn, references side-channel attacks.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<p>In this post I explain the machinery involved, including a primer on branch prediction and CPU caches, so that anybody with a passing knowledge of C and how code is executed on CPUs should be able to follow.</p>
<p>The code for the post is available <a href="https://gist.github.com/bitonic/78887f5d3238bab5e31f3c5a41d404b2">here</a>. All the numbers are from a Xeon E5-1650 v3, an Intel Haswell processor with L1 / L2 / L3 cache of 32kB, 256kB, and 15MB respectively. The code was compiled with <code>clang -O3</code>, and not with <code>gcc</code>, for reasons explained <a href="#compiling">later</a>.</p>
<p>Before starting, I’d like to stress that L1 cache <em>hits</em> are almost certainly <em>not</em> the bottleneck of your application! This is just a very neat trick that illuminates some CPU features, not a guide on how to improve the performance of your average piece of C code.</p>
<h2 id="the-setup-summing-linked-lists">The setup – summing linked lists <a href="#the-setup-summing-linked-lists">#</a></h2>
<p>We have a simple linked list data type, and a function summing all the elements of a given linked list:</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>typedef</span> <span>struct</span> Node <span>{</span></span>
<span id="cb1-2">  <span>uint64_t</span> value<span>;</span></span>
<span id="cb1-3">  <span>struct</span> Node <span>*</span>next<span>;</span> <span>// NULL for the last node</span></span>
<span id="cb1-4"><span>}</span> Node<span>;</span></span>
<span id="cb1-5"></span>
<span id="cb1-6"><span>uint64_t</span> sum1<span>(</span>Node<span>*</span> node<span>)</span> <span>{</span></span>
<span id="cb1-7">  <span>uint64_t</span> value <span>=</span> <span>0</span><span>;</span></span>
<span id="cb1-8">  <span>while</span> <span>(</span>node<span>)</span> <span>{</span></span>
<span id="cb1-9">    value <span>+=</span> node<span>-&gt;</span>value<span>;</span></span>
<span id="cb1-10">    node <span>=</span> node<span>-&gt;</span>next<span>;</span></span>
<span id="cb1-11">  <span>}</span></span>
<span id="cb1-12">  <span>return</span> value<span>;</span></span>
<span id="cb1-13"><span>}</span></span></code></pre></div>
<p>So far so good. Our test case works as follows: build a linked list where the nodes live sequentially in contiguous memory, then see how long it takes to sum them all up:</p>
<div id="cb2"><pre><code><span id="cb2-1"><span>// Allocate 5MB of linked list nodes, and link them sequentially, with</span></span>
<span id="cb2-2"><span>// random data in the `value`s.</span></span>
<span id="cb2-3"><span>uint64_t</span> n <span>=</span> <span>312500</span><span>llu</span><span>;</span> <span>// 312500 * sizeof(Node) = 312500 * 16 bytes = 5000000 bytes</span></span>
<span id="cb2-4">Node <span>*</span>nodes <span>=</span> malloc<span>(</span>n <span>*</span> <span>sizeof</span><span>(</span>Node<span>));</span></span>
<span id="cb2-5"><span>for</span> <span>(</span><span>uint64_t</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> n <span>-</span> <span>1</span><span>;</span> i<span>++)</span> <span>{</span></span>
<span id="cb2-6">  nodes<span>[</span>i<span>].</span>value <span>=</span> random_uint64<span>();</span></span>
<span id="cb2-7">  nodes<span>[</span>i<span>].</span>next <span>=</span> <span>&amp;</span>nodes<span>[</span>i<span>+</span><span>1</span><span>];</span></span>
<span id="cb2-8"><span>}</span></span>
<span id="cb2-9">nodes<span>[</span>n<span>-</span><span>1</span><span>].</span>value <span>=</span> random_uint64<span>();</span></span>
<span id="cb2-10">nodes<span>[</span>n<span>-</span><span>1</span><span>].</span>next <span>=</span> NULL<span>;</span></span>
<span id="cb2-11"></span>
<span id="cb2-12"><span>// Now sum.</span></span>
<span id="cb2-13">sum1<span>(&amp;</span>nodes<span>[</span><span>0</span><span>]);</span></span></code></pre></div>
<p>On a server with a relatively old Xeon E5-1650 v3, running <code>sum1</code> with the sample data takes 0.36 milliseconds, which means that we’re processing our linked list at roughly 14GB/s. In the rest of the post will will identify the bottleneck and get around it with value speculation, bringing the throughput for this dataset to 30GB/s.</p>
<p>The impact of the fix varies depending on the size of the dataset. If it is already entirely in the CPU cache, the improvement is much more pronounced, since otherwise we are quickly constrained by how fast we can read data from RAM. This graph shows the performance improvement over differently sized datasets (higher is better):</p>
<div>
<figure>
<img src="https://mazzo.li/assets/images/value-speculation-chart.svg" alt="Chart showing the performance of various versions of sum, including sum1 as described above. Multiple iterations of the same functions are run, to ensure the data is already in the cache if possible.">

</figure>
</div>
<p>The chart shows the performance of <code>sum1</code> together with the performance of two improved functions, <code>sum2</code> and <code>sum3</code>. We go from a throughput of 14GB/s in <code>sum1</code> to more than 45GB/s in <code>sum3</code> if the data fits entirely in the L1 cache (the 16kB dataset), with the performance decreasing slightly for datasets fitting in the L2 and L3 cache (128kB and 5MB datasets). If the dataset does not fit entirely in any CPU cache (~4GB dataset) we go from 10GB/s to 15GB/s, which is as fast as the RAM allows.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<section id="footnotes" role="doc-endnotes">
<ol start="2">
<li id="fn2"><p>See remarks in the <a href="#results">last section</a> for more data on why I think 15GB/s is the limit without resorting to deeper changes.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<h2 id="instruction-parallelism-and-branch-prediction">Instruction parallelism and branch prediction <a href="#instruction-parallelism-and-branch-prediction">#</a></h2>
<div>

<p>Modern CPUs do not process instructions serially, but rather handle many at the same time. They read many instructions at once, break them down in stages, and then try to fill all the computation units they have with as many tasks from as many instructions as possible.<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> For instance, modern Intel processors are designed for a throughput of 4 instructions per clock cycle, and AMD Zen processors for up to 5 or 6.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>However, branches pose a challenge when wanting to execute instructions in parallel. Let’s go back to our function <code>sum1</code>:</p>
</div>
<section id="footnotes" role="doc-endnotes">
<ol start="3">
<li id="fn3"><p>To expand on this topic, you can start reading on
<a href="https://en.wikipedia.org/wiki/Out-of-order_execution">out-of-order execution</a> and
<a href="https://en.wikipedia.org/wiki/Instruction_pipelining">pipelining</a>.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Agner Fog’s <a href="https://www.agner.org/optimize/microarchitecture.pdf">microarchitecture document</a> contains tons of details about the pipeline characteristics for Intel and AMD x86 processors. The numbers on throughput for each architecture are usually in the “Pipeline” section.<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<div id="cb3"><pre><code><span id="cb3-1"><span>uint64_t</span> sum1<span>(</span>Node<span>*</span> node<span>)</span> <span>{</span></span>
<span id="cb3-2">  <span>uint64_t</span> value <span>=</span> <span>0</span><span>;</span></span>
<span id="cb3-3">  <span>while</span> <span>(</span>node<span>)</span> <span>{</span></span>
<span id="cb3-4">    value <span>+=</span> node<span>-&gt;</span>value<span>;</span></span>
<span id="cb3-5">    node <span>=</span> node<span>-&gt;</span>next<span>;</span></span>
<span id="cb3-6">  <span>}</span></span>
<span id="cb3-7">  <span>return</span> value<span>;</span></span>
<span id="cb3-8"><span>}</span></span></code></pre></div>
<p>and its very readable assembly version:</p>
<div id="cb4"><pre><code><span id="cb4-1"><span>; rdi = node and rax = value.</span></span>
<span id="cb4-2"><span>; rax is the return value register (we're returning value)</span></span>
<span id="cb4-3"><span>sum1:</span></span>
<span id="cb4-4">  <span>xor</span>     <span>rax</span><span>,</span> <span>rax</span>                 <span>; value = 0</span></span>
<span id="cb4-5">  <span>test</span>    <span>rdi</span><span>,</span> <span>rdi</span>                 <span>; if node is NULL, exit, otherwise start loop</span></span>
<span id="cb4-6">  <span>je</span>      end</span>
<span id="cb4-7"><span>loop:</span></span>
<span id="cb4-8">  <span>add</span>     <span>rax</span><span>,</span> <span>qword</span> <span>ptr</span> <span>[</span><span>rdi</span><span>]</span>     <span>; value += node-&gt;value</span></span>
<span id="cb4-9">  <span>mov</span>     <span>rdi</span><span>,</span> <span>qword</span> <span>ptr</span> <span>[</span><span>rdi</span> <span>+</span> <span>8</span><span>]</span> <span>; node = node-&gt;next</span></span>
<span id="cb4-10">  <span>test</span>    <span>rdi</span><span>,</span> <span>rdi</span>                 <span>; if node is not NULL, repeat loop,</span></span>
<span id="cb4-11">  <span>jne</span>     loop                     <span>; otherwise exit</span></span>
<span id="cb4-12"><span>end:</span></span>
<span id="cb4-13">  <span>ret</span></span></code></pre></div>
<p>The loop body is made out of 4 instructions, the last of which a jump. Without special measures, every instruction up to the <code>jne</code> must be executed before proceeding to the next instruction, since we need to know if we’ll go to the beginning of the loop or continue. In other words the conditional jump would introduce a barrier in the instruction level parallelism internal to the CPU.</p>
<p>However, executing many instructions at once is so important that dedicated hardware – the <em>branch predictor</em> – is present in all modern CPUs to make an educated guess on which way we’ll go at every conditional jump. The details of how this works are beyond the scope of this blog post, but conceptually your CPU observes your program as it runs and tries to predict which branch will be taken by remembering what happened in the past.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<section id="footnotes" role="doc-endnotes">
<ol start="5">
<li id="fn5"><p>Apart from the ever useful Agner Fog (see Section 3 of the <a href="https://www.agner.org/optimize/microarchitecture.pdf">microarchitecture document</a>), Dan Luu has a <a href="https://danluu.com/branch-prediction/">nice blogpost</a> explaining less dryly various ways of performing branch prediction.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<p>Even without knowing much about the branch prediction, we expect the predictor to do a great job for our test case – we always go back to the beginning of the loop apart from when we stop consuming the list. On Linux, we can verify that this is the case with <code>perf stat</code>:</p>
<pre><code>$ perf stat ./value-speculation-linux
...
         2,507,580      branch-misses             #    0.04% of all branches</code></pre>
<p>The branch predictor gets it right 99.96% of the time. So the CPU can parallelize our instructions with abandon, right? …right?</p>
<h2 id="data-dependencies-tripping-us-up">Data dependencies tripping us up <a href="#data-dependencies-tripping-us-up">#</a></h2>
<p>Let’s focus on the loop body of <code>sum1</code>:</p>
<div id="cb6"><pre><code><span id="cb6-1"><span>; rdi = node and rax = value.</span></span>
<span id="cb6-2"><span>loop:</span></span>
<span id="cb6-3">  <span>add</span>     <span>rax</span><span>,</span> <span>qword</span> <span>ptr</span> <span>[</span><span>rdi</span><span>]</span>     <span>; value += node-&gt;value</span></span>
<span id="cb6-4">  <span>mov</span>     <span>rdi</span><span>,</span> <span>qword</span> <span>ptr</span> <span>[</span><span>rdi</span> <span>+</span> <span>8</span><span>]</span> <span>; node = node-&gt;next</span></span>
<span id="cb6-5">  <span>test</span>    <span>rdi</span><span>,</span> <span>rdi</span>                 <span>; if node is not NULL, repeat loop,</span></span>
<span id="cb6-6">  <span>jne</span>     loop                     <span>; otherwise exit</span></span></code></pre></div>
<p>To increment <code>value</code> (<code>rax</code>), we need to know the value of <code>node</code> (<code>rdi</code>), which depends on the <code>mov</code> in the previous iteration of the loop. The same is true for the <code>mov</code> itself – it is also dependent on the result of the previous <code>mov</code> to operate. So there’s a <em>data dependency</em> between each iteration of the loop: we must have finished reading <code>node-&gt;next</code> (<code>[rdi + 8]</code>) at iteration <span>n</span> before we can start executing the <code>add</code> and <code>mov</code> at iteration <span>n+1</span>.</p>
<p>Moreover, reading the <code>node-&gt;next</code> (<code>[rdi + 8]</code>) is slower than you might think.</p>
<div>
<figure>
<img src="https://mazzo.li/assets/images/lstopo-ram256g-1.svg" alt="Diagram showing the CPU caches for the processor used in this post. Generated with lstopo.">

</figure>
</div>
<div>

<p>Modern CPUs are a lot better at adding numbers than reading from memory. For this reason, a series of fast caches exist between the CPU and main memory. All reading and writing from main memory normally goes through the cache – if the data we are interested in is not already present, the CPU will load a chunk of memory (a “cache line”, 64 bytes on x86) which contains our desired data into the cache.<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> The fastest cache is usually called L1 (successive caching layers being predictably called L2, L3, …).</p>
<p>Our setup is the best-case scenario when it comes to CPU caches – we read a bunch of memory sequentially, utilizing every byte along the way. However, even if the L1 cache is very fast, it is not free: it takes around 4 CPU cycles to read from it. This will make our <code>mov</code> and <code>add</code> take at least 4 cycles to complete. The other two instructions, <code>je</code> and <code>test</code>, will take only one cycle.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>So the number of cycles needed to go through a single loop iteration is bounded by the 4 cycles it takes to read from L1 cache. The data I get from the Xeon I tested the program with is roughly consistent with this:</p>
</div>
<section id="footnotes" role="doc-endnotes">
<ol start="6">
<li id="fn6"><p>I say “normally” because the cache can be avoided using streaming SIMD instructions, which can write or copy memory bypassing the cache. However these methods are opt-in, and by default all memory goes through the cache.<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Again, Agner Fog’s <a href="https://www.agner.org/optimize/">page on performance</a> is the best resource I could find to source these numbers. For example, if one wanted to find these numbers for a Haswell CPU:</p>
<ul>
<li>The L1 latency (4 cycles) is in section 10.11 of the <a href="https://www.agner.org/optimize/microarchitecture.pdf">microarchitecture guide</a>;</li>
<li>The numbers of cycles it takes to execute <code>mov</code>, <code>add</code>, <code>test</code>, and <code>jne</code> are in the Haswell section of the <a href="https://www.agner.org/optimize/instruction_tables.pdf">instruction tables</a>.</li>
</ul>
<a href="#fnref7" role="doc-backlink">↩︎</a></li>
</ol>
</section>
<pre><code>16kB, 10000 iterations
  sum1:  8465052097154389858,  1.12us,  14.25GB/s,  3.91 cycles/elem,  1.03 instrs/cycle,  3.48GHz,  4.01 instrs/elem
128kB, 10000 iterations
  sum1:  6947699366156898439,  9.06us,  14.13GB/s,  3.95 cycles/elem,  1.01 instrs/cycle,  3.49GHz,  4.00 instrs/elem
5000kB, 100 iterations
  sum1:  2134986631019855758,  0.36ms,  14.07GB/s,  3.96 cycles/elem,  1.01 instrs/cycle,  3.48GHz,  4.00 instrs/elem
4294MB, 1 iterations
  sum1: 15446485409674718527,  0.43 s,   9.94GB/s,  5.60 cycles/elem,  0.71 instrs/cycle,  3.48GHz,  4.00 instrs/elem</code></pre>
<p>The important numbers are <code>cycles/elem</code> and <code>instrs/cycle</code>. We spend roughly 4 cycles per list element (that is to say, per loop iteration), corresponding to a throughput of roughly 1 instruction per cycle. Given that the CPU in question is designed for a throughput of 4 instructions per cycle, we’re wasting a lot of the CPU magic at our disposal, because we’re stuck waiting on the L1 cache.</p>
<h2 id="value-speculation-bailing-us-out">Value speculation bailing us out <a href="#value-speculation-bailing-us-out">#</a></h2>
<p>We finally get to the trick. As discussed, we are stuck waiting on reading what the next node address is. However, in our setup we allocate the list in a contiguous block of memory, and therefore the nodes are always next to each other.</p>
<p>So here’s the key idea: try to guess the next node by just bumping the previous value. If the guess is wrong, set the node to the “real” next value. In C, this is how it would look like:</p>
<div id="cb8"><pre><code><span id="cb8-1"><span>uint64_t</span> faster_sum<span>(</span>Node<span>*</span> node<span>)</span> <span>{</span></span>
<span id="cb8-2">  <span>uint64_t</span> value <span>=</span> <span>0</span><span>;</span></span>
<span id="cb8-3">  Node<span>*</span> next <span>=</span> NULL<span>;</span></span>
<span id="cb8-4">  <span>while</span> <span>(</span>node<span>)</span> <span>{</span></span>
<span id="cb8-5">    value <span>+=</span> node<span>-&gt;</span>value<span>;</span></span>
<span id="cb8-6">    next <span>=</span> node<span>-&gt;</span>next<span>;</span></span>
<span id="cb8-7">    <span>// Guess the next value</span></span>
<span id="cb8-8">    node<span>++;</span></span>
<span id="cb8-9">    <span>// But fix it up if we guessed wrong (in case the nodes are not</span></span>
<span id="cb8-10">    <span>// next to each other).</span></span>
<span id="cb8-11">    <span>if</span> <span>(</span>node <span>!=</span> next<span>)</span> <span>{</span></span>
<span id="cb8-12">      node <span>=</span> next<span>;</span></span>
<span id="cb8-13">    <span>}</span></span>
<span id="cb8-14">  <span>}</span></span>
<span id="cb8-15">  <span>return</span> value<span>;</span></span>
<span id="cb8-16"><span>}</span></span></code></pre></div>
<p>This looks quite bizarre. We are still reading <code>node-&gt;next</code> in the comparison <code>node != next</code> to make sure our guess is right. So at first glance this might not seem like an improvement.</p>
<p>This is where the branch predictor comes in. In the case of lists where most nodes <em>are</em> next to each other (as is the case in our test code), the branch predictor will guess that the <code>if (node != next) { ... }</code> branch is not taken, and therefore we’ll go through loop iterations without having to wait for the L1 read.</p>
<p>Note that when the branch predictor <em>is</em> wrong (for example when the list ends, or if we have non-contiguous nodes) the CPU will need to backtrack and re-run from the failed branch prediction, which is costly (15 to 20 cycles on our processor<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a>). However, if the list is mostly contiguous, the trick works and makes our function 50-200% faster.</p>
<section id="footnotes" role="doc-endnotes">
<ol start="8">
<li id="fn8"><p>See “Misprediction penalty” for Haswell processor in Agner Fog’s
<a href="https://www.agner.org/optimize/microarchitecture.pdf">microarchitecture document</a>.<a href="#fnref8" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<p>However there is one last challenge remaining to reach the final code and show you numbers – convincing compilers that our code is worth compiling.</p>
<h2 id="compiling">Getting compilers to emit the right code <a href="#compiling">#</a></h2>
<p>Let’s go back to the code we showed for value speculation in C:</p>
<div id="cb9"><pre><code><span id="cb9-1"><span>uint64_t</span> faster_sum<span>(</span>Node<span>*</span> node<span>)</span> <span>{</span></span>
<span id="cb9-2">  <span>uint64_t</span> value <span>=</span> <span>0</span><span>;</span></span>
<span id="cb9-3">  Node<span>*</span> next <span>=</span> NULL<span>;</span></span>
<span id="cb9-4">  <span>while</span> <span>(</span>node<span>)</span> <span>{</span></span>
<span id="cb9-5">    value <span>+=</span> node<span>-&gt;</span>value<span>;</span></span>
<span id="cb9-6">    next <span>=</span> node<span>-&gt;</span>next<span>;</span></span>
<span id="cb9-7">    node<span>++;</span></span>
<span id="cb9-8">    <span>if</span> <span>(</span>node <span>!=</span> next<span>)</span> <span>{</span></span>
<span id="cb9-9">      node <span>=</span> next<span>;</span></span>
<span id="cb9-10">    <span>}</span></span>
<span id="cb9-11">  <span>}</span></span>
<span id="cb9-12">  <span>return</span> value<span>;</span></span>
<span id="cb9-13"><span>}</span></span></code></pre></div>
<p>Both <code>gcc</code> and <code>clang</code> easily deduce that the guessing is semantically pointless, and compile our trick away, making the compiled version of <code>faster_sum</code> the same as <code>sum1</code>. This is an instance where the compiler smartness undoes human knowledge about the underlying platform we’re compiling for.</p>
<p>Per Vognsen’s gist uses the following trick to get compilers to behave – this is the first improvement to our <code>sum1</code>, <code>sum2</code>:</p>
<div id="cb10"><pre><code><span id="cb10-1"><span>static</span> <span>uint64_t</span> sum2<span>(</span>Node <span>*</span>node<span>)</span> <span>{</span></span>
<span id="cb10-2">  <span>uint64_t</span> value <span>=</span> <span>0</span><span>;</span></span>
<span id="cb10-3">  <span>while</span> <span>(</span>node<span>)</span> <span>{</span></span>
<span id="cb10-4">    value <span>+=</span> node<span>-&gt;</span>value<span>;</span></span>
<span id="cb10-5">    Node <span>*</span>predicted_next <span>=</span> node <span>+</span> <span>1</span><span>;</span></span>
<span id="cb10-6">    Node <span>*</span>next <span>=</span> node<span>-&gt;</span>next<span>;</span></span>
<span id="cb10-7">    <span>if</span> <span>(</span>next <span>==</span> predicted_next<span>)</span> <span>{</span></span>
<span id="cb10-8">      <span>// Prevent compilers optimizing this apparently meaningless branch away</span></span>
<span id="cb10-9">      <span>// by making them think we're changing predicted_next here.</span></span>
<span id="cb10-10">      <span>//</span></span>
<span id="cb10-11">      <span>// This trick, however, does not work with GCC, only with clang. GCC here</span></span>
<span id="cb10-12">      <span>// derives that `next` and `predicted_next` are the same, and therefore</span></span>
<span id="cb10-13">      <span>// merges them into the same variable, which re-introduces the data</span></span>
<span id="cb10-14">      <span>// dependency we wanted to get rid of.</span></span>
<span id="cb10-15">      asm<span>(</span><span>""</span> <span>:</span> <span>"+r"</span><span>(</span>predicted_next<span>));</span></span>
<span id="cb10-16">      node <span>=</span> predicted_next<span>;</span></span>
<span id="cb10-17">    <span>}</span> <span>else</span> <span>{</span></span>
<span id="cb10-18">      node <span>=</span> next<span>;</span></span>
<span id="cb10-19">    <span>}</span></span>
<span id="cb10-20">  <span>}</span></span>
<span id="cb10-21">  <span>return</span> value<span>;</span></span>
<span id="cb10-22"><span>}</span></span></code></pre></div>
<p>However <code>gcc</code> still doesn’t fully fall for it, as explained in the comment.<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a> Moreover, <code>clang</code>’s generated loop is not as tight as it could, taking 10 instructions per element. So I resorted to manually writing out a better loop, which we’ll call <code>sum3</code>:<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<section id="footnotes" role="doc-endnotes">
<ol start="9">
<li id="fn9"><p>This is why I stuck to <code>clang</code> for this post. I don’t know what compiler Per is using for his tests.<a href="#fnref9" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Here I show the assembly version in Intel syntax, but <a href="https://gist.github.com/bitonic/78887f5d3238bab5e31f3c5a41d404b2#file-value-speculation-linux-c-L121">in the code</a> I write inline assembly, using AT&amp;T syntax since it is better supported.<a href="#fnref10" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<div id="cb11"><pre><code><span id="cb11-1"><span>; rax = value, rcx = next, rdi = node</span></span>
<span id="cb11-2"><span>; Note that rax is the return value register (we are returning the value)</span></span>
<span id="cb11-3"><span>sum3:</span></span>
<span id="cb11-4">  <span>xor</span>     <span>rax</span><span>,</span> <span>rax</span>                   <span>; value = 0</span></span>
<span id="cb11-5">  <span>xor</span>     <span>rcx</span><span>,</span> <span>rcx</span>                   <span>; next = NULL</span></span>
<span id="cb11-6">  <span>test</span>    <span>rdi</span><span>,</span> <span>rdi</span>                   <span>; if node is null, go to the end,</span></span>
<span id="cb11-7">  <span>je</span>      end                        <span>; otherwise start loop</span></span>
<span id="cb11-8"><span>loop_body:</span></span>
<span id="cb11-9">  <span>add</span>     <span>rax</span><span>,</span> <span>qword</span> <span>ptr</span> <span>[</span><span>rdi</span><span>]</span>       <span>; value += node-&gt;value</span></span>
<span id="cb11-10">  <span>mov</span>     <span>rcx</span><span>,</span> <span>qword</span> <span>ptr</span> <span>[</span><span>rdi</span> <span>+</span> <span>8</span><span>]</span>   <span>; next = node-&gt;next</span></span>
<span id="cb11-11">  <span>add</span>     <span>rdi</span><span>,</span> <span>16</span>                    <span>; node++</span></span>
<span id="cb11-12">  <span>cmp</span>     <span>rcx</span><span>,</span> <span>rdi</span>                   <span>; if node is equal to next,</span></span>
<span id="cb11-13">  <span>je</span>      loop_body                  <span>; restart loop, otherwise fix up node</span></span>
<span id="cb11-14">  <span>mov</span>     <span>rdi</span><span>,</span> <span>rcx</span>                   <span>; node = next</span></span>
<span id="cb11-15">  <span>test</span>    <span>rdi</span><span>,</span> <span>rdi</span>                   <span>; if node is not NULL restart the loop,</span></span>
<span id="cb11-16">  <span>jne</span>     loop_body                  <span>; otherwise exit.</span></span>
<span id="cb11-17"><span>end:</span></span>
<span id="cb11-18">  <span>ret</span></span></code></pre></div>
<p>The code relies on the fact that <code>node</code> can’t be <code>NULL</code> after we increment it if it is equal to <code>next</code>, avoiding an additional test, and taking only 5 instructions per element (from <code>loop_body</code> to <code>je loop_body</code> in the happy path).<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<section id="footnotes" role="doc-endnotes">
<ol start="11">
<li id="fn11"><p>The original version of <code>sum3</code> took 6 instructions per cycle, until <a href="https://twitter.com/RhialtoTheM/status/1418926515526459394">Rihalto pointed out</a> a needless jump.<a href="#fnref11" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<h2 id="results">Results <a href="#results">#</a></h2>
<p>These are the final numbers for our four functions:</p>
<pre><code>16kB, 10000 iterations
  sum1:  8465052097154389858,  1.12us,  14.25GB/s,  3.91 cycles/elem,  1.03 instrs/cycle,  3.48GHz,  4.01 instrs/elem
  sum2:  8465052097154389858,  0.57us,  27.97GB/s,  1.99 cycles/elem,  5.02 instrs/cycle,  3.48GHz, 10.01 instrs/elem
  sum3:  8465052097154389858,  0.36us,  44.96GB/s,  1.24 cycles/elem,  4.05 instrs/cycle,  3.48GHz,  5.01 instrs/elem
128kB, 10000 iterations
  sum1:  6947699366156898439,  9.05us,  14.14GB/s,  3.95 cycles/elem,  1.01 instrs/cycle,  3.49GHz,  4.00 instrs/elem
  sum2:  6947699366156898439,  4.51us,  28.38GB/s,  1.97 cycles/elem,  5.09 instrs/cycle,  3.49GHz, 10.00 instrs/elem
  sum3:  6947699366156898439,  3.79us,  33.80GB/s,  1.65 cycles/elem,  3.03 instrs/cycle,  3.49GHz,  5.00 instrs/elem
5000kB, 100 iterations
  sum1:  2134986631019855758,  0.35ms,  14.09GB/s,  3.95 cycles/elem,  1.01 instrs/cycle,  3.48GHz,  4.00 instrs/elem
  sum2:  2134986631019855758,  0.19ms,  26.27GB/s,  2.12 cycles/elem,  4.72 instrs/cycle,  3.48GHz, 10.00 instrs/elem
  sum3:  2134986631019855758,  0.17ms,  28.93GB/s,  1.93 cycles/elem,  2.60 instrs/cycle,  3.48GHz,  5.00 instrs/elem
4294MB, 1 iterations
  sum1: 15446485409674718527,  0.44 s,   9.66GB/s,  5.76 cycles/elem,  0.69 instrs/cycle,  3.48GHz,  4.00 instrs/elem
  sum2: 15446485409674718527,  0.33 s,  13.19GB/s,  4.22 cycles/elem,  2.37 instrs/cycle,  3.48GHz, 10.00 instrs/elem
  sum3: 15446485409674718527,  0.30 s,  14.20GB/s,  3.91 cycles/elem,  1.28 instrs/cycle,  3.47GHz,  5.00 instrs/elem</code></pre>
<p><img src="https://mazzo.li/assets/images/value-speculation-chart.svg"></p>
<p>The numbers are provided <a href="https://gist.github.com/bitonic/78887f5d3238bab5e31f3c5a41d404b2#file-value-speculation-linux-c-L262">by the Linux <code>perf_event_open</code> syscall</a>.</p>
<p>The first three datasets are meant to fit in the L1 / L2 / L3 cache. In those cases, the improvements are very pronounced, and <code>sum3</code> is crunching the data at around 4 instructions per cycle, which should be close to the limit on the processor I tested the code on. When the data does not fit in the cache, the bottleneck becomes filling it, and we process the data at roughly 15 GB/s.</p>
<p>I believe that this is as fast as one can go with “simple” single-threaded reading from RAM,
and it’s consistent with data from <code>sysbench</code>:</p>
<pre><code>$ sysbench memory --memory-block-size=1G --memory-oper=read --threads=1 run
...
102400.00 MiB transferred (15089.75 MiB/sec)
...</code></pre>
<p>The RAM-reading speed could probably be improved using SIMD streaming instructions or by reading from multiple threads, although the implementation would be significantly more complicated.</p>
<p>And so we complete our journey into this low-level trick! If you want more of this, I can’t reccomend <a href="https://twitter.com/pervognsen">Per’s account</a> enough – figuring out how his tricks works has been very educational.</p>
<p>Thanks to <a href="https://scvalex.net/">Alexandru Scvortov</a>, <a href="https://nh2.me/">Niklas Hambüchen</a>, Alex Appetiti, and <a href="https://twitter.com/cartazio">Carter T Schonwald</a> for reading drafts of this post. Niklas also clarified some details regarding RAM speeds, and suggested <code>sysbench</code> to measure single threaded RAM reading speed in particular. Also thanks to Per Vognsen and Jason Rohem for spotting a few typos, and to <a href="https://twitter.com/RhialtoTheM">Rihalto</a> for pointing out a better <code>sum3</code> and some misleading wording.</p>
<h2 id="bonus-track-a-compiler-friendly-c-version">Bonus track – a compiler friendly C version <a href="#bonus-track-a-compiler-friendly-c-version">#</a></h2>
<p><a href="https://twitter.com/_monoid/status/1418663360871141376">Alexander Monakov suggested</a> a more robust C function which works well with both <code>gcc</code> and <code>clang</code>, performs as well as <code>sum3</code>, and does not resort to any assembly:</p>
<div id="cb14"><pre><code><span id="cb14-1"><span>uint64_t</span> sum5<span>(</span>Node <span>*</span>node<span>)</span> <span>{</span></span>
<span id="cb14-2">  <span>uint64_t</span> value <span>=</span> <span>0</span><span>;</span></span>
<span id="cb14-3">  Node <span>*</span>next <span>=</span> NULL<span>;</span></span>
<span id="cb14-4">  <span>for</span> <span>(;</span> node<span>;</span> node <span>=</span> node<span>-&gt;</span>next<span>)</span> <span>{</span></span>
<span id="cb14-5">    <span>for</span> <span>(;;)</span> <span>{</span></span>
<span id="cb14-6">      value <span>+=</span> node<span>-&gt;</span>value<span>;</span></span>
<span id="cb14-7">      <span>if</span> <span>(</span>node <span>+</span> <span>1</span> <span>!=</span> node<span>-&gt;</span>next<span>)</span> <span>{</span></span>
<span id="cb14-8">        <span>break</span><span>;</span></span>
<span id="cb14-9">      <span>}</span></span>
<span id="cb14-10">      node<span>++;</span></span>
<span id="cb14-11">    <span>}</span></span>
<span id="cb14-12">  <span>}</span></span>
<span id="cb14-13">  <span>return</span> value<span>;</span></span>
<span id="cb14-14"><span>}</span></span></code></pre></div>






</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WebVM is a server-less virtual Linux environment running client-side (574 pts)]]></title>
            <link>https://webvm.io/</link>
            <guid>40940225</guid>
            <pubDate>Thu, 11 Jul 2024 20:16:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webvm.io/">https://webvm.io/</a>, See on <a href="https://news.ycombinator.com/item?id=40940225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="loginLink">
                    <p><span id="networkStatus">Connect via Tailscale </span>
	              <span id="ipCopied">Copied! </span>
                    </p>
	            <p><img src="https://webvm.io/assets/tailscale.svg" height="35px">
		  </p></div></div>]]></description>
        </item>
    </channel>
</rss>