<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 04 Jul 2024 11:30:16 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Building a data compression utility in Haskell using Huffman codes (101 pts)]]></title>
            <link>https://lazamar.github.io/haskell-data-compression-with-huffman-codes/</link>
            <guid>40872332</guid>
            <pubDate>Thu, 04 Jul 2024 04:29:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lazamar.github.io/haskell-data-compression-with-huffman-codes/">https://lazamar.github.io/haskell-data-compression-with-huffman-codes/</a>, See on <a href="https://news.ycombinator.com/item?id=40872332">Hacker News</a></p>
Couldn't get https://lazamar.github.io/haskell-data-compression-with-huffman-codes/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The saddest "Just Ship It" story ever (2020) (182 pts)]]></title>
            <link>https://www.kitze.io/posts/saddest-just-ship-it-story-ever</link>
            <guid>40872182</guid>
            <pubDate>Thu, 04 Jul 2024 03:50:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kitze.io/posts/saddest-just-ship-it-story-ever">https://www.kitze.io/posts/saddest-just-ship-it-story-ever</a>, See on <a href="https://news.ycombinator.com/item?id=40872182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I know, I know, at this point you want "Just Ship It" to be an actual person so you could punch it in the face. As an Indie Maker, that sentence can be super frustrating because it's tired, it's cliche, and your response is always "HEY! You don't understand... it's not that easy".  </p>&nbsp;<p>I agree, it's not easy, but it's always the right thing to do. </p>&nbsp;<p>Here's why.  </p>&nbsp;<p>I started building an app on 01.01.2018. It was New Year's Eve and we just had the crappiest night ever. Yes, imagine a night <em>so bad</em> that at midnight you decide "you know what, fuck it, I'm gonna work on WEB DEVELOPMENT".<br>That bad.  </p>&nbsp;<p>The MVP was ready in a few days. I'm not that good of a coder, it's just a simple app. The 0.0.1 alpha version was more than ready. I could've released it, share it with a couple of people, and call it a day. I could've done that with every single version that I made, at any point from 2018 until now. I just wanted to add one more thing. One more feature. Just this one more thing and people will like it. One more screen and everything is gonna make sense. I swear, just this one last thing and it's ready.  </p>&nbsp;<p>BAM, last moment decision from the world's biggest dumbass:
"People wouldn't use this if it doesn't have a proper native mobile app for it. Time to learn React Native and spend a few months on that ü§¶Ô∏è"   </p>&nbsp;<p>God, if time machines were real, past-Kitze would be shoved in a toilet so hard right now. </p>&nbsp;<p>After 2 years of development, juggling between the fucking horror that's the web platform, React Native, Expo, GraphQL, bitching about how there's no ideal tech stack, the good old jQuery and Filezilla days, switching to other projects, releasing <a target="_blank" rel="noopener " href="https://sizzy.co/">other apps</a>, losing passion, finding passion, coming back to the app, etc. etc. etc...  </p>&nbsp;<p>I just dropped it.  </p>&nbsp;<p>I was still using it but I stopped developing it and just dropped the idea of releasing the app, ever.</p>&nbsp;<p>After a while, I was using it, but I realized that I'm missing a lot of features, so I'd either have to go back to developing it, or I'd have to find an alternative.   </p>&nbsp;<p>And boy did I find one.  </p>&nbsp;<p>I was scrolling their landing page and I was happy and furious at the same time. Someone solved the problem that I was solving. It was like someone literally read my mind and started coding. WHAT.  </p>&nbsp;<p>I have previously sent a video of my app to a couple of people (closest I came to shipping it) so I started getting suspicious if someone actually shared the video of my app with these people because they were solving literally the same problem, and they most of the features that I had.  </p>&nbsp;<p>I started getting this overwhelming happy, sad, and panicky feeling. I literally cannot explain how I felt while scrolling their page.</p>&nbsp;<p>One moment I am scrolling their list of features giggling like a little kid with a 48$ bill in a candy store (yea I know 48$ bills don't make sense, but JavaScript doesn't make sense and you're still using it), one moment I want to find these people and THROW THEM IN A PIT OF LIONS.   </p>&nbsp;<p>FUCK.    </p>&nbsp;<p>It's not their fault. I was just slow. I didn't ship on time. I'm gonna go ahead and tattoo "JUST SHIP IT" on my forehead. Nah I wouldn't be able to see it there. On my arm maybe. Nvm, let me go back to scrolling their landing page.  </p>&nbsp;<p>Fuck. They have solved everything that I wanted to solve, and WAY more. Hey, maybe I should be happy? I don't have to code anymore. Yay!? No more web platform? BLISS. Oh crap... the world is never gonna see my app though. But at least I don't have to see React Native anymore. NICE! Wait... BUT I WASTED SO MUCH TIME ON IT. FUCK.  A bunch of mixed feelings.  </p>&nbsp;<p>Here comes the saddest part, so grab a pack of tissues.   </p>&nbsp;<p>After a little bit of hesitation, I made an account. I watched the videos in their help center. Every time I caught myself smiling about a clever way they implemented something I slapped myself. NO. Bad Kitze. You shouldn't like this. THEY'RE COMPETITORS. <em>sigh</em> Sure buddy, whatever you tell yourself. Competitors to a shitty codebase sitting on your hard drive.  </p>&nbsp;<p>For 2 frickin' years, I thought it's too early to release my app because it's clunky, buggy, it's missing features, blah, blah, blah. No one would ever use it, right? I was so wrong.  </p>&nbsp;<p>I started using their app.  </p>&nbsp;<p>Even though they were working on it for the past few years it's still slow, buggy, and super unpolished, it doesn't matter, because they shipped.   </p>&nbsp;<p>Their mobile app is terrible and it needs 10 seconds to sync. It doesn't matter, they shipped. And I'm looking forward to every single update they release.</p>&nbsp;<p>Their backlog of things to do is huge, but it doesn't matter, they ship every single week, and the app is growing along with the community.  </p>&nbsp;<p>"But Kitze, even though tHeY sHiPpEd no one would pay for something unpolished and broken, right?"  </p>&nbsp;<p>Oh, Indie Hackers. So clever, yet so naive.  </p>&nbsp;<p>Today my 30-day trial has expired. A tear rolled down my cheek for every single digit of my credit card that I entered in their app. I am officially not only a subscriber, but also a fan. Every time I'll get a payment notification it's gonna feel like stepping on a lego ... glued to a knife. My bank might as well change the notification from "You have paid 5$ to ThatCompany" to "You never shipped, loser".   </p>&nbsp;<p>My app is officially dead.  </p>&nbsp;<p>99% of you are in the same boat right now, but hopefully just a few weeks into your project. Don't be a dumbass like me. Take a breath, roll your eyes at the cliche saying, but please...  </p>&nbsp;<p>Just Ship It.  </p>&nbsp;<p>P.S I would totally release and show you my app but ... it's not ready yet.  </p>&nbsp;<hr><p>Thanks for reading this! I wrote another article about <a target="_blank" rel="noopener " href="https://kitze.io/posts/github-stars-wont-pay-your-rent">my successful shipping story</a>, so I'd appreciate if you give it a read!</p>&nbsp;</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Vision Pro owners, are you still using it? (137 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40872102</link>
            <guid>40872102</guid>
            <pubDate>Thu, 04 Jul 2024 03:30:24 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40872102">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="40872203"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872203" href="https://news.ycombinator.com/vote?id=40872203&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Yes using it regularly multiple times a week.</p><p>I watch tons of media on it. My background was in film production and I firmly believe this is the best current way to watch films at home , as long as you don‚Äôt mind doing so alone.</p><p>The only better experience visually is a laser projector with active shutter glasses. I literally exclaimed out loud when I saw some of my shots on here for the first time. Depth for stereo movies adds so much, but you lose so much vibrancy and light with passive glasses. This solves both issues. I get why James Cameron said it was a religious experience. For fellow film makers, this is the highest quality way that I‚Äôve experienced my own work.</p><p>It also is probably the only place at home to experience these movies at that quality. Nobody else has 4k 3D HDR with HFR. Nobody.</p><p>So as a previous film buff, it‚Äôs worth it alone for me for that.</p><p>However I also use it for work regularly. I join industry meetings with it, I multitask regularly. I spend more time on the couch working off my laptop with this as my screen now.</p><p>The passthrough and eyesight features have been surprisingly great for being with my family. While people think it‚Äôs sad that I‚Äôm doing my own thing in the headset, the reality is that we all do our own hobbies in the evening after work. I can now spend that time with my partner and interact with them while they do their thing.</p><p>I think it‚Äôll take a while for Apple and the app developers to really get into the swing of things, but it‚Äôs been a huge, positive change for me.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872241"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40872241" href="https://news.ycombinator.com/vote?id=40872241&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>That‚Äôs a great insight.</p><p>I have the Vision Pro and do like the quality and sound from my Sony a90j and Sonos system a bit more, but the size of the screen in the Vision Pro is amazing</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40872306"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872306" href="https://news.ycombinator.com/vote?id=40872306&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>I‚Äôm still using Vision Pro for about 10-15 hours each week, but the bulk of that is spent mirroring my Mac or having focused writing time using the Obsidian iPad app‚Äîthere really aren‚Äôt apps that take full advantage of the platform yet (and I don‚Äôt watch a lot of movies). Still, it‚Äôs been the best way to stay productive away from my desk. The launch was a bit rocky with bugs and missing features, but the recent updates to mirroring and keyboard/mouse support are starting to hint that Apple is focusing in on productivity as a first-class use case. I‚Äôm okay paying the premium knowing that this platform has the potential to keep heading in this direction. It feels like the hardware has a decent amount of headroom.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872196"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872196" href="https://news.ycombinator.com/vote?id=40872196&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>I sent it back before the return deadline, would have considered keeping it if it had supported showing more than one display of my MacBook. I know by now they've released some sort of a "very wide display" in a VisionOS update, but back then it didn't really make sense. I thought, ok, I'll just try using one Mac desktop and all the other apps, messages &amp; browser would be the Vision OS native ones open side by side. Then it turned out I had to connect my bluetooth mouse/keyboard to the Vision device instead (if I wanted to type something into the browser/Messages) and it was too much friction.</p><p>I did like the brief period of working (or just browsing stuff) while lying on the couch, but I knew from the beginning that I didn't need an even lazier position for staring at a screen all day.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872176"><td></td></tr>
            <tr id="40872180"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872180" href="https://news.ycombinator.com/vote?id=40872180&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>I bought and returned mine. Without decent window management integration in OSX it feels limited to only "fun" use cases. If it were able to pair with a Bluetooth keyboard and sync to my MacBook with many screens... I would buy one again immediately.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872215"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40872215" href="https://news.ycombinator.com/vote?id=40872215&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>It does pair with a Bluetooth keyboard and VisionOS 2 introduces an ultrawide monitor mode that‚Äôs equivalent to two displays, fwiw.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872251"><td></td></tr>
                        <tr id="40872161"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872161" href="https://news.ycombinator.com/vote?id=40872161&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Interesting this is top of HN but no comments.</p><p>A lot more folks interested in the answer than able to answer perhaps.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872183"><td></td></tr>
            <tr id="40872179"><td></td></tr>
                <tr id="40872279"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40872279" href="https://news.ycombinator.com/vote?id=40872279&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>Also, importantly it‚Äôs had a very limited release. US only till this last week. Supposedly only 450k units available this year in total, which in the grand scheme is a very low distribution likelihood among a given demographic.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872211"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40872211" href="https://news.ycombinator.com/vote?id=40872211&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Truly, just too many other things I would rather spend $4000 on than a first generation Apple product. Hopefully the next revision can start at $2000, more palatable.</p><p>Something like the Quest link cable and SteamVR support would also be tremendous, but I'm not holding my breath on that.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40872187"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872187" href="https://news.ycombinator.com/vote?id=40872187&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>I use it as a fancy monitor that I can strap to my face and fits in a suitcase. sometimes I want to work lying down and it's great for that. It being based on ipados makes it kinda useless aside from the display.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872247"><td></td></tr>
            <tr id="40872245"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872245" href="https://news.ycombinator.com/vote?id=40872245&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>I've mostly been using mine to watch TV/movies, though when I get some extra time I've been meaning to try the AVLR port that makes it work with SteamVR as a VR headset with joycons serving as controllers.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872261"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872261" href="https://news.ycombinator.com/vote?id=40872261&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>Yes, but in a fairly specific niche. I've found that the primary use case is for travel. It is really quite compact if you disconnect the straps. Being able to have the functional equivalent to desktop monitors is really quite handy for doing work on the road. It does struggles when not using a laptop with it despite having tried to make that work a couple times.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872262"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872262" href="https://news.ycombinator.com/vote?id=40872262&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Yes, but I'll use it a lot more when 2.0 comes out, so I can see my keyboard in environments, which is my biggest complaint.</p><p>Mostly it's the best cinema screen I've ever viewed in my life. "Avatar 2", in 3D and at 48fps, is an absolutely stunning viewing experience. I wish high-framerate movies were more common. They look incredible.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872178"><td></td></tr>
                <tr id="40872194"><td></td></tr>
                <tr id="40872204"><td></td></tr>
            <tr id="40872207"><td></td></tr>
                <tr id="40872253"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40872253" href="https://news.ycombinator.com/vote?id=40872253&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>It generally includes the desk and everything that's on it, the chair, and the whole area surrounding the desk, and sometimes the whole room.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="40872269"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872269" href="https://news.ycombinator.com/vote?id=40872269&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Nope I have not been using mind. It is sitting next to my Oculus Quest now.</p><p>I wish there were more interesting things to do in it.</p><p>Honestly having that strapped to my head for any long thing (such as a movie) is a bit much. It's also heavy-ish and the field of view is still quite limited.</p><p>But most importantly there is really not much to do in it once the novelty wears off.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872288"><td></td></tr>
            <tr id="40872217"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872217" href="https://news.ycombinator.com/vote?id=40872217&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>I hope Apple makes a comparison product to Meta Ray Bans which i use daily.  Though today using my Ray Bans today and asking it questions got tedious and annoying as the reliability of "Hey Meta," working is 60 to 65 percent of the time.  I did wish for a disply to get the info instead of having to ask questions and intently listen.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872189"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872189" href="https://news.ycombinator.com/vote?id=40872189&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Huge VR fan here. I tried the demo in the Apple Store. It was cool and all, but nothing game changing. From a company like Apple - somewhat disappointing even. The technology just isn't there yet for a compelling product.</p><p>Apple should wait until they have this in a glasses form factor before hyping it up any more.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872228"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40872228" href="https://news.ycombinator.com/vote?id=40872228&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Yes. That's what Carmack said when he quit Oculus. The headgear has to get down to swim goggle size to get any traction, and eyeglass size to go mainstream.</p><p>Apple got it down from a brick on your head to a half-brick, but that's not good enough.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872284"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40872284" href="https://news.ycombinator.com/vote?id=40872284&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>It doesn‚Äôt seem like they were even prioritizing size and weight. They made it out of heavy glass and aluminum, and sacrificed size to add the front-facing screen.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40872220"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872220" href="https://news.ycombinator.com/vote?id=40872220&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>For all the people laughing at the Vision Pro and saying it's useless, just remember that when the first iPhone came out people were saying the same thing. It's often the second or third generation of a product where it really starts to shine.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872327"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40872327" href="https://news.ycombinator.com/vote?id=40872327&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>I‚Äôm happy to wait and see what they come up with. Tempted to get a quest in the meantime. AVP was too heavy, quest a bit too limiting.</p><p>I‚Äôll say that I was very happy with my G1 instead of an iPhone back in the day.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872264"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40872264" href="https://news.ycombinator.com/vote?id=40872264&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>There are also plenty of products that just aren‚Äôt that good or useful from the beginning and it doesn‚Äôt matter how many revisions it gets.</p><p>Also, Apples version is on its first generation but it‚Äôs not like VR as a whole is, ‚Äúmodern‚Äù VR has been around for a decade.</p></div></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sans-IO: The secret to effective Rust for network services (112 pts)]]></title>
            <link>https://www.firezone.dev/blog/sans-io</link>
            <guid>40872020</guid>
            <pubDate>Thu, 04 Jul 2024 03:05:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.firezone.dev/blog/sans-io">https://www.firezone.dev/blog/sans-io</a>, See on <a href="https://news.ycombinator.com/item?id=40872020">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>At Firezone, we use Rust<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup> to build secure remote access that scales, be it
from your Android phone, MacOS computer or Linux server. At the core of each app
sits a connectivity library ‚Äî aptly named
<a href="https://www.github.com/firezone/firezone/tree/main/rust/connlib"><code>connlib</code></a>
‚Äî that manages network connections and WireGuard tunnels to secure your
traffic. After several iterations, we‚Äôve landed on a design that we are
extremely happy with. It gives us fast and exhaustive tests, deep customisation
and overall high assurance that it does what we want it to do.</p>
<p><code>connlib</code> is built in Rust and the design we are talking about is known as
sans-IO. Rust's premise of speed and memory-safety makes it a great choice for
building network services. Most parts of our Rust stack aren't particularly
surprising: We use the <code>tokio</code> runtime for asynchronous tasks, <code>tungstenite</code> for
WebSockets, <code>boringtun</code> for the WireGuard implementation, <code>rustls</code> to encrypt
traffic with the API, etc. Yet, once you go beneath the surface of the library,
you will discover something that is perhaps unusual: There are almost no calls
to <code>tokio::spawn</code>, all communication is multiplexed via a single UDP socket and
the same APIs appear to repeat themselves across various layers:
<code>handle_timeout</code>, <code>poll_transmit</code>, <code>handle_input</code>, and so on.</p>
<p>These are the tell-tale signs of a sans-IO design. Instead of sending and
receiving bytes via a socket in multiple places, our protocols are implemented
as pure state machines. Even time is abstracted away: every function that needs
to know the current time receives an <code>Instant</code> parameter instead of calling
<code>Instant::now</code> itself. This pattern isn't something that we invented! The Python
world even has a dedicated <a href="https://sans-io.readthedocs.io/">website</a> about it.
In Rust, it is used by libraries such as:</p>
<ul>
<li><a href="https://github.com/quinn-rs/quinn/tree/main/quinn-proto"><code>quinn</code></a>, an
independent QUIC implementation.</li>
<li><a href="https://github.com/cloudflare/quiche/tree/master/quiche"><code>quiche</code></a>,
cloudflare's QUIC implementation.</li>
<li><a href="https://github.com/algesten/str0m"><code>str0m</code></a>, a sans-IO WebRTC implementation.</li>
</ul>
<p>In this post, we'll go over some of the problems with doing IO the traditional
way, followed by transitioning that to a sans-IO design and the reasons why we
think it is a good idea. As it turns out, Rust lends itself particularly well to
this pattern.</p>
<h2 id="rusts-async-model--the-function-colouring-debate"><a href="#rusts-async-model--the-function-colouring-debate">Rust's async model &amp; the "function colouring" debate</a></h2>
<p>If you've been around the Rust space for a while, you will have likely come
across the "function colouring" debate. In a nutshell, it discusses the
constraint that async functions can only be called from other async functions,
thus "colouring" them. There are various takes on this but what stands out for
me is that the ability to suspend execution and resume later is a pretty
important part of function's API contract. The fact that Rust enforces this at
compile-time is a good thing.</p>
<p>A result of this constraint is that an async function deep down in your stack
"forces" every calling function to also become async in order to <code>.await</code> the
inner function. This can be problematic if the code you want to call isn't
actually yours but a dependency that you are pulling in.</p>
<p>Some people see this as a problem, and they would like to write code that is
agnostic over the "asyncness" of their dependencies. That concern has merit.
Ultimately, at the very bottom of each async call stack sits a <code>Future</code> that
needs to suspend on something. Usually, this is some form of IO, like writing to
a socket, reading from a file, waiting for time to advance, etc. The majority of
async functions however don't actually perform async work themselves. Instead,
they are only async because they depend on other async functions. The code
around those inner async functions would usually also work in a blocking
context, but the author of your dependency happened to pick the async variant.</p>
<p>Let's look at an example of this problem. Firezone's connectivity library
<code>connlib</code> uses <a href="https://datatracker.ietf.org/doc/html/rfc8445">ICE</a> for NAT
traversal and as part of that, we utilise STUN to discover our server-reflexive
candidate, i.e. our public address. STUN is a binary message format and a STUN
binding is a pretty simple protocol: Send a UDP packet to server, server notes
the IP + port it sees as the sending socket and send a UDP packet back
containing that address.</p>
<p>Here is how we could implement this using <code>tokio</code>'s <code>UdpSocket</code> (thank you to
Cloudflare for the public STUN server):</p>
<div><pre><code><span>#[tokio::main]</span>
<span>async</span> <span>fn</span> <span>main</span>() <span>-&gt;</span> anyhow::<span>Result</span>&lt;()&gt; {
    <span>let</span> <span>socket</span> = UdpSocket::<span>bind</span>(<span>"0.0.0.0:0"</span>).<span>await</span>?;
    socket.<span>connect</span>(<span>"stun.cloudflare.com:3478"</span>).<span>await</span>?;
    socket.<span>send</span>(&amp;<span>make_binding_request</span>()).<span>await</span>?;

    <span>let</span> <span>mut </span><span>buf</span> = <span>vec!</span>[<span>0u8</span>; <span>100</span>];
    <span>let</span> <span>num_read</span> = socket.<span>recv</span>(&amp;<span>mut</span> buf).<span>await</span>?;
    <span>let</span> <span>address</span> = <span>parse_binding_response</span>(&amp;buf[..num_read]);

    <span>println!</span>(<span>"Our public IP is: {address}"</span>);

    <span>Ok</span>(())
}
</code></pre></div>
<p>This could be also be written using blocking IO from the standard library:</p>
<div><pre><code><span>fn</span> <span>main</span>() <span>-&gt;</span> anyhow::<span>Result</span>&lt;()&gt; {
    <span>let</span> <span>socket</span> = UdpSocket::<span>bind</span>(<span>"0.0.0.0:0"</span>)?;
    socket.<span>connect</span>(<span>"stun.cloudflare.com:3478"</span>)?;
    socket.<span>send</span>(&amp;<span>make_binding_request</span>())?;

    <span>let</span> <span>mut </span><span>buf</span> = <span>vec!</span>[<span>0u8</span>; <span>100</span>];
    <span>let</span> <span>num_read</span> = socket.<span>recv</span>(&amp;<span>mut</span> buf)?;
    <span>let</span> <span>address</span> = <span>parse_binding_response</span>(&amp;buf[..num_read]);

    <span>println!</span>(<span>"Our public IP is: {address}"</span>);

    <span>Ok</span>(())
}
</code></pre></div>
<div><p>You can find all of these snippets as working programs in the following
repository: <a href="https://github.com/firezone/sans-io-blog-example">https://github.com/firezone/sans-io-blog-example</a>.</p></div>
<p>Notice how this code is virtually identical apart from the use of <code>async</code>? If we
wanted to write a library that allows you to perform STUN, we'd have to decide
on one of them or include both. There are lots of opinions out there as to what
the "best" way of solving this duplication is. Writing sans-IO code is one of
them.</p>
<h2 id="introducing-sans-io"><a href="#introducing-sans-io">Introducing sans-IO</a></h2>
<p>The core idea of sans-IO is similar to the dependency inversion principle from
the OOP world. Whilst some OOP code out there might be a bit extreme in terms of
following patterns (looking at you
<a href="https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/aop/framework/AbstractSingletonProxyFactoryBean.html"><code>AbstractSingletonProxyFactoryBean</code></a>),
I've found it helpful to explicitly spell some of these things out to really get
to the bottom of a particular design.</p>
<p>The dependency inversion principle says that policies (what to do) should not
depend on implementation details (how to do it). Instead, both components should
depend and communicate via abstractions. In other words, the piece of code that
decides to send a message on the network (i.e. the policy) should not depend on
the code that actually sends the message (i.e. the implementation).</p>
<p>That is the heart of the issue in the above example: We are composing our policy
code on top of a UDP socket and thus, forcing everything upwards to either be
<code>async</code> in the <code>tokio</code> example or deal with blocking IO in the <code>std</code> case. The
policy code is the same, yet it is the one we want to test and perhaps share
with others via libraries, regardless of whether or not we use blocking or
non-blocking IO.</p>
<h2 id="applying-dependency-inversion"><a href="#applying-dependency-inversion">Applying dependency inversion</a></h2>
<p>How do we apply the dependency inversion principle then? We introduce
abstractions! When we call <code>UdpSocket::send</code>, what data are we actually passing?
The payload, a <code>SocketAddr</code> and ‚Äî implicitly ‚Äî the socket itself.
The socket can also be identified by means of a <code>SocketAddr</code>: The one we bound
to earlier in our application. Let's package these three things up into an
abstraction. Meet <code>Transmit</code>:</p>
<div><pre><code><span>pub</span> <span>struct</span> <span>Transmit</span> {
    src: SocketAddr,
    dst: SocketAddr,
    payload: <span>Vec</span>&lt;<span>u8</span>&gt;
}
</code></pre></div>
<p>Anywhere where we'd like to send data over our <code>UdpSocket</code>, we should instead
emit a <code>Transmit</code>. But that is only one half of the solution. Where does the
<code>Transmit</code> go? We need to execute this <code>Transmit</code> somewhere! This is the 2nd
half of any sans-IO application. Recall the definition of the
dependency-inversion principle: Policies should not depend on implementations,
instead both should depend on abstractions. <code>Transmit</code> is our abstraction, and
we already know that we need to rewrite our policy code to use it. The actual
implementation details, i.e. our <code>UdpSocket</code> also needs to be made aware of our
new abstraction.</p>
<p>This is where event loops come in. sans-IO code needs to be "driven", almost
similarly as to how a <code>Future</code> in Rust is lazy and needs to be polled by a
runtime to make progress.</p>
<p>Event loops are the implementation of our side-effects and will actually call
<code>UdpSocket::send</code>. That way, the rest of the code turns into a state machine
that only expresses, what should happen at a given moment.</p>
<h3 id="the-state-machine"><a href="#the-state-machine">The state machine</a></h3>
<p>The state machine diagram for our STUN binding request looks like this:</p>
<p><img alt="A UML state diagram for a STUN binding request." loading="lazy" width="500" height="500" decoding="async" data-nimg="1" srcset="https://www.firezone.dev/_next/image?url=%2Fimages%2Fblog%2Fsans-io%2Fstun-binding-state-machine1.svg&amp;w=640&amp;q=75 1x, https://www.firezone.dev/_next/image?url=%2Fimages%2Fblog%2Fsans-io%2Fstun-binding-state-machine1.svg&amp;w=1080&amp;q=75 2x" src="https://www.firezone.dev/_next/image?url=%2Fimages%2Fblog%2Fsans-io%2Fstun-binding-state-machine1.svg&amp;w=1080&amp;q=75"></p><p>Without executing the side-effect of sending a message directly, we need to
rewrite our code to resemble what it actually is: This state machine. As we can
see in our diagram, we have 2 states (not counting entry and exit states):
<code>Sent</code> &amp; <code>Received</code>. These are mutually-exclusive, so we can model them as an
enum:</p>
<div><pre><code><span>enum</span> <span>State</span> {
    Sent,
    Received { address: SocketAddr },
}
</code></pre></div>
<p>Now, that we've laid out our data structure, let's add some functionality to it!</p>
<div><pre><code><span>struct</span> <span>StunBinding</span> {
    state: State,
    buffered_transmits: VecDeque&lt;Transmit&gt;,
}

<span>impl</span> <span>StunBinding</span> {
    <span>fn</span> <span>new</span>(server: SocketAddr) <span>-&gt;</span> <span>Self</span> {
        <span>Self</span> {
            state: State::Sent,
            buffered_transmits: VecDeque::<span>from</span>([Transmit {
                dst: server,
                payload: <span>make_binding_request</span>(),
            }]),
        }
    }

    <span>fn</span> <span>handle_input</span>(&amp;<span>mut</span> <span>self</span>, packet: &amp;[<span>u8</span>]) {
        <span>// Error handling is left as an exercise to the reader ...</span>
        <span>let</span> <span>address</span> = <span>parse_binding_response</span>(packet);

        <span>self</span>.state = State::Received { address };
    }

    <span>fn</span> <span>poll_transmit</span>(&amp;<span>mut</span> <span>self</span>) <span>-&gt;</span> <span>Option</span>&lt;Transmit&gt; {
        <span>self</span>.buffered_transmits.<span>pop_front</span>()
    }

    <span>fn</span> <span>public_address</span>(&amp;<span>self</span>) <span>-&gt;</span> <span>Option</span>&lt;SocketAddr&gt; {
        <span>match</span> <span>self</span>.state {
            State::Sent =&gt; <span>None</span>,
            State::Received { address } =&gt; <span>Some</span>(address),
        }
    }
}
</code></pre></div>
<p>The <code>handle_input</code> function is like the inverse to <code>Transmit</code>. We will use it to
feed incoming data to our state machine, i.e. the result of <code>UdpSocket::recv</code>.
We also add a few auxiliary functions to actually construct a new instance of
our state machine and to query things from it. With this in place, we now have a
state machine that models the behaviour of our program without performing any IO
itself.</p>
<h3 id="the-event-loop"><a href="#the-event-loop">The event loop</a></h3>
<p>Without an event loop, this state machine does nothing. For this example, we can
get away with a pretty simple event loop:</p>
<div><pre><code><span>fn</span> <span>main</span>() <span>-&gt;</span> anyhow::<span>Result</span>&lt;()&gt; {
    <span>let</span> <span>socket</span> = UdpSocket::<span>bind</span>(<span>"0.0.0.0:0"</span>)?;
    <span>let</span> <span>server</span> = <span>"stun.cloudflare.com:3478"</span>
        .<span>to_socket_addrs</span>()?
        .<span>next</span>()
        .<span>context</span>(<span>"Failed to resolve hostname"</span>)?;
    <span>let</span> <span>mut </span><span>binding</span> = StunBinding::<span>new</span>(server);

    <span>let</span> <span>address</span> = <span>loop</span> {
        <span>if</span> <span>let</span> <span>Some</span>(transmit) = binding.<span>poll_transmit</span>() {
            socket.<span>send_to</span>(&amp;transmit.payload, transmit.dst)?;
            <span>continue</span>;
        }

        <span>let</span> <span>mut </span><span>buf</span> = <span>vec!</span>[<span>0u8</span>; <span>100</span>];
        <span>let</span> <span>num_read</span> = socket.<span>recv</span>(&amp;<span>mut</span> buf)?;

        binding.<span>handle_input</span>(&amp;buf[..num_read]);

        <span>if</span> <span>let</span> <span>Some</span>(address) = binding.<span>public_address</span>() {
            <span>break</span> address;
        }
    };

    <span>println!</span>(<span>"Our public IP is: {address}"</span>);

    <span>Ok</span>(())
}
</code></pre></div>
<p>Notice how the event loop is slightly more generic than the previous versions?
The event loop does not make any assumptions about the details of the STUN
binding protocol. It doesn't know that it is request-response for example! From
the event loop's perspective, multiple message could be necessary before we can
figure out our public address.</p>
<p>UDP is an unreliable protocol, meaning our packets could get lost in transit. To
mitigate this, STUN mandates retransmission timers. As it turns out, adding time
to this event loop is fairly trivial.</p>
<h3 id="abstracting-time"><a href="#abstracting-time">Abstracting time</a></h3>
<p>What do we mean when we talk about abstracting time? In most cases, especially
in network protocols, access to the current time is needed to check whether some
amount of time has passed. For example, has it been more than 5s since we sent
our request? Another common one is keep-alive messages: Has it been more than
30s since we sent our last keep-alive?</p>
<p>In all these cases, we don't actually need to know the current <em>wall clock</em>
time. All we need is a <code>Duration</code> to a previous point in time. Rust provides us
with a very convenient abstraction here: <code>Instant</code>. <code>Instant</code> doesn't expose the
current time, but it allows us to measure the <code>Duration</code> between two <code>Instant</code>s.
We can extend our state machine with two APIs that are generic enough to cover
all our time-based needs: <code>poll_timeout</code> and <code>handle_timeout</code>:</p>
<div><pre><code><span>impl</span> <span>StunBinding</span> {
    <span>// ...</span>

    <span>/// Notifies `StunBinding` that time has advanced to `now`.</span>
    <span>fn</span> <span>handle_timeout</span>(&amp;<span>mut</span> <span>self</span>, now: Instant) {}

    <span>/// Returns the timestamp when we next expect `handle_timeout` to be called.</span>
    <span>fn</span> <span>poll_timeout</span>(&amp;<span>self</span>) <span>-&gt;</span> <span>Option</span>&lt;Instant&gt; {
        <span>None</span>
    }

    <span>// ...</span>
}
</code></pre></div>
<p>Similar to <code>handle_input</code> and <code>poll_timeout</code>, these APIs are the abstraction
between our protocol code and the event loop:</p>
<ul>
<li><code>poll_timeout</code>: Used by the event loop to schedule a timer for a wake-up.</li>
<li><code>handle_timeout</code>: Used by the event loop to notify the state machine that a
timer has expired.</li>
</ul>
<p>For demonstration purposes, let's say we want to send a new binding request
every 5s after we have received the last one. Here is how one could implement
this:</p>
<div><pre><code><span>impl</span> <span>StunBinding</span> {
    <span>// ...</span>

    <span>/// Notifies `StunBinding` that time has advanced to `now`.</span>
    <span>fn</span> <span>handle_timeout</span>(&amp;<span>mut</span> <span>self</span>, now: Instant) {
        <span>let</span> <span>last_received_at</span> = <span>match</span> <span>self</span>.state {
            State::Sent =&gt; <span>return</span>,
            State::Received { at, .. } =&gt; at,
        };

        <span>if</span> now.<span>duration_since</span>(last_received_at) &lt; Duration::<span>from_secs</span>(<span>5</span>) {
            <span>return</span>;
        }

        <span>self</span>.buffered_transmits.<span>push_front</span>(Transmit {
            dst: <span>self</span>.server,
            payload: <span>make_binding_request</span>(),
        });
        <span>self</span>.state = State::Sent;
    }

    <span>/// Returns the timestamp when we next expect `handle_timeout` to be called.</span>
    <span>fn</span> <span>poll_timeout</span>(&amp;<span>self</span>) <span>-&gt;</span> <span>Option</span>&lt;Instant&gt; {
        <span>match</span> <span>self</span>.state {
            State::Sent =&gt; <span>None</span>,
            State::Received { at, .. } =&gt; <span>Some</span>(at + Duration::<span>from_secs</span>(<span>5</span>)),
        }
    }

    <span>// ...</span>
}
</code></pre></div>
<p>The only other changes I've made are adding an <code>at</code> field to the
<code>State::Received</code> variant that gets set to the current time upon <code>handle_input</code>:</p>
<div><pre><code><span>impl</span> <span>StunBinding</span> {
    <span>fn</span> <span>handle_input</span>(&amp;<span>mut</span> <span>self</span>, packet: &amp;[<span>u8</span>], now: Instant) {
        <span>let</span> <span>address</span> = <span>parse_binding_response</span>(packet);

        <span>self</span>.state = State::Received { address, at: now };
    }
}
</code></pre></div>
<p>This is an updated version of our state diagram:</p>
<p><img alt="A UML state diagram for a STUN binding request that is being refreshed every 5s." loading="lazy" width="500" height="800" decoding="async" data-nimg="1" srcset="https://www.firezone.dev/_next/image?url=%2Fimages%2Fblog%2Fsans-io%2Fstun-binding-state-machine2.svg&amp;w=640&amp;q=75 1x, https://www.firezone.dev/_next/image?url=%2Fimages%2Fblog%2Fsans-io%2Fstun-binding-state-machine2.svg&amp;w=1080&amp;q=75 2x" src="https://www.firezone.dev/_next/image?url=%2Fimages%2Fblog%2Fsans-io%2Fstun-binding-state-machine2.svg&amp;w=1080&amp;q=75"></p><p>The event loop also changed slightly. Instead of exiting once we know our public
IP, we'll now loop until the user quits the program:</p>
<div><pre><code>    <span>loop</span> {
        <span>if</span> <span>let</span> <span>Some</span>(transmit) = binding.<span>poll_transmit</span>() {
            socket.<span>send_to</span>(&amp;transmit.payload, transmit.dst).<span>await</span>?;
            <span>continue</span>;
        }

        <span>let</span> <span>mut </span><span>buf</span> = <span>vec!</span>[<span>0u8</span>; <span>100</span>];

        tokio::<span>select!</span> {
            <span>Some</span>(time) = &amp;<span>mut</span> timer =&gt; {
                binding.<span>handle_timeout</span>(time);
            },
            res = socket.<span>recv</span>(&amp;<span>mut</span> buf) =&gt; {
                <span>let</span> <span>num_read</span> = res?;
                binding.<span>handle_input</span>(&amp;buf[..num_read], Instant::<span>now</span>());

            }
        }

        timer.<span>reset_to</span>(binding.<span>poll_timeout</span>());

        <span>if</span> <span>let</span> <span>Some</span>(address) = binding.<span>public_address</span>() {
            <span>println!</span>(<span>"Our public IP is: {address}"</span>);
        }
    }
</code></pre></div>
<h2 id="the-premise-of-sans-io"><a href="#the-premise-of-sans-io">The premise of sans-IO</a></h2>
<p>So far, all of this seems like a very excessive overhead for sending a few UDP
packets back and forth. Surely, the 10 line example introduced at the start is
preferable over this state machine and the event loop! The example might be, but
recall the debate around function colouring. In a code snippet without
dependencies like the above example, using <code>async</code> seems like a no-brainer and
really easy. The problem arises once you want to bring in dependencies.
Composing your functionality (i.e. policy) on top of those dependencies imposes
their decisions around async vs blocking IO on you. Libraries like <code>str0m</code> or
<code>quinn-proto</code> which are written in the sans-IO way don't do that. Instead, they
are pure state machines and thus the decision about async vs blocking IO or
which async runtime to use is deferred to the application.</p>
<p>Freedom to use either blocking or non-blocking IO isn't the only benefit to
this. sans-IO design also compose very well, tend to have very flexible APIs,
are easy to test and play well with Rust's features. Let's explore these
additional benefits one by one.</p>
<h3 id="easy-composition"><a href="#easy-composition">Easy composition</a></h3>
<p>Take another look at the API of <code>StunBinding</code>. The main functions exposed to the
event loop are: <code>handle_timeout</code>, <code>handle_input</code>, <code>poll_transmit</code> and
<code>poll_timeout</code>. None of these are specific to the domain of STUN! Most network
protocols can be implemented with these or some variation of them. As a result,
it is very easy to compose these state machines together: want to query 5 STUN
servers for your public IP? No problem. Just make 5 <code>StunBinding</code>s and call them
in order<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>.</p>
<p>In the case of Firezone, you can see this in the example of
<a href="https://github.com/firezone/firezone/tree/main/rust/connlib/snownet"><code>snownet</code></a>,
a library that combines ICE and WireGuard and thereby exposes "magic" IP tunnels
that work in any network setup to the rest of the application.</p>
<p><code>snownet</code> builds on top of <code>str0m</code>, a sans-IO WebRTC library and <code>boringtun</code>, an
(almost<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup>) sans-IO WireGuard implementation. We don‚Äôt need the majority of the
WebRTC stack though. The only thing we are interested in is the <code>IceAgent</code> which
implements <a href="https://datatracker.ietf.org/doc/html/rfc8445">RFC 8445</a>. ICE uses a
clever algorithm that ensures two agents, deployed into arbitrary network
environments find the most optimal communication path to each other. The result
of ICE is a pair of socket addresses that we then use to setup a WireGuard
tunnel. Because <code>str0m</code> is built in a sans-IO fashion, only using the <code>IceAgent</code>
is shockingly trivial: you simply only import that part of the library and
compose its state machine into your existing code. In <code>snownet</code>, a
<a href="https://github.com/firezone/firezone/blob/a5b7507932e9d27e3fc9ed5be7428b9937f2f828/rust/connlib/snownet/src/node.rs#L1289-L1306">connection</a>
simply houses an <code>IceAgent</code> and a wireguard tunnel, dispatching incoming
messages to either one or the other.</p>
<h3 id="flexible-apis"><a href="#flexible-apis">Flexible APIs</a></h3>
<p>sans-IO code needs to be "driven" by an event loop of some sorts because it
"just" expresses the state of the system but doesn‚Äôt cause any side-effects
itself. The event loop is responsible for "querying" the state (like
<code>poll_transmit</code>), executing it and also passing new input to the state machine
(<code>handle_timeout</code> and <code>handle_input</code>). To some people, this may appear as
unnecessary boilerplate but it comes with a great benefit: flexibility.</p>
<ul>
<li>Want to make use of <code>sendmmsg</code> to reduce the number of syscalls when sending
packets? No problem.</li>
<li>Want to multiplex multiple protocols over a single socket? No problem.</li>
</ul>
<p>Writing the event loop yourself is an opportunity to be able to tune our code to
exactly what we want it to do. This also makes maintenance easier for library
authors: They can focus on correctly implementing protocol functionality instead
of having debates around async runtimes or exposing APIs to set socket options.</p>
<p>A good example here is <code>str0m</code>‚Äôs stance on enumerating network interfaces: This
is an IO concern and up to the application on how to achieve it. <code>str0m</code> only
provides an API to add the socket addresses as an ICE candidate to the current
state. As a result, we are able to easily implement optimisations such as
gathering TURN candidates prior to any connection being made, thus reducing
Firezone's connection-setup latency.</p>
<div><p>In ICE, both parties gather candidates (sockets) and then test connectivity
between them. See <a href="https://datatracker.ietf.org/doc/html/rfc8445#section-5.1.1">https://datatracker.ietf.org/doc/html/rfc8445#section-5.1.1</a>
for details.</p></div>
<h3 id="testing-at-the-speed-of-light"><a href="#testing-at-the-speed-of-light">Testing at the speed of light</a></h3>
<p>sans-IO code is essentially side-effect free and thus lends itself extremely
well for (unit) tests. Due to sockets and time being abstracted away, it becomes
a breeze to write tests that advance time by 5 minutes in an instant. All we
need to do is pass a modified <code>Instant</code> to our function and assert, how the code
behaves. To see a real world example of this,
<a href="https://github.com/firezone/firezone/blob/53557f46e452c0fe5195a4326873753a356c6005/rust/connlib/snownet/tests/lib.rs#L123-L127">check out</a>
how we test that <code>snownet</code> closes idle connections after 5 minutes.</p>
<p>Similarly, actually sending data over a socket takes (a little bit of) time and
more importantly, requires allocation of ports etc. In a sans-IO world, "sending
data" in a test is as simple as taking a <code>Transmit</code> from party B and calling
<code>handle_input</code> on the state of party A. No need to go through a network socket!</p>
<p>At Firezone, we took this idea one step further. We implemented a reference
state machine that describes how we want <code>connlib</code> to work. This reference state
machine is used as the source of truth in our tests. We then leverage
<code>proptest</code>'s support for
<a href="https://proptest-rs.github.io/proptest/proptest/state-machine.html">state machine testing</a>
to deterministically sample and execute thousands of scenarios on every CI run
and compare the reference state machine with <code>connlib</code>'s actual state. The
details of this go beyond the scope of this post, so stay tuned for a followup
about that topic in particular too! The key take-away here is that a sans-IO
design enables these kind of tests.</p>
<h3 id="edge-cases-and-io-failures"><a href="#edge-cases-and-io-failures">Edge-cases and IO failures</a></h3>
<p>Not only can we easily test how our code reacts at certain points in time but
the lack of any IO also makes it really easy to test for IO failures and/or
weird behaviours!</p>
<ul>
<li>What happens if this packets gets dropped and we never receive a response?</li>
<li>What happens if we get a malformed response?</li>
<li>What happens if the RTT to the server is really long?</li>
<li>What happens if we don't have a functional IPv6 interface?</li>
<li>What happens if we <em>only</em> have an IPv6 interface?</li>
</ul>
<p>By decoupling our protocol implementation from the actual IO side-effects, we
are forced go back to the drawing board and design our state machine to be
resilient against these problems. Consequently, detecting and dealing with
errors simply becomes part of state machine's input handling which leads to more
robust code and makes it less likely for edge-cases to only be considered as an
after-thought.</p>
<h2 id="rust--sans-io-a-match-made-in-heaven"><a href="#rust--sans-io-a-match-made-in-heaven">Rust + sans-IO: A match made in heaven?</a></h2>
<p>Rust forces us to declare, which component or function in our code owns a
certain value. A common example for these are buffers: When reading from a
<code>UdpSocket</code>, we need to provide a <code>&amp;mut [u8]</code> as a place for the actual bytes
being received. Only the owner of a value can declare it mutable and thus either
mutate itself or temporarily hand out mutable references to other functions.
<code>UdpSocket</code> follows this design: It doesn't declare a buffer on its own,
instead, it only requires temporary, mutable access to it when it is actually
reading from the socket. The explicit modelling of ownership and mutability are
integral to how Rust works and what enable features like the borrow-checker.</p>
<p>In a sans-IO design we only have synchronous APIs, i.e. none of the functions on
a state machines ever block on IO or time. Instead, they are just data
structures.</p>
<p>Those two aspects work exceptionally well together. We can use <code>&amp;mut</code> liberally
to express state changes and thus leverage the borrow-checker to ensure our code
is sound. In comparison, <code>async</code> Rust and <code>&amp;mut</code> almost feel somewhat at odds
with each other.</p>
<p>In Rust, <code>async</code> functions are just syntax sugar for a data structure that
implements <code>Future</code>. Spawning a <code>Future</code> into a runtime<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup> like <code>tokio</code>
requires this data structure to be <code>'static</code> and therefore, it cannot contain
any references, including <code>&amp;mut</code>. To mutate state that isn't local to the
<code>Future</code>, you basically have two options:</p>
<ul>
<li>Use reference-counted pointers and a mutex, i.e. <code>Arc&lt;Mutex&lt;T&gt;&gt;</code></li>
<li>Use "actors" and connect them via channels, i.e. spawn multiple tasks with
loops that read and write to channels</li>
</ul>
<p>Both of these options have a runtime overhead: Locks can result in contention
and sending messages through channels requires copying. In addition, multiple
tasks running inside a runtime operate in a non-deterministic order which can
easily lead to race conditions and in the worst case, deadlocks. It appears that
with either of these options, we arrive at a design that feels brittle, is prone
to deadlocks and no longer employs zero-cost abstractions, yet avoiding all of
these is one of the reasons we wanted to use Rust in the first place!</p>
<p>In the sans-IO world, these problems don't exist. Our protocol code doesn't
spawn any tasks and thus, <code>&amp;mut self</code> is all we need to mutate state. Without
tasks or threads, we also don't need synchronisation primitives like <code>Mutex</code>.
Without channels, there is no need to copy data: The state machine can simply
directly reference the buffer we passed to the socket.</p>
<p>Last but not least, we've also found that ever since we moved to sans-IO, our
code became much easier to understand. No more tracking down of: Where is the
other end of this channel? What if the channel is closed? Which other code is
locking this <code>Mutex</code>? Instead, it is all just nested state machines and regular
function calls.</p>
<h2 id="the-downsides"><a href="#the-downsides">The downsides</a></h2>
<p>There are no silver-bullets and sans-IO is no exception to this. Whilst writing
your own event loop gives you great control, it can also result in subtle bugs
that are initially hard to find.</p>
<p>For example, a bug in the state machine where the value returned from
<code>poll_timeout</code> is not advanced can lead to a busy-looping behaviour in the event
loop.</p>
<p>Also, sequential workflows require more code to be written. In Rust, <code>async</code>
functions compile down to state machines, with each <code>.await</code> point representing
a transition to a different state. This makes it easy for developers to write
sequential code together with non-blocking IO. Without <code>async</code>, we need to write
our own state machines for expressing the various steps. How annoying this will
be in practise depends on your problem domain. Modelling a request-response
protocol is not very difficult as we've seen in the example of a <code>StunBinding</code>.
On the other hand, if need to express larger, sequential workflows, manually
modelling them out as state machines could become tedious.</p>
<p>Finally, the sans-IO design is not particularly wide-spread (yet) in the Rust
community. As a result, there are very few libraries out there that follow it.
Most of them will either implement blocking or non-blocking IO instead of
sans-IO.</p>
<h2 id="closing"><a href="#closing">Closing</a></h2>
<p>Writing sans-IO code is unusual at first but really enjoyable once you get the
hang of it. In part, this is because Rust provides great tools for modelling
state machines. More so, the fact that sans-IO forces you to handle errors as
you would any other input simply feels like the way networking code should be
written.</p>
<p>That being said, there are additional ways of writing async Rust not discussed
in this post. The most notable of those being structured concurrency which sits
somewhere "in the middle" between sans-IO and the async Rust portrayed in this
post. Read <a href="https://without.boats/blog/let-futures-be-futures/">this article</a>
from withoutboats for more on that topic.</p>
<p>Many thanks to <a href="https://github.com/algesten">@algesten</a> for providing feedback
on drafts of this post.</p>
<section data-footnotes="true">
<ol>
<li id="user-content-fn-1">
<p>For more details on Firezone's tech stack, see
<a href="https://www.firezone.dev/kb/architecture/tech-stack">this article</a> in our architecture docs. <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">‚Ü©</a></p>
</li>
<li id="user-content-fn-4">
<p>Be sure to implement proper multiplexing of STUN messages at this point.
Hint: Use the <code>TransactionId</code> and/or the server's address. <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 2">‚Ü©</a></p>
</li>
<li id="user-content-fn-3">
<p><code>boringtun</code> does call <code>Instant::now</code> internally and is thus unfortunately
partly impure, see <a href="https://github.com/cloudflare/boringtun/issues/391">https://github.com/cloudflare/boringtun/issues/391</a>. <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">‚Ü©</a></p>
</li>
<li id="user-content-fn-2">
<p>Technically, a thread-per-core runtime could allow non-<code>'static</code> <code>Future</code>s. <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 4">‚Ü©</a></p>
</li>
</ol>
</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Origins of DS_store (2006) (293 pts)]]></title>
            <link>https://www.arno.org/on-the-origins-of-ds-store</link>
            <guid>40870357</guid>
            <pubDate>Wed, 03 Jul 2024 21:55:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.arno.org/on-the-origins-of-ds-store">https://www.arno.org/on-the-origins-of-ds-store</a>, See on <a href="https://news.ycombinator.com/item?id=40870357">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article itemscope="" itemtype="http://schema.org/Article">


  <header>
    <h2>
      
      <span itemprop="section">Apple Lore</span>
      
      On the origins of DS_store
      
    </h2>
  </header>


  <section itemprop="articleBody">
    <p>If you are a Mac user, or if you have transferred files from Mac to Windows,
you‚Äôre probably familiar with <kbd>.DS_Store</kbd> files. But where does this
name come from?</p>

<p>Back in 1999 I was the technical lead for the Mac OS X Finder at Apple. At that
time the Finder code base was some 8 years old and had reached the end of its
useful life. Making any changes to it require huge engineering effort, and any
changes usually broke two or three seemingly unrelated features. For Mac OS X we
decided to rewrite the Finder from scratch.</p>

<p>Part of the work involved separating its user interface and its core
functionality, the back-end. The back-end of the Finder enumerates files, watch
for changes in the file system, deals with metadata, including icon locations
and folder settings. Internally, those two components were known as Finder_FE
and Finder_BE (Frontend and Backend).</p>

<p>However, we soon started realizing that the Finder backend would be useful
outside of the Finder. Therefore, a plan was hatched to someday make it
available as a public API. Since I had previously been responsible for naming
Icon Services and Navigation Services, we decided to go with Desktop Services
(at the time, we were also considering renaming the Finder to ‚ÄúDesktop‚Äù).
Hence the name of the <kbd>.DS_Store</kbd>, for ‚ÄúDesktop Services Store‚Äù. We
added a ‚Äú.‚Äù in front of it so that it would be considered as an invisible file
by Unix OS, including Mac OS.</p>

<p>Personally, I don‚Äôt think it‚Äôs a great name and I wish we had gone with
something a bit more descriptive, but it‚Äôs too late for that :-)</p>

<p>There is also an unfortunate bug that is not fixed to this day that result in an
excessive creation of <kbd>.DS_Store</kbd> file. Those files should only be
created if the user actually makes adjustments to the view settings or set a
manual location for icons in a folder. That‚Äôs unfortunately not what happens and
visiting a folder pretty much guarantees that a <kbd>.DS_Store</kbd> file will
get created</p>

<p>Incidentally, Finder_BE aka Desktop Services did end up being used by more than
just the Finder: Navigation Services (the Open/Save dialog) now also make use of
it, although it didn‚Äôt in the initial release of Mac OS. However, the Desktop
Services API has still not been fully released.</p>

  </section>



  <hr>
  <section>
    <p itemprop="datePublished" datetime="2006-10-01 00:00:00 -0700">Published Oct 1, 2006</p>
  </section>

  <section>
    
    <p>By 
    </p>
    <p>Published by <span itemprop="publisher" itemscope="" itemtype="http://schema.org/Publisher">
      <span itemprop="name">Arno Gourdol</span></span>
    </p>
    <img itemprop="image" src="">
  </section>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beating NumPy matrix multiplication in 150 lines of C (217 pts)]]></title>
            <link>https://salykova.github.io/matmul-cpu</link>
            <guid>40870345</guid>
            <pubDate>Wed, 03 Jul 2024 21:52:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://salykova.github.io/matmul-cpu">https://salykova.github.io/matmul-cpu</a>, See on <a href="https://news.ycombinator.com/item?id=40870345">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><strong>TL;DR</strong>
The code from the tutorial is available at <a href="https://github.com/salykova/matmul.c">matmul.c</a>. This blog post is the result of my attempt to implement high-performance matrix multiplication on CPU while keeping the code simple, portable and scalable. The implementation follows the <a href="https://en.wikipedia.org/wiki/BLIS_(software)">BLIS</a> design, works for arbitrary matrix sizes, and, when fine-tuned for an AMD Ryzen 7700 (8 cores), outperforms NumPy (=<a href="https://en.wikipedia.org/wiki/OpenBLAS">OpenBLAS</a>), achieving over 1 TFLOPS of peak performance across a wide range of matrix sizes.
<img src="https://salykova.github.io/assets/matmul_cpu/benchmark_mt.png" alt="" width="90%"></p>

<p>By efficiently parallelizing the code with <strong>just 3 lines of OpenMP directives</strong>, it‚Äôs both scalable and easy to understand. The implementation hasn‚Äôt been tested on other CPUs, so I would appreciate feedback on its performance on your hardware. Although the code is portable and targets Intel Core and AMD Zen CPUs with FMA3 and AVX instructions (i.e., all modern Intel Core and AMD Zen CPUs), please don‚Äôt expect peak performance without fine-tuning the hyperparameters, such as <em>the number of threads, kernel, and block sizes</em>, unless you are running it on a Ryzen 7700(X). Additionally, on some Intel CPUs, the OpenBLAS implementation might be notably faster due to AVX-512 instructions, which were intentionally omitted here to support a broader range of processors. Throughout this tutorial, we‚Äôll implement matrix multiplication from scratch, learning how to optimize and parallelize C code using matrix multiplication as an example. This is my first time writing a blog post. If you enjoy it, please subscribe and share it! I would be happy to hear feedback from all of you. This is the first part of my planned two-part blog series. In the second part, we will learn how to optimize matrix multiplication on GPUs. Stay tuned!</p>

<h2 id="intro">Intro</h2>

<p>Matrix multiplication is an essential part of nearly all modern neural networks. For example, most of the time spent during inference in <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)">Transformers</a>  is actually taken up by matrix multiplications. Despite using matmul daily in PyTorch, NumPy, or JAX, I‚Äôve never really thought about how it is designed and implemented to maximize hardware efficiency. To achieve such speeds, NumPy, for instance, relies on external <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLAS</a> (Basic Linear Algebra Subprograms) libraries. These libraries implement highly optimized common linear algebra operations such as dot product, matrix multiplication, vector addition, and scalar multiplication. Examples of BLAS implementations include:</p>

<ol>
  <li><a href="https://en.wikipedia.org/wiki/Math_Kernel_Library">Intel MKL</a> - optimized for Intel CPUs</li>
  <li><a href="https://developer.apple.com/documentation/accelerate">Accelerate</a> - optimized for Apple CPUs</li>
  <li><a href="https://en.wikipedia.org/wiki/BLIS_(software)">BLIS</a> - open-source, multi-vendor support</li>
  <li><a href="https://en.wikipedia.org/wiki/GotoBLAS">GotoBLAS</a> - open-source, multi-vendor support</li>
  <li><a href="https://en.wikipedia.org/wiki/OpenBLAS">OpenBLAS</a> - open-source, based on GotoBLAS</li>
  <li>etc.</li>
</ol>

<p>If you look at the OpenBLAS  <a href="https://github.com/OpenMathLib/OpenBLAS/blob/develop/kernel/x86_64/sgemm_kernel_8x4_haswell.c">code</a>, you‚Äôll notice it‚Äôs a mix of C and low-level assembly code. In fact, OpenBLAS, GotoBLAS, and BLIS are all written in C/FORTRAN/Assembly and contain matmul implementations handcrafted for different CPU types. During runtime, the appropriate function is called depending on the detected CPU device. I challenged myself and asked if it is possible to write a high-performance matmul (across a wide range of matrix sizes) without diving deep into Assembly and Fortran code, at least for my CPU. After some searching on the internet, I found a couple of exciting and educational step-by-step tutorials on how to implement fast matmul from scratch, covering both theoretical and practical aspects:</p>

<ol>
  <li><a href="https://siboehm.com/articles/22/Fast-MMM-on-CPU">Fast Multidimensional Matrix Multiplication on CPU from Scratch</a> by Simon Boehm.</li>
  <li><a href="https://en.algorithmica.org/hpc/algorithms/matmul/">Matrix Multiplication</a> by Sergey Slotin.</li>
  <li><a href="https://en.wikipedia.org/wiki/George_Hotz">Geohot‚Äôs</a> famous stream <a href="https://www.youtube.com/watch?v=VgSQ1GOC86s">Can you multiply a matrix?</a></li>
</ol>

<p>I highly recommend checking out these well-written and well-spoken tutorials with alternative matmul implementations. They helped me better understand the topic and, in some sense, motivated me to write a different implementation. Why? The reason is that all three solutions above work only for specific matrix sizes and do not achieve NumPy‚Äôs multi-threaded speed (except for Geohot‚Äôs implementation, which is comparable to NumPy in terms of speed but again works only for specific matrix sizes and requires an extra <a href="https://github.com/tinygrad/tinygrad/blob/master/extra/gemm/gemm.c#L130">preswizzle</a> step, resulting in a full copy of one of the input matrices). So, I wasn‚Äôt satisfied with the results and continued researching until I stumbled across two fascinating papers: ‚Äú<a href="https://www.cs.utexas.edu/~flame/pubs/GotoTOMS_final.pdf">Anatomy of High-Performance Matrix Multiplication</a>‚Äù and ‚Äú<a href="https://www.cs.utexas.edu/~flame/pubs/blis3_ipdps14.pdf">Anatomy of High-Performance Many-Threaded Matrix Multiplication</a>‚Äù. The former presents the BLAS implementation known as GotoBLAS, developed by <a href="https://en.wikipedia.org/wiki/Kazushige_Goto">Kazushige Goto</a>. The latter briefly reviews the design of matmul op used in BLIS (an extended version of GotoBLAS) and discusses different parallelization possibilities for the matmul algorithm. After reading these papers I felt that the BLIS matmul design could potentially achieve all my goals:</p>

<ul>
  <li>NumPy-like multi-threading performance across a broad range of matrix sizes</li>
  <li>Simple, portable and scalable C code</li>
  <li>Support for a wide variety of processors</li>
</ul>

<p>In the next sections, we will implement the algorithm from the paper and compare it against NumPy.</p>

<h2 id="numpy-performance">NumPy Performance</h2>

<p>By default, if installed via <code>pip</code>, numpy uses OpenBLAS on AMD CPUs. Therefore, throughout this tutorial I will use numpy and OpenBLAS interchangeably. Before performing any benchmarks, it‚Äôs always good practice to specify your hardware specs and development environment to ensure the results can be reproduced:</p>
<ul>
  <li>CPU: Ryzen 7 7700 8 Cores, 16 Threads
    <ul>
      <li>Freq: 3.8 GHz</li>
      <li>Turbo Freq: 5.3 GHz</li>
      <li>L1 Cache: 64 KB (per core)</li>
      <li>L2 Cache: 1MB (per core)</li>
      <li>L3 Cache: 32MB (shared), 16-way associative</li>
    </ul>
  </li>
  <li>RAM: 32GB DDR5 6000 MHz CL36</li>
  <li>Numpy 1.26.4</li>
  <li>Compiler: clang-17</li>
  <li>Compiler flags: <code>-O2 -mno-avx512f -march=native</code></li>
  <li>OS: Ubuntu 22.04.4 LTS</li>
</ul>

<p>To multiply two <code>float32</code> matrices A of shape \(M \times K\) and B of shape \(K \times N\), for each element of the resulting matrix C of shape \(M \times N\), we need to calculate the dot product between a row of A and a column of B. This results in \(K\) (additions) + \(K\) (multiplications) = \(2K\) FLoating Point Operations (FLOP) per element of matrix C or \(2KMN\) FLOP in total.</p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/matmul_naive.png" alt=""></p>

<p>We will measure performance in terms of FLOP per second FLOP/s=FLOPS. In Python, this can be simply done as follows:</p>
<div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>time</span>

<span>A</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>M</span><span>,</span> <span>K</span><span>).</span><span>astype</span><span>(</span><span>np</span><span>.</span><span>float32</span><span>)</span>
<span>B</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>K</span><span>,</span> <span>N</span><span>).</span><span>astype</span><span>(</span><span>np</span><span>.</span><span>float32</span><span>)</span>
<span>FLOP</span> <span>=</span> <span>2</span><span>*</span><span>K</span><span>*</span><span>M</span><span>*</span><span>N</span>

<span>start</span> <span>=</span> <span>time</span><span>.</span><span>perf_counter</span><span>()</span>
<span>C</span> <span>=</span> <span>A</span> <span>@</span> <span>B</span>
<span>end</span> <span>=</span> <span>time</span><span>.</span><span>perf_counter</span><span>()</span>
<span>exec_time</span> <span>=</span> <span>end</span> <span>-</span> <span>start</span>
<span>FLOPS</span> <span>=</span> <span>FLOP</span><span>/</span><span>exec_time</span>
<span>GFLOPS</span> <span>=</span> <span>FLOPS</span><span>/</span><span>1e9</span>
</code></pre></div>

<blockquote>
  <p><strong>Important!</strong> When benchmarking code, try to minimize the number of running tasks, especially when measuring multi-threaded code. Results obtained on Windows are usually lower than on Linux.</p>
</blockquote>

<p>To benchmark numpy‚Äôs matmul, we will use <code>benchmark_numpy.py</code>, which executes the code snippet above for different matrix sizes in a loop and measures peak/average FLOPS. By default, numpy will use all available cores; however, we can easily change this by setting environment variables before importing numpy and matplotlib</p>
<div><pre><code><span>os</span><span>.</span><span>environ</span><span>[</span><span>"OPENBLAS_NUM_THREADS"</span><span>]</span> <span>=</span> <span>"1"</span>
<span>os</span><span>.</span><span>environ</span><span>[</span><span>"MKL_NUM_THREADS"</span><span>]</span> <span>=</span> <span>"1"</span>
<span>os</span><span>.</span><span>environ</span><span>[</span><span>"OMP_NUM_THREADS"</span><span>]</span> <span>=</span> <span>"1"</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
</code></pre></div>

<p>To measure Numpy‚Äôs matmul performance, run</p>
<div><pre><code>python benchmark_numpy.py <span>-NITER</span><span>=</span>200 <span>-ST</span> <span>-SAVEFIG</span>
</code></pre></div>
<p>for single-threaded benchmark and</p>
<div><pre><code>python benchmark_numpy.py <span>-NITER</span><span>=</span>500 <span>-SAVEFIG</span>
</code></pre></div>
<p>for multi-threaded benchmark.</p>

<p>On my machine I got the following results:</p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/benchmark_np_mt.png" alt="" width="90%"></p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/benchmark_np_st.png" alt="" width="90%"></p>

<p>How close are we to the theoretical upper limit achievable on the CPU?</p>

<h2 id="theoretical-limit">Theoretical Limit</h2>

<p>Recall the computer‚Äôs memory hierarchy (for now, ignore the layers between registers and RAM; we will discuss them later).</p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/cpu_mem_no_cache.png" alt="" width="70%"></p>

<p>To perform arithmetic operations on data stored in RAM (off-chip memory, slow and large), the data must first be transferred to the CPU and eventually stored in CPU registers (on-chip memory, fast and small). Modern x86 CPUs support SIMD (Single Instruction Multiple Data) extensions, which allow multiple pieces of data to be processed in parallel. There are various SIMD extensions, but the ones relevant to our discussion are <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">Advanced Vector Extensions</a> (AVX) and <a href="https://en.wikipedia.org/wiki/FMA_instruction_set">Fused Multiply-Add</a> (FMA). Both AVX and FMA operate on data stored in special 256-bit YMM registers. Each YMM register can hold up to 256/32 = 8 packed single-precision (32-bit) floats. The FMA extension allows a multiply-add operation to be performed in one step on data stored in YMM registers. The corresponding assembly instruction is called <code>VFMADD213PS</code> (PS stands for PackedSingle) and takes three registers (<code>YMM1</code>, <code>YMM2</code>, <code>YMM3</code>) as input to calculate <code>YMM1 * YMM2 + YMM3</code> and store the result in <code>YMM3</code>, hence the ‚Äú213‚Äù (there are also <code>vfmadd132ps</code>, <code>vfmadd231ps</code> variants).</p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/fmadd.png" alt=""></p>

<p>According to the <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html">intel intrinsics guide</a> or <a href="https://uops.info/table.html">https://uops.info/table.html</a>, the throughput (TP) of fused-multiply-add is 0.5 cycles/instruction or 2 instructions/cycle:
<img src="https://salykova.github.io/assets/matmul_cpu/fmadd_uops.png" alt=""></p>

<p>Theoretically, the CPU can execute 32 FLOP per cycle = 8 (floats in YMM register) * 2 (add + mul) * 2 (1/TP). On my machine, the CPU boosts up to 5.1 GHz in single-threaded tasks and up to 4.7 GHz in multi-threaded tasks. Therefore, a rough estimation of the maximum achievable FLOPS can be calculated as 5.1GHz * 32 FLOP/cycle = <strong>163 GFLOPS</strong> for single-threaded matmul and 4.7GHz * 32 FLOP/cycle * 8 cores = <strong>1203 GFLOPS</strong> for multi-threaded matmul. Starting from \(M=N=K=1000\), numpy reaches on average 92% of the theoretical maximum single-threaded performance and 85% of the multi-threaded. Can we compete with NumPy using plain C code without thousands of lines of low-level assembly code?</p>

<h2 id="naive-implementation">Naive Implementation</h2>

<p>Without loss of generality in this implementation we will assume that matrices stored in column-major order. A matrix <code>A</code> of shape <code>MxN</code> is stored as contiguous array of length <code>M*N</code> and an element <code>A[row][col]</code> is accessed via C raw pointer <code>ptr[col*M + row]</code>, where <code>0 &lt;= col &lt;= N-1</code> and <code>0 &lt;= row &lt;= M-1</code>.
<img src="https://salykova.github.io/assets/matmul_cpu/mem_layout.png" alt="" width="80%"></p>

<p>The naive implementation
<img src="https://salykova.github.io/assets/matmul_cpu/matmul_naive.png" alt=""></p>

<p>can be implemented as follows:</p>
<div><pre><code><span>void</span> <span>matmul_naive</span><span>(</span><span>float</span><span>*</span> <span>A</span><span>,</span> <span>float</span><span>*</span> <span>B</span><span>,</span> <span>float</span><span>*</span> <span>C</span><span>,</span> <span>const</span> <span>int</span> <span>M</span><span>,</span> <span>const</span> <span>int</span> <span>N</span><span>,</span> <span>const</span> <span>int</span> <span>K</span><span>)</span> <span>{</span>
  <span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>M</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>int</span> <span>j</span> <span>=</span> <span>0</span><span>;</span> <span>j</span> <span>&lt;</span> <span>N</span><span>;</span> <span>j</span><span>++</span><span>)</span> <span>{</span>
      <span>for</span> <span>(</span><span>int</span> <span>p</span> <span>=</span> <span>0</span><span>;</span> <span>p</span> <span>&lt;</span> <span>K</span><span>;</span> <span>p</span><span>++</span><span>)</span> <span>{</span>
        <span>C</span><span>[</span><span>j</span> <span>*</span> <span>M</span> <span>+</span> <span>i</span><span>]</span> <span>+=</span> <span>A</span><span>[</span><span>p</span> <span>*</span> <span>M</span> <span>+</span> <span>i</span><span>]</span> <span>*</span> <span>B</span><span>[</span><span>j</span> <span>*</span> <span>K</span> <span>+</span> <span>p</span><span>];</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</code></pre></div>
<p>We iterate over all rows (first loop) and all columns (second loop) of the matrix <code>C</code> and for each element of <code>C</code> we calculate the dot product (third loop) between the corresponding rows and columns of matrices <code>A</code> and <code>B</code>. It‚Äôs always good to start with simple and robust implementation that can later be used to test optimized implementations for correctness. The file <code>matmul_nave.c</code> contains this implementation.</p>

<p>Running the naive implementation</p>
<div><pre><code>clang-17 <span>-O2</span> <span>-mno-avx512f</span> <span>-march</span><span>=</span>native matmul_naive.c <span>-o</span> matmul_naive.out <span>&amp;&amp;</span> ./matmul_naive.out
</code></pre></div>
<p>results in 2.7 GFLOPS on my machine. Nowhere near our target of 1 TFLOPS.</p>

<h2 id="kernel">Kernel</h2>

<p>Matrix multiplication $C=AB$ can be decomposed into smaller sub-problems. The idea now is that if the smaller sub-problems can be solved quickly, then the entire matmul will be fast. We first partition the matrix $C$ of shape $M \times N$ into small sub-matrices of shape $m_R \times n_R$,  where $n_R \ll N$ and $m_R \ll M$. To calculate $C=AB$, we iterate over $C$ and compute each of its $m_R \times n_R$ sub-matrices.
<img src="https://salykova.github.io/assets/matmul_cpu/matmul_kernel.png" alt=""></p>

<p>The function that calculates these tiny $m_R \times n_R$ sub-matrices $\bar{C}$ of $C$ is called a <strong>kernel</strong> or <strong>micro-kernel</strong>. This is the heart of high-performance matrix multiplication. When we say that a matmul algorithm is optimized for a particular CPU architecture, it often involves kernel optimization. For example, in the BLIS library, the kernels optimized for different processor types can be found under <a href="https://github.com/flame/blis/tree/master/kernels">kernels</a>.</p>

<p>Let‚Äôs take a closer look at the kernel.
<img src="https://salykova.github.io/assets/matmul_cpu/kernel.png" alt=""></p>

<p>To calculate a $m_R \times n_R$ sub-matrix $\bar{C}$ of matrix $C$, we multiply matrix $\bar{A}$ of size $m_R \times K$ and matrix $\bar{B}$ of size $K \times n_R$. If we do this in a naive manner using dot products, we would need to fetch $2K$ (=dot product) elements from RAM to calculate a single element of $\bar{C}$ or $2K m_R n_R$ elements in total to calculate $\bar{C}$. However, there is an alternative strategy that can reduce the number of fetched elements.</p>

<p>We first load matrix $\bar{C}$ into SIMD (=YMM) registers (note that we can do this because both $n_R$ and $m_R$ are small). The subscript $R$ in $n_R$ and $m_R$ stands for ‚Äúregisters‚Äù. Then we iterate over $K$ and in each iteration we load 1 column of $\bar{A}$ and 1 row of $\bar{B}$ into YMM registers (again, note that both the row and the column vectors are small and fit in the registers). Finally, we perform matrix multiplication between the column and the row vectors to update the matrix $\bar{C}$. After $K$ iterations (=rank-1 updates), the matrix $\bar{C}$ is fully computed.
<img src="https://salykova.github.io/assets/matmul_cpu/kernel_rank.png" alt=""></p>

<blockquote>
  <p>Example of matrix multiplication between a column and a row vector. Each column of the resulting matrix is computed by multiplying vector $\mathbf{u}$ with a scalar element of the row vector.
<img src="https://salykova.github.io/assets/matmul_cpu/outer_product.png" alt=""></p>
</blockquote>

<p>Overall we fetched $(m_R + n_R)K + m_R n_R \approx (m_R + n_R)K$  elements into registers. Compared to the naive strategy, we reduced the number by a factor of</p><p>

\[\frac{2m_Rn_RK}{(m_R + n_R)K} = \frac{2m_Rn_R}{m_R + n_R}\]

</p><p>The factor is maximized when both $m_R$, $n_R$ are large and $m_R = n_R$. The values $m_R$ and $n_R$ are usually limited by the available memory in the registers.</p>

<p>Now, let‚Äôs explore how a rank-1 update can be implemented using SIMD instructions. Each rank-1 update is a matrix multiplication between a column of $\bar{A}$ and a row of $\bar{B}$. Note how individual column of $\bar{C}$ is updated via scalar-vector multiplication between a column of $\bar{A}$ and a corresponding scalar element of a row of $\bar{B}$. Thanks to the FMA extension, the update + scalar-vector multiplication can be efficiently calculated via a fused multiply-add instruction. Before executing the FMA instruction, we only need to broadcast the scalar element of the row of $\bar{B}$ to a vector and load it into a YMM register. The parameter $m_R$ determines how many elements are stored in column vectors of $\bar{C}, \bar{A}$ and how many YMM registers we need for this. Since each YMM register can store up to 8 floats, we assume that $m_R$ is a multiple of 8 (8, 16, 24, 32‚Ä¶) and the elements in column vectors are packed into blocks of size 8. Then the number of YMM registers required to store the column vectors can be calculated as $m_R$ / 8. Note that we don‚Äôt need additional YMM registers for the broadcasted column vector of $\bar{B}$ since the same 8-float vector (YMM Register) can be reused to update all 8-float blocks of the column vector of $\bar{C}$.</p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/kernel_registers.png" alt="" width="80%"></p>

<p>Thus, the complete algorithm for a single rank-1 update of the matrix $\bar{C}$ is as follows:</p>
<ol>
  <li>Load matrix $\bar{C}$ into YMM registers</li>
  <li>Load the column vector of matrix $\bar{A}$</li>
  <li>Set n = 1</li>
  <li>Load the n-th scalar element of the row of $\bar{B}$, broadcast it to a vector and place it into YMM register.</li>
  <li>Update the n-th column of $\bar{C}$ via fused matrix multiply</li>
  <li>Increment n by 1.</li>
  <li>Repeat steps 4-6 until all columns of $\bar{C}$ are updated.</li>
</ol>

<p>The last thing we need to discuss before implementing the kernel in C is how to choose the kernel size = $m_R$ and $n_R$. CPUs that support AVX instructions have <strong>16 YMM registers</strong>. From our previous observations, we know that we need $n_R m_R / 8$ registers to store the matrix $\bar{C}$, $m_R/8$ registers to store the column vector of $\bar{A}$ and 1 register for the broadcasted vector of $\bar{B}$. We want $m_R, n_R$ as large as possible and satisfying  the following conditions</p>

<ul>
  <li>$n_R m_R/8 + m_R/8 + 1 &lt;= 16$</li>
  <li>$m_R$ is a multiple of 8</li>
</ul>

<p>In theory we also want $m_R \approx n_R$ to minimize the number of fetched elements. However, in practice, I‚Äôve found out that the non-square $m_R \times n_R= 16 \times 6$ kernel shows the best results on my machine. You are free to try out different kernel sizes, for example, $8 \times 12$, $8 \times 13$, $8 \times 14$ and compare the performance on your CPU.</p>

<p>Let‚Äôs implement the $16 \times 6$ kernel in C. The code can be found in <code>matmul_kernel.c</code>. To use the SIMD instructions we need to include the <code>immintin.h</code> library.</p>


<p>the kernel function is declared as follows:</p>
<div><pre><code><span>void</span> <span>kernel_16x6</span><span>(</span><span>float</span><span>*</span> <span>A</span><span>,</span> <span>float</span><span>*</span> <span>B</span><span>,</span> <span>float</span><span>*</span> <span>C</span><span>,</span> <span>const</span> <span>int</span> <span>M</span><span>,</span> <span>const</span> <span>int</span> <span>N</span><span>,</span> <span>const</span> <span>int</span> <span>K</span><span>);</span>
</code></pre></div>
<p>The function takes as input 3 matrices + their dimensions and calculates a $16\times6$ sub-matrix $\bar{C}$ of $C$. Inside the function, first, declare the variables that reside in YMM registers:</p>
<div><pre><code><span>__m256</span> <span>C_buffer</span><span>[</span><span>2</span><span>][</span><span>6</span><span>];</span>
<span>__m256</span> <span>b_packFloat8</span><span>;</span>
<span>__m256</span> <span>a0_packFloat8</span><span>;</span>
<span>__m256</span> <span>a1_packFloat8</span><span>;</span>
</code></pre></div>
<p>The <code>__m256</code> datatype is a vector of 8 floats (8x32 = 256 bits) that resides in YMM register. <code>C_buffer</code> is a 16x6 sub-matrix of $C$ stored in YMM registers. The first dimension of <code>C_buffer</code> is <code>2</code>, because we need <code>16/8=2</code> registers to store 16 elements. <code>b_packFloat8</code>, <code>a0_packFloat8</code>, <code>a1_packFloat8</code> are column vectors of $\bar{B}$ and $\bar{A}$. Again, we need two vectors to store 16 elements of the column vector of $\bar{A}$.</p>

<p>Next, we load the sub-matrix $\bar{C}$ into YMM registers:</p>
<div><pre><code><span>for</span> <span>(</span><span>int</span> <span>j</span> <span>=</span> <span>0</span><span>;</span> <span>j</span> <span>&lt;</span> <span>6</span><span>;</span> <span>j</span><span>++</span><span>)</span> <span>{</span>
  <span>C_buffer</span><span>[</span><span>0</span><span>][</span><span>j</span><span>]</span> <span>=</span> <span>_mm256_loadu_ps</span><span>(</span><span>&amp;</span><span>C</span><span>[</span><span>j</span> <span>*</span> <span>M</span><span>]);</span>
  <span>C_buffer</span><span>[</span><span>1</span><span>][</span><span>j</span><span>]</span> <span>=</span> <span>_mm256_loadu_ps</span><span>(</span><span>&amp;</span><span>C</span><span>[</span><span>j</span> <span>*</span> <span>M</span> <span>+</span> <span>8</span><span>]);</span>
<span>}</span>
</code></pre></div>
<p>SIMD C functions are well documented and can be found in the <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html">Intel Intrinsics Guide</a>. For example, <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm256_loadu_ps&amp;ig_expand=4100">_mm256_loadu_ps</a></p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/intel_intrin.png" alt=""></p>

<p>In the next step, we iterate over <code>K</code> and, in each iteration, load a column vector of $\bar{A}$, broadcast a scalar value of $\bar{B}$ to a vector, and perform a fused multiply-add operation to update 1 column of <code>C_buffer</code>:</p>
<div><pre><code><span>for</span> <span>(</span><span>int</span> <span>p</span> <span>=</span> <span>0</span><span>;</span> <span>p</span> <span>&lt;</span> <span>K</span><span>;</span> <span>p</span><span>++</span><span>)</span> <span>{</span>
  <span>a0_packFloat8</span> <span>=</span> <span>_mm256_loadu_ps</span><span>(</span><span>&amp;</span><span>A</span><span>[</span><span>p</span> <span>*</span> <span>M</span><span>]);</span>
  <span>a1_packFloat8</span> <span>=</span> <span>_mm256_loadu_ps</span><span>(</span><span>&amp;</span><span>A</span><span>[</span><span>p</span> <span>*</span> <span>M</span> <span>+</span> <span>8</span><span>]);</span>
  <span>b_packFloat8</span> <span>=</span> <span>_mm256_broadcast_ss</span><span>(</span><span>&amp;</span><span>B</span><span>[</span><span>p</span><span>]);</span>
  <span>C_buffer</span><span>[</span><span>0</span><span>][</span><span>0</span><span>]</span> <span>=</span> <span>_mm256_fmadd_ps</span><span>(</span><span>a0_packFloat8</span><span>,</span> <span>b_packFloat8</span><span>,</span> <span>C_buffer</span><span>[</span><span>0</span><span>][</span><span>0</span><span>]);</span>
  <span>C_buffer</span><span>[</span><span>1</span><span>][</span><span>0</span><span>]</span> <span>=</span> <span>_mm256_fmadd_ps</span><span>(</span><span>a1_packFloat8</span><span>,</span> <span>b_packFloat8</span><span>,</span> <span>C_buffer</span><span>[</span><span>1</span><span>][</span><span>0</span><span>]);</span>
  <span>...</span>
<span>}</span>
</code></pre></div>
<p>Then repeat the step for the remaining 5 columns. We manually unroll the loop when updating 6 columns of <code>C_buffer</code> so that <code>clang</code> can optimize the code.</p>

<p>Finally, we write the sub-matrix <code>C_buffer</code> back to <code>C</code>:</p>
<div><pre><code><span>for</span> <span>(</span><span>int</span> <span>j</span> <span>=</span> <span>0</span><span>;</span> <span>j</span> <span>&lt;</span> <span>6</span><span>;</span> <span>j</span><span>++</span><span>)</span> <span>{</span>
  <span>_mm256_storeu_ps</span><span>(</span><span>&amp;</span><span>C</span><span>[</span><span>j</span> <span>*</span> <span>M</span><span>],</span> <span>C_buffer</span><span>[</span><span>0</span><span>][</span><span>j</span><span>]);</span>
  <span>_mm256_storeu_ps</span><span>(</span><span>&amp;</span><span>C</span><span>[</span><span>j</span> <span>*</span> <span>M</span> <span>+</span> <span>8</span><span>],</span> <span>C_buffer</span><span>[</span><span>1</span><span>][</span><span>j</span><span>]);</span>
<span>}</span>
</code></pre></div>

<p>To perform matrix multiplication, we simply iterate over the matrix $C$ and apply the kernel to it‚Äôs sub-matrices:</p>
<div><pre><code><span>#define MR 16
#define NR 6
</span>
<span>void</span> <span>matmul_kernel</span><span>(</span><span>float</span><span>*</span> <span>A</span><span>,</span> <span>float</span><span>*</span> <span>B</span><span>,</span> <span>float</span><span>*</span> <span>C</span><span>,</span> <span>const</span> <span>int</span> <span>M</span><span>,</span> <span>const</span> <span>int</span> <span>N</span><span>,</span> <span>const</span> <span>int</span> <span>K</span><span>)</span> <span>{</span>
  <span>assert</span><span>(</span><span>M</span> <span>%</span> <span>MR</span> <span>==</span> <span>0</span><span>);</span>
  <span>assert</span><span>(</span><span>N</span> <span>%</span> <span>NR</span> <span>==</span> <span>0</span><span>);</span>
  <span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>M</span><span>;</span> <span>i</span> <span>+=</span> <span>MR</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>int</span> <span>j</span> <span>=</span> <span>0</span><span>;</span> <span>j</span> <span>&lt;</span> <span>N</span><span>;</span> <span>j</span> <span>+=</span> <span>NR</span><span>)</span> <span>{</span>
        <span>kernel_16x6</span><span>(</span><span>&amp;</span><span>A</span><span>[</span><span>i</span><span>],</span> <span>&amp;</span><span>B</span><span>[</span><span>j</span> <span>*</span> <span>K</span><span>],</span> <span>&amp;</span><span>C</span><span>[</span><span>j</span> <span>*</span> <span>M</span> <span>+</span> <span>i</span><span>],</span> <span>M</span><span>,</span> <span>N</span><span>,</span> <span>K</span><span>);</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<p>The new implementation</p>
<div><pre><code>clang-17 <span>-O2</span> <span>-mno-avx512f</span> <span>-march</span><span>=</span>native <span>-DTEST</span> <span>-DNITER</span><span>=</span>100 matmul_kernel.c <span>-o</span> matmul_kernel.out <span>&amp;&amp;</span> ./matmul_kernel.out
</code></pre></div>
<p>results in 147 GFLOPS - a gigantic gain compared to the initial 2.7 GFLOPS. Additionally,
we can check the assembly code produced by the compiler via</p>
<div><pre><code>clang-17 <span>-O2</span> <span>-mno-avx512f</span> <span>-march</span><span>=</span>native matmul_kernel.c <span>-S</span> <span>&gt;</span> matmul_kernel.txt
</code></pre></div>
<p>to ensure that the SIMD instructions and the YMM registers are utilized:</p>
<div><pre><code>vbroadcastss	(%rsi,%rbp,4), %ymm14
vbroadcastss	(%rbx,%rbp,4), %ymm15
vfmadd231ps	%ymm14, %ymm12, %ymm3   # ymm3 = (ymm12 * ymm14) + ymm3
vfmadd231ps	%ymm14, %ymm13, %ymm1   # ymm1 = (ymm13 * ymm14) + ymm1
vbroadcastss	(%r13,%rbp,4), %ymm14
vfmadd231ps	%ymm12, %ymm15, %ymm11  # ymm11 = (ymm15 * ymm12) + ymm11
vfmadd231ps	%ymm15, %ymm13, %ymm10  # ymm10 = (ymm13 * ymm15) + ymm10
vfmadd231ps	%ymm14, %ymm12, %ymm2   # ymm2 = (ymm12 * ymm14) + ymm2
vfmadd231ps	%ymm14, %ymm13, %ymm0   # ymm0 = (ymm13 * ymm14) + ymm0
vbroadcastss	(%r12,%rbp,4), %ymm14
vfmadd231ps	%ymm14, %ymm12, %ymm5   # ymm5 = (ymm12 * ymm14) + ymm5
vfmadd231ps	%ymm14, %ymm13, %ymm4   # ymm4 = (ymm13 * ymm14) + ymm4
vbroadcastss	(%r15,%rbp,4), %ymm14
vfmadd231ps	%ymm14, %ymm12, %ymm7   # ymm7 = (ymm12 * ymm14) + ymm7
vfmadd231ps	%ymm14, %ymm13, %ymm6   # ymm6 = (ymm13 * ymm14) + ymm6
vbroadcastss	(%r14,%rbp,4), %ymm14
vfmadd231ps	%ymm14, %ymm12, %ymm9   # ymm9 = (ymm12 * ymm14) + ymm9
vfmadd231ps	%ymm14, %ymm13, %ymm8   # ymm8 = (ymm13 * ymm14) + ymm8
</code></pre></div>

<h2 id="masking-and-packing">Masking And Packing</h2>
<p>You might notice that the current kernel implementation works only for matrix sizes that are multiples of $m_R$ and $n_R$. To make the algorithm work for arbitrary matrix sizes, we need to handle edge cases where the kernel doesn‚Äôt fully overlap with matrix $C$.</p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/kernel_mask.png" alt=""></p>

<p>First of all, we when loading and storing the elements of $C$, we should pick the elements only within the matrix boundary. The case where the number of overlapped columns $n$ is less than $n_R$  is straightforward - we simply iterate over $n$ columns within the $C$ boundary:</p>
<div><pre><code><span># n - number of overlapped columns within C boundary
</span>
<span># "j&lt;n" instead "j&lt;6", since n can be less than 6.
</span><span>for</span> <span>(</span><span>int</span> <span>j</span> <span>=</span> <span>0</span><span>;</span> <span>j</span> <span>&lt;</span> <span>n</span><span>;</span> <span>j</span><span>++</span><span>)</span> <span>{</span>
  <span>C_buffer</span><span>[</span><span>0</span><span>][</span><span>j</span><span>]</span> <span>=</span> <span>_mm256_loadu_ps</span><span>(</span><span>&amp;</span><span>C</span><span>[</span><span>j</span> <span>*</span> <span>M</span><span>]);</span>
  <span>C_buffer</span><span>[</span><span>1</span><span>][</span><span>j</span><span>]</span> <span>=</span> <span>_mm256_loadu_ps</span><span>(</span><span>&amp;</span><span>C</span><span>[</span><span>j</span> <span>*</span> <span>M</span> <span>+</span> <span>8</span><span>]);</span>
<span>}</span>
</code></pre></div>
<p>Handling the case where the number of overlapped rows $m$ differs from $m_R$ is a bit trickier because <code>_mm256_loadu_ps</code> loads 8 elements at once. Fortunately, there is a function called <code>_mm256_maskload_ps</code> which loads 8 floats based on mask bits associated with each data element. It takes as input 2 arguments: <code>const float* data</code> and <code>__m256i mask</code>. <code>__m256i</code> is a 256-bit vector of 8x32-bit integers. The most significant bit (MSB) of each integer represents the mask bits. If a mask bit is zero, the corresponding value in the memory location is not loaded and the corresponding field in the return value is set to zero. For example, MSB of unsigned integer <code>2147483648</code> (binary representation <code>10000000 00000000 00000000 00000000</code>) is <code>1</code>, hence corresponding float in <code>data</code> will be loaded. On the other hand, MSB of unsigned integer <code>2147483647</code> (binary format <code>01111111 11111111 11111111 11111111</code>) is <code>0</code>, hence the corresponding float in <code>data</code> will not be loaded. The function <code>_mm256_maskstore_ps</code> works similarly, except it stores data instead of loading.</p>

<p>If $m \neq m_R$ , we create integer masks by left-shifting the unsigned integer <code>65535</code> (=<code>00000000 00000000 11111111 111111111</code> in binary format) depending on the number of overlapped rows $m$. The function <code>_mm256_setr_epi32</code> creates an 8-integer vector from 8 32-bit integers.</p>
<div><pre><code><span>__m256i</span> <span>masks</span><span>[</span><span>2</span><span>];</span>
<span>if</span> <span>(</span><span>m</span> <span>!=</span> <span>MR</span><span>)</span> <span>{</span>
  <span>const</span> <span>unsigned</span> <span>int</span> <span>bit_mask</span> <span>=</span> <span>65535</span><span>;</span>
  <span>masks</span><span>[</span><span>0</span><span>]</span> <span>=</span> <span>_mm256_setr_epi32</span><span>(</span><span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>15</span><span>),</span> <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>14</span><span>),</span>
                 <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>13</span><span>),</span> <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>12</span><span>),</span>
                 <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>11</span><span>),</span> <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>10</span><span>),</span>
                 <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>9</span><span>),</span> <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>8</span><span>));</span>
  <span>masks</span><span>[</span><span>1</span><span>]</span> <span>=</span> <span>_mm256_setr_epi32</span><span>(</span><span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>7</span><span>),</span> <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>6</span><span>),</span>
                 <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>5</span><span>),</span> <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>4</span><span>),</span>
                 <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>3</span><span>),</span> <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>2</span><span>),</span>
                 <span>bit_mask</span> <span>&lt;&lt;</span> <span>(</span><span>m</span> <span>+</span> <span>1</span><span>),</span> <span>bit_mask</span> <span>&lt;&lt;</span> <span>m</span><span>);</span>

  <span>for</span> <span>(</span><span>int</span> <span>j</span> <span>=</span> <span>0</span><span>;</span> <span>j</span> <span>&lt;</span> <span>n</span><span>;</span> <span>j</span><span>++</span><span>)</span> <span>{</span>
    <span>C_buffer</span><span>[</span><span>0</span><span>][</span><span>j</span><span>]</span> <span>=</span> <span>_mm256_maskload_ps</span><span>(</span><span>&amp;</span><span>C</span><span>[</span><span>j</span> <span>*</span> <span>M</span><span>],</span> <span>masks</span><span>[</span><span>0</span><span>]);</span>
    <span>C_buffer</span><span>[</span><span>1</span><span>][</span><span>j</span><span>]</span> <span>=</span> <span>_mm256_maskload_ps</span><span>(</span><span>&amp;</span><span>C</span><span>[</span><span>j</span> <span>*</span> <span>M</span> <span>+</span> <span>8</span><span>],</span> <span>masks</span><span>[</span><span>1</span><span>]);</span>
  <span>}</span>
<span>}</span>
</code></pre></div>
<p>The same masks are used to store the results back after rank-1 updates.</p>

<p>Additionally, we copy and pad with zeros (if needed) $m \times K$, $K \times n$ blocks of $A$ and $B$ into arrays with static shapes $m_R \times K$, $n_R \times K$.</p>
<div><pre><code><span>void</span> <span>pack_blockA</span><span>(</span><span>float</span><span>*</span> <span>A</span><span>,</span> <span>float</span><span>*</span> <span>blockA_packed</span><span>,</span> <span>const</span> <span>int</span> <span>m</span><span>,</span> <span>const</span> <span>int</span> <span>M</span><span>,</span>
                 <span>const</span> <span>int</span> <span>K</span><span>)</span> <span>{</span>
  <span>for</span> <span>(</span><span>int</span> <span>p</span> <span>=</span> <span>0</span><span>;</span> <span>p</span> <span>&lt;</span> <span>K</span><span>;</span> <span>p</span><span>++</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>m</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
      <span>*</span><span>blockA_packed</span> <span>=</span> <span>A</span><span>[</span><span>p</span> <span>*</span> <span>M</span> <span>+</span> <span>i</span><span>];</span>
      <span>blockA_packed</span><span>++</span><span>;</span>
    <span>}</span>
    <span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>m</span><span>;</span> <span>i</span> <span>&lt;</span> <span>MR</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
      <span>*</span><span>blockA_packed</span> <span>=</span> <span>0</span><span>.</span><span>0</span><span>;</span>
      <span>blockA_packed</span><span>++</span><span>;</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</code></pre></div>
<p>These blocks with static shapes are then passed into the kernel, so that the rank-1 update inside the kernel can remain unchanged and be optimized during compilation time.</p>
<div><pre><code><span>void</span> <span>matmul_pack_mask</span><span>(</span><span>float</span><span>*</span> <span>A</span><span>,</span> <span>float</span><span>*</span> <span>B</span><span>,</span> <span>float</span><span>*</span> <span>C</span><span>,</span> <span>float</span><span>*</span> <span>blockA_packed</span><span>,</span>
                        <span>float</span><span>*</span> <span>blockB_packed</span><span>,</span> <span>const</span> <span>int</span> <span>M</span><span>,</span> <span>const</span> <span>int</span> <span>N</span><span>,</span>
                        <span>const</span> <span>int</span> <span>K</span><span>)</span> <span>{</span>
  <span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>M</span><span>;</span> <span>i</span> <span>+=</span> <span>MR</span><span>)</span> <span>{</span>
    <span>const</span> <span>int</span> <span>m</span> <span>=</span> <span>min</span><span>(</span><span>MR</span><span>,</span> <span>M</span> <span>-</span> <span>i</span><span>);</span>
    <span>pack_blockA</span><span>(</span><span>&amp;</span><span>A</span><span>[</span><span>i</span><span>],</span> <span>blockA_packed</span><span>,</span> <span>m</span><span>,</span> <span>M</span><span>,</span> <span>K</span><span>);</span>
    <span>for</span> <span>(</span><span>int</span> <span>j</span> <span>=</span> <span>0</span><span>;</span> <span>j</span> <span>&lt;</span> <span>N</span><span>;</span> <span>j</span> <span>+=</span> <span>NR</span><span>)</span> <span>{</span>
      <span>const</span> <span>int</span> <span>n</span> <span>=</span> <span>min</span><span>(</span><span>NR</span><span>,</span> <span>N</span> <span>-</span> <span>j</span><span>);</span>
      <span>pack_blockB</span><span>(</span><span>&amp;</span><span>B</span><span>[</span><span>j</span> <span>*</span> <span>K</span><span>],</span> <span>blockB_packed</span><span>,</span> <span>n</span><span>,</span> <span>N</span><span>,</span> <span>K</span><span>);</span>
      <span>kernel_16x6</span><span>(</span><span>blockA_packed</span><span>,</span> <span>blockB_packed</span><span>,</span> <span>&amp;</span><span>C</span><span>[</span><span>j</span> <span>*</span> <span>M</span> <span>+</span> <span>i</span><span>],</span> <span>m</span><span>,</span> <span>n</span><span>,</span> <span>M</span><span>,</span> <span>N</span><span>,</span> <span>K</span><span>);</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<p>The new implementation <code>matmul_cache.c</code> achieves ‚Äúonly‚Äù 56 GFLOPS on my machine:</p>
<div><pre><code>clang-17 <span>-O2</span> <span>-mno-avx512f</span> <span>-march</span><span>=</span>native <span>-DTEST</span> <span>-DNITER</span><span>=</span>100 matmul_pack_mask.c <span>-o</span> matmul_pack_mask.out <span>&amp;&amp;</span> ./matmul_pack_mask.out
</code></pre></div>
<p>We see roughly a 2.6x decrease in performance, mostly because of frequently copying large $K$ dimensional sub-matrices of $A$ and $B$ from main memory. For each $m_R \times K$ sub-matrix of $A$ the entire(!) matrix $B$ is copied. Let‚Äôs optimize data reuse and cache management to finally achieve numpy‚Äôs level of performance for arbitrary matrix sizes.</p>

<h2 id="caching">Caching</h2>

<p>Recall the CPU‚Äôs memory system diagram. Initially, we‚Äôve ignored the intermediate layer between main-memory (DRAM) and the CPU‚Äôs registers - the CPU Cache.</p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/cpu_mem.png" alt="" width="70%"></p>

<p>Unlike DRAM, the cache is on-chip memory used to store frequently and recently accessed data from main memory. This minimizes data transfers between main memory and registers. Although faster than DRAM, the cache has limited capacity. CPUs typically employ a multi-level cache hierarchy for efficient data access. Levels like L1, L2, and L3 offer progressively larger capacities but slower access times, with L1 being the fastest and closest to the core.</p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/cpu_arch.png" alt=""></p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/core_arch.png" alt="">
<em>Intel Core i9-13900K labelled die shot. Source: <a href="https://www.youtube.com/watch?v=dX9CGRZwD-w">How are Microchips Made?</a></em></p>

<p>To enhance access speed, CPUs transfer data between main memory and cache in fixed-size chunks called <strong>cache lines</strong> or <strong>cache blocks</strong>. When a cache line is transferred, a corresponding cache entry is created to store it. On Ryzen 7700, the cache line size is <a href="https://en.wikichip.org/wiki/amd/microarchitectures/zen_4#Memory_Hierarchy">64 bytes</a>. The cache takes advantage of how we typically access data. When a single floating-point number from a continuous array in memory is requested, the cache cleverly grabs the next 15 floats along the way and stores them as well. This is why reading data sequentially from a contiguous array is much faster than jumping around to random memory locations. When the processor needs to read or write to a memory location, it first checks the cache for a corresponding entry. If the processor finds the memory location in the cache, a <strong>cache hit</strong> occurs. However, if the memory location is not found in the cache, a <strong>cache miss</strong> occurs. In the case of a cache miss, the cache allocates a new entry and copies the data from main memory. If the cache is full, a <a href="https://en.wikipedia.org/wiki/Cache_replacement_policies">cache replacement policy</a> kicks in to determine which data gets evicted to make room for new information. Several cache replacement policies exist, with LRU (Least Recently Used), LFU (Least Frequently Used), and LFRU (Least Frequently Recently Used) being the most widely used.</p>

<p>Similar to registers, once data is loaded into the cache, we want to reuse the data as much as possible to reduce main memory accesses. Given the cache‚Äôs limited capacity, storing entire input matrices input matrices $C, B, A$  in the cache isn‚Äôt feasible. Instead, we divide them into smaller blocks, load these blocks into the cache, and reuse them for rank-1 updates. This technique is often referred to as <strong>tiling</strong> or <strong>cache blocking</strong>, allowing us to handle matrices of arbitrary size effectively.</p>

<p>The final single-threaded matrix multiplication implementation, including the cache blocking, can be visualized as shown in the image borrowed from the official <a href="https://github.com/flame/blis/blob/master/docs/Multithreading.md">BLIS repository</a>:</p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/blis_design.png" alt=""></p>

<p>Let‚Äôs step through the diagram and discuss it.
In the outer-most loop (5th loop) we iterate over dimension $N$, dividing matrix $C$ into blocks $C_j$ of size $M \times n_c$  and matrix $B$  into blocks $B_j$ of size $K \times n_c$. The subscript $c$ in $n_c$ stands for <em>cache</em>.
In the 4th loop we iterate over dimension $K$ and divide matrix $A$ into $A_j$ of size $M \times k_c$  and $B_j$ into $B_p$ of size $k_c \times n_c$. Notice $B_p$ has fixed, limited size and can now be loaded into the cache. $B_p$ is packed into $\tilde{B}_p$, padded with zeros, if necessary, and loaded into the L3 cache. I
In the 3rd loop we iterate over dimension $M$ and divide $C_j$ into $C_i$ (there is a typo in the diagram) of size $m_c \times n_c$ and $A_p$  into $A_j$ of size $m_c \times k_c$. Matrix $A_j$ is now restricted in size and can be loaded entirely into the L2 cache. $A_j$ is packed into $\tilde{A}_j$ and padded with zeros if needed. Note how we reuse the same $\tilde{B}_p$ block from the L3 cache for different $A_j$ blocks. Both $m_c$ and $n_c$ are chosen to be a multiple of $m_r$ and $n_r$ respectively.</p>

<p>In the last two loops we simply iterate over cached blocks and divide them into $m_R \times k_c$ and $k_c \times n_R$ panels. These panels are then passed to the kernel to perform rank-1 updates on the $m_R \times n_R$ sub-matrix of $C$, similarly to what we have already done in the previous chapter. Each panel of $\tilde{B}_p$ is loaded into the L1 cache and reused for multiple panels of $\tilde{A}_j$.
Keep in mind that $\tilde{A}_j$ and $\tilde{B}_p$ are packed differently. During rank-1 updates we sequentially read a panel of $\tilde{A}_j$ column by column and a panel of $\tilde{B}_p$ row by row. Thus,  each panel inside $\tilde{A}_j$ is stored in column-major order, while each panel inside $\tilde{B}_p$ is stored in row-major order.</p>

<p>Different CPU models have varying cache sizes. To achieve peak performance, it‚Äôs crucial to optimize three key parameters: cache sizes for L1, L2, and L3 cashes (represented by $k_c$‚Äã, $m_c$‚Äã, and $n_c$‚Äã respectively). Theoretically, these parameters should be chosen so that:</p>

<ul>
  <li>The matrix $k_c‚Äã \times n_c$‚Äã fills the entire L3 cache.</li>
  <li>The matrix $m_c‚Äã \times k_c‚Äã$ fills the entire L2 cache.</li>
  <li>The matrix $k_c‚Äã \times n_R$‚Äã fills the entire L1 cache.</li>
</ul>

<p>While these values provide a good starting point, using larger values often leads to better performance in practice. Unfortunately (or fortunately), we cannot manually place data into the cache or control which cache levels store the data; the CPU manages this automatically using cache replacement policies. Therefore, cache blocking and cache reuse must be implemented at the algorithm level through, for example, well-designed loops and strategic data access patterns.</p>

<p>The implementation straightforwardly follows the algorithm depicted in the diagram:</p>
<div><pre><code><span>void</span> <span>matmul_cache</span><span>(</span><span>float</span><span>*</span> <span>A</span><span>,</span> <span>float</span><span>*</span> <span>B</span><span>,</span> <span>float</span><span>*</span> <span>C</span><span>,</span> <span>const</span> <span>int</span> <span>M</span><span>,</span> <span>const</span> <span>int</span> <span>N</span><span>,</span>
                  <span>const</span> <span>int</span> <span>K</span><span>)</span> <span>{</span>
  <span>for</span> <span>(</span><span>int</span> <span>j</span> <span>=</span> <span>0</span><span>;</span> <span>j</span> <span>&lt;</span> <span>N</span><span>;</span> <span>j</span> <span>+=</span> <span>NC</span><span>)</span> <span>{</span> <span>// 5th loop</span>
    <span>const</span> <span>int</span> <span>nb</span> <span>=</span> <span>min</span><span>(</span><span>NC</span><span>,</span> <span>N</span> <span>-</span> <span>j</span><span>);</span>
    <span>for</span> <span>(</span><span>int</span> <span>p</span> <span>=</span> <span>0</span><span>;</span> <span>p</span> <span>&lt;</span> <span>K</span><span>;</span> <span>p</span> <span>+=</span> <span>KC</span><span>)</span> <span>{</span> <span>// 4th loop</span>
      <span>const</span> <span>int</span> <span>kb</span> <span>=</span> <span>min</span><span>(</span><span>KC</span><span>,</span> <span>K</span> <span>-</span> <span>p</span><span>);</span>
      <span>pack_blockB</span><span>(</span><span>&amp;</span><span>B</span><span>[</span><span>j</span> <span>*</span> <span>K</span> <span>+</span> <span>p</span><span>],</span> <span>blockB_packed</span><span>,</span> <span>nb</span><span>,</span> <span>kb</span><span>,</span> <span>K</span><span>);</span>
      <span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>M</span><span>;</span> <span>i</span> <span>+=</span> <span>MC</span><span>)</span> <span>{</span> <span>// 3rd loop</span>
        <span>const</span> <span>int</span> <span>mb</span> <span>=</span> <span>min</span><span>(</span><span>MC</span><span>,</span> <span>M</span> <span>-</span> <span>i</span><span>);</span>
        <span>pack_blockA</span><span>(</span><span>&amp;</span><span>A</span><span>[</span><span>p</span> <span>*</span> <span>M</span> <span>+</span> <span>i</span><span>],</span> <span>blockA_packed</span><span>,</span> <span>mb</span><span>,</span> <span>kb</span><span>,</span> <span>M</span><span>);</span>
        <span>for</span> <span>(</span><span>int</span> <span>jr</span> <span>=</span> <span>0</span><span>;</span> <span>jr</span> <span>&lt;</span> <span>nb</span><span>;</span> <span>jr</span> <span>+=</span> <span>NR</span><span>)</span> <span>{</span> <span>// 2nd loop</span>
          <span>const</span> <span>int</span> <span>nr</span> <span>=</span> <span>min</span><span>(</span><span>NR</span><span>,</span> <span>nb</span> <span>-</span> <span>jr</span><span>);</span>
          <span>for</span> <span>(</span><span>int</span> <span>ir</span> <span>=</span> <span>0</span><span>;</span> <span>ir</span> <span>&lt;</span> <span>mb</span><span>;</span> <span>ir</span> <span>+=</span> <span>MR</span><span>)</span> <span>{</span> <span>// 1st loop</span>
            <span>const</span> <span>int</span> <span>mr</span> <span>=</span> <span>min</span><span>(</span><span>MR</span><span>,</span> <span>mb</span> <span>-</span> <span>ir</span><span>);</span>
            <span>kernel_16x6</span><span>(</span><span>&amp;</span><span>blockA_packed</span><span>[</span><span>ir</span> <span>*</span> <span>kb</span><span>],</span> <span>&amp;</span><span>blockB_packed</span><span>[</span><span>jr</span> <span>*</span> <span>kb</span><span>],</span>
                        <span>&amp;</span><span>C</span><span>[(</span><span>j</span> <span>+</span> <span>jr</span><span>)</span> <span>*</span> <span>M</span> <span>+</span> <span>(</span><span>i</span> <span>+</span> <span>ir</span><span>)],</span> <span>mr</span><span>,</span> <span>nr</span><span>,</span> <span>kb</span><span>,</span> <span>M</span><span>);</span>
          <span>}</span>
        <span>}</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</code></pre></div>
<p>Before implementing the multi-threaded version of the algorithm, let‚Äôs benchmark our current implementation and compare it against numpy:</p>
<div><pre><code>python benchmark_numpy.py <span>-ST</span>

clang-17 <span>-O2</span> <span>-mno-avx512f</span> <span>-march</span><span>=</span>native benchmark_st.c <span>-o</span> benchmark_st.out <span>&amp;&amp;</span> ./benchmark_st.out

python plot_benchmark.py
</code></pre></div>

<p><img src="https://salykova.github.io/assets/matmul_cpu/benchmark_st.png" alt=""></p>

<h2 id="multithreading">Multithreading</h2>

<p>There are indeed many loops that can be potentially parallelized. To achieve high-performance, we want to parallelize both packing and arithmetic operations. Let‚Äôs start with the arithmetic operations. The 5th, 4th, 3rd loops around the micro-kernel iterate over matrix dimensions in chunks of cache block sizes $n_c$, $k_c$, $m_c$. To efficiently parallelize the loops and keep all threads busy, we want number of iterations (=matrix dimension / cache block size) to be at least = number of threads (generally, the more the better). In other words, the input matrix dimension should be at least = number of threads  * cache block size. As we discussed earlier, we also want cache blocks to fully occupy the corresponding cache levels. On modern CPUs, this second requirement results in cache block sizes of thousand(s) of elements. For example, on my Ryzen 7700, cache block sizes of $n_c=1535$, $m_c=1024, k_c=2000$ attain the best performance in the single-threaded case. Given the number of available threads on Ryzen 7700, $Nthreads=16$, we need input matrices with dimensions of at least $2000 \times 16$ to be able to distribute the work over all threads.</p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/blis_design.png" alt=""></p>

<p>In contrast, the last two loops iterate over cache blocks, dividing them into $m_r, n_r$ blocks. Since $n_r, m_r$ are typically very small (&lt;20), these loops are ideal candidates for parallelization. Moreover, we can choose $m_c, n_c$ to be multiples of $Nthreads$ so that the work is evenly distributed across all threads.</p>

<p>On my machine, parallelizing the second loop results in much better performance compared to the first loop (possibly due to large $n_c$ and little work in each iteration of the first loop). We will therefore parallelize the second loop using OpenMP directives (more on OpenMP <a href="https://ppc.cs.aalto.fi/ch2/openmp/">here</a>, <a href="https://ppc.cs.aalto.fi/ch3/">here</a> and <a href="https://curc.readthedocs.io/en/latest/programming/OpenMP-C.html">here</a>):</p>
<div><pre><code><span>#pragma omp parallel for num_threads(NTHREADS) schedule(static)
</span>  <span>for</span> <span>(</span><span>int</span> <span>jr</span> <span>=</span> <span>0</span><span>;</span> <span>jr</span> <span>&lt;</span> <span>nb</span><span>;</span> <span>jr</span> <span>+=</span> <span>NR</span><span>)</span>
</code></pre></div>
<blockquote>
  <p>It‚Äôs also possible to parallelize the 2nd and 1st loops using <code>#pragma omp parallel for collapse(2)</code>, which leads to similar performance when parallelizing only the 2nd loop.</p>
</blockquote>

<p>Together with arithmetic operations, we also want to accelerate the packing of both $\tilde{A}$ and $\tilde{B}$:</p>
<div><pre><code><span>void</span> <span>pack_blockA</span><span>(</span><span>float</span><span>*</span> <span>A</span><span>,</span> <span>float</span><span>*</span> <span>blockA_packed</span><span>,</span> <span>const</span> <span>int</span> <span>mb</span><span>,</span> <span>const</span> <span>int</span> <span>kb</span><span>,</span> <span>const</span> <span>int</span> <span>M</span><span>)</span>
<span>#pragma omp parallel for num_threads(NTHREADS) schedule(static)
</span>  <span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>mb</span><span>;</span> <span>i</span> <span>+=</span> <span>MR</span><span>)</span>
</code></pre></div>

<div><pre><code><span>void</span> <span>pack_blockB</span><span>(</span><span>float</span><span>*</span> <span>B</span><span>,</span> <span>float</span><span>*</span> <span>blockB_packed</span><span>,</span> <span>const</span> <span>int</span> <span>nb</span><span>,</span> <span>const</span> <span>int</span> <span>kb</span><span>,</span> <span>const</span> <span>int</span> <span>K</span><span>)</span>
<span>#pragma omp parallel for num_threads(NTHREADS) schedule(static)
</span>  <span>for</span> <span>(</span><span>int</span> <span>j</span> <span>=</span> <span>0</span><span>;</span> <span>j</span> <span>&lt;</span> <span>nb</span><span>;</span> <span>j</span> <span>+=</span> <span>NR</span><span>)</span>
</code></pre></div>
<p>Similar to arithmetic operations, the packing loops can be easily parallelized due to the high number of iterations and the flexibility of choosing  $m_c, k_c, n_c$.</p>

<p>Running</p>
<div><pre><code>clang-17 <span>-O2</span> <span>-mno-avx512f</span> <span>-march</span><span>=</span>native <span>-DNITER</span><span>=</span>100 <span>-fopenmp</span> matmul_parallel.c <span>-o</span> matmul_parallel.out <span>&amp;&amp;</span> ./matmul_parallel.out
</code></pre></div>
<p>shows around 1 TFLOPS. Don‚Äôt forget to add the <code>-fopenmp</code> compiler flag to use OpenMP directives. You might also need to install <code>libomp-dev</code> with <code>sudo apt install libomp-dev</code>.</p>

<p>Let‚Äôs check the CPU utilization</p>

<p><img src="https://salykova.github.io/assets/matmul_cpu/htop.png" alt=""></p>

<p>and benchmark the multithreading implementation:</p>
<div><pre><code>python benchmark_numpy.py
clang-17 <span>-O2</span> <span>-mno-avx512f</span> <span>-march</span><span>=</span>native <span>-fopenmp</span> benchmark_mt.c <span>-o</span> benchmark_mt.out <span>&amp;&amp;</span> ./benchmark_mt.out
python plot_benchmark.py
</code></pre></div>
<p><img src="https://salykova.github.io/assets/matmul_cpu/benchmark_mt.png" alt=""></p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Joy of Reading Books You Don't Understand (229 pts)]]></title>
            <link>https://reactormag.com/the-joy-of-reading-books-you-dont-entirely-understand/</link>
            <guid>40870280</guid>
            <pubDate>Wed, 03 Jul 2024 21:44:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reactormag.com/the-joy-of-reading-books-you-dont-entirely-understand/">https://reactormag.com/the-joy-of-reading-books-you-dont-entirely-understand/</a>, See on <a href="https://news.ycombinator.com/item?id=40870280">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>At present, I have an alarming number of tabs open. I‚Äôm absolutely not going to tell you how many, or how many are open on my phone. There are 15 pages of notes in my now-finished notebook that are about the same subject that led to all these tabs. A lot of these tabs concern the history of a country I don‚Äôt live in. Some are mythology. It‚Äôs a real cornucopia of delights, and it‚Äôs also very distracting. There are so many rich and fascinating rabbit holes a person might fall down.&nbsp;</p>
<p>This is all because I‚Äôve been reading a book that I don‚Äôt entirely understand, and frankly, it‚Äôs wonderful.</p>
<p>A very long time ago, I read <a href="https://bookshop.org/p/books/quicksilver-volume-one-of-the-baroque-cycle-neal-stephenson/1517020?ean=9780060593087" target="_blank" rel="noreferrer noopener" title=" opens in a new window">Neal Stephenson‚Äôs Baroque Cycle</a>, one at a time, as the books came out. I am‚ÄîI cannot stress this enough‚Äî<em>very</em> bad at remembering historical details. Part of this I blame on high school. Part of this is just the way my brain works. I can tell you the basic plot of most books I‚Äôve ever read, but I cannot tell you the names and dates involved with specific moments in the world‚Äôs past. While I read Stephenson‚Äôs sprawling series, I spent a lot of time referencing the encyclopedia, because I did not know, necessarily, which characters were based on real humans and which were entirely made up. It was really quite educational. (I also learned about kidney stones, which was less pleasant. But still kind of interesting.)</p>
<p>I could have just let it go, let the books roll me along in blissful ignorance. I understood the story structure and the characters just fine. I knew what he was getting at. It was just all that history that kept throwing me: Who? When? <em>Why</em>? But what happened, as I looked up names and places and dates and wars, is that I began to take almost as much joy in that process as I did in reading the books. The two things remain twined in my head, all these years later, and maybe some part of me is always looking for something else like that‚Äîsomething that will offer me a book, a story to read and inhabit, but also an adventure in not-knowing.</p>
<p>In recent years, I feel like it has been less common to find books to challenge me, and by me I mean their readers, and by ‚Äúbooks‚Äù what I really mean is ‚Äúpublishing,‚Äù which can feel very focused on the sure thing, the brand name, the splashy debut that somehow speaks to millions and millions of people. Still, there are challenging, mystifying, weird-ass books being published all the time. To be fair, a weird-ass, mystifying, challenging book isn‚Äôt inherently a good book, or a book you want to spend your finite reading time on. We only get to read so many books in a month, or a year, or a life. There is value in escapism and familiarity and comfort.</p>
<p>But I still want to advocate for <em>sometimes</em>, at least sometimes, going out on a limb, out on a genre vacation, or just out into the wilds of a tale you don‚Äôt feel like you entirely understand.&nbsp;</p>
<p>It can feel, too often, like these books bobble and vanish in the big world of Book Discourse. I have searched weird corners of the internet for people talking about <a href="https://bookshop.org/p/books/the-library-of-broken-worlds/18383203?ean=9781338290622" target="_blank" rel="noreferrer noopener" title=" opens in a new window">Alaya Dawn Johnson‚Äôs <em>The Library of Broken Worlds</em></a>, which requires patience, and a willingness to trust her incredible, vivid, dizzying worldbuilding. I think sometimes about how many books there are that American, English-language readers will never get to see, simply because they were too <em>something</em> to get translated here. I think about how lucky we are that Riverhead keeps publishing the great and unmatched <a href="https://bookshop.org/contributors/helen-oyeyemi-7311fcee-faa8-4cb9-a4b1-f8ae71227fae" target="_blank" rel="noreferrer noopener" title=" opens in a new window">Helen Oyeyemi</a>, whose books are works of art that I can‚Äôt ever quite fit my head around‚Äîwhich is as it should be, for there is always something else to find in them. I think about how lucky we are that we get to read trippy and furious books like <a href="https://bookshop.org/p/books/jonathan-abernathy-you-are-kind-molly-mcghee/19674051?ean=9781662602115" target="_blank" rel="noreferrer noopener" title=" opens in a new window">Molly McGhee‚Äôs <em>Jonathan Abernathy You Are Kind</em></a>, which is both deceptively easy to read and hard to fully fathom. Or perhaps what‚Äôs ‚Äúhard‚Äù about it is that it‚Äôs hard to accept exactly how clearly it speaks to this moment in time.&nbsp;</p>
<p>McGhee‚Äôs Twitter bio used to say something about how literary and genre fiction ought to touch tongues more often, and I think about that, too: About the science fiction and fantasy that appears in the other section of bookstore, about all the SFF writers overlooked by the mainstream even as their prose is crystalline, elegant, looping, rich, just stunning. We build so many walls for ourselves about what we do and don‚Äôt do, read and don‚Äôt read. Some of it is simply practical: We‚Äôre back to the question of time, and how much of it we do or don‚Äôt have. When someone says ‚ÄúI am only reading X kinds of books,‚Äù they are drawing boundaries around their time as much as their taste.</p>
<p>I want, though, for us to have the time, the space, the mental bandwidth to welcome uncertainty, to crank up our curiosity and give the weird or confusing or just slightly unexpected books a chance. And I want it to be totally okay and acceptable and normal to say ‚ÄúI don‚Äôt entirely understand what I just read, but I loved it.‚Äù</p>
<p>When I started writing reviews, in the mid-2000s, there was a real pressure to be authoritative. To speak with your whole chest, even if you didn‚Äôt really know what you were on about. I‚Äôve always been a little suspicious of this tendency‚Äîof an unwillingness to be transparent about the fact that every reader (and writer!) is coming from their own specific background and none of us knows everything about everything. Subjectivity is inevitable.&nbsp;</p>
<p>Maybe, just maybe, this requirement that we all pretend to know what we‚Äôre talking about at all times is a limiting thing. On today‚Äôs bookternet, a lot of us can go off about tropes and western story structure and the hero‚Äôs journey and probably also several other kinds of story structure we read about once or twice and maybe even there‚Äôs some of that Save the Cat guy baked in there, too. So it‚Äôs easy, in a way, to keep reading books from this sort of narrative tradition, because we know a bit of what we‚Äôre talking about. I can pick up a retelling of a Greek myth and know the basic beats because I grew up steeped in those stories.</p>
<p>But there are so many other stories, and so many other ways to tell them.</p>
<p>What set me off on this path of delirious not-knowing is that I read <a href="https://bookshop.org/p/books/rakesfall-vajra-chandrasekera/20464409?ean=9781250847683" target="_blank" rel="noreferrer noopener" title=" opens in a new window">Vajra Chandrasekera‚Äôs <em>Rakesfall</em></a>. I read it on a plane, and I felt, later, like I dreamed it. Whole scenes existed in my mind stripped of any context, the way you might remember dreams.</p>
<p>And then I read it again, with a pen and a notebook and my phone and laptop at hand. I opened a million tabs, and revisited the general outline of the <em>Ramayana</em>, which I know as a Penguin Classic I read in book group some years back, not at all the way I know the stories and myths I met in textbooks as a child. I put off drafting a review of the book in favor of reading every interview with the author I could find. I put pieces together and, outside of my airplane dream-state, began to see where the story restarted, where it looped, where it ate its own tail and then birthed itself again.</p>
<p>There is so much I don‚Äôt entirely understand in this book, because I <em>can‚Äôt</em>; I‚Äôm a white American who does not have the cultural context to fully understand all the things that this story encompasses. And what I‚Äôm saying is: Good. Good, let me bask in that. Good, let me <em>admit</em> to that.</p>
<p>There is real joy to be found in not immediately understanding exactly what a book is doing. Joy in seeing that something outside of the narrative structure we‚Äôre familiar with is at play; joy in discovering a different sense of vastness and fluidity. Joy in waiting, patiently, with rich anticipation, for the seemingly disparate pieces of a narrative to mesh, to become something huge and beautiful. Joy in realizing, several chapters into a book, that you could not possibly say what it was ‚Äúabout‚Äù until reading to the end, and maybe not even then.<svg width="31" height="13" viewBox="0 0 31 13" fill="none" xmlns="http://www.w3.org/2000/svg" aria-labelledby="icon-paragraph-end">
<title id="icon-paragraph-end">icon-paragraph-end</title>
<path d="M15.0891 10.4122C15.138 10.2591 15.186 10.128 15.221 9.99444C15.256 9.86009 15.278 9.72248 15.3114 9.55963C15.1217 9.57836 14.9621 9.59546 14.8025 9.6093C13.871 9.69154 12.9387 9.73796 12.0039 9.68096C11.8313 9.67037 11.7222 9.7119 11.6261 9.86824C11.5463 9.9977 11.4177 10.1068 11.2906 10.1948C11.1498 10.2925 10.9853 10.2794 10.82 10.2241C10.7484 10.2004 10.6555 10.198 10.5847 10.2216C10.3722 10.2933 10.1645 10.3804 9.9569 10.4651C9.50336 10.6491 8.98712 10.4521 8.82427 10.1093C8.80799 10.0759 8.73389 10.0555 8.68667 10.0547C8.62234 10.0547 8.55801 10.0816 8.49369 10.093C8.2543 10.1361 8.06865 10.0457 7.9343 9.85195C7.77877 9.62722 7.73643 9.37398 7.77959 9.10772C7.8032 8.96523 7.8944 8.87811 8.03282 8.83169C8.20137 8.77551 8.20056 8.76492 8.16392 8.59393C8.14437 8.50355 8.12972 8.41154 8.12646 8.31953C8.11669 8.0093 8.22906 7.83993 8.5173 7.73653C8.86906 7.6095 9.22407 7.49144 9.57583 7.3636C9.63364 7.34243 9.69389 7.3009 9.72972 7.25205C9.82173 7.12665 9.94224 7.06884 10.0855 7.11525C10.3233 7.19261 10.4446 7.13072 10.5814 6.9011C10.7076 6.69021 10.6881 6.54283 10.5936 6.35963C10.5309 6.2383 10.4813 6.10884 10.4056 5.99647C10.3176 5.86782 10.2215 5.73754 10.1051 5.63657C9.65562 5.24736 9.19639 4.86955 8.73959 4.48848C8.67934 4.43799 8.61501 4.39239 8.55231 4.34435C8.56127 4.32888 8.56941 4.31423 8.57837 4.29876C8.64595 4.27107 8.71354 4.2442 8.7803 4.21652C10.3396 3.57407 11.8973 2.92756 13.4582 2.29163C14.2203 1.98058 14.9898 1.68338 15.8024 1.52704C16.2519 1.44073 16.7013 1.40165 17.1427 1.58567C17.1809 1.60195 17.2501 1.57264 17.2925 1.54577C17.9406 1.14272 18.601 0.75839 19.3208 0.500273C20.372 0.123274 21.4574 -0.0859885 22.577 0.0337065C23.4971 0.132231 24.3333 0.491316 25.0759 1.03686C25.4497 1.31127 25.7745 1.65325 26.1198 1.96755C26.2338 2.07096 26.347 2.17682 26.4512 2.29C26.6955 2.55707 27.013 2.63035 27.3444 2.5758C28.1522 2.44308 28.955 2.28104 29.7603 2.13203C29.8686 2.11168 29.9769 2.09295 30.0868 2.08236C30.1186 2.07911 30.1536 2.10679 30.1878 2.12063C30.1699 2.1532 30.1609 2.20124 30.1332 2.21671C30.042 2.26883 29.9468 2.31687 29.8474 2.35269C29.2221 2.57743 28.5943 2.79646 27.9681 3.02038C27.8232 3.07249 27.6807 3.13112 27.5382 3.18975C27.2768 3.29723 27.0961 3.48288 26.9853 3.74506C26.4276 5.05845 25.5384 6.09092 24.3341 6.85143C24.1257 6.98253 23.9034 7.09082 23.6892 7.21378C23.6469 7.2382 23.5915 7.27566 23.5818 7.31637C23.493 7.68686 23.2553 7.95393 22.9866 8.20146C22.6641 8.49948 22.2668 8.67129 21.8816 8.86671C20.946 9.34141 19.9356 9.55475 18.9137 9.73551C17.7607 9.93907 16.6085 10.1516 15.4564 10.3592C15.3407 10.3804 15.2243 10.3934 15.0883 10.4138L15.0891 10.4122ZM23.6819 1.54414C23.6078 1.53193 23.55 1.51646 23.4906 1.51239C22.5925 1.45132 21.7041 1.52134 20.8247 1.71269C19.8883 1.91626 18.9764 2.19636 18.0913 2.56359C18.0497 2.58069 17.9602 2.57743 17.9488 2.55463C17.9105 2.48216 17.9463 2.42028 18.0253 2.38852C18.0457 2.38038 18.0652 2.37061 18.0856 2.36084C18.93 1.97244 19.8118 1.70048 20.7197 1.51076C20.955 1.4619 21.1936 1.42445 21.4509 1.37804C21.1667 0.944854 21.3133 0.576812 21.6235 0.234827C21.582 0.221799 21.5698 0.216099 21.5575 0.215285C21.5266 0.213656 21.4957 0.212842 21.4655 0.21447C20.854 0.238898 20.2588 0.354522 19.6799 0.549128C18.9747 0.786075 18.3079 1.11096 17.6809 1.50424C16.8381 2.03351 16.0182 2.59778 15.1901 3.14985C15.1323 3.18812 15.0867 3.24593 15.0134 3.31514C15.072 3.31107 15.0932 3.31025 15.1144 3.307C15.5679 3.24104 16.019 3.1645 16.4742 3.10995C17.4334 2.99514 18.3942 2.90639 19.3607 3.01712C19.8843 3.07656 20.3972 3.17672 20.8646 3.43728C20.9151 3.46496 20.9607 3.5016 21.025 3.54639C20.9656 3.56756 20.9338 3.57977 20.9013 3.59117C20.0951 3.85906 19.2833 4.11148 18.4846 4.39891C16.6191 5.06904 14.7341 5.67647 12.8206 6.19433C12.6097 6.25133 12.4005 6.31566 12.1863 6.37754C12.1383 6.27087 12.1008 6.17235 12.0503 6.08034C11.9876 5.96716 11.9168 5.95169 11.8118 6.02578C11.7116 6.09744 11.6155 6.17479 11.5203 6.25377C11.197 6.52248 10.8843 6.80502 10.6897 7.18446C10.6164 7.32777 10.5822 7.49958 10.5692 7.66161C10.557 7.81144 10.6083 7.82935 10.7451 7.76502C11.0366 7.62904 11.3257 7.48818 11.6188 7.35708C13.8075 6.37835 16.0613 5.56655 18.329 4.79382C19.4429 4.41438 20.5699 4.07809 21.7318 3.88023C21.8767 3.8558 22.029 3.86476 22.1772 3.87534C22.2195 3.8786 22.257 3.94211 22.2969 3.97875C22.2529 4.00237 22.2122 4.03982 22.1658 4.04715C21.025 4.23524 19.9071 4.51453 18.8078 4.87117C16.6997 5.55515 14.6217 6.31891 12.5723 7.16166C11.9616 7.41245 11.3615 7.69011 10.7557 7.95312C10.649 7.99953 10.6409 8.06385 10.6783 8.16401C10.7207 8.278 10.7825 8.23566 10.8534 8.20553C11.5764 7.90182 12.2897 7.5753 13.0242 7.30171C14.6584 6.69428 16.2983 6.10069 17.9447 5.52746C19.5105 4.98273 21.1146 4.57153 22.7439 4.26619C22.8253 4.25071 22.8384 4.21977 22.8408 4.14649C22.8489 3.8558 22.8498 3.5643 22.8823 3.27606C22.9515 2.65234 23.1616 2.08643 23.6184 1.63371C23.6388 1.61335 23.6526 1.58567 23.6803 1.54414H23.6819ZM14.5713 8.99373C14.5761 9.01164 14.581 9.03037 14.5859 9.04828C14.6315 9.05154 14.6779 9.05887 14.7235 9.05643C14.9043 9.04991 15.0842 9.04096 15.265 9.03119C16.4001 8.97012 17.5351 8.90986 18.6588 8.72747C19.0936 8.65663 19.526 8.57358 19.9144 8.34477C20.3223 8.10457 20.6871 7.81876 20.9281 7.40187C20.9867 7.3009 21.012 7.18121 21.0527 7.06965C21.0397 7.06151 21.0275 7.05337 21.0144 7.04523C20.9298 7.06558 20.8443 7.08187 20.7612 7.10711C19.6888 7.42385 18.6173 7.74141 17.5457 8.0606C16.6769 8.31871 15.8073 8.57765 14.9401 8.83984C14.8131 8.87811 14.6934 8.94243 14.5713 8.99373ZM26.0343 2.38526C25.5368 2.05305 25.01 1.69722 24.3455 1.63208C24.0605 1.6044 23.8236 1.68419 23.6371 1.90079C23.3456 2.24033 23.1877 2.64257 23.1014 3.07575C23.0769 3.19789 23.1062 3.21906 23.2276 3.17997C24.0361 2.92104 24.8683 2.83147 25.7135 2.83717C25.8079 2.83717 25.9016 2.83717 25.9708 2.83717C25.9903 2.69387 26.0091 2.56114 26.0335 2.38526H26.0343ZM11.0521 5.29947C11.1473 5.26364 11.2288 5.23352 11.3094 5.20176C12.3956 4.77916 13.485 4.36552 14.6266 4.11229C14.7895 4.07646 14.9548 4.04389 15.1119 3.99097C15.1657 3.97306 15.2341 3.89244 15.2316 3.8444C15.2292 3.79718 15.1478 3.72145 15.0948 3.71575C14.9572 3.7011 14.8131 3.69865 14.6779 3.72471C13.9109 3.87534 13.1797 4.14323 12.4501 4.41519C12.0121 4.57886 11.5878 4.76939 11.1978 5.03077C11.1115 5.08858 11.0358 5.15046 11.0504 5.29866L11.0521 5.29947ZM25.8625 3.93967C25.8576 3.92909 25.8527 3.91769 25.8478 3.9071C25.7811 3.91524 25.711 3.91361 25.6483 3.93397C25.0205 4.13509 24.3944 4.3411 23.7666 4.54222C23.6705 4.57316 23.651 4.63504 23.6836 4.71402C23.7275 4.81906 23.7772 4.92247 23.8374 5.01937C23.9262 5.16268 23.9865 5.18548 24.1273 5.09754C24.6566 4.76614 25.1801 4.42741 25.7045 4.08786C25.764 4.04959 25.8104 3.99015 25.8625 3.94048V3.93967ZM9.18336 8.61347C8.81287 8.73724 8.45786 8.84961 8.10855 8.97663C7.98478 9.02141 7.94732 9.16798 7.98234 9.31699C8.00351 9.40655 8.05318 9.43587 8.13542 9.40818C8.50427 9.28116 8.87313 9.15169 9.25257 9.01897C9.22977 8.88706 9.2086 8.76004 9.18336 8.61347ZM9.56606 7.59077C9.20534 7.70965 8.8544 7.82365 8.5059 7.94172C8.40168 7.97673 8.33979 8.05245 8.33328 8.16726C8.32677 8.28777 8.36585 8.32197 8.47903 8.2837C8.76565 8.18518 9.05064 8.0834 9.33644 7.98243C9.51883 7.9181 9.57909 7.82039 9.56687 7.59077H9.56606ZM22.1047 0.752691C22.3026 0.702207 22.4874 0.665566 22.6641 0.605311C22.7219 0.585769 22.805 0.49783 22.7968 0.457117C22.7838 0.396048 22.7097 0.319509 22.647 0.303224C22.5053 0.265768 22.3539 0.242969 22.2073 0.247855C22.1202 0.251112 22.0225 0.302409 21.9525 0.360221C21.867 0.431875 21.8059 0.552385 21.8694 0.652538C21.9069 0.711978 22.0241 0.720935 22.1039 0.751876L22.1047 0.752691ZM9.76636 8.18843C9.82336 8.35454 9.88443 8.5638 9.96993 8.76248C9.98947 8.80727 10.0921 8.84472 10.1515 8.83821C10.2394 8.82925 10.2541 8.74375 10.2419 8.66233C10.2378 8.63627 10.2346 8.61022 10.2305 8.58416C10.1971 8.38141 10.1686 8.17785 10.1279 7.97673C10.1132 7.90263 10.0855 7.82202 10.0383 7.76502C9.96667 7.67871 9.88606 7.69581 9.85104 7.80248C9.81603 7.90996 9.80138 8.02396 9.76636 8.18843ZM26.6686 3.09773C26.7671 3.06924 26.8697 3.04969 26.9634 3.00817C27.0016 2.99107 27.0407 2.92837 27.0407 2.88684C27.0407 2.85672 26.9813 2.80868 26.9406 2.79972C26.7997 2.76878 26.6564 2.74435 26.5131 2.73295C26.4219 2.72562 26.3836 2.79728 26.382 2.88196C26.3779 3.03911 26.4325 3.07575 26.6694 3.09773H26.6686ZM23.208 1.14923C23.5028 1.19401 23.5663 1.16552 23.6469 0.964396C23.6762 0.891113 23.721 0.814574 23.6111 0.764904C23.5052 0.716049 23.3375 0.744548 23.3049 0.828416C23.2658 0.929383 23.2422 1.03524 23.2088 1.14923H23.208Z" fill="#CA3624"></path>
<path d="M7.43167 10.071C7.46669 10.0995 7.51473 10.1223 7.53101 10.1589C7.53834 10.176 7.49111 10.2363 7.45773 10.2493C7.21101 10.3438 6.96348 10.4342 6.7135 10.5197C4.56876 11.2517 2.42321 11.9821 0.27766 12.7124C0.219034 12.732 0.161222 12.7564 0.100967 12.7662C0.0692112 12.7711 0.0333843 12.7499 0 12.7401C0.0154708 12.7027 0.0211704 12.6522 0.048855 12.6302C0.087939 12.5984 0.143308 12.5846 0.192978 12.5675C1.97945 11.9584 3.76673 11.3502 5.55401 10.7411C6.07024 10.5653 6.58648 10.3886 7.1019 10.2086C7.20938 10.1712 7.31198 10.1207 7.43167 10.0702V10.071Z" fill="#CA3624"></path>
<path d="M0.537992 12.0546C0.528221 12.0196 0.509494 11.9846 0.511936 11.9504C0.514379 11.9162 0.527407 11.8673 0.552649 11.8527C0.603947 11.8217 0.666644 11.8079 0.724456 11.7875C2.77718 11.0832 4.8291 10.3781 6.88182 9.67374C6.93964 9.65339 6.99501 9.61593 7.05445 9.60942C7.10086 9.60453 7.15134 9.63466 7.2002 9.6485C7.16926 9.68758 7.14564 9.74703 7.10574 9.76168C6.67012 9.91639 6.23205 10.0638 5.79561 10.2144C4.11418 10.7942 2.43357 11.3739 0.752955 11.9536C0.699214 11.9724 0.644659 11.9895 0.590104 12.0074C0.573005 12.0229 0.555906 12.0383 0.538807 12.0546H0.537992Z" fill="#CA3624"></path>
<path d="M7.47241 8.79712C7.49277 8.8134 7.53755 8.83213 7.53755 8.85249C7.53755 8.88994 7.52452 8.94368 7.49684 8.96323C7.45205 8.9958 7.39261 9.00801 7.33806 9.02511C5.94569 9.48272 4.5525 9.94033 3.16013 10.3988C3.06405 10.4305 2.97041 10.4696 2.87433 10.4997C2.82059 10.5168 2.74161 10.5632 2.72695 10.4663C2.72125 10.4297 2.77906 10.3597 2.82141 10.3442C3.14385 10.2261 3.46874 10.1154 3.79444 10.0071C4.9637 9.6187 6.1346 9.23193 7.30468 8.84516C7.35434 8.82888 7.40564 8.81585 7.4716 8.79712H7.47241Z" fill="#CA3624"></path>
<path d="M8.55146 10.4764C8.50586 10.5098 8.46515 10.557 8.41385 10.5749C7.74046 10.8078 7.06545 11.0358 6.39125 11.2662C6.03786 11.3875 5.68529 11.5121 5.33191 11.6334C5.28712 11.6489 5.23827 11.6611 5.19186 11.6595C5.17394 11.6595 5.13649 11.6082 5.14219 11.5968C5.16091 11.5577 5.1886 11.5089 5.22443 11.495C5.42555 11.4177 5.62992 11.3485 5.83349 11.2776C6.60702 11.0114 7.38056 10.7459 8.1541 10.4788C8.23716 10.4503 8.31451 10.4023 8.39838 10.3836C8.44316 10.3738 8.4969 10.408 8.54657 10.4226C8.5482 10.4406 8.54983 10.4585 8.55146 10.4756V10.4764Z" fill="#CA3624"></path>
<path d="M4.88158 5.90776C4.86203 5.89391 4.82376 5.88089 4.81969 5.86053C4.81318 5.83122 4.82051 5.78399 4.84086 5.76689C4.87995 5.73351 4.93043 5.71071 4.97847 5.69279C5.96534 5.31661 6.95303 4.94205 7.9399 4.56668C7.94805 4.56343 7.957 4.5561 7.96433 4.55773C8.01563 4.56587 8.06611 4.57646 8.1166 4.58623C8.0881 4.63182 8.06937 4.70267 8.02866 4.71976C7.81858 4.81015 7.6028 4.88669 7.38866 4.9673C6.60616 5.26287 5.82448 5.55844 5.04198 5.85402C4.99313 5.87274 4.94264 5.88821 4.88076 5.90857L4.88158 5.90776Z" fill="#CA3624"></path>
<path d="M7.25558 5.73729C7.23197 5.721 7.18963 5.70472 7.19044 5.69006C7.19207 5.64609 7.19696 5.58747 7.22545 5.56222C7.26942 5.52233 7.33131 5.50115 7.38912 5.47998C7.7596 5.33993 8.1309 5.20151 8.5022 5.06472C8.56083 5.04355 8.64144 4.99306 8.66424 5.09077C8.67319 5.13067 8.61701 5.21454 8.57141 5.23327C8.28235 5.35133 7.98759 5.45556 7.69528 5.56629C7.55115 5.62085 7.40866 5.67785 7.25558 5.7381V5.73729Z" fill="#CA3624"></path>
<path d="M23.6822 1.54485C23.6537 1.58557 23.6406 1.61325 23.6203 1.63442C23.1635 2.08715 22.9534 2.65386 22.8842 3.27677C22.8524 3.56501 22.8516 3.85651 22.8427 4.1472C22.8402 4.22048 22.8272 4.25143 22.7458 4.2669C21.1156 4.57306 19.5124 4.98344 17.9466 5.52817C16.3002 6.10141 14.6602 6.69499 13.026 7.30243C12.2916 7.5752 11.5783 7.90253 10.8553 8.20625C10.7844 8.23556 10.7225 8.27871 10.6802 8.16472C10.6427 8.06375 10.6509 7.99942 10.7575 7.95383C11.3633 7.69082 11.9634 7.41398 12.5741 7.16237C14.6236 6.31962 16.7016 5.55586 18.8097 4.87189C19.9089 4.51524 21.0269 4.23596 22.1676 4.04786C22.2141 4.04053 22.2556 4.00308 22.2987 3.97947C22.2588 3.94364 22.2214 3.87931 22.179 3.87606C22.0317 3.86547 21.8786 3.85651 21.7337 3.88094C20.5709 4.07962 19.444 4.41509 18.3309 4.79453C16.0632 5.56726 13.8102 6.37906 11.6206 7.35779C11.3275 7.48889 11.0376 7.62894 10.747 7.76574C10.6102 7.83006 10.5589 7.81215 10.5711 7.66233C10.5849 7.50029 10.6183 7.32767 10.6916 7.18517C10.8854 6.80573 11.1981 6.52319 11.5221 6.25448C11.6174 6.1755 11.7135 6.09815 11.8136 6.02649C11.9187 5.9524 11.9895 5.96787 12.0522 6.08105C12.1027 6.17306 12.1401 6.27158 12.1882 6.37825C12.4023 6.31637 12.6116 6.25204 12.8225 6.19504C14.736 5.67799 16.621 5.06975 18.4864 4.39962C19.2852 4.11219 20.097 3.86059 20.9031 3.59188C20.9357 3.5813 20.9674 3.56908 21.0269 3.5471C20.9617 3.50231 20.9161 3.46567 20.8665 3.43799C20.3991 3.17743 19.8861 3.07809 19.3626 3.01783C18.396 2.9071 17.4352 2.99585 16.476 3.11066C16.0217 3.16521 15.5698 3.24175 15.1162 3.30771C15.0951 3.31097 15.0731 3.31097 15.0153 3.31585C15.0885 3.24664 15.1341 3.18883 15.192 3.15056C16.02 2.59931 16.8408 2.03422 17.6828 1.50496C18.3097 1.11167 18.9766 0.786786 19.6817 0.549838C20.2607 0.355232 20.8559 0.238794 21.4674 0.215181C21.4983 0.214367 21.5293 0.215181 21.5594 0.215995C21.5716 0.215995 21.5838 0.223324 21.6254 0.235537C21.3151 0.577523 21.1686 0.945565 21.4527 1.37875C21.1954 1.42516 20.9569 1.46261 20.7215 1.51147C19.8136 1.70119 18.9318 1.97396 18.0874 2.36155C18.0671 2.3705 18.0475 2.38028 18.0272 2.38923C17.9482 2.42099 17.9124 2.48287 17.9506 2.55534C17.9629 2.57814 18.0516 2.5814 18.0931 2.5643C18.9782 2.19707 19.8902 1.91697 20.8266 1.7134C21.706 1.52205 22.5943 1.45203 23.4924 1.5131C23.5511 1.51717 23.6097 1.53183 23.6838 1.54485H23.6822ZM20.8111 0.933351C20.7891 0.895081 20.7696 0.845412 20.737 0.807956C20.6018 0.654877 20.2941 0.696404 20.1605 0.882053C20.0604 1.02129 20.1133 1.17274 20.2778 1.22241C20.3926 1.25661 20.7272 1.1247 20.7834 1.0221C20.7956 1.00012 20.7989 0.972435 20.8111 0.934165V0.933351ZM19.0092 1.59615C19.2217 1.59778 19.3821 1.49111 19.3862 1.34455C19.3894 1.22404 19.3145 1.15157 19.1867 1.1532C19.0116 1.15483 18.8292 1.29569 18.8317 1.4276C18.8333 1.52938 18.9025 1.59534 19.0092 1.59615Z" fill="white"></path>
<path d="M14.5713 8.99404C14.6942 8.94192 14.8131 8.87841 14.9401 8.84014C15.8081 8.57714 16.6769 8.31902 17.5457 8.0609C18.6173 7.74253 19.6889 7.42497 20.7612 7.10742C20.8443 7.08299 20.9298 7.06589 21.0145 7.04553C21.0275 7.05367 21.0397 7.06182 21.0527 7.06996C21.012 7.18151 20.9868 7.30121 20.9281 7.40217C20.6871 7.81826 20.3215 8.10487 19.9144 8.34508C19.526 8.57388 19.0936 8.65775 18.6588 8.72778C17.5352 8.91017 16.4001 8.97042 15.265 9.03149C15.0851 9.04126 14.9043 9.05022 14.7236 9.05673C14.678 9.05836 14.6315 9.05103 14.5859 9.04859C14.5811 9.03068 14.5762 9.01195 14.5713 8.99404Z" fill="white"></path>
<path d="M26.0348 2.38542C26.0103 2.5613 25.9916 2.69484 25.9721 2.83733C25.9029 2.83733 25.8092 2.83733 25.7148 2.83733C24.8696 2.83082 24.0374 2.9212 23.2289 3.18013C23.1075 3.21922 23.0782 3.19805 23.1027 3.07591C23.189 2.64273 23.3469 2.24049 23.6384 1.90094C23.8249 1.68435 24.0618 1.60456 24.3468 1.63224C25.0113 1.69738 25.5381 2.05321 26.0356 2.38542H26.0348Z" fill="white"></path>
<path d="M11.0518 5.29969C11.0364 5.15149 11.1121 5.08961 11.1992 5.0318C11.5892 4.77042 12.0135 4.57908 12.4515 4.41623C13.1811 4.14427 13.9123 3.87556 14.6793 3.72574C14.8145 3.69968 14.9586 3.70131 15.0962 3.71678C15.1491 3.72248 15.2297 3.79821 15.233 3.84543C15.2354 3.89348 15.1671 3.97409 15.1133 3.992C14.9562 4.04574 14.7909 4.0775 14.628 4.11332C13.4856 4.36574 12.397 4.7802 11.3108 5.20279C11.2301 5.23455 11.1487 5.26386 11.0535 5.3005L11.0518 5.29969Z" fill="white"></path>
<path d="M25.8625 3.93866C25.8104 3.98833 25.7639 4.04695 25.7045 4.08604C25.1809 4.42558 24.6566 4.76431 24.1273 5.09571C23.9873 5.18365 23.927 5.16166 23.8374 5.01754C23.778 4.92065 23.7275 4.81724 23.6835 4.7122C23.651 4.63321 23.6705 4.57133 23.7666 4.54039C24.3944 4.33927 25.0205 4.13326 25.6483 3.93214C25.711 3.91179 25.781 3.91342 25.8478 3.90527C25.8527 3.91586 25.8576 3.92726 25.8625 3.93784V3.93866Z" fill="white"></path>
<path d="M9.18325 8.61365C9.20849 8.76021 9.22966 8.88724 9.25246 9.01914C8.8722 9.15187 8.50416 9.28133 8.1353 9.40836C8.05307 9.43686 8.0034 9.40754 7.98223 9.31716C7.94721 9.16897 7.98467 9.0224 8.10843 8.9768C8.45775 8.84978 8.81276 8.73741 9.18325 8.61365Z" fill="white"></path>
<path d="M9.56532 7.59131C9.57754 7.82093 9.5181 7.91782 9.33489 7.98296C9.04909 8.08393 8.7641 8.18571 8.47748 8.28424C8.36512 8.32251 8.32522 8.28831 8.33173 8.1678C8.33825 8.05299 8.40013 7.97726 8.50435 7.94225C8.85367 7.82418 9.2038 7.711 9.56451 7.59131H9.56532Z" fill="white"></path>
<path d="M22.1045 0.752621C22.0247 0.721679 21.9075 0.712723 21.87 0.653282C21.8065 0.553129 21.8676 0.43262 21.9531 0.360966C22.0223 0.303154 22.1208 0.251042 22.2079 0.248599C22.3545 0.243714 22.5051 0.265698 22.6476 0.303968C22.7103 0.320253 22.7844 0.396793 22.7975 0.457862C22.8056 0.49776 22.7234 0.586514 22.6647 0.606056C22.488 0.66631 22.3032 0.702952 22.1053 0.753435L22.1045 0.752621Z" fill="white"></path>
<path d="M9.76562 8.18845C9.80064 8.02397 9.81448 7.90998 9.85031 7.8025C9.88532 7.69583 9.96593 7.67873 10.0376 7.76504C10.0848 7.82204 10.1117 7.90265 10.1272 7.97675C10.1671 8.17787 10.1964 8.38143 10.2297 8.58418C10.2338 8.61023 10.2371 8.63629 10.2411 8.66235C10.2534 8.74377 10.2387 8.82927 10.1508 8.83822C10.0913 8.84474 9.98792 8.80728 9.96919 8.7625C9.88451 8.56382 9.82262 8.35537 9.76562 8.18845Z" fill="white"></path>
<path d="M26.6685 3.09762C26.4315 3.07563 26.377 3.03899 26.3811 2.88184C26.3835 2.79797 26.4218 2.72632 26.5122 2.73283C26.6555 2.74423 26.7988 2.76948 26.9396 2.7996C26.9795 2.80856 27.0398 2.8566 27.0398 2.88673C27.0398 2.92825 27.0007 2.99095 26.9624 3.00805C26.8696 3.04876 26.767 3.06912 26.6677 3.09762H26.6685Z" fill="white"></path>
<path d="M23.2078 1.14955C23.2412 1.03555 23.2657 0.928887 23.3039 0.828734C23.3365 0.744867 23.505 0.717182 23.6101 0.765223C23.72 0.815706 23.6752 0.892246 23.6459 0.964714C23.5661 1.16502 23.5018 1.19433 23.207 1.14955H23.2078Z" fill="white"></path>
<path d="M20.8109 0.934089C20.7995 0.973173 20.7954 1.00004 20.7832 1.02203C20.727 1.12544 20.3923 1.25653 20.2775 1.22233C20.1122 1.17266 20.0601 1.02203 20.1603 0.881977C20.293 0.695514 20.6016 0.654801 20.7368 0.80788C20.7701 0.845336 20.7889 0.895819 20.8109 0.933275V0.934089Z" fill="#CA3624"></path>
<path d="M19.0076 1.5963C18.9009 1.5963 18.8317 1.52953 18.8301 1.42775C18.8277 1.29584 19.0101 1.15579 19.1851 1.15335C19.3138 1.15172 19.3887 1.22337 19.3846 1.3447C19.3805 1.49045 19.2201 1.59711 19.0076 1.5963Z" fill="#CA3624"></path>
</svg></p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Xcapture-BPF ‚Äì like Linux top, but with Xray vision (275 pts)]]></title>
            <link>https://0x.tools/</link>
            <guid>40869877</guid>
            <pubDate>Wed, 03 Jul 2024 20:52:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://0x.tools/">https://0x.tools/</a>, See on <a href="https://news.ycombinator.com/item?id=40869877">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <h2><a href="https://0x.tools/">0x.tools</a></h2>
      

      
<p><em>By <a href="https://tanelpoder.com/">Tanel Poder</a></em></p>

<p><strong>0x.tools</strong> (<a href="https://github.com/tanelpoder/0xtools">GitHub</a>) is a set of open-source utilities for analyzing application performance on Linux. It has a goal of deployment simplicity and minimal dependencies, to reduce friction of systematic troubleshooting. There‚Äôs no need to upgrade the OS, install kernel modules, heavy monitoring frameworks, Java agents or databases. Some of the tools even work on over-decade-old Linux kernels, like version 2.6.18 from 18 years ago.</p>

<p><strong>0x.tools</strong> allow you to measure individual thread level activity, like thread sleep states, currently executed system calls and kernel wait locations. Additionally, you can drill down into CPU usage of any thread or the system as a whole. You can be <em>systematic</em> in your troubleshooting - no need for guessing or genius wizard magic based on secondary metrics like system utilization stats.</p>



<h2 id="xcapture-bpf-202-beta">xcapture-bpf 2.0.2 beta</h2>

<p><strong>xcapture-bpf</strong> (and <strong>xtop</strong>) are like the Linux <code>top</code> tool, but extended with x-ray vision and ability to view your performance data from any chosen angle (that eBPF allows to instrument). You can use it for system level overview and drill down into indivual threads‚Äô activity and soon even into individual kernel events like lock waits or memory stalls. eBPF is not only customizable, it‚Äôs completely programmable and I plan to take full advantage of it. I have so far implemented less than 5% of everything this method and the new tool is capable of, stay tuned for more!</p>

<p>This (2-minute) asciicast box below is pretty high for a reason, play it and you‚Äôll see, command line nerds should love it ;-)<br>
</p>

<h3 id="xcapture-bpf-terminal-highlighting-and-stacktiles-in-action">xcapture-bpf terminal highlighting and stacktiles in action</h3>

<p>I included a screenshot image below, to show how the terminal text search/highlighting <em>and scrolling</em> capabilities work nicely together with my new <em>stacktiles</em> formatting method, this way you can fit more relevant things on your screen and not have to switch windows or scroll around that much, while keeping some structure and sanity in place with all the stack traces.</p>

<p><img src="https://0x.tools/images/xcapture-bpf-stacktiles.png" alt="xcapture-bpf and stacktiles in terminal"></p>

<p>The stacktiles do not have to contain only stacks of function names, but could contain other things, like filenames or any other thing, like top memory allocation reasons (and amounts) done under a code location reported below, etc.</p>

<h3 id="xcapture-bpf-installation">xcapture-bpf installation</h3>

<p><code>xcapture-bpf</code> is still in beta, don‚Äôt run it on busy production systems yet. As it uses eBPF (and currently BCC with python3 as a reporting frontend), you‚Äôd need to be at least on RHEL 8.1 (or a clone) or Ubuntu 24.04 (Ubuntu 22.04‚Äôs BCC has some kernel header compatibility issue and the 20.04 kernel does not have the required eBPF features available. These are the only versions I‚Äôve tested with so far, on x86_64 and arm64 platforms.</p>

<p>On RHEL8, you can install the prerequisites with this:</p>

<div><pre><code>$ sudo dnf install bcc bcc-tools python3 python3-bcc
$ git clone git@github.com:tanelpoder/0xtools.git

$ ls -l 0xtools/bin/xcapture-bpf*
-rwxrwxr-x. 1 tanel tanel 25724 Jul  2 22:04 0xtools/bin/xcapture-bpf
-rw-rw-r--. 1 tanel tanel 12127 Jul  2 15:34 0xtools/bin/xcapture-bpf.c

$ cat 0xtools/bin/xtop 
#!/usr/bin/bash

CURDIR="$(dirname "$(realpath "$0")")"

${CURDIR}/xcapture-bpf --xtop --clear-screen $*

$ cd 0xtools/bin
$ sudo ./xtop
</code></pre></div>

<p>If you don‚Äôt want to clone/download the whole 0xtools repository, then for <code>xcapture-bpf</code>, you only need the 2 xcapture-bpf* files listed above. No need to compile the .c file as the BCC toolset takes care of it on the fly. <code>xtop</code> is just a simple shell wrapper for convenience (and now there‚Äôs an ‚Äúxtop‚Äù in the Linux command line namespace! ;-)</p>

<p><code>bcc-tools</code> are not really needed for xcapture-bpf itself, but they‚Äôre worth checking out, if you‚Äôre gonna play with eBPF tools anyway.</p>

<h3 id="xcapture-bpf-launch-video-2024-06-25">xcapture-bpf launch video (2024-06-25)</h3>
<p>I have uploaded my 0xtools v2 beta (with eBPF) nerd-launch video here:</p>

<iframe width="980" height="614" src="https://www.youtube.com/embed/_nLAdkDhaI0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""> </iframe>

<p>Slides, code, discussion here:</p>
<ul>
  <li><a href="https://github.com/tanelpoder/0xtools/discussions/38">https://github.com/tanelpoder/0xtools/discussions/38</a></li>
</ul>

<p>The details about the rest of the 0xtools are below (all the other tools just read various /proc files, no eBPF needed for them).</p>

<h2 id="table-of-contents">Table of Contents</h2>

<ol>
  <li><a href="#included-tools">Included Tools</a></li>
  <li><a href="#usage--example-output">Example Output</a></li>
  <li><a href="#installation--usage">Installation &amp; Usage</a></li>
  <li><a href="#faq">FAQ</a></li>
  <li><a href="#whats-next">What‚Äôs next</a></li>
  <li><a href="#articles">Articles</a></li>
</ol>



<p>You get two classes of utilities:</p>

<ol>
  <li>Real-time interactive tools for analyzing current system behavior as it is happening.</li>
  <li>Low-overhead thread activity samplers for <strong>always-on low-frequency profiling of production systems</strong>. The continuously captured data allows you to ‚Äúgo back in time‚Äù and systematically troubleshoot even intermittent problems right after (or during) their first occurrence.</li>
</ol>

<table>
  <thead>
    <tr>
      <th>Command</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>psn</td>
      <td>Show current top thread activity by sampling /proc files</td>
    </tr>
    <tr>
      <td>xcapture</td>
      <td>Low-overhead thread state sampler reading /proc files</td>
    </tr>
    <tr>
      <td>xcapture-bpf</td>
      <td><a href="https://github.com/tanelpoder/0xtools/discussions/38">Low-overhead programmable thread state sampler build with eBPF</a> (beta)</td>
    </tr>
    <tr>
      <td>syscallargs</td>
      <td><a href="https://tanelpoder.com/posts/list-linux-system-call-arguments-with-syscallargs/">List all system calls with their arguments</a></td>
    </tr>
    <tr>
      <td>schedlat</td>
      <td>Show single process‚Äôes CPU scheduling latency as a % of its runtime</td>
    </tr>
    <tr>
      <td>run_xcapture.sh</td>
      <td>A simple ‚Äúdaemon‚Äù script for keeping xcapture running</td>
    </tr>
    <tr>
      <td>run_xcpu.sh</td>
      <td>Low-frequency continuous stack sampling for threads on CPU (using perf)</td>
    </tr>
  </tbody>
</table>

<p><code>xcapture</code> is written in C for efficiency reasons and it consists of just a single C source file and a single header file for system call name translation. All other tools are Python or shell scripts.</p>

<h2 id="usage--example-output">Usage &amp; Example Output</h2>

<p>Sample Linux thread activity and show fixed-width output on screen:</p>

<div><pre><code>$ xcapture

0xTools xcapture v1.0 by Tanel Poder [https://0x.tools]

Sampling /proc...

DATE       TIME             PID     TID USERNAME        ST COMMAND                   SYSCALL                   WCHAN                    
2020-10-17 12:01:50.583    6404    7524 mysql           R  (mysqld)                  fsync                     wait_on_page_bit          
2020-10-17 12:01:50.583    6404    8944 mysql           D  (mysqld)                  fsync                     wait_on_page_bit          
2020-10-17 12:01:50.583    6404    8946 mysql           D  (mysqld)                  fsync                     wait_on_page_bit          
2020-10-17 12:01:50.583    6404   76046 mysql           D  (mysqld)                  fsync                     wait_on_page_bit          
2020-10-17 12:01:50.583    6404   76811 mysql           D  (mysqld)                  fdatasync                 xfs_log_force_lsn         
2020-10-17 12:01:50.583    6404   76815 mysql           D  (mysqld)                  fsync                     blkdev_issue_flush        
2020-10-17 12:01:50.583    8803    8803 root            R  (md10_resync)             [running]                 0                         

DATE       TIME             PID     TID USERNAME        ST COMMAND                   SYSCALL                   WCHAN                    
2020-10-17 12:01:51.623    6404    7521 mysql           D  (mysqld)                  pwrite64                  xfs_file_buffered_aio_write 
2020-10-17 12:01:51.623    6404    7524 mysql           D  (mysqld)                  fsync                     xfs_log_force_lsn         
2020-10-17 12:01:51.623    6404    7767 mysql           D  (mysqld)                  fsync                     xfs_log_force_lsn         
2020-10-17 12:01:51.623    6404    8398 mysql           D  (mysqld)                  fsync                     call_rwsem_down_read_failed 
2020-10-17 12:01:51.623    6404    5446 mysql           D  (mysqld)                  fsync                     xfs_log_force_lsn         
2020-10-17 12:01:51.623    6404    8941 mysql           D  (mysqld)                  pwrite64                  xfs_file_buffered_aio_write 
2020-10-17 12:01:51.623    6404    8944 mysql           D  (mysqld)                  pwrite64                  xfs_file_buffered_aio_write 
2020-10-17 12:01:51.623    6404    8945 mysql           D  (mysqld)                  pwrite64                  xfs_file_buffered_aio_write 
2020-10-17 12:01:51.623    6404   76045 mysql           D  (mysqld)                  fsync                     call_rwsem_down_read_failed 
2020-10-17 12:01:51.623    6404   76046 mysql           D  (mysqld)                  pwrite64                  xfs_file_buffered_aio_write 
2020-10-17 12:01:51.623    6404   76810 mysql           D  (mysqld)                  pwrite64                  xfs_file_buffered_aio_write 
2020-10-17 12:01:51.623    6404   76811 mysql           D  (mysqld)                  fdatasync                 xfs_log_force_lsn         
2020-10-17 12:01:51.623    6404   76812 mysql           D  (mysqld)                  fsync                     wait_on_page_bit          
2020-10-17 12:01:51.623    8803    8803 root            D  (md10_resync)             [no_syscall]              msleep                    
</code></pre></div>

<details>
  <summary>Watch a SVG video of xcapture in action!</summary>
  <p><img src="https://0x.tools/images/xcapture-example.svg">
  </p>
</details>

<hr>

<p>Sample threads in all states (including Sleeping) and write output into hourly CSV files:</p>

<div><pre><code>$ xcapture -a -o /data/xcap &amp;

$ head 2020-10-16.21.csv
TS,PID,TID,USERNAME,ST,COMMAND,SYSCALL,WCHAN,EXE,CMDLINE,KSTACK
2020-10-16 21:00:00.001,5335,5335,root,R,(collectl),[running],0,perl,/usr/bin/perl,
2020-10-16 21:00:00.001,8803,8803,root,D,(md10_resync),[no_syscall],msleep,-,-,-&gt;ret_from_fork_nospec_begin()-&gt;kthread()-&gt;md_thread()-&gt;md_do_sync()-&gt;msleep()
2020-10-16 21:00:01.038,8803,8803,root,R,(md10_resync),[no_syscall],md_do_sync,-,-,-&gt;ret_from_fork_nospec_begin()-&gt;kthread()-&gt;md_thread()-&gt;md_do_sync()
2020-10-16 21:00:02.075,8803,8803,root,D,(md10_resync),[no_syscall],md_do_sync,-,-,-&gt;ret_from_fork_nospec_begin()-&gt;kthread()-&gt;md_thread()-&gt;md_do_sync()
2020-10-16 21:00:02.075,16762,16762,oracle,R,(ora_m000_lin19c),[running],0,oracle,ora_m000_LIN19C,-&gt;do_blockdev_direct_IO()-&gt;dio_complete()
2020-10-16 21:00:03.112,8803,8803,root,R,(md10_resync),[no_syscall],md_do_sync,-,-,-&gt;ret_from_fork_nospec_begin()-&gt;kthread()-&gt;md_thread()-&gt;md_do_sync()
2020-10-16 21:00:04.149,8803,8803,root,D,(md10_resync),[no_syscall],msleep,-,-,-&gt;ret_from_fork_nospec_begin()-&gt;kthread()-&gt;md_thread()-&gt;md_do_sync()-&gt;msleep()
2020-10-16 21:00:05.186,8803,8803,root,D,(md10_resync),[no_syscall],md_do_sync,-,-,-&gt;ret_from_fork_nospec_begin()-&gt;kthread()-&gt;md_thread()-&gt;md_do_sync()
2020-10-16 21:00:05.186,65913,65913,oracle,D,(ora_ckpt_lin122),pwrite64,blkdev_issue_flush,oracle,ora_ckpt_LIN122,-&gt;system_call_fastpath()-&gt;SyS_pwrite64()-&gt;vfs_write()-&gt;do_sync_write()-&gt;xfs_file_aio_write()-&gt;generic_write_sync()-&gt;xfs_file_fsync()-&gt;xfs_blkdev_issue_flush()-&gt;blkdev_issue_flush()
</code></pre></div>

<p>You can ‚ÄúQuery‚Äù the thread activity history for performance analysis on the command line (or just load the CSV into any database):</p>

<p>Query CSV files with standard Linux text processing tools. It‚Äôs like SQL but with different keywords: <code>grep</code> for filtering, <code>cut</code>, <code>awk</code> for column projection, <code>uniq</code> for group by and <code>sort</code> for ordering. Filename patterns like <code>cat 2020-10-??.0[89].csv</code> could be used for scanning through only the files of interest (partition pruning):</p>

<div><pre><code>$ cat 2020-10-13.01.csv | awk -F, '{ printf("%2s %-20s %-20s %s\n",$5,$4,$7,$10) }' | sort | uniq -c | sort -nbr | head -20
   2303  D root                 read                 -
   1761  R tanel                [running]            stress
   1384  D postgres             pread64              postgres: tanel pgbench [local] UPDATE
    894  R root                 [running]            -
    229  R root                 read                 -
    229  D mysql                fsync                /usr/sbin/mysqld
    144  R tanel                [running]            -
    115  - -                    -                    -
    110  D oracle               io_submit            ora_ckpt_LINPRD
    101  D root                 [running]            -
     73  D root                 read                 dd
     58  R root                 [running]            /opt/oracle.ahf/jre/bin/java
     55  R mysql                [running]            /usr/sbin/mysqld
     52  D tanel                [no_syscall]         stress
     51  R oracle               [running]            oracleLIN19C
     50  R root                 [running]            dd
     35  R oracle               [running]            xe_mz01_XE
     32  R tanel                [running]            /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.262.b10-0.el7_8.x86_64/jre/bin/java
     29  R oracle               [running]            pidstat
     27  D oracle               pread64              oracleLIN19C
</code></pre></div>

<p>Or you can query CSV files with <code>q-text-as-data</code>:</p>

<div><pre><code>$ q -d, -bTH '
select count(*) avgthr, username,st,syscall,wchan
from 2020-10-13.01.csv
group by username,st,syscall,wchan
order by 1 desc' | head -20
1955	tanel   	R	[running]   	0                               
1384	postgres	D	pread64     	generic_file_read_iter          
1084	root    	D	read        	raise_barrier                   
1041	root    	R	[running]   	0                               
712 	root    	D	read        	msleep                          
341 	oracle  	R	[running]   	0                               
317 	root    	D	read        	md_super_wait                   
123 	mysql   	D	fsync       	__xfs_log_force_lsn             
115 	-       	-	-           	-                               
92  	oracle  	D	io_submit   	md_write_start                  
92  	root    	R	read        	raise_barrier                   
79  	root    	D	read        	wait_barrier                    
66  	oracle  	R	nanosleep   	hrtimer_nanosleep               
66  	root    	D	[running]   	0                               
52  	mysql   	R	[running]   	0                               
51  	root    	R	read        	worker_thread                   
48  	mysql   	D	fsync       	submit_bio_wait                 
48  	root    	D	read        	0                               
41  	tanel   	D	[no_syscall]	rq_qos_wait                     
39  	root    	D	read        	md_bitmap_cond_end_sync         
</code></pre></div>

<p>Or you can do tabular data analysis in your terminal with the awesome <a href="https://www.visidata.org/">VisiData</a> tool. Note that the video below does not have sound (it‚Äôs not your computer :-)</p>

<iframe width="980" height="614" src="https://www.youtube.com/embed/hW9fVLOjB10" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>

<p><em>This brings me back memories of Lotus 1-2-3 on a crappy 286 with floppy drives and MS-DOS!</em></p>

<h2 id="installation--usage">Installation &amp; Usage</h2>

<p><code>xcapture</code>, <code>schedlat</code> and <code>psn</code> sample the Linux <em>/proc filesystem</em> just like standard tools like <code>ps</code>, <code>top</code> and <code>lsof</code> do. The /proc filesystem is essentially Linux kernel presenting useful metrics into userspace as user-readable files. So, you do not need any additional Linux configuration or anything fancy to be installed on your hosts. 0x.tools require Linux kernel version 2.6 or later, so they will work even on your legacy installations (like RHEL 5, CentOS 5) from 15 years ago.</p>

<p>For running <code>psn</code> on CentOS 5 (RHEL 5 clones), you need to have Python 2.6+ on it (it can be installed from EPEL repo).</p>

<div><pre><code>$ git clone https://github.com/tanelpoder/0xtools
$ make
$ sudo make install
</code></pre></div>

<p>Running 0xTools utilities:</p>

<h3 id="xcapture">xCapture</h3>

<div><pre><code>$ xcapture

0x.Tools xcapture v1.0 by Tanel Poder [https://0x.tools]

Usage:
  xcapture [options]

  By default, sample all /proc tasks in states R, D every second and print to stdout

  Options:
    -a             capture tasks in additional states, even the ones Sleeping (S)
    -A             capture tasks in All states, including Zombie (Z), Exiting (X), Idle (I)
    -c &lt;c1,c2&gt;     print additional columns (for example: -c exe,cmdline,kstack)
    -d &lt;N&gt;         seconds to sleep between samples (default: 1)
    -E &lt;string&gt;    custom task state Exclusion filter (default: XZIS)
    -h             display this help message
    -o &lt;dirname&gt;   write wide output into hourly CSV files in this directory instead of stdout


$ xcapture -c exe,kstack
$ xcapture -o .
$ xcapture -o /data/perf_archive/xcap

</code></pre></div>

<h3 id="linux-process-snapper">Linux Process Snapper</h3>

<p>Linux Process Snapper is a Python script meant for troubleshooting currently on-going issues (no historical capture). It currently reports more fields directly from /proc than xcapture captures (like filenames accessed by IO system calls). I plan to improve this tool so that it could use xcapture CSV files as an input, in addition to current real-time monitoring.</p>

<p>IO bottleneck example: My ‚Äúpipeline‚Äù is bottlenecked by writes to the output file, not input reads:</p>

<div><pre><code>$ psn -p 18286 -G syscall,filename

Linux Process Snapper v0.14 by Tanel Poder [https://0x.tools]
Sampling /proc/stat, syscall for 5 seconds... finished.


=== Active Threads ==================================================================================

 samples | avg_threads | comm | state                  | syscall   | filename                        
-----------------------------------------------------------------------------------------------------
      79 |        0.79 | (dd) | Disk (Uninterruptible) | write     | /backup/tanel/test (stdout)
       7 |        0.07 | (dd) | Disk (Uninterruptible) | [running] |                                 
       5 |        0.05 | (dd) | Running (ON CPU)       | write     | /backup/tanel/test (stdout)     
       4 |        0.04 | (dd) | Disk (Uninterruptible) | read      | /reco/fio/mmapfile.0.0 (stdin)  
       3 |        0.03 | (dd) | Running (ON CPU)       | [running] |                                 
       2 |        0.02 | (dd) | Running (ON CPU)       | read      | /reco/fio/mmapfile.0.0 (stdin)  ```
</code></pre></div>

<p>MySQL I/O bottleneck example: there‚Äôs some OS kernel inode level semaphore contention due to frequent use of fsync():</p>

<div><pre><code>$ sudo psn -p "mysqld|kwork" -G syscall,wchan

Linux Process Snapper v0.14 by Tanel Poder [https://0x.tools]
Sampling /proc/syscall, stat, wchan for 5 seconds... finished.


=== Active Threads ========================================================================================

 samples | avg_threads | comm          | state                  | syscall   | wchan                        
-----------------------------------------------------------------------------------------------------------
      25 |        3.12 | (mysqld)      | Disk (Uninterruptible) | fsync     | _xfs_log_force_lsn
      16 |        2.00 | (mysqld)      | Running (ON CPU)       | [running] | 0                            
      14 |        1.75 | (mysqld)      | Disk (Uninterruptible) | pwrite64  | call_rwsem_down_write_failed
       8 |        1.00 | (mysqld)      | Disk (Uninterruptible) | fsync     | submit_bio_wait              
       4 |        0.50 | (mysqld)      | Disk (Uninterruptible) | pread64   | io_schedule                  
       4 |        0.50 | (mysqld)      | Disk (Uninterruptible) | pwrite64  | io_schedule                  
       3 |        0.38 | (mysqld)      | Disk (Uninterruptible) | pread64   | 0                            
       3 |        0.38 | (mysqld)      | Running (ON CPU)       | [running] | io_schedule                  
       3 |        0.38 | (mysqld)      | Running (ON CPU)       | pread64   | 0                            
       2 |        0.25 | (mysqld)      | Disk (Uninterruptible) | [running] | 0                            
       1 |        0.12 | (kworker/*:*) | Running (ON CPU)       | read      | worker_thread                
       1 |        0.12 | (mysqld)      | Disk (Uninterruptible) | fsync     | io_schedule                  
       1 |        0.12 | (mysqld)      | Disk (Uninterruptible) | futex     | call_rwsem_down_write_failed 
       1 |        0.12 | (mysqld)      | Disk (Uninterruptible) | poll      | 0                            
       1 |        0.12 | (mysqld)      | Disk (Uninterruptible) | pwrite64  | _xfs_log_force_lsn           
       1 |        0.12 | (mysqld)      | Running (ON CPU)       | fsync     | submit_bio_wait              
       1 |        0.12 | (mysqld)      | Running (ON CPU)       | futex     | futex_wait_queue_me      
</code></pre></div>

<p>More info and examples are available at Tanel Poder‚Äôs <a href="https://tanelpoder.com/psnapper">Linux Performance Troubleshooting Page</a></p>

<h3 id="schedlat">SchedLat</h3>

<div><pre><code>$ ./schedlat.py 29801
SchedLat by Tanel Poder [https://0x.tools]

PID=29801 COMM=oracle_29801_li

TIMESTAMP              %CPU   %LAT   %SLP
2020-02-26 23:17:35   100.0    0.0    0.0   &lt;&lt;-- no CPU shortage, process 100% on CPU
2020-02-26 23:17:36   100.0    0.0    0.0
2020-02-26 23:17:37   100.0    0.0    0.0
2020-02-26 23:17:38   100.0    0.0    0.0   &lt;&lt;-- %SLP = 100-(%CPU+%LAT), when Linux reports slightly
2020-02-26 23:17:39    98.0    0.0    2.0        more than "100%" of CPU+LAT, then the derived
2020-02-26 23:17:40     0.0    0.0  100.0        "remaining time" SLP% may show a negative value
2020-02-26 23:17:41     0.0    0.0  100.0
2020-02-26 23:17:42     0.0    0.0  100.0   &lt;&lt;-- no CPU shortage, process sleeping
2020-02-26 23:17:43     0.4    0.0   99.6
2020-02-26 23:17:44    33.5    0.2   66.3   &lt;&lt;-- no CPU shortage, process doing synchronous I/Os 
2020-02-26 23:17:45    55.5    0.2   44.2        in a loop (thus taken off CPU frequently by scheduler)
2020-02-26 23:17:46    53.9    0.2   45.9
2020-02-26 23:17:47    54.5    0.2   45.3
2020-02-26 23:17:48    59.1    0.2   40.7
2020-02-26 23:17:49     4.4    0.0   95.6
2020-02-26 23:17:50    58.5    0.1   41.4
2020-02-26 23:17:51    95.7    0.0    4.3
2020-02-26 23:17:52     0.3    0.0   99.7 
2020-02-26 23:17:53     0.1    0.0   99.9
2020-02-26 23:17:54     0.1    0.0   99.9
2020-02-26 23:17:55     0.3    1.1   98.6
2020-02-26 23:17:56     0.1    6.0   93.9
2020-02-26 23:17:57     0.1   15.0   84.9
2020-02-26 23:17:58     0.1   13.8   86.1
2020-02-26 23:17:59     9.6   61.4   29.0   &lt;&lt;-- CPU shortage + process doing synchronous I/Os in a loop
2020-02-26 23:18:00    14.6   83.9    1.5   &lt;&lt;-- and spending more time in CPU runqueue after every I/O
2020-02-26 23:18:01    31.4   59.7    8.9
2020-02-26 23:18:02    13.0   13.9   73.1
2020-02-26 23:18:03     0.3    5.3   94.4
</code></pre></div>

<p>There are more details in my <a href="https://tanelpoder.com/posts/schedlat-low-tech-script-for-measuring-cpu-scheduling-latency-on-linux/">Measuring Linux CPU Scheduling Latency</a> blog entry.</p>

<h2 id="cpu-profiling">CPU profiling</h2>

<p>When you look into the <code>run_xcpu.sh</code>, you‚Äôll see that I‚Äôm currently using just <code>perf</code> under the hood with 1 Hz frequency. You can have it <em>always-on</em> no noticeable performance overhead!</p>

<div><pre><code>$ cat bin/run_xcpu.sh
...
perf record -g -F 1 -a \
            --switch-output=1m \
            --timestamp-filename \
            --timestamp \
            -o $1/xcpu

...
</code></pre></div>

<p>With the above arguments, perf writes the sampled on-CPU stack traces into 1-minute granularity files.</p>

<p>Then all you need to do is run perf on the file with the right timestamp, to zoom in to the time of your performance problem:</p>

<div><pre><code>$ perf report -s sym,dso -i xcpu.2020101619323791
</code></pre></div>

<p><img src="https://0x.tools/images/perf-example.png">
<em>Perf CPU usage profile, including kernel-mode and interrupts CPU usage</em></p>

<h2 id="faq">FAQ</h2>

<h3 id="how-is-the-0xtools-toolset-licensed">How is the 0x.tools toolset licensed?</h3>
<p>0x.tools is an open source, GPL v3-licensed product, so you can use it like most other standard command line tools in your Linux distribution.</p>

<h3 id="what-is-the-measurement-overhead">What is the measurement overhead?</h3>
<p>0x.tools <code>xcapture</code> is designed to have very low overhead, well under 1% of your server‚Äôs CPU capacity, even when sampling every second. Note that xcapture does not invoke any tracing, but samples already built-in kernel instrumentation from /proc file system asynchronously and independently. Therefore it won‚Äôt slow any of your existing applications down, but uses a small percentage of one CPU in the system for its sampling. In extreme cases (with tens of thousands of active threads), you can reduce sampling frequency to reduce xcapture CPU usage.</p>

<p>The <code>run_xcpu.sh</code> CPU sampling script uses standard Linux <code>perf</code> utility under the hood, with just 1 Hz sampling rate by default. Thanks to the low-frequency sampling, perf will not cause noticeable overhead for your applications.</p>

<h3 id="is-it-safe-to-use-in-production">Is it safe to use in production?</h3>
<p>0x.tools are <em>designed</em> to be safely used in production, including traditional enterprise environments where you can‚Äôt just upgrade to latest OS version at will or load custom kernel modules. All the code is open source, without any dependencies outside the standard Linux utilities and libraries, skimming through a few hundred lines of 0x.tools C and Python code should be doable in matter of minutes.</p>

<p>As with all software and tools, I recommend to try them first on a test system (ideally similar to production) and see how it works, before deploying to production.</p>

<h3 id="why-not-just-use-perf-for-everything-including-xcapture">Why not just use perf for everything (including xcapture)?</h3>
<p>Perf sampling captures only on-CPU activity by default. If you have 32 CPUs, it will check what code is running on them at every sample, but does not aim to walk through the hundreds (or thousands) of OS threads that happen to be sleeping. While it is possible to enable tracing for <a href="http://www.brendangregg.com/offcpuanalysis.html">off-cpu events</a> in Perf, it comes with a high tracing overhead (and later, overhead of post-processing these high-frequency events).</p>

<h3 id="why-not-just-use-bpf-instead-of-proc-sampling">Why not just use BPF instead of /proc sampling?</h3>
<p>In short, eBPF is not available for wide-scale production use in traditional enterprises (think banks, telcos and other Fortune 500s with decades of IT history). This may come as a surprise if you‚Äôve worked only for startups running latest ephemeral Ubuntu containers in the cloud :-) For example RedHat started actually supporting eBPF in RHEL 8.1 (Released Nov 2019). The enterprises I work with, still have RHEL6 (kernel 2.6.32) as their mostly widely used OS version, with RHEL7 (and CentOS 7) gaining traction. So ‚Äúlet‚Äôs just do a major OS upgrade‚Äù for troubleshooting this performance spike is out of the question.</p>

<p>Nevertheless, I have written an eBPF sampler prototype already, it combines both thread state and CPU usage profiling into one tool. But I wanted to productionize the simpler, widely available /proc file-based profiler first, for practical reasons.</p>

<h3 id="why-not-just-use-distributed-tracing-like-opentracing-zipkin-jaeger">Why not just use distributed tracing like OpenTracing, Zipkin, Jaeger?</h3>

<p>These powerful, but complex frameworks are high level end-to-end tracers of request flow through application layers and components. They are designed to point out in which component of your distributed multi-tier system most of the user response was spent, but they do not drill down into the reason why. 0x.tools are designed to fill that gap.</p>

<h3 id="why-not-just-use-something-like-prometheus">Why not just use something like Prometheus?</h3>

<p>Prometheus is designed for shipping, storing and serving system &amp; appliction time-series metrics captured from a large fleet of servers &amp; applications. You can plot nice dashboards with charts showing various latency, request count and system utilization metrics over time. Such time-series metrics are useful background info, but do not allow you to drill down into the low level <em>reasons</em> of increased system activity, application resource usage or misbehavior of the OS kernel itself.</p>

<h2 id="whats-next">What‚Äôs next?</h2>

<p>There are a lot of new features and utilities that can be added to 0xTools suite. Before I go there, I will work on some packaging &amp; productionization things first (scripts for automatic compression &amp; archiving of the captured files, installation via a RPM/DEB package, built-in data visualization). Feel free to submit ideas and issues in the <a href="https://github.com/tanelpoder/0xtools">0x.Tools GitHub repo</a>.</p>

<ul>
  <li>
    <p>I also deliver <a href="https://tanelpoder.com/consulting">consulting</a> and <a href="https://tanelpoder.com/seminar">training</a> around systematic Linux troubleshooting &amp; tuning, including helping you to come up with a strategy for rolling out <strong>always-on profiling for production systems</strong> in your company.</p>
  </li>
  <li>
    <p>Get 0x.Tools updates via Twitter <a href="https://twitter.com/0xtools">@0xtools</a>.</p>
  </li>
</ul>

<h3 id="articles">Articles</h3>

<ul>
  <li>
    <p><a href="https://tanelpoder.com/categories/linux/">Linux performance &amp; troubleshooting</a> articles by Tanel Poder</p>
  </li>
  <li>
    <p><a href="https://youtu.be/YEWp3O7Kem8">Profiling Linux Activity for Performance And Troubleshooting</a> video by Tanel Poder</p>
  </li>
  <li>
    <p><a href="http://mysqlentomologist.blogspot.com/2021/01/linux-proc-filesystem-for-mysql-dbas_8.html">Using 0xtools with MySQL</a> series by <a href="https://twitter.com/mysqlbugs">Valerii Kravchuk</a></p>
  </li>
</ul>

<p><a href="https://0x.tools/#">Back to top</a></p>


      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI's $600B Question (312 pts)]]></title>
            <link>https://www.sequoiacap.com/article/ais-600b-question/</link>
            <guid>40869461</guid>
            <pubDate>Wed, 03 Jul 2024 19:55:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sequoiacap.com/article/ais-600b-question/">https://www.sequoiacap.com/article/ais-600b-question/</a>, See on <a href="https://news.ycombinator.com/item?id=40869461">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
		<article>
	
<section><p>The AI bubble is reaching a tipping point. Navigating what comes next will be essential.</p></section>



<section>
<p>In September 2023, I published <a href="https://www.sequoiacap.com/article/follow-the-gpus-perspective/">AI‚Äôs $200B Question</a>. The goal of the piece was to ask the question: ‚ÄúWhere is all the revenue?‚Äù&nbsp;</p>



<p>At that time, I noticed a big gap between the revenue expectations implied by the AI infrastructure build-out, and actual revenue growth in the AI ecosystem, which is also a proxy for end-user value. I described this as a ‚Äú$125B hole that needs to be filled <em>for each year of CapEx at today‚Äôs levels</em>.‚Äù&nbsp;</p>



<p>This week, Nvidia completed its ascent to become the most valuable company in the world. In the weeks leading up to this, I‚Äôve received numerous requests for the updated math behind my analysis. Has AI‚Äôs $200B question been solved, or exacerbated?</p>



<p>If you run this analysis again today, here are the results you get: AI‚Äôs $200B question is now AI‚Äôs $600B question.</p>
</section>



<figure><img fetchpriority="high" decoding="async" width="2410" height="785" src="https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png" alt="" srcset="https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png 2410w, https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png?resize=300,98 300w, https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png?resize=768,250 768w, https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png?resize=1024,334 1024w, https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png?resize=1536,500 1536w, https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png?resize=2048,667 2048w, https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png?resize=220,72 220w, https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png?resize=1920,625 1920w, https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png?resize=680,221 680w, https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png?resize=930,303 930w, https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png?resize=1440,469 1440w, https://www.sequoiacap.com/wp-content/uploads/sites/6/2024/06/600B-table-1.png?resize=480,156 480w" sizes="(max-width: 2410px) 100vw, 2410px"></figure>



<section>
<p>Note: It‚Äôs easy to calculate this metric directly. All you have to do is to take Nvidia‚Äôs run-rate revenue forecast and multiply it by 2x to reflect the total cost of AI data centers (GPUs are half of the total cost of ownership‚Äîthe other half includes energy, buildings, backup generators, etc)<sup>1</sup>. Then you multiply by 2x again, to reflect a 50% gross margin for the end-user of the GPU, (e.g., the startup or business buying AI compute from Azure or AWS or GCP, who needs to make money as well).</p>



<p>What has changed since September 2023?&nbsp;</p>



<ol>
<li><strong>The supply shortage has subsided: </strong>Late 2023 was the peak of the GPU supply shortage. Startups were calling VCs, calling anyone that would talk to them, asking for help getting access to GPUs. Today, that concern has been almost entirely eliminated. For most people I speak with, it‚Äôs relatively easy to get GPUs now with reasonable lead times.</li>



<li><strong>GPU stockpiles are growing:</strong> Nvidia reported in Q4 that about half of its data center revenue came from the large cloud providers. Microsoft alone likely represented approximately <a href="https://platformonomics.com/2024/02/follow-the-capex-triangulating-nvidia/comment-page-1/">22% of Nvidia‚Äôs Q4 revenue</a>. Hyperscale CapEx is reaching historic levels. These investments were a major theme of Big Tech Q1 ‚Äò24 earnings, with CEOs effectively telling the market: ‚ÄúWe‚Äôre going to invest in GPUs whether you like it or not.‚Äù Stockpiling hardware is not a new phenomenon, and the catalyst for a reset will be once the stockpiles are large enough that demand decreases.</li>



<li><strong>OpenAI still has the lion‚Äôs share of AI revenue:</strong> The Information recently reported that OpenAI‚Äôs revenue is now <a href="https://www.theinformation.com/articles/openais-annualized-revenue-doubles-to-3-4-billion-since-late-2023?rc=0uxjjk">$3.4B</a>, up from $1.6B in late 2023. While we‚Äôve seen a handful of startups scale revenues into the &lt;$100M range, the gap between OpenAI and everyone else continues to loom large. Outside of ChatGPT, how many AI products are consumers really using today? Consider how much value you get from Netflix for $15.49/month or Spotify for $11.99. Long term, AI companies will need to deliver significant value for consumers to continue opening their wallets.&nbsp;&nbsp;</li>



<li><strong>The $125B hole is now a $500B hole: </strong>In the last analysis, I generously assumed that each of Google, Microsoft, Apple and Meta will be able to generate $10B annually from new AI-related revenue. I also assumed $5B in new AI revenue for each of Oracle, ByteDance, Alibaba, Tencent, X, and Tesla. Even if this remains true and we add a few more companies to the list, the $125B hole is now going to become a $500B hole.&nbsp;</li>



<li><strong>It‚Äôs not over‚Äîthe B100 is coming</strong>:<strong> </strong>Earlier this year, Nvidia announced their B100 chip, which will have <a href="https://www.nextplatform.com/2024/03/18/with-blackwell-gpus-ai-gets-cheaper-and-easier-competing-with-nvidia-gets-harder/">2.5x better performance</a> for only 25% more cost. I expect this will lead to a final surge in demand for NVDA chips. The B100 represents a dramatic cost vs. performance improvement over the H100, and there will likely be yet another supply shortage as everyone tries to get their hands on B100s later this year.</li>
</ol>



<p>One of the major rebuttals to my last piece was that ‚ÄúGPU CapEx is like building railroads‚Äù and eventually the trains will come, as will the destinations‚Äîthe new agriculture exports, amusement parks, malls, etc. I actually agree with this, but I think it misses a few points:</p>



<ol>
<li><strong>Lack of pricing power:</strong> In the case of physical infrastructure build outs, there is some intrinsic value associated with the infrastructure you are building. If you own the tracks between San Francisco and Los Angeles, you likely have some kind of monopolistic pricing power, because there can only be so many tracks laid between place A and place B. In the case of GPU data centers, there is much less pricing power. GPU computing is increasingly turning into a commodity, metered per hour. Unlike the CPU cloud, which became an oligopoly, new entrants building dedicated AI clouds continue to flood the market. Without a monopoly or oligopoly, high fixed cost + low marginal cost businesses almost always see prices competed down to marginal cost (e.g., airlines).</li>



<li><strong>Investment incineration: </strong>Even in the case of railroads‚Äîand in the case of many new technologies‚Äîspeculative investment frenzies often lead to high rates of capital incineration. <a href="https://www.amazon.com/Engines-That-Markets-Alisdair-Nairn/dp/0857195999">The Engines that Moves Markets</a> is one of the best textbooks on technology investing, and the major takeaway‚Äîindeed, focused on railroads‚Äîis that a lot of people lose a lot of money during speculative technology waves. It‚Äôs hard to pick winners, but much easier to pick losers (canals, in the case of railroads).</li>



<li><strong>Depreciation: </strong>We know from the history of technology that semiconductors tend to get better and better. Nvidia is going to keep producing better next-generation chips like the B100. This will lead to more rapid depreciation of the last-gen chips. Because the market under-appreciates the B100 and the rate at which next-gen chips will improve, it overestimates the extent to which H100s purchased today will hold their value in 3-4 years. Again, this parallel doesn‚Äôt exist for physical infrastructure, which does not follow any ‚ÄúMoore‚Äôs Law‚Äù type curve, such that cost vs. performance continuously improves.&nbsp;</li>



<li><strong>Winners vs. losers:</strong> I think we need to look carefully at winners and losers‚Äîthere are always winners during periods of excess infrastructure building. AI is likely to be the next transformative technology wave, and as I mentioned in the last piece, declining prices for GPU computing is actually good for long-term innovation and good for startups. If my forecast comes to bear, it will cause harm primarily to investors. Founders and company builders will continue to build in AI‚Äîand they will be more likely to succeed, because they will benefit both from lower costs and from learnings accrued during this period of experimentation.&nbsp;</li>
</ol>



<p>A huge amount of economic value is going to be created by AI. Company builders focused on delivering value to end users will be rewarded handsomely. We are living through what has the potential to be a generation-defining technology wave. Companies like Nvidia deserve enormous credit for the role they‚Äôve played in enabling this transition, and are likely to play a critical role in the ecosystem for a long time to come.</p>



<p>Speculative frenzies are part of technology, and so they are not something to be afraid of. Those who remain level-headed through this moment have the chance to build extremely important companies. But we need to make sure not to believe in the delusion that has now spread from Silicon Valley to the rest of the country, and indeed the world. That delusion says that we‚Äôre all going to get rich quick, because AGI is coming tomorrow, and we all need to stockpile the only valuable resource, which is GPUs.&nbsp;</p>



<p>In reality, the road ahead is going to be a long one. It will have ups and downs. But almost certainly it will be worthwhile.</p>



<p><em>If you are building in this space, we‚Äôd love to hear from you. Please reach out at </em><a href="mailto:dcahn@sequoiacap.com"><em>dcahn@sequoiacap.com</em></a></p>



<ol>
<li>Some commenters challenged my 50% assumption on non-GPU data center costs, which I summarized as energy costs. Nvidia actually came to the exact same metric, which you can see on <a href="https://s201.q4cdn.com/141608511/files/doc_presentations/2023/Oct/01/ndr_presentation_oct_2023_final.pdf">Page 14 of their October 2023</a> analyst day presentation, published a few days after my last piece.</li>
</ol>
</section>











<div data-columns="12">

					<p>
						JOIN OUR MAILING LIST					</p>

					<h2>
						Get the best stories from the Sequoia community.					</h2>

					
					
					

				</div>


</article>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Voice Isolator: Strip background noise for film, podcast, interview production (154 pts)]]></title>
            <link>https://elevenlabs.io/voice-isolator</link>
            <guid>40869421</guid>
            <pubDate>Wed, 03 Jul 2024 19:51:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elevenlabs.io/voice-isolator">https://elevenlabs.io/voice-isolator</a>, See on <a href="https://news.ycombinator.com/item?id=40869421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2>Extract vocals</h2><p>Enhance your audio and clean up vocals with our AI Voice Isolator. Simply upload a file and remove street noise, mic feedback, and any other unwanted background noise. </p></div><div><p><h2>Frequently asked questions</h2></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Luakit: A fast, extensible, and customizable web browser (157 pts)]]></title>
            <link>https://luakit.github.io/</link>
            <guid>40868425</guid>
            <pubDate>Wed, 03 Jul 2024 17:58:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://luakit.github.io/">https://luakit.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=40868425">Hacker News</a></p>
<div id="readability-page-1" class="page">
	<nav>
		<a href="#">Home</a>
		<a href="https://luakit.github.io/news/">Releases</a>
		<a href="#sec-download">Download</a>
		<a href="https://luakit.github.io/docs/">Documentation</a>
		<a href="https://luakit.github.io/help/">Help &amp; Support</a>
		<a href="https://luakit.github.io/contributing/">Contributing</a>
		<a href="https://github.com/luakit/luakit" target="_blank">GitHub</a>
	</nav>
	<header>
		<div>
			
			<p>
				A fast, extensible, and<br> customizable web browser
			</p>
		</div>
	</header>
	<section id="sec-description">
		<p>Luakit is a highly configurable browser framework based on the <a href="http://webkit.org/" target="_blank">WebKit</a> web content engine and the <a href="http://gtk.org/" target="_blank">GTK+</a> toolkit. It is very fast, extensible with <a href="http://lua.org/" target="_blank">Lua</a>, and licensed under the <a href="https://raw.github.com/luakit/luakit/develop/COPYING.GPLv3" target="_blank">GNU GPLv3</a>
			license.  It is primarily targeted at power users, developers and anyone who wants to have fine-grained control over their web browser‚Äôs behaviour and
			interface.</p>
	</section>
	<section id="sec-release">
		<p>
			<h2>Latest release ‚Äî <a href="https://luakit.github.io/news/luakit-2.3.6.html">Luakit 2.3.6</a></h2>
			<h3>... or view the <a href="https://luakit.github.io/news/index.html">list of older releases</a></h3>
		</p>
	</section>
	<div id="sec-screenshots">
			<h2>Screenshots</h2>

			<div><div>
					<p><a href="https://luakit.github.io/screenshots/1.png"><img src="https://luakit.github.io/thumbs/1.png" alt="Screenshot of Luakit."></a></p><p>Luakit in normal mode when reading a web page.</p>
				</div><div>
					<p><a href="https://luakit.github.io/screenshots/2.png"><img src="https://luakit.github.io/thumbs/2.png" alt="Screenshot of Luakit."></a></p><p>Luakit in follow mode with several link hints visible.</p>
				</div><div>
					<p><a href="https://luakit.github.io/screenshots/3.png"><img src="https://luakit.github.io/thumbs/3.png" alt="Screenshot of Luakit."></a></p><p>Luakit showing the ad blocker settings page, listing currently active filter lists.</p>
				</div><div>
					<p><a href="https://luakit.github.io/screenshots/4.png"><img src="https://luakit.github.io/thumbs/4.png" alt="Screenshot of Luakit."></a></p><p>Luakit with a different theme and with vertical tabs enabled.</p>
			</div></div>
		</div>
	<div id="sec-download">
			<h2>Downloading Luakit</h2>

			<h3>Supported Operating Systems</h3>

			
                        <p><small><sup>*</sup>Not actively supported, but known to work.</small>
		</p></div>

	<div id="security-notice">
			<h3>Important WebKit Security Notice</h3>

			<p>While switching to the
			WebKit 2 API means a vastly improved security situation, not all distributions of Linux package the
			most up-to-date version of WebKitGTK+, and several package very outdated versions that
			have many known vulnerabilities. As of September 2019, Arch, Debian, Fedora,
			Gentoo, and Ubuntu all have the latest version of WebKitGTK+, but
			OpenSUSE ships an outdated and vulnerable version in their
			stable channel.

			</p><p>If you use Luakit for browsing, it is <i>your</i> responsibility to ensure that
			your distribution packages an up-to-date version of WebKitGTK+!</p>
		</div>

	<div>
			<h3>Installing on Windows 10</h3>
			<ol>
				<li><p>First, install the <a href="https://msdn.microsoft.com/en-us/commandline/wsl/install_guide" target="_blank">Windows Subsystem for Linux</a> if you have not already done so, as WebKitGTK+ does not natively support Windows.
				</p></li><li><p>Download, build, and install luakit from source, following the instructions below.
			</p></li></ol>
		</div>


	<div>
			<h3>Installing on Linux</h3>
			<ul>
				<li><p>Arch Linux users can install the <a href="https://archlinux.org/packages/community/x86_64/luakit/">luakit</a> community package or the
					<a href="https://aur.archlinux.org/packages/luakit-git/" target="_blank">luakit-git</a> package from the AUR.
				</p></li><li><p>Other users will need to download and build from source. A Debian package is in the works. Luakit contains only around 9000 lines of code, so this process is usually very fast.
			</p></li></ul>
		</div>

	<div>
			<h3>Installing on BSD</h3>
			<p>FreeBSD and OpenBSD users can install the <code>luakit</code> package.</p>
			<p>When installing from source, OpenBSD users should build with clang instead of GCC, as the GCC version shipped with OpenBSD is outdated and its use will result in compilation errors.</p>
		</div>

	<div>
			<h3>Installing from source</h3>
			<ol>
				<li><p>Ensure you have the following required dependencies installed:</p>
				<ul>
					<li><a href="https://www.gtk.org/" target="_blank">GTK 3</a></li>
					<li><a href="https://webkitgtk.org/" target="_blank">WebKitGTK+</a> (webkit2gtk)</li>
					<li><a href="https://www.lua.org/" target="_blank">Lua 5.1</a> or <a href="http://luajit.org/" target="_blank">LuaJIT</a></li>
					<li><a href="https://keplerproject.github.io/luafilesystem/" target="_blank">lfs (Lua File System)</a></li>
					<li><a href="https://sqlite.org/" target="_blank">SQLite 3</a></li>
				</ul>
				</li><li><p>Download the latest development version:
				<a href="https://github.com/luakit/luakit/zipball/develop">zip</a>,
				<a href="https://github.com/luakit/luakit/tarball/develop">tar</a>.
				Alternatively, clone the project with <a href="http://git-scm.com/" target="_blank">Git</a>
				by running:</p><pre>git clone git://github.com/luakit/luakit</pre>
				</li><li><p>Change into the target directory and run <code>make install</code>. You will probably also want to customize the prefix, which is <code>/usr/local</code> by default.
				Full instructions on building Luakit are available in the <code>README.md</code> file.</p>
				</li><li><p>After successfully building and installing luakit, run <code>luakit</code> from the command line or launch luakit from your application launcher.</p>
			</li></ol>
		</div>

	


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[A practical introduction to constraint programming using CP-SAT and Python (206 pts)]]></title>
            <link>https://pganalyze.com/blog/a-practical-introduction-to-constraint-programming-using-cp-sat</link>
            <guid>40867746</guid>
            <pubDate>Wed, 03 Jul 2024 16:48:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pganalyze.com/blog/a-practical-introduction-to-constraint-programming-using-cp-sat">https://pganalyze.com/blog/a-practical-introduction-to-constraint-programming-using-cp-sat</a>, See on <a href="https://news.ycombinator.com/item?id=40867746">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Imagine you're an e-commerce giant that would like to build a new warehouse to improve service to your customers, but you need to know what is the best location for it. Or you're a global shipping company that assigns packages to their delivery trucks and has to choose the best routes in order to save gas and reduce driver overtime. Or an airline that is looking to offer service to a new location, and needs to know which types of planes they should use and on what schedule, to maximize the resulting revenue.</p>
<p>These kinds of problems mentioned above are known as <strong>discrete optimization problems</strong>. There exist several methods that can be used to tackle such problems. In this article, we will discuss the theory and practice for one of them, called <strong><a href="https://en.wikipedia.org/wiki/Constraint_programming">constraint programming</a></strong>.</p>
<p>This is the first part in a two part series on constraint programming, used inside <a href="https://pganalyze.com/docs/indexing-engine/cp-model">pganalyze Indexing Engine</a> and <a href="https://pganalyze.com/docs/index-advisor/getting-started">pganalyze Index Advisor</a>. We're sharing this knowledge to help the wider programming community become familiar with how to utilize solvers like <a href="https://developers.google.com/optimization/cp/cp_solver">CP-SAT</a> in practice.</p>
<div>
<ul>
<li>
<p><a href="#a-declarative-paradigm">A declarative paradigm</a></p>
</li>
<li>
<p><a href="#the-basics-of-constraint-programming-cp">The basics of constraint programming (CP)</a></p>
</li>
<li>
<p><a href="#a-practical-example-with-python-and-cp-sat">A practical example with Python and CP-SAT</a></p>
<ul>
<li><a href="#an-empty-model">An empty model</a></li>
<li><a href="#the-data">The data</a></li>
<li><a href="#the-variables">The variables</a></li>
<li><a href="#the-constraints">The constraints</a></li>
<li><a href="#solving-the-model">Solving the model</a></li>
<li><a href="#adding-more-constraints">Adding more constraints</a></li>
<li><a href="#interlude-the-solver-status">Interlude: The solver status</a></li>
<li><a href="#sorry-emma">"Sorry, Emma"</a></li>
<li><a href="#objective-distributing-the-shifts-more-evenly">Objective: Distributing the shifts more evenly</a></li>
</ul>
</li>
<li>
<p><a href="#concluding-remarks">Concluding remarks</a></p>
</li>
</ul>
</div>
<h2 id="a-declarative-paradigm"><a href="#a-declarative-paradigm" aria-label="a declarative paradigm permalink"></a>A declarative paradigm</h2>
<p>Constraint programming (CP) is a declarative paradigm used to solve discrete optimization problems. This contrasts with the imperative paradigm that we are generally used to. When programming imperatively, we describe the steps necessary to reach a result. For example, suppose that we want to know who the adults are from a given list of people:</p>








<table><colgroup>
<col></colgroup><colgroup><col>
</colgroup><thead><tr><th scope="col">Name</th><th scope="col">Age</th></tr></thead><tbody><tr><td>Phil</td><td>20</td></tr><tr><td>Emma</td><td>17</td></tr><tr><td>David</td><td>11</td></tr><tr><td>Thomas</td><td>51</td></tr><tr><td>Sarah</td><td>45</td></tr><tr><td>Rebecca</td><td>6</td></tr></tbody></table>
<p>A typical imperative approach would explain the sequence of operations required to get the desired result:</p>
<div data-language="python"><pre><code>adult_people <span>=</span> <span>[</span><span>]</span>
<span>for</span> person <span>in</span> people<span>:</span>
    <span>if</span> person<span>.</span>Age <span>&gt;=</span> <span>18</span><span>:</span>
        adult_people <span>+=</span> person<span>.</span>Name</code></pre></div>

<p>Meanwhile, the same result can be described declaratively as:</p>
<div data-language="sql"><pre><code><span>SELECT</span> person_name <span>FROM</span> people <span>WHERE</span> age <span>&gt;=</span> <span>18</span><span>;</span></code></pre></div>

<p>Both approaches are equivalent in their outcome, but the two processes are different. In the imperative case, the program follows each step in sequence in order to reach the result. In the declarative case, the program is given a description of the desired result (using the available constructs of the language) and gets there by itself.</p>
<h2 id="the-basics-of-constraint-programming-cp"><a href="#the-basics-of-constraint-programming-cp" aria-label="the basics of constraint programming cp permalink"></a>The basics of constraint programming (CP)</h2>
<p>Similarly to the declarative example mentioned above, with CP we describe the desired result to a problem. This description is called a <strong>model</strong>. The main components of a model are variables and constraints. Variables represent <strong>what</strong> we are looking for, and each variable has an associated <strong>domain</strong> which is the set of values that this variable is allowed to take. Constraints describe <strong>relationships</strong> between variables.</p>
<p>A solution is an assignment of values to the variables (from their domains) such that the constraints are satisfied. Let's jump straight in with a simple example:</p>
<blockquote>
<div><p>Alice, Bob, and Carol each have $20, and they want to pool their money to purchase a candy bar worth $50 (yes, inflation is running wild). Alice has stated that she will put in at least as much money as Bob. Carol only has $5 bills, so her contribution will be a multiple of that. None of them want to contribute the exact same amount as any other.
</p><p>
How much should each of them chip in?</p></div>
</blockquote>
<p>Here, we are looking for the amount of money each person should contribute to the purchase of the candy bar. This means that we need one variable per person, indicating the amount of money that this person should contribute toward the purchase (<code>a</code> for Alice, <code>b</code> for Bob, and <code>c</code> for Carol). We start by setting the domains of these variables (the symbol <code>‚àà</code> means "in"):</p>
<div data-language="text"><pre><code>a ‚àà {0, ..., 20}
b ‚àà {0, ..., 20}
c ‚àà {0, ..., 20}</code></pre></div>
<p>These domains ensure that the final values of the variables represent amounts of money that the contributors actually have in their pockets (that is, a maximum of $20). Next, we want to make sure that the combined contributions are enough to cover the price of the candy bar, so we add the constraint:</p>

<p>Alice will contribute at least as much as Bob, so we translate this into:</p>

<p>Carol's contribution must be a multiple of 5:</p>

<p>Finally, the amount of each of their contributions must be unique. We could model this using the following constraints:</p>

<p>We only have a handful of people in this example so these disequalities would work well. But what if they were hundreds, or thousands? It turns out that CP has a rich catalogue of expressive constraints that can encapsulate complex concepts, called <strong>global constraints</strong>. An alternative to the above would be to use the so-called <code>alldifferent</code> constraint, which ensures that a set of variables are all assigned different values:</p>

<p>This completes the model of this problem. You will note that we have not assigned any values to <code>a</code>, <code>b</code>, or <code>c</code> ourselves. We have simply defined three variables and their domains, and described the properties of the problem using constraints on those variables. Our job is done.</p>
<p>The piece of software that interprets this model and returns a solution is called a <strong>solver</strong>. The inner workings of a solver are outside the scope of this article, so for our purposes we will consider the solver as a black box that takes a model as input, and returns a valid solution:</p>

<p>The solution returned is valid as each variable takes a value from its domain and all the constraints are satisfied. However, we see that Carol contributes almost twice Bob's amount. Perhaps there exists another valid solution where all parties contribute more equally to the purchase?</p>
<p>We can add an <strong>objective</strong> to our CP model to try to reach this goal. Adding an objective to a model allows us to minimize or maximize an expression, without compromising the validity of the resulting solution (with respect to the constraints). If we can somehow minimize the amount of money spent by the largest contributor, this should push the three contributions closer together. Our objective will then be to find a valid solution where this value is minimal:</p>
<div data-language="text"><pre><code>x ‚àà {0, ..., 20}
maximum(x, [a, b, c])
minimize: x</code></pre></div>
<p>To achieve this, we create a new variable <code>x</code> representing the amount of the largest contribution. The <code>maximum</code> constraint takes care of assigning to <code>x</code> the largest value from <code>[a, b, c]</code>. The objective is then to minimize <code>x</code>. The solution returned by the solver is now:</p>
<div data-language="text"><pre><code>a = 18
b = 17
c = 15
x = 18</code></pre></div>
<p>Previously there was a $9 difference between the largest and smallest contributions. With the objective we introduced this has now been reduced to $3, and this is as fair as this is going to get. Now that the basic concepts are cleared up, let's move on to a more challenging problem.</p>
<h2 id="a-practical-example-with-python-and-cp-sat"><a href="#a-practical-example-with-python-and-cp-sat" aria-label="a practical example with python and cp sat permalink"></a>A practical example with Python and CP-SAT</h2>
<p>Let's use this new CP knowledge to solve a more complex real-world example: the scheduling of employees for a small business.</p>
<blockquote>
<div><p>A store owner wishes to create the weekly work schedule for its employees. The store is open from 8AM to 8PM every day, and each day is divided into three shifts of 4 hours: morning, afternoon, and evening. There are two roles in the store: cashier and restocker.
</p></div>
<ul>
<li>
<p>Some employees are qualified to do either role, but others can only be a cashier, or a restocker.</p>
</li>
<li>
<p>There has to be a cashier scheduled at all times, but restocking only takes about 4 hours every day. Hence, for the restocking task we only need to schedule an employee for a single shift every day. This can be any shift, but two restocking shifts cannot be scheduled one after the other. If a restocking is scheduled on the evening shift on Tuesday, for example, we cannot schedule the Wednesday restocking on the morning shift.</p>
</li>
<li>
<p>An employee that is qualified in both roles can still only be assigned to one role per shift.</p>
</li>
<li>
<p>Employees cannot work more than 8 hours per day, which is 2 shifts. If they do work 2 shifts in a day, we must ensure that there is no idle time between these shifts‚Äîfor example, we can't schedule them on both the morning and the evening shifts of the same day, as they would be idle for 4 hours during the afternoon shift.</p>
</li>
</ul>
</blockquote>
<p>This is the basic premise of the problem. Let's break this down into manageable parts.</p>
<h3 id="an-empty-model"><a href="#an-empty-model" aria-label="an empty model permalink"></a>An empty model</h3>
<p>We first start by creating an empty model using <a href="https://developers.google.com/optimization/cp/cp_solver">CP-SAT</a>, an open-source CP solver developed by Google as part of it's <a href="https://developers.google.com/optimization">OR-Tools</a> project.</p>
<div data-language="python"><pre><code><span>from</span> ortools<span>.</span>sat<span>.</span>python <span>import</span> cp_model

model <span>=</span> cp_model<span>.</span>CpModel<span>(</span><span>)</span></code></pre></div>
<h3 id="the-data"><a href="#the-data" aria-label="the data permalink"></a>The data</h3>
<blockquote>
<p>A store owner wishes to create the <span>weekly work schedule</span> for its <span>employees</span>. The store is open from 8AM to 8PM every day, and each day is divided into <span>three shifts</span> of 4 hours: <span>morning, afternoon, and evening</span>. There are <span>two roles</span> in the store: <span>cashier and restocker</span>. Some employees are <span>qualified</span> to do either role, but others can only be a cashier, or a restocker.</p>
</blockquote>
<p>Let's create a list of employees and the roles they are qualified for:</p>
<div data-language="python"><pre><code>employees <span>=</span> <span>{</span><span>"Phil"</span><span>:</span> <span>[</span><span>"Restocker"</span><span>]</span><span>,</span>
             <span>"Emma"</span><span>:</span> <span>[</span><span>"Cashier"</span><span>,</span> <span>"Restocker"</span><span>]</span><span>,</span>
             <span>"David"</span><span>:</span> <span>[</span><span>"Cashier"</span><span>,</span> <span>"Restocker"</span><span>]</span><span>,</span>
             <span>"Rebecca"</span><span>:</span> <span>[</span><span>"Cashier"</span><span>]</span><span>}</span></code></pre></div>
<p>The schedule is said to span a week, and we are told that there are three types of shifts, and two types of roles:</p>
<div data-language="python"><pre><code>days <span>=</span> <span>[</span><span>"Monday"</span><span>,</span>
        <span>"Tuesday"</span><span>,</span>
        <span>"Wednesday"</span><span>,</span>
        <span>"Thursday"</span><span>,</span>
        <span>"Friday"</span><span>,</span>
        <span>"Saturday"</span><span>,</span>
        <span>"Sunday"</span><span>]</span>

shifts <span>=</span> <span>[</span><span>"Morning"</span><span>,</span>
          <span>"Afternoon"</span><span>,</span>
          <span>"Evening"</span><span>]</span>

roles <span>=</span> <span>[</span><span>"Cashier"</span><span>,</span>
         <span>"Restocker"</span><span>]</span></code></pre></div>
<h3 id="the-variables"><a href="#the-variables" aria-label="the variables permalink"></a>The variables</h3>
<p>Now, let's define <strong>what</strong> we are looking for. To describe the schedule, we need to refer to employees, roles, days, and shifts: <strong>Does Emma work as a restocker on the Monday evening shift?</strong> This can be achieved using boolean variables. A boolean variable is a variable with a domain of <code>{0, 1}</code>.</p>
<div data-language="python"><pre><code>schedule <span>=</span> <span>{</span>e<span>:</span>
             <span>{</span>r<span>:</span>
               <span>{</span>d<span>:</span>
                 <span>{</span>s<span>:</span> model<span>.</span>new_bool_var<span>(</span><span><span>f"schedule_</span><span><span>{</span>e<span>}</span></span><span>_</span><span><span>{</span>r<span>}</span></span><span>_</span><span><span>{</span>d<span>}</span></span><span>_</span><span><span>{</span>s<span>}</span></span><span>"</span></span><span>)</span>
                   <span>for</span> s <span>in</span> shifts<span>}</span>
                 <span>for</span> d <span>in</span> days<span>}</span>
               <span>for</span> r <span>in</span> roles<span>}</span>
             <span>for</span> e <span>in</span> employees<span>}</span></code></pre></div>
<p>The function <code>model.new_bool_var()</code> creates and then returns a boolean variable, which we store in <code>schedule</code>. Within this structure, <code>schedule["Emma"]["Restocker"]["Monday"]["Evening"]</code> refers to one of those boolean variables. This variable is equal to <code>1</code> if Emma <strong>does</strong> work as a restocker on the Monday evening shift, or to <code>0</code> if she <strong>doesn't</strong>.</p>
<p>Our <code>schedule</code> variables are currently unconstrained. By following the problem description presented earlier and constraining these variables accordingly, we should be able to get a schedule that satisfies the requirements of the store owner.</p>
<h3 id="the-constraints"><a href="#the-constraints" aria-label="the constraints permalink"></a>The constraints</h3>
<p>Let's go through the rest of the problem description to correctly constrain the <code>schedule</code> variables. To add a new constraint to the model, we simply use <code>model.add(...)</code>.</p>
<blockquote>
<p>There has to be a <span>cashier</span> scheduled at <span>all times</span>.</p>
</blockquote>
<p>Since the schedule is comprised of boolean variables, it's easy to see how we can put limits on subsets of these variables by summing them and enforcing constraints on these sums:</p>
<div data-language="python"><pre><code><span>for</span> d <span>in</span> days<span>:</span>
    <span>for</span> s <span>in</span> shifts<span>:</span>
        model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span><span>"Cashier"</span><span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> e <span>in</span> employees<span>)</span> <span>==</span> <span>1</span><span>)</span></code></pre></div>
<p>If we need a cashier at all times, this means that for every day-shift pair, the sum of employees assigned the <code>"Cashier"</code> role has to be equal to <code>1</code>.</p>
<blockquote>
<p>For the <span>restocking</span> task we only need to schedule an employee for <span>a single shift every day</span>.</p>
</blockquote>
<p>Similarly to the previous constraint, we now want the sum of all employees assigned the <code>"Restocker"</code> role for all shifts of a given day to be equal to <code>1</code>:</p>
<div data-language="python"><pre><code><span>for</span> d <span>in</span> days<span>:</span>
    model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span><span>"Restocker"</span><span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> e <span>in</span> employees <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>1</span><span>)</span></code></pre></div>
<blockquote>
<p>This [restocking] can be any shift, but <span>two restocking shifts cannot be scheduled one after the other</span>.</p>
</blockquote>
<p>Because of the previous constraint, we already know that two restocking shifts cannot take place on the same day. The only way two restocking shifts could be scheduled one after the other would be to assign one on the evening shift of a day, and another one on the morning shift of the next day. By enforcing that the sum of restocking shifts for each evening-morning pair is not greater than <code>1</code>, we ensure that <strong>at most one</strong> restocking shift is scheduled for those pairs:</p>
<div data-language="python"><pre><code><span>for</span> i <span>in</span> <span>range</span><span>(</span><span>len</span><span>(</span>days<span>)</span><span>-</span><span>1</span><span>)</span><span>:</span>
    model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span><span>"Restocker"</span><span>]</span><span>[</span>days<span>[</span>i<span>]</span><span>]</span><span>[</span><span>"Evening"</span><span>]</span> <span>+</span> schedule<span>[</span>e<span>]</span><span>[</span><span>"Restocker"</span><span>]</span><span>[</span>days<span>[</span>i<span>+</span><span>1</span><span>]</span><span>]</span><span>[</span><span>"Morning"</span><span>]</span> <span>for</span> e <span>in</span> employees<span>)</span> <span>&lt;=</span> <span>1</span><span>)</span></code></pre></div>
<blockquote>
<p>An <span>employee</span> that is qualified in both roles can still <span>only be assigned to one role per shift</span>.</p>
</blockquote>
<p>For every employee, the sum of all assigned roles for all day-shift pairs is either going to be <code>1</code> (they work a <em>single</em> role on this day-shift slot), or <code>0</code> (they don't work on that day-shift slot):</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    <span>for</span> d <span>in</span> days<span>:</span>
        <span>for</span> s <span>in</span> shifts<span>:</span>
            model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles<span>)</span> <span>&lt;=</span> <span>1</span><span>)</span></code></pre></div>
<blockquote>
<p>Some employees are qualified to do either role, but <span>others can only be a cashier, or a restocker</span>.</p>
</blockquote>
<p>To prevent an employee from being assigned a role that they are not qualified for, we simply match the value of that role to 0 (or put differently, we're adding a constraint asserting that it's zero) everywhere for that employee:</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    <span>for</span> r <span>in</span> roles<span>:</span>
        <span>for</span> d <span>in</span> days<span>:</span>
            <span>for</span> s <span>in</span> shifts<span>:</span>
                <span>if</span> r <span>not</span> <span>in</span> employees<span>[</span>e<span>]</span><span>:</span>
                    model<span>.</span>add<span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>==</span> <span>0</span><span>)</span></code></pre></div>
<blockquote>
<p><span>Employees cannot work more</span> than 8 hours per day, which is <span>2 shifts</span>. If they do work 2 shifts in a day, we must ensure that there is <span>no idle time between these shifts</span>‚Äîin other words, we can't schedule them on both the morning and the evening shifts of the same day, as they would be idle for 4 hours during the afternoon shift.</p>
</blockquote>
<p>It turns out that a single constraint can take care of both of these requirements: An employee can work <strong>either</strong> the morning shift, <strong>or</strong> the evening shift, <strong>or</strong> neither of these shifts:</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    <span>for</span> d <span>in</span> days<span>:</span>
        model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span><span>"Morning"</span><span>]</span> <span>+</span> schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span><span>"Evening"</span><span>]</span> <span>for</span> r <span>in</span> roles<span>)</span> <span>&lt;=</span> <span>1</span><span>)</span></code></pre></div>
<p>Note that the above constraint does not need to specify anything about the afternoon shift. If the employee works in the morning, they can't work in the evening, and vice-versa. This both ensures that the employee works a maximum of two shifts per day, and also that there is no idle time, since there can only be idle time if the employee works both the morning <strong>and</strong> the evening shift.</p>
<p>The modeling of the problem is now complete. Let's see what the results are.</p>
<h3 id="solving-the-model"><a href="#solving-the-model" aria-label="solving the model permalink"></a>Solving the model</h3>
<p>To solve the model, we simply call a solver, with the model as an argument:</p>
<div data-language="python"><pre><code>solver <span>=</span> cp_model<span>.</span>CpSolver<span>(</span><span>)</span>

solver<span>.</span>solve<span>(</span>model<span>)</span></code></pre></div>
<p>After the solving process, we can get the resulting values of the <code>schedule</code> variables with <code>solver.value(...)</code>. We have organized these values in the schedule shown below:</p>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   7   |
Emma       |   |   |   | C |   |   | C |   |   | C |   |   | C |   |   | C |   |   | C |   |   |   6   |
David      | C |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   1   |
Rebecca    |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |  14   |</code></pre></div>
<p>The resulting schedule satisfies the constraints of the problem. Now let's think of additional real-world constraints that would make this more interesting.</p>
<h3 id="adding-more-constraints"><a href="#adding-more-constraints" aria-label="adding more constraints permalink"></a>Adding more constraints</h3>
<p>We see that Rebecca works 14 shifts for that week. The store owner is not keen on paying overtime wages, so they wish to cap each employee's schedule to a maximum of 40 hours per week (10 work shifts):</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> days <span>for</span> s <span>in</span> shifts<span>)</span> <span>&lt;=</span> <span>10</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   7   |
Emma       |   | C | C |   | C | C | C |   |   | C |   |   | C |   |   | C |   |   | C |   |   |   9   |
David      | C |   |   | C |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   2   |
Rebecca    |   |   |   |   |   |   |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |  10   |</code></pre></div>
<p>Phil is a full-time student and would like to work exactly 4 shifts per week. He also cannot work the morning and afternoon shifts during the week, in order to attend his classes.</p>
<div data-language="python"><pre><code>model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span><span>"Phil"</span><span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> days <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>4</span><span>)</span>

model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span><span>"Phil"</span><span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> days <span>if</span> d <span>not</span> <span>in</span> <span>[</span><span>"Saturday"</span><span>,</span> <span>"Sunday"</span><span>]</span> <span>for</span> s <span>in</span> shifts <span>if</span> s <span>in</span> <span>[</span><span>"Morning"</span><span>,</span> <span>"Afternoon"</span><span>]</span><span>)</span> <span>==</span> <span>0</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   |   |   |   | R |   |   | R |   |   | R |   |   |   |   | R |   |   |   |   |   4   |
Emma       | C | R |   |   |   | C | C |   |   | C |   |   | C | R |   | C |   |   | C | R |   |  10   |
David      |   | C | C | C | C |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   4   |
Rebecca    |   |   |   |   |   |   |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |  10   |</code></pre></div>
<p>Phil and Emma do not get along very well, and as such we don't want them working the same shifts.</p>
<div data-language="python"><pre><code><span>for</span> d <span>in</span> days<span>:</span>
    <span>for</span> s <span>in</span> shifts<span>:</span>
        model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> e <span>in</span> <span>[</span><span>"Phil"</span><span>,</span> <span>"Emma"</span><span>]</span> <span>for</span> r <span>in</span> roles<span>)</span> <span>&lt;=</span> <span>1</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   | R |   |   | R |   |   | R |   |   | R |   |   |   |   |   |   |   |   |   |   4   |
Emma       | C |   |   | C | C |   | C | C |   | C | C |   | C | C |   |   | C |   |   |   |   |  10   |
David      |   | C | C |   |   | C |   |   | C |   |   | C |   | R |   |   | R | C | C | R |   |  10   |
Rebecca    |   |   |   |   |   |   |   |   |   |   |   |   |   |   | C | C |   |   |   | C | C |   4   |</code></pre></div>
<p>No employee really likes to work on the weekend, so we would like to distribute these shifts equally between all employees. There are 8 such shifts (3 cashier shifts and 1 restocker shift on each day), and since we have 4 employees, we can give them all 2 shifts each for the weekend.</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> <span>[</span><span>"Saturday"</span><span>,</span> <span>"Sunday"</span><span>]</span> <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>2</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   |   |   |   | R |   |   | R |   |   |   |   |   |   | R |   |   | R |   |   |   4   |
Emma       | R | C |   | C | C |   | C |   |   |   |   | C |   | C | C |   |   |   |   | C | C |  10   |
David      | C |   |   |   |   | C |   | C | C | C | R |   | C | R |   |   |   | C | C |   |   |  10   |
Rebecca    |   |   | C |   |   |   |   |   |   |   | C |   |   |   |   | C | C |   |   |   |   |   4   |</code></pre></div>
<p>Emma would like to take Monday to Friday off, and asks us if this is possible. Let's see:</p>
<div data-language="python"><pre><code>model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span><span>"Emma"</span><span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> <span>[</span><span>"Monday"</span><span>,</span> <span>"Tuesday"</span><span>,</span> <span>"Wednesday"</span><span>,</span> <span>"Thursday"</span><span>,</span> <span>"Friday"</span><span>]</span> <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>0</span><span>)</span></code></pre></div>

<p>The solver returns a status of "infeasible". What does this mean?</p>
<h3 id="interlude-the-solver-status"><a href="#interlude-the-solver-status" aria-label="interlude the solver status permalink"></a>Interlude: The solver status</h3>
<p>The solver takes as input a model, and returns a status and a solution. Consider this simple case:</p>
<div data-language="text"><pre><code>x ‚àà {0, ..., 10}
y ‚àà {0, ..., 10}
x + y &gt;= 5
minimize: x + y

Status: OPTIMAL
x = 5
y = 0</code></pre></div>
<p>The variables <code>x</code> and <code>y</code> both have the domain <code>{0, ..., 10}</code>. The sum of the values assigned to them must be 5 or greater, and the objective is to minimize this sum. The solution <code>(x, y) = (5, 0)</code> is called <strong>optimal</strong>. A solution is optimal when there does not exist another solution that is <strong>strictly better</strong> than it. For instance, the solution <code>(x, y) = (3, 2)</code> would also be optimal.</p>
<p>Now consider the case:</p>
<div data-language="text"><pre><code>x ‚àà {0, ..., 10}
x &gt;= 15

Status: INFEASIBLE</code></pre></div>
<p>An <strong>infeasible</strong> status means that there exists no assignment of values to the variables such that the constraints are satisfied. In the above example, it is not possible to assign a value to <code>x</code> that is at least as large as 15, since the largest value in its domain is 10.</p>
<p>There are also other possibilities. Let's take a look at this last example:</p>
<div data-language="text"><pre><code>x ‚àà {0, ..., 10}
y ‚àà {0, ..., 10}
...  // Many more variables
x + y &lt;= 12
x &gt;= 5
... // Many more constraints
minimize: x - 3*y + ...  // Very complex objective</code></pre></div>
<p>Sometimes, a problem is so large and complex, and takes so much time to solve, that we must interrupt the solver due to time constraints. In such a case, the solver will return one of two statuses:</p>
<ul>
<li>If a solution has been found, the status returned is <strong>feasible</strong>. A feasible solution means that the solution satisfies the constraints, but the solver does not know if that solution is optimal. In other words, we have a solution that works, but there may still exist a better one.</li>
<li>If no solution has been found, the status returned is <strong>unknown</strong>. This means that while the solver has not found a solution, it does not know whether one exists (feasible) or if the problem has no solution (infeasible).</li>
</ul>
<p>Let's get back to our schedule.</p>
<h3 id="sorry-emma"><a href="#sorry-emma" aria-label="sorry emma permalink"></a>"Sorry, Emma"</h3>
<p>With the infeasible status returned to us previously, we are forced to inform Emma that she can't take the whole week off, otherwise it would be impossible to fill the schedule without violating some of the other constraints. She decides to only take Monday to Wednesday off instead:</p>
<div data-language="python"><pre><code>model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span><span>"Emma"</span><span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> <span>[</span><span>"Monday"</span><span>,</span> <span>"Tuesday"</span><span>,</span> <span>"Wednesday"</span><span>]</span> <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>0</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   | R |   |   |   |   |   | R |   |   |   |   |   |   | R |   |   | R |   |   |   4   |
Emma       |   |   |   |   |   |   |   |   |   |   | R | C |   | R | C |   |   |   |   | C | C |   6   |
David      | C |   |   | C | R |   | C |   |   | C | C |   | C | C |   |   |   | C | C |   |   |  10   |
Rebecca    |   | C | C |   | C | C |   | C | C |   |   |   |   |   |   | C | C |   |   |   |   |   8   |</code></pre></div>
<p>Phil works exactly 4 shifts as he wants, but the other shifts are not very well distributed: Emma gets 6, David gets 10, and Rebecca gets 8. Let's see if we can improve this by adding an objective.</p>
<h3 id="objective-distributing-the-shifts-more-evenly"><a href="#objective-distributing-the-shifts-more-evenly" aria-label="objective distributing the shifts more evenly permalink"></a>Objective: Distributing the shifts more evenly</h3>
<p>We would like to distribute the shifts as fairly as possible between Emma, David, and Rebecca. We will recall that an objective allows us to minimize the value of an expression. We could, for example, minimize the shift difference between the employee who is assigned the fewest shifts, and the one who is assigned the most. Currently, Emma is assigned 6 shifts to David's 10, so this difference stands at 4 shifts.</p>
<p>We start by creating integer variables to track the number of shifts assigned to each employee, and enforcing the values of these variables:</p>
<div data-language="python"><pre><code><span># total_shifts[e] indicates the number of shifts worked by employee `e`</span>
total_shifts <span>=</span> <span>{</span>e<span>:</span> model<span>.</span>new_int_var<span>(</span><span>0</span><span>,</span> <span>10</span><span>,</span> <span><span>f"total_shifts_</span><span><span>{</span>e<span>}</span></span><span>"</span></span><span>)</span>
                <span>for</span> e <span>in</span> employees<span>}</span>

<span>for</span> e <span>in</span> employees<span>:</span>
    model<span>.</span>add<span>(</span>total_shifts<span>[</span>e<span>]</span> <span>==</span> <span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> days <span>for</span> s <span>in</span> shifts<span>)</span><span>)</span></code></pre></div>
<p>Integer variables are created with <code>model.new_int_var(...)</code>, and the parameters allow us to specify the lower and upper bounds of the domain (in this case, the domain is <code>{0, ..., 10}</code>). We then create variables to track the highest and lowest number of shifts assigned to any employee (with the exception of Phil, who works part-time):</p>
<div data-language="python"><pre><code>min_shifts <span>=</span> model<span>.</span>new_int_var<span>(</span><span>0</span><span>,</span> <span>10</span><span>,</span> <span>"min_shifts"</span><span>)</span>
model<span>.</span>add_min_equality<span>(</span>min_shifts<span>,</span> <span>[</span>total_shifts<span>[</span>e<span>]</span> <span>for</span> e <span>in</span> employees <span>if</span> e <span>!=</span> <span>"Phil"</span><span>]</span><span>)</span>

max_shifts <span>=</span> model<span>.</span>new_int_var<span>(</span><span>0</span><span>,</span> <span>10</span><span>,</span> <span>"max_shifts"</span><span>)</span>
model<span>.</span>add_max_equality<span>(</span>max_shifts<span>,</span> <span>[</span>total_shifts<span>[</span>e<span>]</span> <span>for</span> e <span>in</span> employees <span>if</span> e <span>!=</span> <span>"Phil"</span><span>]</span><span>)</span></code></pre></div>
<p>The constraints <code>model.add_min_equality(...)</code> and <code>model.add_max_equality(...)</code> work in the same fashion as the <code>maximum(...)</code> constraint presented in the candy bar example (with Alice, Bob, and Carol). For example, <code>model.add_min_equality(var, list)</code> assigns to integer variable <code>var</code> the smallest value among the integer variables in <code>list</code>.</p>
<p>Finally, we minimize the difference between <code>max_shifts</code> and <code>min_shifts</code>:</p>
<div data-language="python"><pre><code>model<span>.</span>minimize<span>(</span>max_shifts <span>-</span> min_shifts<span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   | R |   |   |   |   |   | R |   |   |   |   |   |   | R |   |   | R |   |   |   4   |
Emma       |   |   |   |   |   |   |   |   |   |   | R | C |   | R | C |   |   |   |   | C | C |   6   |
David      | C |   |   | C | R |   | C |   |   |   | C |   | C | C |   |   |   | C | C |   |   |   9   |
Rebecca    |   | C | C |   | C | C |   | C | C | C |   |   |   |   |   | C | C |   |   |   |   |   9   |</code></pre></div>
<p>This looks pretty good. David and Rebecca both get 9 shifts. Emma only gets 6, but that is understandable as she is taking 3 days off that week. Phil gets exactly 4 shifts, as he is supposed to. Hopefully, everyone will be happy with this schedule.</p>

<p>We have introduced the basics of constraint programming and discussed its main components. We have built a model to generate work schedules that can take into account the types of constraints that one could reasonably expect to be faced with in the real world.</p>
<p>Thanks to this model, we could easily see that it would not be possible for Emma to take five days off as she initially wanted, and we were able to distribute the work shifts as fairly as possible between the employees. In the end, we created a schedule that satisfied both the requiremends of the store owner, as well as the needs of the employees.</p>
<p>In the next article, we will discuss how to use constraint programming for index selection in Postgres.</p>
<p><strong>The code presented in this article can be found on the <a href="https://github.com/pganalyze/cp-sat-python-example">pganalyze GitHub</a>.</strong></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Cheapest NAS (235 pts)]]></title>
            <link>https://sigwait.org/~alex/blog/2024/07/01/the-cheapest-nas.html</link>
            <guid>40867709</guid>
            <pubDate>Wed, 03 Jul 2024 16:45:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sigwait.org/~alex/blog/2024/07/01/the-cheapest-nas.html">https://sigwait.org/~alex/blog/2024/07/01/the-cheapest-nas.html</a>, See on <a href="https://news.ycombinator.com/item?id=40867709">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<section>
<article data-file="2024/07/01/the-cheapest-nas.md">

  <nav>
    <span id="nbe_post--next"></span>
    <span id="nbe_post--prev"></span>
  </nav>

  <h2>The cheapest NAS</h2>

  <p>Latest update: <time datetime="2024-07-03T16:09:11.072Z">2024-07-03 19:09:11</time></p>

  

  <p>I wanted to replace my old trusty 'router' (with an attached
HDD)--that was not working as a router, but as a network drive after
flashing OpenWRT onto it--I wanter to replace it with an SBC+HDD
combo.</p>
<p>This new device should not only preserve all the services the old one
provided (samba, git, rsyncd, a dnf repo), but also perform faster,
for having a potato instead of a CPU, the ex-router struggled with
rsync over ssh &amp;, being gravely RAM limited, choked when I did 'git
push' commits containing binaries &gt; 15MB.</p>
<p>Searching for a suitable SBC led me to libre.computer, a company I had
never heard of before. At first glance, they had the el cheapo
<a href="https://libre.computer/products/aml-s805x-ac/">AML-S805X-AC</a> board I needed:</p>
<ul>
<li>LAN port (but 100 Mb only);</li>
<li>2 USB-A (but 2.0 only);</li>
<li>4-core ARM Cortex-A53;</li>
<li>1 GB RAM;</li>
<li>booting from an USB;</li>
<li>up-to-date Debian;</li>
<li>easy to buy without hunting it down.</li>
</ul>
<p>100Mb may seem like a joke nowadays, but the main purpose of such a
toy NAS for me is to keep a copy of a directory with ~200K small
files. Having 1Gb would only marginally improve the syncing speed even
if the SBC supported USB 3.0.</p>
<p>But this is just a board. I also needed an hdd enclosure with an
external power supply (for the device provides up to 900mA for
each USB-A), at least 3A power supply &amp; a micro-USB cable that can
handle 3A.</p>
<table id="tcnagcb-1">
<thead>
<tr><th>Item</th><th>Price, ‚Ç¨</th><th>Comment</th></tr>
</thead>
<tbody>
<tr><td>SBC</td><td>20</td><td></td></tr>
<tr><td>HDD enclosure</td><td>12</td><td></td></tr>
<tr><td>3A Power Supply</td><td>5</td><td></td></tr>
<tr><td>Micro-USB cable</td><td>3</td><td></td></tr>
<tr><td>4 bolts, 12 nuts</td><td>0</td><td>I think the ones I found are older than me</td></tr>
<tr><td>TTL to USB dongle</td><td>3</td><td>Optional, the board has an HDMI output</td></tr>
<tr><th>Total</th><td>43</td><td></td></tr>
</tbody>
</table>



<p>(I didn't include an HDD in the table, for I presume everyone has a
couple of them lying around.)</p>
<p>When I bought the HDD enclosure, I didn't read the description
carefully &amp; thought it was going to be a small plastic container for
2.5-inch drives, but when the package arrived, it turned out to be a
box for 3.5-inch ones. Hence, I decided to shove the SBC into it too.</p>
<img alt="" src="https://sigwait.org/~alex/blog/2024/07/01/innards.avif">
<img alt="" src="https://sigwait.org/~alex/blog/2024/07/01/rear.avif">

<p>After connecting the TTL-to-USB dongle to the board's GPIO</p>
<img alt="" src="https://sigwait.org/~alex/blog/2024/07/01/gpio.avif">

<p>&amp; typing</p>
<pre><code>$ sudo screen /dev/ttyUSB0 115200
</code></pre>
<p>one of the 1st readouts appeared as:</p>
<pre><code>U-Boot 2023.07+ (Nov 03 2023 - 15:10:36 -0400) Libre Computer AML-S805X-AC

Model: Libre Computer AML-S805X-AC
SoC:   Amlogic Meson GXL (S805X) Revision 21:d (34:2)
DRAM:  512 MiB (effective 1 GiB)
</code></pre>
<p>What does the last line mean exactly? After I dd'ed Debian-12 onto a
flash drive, free(1) said it saw 1GB. Anyway, libre.computer has an
official OS image, based on stock Debian:</p>
<pre><code>$ fdisk debian-12-base-arm64+arm64.img -l
Disk debian-12-base-arm64+arm64.img: 2.25 GiB, 2415919104 bytes, 4718592 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x71f3f7cf

Device                          Boot  Start     End Sectors  Size Id Type
debian-12-base-arm64+arm64.img1 *      2048  524287  522240  255M ef EFI (FAT-12/16/32)
debian-12-base-arm64+arm64.img2      524288 4718591 4194304    2G 83 Linux
</code></pre>
<p>Yes, it has an EFI partition with the MBR layout! The 2nd partition is
btrfs (supposedly it's faster &amp; more gentle to flash storage than
ext4; no idea if both claims are true). You can examine its contents via:</p>
<pre><code>$ sudo mount -o loop,offset=$((524288*512)) debian-12-base-arm64+arm64.img ~/mnt/misc
</code></pre>
<p>This partition gets auto-resized on the 1st boot to fill the rest of
the free space available on the drive. Doing this on USB dongles
proved to be a miserable experience: of the 3 I had available, one
permanently got stuck on resizing, and another, despite finishing the
operation, became so sluggish afterwards that a 20-year-old PC would've
felt snappier.</p>
<p>This is I didn't like at all. There is no repo with from which the OS
image gets generated. The explanation is <a href="https://hub.libre.computer/t/source-code-git-repository-for-bootloaders-and-firmwares/2743/6">bizarre</a>:</p>
<blockquote>
<p>"The distribution builder is a proprietary commercial offering as it
involves a lot of customer IP and integrations so it cannot be
public."</p>
</blockquote>
<p>but with an consolation advice:</p>
<blockquote>
<p>"If you want to study them [images], bootstrap and do a diff. We
don't make any changes to the standard distros outside of setting a
few configs since we're not distro maintainers."</p>
</blockquote>
<p>Make of it what you will.</p>
<p>Then I connected the HDD enclosure to the board. This time, the
process went much, much faster (though there were still some
unexpected delays in random places). Right after logging in, I started
getting <code>uas_eh_abort_handler</code> errors from the kernel. It turns out I
got one of the worst HDD enclosure innards possible, if you believe
reviews from the interwebs:</p>
<pre><code>$ lsusb
Bus 001 Device 002: ID 152d:0578 JMicron Technology Corp. / JMicron USA Technology Corp. JMS578 SATA 6Gb/s
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
</code></pre>
<p>The remedy is to turn UAS off via adding
<code>usb-storage.quirks=152d:0578:u</code> to the kernel cmdline. It did help,
the delays went away, although 'benchmarks' became hardly thrilling:</p>
<pre><code>$ lsusb -t
/:  Bus 01.Port 1: Dev 1, Class=root_hub, Driver=xhci-hcd/2p, 480M
    |__ Port 2: Dev 2, If 0, Class=Mass Storage, Driver=usb-storage, 480M
$ sync; time sh -c "dd if=/dev/urandom of=1 bs=500k count=1k &amp;&amp; sync"; rm 1
1024+0 records in
1024+0 records out
524288000 bytes (524 MB, 500 MiB) copied, 15.1014 s, 34.7 MB/s

real    0m21.876s
user    0m0.001s
sys     0m7.976s
</code></pre>
<p>which means <math alttext="524/21.876 = 23.95">
<mfrac><mn>524</mn><mn>21.876</mn></mfrac><mo>=</mo><mn>23.95</mn>
</math> MB/s on an ext4 partition.</p>
<p>Would I recommend this setup? I wouldn't. One of the reasons I've
chosen the path with an SBC instead of a common micro-ITX route is to
save on <a href="https://sigwait.org/~alex/blog/2024/07/01/imf.html">power consumption</a>. If you don't have similar
problems, I see 0 reasons to struggle with such a finicky Chinese
device.</p>


  <br>Tags: <a href="https://sigwait.org/~alex/blog/t/da44800a6e351c6e94a3d3b6060d2be1.html">–æ–π—Ç—ñ</a>
  <br>Authors: <a href="https://sigwait.org/~alex/blog/a/4e42f7dd43ecbfe104de58610557c5ba.html">ag</a>

</article>
</section>

  <nav>
    

    
  </nav>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making a Linux-managed network switch (198 pts)]]></title>
            <link>https://blog.brixit.nl/making-a-linux-managed-network-switch/</link>
            <guid>40866442</guid>
            <pubDate>Wed, 03 Jul 2024 14:47:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.brixit.nl/making-a-linux-managed-network-switch/">https://blog.brixit.nl/making-a-linux-managed-network-switch/</a>, See on <a href="https://news.ycombinator.com/item?id=40866442">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>Network switches are simple devices, packets go in, packets go out. Luckily people have figured out how to make it complicated instead and invented managed switches.</p>

<p>Usually this is done by adding a web-interface for configuring the settings and see things like port status. If you have more expensive switches then you'd even get access to some alternate interfaces like telnet and serial console ports.</p>

<p>There is a whole second category of managed switches though that people don't initially think of. These are the network switches that are inside consumer routers. These routers are little Linux devices that have a switch chip inside of them, one or more ports are internally connected to the CPU and the rest are on the outside as physical ports.</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1719959978/RB2011UiAS-160620170256_160656.png"><figcaption>Mikrotik RB2011 block diagram from mikrotik.com</figcaption></figure>

<p>Here is an example of such a device that actually has this documented. I always thought that the configuration of these switch connected ports was just a nice abstraction by the webinterface but I was suprised to learn that with the DSA and switchdev subsystem in Linux these ports are actually fully functioning "local" network ports. Due to this practically only being available inside integrated routers It's pretty hard to play around with unfortunately.</p>

<p>What is shown as a single line on this diagram is actually the connection of the SoC of the router and the switch over the SGMII bus (or maybe RGMII in this case) and a management bus that's either SMI or MDIO. Network switches have a lot of these fun acronyms that even with the full name written out make little sense unless you know how all of this  fits together.</p>

<p>Controlling your standard off-the-shelf switch using this system simply isn't possible because the required connections of  the switch chip aren't exposed for this. So there's only one option left...</p>

<h2>Making my own gigabit network switch</h2>

<p>Making my own network switch can't be <i>that</i> hard right? Those things are available for the price of a cup of coffee and are most likely highly integrated to reach that price point. Since I don't see any homemade switches around on the internet I guess the chips for those must be pretty hard to get...</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1719960715/image.png"></figure>

<p>Nope, very easy to get. There's even a datasheet available for these. So I created a new KiCad project and started creating some footprints and symbols.</p>

<p>I'm glad there's any amount of datasheet available for this chip since that's not usually the case for Realtek devices, but it's still pretty minimal. I resorted to finding any devices that has schematics available for similar Realtek chips to find out how to integrate it and looking at a lot of documentation for how to implement ethernet in a design at all.</p>

<p>The implementation for the chip initially looked very complicated, there's about 7 different power nets it requires and there are several pretty badly documented communication interfaces. After going through other implementations it seem like the easiest way to power it is just connect all the nets with overlapping voltage ranges together and you're left with only needing a 3.3V and 1.1V regulator.</p>

<p>The extra communication busses are for all the extra ports I don't seem to need. The switch chip I selected is the RTL8367S which is a very widely used 5-port gigabit switch chip, but it's actually not a 5-port chip. It's a 7 port switch chip where 5 ports have an integrated PHY and two are for CPU connections.</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1719961532/image.png"><figcaption>CPU connection block diagram from the RTL8367S datasheet</figcaption></figure>

<p>My plan is different though, while there are these CPU ports available there is actually nothing in the Linux switchdev subsystem that requires the CPU connection to be to those ports. Instead I'll be connecting to port 0 on the switch with a network cable and as far  as the switchdev driver knows there's no ethernet PHY in between.</p>

<p>The next hurdle is the configuration of the switch chip, there's several configuration systems available and the datasheet does not really describe what is the minimum required setup to actually get it to function as a regular dumb switch. To sum up the configuration options of the chip:</p>

<ul><li>There's 8 pins on the chip that are read when it's starting up. These pins are shared with the led pins for the ports so that makes designing pretty annoying. Switching the setting from pull-up to pull-down also requires the led to be connected in the different orientation.</li>
<li>There's an i2c bus that can be connected to an eeprom chip. The pins for this are shared with the SMI bus that I require to make this chip talk to Linux though. There is pin configuration to select from one of two eeprom size ranges but does not further specify what this setting actually changes.</li>
<li>There's a SPI bus that supports connecting a NOR flash chip to it. This can store either configuration registers or firmware for the embedded 8051 core depending on the configuration of the bootup pins. The SPI bus pins are also shared with one of the CPU network ports.</li>
<li>There is a serial port available but from what I guess it probably does nothing at all unless there's firmware loaded in the 8051.</li>
</ul>

<p>My solution to figuring out is to just order a board and solder connections differently until it works. I've added a footprint for a flash chip that I ended up not needing and for all the configuration pins I added solder jumpers. I left out all the leds since making that configurable would be pretty hard.</p>

<p>The next step is figuring out how to do ethernet properly. There has been a lot of documentation written about this and they all make it sound like gigabit ethernet requires perfect precision engineering, impedance managed boards and a blessing from the ethernet gods themselves to work. This does not seem to match up with the reality that these switches are very very cheaply constructed and seem to work just fine. So I decided to open up a switch to check how many of these coupling capacitors and impedance matching planes are actually used in a real design. The answer seems to be that it doesn't matter that much.</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1719962591/image.png"></figure>

<p>This is the design I have ended up with now but it is not what is on my test PCB. I got it almost right the first time though :D</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1719962813/image.png"></figure>

<p>The important parts seem to be matching the pair skew but matching the length of the 4 network pairs is completely useless, this is mainly because network cables don't have the same twisting rate for the 4 pairs and so the length of these are already significantly different inside the cable.</p>

<p>The pairs between the transformer and the RJ45 jack has it's own ground plane that's coupled to the main ground through a capacitor. The pairs after the transformer are  just on the main board ground fill.</p>

<p>What I did wrong on my initial board revision was forgetting the capacitor that connects the center taps of the transformer on the switch side to ground making the signals on that side referenced to board ground. This makes ethernet very much not work anymore so I had to manually cut tiny traces on the board to disconnect that short to ground. In my test setup the capacitor just doesn't exist and all the center taps float. This seems to work just fine but the final design does have that capacitor added.</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1720003020/fixed.JPG"><figcaption>Cut ground traces on the ethernet transformer</figcaption></figure>

<p>The end result is this slightly weird gigabit switch. It has 4 ports facing one direction and one facing backwards and it is powered over a 2.54mm pinheader. I have also added a footprint for a USB Type-C connector to have an easy way to power it without bringing out the DuPont wires.</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1720007603/IMG_20240626_221246.jpg"></figure>

<h2>Connecting it to Linux</h2>

<p>For my test setup I've picked the PINE64 A64-lts board since it has the connectors roughly in the spots where I want them. It not being an x86 platform is also pretty important because configuration requires a device tree change, can't do that on a platform that doesn't use device trees.</p>

<p>The first required thing was rebuilding the kernel for the board since most kernels simply don't have these kernel modules enabled. For this I enabled these options:</p>

<ul><li><code>CONFIG_NET_DSA</code> for the Distributed Switch Architecture system</li>
<li><code>CONFIG_NET_DSA_TAG_RTL8_4</code> for having port tagging for this Realtek switch chip</li>
<li><code>CONFIG_NET_SWITCHDEV</code> the driver system for network switches</li>
<li><code>CONFIG_NET_DSA_REALTEK</code>, <code>CONFIG_NET_DSA_REALTEK_SMI</code>, <code>CONFIG_NET_DSA_REALTEK_RTL8365MB</code> for the actual switch chip driver</li>
</ul>

<p>Then the more complicated part was figuring out how to actually get this all loaded. In theory it is possible to create a device tree overlay for this and get it loaded by U-Boot. I decided to not do that and patch the device tree for the A64-lts board instead since I'm rebuilding the kernel anyway. The device tree change I ended up with is this:</p>

<pre><code>diff --git a/arch/arm64/boot/dts/allwinner/sun50i-a64-pine64-lts.dts b/arch/arm64/boot/dts/allwinner/sun50i-a64-pine64-lts.dts
index 596a25907..10c1a5187 100644
--- a/arch/arm64/boot/dts/allwinner/sun50i-a64-pine64-lts.dts
+++ b/arch/arm64/boot/dts/allwinner/sun50i-a64-pine64-lts.dts
@@ -18,8 +18,78 @@ led {
 			gpios = &lt;&amp;r_pio 0 7 GPIO_ACTIVE_LOW&gt;; /* PL7 */
 		};
 	};
+
+switch {
+	compatible = "realtek,rtl8365rb";
+	mdc-gpios = &lt;&amp;pio 2 5 GPIO_ACTIVE_HIGH&gt;; // PC5
+	mdio-gpios = &lt;&amp;pio 2 7 GPIO_ACTIVE_HIGH&gt;; // PC7
+	reset-gpios = &lt;&amp;pio 8 5 GPIO_ACTIVE_LOW&gt;; // PH5
+	realtek,disable-leds;
+
+	mdio {
+		compatible = "realtek,smi-mdio";
+		#address-cells = &lt;1&gt;;
+		#size-cells = &lt;0&gt;;
+
+		ethphy0: ethernet-phy@0 {
+			reg = &lt;0&gt;;
+		};
+
+		ethphy1: ethernet-phy@1 {
+			reg = &lt;1&gt;;
+		};
+
+		ethphy2: ethernet-phy@2 {
+			reg = &lt;2&gt;;
+		};
+
+		ethphy3: ethernet-phy@3 {
+			reg = &lt;3&gt;;
+		};
+
+		ethphy4: ethernet-phy@4 {
+			reg = &lt;4&gt;;
+		};
+	};
+
+	ports {
+		#address-cells = &lt;1&gt;;
+		#size-cells = &lt;0&gt;;
+
+		port@0 {
+			reg = &lt;0&gt;;
+			label = "cpu";
+			ethernet = &lt;&amp;emac&gt;;
+		};
+
+		port@1 {
+			reg = &lt;1&gt;;
+			label = "lan1";
+			phy-handle = &lt;&amp;ethphy1&gt;;
+		};
+
+		port@2 {
+			reg = &lt;2&gt;;
+			label = "lan2";
+			phy-handle = &lt;&amp;ethphy2&gt;;
+		};
+
+		port@3 {
+			reg = &lt;3&gt;;
+			label = "lan3";
+			phy-handle = &lt;&amp;ethphy3&gt;;
+		};
+
+		port@4 {
+			reg = &lt;4&gt;;
+			label = "lan4";
+			phy-handle = &lt;&amp;ethphy4&gt;;
+		};
+	};
+};
 };
 </code></pre>

<p>It loads the driver for the switch with the <code>realtek,rtl8365rb</code>, this driver supports a whole range of Realtek switch chips including the RTL8367S I've used in this design. I've removed the CPU ports from the documentation example and just added the definitions of the 5 regular switch ports.</p>

<p>The important part is in <code>port@0</code>, this is the port that is facing backwards on my switch and is connected to the A64-lts, I've linked it up to <code>&amp;emac</code> which is a reference to the ethernet port of the computer.  The rest of the ports are  linked up to their respective PHYs in the switch chip. </p>

<p>In the top of the code there's also 3 GPIOs defined, these link up to SDA/SCL and Reset on the switch PCB to make the communication work. After booting up the system the result is this:</p>

<pre><code>1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: &lt;BROADCAST,MULTICAST&gt; mtu 1508 qdisc noop state DOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
3 lan1@eth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
4 lan2@eth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
5 lan3@eth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
6 lan4@eth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff</code></pre>

<p>I have the <code>eth0</code> device here like normal and then I have the 4 interfaces for the ports on the switch I defined in the device tree. To make it actually do something the interfaces actually need to be brought online first:</p>

<pre><code>$ ip link set eth0 up
$ ip link set lan1 up
$ ip link set lan2 up
$ ip link set lan3 up
$ ip link set lan4 up
$ ip link
1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1508 qdisc mq state UP qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
3: lan1@eth0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
4: lan2@eth0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
5: lan3@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
6: lan4@eth0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff</code></pre>

<p>Now the switch is up you can see I have a cable plugged into the third port. This system hooks into a lot of the Linux networking so it Just Works(tm) with a lot of tooling. Some examples:</p>

<ul><li>Add a few of the lan ports into a standard Linux bridge and the switchdev system will bridge those ports together in the switch chip so Linux doesn't have to forward that traffic.</li>
<li>Thinks like <code>ethtool lan3</code> just work to get information about the link. and with <code>ethtool -S lan3</code> all the standard status return info which includes packets that have been fully handled by the switch.</li>
</ul>

<h2>Limitations</h2>

<p>There's a few things that makes this not very nice to work with. First of all the requirement of either building a custom network switch or tearing open an existing one and finding the right connections. </p>

<p>It's not really possible  to use this system on regular computers/servers since you need device trees to configure the kernel for this and most computers don't have kernel-controlled GPIO pins available to hook up a switch.</p>

<p>As far as I can find there's also no way to use this with a network port on the computer side that's not fixed, USB network interfaces don't have a device tree node handle to refer to to set the conduit port.</p>

<p>There is a chance some of these limitations are possible to work around, maybe there's some weird USB device that exposes pins on the GPIO subsystem, maybe there's a way to load switchdev without being on an ARM device but that would certainly take a bit more documentation...</p>


                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Do not taunt happy fun branch predictor (2023) (251 pts)]]></title>
            <link>https://www.mattkeeter.com/blog/2023-01-25-branch/</link>
            <guid>40866374</guid>
            <pubDate>Wed, 03 Jul 2024 14:42:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mattkeeter.com/blog/2023-01-25-branch/">https://www.mattkeeter.com/blog/2023-01-25-branch/</a>, See on <a href="https://news.ycombinator.com/item?id=40866374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<!-- End header -->






<p>I've been writing a lot of AArch64 assembly, for <em>reasons</em>.</p>
<p>I recently came up with a "clever" idea to eliminate one jump from an inner
loop, and was surprised to find that it slowed things down.  Allow me to explain
my terrible error, so that you don't fall victim in the future.</p>
<p>A toy model of the relevant code looks something like this:</p>
<pre><code>float run(const float* data, size_t n) {
    float g = 0.0;
    while (n) {
        n--;
        const float f = *data++;
        foo(f, &amp;g);
    }
    return g;
}

static void foo(float f, float* g) {
    // do some stuff, modifying g
}
</code></pre>
<p>(eliding headers and the forward declaration of <code>foo</code> for space)</p>
<p>A simple translation into AArch64 assembly gives something like this:</p>
<pre><code>// x0: const float* data
// x1: size_t n
// Returns a single float in s0

// Prelude: store frame and link registers
stp   x29, x30, [sp, #-16]!

// Initialize g = 0.0
fmov s0, #0.0

loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    bl foo   // call the function
    b loop   // keep looping

foo:
    // Do some work, reading from s1 and accumulating into s0
    // ...
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>Here, <code>foo</code> is kinda like a <a href="https://github.com/rust-lang/rfcs/blob/master/text/1201-naked-fns.md">naked
function</a>:
it uses the same stack frame and registers as the parent function, reads from
<code>s1</code>, and writes to <code>s0</code>.</p>
<p>The call to <code>foo</code> uses the the <code>bl</code> instruction, which is "branch and link":
it jumps to the given label, and stores the <strong>next</strong> instruction address in the
link register (<code>lr</code> or <code>x30</code>).</p>
<p>When <code>foo</code> is done, the <code>ret</code> instruction jumps to the address in the link
register, which is the instruction following the original <code>bl</code>.</p>
<p>Looking at this code, I was struck by the fact that it does two branches,
one after the other.  Surely, it would be more efficient to only branch once.</p>
<p>I had the clever idea to do so <strong>without changing <code>foo</code></strong>:</p>
<pre><code>stp   x29, x30, [sp, #-16]!
fmov s0, #0.0

bl loop // Set up x30 to point to the loop entrance
loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

foo:
    // Do some work, accumulating into `s0`
    // ...
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This is a little subtle:</p>
<ul>
<li>The first call to <code>bl loop</code> stores the beginning of the <code>loop</code> block in <code>x30</code></li>
<li>After checking for loop termination, we fall through into the <code>foo</code> function
(without a branch!)</li>
<li><code>foo</code> still ends with <code>ret</code>, which returns to the <code>loop</code> block (because
that's what's in <code>x30</code>).</li>
</ul>
<p>Within the body of the loop, we never change <code>x30</code>, so the repeated <code>ret</code>
instructions always return to the same place.</p>
<p>I set up a benchmark using a very simple <code>foo</code>:</p>
<pre><code>foo:
    fadd s0, s0, s1
    ret
</code></pre>
<p>With this <code>foo</code>, the function as a whole sums the incoming array of <code>float</code>
values.</p>
<p>Benchmarking with <a href="https://docs.rs/criterion/latest/criterion/"><code>criterion</code></a>
(on an M1 Max CPU),
with a 1024-element array:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Original </td><td>969 ns
</td></tr><tr><td>"Optimized"</td><td>3.85 ¬µs
</td></tr></tbody></table>
<p>The "optimized" code with one jump per loop is about <strong>4x slower</strong>
than the original version with two jumps per loop!</p>
<p>I found this surprising, so I asked a few colleagues about it.</p>
<p>Between <a href="https://hachyderm.io/@cliffle">Cliff</a> and
<a href="https://discuss.systems/@cross">Dan</a>,
the consensus was that mismatched <code>bl</code> / <code>ret</code>
pairs were confusing the
<a href="https://en.wikipedia.org/wiki/Branch_predictor">branch predictor</a>.</p>
<p>The <a href="https://developer.arm.com/documentation/102374/0101/Function-calls">ARM documentation</a> agrees:</p>
<blockquote>
<p>Why do we need a special function return instruction? Functionally, BR LR
would do the same job as RET. Using RET tells the processor that this is a
function return. Most modern processors, and all Cortex-A processors, support
branch prediction. Knowing that this is a function return allows processors to
more accurately predict the branch.</p>
<p>Branch predictors guess the direction the program flow will take across
branches. The guess is used to decide what to load into a pipeline with
instructions waiting to be processed. If the branch predictor guesses
correctly, the pipeline has the correct instructions and the processor does
not have to wait for instructions to be loaded from memory.</p>
</blockquote>
<p>More specifically, the branch predictor probably keeps an internal stack of
function return addresses, which is pushed to whenever a <code>bl</code> is executed. When
the branch predictor sees a <code>ret</code> coming down the pipeline, it assumes that
you're returning to the address associated with the most recent <code>bl</code> (and begins
prefetching / speculative execution / whatever), then pops that top address from
its internal stack.</p>
<p>This works if you've got matched <code>bl</code> / <code>ret</code> pairs, but the prediction will
fail if the same address is used by multiple <code>ret</code> instructions; you'll end up
with (<em>vague handwaving</em>) useless prefetching, incorrect speculative execution,
and pipeline stalls / flushes</p>
<p>Dan made the great suggestion of replacing <code>ret</code> with <code>br x30</code> to test this
theory.  Sure enough, this fixes the performance regression:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Matched <code>bl</code> / <code>ret</code> </td><td>969 ns
</td></tr><tr><td>One <code>bl</code>, many <code>ret</code></td><td>3.85 ¬µs
</td></tr><tr><td>One <code>bl</code>, many <code>br x30</code></td><td>913 ns
</td></tr></tbody></table>
<p>In fact, it's slightly faster, probably because it's only doing one branch
per loop instead of two!</p>
<p>To further test the "branch predictor" theory, I opened up Instruments and
examined performance counters for the first two programs. Picking out the worst
offenders, the results seem conclusive:</p>
<table>
<tbody><tr><th>Counter</th><th>Matched <code>bl</code> / <code>ret</code></th><th>One <code>bl</code>, many <code>ret</code>
</th></tr><tr><td><code>BRANCH_RET_INDIR_MISPRED_NONSPECIFIC</code></td><td>92</td><td>928,644,975
</td></tr><tr><td><code>FETCH_RESTART</code></td><td>61,121</td><td>987,765,276
</td></tr><tr><td><code>MAP_DISPATCH_BUBBLE</code></td><td>1,155,632</td><td>7,350,085,139
</td></tr><tr><td><code>MAP_REWIND</code></td><td>6,412,734</td><td>2,789,499,545
</td></tr></tbody></table>
<p>These measurements are captured while summing an array of 1B elements.  We see
that with mismatched <code>bl</code> / <code>ret</code> pairs, the return branch predictor fails about
93% of the time!</p>
<p>Apple doesn't fully document these counters, but I'm guessing that the other
counters are downstream effects of bad branch prediction:</p>
<ul>
<li><code>FETCH_RESTART</code> is presumably bad prefetching</li>
<li><code>MAP_DISPATCH_BUBBLE</code> probably refers to <a href="https://en.wikipedia.org/wiki/Pipeline_stall">pipeline stalls</a></li>
<li><code>MAP_REWIND</code> might be bad speculative execution that needs to be rewound</li>
</ul>
<p>In conclusion,
<a href="https://www.youtube.com/watch?v=GmqeZl8OI2M">do not taunt happy fun branch predictor</a>
with asymmetric usage of <code>bl</code> and <code>ret</code> instructions.</p>
<hr>
<h2>Appendix: Going Fast</h2>
<p>Take a second look at this program:</p>
<pre><code>stp   x29, x30, [sp, #-16]!
fmov s0, #0.0

loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    bl foo   // call the function
    b loop   // keep looping

foo:
    fadd s0, s0, s1
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>Upon seeing this program, it's a common reaction to ask "why is <code>foo</code> a
subroutine at all?"</p>
<p>The answer is "because this is a didactic example, not code that's trying
to go as fast as possible".</p>
<p>Still, it's a fair question.  You wanna go fast?  Let's go fast.</p>
<p>If we know the contents of <code>foo</code> when building this
function (and it's shorter than the maximum jump distance), we can remove the
<code>bl</code> and <code>ret</code> entirely:</p>
<pre><code>loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    // foo is completely inlined here
    fadd s0, s0, s1

    b loop

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This is a roughly 6% speedup: from 969 ns to 911 ns.</p>
<p>We can get faster still by trusting the compiler:</p>
<pre><code>pub fn sum_slice(f: &amp;[f32]) -&gt; f32 {
    f.iter().sum()
}
</code></pre>
<p>This brings us down to 833 ns, a significant improvement!</p>
<p><a href="https://godbolt.org/z/Kv77abW6c">Looking at the assembly</a>,
it's doing some loop unrolling.
However, even when compiled with <code>-C target-cpu=native</code>, it's not generating
<a href="https://developer.arm.com/Architectures/Neon">NEON SIMD instructions</a>.
Can we beat it?</p>
<p><strong>We sure can!</strong></p>
<pre><code>stp   x29, x30, [sp, #-16]!

fmov s0, #0.0
dup v1.4s, v0.s[0]
dup v2.4s, v0.s[0]

loop:  // 1x per loop
    ands xzr, x1, #3
    b.eq simd

    sub x1, x1, #1
    ldr s3, [x0], #4

    fadd s0, s0, s3
    b loop

simd:  // 4x SIMD per loop
    ands xzr, x1, #7
    b.eq simd2

    sub x1, x1, #4
    ldp d3, d4, [x0], #16
    mov v3.d[1], v4.d[0]

    fadd v1.4s, v1.4s, v3.4s

    b simd

simd2:  // 2 x 4x SIMD per loop
    cmp x1, #0
    b.eq exit

    sub x1, x1, #8

    ldp d3, d4, [x0], #16
    mov v3.d[1], v4.d[0]
    fadd v1.4s, v1.4s, v3.4s

    ldp d5, d6, [x0], #16
    mov v5.d[1], v6.d[0]
    fadd v2.4s, v2.4s, v5.4s

    b simd2

exit: // function exit
    fadd v2.4s, v2.4s, v1.4s
    mov s1, v2.s[0]
    fadd s0, s0, s1
    mov s1, v2.s[1]
    fadd s0, s0, s1
    mov s1, v2.s[2]
    fadd s0, s0, s1
    mov s1, v2.s[3]
    fadd s0, s0, s1

    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This code includes three different loops:</p>
<ul>
<li>The first loop (<code>loop</code>) sums individual values
into <code>s0</code> until we have a multiple of four values remaining</li>
<li>The second loop (<code>simd</code>) uses SIMD instructions to sum 4 values at a time
into the vector register <code>v1</code>, until we have a multiple of 8 values remaining</li>
<li>The last loop (<code>simd2</code>) is the same as <code>simd</code>, but is unrolled 2x so it
handles 8 values per loop iteration, summing into <code>v1</code> and <code>v2</code></li>
</ul>
<p>At the function exit, we accumulate the values in the vector registers <code>v1</code>/<code>v2</code>
into <code>s0</code>, which is returned.</p>
<p>The type punning here is particularly cute:</p>
<pre><code>ldp d3, d4, [x0], #16
mov v3.d[1], v4.d[0]
fadd v1.4s, v1.4s, v3.4s
</code></pre>
<p>Remember, <code>x0</code> holds a <code>float*</code>.  We pretend that it's a <code>double*</code> to load 128
bits (i.e. 4x <code>float</code> values) into <code>d3</code> and <code>d4</code>.  Then, we move the "double" in <code>d4</code>
to occupy the top 64 bits of the <code>v3</code> vector register (of which <code>d3</code> is the
<em>lower</em> 64 bits).</p>
<p>Of course, each "double" is two floats, but that doesn't matter when shuffling
them around.  When summing with <code>fadd</code>, we tell the processor to treat them as
four floats (the <code>.4s</code> suffix), and everything works out fine.</p>
<p>How fast are we now?</p>
<p>This runs in 94 ns, or about <strong>8.8x faster</strong> than our previous best.</p>
<p>Here's a summary of performance:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Matched <code>bl</code> / <code>ret</code> </td><td>969 ns
</td></tr><tr><td>One <code>bl</code>, many <code>ret</code></td><td>3.85 ¬µs
</td></tr><tr><td>One <code>bl</code>, many <code>br x30</code></td><td>913 ns
</td></tr><tr><td>Plain loop with <code>b</code></td><td>911 ns
</td></tr><tr><td>Rewrite it in Rust</td><td>833 ns
</td></tr><tr><td>SIMD + manual loop unrolling</td><td>94 ns
</td></tr></tbody></table>
<p>Could we get even faster?  I'm sure it's possible; I make no claims to being
the <a href="https://www.agner.org/optimize/">Agner Fog</a> of AArch64 assembly.</p>
<p>Still, this is a reasonable point to wrap up: we've demystified the initial
performance regression, and had some fun hand-writing assembly to go very
fast indeed.</p>
<p>The SIMD code does come with one asterisk, though: because floating-point
addition is not associative, and it performs the summation in a different
order, it <strong>may not get the same result</strong> as straight-line code.  In retrospect,
this is likely why the compiler doesn't generate SIMD instructions to compute
the sum!</p>
<p>Does this matter for your use case?  Only you can know!</p>
<hr>
<p>All of the code from this post is
<a href="https://github.com/mkeeter/arm64-test">published to GitHub</a>.</p>
<p>You can reproduce benchmarks by running <code>cargo bench</code> on an ARM64 machine.</p>

<!-- Begin footer -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Has anyone successfully pivoted from web dev to AI/ML development? (130 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40866311</link>
            <guid>40866311</guid>
            <pubDate>Wed, 03 Jul 2024 14:35:13 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40866311">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="40866311">
      <td><span></span></td>      <td><center><a id="up_40866311" href="https://news.ycombinator.com/vote?id=40866311&amp;how=up&amp;goto=item%3Fid%3D40866311"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=40866311">Ask HN: Has anyone successfully pivoted from web dev to AI/ML development?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_40866311">106 points</span> by <a href="https://news.ycombinator.com/user?id=ent_superpos">ent_superpos</a> <span title="2024-07-03T14:35:13"><a href="https://news.ycombinator.com/item?id=40866311">4 hours ago</a></span> <span id="unv_40866311"></span> | <a href="https://news.ycombinator.com/hide?id=40866311&amp;goto=item%3Fid%3D40866311">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Has%20anyone%20successfully%20pivoted%20from%20web%20dev%20to%20AI%2FML%20development%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=40866311&amp;auth=1344ad73a11754902765b72c2daad9533241ef0b">favorite</a> | <a href="https://news.ycombinator.com/item?id=40866311">65&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>I am currently working as a senior full-stack web software engineer. I have a BSc in Computer Science, and on my own, I've been learning more about AI/ML/deep learning. I really enjoy working with it, and I'd love to find a way to work on AI stuff professionally. The problem is that I've been working as a web developer professionally for about 10 years now, and I have no idea how I would pivot to more of a AI/data science role.</p><p>Does anyone have an experience of making this transition? As a web dev, I am senior level, but I'm sure I'd have to start from scratch on some things in the AI space. At least I have a good foundation of programming in general, math, and computer science.</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Living in a Lucid Dream (118 pts)]]></title>
            <link>https://www.noemamag.com/living-in-a-lucid-dream/</link>
            <guid>40866155</guid>
            <pubDate>Wed, 03 Jul 2024 14:18:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.noemamag.com/living-in-a-lucid-dream/">https://www.noemamag.com/living-in-a-lucid-dream/</a>, See on <a href="https://news.ycombinator.com/item?id=40866155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="article">

        <div role="group">
    <p>Credits</p>
    <p>Claire L. Evans is a writer and musician exploring ecology, technology and culture.</p>
</div>

<p>The first time it happened, it was an accident. But every dream is.</p>



<p>It would have been my last REM cycle of the night, had I been able to sleep. Instead, for the previous six hours, I had counted sheep, had dressed for imaginary occasions in my mind, had tried the Army sleep techniques, alternately imagined myself in a black velvet hammock and in a canoe on a calm, still lake. I‚Äôd meditated. I‚Äôd thought of my mother, a lifelong insomniac who has rarely slept more than four hours a night in her life. I‚Äôd tried everything and given up. All I could do was wait for morning.</p>







<p>The dream grabbed my ankles first, pulled at me like someone dislodging a drain. Out it tossed me through my sliding glass window, over the garden, over my quiet street, over the dark sleeping skyline, a ragdoll flung into the Santa Anas. I soared high enough to see Los Angeles‚Äôs motherboard of electric light. I could see the city in perfect detail below. I was asleep ‚Äî no, I was awake! I <em>felt</em> the cold whipping through my hair as I tumbled like a deflating balloon through the sky. I felt the miles beneath me. I felt the warm pillow beneath my cheek. I dove, flew, dipped, conscious of it all.</p>



<p>So <em>this</em> is a lucid dream, I thought.</p>



<p>More than half of adults will have this <a href="https://pubmed.ncbi.nlm.nih.gov/27337287/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">experience</a> or one like it at some point in their lives. They‚Äôll go to sleep, and as their REM cycles accumulate, as night shades into morning, as their sports car turns into a banana, they will suddenly realize, as I did: This is not real. This is a dream.</p>



<p>The flash of lucidity can come as quite a shock, enough to startle a novice into waking. But if you can hang on and stay conscious, you‚Äôll be in for the ride of your life.</p>



<p>After the first one, my curiosity was piqued. I started lurking in online forums, reading accounts of similar experiences from communities of ‚Äú<a href="https://www.reddit.com/r/Oneironauts/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">oneironauts</a>,‚Äù or dream explorers. From them I learned that lucid dreams need not be a fluke experience: I could cultivate and, with some practice, control them.</p>



<p>The first thing I needed to do was establish a baseline awareness of the difference between waking and dreaming using a technique called ‚Äúcritical state testing.‚Äù This is how it works: Several times a day, ask yourself if what you are experiencing is a dream. To make sure you‚Äôre awake, count your fingers. Plug your nose. Look at your watch, then look again to see if the numbers have moved.</p>



<p>If you do this often enough, the habit will spill over into your dreams. And when it does, you‚Äôll find that your fingers are jelly. That you can breathe with your nose plugged. That your watch is unreadable. ‚Äúdream standard time,‚Äù the psychophysiologist Stephen LaBerge called it in his 1990 manual, ‚ÄúExploring the World of Lucid Dreaming‚Äù:asleep and awake at once.</p>



<h2 id="h-glimpses-from-the-dreamworld"><strong>Glimpses From The Dreamworld</strong></h2>



<p>Lucid dreams are as ancient as the mind. They‚Äôve long been central to the Vajrayana Buddhist tradition, which teaches the cultivation of conscious awareness even in deep sleep. In the West, the philosophical literature of lucidity reaches back to <a href="https://classics.mit.edu/Aristotle/dreams.html" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Aristotle</a>. Ren√© Descartes, in his first ‚Äú<a href="https://yale.learningu.org/download/041e9642-df02-4eed-a895-70e472df2ca4/H2665_Descartes'%2520Meditations.pdf" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Meditation</a>,‚Äù famously proposed that it‚Äôs impossible to prove the difference between dreaming and wakefulness on empirical grounds alone. Friedrich Nietzsche <a href="https://www.themarginalian.org/2016/04/21/nietzsche-on-dreams/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">wrote</a> accounts of his own lucid dreams.</p>



<p>But in the modern sciences, the subject remained unexplored well into the 20th century. Even by the late 1970s, most scientists and psychologists believed that lucid dreams were the product of fleeting awakenings during sleep, misremembered in the morning. The very idea of conscious awareness during sleep was considered impossible to measure. After all, subjective accounts of dreamers couldn‚Äôt be quantified ‚Äî how could a dreamer really know the difference between <em>dreaming </em>that they were conscious and actually <em>being </em>conscious?</p>



<p>Anyone who has ever had a lucid dream knows this difference intuitively, but without a measurable physiological sign, lucidity was relegated to the anecdotal, subjective experience of dreamers. That is, until someone thought to dig deeper, and look ‚Äî as though lifting a veil ‚Äî behind their eyelids.</p>



<p>In the early 1950s, a graduate student named <a href="https://www.smithsonianmag.com/science-nature/the-stubborn-scientist-who-unraveled-a-mystery-of-the-night-91514538/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Eugene Aserinsky</a> hooked his 8-year-old son Armond up to a rudimentary brain-wave machine he had dragged up from the basement of a University of Chicago building. Aserinksy did not relish sleep research, at the time on the fringes of science, and the prospect of spending his nights awake, observing sleeping subjects, seemed ‚Äúas exciting as warm milk.‚Äù</p>



<p>In the bleary morning, looking over a half-mile of polygraph paper, he despaired. The ink pens had drawn jagged lines that looked suspiciously like the <span data-note="The movement of the eye between fixation points.">saccades</span> his son‚Äôs eyes made when they‚Äôd calibrated the machine. They showed the eye movements of a waking person. But his son had been out cold on the lab‚Äôs army cot all night. ‚ÄúThe research project was blowing up before me,‚Äù he later recalled.</p>


<!-- Quote Block Template -->

<figure>

  <blockquote>

    <p>
      ‚ÄúThe dream grabbed my ankles first, pulled at me like someone dislodging a drain. Out it tossed me through my sliding glass window, over the garden, over my quiet street, over the dark sleeping skyline, a ragdoll flung into the Santa Anas.‚Äù    </p>

    
    
  </blockquote>
</figure>




<p>As it turned out, there was nothing wrong with the machine. Following further study, Aserinsky discovered that his son‚Äôs sleeping brain was not ‚Äî as believed by practically everyone, particularly his thesis advisor, the venerable sleep scientist Nathaniel Kleitman ‚Äî simply <em>off. </em>Rather, brains roar to life in the darkness, in periods of active, ‚Äúparadoxical‚Äù sleep that seemed to coincide with dreaming. Aserinsky considered naming the phenomenon ‚Äújerky eye movements‚Äù but, hoping to avoid being called a jerk himself, opted instead for ‚Äú<a href="https://www.science.org/doi/10.1126/science.118.3062.273" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">rapid eye movements</a>,‚Äù or REM.</p>



<p>When Aserinsky left dream science behind in 1953, he passed the baton to a medical student, William C. Dement, who‚Äôd go on to establish the world‚Äôs first sleep disorders clinic at Stanford University. Years later, during a Stanford sleep study, Dement woke a subject during a period of unusually consistent back-and-forth eye movements. As LaBerge, once a student of Dement‚Äôs, recounted it, the subject had been dreaming about a ping-pong match. His eye movements during REM sleep had corresponded to his dream-gaze: left, right, left, right, following the phantom ball.</p>



<p>This anecdote sparked an idea in LaBerge. The study of lucid dreaming had always been stymied by its dependence on subjective self-reports. But eye movements are measurable. If a subject could <em>deliberately</em> move their eyes in a dream, LaBerge reasoned, then they could signal to outside observers, in real-time, that they were <em>awake</em> in there.</p>



<p>He designed an experiment around this premise, tucking a group of experienced lucid dreamers into bed at the Stanford sleep lab and asking them to make a series of prearranged eye movements the moment they became lucid. The eye measurements, unmistakable ping-pong volleys on the polygraph paper, corresponded precisely with the dreamers‚Äô waking reports ‚Äî all in the depths of REM sleep.</p>



<p>These ‚Äúsignal-verified‚Äù lucid dreams changed sleep research forever. Modern lucid dream studies rely on left-right-left-right eye signals to time-stamp experimental tasks and receive messages from the dreamworld. Lucid dreamers have been asked to signal before and after counting to 10, to measure if time unspools in dreams at the same speed as in waking life. (<a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2013.01013/full" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">It does</a>.) They‚Äôve been tasked to signal, with their eyes alone, the answers to simple math problems piped into speakers in the sleep room to establish if two-way communication is possible between a dreamer and a waking experimenter. (<a href="https://www.cell.com/current-biology/fulltext/S0960-9822(21)00059-2?_returnURL=https://linkinghub.elsevier.com/retrieve/pii/S0960982221000592?showall=true" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">It is</a>.) Eyes, windows of the soul, open the door to dreams too.</p>



<p>But what is <em>behind</em> that door, exactly? And what is it the lucid dreamers see in there?</p>



<h2 id="h-alive-and-mystically-beautiful"><strong>Alive And Mystically Beautiful</strong></h2>



<p>I began my critical state testing without much expectation of what it might bring. At first, I felt weird counting my fingers. Would my friends think I‚Äôd had a stroke? Had I been Reddit-brained? Was I no better than one of those people who believes life is a simulation? But the ritual was a good reminder to put down my phone. I began enjoying the blips of mindfulness it brought to my otherwise scatterbrained existence. My own hands took on the trippy quality that I remembered from being on mushrooms in college. It only took a few days for the habit to show up in my dreams.</p>



<p>Dreams are almost always phenomenally embodied. We look out onto the dreamscape and experience its uncanny forms and flavors largely from within an immersed, first-person perspective. Philosophers like to say that a dream is a ‚Äúself-in-a-world‚Äù experience. Even as its fantastical physics support impossible actions like flying, it can never quite succeed in tearing mind from limb. You can never witness your own death in a dream, for example, because if you did, there would be no <em>you</em> left to dream it. As the philosopher <a href="https://press.princeton.edu/books/hardcover/9780691220093/when-animals-dream" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">David M. Pe√±a-Guzm√°n</a> has written, ‚ÄúThere is no dream without a dream ego, no dream ego without a dream body, and no dream body without a dream body-image.‚Äù</p>



<p>But our dream bodies are only loose sketches of the real thing. When I‚Äôm awake, my hands have ink stains and nagging splinters. I have 10 fingers, and the signet ring I wear on my right hand reads ‚ÄúJB,‚Äù my husband‚Äôs initials. The first time I thought to examine my hands in a dream, though, they looked like a bouquet of wilting fingers. The signet ring was etched with unreadable glyphs. It‚Äôs difficult to put into words the feeling this gave me. One of the key attributes of dreams is that they feel real in the moment. When I looked at my hands and saw the mutant flippers of a Midjourney hallucination, I felt the walls drop. My whole body flushed. Nothing was real here ‚Äî least of all <em>me</em>.</p>



<p>The imprecise texture of the dream-body is a marked contrast to the dream landscape, which can be extraordinarily detailed, even growing in complexity under close examination. One of the earliest written accounts of a lucid dream in medical literature, <a href="https://dreamscience.ca/en/documents/New%2520content/lucid%2520dreaming%2520pdfs/vanEeden_PSPR_26_1-12_1913.pdf" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">recorded</a> by the Dutch psychiatrist Frederik van Eeden in the late 19th century, highlights this phenomenon:</p>



<div>
<p>I was floating through a landscape with bare trees, knowing that it was April, and I remarked that the perspective of the branches and twigs changed quite naturally. Then I made the reflection, during sleep, that my fancy would never be able to invent or to make an image as intricate as the perspective movement of little twigs seen in floating by.
          </p>
        


<!-- Quote Block Template -->

<figure>

  <blockquote>

    <p>
      ‚ÄúLucid dreams are as ancient as the mind.‚Äù    </p>

    
    
  </blockquote>
</figure>

</div>



<p>In his dream, van Eeden experienced a moment of reflection as he observed the remarkable naturalness of each little branch and twig passing beneath his flying body. The trees didn‚Äôt appear to him as imprecise, or ‚Äúdreamlike‚Äù ‚Äî quite the opposite. He even thought to himself that such a landscape, although fantastic, <em>must</em> be real because there was no way his own mind could have rendered those spare branches in such detail. Such reflections are considered ‚Äúpre-lucid‚Äù because they signal the dreamer‚Äôs stirring awareness ‚Äî their nagging feeling that something is <em>off.</em></p>



<p>In another classic dream <a href="https://www.biblio.com/book/astral-projection-record-out-body-experiences/d/1459779131" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">report</a>, the early 20th-century occultist and researcher Hugh Callaway recounted observing that the paving stones outside his home had changed orientation, turning parallel to the curb. This act of noticing brought the true nature of his dream into focus:</p>



<div><p>Then the solution flashed upon me: though this glorious summer morning seemed as real as it could be, I was dreaming! ‚Ä¶ Instantly, the vividness of life increased a hundred fold. Never had sea and sky and trees shone with such glamorous beauty; even the commonplace houses seemed alive and mystically beautiful.</p></div>



<p>In dreams as in waking life, attention makes the world alive. As many oneironauts have by now realized, critical state testing is not very different from mindfulness, meditation or similar practices of closely examining, sensing and being present with the world. These practices don‚Äôt only help us distinguish between waking and dreaming; they enrich our experience of both states.</p>



<p>In my own lucid dreams, a ripe, summer stone-fruit from the market explodes with flavor. The sensation of someone licking my belly feels wet. Pain and pleasure unfold in their phenomenal fullness. But this is always the case. The sea and sky always shine with their glamorous beauty. The world is detailed and rich. How often do I really <em>taste </em>a peach? How often do I take the time to examine the paving stones? Of course it‚Äôs trippy to examine my own hands: I never do it. I rarely look at <em>anything</em> so closely.</p>



<h2 id="h-higher-consciousness"><strong>Higher Consciousness</strong></h2>



<p>As the philosophers <a href="https://psycnet.apa.org/record/2007-09897-009" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Jennifer Windt and Thomas Metzinger</a> have observed, we travel through our lives as ‚Äúnaive realists,‚Äù assuming that as we perceive, touch and taste the world, we‚Äôre interfacing directly with some external, stable reality. But in truth, all experience, including our conscious experience of selfhood, relies on input from a chaotic world and is mediated by our limited sensory organs and shaped by our subjective internal landscape. Our model of reality is, in philosophical terms, ‚Äútransparent,‚Äù so convincing as to be invisible. In this sense, dreams aren‚Äôt so different from waking experience: They‚Äôre both illusions we take, falsely, to be real.</p>



<p>‚ÄúIn normal experience, we give ourselves over to the world without really questioning whether it holds at the seams or not,‚Äù explained Pe√±a-Guzm√°n, whose work explores the dreaming lives of animals. ‚ÄúWhat happens in lucidity is that you begin to attend to the seams and to the cracks ‚Äî you begin attending to the fact that this is a constructed space, a model, a simulation.‚Äù</p>



<p>The process of attending to the seams is what cognitive scientists call <em>metacognition</em>: the ability to think about thinking. Many philosophers believe that metacognition in dreams requires the capacity for language, since one cannot make the judgement ‚ÄúI am in a dream‚Äù without a subject and a predicate. But Pe√±a-Guzm√°n argues that animals may be able to recognize the oddness of their dreams in a more affective and embodied way ‚Äî to feel, without forming a linguistic judgement, that their world‚Äôs gone weird. As with lucidity in people, this is above all a matter of <em>noticing.</em></p>



<p>That‚Äôs why techniques to cultivate lucid dreams, like counting fingers or examining the numbers on a clock, ask us to scrutinize the world. In people&nbsp;‚Äî and I sincerely hope in animals too ‚Äî close attention sparks sudden clarity. This may take many forms. As Pe√±a-Guzm√°n said, a dog may dream in smell, a bird in song. ‚ÄúThink about an electric eel that senses and produces electricity,‚Äù he said. ‚ÄúIt‚Äôs conceivable that eel will have electric dreams, and there‚Äôs no way for us to even imagine what that is, because we don‚Äôt know what it means to experience electricity as meaningful. But they do.‚Äù</p>



<p>For humans, this phenomenal feast has another, more specifically cognitive flavor, too: the pleasure of awareness itself. Dreaming and waking perception are both illusory; they‚Äôre models constructed by our brains that turn sensory stimulus, or its absence, into meaning. In waking life, short of a heavy psychedelic experience, that illusion is all-encompassing; there‚Äôs no other level of consciousness to ‚Äúwake up‚Äù into.</p>


<!-- Quote Block Template -->

<figure>

  <blockquote>

    <p>
      ‚ÄúIn dreams as in waking life, attention makes the world alive.‚Äù    </p>

    
    
  </blockquote>
</figure>




<p>But in lucid dreams, we can examine the construction closely. Does this make a lucid dream <em>more </em>conscious than waking? Is it something, perhaps, just shy of enlightenment?</p>



<p>I asked the philosopher Jennifer Windt these questions when I reached her across the sleep-wake divide; as Los Angeles shaded into night, it was already the next morning in Melbourne, where Windt is senior lecturer of philosophy at Monash University. She winced. ‚ÄúI would struggle with that description,‚Äù she said. ‚ÄúI think that really assumes that consciousness can be neatly ordered in levels.‚Äù Windt is part of a school of philosophers painting a far less hierarchical, more multidimensional portrait of consciousness, one informed by close study of edge cases like out-of-body experiences, lucid dreams, mind-wanderings and hyper-realistic false awakenings.</p>



<p>‚ÄúTraditionally, it‚Äôs been thought that not only are sleep and waking opposites, but that they impose a kind of rift, a sharp distinction between conscious states,‚Äù she explained. But recent empirical and theoretical work supports the idea that consciousness takes many forms along a spectrum between sleep and waking. Daydreams and mind-wanderings, for example, may be caused by spells of ‚Äúlocal sleep‚Äù in the waking brain, and lucid dreams, which are linked to the reactivation of the dorsolateral prefrontal cortex ‚Äî the seat of executive ego function, which is turned off during normal REM sleep ‚Äî can be thought of as shades of waking consciousness in the gradient of sleep.</p>



<p>According to Windt‚Äôs work, a dream is an immersive, spatiotemporal hallucination, an experience of being present in a world where thoughts constitute reality. But perhaps such experiences can happen when we‚Äôre awake, too. ‚ÄúI think we should remain open to the possibility that if we define these states phenomenologically,‚Äù Windt said, ‚Äúwe might find them occurring in different behavioral states as well.‚Äù</p>



<h2 id="h-what-a-good-hologram"><strong>What A Good Hologram</strong></h2>



<p>Every time I go to sleep, I think a new world into being and put myself at its center. I used to take this nightly miracle for granted because in dreams we all think with an immediacy and directness that leaves no room for distanced self-awareness. When the lucidity comes on, however, autobiographical memories come rushing back. I can compare the dream to waking life or to previous dreams. I can see clearly that the dream phone in my dream hand is made of Jell-O because I have access to ‚Äú<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2737577/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">secondary consciousness</a>,‚Äù or the meta-awareness of my own state.</p>



<p>I can‚Äôt say I‚Äôm putting that secondary consciousness to much use yet; mostly, I look for the seams. I find myself at a party in an unfamiliar house and I touch the walls, raid the fridge. What‚Äôs in there? Do the vegetables vanish when I close the door or do they have a little hoedown, like in an old cartoon? I wander into the den and turn on the TV. On the news, there‚Äôs a map of a country called ‚ÄúOrovno.‚Äù Did I name this country, draw this map? <em>Borges would love this shit</em>, I think. I ask a dream-character his name, trying to ascertain if he is, in some sense, me. He says: ‚ÄúJeremy Allen White.‚Äù Ok, maybe not.</p>



<p>Lucidity advocates like LaBerge, who parlayed his early Stanford lucidity research into a long career as a dream guru, advise their acolytes that dream-control is a learned skill. With time, they promise, I can become a master of my dream domain, commanding characters to my will and redecorating the landscape to my tastes. I can stock my own fridge, draw my own maps, use the dream as a rehearsal space, perfect my tennis backhand, conquer my fear of public speaking. This emphasis on control and optimization doesn‚Äôt entice me much. It seems to diminish some of what makes dreams interesting to begin with ‚Äî their weirdness and mystery, their associative logic.</p>



<p>‚ÄúThe thing that makes a dream exciting, at least for me, is that it‚Äôs <em>not</em> the waking world,‚Äù said Adam Haar Horowitz, a dream researcher and cognitive scientist who, when I reached him over Zoom at his home in rural Alaska, is fittingly curled up on his bed. Horowitz is the co-inventor of the <a href="https://www.media.mit.edu/projects/sleep-creativity/overview/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Dormio</a>, a dream engineering device that makes it possible to intentionally incubate specific dreams. The Dormio takes advantage of the moments between being awake and falling asleep known as the hypnagogic state; just as visions begin to form in the darkness and you sense yourself falling, the device whispers prompts into your ear, like ‚Äúdream of a tree.‚Äù</p>



<p>Hypnagogia is when the full-fledged visions of the dreamworld begin to appear against the background of perception. As they coalesce into immersive hallucinations, they are remarkably suggestible. Horowitz‚Äôs technique involves waking sleepers at precise intervals, helping narrate the dream as it takes shape; he does so to foster creativity, treat recurrent nightmares in veterans and incubate bereavement dreams for people who have lost loved ones.</p>



<p>In these, he explained, lucidity would spoil the experience. Imagine sitting across the kitchen table from your deceased parent. ‚ÄúYou don‚Äôt know it‚Äôs a dream,‚Äù Horowitz said. ‚ÄúThat‚Äôs the beautiful thing. You‚Äôre sitting with them. Why would I want to be in a dream and <em>know</em> it‚Äôs a dream? I want to be in the room and want to have the conversation with the person. I don‚Äôt want to poke them and say, ‚ÄòWow, what a good hologram.‚Äô‚Äù</p>


<!-- Quote Block Template -->

<figure>

  <blockquote>

    <p>
      ‚ÄúNo wonder so many cultures have rituals of group dreaming.‚Äù    </p>

    
    
  </blockquote>
</figure>




<p>Horowitz finds the culture of lucid dreaming, with its emphasis on individuality and self-optimization, to be somewhat suspect. He asks me, rhetorically: What do people <em>do</em> in lucid dreams? They fulfill fantasies of control, seek personal thrills like flying, have sex with Jeremy Allen White.</p>



<p>The Dormio system is the only dream incubation tool of its type in experimental use, but there are plenty of consciousness-hacking <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6517539/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">devices</a> on the market intended to induce lucid dreams on demand. LaBerge has developed several over the decades with names like the DreamLight and the NovaDreamer. A well-funded AI startup,<a href="https://www.prophetic.com/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer"> Prophetic</a>, is currently training machine learning models on EEG and fMRI lucid dream data, hoping to beam their findings via transcranial-focused ultrasound directly into willing brains; their Halo device, ‚Äúthe most advanced consumer neurotechnology wearable ever created,‚Äù will soon be available.</p>



<p>As the media scholar Aleena Chia has <a href="https://openpublishing.library.umass.edu/cpo/article/id/53/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">written</a>, such technologies promise a ‚Äúyoutopia of automatized dream entertainment.‚Äù In certain sectors of Silicon Valley, lucid dreaming is talked about like virtual reality without a headset. ‚ÄúWhat a lame way to relate to your own consciousness,‚Äù said Horowitz.</p>



<p>Dream incubation, on the other hand, he said, ‚Äúis a really holy, really humble, really hands-off tradition‚Äù with ancient roots. In classical Greek antiquity, dream-seekers traveled to dedicated temples in order to seek specific dreams of healing or guidance; after rituals of purification and supplication, they slept, communally, in sanctuaries dedicated to this purpose. As the religious scholar Kimberley C. Patton has proposed in her work, these temples were ‚Äúgod-haunted‚Äù places where the gods used dreams to speak between worlds. In the tradition of incubation, she has written, ‚ÄúGods and human beings are the co-creators of dreams in the darkness of our mutual sleep.‚Äù</p>



<p>In the morning, the temple dreamers interpreted their visions with the guidance of priests. Nothing about the dream was private; it was something to be plumbed in public alongside others upon waking with the morning. The dream‚Äôs prescriptions, a cure proffered by the gods, were enacted in waking life, with the full support of the dreamers‚Äô communities. It is from this tradition, far more ancient than Freud, that we get dream interpretation. Similar rituals of public dream incubation have existed throughout history and all over the world, in medieval Japanese Buddhism, in Shia and Sufi Islamic traditions, in Bengal, among the ancient and modern highland Maya, and in many North American Indigenous cultures.</p>



<p>I‚Äôve had a few flying dreams since my first lucid dream. I‚Äôve taken swan dives off fire escapes, soared over fields of wildflowers, been pulled into the sun itself. It can feel exhilarating to fly, to feel velocity amid total stillness. To linger in the light of an imagined sky. But it‚Äôs cold up there, too, trapped inside the dream. I can see it all, but I have nobody to share it with. No wonder so many cultures have rituals of group dreaming. No wonder they traveled far and wide to dream together, to seek succor and interpretation from oneiromantic priests. It makes the night less lonely.</p>

          
        
      </div></div>]]></description>
        </item>
    </channel>
</rss>