<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 30 Jul 2023 20:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: San Francisco Compute – 512 H100s at <$2/hr for research and startups (110 pts)]]></title>
            <link>https://sfcompute.org/</link>
            <guid>36933603</guid>
            <pubDate>Sun, 30 Jul 2023 17:25:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sfcompute.org/">https://sfcompute.org/</a>, See on <a href="https://news.ycombinator.com/item?id=36933603">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><i>By Alex Gajewski and Evan Conrad</i></p>
            <p>(we previously ran <a href="https://aigrant.org/">AI
                    Grant</a> together)
            </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Richard Feynman's 1964 Messenger Lectures at Cornell University (106 pts)]]></title>
            <link>https://www.feynmanlectures.caltech.edu/messenger.html</link>
            <guid>36933209</guid>
            <pubDate>Sun, 30 Jul 2023 16:55:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.feynmanlectures.caltech.edu/messenger.html">https://www.feynmanlectures.caltech.edu/messenger.html</a>, See on <a href="https://news.ycombinator.com/item?id=36933209">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="Messenger-S1">
<h3>About Feynman's Messenger Lectures</h3>

<p>
In 1963 Richard Feynman was invited to give the 1964 Messenger Lectures at Cornell University, an annual tradition since 1924, when Hiram Messenger gifted Cornell with "a fund to provide a course of lectures on the Evolution of Civilization for the special purpose of raising the moral standard of our political, business, and social life", to be "delivered by the ablest non-resident lecturer or lecturers obtainable".</p>

<p>
Feynman had been a physics professor at Cornell from 1945 to 1950, during which time he did the work for which he was awarded a Nobel Prize in 1965. While at Cornell Feynman became well-known in the physics community for his innovations in quantum electrodynamics and idosyncratic style. He was at Caltech in 1963, when he was invited to become the 41st Messenger Lecturer<a id="footnote_source_1" href="#footnote_1"><sup>1</sup></a>, by which time he had become known to a much wider audience through his recently published book, Volume I of <i>The Feynman Lectures on Physics.</i><a id="footnote_source_2" href="#footnote_2"><sup>2</sup></a></p>

<p>
 According to the Cornell Faculty Website, "A Messenger Lecturer typically gives three lectures/presentations over the course of a one-week visit. At least one of these must be a lecture that is suitable for a general audience."<a id="footnote_source_3" href="#footnote_3"><sup>3</sup></a> Feynman, however, chose to give a series of <i>six</i> lectures, <i>all</i> for a general audience, which he titled <i>The Character of Physical Law</i>.<a id="footnote_source_4" href="#footnote_4"><sup>4</sup></a> He had plenty of material to draw from his recently completed introductory physics course, the basis of <i>The Feynman Lectures on Physics</i>. For this reason one finds many similarities, parallels, and even identical parts in the lectures of <i>The Character of Physical Law</i> and several of the lectures in <i>The Feynman Lectures on Physics</i>.</p>
 
 <p>
Feynman's Messenger Lectures were videotaped by the BBC, who in 1965 published a hardbound book of edited lecture transcripts under the title, <i>The Character of Physical Law</i>. In 1967 the paperback rights were licensed to MIT Press who continues to print the book today.<a id="footnote_source_5" href="#footnote_5"><sup>5</sup></a> The videotapes were transferred to film, and in the late 1960s through the '70s copies of the films were in wide distribution at colleges and universities.<a id="footnote_source_6" href="#footnote_6"><sup>6</sup></a> Sadly, however, these wonderful films of Feynman lecturing at the peak of his prowess went out of distribution and became generally unavailable in the 1980s.</p>

<p>
In 2009, when Microsoft Research introduced their Silverlight framework for media-rich web applications, Bill Gates licensed rights to stream the BBC's films of Feynman’s Messenger Lectures online. Hoping to encourage others to make educational content available for free, he used them in the first Silverlight demo, “Project Tuva.”<a id="footnote_source_7" href="#footnote_7"><sup>7</sup></a> The publication of Feynman’s Messenger Lectures for free online viewing, with special features such as searchable synchronized scrolling transcripts, links to related online material, and  commentary, was an instant hit with Feynman fans, students and physicists. The Silverlight framework, however, was not widely adopted, and in 2016 Project Tuva was retired. The videos were still available for viewing on the Microsoft Research Website (though without the special features) until 2021, when their BBC license expired. The license has since been generously renewed by Bill Gates so that the videos can continue to be shown online to users of The Feynman Lectures Website.</p>
</div><div id="Messenger-S2">
<h3>The Feynman Messenger Lectures Video Viewer</h3>

<p>
Clicking on a lecture title above will open <i>The Feynman Messenger Lectures Video Viewer,</i> an application based on the Microsoft Azure Media Player for displaying the films of Feynman's lecture series <i>The Character of Physical Law</i> in high definition video<a id="footnote_source_8" href="#footnote_8"><sup>8</sup></a> with a searchable interactive auto-scrolling transcript. The Viewer allows one to resize the video/transcript areas (even during play), and has a simple tabbed user interface. For detailed instructions on using the Viewer please refer to its "Help" tab after opening the application with the links above.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Musl 1.2.4 adds TCP DNS fallback (125 pts)]]></title>
            <link>https://www.openwall.com/lists/musl/2023/05/02/1</link>
            <guid>36933028</guid>
            <pubDate>Sun, 30 Jul 2023 16:39:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openwall.com/lists/musl/2023/05/02/1">https://www.openwall.com/lists/musl/2023/05/02/1</a>, See on <a href="https://news.ycombinator.com/item?id=36933028">Hacker News</a></p>
<div id="readability-page-1" class="page">


<table>
<tbody><tr>

<td>
<a href="https://www.openwall.com/"><img src="https://www.openwall.com/logo.png" width="182" height="80" alt="Openwall"></a>
</td><td>
<div>
<ul>
<li><a href="https://www.openwall.com/">Products</a>
<ul>
<li><a href="https://www.openwall.com/Owl/">Openwall GNU/*/Linux &nbsp; <i>server OS</i></a>
</li><li><a href="https://www.openwall.com/lkrg/">Linux Kernel Runtime Guard</a>
</li><li><a href="https://www.openwall.com/john/">John the Ripper &nbsp; <i>password cracker</i></a>
<ul>
<li><a href="https://www.openwall.com/john/">Free &amp; Open Source for any platform</a>
</li><li><a href="https://www.openwall.com/john/cloud/">in the cloud</a>
</li><li><a href="https://www.openwall.com/john/pro/linux/">Pro for Linux</a>
</li><li><a href="https://www.openwall.com/john/pro/macosx/">Pro for macOS</a>
</li></ul>
</li><li><a href="https://www.openwall.com/wordlists/">Wordlists &nbsp; <i>for password cracking</i></a>
</li><li><a href="https://www.openwall.com/passwdqc/">passwdqc &nbsp; <i>policy enforcement</i></a>
<ul>
<li><a href="https://www.openwall.com/passwdqc/">Free &amp; Open Source for Unix</a>
</li><li><a href="https://www.openwall.com/passwdqc/windows/">Pro for Windows (Active Directory)</a>
</li></ul>
</li><li><a href="https://www.openwall.com/yescrypt/">yescrypt &nbsp; <i>KDF &amp; password hashing</i></a>
</li><li><a href="https://www.openwall.com/yespower/">yespower &nbsp; <i>Proof-of-Work (PoW)</i></a>
</li><li><a href="https://www.openwall.com/crypt/">crypt_blowfish &nbsp; <i>password hashing</i></a>
</li><li><a href="https://www.openwall.com/phpass/">phpass &nbsp; <i>ditto in PHP</i></a>
</li><li><a href="https://www.openwall.com/tcb/">tcb &nbsp; <i>better password shadowing</i></a>
</li><li><a href="https://www.openwall.com/pam/">Pluggable Authentication Modules</a>
</li><li><a href="https://www.openwall.com/scanlogd/">scanlogd &nbsp; <i>port scan detector</i></a>
</li><li><a href="https://www.openwall.com/popa3d/">popa3d &nbsp; <i>tiny POP3 daemon</i></a>
</li><li><a href="https://www.openwall.com/blists/">blists &nbsp; <i>web interface to mailing lists</i></a>
</li><li><a href="https://www.openwall.com/msulogin/">msulogin &nbsp; <i>single user mode login</i></a>
</li><li><a href="https://www.openwall.com/php_mt_seed/">php_mt_seed &nbsp; <i>mt_rand() cracker</i></a>
</li></ul>
</li><li><a href="https://www.openwall.com/services/">Services</a>
</li><li id="narrow-li-1"><a>Publications</a>
<ul>
<li><a href="https://www.openwall.com/articles/">Articles</a>
</li><li><a href="https://www.openwall.com/presentations/">Presentations</a>
</li></ul>
</li><li><a>Resources</a>
<ul>
<li><a href="https://www.openwall.com/lists/">Mailing lists</a>
</li><li><a href="https://openwall.info/wiki/">Community wiki</a>
</li><li><a href="https://github.com/openwall">Source code repositories (GitHub)</a>
</li><li><a href="https://cvsweb.openwall.com/">Source code repositories (CVSweb)</a>
</li><li><a href="https://www.openwall.com/mirrors/">File archive &amp; mirrors</a>
</li><li><a href="https://www.openwall.com/signatures/">How to verify digital signatures</a>
</li><li><a href="https://www.openwall.com/ove/">OVE IDs</a>
</li></ul>
</li><li id="last-li"><a href="https://www.openwall.com/news">What's new</a>
</li></ul>
</div>


</td></tr></tbody></table>




<a href="https://www.openwall.com/lists/musl/2023/05/01/3">[&lt;prev]</a> <a href="https://www.openwall.com/lists/musl/2023/05/02/2">[next&gt;]</a> <a href="https://www.openwall.com/lists/musl/2023/05/02/">[day]</a> <a href="https://www.openwall.com/lists/musl/2023/05/">[month]</a> <a href="https://www.openwall.com/lists/musl/2023/">[year]</a> <a href="https://www.openwall.com/lists/musl/">[list]</a>
<pre>Date: Mon, 1 May 2023 23:47:32 -0400
From: Rich Felker &lt;dalias@...c.org&gt;
To: musl@...ts.openwall.com
Subject: musl 1.2.4 released

This release adds TCP fallback to the DNS stub resolver, fixing the
longstanding inability to query large DNS records and incompatibility
with recursive nameservers that don't give partial results in
truncated UDP responses. It also makes a number of other bug fixes and
improvements in DNS and related functionality, including making both
the modern and legacy API results differentiate between NODATA and
NxDomain conditions so that the caller can handle them differently.

On the API level, the legacy "LFS64" ("large file support")
interfaces, which were provided by macros remapping them to their
standard names (#define stat64 stat and similar) have been deprecated
and are no longer provided under the _GNU_SOURCE feature profile, only
under explicit _LARGEFILE64_SOURCE. The latter will also be removed in
a future version. Builds broken by this change can be fixed short-term
by adding -D_LARGEFILE64_SOURCE to CFLAGS, but should be fixed to use
the standard interfaces.

The dynamic linker (and static-PIE entry point code) adds support for
the new compact "RELR" format for relative relocations which recent
linkers can generate. Use of this linker feature for dynamic-linked
programs will make them depend on having musl 1.2.4 or later available
at runtime. Static-linkied PIE binaries using it, as always, are
self-contained and have no such dependency.

A large number of bugs have been fixed, including many in the wide
printf family of functions, incorrect ordering of digits vs non-digits
in strverscmp, and several rare race-condition corner cases in thread
synchronization logic at thread exit time, in multi-threaded fork,
pthread_detach, and POSIX semaphores.


<a href="https://musl.libc.org/releases/musl-1.2.4.tar.gz" rel="nofollow">https://musl.libc.org/releases/musl-1.2.4.tar.gz</a>
<a href="https://musl.libc.org/releases/musl-1.2.4.tar.gz.asc" rel="nofollow">https://musl.libc.org/releases/musl-1.2.4.tar.gz.asc</a>



Special thanks goes out to musl's release sponsors for supporting the
project financially via Patreon and GitHub Sponsors at the $32/month
level or higher:

* Andrew Kelley / ziglang
* Danny McClanahan
* enduser
* Eric Pruitt
* Evan Phoenix
* Greg Krimer
* Hurricane Labs
* Justin Cormack
* Kentik
* Laurent Bercot
* Michael Sartain
* Mirza Prasovic
* Moinak Bhattacharyya
* Nabu Casa
* Nathan Hoad
* Neal Gompa
* Stackhero
* Tyler James Frederick

For information on becoming a sponsor, visit one of:

<a href="https://github.com/sponsors/richfelker" rel="nofollow">https://github.com/sponsors/richfelker</a>
<a href="https://patreon.com/musl" rel="nofollow">https://patreon.com/musl</a>
</pre>
<p><a href="http://www.openwall.com/blists/">Powered by blists</a> - <a href="http://lists.openwall.net/">more mailing lists</a>


</p><p>
Confused about <a href="https://www.openwall.com/lists/">mailing lists</a> and their use?
<a href="https://en.wikipedia.org/wiki/Electronic_mailing_list">Read about mailing lists on Wikipedia</a>
and check out these
<a href="https://www.complang.tuwien.ac.at/anton/mail-news-errors.html">guidelines on proper formatting of your messages</a>.
</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I feel hopeless, rejected, and a burden on society-one week of empathy training (211 pts)]]></title>
            <link>https://shkspr.mobi/blog/2019/07/i-feel-hopeless-rejected-and-a-burden-on-society-one-week-of-empathy-training/</link>
            <guid>36932524</guid>
            <pubDate>Sun, 30 Jul 2023 15:53:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shkspr.mobi/blog/2019/07/i-feel-hopeless-rejected-and-a-burden-on-society-one-week-of-empathy-training/">https://shkspr.mobi/blog/2019/07/i-feel-hopeless-rejected-and-a-burden-on-society-one-week-of-empathy-training/</a>, See on <a href="https://news.ycombinator.com/item?id=36932524">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemprop="blogPost"><div itemprop="https://schema.org/articleBody">
<p>I've spent a week cosplaying as a disabled user. And I hate it.</p><p>A couple of months ago, I attended a private talk given by a disabled colleague of mine. "Everyone should believe disabled people's stories about accessibility problems," she said. "But, given that people don't, here's what I want you to do. Spend one week pretending to be disabled. Pick a disability and try to interact with services as though you have that impairment. Build up some empathy."</p><p>So I did.</p><p>For a week, I pretended that I was in a wheelchair. I didn't go the full way and buy a cheap chair and try and commute in it. Instead, whenever I was invited to speak at an event, or go to a meeting, I asked if the venue was accessible.  To my delight, all of them were.  A couple of people told me they'd arrange ramps to the stage, or that they'd need to adjust a podium height if I wanted.</p><p>Except one. I turned up to find the talk had been moved to the 3rd floor of a building with no lift. I'd specifically asked the organisers if the room was wheelchair friendly. They'd had more people turn up than expected, so moved to a bigger room.  At no point did the organisers contact me.</p><p>I turned up (without a chair) and briefly considered leaving. Instead I sent a sternly worded email.</p><p>The week left me feeling fairly hopeful. OK, it wasn't a full test - and there was a failure - but in my little bubble of society, people are (mostly) welcoming to wheelchair users.</p><p>Then it all went wrong.</p><p>The next week, I tried something different. Approximately <a href="https://www.thecommunicationtrust.org.uk/media/2612/communication_difficulties_-_facts_and_stats.pdf">10% of people in the UK have a speech disorder</a>. In the USA, <a href="https://www.nidcd.nih.gov/health/statistics/statistics-voice-speech-and-language">approximately 7.5 million people have trouble using their voices</a>.</p><p>So, I tried to spend a week without using the phone to contact companies. It was a fucking disaster.</p><p>I wanted to upgrade my Internet access to a faster speed. Virgin Media provide a web chat - and after a few hours of waiting (seriously!) I had this frustrating exchange (edited for clarity - typos left intact):</p><ul><li>John: If you wish to avail of this deal . I advise you to call our Customer Care Team.</li><li>Terence: I can't use the phone due to my disability. Can I chat online to do it?</li><li>John: To reach our customer care. You can just download the app and quickly chat with the team there;</li><li>Terence: I've tried using the app - but no one answers. Please can you help me. I've been a customer for 6 years.</li><li>John: We appreciate your loyalty Terence, but we are are only limited to regular upgrade transacations.</li><li>Terence: So disabled customers can't upgrade via chat?</li><li>John: For persons with diabilities, there are options at "Contact Us"</li><li>Terence: I tried that - and it redirected me here. Is there anyone who can help?</li><li>John: I'm so sorry ternce but transactiosn like this can only be arranged by calling Custome care team.</li></ul><p>So I asked to cancel my account.</p><ul><li>Terence: If I want to cancel my account (without using the phone) what can I do?</li><li>John: the only option though is by calling . Call 150 from your Virgin Media phone or mobile, or call 0345 454 1111 * from any other phone Monday to Friday, 8am until 9pm Saturday, 8am until 8pm and Sunday 8am until 6pm For our text relay service call us free on 18001 0800 052 2164 You can also contact us through a sign language interpreter. Open 7 days a week, 8am until midnight. *For call costs to our team from a Virgin Media home phone, visit our Call costs page. Calls from other networks and mobile may vary.</li><li>Terence: This is discrimination. I don't know sign language and I don't have text relay. I can't use my voice. I want to contact someone to cancel my account.</li><li>John: If the voice is the issue, i advise you ask someone to call in  your behalf.</li><li>Terence: I am perfectly capable of managing my affairs - and I don't want to give my password to someone else.</li></ul><p>And so it went on. I spent hours chatting with different people, and with managers. None of them could help me with an upgrade, or with a cancellation.</p><p>Virgin accessibility police says:</p><blockquote><p>
2.1 We are committed to ensuring both vulnerable and disabled customers get fair and appropriate treatment.<br>
2.2 To ensure we meet the needs of current and prospective customers, our sales and support teams are trained to identify and support the accessibility and vulnerability needs people may have.<br>
<a href="https://www.virginmedia.com/corporate/media-centre/public-policy-statements/accessibility-and-vulnerability-policy">https://www.virginmedia.com/corporate/media-centre/public-policy-statements/accessibility-and-vulnerability-policy</a></p></blockquote><p>As far as I can see, that's a load of bunkum. If you don't have a voice, you're locked out of Virgin's upgrade and cancellation routes.</p><p>I raised a complaint, and got back this fairly generic and dismissive response:</p><blockquote><p>
Please accept my apologies for this experience, , this is not the experience we want for our customers.   We have fed your comments back to the relevant team, this will help us to highlight certain training needs and form coaching. There are areas where improvements can always be made, and as a customer-orientated organisation we are always endeavouring to improve both how we deal with customers and the range and quality of the services we offer.</p></blockquote><p>No actual resolution. It made me feel like a burden for even asking for help. I can't go through the "normal" channels - I have to rely on the good graces of a complaints team.  It was frustrating and demoralising.</p><p>The same thing happened with Thames Water.  If you want to move your account, they ask you to fill in a form online. Hurrah! Until you get to one bit of it, where it tells you to ring a phone number.</p><blockquote data-width="550" data-dnt="true"><p lang="en" dir="ltr">Dear <a href="https://twitter.com/thameswater?ref_src=twsrc%5Etfw">@thameswater</a>, I'm trying to move house. Your website says the contact number is 0800 000 0000.<br>I assume that's placeholder text as the number is answered by the Prudential!<br>What number should I call?<a href="https://t.co/fT3DzPIy4M">https://t.co/fT3DzPIy4M</a> <a href="https://t.co/AGQ2FqMyr0">pic.twitter.com/AGQ2FqMyr0</a></p><p>— Terence Eden (@edent) <a href="https://twitter.com/edent/status/1155502091122724864?ref_src=twsrc%5Etfw">July 28, 2019</a></p></blockquote><p>I had a frustrating chat on Twitter with Thames Water. They admitted the phone number was wrong, and struggled to provide me with contact details.</p><p>I tried to use their complaints process, but that requires a 10 digit account number. But Thames have upgraded me to a 12 digit number - so their own form doesn't work!</p><p>So now I'm stuck in limbo. Waiting for someone to get back to me. I've told them not to call - but I bet you they try to ring me.</p><p>My bank had similar issues. UK banking is <em>great</em> for most online users. I was able to set up new payees, order a new card, cancel Direct Debit - all without using my voice.  And then I tried to buy a house...</p><p>I needed to transfer a large sum of money in order to put a deposit down on my new place. It was larger than the standard transfer limit. And the <em>only</em> solution was to call them up.</p><p>They do have an online messaging service, but from experience it's slow to answer - and I needed to transfer the money immediately (the home buying process in England is dysfunctional).  If I truly had no voice, I'd have lost the house I was trying to buy.</p><p>I appreciate the need for security. And for double-checking transactions. And all that good stuff. But I was trapped. So I caved in and called.</p><h2 id="i-have-no-mouth-and-i-must-scream"><a href="#i-have-no-mouth-and-i-must-scream">I have no mouth and I must scream</a></h2><p>You should believe your disabled friends and colleagues when they tell you how crap the world can be.</p><p>You should also try empathy building exercises. Here are some examples, please add your own in the comments:</p><ul><li>Go a couple of weeks without using the phone. Which services are closed off to you?</li><li>Tell people you need an accessible venue for your meetings.  How do they respond?</li><li>Turn off images in your browser. Is there enough alt-text for you to navigate the web?</li><li>Switch on subtitles and mute your favourite shows. Do they even have subtitles? What do you miss?</li><li>Hire or buy a wheelchair for a week. How easy is your office to navigate? (Please don't block the accessible loos though!)</li><li>Buy a pair of <a href="http://www.inclusivedesigntoolkit.com/gloves/gloves.html">arthritis simulating gloves</a>. What does the world feel like with limited mobility?</li></ul><p>But, most of all, record how it makes you feel. After a few fruitless hours pleading with my ISP, I was ready to kick something. Now imagine that every day.</p><p>Whether you work in tech or not - it is your duty to make sure that no one feels demoralised or rejected because of the systems you build.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Worldcoin: A solution in search of its problem (135 pts)]]></title>
            <link>https://newsletter.mollywhite.net/p/worldcoin-a-solution-in-search-of</link>
            <guid>36931806</guid>
            <pubDate>Sun, 30 Jul 2023 14:59:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsletter.mollywhite.net/p/worldcoin-a-solution-in-search-of">https://newsletter.mollywhite.net/p/worldcoin-a-solution-in-search-of</a>, See on <a href="https://news.ycombinator.com/item?id=36931806">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Having my eyeballs scanned by a shiny chrome orb so I can someday receive cryptocurrency disbursements because artificial intelligence has stolen my job sounds like something from the pages of a half-baked sci-fi novel. It also sounds like the kind of operation that venture capitalists would value at over a billion dollars.</p><p>The premise is simple, they say: As artificial intelligence becomes more sophisticated, approaching the level of human-superior artificial general intelligence, it will both create wealth and disrupt labor markets as human workers are replaced by machines. It will also become increasingly challenging to distinguish human from machine, as the current-day problem of bots mimicking humans online is made worse by sophisticated AI fakes. Or at least, that’s today’s problem. Check back in a month or two and see if it’s changed.</p><p><span>Worldcoin is, at the moment, a project to distribute cryptocurrency tokens (</span><em>also</em><span> called Worldcoin, or WLD) to those who confirm they are human by having their irises scanned by a custom piece of hardware that both captures its subject’s unique iris “fingerprint” and performs biometric scans to ensure it’s scanning a living, breathing human being and not a printout or some other fake. That custom hardware just so happens to be a chrome orb that evokes HAL 9000.</span></p><p><span>Worldcoin was founded by Sam Altman, the “tech visionary” </span><em>du jour</em><span> who is behind OpenAI. That’s right, the guy who’s going to sell us all the solution to a worsening AI-powered bot infestation of the Internet and to AI-induced mass unemployment is the same guy who’s making the AI in question.</span></p><p><span>He is selling the antidote to the poison he is, coincidentally, </span><em>also</em><span> selling.</span></p><p><span>In June 2022 I wrote an essay titled “</span><a href="https://blog.mollywhite.net/is-acceptably-non-dystopian-self-sovereign-identity-even-possible/" rel="">Is ‘acceptably non-dystopian’ self-sovereign identity even possible?</a><span>“. This was prompted not by Worldcoin, but by a </span><em>different</em><span> identity-related project: Vitalik Buterin’s conception of “soulbound tokens”. In a May 2022 </span><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4105763" rel="">paper</a><span> he co-authored with E. Glen Weyl and Puja Ohlhaver, he wrote that these identity projects (and related ideas they group under the umbrella of “decentralized society”) must meet the rather low bar of being merely “acceptably non-dystopian” in order to be worth pursuing.</span></p><p><span>In that essay, I wrote about the problem of decentralized identity: that is, how do you determine that someone is who they say they are without relying on a centralized authority (e.g., government-issued identification)? This has been a popular topic in the cryptoverse because of the Sybil problem: the challenge of ensuring that one individual does not operate multiple identities (or crypto wallets) while also respecting anonymity (or pseudonymity). Worldcoin is among the projects trying to solve this problem, sometimes termed “</span><a href="https://en.wikipedia.org/wiki/Proof_of_personhood" rel="">proof of personhood</a><span>”, but hardly the only one. Proof of Humanity, BrightID, and others are doing so as well. They use a range of approaches, from biometrics (Worldcoin) to web-of-trust vouching (BrightID) to some sort of a mix (Proof of Humanity incorporates both vouching and uploading a video of one’s face for verification).</span></p><p>Identity projects aim to answer one or several of the following questions:</p><ol><li><p>Is this user a human?</p></li><li><p><span>Is this user a </span><em>unique</em><span> human? (i.e., do they only control one identity in a given network?)</span></p></li><li><p>Can this user prove they meet some criteria? (e.g., are they over 18? are they a U.S. citizen?)</p></li><li><p>Can this user prove they are a specific person? (e.g., does Molly White control this identity?)</p></li></ol><p><span>If you’ve ever solved a CAPTCHA you probably understand why it’s useful to answer the first question — bot prevention is important and inarguably becoming </span><em>more</em><span> important as bots become more convincing, more disruptive, and more capable of evading anti-bot measures.</span></p><p>The second is useful for ensuring fairness in systems such as voting. In the cryptocurrency world, many DAOs follow the one-token-one-vote model, which makes them susceptible to control by the wealthiest actors. A robust proof-of-personhood solution could, ideally, allow for one-person-one-vote without sacrificing crypto’s beloved pseudonymity. Some also envision decentralized proof of uniqueness as helping with fraud protection in other systems, ranging from ensuring people only get one NFT in an NFT airdrop all the way to critical social welfare programs or universal basic income schemes.</p><p>The third question is distinct from the fourth in that there are times when people might wish to provably answer specific yes or no questions such as “are you over 18?” or “are you a U.S. citizen?” without disclosing their full identity. This is not widely done today: websites generally either ask if you meet the criteria (and simply trust that you aren’t lying), or require you to submit a government-issued ID to prove it (and thus also require you to disclose your full identity to them).</p><p>The fourth question is critical for high-importance activities, like signing legal documents, opening a bank account, or applying for a loan.</p><p><span>Worldcoin is primarily concerned with answering questions 1 and 2: ensuring everyone who is represented in the Worldcoin network is a real person, and only controls one identity in the system. The reasons </span><em>why</em><span> they want to do this have shifted since the project emerged in 2019, making the project hard to pin down. First they seemed most focused on wealth redistribution. Then, during the height of crypto hype, the story centered around onboarding new users to crypto and solving its Sybil problem. More recently, Worldcoin pivoted to the more AI-focused story now that AI is the hot big thing and founder Sam Altman has become its poster boy.</span></p><p><span>The types of things Worldcoin says it could one day do are lofty: “considerably increase economic opportunity, scale a reliable solution for distinguishing humans from AI online while preserving privacy, enable global democratic processes, and show a potential path to AI-funded UBI [universal basic income].“</span></p><p>That’s right: the founders aren’t looking to merely create the next generation of CAPTCHAs, they want to form the base of future democracy and social welfare.</p><p><span>To join the Worldcoin network, people download the World App crypto wallet on their smartphone. They then find a nearby Orb, submit to its iris scan and other biometric humanness-detection, become “</span><em>Orb-verified”</em><span> and receive a World ID. To accomplish this, the Orb scans the iris, applies its special algorithm so that it can compare the iris scan to the others in its database and ensure uniqueness (while accounting for the fact that two scans of the same iris may not be visually identical due to factors such as lighting or angle), and verifies the ID if the iris is indeed new to the database.</span></p><p><span>Worldcoin is very quick to insist that they do not store the iris scan data directly, but rather store an “IrisCode”, which they describe as a mere “set of numbers” on its website.</span></p><p><span> The IrisCode has been widely described in media as a “</span><a href="https://en.wikipedia.org/wiki/Cryptographic_hash_function" rel="">hashed</a><span>” version of the iris scan, and indeed used to be called an “IrisHash” by WorldCoin, but references to hashes seem to have been (somewhat incompletely) scrubbed from the website as of late. Worldcoin tries to insist that they don’t store sensitive biometrics, a claim that requires everyone to simply go along with their assumption that a per-person unique IrisCode </span><em>itself</em><span> is not sensitive data. It’s also not terribly clear yet what kind of data might be leaked by the IrisCode — for example, Vitalik Buterin has questioned if some traits captured in the code might reflect things like sex, ethnicity, or medical conditions.</span></p><p><span>Worldcoin is also considerably less forward about the fact that they encourage users who sign up to “opt in” to image custody. For those who opt in, WorldCoin continues to store the original iris images, they say “because the algorithm that computes the iris code is still evolving to make sure it can support signing up everyone”.</span></p><p><span> With Orbs still relatively scarce, users face the risk of being removed from the pool of verified World ID holders as the algorithm is refined, unless they either have ongoing access to an Orb at which they can be re-scanned, or acquiesce to their original data being retained.</span></p><p>Because the vague promises of maybe someday enabling DAO voting or AI-necessitated UBI are both intangible and probably unappealing to many of the massive number of people Worldcoin hopes to onboard (ranging from several billion to every single human on the planet, depending on which exec you ask and when), Worldcoin has decided to just pay people for their eyeballs.</p><p><span>But rather than handing out cold hard cash, those who sign up receive 25 Worldcoin tokens (WLD), and the opportunity to claim 1 WLD per week going forward. Or at least those in approved jurisdictions do — US-based users can’t receive tokens in return due to pesky regulatory concerns, and in several states or cities can’t be scanned at all.</span></p><p><span> The app is also not available in some jurisdictions, including mainland China.</span></p><p><span> And privacy regulators are already sniffing around in various European markets where Worldcoin has recently started scanning irises in a push following its big launch this week.</span></p><p><span>At the beginning of Worldcoin’s iris-scanning endeavours, the WLD that people received was no more than an IOU, since the token hadn’t yet launched. Since the token launch on July 24, the price has fluctuated between $1.94 and $2.69 — as of writing, it is hovering at around $2.35, making the initial 25-token distribution worth around $59 to anyone who immediately cashes out.</span></p><p>If by now you’ve found yourself thinking “scan my irises and give the data to a bunch of VC-backed tech bros in exchange for tokens that may or may not be worth around $60? sign me up!”, well, keep reading.</p><p><span>A caveat: Worldcoin is at this stage so incredibly vague about what exactly they envision people using the project </span><em>for</em><span> that it is difficult to analyze. I would certainly be asking very different questions of a project that simply aims to ensure people are only receiving their fair allotment of promotional NFTs than of one with aspirations of becoming the voting apparatus for “global democracy” or the operator of a worldwide universal basic income program.</span></p><p><span>Before launching in to future-facing issues with Worldcoin, it’s worth touching on its history a little bit. In April 2022, </span><em><a href="https://www.technologyreview.com/2022/04/06/1048981/worldcoin-cryptocurrency-biometrics-web3/" rel="">MIT Technology Review</a></em><span> and </span><em><a href="https://www.buzzfeednews.com/article/richardnieva/worldcoin-crypto-eyeball-scanning-orb-problems" rel="">BuzzFeed News</a></em><span> nearly simultaneously published longform articles stemming from their investigations of the project, particularly focusing on their experimentation in low-income communities, often in developing countries, and on individuals who did not always understand what they were agreeing to. The articles detailed numerous issues with the company, including unconscionable treatment of their hired “Orb operators”, the widespread use of questionable tactics to entice new people to sign up, inconsistent messaging about exactly what kind of data was being collected or preserved, and lack of compliance with local data privacy policies. Both articles are well worth the read.</span></p><p><span>Much of Worldcoin’s promises are predicated on the questionable idea that highly sophisticated artificial intelligence, even artificial general intelligence, is right around the corner. It also hinges on the “robots will take our jobs!” panic — a </span><a href="https://timeline.com/robots-have-been-about-to-take-all-the-jobs-for-more-than-200-years-5c9c08a2f41d" rel="">staple of the last couple centuries</a><span> — finally coming to bear. Worldcoin offers other use cases for its product too, like DAO voting, but it is not the promise to solve DAO voting that earned them a multi-billion dollar valuation from venture capitalists.</span></p><p><span>Other use cases that Worldcoin has offered seem to assume that various entities — governments, software companies, etc. — would actually </span><em>want</em><span> to use the Worldcoin system. This seems highly dubious to me, particularly given that many governments have established identification systems that already enjoy widespread use. Some even employ biometrics of their own, like India’s </span><a href="https://en.wikipedia.org/wiki/Aadhaar" rel="">Aadhaar</a><span>. There’s also the scalability question: Worldcoin operates on the Optimism Ethereum layer-2 blockchain, a much speedier alternative to the layer-1 Ethereum chain to be sure, but any blockchain is liable to be a poor candidate for handling the kind of volume demanded by a multi-billion user system processing everyday transactions.</span></p><p>And finally, bafflingly, Worldcoin seems to think it —&nbsp;a VC-backed corporation — is best positioned to save the world from this forecasted AI-induced economic upheaval via “AI-funded universal basic income”.</p><p>If you ask the proof-of-personhood folks, centralized identity systems suffer from unacceptable flaws, namely lack of privacy and the risk that the maintainer of the identity system could act maliciously towards members of the network (or could be corrupted or taken over by someone who does). They have some good points.</p><p>But Worldcoin itself is enormously centralized, and at this point, talk of decentralization is little more than handwavy promises. The custom Orb hardware presents a massive obstacle to decentralization that Worldcoin doesn’t seem to have meaningfully grappled with. If Worldcoin is the only group producing these Orbs, they exercise sole control over them — which, in turn, provides them the sole ability to introduce backdoors.</p><p><span>If the Orbs hardware is “decentralized”, which Worldcoin says they intend to do,</span></p><p><span> they then have to ensure that the Orbs are properly constructed following the design specifications they’ve provided, and haven’t been modified to maliciously create IDs outside of the intended mechanism. Worldcoin speaks vaguely of a third-party auditing system and allowlisting process that would attempt to catch any such malicious Orbs, but the scale of such audits required to allow for the quantity of Orbs to achieve billions of signups would be enormous. Furthermore, because Worldcoin incorporates cryptocurrency distributions, any bad actor who was able to slip through a malicious Orb capable of generating fake IDs could rapidly siphon WLD tokens, and even if discovered, the past distribution could not be reversed — they could only be prevented from continuing to create new IDs.</span></p><p><span>The cryptocurrency industry is rife with projects that embrace the idea of “progressive decentralization”: beginning out as a highly centralized project run by a small group, but promising to eventually turn over control of the project to a DAO. Few ever follow through,</span></p><p><span> but it is a convenient way to stave off criticism.</span></p><p>When questioned about the wisdom of attempting to form a huge database of iris scans, Worldcoin argues that only the IrisCode is stored.</p><p>When questioned about the wisdom of creating a system to accomplish everything from voting to welfare to everyday purchases, irrevocably tied to an individual person, all using public blockchains, Worldcoin argues “zero knowledge proofs!” </p><p>End of argument. Concerns assuaged. And indeed, some Worldcoin boosters seem satisfied with these very superficial answers, likely dazzled by the technical details that Worldcoin throws around with their posts about Gabor wavelets and phase-quadrant demodulation and poseidon hashes.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png" width="1456" height="934" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:934,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:686045,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Excerpt from a Worldcoin </span><a href="https://worldcoin.org/blog/engineering/iris-feature-extraction" rel="">post</a></figcaption></figure></div><p>But simply saying “we transform the iris data into something else” and “we’ll use zero knowledge proofs” should not be sufficient.</p><p><span>It is necessary to understand what kind of data can be leaked from the iris hashing algorithm — an algorithm that the team acknowledges is frequently changing. It is </span><em>also</em><span> necessary to understand what kinds of attacks could be enabled both on the network or on an individual participant if a malicious actor was able to obtain access to a participant’s data, ranging from a person’s WorldID account, to their IrisCode, to their full unhashed iris scan. This is a really critical issue, because there is no “password reset” when it comes to iris data. Questions about account recovery also remain unanswered — in previous reports, a person who uninstalled their World App was never able to regain access to their account.</span></p><p>There are also unanswered questions about the scalability of such an algorithm to the populations that Worldcoin is hoping to reach — past iterations of the algorithm have allowed one person to register multiple times, or have denied people access when they hadn’t already created an ID. While Worldcoin may have developed an algorithm that reliably distinguishes unique irises among its test pool, it’s not clear that it will work with a pool orders of magnitude larger.</p><p><span>As for zero knowledge proofs, Worldcoin trots this out as an answer to concerns about potentially connecting substantial amounts of sensitive data (transaction history and so forth) to a single permanent identifier. ZK proofs are a way of proving that something is true (e.g., “I have a valid World ID”, or “I’ve not yet received my WLD distribution this week”) without revealing additional details (e.g., which World ID is mine). But the implementation of this would be critical, and much of the burden here would lie on third parties — corporations, governments, etc. — that Worldcoin envisions adopting their system. A permanent record of potentially incredibly sensitive transaction histories, irrevocably linked to a biometric identifier, is a nightmare scenario. Worldcoin acknowledges this issue with no further elaboration: “While the Protocol is made to be used in a privacy-preserving manner, privacy cannot be enforced outside of the Protocol.”</span></p><p>Furthermore, Worldcoin relies heavily on each user’s ability and technical know-how to keep track of their private key, writing, “private keys need to remain private, as otherwise, a user can deanonymize themselves, even to actions they have performed in the past“.</p><p><span>Anti-surveillance advocate and Nym co-founder Harry Halpin describes Worldcoin’s knee-jerk “ZK proofs!” dismissal of privacy concerns — a </span><a href="https://twitter.com/molly0xFFF/status/1629730719437078529" rel="">frequent habit in the wider crypto world</a><span> — as “zero-knowledge washing“: “taking a fundamentally evil concept or dubious concept and trying to make it look socially acceptable by adding some zero-knowledge fairy dust on top of it.” He also expresses doubts about the longevity of ZK proofs, expecting “quantum computing [to] break zero-knowledge proofs“ in five to ten years,</span></p><p><span> though he and I differ somewhat on that particular prediction.</span></p><p><span>Those who resist Worldcoin’s initial rebuttals and continue to push the company on privacy concerns are then faced with Worldcoin’s next tactic: arguing that we </span><em>already</em><span> give up privacy in today’s society, so what’s a little more? Spokespeople trot out whataboutism with Apple and Google biometric scanning, and various backers argue that iris images are already widely distributed: “You have a headshot on your website. You walk around with your eyes open in front of cameras all day long.“</span></p><p><span>And if that doesn’t work? Well, there’s always investor Kyle Samani’s argument: “When it comes to Worldcoin, you don’t have to scan your eyeball. Like if you don’t want to, then fucking don’t.”</span></p><p><span> This option is reasonable — indeed, quite tempting — if Worldcoin is relegated to the realm of the trivial, enabling NFT airdrops and the like. When it comes to more important use cases, “optional” biometrics suddenly become much more problematic. For example, in India, HIV patients have found themselves needing to submit their “optional” biometrics-linked Aadhaar identification number in order to access antiretroviral therapies.</span></p><p><span>It’s not yet clear how Worldcoin envisions WLD functioning. They refer to it as a “digital currency” which “could… become the widest distributed digital asset”. If Worldcoin is to function as a currency, as you might expect of an asset that’s being distributed in a universal basic income program, it would need to overcome the same types of issues that keep Bitcoin from functioning </span><a href="https://en.wikipedia.org/wiki/Economics_of_bitcoin#Classification" rel="">anything like a currency</a><span>.</span></p><p>If it is to be more of a speculative asset that people hoard in hopes of the price going up, it seems ill-suited to Worldcoin’s UBI ambitions.</p><p><span>Furthermore, the initial token distribution looks a lot more like what you would expect out of the venture capital world than out of a public good organization. Worldcoin has generously reserved for insiders 25% of the WLD supply (up from an initial 20%, because development was evidently more “complex and costly” than anticipated).</span></p><p><span>It stands to reason that Worldcoin’s token distribution looks VC backed, and that’s because it is. In May, Worldcoin raised $115 million in a Series C round led by Blockchain Capital and joined by Bain Capital Crypto, Distributed Global, and, of course, </span><a href="https://newsletter.mollywhite.net/p/andreessen-horowitzs-state-of-crypto" rel="">Andreessen Horowitz</a><span>.</span></p><p><span> One wonders how they will balance their do-gooder mission with their need to generate massive returns for their backers.</span></p><p>Worldcoin’s loftier goals include “enabl[ing] global democratic processes”, providing global access to financial services, and even paying out AI-funded global universal basic income.</p><p>That is, if you have a smartphone and the technical know-how to use it, Internet connectivity, and access to an Orb. For Worldcoin’s more financial ambitions, people would also need access to an exchange where they could swap WLD for their local currency, or WLD would need to be widely adopted as a form of payment by merchants.</p><p><span>Today, an estimated 66% of the world uses a smartphone.</span></p><p><span> Around the same percentage has access to the Internet, but this varies immensely by region, and is impacted by other factors including wealth and gender.</span></p><p><span>Access to Orbs is a much more existential issue for the project: there are 346 Orbs out there in the world right now: that is one per every 23 million people. Worldcoin announced they will be rolling out</span></p><p><span> more Orbs to reach a total number of 1,500 — meaning that then a mere 5.3 million people would have to travel to and line up per Orb.</span></p><p><span> Sam Altman has recently boasted (without evidence) that one person is being signed up every eight seconds.</span></p><p><span> With their claimed two million signups as a head start, at that rate they’ll have all 8 billion people in the world signed up by 2055 (assuming no population change, or change in rate of signup).</span></p><p>Finally, if Worldcoin truly wishes to onboard every person in the world, or be used for critical tasks, they will at some point have to grapple with the fact that not everyone has irises that can be scanned, due to factors including birth defects, surgeries, or disease.</p><p>“Show me the incentive and I'll show you the outcome,” says Charlie Munger.</p><p>What will happen when you promise people anywhere from $10 to $100 for scanning their eyeball? What if that’s not dollars, but denominated in a crypto token, making it appealing to speculators? And what if some people don’t have the option to scan their own eyeballs to achieve access to it?</p><p><span>A black market for Worldcoin accounts has </span><a href="https://web3isgoinggreat.com/?id=sam-altmans-worldcoin-project-incentivizes-a-black-market-for-biometric-data-taken-from-people-in-developing-nations" rel="">already emerged</a><span> in Cambodia, Nigeria, and elsewhere, where people are being paid to sign up for a World ID and then transfer ownership to buyers elsewhere — many of whom are in China, where Worldcoin is restricted. There is no ongoing verification process to ensure that a World ID </span><em>continues</em><span> to belong to the person who signed up for it, and no way for the eyeball-haver to recover an account that is under another person’s control. Worldcoin acknowledges that they have no clue how to resolve the issue: “Innovative ideas in mechanism design and the attribution of social relationships will be necessary.“ The lack of ongoing verification also means that there is no mechanism by which people can be removed from the program once they pass away, but perhaps Worldcoin will add survivors’ benefits to its list of use cases and call that a feature.</span></p><p>Relatively speaking, scanning your iris and selling the account is fairly benign. But depending on the popularity of Worldcoin, the eventual price of WLD, and the types of things a World ID can be used to accomplish, the incentives to gain access to others’ accounts could become severe. Coercion at the individual or state level is absolutely within the realm of possibility, and could become dangerous.</p><p>Worldcoin seems to be embracing a modified version of the “move fast and break things” mantra that has become all too popular in the tech world. “Build a massive database of biometric data and then figure out what to do with it someday” is a little less catchy, though.</p><p>The issues with Worldcoin that I list here are far from exhaustive, and I’ve included some further reading below from others who’ve shared their thoughts.</p><ul><li><p><span>Vitalik Buterin, “</span><a href="https://vitalik.ca/general/2023/07/24/biometric.html" rel="">What do I think about biometric proof of personhood?</a><span>” July 24, 2023.</span></p></li><li><p><span>“</span><a href="https://blockworks.co/news/worldcoin-privacy-concerns" rel="">Worldcoin isn’t as bad as it sounds: It’s worse</a><span>”. </span><em>Blockworks</em><span>, July 26, 2023.</span></p></li></ul><ul><li><p><span>Molly White, “</span><a href="https://blog.mollywhite.net/is-acceptably-non-dystopian-self-sovereign-identity-even-possible/#verifiable-attestations" rel="">Is "acceptably non-dystopian" self-sovereign identity even possible?</a><span>” June 10, 2022. </span></p></li><li><p><span>Richard Nieva and Aman Sethi, “</span><a href="https://www.buzzfeednews.com/article/richardnieva/worldcoin-crypto-eyeball-scanning-orb-problems" rel="">Worldcoin Promised Free Crypto If They Scanned Their Eyeballs With ‘The Orb.’ Now They Feel Robbed.</a><span>” BuzzFeed News, April 5, 2022.</span></p></li><li><p><span>Eileen Guo and Adi Renaldi, “</span><a href="https://www.technologyreview.com/2022/04/06/1048981/worldcoin-cryptocurrency-biometrics-web3/" rel="">Deception, exploited workers, and cash handouts: How Worldcoin recruited its first half a million test users</a><span>”. MIT Technology Review, April 6, 2022.</span></p></li></ul></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X to Close – The origins of the use of [x] in UI design. (2014) (105 pts)]]></title>
            <link>https://medium.com/re-form/x-to-close-417936dfc0dc</link>
            <guid>36931344</guid>
            <pubDate>Sun, 30 Jul 2023 14:16:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/re-form/x-to-close-417936dfc0dc">https://medium.com/re-form/x-to-close-417936dfc0dc</a>, See on <a href="https://news.ycombinator.com/item?id=36931344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="subtitle">The origins of the use of [x] in UI design.</h2><div><a rel="noopener follow" href="https://medium.com/@laurenarcher?source=post_page-----417936dfc0dc--------------------------------"><div aria-hidden="false"><p><img alt="Lauren Archer" src="https://miro.medium.com/v2/resize:fill:88:88/0*HsX1xYPTYOkPoGvf.jpeg" width="44" height="44" loading="lazy"></p></div></a><a href="https://medium.com/re-form?source=post_page-----417936dfc0dc--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="re:form" src="https://miro.medium.com/v2/resize:fill:48:48/1*IwSEKGLwbf_w_3kwfzIU8Q.jpeg" width="24" height="24" loading="lazy"></p></div></a></div></div><div><figure><a href="http://ad.doubleclick.net/ddm/trackclk/N8295.1674088NATIV.LY/B8154727.110922782;dc_trk_aid=283990515;dc_trk_cid=58990176"></a></figure><p id="ddbc">X’s are everywhere in user interface (UI) design. A powerful symbol, [x] is capable of closing windows and popups, toolbars and tabs and anything else that might otherwise be cluttering up your screen.</p><figure><figcaption>Twitter X</figcaption></figure><p id="8565">Clicking on [x] to close a feature has become an instinctual part of using a computer and a standard in web and software design. Although it may seem like the ubiquitous [x] has always been a part of Graphical User Interfaces (GUI), a quick jaunt through the history of GUIs reveals that this actually isn’t the case.</p><p id="c6cd">So where and when did the [x] first enter into the UI lexicon?</p><figure><figcaption>Chrome X</figcaption></figure><p id="a461">To track the [x] back to its origin, let’s start with the status quo: Microsoft.</p><p id="2983">If you are using Windows then you should be able to spot at least one [x] on your screen right now.</p><figure><figcaption>Windows 7 X</figcaption></figure><p id="94cc">But <a href="http://toastytech.com/guis/win101.html" rel="noopener ugc nofollow" target="_blank">Windows 1.0</a> didn’t use an [x] to close.</p><figure><figcaption>Windows 1.0</figcaption></figure><p id="a2b1"><a href="http://toastytech.com/guis/win203.html" rel="noopener ugc nofollow" target="_blank">Nor did 2.0.</a></p><figure><figcaption>Windows 2.0</figcaption></figure><p id="f9fd"><a href="http://toastytech.com/guis/win30.html" rel="noopener ugc nofollow" target="_blank">Or 3.0?</a></p><figure><figcaption>Windows 3.0</figcaption></figure><p id="8fd3">The [x] button didn’t show up until <a rel="noopener" href="https://medium.com/">Windows 95</a>, when the close button was moved to the right hand side, joining minimize and maximize.</p><figure><figcaption>Windows 95</figcaption></figure><p id="7795">There is even evidence that this was a late addition to Windows 95. In this early demo (Codename: Chicago), the minimize and maximize buttons have been redesigned, but the close button remains the same, and to the left as before.</p><figure><figcaption>Windows Chicago<br>August 1993</figcaption></figure><p id="e939">So, who was responsible for this last minute change? As far as I can tell, this person is responsible for the proliferation and widespread use of [x] in UI design today.</p><p id="6839">The intent of Windows 95 was always to get a computer on every desk and in every home. Design changes from Windows 3.0 were made specifically in response to usability feedback. The goal was to ensure that any computer novice would be able to learn Windows 95.</p><p id="4365">It worked.</p><p id="f699">Windows 95 eliminated all other OS competition, and was adopted by businesses and for home use worldwide.</p><p id="c0d3">But our goal today isn’t to pinpoint when the [x] became popular, but rather to find out when it first entered into UI design.</p><p id="131c">Can we find an earlier example of [x] in a GUI?</p><p id="beca"><a href="http://toastytech.com/guis/indexos2.html" rel="noopener ugc nofollow" target="_blank">Mac OS </a>didn’t use an [x] to close. Only in OS X did an [x] first appear, and then only when you hover over the red close button.</p><figure><figcaption>Mac OS 2: Pretty Colours!</figcaption></figure><p id="816c">And <a href="http://whiteandnoisy.org/linux.html" rel="noopener ugc nofollow" target="_blank">Linux GUI’s</a> started to use the [x] symbol only after the release of Windows 95.</p><figure><figcaption>X Window System</figcaption></figure><p id="dd2b">We aren’t getting very far this way, so let’s go back to the very beginning. Back before Windows or Linux or Mac OS. Back to the very first GUI to fully utilize the <a href="http://en.wikipedia.org/wiki/Desktop_metaphor" rel="noopener ugc nofollow" target="_blank">‘desktop metaphor’</a> that we are all so familiar with: The 8010 Information System from Xerox.</p><figure><figcaption>Xerox 8108</figcaption></figure><p id="c78d">Also known as “The Xerox Star”, “ViewPoint”, or “GlobalView”, Xerox started the development of the 8101 in 1977 and first sold the system in 1981 as “Dandelion”. This is the GUI that Apple allegedly modeled their Lisa/Mac OS after, inspired by a tour of the Xerox headquarters in December 1979.</p><p id="a2a1">No [x], though:</p><figure><figcaption>Xerox Star</figcaption></figure><p id="d51f">Recall that no [x] appears in early Apple operating systems either:</p><figure><figcaption>Mac OS 1</figcaption></figure><p id="ee28">There is no [x] to be found in the<a href="http://toastytech.com/guis/vision.html" rel="noopener ugc nofollow" target="_blank"> Visi On GUI</a>, the first integrated graphical software environment for IBM PCs, released in 1983:</p><figure><figcaption>Visi On</figcaption></figure><p id="f143">The<a href="http://toastytech.com/guis/gem11.html" rel="noopener ugc nofollow" target="_blank"> GEM user interface</a>, developed by Digital Research for and DOS-based computers in 1984, didn’t have an [x]:</p><figure><figcaption>GEM</figcaption></figure><p id="b758">But! What is this? Could it be?</p><figure><figcaption>Atari TOS 1.0</figcaption></figure><p id="5788">This is a screenshot of <a href="http://toastytech.com/guis/tos.html" rel="noopener ugc nofollow" target="_blank">Atari TOS 1.0</a>. Built on top of GEM to be ported to the Atari ST in 1985, from the computers division of Atari Corp. It is the earliest example of the [x] button I’ve been able to find.</p><p id="1e8d">So why here? Why now?</p><p id="3d58">This may be another example of Atari, an American company, borrowing from Japanese culture. The first example, of course, being the name Atari itself, a Japanese term from the game Go that means “to hit the target”.</p><p id="b68c">The use of [x] for close and [o] for open could come from the Japanese symbols batsu and maru.</p><figure><figcaption>Maru (o) and Batsu (x)</figcaption></figure><p id="51eb">Batsu (x) is the symbol for incorrect, and can represent false, bad, wrong or attack, while maru (o) means correct, true, good, whole, or something precious. Batsu and maru are also common hand gestures. Cross your arms over your chest for batsu, circle your arms over your head for maru.</p><figure><figcaption>Playstation Controller</figcaption></figure><p id="3ce3">Another familiar example of batsu/maru is in the Playstation controller design, where maru and batsu are used for yes and no.</p><p id="6b38">This is just a theory, however. Not being there myself at the time, I can’t be sure.</p><p id="f3c1">For the sake of being thorough let’s see if we can go back any further.</p><p id="dae2">The first line-based text editor was Quick Editor or <a href="http://cm.bell-labs.com/who/dmr/qedman.html" rel="noopener ugc nofollow" target="_blank">qed</a>, written by Ken Thompson in 1965, who later helped to develop Unix. Qed uses [q] for the quit command.</p><p id="a094">Early text editors used a bunch of different escape commands: [q], [e], [c], and [ESC], but [x] was never a popular option. <a href="https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;cd=3&amp;cad=rja&amp;ved=0CEAQFjAC&amp;url=http%3A%2F%2Fwww.gnu.org%2Fs%2Fed%2Fmanual%2Fed_manual.html&amp;ei=AUP5UtfrIeTW2AW5_oGoCw&amp;usg=AFQjCNFF6ETHooKV-YRiBoPd7_EZp3R65w&amp;sig2=p8wLvQ4lnpi9P06efsJObA&amp;bvm=bv.61190604%2Cd.b2I" rel="noopener ugc nofollow" target="_blank">Ed</a>, em and <a href="http://pic.dhe.ibm.com/infocenter/zos/v1r13/index.jsp?topic=%2Fcom.ibm.zos.r13.bpxa500%2Fex.htm" rel="noopener ugc nofollow" target="_blank">ex</a>, early text editors spawned from qed around 1971, didn’t use [x.]</p><p id="ef8a"><a href="http://ex-vi.sourceforge.net/vi.html" rel="noopener ugc nofollow" target="_blank">Vi</a>, <a href="http://www.radford.edu/~mhtay/CPSC120/VIM_Editor_Commands.htm" rel="noopener ugc nofollow" target="_blank">vim</a>, <a href="http://www.cs.colostate.edu/helpdocs/emacs.html" rel="noopener ugc nofollow" target="_blank">emacs</a> or <a href="http://help.fdos.org/en/hhstndrd/base/edlin.htm" rel="noopener ugc nofollow" target="_blank">edlin</a>?</p><p id="f0f0">No [x] to close these 1980's text editors either. X was commonly used to delete characters in-line, but not to close the program.</p><p id="1cf0">[x] is a true icon, not representing a letter but representing an action, and only adopted to represent ‘close’ well after the development of graphics-oriented operating systems. The first appearance of [x] in GUI design was likely the Atari TOS, possibly influenced by the Japanese batsu and maru conventions. Thanks to a last minute design change in Windows 95, and the mass adoption of Windows worldwide, [x] has become the standard symbol for ‘close’, a symbol that dominates web, app and software design today.</p><p id="eff7">That’s all for now.</p><p id="1d60">[x]</p><p id="a1ca"><em>Screenshots from </em><a href="http://toastytech.com/guis/" rel="noopener ugc nofollow" target="_blank"><em>http://toastytech.com/guis/</em></a><em> and </em><a href="http://whiteandnoisy.org/" rel="noopener ugc nofollow" target="_blank"><em>http://whiteandnoisy.org/</em></a></p><p id="c186">UPDATE:</p><p id="bbb0">So this little article has travelled pretty far! There were <a href="https://news.ycombinator.com/item?id=8171340" rel="noopener ugc nofollow" target="_blank">a lot of good tips, comments and insights</a> into the origin of [x] but none as good as this email that I received from Windows 95 team member Daniel Oran.</p><p id="46ae"><em>“Hi Lauren,</em></p><p id="9e64"><em>A friend forwarded me your Medium piece, “X to Close.” He remembered that I had worked on Windows 95 at Microsoft — I </em><a href="http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=5757371" rel="noopener ugc nofollow" target="_blank"><em>created</em></a><em> the Start Button and Taskbar — and thought I’d be amused. I was! :-)</em></p><p id="63cd"><em>It’s fun to see how history gets written when you actually lived those long-ago events. I joined Microsoft in 1992 as a program manager for the user interface of “Chicago,” which was the code name for what eventually became Windows 95.</em></p><blockquote><p id="ee13"><strong><em>So, who was responsible for this last minute change? As far as I can tell, this person is responsible for the proliferation and widespread use of [x] in UI design today.</em></strong></p></blockquote><p id="c4a6"><em>It wasn’t a last-minute change. During 1993, we considered many variations of the close-button design. And the source wasn’t Atari. It was </em><a href="http://vimeo.com/30110130" rel="noopener ugc nofollow" target="_blank"><em>NeXT</em></a><em>, which had an X close button in the upper right, along with the grayscale faux-3D look that we borrowed for Windows 95.</em></p><p id="fc0e"><em>I wanted to put the Windows X close button in the upper left, but that conflicted with the existing Windows Alt-spacebar menu and also a new program icon, which we borrowed from</em><a href="http://www.os2museum.com/wp/?page_id=112" rel="noopener ugc nofollow" target="_blank"><em>OS/</em></a><em>2, on which Microsoft had originally partnered with IBM.</em></p><p id="82cf"><em>Attached is the earliest Chicago bitmap I could find that includes an X close button. It’s dated 9/22/1993. (In attaching the file to this email, I just realized that it’s so old that it has only an eight-character name. Before Windows 95, that was the limit.)</em></p><p id="5ce0"><em>Thanks for your very entertaining essay!</em></p><p id="6877"><em>Best,</em></p><p id="b15c"><em>Danny”</em></p><figure><figcaption><em>Windows Chicago 9/22/1993.</em></figcaption></figure><p id="a74c">I guess you could say case [x]ed.</p><figure><figcaption><a href="http://vimeo.com/30110130" rel="noopener ugc nofollow" target="_blank">N</a>eXT 1988</figcaption></figure><p id="0c79"><a href="https://news.ycombinator.com/item?id=8171340" rel="noopener ugc nofollow" target="_blank">Thanks again to everyone who helped track down earlier examples of GUIs and early text editors that used [x] to close as well.</a> Fascinating!</p><p id="af24"><em>This story was originally published by Lauren Archer on February 10, and was added to the re:form collection today because we loved Lauren’s smart take on UI design.</em></p><figure><a href="http://www.medium.com/re-form"></a></figure><p id="15a9"><em>You can follow Lauren Archer on Twitter at @</em><a href="https://twitter.com/laurenarcher" rel="noopener ugc nofollow" target="_blank"><em>laurenarcher</em></a><em>. Subscribe to re:form’s </em><a href="https://medium.com/feed/re-form" rel="noopener"><em>RSS feed</em></a><em>, sign up to receive our stories </em><a href="http://eepurl.com/Z84dH" rel="noopener ugc nofollow" target="_blank"><em>by email</em></a><em>, and follow the main page </em><a href="https://medium.com/re-form" rel="noopener"><em>here</em></a><em>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Long History of Nobody Wants to Work Anymore (255 pts)]]></title>
            <link>https://mstdn.ca/@paulisci/110798058470040619</link>
            <guid>36931129</guid>
            <pubDate>Sun, 30 Jul 2023 13:49:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mstdn.ca/@paulisci/110798058470040619">https://mstdn.ca/@paulisci/110798058470040619</a>, See on <a href="https://news.ycombinator.com/item?id=36931129">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[New insights into the origin of the Indo-European languages (161 pts)]]></title>
            <link>https://www.mpg.de/20666229/0725-evan-origin-of-the-indo-european-languages-150495-x</link>
            <guid>36930321</guid>
            <pubDate>Sun, 30 Jul 2023 12:03:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mpg.de/20666229/0725-evan-origin-of-the-indo-european-languages-150495-x">https://www.mpg.de/20666229/0725-evan-origin-of-the-indo-european-languages-150495-x</a>, See on <a href="https://news.ycombinator.com/item?id=36930321">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  
  
  <p>Linguistics and genetics combine to suggest a new hybrid hypothesis for the origin of the Indo-European languages</p>
  

  

  <p>An international team of linguists and geneticists led by researchers from the Max Planck Institute for Evolutionary Anthropology in Leipzig has achieved a significant breakthrough in our understanding of the origins of Indo-European, a family of languages spoken by nearly half of the world’s population.</p>
  
  
<figure data-description="A hybrid hypothesis for the origin and spread of the Indo-European languages. The language family began to diverge from around 8100 years ago, out of a homeland immediately south of the Caucasus. One migration reached the Pontic-Caspian and Forest Steppe around 7000 years ago, and from there subsequent migrations spread into parts of Europe around 5000 years ago." data-picture="base64;<picture class="" data-iesrc="/20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MTQwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--dd8ea4ce53e785c11ff182e1b16e60204a1c3e1d" data-alt="A hybrid hypothesis for the origin and spread of the Indo-European languages. The language family began to diverge from around 8100 years ago, out of a homeland immediately south of the Caucasus. One migration reached the Pontic-Caspian and Forest Steppe around 7000 years ago, and from there subsequent migrations spread into parts of Europe around 5000 years ago." data-class=""><source media="(max-width: 767px)" srcset="/20667325/original-1690297060.webp?t=eyJ3aWR0aCI6NDE0LCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--a6895bf05f58d993a02b1e907e2b84d2e5a86f24 414w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6Mzc1LCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--8f5238ac864dd928701b937c6afb26f2af9183e6 375w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MzIwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--67fdc411cfb51174ff935a98c60a0cb9153f3156 320w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6NDExLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--6412f895379ad2fb2db6c5d8d15be02147174867 411w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6NDgwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--581031a026ed8b6c97ad06df5bbbccadee09522a 480w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MzYwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--4e3a2b460fe9d1923dad3f0a53296c13ae0f7b43 360w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6ODI4LCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--6abb83f5b0f6f4e1b5ce9e6a24ab71b519d165b2 828w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6NzUwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--9896a242a63dd7cec7a3b70f9b94bb355b21033b 750w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6NjQwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--ab625ea9da349dbdf94c1be297f78baf31150123 640w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6ODIyLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--51b01d26c5550109b7a4beb86f868a12f88ec6fe 822w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6OTYwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--b9ddd03cf848d6df3eea7090efdac1da5bd6c98b 960w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6NzIwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--c5cd9de937ea7f11f75a1dc948ced6f7b803f37c 720w" sizes="100vw" type="image/webp" /><source media="(max-width: 767px)" srcset="/20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6NDE0LCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--4d112561c8d197b15a12922eb97525e821be1a9d 414w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6Mzc1LCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--b5ec5cda0a5edcc4d7795b6bbaa6d950d2e3bbd2 375w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MzIwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--1522bde8d9b032360c0e5f80b804b84bbcb564d4 320w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6NDExLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--03bf28f91403f6744b824be9bf9c7fa356dcd0fc 411w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6NDgwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--d970bb12a83d3092bd50144eefa905fae76a9095 480w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MzYwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--58499507f157b124137ab57fd662ae0a3c24b579 360w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6ODI4LCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--ae63a14884d3ad97c92f97eac48315a84662ea99 828w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6NzUwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--8a24e92a5e70a710a97ef2e432212a86c4dd488e 750w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6NjQwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--5225ab33a56f2769c3f351d9b8ea3e281db93224 640w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6ODIyLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--007a3706ff0090932c500367126841f6d8e0e10e 822w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6OTYwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--7ff94056bf83a1a9fc9414a688e8575e98ea7eee 960w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6NzIwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--8f1e67194eee41de39d67fd1ea652898fb781132 720w" sizes="100vw" type="image/jpeg" /><source media="(min-width: 768px) and (max-width: 991px)" srcset="/20667325/original-1690297060.webp?t=eyJ3aWR0aCI6OTAwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--579fb3fb3febb0cf7387f34cd7f99e2ed8ed4312 900w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MTgwMCwiZmlsZV9leHRlbnNpb24iOiJ3ZWJwIiwib2JqX2lkIjoyMDY2NzMyNX0%3D--ffbfa9ea72be1e7e40281e24bcf3e13d3e550899 1800w" sizes="900px" type="image/webp" /><source media="(min-width: 768px) and (max-width: 991px)" srcset="/20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6OTAwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--3451763374b584702a6027d7517f874fd781bd31 900w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MTgwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--c786a6de7d1b625b6fda3b2eac76ce16f75a582f 1800w" sizes="900px" type="image/jpeg" /><source media="(min-width: 992px) and (max-width: 1199px)" srcset="/20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MTIwMCwiZmlsZV9leHRlbnNpb24iOiJ3ZWJwIiwicXVhbGl0eSI6ODYsIm9ial9pZCI6MjA2NjczMjV9--46e055c2eacbf2fdff98be1a8f41bb5dc40f314d 1200w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MjQwMCwiZmlsZV9leHRlbnNpb24iOiJ3ZWJwIiwib2JqX2lkIjoyMDY2NzMyNX0%3D--5a8cdb718d73b073cc98ab562be645f67fd1fb1a 2400w" sizes="1200px" type="image/webp" /><source media="(min-width: 992px) and (max-width: 1199px)" srcset="/20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MTIwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--6ad579df03a590bb268bf07c515b1451435ab827 1200w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MjQwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--f72d84bb1388f2e13a03a47498350c3c2b865cbb 2400w" sizes="1200px" type="image/jpeg" /><source media="(min-width: 1200px)" srcset="/20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MTQwMCwiZmlsZV9leHRlbnNpb24iOiJ3ZWJwIiwicXVhbGl0eSI6ODYsIm9ial9pZCI6MjA2NjczMjV9--73b2428ab32a7a43088577907538ec76f9c5e4af 1400w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MjgwMCwiZmlsZV9leHRlbnNpb24iOiJ3ZWJwIiwib2JqX2lkIjoyMDY2NzMyNX0%3D--7b4a68f127b0e33d887c6cedd2d275fc75bd639e 2800w" sizes="1400px" type="image/webp" /><source media="(min-width: 1200px)" srcset="/20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MTQwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--dd8ea4ce53e785c11ff182e1b16e60204a1c3e1d 1400w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MjgwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--0266dfde40a67099b7270dffaf277376497f591a 2800w" sizes="1400px" type="image/jpeg" /><img alt="A hybrid hypothesis for the origin and spread of the Indo-European languages. The language family began to diverge from around 8100 years ago, out of a homeland immediately south of the Caucasus. One migration reached the Pontic-Caspian and Forest Steppe around 7000 years ago, and from there subsequent migrations spread into parts of Europe around 5000 years ago." class="" title="A hybrid hypothesis for the origin and spread of the Indo-European languages. The language family began to diverge from around 8100 years ago, out of a homeland immediately south of the Caucasus. One migration reached the Pontic-Caspian and Forest Steppe around 7000 years ago, and from there subsequent migrations spread into parts of Europe around 5000 years ago." src="/20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MTQwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--dd8ea4ce53e785c11ff182e1b16e60204a1c3e1d" /></picture>">
      
      

    
</figure>

<p>For over two hundred years, the origin of the Indo-European languages has been disputed. Two main theories have recently dominated this debate: the ‘Steppe’ hypothesis, which proposes an origin in the Pontic-Caspian Steppe around 6000 years ago, and the ‘Anatolian’ or ‘farming’ hypothesis, suggesting an older origin tied to early agriculture around 9000 years ago. Previous phylogenetic analyses of Indo-European languages have come to conflicting conclusions about the age of the family, due to the combined effects of inaccuracies and inconsistencies in the datasets they used and limitations in the way that phylogenetic methods analyzed ancient languages.</p><p>To solve these problems, researchers from the Department of Linguistic and Cultural Evolution at the Max Planck Institute for Evolutionary Anthropology assembled an international team of over 80 language specialists to construct a new dataset of core vocabulary from 161 Indo-European languages, including 52 ancient or historical languages. This more comprehensive and balanced sampling, combined with rigorous protocols for coding lexical data, rectified the problems in the datasets used by previous studies.</p><h2>Indo-European estimated to be around 8100 years old</h2><p>The team used recently developed ancestry-enabled Bayesian phylogenetic analysis to test whether ancient written languages, such as Classical Latin and Vedic Sanskrit, were the direct ancestors of modern Romance and Indic languages, respectively. Russell Gray, Head of the Department of Linguistic and Cultural Evolution and senior author of the study, emphasized the care they had taken to ensure that their inferences were robust. “Our chronology is robust across a wide range of alternative phylogenetic models and sensitivity analyses”, he stated. These analyses estimate the Indo-European family to be approximately 8100 years old, with five main branches already split off by around 7000 years ago.</p><p>These results are not entirely consistent with either the Steppe or the farming hypotheses. The first author of the study, Paul Heggarty, observed that “Recent ancient DNA data suggest that the Anatolian branch of Indo-European did not emerge from the Steppe, but from further south, in or near the northern arc of the Fertile Crescent — as the earliest source of the Indo-European family. Our language family tree topology, and our lineage split dates, point to other early branches that may also have spread directly from there, not through the Steppe.”</p><h2>New insights from genetics and linguistics</h2><p>The authors of the study therefore proposed a new hybrid hypothesis for the origin of the Indo-European languages, with an ultimate homeland south of the Caucasus and a subsequent branch northwards onto the Steppe, as a secondary homeland for some branches of Indo-European entering Europe with the later Yamnaya and Corded Ware-associated expansions. “Ancient DNA and language phylogenetics thus combine to suggest that the resolution to the 200-year-old Indo-European enigma lies in a hybrid of the farming and Steppe hypotheses”, remarked Gray.</p><p>Wolfgang Haak, a Group Leader in the Department of Archaeogenetics at the Max Planck Institute for Evolutionary Anthropology, summarizes the implications of the new study by stating, “Aside from a refined time estimate for the overall language tree, the tree topology and branching order are most critical for the alignment with key archaeological events and shifting ancestry patterns seen in the ancient human genome data. This is a huge step forward from the mutually exclusive, previous scenarios, towards a more plausible model that integrates archaeological, anthropological and genetic findings.”</p>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Emacs 29.1 Released (309 pts)]]></title>
            <link>https://emacsredux.com/blog/2023/07/30/emacs-29-1-released/</link>
            <guid>36929514</guid>
            <pubDate>Sun, 30 Jul 2023 09:47:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://emacsredux.com/blog/2023/07/30/emacs-29-1-released/">https://emacsredux.com/blog/2023/07/30/emacs-29-1-released/</a>, See on <a href="https://news.ycombinator.com/item?id=36929514">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p>Today is a great day for Emacs - Emacs 29.1 has just been released<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>!
Every Emacs release is special, but I haven’t been so excited about a new version of Emacs
in ages. Why so?</p>

<p><strong>Reason #1</strong> - <a href="https://batsov.com/articles/2021/12/06/emacs-is-not-a-proper-gtk-application/">pure GTK
front-end</a>
(a.k.a. <code>pgtk</code>). This also means that now Emacs supports natively Wayland. Which in tern means that it’s easier than ever to run <a href="https://emacsredux.com/blog/2021/12/19/using-emacs-on-windows-11-with-wsl2/">Emacs in Windows’s WSL</a>. This is huge!</p>

<p><strong>Reason #2</strong> - built-in support for the massively popular <a href="https://microsoft.github.io/language-server-protocol/">Language Server Protocol</a> via <a href="https://github.com/joaotavora/eglot">eglot</a>. <code>eglot</code> has existed for a while, but it’s nice
to see it bundled with Emacs going forward. This will certainly make Emacs better positioned to complete with “modern” editors like VS Code.</p>

<p><strong>Reason #3</strong> - built-in support for
<a href="https://tree-sitter.github.io/tree-sitter/">TreeSitter</a>. This means that a few
years down the road we’ll have many Emacs major modes that are much faster, robust
and feature-rich. It’s infinitely easier to built a major mode using a real
parser instead of using regular expressions.  Lots of built-in modes have
already been updated to have a version using <code>TreeSitter</code> internally. Frankly, I
can’t think of a bigger improvement in Emacs in the almost 20 years I’ve been an
Emacs user. Exciting times ahead!</p>

<p>You can read all about the new release <a href="https://github.com/emacs-mirror/emacs/blob/master/etc/NEWS.29">here</a>. I’ll likely write a few articles about some of the new features in the weeks and months to come. In Emacs We Trust! M-x Forever!</p>

<p><strong>P.S.</strong> Feel free to share in the comments what are you most excited about.</p>



    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ffmprovisr – Making FFmpeg Easier (301 pts)]]></title>
            <link>https://amiaopensource.github.io/ffmprovisr/</link>
            <guid>36929499</guid>
            <pubDate>Sun, 30 Jul 2023 09:44:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://amiaopensource.github.io/ffmprovisr/">https://amiaopensource.github.io/ffmprovisr/</a>, See on <a href="https://news.ycombinator.com/item?id=36929499">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <div>
      <h2 id="about">About ffmprovisr</h2>
      <h3>Making FFmpeg Easier</h3>
      <p>FFmpeg is a powerful tool for manipulating audiovisual files. Unfortunately, it also has a steep learning curve, especially for users unfamiliar with a command line interface. This app helps users through the command generation process so that more people can reap the benefits of FFmpeg.</p>
      <p>Each button displays helpful information about how to perform a wide variety of tasks using FFmpeg. To use this site, click on the task you would like to perform. A new window will open up with a sample command and a description of how that command works. You can copy this command and understand how the command works with a breakdown of each of the flags.</p>
      <p>This page does not have search functionality, but you can open all recipes (second option in the sidebar) and use your browser's search tool (often ctrl+f or cmd+f) to perform a keyword search through all recipes.</p>
      <h3>Tutorials</h3>
      <p>For FFmpeg basics, check out the program’s <a href="https://ffmpeg.org/" target="_blank">official website</a>.</p>
      <p>For instructions on how to install FFmpeg on Mac, Linux, and Windows, refer to Reto Kromer’s <a href="https://avpres.net/FFmpeg/#ch1" target="_blank">installation instructions</a>.</p>
      <p>For Bash and command line basics, try the <a href="https://learnpythonthehardway.org/book/appendixa.html" target="_blank">Command Line Crash Course</a>. For a little more context presented in an ffmprovisr style, try <a href="https://explainshell.com/" target="_blank">explainshell.com</a>!</p>
      <h3>License</h3>
      <p>
        <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank"><img alt="Creative Commons License" src="https://amiaopensource.github.io/ffmprovisr/img/cc.png"></a><br>
        This work is licensed under a <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">Creative Commons Attribution 4.0 International License</a>.
      </p>
      <h3>Sibling projects</h3>
      <p><a href="https://dd388.github.io/crals/" target="_blank">Script Ahoy</a>: Community Resource for Archivists and Librarians Scripting</p>
      <p><a href="https://datapraxis.github.io/sourcecaster/" target="_blank">The Sourcecaster</a>: an app that helps you use the command line to work through common challenges that come up when working with digital primary sources.</p>
      <p><a href="https://pugetsoundandvision.github.io/micropops/" target="_blank">Micropops</a>: One liners and automation tools from Moving Image Preservation of Puget Sound</p>
      <p><a href="https://amiaopensource.github.io/cable-bible/" target="_blank">Cable Bible</a>: A Guide to Cables and Connectors Used for Audiovisual Tech</p>
      <p><a href="https://eaasi.gitlab.io/program_docs/qemu-qed/" target="_blank">QEMU QED</a>: instructions for using QEMU (Quick EMUlator), a command line application for computer emulation and virtualization</p>
      <p><a href="https://amiaopensource.github.io/ffmpeg-artschool/" target="_blank">ffmpeg-artschool</a>: An AMIA workshop featuring scripts, exercises, and activities to make art using FFmpeg</p>
    </div>

    <div>
    <h2 id="basics">Learn about FFmpeg basics</h2>
      <!-- Basic structure of an FFmpeg command -->
      <p><label for="basic-structure">Basic structure of an FFmpeg command</label>
      </p><div>
        <h5>Basic structure of an FFmpeg command</h5>
        <p>At its basis, an FFmpeg command is relatively simple. After you have installed FFmpeg (see instructions <a href="https://avpres.net/FFmpeg/#ch1" target="_blank">here</a>), the program is invoked simply by typing <code>ffmpeg</code> at the command prompt.</p>
        <p>Subsequently, each instruction that you supply to FFmpeg is actually a pair: a flag, which designates the <em>type</em> of action you want to carry out; and then the specifics of that action. Flags are always prepended with a hyphen.</p>
        <p>For example, in the instruction <code>-i <em>input_file.ext</em></code>, the <code>-i</code> flag tells FFmpeg that you are supplying an input file, and <code>input_file.ext</code> states which file it is.</p>
        <p>Likewise, in the instruction <code>-c:v prores</code>, the flag <code>-c:v</code> tells FFmpeg that you want to encode the video stream, and <code>prores</code> specifies which codec is to be used. (<code>-c:v</code> is shorthand for <code>-codec:v</code>/<code>-codec:video</code>).</p>
        <p>A very basic FFmpeg command looks like this:</p>
        <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-i <em>input_file.ext</em></dt><dd>path and name of the input file</dd>
          <dt><em>-flag some_action</em></dt><dd>tell FFmpeg to do something, by supplying a valid flag and action</dd>
          <dt><em>output_file.ext</em></dt><dd>path and name of the output file.<br>
            Because this is the last part of the command, the filename you type here does not have a flag designating it as the output file.</dd>
        </dl>
        
      </div>
      <!-- End Basic structure of an FFmpeg command -->

      <!-- Streaming vs. Saving -->
      <p><label for="streaming-saving">Streaming vs. Saving</label>
      </p><div>
        <h5>Streaming vs. Saving</h5>
        <p>FFplay allows you to stream created video and FFmpeg allows you to save video.</p>
        <p>The following command creates and saves a 10-second video of SMPTE bars:</p>
        <p><code>ffmpeg -f lavfi -i smptebars=size=640x480 -t 5 output_file</code></p>
        <p>This command plays and streams SMPTE bars but does not save them on the computer:</p>
        <p><code>ffplay -f lavfi smptebars=size=640x480</code></p>
        <p>The main difference is small but significant: the <code>-i</code> flag is required for FFmpeg but not required for FFplay. Additionally, the FFmpeg script needs to have <code>-t 5</code> and <code>output.mkv</code> added to specify the length of time to record and the place to save the video.</p>
        
      </div>
      <!-- End Streaming vs. Saving -->
    </div>
    <div>

    <h2 id="concepts">Learn about more advanced FFmpeg concepts</h2>
    <!-- Loop usage explanation -->
    <p><label for="batch-loop">Batch and Loop script usage</label>
    </p><div>
      <h5>Batch and Loop script usage</h5>
      <p><code>ffmpeg -nostdin -i <em>input_file</em> ...</code></p>
      <p>One of the frequent uses of FFmpeg is to run batch commands within loops to, for example, generate access files for an entire collection at once.</p>
      <p>When running an FFmpeg command within a loop it is often necessary to use the <code>-nostdin</code> flag prior to the input in order to ensure successful execution of the commands. This is needed to override FFmpeg's default behavior of enabling interaction on standard input which can result in errors as loop inputs are fed to the ongoing command.</p>
      
    </div>
    <!-- End loop usage explanation -->

    <!-- Codec Defaults explanation -->
    <p><label for="codec-defaults">Codec defaults</label>
    </p><div>
      <h5>Codec Defaults</h5>
      <p>Unless specified, FFmpeg will automatically set codec choices and codec parameters based off of internal defaults. These defaults are applied based on the file type used in the output (for example <code>.mov</code> or <code>.wav</code>).</p>
      <p>When creating or transcoding files with FFmpeg, it is important to consider codec settings for both audio and video, as the default options may not be desirable in your particular context. The following is a brief list of codec defaults for some common file types:</p>
      <ul>
        <li><code>.avi</code>: Audio Codec: mp3, Video Codec: mpeg4</li>
        <li><code>.mkv</code>: Audio Codec: ac3, Video Codec: H.264</li>
        <li><code>.mov</code>: Audio Codec: AAC, Video Codec: H.264</li>
        <li><code>.mp4</code>: Audio Codec: AAC, Video Codec: H.264</li>
        <li><code>.mpg</code>: Audio Codec: mp2, Video Codec: mpeg1video</li>
        <li><code>.mxf</code>: Audio Codec: pcm_s16le, Video Codec: mpeg2video</li>
        <li><code>.wav</code>: Audio Codec: pcm_s16le (16 bit PCM)</li>
      </ul>
      
    </div>
    <!-- End Codec Defaults -->

    <!-- Filtergraph explanation -->
    <p><label for="filtergraphs">Filtergraphs</label>
    </p><div>
      <h5>Filtergraphs</h5>
      <p>Many FFmpeg commands use filters that manipulate the video or audio stream in some way: for example, <a href="https://ffmpeg.org/ffmpeg-filters.html#hflip" target="_blank">hflip</a> to horizontally flip a video, or <a href="https://ffmpeg.org/ffmpeg-filters.html#amerge-1" target="_blank">amerge</a> to merge two or more audio tracks into a single stream.</p>
      <p>The use of a filter is signaled by the flag <code>-vf</code> (video filter) or <code>-af</code> (audio filter), followed by the name and options of the filter itself. For example, take the <a href="#convert-colorspace">convert colorspace</a> command:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf colormatrix=<em>src</em>:<em>dst</em> <em>output_file</em></code>
      </p><p>Here, <a href="https://ffmpeg.org/ffmpeg-filters.html#colormatrix" target="_blank">colormatrix</a> is the filter used, with <em>src</em> and <em>dst</em> representing the source and destination colorspaces. This part following the <code>-vf</code> is a <strong>filtergraph</strong>.</p>
      <p>It is also possible to apply multiple filters to an input, which are sequenced together in the filtergraph. A chained set of filters is called a filter chain, and a filtergraph may include multiple filter chains. Filters in a filterchain are separated from each other by commas (<code>,</code>), and filterchains are separated from each other by semicolons (<code>;</code>). For example, take the <a href="#inverse-telecine">inverse telecine</a> command:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf "fieldmatch,yadif,decimate" <em>output_file</em></code></p>
      <p>Here we have a filtergraph including one filter chain, which is made up of three video filters.</p>
      <p>It is often prudent to enclose your filtergraph in quotation marks; this means that you can use spaces within the filtergraph. Using the inverse telecine example again, the following filter commands are all valid and equivalent:</p>
      <ul>
        <li><code>-vf fieldmatch,yadif,decimate</code></li>
        <li><code>-vf "fieldmatch,yadif,decimate"</code></li>
        <li><code>-vf "fieldmatch, yadif, decimate"</code></li>
      </ul>
      <p>but <code>-vf fieldmatch, yadif, decimate</code> is not valid.</p>
      <p>The ordering of the filters is significant. Video filters are applied in the order given, with the output of one filter being passed along as the input to the next filter in the chain. In the example above, <code>fieldmatch</code> reconstructs the original frames from the inverse telecined video, <code>yadif</code> deinterlaces (this is a failsafe in case any combed frames remain, for example if the source mixes telecined and real interlaced content), and <code>decimate</code> deletes duplicated frames. Clearly, it is not possible to delete duplicated frames before those frames are reconstructed.</p>
      <h4>Notes</h4>
      <ul>
        <li><code>-vf</code> is an alias for <code>-filter:v</code></li>
        <li>If the command involves more than one input or output, you must use the flag <code>-filter_complex</code> instead of <code>-vf</code>.</li>
        <li>Straight quotation marks ("like this") rather than curved quotation marks (“like this”) should be used.</li>
      </ul>
      <p>For more information, check out the FFmpeg wiki <a href="https://trac.ffmpeg.org/wiki/FilteringGuide" target="_blank">Filtering Guide</a>.</p>
      
    </div>
    <!-- End Filtergraph explanation -->

    <!-- Stream mapping explanation -->
    <p><label for="stream-mapping">Stream mapping</label>
    </p><div>
      <h5>Stream mapping</h5>
      <p>Stream mapping is the practice of defining which of the streams (e.g., video or audio tracks) present in an input file will be present in the output file. FFmpeg recognizes five stream types:</p>
      <ul>
        <li><code>a</code> - audio</li>
        <li><code>v</code> - video</li>
        <li><code>s</code> - subtitle</li>
        <li><code>d</code> - data (including timecode tracks)</li>
        <li><code>t</code> - attachment</li>
      </ul>
      <p>Mapping is achieved by use of the <code>-map</code> flag, followed by an action of the type <code>file_number:stream_type[:stream_number]</code>. Numbering is zero-indexed, and it's possible to map by stream type and/or overall stream order within the input file. For example:</p>
      <ul>
        <li><code>-map 0:v</code> means ‘take all video streams from the first input file’.</li>
        <li><code>-map 0:3</code> means ‘take the fourth stream from the first input file’.</li>
        <li><code>-map 0:a:2</code> means ‘take the third audio stream from the first input file’.</li>
        <li><code>-map 0:0 -map 0:2</code> means ‘take the first and third streams from the first input file’.</li>
        <li><code>-map 0:1 -map 1:0</code> means ‘take the second stream from the first input file and the first stream from the second input file’.</li>
      </ul>
      <p>When no mapping is specified in an ffmpeg command, the default for video files is to take just one video and one audio stream for the output: other stream types, such as timecode or subtitles, will not be copied to the output file by default. If multiple video or audio streams are present, the best quality one is automatically selected by FFmpeg.</p>
      <p>To map <em>all</em> streams in the input file to the output file, use <code>-map 0</code>. However, note that not all container formats can include all stream types: for example, .mp4 cannot contain timecode.</p>
      <h4>Mapping with a failsafe</h4>
      <p>To safely process files that may or may not contain given a type of stream, you can add a trailing <code>?</code> to your map commands: for example, <code>-map 0:a?</code> instead of <code>-map 0:a</code>.</p>
      <p>This makes the map optional: audio streams will be mapped over if they are present in the file—but if the file contains no audio streams, the transcode will proceed as usual, minus the audio stream mapping. Without adding the trailing <code>?</code>, FFmpeg will exit with an error on that file.</p>
      <p>This is especially recommended when batch processing video files: it ensures that all files in your batch will be transcoded, whether or not they contain audio streams.</p>
      <p>For more information, check out the FFmpeg wiki <a href="https://trac.ffmpeg.org/wiki/Map" target="_blank">Map</a> page, and the official FFmpeg <a href="https://ffmpeg.org/ffmpeg.html#Advanced-options" target="_blank">documentation on <code>-map</code></a>.</p>
      
    </div>
    <!-- End Stream Mapping explanation -->

    </div>
    <div>
    <h2 id="rewrap">Change container (rewrap)</h2>

    <!-- Basic rewrap command -->
    <p><label for="basic-rewrap">Basic rewrap command</label>
    </p><div>
      <h5>Rewrap a file</h5>
      <p><code>ffmpeg -i <em>input_file.ext</em> -c copy -map 0 <em>output_file.ext</em></code></p>
      <p>This script will rewrap a video file. It will create a new video video file where the inner content (the video, audio, and subtitle data) of the original file is unchanged, but these streams are rehoused within a different container format.</p>
      <p><strong>Note:</strong> rewrapping is also known as remuxing, short for re-multiplexing.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file.ext</em></dt><dd>path and name of the input file</dd>
        <dt>-c copy</dt><dd>copy the streams directly, without re-encoding.</dd>
        <dt>-map 0</dt><dd>map all streams of the input to the output.<br>
        By default, FFmpeg will only map one stream of each type (video, audio, subtitles) to the output file. However, files may have multiple streams of a given type - for example, a video may have several audio tracks for different languages. Therefore, if you want to preserve all the streams in the original, it's necessary to use this option.</dd>
        <dt><em>output_file.ext</em></dt><dd>path and name of the output file.<br>
        The new container you are rewrapping to is defined by the filename extension used here, e.g. .mkv, .mp4, .mov.</dd>
      </dl>
      <h4>Important caveat</h4>
      <p>It may not be possible to rewrap a file's contents to a new container without re-encoding one or more of the streams within (that is, the video, audio, and subtitle tracks). Some containers can only contain streams of a certain encoding type: for example, the .mp4 container does not support uncompressed audio tracks. (In practice .mp4 goes hand-in-hand with a H.264-encoded video stream and an AAC-encoded video stream, although other types of video and audio streams are possible). Another example is that the Matroska container does not allow data tracks; see the <a href="#mkv-to-mp4">MKV to MP4 recipe</a>.</p>
      <p>In such cases, FFmpeg will throw an error. If you encounter errors of this kind, you may wish to consult the <a href="#transcode">list of transcoding recipes</a>.</p>
      
    </div>
    <!-- End Basic rewrap command -->

    <!-- BWF -->
    <p><label for="bwf">Convert to (or create) Broadcast WAV</label>
    </p><div>
      <h5>Generate Broadcast WAV</h5>
      <p><code>ffmpeg -i <em>input_file.wav</em> -c copy -write_bext 1 -metadata field_name='Content' <em>output_file.wav</em></code></p>
      <p>This command will write a file in the Broadcast Wave Format (BWF) containing a BEXT chunk with related metadata.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file.wav</em></dt><dd>path and name of the input file</dd>
        <dt>-c copy</dt><dd>this will copy the encoding/sample rate etc from the input. If not using a WAV as the input file you will have to specify codec settings in place of this.</dd>
        <dt>-write_bext 1</dt><dd>tells FFmpeg to write a BEXT chunk, the part of the file where BWF metadata is stored.</dd>
        <dt>-metadata field_name='Content'</dt><dd>This is where you can specify which BEXT fields to write, and what information to fill them with by replacing <code>field_name</code> and <code>'Content'</code> respectively. See below for additional details.</dd>
      </dl>
      <p>Notes: You can choose which fields to write by repeating <code>-metadata field_name='Content'</code> for each desired field. Flags for commonly used fields (such as those recommended by the <a href="http://www.digitizationguidelines.gov/audio-visual/documents/Embed_Guideline_20120423.pdf">FADGI guidelines</a>) are as follows:</p>
      <ul>
        <li>description</li>
        <li>originator</li>
        <li>originator_reference</li>
        <li>origination_date</li>
        <li>origination_time</li>
        <li>coding_history</li>
        <li>IARL</li>
      </ul>
      <p>Example: <code>-metadata originator='US, UW Libraries'</code></p>
      <p>Additionally, users should be aware that BWF metadata fields are limited by characters, with some such as OriginatorReference maxing out at 32. Specific information can be found in the <a href="https://tech.ebu.ch/docs/tech/tech3285.pdf">Broadcast Wave Format specification</a>. Additional examples of BWF metadata usage can be found in the <a href="http://www.dlib.indiana.edu/projects/sounddirections/papersPresent/sd_bp_07.pdf">Sound Directions report</a> by Indiana University and Harvard.</p>
      
    </div>
    <!-- ends BWF -->

    <!-- Rewrap DV -->
    <p><label for="rewrap-dv">Rewrap DV video to .dv file</label>
    </p><div>
      <h5>Rewrap DV video to .dv file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -f rawvideo -c:v copy <em>output_file.dv</em></code></p>
      <p>This script will take a video that is encoded in the <a href="https://en.wikipedia.org/wiki/DV" target="_blank">DV Codec</a> but wrapped in a different container (such as MOV) and rewrap it into a raw DV file (with the .dv extension). Since DV files potentially contain a great deal of provenance metadata within the DV stream, it is necessary to rewrap files in this method to avoid unintentional stripping of this metadata.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path and name of the input file</dd>
        <dt>-f rawvideo</dt><dd>this tells FFmpeg to pass the video stream as raw video data without remuxing. This step is what ensures the survival of embedded metadata versus a standard rewrap.</dd>
        <dt>-c:v copy</dt><dd>copy the DV stream directly, without re-encoding.</dd>
        <dt><em>output_file.dv</em></dt><dd>tells FFmpeg to use the DV wrapper for the output.</dd>
      </dl>
      
    </div>
    <!-- Rewrap DV -->

    </div>
    <div>
    <h2 id="transcode">Change codec (transcode)</h2>

    <!-- Transcode to ProRes -->
    <p><label for="to_prores">Transcode to deinterlaced Apple ProRes LT</label>
    </p><div>
      <h5>Transcode into a deinterlaced Apple ProRes LT</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v prores -profile:v 1 -vf yadif -c:a pcm_s16le <em>output_file</em></code></p>
      <p>This command transcodes an input file into a deinterlaced Apple ProRes 422 LT file with 16-bit linear PCM encoded audio. The file is deinterlaced using the yadif filter (Yet Another De-Interlacing Filter).</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v prores</dt><dd>tells FFmpeg to transcode the video stream into Apple ProRes 422</dd>
        <dt>-profile:v <em>1</em></dt><dd>Declares profile of ProRes you want to use. The profiles are explained below:
        <ul>
          <li>0 = ProRes 422 (Proxy)</li>
          <li>1 = ProRes 422 (LT)</li>
          <li>2 = ProRes 422 (Standard)</li>
          <li>3 = ProRes 422 (HQ)</li>
        </ul></dd>
        <dt>-vf yadif</dt><dd>Runs a deinterlacing video filter (yet another deinterlacing filter) on the new file. <code>-vf</code> is an alias for <code>-filter:v</code>.</dd>
        <dt>-c:a pcm_s16le</dt><dd>tells FFmpeg to encode the audio stream in 16-bit linear PCM</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file<br>
        There are currently three possible containers for ProRes 422 and 4444 which are all supported by FFmpeg: QuickTime (<code>.mov</code>), Matroska (<code>.mkv</code>) and Material eXchange Format (<code>.mxf</code>).</dd>
      </dl>
      <p>FFmpeg comes with more than one ProRes encoder:</p>
      <ul>
        <li><code>prores</code> is much faster, can be used for progressive video only, and seems to be better for video according to Rec. 601 (Recommendation ITU-R BT.601).</li>
        <li><code>prores_ks</code> generates a better file, can also be used for interlaced video, allows also encoding of ProRes 4444 (<code>-c:v prores_ks -profile:v 4</code>) and ProRes 4444 XQ (<code>-c:v prores_ks -profile:v 5</code>), and seems to be better for video according to Rec. 709 (Recommendation ITU-R BT.709).</li>
      </ul>
      
    </div>
    <!-- ends Transcode to ProRes -->

    <!-- Transcode to H.264 -->
    <p><label for="transcode_h264">Transcode to an H.264 access file</label>
    </p><div>
      <h5>Transcode to H.264</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -pix_fmt yuv420p -c:a aac <em>output_file</em></code></p>
      <p>This command takes an input file and transcodes it to H.264 with an .mp4 wrapper, audio is transcoded to AAC. The libx264 codec defaults to a “medium” preset for compression quality and a CRF of 23. CRF stands for constant rate factor and determines the quality and file size of the resulting H.264 video. A low CRF means high quality and large file size; a high CRF means the opposite.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx264</dt><dd>tells FFmpeg to encode the video stream as H.264</dd>
        <dt>-pix_fmt yuv420p</dt><dd>libx264 will use a chroma subsampling scheme that is the closest match to that of the input. This can result in Y′C<sub>B</sub>C<sub>R</sub> 4:2:0, 4:2:2, or 4:4:4 chroma subsampling. QuickTime and most other non-FFmpeg based players can’t decode H.264 files that are not 4:2:0. In order to allow the video to play in all players, you can specify 4:2:0 chroma subsampling.</dd>
        <dt>-c:a aac</dt><dd>encode audio as AAC.<br>
          AAC is the codec most often used for audio streams within an .mp4 container.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>In order to optimize the file for streaming, you can add this preset:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -pix_fmt yuv420p -c:a aac -movflags +faststart <em>output_file</em></code></p>
      <dl>
        <dt>-movflags +faststart</dt><dd>This tells FFmpeg to move some of the essential metadata to the start of the file, which permits starting viewing before the file finishes downloading (an ideal characteristic for streaming).</dd>
      </dl>
      <p>In order to use the same basic command to make a higher quality file, you can add some of these presets:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -pix_fmt yuv420p -preset veryslow -crf 18 -c:a aac <em>output_file</em></code></p>
      <dl>
        <dt>-preset <em>veryslow</em></dt><dd>This option tells FFmpeg to use the slowest preset possible for the best compression quality.<br>
        Available presets, from slowest to fastest, are: <code>veryslow</code>, <code>slower</code>, <code>slow</code>, <code>medium</code>, <code>fast</code>, <code>faster</code>, <code>veryfast</code>, <code>superfast</code>, <code>ultrafast</code>.</dd>
        <dt>-crf <em>18</em></dt><dd>Specifying a lower CRF will make a larger file with better visual quality. For H.264 files being encoded with a 4:2:0 chroma subsampling scheme (i.e., using <code>-pix_fmt yuv420p</code>), the scale ranges between 0-51 for 8-bit content, with 0 being lossless and 51 the worst possible quality.<br>
          If no crf is specified, <code>libx264</code> will use a default value of 23. 18 is often considered a “visually lossless” compression.</dd>
      </dl>
      <p>By default, this recipe will include one track of each type (e.g. audio, video) in the output file. If you wish to include more tracks, consult the <a href="#stream-mapping">entry on stream mapping</a>.</p>
      <p>For more information, see the <a href="https://trac.ffmpeg.org/wiki/Encode/H.264" target="_blank">FFmpeg and H.264 Encoding Guide</a> on the FFmpeg wiki.</p>
      
    </div>
    <!-- ends Transcode to H.264 -->

    <!-- Transcode to H.264 or H.265 using the GPU -->
    <p><label for="transcode_gpu">Transcode to H.264/H.265 using the GPU</label>
    </p><div>
      <h5>Transcode to H.264/H.265 using the GPU</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v h264_nvenc -preset llhq -rc:v vbr_hq -cq:v 19 -b:v 8000k -maxrate:v 12000k -profile:v high -c:a copy <em>output_file</em></code></p>
      <p>This command takes an input file and transcodes it to H.264 using the encoding functionality of an Nvidia GPU (without transcoding the audio). If you're using H.264 with AAC or AC3 audio, you can output to an .mp4 file; if you're using HEVC and/or more exotic audio, you should output to .mkv. While Nvidia's fixed-function hardware can be 10x as performant as encoding on the CPU, it requires a few more parameters in order to optimize quality at lower bitrates.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v <em>h264_nvenc</em></dt><dd>tells FFmpeg to encode the video stream as H.264 using Nvidia's encoder.</dd>
        <dt>-preset <em>llhq</em></dt><dd>uses the "low latency, high quality" encoding preset, a good default when working with nvenc.</dd>
        <dt>-rc:v <em>vbr_hq</em></dt><dd>means "variable bitrate, high quality," allowing you to set a minimum and maximum bitrate for the encode.</dd>
        <dt>-cq:v <em>19</em></dt><dd>is the same as the CRF quality level specified using x264 or other CPU-based encoders, where 0 is lossless, 51 is the worst possible quality, and values from 18-23 are typical.</dd>
        <dt>-b:v <em>8000k -maxrate:v 12000k</em></dt><dd>corresponds to a minimum bitrate of 8 megabits (8000k) per second, and a maximum of 12 megabits per second. nvenc is not as good at estimating bitrates as CPU-based encoders, and without this data, will occasionally choose a visibly lower bitrate. The 8-12 mbit range is generally a good one for high-quality 1080p h264.</dd>
        <dt>-profile:v <em>high</em></dt><dd>uses the "high quality" profile of h264, something that's been baked in to the spec for a long time so that older players can declare compatibility; almost all h264 video now uses high.</dd>
        <dt>-c:a <em>copy</em></dt><dd>will skip reencoding the audio stream, and copy the audio from the source file.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>In order to encode to HEVC instead, and optionally transcode the audio, you can try changing the command like this:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v hevc_nvenc -preset llhq -rc:v vbr_hq -cq:v 19 -b:v 5000k -maxrate:v 8000k -profile:v main10 -c:a aac <em>output_file</em></code></p>
      <dl>
        <dt>-c:v <em>hevc_nvenc</em></dt><dd>encodes to HEVC (also called H.265), a more efficient codec supported on GPUs from approximately 2015 and newer.</dd>
        <dt>-b:v <em>5000k -maxrate:v 8000k</em></dt><dd>specifies a slightly lower bitrate than when using h264, per HEVC's greater efficiency.</dd>
        <dt>-profile:v <em>main10</em></dt><dd>declares the "main10" profile for working with HEVC; one of the primary advantages of this codec is better support for 10-bit video, enabling consumer HDR.</dd>
        <dt>-c:a <em>aac</em></dt><dd>reencodes the audio to AAC with default parameters, a very common and widely supported format for access copies.</dd>
      </dl>
      <p>Much of the information in this entry was taken from <a href="https://superuser.com/a/1236387" target="_blank">this superuser.com post</a> provided by an Nvidia developer, one of the best sources of information on the ffmpeg Nvidia encoders.</p>
      
    </div>
    <!-- ends Transcode to H.264 or H.265 using the GPU -->

    <!-- H.264 from DCP -->
    <p><label for="dcp_to_h264">Transcode from DCP to an H.264 access file</label>
    </p><div>
      <h5>H.264 from DCP</h5>
      <p><code>ffmpeg -i <em>input_video_file</em>.mxf -i <em>input_audio_file</em>.mxf -c:v libx264 -pix_fmt yuv420p -c:a aac <em>output_file.mp4</em></code></p>
      <p>This will transcode MXF wrapped video and audio files to an H.264 encoded MP4 file. Please note this only works for unencrypted, single reel DCPs.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_video_file</em></dt><dd>path and name of the video input file. This extension must be <code>.mxf</code></dd>
        <dt>-i <em>input_audio_file</em></dt><dd>path and name of the audio input file. This extension must be <code>.mxf</code></dd>
        <dt>-c:v libx264</dt><dd>transcodes video to H.264</dd>
        <dt>-pix_fmt yuv420p</dt><dd>sets pixel format to yuv420p for greater compatibility with media players</dd>
        <dt>-c:a aac</dt><dd>re-encodes using the AAC audio codec<br>
        Note that sadly MP4 cannot contain sound encoded by a PCM (Pulse-Code Modulation) audio codec</dd>
        <dt><em>output_file.mp4</em></dt><dd>path, name and .mp4 extension of the output file</dd>
      </dl>
      <p>Variation: Copy PCM audio streams by using Matroska instead of the MP4 container</p>
      <p><code>ffmpeg -i <em>input_video_file</em>.mxf -i <em>input_audio_file</em>.mxf -c:v libx264 -pix_fmt yuv420p -c:a copy <em>output_file.mkv</em></code></p>
      <dl>
        <dt>-c:a copy</dt><dd>re-encodes using the same audio codec</dd>
        <dt><em>output_file.mkv</em></dt><dd>path, name and .mkv extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends H.264 from DCP -->

    <!-- Transcode to FFV1.mkv -->
    <p><label for="create_FFV1_mkv">Transcode your file with the FFV1 Version 3 Codec in a Matroska container</label>
    </p><div>
      <h5>Create FFV1 Version 3 video in a Matroska container with framemd5 of input</h5>
      <p><code>ffmpeg -i <em>input_file</em> -map 0 -dn -c:v ffv1 -level 3 -g 1 -slicecrc 1 -slices 16 -c:a copy <em>output_file</em>.mkv -f framemd5 -an <em>framemd5_output_file</em></code></p>
      <p>This will losslessly transcode your video with the FFV1 Version 3 codec in a Matroska container. In order to verify losslessness, a framemd5 of the source video is also generated. For more information on FFV1 encoding, <a href="https://trac.ffmpeg.org/wiki/Encode/FFV1" target="_blank">try the FFmpeg wiki</a>.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command.</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file.</dd>
        <dt>-map 0</dt><dd>Map all streams that are present in the input file. This is important as FFmpeg will map only one stream of each type (video, audio, subtitles) by default to the output video.</dd>
        <dt>-dn</dt><dd>ignore data streams (data no). The Matroska container does not allow data tracks.</dd>
        <dt>-c:v ffv1</dt><dd>specifies the FFV1 video codec.</dd>
        <dt>-level 3</dt><dd>specifies Version 3 of the FFV1 codec.</dd>
        <dt>-g 1</dt><dd>specifies intra-frame encoding, or GOP=1.</dd>
        <dt>-slicecrc 1</dt><dd>Adds CRC information for each slice. This makes it possible for a decoder to detect errors in the bitstream, rather than blindly decoding a broken slice. (Read more <a href="http://ndsr.nycdigital.org/diving-in-head-first/" target="_blank">here</a>).</dd>
        <dt>-slices 16</dt><dd>Each frame is split into 16 slices. 16 is a good trade-off between filesize and encoding time.</dd>
        <dt>-c:a copy</dt><dd>copies all mapped audio streams.</dd>
        <dt><em>output_file</em>.mkv</dt><dd>path and name of the output file. Use the <code>.mkv</code> extension to save your file in a Matroska container.</dd>
        <dt>-f framemd5</dt><dd>Decodes video with the framemd5 muxer in order to generate MD5 checksums for every frame of your input file. This allows you to verify losslessness when compared against the framemd5s of the output file.</dd>
        <dt>-an</dt><dd>ignores the audio stream when creating framemd5 (audio no)</dd>
        <dt><em>framemd5_output_file</em></dt><dd>path, name and extension of the framemd5 file.</dd>
      </dl>
      
    </div>
    <!-- ends Transcode to FFV1.mkv-->

    <!-- Rip DVD -->
    <p><label for="dvd_to_file">Convert DVD to H.264</label>
    </p><div>
      <h5>Convert DVD to H.264</h5>
      <p><code>ffmpeg -i concat:<em>input_file_1</em>\|<em>input_file_2</em>\|<em>input_file_3</em> -c:v libx264 -c:a aac <em>output_file</em>.mp4</code></p>
      <p>This command allows you to create an H.264 file from a DVD source that is not copy-protected.</p>
      <p>Before encoding, you’ll need to establish which of the .VOB files on the DVD or .iso contain the content that you wish to encode. Inside the VIDEO_TS directory, you will see a series of files with names like VTS_01_0.VOB, VTS_01_1.VOB, etc. Some of the .VOB files will contain menus, special features, etc, so locate the ones that contain target content by playing them back in VLC.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i concat:<em>input files</em></dt><dd>lists the input VOB files and directs FFmpeg to concatenate them. Each input file should be separated by a backslash and a pipe, like so:<br>
        <code>-i concat:VTS_01_1.VOB\|VTS_01_2.VOB\|VTS_01_3.VOB</code><br>
        The backslash is simply an escape character for the pipe (<strong>|</strong>).</dd>
        <dt>-c:v libx264</dt><dd>sets the video codec as H.264</dd>
        <dt>-c:a aac</dt><dd>encode audio as AAC.<br>
          AAC is the codec most often used for audio streams within an .mp4 container.</dd>
        <dt><em>output_file.mp4</em></dt><dd>path and name of the output file</dd>
      </dl>
      <p>It’s also possible to adjust the quality of your output by setting the <strong>-crf</strong> and <strong>-preset</strong> values:</p>
      <p><code>ffmpeg -i concat:<em>input_file_1</em>\|<em>input_file_2</em>\|<em>input_file_3</em> -c:v libx264 -crf 18 -preset veryslow -c:a aac <em>output_file</em>.mp4</code></p>
      <dl>
        <dt>-crf 18</dt><dd>sets the constant rate factor to a visually lossless value. Libx264 defaults to a <a href="https://trac.ffmpeg.org/wiki/Encode/H.264#crf" target="_blank">crf of 23</a>, considered medium quality; a smaller CRF value produces a larger and higher quality video.</dd>
        <dt>-preset veryslow</dt><dd>A slower preset will result in better compression and therefore a higher-quality file. The default is <strong>medium</strong>; slower presets are <strong>slow</strong>, <strong>slower</strong>, and <strong>veryslow</strong>.</dd>
      </dl>
      <p>Bear in mind that by default, libx264 will only encode a single video stream and a single audio stream, picking the ‘best’ of the options available. To preserve all video and audio streams, add <strong>-map</strong> parameters:</p>
      <p><code>ffmpeg -i concat:<em>input_file_1</em>\|<em>input_file_2</em> -map 0:v -map 0:a -c:v libx264 -c:a aac <em>output_file</em>.mp4</code></p>
      <dl>
        <dt>-map 0:v</dt><dd>encodes all video streams</dd>
        <dt>-map 0:a</dt><dd>encodes all audio streams</dd>
      </dl>
      
    </div>
    <!-- ends rip DVD -->

    <!-- Transcode to H.265 -->
    <p><label for="transcode_h265">Transcode to an H.265/HEVC MP4</label>
    </p><div>
      <h5>Transcode to H.265/HEVC</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx265 -pix_fmt yuv420p -c:a copy <em>output_file</em></code></p>
      <p>This command takes an input file and transcodes it to H.265/HEVC in an .mp4 wrapper, keeping the audio codec the same as in the original file.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx265</dt><dd>tells FFmpeg to encode the video as H.265</dd>
        <dt>-pix_fmt yuv420p</dt><dd>libx265 will use a chroma subsampling scheme that is the closest match to that of the input. This can result in Y′C<sub>B</sub>C<sub>R</sub> 4:2:0, 4:2:2, or 4:4:4 chroma subsampling. For widest accessibility, it’s a good idea to specify 4:2:0 chroma subsampling.</dd>
        <dt>-c:a copy</dt><dd>tells FFmpeg not to change the audio codec</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>The libx265 encoding library defaults to a ‘medium’ preset for compression quality and a CRF of 28. CRF stands for ‘constant rate factor’ and determines the quality and file size of the resulting H.265 video. The CRF scale ranges from 0 (best quality [lossless]; largest file size) to 51 (worst quality; smallest file size).</p>
      <p>A CRF of 28 for H.265 can be considered a medium setting, <a href="https://trac.ffmpeg.org/wiki/Encode/H.265#ConstantRateFactorCRF" target="_blank">corresponding</a> to a CRF of 23 in <a href="#transcode_h264">encoding H.264</a>, but should result in about half the file size.</p>
      <p>To create a higher quality file, you can add these presets:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx265 -pix_fmt yuv420p -preset veryslow -crf 18 -c:a copy <em>output_file</em></code></p>
      <dl>
        <dt>-preset <em>veryslow</em></dt><dd>This option tells FFmpeg to use the slowest preset possible for the best compression quality.</dd>
        <dt>-crf <em>18</em></dt><dd>Specifying a lower CRF will make a larger file with better visual quality. 18 is often considered a ‘visually lossless’ compression.</dd>
      </dl>
      
    </div>
    <!-- ends Transcode to H.265 -->

    <!-- Transcode to Ogg/Theora -->
    <p><label for="transcode_ogg">Transcode to an Ogg Theora</label>
    </p><div>
      <h5>Transcode to Ogg/Theora</h5>
      <p><code>ffmpeg -i <em>input_file</em> -acodec libvorbis -b:v 690k <em>output_file</em></code></p>
      <p>This command takes an input file and transcodes it to Ogg/Theora in an .ogv wrapper with 690k video bitrate.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-acodec libvorbis</dt><dd>tells FFmpeg to encode the audio using libvorbis</dd>
        <dt>-b:v 690k</dt><dd>specifies the 690k video bitrate</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file (make sure to include the <code>.ogv</code> filename suffix)</dd>
      </dl>
      <p>This recipe is based on <a href="http://paulrouget.com/e/converttohtml5video" target="_blank">Paul Rouget's recipes</a>.</p>
      
    </div>
    <!-- ends Transcode to Ogg/Theora -->

    
    <!-- Here comes audio-only transcoding -->

    <!-- WAV to MP3 -->
    <p><label for="wav_to_mp3">Convert WAV to MP3</label>
    </p><div>
      <h5>WAV to MP3</h5>
      <p><code>ffmpeg -i <em>input_file</em>.wav -write_id3v1 1 -id3v2_version 3 -dither_method triangular -out_sample_rate 48k -qscale:a 1 <em>output_file</em>.mp3</code></p>
      <p>This will convert your WAV files to MP3s.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path and name of the input file</dd>
        <dt>-write_id3v1 1</dt><dd>This will write metadata to an ID3v1 tag at the head of the file, assuming you’ve embedded metadata into the WAV file.</dd>
        <dt>-id3v2_version 3</dt><dd>This will write metadata to an ID3v2.3 tag at the tail of the file, assuming you’ve embedded metadata into the WAV file.</dd>
        <dt>-dither_method triangular</dt><dd>Dither makes sure you don’t unnecessarily truncate the dynamic range of your audio.</dd>
        <dt>-out_sample_rate 48k</dt><dd>Sets the audio sampling frequency to 48 kHz. This can be omitted to use the same sampling frequency as the input.</dd>
        <dt>-qscale:a 1</dt><dd>This sets the encoder to use a constant quality with a variable bitrate of between 190-250kbit/s. If you would prefer to use a constant bitrate, this could be replaced with <code>-b:a 320k</code> to set to the maximum bitrate allowed by the MP3 format. For more detailed discussion on variable vs constant bitrates see <a href="https://trac.ffmpeg.org/wiki/Encode/MP3" target="_blank">here.</a></dd>
        <dt><em>output_file</em></dt><dd>path and name of the output file</dd>
      </dl>
      <p>A couple notes</p>
      <ul>
        <li>About ID3v2.3 tag: ID3v2.3 is better supported than ID3v2.4, FFmpeg's default ID3v2 setting.</li>
        <li>About dither methods: FFmpeg comes with a variety of dither algorithms, outlined in the <a href="https://ffmpeg.org/ffmpeg-resampler.html" target="_blank">official docs</a>, though some may lead to unintended, drastic digital clipping on some systems.</li>
      </ul>
      
    </div>
    <!-- ends WAV to MP3 -->

    <!-- append notice to access mp3 -->
    <p><label for="append_mp3">Generate two access MP3s (with and without copyright)</label>
    </p><div>
      <h5>Generate two access MP3s from input. One with appended audio (such as a copyright notice) and one unmodified.</h5>
      <p><code>ffmpeg -i <em>input_file</em> -i <em>input_file_to_append</em> -filter_complex "[0:a:0]asplit=2[a][b];[b]afifo[bb];[1:a:0][bb]concat=n=2:v=0:a=1[concatout]" -map "[a]" -codec:a libmp3lame -dither_method triangular -qscale:a 2 <em>output_file.mp3</em> -map "[concatout]" -codec:a libmp3lame -dither_method triangular -qscale:a 2 <em>output_file_appended.mp3</em></code></p>
      <p>This script allows you to generate two derivative audio files from a master while appending audio from a separate file (for example a copyright or institutional notice) to one of them.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file (the master file)</dd>
        <dt>-i <em>input_file_to_append</em></dt><dd>path, name and extension of the input file (the file to be appended to access file)</dd>
        <dt>-filter_complex</dt><dd>enables the complex filtering to manage splitting the input to two audio streams</dd>
        <dt>[0:a:0]asplit=2[a][b];</dt><dd><code>asplit</code> allows audio streams to be split up for separate manipulation. This command splits the audio from the first input (the master file) into two streams "a" and "b"</dd>
        <dt>[b]afifo[bb];</dt><dd>this buffers the stream "b" to help prevent dropped samples and renames stream to "bb"</dd>
        <dt>[1:a:0][bb]concat=n=2:v=0:a=1[concatout]</dt><dd><code>concat</code> is used to join files. <code>n=2</code> tells the filter there are two inputs. <code>v=0:a=1</code> Tells the filter there are 0 video outputs and 1 audio output. This command appends the audio from the second input to the beginning of stream "bb" and names the output "concatout"</dd>
        <dt>-map "[a]"</dt><dd>this maps the unmodified audio stream to the first output</dd>
        <dt>-codec:a libmp3lame -dither_method triangular -qscale:a 2</dt><dd>sets up MP3 options (using constant quality)</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file (unmodified)</dd>
        <dt>-map "[concatout]"</dt><dd>this maps the modified stream to the second output</dd>
        <dt>-codec:a libmp3lame -dither_method triangular -qscale:a 2</dt><dd>sets up MP3 options (using constant quality)</dd>
        <dt><em>output_file_appended</em></dt><dd>path, name and extension of the output file (with appended notice)</dd>
      </dl>
      
    </div>
    <!-- ends append notice to access mp3 -->

    <!-- WAV to AAC/MP4 -->
    <p><label for="wav_to_mp4">Convert WAV to AAC/MP4</label>
    </p><div>
      <h5>WAV to AAC/MP4</h5>
      <p><code>ffmpeg -i <em>input_file</em>.wav -c:a aac -b:a 128k -dither_method triangular -ar 44100 <em>output_file</em>.mp4</code></p>
      <p>This will convert your WAV file to AAC/MP4.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path and name of the input file</dd>
        <dt>-c:a aac</dt><dd>sets the audio codec to AAC</dd>
        <dt>-b:a 128k</dt><dd>sets the bitrate of the audio to 128k</dd>
        <dt>-dither_method triangular</dt><dd>Dither makes sure you don’t unnecessarily truncate the dynamic range of your audio.</dd>
        <dt>-ar 44100</dt><dd>sets the audio sampling frequency to 44100 Hz, or 44.1 kHz, or “CD quality”</dd>
        <dt><em>output_file</em></dt><dd>path and name of the output file</dd>
      </dl>
      <p>A note about dither methods. FFmpeg comes with a variety of dither algorithms, outlined in the <a href="https://ffmpeg.org/ffmpeg-resampler.html" target="_blank">official docs</a>, though some may lead to unintended, not-subtle digital clipping on some systems.</p>
      
    </div>
    <!-- ends WAV to AAC/MP4 -->

    </div>
    <div>
    <h2 id="video-properties">Change video properties</h2>

    <!-- 4:3 to 16:9 -->
    <p><label for="SD_HD">Transform 4:3 aspect ratio into 16:9 with pillarbox</label>
    </p><div>
      <h5>Transform 4:3 aspect ratio into 16:9 with pillarbox</h5>
      <p>Transform a video file with 4:3 aspect ratio into a video file with 16:9 aspect ratio by correct pillarboxing.</p>
      <p><code>ffmpeg -i <em>input_file</em> -filter:v "pad=ih*16/9:ih:(ow-iw)/2:(oh-ih)/2" -c:a copy <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter:v "pad=ih*16/9:ih:(ow-iw)/2:(oh-ih)/2"</dt><dd>video padding<br>This resolution independent formula is actually padding any aspect ratio into 16:9 by pillarboxing, because the video filter uses relative values for input width (iw), input height (ih), output width (ow) and output height (oh).</dd>
        <dt>-c:a copy</dt><dd>re-encodes using the same audio codec<br>
        For silent videos you can replace <code>-c:a copy</code> by <code>-an</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends 4:3 to 16:9 -->

    <!-- 16:9 to 4:3 -->
    <p><label for="HD_SD">Transform 16:9 aspect ratio video into 4:3 with letterbox</label>
    </p><div>
      <h5>Transform 16:9 aspect ratio video into 4:3 with letterbox</h5>
      <p>Transform a video file with 16:9 aspect ratio into a video file with 4:3 aspect ratio by correct letterboxing.</p>
      <p><code>ffmpeg -i <em>input_file</em> -filter:v "pad=iw:iw*3/4:(ow-iw)/2:(oh-ih)/2" -c:a copy <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter:v "pad=iw:iw*3/4:(ow-iw)/2:(oh-ih)/2"</dt><dd>video padding<br>
        This resolution independent formula is actually padding any aspect ratio into 4:3 by letterboxing, because the video filter uses relative values for input width (iw), input height (ih), output width (ow) and output height (oh).</dd>
        <dt>-c:a copy</dt><dd>re-encodes using the same audio codec<br>
        For silent videos you can replace <code>-c:a copy</code> by <code>-an</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends 16:9 to 4:3 -->

    <!-- Flip image -->
    <p><label for="flip_image">Flip video image</label>
    </p><div>
      <h5>Flip the video image horizontally and/or vertically</h5>
      <p><code>ffmpeg -i <em>input_file</em> -filter:v "hflip,vflip" -c:a copy <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter:v "hflip,vflip"</dt><dd>flips the image horizontally and vertically<br>By using only one of the parameters hflip or vflip for filtering the image is flipped on that axis only. The quote marks are not mandatory.</dd>
        <dt>-c:a copy</dt><dd>re-encodes using the same audio codec<br>
        For silent videos you can replace <code>-c:a copy</code> by <code>-an</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Flip image -->

    <!-- SD to HD -->
    <p><label for="SD_HD_2">Transform SD to HD with pillarbox</label>
    </p><div>
      <h5>Transform SD into HD with pillarbox</h5>
      <p>Transform a SD video file with 4:3 aspect ratio into an HD video file with 16:9 aspect ratio by correct pillarboxing.</p>
      <p><code>ffmpeg -i <em>input_file</em> -filter:v "colormatrix=bt601:bt709, scale=1440:1080:flags=lanczos, pad=1920:1080:240:0" -c:a copy <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter:v "colormatrix=bt601:bt709, scale=1440:1080:flags=lanczos, pad=1920:1080:240:0"</dt><dd>set colour matrix, video scaling and padding<br>Three filters are applied:
          <ol>
            <li>The luma coefficients are modified from SD video (according to Rec. 601) to HD video (according to Rec. 709) by a color matrix. Note that today Rec. 709 is often used also for SD and therefore you may cancel this parameter.</li>
            <li>The scaling filter (<code>scale=1440:1080</code>) works for both upscaling and downscaling. We use the Lanczos scaling algorithm (<code>flags=lanczos</code>), which is slower but gives better results than the default bilinear algorithm.</li>
            <li>The padding filter (<code>pad=1920:1080:240:0</code>) completes the transformation from SD to HD.</li>
          </ol></dd>
        <dt>-c:a copy</dt><dd>re-encodes using the same audio codec<br>
        For silent videos you can replace <code>-c:a copy</code> with <code>-an</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>If your source is interlaced, you will want to deinterlace prior to scaling. In that case, your command would look like this:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -filter:v "yadif, colormatrix=bt601:bt709, scale=1440:1080:flags=lanczos, pad=1920:1080:240:0" -c:a copy <em>output_file</em></code></p>
      <p>See the <a href="#ntsc_to_h264">Interlaced NTSC to MP4 recipe</a> for a fuller explanation of the deinterlacing step.</p>
      
    </div>
    <!-- ends SD to HD -->

    <!-- Change display aspect ratio without re-encoding video-->
    <p><label for="change_DAR">Change display aspect ratio without re-encoding</label>
    </p><div>
      <h5>Change Display Aspect Ratio without re-encoding video</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v copy -aspect 4:3 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v copy</dt><dd>Copy all mapped video streams.</dd>
        <dt>-aspect 4:3</dt><dd>Change Display Aspect Ratio to <code>4:3</code>. Experiment with other aspect ratios such as <code>16:9</code>. If used together with <code>-c:v copy</code>, it will affect the aspect ratio stored at container level, but not the aspect ratio stored in encoded frames, if it exists.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Change display aspect ratio without re-encoding video -->

    <!-- Convert colorspace -->
    <p><label for="convert-colorspace">Convert colorspace of video</label>
    </p><div>
      <h5>Transcode video to a different colorspace</h5>
      <p>This command uses a filter to convert the video to a different colorspace.</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf colormatrix=src:dst <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx264</dt><dd>tells FFmpeg to encode the video stream as H.264</dd>
        <dt>-vf colormatrix=<em>src</em>:<em>dst</em></dt><dd>the video filter <strong>colormatrix</strong> will be applied, with the given source and destination colorspaces.<br>
        Accepted values include <code>bt601</code> (Rec.601), <code>smpte170m</code> (Rec.601, 525-line/<a href="https://en.wikipedia.org/wiki/NTSC#NTSC-M" target="_blank">NTSC</a> version), <code>bt470bg</code> (Rec.601, 625-line/<a href="https://en.wikipedia.org/wiki/PAL#PAL-B.2FG.2FD.2FK.2FI" target="_blank">PAL</a> version), <code>bt709</code> (Rec.709), and <code>bt2020</code> (Rec.2020).<br>
        For example, to convert from Rec.601 to Rec.709, you would use <code>-vf colormatrix=bt601:bt709</code>.</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p><strong>Note:</strong> Converting between colorspaces with FFmpeg can be done via either the <strong>colormatrix</strong> or <strong>colorspace</strong> filters, with colorspace allowing finer control (individual setting of colorspace, transfer characteristics, primaries, range, pixel format, etc). See <a href="https://trac.ffmpeg.org/wiki/colorspace" target="_blank">this</a> entry on the FFmpeg wiki, and the FFmpeg documentation for <a href="https://ffmpeg.org/ffmpeg-filters.html#colormatrix" target="_blank">colormatrix</a> and <a href="https://ffmpeg.org/ffmpeg-filters.html#colorspace" target="_blank">colorspace</a>.</p>
      <hr>
      <h4>Convert colorspace and embed colorspace metadata</h4>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf colormatrix=src:dst -color_primaries <em>val</em> -color_trc <em>val</em> -colorspace <em>val</em> <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx264</dt><dd>encode video as H.264</dd>
        <dt>-vf colormatrix=<em>src</em>:<em>dst</em></dt><dd>the video filter <strong>colormatrix</strong> will be applied, with the given source and destination colorspaces.</dd>
        <dt>-color_primaries <em>val</em></dt><dd>tags video with the given color primaries.<br>
        Accepted values include <code>smpte170m</code> (Rec.601, 525-line/NTSC version), <code>bt470bg</code> (Rec.601, 625-line/PAL version), <code>bt709</code> (Rec.709), and <code>bt2020</code> (Rec.2020).
        </dd><dt>-color_trc <em>val</em></dt><dd>tags video with the given transfer characteristics (gamma).<br>
        Accepted values include <code>smpte170m</code> (Rec.601, 525-line/NTSC version), <code>gamma28</code> (Rec.601, 625-line/PAL version)<sup><a href="#fn1" id="ref1">1</a></sup>, <code>bt709</code> (Rec.709), <code>bt2020_10</code> (Rec.2020 10-bit), and <code>bt2020_12</code> (Rec.2020 12-bit).</dd>
        <dt>-colorspace <em>val</em></dt><dd>tags video as being in the given colourspace.<br>
        Accepted values include <code>smpte170m</code> (Rec.601, 525-line/NTSC version), <code>bt470bg</code> (Rec.601, 625-line/PAL version), <code>bt709</code> (Rec.709), <code>bt2020_cl</code> (Rec.2020 constant luminance), and <code>bt2020_ncl</code> (Rec.2020 non-constant luminance).</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <h4>Examples</h4>
      <p>To Rec.601 (525-line/NTSC):</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf colormatrix=bt709:smpte170m -color_primaries smpte170m -color_trc smpte170m -colorspace smpte170m <em>output_file</em></code></p>
      <p>To Rec.601 (625-line/PAL):</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf colormatrix=bt709:bt470bg -color_primaries bt470bg -color_trc gamma28 -colorspace bt470bg <em>output_file</em></code></p>
      <p>To Rec.709:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf colormatrix=bt601:bt709 -color_primaries bt709 -color_trc bt709 -colorspace bt709 <em>output_file</em></code></p>
      <p>MediaInfo output examples:</p>
      <p><img src="https://amiaopensource.github.io/ffmprovisr/img/colourspace_metadata_mediainfo.png" alt="MediaInfo screenshots of colorspace metadata"></p><p><span>⚠</span> Using this command it is possible to add Rec.709 tags to a file that is actually Rec.601 (etc), so apply with caution!</p>
      <p>These commands are relevant for H.264 and H.265 videos, encoded with <code>libx264</code> and <code>libx265</code> respectively.</p>
      <p><strong>Note:</strong> If you wish to embed colorspace metadata <em>without</em> changing to another colorspace, omit <code>-vf colormatrix=src:dst</code>. However, since it is <code>libx264</code>/<code>libx265</code> that writes the metadata, it’s not possible to add these tags without re-encoding the video stream.</p>
      <p>For all possible values for <code>-color_primaries</code>, <code>-color_trc</code>, and <code>-colorspace</code>, see the FFmpeg documentation on <a href="https://ffmpeg.org/ffmpeg-codecs.html#Codec-Options" target="_blank">codec options</a>.</p>
      <hr>
      <p id="fn1">1. Out of step with the regular pattern, <code>-color_trc</code> doesn’t accept <code>bt470bg</code>; it is instead here referred to directly as gamma.<br>
      In the Rec.601 standard, 525-line/NTSC and 625-line/PAL video have assumed gammas of 2.2 and 2.8 respectively. <a href="#ref1" title="Jump back.">↩</a></p>
      
    </div>
    <!-- ends Convert colorspace -->

    <!-- Modify speed -->
    <p><label for="modify_speed">Modify image and sound speed</label>
    </p><div>
      <h5>Modify image and sound speed</h5>
      <p>E.g. for converting 24fps to 25fps with audio pitch compensation for PAL access copies. (Thanks @kieranjol!)</p>
      <p><code>ffmpeg -i <em>input_file</em> -r <em>output_fps</em> -filter_complex "[0:v]setpts=<em>input_fps</em>/<em>output_fps</em>*PTS[v]; [0:a]atempo=<em>output_fps</em>/<em>input_fps</em>[a]" -map "[v]" -map "[a]" <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-r <em>output_fps</em></dt><dd>sets the frame rate of the <em>output_file</em></dd>
        <dt>-filter_complex "[0:v]setpts=<em>input_fps</em>/<em>output_fps</em>*PTS[v]; [0:a]atempo=<em>output_fps</em>/<em>input_fps</em>[a]"</dt><dd>A complex filter is needed here, in order to handle video stream and the audio stream separately. The <code>setpts</code> video filter modifies the PTS (presentation time stamp) of the video stream, and the <code>atempo</code> audio filter modifies the speed of the audio stream while keeping the same sound pitch. Note that the parameter order for the image and for the sound are inverted:
        <ul>
          <li>In the video filter <code>setpts</code> the numerator <code>input_fps</code> sets the input speed and the denominator <code>output_fps</code> sets the output speed; both values are given in frames per second.</li>
          <li>In the sound filter <code>atempo</code> the numerator <code>output_fps</code> sets the output speed and the denominator <code>input_fps</code> sets the input speed; both values are given in frames per second.</li>
        </ul>
        The different filters in a complex filter can be divided either by comma or semicolon. The quotation marks allow to insert a space between the filters for readability.</dd>
        <dt>-map "[v]"</dt><dd>maps the video stream and</dd>
        <dt>-map "[a]"</dt><dd>maps the audio stream together into:</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Modify speed -->

    <!-- Fade both video and audio streams -->
    <p><label for="fade_streams">Fade both video and audio streams</label>
    </p><div>
      <h5>Fade both video and audio streams</h5>
      <p><code>ffmpeg -i <em>input_file</em> -filter:v "fade=in:st=IN_POINT:d=DURATION, fade=out:st=OUT_POINT:d=DURATION" -filter:a "afade=in:st=OUT_POINT:d=DURATION, afade=out:st=IN_POINT:d=DURATION" -c:v libx264 -c:a aac <em>output_file</em></code></p>
      <p>This command fades your video in and out. Change IN_POINT, OUT_POINT, and DURATION to the time in seconds (expressed as integers).</p>
      <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
          <dt>-filter:v "fade=in:st=IN_POINT:d=DURATION, fade=out:st=OUT_POINT:d=DURATION"</dt><dd>applies a video filter that fades your video in and out. <code>st</code> sets the start and <code>d</code> sets the duration.</dd>
          <dt>-filter:a "afade=in:st=IN_POINT:d=DURATION, afade=out:st=OUT_POINT:d=DURATION"</dt><dd>applies an audio filter that fades your video in and out. <code>st</code> sets the start and <code>d</code> sets the duration.</dd>
          <dt>-c:v <em>video_codec</em></dt><dd>as a video filter is used, it is not possible to use <code>-c copy</code>. The video must be re-encoded with whatever video codec is chosen, e.g. <code>ffv1</code>, <code>v210</code> or <code>prores</code>.</dd>
          <dt>-c:a <em>audio_codec</em></dt><dd>as an audio filter is used, it is not possible to use <code>-c copy</code>. The audio must be re-encoded with whatever audio codec is chosen, e.g. <code>aac</code>.</dd>
          <dt><em>output_file</em></dt><dd>path, name and extension of the output_file</dd>
      </dl>
      
    </div>
    <!-- ends Fade both video and audio streams -->

    <!-- Synchronize video and audio streams -->
    <p><label for="sync_streams">Synchronize video and audio streams</label>
    </p><div>
      <h5>Synchronize video and audio streams</h5>
      <p><code>ffmpeg -i <em>input_file</em> -itsoffset 0.125 -i <em>input_file</em> -map 1:v -map 0:a -c copy <em>output_file</em></code></p>
      <p>A command to slip the video channel approximate 2 frames (0.125 for a 25fps timeline) to align video and audio drift, if generated during video tape capture for example.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-itsoffset 0.125</dt><dd>uses itsoffset command to set offset to 0.125 of a second. The offset time must be a time duration specification, see <a href="https://ffmpeg.org/ffmpeg-utils.html#time-duration-syntax" target="_blank">FFMPEG Utils Time Duration Syntax</a>.</dd>
        <dt>-i <em>input_file</em></dt><dd>repeat path, name and extension of the input file</dd>
        <dt>-map 1:v -map 0:a</dt><dd>selects the video channel for itsoffset command. To slip the audio channel reverse the selection to -map 0:v -map 1:a.</dd>
        <dt>-c copy</dt><dd>copies the encode settings of the input_file to the output_file</dd>
        <dt><em>output_file_resync</em></dt><dd>path, name and extension of the output_file</dd>
      </dl>
      
    </div>
    <!-- ends Synchronize video and audio streams -->

    <!-- Make stream properties explicate -->
    <p><label for="clarify_stream">Clarify stream properties</label>
    </p><div>
      <h5>Set stream properties</h5>
      <h2>Find undetermined or unknown stream properties</h2>
      <p>These examples use QuickTime inputs and outputs. The strategy will vary or may not be possible in other file formats. In the case of these examples it is the intention to make a lossless copy while clarifying an unknown characteristic of the stream.</p>
      <p><code>ffprobe <em>input_file</em> -show_streams</code></p>
      <dl>
        <dt>ffprobe</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-show_streams</dt><dd>Shows metadata of stream properties</dd>
      </dl>
      <p>Values that are set to 'unknown' and 'undetermined' may be unspecified within the stream. An unknown aspect ratio would be expressed as '0:1'. Streams with many unknown properties may have interoperability issues or not play as intended. In many cases, an unknown or undetermined value may be accurate because the information about the source is unclear, but often the value is intended to be known. In many cases the stream will played with an assumed value if undetermined (for instance a display_aspect_ratio of '0:1' may be played as 'WIDTH:HEIGHT'), but this may or may not be what is intended. Use carefully.</p>
      <h2>Set aspect ratio</h2>
      <p>If the display_aspect_ratio is set to '0:1' it may be clarified with the <em>-aspect</em> option and stream copy.</p>
      <p><code>ffmpeg -i <em>input_file</em> -c copy -map 0 -aspect DAR_NUM:DAR_DEN <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c copy</dt><dd>Using stream copy for all streams</dd>
        <dt>-map 0</dt><dd>tells FFmpeg to map all streams of the input to the output.</dd>
        <dt>-aspect DAR_NUM:DAR_DEN</dt><dd>Replace DAR_NUM with the display aspect ratio numerator and DAR_DEN with the display aspect ratio denominator, such as <em>-aspect 4:3</em> or <em>-aspect 16:9</em>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <h2>Adding other stream properties.</h2>
      <p>Other properties may be clarified in a similar way. Replace <em>-aspect</em> and its value with other properties such as shown in the options below. Note that setting color values in QuickTime requires that <em>-movflags write_colr</em> is set.</p>
      <dl>
        <dt>-color_primaries <em>VALUE</em> -movflags write_colr</dt><dd>Set a new color_primaries value.</dd>
        <dt>-color_trc <em>VALUE</em> -movflags write_colr</dt><dd>Set a new color_transfer value.</dd>
        <dt>-field_order <em>VALUE</em></dt><dd>Set interlacement values.</dd>
      </dl>
      <p>The possible values for <code>-color_primaries</code>, <code>-color_trc</code>, and <code>-field_order</code> are given in the <a href="https://ffmpeg.org/ffmpeg-all.html#toc-Codec-Options" target="_blank">Codec Options</a> section of the FFmpeg docs - scroll down to near the bottom of the section.</p>
      
    </div>
    <!-- ends Make stream properties explicate -->

    <!-- Crop video -->
    <p><label for="crop_video">Crop video</label>
    </p><div>
      <h5>Crop video</h5>
      <p><code>ffmpeg -i <em>input_file</em> -vf "crop=<em>width</em>:<em>height</em>" <em>output_file</em></code></p>
      <p>This command crops the input video to the dimensions defined</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-vf "<em>width</em>:<em>height</em>"</dt><dd>Crops the video to the given width and height (in pixels).<br>
          By default, the crop area is centered: that is, the position of the top left of the cropped area is set to x = (<em>input_width</em> - <em>output_width</em>) / 2, y = <em>input_height</em> - <em>output_height</em>) / 2.
        </dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>It's also possible to specify the crop position by adding the x and y coordinates representing the top left of your cropped area to your crop filter, as such:</p>
      <p><code>ffmpeg -i <em>input_file</em> -vf "crop=<em>width</em>:<em>height</em>[:<em>x_position</em>:<em>y_position</em>]" <em>output_file</em></code></p>
      <h5>Examples</h5>
      <p>The original frame, a screenshot of Maggie Cheung in the film <i>Hero</i>:</p>
      <p><img src="https://amiaopensource.github.io/ffmprovisr/img/crop_example_orig.png" alt="VLC screenshot of Maggie Cheung"></p><p>Result of the command <code>ffmpeg -i <em>maggie.mov</em> -vf "crop=500:500" <em>output_file</em></code>:</p>
      <p><img src="https://amiaopensource.github.io/ffmprovisr/img/crop_example_aftercrop1.png" alt="VLC screenshot of Maggie Cheung, cropped from original"></p><p>Result of the command <code>ffmpeg -i <em>maggie.mov</em> -vf "crop=500:500:0:0" <em>output_file</em></code>, appending <code>:0:0</code> to crop from the top left corner:</p>
      <p><img src="https://amiaopensource.github.io/ffmprovisr/img/crop_example_aftercrop2.png" alt="VLC screenshot of Maggie Cheung, cropped from original"></p><p>Result of the command <code>ffmpeg -i <em>maggie.mov</em> -vf "crop=500:300:500:30" <em>output_file</em></code>:</p>
      <p><img src="https://amiaopensource.github.io/ffmprovisr/img/crop_example_aftercrop3.png" alt="VLC screenshot of Maggie Cheung, cropped from original"></p>
    </div>
    <!-- ends Crop video -->

    <!-- Change video color to black and white -->
    <p><label for="col_change">Change video color to black and white</label>
    </p><div>
      <h5>Change video color to black and white</h5>
      <p><code>ffmpeg -i <em>input_file</em> -filter_complex hue=s=0 -c:a copy <em>output_file</em></code></p>
      <p>A basic command to alter color hue to black and white using filter_complex (credit @FFMPEG via Twitter).</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter_complex hue=s=0</dt><dd>uses filter_complex command to set the hue to black and white</dd>
        <dt>-c:a copy</dt><dd>copies the encode settings of the input_file to the output_file</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output_file</dd>
      </dl>
      <p>An alternative that preserves interlacing information for a ProRes 422 HQ file generated, for example, from a tape master (credit Dave Rice):</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v prores_ks -flags +ildct -map 0 -c:a copy -profile:v 3 -vf hue=s=0 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v prores_ks</dt><dd>encodes the video to ProRes (prores_ks marks the stream as interlaced, unlike prores)</dd>
        <dt>-flags +ildct</dt><dd>ensures that the output_file has interlaced field encoding, using interlace aware discrete cosine transform</dd>
        <dt>-map 0</dt><dd>ensures ffmpeg maps all streams of the input_file to the output_file</dd>
        <dt>-c:a copy</dt><dd>copies the encode settings of the input_file to the output_file</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Change video color to black and white -->

    </div>
    <div>
    <h2 id="audio-files">Change or view audio properties</h2>

    <!-- Extract audio from an AV file -->
    <p><label for="extract_audio">Extract audio without loss from an AV file</label>
    </p><div>
      <h5>Extract audio from an AV file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:a copy -vn <em>output_file</em></code></p>
      <p>This command extracts the audio stream without loss from an audiovisual file.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:a copy</dt><dd>re-encodes using the same audio codec</dd>
        <dt>-vn</dt><dd>no video stream</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Extract audio from am AV file -->

    <!-- Combine audio tracks  -->
    <p><label for="combine_audio">Combine audio tracks</label>
    </p><div>
      <h5>Combine audio tracks into one in a video file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -filter_complex "[0:a:0][0:a:1]amerge[out]" -map 0:v -map "[out]" -c:v copy -shortest <em>output_file</em></code></p>
      <p>This command combines two audio tracks present in a video file into one stream. It can be useful in situations where a downstream process, like YouTube’s automatic captioning, expect one audio track. To ensure that you’re mapping the right audio tracks run ffprobe before writing the script to identify which tracks are desired. More than two audio streams can be combined by extending the pattern present in the -filter_complex option.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter_complex</dt><dd>tells ffmpeg that we will be using a complex filter</dd>
        <dt>"</dt><dd>quotation mark to start filtergraph</dd>
        <dt>[0:a:0][0:a:1]amerge[out]</dt><dd>combines the two audio tracks into one</dd>
        <dt>"</dt><dd>quotation mark to end filtergraph</dd>
        <dt>-map 0:v</dt><dd>map the video</dd>
        <dt>-map "[out]"</dt><dd>map the combined audio defined by the filter</dd>
        <dt>-c:v copy</dt><dd>copy the video</dd>
        <dt>-shortest</dt><dd>limit to the shortest stream</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the video output file</dd>
      </dl>
      
    </div>
    <!-- ends Combine audio tracks -->

    <!-- phase shift -->
    <p><label for="phase_shift">Inverses the audio phase of the second channel</label>
    </p><div>
      <h5>Flip audio phase shift</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af pan="stereo|c0=c0|c1=-1*c1" <em>output_file</em></code></p>
      <p>This command inverses the audio phase of the second channel by rotating it 180°.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af</dt><dd>specifies that the next section should be interpreted as an audio filter</dd>
        <dt>pan=</dt><dd>tell the quoted text below to use the <a href="https://ffmpeg.org/ffmpeg-filters.html#pan-1" target="_blank">pan filter</a></dd>
        <dt>"stereo|c0=c0|c1=-1*c1"</dt><dd>maps the output's first channel (c0) to the input's first channel and the output's second channel (c1) to the inverse of the input's second channel</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends phase shift -->

    <!-- loudnorm metadata -->
    <p><label for="loudnorm_metadata">Calculate Loudness Levels</label>
    </p><div>
      <h5>Calculate Loudness Levels</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af loudnorm=print_format=json -f null -</code></p>
      <p>This filter calculates and outputs loudness information in json about an input file (labeled input) as well as what the levels would be if loudnorm were applied in its one pass mode (labeled output). The values generated can be used as inputs for a 'second pass' of the loudnorm filter allowing more accurate loudness normalization than if it is used in a single pass.</p>
      <p>These instructions use the loudnorm defaults, which align well with PBS recommendations for target loudness. More information can be found at the <a href="https://ffmpeg.org/ffmpeg-filters.html#loudnorm" target="_blank">loudnorm documentation</a>.</p>
      <p>Information about PBS loudness standards can be found in the <a href="http://bento.cdn.pbs.org/hostedbento-prod/filer_public/PBS_About/Producing/Red%20Book/TOS%20Pt%201%20Submission%202016.pdf" target="_blank">PBS Technical Operating Specifications</a> document. Information about EBU loudness standards can be found in the <a href="https://tech.ebu.ch/docs/r/r128-2014.pdf" target="_blank">EBU R 128</a> recommendation document.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af loudnorm</dt><dd>activates the loudnorm filter</dd>
        <dt>print_format=json</dt><dd>sets the output format for loudness information to json. This format makes it easy to use in a second pass. For a more human readable output, this can be set to <code>print_format=summary</code></dd>
        <dt><em>-f null -</em></dt><dd>sets the file output to null (since we are only interested in the metadata generated)</dd>
      </dl>
      
    </div>
    <!-- ends loudnorm metadata -->

    <!-- RIAA equalization -->
    <p><label for="riaa_eq">RIAA Equalization</label>
    </p><div>
      <h5>RIAA Equalization</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af aemphasis=type=riaa <em>output_file</em></code></p>
      <p>This will apply RIAA equalization to an input file allowing correct listening of audio transferred 'flat' (without EQ) from records that used this EQ curve. For more information about RIAA equalization see the <a href="https://en.wikipedia.org/wiki/RIAA_equalization" target="_blank">Wikipedia page</a> on the subject.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af aemphasis=type=riaa</dt><dd>activates the aemphasis filter and sets it to use RIAA equalization</dd>
        <dt><em>output_file</em></dt><dd>path and name of output file</dd>
      </dl>
      
    </div>
    <!-- ends RIAA equalization -->

    <!-- CD De-emphasis -->
    <p><label for="cd_eq">Reverse CD Pre-Emphasis</label>
    </p><div>
      <h5>Reverse CD Pre-Emphasis</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af aemphasis=type=cd <em>output_file</em></code></p>
      <p>This will apply de-emphasis to reverse the effects of CD pre-emphasis in the somewhat rare case of CDs that were created with this technology. Use this command to create more accurate listening copies of files that were ripped 'flat' (without any de-emphasis) where the original source utilized emphasis. For more information about CD pre-emphasis see the <a href="https://wiki.hydrogenaud.io/index.php?title=Pre-emphasis" target="_blank">Hydrogen Audio page</a> on this subject.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af aemphasis=type=cd</dt><dd>activates the aemphasis filter and sets it to use CD equalization</dd>
        <dt><em>output_file</em></dt><dd>path and name of output file</dd>
      </dl>
      
    </div>
    <!-- CD De-emphasis -->

    <!-- one pass loudnorm -->
    <p><label for="loudnorm_one_pass">One Pass Loudness Normalization</label>
    </p><div>
      <h5>One Pass Loudness Normalization</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af loudnorm=dual_mono=true -ar 48k <em>output_file</em></code></p>
      <p>This will normalize the loudness of an input using one pass, which is quicker but less accurate than using two passes. This command uses the loudnorm filter defaults for target loudness. These defaults align well with PBS recommendations, but loudnorm does allow targeting of specific loudness levels. More information can be found at the <a href="https://ffmpeg.org/ffmpeg-filters.html#loudnorm" target="_blank">loudnorm documentation</a>.</p>
      <p>Information about PBS loudness standards can be found in the <a href="https://www-tc.pbs.org/capt/Producing/TOS-2012-Pt2-Distribution.pdf" target="_blank">PBS Technical Operating Specifications</a> document. Information about EBU loudness standards can be found in the <a href="https://tech.ebu.ch/docs/r/r128-2014.pdf" target="_blank">EBU R 128</a> recommendation document.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af loudnorm</dt><dd>activates the loudnorm filter with default settings</dd>
        <dt>dual_mono=true</dt><dd>(optional) Use this for mono files meant to be played back on stereo systems for correct loudness. Not necessary for multi-track inputs.</dd>
        <dt>-ar 48k</dt><dd>Sets the output sample rate to 48 kHz. (The loudnorm filter upsamples to 192 kHz so it is best to manually set a desired output sample rate).</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension for output file</dd>
      </dl>
      
    </div>
    <!-- ends one pass loudnorm -->

    <!-- two pass loudnorm -->
    <p><label for="loudnorm_two_pass">Two Pass Loudness Normalization</label>
    </p><div>
      <h5>Two Pass Loudness Normalization</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af loudnorm=dual_mono=true:measured_I=<em>input_i</em>:measured_TP=<em>input_tp</em>:measured_LRA=<em>input_lra</em>:measured_thresh=<em>input_thresh</em>:offset=<em>target_offset</em>:linear=true -ar 48k <em>output_file</em></code></p>
      <p>This command allows using the levels calculated using a <a href="#loudnorm_metadata">first pass of the loudnorm filter</a> to more accurately normalize loudness. This command uses the loudnorm filter defaults for target loudness. These defaults align well with PBS recommendations, but loudnorm does allow targeting of specific loudness levels. More information can be found at the <a href="https://ffmpeg.org/ffmpeg-filters.html#loudnorm" target="_blank">loudnorm documentation</a>.</p>
      <p>Information about PBS loudness standards can be found in the <a href="https://www-tc.pbs.org/capt/Producing/TOS-2012-Pt2-Distribution.pdf" target="_blank">PBS Technical Operating Specifications</a> document. Information about EBU loudness standards can be found in the <a href="https://tech.ebu.ch/docs/r/r128-2014.pdf" target="_blank">EBU R 128</a> recommendation document.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af loudnorm</dt><dd>activates the loudnorm filter with default settings</dd>
        <dt>dual_mono=true</dt><dd>(optional) use this for mono files meant to be played back on stereo systems for correct loudness. Not necessary for multi-track inputs.</dd>
        <dt>measured_I=<em>input_i</em></dt><dd>use the 'input_i' value (integrated loudness) from the first pass in place of input_i</dd>
        <dt>measured_TP=<em>input_tp</em></dt><dd>use the 'input_tp' value (true peak) from the first pass in place of input_tp</dd>
        <dt>measured_LRA=<em>input_lra</em></dt><dd>use the 'input_lra' value (loudness range) from the first pass in place of input_lra</dd>
        <dt>measured_LRA=<em>input_thresh</em></dt><dd>use the 'input_thresh' value (threshold) from the first pass in place of input_thresh</dd>
        <dt>offset=<em>target_offset</em></dt><dd>use the 'target_offset' value (offset) from the first pass in place of target_offset</dd>
        <dt>linear=true</dt><dd>tells loudnorm to use linear normalization</dd>
        <dt>-ar 48k</dt><dd>Sets the output sample rate to 48 kHz. (The loudnorm filter upsamples to 192 kHz so it is best to manually set a desired output sample rate).</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension for output file</dd>
      </dl>
      
    </div>
    <!-- ends two pass loudnorm -->

    <!-- Fix A/V async 1 -->
    <p><label for="avsync_aresample">Fix A/V sync issues by resampling audio</label>
    </p><div>
      <h5>Fix AV Sync: Resample audio</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v copy -c:a pcm_s16le -af "aresample=async=1000" <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v copy</dt><dd>Copy all mapped video streams.</dd>
        <dt>-c:a pcm_s16le</dt><dd>tells FFmpeg to encode the audio stream in 16-bit linear PCM (<a href="https://en.wikipedia.org/wiki/Endianness#Little-endian" target="_blank">little endian</a>)</dd>
        <dt>-af "aresample=async=1000"</dt><dd>Uses the <a href="https://ffmpeg.org/ffmpeg-filters.html#aresample-1" target="_blank">aresample</a> filter to stretch/squeeze samples to given timestamps, with a maximum of 1000 samples per second compensation.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file. Try different file extensions such as mkv, mov, mp4, or avi.</dd>
      </dl>
      
    </div>
    <!-- ends Fix A/V async 1 -->

    </div>
    <div>
    <h2 id="join-trim">Join, trim, or create an excerpt</h2>

    <!-- Join files of the same type together -->
    <p><label for="join_files">Join (concatenate) two or more files of the same type</label>
    </p><div>
      <h5>Join files together</h5>
      <p><code>ffmpeg -f concat -i mylist.txt -c copy <em>output_file</em></code></p>
      <p>This command takes two or more files of the same file type and joins them together to make a single file. All that the program needs is a text file with a list specifying the files that should be joined. If possible, run the command from the same directory where the files and the text file reside. Otherwise you'll have to use <code>-safe 0</code>, see below for more information. However, it only works properly if the files to be combined have the exact same codec and technical specifications. Be careful, FFmpeg may appear to have successfully joined two video files with different codecs, but may only bring over the audio from the second file or have other weird behaviors. Don’t use this command for joining files with different codecs and technical specs and always preview your resulting video file!</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f concat</dt><dd>forces ffmpeg to concatenate the files and to keep the same file format</dd>
        <dt>-i <em>mylist.txt</em></dt><dd>path, name and extension of the input file. Per the <a href="https://ffmpeg.org/ffmpeg-formats.html#Options" target="_blank">FFmpeg documentation</a>, it is preferable to specify relative rather than absolute file paths, as allowing absolute file paths may pose a security risk.<br>
        This text file contains the list of files (without their absolute path) to be concatenated and should be formatted as follows:
<pre>  file '<em>first_file.ext</em>'
  file '<em>second_file.ext</em>'
  . . .
  file '<em>last_file.ext</em>'
</pre>
  In the above, <strong>file</strong> is simply the word "file". Straight apostrophes ('like this') rather than curved quotation marks (‘like this’) must be used to enclose the file paths.<br>
  <strong>Note:</strong> If specifying absolute file paths in the .txt file, add <code>-safe 0</code> before the input file.<br>
  e.g.: <code>ffmpeg -f concat -safe 0 -i mylist.txt -c copy <em>output_file</em></code></dd>
        <dt>-c copy</dt><dd>use stream copy mode to re-mux instead of re-encode</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>For more information, see the <a href="https://trac.ffmpeg.org/wiki/Concatenate" target="_blank">FFmpeg wiki page on concatenating files</a>.</p>
      
    </div>
    <!-- ends Join files of the same type together -->

    <!-- Join files of different types together -->
    <p><label for="join_different_files">Join (concatenate) two or more files of different types</label>
     </p><div>
      <h5>Join files together</h5>
      <p><code>ffmpeg -i input_1.avi -i input_2.mp4 -filter_complex "[0:v:0][0:a:0][1:v:0][1:a:0]concat=n=2:v=1:a=1[video_out][audio_out]" -map "[video_out]" -map "[audio_out]" <em>output_file</em></code></p>
      <p>This command takes two or more files of the different file types and joins them together to make a single file.</p>
      <p>The input files may differ in many respects - container, codec, chroma subsampling scheme, framerate, etc. However, the above command only works properly if the files to be combined have the same dimensions (e.g., 720x576). Also note that if the input files have different framerates, then the output file will be of variable framerate.</p>
      <p>Some aspects of the input files will be normalized: for example, if an input file contains a video track and an audio track that do not have exactly the same duration, the shorter one will be padded. In the case of a shorter video track, the last frame will be repeated in order to cover the missing video; in the case of a shorter audio track, the audio stream will be padded with silence.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_1.ext</em></dt><dd>path, name and extension of the first input file</dd>
        <dt>-i <em>input_2.ext</em></dt><dd>path, name and extension of the second input file</dd>
        <dt>-filter_complex</dt><dd>states that a complex filtergraph will be used</dd>
        <dt>"</dt><dd>quotation mark to start filtergraph</dd>
        <dt>[0:v:0][0:a:0]</dt><dd>selects the first video stream and first audio stream from the first input.<br>
          Each reference to a specific stream is enclosed in square brackets. In the first stream reference, <code>0:v:0</code>, the first zero refers to the first input file, <code>v</code> means video stream, and the second zero indicates that it is the <em>first</em> video stream in the file that should be selected. Likewise, <code>0:a:0</code> means the first audio stream in the first input file.<br>
          As demonstrated above, ffmpeg uses zero-indexing: <code>0</code> means the first input/stream/etc, <code>1</code> means the second input/stream/etc, and <code>4</code> would mean the fifth input/stream/etc.</dd>
        <dt>[1:v:0][1:a:0]</dt><dd>As described above, this means select the first video and audio streams from the second input file.</dd>
        <dt>concat=</dt><dd>starts the <code>concat</code> filter</dd>
        <dt>n=2</dt><dd>states that there are two input files</dd>
        <dt>:</dt><dd>separator</dd>
        <dt>v=1</dt><dd>sets the number of output video streams.<br>
          Note that this must be equal to the number of video streams selected from each segment.</dd>
        <dt>:</dt><dd>separator</dd>
        <dt>a=1</dt><dd>sets the number of output audio streams.<br>
          Note that this must be equal to the number of audio streams selected from each segment.</dd>
        <dt>[video_out]</dt><dd>name of the concatenated output video stream. This is a variable name which you define, so you could call it something different, like “vOut”, “outv”, or “banana”.</dd>
        <dt>[audio_out]</dt><dd>name of the concatenated output audio stream. Again, this is a variable name which you define.</dd>
        <dt>"</dt><dd>quotation mark to end filtergraph</dd>
        <dt>-map "[video_out]"</dt><dd>map the concatenated video stream into the output file by referencing the variable defined above</dd>
        <dt>-map "[audio_out]"</dt><dd>map the concatenated audio stream into the output file by referencing the variable defined above</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>If no characteristics of the output files are specified, ffmpeg will use the default encodings associated with the given output file type. To specify the characteristics of the output stream(s), add flags after each <code>-map "[out]"</code> part of the command.</p>
      <p>For example, to ensure that the video stream of the output file is visually lossless H.264 with a 4:2:0 chroma subsampling scheme, the command above could be amended to include the following:<br>
        <code>-map "[video_out]" -c:v libx264 -pix_fmt yuv420p -preset veryslow -crf 18</code></p>
      <p>Likewise, to encode the output audio stream as mp3, the command could include the following:<br>
        <code>-map "[audio_out]" -c:a libmp3lame -dither_method triangular -qscale:a 2</code></p>
      <h4>Variation: concatenating files of different resolutions</h4>
      <p>To concatenate files of different resolutions, you need to resize the videos to have matching resolutions prior to concatenation. The most basic way to do this is by using a scale filter and giving the dimensions of the file you wish to match:</p>
      <p><code>-vf scale=1920:1080:flags=lanczos</code></p>
      <p>(The Lanczos scaling algorithm is recommended, as it is slower but better than the default bilinear algorithm).</p>
      <p>The rescaling should be applied just before the point where the streams to be used in the output file are listed. Select the stream you want to rescale, apply the filter, and assign that to a variable name (<code>rescaled_video</code> in the below example). Then you use this variable name in the list of streams to be concatenated.</p>
      <p><code>ffmpeg -i input_1.avi -i input_2.mp4 -filter_complex "[0:v:0] scale=1920:1080:flags=lanczos [rescaled_video], [rescaled_video] [0:a:0] [1:v:0] [1:a:0] concat=n=2:v=1:a=1 [video_out] [audio_out]" -map "[video_out]" -map "[audio_out]" <em>output_file</em></code></p>
      <p>However, this will only have the desired visual output if the inputs have the same aspect ratio. If you wish to concatenate an SD and an HD file, you will also wish to pillarbox the SD file while upscaling. (See the <a href="#SD_HD_2">Convert 4:3 to pillarboxed HD</a> command). The full command would look like this:</p>
      <p><code>ffmpeg -i input_1.avi -i input_2.mp4 -filter_complex "[0:v:0] scale=1440:1080:flags=lanczos, pad=1920:1080:(ow-iw)/2:(oh-ih)/2 [to_hd_video], [to_hd_video] [0:a:0] [1:v:0] [1:a:0] concat=n=2:v=1:a=1 [video_out] [audio_out]" -map "[video_out]" -map "[audio_out]" <em>output_file</em></code></p>
      <p>Here, the first input is an SD file which needs to be upscaled to match the second input, which is 1920x1080. The scale filter enlarges the SD input to the height of the HD frame, keeping the 4:3 aspect ratio; then, the video is pillarboxed within a 1920x1080 frame.</p>
      <h4>Variation: concatenating files of different framerates</h4>
      <p>If the input files have different framerates, then the output file may be of variable framerate. To explicitly obtain an output file of constant framerate, you may wish convert an input (or multiple inputs) to a different framerate prior to concatenation.</p>
      <p>You can speed up or slow down a file using the <code>fps</code> and <code>atempo</code> filters (see also the <a href="#modify_speed">Modify speed</a> command).</p>
      <p>Here's an example of the full command, in which input_1 is 30fps, input_2 is 25fps, and 25fps is the desired output speed.</p>
      <p><code>ffmpeg -i input_1.avi -i input_2.mp4 -filter_complex "[0:v:0] fps=fps=25 [video_to_25fps]; [0:a:0] atempo=(25/30) [audio_to_25fps]; [video_to_25fps] [audio_to_25fps] [1:v:0] [1:a:0] concat=n=2:v=1:a=1 [video_out] [audio_out]" -map "[video_out]" -map "[audio_out]" <em>output_file</em></code></p>
      <p>Note that the <code>fps</code> filter will drop or repeat frames as necessary in order to achieve the desired frame rate - see the FFmpeg <a href="https://ffmpeg.org/ffmpeg-filters.html#fps-1" target="_blank">fps docs</a> for more details.</p>
      <p>For more information, see the <a href="https://trac.ffmpeg.org/wiki/Concatenate#differentcodec" target="_blank">FFmpeg wiki page on concatenating files of different types</a>.</p>
      
    </div>
    <!-- ends Join files of the different types together -->

    <!-- Split file into segments -->
    <p><label for="segment_file">Split one file into several smaller segments</label>
    </p><div>
      <h5>Split file into segments</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c copy -map 0 -f segment -segment_time 60 -reset_timestamps 1 <em>output_file-%03d.mkv</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>Starts the command.</dd>
        <dt>-i <em>input_file</em></dt><dd>Takes in a normal file.</dd>
        <dt>-c copy</dt><dd>Use stream copy mode to re-mux instead of re-encode.</dd>
        <dt>-map 0</dt><dd>tells FFmpeg to map all streams of the input to the output.</dd>
        <dt>-f segment</dt><dd>Use <a href="https://ffmpeg.org/ffmpeg-formats.html#toc-segment_002c-stream_005fsegment_002c-ssegment" target="_blank">segment muxer</a> for generating the output.</dd>
        <dt>-segment_time 60</dt><dd>Set duration of each segment (in seconds). This example creates segments with max. duration of 60s each.</dd>
        <dt>-reset_timestamps 1</dt><dd>Reset timestamps of each segment to 0. Meant to ease the playback of the generated segments.</dd>
        <dt><em>output_file-%03d.mkv</em></dt>
        <dd>
        <p>Path, name and extension of the output file.<br>
          In order to have an incrementing number in each segment filename, FFmpeg supports <a href="http://www.cplusplus.com/reference/cstdio/printf/" target="_blank">printf-style</a> syntax for a counter.</p>
          <p>In this example, '%03d' means: 3-digits, zero-padded<br>
            Examples:</p>
          <ul>
            <li><code>%03d</code>: 000, 001, 002, ... 999</li>
            <li><code>%05d</code>: 00000, 00001, 00002, ... 99999</li>
            <li><code>%d</code>: 0, 1, 2, 3, 4, ... 23, 24, etc. </li>
          </ul>
        </dd>
      </dl>
      
    </div>
    <!-- ends Split file into segments -->

    <!-- Trim -->
    <p><label for="trim">Trim file</label>
    </p><div>
      <h5>Trim a video without re-encoding</h5>
      <p><code>ffmpeg -i <em>input_file</em> -ss 00:02:00 -to 00:55:00 -c copy -map 0 <em>output_file</em></code></p>
      <p>This command allows you to create an excerpt from a file without re-encoding the audiovisual data.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-ss 00:02:00</dt><dd>sets in point at 00:02:00</dd>
        <dt>-to 00:55:00</dt><dd>sets out point at 00:55:00</dd>
        <dt>-c copy</dt><dd>use stream copy mode (no re-encoding)<br>
        </dd><dt>-map 0</dt><dd>tells FFmpeg to map all streams of the input to the output.<br>
        <strong>Note:</strong> watch out when using <code>-ss</code> with <code>-c copy</code> if the source is encoded with an interframe codec (e.g., H.264). Since FFmpeg must split on i-frames, it will seek to the nearest i-frame to begin the stream copy.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>Variation: trim file by setting duration, by using <code>-t</code> instead of <code>-to</code></p>
      <p><code>ffmpeg -i <em>input_file</em> -ss 00:05:00 -t 10 -c copy <em>output_file</em></code></p>
      <dl>
        <dt>-ss 00:05:00 -t 10</dt><dd>Beginning five minutes into the original video, this command will create a 10-second-long excerpt.</dd>
      </dl>
      <p>Note: In order to keep the original timestamps, without trying to sanitize them, you may add the <code>-copyts</code> option.</p>
      
    </div>
    <!-- ends Trim -->

    <!-- Excerpt from beginning -->
    <p><label for="excerpt_from_start">Create an excerpt, starting from the beginning of the file</label>
    </p><div>
      <h5>Excerpt from beginning</h5>
      <p><code>ffmpeg -i <em>input_file</em> -t <em>5</em> -c copy -map 0 <em>output_file</em></code></p>
      <p>This command captures a certain portion of a file, starting from the beginning and continuing for the amount of time (in seconds) specified in the script. This can be used to create a preview file, or to remove unwanted content from the end of the file. To be more specific, use timecode, such as 00:00:05.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-t <em>5</em></dt><dd>tells FFmpeg to stop copying from the input file after a certain time, and specifies the number of seconds after which to stop copying. In this case, 5 seconds is specified.</dd>
        <dt>-c copy</dt><dd>use stream copy mode to re-mux instead of re-encode</dd>
        <dt>-map 0</dt><dd>tells FFmpeg to map all streams of the input to the output.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Excerpt from beginning -->

    <!-- Excerpt to end -->
    <p><label for="excerpt_to_end">Create a new file with the first five seconds trimmed off the original</label>
    </p><div>
      <h5>Excerpt to end</h5>
      <p><code>ffmpeg -i <em>input_file</em> -ss <em>5</em> -c copy -map 0 <em>output_file</em></code></p>
      <p>This command copies a file starting from a specified time, removing the first few seconds from the output. This can be used to create an excerpt, or remove unwanted content from the beginning of a file.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-ss <em>5</em></dt><dd>tells FFmpeg what timecode in the file to look for to start copying, and specifies the number of seconds into the video that FFmpeg should start copying. To be more specific, you can use timecode such as 00:00:05.</dd>
        <dt>-c copy</dt><dd>use stream copy mode to re-mux instead of re-encode</dd>
        <dt>-map 0</dt><dd>tells FFmpeg to map all streams of the input to the output.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Excerpt to end -->

    <!-- Excerpt from end -->
    <p><label for="excerpt_from_end">Create a new file with the final five seconds of the original</label>
    </p><div>
      <h5>Excerpt from end</h5>
      <p><code>ffmpeg -sseof <em>-5</em> -i <em>input_file</em> -c copy -map 0 <em>output_file</em></code></p>
      <p>This command copies a file starting from a specified time before the end of the file, removing everything before from the output. This can be used to create an excerpt, or extract content from the end of a file (e.g. for extracting the closing credits).</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-sseof <em>-5</em></dt><dd>This parameter must stay before the input file. It tells FFmpeg what timecode in the file to look for to start copying, and specifies the number of seconds from the end of the video that FFmpeg should start copying. The end of the file has index 0 and the minus sign is needed to reference earlier portions. To be more specific, you can use timecode such as -00:00:05. Note that in most file formats it is not possible to seek exactly, so FFmpeg will seek to the closest point before.</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c copy</dt><dd>use stream copy mode to re-mux instead of re-encode</dd>
        <dt>-map 0</dt><dd>tells FFmpeg to map all streams of the input to the output.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Excerpt from end -->

    <!-- Trim start silence -->
    <p><label for="trim_start_silence">Trim silence from beginning of an audio file</label>
    </p><div>
      <h5>Remove silent portion at the beginning of an audio file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af silenceremove=start_threshold=-57dB:start_duration=1:start_periods=1  -c:a <em>your_codec_choice</em> -ar <em>your_sample_rate_choice</em> <em>output_file</em></code></p>
      <p>This command will automatically remove silence at the beginning of an audio file. The threshold for what qualifies as silence can be changed - this example uses anything under -57 dB, which is a decent level for accounting for analogue hiss.</p>
      <p><strong>Note:</strong> Since this command uses a filter, the audio stream will be re-encoded for the output. If you do not specify a sample rate or codec, this command will use the sample rate from your input and <a href="#codec-defaults">the codec defaults for your output format</a>. Take care that you are getting your intended results!</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file (e.g. input_file.wav)</dd>
        <dt>-af silenceremove</dt><dd>applies the silence remove filter</dd>
        <dt>start_threshold=-57dB</dt><dd>tells the filter the threshold for what to call 'silence' for the purpose of removal. This can be increased or decreased as necessary.</dd>
        <dt>start_duration=1</dt><dd>This tells the filter how much non-silent audio must be detected before it stops trimming. With a value of <code>0</code> the filter would stop after detecting any non-silent audio. A setting of <code>1</code> allows it to continue trimming through short 'pops' such as those caused by engaging the playback device, or the recorded sound of a microphone being plugged in.</dd>
        <dt>start_periods=1</dt><dd>This tells the filter to trim the first example of silence it discovers from the beginning of the file. This value could be increased to remove subsequent silent portions from the file if desired.</dd>
        <dt>-c:a <em>your_codec_choice</em></dt><dd>This tells the filter what codec to use, and must be specified to avoid defaults. If you want 24 bit PCM, your value would be <code>-c:a pcm_s24le</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file (e.g. output_file.wav).</dd>
      </dl>
    </div>
    <!-- ends Trim start silence -->

    <!-- Trim end silence -->
    <p><label for="trim_end_silence">Trim silence from the end of an audio file</label>
    </p><div>
      <h5>Remove silent portion from the end of an audio file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af areverse,silenceremove=start_threshold=-57dB:start_duration=1:start_periods=1,areverse -c:a <em>your_codec_choice</em> -ar <em>your_sample_rate_choice</em> <em>output_file</em></code></p>
      <p>This command will automatically remove silence at the end of an audio file. Since the <code>silenceremove</code> filter is best at removing silence from the beginning of files, this command used the <code>areverse</code> filter twice to reverse the input, remove silence and then restore correct orientation.</p>
      <p><strong>Note:</strong> Since this command uses a filter, the audio stream will be re-encoded for the output. If you do not specify a sample rate or codec, this command will use the sample rate from your input and <a href="#codec-defaults">the codec defaults for your output format</a>. Take care that you are getting your intended results!</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file (e.g. input_file.wav)</dd>
        <dt>-af areverse,</dt><dd>starts the filter chain with reversing the input</dd>
        <dt>silenceremove</dt><dd>applies the silence remove filter</dd>
        <dt>start_threshold=-57dB</dt><dd>tells the filter the threshold for what to call 'silence' for the purpose of removal. This can be increased or decreased as necessary.</dd>
        <dt>start_duration=1</dt><dd>This tells the filter how much non-silent audio must be detected before it stops trimming. With a value of <code>0</code> the filter would stop after detecting any non-silent audio. A setting of <code>1</code> allows it to continue trimming through short 'pops' such as those caused by engaging the playback device, or the recorded sound of a microphone being plugged in.</dd>
        <dt>start_periods=1</dt><dd>This tells the filter to trim the first example of silence it discovers.</dd>
        <dt>areverse</dt><dd>applies the audio reverse filter again to restore input to correct orientation.</dd>
        <dt>-c:a <em>your_codec_choice</em></dt><dd>This tells the filter what codec to use, and must be specified to avoid defaults. If you want 24 bit PCM, your value would be <code>-c:a pcm_s24le</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file (e.g. output_file.wav).</dd>
      </dl>
    </div>
    <!-- ends Trim end silence -->

    </div>
    <div>
    <h2 id="interlacing">Work with interlaced video</h2>

    <!-- NTSC to H.264 -->
    <p><label for="ntsc_to_h264">Upscaled, pillar-boxed HD H.264 access files from SD NTSC source</label>
    </p><div>
      <h5>Upscaled, Pillar-boxed HD H.264 Access Files from SD NTSC source</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -filter:v "yadif, scale=1440:1080:flags=lanczos, pad=1920:1080:(ow-iw)/2:(oh-ih)/2, format=yuv420p" <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx264</dt><dd>encodes video stream with libx264 (h264)</dd>
        <dt>-filter:v</dt><dd>a video filter will be used</dd>
        <dt>"</dt><dd>quotation mark to start filtergraph</dd>
        <dt>yadif</dt><dd>deinterlacing filter (‘yet another deinterlacing filter’)<br>
         By default, <a href="https://ffmpeg.org/ffmpeg-filters.html#yadif-1" target="_blank">yadif</a> will output one frame for each frame. Outputting one frame for each <em>field</em> (thereby doubling the frame rate) with <code>yadif=1</code> may produce visually better results.</dd>
        <dt>scale=1440:1080:flags=lanczos</dt><dd>resizes the image to 1440x1080, using the Lanczos scaling algorithm, which is slower but better than the default bilinear algorithm.</dd>
        <dt>pad=1920:1080:(ow-iw)/2:(oh-ih)/2</dt><dd>pads the area around the 4:3 input video to create a 16:9 output video</dd>
        <dt>format=yuv420p</dt><dd>specifies a pixel format of Y′C<sub>B</sub>C<sub>R</sub> 4:2:0</dd>
        <dt>"</dt><dd>quotation mark to end filtergraph</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p><strong>Note:</strong> the very same scaling filter also downscales a bigger image size into HD.</p>
      
    </div>
    <!-- ends NTSC to H.264 -->

    <!-- Deinterlace video -->
    <p><label for="deinterlace">Deinterlace video</label>
    </p><div>
      <h5>Deinterlace a video</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf "yadif,format=yuv420p" <em>output_file</em></code></p>
      <p>This command takes an interlaced input file and outputs a deinterlaced H.264 MP4.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx264</dt><dd>tells FFmpeg to encode the video stream as H.264</dd>
        <dt>-vf</dt><dd>video filtering will be used (<code>-vf</code> is an alias of <code>-filter:v</code>)</dd>
        <dt>"</dt><dd>start of filtergraph (see below)</dd>
        <dt>yadif</dt><dd>deinterlacing filter (‘yet another deinterlacing filter’)<br>
        By default, <a href="https://ffmpeg.org/ffmpeg-filters.html#yadif-1" target="_blank">yadif</a> will output one frame for each frame. Outputting one frame for each <em>field</em> (thereby doubling the frame rate) with <code>yadif=1</code> may produce visually better results.</dd>
        <dt>,</dt><dd>separates filters</dd>
        <dt>format=yuv420p</dt><dd>chroma subsampling set to 4:2:0<br>
        By default, <code>libx264</code> will use a chroma subsampling scheme that is the closest match to that of the input. This can result in Y′C<sub>B</sub>C<sub>R</sub> 4:2:0, 4:2:2, or 4:4:4 chroma subsampling. QuickTime and most other non-FFmpeg based players can’t decode H.264 files that are not 4:2:0, therefore it’s advisable to specify 4:2:0 chroma subsampling.</dd>
        <dt>"</dt><dd>end of filtergraph</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p><code>"yadif,format=yuv420p"</code> is an FFmpeg <a href="https://trac.ffmpeg.org/wiki/FilteringGuide#FiltergraphChainFilterrelationship" target="_blank">filtergraph</a>. Here the filtergraph is made up of one filter chain, which is itself made up of the two filters (separated by the comma).<br>
      The enclosing quote marks are necessary when you use spaces within the filtergraph, e.g. <code>-vf "yadif, format=yuv420p"</code>, and are included above as an example of good practice.</p>
      <p><strong>Note:</strong> FFmpeg includes several deinterlacers apart from <a href="https://ffmpeg.org/ffmpeg-filters.html#yadif-1" target="_blank">yadif</a>: <a href="https://ffmpeg.org/ffmpeg-filters.html#bwdif" target="_blank">bwdif</a>, <a href="https://ffmpeg.org/ffmpeg-filters.html#w3fdif" target="_blank">w3fdif</a>, <a href="https://ffmpeg.org/ffmpeg-filters.html#kerndeint" target="_blank">kerndeint</a>, and <a href="https://ffmpeg.org/ffmpeg-filters.html#nnedi" target="_blank">nnedi</a>.</p>
      <p>For more H.264 encoding options, see the latter section of the <a href="#transcode_h264">encode H.264 command</a>.</p>
      <div>
        <h2>Example</h2>
        <p>Before and after deinterlacing:</p>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/interlaced_video.png" alt="VLC screenshot of original interlaced video">
        <img src="https://amiaopensource.github.io/ffmprovisr/img/deinterlaced_video.png" alt="VLC screenshot of deinterlaced video">
      </p></div>
      
    </div>
    <!-- ends Deinterlace video -->

    <!-- Inverse telecine -->
    <p><label for="inverse-telecine">Inverse telecine</label>
    </p><div>
      <h5>Inverse telecine a video file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf "fieldmatch,yadif,decimate" <em>output_file</em></code></p>
      <p>The inverse telecine procedure reverses the <a href="https://en.wikipedia.org/wiki/Three-two_pull_down" target="_blank">3:2 pull down</a> process, restoring 29.97fps interlaced video to the 24fps frame rate of the original film source.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx264</dt><dd>encode video as H.264</dd>
        <dt>-vf "fieldmatch,yadif,decimate"</dt><dd>applies these three video filters to the input video.<br>
        <a href="https://ffmpeg.org/ffmpeg-filters.html#fieldmatch" target="_blank">Fieldmatch</a> is a field matching filter for inverse telecine - it reconstructs the progressive frames from a telecined stream.<br>
        <a href="https://ffmpeg.org/ffmpeg-filters.html#yadif-1" target="_blank">Yadif</a> (‘yet another deinterlacing filter’) deinterlaces the video. (Note that FFmpeg also includes several other deinterlacers).<br>
        <a href="https://ffmpeg.org/ffmpeg-filters.html#decimate-1" target="_blank">Decimate</a> deletes duplicated frames.</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p><code>"fieldmatch,yadif,decimate"</code> is an FFmpeg <a href="https://trac.ffmpeg.org/wiki/FilteringGuide#FiltergraphChainFilterrelationship" target="_blank">filtergraph</a>. Here the filtergraph is made up of one filter chain, which is itself made up of the three filters (separated by commas).<br>
      The enclosing quote marks are necessary when you use spaces within the filtergraph, e.g. <code>-vf "fieldmatch, yadif, decimate"</code>, and are included above as an example of good practice.</p>
      <p>Note that if applying an inverse telecine procedure to a 29.97i file, the output framerate will actually be 23.976fps.</p>
      <p>This command can also be used to restore other framerates.</p>
      <div>
        <h2>Example</h2>
        <p>Before and after inverse telecine:</p>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/ivtc_originalvideo.gif" alt="GIF of original video">
        <img src="https://amiaopensource.github.io/ffmprovisr/img/ivtc_result.gif" alt="GIF of video after inverse telecine">
      </p></div>
      
    </div>
    <!-- ends Inverse telecine -->

    <!-- Set field order -->
    <p><label for="set_field_order">Set field order for interlaced video</label>
    </p><div>
      <h5>Change field order of an interlaced video</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v <em>video_codec</em> -filter:v setfield=tff <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter:v setfield=tff</dt><dd>Sets the field order to top field first. Use <code>setfield=bff</code> for bottom field first.</dd>
        <dt>-c:v <em>video_codec</em></dt><dd>As a video filter is used, it is not possible to use <code>-c copy</code>. The video must be re-encoded with whatever video codec is chosen, e.g. <code>ffv1</code>, <code>v210</code> or <code>prores</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Set field order -->

    <!-- Check interlacement -->
    <p><label for="check_interlacement">Identify interlacement patterns in a video file</label>
    </p><div>
      <h5>Check video file interlacement patterns</h5>
      <p><code>ffmpeg -i <em>input file</em> -filter:v idet -f null -</code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter:v idet</dt><dd>This calls the <a href="https://ffmpeg.org/ffmpeg-filters.html#idet" target="_blank">idet (detect video interlacing type) filter</a>.</dd>
        <dt>-f null</dt><dd>Video is decoded with the <code>null</code> muxer. This allows video decoding without creating an output file.</dd>
        <dt>-</dt><dd>FFmpeg syntax requires a specified output, and <code>-</code> is just a place holder. No file is actually created.</dd>
      </dl>
      
    </div>
    <!-- ends Check interlacement -->

    </div>
    <div>
    <h2 id="overlay">Overlay timecode or text</h2>

    <!-- Text Watermark -->
    <p><label for="text_watermark">Create opaque centered text watermark</label>
    </p><div>
      <h5>Create centered, transparent text watermark</h5>
      <p>E.g For creating access copies with your institutions name</p>
      <p><code>ffmpeg -i <em>input_file</em> -vf drawtext="fontfile=<em>font_path</em>:fontsize=<em>font_size</em>:text=<em>watermark_text</em>:fontcolor=<em>font_color</em>:alpha=0.4:x=(w-text_w)/2:y=(h-text_h)/2" <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-vf drawtext=</dt><dd>This calls the drawtext filter with the following options:
        <dl>
          <dt>fontfile=<em>font_path</em></dt><dd>Set path to font. For example in macOS: <code>fontfile=/Library/Fonts/AppleGothic.ttf</code></dd>
          <dt>fontsize=<em>font_size</em></dt><dd>Set font size. <code>35</code> is a good starting point for SD. Ideally this value is proportional to video size, for example use ffprobe to acquire video height and divide by 14.</dd>
          <dt>text=<em>watermark_text</em></dt><dd>Set the content of your watermark text. For example: <code>text='FFMPROVISR EXAMPLE TEXT'</code></dd>
          <dt>fontcolor=<em>font_color</em></dt><dd>Set color of font. Can be a text string such as <code>fontcolor=white</code> or a hexadecimal value such as <code>fontcolor=0xFFFFFF</code></dd>
          <dt>alpha=0.4</dt><dd>Set transparency value.</dd>
          <dt>x=(w-text_w)/2:y=(h-text_h)/2</dt><dd>Sets <em>x</em> and <em>y</em> coordinates for the watermark. These relative values will centre your watermark regardless of video dimensions.</dd>
        </dl>
        Note: <code>-vf</code> is a shortcut for <code>-filter:v</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file.</dd>
      </dl>
      
    </div>
    <!-- ends Text watermark -->

    <!-- Transparent Image Watermark -->
    <p><label for="image_watermark">Overlay image watermark on video</label>
    </p><div>
      <h5>Overlay image watermark on video</h5>
      <p><code>ffmpeg -i <em>input_video file</em> -i <em>input_image_file</em> -filter_complex overlay=main_w-overlay_w-5:5 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_video_file</em></dt><dd>path, name and extension of the input video file</dd>
        <dt>-i <em>input_image_file</em></dt><dd>path, name and extension of the image file</dd>
        <dt>-filter_complex overlay=main_w-overlay_w-5:5</dt><dd>This calls the overlay filter and sets x and y coordinates for the position of the watermark on the video. Instead of hardcoding specific x and y coordinates, <code>main_w-overlay_w-5:5</code> uses relative coordinates to place the watermark in the upper right hand corner, based on the width of your input files. Please see the <a href="https://ffmpeg.org/ffmpeg-all.html#toc-Examples-102" target="_blank">FFmpeg documentation for more examples.</a></dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Image Watermark -->

    <!-- Burn in timecode-->
    <p><label for="burn_in_timecode">Burn in timecode</label>
    </p><div>
      <h5>Create a burnt in timecode on your image</h5>
      <p><code>ffmpeg -i <em>input_file</em> -vf drawtext="fontfile=<em>font_path</em>:fontsize=<em>font_size</em>:timecode=<em>starting_timecode</em>:fontcolor=<em>font_colour</em>:box=1:boxcolor=<em>box_colour</em>:rate=<em>timecode_rate</em>:x=(w-text_w)/2:y=h/1.2" <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-vf drawtext=</dt><dd>This calls the drawtext filter with the following options:
        </dd><dt>"</dt><dd>quotation mark to start drawtext filter command</dd>
        <dt>fontfile=<em>font_path</em></dt><dd>Set path to font. For example in macOS: <code>fontfile=/Library/Fonts/AppleGothic.ttf</code></dd>
        <dt>fontsize=<em>font_size</em></dt><dd>Set font size. <code>35</code> is a good starting point for SD. Ideally this value is proportional to video size, for example use ffprobe to acquire video height and divide by 14.</dd>
        <dt>timecode=<em>starting_timecode</em></dt><dd>Set the timecode to be displayed for the first frame. Timecode is to be represented as <code>hh:mm:ss[:;.]ff</code>. Colon escaping is determined by O.S, for example in Ubuntu <code>timecode='09\\:50\\:01\\:23'</code>. Ideally, this value would be generated from the file itself using ffprobe.</dd>
        <dt>fontcolor=<em>font_color</em></dt><dd>Set color of font. Can be a text string such as <code>fontcolor=white</code> or a hexadecimal value such as <code>fontcolor=0xFFFFFF</code></dd>
        <dt>box=1</dt><dd>Enable box around timecode</dd>
        <dt>boxcolor=<em>box_color</em></dt><dd>Set color of box. Can be a text string such as <code>fontcolor=black</code> or a hexadecimal value such as <code>fontcolor=0x000000</code></dd>
        <dt>rate=<em>timecode_rate</em></dt><dd>Framerate of video. For example <code>25/1</code></dd>
        <dt>x=(w-text_w)/2:y=h/1.2</dt><dd>Sets <em>x</em> and <em>y</em> coordinates for the timecode. These relative values will horizontally centre your timecode in the bottom third regardless of video dimensions.</dd>
        <dt>"</dt><dd>quotation mark to end drawtext filter command</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file.</dd>
      </dl>
      <p>Note: <code>-vf</code> is a shortcut for <code>-filter:v</code>.</p>
      
    </div>
    <!-- ends Burn in timecode -->

    <!-- Embed subtitles-->
    <p><label for="embed_subtitles">Embed subtitles</label>
    </p><div>
      <h5>Embed a subtitle file into a movie file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -i <em>subtitles_file</em> -c copy -c:s mov_text <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-i <em>subtitles_file</em></dt><dd>path to subtitles file, e.g. <code>subtitles.srt</code></dd>
        <dt>-c copy</dt><dd>enable stream copy (no re-encode)</dd>
        <dt>-c:s mov_text</dt><dd>Encode subtitles using the <code>mov_text</code> codec. Note: The <code>mov_text</code> codec works for MP4 and MOV containers. For the MKV container, acceptable formats are <code>ASS</code>, <code>SRT</code>, and <code>SSA</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>Note: <code>-c:s</code> is a shortcut for <code>-scodec</code></p>
      
    </div>
    <!-- ends Embed subtitles -->

    </div>
    <div>
    <h2 id="create-images">Create thumbnails or GIFs</h2>

    <!-- One thumbnail -->
    <p><label for="one_thumbnail">Export one thumbnail per video file</label>
    </p><div>
      <h5>One thumbnail</h5>
      <p><code>ffmpeg -i <em>input_file</em> -ss 00:00:20 -vframes 1 thumb.png</code></p>
      <p>This command will grab a thumbnail 20 seconds into the video.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-ss <em>00:00:20</em></dt><dd>seeks video file to 20 seconds into the video</dd>
        <dt>-vframes <em>1</em></dt><dd>sets the number of frames (in this example, one frame)</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends One thumbnail -->

    <!-- Multi thumbnail -->
    <p><label for="multi_thumbnail">Export many thumbnails per video file</label>
    </p><div>
      <h5>Many thumbnails</h5>
      <p><code>ffmpeg -i <em>input_file</em> -vf fps=1/60 out%d.png</code></p>
      <p>This will grab a thumbnail every minute and output sequential png files.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-ss <em>00:00:20</em></dt><dd>seeks video file to 20 seconds into the video</dd>
        <dt>-vf fps=1/60</dt><dd>Creates a filtergraph to use for the streams. The rest of the command identifies filtering by frames per second, and sets the frames per second at 1/60 (which is one per minute). Omitting this will output all frames from the video.</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file. In the example out%d.png where %d is a regular expression that adds a number (d is for digit) and increments with each frame (out1.png, out2.png, out3.png…). You may also chose a regular expression like out%04d.png which gives 4 digits with leading 0 (out0001.png, out0002.png, out0003.png, …).</dd>
      </dl>
      
    </div>
    <!-- ends Multi thumbnail -->

    <!-- Images to GIF -->
    <p><label for="img_to_gif">Create GIF from still images</label>
    </p><div>
      <h5>Images to GIF</h5>
      <p><code>ffmpeg -f image2 -framerate 9 -pattern_type glob -i <em>"input_image_*.jpg"</em> -vf scale=250x250 <em>output_file</em>.gif</code></p>
      <p>This will convert a series of image files into a GIF.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f image2</dt><dd>forces input or output file format. <code>image2</code> specifies the image file demuxer.</dd>
        <dt>-framerate 9</dt><dd>sets framerate to 9 frames per second</dd>
        <dt>-pattern_type glob</dt><dd>tells FFmpeg that the following mapping should "interpret like a <a href="https://en.wikipedia.org/wiki/Glob_%28programming%29" target="_blank">glob</a>" (a "global command" function that relies on the * as a wildcard and finds everything that matches)</dd>
        <dt>-i <em>"input_image_*.jpg"</em></dt><dd>maps all files in the directory that start with input_image_, for example input_image_001.jpg, input_image_002.jpg, input_image_003.jpg... etc.<br>
        (The quotation marks are necessary for the above “glob” pattern!)</dd>
        <dt>-vf scale=250x250</dt><dd>filter the video to scale it to 250x250; <code>-vf</code> is an alias for <code>-filter:v</code></dd>
        <dt><em>output_file.gif</em></dt><dd>path and name of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Images to GIF -->

    <!-- Create GIF -->
    <p><label for="create_gif">Create GIF from a video</label>
    </p><div>
      <h5>Create GIF</h5>
      <p>Create high quality GIF</p>
      <p><code>ffmpeg -ss HH:MM:SS -i <em>input_file</em> -filter_complex "fps=10,scale=500:-1:flags=lanczos,palettegen" -t 3 <em>palette.png</em></code></p>
      <p><code>ffmpeg -ss HH:MM:SS -i <em>input_file</em> -i palette.png -filter_complex "[0:v]fps=10, scale=500:-1:flags=lanczos[v], [v][1:v]paletteuse" -t 3 -loop 6 <em>output_file</em></code></p>
      <p>The first command will use the palettegen filter to create a custom palette, then the second command will create the GIF with the paletteuse filter. The result is a high quality GIF.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-ss <em>HH:MM:SS</em></dt><dd>starting point of the GIF. If a plain numerical value is used it will be interpreted as seconds</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter_complex "fps=<em>framerate</em>, scale=<em>width</em>:<em>height</em>, palettegen"</dt><dd>a complex filtergraph.<br>
        Firstly, the fps filter sets the frame rate.<br>
        Then the scale filter resizes the image. You can specify both the width and the height, or specify a value for one and use a scale value of <em>-1</em> for the other to preserve the aspect ratio. (For example, <code>500:-1</code> would create a GIF 500 pixels wide and with a height proportional to the original video). In the first script above, <code>:flags=lanczos</code> specifies that the Lanczos rescaling algorithm will be used to resize the image.<br>
        Lastly, the palettegen filter generates the palette.</dd>
        <dt>-t <em>3</em></dt><dd>duration in seconds (here 3; can be specified also with a full timestamp, i.e. here 00:00:03)</dd>
        <dt>-loop <em>6</em></dt><dd>sets the number of times to loop the GIF. A value of <em>-1</em> will disable looping. Omitting <em>-loop</em> will use the default, which will loop infinitely.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>The second command has a slightly different filtergraph, which breaks down as follows:</p>
      <dl>
        <dt>-filter_complex "[0:v]fps=10, scale=500:-1:flags=lanczos[v], [v][1:v]paletteuse"</dt><dd><code>[0:v]fps=10,scale=500:-1:flags=lanczos[v]</code>: applies the fps and scale filters described above to the first input file (the video).<br>
        <code>[v][1:v]paletteuse"</code>: applies the <code>paletteuse</code> filter, setting the second input file (the palette) as the reference file.</dd>
      </dl>
      <p>Simpler GIF creation</p>
      <p><code>ffmpeg -ss HH:MM:SS -i <em>input_file</em> -vf "fps=10,scale=500:-1" -t 3 -loop 6 <em>output_file</em></code></p>
      <p>This is a quick and easy method. Dithering is more apparent than the above method using the palette filters, but the file size will be smaller. Perfect for that “legacy” GIF look.</p>
      
    </div>
    <!-- ends Create GIF -->

    </div>
    <div>
    <h2 id="create-video">Create a video from images</h2>

    <!-- Images to video -->
    <p><label for="images_2_video">Transcode an image sequence into uncompressed 10-bit video</label>
    </p><div>
      <h5>Transcode an image sequence into uncompressed 10-bit video</h5>
      <p><code>ffmpeg -f image2 -framerate 24 -i <em>input_file_%06d.ext</em> -c:v v210 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f image2</dt><dd>forces the image file de-muxer for single image files</dd>
        <dt>-framerate 24</dt><dd>Sets the input framerate to 24 fps. The image2 demuxer defaults to 25 fps.</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file<br>
        This must match the naming convention actually used! The regex %06d matches six digits long numbers, possibly with leading zeroes. This allows to read in ascending order, one image after the other, the full sequence inside one folder. For image sequences starting with 086400 (i.e. captured with a timecode starting at 01:00:00:00 and at 24 fps), add the flag <code>-start_number 086400</code> before <code>-i input_file_%06d.ext</code>. The extension for TIFF files is .tif or maybe .tiff; the extension for DPX files is .dpx (or eventually .cin for old files).</dd>
        <dt>-c:v v210</dt><dd>encodes an uncompressed 10-bit video stream</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Images to video -->

    <!-- Create video from image and audio -->
    <p><label for="image-audio">Create video from image and audio</label>
    </p><div>
      <h5>Create a video from an image and audio file.</h5>
      <p><code>ffmpeg -r 1 -loop 1 -i <em>image_file</em> -i <em>audio_file</em> -acodec copy -shortest -vf scale=1280:720 <em>output_file</em></code></p>
      <p>This command will take an image file (e.g. image.jpg) and an audio file (e.g. audio.mp3) and combine them into a video file that contains the audio track with the image used as the video. It can be useful in a situation where you might want to upload an audio file to a platform like YouTube. You may want to adjust the scaling with -vf to suit your needs.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-r <em>1</em></dt><dd>set the framerate</dd>
        <dt>-loop <em>1</em></dt><dd>loop the first input stream</dd>
        <dt>-i <em>image_file</em></dt><dd>path, name and extension of the image file</dd>
        <dt>-i <em>audio_file</em></dt><dd>path, name and extension of the audio file</dd>
        <dt>-acodec copy</dt><dd>copy the audio. -acodec is an alias for -c:a</dd>
        <dt>-shortest</dt><dd>finish encoding when the shortest input stream ends</dd>
        <dt>-vf scale=1280:720</dt><dd>filter the video to scale it to 1280x720 for YouTube. -vf is an alias for -filter:v</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the video output file</dd>
      </dl>
      
    </div>
    <!-- ends Create video from image and audio -->

    </div>
    <div>
    <h2 id="filters-scopes">Use filters or scopes</h2>

    <!-- abitscope -->
    <p><label for="abitscope">Audio Bitscope</label>
    </p><div>
      <h5>Creates a visualization of the bits in an audio stream</h5>
      <p><code>ffplay -f lavfi "amovie=<em>input_file</em>, asplit=2[out1][a], [a]abitscope=colors=purple|yellow[out0]"</code></p>
      <p>This filter allows visual analysis of the information held in various bit depths of an audio stream. This can aid with identifying when a file that is nominally of a higher bit depth actually has been 'padded' with null information. The provided GIF shows a 16 bit WAV file (left) and then the results of converting that same WAV to 32 bit (right). Note that in the 32 bit version, there is still only information in the first 16 bits.</p>
      <dl>
        <dt>ffplay -f lavfi</dt><dd>starts the command and tells ffplay that you will be using the lavfi virtual device to create the input</dd>
        <dt>"</dt><dd>quotation mark to start the lavfi filtergraph</dd>
        <dt>amovie=<em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>asplit=2[out1][a]</dt><dd>splits the audio stream in two. One of these [a] will be passed to the filter, and the other [out1] will be the audible stream.</dd>
        <dt>[a]abitscope=colors=purple|yellow[out0]</dt><dd>sends stream [a] into the abitscope filter, sets the colors for the channels to purple and yellow, and outputs the results to [out0]. This is what will be the visualization.</dd>
        <dt>"</dt><dd>quotation mark to end the lavfi filtergraph</dd>
      </dl>
      <div>
        <h2>Comparison of mono 16 bit and mono 16 bit padded to 32 bit.</h2>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/16_32_abitscope.gif" alt="bit_scope_comparison">
      </p></div>
      
    </div>
    <!-- ends abitscope -->

    <!-- astats -->
    <p><label for="astats">Play a graphical output showing decibel levels of an input file</label>
    </p><div>
      <h5>Plays a graphical output showing decibel levels of an input file</h5>
      <p><code>ffplay -f lavfi "amovie='input.mp3', astats=metadata=1:reset=1, adrawgraph=lavfi.astats.Overall.Peak_level:max=0:min=-30.0:size=700x256:bg=Black[out]"</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffplay to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter input virtual device</a></dd>
        <dt>"</dt><dd>quotation mark to start the lavfi filtergraph</dd>
        <dt>movie='<em>input.mp3</em>'</dt><dd>declares audio source file on which to apply filter</dd>
        <dt>,</dt><dd>comma signifies the end of audio source section and the beginning of the filter section</dd>
        <dt>astats=metadata=1</dt><dd>tells the astats filter to ouput metadata that can be passed to another filter (in this case adrawgraph)</dd>
        <dt>:</dt><dd>divides between options of the same filter</dd>
        <dt>reset=1</dt><dd>tells the filter to calculate the stats on every frame (increasing this number would calculate stats for groups of frames)</dd>
        <dt>,</dt><dd>comma divides one filter in the chain from another</dd>
        <dt>adrawgraph=lavfi.astats.Overall.Peak_level:max=0:min=-30.0</dt><dd>draws a graph using the overall peak volume calculated by the astats filter. It sets the max for the graph to 0 (dB) and the minimum to -30 (dB). For more options on data points that can be graphed see the <a href="https://ffmpeg.org/ffmpeg-filters.html#astats-1" target="_blank">FFmpeg astats documentation</a></dd>
        <dt>size=700x256:bg=Black</dt><dd>sets the background color and size of the output</dd>
        <dt>[out]</dt><dd>ends the filterchain and sets the output</dd>
        <dt>"</dt><dd>quotation mark to end the lavfi filtergraph</dd>
      </dl>
      <div>
        <h2>Example of filter output</h2>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/astats_levels.gif" alt="astats example">
      </p></div>
      
    </div>
    <!-- ends astats -->

    <!-- BRNG -->
    <p><label for="brng">Identify pixels out of broadcast range</label>
    </p><div>
      <h5>Shows all pixels outside of broadcast range</h5>
      <p><code>ffplay -f lavfi "movie='<em>input.mp4</em>', signalstats=out=brng:color=cyan[out]"</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffplay to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter input virtual device</a></dd>
        <dt>"</dt><dd>quotation mark to start the lavfi filtergraph</dd>
        <dt>movie='<em>input.mp4</em>'</dt><dd>declares video file source to apply filter</dd>
        <dt>,</dt><dd>comma signifies closing of video source assertion and ready for filter assertion</dd>
        <dt>signalstats=out=brng</dt><dd>tells ffplay to use the signalstats command, output the data, use the brng filter</dd>
        <dt>:</dt><dd>indicates there’s another parameter coming</dd>
        <dt>color=cyan[out]</dt><dd>sets the color of out-of-range pixels to cyan</dd>
        <dt>"</dt><dd>quotation mark to end the lavfi filtergraph</dd>
      </dl>
      <div>
        <h2>Example of filter output</h2>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/outside_broadcast_range.gif" alt="BRNG example">
      </p></div>
      
    </div>
    <!-- ends BRNG -->

    <!-- Vectorscope -->
    <p><label for="vectorscope">Vectorscope from video to screen</label>
    </p><div>
      <h5>Plays vectorscope of video</h5>
      <p><code>ffplay <em>input_file</em> -vf "split=2[m][v], [v]vectorscope=b=0.7:m=color3:g=green[v], [m][v]overlay=x=W-w:y=H-h"</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-vf</dt><dd>creates a filtergraph to use for the streams</dd>
        <dt>"</dt><dd>quotation mark to start filtergraph</dd>
        <dt>split=2[m][v]</dt><dd>Splits the input into two identical outputs and names them [m] and [v]</dd>
        <dt>,</dt><dd>comma signifies there is another parameter coming</dd>
        <dt>[v]vectorscope=b=0.7:m=color3:g=green[v]</dt><dd>asserts usage of the vectorscope filter and sets a light background opacity (b, alias for bgopacity), sets a background color style (m, alias for mode), and graticule color (g, alias for graticule)</dd>
        <dt>,</dt><dd>comma signifies there is another parameter coming</dd>
        <dt>[m][v]overlay=x=W-w:y=H-h</dt><dd>declares where the vectorscope will overlay on top of the video image as it plays</dd>
        <dt>"</dt><dd>quotation mark to end filtergraph</dd>
      </dl>
      
    </div>
    <!-- ends Vectorscope -->

    <!--Side by Side Videos/Temporal Difference Filter-->
    <p><label for="tempdif">Side by Side Videos/Temporal Difference Filter</label>
    </p><div>
      <h5>This will play two input videos side by side while also applying the temporal difference filter to them</h5>
      <p><code>ffmpeg -i input01 -i input02 -filter_complex "[0:v:0]tblend=all_mode=difference128[a];[1:v:0]tblend=all_mode=difference128[b];[a][b]hstack[out]" -map [out] -f nut -c:v rawvideo - | ffplay -</code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input01</em> -i <em>input02</em></dt><dd>Designates the files to use for inputs one and two respectively</dd>
        <dt>-filter_complex</dt><dd>Lets FFmpeg know we will be using a complex filter (this must be used for multiple inputs)</dd>
        <dt>"</dt><dd>quotation mark to start filtergraph</dd>
        <dt>[0:v:0]tblend=all_mode=difference128[a]</dt><dd>Applies the tblend filter (with the settings all_mode and difference128) to the first video stream from the first input and assigns the result to the output [a]</dd>
        <dt>[1:v:0]tblend=all_mode=difference128[b]</dt><dd>Applies the tblend filter (with the settings all_mode and difference128) to the first video stream from the second input and assigns the result to the output [b]</dd>
        <dt>[a][b]hstack[out]</dt><dd>Takes the outputs from the previous steps ([a] and [b] and uses the hstack (horizontal stack) filter on them to create the side by side output. This output is then named [out])</dd>
        <dt>"</dt><dd>quotation mark to end filtergraph</dd>
        <dt>-map [out]</dt><dd>Maps the output of the filter chain</dd>
        <dt>-f nut</dt><dd>Sets the format for the output video stream to <a href="https://ffmpeg.org/ffmpeg-formats.html#nut" target="_blank">Nut</a></dd>
        <dt>-c:v rawvideo</dt><dd>Sets the video codec of the output video stream to raw video</dd>
        <dt>-</dt><dd>tells FFmpeg that the output will be piped to a new command (as opposed to a file)</dd>
        <dt>|</dt><dd>Tells the system you will be piping the output of the previous command into a new command</dd>
        <dt>ffplay -</dt><dd>Starts ffplay and tells it to use the pipe from the previous command as its input</dd>
      </dl>
      <div>
        <h2>Example of filter output</h2>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/tempdif.gif" alt="astats example">
      </p></div>
      
    </div>
    <!-- ends Side by Side Videos/Temporal Difference Filter -->

    <!-- xstack -->
    <p><label for="xstack">Use xstack to arrange output layout of multiple video sources</label>
    </p><div>
      <h5>This filter enables vertical and horizontal stacking of multiple video sources into one output.</h5>
      <p>This filter is useful for the creation of output windows such as the one utilized in <a href="https://github.com/amiaopensource/vrecord" target="_blank">vrecord.</a></p>
      <p><code>ffplay -f lavfi -i <em>testsrc</em> -vf "split=3[a][b][c],[a][b][c]xstack=inputs=3:layout=0_0|0_h0|0_h0+h1[out]"</code></p>
      <p>The following example uses the 'testsrc' virtual input combined with the <a href="https://ffmpeg.org/ffmpeg-filters.html#split_002c-asplit" target="_blank">split filter</a> to generate the multiple inputs.</p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt>-f lavfi -i testsrc</dt><dd>tells ffplay to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter's virtual device input 'testsrc'</a></dd>
        <dt>-vf</dt><dd>tells ffmpeg that you will be applying a filter chain to the input</dd>
        <dt>split=3[a][b][c],</dt><dd>splits the input into three separate signals within the filter graph, named a, b and c respectively. (These are variables and any names could be used as long as they are kept consistent in following steps). The <code>,</code> separates this from the next part of the filter chain.</dd>
        <dt>[a][b][c]xstack=inputs=3:</dt><dd>tells ffmpeg that you will be using the xstack filter on the three named inputs a,b and c. The final <code>:</code> is a necessary divider between the number of inputs, and the orientation of outputs portion of the xstack command.</dd>
        <dt>layout=0_0|0_h0|0_h0+h1</dt><dd>This is where the locations of the video sources in the output stack are designated. The locations are specified in order of input (so in this example <code>0_0</code> corresponds to input <code>[a]</code>. Inputs must be separated with a <code>|</code>. The two numbers represent columns and rows, with counting starting at zero rather than one. In this example, <code>0_0</code> means that input <code>[a]</code> is placed at the first row of the first column in the output. <code>0_h0</code> places the next input in the first column, at a row corresponding with the height of the first input. <code>0_h0+h1</code> places the final input in the first column, at a row corresponding with the height of the first input plus the height of the second input. This has the effect of creating a vertical stack of the three inputs. This could be made a horizontal stack by changing this portion of the command to <code>layout=0_0|w0_0|w0+w1_0</code>.</dd>
        <dt>[out]</dt><dd>this ends the filter chain and designates the final output.</dd>
      </dl>
      
      
    </div>
    <!-- ends xstack -->

    </div>
    <div>
    

    <!-- Pull specs -->
    <p><label for="pull_specs">Pull specs from video file</label>
    </p><div>
      <h5>Pull specs from video file</h5>
      <p><code>ffprobe -i <em>input_file</em> -show_format -show_streams -show_data -print_format xml</code></p>
      <p>This command extracts technical metadata from a video file and displays it in xml.</p>
      <dl>
        <dt>ffprobe</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-show_format</dt><dd>outputs file container informations</dd>
        <dt>-show_streams</dt><dd>outputs audio and video codec informations</dd>
        <dt>-show_data</dt><dd>adds a short “hexdump” to show_streams command output</dd>
        <dt>-print_format</dt><dd>Set the output printing format (in this example “xml”; other formats include “json” and “flat”)</dd>
      </dl>
      <p>See also the <a href="https://ffmpeg.org/ffprobe.html" target="_blank"> FFmpeg documentation on ffprobe</a> for a full list of flags, commands, and options.</p>
      
    </div>
    <!-- ends Pull specs -->

    <!-- Strip metadata -->
    <p><label for="strip_metadata">Strip metadata</label>
    </p><div>
      <h5>Strips metadata from video file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -map_metadata -1 -c:v copy -c:a copy <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-map_metadata -1</dt><dd>sets metadata copying to -1, which copies nothing</dd>
        <dt>-c:v copy</dt><dd>copies video track</dd>
        <dt>-c:a copy</dt><dd>copies audio track</dd>
        <dt><em>output_file</em></dt><dd>Makes copy of original file and names output file</dd>
      </dl>
      <p>Note: <code>-c:v</code> and <code>-c:a</code> are shortcuts for <code>-vcodec</code> and <code>-acodec</code>.</p>
      
    </div>
    <!-- ends Strip metadata -->

    </div>
    <div>
    <h2 id="preservation">Preservation tasks</h2>

    <!-- batch processing (Mac/Linux) -->
    <p><label for="batch_processing_bash">Batch processing (Mac/Linux)</label>
    </p><div>
      <h5>Create Bash script to batch process with FFmpeg</h5>
      <p>Bash scripts are plain text files saved with a .sh extension. This entry explains how they work with the example of a bash script named “Rewrap-MXF.sh”, which rewraps .mxf files in a given directory to .mov files.</p>
      <p>“Rewrap-MXF.sh” contains the following text:</p>
      <p><code>for file in *.mxf; do ffmpeg -i "$file" -map 0 -c copy "${file%.mxf}.mov"; done</code></p>
      <dl>
        <dt>for file in *.mxf</dt><dd>starts the loop, and states what the input files will be. Here, the FFmpeg command within the loop will be applied to all files with an extension of .mxf.<br>
        The word ‘file’ is an arbitrary variable which will represent each .mxf file in turn as it is looped over.</dd>
        <dt>do ffmpeg -i "$file"</dt><dd>carry out the following FFmpeg command for each input file.<br>
        Per Bash syntax, within the command the variable is referred to by <strong>“$file”</strong>. The dollar sign is used to reference the variable ‘file’, and the enclosing quotation marks prevents reinterpretation of any special characters that may occur within the filename, ensuring that the original filename is retained.</dd>
        <dt>-map 0</dt><dd>retain all streams</dd>
        <dt>-c copy</dt><dd>enable stream copy (no re-encode)</dd>
        <dt>"${file%.mxf}.mov";</dt><dd>retaining the original file name, set the output file wrapper as .mov</dd>
        <dt>done</dt><dd>complete; all items have been processed.</dd>
      </dl>
      <p><strong>Note:</strong> the shell script (.sh file) and all .mxf files to be processed must be contained within the same directory, and the script must be run from that directory.<br>
        Execute the .sh file with the command <code>sh Rewrap-MXF.sh</code>.</p>
      <p>Modify the script as needed to perform different transcodes, or to use with ffprobe. :)</p>
      <p>The basic pattern will look similar to this:<br>
      <code>for item in *.ext; do ffmpeg -i $item <em>(FFmpeg options here)</em> "${item%.ext}_suffix.ext"</code></p>
      <p>e.g., if an input file is bestmovie002.avi, its output will be bestmovie002_suffix.avi.</p>
      <p>Variation: recursively process all MXF files in subdirectories using <code>find</code> instead of <code>for</code>:</p>
      <p><code>find input_directory -iname "*.mxf" -exec ffmpeg -i {} -map 0 -c copy {}.mov \;</code></p>
      
    </div>
    <!-- ends batch processing (Mac/Linux) -->

    <!-- batch processing (Windows) -->
    <p><label for="batch_processing_win">Batch processing (Windows)</label>
    </p><div>
      <h5>Create PowerShell script to batch process with FFmpeg</h5>
      <p>As of Windows 10, it is possible to run Bash via <a href="https://msdn.microsoft.com/en-us/commandline/wsl/about" target="_blank">Bash on Ubuntu on Windows</a>, allowing you to use <a href="#batch_processing_bash">bash scripting</a>. To enable Bash on Windows, see <a href="https://msdn.microsoft.com/en-us/commandline/wsl/install_guide" target="_blank">these instructions</a>.</p>
      <p>On Windows, the primary native command line program is <strong>PowerShell</strong>. PowerShell scripts are plain text files saved with a .ps1 extension. This entry explains how they work with the example of a PowerShell script named “rewrap-mp4.ps1”, which rewraps .mp4 files in a given directory to .mkv files.</p>
      <p>“rewrap-mp4.ps1” contains the following text:</p>
      <pre><code>$inputfiles = ls *.mp4
  foreach ($file in $inputfiles) {
  $output = [io.path]::ChangeExtension($file, '.mkv')
  ffmpeg -i $file -map 0 -c copy $output
  }</code></pre>
      <dl>
        <dt>$inputfiles = ls *.mp4</dt><dd>Creates the variable <code>$inputfiles</code>, which is a list of all the .mp4 files in the current folder.<br>
        In PowerShell, all variable names start with the dollar-sign character.</dd>
        <dt>foreach ($file in $inputfiles)</dt><dd>Creates a loop and states the subsequent code block will be applied to each file listed in <code>$inputfiles</code>.<br>
        <code>$file</code> is an arbitrary variable which will represent each .mp4 file in turn as it is looped over.</dd>
        <dt>{</dt><dd>Opens the code block.</dd>
        <dt>$output = [io.path]::ChangeExtension($file, '.mkv')</dt><dd>Sets up the output file: it will be located in the current folder and keep the same filename, but will have an .mkv extension instead of .mp4.</dd>
        <dt>ffmpeg -i $file</dt><dd>Carry out the following FFmpeg command for each input file.<br>
        <strong>Note:</strong> To call FFmpeg here as just ‘ffmpeg’ (rather than entering the full path to ffmpeg.exe), you must make sure that it’s correctly configured. See <a href="http://adaptivesamples.com/how-to-install-ffmpeg-on-windows/" target="_blank">this article</a>, especially the section ‘Add to Path’.</dd>
        <dt>-map 0</dt><dd>retain all streams</dd>
        <dt>-c copy</dt><dd>enable stream copy (no re-encode)</dd>
        <dt>$output</dt><dd>The output file is set to the value of the <code>$output</code> variable declared above: i.e., the current file name with an .mkv extension.</dd>
        <dt>}</dt><dd>Closes the code block.</dd>
      </dl>
      <p><strong>Note:</strong> the PowerShell script (.ps1 file) and all .mp4 files to be rewrapped must be contained within the same directory, and the script must be run from that directory.</p>
      <p>Execute the .ps1 file by typing <code>.\rewrap-mp4.ps1</code> in PowerShell.</p>
      <p>Modify the script as needed to perform different transcodes, or to use with ffprobe. :)</p>
      
    </div>
    <!-- ends batch processing (Windows) -->

    <!-- Check decoder errors -->
    <p><label for="check_decoder_errors">Check decoder errors</label>
    </p><div>
      <h5>Check decoder errors</h5>
      <p><code>ffmpeg -i <em>input_file</em> -f null -</code></p>
      <p>This decodes your video and prints any errors or found issues to the screen.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-f null</dt><dd>Video is decoded with the <code>null</code> muxer. This allows video decoding without creating an output file.</dd>
        <dt>-</dt><dd>FFmpeg syntax requires a specified output, and <code>-</code> is just a place holder. No file is actually created.</dd>
      </dl>
      
    </div>
    <!-- ends Check decoder errors -->

    <!-- Check FFV1 fixity -->
    <p><label for="check_FFV1_fixity">Check FFV1 fixity</label>
    </p><div>
      <h3>Check FFV1 Version 3 fixity</h3>
      <p><code>ffmpeg -report -i <em>input_file</em> -f null -</code></p>
      <p>This decodes your video and displays any CRC checksum mismatches. These errors will display in your terminal like this: <code>[ffv1 @ 0x1b04660] CRC mismatch 350FBD8A!at 0.272000 seconds</code></p>
      <p>Frame CRCs are enabled by default in FFV1 Version 3.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-report</dt><dd>Dump full command line and console output to a file named <em>ffmpeg-YYYYMMDD-HHMMSS.log</em> in the current directory. It also implies <code>-loglevel verbose</code>.</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-f null</dt><dd>Video is decoded with the <code>null</code> muxer. This allows video decoding without creating an output file.</dd>
        <dt>-</dt><dd>FFmpeg syntax requires a specified output, and <code>-</code> is just a place holder. No file is actually created.</dd>
      </dl>
      
    </div>
    <!-- ends Check FFV1 Fixity -->

    <!-- Create frame md5s -->
    <p><label for="create_frame_md5s_v">Create MD5 checksums (video frames)</label>
    </p><div>
      <h5>Create MD5 checksums (video frames)</h5>
      <p><code>ffmpeg -i <em>input_file</em> -f framemd5 -an <em>output_file</em></code></p>
      <p>This will create an MD5 checksum per video frame.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-f framemd5</dt><dd>library used to calculate the MD5 checksums</dd>
        <dt>-an</dt><dd>ignores the audio stream (audio no)</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>You may verify an MD5 checksum file created this way by using a <a href="https://amiaopensource.github.io/ffmprovisr/scripts/check_video_framemd5.sh" target="_blank">Bash script</a>.</p>
      
    </div>
    <!-- ends Create frame md5s -->

    <!-- Create frame md5s (audio) -->
    <p><label for="create_frame_md5s_a">Create MD5 checksums (audio samples)</label>
    </p><div>
      <h5>Create MD5 checksums (audio samples)</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af "asetnsamples=n=48000" -f framemd5 -vn <em>output_file</em></code></p>
      <p>This will create an MD5 checksum for each group of 48000 audio samples.<br>
        The number of samples per group can be set arbitrarily, but it's good practice to match the samplerate of the media file (so you will get one checksum per second).</p>
      <p>Examples for other samplerates:</p>
      <ul>
        <li>44.1 kHz: "asetnsamples=n=44100"</li>
        <li>96 kHz: "asetnsamples=n=96000"</li>
      </ul>
      <p><strong>Note:</strong> This filter transcodes audio to 16 bit PCM by default. The generated framemd5s will represent this value. Validating these framemd5s will require using the same default settings. Alternatively, when your file has another quantization rates (e.g. 24 bit), then you might add the audio codec <code>-c:a pcm_s24le</code> to the command, for compatibility reasons with other tools, like <a href="https://mediaarea.net/BWFMetaEdit" target="_blank">BWF MetaEdit</a>.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af "asetnsamples=n=<em>48000</em>"</dt><dd>the audio filter sets the sampling rate</dd>
        <dt>-f framemd5</dt><dd>library used to calculate the MD5 checksums</dd>
        <dt>-vn</dt><dd>ignores the video stream (video no)</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>You may verify an MD5 checksum file created this way by using a <a href="https://amiaopensource.github.io/ffmprovisr/scripts/check_audio_framemd5.sh" target="_blank">Bash script</a>.</p>
      
    </div>
    <!-- ends Create frame md5s (audio) -->

    <!-- Create stream md5s -->
    <p><label for="create_stream_md5s">Create MD5 checksum(s) for A/V stream data only</label>
    </p><div>
      <h5>Create stream MD5s</h5>
      <p><code>ffmpeg -i <em>input_file</em> -map 0:v:0 -c:v copy -f md5 <em>output_file_1</em> -map 0:a:0 -c:a copy -f md5 <em>output_file_2</em></code></p>
      <p>This will create MD5 checksums for the first video and the first audio stream in a file. If only one of these is necessary (for example if used on a WAV file) either part of the command can be excluded to create the desired MD5 only. Use of this kind of checksum enables integrity of the A/V information to be verified independently of any changes to surrounding metadata.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-map 0:v:0</dt><dd>selects the first video stream from the input</dd>
        <dt>-c:v copy</dt><dd>ensures that FFmpeg will not transcode the video to a different codec before generating the MD5</dd>
        <dt><em>output_file_1</em></dt><dd>is the output file for the video stream MD5. Example file extensions are <code>.md5</code> and <code>.txt</code></dd>
        <dt>-map 0:a:0</dt><dd>selects the first audio stream from the input</dd>
        <dt>-c:a copy</dt><dd>ensures that FFmpeg will not transcode the audio to a different codec before generating the MD5 (by default FFmpeg will use 16 bit PCM for audio MD5s).</dd>
        <dt><em>output_file_2</em></dt><dd>is the output file for the audio stream MD5.</dd>
      </dl>
      <p><strong>Note:</strong> The MD5s generated by running this command on WAV files are compatible with those embedded by the <a href="https://mediaarea.net/BWFMetaEdit" target="_blank">BWF MetaEdit</a> tool and can be compared.</p>
      
    </div>
    <!-- ends Create stream md5s -->

    <!-- Get checksum for video/audio stream -->
    <p><label for="get_stream_checksum">Get checksum for video/audio stream</label>
    </p><div>
      <h5>Get checksum for video/audio stream</h5>
      <p><code>ffmpeg -loglevel error -i <em>input_file</em> -map 0:v:0 -f hash -hash md5 -</code></p>
      <p>This script will perform a fixity check on a specified audio or video stream of the file, useful for checking that the content within a video has not changed even if the container format has changed.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-loglevel error</dt><dd>sets the verbosity of logging to show all errors</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-map 0:v:0</dt><dd>designated the first video stream as the stream on which to perform this hash generation operation. <code>-map 0</code> can be used to run the operation on all streams.</dd>
        <dt>-f hash -hash md5</dt><dd>produce a checksum hash, and set the hash algorithm to md5. See the official <a href="https://ffmpeg.org/ffmpeg-formats.html#hash" target="_blank">documentation on hash</a> for other algorithms.</dd>
        <dt>-</dt><dd>FFmpeg syntax requires a specified output, and <code>-</code> is just a place holder. No file is actually created.</dd>
      </dl>
      
    </div>
    <!-- ends Get checksum for video/audio stream -->

    <!-- Get checksum for all video/audio streams -->
    <p><label for="get_streamhash">Get individual checksums for all video/audio streams ("Streamhash")</label>
    </p><div>
      <h5>Get individual checksums for all video/audio streams ("Streamhash")</h5>
      <p><code>ffmpeg -i <em>input_file</em> -map 0 -f streamhash -hash md5 - -v quiet</code></p>
      <p>The outcome is very similar to that of "-f hash", except you get one hash per-stream, instead of one (summary) hash. Another benefit is that you don't have to know which streams, or how many to expect in the source file. This is very handy for hashing mixed born-digital material.</p>
      <p>This script will perform a fixity check on all audio and video streams in the file and return one hashcode for each one.  This is useful for e.g. being able to change to container/codec format later on and validate it matches the original source.</p>
      <p>The output is formatted for easily processing it further in any kind of programming/scripting language.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-map 0</dt><dd>map ALL streams from input file to output. If you omit this, ffmpeg chooses only the first "best" (*) stream: 1 for audio, 1 for video (not all streams).</dd>
        <dt>-f streamhash -hash md5</dt><dd>produce a checksum hash per-stream, and set the hash algorithm to md5. See the official <a href="https://www.ffmpeg.org/ffmpeg-formats.html#streamhash-1" target="_blank">documentation on streamhash</a> for other algorithms and more details.</dd>
        <dt>-</dt><dd>FFmpeg syntax requires a specified output, and <code>-</code> is just a place holder. No file is actually created. Choose an output filename to write the hashcode lines into a textfile.</dd>
        <dt>-v quiet</dt><dd>(Optional) Disables FFmpeg's processing output. With this option it's easier to see the text output of the hashes.</dd>
      </dl>
      <p>The output looks like this, for example (1 video, 2 audio streams):
      <code>
      0,v,MD5=89bed8031048d985b48550b6b4cb171c<br>
      0,a,MD5=36daadb543b63610f63f9dcff11680fb<br>
      1,a,MD5=f21269116a847f887710cfc67ecc3e6e
      </code></p>
      
    </div>
    <!-- ends Get checksum for all video/audio streams -->

    <!-- QCTools Report -->
    <p><label for="qctools">QCTools report (with audio)</label>
    </p><div>
      <h5>Creates a QCTools report</h5>
      <p><code>ffprobe -f lavfi -i "movie=<em>input_file</em>:s=v+a[in0][in1], [in0]signalstats=stat=tout+vrep+brng, cropdetect=reset=1:round=1, idet=half_life=1, split[a][b];[a]field=top[a1];[b]field=bottom, split[b1][b2];[a1][b1]psnr[c1];[c1][b2]ssim[out0];[in1]ebur128=metadata=1, astats=metadata=1:reset=1:length=0.4[out1]" -show_frames -show_versions -of xml=x=1:q=1 -noprivate | gzip &gt; <em>input_file</em>.qctools.xml.gz</code></p>
      <p>This will create an XML report for use in <a href="https://github.com/bavc/qctools" target="_blank">QCTools</a> for a video file with one video track and one audio track. See also the <a href="https://github.com/bavc/qctools/blob/master/docs/data_format.md#creating-a-qctools-document" target="_blank">QCTools documentation</a>.</p>
      <dl>
        <dt>ffprobe</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffprobe to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i</dt><dd>input file and parameters</dd>
        <dt>"movie=<em>input_file</em>:s=v+a[in0][in1], [in0]signalstats=stat=tout+vrep+brng, cropdetect=reset=1:round=1, idet=half_life=1, split[a][b];[a]field=top[a1];[b]field=bottom, split[b1][b2];[a1][b1]psnr[c1];[c1][b2]ssim[out0];[in1]ebur128=metadata=1, astats=metadata=1:reset=1:length=0.4[out1]"</dt>
        <dd>This very large lump of commands declares the input file and passes in a request for all potential data signal information for a file with one video and one audio track</dd>
        <dt>-show_frames</dt><dd>asks for information about each frame and subtitle contained in the input multimedia stream</dd>
        <dt>-show_versions</dt><dd>asks for information related to program and library versions</dd>
        <dt>-of xml=x=1:q=1</dt><dd>sets the data export format to XML</dd>
        <dt>-noprivate</dt><dd>hides any private data that might exist in the file</dd>
        <dt>| gzip</dt><dd>The | is to "pipe" (or push) the data into a compressed file format</dd>
        <dt><code>&gt;</code></dt><dd>redirects the standard output (the data made by ffprobe about the video)</dd>
        <dt><em>input_file</em>.qctools.xml.gz</dt><dd>names the zipped data output file, which can be named anything but needs the extension qctools.xml.gz for compatibility issues</dd>
      </dl>
      
    </div>
    <!-- ends QCTools Report -->

    <!-- QCTools Report (no audio) -->
    <p><label for="qctools_no_audio">QCTools report (no audio)</label>
    </p><div>
      <h5>Creates a QCTools report</h5>
      <p><code>ffprobe -f lavfi -i "movie=<em>input_file</em>,signalstats=stat=tout+vrep+brng, cropdetect=reset=1:round=1, idet=half_life=1, split[a][b];[a]field=top[a1];[b]field=bottom,split[b1][b2];[a1][b1]psnr[c1];[c1][b2]ssim" -show_frames -show_versions -of xml=x=1:q=1 -noprivate | gzip &gt; <em>input_file</em>.qctools.xml.gz</code></p>
      <p>This will create an XML report for use in <a href="https://github.com/bavc/qctools" target="_blank">QCTools</a> for a video file with one video track and NO audio track. See also the <a href="https://github.com/bavc/qctools/blob/master/docs/data_format.md#creating-a-qctools-document" target="_blank">QCTools documentation</a>.</p>
      <dl>
        <dt>ffprobe</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffprobe to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i</dt><dd>input file and parameters</dd>
        <dt>"movie=<em>input_file</em>,signalstats=stat=tout+vrep+brng, cropdetect=reset=1:round=1, idet=half_life=1, split[a][b];[a]field=top[a1];[b]field=bottom,split[b1][b2];[a1][b1]psnr[c1];[c1][b2]ssim"</dt>
        <dd>This very large lump of commands declares the input file and passes in a request for all potential data signal information for a file with one video and one audio track</dd>
        <dt>-show_frames</dt><dd>asks for information about each frame and subtitle contained in the input multimedia stream</dd>
        <dt>-show_versions</dt><dd>asks for information related to program and library versions</dd>
        <dt>-of xml=x=1:q=1</dt><dd>sets the data export format to XML</dd>
        <dt>-noprivate</dt><dd>hides any private data that might exist in the file</dd>
        <dt>| gzip</dt><dd>The | is to "pipe" (or push) the data into a compressed file format</dd>
        <dt><code>&gt;</code></dt><dd>redirects the standard output (the data made by ffprobe about the video)</dd>
        <dt><em>input_file</em>.qctools.xml.gz</dt><dd>names the zipped data output file, which can be named anything but needs the extension qctools.xml.gz for compatibility issues</dd>
      </dl>
      
    </div>
    <!-- ends QCTools Report (no audio) -->

    <!-- Read/Extract EIA-608 Closed Captions -->
    <p><label for="readeia608">Read/Extract EIA-608 Closed Captioning</label>
    </p><div>
      <h5>Read/Extract EIA-608 (Line 21) closed captioning</h5>
      <p><code>ffprobe -f lavfi -i movie=<em>input_file</em>,readeia608 -show_entries frame=pkt_pts_time:frame_tags=lavfi.readeia608.0.line,lavfi.readeia608.0.cc,lavfi.readeia608.1.line,lavfi.readeia608.1.cc -of csv &gt; <em>input_file</em>.csv</code></p>
      <p>This command uses FFmpeg's <a href="https://ffmpeg.org/ffmpeg-filters.html#readeia608" target="_blank">readeia608</a> filter to extract the hexadecimal values hidden within <a href="https://en.wikipedia.org/wiki/EIA-608" target="_blank">EIA-608 (Line 21)</a> Closed Captioning, outputting a csv file. For more information about EIA-608, check out Adobe's <a href="https://www.adobe.com/content/dam/Adobe/en/devnet/video/pdfs/introduction_to_closed_captions.pdf" target="_blank">Introduction to Closed Captions</a>.</p>
      <p>If hex isn't your thing, closed captioning <a href="http://www.theneitherworld.com/mcpoodle/SCC_TOOLS/DOCS/CC_CHARS.HTML" target="_blank">character</a> and <a href="http://www.theneitherworld.com/mcpoodle/SCC_TOOLS/DOCS/CC_CODES.HTML" target="_blank">code</a> sets can be found in the documentation for SCTools.</p>
      <dl>
        <dt>ffprobe</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffprobe to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">libavfilter</a> input virtual device</dd>
        <dt>-i <em>input_file</em></dt><dd>input file and parameters</dd>
        <dt>readeia608 -show_entries frame=pkt_pts_time:frame_tags=lavfi.readeia608.0.line,lavfi.readeia608.0.cc,lavfi.readeia608.1.line,lavfi.readeia608.1.cc -of csv</dt><dd>specifies the first two lines of video in which EIA-608 data (hexadecimal byte pairs) are identifiable by ffprobe, outputting comma separated values (CSV)</dd>
        <dt>&gt;</dt><dd>redirects the standard output (the data created by ffprobe about the video)</dd>
        <dt><em>output_file</em>.csv</dt><dd>names the CSV output file</dd>
      </dl>
      <div>
        <h4>Example</h4>
        <p>Side-by-side video with true EIA-608 captions on the left, zoomed in view of the captions on the right (with hex values represented). To achieve something similar with your own captioned video, try out the EIA608/VITC viewer in <a href="https://github.com/bavc/qctools" target="_blank">QCTools</a>.</p>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/eia608_captions.gif" alt="GIF of Closed Captions">
      </p></div>
      
    </div>
    <!-- ends Read/Extract EIA-608 Closed Captions -->

    </div>
    <div>
    <h2 id="test-files">Generate test files</h2>

    <!-- Mandelbrot -->
    <p><label for="mandelbrot">Make a mandelbrot test pattern video</label>
    </p><div>
      <h5>Makes a mandelbrot test pattern video</h5>
      <p><code>ffmpeg -f lavfi -i mandelbrot=size=1280x720:rate=25 -c:v libx264 -t 10 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells FFmpeg to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i mandelbrot=size=1280x720:rate=25</dt><dd>asks for the <a href="https://ffmpeg.org/ffmpeg-filters.html#mandelbrot" target="_blank">mandelbrot test filter</a> as input. Adjusting the <code>size</code> and <code>rate</code> options allows you to choose a specific frame size and framerate.</dd>
        <dt>-c:v libx264</dt><dd>transcodes video from rawvideo to H.264. Set <code>-pix_fmt</code> to <code>yuv420p</code> for greater H.264 compatibility with media players.</dd>
        <dt>-t 10</dt><dd>specifies recording time of 10 seconds</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file. Try different file extensions such as mkv, mov, mp4, or avi.</dd>
      </dl>
      
    </div>
    <!-- ends Mandelbrot -->

    <!-- SMPTE bars -->
    <p><label for="smpte_bars">Make a SMPTE bars test pattern video</label>
    </p><div>
      <h3>Makes a SMPTE bars test pattern video</h3>
      <p><code>ffmpeg -f lavfi -i smptebars=size=720x576:rate=25 -c:v prores -t 10 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells FFmpeg to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i smptebars=size=720x576:rate=25</dt><dd>asks for the <a href="https://ffmpeg.org/ffmpeg-filters.html#allrgb_002c-allyuv_002c-color_002c-haldclutsrc_002c-nullsrc_002c-rgbtestsrc_002c-smptebars_002c-smptehdbars_002c-testsrc_002c-testsrc2_002c-yuvtestsrc" target="_blank">smptebars test filter</a> as input. Adjusting the <code>size</code> and <code>rate</code> options allows you to choose a specific frame size and framerate.</dd>
        <dt>-c:v prores</dt><dd>transcodes video from rawvideo to Apple ProRes 4:2:2.</dd>
        <dt>-t 10</dt><dd>specifies recording time of 10 seconds</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file. Try different file extensions such as mov or avi.</dd>
      </dl>
      
    </div>
    <!-- ends SMPTE bars -->

    <!-- Test pattern video -->
    <p><label for="test">Make a test pattern video</label>
    </p><div>
      <h5>Make a test pattern video</h5>
      <p><code>ffmpeg -f lavfi -i testsrc=size=720x576:rate=25 -c:v v210 -t 10 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells FFmpeg to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">libavfilter</a> input virtual device</dd>
        <dt>-i testsrc=size=720x576:rate=25</dt><dd>asks for the testsrc filter pattern as input. Adjusting the <code>size</code> and <code>rate</code> options allows you to choose a specific frame size and framerate.<br>
          The different test patterns that can be generated are listed <a href="https://ffmpeg.org/ffmpeg-filters.html#allrgb_002c-allyuv_002c-color_002c-haldclutsrc_002c-nullsrc_002c-rgbtestsrc_002c-smptebars_002c-smptehdbars_002c-testsrc_002c-testsrc2_002c-yuvtestsrc" target="_blank">here</a>.</dd>
        <dt>-c:v v210</dt><dd>transcodes video from rawvideo to 10-bit Uncompressed Y′C<sub>B</sub>C<sub>R</sub> 4:2:2. Alter this setting to set your desired codec.</dd>
        <dt>-t 10</dt><dd>specifies recording time of 10 seconds</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file. Try different file extensions such as mkv, mov, mp4, or avi.</dd>
      </dl>
      
    </div>
    <!-- ends Test pattern video -->

    <!-- Play HD SMPTE bars -->
    <p><label for="play_hd_smpte">Play HD SMPTE bars</label>
    </p><div>
      <h5>Play HD SMPTE bars</h5>
      <p>Test an HD video projector by playing the SMPTE color bars pattern.</p>
      <p><code>ffplay -f lavfi -i smptehdbars=size=1920x1080</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffplay to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i smptehdbars=size=1920x1080</dt><dd>asks for the <a href="https://ffmpeg.org/ffmpeg-filters.html#allrgb_002c-allyuv_002c-color_002c-haldclutsrc_002c-nullsrc_002c-rgbtestsrc_002c-smptebars_002c-smptehdbars_002c-testsrc_002c-testsrc2_002c-yuvtestsrc" target="_blank">smptehdbars filter pattern</a> as input and sets the HD resolution. This generates a color bars pattern, based on the SMPTE RP 219–2002.</dd>
      </dl>
      
    </div>
    <!-- ends Play HD SMPTE bars -->

    <!-- Play VGA SMPTE bars -->
    <p><label for="play_vga_smpte">Play VGA SMPTE bars</label>
    </p><div>
      <h5>Play VGA SMPTE bars</h5>
      <p>Test a VGA (SD) video projector by playing the SMPTE color bars pattern.</p>
      <p><code>ffplay -f lavfi -i smptebars=size=640x480</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffplay to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i smptebars=size=640x480</dt><dd>asks for the <a href="https://ffmpeg.org/ffmpeg-filters.html#allrgb_002c-allyuv_002c-color_002c-haldclutsrc_002c-nullsrc_002c-rgbtestsrc_002c-smptebars_002c-smptehdbars_002c-testsrc_002c-testsrc2_002c-yuvtestsrc" target="_blank">smptebars filter pattern</a> as input and sets the VGA (SD) resolution. This generates a color bars pattern, based on the SMPTE Engineering Guideline EG 1–1990.</dd>
      </dl>
      
    </div>
    <!-- ends Play VGA SMPTE bars -->

    <!-- Sine wave -->
    <p><label for="sine_wave">Generate a sine wave test audio file</label>
    </p><div>
      <h5>Sine wave</h5>
      <p>Generate a test audio file playing a sine wave.</p>
      <p><code>ffmpeg -f lavfi -i "sine=frequency=1000:sample_rate=48000:duration=5" -c:a pcm_s16le <em>output_file</em>.wav</code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells FFmpeg to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i "sine=frequency=1000:sample_rate=48000:duration=5"</dt><dd>Sets the signal to 1000 Hz, sampling at 48 kHz, and for 5 seconds</dd>
        <dt>-c:a pcm_s16le</dt><dd>encodes the audio codec in <code>pcm_s16le</code> (the default encoding for wav files). <code>pcm</code> represents pulse-code modulation format (raw bytes), <code>16</code> means 16 bits per sample, and <code>le</code> means "little endian"</dd>
        <dt><em>output_file</em>.wav</dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Sine wave -->

    <!-- SMPTE bars + Sine wave -->
    <p><label for="smpte_bars_and_sine_wave">SMPTE bars + Sine wave audio</label>
    </p><div>
      <h5>SMPTE bars + Sine wave audio</h5>
      <p>Generate a SMPTE bars test video + a 1kHz sine wave as audio testsignal.</p>
      <p><code>ffmpeg -f lavfi -i "smptebars=size=720x576:rate=25" -f lavfi -i "sine=frequency=1000:sample_rate=48000" -c:a pcm_s16le -t 10 -c:v ffv1 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells FFmpeg to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">libavfilter</a> input virtual device</dd>
        <dt>-i smptebars=size=720x576:rate=25</dt><dd>asks for the <a href="https://ffmpeg.org/ffmpeg-filters.html#allrgb_002c-allyuv_002c-color_002c-haldclutsrc_002c-nullsrc_002c-rgbtestsrc_002c-smptebars_002c-smptehdbars_002c-testsrc_002c-testsrc2_002c-yuvtestsrc" target="_blank">smptebars test filter</a> as input. Adjusting the <code>size</code> and <code>rate</code> options allows you to choose a specific frame size and framerate.</dd>
        <dt>-f lavfi</dt><dd>use libavfilter again, but now for audio</dd>
        <dt>-i "sine=frequency=1000:sample_rate=48000"</dt><dd>Sets the signal to 1000 Hz, sampling at 48 kHz.</dd>
        <dt>-c:a pcm_s16le</dt><dd>encodes the audio codec in <code>pcm_s16le</code> (the default encoding for wav files). <code>pcm</code> represents pulse-code modulation format (raw bytes), <code>16</code> means 16 bits per sample, and <code>le</code> means "little endian"</dd>
        <dt>-t 10</dt><dd>specifies recording time of 10 seconds</dd>
        <dt>-c:v ffv1</dt><dd>Encodes to <a href="https://en.wikipedia.org/wiki/FFV1" target="_blank">FFV1</a>. Alter this setting to set your desired codec.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends SMPTE bars + Sine wave -->

    <!-- Broken File -->
    <p><label for="broken_file">Make a broken file</label>
    </p><div>
      <h5>Makes a broken test file</h5>
      <p>Modifies an existing, functioning file and intentionally breaks it for testing purposes.</p>
      <p><code>ffmpeg -i <em>input_file</em> -bsf noise=1 -c copy <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>takes in a normal file</dd>
        <dt>-bsf noise=1</dt><dd>sets bitstream filters for all to 'noise'. Filters can be set on specific filters using syntax such as <code>-bsf:v</code> for video, <code>-bsf:a</code> for audio, etc. The <a href="https://ffmpeg.org/ffmpeg-bitstream-filters.html#noise" target="_blank">noise filter</a> intentionally damages the contents of packets without damaging the container. This sets the noise level to 1 but it could be left blank or any number above 0.</dd>
        <dt>-c copy</dt><dd>use stream copy mode to re-mux instead of re-encode</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Broken File -->

    <!-- Game of Life -->
    <p><label for="game_of_life">Conway's Game of Life</label>
    </p><div>
      <h5>Conway's Game of Life</h5>
      <p>Simulates <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life" target="_blank">Conway's Game of Life</a></p>
      <p><code>ffplay -f lavfi life=s=300x200:mold=10:r=60:ratio=0.1:death_color=#c83232:life_color=#00ff00,scale=1200:800</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffplay to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>life=s=300x200</dt><dd>use the life filter and set the size of the video to 300x200</dd>
        <dt>:</dt><dd>indicates there’s another parameter coming</dd>
        <dt>mold=10:r=60:ratio=0.1</dt><dd>sets up the rules of the game: cell mold speed, video rate, and random fill ratio</dd>
        <dt>:</dt><dd>indicates there’s another parameter coming</dd>
        <dt>death_color=#c83232:life_color=#00ff00</dt><dd>specifies color for cell death and cell life; mold_color can also be set</dd>
        <dt>,</dt><dd>comma signifies closing of video source assertion and ready for filter assertion</dd>
        <dt>scale=1200:800</dt><dd>scale to 1280 width and 800 height</dd>
      </dl>
      <p><img src="https://amiaopensource.github.io/ffmprovisr/img/life.gif" alt="GIF of above command"></p><p>To save a portion of the stream instead of playing it back infinitely, use the following command:</p>
      <p><code>ffmpeg -f lavfi -i life=s=300x200:mold=10:r=60:ratio=0.1:death_color=#c83232:life_color=#00ff00,scale=1200:800 -t 5 <em>output_file</em></code></p>
      
    </div>
    <!-- ends Game of Life -->

    </div>
    <div>
    <h2 id="ocr">Use OCR</h2>

    <!-- Show OCR -->
    <p><label for="ocr_on_top">Play video with OCR</label>
    </p><div>
      <h5>Plays video with OCR on top</h5>
      <p><code>ffplay input_file -vf "ocr,drawtext=fontfile=/Library/Fonts/Andale Mono.ttf:text=%{metadata\\\:lavfi.ocr.text}:fontcolor=white"</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-vf</dt><dd>creates a filtergraph to use for the streams</dd>
        <dt>"</dt><dd>quotation mark to start filtergraph</dd>
        <dt>ocr,</dt><dd>tells ffplay to use ocr as source and the comma signifies that the script is ready for filter assertion</dd>
        <dt>drawtext=fontfile=/Library/Fonts/Andale Mono.ttf</dt><dd>tells ffplay to drawtext and use a specific font (Andale Mono) when doing so</dd>
        <dt>:</dt><dd>indicates there’s another parameter coming</dd>
        <dt>text=%{metadata\\\:lavfi.ocr.text}</dt><dd>tells ffplay what text to use when playing. In this case, calls for metadata that lives in the lavfi.ocr.text library</dd>
        <dt>:</dt><dd>indicates there’s another parameter coming</dd>
        <dt>fontcolor=white</dt><dd>specifies font color as white</dd>
        <dt>"</dt><dd>quotation mark to end filtergraph</dd>
      </dl>
      
    </div>
    <!-- ends Show OCR -->

    <!-- Export OCR -->
    <p><label for="ffprobe_ocr">Export OCR from video to screen</label>
    </p><div>
      <h5>Exports OCR data to screen</h5>
      <p><code>ffprobe -show_entries frame_tags=lavfi.ocr.text -f lavfi -i "movie=<em>input_file</em>,ocr"</code></p>
      <dl>
        <dt>ffprobe</dt><dd>starts the command</dd>
        <dt>-show_entries</dt><dd>sets a list of entries to show</dd>
        <dt>frame_tags=lavfi.ocr.text</dt><dd>shows the <em>lavfi.ocr.text</em> tag in the frame section of the video</dd>
        <dt>-f lavfi</dt><dd>tells ffprobe to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter input virtual device</a></dd>
        <dt>-i "movie=<em>input_file</em>,ocr"</dt><dd>declares 'movie' as <em>input_file</em> and passes in the 'ocr' command</dd>
      </dl>
      
    </div>
    <!-- ends Exports OCR -->

    </div>
    <div>
    <h2 id="perceptual-similarity">Compare perceptual similarity of videos</h2>

    <!-- Compare Video Fingerprints -->
    <p><label for="compare_video_fingerprints">Compare Video Fingerprints</label>
    </p><div>
      <h5>Compare two video files for content similarity using perceptual hashing</h5>
      <p><code>ffmpeg -i <em>input_one</em> -i <em>input_two</em> -filter_complex signature=detectmode=full:nb_inputs=2 -f null -</code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_one</em> -i <em>input_two</em></dt><dd>assigns the input files</dd>
        <dt>-filter_complex</dt><dd>enables using more than one input file to the filter</dd>
        <dt>signature=detectmode=full</dt><dd>Applies the signature filter to the inputs in 'full' mode. The other option is 'fast'.</dd>
        <dt>nb_inputs=2</dt><dd>tells the filter to expect two input files</dd>
        <dt>-f null -</dt><dd>Sets the output of FFmpeg to a null stream (since we are not creating a transcoded file, just viewing metadata).</dd>
      </dl>
      
    </div>
    <!-- ends Compare Video Fingerprints -->

    <!-- Generate Video Fingerprint -->
    <p><label for="generate_video_fingerprint">Generate Video Fingerprint</label>
    </p><div>
      <h5>Generate a perceptual hash for an input video file</h5>
      <p><code>ffmpeg -i <em>input</em> -vf signature=format=xml:filename="output.xml" -an -f null -</code></p>
      <dl>
        <dt>ffmpeg -i <em>input</em></dt><dd>starts the command using your input file</dd>
        <dt>-vf signature=format=xml</dt><dd>applies the signature filter to the input file and sets the output format for the fingerprint to xml</dd>
        <dt>filename="output.xml"</dt><dd>sets the output for the signature filter</dd>
        <dt>-an</dt><dd>tells FFmpeg to ignore the audio stream of the input file</dd>
        <dt>-f null -</dt><dd>Sets the FFmpeg output to a null stream (since we are only interested in the output generated by the filter).</dd>
      </dl>
      
    </div>
    <!-- ends Generate Video Fingerprint -->

    </div>
    <div>
      <h2 id="other">Other</h2>

      <!-- Play image sequence -->
      <p><label for="play_im_seq">Play an image sequence</label>
      </p><div>
        <h5>Play an image sequence</h5>
        <p>Play an image sequence directly as moving images, without having to create a video first.</p>
        <p><code>ffplay -framerate 5 <em>input_file_%06d.ext</em></code></p>
        <dl>
          <dt>ffplay</dt><dd>starts the command</dd>
          <dt>-framerate 5</dt><dd>plays image sequence at rate of 5 images per second<br>
          <strong>Note:</strong> this low framerate will produce a slideshow effect.</dd>
          <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file<br>
          This must match the naming convention used! The regex %06d matches six-digit-long numbers, possibly with leading zeroes. This allows the full sequence to be read in ascending order, one image after the other.<br>
          The extension for TIFF files is .tif or maybe .tiff; the extension for DPX files is .dpx (or even .cin for old files). Screenshots are often in .png format.</dd>
        </dl>
        <p><strong>Notes:</strong></p>
        <p>If <code>-framerate</code> is omitted, the playback speed depends on the images’ file sizes and on the computer’s processing power. It may be rather slow for large image files.</p>
        <p>You can navigate durationally by clicking within the playback window. Clicking towards the left-hand side of the playback window takes you towards the beginning of the playback sequence; clicking towards the right takes you towards the end of the sequence.</p>
        
      </div>
      <!-- ends Play image sequence -->

      <!-- Split audio and video tracks -->
      <p><label for="split_audio_video">Split audio and video tracks</label>
      </p><div>
        <h5>Split audio and video tracks</h5>
        <p><code>ffmpeg -i <em>input_file</em> -map 0:v:0 <em>video_output_file</em> -map 0:a:0 <em>audio_output_file</em></code></p>
        <p>This command splits the original input file into a video and audio stream. The -map command identifies which streams are mapped to which file. To ensure that you’re mapping the right streams to the right file, run ffprobe before writing the script to identify which streams are desired.</p>
        <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
          <dt>-map 0:v:0</dt><dd>grabs the first video stream and maps it into:</dd>
          <dt><em>video_output_file</em></dt><dd>path, name and extension of the video output file</dd>
          <dt>-map 0:a:0</dt><dd>grabs the first audio stream and maps it into:</dd>
          <dt><em>audio_output_file</em></dt><dd>path, name and extension of the audio output file</dd>
        </dl>
        
      </div>
      <!-- ends Split audio and video tracks -->

      <!-- Merge audio and video tracks -->
      <p><label for="merge_audio_video">Merge audio and video tracks</label>
      </p><div>
        <h3>Merge audio and video tracks</h3>
        <p><code>ffmpeg -i <em>video_file</em> -i <em>audio_file</em> -map 0:v -map 1:a -c copy <em>output_file</em></code></p>
        <p>This command takes a video file and an audio file as inputs, and creates an output file that combines the video stream in the first file with the audio stream in the second file.</p>
        <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-i <em>video_file</em></dt><dd>path, name and extension of the first input file (the video file)</dd>
          <dt>-i <em>audio_file</em></dt><dd>path, name and extension of the second input file (the audio file)</dd>
          <dt>-map <em>0:v</em></dt><dd>selects the video streams from the first input file</dd>
          <dt>-map <em>1:a</em></dt><dd>selects the audio streams from the second input file</dd>
          <dt>-c copy</dt><dd>copies streams without re-encoding</dd>
          <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
        </dl>
        <p><strong>Note:</strong> in the example above, the video input file is given prior to the audio input file. However, input files can be added any order, as long as they are indexed correctly when stream mapping with <code>-map</code>. See the entry on <a href="#stream-mapping">stream mapping</a>.</p>
        <h4>Variation:</h4>
        <p>Include the audio tracks from both input files with the following command:</p>
        <p><code>ffmpeg -i <em>video_file</em> -i <em>audio_file</em> -map 0:v -map 0:a -map 1:a -c copy <em>output_file</em></code></p>
        
      </div>
      <!-- ends Merge audio and video tracks -->

      <!-- Create ISO -->
      <p><label for="create_iso">Create ISO files for DVD access</label>
      </p><div>
        <h5>Create ISO files for DVD access</h5>
        <p>Create an ISO file that can be used to burn a DVD. Please note, you will have to install dvdauthor. To install dvd author using Homebrew run: <code>brew install dvdauthor</code></p>
        <p><code>ffmpeg -i <em>input_file</em> -aspect <em>4:3</em> -target ntsc-dvd <em>output_file</em>.mpg</code></p>
        <p>This command will take any file and create an MPEG file that dvdauthor can use to create an ISO.</p>
        <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
          <dt>-aspect 4:3</dt><dd>declares the aspect ratio of the resulting video file. You can also use 16:9.</dd>
          <dt>-target ntsc-dvd</dt><dd>specifies the region for your DVD. This could be also pal-dvd.</dd>
          <dt><em>output_file</em>.mpg</dt><dd>path and name of the output file. The extension must be <code>.mpg</code></dd>
        </dl>
        
      </div>
      <!-- ends Create ISO -->

      <!-- Scene Detection using YDIF -->
      <p><label for="csv-ydif">CSV with timecodes and YDIF</label>
      </p><div>
        <h5>Exports CSV for scene detection using YDIF</h5>
        <p><code>ffprobe -f lavfi -i movie=<em>input_file</em>,signalstats -show_entries frame=pkt_pts_time:frame_tags=lavfi.signalstats.YDIF -of csv</code></p>
        <p>This ffprobe command prints a CSV correlating timestamps and their YDIF values, useful for determining cuts.</p>
        <dl>
          <dt>ffprobe</dt><dd>starts the command</dd>
          <dt>-f lavfi</dt><dd>uses the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter input virtual device</a> as chosen format</dd>
          <dt>-i movie=<em>input file</em></dt><dd>path, name and extension of the input video file</dd>
          <dt>,</dt><dd>comma signifies closing of video source assertion and ready for filter assertion</dd>
          <dt>signalstats</dt><dd>tells ffprobe to use the signalstats command</dd>
          <dt>-show_entries</dt><dd>sets list of entries to show per column, determined on the next line</dd>
          <dt>frame=pkt_pts_time:frame_tags=lavfi.signalstats.YDIF</dt><dd>specifies showing the timecode (<code>pkt_pts_time</code>) in the frame stream and the YDIF section of the frame_tags stream</dd>
          <dt>-of csv</dt><dd>sets the output printing format to CSV. <code>-of</code> is an alias of <code>-print_format</code>.</dd>
        </dl>
        
      </div>
      <!-- ends sample Scene Detection using YDIF -->

      <!-- Cover head switching noise -->
      <p><label for="cover_head">Cover head switching noise</label>
      </p><div>
        <h5>Cover head switching noise</h5>
        <p><code>ffmpeg -i <em>input_file</em> -filter:v drawbox=w=iw:h=7:y=ih-h:t=max <em>output_file</em></code></p>
        <p>This command will draw a black box over a small area of the bottom of the frame, which can be used to cover up head switching noise.</p>
        <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
          <dt>-filter:v drawbox=</dt>
          <dd>This calls the drawtext filter with the following options:
            <dl>
              <dt>w=in_w</dt><dd>Width is set to the input width. Shorthand for this command would be w=iw</dd>
              <dt>h=7</dt><dd>Height is set to 7 pixels.</dd>
              <dt>y=ih-h</dt><dd>Y represents the offset, and ih-h sets it to the input height minus the height declared in the previous parameter, setting the box at the bottom of the frame.</dd>
              <dt>t=max</dt><dd>T represents the thickness of the drawn box. Default is 3.</dd>
            </dl>
          </dd>
          <dt><em>output_file</em></dt><dd>path and name of the output file</dd>
        </dl>
        
      </div>
      <!-- ends Cover head switching noise -->

      <!-- Record and live-stream simultaneously -->
      <p><label for="record-and-stream">Record and live-stream simultaneously</label>
      </p><div>
        <h5>Record and live-stream simultaneously</h5>
        <p><code>ffmpeg -re -i <em>${INPUTFILE}</em> -map 0 -flags +global_header -vf scale="1280:-1,format=yuv420p" -pix_fmt yuv420p -level 3.1 -vsync passthrough -crf 26 -g 50 -bufsize 3500k -maxrate 1800k -c:v libx264 -c:a aac -b:a 128000 -r:a 44100 -ac 2 -t ${STREAMDURATION} -f tee <em>"[movflags=+faststart]${TARGETFILE}|[f=flv]${STREAMTARGET}"</em></code></p>
        <p>I use this script to stream to a RTMP target and record the stream locally as .mp4 with only one ffmpeg-instance.</p>
        <p>As input, I use <code>bmdcapture</code> which is piped to ffmpeg. But it can also be used with a static videofile as input.</p>
        <p>The input will be scaled to 1280px width, maintaining height. Also the stream will stop after a given time (see <code>-t</code> option.)</p>
        <h4>Notes</h4>
        <ol>
          <li>I recommend to use this inside a shell script - then you can define the variables <code>${INPUTFILE}</code>, <code>${STREAMDURATION}</code>, <code>${TARGETFILE}</code>, and <code>${STREAMTARGET}</code>.</li>
          <li>This is in daily use to live-stream a real-world TV show. No errors for nearly 4 years. Some parameters were found by trial-and-error or empiric testing. So suggestions/questions are welcome.</li>
        </ol>
        <dl>
           <dt>ffmpeg</dt><dd>starts the command</dd>
           <dt>-re</dt><dd>Read input at native framerate</dd>
           <dt>-i input.mov</dt><dd>The input file. Can also be a <code>-</code> to use STDIN if you pipe in from webcam or SDI.</dd>
           <dt>-map 0</dt><dd>map ALL streams from input file to output</dd>
           <dt>-flags +global_header</dt><dd>Don't place extra data in every keyframe</dd>
           <dt>-vf scale="1280:-1"</dt><dd>Scale to 1280 width, maintain aspect ratio.</dd>
           <dt>-pix_fmt yuv420p</dt><dd>convert to 4:2:0 chroma subsampling scheme</dd>
           <dt>-level 3.1</dt><dd>H.264 Level (defines some thresholds for bitrate)</dd>
           <dt>-vsync passthrough</dt><dd>Each frame is passed with its timestamp from the demuxer to the muxer.</dd>
           <dt>-crf 26</dt><dd>Constant rate factor - basically the quality</dd>
           <dt>-g 50</dt><dd>GOP size.</dd>
           <dt>-bufsize 3500k</dt><dd>Ratecontrol buffer size (~ maxrate x2)</dd>
           <dt>-maxrate 1800k</dt><dd>Maximum bit rate</dd>
           <dt>-c:v libx264</dt><dd>encode output video stream as H.264</dd>
           <dt>-c:a aac</dt><dd>encode output audio stream as AAC</dd>
           <dt>-b:a 128000</dt><dd>The audio bitrate</dd>
           <dt>-r:a 44100</dt><dd>The audio samplerate</dd>
           <dt>-ac 2</dt><dd>Two audio channels</dd>
           <dt>-t ${STREAMDURATION}</dt><dd>Time (in seconds) after which the stream should automatically end.</dd>
           <dt>-f tee</dt><dd>Use multiple outputs. Outputs defined below.</dd>
           <dt>"[movflags=+faststart]target-file.mp4|[f=flv]rtmp://stream-url/stream-id"</dt><dd>The outputs, separated by a pipe (|). The first is the local file, the second is the live stream. Options for each target are given in square brackets before the target.</dd>
        </dl>
        
      </div>
      <!-- END Record and live-stream at the same time -->

      <!-- View Subprogram Info -->
      <p><label for="view_subprogram_info">View FFmpeg subprogram information</label>
      </p><div>
        <h5>View information about a specific decoder, encoder, demuxer, muxer, or filter</h5>
        <p><code>ffmpeg -h <em>type=name</em></code></p>
        <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-h</dt><dd>Call the help option</dd>
          <dt>type=name</dt>
          <dd>tells FFmpeg which kind of option you want, for example:
            <ul>
              <li><code>encoder=libx264</code></li>
              <li><code>decoder=mp3</code></li>
              <li><code>muxer=matroska</code></li>
              <li><code>demuxer=mov</code></li>
              <li><code>filter=crop</code></li>
            </ul>
          </dd>
        </dl>
        
      </div>
      <!-- ends View Subprogram info -->
    </div>

    <div>
      
      <p>This section introduces and explains the usage of some additional command line tools similar to FFmpeg for use in digital preservation workflows (and beyond!).</p>
    </div>

    <div>
      <h2 id="cdda">CDDA (Audio CD) Ripping Tools</h2>
      <!-- Find Drive Offset for Exact CD Ripping -->
      <p><label for="find-offset">Find Drive Offset for Exact CD Ripping</label>
      </p><div>
        <h5>Find Drive Offset for Exact CD Ripping</h5>
        <p>If you want to make CD rips that can be verified via checksums to other rips of the same content, you need to know the offset of your CD drive. Put simply, different models of CD drives have different offsets, meaning they start reading in slightly different locations. This must be compensated for in order for files created on different (model) drives to generate the same checksum. For a more detailed explanation of drive offsets see the explanation <a href="https://dbpoweramp.com/spoons-audio-guide-cd-ripping.htm" target="_blank">here.</a> In order to find your drive offset, first you will need to know exactly what model your drive is, then you can look it up in the list of drive offsets by Accurate Rip.</p>
        <p>Often it can be difficult to tell what model your drive is simply by looking at it - it may be housed inside your computer or have external branding that is different from the actual drive manufacturer. For this reason, it can be useful to query your drive with CD ripping software in order to ID it. The following commands should give you a better idea of what drive you have.</p>
        <p><strong>Cdda2wav:</strong> <code>cdda2wav -scanbus</code> or simply <code>cdda2wav</code></p>
        <p><strong>CD Paranoia:</strong> <code>cdparanoia -vsQ</code></p>
        <p>Once you have IDed your drive, you can search the <a href="http://www.accuraterip.com/driveoffsets.htm">Accurate Rip CD drive offset list</a> to find the correct offset for your drive as sourced by the community.</p>
        <p><strong>Note:</strong> A very effective GUI based tool (macOS specific) for both for discovering drive offset as well as accurately ripping CDDAs is <a href="https://tmkk.undo.jp/xld/index_e.html">XLD</a>. Instructions for calibrating XLD can be found at <a href="https://wiki.hydrogenaud.io/index.php?title=XLD_Configuration">this page</a>.</p>
        
      </div>
      <!-- Find Drive Offset for Exact CD Ripping -->

      <!-- Rip with CD Paranoia -->
        <p><label for="cdparanoia">Rip a CD with CD Paranoia</label>
        </p><div>
          <h5>Rip a CD with CD Paranoia</h5>
          <p><code>cdparanoia -L -B -O <em>[Drive Offset]</em> <em>[Starting Track Number]</em>-<em>[Ending Track Number]</em> <em>output_file.wav</em></code></p>
          <p>This command will use CD Paranoia to rip a CD into separate tracks while compensating for the sample offset of the CD drive. (For more information about drive offset see <a href="#find-offset">the related ffmprovisr command.</a>)</p>
          <dl>
            <dt>cdparanoia</dt><dd>begins the cdparanoia command.</dd>
            <dt>-L</dt><dd>creates verbose logfile.</dd>
            <dt>-B</dt><dd>puts CD Paranoia into 'batch' mode, which will automatically split tracks into separate files.</dd>
            <dt>-O [Drive Offset]</dt><dd>allows you to specify the sample offset of your drive. Skip this flag to rip without offset correction.</dd>
            <dt>[Starting Track Number]-[Ending Track Number]</dt><dd>specifies which tracks to write. For example <code>1-4</code> would rip tracks one through four.</dd>
            <dt><em>output_file.wav</em></dt><dd>the desired name for your output file(s) (for example the CD name). CD Paranoia will prepend this with track numbers.</dd>
          </dl>
          
        </div>
      <!-- ends Rip with CD Paranoia -->

      <!-- Rip with CDDA2WAV -->
      <p><label for="cdda2wav">Rip a CD with Cdda2wav</label>
      </p><div>
        <h5>Rip a CD with Cdda2wav</h5>
        <p><code>cdda2wav -L0 -t all -cuefile -paranoia paraopts=retries=200,readahead=600,minoverlap=sectors-per-request-1 -verbose-level all <em>output.wav</em></code></p>
        <p>Cdda2wav is a tool that uses the <a href="https://www.xiph.org/paranoia/">Paranoia library</a> to facilitate accurate ripping of audio CDs (CDDA). It can be installed via Homebrew with the command <code> brew install cdrtools</code>. This command will accurately rip an audio CD into a single wave file, while querying the CDDB database for track information and creating a cue sheet. This cue sheet can then be used either for playback of the WAV file or to split it into individual access files. Any <a href="https://en.wikipedia.org/wiki/CD-Text">cdtext</a> information that is discovered will be stored as a sidecar. For more information about cue sheets see <a href="https://en.wikipedia.org/wiki/Cue_sheet_(computing)">this Wikipedia article.</a></p>
        <p><strong>Notes: </strong>On macOS the CD must be unmounted before this command is run. This can be done with the command <code>sudo umount '/Volumes/Name_of_CD'</code></p>
        <p>As of writing, when using the default Homebrew installed version of Cdda2wav some drives will falsely report errors on all rips. If this is occurring, a possible solution is to use the command <code>brew install --devel cdrtools</code> to install a more recent build that incorporates a fix.</p>
        <dl>
          <dt>cdda2wav</dt><dd>begins the Cdda2wav command</dd>
          <dt>-L0</dt><dd>tells Cdda2wav to query the CDDB database for track name information. L0 is 'interactive mode' meaning Cdda2wav will ask you to confirm choices in the event of multiple matches. Change this to <code>-L1</code> to automatically select the first database match.</dd>
          <dt>-t all</dt><dd>tells Cdda2wav to rip the entire CD to one file</dd>
          <dt>-cuefile</dt><dd>tells Cdda2wav to create a cue file of CD contents</dd>
          <dt>-paranoia</dt><dd>enables the Paranoia library for ripping</dd>
          <dt>paraopts=retries=200,readahead=600,minoverlap=sectors-per-request-1</dt><dd>configures ripping to a generically conservative setting for retries and caching. These values were taken from the Cdda2wav man file and can be changed depending on needs, such as for more/less retry attempts. For more information see the Cdda2wav man file (also available online <a href="https://linux.die.net/man/1/cdda2wav">here)</a>.</dd>
          <dt>-verbose-level all</dt><dd>sets terminal information to the most verbose view</dd>
          <dt><em>output.wav</em></dt><dd>the desired name for your output file (for example the CD name).</dd>
        </dl>
        
      </div>
      <!-- ends Rip with CDDA2WAV -->

      <!-- Check for CD Emphasis -->
        <p><label for="cd-emph-check">Check/Compensate for CD Emphasis</label>
        </p><div>
          <h5>Check/Compensate for CD Emphasis</h5>
          <p>While somewhat rare, certain CDs had 'emphasis' applied as a form of noise reduction. This seems to mostly affect early (1980s) era CDs and some CDs pressed in Japan. Emphasis is part of the <a href="https://en.wikipedia.org/wiki/Compact_Disc_Digital_Audio#Standard">Red Book standard</a> and, if present, must be compensated for to ensure accurate playback. CDs that use emphasis contain flags on tracks that tell the CD player to de-emphasize the audio on playback. When ripping a CD with emphasis, it is important to take this into account and either apply de-emphasis while ripping, or if storing a 'flat' copy, create another de-emphasized listening copy.</p>
          <p>The following commands will output information about the presence of emphasis when run on a target CD:</p>
          <p><strong>Cdda2wav:</strong> <code>cdda2wav -J</code></p>
          <p><strong>CD Paranoia:</strong> <code>cdparanoia -Q</code></p>
          <p>In order to compensate for emphasis during ripping while using Cdda2wav, the <code>-T</code> flag can be added to the <a href="#cdda2wav">standard ripping command</a>. For a recipe to compensate for a flat rip, see the section on <a href="#cd_eq">de-emphasizing with FFmpeg</a>.
          </p>
        </div>
      <!-- Check for CD Emphasis -->
    </div>
    <!-- ends CDDA Tools -->

    <div>
      <h2 id="imagemagick">ImageMagick</h2>

      <!-- About ImageMagick -->
      <p><label for="im-basics">About ImageMagick</label>
      </p><div>
        <h5>About ImageMagick</h5>
        <p>ImageMagick is a free and open-source software suite for displaying, converting, and editing raster image and vector image files.</p>
        <p>It's official website can be found <a href="https://www.imagemagick.org/script/index.php" target="_blank">here</a>.</p>
        <p>Another great resource with lots of supplemental explanations of filters is available at <a href="http://www.fmwconcepts.com/imagemagick/index.php" target="_blank">Fred's ImageMagick Scripts</a>.</p>
        <p>Unlike many other command line tools, ImageMagick isn't summoned by calling its name. Rather, ImageMagick installs links to several more specific commands: <code>convert</code>, <code>montage</code>, and <code>mogrify</code>, to name a few.</p>
        
      </div>
      <!-- End About ImageMagick -->

      <!-- Compare two images -->
      <p><label for="im_compare">Compare two images</label>
      </p><div>
        <h5>Compare two images</h5>
        <p><code>compare -metric ae <em>image1.ext image2.ext</em> null:</code></p>
        <p>Compares two images to each other.</p>
        <dl>
          <dt>compare</dt><dd>starts the command</dd>
          <dt>-metric ae</dt><dd>applies the absolute error count metric, returning the number of different pixels. <a href="https://www.imagemagick.org/script/command-line-options.php#metric" target="_blank">Other parameters</a> are available for image comparison.</dd>
          <dt><em>image1.ext image2.ext</em></dt><dd>takes two images as input</dd>
          <dt>null:</dt><dd>throws away the comparison image that would be generated</dd>
        </dl>
        
      </div>
      <!-- ends Compare two images -->

      <!-- Create thumbnails -->
      <p><label for="im_thumbs">Create thumbnails of images</label>
      </p><div>
        <h5>Create thumbnails</h5>
        <p>Creates thumbnails for all files in a folder and saves them in that folder.</p>
        <p><code>mogrify -resize 80x80 -format jpg -quality 75 -path thumbs *.jpg</code></p>
        <dl>
          <dt>montage</dt><dd>starts the command</dd>
          <dt>-resize 80x80</dt><dd>resizes copies of original images to 80x80 pixels</dd>
          <dt>-format jpg</dt><dd>reformats original images to jpg</dd>
          <dt>-quality 75</dt><dd>sets quality to 75 (out of 100), adding light compression to smaller files</dd>
          <dt>-path thumbs</dt><dd>specifies where to save the thumbnails -- this goes to a folder within the active folder called "thumbs".<br>
          Note: You will have to make this folder if it doesn't already exist.</dd>
          <dt><em>*.jpg</em></dt><dd>The asterisk acts as a "wildcard" to be applied to every file in the directory.</dd>
        </dl>
        
      </div>
      <!-- ends Create thumbnails -->

      <!-- Create grid of images -->
      <p><label for="im_grid">Creates grid of images from text file</label>
      </p><div>
        <h3>Create grid of images</h3>
        <p><code>montage @<em>list.txt</em> -tile 6x12 -geometry +0+0 <em>output_grid.jpg</em></code></p>
        <dl>
          <dt>montage</dt><dd>starts the command</dd>
          <dt>@list.txt</dt><dd>path and name of a text file containing a list of filenames, one per each line</dd>
          <dt>-tile 6x12</dt><dd>specifies the dimensions of the proposed grid (6 images wide, 12 images long)</dd>
          <dt>-geometry +0+0</dt><dd>specifies to include no spacing around any of the tiles; they will be flush against each other</dd>
          <dt><em>output_grid.jpg</em></dt><dd>path and name of the output file</dd>
        </dl>
        
      </div>
      <!-- ends Create grid of images -->

      <!-- Get file signature data -->
      <p><label for="im_sig_data">Get file signature data</label>
      </p><div>
        <h5>Get file signature data</h5>
        <p><code>convert -verbose <em>input_file.ext</em> | grep -i signature </code></p>
        <p>Gets signature data from an image file, which is a hash that can be used to uniquely identify the image.</p>
        <dl>
          <dt>convert</dt><dd>starts the command</dd>
          <dt>-verbose</dt><dd>sets verbose flag for collecting the most data</dd>
          <dt><em>input_file.ext</em></dt><dd>path and name of image file</dd>
          <dt>|</dt><dd>pipe the data into something else</dd>
          <dt>grep</dt><dd>starts the grep command</dd>
          <dt>-i signature</dt><dd>ignore case and search for the phrase "signature"</dd>
        </dl>
        
      </div>
      <!-- ends Get file signature data -->

      <!-- Remove exif data -->
      <p><label for="im_strip">Removes exif metadata</label>
      </p><div>
        <h5>Remove exif data</h5>
        <p><code>mogrify -path ./stripped/ -strip *.jpg</code></p>
        <p>Removes (strips) exif data and moves clean files to a new folder.</p>
        <dl>
          <dt>mogrify</dt><dd>starts the command</dd>
          <dt>-path ./stripped/</dt><dd>sets directory within current directory called "stripped"</dd>
          <dt>-strip</dt><dd>removes exif metadata</dd>
          <dt>*.jpg</dt><dd>applies command to all .jpgs in current folder</dd>
        </dl>
        
      </div>
      <!-- ends Remove exif data -->

      <!-- Resize to width -->
      <p><label for="im_resize">Resizes image to specific pixel width</label>
      </p><div>
        <h5>Resize to width</h5>
        <p><code>convert <em>input_file.ext</em> -resize 750 <em>output_file.ext</em></code></p>
        <p>This script will also convert the file format, if the output has a different file extension than the input.</p>
        <dl>
          <dt>convert</dt><dd>starts the command</dd>
          <dt>-i <em>input_file.ext</em></dt><dd>path and name of the input file</dd>
          <dt>-resize 750</dt><dd>resizes the image to 750 pixels wide, retaining aspect ratio</dd>
          <dt><em>output_file.ext</em></dt><dd>path and name of the output file</dd>
        </dl>
        
      </div>
      <!-- ends Resize to width -->
    </div>
    <div>
      <h2 id="flac">flac</h2>
      <!-- flac tool -->
      <p><label for="flac-tool">Transcoding to/from FLAC</label>
      </p><div>
        <h5>About flac tool</h5>
        <p>The flac tool is the tool created by the FLAC project to transcode to/from FLAC and to manipulate metadata in FLAC files. One advantage it has over other tools used to transcode into FLAC is the capability of embedding foreign metadata (such as BWF metadata). This means that it is possible to compress a BWF file into FLAC and maintain the ability to transcode back into an identical BWF, metadata and all. For a more detailed explanation, see <a href="http://dericed.com/2013/flac-in-the-archives/" target="_blank">Dave Rice's article</a> on the topic, from which the following commands are adapted.</p>
        <h3>Transcode to FLAC</h3>
        <p>Use this command to transcode from WAV to FLAC while maintaining BWF metadata</p>
        <p><code>flac --best --keep-foreign-metadata --preserve-modtime --verify <em>input.wav</em></code></p>
        <dl>
          <dt>flac</dt><dd>starts the command</dd>
          <dt>-i <em>input_file.ext</em></dt><dd>path and name of the input file</dd>
          <dt>--best</dt><dd>sets the file for the most efficient compression (resulting in a smaller file at the expense of a slower process).</dd>
          <dt>--keep-foreign-metadata</dt><dd>tells the flac tool to maintain original metadata within the FLAC file.</dd>
          <dt>--preserve-modtime</dt><dd>preserves the file timestamps of the input file.</dd>
          <dt>--verify</dt><dd>verifies the validity of the output file.</dd>
        </dl>
        <h3>Transcode from FLAC</h3>
        <p>Use this command to transcode from FLAC to reconstruct original BWF file. Command is the same as the prior command with the exception of substituting <code>--decode</code> for <code>best</code> and changing the input to a <code>.flac</code> file.</p>
        <p><code>flac --decode --keep-foreign-metadata --preserve-modtime --verify <em>input.flac</em></code></p>
        
      </div>
    </div>
    <!-- End About flac -->
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How the Rich Reap Huge Tax Breaks From Private Nonprofits (271 pts)]]></title>
            <link>https://www.propublica.org/article/how-private-nonprofits-ultrawealthy-tax-deductions-museums-foundation-art</link>
            <guid>36929335</guid>
            <pubDate>Sun, 30 Jul 2023 09:14:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.propublica.org/article/how-private-nonprofits-ultrawealthy-tax-deductions-museums-foundation-art">https://www.propublica.org/article/how-private-nonprofits-ultrawealthy-tax-deductions-museums-foundation-art</a>, See on <a href="https://news.ycombinator.com/item?id=36929335">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pp-location="article body">

                    <a href="https://www.propublica.org/series/the-secret-irs-files">
                                                        

                    <img width="400" height="267" sizes="(min-width: 1520px) 257px, (min-width: 1260px) calc(20vw - 10px), (min-width: 960px) calc(20vw - 50px), (min-width: 780px) calc(25vw - 40px), 96px" alt="" srcset="https://img.assets-c3.propublica.org/images/series/series-3x2.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=133&amp;q=75&amp;w=200&amp;s=f7f84662589016263acf56bb7d4a7d56 200w, https://img.assets-c3.propublica.org/images/series/series-3x2.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=75&amp;w=400&amp;s=d49a219703a3db1bf64aa501170d3481 400w, https://img.assets-c3.propublica.org/images/series/series-3x2.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=400&amp;q=75&amp;w=600&amp;s=591a47372f5143f8e18a218b59a8f9ab 600w" loading="lazy">
                                
            </a>
        
                    <div data-pp-location="top-note">
                

                                                
            <p>ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up to receive <a href="https://www.propublica.org/newsletters/the-big-story?source=www.propublica.org&amp;placement=top-note&amp;region=national">our biggest stories</a> as soon as they’re published.</p>

                

            </div><!-- end .article-body__top-notes -->
        
        




                    
<p data-pp-blocktype="copy" data-pp-id="1.0">Once a week, a little past noon on Wednesdays, a line of cars forms outside the wrought-iron gates of the Carolands mansion, 20 miles south of downtown San Francisco. From the entrance, you can see the southeast facade of the 98-room Beaux Arts chateau, which was built a century ago by an heiress to the Pullman railroad-car fortune. Not visible from that vantage point is the stately reflecting pool, or the gardens, whose original designer took inspiration from Versailles.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="2.0">I was sitting just outside this splendor, idling in my rented Toyota Corolla, on a clear day last winter. Like the other people in the line of cars, I was about to enjoy a rare treat. Carolands is an architectural landmark, but it’s open only two hours a week. Would-be visitors apply a month in advance, hoping to win a lottery for tickets. Like most lotteries, this one has long odds. I had applied unsuccessfully for the three tours scheduled for February. Finally, I resorted to my journalist’s privilege: I emailed and called the director of the foundation that owns the estate, explaining that I was a reporter planning to be in the area for a few days. Could she help? Eventually, she called back and offered me a place on a tour.</p>
        
    
                        


   
            
            
            
            

   
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="4.0">It wasn’t supposed to be this difficult. When billionaire Charles Johnson sought a tax break in 2013 for donating his mansion to his private foundation, the organization assured the Internal Revenue Service and state officials that the public would be welcome. “The Foundation will fulfill its charitable and educational purpose by opening the Carolands Estate to the public,” it stated in its <a href="https://www.documentcloud.org/documents/23879863-carolands_1023_public#document/p16/a2365104">application for tax-exempt status</a>, which included a pamphlet for a self-guided tour. The foundation later told a California tax regulator that the estate was <a href="https://www.documentcloud.org/documents/23885843-carolands-california-boe-277-public#document/p38/a2365102">open to the public every weekday from 9-5</a>.</p>
        
    
                    

<figure data-pp-id="5" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3888" height="2592" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=dbed06995f98176aa5c769074bb133df" srcset="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=75&amp;w=400&amp;s=3ee5b5d35884ae9d26fad20dd85e9e3a 400w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=dbed06995f98176aa5c769074bb133df 800w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=1200&amp;s=9bc929473e42856a18c61d3a9dbcaf4a 1200w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=867&amp;q=75&amp;w=1300&amp;s=22c36e389196647812320c3b4c73610d 1300w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=967&amp;q=75&amp;w=1450&amp;s=4e82543616459e2a60683d8afe75c401 1450w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1067&amp;q=75&amp;w=1600&amp;s=de48be386171f71df2fb7111691eb371 1600w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1333&amp;q=75&amp;w=2000&amp;s=478825e7373cf07a09932073c5df987b 2000w">

            
    
<figcaption>
        <span>The Carolands Estate</span>
    
        <span>
        <span>Credit: </span>
        San Francisco Chronicle/AP Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="6.0">There was a lot of money at stake. Johnson, a Republican megadonor and part owner of the San Francisco Giants, had gotten an appraisal valuing the property at $130 million, a price higher than any publicly reported home sale in the U.S. up to that time, and five times the $26 million he and his wife, Ann, had reportedly paid 14 years earlier to buy and restore what then was a dilapidated property.</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="8.0">The plan worked. The IRS granted the foundation tax-exempt status. That allowed the Johnsons to collect more than $38 million in tax savings from the estate over five years, confidential tax records show.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="9.0">But the Johnsons never opened Carolands to the public for 40 hours a week. Instead, the foundation bestows tickets on a few dozen lottery winners, who receive two-hour tours, led by docents, most Wednesdays at 1 p.m. Self-guided tours, like the ones described in the attachments to Johnson’s IRS application, are not offered. “It sounds like a vanity project with little to no public benefit,” said Roger Colinvaux, a professor of law at The Catholic University of America who specializes in the tax law of nonprofit organizations. (Experts also questioned Carolands’ $130 million valuation — which turbocharged the Johnsons’ deduction — while acknowledging that as long as it’s based on a qualified appraisal, which it was, the IRS is unlikely to challenge the size of the deduction.)</p>
        
    
                    

<figure data-pp-id="10" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="2400" height="3600" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1200&amp;q=75&amp;w=800&amp;s=57bca0114e9469e43e90d70835aa55b2" srcset="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=600&amp;q=75&amp;w=400&amp;s=e02704b50d8d947c0e8d9435e907efb7 400w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1200&amp;q=75&amp;w=800&amp;s=57bca0114e9469e43e90d70835aa55b2 800w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1800&amp;q=75&amp;w=1200&amp;s=0195dbd6c6b24333769bdf485d0bec1d 1200w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1950&amp;q=75&amp;w=1300&amp;s=a827265a9b23031ee6c86b572beb5bfa 1300w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2175&amp;q=75&amp;w=1450&amp;s=feeba1156ef48ee925948790038d3df0 1450w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2400&amp;q=75&amp;w=1600&amp;s=8a20e641f9bc816b2056382b7e0e424a 1600w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=3000&amp;q=75&amp;w=2000&amp;s=962fac060f714f82ca17f62eb25db3e4 2000w">

            
    
<figcaption>
        <span>Charles Johnson and his wife, Ann, collected more than $38 million in tax deductions as a result of donating their estate.</span>
    
        <span>
        <span>Credit: </span>
        Mike Coppola/Getty Images for the New York Philharmonic
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="11.0">For the ultrawealthy, donating valuables like artwork, real estate and stocks to their own charitable foundation is an alluring way to cut their tax bills. In exchange for generous tax breaks, they are supposed to use the assets to serve the public: Art might be put on display where people can see it, or stock sold to fund programs to fight child poverty. Across the U.S., such foundations <a href="https://fred.stlouisfed.org/series/BOGZ1FL164090015Q">hold over $1 trillion in assets</a>.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="12.0">But a ProPublica investigation reveals that some foundation donors have obtained millions of dollars in tax deductions without holding up their end of the bargain, and sometimes they personally benefit from donations that are supposed to be a boon to the public. A tech billionaire used his charitable foundation to buy his girlfriend’s house, then stayed there with her while he was going through a divorce. A real estate mogul keeps his nonprofit art museum in his guesthouse and told ProPublica that he hadn’t shown it to a member of the public since before the pandemic. And a venture capitalist couple’s foundation bought the multimillion dollar house next to their own without ever opening the property to the public.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="13.0">Unlike public charities, private foundations are typically funded by a single donor or family, who retain a high degree of control long after receiving a tax break for ostensibly giving their possessions away. “This is the classic problem with private foundations: Substantial contributors can see it as their thing,” said Philip Hackney, a law professor at the University of Pittsburgh and former IRS attorney. “There’s generally not a coalition who cares, other than the family, so there’s nothing to ensure that the assets are used for a particular purpose,” he added.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="14.0">In theory, it’s illegal to fail to provide a public benefit or to make personal use of foundation assets. But the rules defining what’s in the public interest are vague, according to tax experts; for example, Congress has never defined how many hours a museum would need to be open to be considered accessible to the public. And with the IRS depleted by a decade of budget cuts, enforcement has been lax. The agency examines an average of 225 returns among the 100,000 filed by private foundations each year, according to agency statistics.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="15.0">Peter Kanter, an attorney representing the Carolands Foundation, told ProPublica that “we believe pretty strongly that the foundation is serving its purpose of preserving and showcasing this historic and unique property to the public.” He said that tours are limited because the foundation has only a few volunteer docents who are knowledgeable about the home, and because significantly higher traffic might compromise the foundation’s ability to preserve its unique architecture. Kanter also emphasized the public value of free charitable events that the foundation occasionally hosts for other nonprofits at the estate.</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="17.0">At the Carolands, guides didn’t emphasize benefits to the public — just the opposite. A docent told my tour group that the foundation prefers lotteries to holding regular hours and charging admission. This, he explained, preserves the home for those who “really want to see it.” Indeed, exclusivity and rarefied taste were a theme of the tour, which included tales of the exacting specifications of Harriett Carolan, the Pullman heiress, a Francophile who imported an entire salon that had been built in France on the eve of the revolution. (For their parts, when Ann and Charles Johnson unveiled the restored chateau at a costume party, they dressed as Marie Antoinette and Louis XVI.)</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="18.0">Before the tour, one of the docents asked how many of us had ever visited a nearby historical mansion, called the Filoli estate, built in the same era as the Carolands. Many hands shot up among the tour group. When he asked if any of us had visited the Carolands before, no one raised their hand.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="19.0">Curious, I popped by Filoli the following afternoon. It is run by a public charity and is open from 10 to 5 every day. In contrast to the Carolands, I was able to simply show up, pay admission and enter. Inside, I encountered dozens of employees who provided helpful information and watched over the manor and its gardens while more than a hundred visitors wandered about. Photography, which had been prohibited inside the Carolands, was permitted at Filoli.</p>
        
    
                    
<hr>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="21.0">Congress and the IRS have never clearly defined what qualifies as a “public benefit.” By contrast, identifying a private benefit is much simpler. Decades ago Congress prohibited what it called self-dealing by insiders. The laws are designed to keep them from using or profiting from foundation assets. Among other things, the rules bar leases between a donor and their foundation. Violations can incur a penalty known as an excise tax.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="22.0">At least one billionaire appears to have run afoul of those real estate rules, according to tax experts. Since 2009, Ken Xie, CEO of a cybersecurity company called Fortinet, has gotten more than $30 million in income tax deductions for contributing shares of his business to a private foundation that he started to support various charitable causes.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="23.0">In 2017, Xie’s foundation (whose <a href="https://projects.propublica.org/nonprofits/organizations/271208110/202233059349101733/full">sole officers are Xie and his brother</a>) spent $3 million to purchase a home in Cupertino, California, from his new girlfriend while he was going through an acrimonious divorce. After the foundation purchased the home, Xie allowed his girlfriend to continue living there; he also stayed there for a time. These details emerged in a lawsuit filed by the now-ex-girlfriend, who was permitted to file the suit anonymously, in county court. (The suit is ongoing.) According to leases filed in the case, the foundation charged her rent, but Xie agreed to pay half of it.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="24.0">Xie himself appears to have been aware that he risked violating the rules. In a December 2019 text message to his girlfriend that was included in the court case, Xie wrote, “I covered some house part but also try not creat issue related to foundation and tax, believe will make some progress next few months by transfer house out of foundation, may need 2 step by first transfer to other entity.” The next month, his foundation transferred the property to an LLC.</p>
        
    
                    

<figure data-pp-id="25" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="4884" height="3159" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=517&amp;q=75&amp;w=800&amp;s=d8f05dfbfe4d4cea130b5eb1da68b920" srcset="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=259&amp;q=75&amp;w=400&amp;s=97ec454f4a21a013213b43409981c38c 400w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=517&amp;q=75&amp;w=800&amp;s=d8f05dfbfe4d4cea130b5eb1da68b920 800w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=776&amp;q=75&amp;w=1200&amp;s=ca0f8ec1b0693d1a0f3adb54ba296ed3 1200w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=841&amp;q=75&amp;w=1300&amp;s=dcfe70cf7df317bbf5f82302cb6f45d5 1300w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=938&amp;q=75&amp;w=1450&amp;s=9d92de9b1c4fd9f2a0cbb7774b7699b3 1450w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1035&amp;q=75&amp;w=1600&amp;s=276ced8c08c8de667fe5c74f34b6227b 1600w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1294&amp;q=75&amp;w=2000&amp;s=7d35628aec1f78040b79434a8697e08f 2000w">

            
    
<figcaption>
        <span>Ken Xie, CEO of cybersecurity company Fortinet, has earned more than $30 million worth of tax deductions by donating shares to a private foundation.</span>
    
        <span>
        <span>Credit: </span>
        K.Y. Cheng/South China Morning Post/Getty Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="26.0">In an email to ProPublica, Gordon Finwall, a lawyer for Xie, said the foundation is “fully committed to complying with all applicable rules and regulations.” He acknowledged that Xie “spent some time at the Cupertino property in 2017 and 2018,” but asserted that the sublease was never in effect and Xie never paid his ex-girlfriend any rent.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="27.0">Two days after I emailed Finwall in April inquiring about the Xie Foundation’s purchase of the house, the foundation <a href="https://www.documentcloud.org/documents/23882476-xie_foundation_2017_amendment#document/p2/a2363407">filed records with the California attorney general’s office</a>, stating that it had “discovered a self-dealing event” and including a federal tax return with the word “amended” handwritten at the top. In his email to ProPublica, Finwall said that, after amending its returns, the foundation “paid some excise taxes related to Mr. Xie’s stay at the property.” Finwall also said that Xie had planned to file the amended returns months earlier but didn’t do so because his accountant mailed the IRS forms to Xie at an outdated address.</p>
        
    
                    
<hr>

        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="30.0">Despite the blurriness of many rules relating to foundations, the issue of public access has given rise to controversy in the past. After a New York Times article in 2015 exposed the <a href="https://www.nytimes.com/2015/01/11/business/art-collectors-gain-tax-benefits-from-private-museums.html">limited hours of many private museums</a>, the Senate Finance Committee, under then-chairman Orrin Hatch, launched an investigation. <a href="https://www.finance.senate.gov/chairmans-news/hatch-concludes-review-into-tax-exempt-private-museums-notes-concerning-findings">Hatch expressed concerns</a> about museums that require advance reservations and maintain limited public hours. He questioned instances where “founding donors continue to play an active role in management and operations of the museum” and “museum buildings are adjacent to the donor’s private residence.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="31.0">But no meaningful rule changes followed the investigation. And absent new laws, cracking down on abusive foundations would require the IRS to put scarce resources into an area that many experts said simply isn’t a priority, particularly after the agency’s previous attempt to police abuse by political nonprofits a decade ago caused a conservative firestorm.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="32.0">The agency doesn’t appear likely to increase oversight any time soon. A recently published budget blueprint outlining IRS priorities for the $80 billion in new funding it received from the Inflation Reduction Act made no mention of increasing audits of private foundations.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="33.0">“The IRS protects the public interest by applying the tax law with integrity and fairness to all,” the agency wrote in a statement to ProPublica. The statement cited a compliance program that “focuses on high-risk issues” among tax-exempt organizations, and it asserted that the program “deploys the right resources to address noncompliance issues.” The IRS also pointed to a recent tax court case that it won against a foundation that, among other things, kept a collection of African artifacts in a basement with no public access. And an agency spokesperson highlighted a rule stating that foundations can lose their exempt status if they operate in a manner “materially different” than what they claimed they would do in their initial application.</p>
        
    
                    
<hr>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="35.0">Despite the attention spurred by the Hatch investigation, some foundations seem to have continued undeterred. Consider the Lijin Gouhua Foundation. Collecting Chinese paintings and sharing them with the public was the stated mission of the organization, which was launched by Bay Area venture capitalists J. Sanford “Sandy” Miller and his then-wife, Vinie Zhang Miller, in 2006. Since then, the couple generated $5.6 million worth of income tax write-offs largely from donating shares of tech companies like Twitter and Snapchat to their private foundation.</p>
        
    
                                  
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="37.0">When the couple cashed in the foundation’s stock to buy a potential museum space for the art in 2017, they opted against a high-traffic location where lots of people could easily access it. Instead, they chose <a href="https://www.redfin.com/CA/Woodside/1540-Portola-Rd-94062/home/2009633">the $3.1 million house</a> adjacent to <a href="https://www.youtube.com/watch?v=hBpp_3YhK_0">their own estate in Woodside</a>, an exclusive enclave outside of San Francisco.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="38.0">“A private museum is usually by appointment only,” Vinie Miller said when asked about the out-of-the-way location. “We wouldn’t hold long showing hours. It’s usually people we have a relationship with.” She said that the main way for the public to access the collection was through loans of artwork the foundation has made to universities, other museums and galleries. (In an email, Sandy Miller wrote: “Please be advised that I am not married to Vinie and that I have no involvement with the Lijin Gouhua Foundation.” Public records show Vinie filed for divorce from him in 2019; Sandy ceased to be listed as president of the foundation on IRS filings that year as well.)</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="39.0">The museum that was purchased with the foundation’s tax-exempt funds never actually opened. Vinie Miller said the plan was “hypothetical” and that the foundation held the home as an investment instead. That’s at odds with the foundation’s publicly available tax returns, which have listed the property as being used for charitable purposes. (Miller did not respond to a follow-up question asking about the discrepancy between her statements and the foundation’s tax returns.) As Colinvaux, the specialist in nonprofits, put it, “If it’s an investment asset, then it’s not a charitable use asset, and they shouldn’t be counting it as such” on their IRS filings.</p>
        
    
                    
<hr>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="41.0">In one similar instance involving another foundation, the IRS expressed hesitation about the organization’s plans, then backed off. In 2006, San Diego real estate magnate Matthew Strauss sought a $4 million write-off for the guesthouse that held part of his contemporary art collection. <a href="https://www.documentcloud.org/documents/23879865-strauss_1023_public#document/p215/a2365113">An IRS employee wrote</a> that it appeared Strauss and his wife “are using the assets of the Foundation (the guest house gallery) as a facility for housing and displaying a large portion of their personal art collection for their enjoyment and benefit as well as the enjoyment and benefit of invited guests.” The employee wanted to know when actual art would be donated, what kind of access the public would have to the gallery, and how the couple planned to inform people that they could visit, among other things.</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="43.0">The couple’s lawyer assured the IRS representative that she’d gotten the wrong impression. The Strausses would host no personal events there and the public would have access to view the collection “upon request.” The couple anticipated donating “substantially all” of their $50 million collection to the foundation. They couldn’t say when, but the couple planned to make donations “in a fashion that minimizes income taxes.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="44.0">As 2006 turned into 2007 with no sign that the IRS would bless its museum tax deduction, the couple sought political help. In January, the head of the IRS’ tax-exempt division <a href="https://www.documentcloud.org/documents/23879865-strauss_1023_public#document/p15/a2365110">received a letter</a> from the office of <a href="https://projects.propublica.org/represent/members/F000062-dianne-feinstein">Sen. Dianne Feinstein (D.-Calif.)</a>, inquiring about the delay in approving the application from the couple, who’d given her more than $15,000 over the past few election cycles. That June, their application was approved. (“The senator was not advocating in support of the constituent’s application, but instead requested clarification on the case after nine months of an inability to resolve the case,” a spokesperson for Feinstein said, noting that her office frequently sends such letters on behalf of constituents).</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="45.0">As of 2021, 15 years after the Strausses’ lawyer told the IRS they would donate $50 million in art, the foundation holds $6 million worth. The rest remained in a private trust.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="46.0">To learn more about Strausses’ gallery, I tried to schedule a visit earlier this year. As with Carolands, I was able to get in, but it took some effort. The foundation’s website doesn’t list an address or hours of operation. A contact form available for visitors to inquire about tours wasn’t working when I tried it repeatedly. I ultimately had to pester employees of Strauss’ real estate company for a couple of weeks before someone responded and asked me to submit a biography for their boss to review. (My bio described me as a reporter with ProPublica, with the first coverage area listed as “tax policy.”)</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="47.0">Soon after I sent in my biography, I received a call from Matthew Strauss himself. After a brief conversation, he declared me “worthy” of the first tour he said he’d given in three years and sent along directions to the museum.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="48.0">I didn’t see any signs outside the couple’s estate, nicknamed Rancho Del Arte, that indicated a museum could be found anywhere on the premises. From the outside, their guesthouse seemed relatively unassuming, its multimillion-dollar value betrayed only by the horse stables and privacy hedges of the nearby mansions I passed on the way in. A path wide enough for a golf cart wound its way through a grove of palm trees, past oversized sculptures and a private tennis court, to the Strausses’ own sprawling abode a hundred yards or so away.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="49.0">The inside was more remarkable. The Strausses remodeled the building in the early 2000s with custom fixtures to illuminate works from their collection of contemporary art. Sounds and music from dueling audiovisual works on the main floor flooded the space, while the click-clack of a never-ending ping pong game echoed up from a conceptual piece in the basement. These noisier forms shared space with paintings on canvas and metal and with textured mixed-media compositions.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="50.0">Dressed in sweats and sporting a Bentley baseball cap, Strauss personally led my solo tour, meandering from one prized possession to the next. He exhibited an uncanny memory for how he obtained each piece, likening the acquisition process to the thrill of a hunt. (“Once you get the fox, it’s not as much fun.”) He spoke of one painting as “my poor man’s ‘Mona Lisa’” and another as “my victory piece.”</p>
        
    
                    

<figure data-pp-id="51" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="4928" height="3264" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=530&amp;q=75&amp;w=800&amp;s=512f9303262432a140a2d0897c41259b" srcset="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=265&amp;q=75&amp;w=400&amp;s=7fccc04a3d93fe1e425e093bb5460ded 400w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=530&amp;q=75&amp;w=800&amp;s=512f9303262432a140a2d0897c41259b 800w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=795&amp;q=75&amp;w=1200&amp;s=4a2bc508e1191df4b615c5bfd3ef9abf 1200w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=861&amp;q=75&amp;w=1300&amp;s=866b089a8068f31bc788cf4fa3d4bf6f 1300w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=960&amp;q=75&amp;w=1450&amp;s=ba8a1f63e217bf8c7f7e153b2f8e139e 1450w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1060&amp;q=75&amp;w=1600&amp;s=444e4edcfa80bcc52fa25002b7a71b61 1600w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1325&amp;q=75&amp;w=2000&amp;s=e9119f2a57ab8df01f1cea479852dc3b 2000w">

            
    
<figcaption>
        <span>Matthew Strauss in front of “Sunshine and Snow,” by Kenneth Noland, at his foundation’s museum.</span>
    
        <span>
        <span>Credit: </span>
        Jeff Ernsthausen/ProPublica
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="52.0">Halfway through my visit, we stopped to take in the view from the museum’s balcony. “At this point, you can see why I had to buy this property,” he told me, explaining that he’d bought the guesthouse from his neighbor in the late 1990s to keep anyone else from moving in. “Anybody here, they would have knocked it down, and you know, really ruined our privacy.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="53.0">As the tour continued from room to room, Strauss leaned into his persona as a friendly professor. He asked probing questions about each modern piece before delving into centuries of art history. “I really show [people] how to look at art, I don’t just tell them ‘This is So-and-So,’” he said, recalling the tours he used to give to college students.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="54.0">Before the pandemic, the foundation would conduct a dozen or two dozen tours each year, drawing a total of about 400 visitors to the gallery, according to the foundation’s website. But even as California’s other museums welcomed guests back in the spring of 2021, the foundation remained dormant.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="55.0">Strauss acknowledges the tax benefits of having the foundation and maintained that he had made efforts to make his art available to the public. “I feel like I have an obligation to show it, but it’s got to be under favorable conditions,” he said. He’d told me he’d like to get tours going again, but only when schools and universities stop requiring masks and start treating COVID-19 “like normal.”</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="57.0">Strauss said he gets requests from individuals to see the collection “all the time.” But, he added, “to show one or two, it’s not worthy. It’ll wear me out.” Letting people come on their own was out of the question (they might damage the art), as was having regular public hours (it’s a zoning issue, he said, and the neighbors would never go for it). Strauss declined to respond to a list of follow-up questions that I sent after the tour.</p>
        
    
                                  
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="59.0">A couple months from turning 90, Strauss was more focused on the big picture. Sooner or later, he said, he plans to give away most of the collection, which he estimates to be worth hundreds of millions of dollars. Most of his personal collection will go to the Museum of Contemporary Art San Diego, while the foundation’s assets will go to the University of California, San Diego under a deal that is in the process of being finalized.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="60.0">As we made our way through the gallery, Strauss paused before a reproduction of a Life magazine cover featuring the 1964 World’s Fair in New York. Did anything catch my eye about it, he asked. </p>

<p data-pp-blocktype="copy" data-pp-id="60.1">I stared for a moment.</p>

<p data-pp-blocktype="copy" data-pp-id="60.2">“Why don’t you knock on it,” he suggested. “Maybe that’ll help you.” </p>

<p data-pp-blocktype="copy" data-pp-id="60.3">Strauss sensed my hesitation to touch the art — he wanted me to see it was made of metal — and tried to put me at ease.</p>

<p data-pp-blocktype="copy" data-pp-id="60.4">“You’re not supposed to,” he chuckled. “But this is my museum!”</p>

<p data-pp-blocktype="copy" data-pp-id="60.5"><em>For this story, ProPublica reviewed a nationwide database of parcels provided by the real estate data analytics firm Regrid to find homes owned by private foundations.</em></p>
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    
<div>
            <p id="help-us-report-on-taxes-and-the-ultrawealthy">Help Us Report on Taxes and the Ultrawealthy</p>
                <p>Do you have expertise in tax law, accounting or wealth management? Do you have tips to share? Here’s how to get in touch. We are looking for both specific tips and broader expertise.</p>
    </div>


                        

<div>
    <div>
        <p><span>
            <strong>I may know something related to your investigation.</strong>
        </span>
        <a data-toggle-section="section-tip-online" href="https://projects.propublica.org/tips/help-us-report-on-taxes-and-ultrawealthy/#form">Submit a Tip Through Our Form</a>
        <span>
            <strong>We take privacy seriously.</strong> Any tips you submit via
            the above form are encrypted on our end. But if you wish for
            additional anonymity, please get in touch via one of these methods:
        </span></p>
    </div>
    <p><span>
            <strong>I am knowledgeable in one of these areas and can answer
                questions or help you understand technicalities.</strong>
        </span>
        <a data-toggle-section="section-expert-help" href="https://projects.propublica.org/tips/help-us-report-on-taxes-and-ultrawealthy/#expert">Volunteer Your Expertise</a>
    </p>
</div>
            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

            
        
    
                    <div data-pp-location="bottom-note">
                        <p><a href="https://www.propublica.org/people/paul-kiel">Paul Kiel</a> and <a href="https://www.propublica.org/people/andrea-suozzo">Andrea Suozzo</a> contributed data analysis.</p>

        </div><!-- end .article-body__bottom-notes -->
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chicago95 – Windows 95 Theme for Linux (195 pts)]]></title>
            <link>https://github.com/grassmunk/Chicago95</link>
            <guid>36929096</guid>
            <pubDate>Sun, 30 Jul 2023 08:28:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/grassmunk/Chicago95">https://github.com/grassmunk/Chicago95</a>, See on <a href="https://news.ycombinator.com/item?id=36929096">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Chicago95</h2>
<h4 tabindex="-1" dir="auto">XFCE / Xubuntu Windows 95 Total Conversion</h4>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/grassmunk/Chicago95/blob/master/Screenshots/Chicago95_Desktop.png"><img src="https://github.com/grassmunk/Chicago95/raw/master/Screenshots/Chicago95_Desktop.png" alt="Chicago95 Desktop"></a>
</p>
<p dir="auto"><em>Click <a href="https://github.com/grassmunk/Chicago95/blob/master/Screenshots/SCREENSHOTS.md">here</a> for more screenshots</em></p>
<p dir="auto">I was unhappy with the various XFCE/GTK2/GTK3 Windows 95 based themes and decided to make one that was more consistent across the board for theming.</p>
<h3 tabindex="-1" dir="auto">Included in this theme:</h3>
<ul dir="auto">
<li>Icons to complete the icon theme started with Classic95</li>
<li>GTK2 and GTK3 themes</li>
<li>Edited Redmond XFWM theme to more accurately reflect Windows 95</li>
<li>Chicago95 Plus! A tool to preview and install Windows 95/98/ME/XP themes</li>
<li>Plymouth theme created from scratch</li>
<li>An MS-DOS inspired theme for oh-my-zsh</li>
<li>Partial support for HiDPI monitors</li>
<li>Partial icon theme for LibreOffice 6+</li>
</ul>
<h3 tabindex="-1" dir="auto">Requirements:</h3>
<ul dir="auto">
<li>GTK+ 3.22 or 3.24</li>
<li>Xfce 4.12, 4.14, 4.16</li>
<li>gtk2-engines-pixbuf (Recommended for GTK2 applications)</li>
<li>The xfce4-panel-profiles package</li>
<li>A Window compositor</li>
</ul>
<p dir="auto">(If your are using an older desktop that uses GTK3.18, you can use <a href="https://github.com/EMH-Mark-I/Chicago95-Custom-XUbuntu-16.04-">this forked version of the theme.</a>)</p>
<hr>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto"><a href="https://github.com/grassmunk/Chicago95/blob/master/INSTALL.md">Click here</a> for Chicago95 documentation and extra features.</p>
<hr>
<h2 tabindex="-1" dir="auto">Installation</h2>
<h3 tabindex="-1" dir="auto">Packages:</h3>
<table>
<thead>
<tr>
<th>Distro</th>
<th>Package Name/Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>Debian 9</td>
<td><a href="https://software.opensuse.org//download.html?project=home%3Abgstack15%3AChicago95&amp;package=chicago95-theme-all" rel="nofollow">obs-repo</a></td>
</tr>
<tr>
<td>Debian 10</td>
<td><a href="https://software.opensuse.org//download.html?project=home%3Abgstack15%3AChicago95&amp;package=chicago95-theme-all" rel="nofollow">obs-repo</a></td>
</tr>
<tr>
<td>Debian Testing/Unstable</td>
<td><a href="https://software.opensuse.org//download.html?project=home%3Abgstack15%3AChicago95&amp;package=chicago95-theme-all" rel="nofollow">obs-repo</a></td>
</tr>
<tr>
<td>Ubuntu 18.04 - 20.04</td>
<td><a href="https://software.opensuse.org//download.html?project=home%3Abgstack15%3AChicago95&amp;package=chicago95-theme-all" rel="nofollow">obs-repo</a></td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto">Manual installation and setup instructions:</h3>
<p dir="auto"><a href="https://github.com/grassmunk/Chicago95/blob/master/INSTALL.md">Click here</a> for install steps.</p>
<h3 tabindex="-1" dir="auto">Install a Microsoft Windows Plus! theme:</h3>
<p dir="auto"><a href="https://github.com/grassmunk/Chicago95/blob/master/Plus/README.MD">Click here</a> for installing custom themes.</p>
<h3 tabindex="-1" dir="auto">Install the Plymouth boot splash theme:</h3>
<p dir="auto"><a href="https://github.com/grassmunk/Chicago95/blob/master/Plymouth">Click here</a> for install steps.</p>
<h3 tabindex="-1" dir="auto">Install the LibreOffice icon theme:</h3>
<p dir="auto"><a href="https://github.com/grassmunk/Chicago95/blob/master/Extras/libreoffice-chicago95-iconset/README.md">Click here</a> for installing the LibreOffice Chicago95 icon theme.</p>
<hr>
<h2 tabindex="-1" dir="auto">Miscellaneous</h2>
<h3 tabindex="-1" dir="auto">KDE Support (experimental):</h3>
<ul dir="auto">
<li>SDDM Logon Manager:: Click <code>Install from file...</code> in Loggin Screen (SDDM) manager. Select <code>SDDM/Chicago95.tar.gz</code> to install the theme.</li>
<li>Splash Screen: <code>plasmapkg2 -t lookandfeel -i KDE/Splash/chicago95.splashscreen</code></li>
</ul>
<h3 tabindex="-1" dir="auto">Screenshots:</h3>
<p dir="auto"><a href="https://github.com/grassmunk/Chicago95/blob/master/Screenshots/SCREENSHOTS.md">Click here to view screenshots</a></p>
<h3 tabindex="-1" dir="auto">IRC server:</h3>
<p dir="auto"><a href="https://web.emhmki.org:8443/" rel="nofollow">Click here</a> to connect to the IRC server. Please read server rules and be kind.</p>
<hr>
<h3 tabindex="-1" dir="auto">Code and license</h3>
<p dir="auto">License: GPL-3.0+/MIT</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists may have found mechanism behind cognitive decline in aging (312 pts)]]></title>
            <link>https://news.cuanschutz.edu/news-stories/scientists-may-have-found-mechanism-behind-cognitive-decline-in-aging</link>
            <guid>36929090</guid>
            <pubDate>Sun, 30 Jul 2023 08:28:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.cuanschutz.edu/news-stories/scientists-may-have-found-mechanism-behind-cognitive-decline-in-aging">https://news.cuanschutz.edu/news-stories/scientists-may-have-found-mechanism-behind-cognitive-decline-in-aging</a>, See on <a href="https://news.ycombinator.com/item?id=36929090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="hs_cos_wrapper_module_151456960811572" data-hs-cos-general-type="widget" data-hs-cos-type="module" data-widget-type="custom_widget" data-x="0" data-w="12">
<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p>Scientists at the University of Colorado Anschutz Medical Campus have discovered what they believe to be the central mechanism behind cognitive decline associated with normal aging.</p>
<!--more-->
<p>“The mechanism involves the mis-regulation of a brain protein known as CaMKII which is crucial for memory and learning,” said the study’s co-senior author <a href="https://medschool.cuanschutz.edu/pharmacology/faculty/primary-faculty/ulli-bayer-phd" rel="noopener">Ulli Bayer,</a> PhD, professor of pharmacology at the <a href="https://medschool.cuanschutz.edu/" rel="noopener"><span>University of Colorado School of Medicine</span></a>. “This study directly suggests specific pharmacological treatment strategies.”</p>
<p>The <a href="https://www.science.org/doi/10.1126/scisignal.ade5892" rel="noopener">study was published</a> today in the journal `Science Signaling.’</p>
<p>Researchers using mouse models found that altering the CaMKII brain protein caused similar cognitive effects as those that happen through normal aging.</p>
<p>Bayer said that aging in mice and humans both decrease a process known as S-nitrosylation, the modification of a specific brain proteins including CaMKII.</p>
<p>“The current study now shows a decrease in this modification of CaMKII is sufficient to cause impairments in synaptic plasticity and in memory that are similar in aging,” Bayer said.</p>
<p>Normal aging reduces the amount of nitric oxide in the body. That in turn reduces nitrosylation which decreases memory and learning ability, the study said.</p>
<p>Bayer said the new research opens the way toward developing drugs and other therapeutic interventions that could normalize the nitrosylation of the protein. He said that holds out the possibility of treating or staving off normal cognitive decline for an unknown period of time.</p>
<p>He pointed out that this would only work in normal age-related cognitive decline, not the decline seen in Alzheimer’s disease and dementia.</p>
<p>“We know this protein can be targeted,” Bayer said. “And we think it could be done pharmacologically. That is the next logical step.”</p></span>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fighting for the open web [image] (181 pts)]]></title>
            <link>https://www.davidrevoy.com/article982/fighting-for-the-open-web</link>
            <guid>36928529</guid>
            <pubDate>Sun, 30 Jul 2023 06:45:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.davidrevoy.com/article982/fighting-for-the-open-web">https://www.davidrevoy.com/article982/fighting-for-the-open-web</a>, See on <a href="https://news.ycombinator.com/item?id=36928529">Hacker News</a></p>
<div id="readability-page-1" class="page"><section class="page">
			<article role="article" id="post-982">
          
          <!-- Content -->
          <div>
                <div>
                  <p><a href="https://www.davidrevoy.com/data/images/blog/2023/2023-07-27_fighting-for-the-open-web.jpg" title=""><img src="https://www.davidrevoy.com/plugins/vignette/plxthumbnailer.php?src=https://www.davidrevoy.com/data/images/blog/2023/2023-07-27_fighting-for-the-open-web.jpg&amp;w=1280&amp;h=1400&amp;zc=4&amp;s=1&amp;q=92" alt=""></a>                  <br>
                  <b>Fighting for the open web.</b><br> <small><time datetime="2023-07-27">27 july 2023</time></small> 			
                </p>
              </div>
                  
                  <p>A picture I had in mind about the topic of <a href="https://en.wikipedia.org/wiki/Web_Environment_Integrity">Web Environment Integrity</a>. </p>
<p><a href="https://www.peppercarrot.com/en/viewer/misc__2023-07-27_Fighting-For-The-Open-Web_by-David-Revoy.html">Source here</a></p>               </div>
          
        
        
     </article>
		</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microbially produced protein that is much sweeter than sugar (150 pts)]]></title>
            <link>https://www.nature.com/articles/s41587-023-01865-x</link>
            <guid>36927940</guid>
            <pubDate>Sun, 30 Jul 2023 04:54:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/s41587-023-01865-x">https://www.nature.com/articles/s41587-023-01865-x</a>, See on <a href="https://news.ycombinator.com/item?id=36927940">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <div><p>A biotech answer to the world’s epic sugar consumption could be in the works — if Amai Proteins gets its way. The synthetic biology company is gearing up to commercialize a microbially produced protein that on average is 3,000 times sweeter than sugar and, if widely adopted, could put a dent in the global metabolic disease and obesity epidemic. Amai is aiming for this protein to replace up to 70% of the added sugar without the health hazards and off-flavors of other synthetic and natural <a href="https://www.who.int/news/item/15-05-2023-who-advises-not-to-use-non-sugar-sweeteners-for-weight-control-in-newly-released-guideline">sweeteners</a>.</p><div data-test="illustration" id="i1"><div><picture><source type="image/webp" srcset="https://media.springernature.com/lw703/springer-static/image/art%3A10.1038%2Fs41587-023-01865-x/MediaObjects/41587_2023_1865_Figa_HTML.png?as=webp"><img alt="" aria-describedby="i1-desc" width="703" src="https://media.springernature.com/lw703/springer-static/image/art%3A10.1038%2Fs41587-023-01865-x/MediaObjects/41587_2023_1865_Figa_HTML.png"></picture></div><p><span data-test="illustration-credit">
            Credit: Pazit Assulin, Amai Proteins</span></p></div></div><p>The origins of Amai’s product can be traced to the equatorial belt, where a natural protein called monellin was discovered in tropical berries. Although it is a protein, monellin also <a href="https://doi.org/10.1111/bcpt.13239">docks to</a> the same sweet receptors as sugar, so has the same mouthfeel and taste profile. But there is a glitch. “This is a hyper-sweet protein, but it denatures at 45 degrees; just like boiling an egg, it loses functionality,” says Ilan Samish, Amai’s Founder and CEO.</p><p>Samish modified this plant protein with a method called agile integrative computational protein design (AI-CPD), a technique with which Samish worked and <a href="https://doi.org/10.1007/978-1-4939-6637-0">published</a> as an academic researcher. Taking inspiration from extremophiles — organisms that withstand harsh pH, salt and temperature conditions — Samish applied AI-CPD to tweak monellin’s protein <a href="https://doi.org/10.1038/328739a0">structure</a> and improve its characteristics — mainly stability, a critical feature for mass produced food products. “A protein is a sequence of pearls [amino acids]; we can change the sequence to build a new protein inspired by life in extreme conditions.” The result is a designer protein that is stable even at high temperatures and optimized for use as a food additive.</p><p>Next, the scientists at the Rehovot-based company biomanufacture the designer protein in yeast. “For us, fermentation is just like a brewery,” says Samish. Once the output is harvested, the yield is a pure protein, a white powder they call sweelin.</p><p>The company has tested sweelin in a range of foods, achieving a 70% reduction in the sugar content of ketchup and a 50% reduction in that of chocolate, for example, without changing palatability. “Supertasters can’t tell the difference,” says Samish. A huge advantage is that, unlike other sweeteners, sweelin is unlikely to interact with the gut microbiome. And because these sweet molecules are proteins, they are digested into amino acids without activating the insulin response.</p><p>Amai has so far raised $30 million and expects to close a series A investment soon. The company has ongoing collaborations with food manufacturers. The goal is to launch commercially in 2023 once they receive Generally Recognized as Safe status, first in the form of a self-affirmation and then from the US Food and Drug Administration. Beyond producing sweet food additives, the company anticipates optimizing other proteins for different uses. “Old-school agriculture cannot be sustained; we need to produce new tasty, healthy, cost-efficient and sustainable designer proteins for the mass food market. This is what we can do with synthetic biology.”</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Welcome to Wikifunctions (274 pts)]]></title>
            <link>https://www.wikifunctions.org/wiki/Wikifunctions:Main_Page</link>
            <guid>36927695</guid>
            <pubDate>Sun, 30 Jul 2023 04:02:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wikifunctions.org/wiki/Wikifunctions:Main_Page">https://www.wikifunctions.org/wiki/Wikifunctions:Main_Page</a>, See on <a href="https://news.ycombinator.com/item?id=36927695">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mainpage_header">
<p><span typeof="mw:File"><a href="https://www.wikifunctions.org/wiki/File:Wikifunctions-logo.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Wikifunctions-logo.svg/75px-Wikifunctions-logo.svg.png" decoding="async" width="75" height="75" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Wikifunctions-logo.svg/113px-Wikifunctions-logo.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Wikifunctions-logo.svg/150px-Wikifunctions-logo.svg.png 2x" data-file-width="512" data-file-height="513"></a></span>
</p>
<h2><span id="Welcome_to_Wikifunctions" data-mw-thread-id="h-Welcome_to_Wikifunctions"><span data-mw-comment-start="" id="h-Welcome_to_Wikifunctions"></span>Welcome to Wikifunctions<span data-mw-comment-end="h-Welcome_to_Wikifunctions"></span></span></h2>
<p>Wikifunctions is a <a href="https://foundation.wikimedia.org/wiki/Policy:Terms_of_Use" title="foundation:Policy:Terms of Use">free</a> library of <a href="https://www.wikifunctions.org/wiki/Wikifunctions:Introduction" title="Wikifunctions:Introduction">functions</a> that <i>(soon)</i> anyone can edit.</p>
</div><div id="audiences">
	<div>
		<h3><span id="Welcome.21"></span><span data-mw-comment-start="" id="h-Welcome!-Welcome_to_Wikifunctions"></span><span id="Welcome!" data-mw-thread-id="h-Welcome!-Welcome_to_Wikifunctions"><span>Welcome!</span></span><span data-mw-comment-end="h-Welcome!-Welcome_to_Wikifunctions"></span></h3>
		<div id="mainpage-welcome" title="Welcome">
<p><b>Wikifunctions</b> is a Wikimedia project for everyone to collaboratively create and maintain a library of code functions to support the Wikimedia projects and beyond, in the world's natural and programming languages. 
</p><p>A "function" is a sequence of programming instructions that makes a calculation based on data you provide. Functions can answer questions, such as how many days have passed between two dates, or the distance between two cities.
</p>
		</div>
	</div>
	<div>
		<h3><span data-mw-comment-start="" id="h-Get_started-Welcome_to_Wikifunctions"></span><span id="Get_started" data-mw-thread-id="h-Get_started-Welcome_to_Wikifunctions"><span>Get started</span></span><span data-mw-comment-end="h-Get_started-Welcome_to_Wikifunctions"></span></h3>
		
	</div>
</div><div id="many-examples">
		<h3><span data-mw-comment-start="" id="h-Existing_function_examples-Welcome_to_Wikifunctions"></span><span id="Existing_function_examples" data-mw-thread-id="h-Existing_function_examples-Welcome_to_Wikifunctions"><span>Existing function examples</span></span><span data-mw-comment-end="h-Existing_function_examples-Welcome_to_Wikifunctions"></span></h3>
		
	</div><div id="sister" title="Sister projects">
<div><figure typeof="mw:File"><span><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/Wikimedia-logo.svg/45px-Wikimedia-logo.svg.png" decoding="async" width="45" height="45" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/Wikimedia-logo.svg/68px-Wikimedia-logo.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/Wikimedia-logo.svg/90px-Wikimedia-logo.svg.png 2x" data-file-width="1024" data-file-height="1024"></span><figcaption></figcaption></figure></div>
<p>Wikifunctions is part of the non-profit, multilingual, free-content Wikimedia family.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GNU Boot sent a cease and desist to Libreboot (277 pts)]]></title>
            <link>https://libreboot.org/news/gnuboot.html#gnu-boot-cease-and-desist-email</link>
            <guid>36926852</guid>
            <pubDate>Sun, 30 Jul 2023 01:40:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://libreboot.org/news/gnuboot.html#gnu-boot-cease-and-desist-email">https://libreboot.org/news/gnuboot.html#gnu-boot-cease-and-desist-email</a>, See on <a href="https://news.ycombinator.com/item?id=36926852">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><a href="https://libreboot.org/news/">Return to index</a></p>
<p>Article published by: Leah Rowe</p>
<p>Date of publication: 17 July 2023</p>
<p>People have been waiting for me to break the silence about this. I go on about it on IRC. This article is intended to address it once and for all, officially.</p>
<p>I waited so long, because until recently there really wasn’t anything tangible to talk about; why talk about vaporware? Why indeed.</p>
<div><h2 id="introduction">Introduction!</h2></div>
<p>This doesn’t need to be an overly long post, so it won’t be. There is a <em>fork</em> of Libreboot, named GNU Boot, which you can find here: <a href="https://savannah.gnu.org/projects/gnuboot/">https://savannah.gnu.org/projects/gnuboot/</a></p>
<p>Long story short, when I saw this, I decided that I would try to <em>help</em> the project. More on this next:</p>
<div><h2 id="non-genuine-boot-20230717-release">non-GeNUine Boot 20230717 release</h2></div>
<p>If you want to skip the lecture, just read these first and re-visit this page (the one you’re reading now) afterwards for more context:</p>
<ul>
<li><strong>non-GeNUine Boot 20230717, unofficial release (produced by <em>me</em>): <a href="https://notgnuboot.vimuser.org/news/nongenuineboot20230717.html">https://notgnuboot.vimuser.org/news/nongenuineboot20230717.html</a> - based on the recent <a href="https://libreboot.org/news/libreboot20230625.html">Libreboot 20230625</a> release</strong>, but modified to comply with GNU Boot policy, as best as I could approximate. I <em>encourage them</em> to re-use this work. It’s roughly <em>8 months</em> ahead of their current work.</li>
</ul>
<p>Or generally: <strong><a href="https://notgnuboot.vimuser.org/">https://notgnuboot.vimuser.org/</a> - non-GeNUine Boot website</strong></p>
<p>These links, above, are for an <em>unofficial</em> fork of Libreboot that <em>I</em> have done myself, proposed for re-use by the new GNU Boot project. I am <em>not</em> a member of the GNU Boot project, but I do want to see it succeed.</p>
<p>GNU Boot? What is that, you ask me? It is a fork of Libreboot by the GNU project, but it currently does not have a website and does not have any releases of its own. My intent is to <em>help them</em>, and they are free - encouraged - to re-use my work, linked above.</p>
<div><h2 id="gnu-forked-libreboot">GNU forked Libreboot?</h2></div>
<div><h2 id="why">Why?</h2></div>
<p>They forked Libreboot, due to disagreement with Libreboot’s <a href="https://libreboot.org/news/policy.html">Binary Blob Reduction Policy</a>. This is a pragmatic policy, enacted in November 2022, to increase the number of coreboot users by increasing the amount of hardware supported in Libreboot. Libreboot’s <a href="https://libreboot.org/freedom-status.html">Freedom Status</a> page describes in great detail, how that policy is implemented - the last few Libreboot releases have <em>vastly</em> expanded the list of hardware supported, which you can read <a href="https://libreboot.org/docs/hardware/">here</a>.</p>
<p>I wish GNU Boot all the best success. Truly. Although I think their project is entirely misguided (for reasons explained by modern Libreboot policy), I do think there is value in it. It provides continuity for those who wish to use something resembling the old Libreboot project; some context:</p>
<div><h2 id="osboot">osboot</h2></div>
<p>Previously, another project started by me named <a href="https://web.archive.org/web/20220714144846/https://osboot.org/">osboot</a> existed - osboot, created in December 2020, ran for just under two years as a separate project, and it very much resembled what Libreboot is today.</p>
<p>osboot was a fork of Libreboot, that I created <em>myself</em>, and maintained in parallel to Libreboot. The old osboot Git repositories are <em>still available</em> here, archived for historical purposes: <a href="https://notabug.org/osboot">https://notabug.org/osboot</a></p>
<div><h2 id="osbootlibreboot-merge">osboot/libreboot merge</h2></div>
<p>In November 2022, I <em>shut down</em> osboot’s website and redirected it to the Libreboot website, merging all of its documentation and additional code into Libreboot. Libreboot <em>adopted</em> OSBoot policy, verbatim. The <a href="https://libreboot.org/news/policy.html">Binary Blob Reduction Policy</a> <em>is</em> that policy - the <a href="https://web.archive.org/web/20221107235850/https://libreboot.org/news/policy.html">old Libreboot policy</a> was declared obsolete, and abandoned - the main problem with it, and the problem with GNU Boot today which is based on it, is that it limited the amount of hardware that Libreboot could support.</p>
<p>OSBoot was always the superior project, and Libreboot was practically dead, so I saw nothing to lose and just did it. I merged them together.</p>
<div><h2 id="so-why-talk-about-gnu-boot">So why talk about GNU Boot?</h2></div>
<p>Ordinarily, I would ignore other projects; it’s not that I’m bothered by them, it’s just that I have Libreboot, which pleases me, and therefore I have no need to worry about the others. They can sort themselves out. I work collaboratively with a few other coreboot distros; for example, I sometimes provide advice or ideas to the <a href="https://osresearch.net/">Heads</a> project (a very interesting project, superior to Libreboot in many ways). I recently helped them by offering to host tarballs for them, that they use in their build system.</p>
<p>But that’s just the problem: when GNU Boot first launched, as a failed <em>hostile fork</em> of Libreboot <em>under the same name</em>, I observed: their code repository was based on Libreboot from late 2022, and their website based on Libreboot in late 2021. Their same-named Libreboot site was announced during LibrePlanet 2023, by this video: <a href="https://media.libreplanet.org/u/libreplanet/m/taking-control-over-the-means-of-production-free-software-boot/">https://media.libreplanet.org/u/libreplanet/m/taking-control-over-the-means-of-production-free-software-boot/</a> - their speaker is Denis Carikli, an early contributor to Libreboot, who you can read about here: <a href="https://libreplanet.org/2023/speakers/#6197">https://libreplanet.org/2023/speakers/#6197</a>. Denis is one of the founders of that project.</p>
<p>Well, now they are calling themselves <em>GNU Boot</em>, and it is indeed GNU, but it still has the same problem as of <em>today</em>: still based on very old Libreboot, and they don’t <em>even</em> have a website. According to Savannah, GNU Boot was created on 11 June 2023. Yet no real development, in over a month since then.</p>
<p>I have this itch in the back of my mind, that says: if you’re going to do something, you should <em>do it</em>. When someone expresses disagreement with what I say, I can respect it if it’s more than just words, which is all what they had given at the time of this article.</p>
<p>I value <em>technical excellence</em>.</p>
<div><h2 id="so-why-talk-about-it">So <em>why talk about it??</em></h2></div>
<p>Simple: I’ve decided that I want to <strong>help them</strong>. Refer to the links above, in the early section of this article. I decided recently that I’d simply make a release <em>for them</em>, exactly to their specifications (GNU Free System Distribution Guidelines), talking favourably about FSF/GNU, and so on. I’m in a position to <em>do it</em> (thus scratching the itch), so why not?</p>
<p><strong>I did this release for them: <a href="https://notgnuboot.vimuser.org/news/nongenuineboot20230717.html">https://notgnuboot.vimuser.org/news/nongenuineboot20230717.html</a></strong> - it’s designated <em>non-GeNUine Boot 20230717</em>, and I encourage them to re-use this in their project, to get off the ground. This completely leapfrogs their current development; it’s months ahead. <em>Months</em>. <strong>It’s 8 months ahead</strong>, since their current revision is based upon Libreboot from around ~October 2022.</p>
<p>The most remarkable thing of all is this: in December 2022 is when I first learned of their supposed effort. They tried to poach several Libreboot developers behind my back, but none of them were interested it seems, and one of them leaked the existence of their effort to me. I knew <em>three months</em> before they announced that they were going to announce something, and I reliably predicted it’d be at LibrePlanet.</p>
<p>The most absurd thing of that is: why did they not contact <em>me</em>?</p>
<p>The GNU people should have simply contacted me from the start. I <em>would</em> have helped them. I did Libreboot releases under their policies for <em>years</em>, and I know what I’m doing. Ideology aside, I enjoy fun technical challenges; I have a wide depth of knowledge and expertise. <em>I offer it now</em>, as I have today, and will continue to do so. I offer my <em>support</em>, in service to it, even if I would personally never use nor recommend their project. One of the purposes of today’s article is simply to tell people they exist, because I hope maybe they’ll get more devs. They use the same build system as Libreboot, so Libreboot could even merge a lot of any actual code/ideas that they produce (and they can merge our work - <em>and I want them to do that</em>).</p>
<p>There were/are more things to talk about, but I’m not really interested in writing more. Free as in freedom? Libreboot is a free software project, yet GNU propaganda says otherwise.</p>
<p>GNU Boot is <a href="https://libreboot.org/policy.html#problems-with-fsdg">inferior</a> to Libreboot in every way, just as Libreboot was inferior to OSBoot before the Libreboot/OSBoot merge; since modern (post-merge) Libreboot still provides the same blob-free configurations on mainboards when that is possible, GNU Boot is also a <em>pointless</em> project, just as Libreboot was before I merged osboot with it, but I digress.</p>
<p>What more is there to say?</p>
<p>Happy hacking!</p>
<div><h2 id="update-21-july-2023">UPDATE (21 July 2023)</h2></div>
<p>The non-GeNUine Boot website, and the non-GeNUine release itself, was originally <em>named</em> GNU Boot, but clearly marked as <em>unofficial</em>, with the hope that the GNU project would adapt and re-use it for their project. I did this, specifically to help them get up to date. They currently use Libreboot from about 8 months ago (late 2022), and that revision used <em>coreboot</em> releases from ~mid 2021.</p>
<p>Modern Libreboot uses coreboot from early 2023, and contains many bug fixes in its build system, owing to an extensive <a href="https://libreboot.org/news/audit.html">build system audit</a>; GNU Boot still contains all of the bugs that existed, prior to the audit. Bugs such as: errors literally not being handled, in many critical areas of the build system, due to improper use of subshells within shell scripts (Libreboot’s build system is implemented with shell scripts), improper handling of git credentials in the coreboot build system, fam15h boards no longer compiling correct on modern Linux distros… the list goes on. All fixed, in newer Libreboot, including the recent release.</p>
<div><h2 id="gnu-boot-cease-and-desist-email">GNU Boot cease and desist email</h2></div>
<p>The GNU Boot people actually sent me a cease and desist email, citing trademark infringement. Amazing.</p>
<p>Despite the <a href="https://notgnuboot.vimuser.org/">nonGeNUine Boot</a> site having clearly stating that it’s unofficial, and <em>not</em> the GNU Boot project. I literally made it to help them. You know, to help them use newer Libreboot because they use old Libreboot and even older coreboot.</p>
<p>Anyway, I complied with their polite request and have renamed the project to non-GeNUine Boot. The release archive was re-compiled, under this new brand name and the website was re-written accordingly.</p>
<p>Personally, I like the new name better.</p>
<p>Here is a screenshot of the cease and desist request that I received, from <em>Adrien ‘neox’ Bourmault</em> who is a founding member of the GNU Boot project:</p>
<p><img loading="lazy" src="https://av.vimuser.org/email.png"></p>
<p>This, after they themselves tried to steal the name <em>Libreboot</em> for their fork, when they first announced themselves on 19 March 2023 at LibrePlanet, only renaming to <em>GNU Boot</em> months later (on 11 June 2023). Utter hypocrisy, and a great irony to boot.</p>
<p>I may very well send patches. <em>If I want to</em>.</p>

<p>Markdown file for this page: <a href="https://libreboot.org/news/gnuboot.md">https://libreboot.org/news/gnuboot.md</a></p>
<p><a href="https://libreboot.org/feed.xml">Subscribe to RSS for this site</a></p>
<p><a href="https://libreboot.org/sitemap.html">Site map</a></p>
<p>This HTML page was generated by the <a href="https://untitled.vimuser.org/">untitled static site generator</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Snowflake (276 pts)]]></title>
            <link>https://snowflake.torproject.org/</link>
            <guid>36926569</guid>
            <pubDate>Sun, 30 Jul 2023 00:59:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://snowflake.torproject.org/">https://snowflake.torproject.org/</a>, See on <a href="https://news.ycombinator.com/item?id=36926569">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
        

        <p><img src="https://snowflake.torproject.org/snowflake-schematic.png" alt="Diagram"></p>
  
        <p data-msgid="__MSG_websiteIntro__">Snowflake is a system that <strong>allows people from all over the world to access censored websites and applications</strong>. Similar to how VPNs assist users in getting around Internet censorship, Snowflake helps you avoid being noticed by Internet censors by making your Internet activity appear as though you're using the Internet for a regular video or voice call.</p>

        <p data-msgid="__MSG_websiteIntro2__">There are numerous tools available, such as Snowflake, that "transform" Internet activity, each using a different technique. Some redirect Internet traffic to appear to be coming from popular cloud providers like Microsoft Azure and Amazon Web Services. Others scramble Internet traffic in order to make it appear completely random.</p>
                        
        <p data-msgid="__MSG_websiteIntro3__">It therefore becomes costly for censors to consider blocking such circumvention tools since it would require blocking large parts of the Internet in order to achieve the initial targeted goal.</p>

        <div>

          <section id="browser">
            <h2 data-msgid="__MSG_browser__">Use Snowflake to bypass censorship</h2>

            <p data-msgid="__MSG_censoredUsers__">Unlike VPNs, you do not need to install a separate application to connect to a Snowflake proxy and bypass censorship. It is usually a circumvention feature embedded within existing apps. Currently Snowflake is available inside <a href="https://torproject.org/download">Tor Browser</a> on Desktop and Android, <a href="https://onionbrowser.com/">Onion Browser</a> on iOS, and <a href="https://orbot.app/">Orbot</a> on Android and iOS. If you have downloaded and installed any of these apps, and they are censored in your country, you can bypass the censorship by activating Snowflake through the apps' settings page.</p>

         </section>

         <section>
          <p><img src="https://snowflake.torproject.org/screenshot.png" alt="Tor Browser screenshot"></p>
 
        </section>

        </div>

        <section id="extension">
            <h2 data-msgid="__MSG_extension__">Help people circumvent censorship: operate a Snowflake proxy</h2>

            <p data-msgid="__MSG_installExtension__">Did you know that Snowflake proxies are operated entirely by volunteers? In other words, a user gets matched with a random Snowflake volunteer proxy, which is run by a volunteer like you! So, if you want to help people bypass censorship, consider installing and running a Snowflake proxy. The only prerequisite is that the Internet in your country is <strong>not</strong> heavily censored already.</p>

            <p data-msgid="__MSG_volunteer__">You can join thousands of volunteers from around the world who have a Snowflake proxy installed and running. There is no need to worry about which websites people are accessing through your Snowflake proxy. Their visible browsing IP address will match their Tor exit node, not yours.</p>

            <p data-msgid="__MSG_installOptions__">There are different ways to run a Snowflake proxy (beginner to advanced):</p>

            <h3 data-msgid="__MSG_installWebExtensionIntro__">Install the web extension</h3>

            <p data-msgid="__MSG_installWebExtension__">The web extension is the easiest way to run a Snowflake proxy. Simply install it on <a href="https://addons.mozilla.org/en-US/firefox/addon/torproject-snowflake/">Firefox</a> or <a href="https://chrome.google.com/webstore/detail/snowflake/mafpmfcccpbjnhfhjnllmmalhifmlcie">Chrome</a>, enable the extension, and watch the icon turn green when a user connects through your proxy!</p>
            <p>
            <a href="https://addons.mozilla.org/en-US/firefox/addon/torproject-snowflake/">
              <img src="https://snowflake.torproject.org/200px-Firefox_logo.png" alt="Install in Firefox" height="100"><br>
              <span data-msgid="__MSG_installFirefox__">Install in Firefox</span>
            </a>
            <a href="https://chrome.google.com/webstore/detail/snowflake/mafpmfcccpbjnhfhjnllmmalhifmlcie">
              <img src="https://snowflake.torproject.org/200px-Chrome_logo.png" alt="Install in Chrome" height="100"><br>
              <span data-msgid="__MSG_installChrome__">Install in Chrome</span>
            </a>
            <a href="https://chrome.google.com/webstore/detail/snowflake/mafpmfcccpbjnhfhjnllmmalhifmlcie">
              <img src="https://snowflake.torproject.org/200px-Edge_logo.png" alt="Install in Edge" height="100"><br>
              <span data-msgid="__MSG_installEdge__">Install in Edge</span>
            </a>
            </p>

            <h3 data-msgid="__MSG_webBadgeTitle__">Leave this browser tab open or embed a web badge on your website</h3>

            <p data-msgid="__MSG_webBadgeInstructions__">If you switch on the Snowflake below and leave the browser tab open, a user can connect through your new proxy!</p>

            

            <p data-msgid="__MSG_possible__">Alternatively, you can embed a Snowflake proxy yourself inside a page in your own website (e.g., <a href="https://relay.love/">relay.love</a>). Visitors to your site can enter the page, enable the proxy, and leave it open to allow people to proxy through it (it behaves and looks exactly like the web extension).</p>

            


            <h3 data-msgid="__MSG_StandaloneTitle__">Run a standalone proxy</h3>
            <p data-msgid="__MSG_installStandalone__">If you would like to run a command-line version of the Snowflake proxy on your desktop or server, see our <a href="https://community.torproject.org/relay/setup/snowflake/standalone/">guide</a> for running a Snowflake standalone proxy.</p>

          </section>

        <section id="faq">

          <h2 data-msgid="__MSG_faq__">Seeking support with using Snowflake</h2>
 
          <p data-msgid="__MSG_support__">If you encounter issues while trying to connect to Tor using Snowflake, the Tor support channel can be reached on <a href="https://t.me/TorProjectSupportBot">Telegram</a>. You can also browse the <a href="https://support.torproject.org/censorship/">Tor Support Portal</a> and the <a href="https://forum.torproject.net/tag/snowflake">Tor Forum</a> for answers.</p>

        </section>

        <section id="bugs">
          <h2 data-msgid="__MSG_reportingBugs__">Reporting Bugs</h2>

          <p data-msgid="__MSG_fileBug__">If you encounter problems with Snowflake - whether you're using it or running it -, please consider filing a bug report.  There are two ways to file a bug report:</p>

          <ol>
            <li data-msgid="__MSG_bugTracker__"><a href="https://gitlab.onionize.space/">Request an account</a> at the Tor Project GitLab, then <a href="https://gitlab.torproject.org/tpo/anti-censorship/pluggable-transports/snowflake/-/issues">open a new issue</a> in the Snowflake project.</li>
            <li data-msgid="__MSG_sharedAccount__">File an <a href="https://anonticket.onionize.space/">anonymous ticket</a> by generating an identifier and logging in with it. Then, find the Snowflake project in the <strong>List of all projects</strong> and create a new issue.</li>
          </ol>

          <p data-msgid="__MSG_descriptive__">Please try to be as descriptive as possible with your ticket and if possible include log messages that will help us reproduce the bug.</p>
        </section>

        <section id="overview">
          
          <h2 data-msgid="__MSG_howSnowflakeWorks__">Learn more about how Snowflake works</h2>

          <p data-msgid="__MSG_snowflakeTechOverview__">Snowflake is a new circumvention technology, part of the <a href="https://gitweb.torproject.org/torspec.git/tree/pt-spec.txt">Pluggable Transports</a> family, that is continuously being improved. Curious to learn more about its architecture? Feel free to check this <a href="https://gitlab.torproject.org/tpo/anti-censorship/pluggable-transports/snowflake/-/wikis/Technical%20Overview">Technical overview</a> (in English).</p>

          <p data-msgid="__MSG_contactSnowflakeDevs__">If you're interested in making use of Snowflake inside your application, get in touch with <a href="https://lists.torproject.org/cgi-bin/mailman/listinfo/anti-censorship-team">anti-censorship team</a>.</p>

       </section>

      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Pencils don’t draw straight on replaced iPad screens (139 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/07/apple-pencils-cant-draw-straight-on-third-party-replacement-ipad-screens/</link>
            <guid>36926276</guid>
            <pubDate>Sun, 30 Jul 2023 00:14:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/07/apple-pencils-cant-draw-straight-on-third-party-replacement-ipad-screens/">https://arstechnica.com/gadgets/2023/07/apple-pencils-cant-draw-straight-on-third-party-replacement-ipad-screens/</a>, See on <a href="https://news.ycombinator.com/item?id=36926276">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      Jittery repair prospects    —
</h4>
            
            <h2 itemprop="description">It's similar to the Face ID failures of the iPhone 13's screen, later fixed.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/07/ipad_drawing_top-800x604.png" alt="Gloved hands using an Apple Pencil on an iPad Pro with squiggly results">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/07/ipad_drawing_top.png" data-height="705" data-width="934">Enlarge</a> <span>/</span> iCorrect's attempts to draw a straight line on an iPad Pro with a third-party replacement screen led them to look at the screen's embedded chips for parts-pairing problems.</p></figcaption>  </figure>

  




<!-- cache hit 64:single/related:1273a2aa2dc5ab6bb8eb636425bb45a9 --><!-- empty -->
<p>The latest part of an Apple device to demand a repair by its maker appears to be the screens on newer iPads. Reports from repair shops and customers suggest that Apple Pencils no longer work properly on non-genuine Apple screens, as they draw squiggly lines on a diagonal instead of straight.</p>
<p>Ricky Panesar, CEO of UK repair firm iCorrect, <a href="https://www.forbes.com/sites/jaymcgregor/2023/07/27/apple-punishing-ipad-pro-buyers-with-new-pencil-software-lockdown/?sh=71a333304b5f">told Forbes</a> that screens replaced on newer iPad Pros (fifth and sixth-generation 12.9-inch and third and fourth-generation 11-inch models) do not deliver straight lines when an <a href="https://amzn.to/3QhfZdL" target="_blank" rel="noopener">Apple Pencil</a> is used to draw at an angle. "They have a memory chip that sits on the screen that's programmed to only allow the Pencil functionality to work if the screen is connected to the original logic board," Panesar told Forbes.</p>
<p>A <a href="https://www.reddit.com/r/ipad/comments/13q5dvo/jittery_diagonal_lines_on_ipad_mini_using_apple/">Reddit post from May 23</a> from a user reporting "jittery" diagonal lines from an Apple <a href="https://amzn.to/3QhfZdL" target="_blank" rel="noopener">Pencil</a> on a newly replaced iPad mini screen suggests the issue may affect more than just the Pro line of iPads.</p>
<figure><p><iframe type="text/html" width="980" height="550" src="https://www.youtube.com/embed/0sWmBNj6Eok?start=0&amp;wmode=transparent" frameborder="0" allowfullscreen=""></iframe></p><figcaption><p>Video demonstrating the specific malfunctioning of an Apple Pencil on a newer iPad Pro with a replacement screen by UK repair firm iCorrect.</p></figcaption></figure>
<p>Apple continues to suggest, in varying degrees of forcefulness, that it be the only company to repair its customers' devices. At the least, the company seems to believe that only Apple Genuine Replacement Parts should be used by <a href="https://arstechnica.com/gadgets/2019/08/apple-gives-third-party-repair-shops-more-access-to-authorized-parts/">licensed technicians</a> or—with <a href="https://arstechnica.com/gadgets/2022/08/macbook-self-repair-program-highlights-apples-flawed-repairability-progress/">a suitcase full of tools</a>—<a href="https://arstechnica.com/gadgets/2021/11/apple-will-soon-send-parts-and-tools-to-users-who-want-to-repair-their-phones/">individuals</a>. Every other kind of repair is subject to complications by Apple's tying of parts to individual devices, known as <a href="https://arstechnica.com/tech-policy/2023/05/france-is-fighting-to-save-your-iphone-from-an-early-death">serialization</a>. Batteries, screens, and Touch ID sensors are all subject to display warnings to users or lose some functionality when transplanted outside of Apple's repair network.</p>                                            
                                                        
<p>This stance, and the company's size and influence, makes Apple a primary target of <a href="https://arstechnica.com/gadgets/2022/05/the-same-phone-for-25-years-ifixit-on-right-to-repairs-remaining-obstacles-hope/">right-to-repair campaigns</a>, as well as legislation in the EU seeking to boost their devices' interoperability, including <a href="https://arstechnica.com/gadgets/2022/12/the-clock-is-rapidly-ticking-on-apples-lightning-charger/">USB-C mandates</a>.</p>
<p>This latest complaint by the repair community about screen pairing echoes an issue that arose with the then-new iPhone 13 line in 2021. Repair techs found that replacing the screen, even with a genuine Apple screen with the Face ID module laboriously transferred over (but not paired by Apple's proprietary software), would <a href="https://arstechnica.com/gadgets/2021/11/apple-will-no-longer-break-face-id-on-repaired-iphone-13s/">disable Face ID on the device</a>. Apple <a href="https://www.theverge.com/2021/11/9/22772433/apple-iphone-13-screen-replacements-face-id-software-update">later told The Verge</a> that it would "release a software update" to fix the issue without detailing whether it was a bug or an intentional design choice.</p>
<p>Ars contacted Apple for comment and will update this article if we receive a response.</p>
<p><em>Disclosure: Kevin Purdy previously worked for iFixit. He holds no financial interest in the company.</em></p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ASML EUV Lithography Machine Could Keep Moore’s Law on Track (198 pts)]]></title>
            <link>https://spectrum.ieee.org/high-na-euv</link>
            <guid>36926228</guid>
            <pubDate>Sun, 30 Jul 2023 00:07:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/high-na-euv">https://spectrum.ieee.org/high-na-euv</a>, See on <a href="https://news.ycombinator.com/item?id=36926228">Hacker News</a></p>
<div id="readability-page-1" class="page"><p> This photo-illustration of the EXE:5000, ASML’s high-numerical-aperture extreme-ultraviolet-lithography machine, shows its massive scale. </p><div data-elid="2662495820" data-post-url="https://spectrum.ieee.org/high-na-euv" data-authors="Jan van Schoot" data-headline="This Machine Could Keep Moore’s Law on Track" data-page-title="This Machine Could Keep Moore’s Law on Track - IEEE Spectrum"><p><strong>Over the last half-century</strong>, we’ve come to think of <a href="https://spectrum.ieee.org/tag/moore-s-law?__rblms_page_revalidate=1" target="_self">Moore’s Law</a>—the roughly biennial doubling of the number of transistors in a given area of silicon, the gains that drive computing forward—as something that just happens, as though it were a natural, inevitable process, akin to evolution or aging. The reality, of course, is much different. Keeping pace with Moore’s Law requires almost unimaginable expenditures of time, energy, and human ingenuity—thousands of people on multiple continents and endless acres of some of the most complex machinery on the planet.
</p><p>
	Perhaps the most essential of these machines performs 
	<a href="https://spectrum.ieee.org/tag/euv?__rblms_page_revalidate=1" target="_self">extreme-ultraviolet (EUV) photolithography</a>. EUV lithography, the product of decades of R&amp;D, is now the driving technology behind the past two generations of cutting-edge chips, used in every top-end smartphone, tablet, laptop, and server in the last three years. Yet Moore’s Law must march on, and chipmakers continue to advance their road maps, meaning they’ll need to shrink device geometries even further.
</p><p>
	So at 
	<a href="https://www.asml.com/en" target="_blank">ASML</a>, my colleagues and I are developing the next generation of lithography. Called high-numerical-aperture EUV lithography, it involves a major overhaul of the system’s internal optics. High-NA EUV should be ready for commercial use in 2025, and chipmakers are depending on its capabilities to keep their promised advances through the end of this decade
</p><h2>The 3 factors of photolithography
</h2><p>
	Moore’s Law relies on improving the resolution of photolithography so chipmakers can lay down finer and finer circuits. Over the last 35 years, engineers have achieved a resolution reduction of two orders of magnitude by working on a combination of three factors: the wavelength of the light; k
	<sub>1</sub>, a coefficient that encapsulates process-related factors; and numerical aperture (NA), a measure of the range of angles over which the system can emit light.
</p><p data-rm-resized-container="25%"><img alt="An equation with four variablesCAPTION: The critical dimension\u2014the resolution of a photolithography system\u2014is equal to the wavelength of light used divided by the numerical aperture and multiplied by a quality, k1, related to process improvements. " data-rm-shortcode-id="fdb861cd8f18df59eb46c689063ed42d" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/an-equation-with-four-variablescaption-the-critical-dimension-u2014the-resolution-of-a-photolithography-system-u2014is-equal-to.png?id=34683428&amp;width=980" height="1667" id="65497" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/an-equation-with-four-variablescaption-the-critical-dimension-u2014the-resolution-of-a-photolithography-system-u2014is-equal-to.png?id=34683428&amp;width=980" width="2500"><small placeholder="Add Photo Credit...">Source: <a href="https://spectrum.ieee.org/">IEEE Spectrum</a></small></p><p>
	The critical dimension—that is, the smallest possible feature size you can print with a certain photolithography-exposure tool—is proportional to the wavelength of light divided by the numerical aperture of the optics. So you can achieve smaller critical dimensions by using either shorter light wavelengths or larger numerical apertures or a combination of the two. The k
	<sub>1</sub> value can be pushed as close as possible to its physical lower limit of 0.25 by improving manufacturing-process control, for example.
</p><p>
	In general, the most economical ways to boost resolution are by increasing the numerical aperture and by improving tool and process control to allow for a smaller k
	<sub>1</sub>. Only after chipmakers run out of options to further improve NA and k<sub>1</sub> do they resort to reducing the wavelength of the light source.
</p><p>
	Nevertheless, the industry has had to make that wavelength change a number of times. The historical progression of wavelengths went from 365 nanometers, generated using a mercury lamp, to 248 nm, via a krypton-fluoride laser, in the late 1990s, and then to 193 nm, from an argon-fluoride laser, at the beginning of this century. For each generation of wavelength, the numerical aperture of lithography systems was progressively increased before industry jumped to a shorter wavelength.
</p><p>
	For example, as the use of 193 nm was coming to an end, a novel approach to increasing NA was introduced: 
	<a href="https://spectrum.ieee.org/chip-makings-wet-new-world" target="_self">immersion lithography</a>. By placing water between the bottom of the lens and the wafer, the NA could be significantly enlarged from 0.93 to 1.35. From its introduction around 2006, 193-nm immersion lithography was the industry workhorse for leading-edge lithography
</p><p><img alt="A chart shows dots descending from right to left. The dots are grouped by color and labelled" data-rm-shortcode-id="ffef84c78440b0a719eb798bb8bcd14f" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-chart-shows-dots-descending-from-right-to-left-the-dots-are-grouped-by-color-and-labelled.png?id=34684539&amp;width=980" height="7009" id="0b41d" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-chart-shows-dots-descending-from-right-to-left-the-dots-are-grouped-by-color-and-labelled.png?id=34684539&amp;width=980" width="10417"><small placeholder="Add Photo Caption...">The resolution of photolithography has improved about 10,000-fold over the last four decades. That’s due in part to using smaller and smaller wavelengths of light, but it has also required greater numerical aperture and improved processing techniques.</small><small placeholder="Add Photo Credit...">Source: ASML</small></p><h2>The dawn of EUV<br></h2><p>
	But as the need to print features smaller than 30 nm increased, and because the NA of 193-nm lithography had been maxed out, keeping up with Moore’s Law grew more and more complex. To create features smaller than 30 nm requires either using multiple patterns to produce a single layer of chip features—a technologically and economically burdensome technique—or another change of wavelength. It took more than 20 years and an 
	<a href="https://spectrum.ieee.org/euv-lithography-finally-ready-for-chip-manufacturing" target="_self">unparalleled development effort</a> to bring the next new wavelength online: 13.5-nm EUV.
</p><p>
	EUV necessitates an entirely new way to generate light. It’s a remarkably complex process that involves hitting molten tin droplets in midflight with a powerful CO<sub>2</sub> laser. The laser vaporizes the tin into a plasma, emitting a spectrum of photonic energy. From this spectrum, the EUV optics harvest the required 13.5-nm wavelength and direct it through a series of mirrors before it is reflected off a patterned mask to project that pattern onto the wafer. And all of this must be done in an ultraclean vacuum, because the 13.5-nm wavelength is absorbed by air. (In previous generations of photolithography, light was directed through the mask to project a pattern onto the wafer. But EUV is so readily absorbed that the mask and other optics must be reflective instead.)
</p><p><img alt="A cutaway of a rectangular machine. Purple beams bounce off objects within the machine." data-rm-shortcode-id="2cfd20962a6e1b5082da079d8a9b85ae" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-cutaway-of-a-rectangular-machine-purple-beams-bounce-off-objects-within-the-machine.jpg?id=34683370&amp;width=980" height="1400" id="41478" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-cutaway-of-a-rectangular-machine-purple-beams-bounce-off-objects-within-the-machine.jpg?id=34683370&amp;width=980" width="2500"><small placeholder="Add Photo Caption...">In a vacuum chamber, EUV light [purple] reflects off multiple mirrors before bouncing off the photomask [top center]. From there the light continues its journey until it is projected onto the wafer [bottom center], carrying the photomask’s pattern. The illustration shows today’s commercial system with a 0.33 numerical aperture. The optics in future systems, with an NA of 0.55, will be different.</small><small placeholder="Add Photo Credit...">Source: ASML</small></p><p>
	The switch to EUV from 193-nanometer light did part of the job of decreasing the critical dimension. A process called “design for manufacturing,” which involves setting the design rules of circuit blocks to take advantage of photolithography’s limits, has done a lot to reduce k
	<sub>1</sub>. Now it’s time to boost numerical aperture again, from today’s 0.33 to 0.55.<br></p><h2>Making high-NA EUV work </h2><p>
	Increasing the NA from today’s 0.33 to the target value of 0.55 inevitably entails a cascade of other adjustments. Projection systems like EUV lithography have an NA at the wafer and also at the mask. When you increase the NA at the wafer, it also increases the NA at the mask. Consequently, at the mask, the incoming and outgoing cones of light become larger and must be angled away from each other to avoid overlapping. Overlapping cones of light produce an asymmetric diffraction pattern, resulting in unpleasant imaging effects.
</p><p>
	But there’s a limit to this angle. Because the reflective masks needed for EUV lithography are actually made of multiple layers of material, you can’t ensure getting a proper reflection above a certain reflective angle. EUV masks have a maximum reflective angle of 11 degrees. There are other challenges as well, but reflective angle is the biggest.
</p><p><img alt="A line chart curves up and then descends to the right. A point on the descent is highlighted." data-rm-shortcode-id="44fd43e4e03eea99eebe0c03dd556ebf" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-line-chart-curves-up-and-then-descends-to-the-right-a-point-on-the-descent-is-highlighted.png?id=34683768&amp;width=980" height="3368" id="ea75c" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-line-chart-curves-up-and-then-descends-to-the-right-a-point-on-the-descent-is-highlighted.png?id=34683768&amp;width=980" width="5167"><small placeholder="Add Photo Caption..."> If the EUV light strikes the photomask at too steep an angle, it will not reflect properly.</small><small placeholder="Add Photo Credit...">Source: ASML</small></p><p><img alt="A row of 3 images shows purple cones pointing toward a patterned square." data-rm-shortcode-id="8cb40cfc98362e987a84b6370a4871fe" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-row-of-3-images-shows-purple-cones-pointing-toward-a-patterned-square.jpg?id=34686313&amp;width=980" height="556" id="6fe6f" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-row-of-3-images-shows-purple-cones-pointing-toward-a-patterned-square.jpg?id=34686313&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">The angle of reflection at the mask in today’s EUV is at its limit [left] Increasing the numerical aperture of EUV would result in an angle of reflection that is too wide [center]. So high-NA EUV uses anamorphic optics, which allow the angle to increase in only one direction [right]. The field that can be imaged this way is half the size, so the pattern on the mask must be distorted in one direction, but that’s good enough to maintain throughput through the machine.</small><small placeholder="Add Photo Credit...">Source: ASML</small></p><p>
	The only way to overcome this challenge is to increase a quality called demagnification. Demagnification is exactly what it sounds like—taking the reflected pattern from the mask and shrinking it. To compensate for the reflective-angle problem, my colleagues and I had to double the demagnification to 8x. As a consequence, the part of the mask imaged will be much smaller on the wafer. This smaller image field means it will take longer to produce the complete chip pattern. Indeed, this requirement would reduce the throughput of our high-NA scanner to under 100 wafers per hour—a productivity level that would make chip manufacturing uneconomical.
	<br></p><p>
	Thankfully, we found that it is necessary to increase the demagnification in only one direction—the one in which the largest reflective angles occur. The demagnification in the other direction can remain unchanged. This results in an acceptable field size on the wafer—about half the size used in today’s EUV systems, or 26 by 16.5 millimeters instead of 26 by 33 mm. This kind of direction-dependent, or anamorphic, demagnification forms the basis of our high-NA system. The optics manufacturer Carl Zeiss has made a herculean effort to design and manufacture an anamorphic lens with the specifications required for our new machine.
</p><p>
	To ensure the same productivity levels with the half-size field, we had to redevelop the system’s reticle and wafer stages—the platforms that hold the mask and wafer, respectively—and move them in sync with each other as the scanning process takes place. The redesign resulted in nanometer-precision stages with acceleration improved by a factor of four.
</p><h2>High-NA EUV in production in 2025 </h2><p>
	The first high-NA EUV system, the ASML EXE:5000, will be installed in a new lab that we’re opening jointly with the Belgium-based nanoelectronics research facility Imec, in early 2024. This lab will allow customers, mask makers, photoresist suppliers, and others to develop the infrastructure needed to make high-NA EUV a reality.
</p><p>
	And it is essential that we do make it a reality, because high-NA EUV is a critical component in keeping Moore’s Law alive. Getting to 0.55 NA won’t be the final step, though. From there, ASML, Zeiss, and the entire semiconductor ecosystem will be stretching even further toward technologies that are better, faster, and innovative in ways we can hardly imagine yet. 
	<span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Button Pushes You (2022) (122 pts)]]></title>
            <link>https://despens.systems/2022/06/button-pushes-you/</link>
            <guid>36926165</guid>
            <pubDate>Sat, 29 Jul 2023 23:58:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://despens.systems/2022/06/button-pushes-you/">https://despens.systems/2022/06/button-pushes-you/</a>, See on <a href="https://news.ycombinator.com/item?id=36926165">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header><p>2022-06-04</p></header><section><p>A “call to action,” also known as <em>CTA</em>, is an interface design technique based on “engaging” text labels on widgets to direct users towards a previously defined goal within an application, website, device, etc. (That goal is usually defined by the designers of the system.) Every computer user is probably familiar with buttons labeled in an ambiguous voice, oscillating in between presenting dialog options from the perspective of the user (“I agree to terms and conditions”—the user is talking) and the system offering and suggesting options (“Follow us”—the system is talking).</p><p>In most cases buttons labeled in this way can be considered pretty classic interface design: they’re presenting actions a user can activate to change the state of the system according to the user’s mental model. For instance, after pushing “I agree to terms and conditions” the system gained information from the user and will present different options to them; after pushing “Follow us” the system is reconfigured to frequently communicate with the user.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p><p>A few years ago, a new style of button labeling emerged that appears only sightly different, but turns around the whole idea of what a button is. Such buttons are labeled “Get started,” “Explore,” “Launch experience,” etc. and are links to other parts of a system. Pushing them doesn’t change anything in the system’s state, as could be expected from a classic button. Instead, they’re supposed to reconfigure the user’s state. Users have to accept the spelled out mantra and change their attitude before accessing the next piece of information. The work required for this reconfiguration is entirely on the side of the user, the computer doesn’t act as a tool to complete this mini task, the user has to do it on their own.</p><p>Below are a few examples screenshotted from random mainstream websites.</p><figure><a href="https://despens.systems/2022/06/button-pushes-you/start-now.png"><img src="https://despens.systems/2022/06/button-pushes-you/start-now_hu23385a22429f5f26f328934b4a8cf00e_8991_150x0_resize_mitchellnetravali_3.png" width="150" height="56" srcset="https://despens.systems/2022/06/button-pushes-you/start-now_hu23385a22429f5f26f328934b4a8cf00e_8991_150x0_resize_mitchellnetravali_3.png 1x, https://despens.systems/2022/06/button-pushes-you/start-now.png 2x, https://despens.systems/2022/06/button-pushes-you/start-now_hu23385a22429f5f26f328934b4a8cf00e_8991_150x0_resize_mitchellnetravali_3.png"></a></figure><figure><a href="https://despens.systems/2022/06/button-pushes-you/start-doing.png"><img src="https://despens.systems/2022/06/button-pushes-you/start-doing_hud7cd3b2798636930afbb98cee5514fae_9448_150x0_resize_mitchellnetravali_3.png" width="150" height="49" srcset="https://despens.systems/2022/06/button-pushes-you/start-doing_hud7cd3b2798636930afbb98cee5514fae_9448_150x0_resize_mitchellnetravali_3.png 1x, https://despens.systems/2022/06/button-pushes-you/start-doing.png 2x, https://despens.systems/2022/06/button-pushes-you/start-doing_hud7cd3b2798636930afbb98cee5514fae_9448_150x0_resize_mitchellnetravali_3.png"></a></figure><figure><a href="https://despens.systems/2022/06/button-pushes-you/create.png"><img src="https://despens.systems/2022/06/button-pushes-you/create_huffaae38e25742b8c1b10d4faeb33f0cd_6141_150x0_resize_mitchellnetravali_3.png" width="150" height="76" srcset="https://despens.systems/2022/06/button-pushes-you/create_huffaae38e25742b8c1b10d4faeb33f0cd_6141_150x0_resize_mitchellnetravali_3.png 1x, https://despens.systems/2022/06/button-pushes-you/create.png 2x, https://despens.systems/2022/06/button-pushes-you/create_huffaae38e25742b8c1b10d4faeb33f0cd_6141_150x0_resize_mitchellnetravali_3.png"></a></figure><p>The “Start now” button prompts users to change plans they might have had and “start” right away, instead of maybe next week after completing another project, or on returning from vacation. The “Start doing” button avoids being specific about what should be done and is thereby referring to doing as a value in itself. The user should transform themselves into a “doer,” rather than being considerate, evaluating options, or—lazy, a looser, a mere consumer. The “Create” button welcomes confident “creators.” Before proceeding, users should identify with this new aristocratic class.</p><figure><a href="https://despens.systems/2022/06/button-pushes-you/explore.png"><img src="https://despens.systems/2022/06/button-pushes-you/explore_hub8b1747f8f1d6e7463ef62ef44816422_43449_150x0_resize_mitchellnetravali_3.png" width="150" height="72" srcset="https://despens.systems/2022/06/button-pushes-you/explore_hub8b1747f8f1d6e7463ef62ef44816422_43449_150x0_resize_mitchellnetravali_3.png 1x, https://despens.systems/2022/06/button-pushes-you/explore.png 2x, https://despens.systems/2022/06/button-pushes-you/explore_hub8b1747f8f1d6e7463ef62ef44816422_43449_150x0_resize_mitchellnetravali_3.png"></a></figure><figure><a href="https://despens.systems/2022/06/button-pushes-you/explore-more.png"><img src="https://despens.systems/2022/06/button-pushes-you/explore-more_huf7773008956bb73b1a5fa18dd5f290da_8123_150x0_resize_mitchellnetravali_3.png" width="150" height="56" srcset="https://despens.systems/2022/06/button-pushes-you/explore-more_huf7773008956bb73b1a5fa18dd5f290da_8123_150x0_resize_mitchellnetravali_3.png 1x, https://despens.systems/2022/06/button-pushes-you/explore-more.png 2x, https://despens.systems/2022/06/button-pushes-you/explore-more_huf7773008956bb73b1a5fa18dd5f290da_8123_150x0_resize_mitchellnetravali_3.png"></a></figure><figure><a href="https://despens.systems/2022/06/button-pushes-you/discover-more.png"><img src="https://despens.systems/2022/06/button-pushes-you/discover-more_hu14e96708ce7d2aea6c1145287a8da9bd_11969_200x0_resize_mitchellnetravali_3.png" width="200" height="65" srcset="https://despens.systems/2022/06/button-pushes-you/discover-more_hu14e96708ce7d2aea6c1145287a8da9bd_11969_200x0_resize_mitchellnetravali_3.png 1x, https://despens.systems/2022/06/button-pushes-you/discover-more.png 2x, https://despens.systems/2022/06/button-pushes-you/discover-more_hu14e96708ce7d2aea6c1145287a8da9bd_11969_200x0_resize_mitchellnetravali_3.png"></a></figure><p>“Explore” buttons usually link to lists of offerings that should ideally be consumed with the user being in an exploratory mood. The “Explore more” button requires that the user already has done some exploring and wants to continue. The “Discover more” button requires that the user already has made at least one discovery and expects to find more. Overall these buttons need the user to have gained a positive impression of the resource they’re currently browsing.</p><figure><a href="https://despens.systems/2022/06/button-pushes-you/why-zoom.png"><img src="https://despens.systems/2022/06/button-pushes-you/why-zoom_hua84075144909958c7da9341ea5bae042_9847_200x0_resize_mitchellnetravali_3.png" width="200" height="71" srcset="https://despens.systems/2022/06/button-pushes-you/why-zoom_hua84075144909958c7da9341ea5bae042_9847_200x0_resize_mitchellnetravali_3.png 1x, https://despens.systems/2022/06/button-pushes-you/why-zoom.png 2x, https://despens.systems/2022/06/button-pushes-you/why-zoom_hua84075144909958c7da9341ea5bae042_9847_200x0_resize_mitchellnetravali_3.png"></a></figure><p>The “Why Zoom” button demands the user to be curious about or have existential questions about the Zoom software.</p><h2 id="navigation-and-expression">Navigation and Expression</h2><p>In this framework, the interface guiding users to resources has developed from classic hyperlinking, to Call To Action, to what I suggest calling “Button Pushes You”:</p><table><thead><tr><th>Hypertext</th><th>CTA</th><th>BPY</th></tr></thead><tbody><tr><td>index</td><td>see images</td><td>explore</td></tr><tr><td>user account</td><td>sign up</td><td>create</td></tr><tr><td>manual</td><td>get help</td><td>learn</td></tr></tbody></table><p>Classic <em>Hypertext</em> link labels use nouns to describe the resource they’re pointing to. A Call To Action (<em>CTA</em>) uses verbs telling the users what they should do, and why. It can be represented by both links and buttons. Button Pushes You (<em>BPY</em>) takes the shape of a button in most cases; the label is short, avoids nouns, and tells the user how to assess the information they’re going to encounter when following the button-shaped link.</p><p>Again, BPY at first glance might look like the classic hypertext technique, in which the author of a link creates context with the link label. A classic example would be a link to a policitians site labeled “biggest idiot ever.” However this is clearly the author becoming visible and stating their own opinion. BPY is all about stating the user’s opinion.</p><p>The net art piece <em>Kill That Cat</em> by Mouchette (Martine Nedame), 1999, clearly lays out how users that push a button are reconfiguring themselves rather than the system. On the entrance page of the work, a picture of a cat and a button labeled “KILL THAT CAT” quickly move around in the browser window. When the user manages to catch the button with their mouse cursor and push it, they are presented with a guestbook interface in which they have to justify their action of killing the cat. Of course no cat is killed, the button just acts as a link to the guestbook.</p><figure><a href="https://despens.systems/2022/06/button-pushes-you/cat-1.png"><img src="https://despens.systems/2022/06/button-pushes-you/cat-1_hu0803680cf1734e811de531c14a23e92b_502697_600x0_resize_mitchellnetravali_3.png" width="600" height="450" srcset="https://despens.systems/2022/06/button-pushes-you/cat-1_hu0803680cf1734e811de531c14a23e92b_502697_600x0_resize_mitchellnetravali_3.png 1x, https://despens.systems/2022/06/button-pushes-you/cat-1.png 2x, https://despens.systems/2022/06/button-pushes-you/cat-1_hu0803680cf1734e811de531c14a23e92b_502697_600x0_resize_mitchellnetravali_3.png"></a></figure><figure><a href="https://despens.systems/2022/06/button-pushes-you/cat-2.png"><img src="https://despens.systems/2022/06/button-pushes-you/cat-2_hu31217225023f1ca8d821baaa57c11b4f_551938_600x0_resize_mitchellnetravali_3.png" width="600" height="450" srcset="https://despens.systems/2022/06/button-pushes-you/cat-2_hu31217225023f1ca8d821baaa57c11b4f_551938_600x0_resize_mitchellnetravali_3.png 1x, https://despens.systems/2022/06/button-pushes-you/cat-2.png 2x, https://despens.systems/2022/06/button-pushes-you/cat-2_hu31217225023f1ca8d821baaa57c11b4f_551938_600x0_resize_mitchellnetravali_3.png"></a><figcaption><p>Both screenshots: Mouchette, <em>Kill That Cat</em>, 1999. Screenshot, 2022, Netscape Communicator 4.8 for Windows. <a href="https://sites.rhizome.org/anthology/mouchette-kill-that-cat.html">As presented in Rhizome’s Net Art Anthology</a>, 2016-12-08.</p></figcaption></figure><h2 id="button-standards">Button standards</h2><p>Are buttons different from links, and is that even important? As a foundational element of graphical user interfaces, a button is an surface on a display that through its visual design signals that it can be activated with a pointing device like a mouse, pen, or via touch. A button also serves a communicative role. Activating it is supposed to change the state of an application. For instance, buttons confirm a purchase, mute or enable sound, change the size of a virtual pencil’s tip, and so forth. Human interface guidelines of dominant players in the field show little variation in between them.</p><p>Apple defines buttons as elements that immediately change something in an application:</p><blockquote><p>A push button appears within a view and initiates an instantaneous app-specific action, such as printing a document or deleting a file.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p></blockquote><p>Microsoft’s version reads quite the same, and additionally provides designers with suggestions when hyperlinks would be more appropriate to use than buttons. This might be the best and most precise guideline on buttons in this list.</p><blockquote><p>A button gives the user a way to trigger an immediate action. Some buttons are specialized for particular tasks, such as navigation, repeated actions, or presenting menus.<br>[…]<br>Don’t use a Button control when the action is to navigate to another page; instead, use a HyperlinkButton control.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></p></blockquote><p>Google’s “classic” Material Design version 2 component documentation even sounds a bit like an advertisement for what buttons can do:</p><blockquote><p>Buttons allow users to take actions, and make choices, with a single tap.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p></blockquote><p>The new and updated guide for Material Design 3, which will probably soon replace previous versions, already points towards a shift for the role of buttons. The description skips foundational statements and jumps right into declaring that</p><blockquote><p>Material Design includes nine types of buttons.</p></blockquote><p>Then it goes to great lengths sorting them by “level of emphasis”:</p><blockquote><p>A button’s level of emphasis helps determine its appearance, typography, and placement.<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></p></blockquote><p>The importance of a button, as decided by the designer of a system using Material Design, is the only determining factor for its visual appearance. The supposedly least important buttons, called “Text Buttons,” don’t have an outline or elevated appearance; they’re just text in different color, in other words, they look exactly like hyperlinks. The idea is that the more visual features of a classic button a Material Design button exposes—outline, distinct background color, elevated apprarance—the more likely a user is to “tap.”</p><figure><a href="https://despens.systems/2022/06/button-pushes-you/material-3-buttons.png"><img src="https://despens.systems/2022/06/button-pushes-you/material-3-buttons_hue7aea8edbeb7cb97211b437793622df8_45145_600x0_resize_mitchellnetravali_3.png" width="600" height="301" srcset="https://despens.systems/2022/06/button-pushes-you/material-3-buttons_hue7aea8edbeb7cb97211b437793622df8_45145_600x0_resize_mitchellnetravali_3.png 1x, https://despens.systems/2022/06/button-pushes-you/material-3-buttons.png 2x, https://despens.systems/2022/06/button-pushes-you/material-3-buttons_hue7aea8edbeb7cb97211b437793622df8_45145_600x0_resize_mitchellnetravali_3.png"></a><figcaption><p>Material Design 3 buttons sorted from highest emphasis (1) to lowest emphasis (5). Cropped image copied 2022-06-04 from <a href="https://m3.material.io/components/all-buttons">All buttons</a> from Google’s Material Design 3 documentation.</p></figcaption></figure><p>This means that Material Design 3 acknowledges that an element that looks like a button communicates something different than an element that looks like a hyperlink. If nothing else, the level of activity and manipulation is understood to be higher when more button signifiers are present. Yet Google’s guidelines choose to not use this for communicating choices and functions or non-functions to users. Instead, they’re nudging users to follow links by pushing buttons, so they reconfigure themselves to think that they changed something.</p><p>As Material Design 3 has formalized BPY, it has to be expected that these types of buttons will become an accepted standard for all kinds of user interfaces, and designers will strive to name and strcuture products and activities accordingly. BPY represents a shift to turning user interfaces into a decision theatre that, by redefining long established elements, tricks users into performing work for the system they’re using.</p><p>(This article is based on <a href="https://post.lurk.org/@despens/108296474653352073">a thread on post.lurk.org</a>.)</p></section></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Room Temperature Superconductor Drama and Hope (202 pts)]]></title>
            <link>https://twitter.com/8teAPi/status/1684385895565365248</link>
            <guid>36925688</guid>
            <pubDate>Sat, 29 Jul 2023 22:58:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/8teAPi/status/1684385895565365248">https://twitter.com/8teAPi/status/1684385895565365248</a>, See on <a href="https://news.ycombinator.com/item?id=36925688">Hacker News</a></p>
Couldn't get https://twitter.com/8teAPi/status/1684385895565365248: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Big Tobacco knew radioactive Po210 in cigarettes posed cancer risk, kept quiet (282 pts)]]></title>
            <link>https://www.uclahealth.org/news/big-tobacco-knew-radioactive-particles-in-cigarettes</link>
            <guid>36925019</guid>
            <pubDate>Sat, 29 Jul 2023 21:47:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.uclahealth.org/news/big-tobacco-knew-radioactive-particles-in-cigarettes">https://www.uclahealth.org/news/big-tobacco-knew-radioactive-particles-in-cigarettes</a>, See on <a href="https://news.ycombinator.com/item?id=36925019">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Tobacco companies knew that cigarette smoke contained radioactive alpha particles for more than four decades and developed "deep and intimate" knowledge of these particles' cancer-causing potential, but they deliberately kept their findings from the public, according to a new study by UCLA researchers.</p>  <p>The analysis of dozens of previously unexamined internal tobacco industry documents, made available in 1998 as the result of a legal settlement, reveals that the industry was aware of cigarette radioactivity some five years earlier than previously thought and that tobacco companies, concerned about the potential lung cancer risk, began in-depth investigations into the possible effects of radioactivity on smokers as early as the 1960s.</p>  <p>"The documents show that the industry was well aware of the presence of a radioactive substance in tobacco as early as 1959," the authors write. "Furthermore, the industry was not only cognizant of the potential 'cancerous growth' in the lungs of regular smokers, but also did quantitative radiobiological calculations to estimate the long-term lung radiation absorption dose of ionizing alpha particles emitted from cigarette smoke."</p>  <p>The study, published&nbsp;online&nbsp;Sept. 27&nbsp;in Nicotine &amp; Tobacco Research, the peer-reviewed journal of the Society for Research on Nicotine and Tobacco, adds to a growing body of research detailing the industry's knowledge of cigarette smoke radioactivity and its efforts to suppress that information.</p>  <p>"They knew that the cigarette smoke was radioactive way back then and that it could potentially result in cancer, and they deliberately kept that information under wraps," said the study's first author, Hrayr S. Karagueuzian, an adjunct professor of cardiology who conducts research at UCLA's Cardiovascular Research Laboratory, part of the David Geffen School of Medicine at UCLA. "Specifically, we show here that the industry used misleading statements to obfuscate the hazard of ionizing alpha particles to the lungs of smokers and, more importantly, banned any and all publication on tobacco smoke radioactivity."</p>  <p>The radioactive substance — which the UCLA study shows was first brought to the attention of the tobacco industry in 1959 — was identified in 1964 as the isotope polonium-210, which emits carcinogenic alpha radiation. Polonium-210 can be found in all commercially available domestic and foreign cigarette brands, Karagueuzian said, and is absorbed by tobacco leaves through naturally occurring radon gas in the atmosphere and through high-phosphate chemical fertilizers used by tobacco growers. The substance is eventually inhaled by smokers into the lungs.</p>  <p>The study outlines the industry's growing concerns about the cancer risk posed by polonium-210 inhalation and the research that industry scientists conducted over the decades to assess the radioactive isotope's potential effect on smokers — including one study that quantitatively measured the potential lung burden from radiation exposure in a two-pack-a-day smoker over a two-decade period.</p>  <p>Karagueuzian and his colleagues made independent calculations using industry and academic data and arrived at results that very closely mirrored those of that industry study, which was conducted nearly a quarter-century ago. They then compared those results to rates used by the Environmental Protection Agency to estimate lung cancer risk among individuals exposed to similar amounts of alpha particle–emitting radon gas in their homes.</p>  <p>"The gathered data from the documents on the relevant radiobiological parameters of the alpha particles — such as dose, distribution and retention time — permitted us to duplicate the industry's secretly estimated radiation absorbed dose by regular smokers over a 20- or 25-year period, which equaled 40 to 50 rads," he said. "These levels of rads, according to the EPA's estimate of lung cancer risk in residents exposed to radon gas, equal 120 to 138 deaths per 1,000 regular smokers over a 25-year period."</p>  <p>Despite the potential risk of lung cancer, tobacco companies declined to adopt a technique discovered in 1959, and another discovered&nbsp;1980, that could have helped eliminate polonium-210 from tobacco, the researchers said. The technique, known as an acid-wash, was found to be highly effective in removing the radioisotope from tobacco plants, where it forms a water-insoluble complex with the sticky, hair-like structures called trichomes that cover the leaves.</p>  <p>And while the industry frequently cited concerns over the cost and the possible environmental impact as rationales for not using the acid wash, UCLA researchers uncovered documents that they say indicate the reason may have been far different.</p>  <p>"The industry was concerned that the acid media would ionize the nicotine, making it more difficult to be absorbed into the brains of smokers and depriving them of that instant nicotine rush that fuels their addiction," Karagueuzian said. "The industry also were well aware that the curing of the tobacco leaves for more than a one-year period also would not eliminate the polonium-210, which has a half-life of 135 days, from the tobacco leaves because it was derived from its parent, lead-210, which has a half-life of 22 years."<em></em></p>  <div><p>Karagueuzian said the insoluble alpha particles bind with resins in the cigarette smoke and get stuck and accumulate at the bronchial bifurcations of the lungs, forming "hot spots," instead of dispersing throughout the lungs. In fact, previous research on lung autopsies in smokers who died of lung cancer showed that malignant growths were primarily located at the same bronchial bifurcations where these hot spots reside.</p></div> <p>"We used to think that only the chemicals in the cigarettes were causing lung cancer," Karagueuzian said. "But the case of the these hot spots, acknowledged by the industry and academia alike, makes a strong case for an increased probability of long-term development of malignancies caused by the alpha particles. If we're lucky, the alpha particle–irradiated cell dies. If it doesn't, it could mutate and become cancerous."</p>  <p>Karagueuzian said the findings are very timely in light of the June 2009 passage of the Family Smoking Prevention and Tobacco Control Act, which grants the U.S. Food and Drug Administration broad authority to regulate and remove harmful substances — with the exception of nicotine — from tobacco products. The UCLA research, he said, makes a strong case that the FDA ought to consider making the removal of alpha particles from tobacco products a top priority.</p>  <p>"Such a move could have a considerable public health impact, due to the public's graphic perception of radiation hazards," he said.</p>  <p>To uncover the information, Karagueuzian and his team combed through the internal tobacco industry documents made available online as part of the landmark 1998 Tobacco Master Settlement Agreement. Documents from Philip Morris, R.J. Reynolds, Lorillard, Brown I Williamson, the American Tobacco Company, the Tobacco Institutes and the Council for Tobacco Research, as well as the Bliley documents, were examined, Karagueuzian said.</p>  <p>The team searched for key terms such as "polonium-210," "atmospheric fallout," "bronchial epithelium," "hot particle" and "lung cancer," among others.</p>  <p>Karagueuzian said the earliest causal link between alpha particles and cancer was made around 1920, when alpha particle–emitting radium paint was used to paint luminescent numbers on watch dials. The painting was done by hand, and the workers commonly used their lips to produce a point on the tip of the paint brush. Many workers accumulated significant burdens of alpha particles through ingestion and absorption of radium-226 into the bones and subsequently developed jaw and mouth cancers. The practice was eventually discontinued.</p>  <p>Another example involves liver cancer in patients exposed to chronic low-dose internal alpha particles emitted from the poorly soluble deposits of thorium dioxide after receiving the contrast agent Thorotrast. It has been suggested that the liver cancers resulted from point mutations of the tumor suppressor gene p53 by the accumulated alpha particles present in the contrast media. The use of Thorotrast as contrast agent was stopped in the 1950s.</p>  <p>In addition to Karagueuzian, authors of the study include the late Amos Norman, professor emeritus in the departments of radiation oncology and radiological sciences at UCLA; James Sayre, of the departments of biostatistics and radiological sciences at UCLA; and Celia White, who served from 1999 to 2002 as director of content and services at the Legacy Tobacco Documents Library, which contains more than 13 million documents created by major tobacco companies related to their advertising, manufacturing, marketing, sales and scientific research activities.</p>  <p>The study was funded by the University of California Tobacco-Related Disease Research Program, established by the passage of California's SB1613 in 1989 to fund a comprehensive University of California grant program to support research into the prevention, causes and treatment of tobacco-related diseases.</p>  <p>The authors report no conflict of interest.</p>   
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Red algae proteins grafted into tobacco double plant growth (111 pts)]]></title>
            <link>https://news.cornell.edu/stories/2023/07/red-algae-proteins-grafted-tobacco-double-plant-growth</link>
            <guid>36924832</guid>
            <pubDate>Sat, 29 Jul 2023 21:27:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.cornell.edu/stories/2023/07/red-algae-proteins-grafted-tobacco-double-plant-growth">https://news.cornell.edu/stories/2023/07/red-algae-proteins-grafted-tobacco-double-plant-growth</a>, See on <a href="https://news.ycombinator.com/item?id=36924832">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A Cornell researcher and her colleagues have solved one key piece of the molecular puzzle needed to dramatically improve plant productivity and increase carbon sequestration: They have successfully transferred key regions of a highly efficient red algae into a tobacco plant, using bacteria as an intermediary.</p>
<p>The <a href="https://www.nature.com/articles/s41477-023-01436-7">study</a> was co-authored by <a href="https://cals.cornell.edu/laura-gunn">Laura Gunn</a>, assistant professor in the School of Integrative Plant Science Plant Biology Section in the College of Agriculture and Life Sciences, and featured on the cover of the June 8 issue of Nature Plants.</p>
<p>The study centers on Rubisco, the most abundant protein across every ecosystem on Earth. Rubisco performs the first step of photosynthesis by fixing carbon, and it appears in various forms in a wide array of organisms, including plants, red and green algae and bacteria. Rubisco is slow and struggles to differentiate between oxygen and carbon dioxide, a problem Gunn and <a href="https://news.cornell.edu/stories/2021/08/scientists-take-step-improve-crops-photosynthesis-yields">several other Cornellians</a> are working on. As a result, Rubisco often limits plant growth and crop yield.</p>
<p>One species of red algae, Griffithsia monilis (Gm), contains Rubisco that is 30% more efficient at fixing carbon than Rubisco in other organisms, including terrestrial crops. For at least 20 years, scientists have been interested in transplanting the highly efficient GmRubisco into plants such as rice, wheat, soybean and tobacco to increase their productivity; however, until now, no one has been able to successfully coax plants to express it. This is because Rubisco requires multiple “chaperones” that are essential for the protein to fold, assemble and be active&nbsp; – there are seven such helpers in tobacco plants – and most of the chaperones in red algae are unknown, Gunn said.</p>
<p>In their study, Gunn and her co-authors were able to solve the 3D structure of GmRubisco and use this information to successfully graft a small number of regions from Rhodobacter sphaeroides (RsRubisco) into a bacterial Rubisco.</p>
<p>“RsRubisco is not very efficient, but it is very closely related to GmRubisco – they’re like cousins – which means that unlike land-plant Rubisco, it accepts the grafted sequences,” Gunn said. “RsRubisco also doesn't need any special chaperones for it to fold and assemble in land plants.”</p>
<p>The change increased the carboxylation rate – the speed at which Rubisco starts the carbon fixation process – by 60%, increased carboxylation efficiency by 22% and improved RsRubisco’s ability to distinguish between carbon dioxide and oxygen by 7%. The authors then transplanted their bacterial mutant into tobacco, where it doubled photosynthesis and plant growth, compared to tobacco grown with unaltered RsRubisco. Tobacco is the easiest land plant in which to manipulate Rubisco and so serves as the test case for developing a more efficient Rubisco that can be transferred to more agronomically relevant species, Gunn said.</p>
<p>“We’re not at the point where we’re outperforming wild-type tobacco, but we’re on the right trajectory,” Gunn said. “We only need fairly modest improvements to Rubisco performance, because even a very small increase over a whole growing season can lead to massive changes in plant growth and yield, and the potential applications span many sectors: higher agricultural production; more efficient and affordable biofuel production; carbon sequestration approaches; and artificial energy possibilities.”</p>
<p>The research was supported by the Australian Research Council Centre of Excellence for Translational Photosynthesis, Formas Future Research Leaders and the European Regional Development Fund.</p>
<p><em>Krisy Gashler is a freelance writer for the College of Agriculture and Life Sciences.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Gogit – Just enough Git (in Go) to push itself to GitHub (164 pts)]]></title>
            <link>https://benhoyt.com/writings/gogit/</link>
            <guid>36924267</guid>
            <pubDate>Sat, 29 Jul 2023 20:34:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://benhoyt.com/writings/gogit/">https://benhoyt.com/writings/gogit/</a>, See on <a href="https://news.ycombinator.com/item?id=36924267">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">

<p>July 2023</p>

<blockquote>
  <p><strong>Go to:</strong> <a href="#technical-summary">Tech summary</a> | <a href="#error-handling">Error handling</a> | <a href="#performance">Performance</a> | <a href="#comparison-with-python-version">vs Python</a> | <a href="#conclusion">Conclusion</a></p>
</blockquote>

<p>A few years ago I wrote <a href="https://benhoyt.com/writings/pygit/">pygit</a>, a small Python program that’s just enough of a Git client to create a repository, add some commits, and push itself to GitHub.</p>

<p>I wanted to compare what it would look like in Go, to see if it was reasonable to write small scripts in Go – quick ’n’ dirty code where performance isn’t a big deal, and stack traces are all you need for error handling.</p>

<p>The result is <a href="https://github.com/benhoyt/gogit/blob/master/gogit.go">gogit</a>, a 400-line Go program that can initialise a repository, commit, and push to GitHub. It’s written in ordinary Go … except for error handling, which is just too verbose in idiomatic Go to work well for scripting (more on that <a href="#error-handling">below</a>).</p>

<h2 id="technical-summary">Technical summary</h2>

<p>I won’t go into detail about how Git works here (there’s a bit more in my <a href="https://benhoyt.com/writings/pygit/">pygit article</a>), suffice to say that the Git data model is pretty neat. It uses a simple file-based object store in <code>.git/objects</code>, where each object has a <a href="https://en.wikipedia.org/wiki/SHA-1">40-character hash</a> and can be a <em>commit</em>, a <em>tree</em> (directory listing), or a <em>blob</em> (committed file). That’s it – the <a href="https://github.com/benhoyt/gogit/blob/e94720d64dd93ca7aea17a314ea6430fc5f6b90c/gogit.go#L111-L164">gogit code</a> to write commits, trees, and blobs is about 50 lines.</p>

<p>I’ve implemented even less than pygit: only <code>init</code>, <code>commit</code>, and <code>push</code>. Gogit doesn’t even support the index (staging area), so instead of <code>gogit add</code>, you just <code>gogit commit</code> with the list of paths you want to commit each time. As <a href="https://github.com/benhoyt/pygit/blob/aa8d8bb62ae273ae2f4f167e36f24f40a11634b9/pygit.py#L231-L246">pygit’s code</a> shows, dealing with the index is messy. It’s also unnecessary, and I wanted gogit to be an exercise in minimalism.</p>

<p>Gogit also drops the commands <code>cat-file</code>, <code>hash-object</code>, and <code>diff</code> – those aren’t required for committing and pushing to GitHub. I did use Git’s <code>cat-file</code> during debugging, however.</p>

<p>Here are the commands I used to create the repo, commit, and push to GitHub (note the use of <a href="https://pkg.go.dev/cmd/go#hdr-Compile_and_run_Go_program"><code>go run</code></a> to compile and execute the “script”):</p>

<div><pre><code># Initialise the repo
$ go run . init

# Make the first commit (other commits are similar)
$ export GIT_AUTHOR_NAME='Ben Hoyt'
$ export GIT_AUTHOR_EMAIL=benhoyt@gmail.com
$ go run . commit -m 'Initial commit' gogit.go go.mod LICENSE.txt
commited 0580a17 to master

# Push updates to GitHub
$ export GIT_USERNAME=benhoyt
$ export GIT_PASSWORD=...
$ go run . push https://github.com/benhoyt/gogit
updating remote master from 0000000 to 0580a17 (5 objects)
</code></pre></div>

<h2 id="error-handling">Error handling</h2>

<p>The verbosity of Go’s error handling has been much-maligned. It’s simple and explicit, but every call to a function that may fail takes an additional three lines of code to handle the error:</p>

<div><pre><code><span>mode</span><span>,</span> <span>err</span> <span>:=</span> <span>strconv</span><span>.</span><span>ParseInt</span><span>(</span><span>modeStr</span><span>,</span> <span>8</span><span>,</span> <span>64</span><span>)</span>
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
    <span>return</span> <span>err</span>
<span>}</span>
</code></pre></div>

<p>It’s not as big a deal when writing production code, because then you want more control over error handling anyway – nicely-wrapped errors, or human-readable messages, for example:</p>

<div><pre><code><span>mode</span><span>,</span> <span>err</span> <span>:=</span> <span>strconv</span><span>.</span><span>ParseInt</span><span>(</span><span>modeStr</span><span>,</span> <span>8</span><span>,</span> <span>64</span><span>)</span>
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
    <span>return</span> <span>fmt</span><span>.</span><span>Errorf</span><span>(</span><span>"mode must be an octal number, not %q"</span><span>,</span> <span>modeStr</span><span>)</span>
<span>}</span>
</code></pre></div>

<p>In a simple script, however, <a href="https://www.reddit.com/r/golang/comments/6v63c2/comment/dly1pis/">all the error handling you need</a> is to show a message, print a stack trace, and exit the program. That’s what happens in Python when you don’t catch exceptions, and it’s easy to emulate in Go with a <a href="https://github.com/benhoyt/gogit/blob/e94720d64dd93ca7aea17a314ea6430fc5f6b90c/gogit.go#L84-L101">couple of helper functions</a>:</p>

<div><pre><code><span>func</span> <span>check0</span><span>(</span><span>err</span> <span>error</span><span>)</span> <span>{</span>
    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
        <span>panic</span><span>(</span><span>err</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>func</span> <span>check</span><span>[</span><span>T</span> <span>any</span><span>](</span><span>value</span> <span>T</span><span>,</span> <span>err</span> <span>error</span><span>)</span> <span>T</span> <span>{</span>
    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
        <span>panic</span><span>(</span><span>err</span><span>)</span>
    <span>}</span>
    <span>return</span> <span>value</span>
<span>}</span>

<span>func</span> <span>assert</span><span>(</span><span>cond</span> <span>bool</span><span>,</span> <span>format</span> <span>string</span><span>,</span> <span>args</span> <span>...</span><span>any</span><span>)</span> <span>{</span>
    <span>if</span> <span>!</span><span>cond</span> <span>{</span>
        <span>panic</span><span>(</span><span>fmt</span><span>.</span><span>Sprintf</span><span>(</span><span>format</span><span>,</span> <span>args</span><span>...</span><span>))</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>Now that Go has generics you can easily define a <code>check</code> function which returns a result. However, you still need variants based on the number of results returned. Normally this is zero or one, with one being most common, so I’ve named that variant just <code>check</code>, and the zero-results one <code>check0</code>. I’ve also defined <code>assert</code>, which takes a boolean and a formatted message instead of an error.</p>

<p>These helpers allow you to turn this code:</p>

<div><pre><code><span>func</span> <span>writeTree</span><span>(</span><span>paths</span> <span>[]</span><span>string</span><span>)</span> <span>([]</span><span>byte</span><span>,</span> <span>error</span><span>)</span> <span>{</span>
    <span>sort</span><span>.</span><span>Strings</span><span>(</span><span>paths</span><span>)</span> <span>// tree object needs paths sorted</span>
    <span>var</span> <span>buf</span> <span>bytes</span><span>.</span><span>Buffer</span>
    <span>for</span> <span>_</span><span>,</span> <span>path</span> <span>:=</span> <span>range</span> <span>paths</span> <span>{</span>
        <span>st</span><span>,</span> <span>err</span> <span>:=</span> <span>os</span><span>.</span><span>Stat</span><span>(</span><span>path</span><span>)</span>
        <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
            <span>return</span> <span>nil</span><span>,</span> <span>err</span>
        <span>}</span>
        <span>if</span> <span>st</span><span>.</span><span>IsDir</span><span>()</span> <span>{</span>
            <span>panic</span><span>(</span><span>"sub-trees not supported"</span><span>)</span>
        <span>}</span>
        <span>data</span><span>,</span> <span>err</span> <span>:=</span> <span>os</span><span>.</span><span>ReadFile</span><span>(</span><span>path</span><span>)</span>
        <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
            <span>return</span> <span>nil</span><span>,</span> <span>err</span>
        <span>}</span>
        <span>hash</span><span>,</span> <span>err</span> <span>:=</span> <span>hashObject</span><span>(</span><span>"blob"</span><span>,</span> <span>data</span><span>)</span>
        <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
            <span>return</span> <span>nil</span><span>,</span> <span>err</span>
        <span>}</span>
        <span>fmt</span><span>.</span><span>Fprintf</span><span>(</span><span>&amp;</span><span>buf</span><span>,</span> <span>"%o %s</span><span>\x00</span><span>%s"</span><span>,</span> <span>st</span><span>.</span><span>Mode</span><span>()</span><span>.</span><span>Perm</span><span>()</span><span>|</span><span>0</span><span>o100000</span><span>,</span> <span>path</span><span>,</span> <span>hash</span><span>)</span>
    <span>}</span>
    <span>return</span> <span>hashObject</span><span>(</span><span>"tree"</span><span>,</span> <span>buf</span><span>.</span><span>Bytes</span><span>())</span>
<span>}</span>
</code></pre></div>

<p>Into the following, reducing the function body from 21 to 10 lines, which is comparable to the brevity of Python:</p>

<div><pre><code><span>func</span> <span>writeTree</span><span>(</span><span>paths</span> <span>[]</span><span>string</span><span>)</span> <span>[]</span><span>byte</span> <span>{</span>
    <span>sort</span><span>.</span><span>Strings</span><span>(</span><span>paths</span><span>)</span> <span>// tree object needs paths sorted</span>
    <span>var</span> <span>buf</span> <span>bytes</span><span>.</span><span>Buffer</span>
    <span>for</span> <span>_</span><span>,</span> <span>path</span> <span>:=</span> <span>range</span> <span>paths</span> <span>{</span>
        <span>st</span> <span>:=</span> <span>check</span><span>(</span><span>os</span><span>.</span><span>Stat</span><span>(</span><span>path</span><span>))</span>
        <span>assert</span><span>(</span><span>!</span><span>st</span><span>.</span><span>IsDir</span><span>(),</span> <span>"sub-trees not supported"</span><span>)</span>
        <span>data</span> <span>:=</span> <span>check</span><span>(</span><span>os</span><span>.</span><span>ReadFile</span><span>(</span><span>path</span><span>))</span>
        <span>hash</span> <span>:=</span> <span>hashObject</span><span>(</span><span>"blob"</span><span>,</span> <span>data</span><span>)</span>
        <span>fmt</span><span>.</span><span>Fprintf</span><span>(</span><span>&amp;</span><span>buf</span><span>,</span> <span>"%o %s</span><span>\x00</span><span>%s"</span><span>,</span> <span>st</span><span>.</span><span>Mode</span><span>()</span><span>.</span><span>Perm</span><span>()</span><span>|</span><span>0</span><span>o100000</span><span>,</span> <span>path</span><span>,</span> <span>hash</span><span>)</span>
    <span>}</span>
    <span>return</span> <span>hashObject</span><span>(</span><span>"tree"</span><span>,</span> <span>buf</span><span>.</span><span>Bytes</span><span>())</span>
<span>}</span>
</code></pre></div>

<p>It’s not perfect, because the word <code>check</code> slightly obscures the function you’re calling, but it does makes writing quick ’n’ dirty scripts a lot nicer.</p>

<p>You even get “better” errors than a plain <code>return err</code>, because the stack trace shows you exactly what function and line of code was being executed:</p>

<div><pre><code>$ go run . push https://github.com/benhoyt/gogit
panic: Get "https://github.com/benhoyt/gogit/info/refs?service=git-receive-pack":
    context deadline exceeded (Client.Timeout exceeded while awaiting headers)

goroutine 1 [running]:
main.check[...](...)
    /home/ben/h/gogit/gogit.go:94
main.getRemoteHash(0x416ad0?, {0x7ffe1f0152d9?, 0x4b87d4?}, {0xc00001c00d, 0x7}, {0xc00001a00d, 0x28})
    /home/ben/h/gogit/gogit.go:245 +0x6da
main.push({0x7ffe1f0152d9, 0x20}, {0xc00001c00d, 0x7}, {0xc00001a00d, 0x28})
    /home/ben/h/gogit/gogit.go:217 +0xd9
main.main()
    /home/ben/h/gogit/gogit.go:73 +0x21e
exit status 2
</code></pre></div>

<p><a href="https://github.com/benhoyt/gogit/commit/ad75a5fbf67924d5a62dc9794e772d069610d085">Changing from <code>return err</code> to <code>check</code></a> reduced the number of lines of code from 607 to 415, a reduction by 32%.</p>

<p>If you want to pursue this approach further, there’s even a library written by Joe Tsai and Josh Bleecher Snyder called <a href="https://github.com/dsnet/try"><code>try</code></a> that uses <a href="https://pkg.go.dev/builtin#recover"><code>recover</code></a> to do this “properly”. Interesting stuff! I’m still hoping the Go team figures out a way to make error handling less verbose.</p>

<h2 id="performance">Performance</h2>

<p>This is going to be a short section, because I don’t care about speed in this program, and the Go version is likely as fast or faster than the Python version. Go can be significantly faster, but we’re dealing with tiny files, and in Python, all the interesting code like hashing and writing to disk is written in C anyway.</p>

<p>Memory usage is another aspect of performance. Again, we’re dealing with small files here, so it’s not an issue to read everything into memory. In Python, you can do streaming, but it’s not as consistently easy as in Go, due to the amazing <a href="https://pkg.go.dev/io#Reader"><code>io.Reader</code></a> and <a href="https://pkg.go.dev/io#Writer"><code>io.Writer</code></a> interfaces.</p>

<p>That said, it’s still a bit easier in Go to read everything into <code>[]byte</code> or <code>string</code> and operate on those, so that’s what I’ve done in gogit. We’re talking about a few KB of memory, and my machine has a few GB.</p>

<h2 id="comparison-with-python-version">Comparison with Python version</h2>

<p>As it stands, Pygit is about 600 lines of code, and gogit about 400. However, that’s a bit misleading, as I removed several features when writing the Go version: there’s no support for the Git index, and there’s no <code>cat-file</code>, <code>hash-object</code>, or <code>diff</code>.</p>

<p>I did a quick test by removing those functions from the Python version, and it ends up at 360 lines of code. I consider 400 in Go versus 360 in Python not bad – it’s only 10% longer. And the Go version includes 20 lines of imports and 20 lines for the check/assert functions. So they’re really almost identical in size!</p>

<p>Let’s look at a couple of specific functions. First, <code>find_object</code>, which looks in the Git object store to find an object with the given prefix. Here’s the Python version:</p>

<div><pre><code><span>def</span> <span>find_object</span><span>(</span><span>sha1_prefix</span><span>):</span>
    <span>obj_dir</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>'.git'</span><span>,</span> <span>'objects'</span><span>,</span> <span>sha1_prefix</span><span>[:</span><span>2</span><span>])</span>
    <span>rest</span> <span>=</span> <span>sha1_prefix</span><span>[</span><span>2</span><span>:]</span>
    <span>objects</span> <span>=</span> <span>[</span><span>name</span> <span>for</span> <span>name</span> <span>in</span> <span>os</span><span>.</span><span>listdir</span><span>(</span><span>obj_dir</span><span>)</span> <span>if</span> <span>name</span><span>.</span><span>startswith</span><span>(</span><span>rest</span><span>)]</span>
    <span>if</span> <span>not</span> <span>objects</span><span>:</span>
        <span>raise</span> <span>ValueError</span><span>(</span><span>'object {!r} not found'</span><span>.</span><span>format</span><span>(</span><span>sha1_prefix</span><span>))</span>
    <span>if</span> <span>len</span><span>(</span><span>objects</span><span>)</span> <span>&gt;=</span> <span>2</span><span>:</span>
        <span>raise</span> <span>ValueError</span><span>(</span><span>'multiple objects ({}) with prefix {!r}'</span><span>.</span><span>format</span><span>(</span>
                <span>len</span><span>(</span><span>objects</span><span>),</span> <span>sha1_prefix</span><span>))</span>
    <span>return</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>obj_dir</span><span>,</span> <span>objects</span><span>[</span><span>0</span><span>])</span>
</code></pre></div>

<p>And here’s the Go version:</p>

<div><pre><code><span>func</span> <span>findObject</span><span>(</span><span>hashPrefix</span> <span>string</span><span>)</span> <span>string</span> <span>{</span>
    <span>objDir</span> <span>:=</span> <span>filepath</span><span>.</span><span>Join</span><span>(</span><span>".git/objects"</span><span>,</span> <span>hashPrefix</span><span>[</span><span>:</span><span>2</span><span>])</span>
    <span>rest</span> <span>:=</span> <span>hashPrefix</span><span>[</span><span>2</span><span>:</span><span>]</span>
    <span>entries</span><span>,</span> <span>_</span> <span>:=</span> <span>os</span><span>.</span><span>ReadDir</span><span>(</span><span>objDir</span><span>)</span>
    <span>var</span> <span>matches</span> <span>[]</span><span>string</span>
    <span>for</span> <span>_</span><span>,</span> <span>entry</span> <span>:=</span> <span>range</span> <span>entries</span> <span>{</span>
        <span>if</span> <span>strings</span><span>.</span><span>HasPrefix</span><span>(</span><span>entry</span><span>.</span><span>Name</span><span>(),</span> <span>rest</span><span>)</span> <span>{</span>
            <span>matches</span> <span>=</span> <span>append</span><span>(</span><span>matches</span><span>,</span> <span>entry</span><span>.</span><span>Name</span><span>())</span>
        <span>}</span>
    <span>}</span>
    <span>assert</span><span>(</span><span>len</span><span>(</span><span>matches</span><span>)</span> <span>&gt;</span> <span>0</span><span>,</span> <span>"object %q not found"</span><span>,</span> <span>hashPrefix</span><span>)</span>
    <span>assert</span><span>(</span><span>len</span><span>(</span><span>matches</span><span>)</span> <span>==</span> <span>1</span><span>,</span> <span>"multiple objects with prefix %q"</span><span>,</span> <span>hashPrefix</span><span>)</span>
    <span>return</span> <span>filepath</span><span>.</span><span>Join</span><span>(</span><span>objDir</span><span>,</span> <span>matches</span><span>[</span><span>0</span><span>])</span>
<span>}</span>
</code></pre></div>

<p>A lot of things are similar, for example the <code>os.path.join</code> vs <code>filepath.Join</code>, <code>os.listdir</code> vs <code>os.ReadDir</code>, and so on. But note the list comprehension in Python – a one-liner – is a five-line <code>for</code> loop in Go. I do miss list comprehensions when scripting in Go…</p>

<p>Let’s look at another one, the <code>commit</code> function, first in Python:</p>

<div><pre><code><span>def</span> <span>commit</span><span>(</span><span>message</span><span>,</span> <span>author</span><span>):</span>
    <span>tree</span> <span>=</span> <span>write_tree</span><span>()</span>
    <span>parent</span> <span>=</span> <span>get_local_master_hash</span><span>()</span>
    <span>timestamp</span> <span>=</span> <span>int</span><span>(</span><span>time</span><span>.</span><span>mktime</span><span>(</span><span>time</span><span>.</span><span>localtime</span><span>()))</span>
    <span>utc_offset</span> <span>=</span> <span>-</span><span>time</span><span>.</span><span>timezone</span>
    <span>author_time</span> <span>=</span> <span>'{} {}{:02}{:02}'</span><span>.</span><span>format</span><span>(</span>
            <span>timestamp</span><span>,</span>
            <span>'+'</span> <span>if</span> <span>utc_offset</span> <span>&gt;</span> <span>0</span> <span>else</span> <span>'-'</span><span>,</span>
            <span>abs</span><span>(</span><span>utc_offset</span><span>)</span> <span>//</span> <span>3600</span><span>,</span>
            <span>(</span><span>abs</span><span>(</span><span>utc_offset</span><span>)</span> <span>//</span> <span>60</span><span>)</span> <span>%</span> <span>60</span><span>)</span>
    <span>lines</span> <span>=</span> <span>[</span><span>'tree '</span> <span>+</span> <span>tree</span><span>]</span>
    <span>if</span> <span>parent</span><span>:</span>
        <span>lines</span><span>.</span><span>append</span><span>(</span><span>'parent '</span> <span>+</span> <span>parent</span><span>)</span>
    <span>lines</span><span>.</span><span>append</span><span>(</span><span>'author {} {}'</span><span>.</span><span>format</span><span>(</span><span>author</span><span>,</span> <span>author_time</span><span>))</span>
    <span>lines</span><span>.</span><span>append</span><span>(</span><span>'committer {} {}'</span><span>.</span><span>format</span><span>(</span><span>author</span><span>,</span> <span>author_time</span><span>))</span>
    <span>lines</span><span>.</span><span>append</span><span>(</span><span>''</span><span>)</span>
    <span>lines</span><span>.</span><span>append</span><span>(</span><span>message</span><span>)</span>
    <span>lines</span><span>.</span><span>append</span><span>(</span><span>''</span><span>)</span>
    <span>data</span> <span>=</span> <span>'</span><span>\n</span><span>'</span><span>.</span><span>join</span><span>(</span><span>lines</span><span>).</span><span>encode</span><span>()</span>
    <span>sha1</span> <span>=</span> <span>hash_object</span><span>(</span><span>data</span><span>,</span> <span>'commit'</span><span>)</span>
    <span>master_path</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>'.git'</span><span>,</span> <span>'refs'</span><span>,</span> <span>'heads'</span><span>,</span> <span>'master'</span><span>)</span>
    <span>write_file</span><span>(</span><span>master_path</span><span>,</span> <span>(</span><span>sha1</span> <span>+</span> <span>'</span><span>\n</span><span>'</span><span>).</span><span>encode</span><span>())</span>
    <span>return</span> <span>sha1</span>
</code></pre></div>

<p>Then in Go:</p>

<div><pre><code><span>func</span> <span>commit</span><span>(</span><span>message</span><span>,</span> <span>author</span> <span>string</span><span>,</span> <span>paths</span> <span>[]</span><span>string</span><span>)</span> <span>string</span> <span>{</span>
    <span>tree</span> <span>:=</span> <span>writeTree</span><span>(</span><span>paths</span><span>)</span>
    <span>var</span> <span>buf</span> <span>bytes</span><span>.</span><span>Buffer</span>
    <span>fmt</span><span>.</span><span>Fprintln</span><span>(</span><span>&amp;</span><span>buf</span><span>,</span> <span>"tree"</span><span>,</span> <span>hex</span><span>.</span><span>EncodeToString</span><span>(</span><span>tree</span><span>))</span>
    <span>parent</span> <span>:=</span> <span>getLocalHash</span><span>()</span>
    <span>if</span> <span>parent</span> <span>!=</span> <span>""</span> <span>{</span>
        <span>fmt</span><span>.</span><span>Fprintln</span><span>(</span><span>&amp;</span><span>buf</span><span>,</span> <span>"parent"</span><span>,</span> <span>parent</span><span>)</span>
    <span>}</span>
    <span>now</span> <span>:=</span> <span>time</span><span>.</span><span>Now</span><span>()</span>
    <span>offset</span> <span>:=</span> <span>now</span><span>.</span><span>Format</span><span>(</span><span>"-0700"</span><span>)</span>
    <span>fmt</span><span>.</span><span>Fprintln</span><span>(</span><span>&amp;</span><span>buf</span><span>,</span> <span>"author"</span><span>,</span> <span>author</span><span>,</span> <span>now</span><span>.</span><span>Unix</span><span>(),</span> <span>offset</span><span>)</span>
    <span>fmt</span><span>.</span><span>Fprintln</span><span>(</span><span>&amp;</span><span>buf</span><span>,</span> <span>"committer"</span><span>,</span> <span>author</span><span>,</span> <span>now</span><span>.</span><span>Unix</span><span>(),</span> <span>offset</span><span>)</span>
    <span>fmt</span><span>.</span><span>Fprintln</span><span>(</span><span>&amp;</span><span>buf</span><span>)</span>
    <span>fmt</span><span>.</span><span>Fprintln</span><span>(</span><span>&amp;</span><span>buf</span><span>,</span> <span>message</span><span>)</span>
    <span>data</span> <span>:=</span> <span>buf</span><span>.</span><span>Bytes</span><span>()</span>
    <span>hash</span> <span>:=</span> <span>hashObject</span><span>(</span><span>"commit"</span><span>,</span> <span>data</span><span>)</span>
    <span>check0</span><span>(</span><span>os</span><span>.</span><span>WriteFile</span><span>(</span><span>".git/refs/heads/master"</span><span>,</span> <span>[]</span><span>byte</span><span>(</span><span>hex</span><span>.</span><span>EncodeToString</span><span>(</span><span>hash</span><span>)</span><span>+</span><span>"</span><span>\n</span><span>"</span><span>),</span> <span>0</span><span>o664</span><span>))</span>
    <span>return</span> <span>hex</span><span>.</span><span>EncodeToString</span><span>(</span><span>hash</span><span>)</span>
<span>}</span>
</code></pre></div>

<p>Interestingly, this time the Python version is longer: 23 lines versus Go’s 19. This mostly comes down to the better handling of timestamps. Go’s standard library isn’t perfect, but its <a href="https://pkg.go.dev/time"><code>time</code></a> package is better than Python’s <code>time</code> and <code>datetime</code> packages put together.</p>

<p>In general, Go’s standard library seems much more coherent and better-designed than Python’s, which feels like it was designed by many different people over several decades (because it was).</p>

<h2 id="conclusion">Conclusion</h2>

<p>When used with <code>panic</code>-based error handling, Go is good for writing quick ’n’ dirty command line scripts.</p>

<p>To be honest, I’d still probably reach for Python first for throwaway scripts, because of its terser syntax, list (and other) comprehensions, and exception handling by default.</p>

<p>However, for anything more than a throwaway script, I’d quickly move to Go. Its standard library is better-designed, its <code>io.Reader</code> and <code>io.Writer</code> interfaces are excellent, and its lightweight static typing helps catch bugs without getting in the way.</p>

<p>I’d love it if you <a href="https://github.com/sponsors/benhoyt/">sponsored me on GitHub</a> – it will motivate me to work on my open source projects and write more good content. Thanks!</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Police are not primarily crime fighters, according to the data (123 pts)]]></title>
            <link>https://www.reuters.com/legal/government/police-are-not-primarily-crime-fighters-according-data-2022-11-02/</link>
            <guid>36924117</guid>
            <pubDate>Sat, 29 Jul 2023 20:17:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/government/police-are-not-primarily-crime-fighters-according-data-2022-11-02/">https://www.reuters.com/legal/government/police-are-not-primarily-crime-fighters-according-data-2022-11-02/</a>, See on <a href="https://news.ycombinator.com/item?id=36924117">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-testid="paragraph-0">(Reuters) - A new report adds to a growing line of research showing that police departments don’t solve serious or violent crimes with any regularity, and in fact, spend very little time on crime control, in contrast to popular narratives.</p><p data-testid="paragraph-1">The <a data-testid="Link" href="https://catalyst-ca.cdn.prismic.io/catalyst-ca/126c30a8-852c-416a-b8a7-55a90c77a04e_APCA+ACLU+REIMAGINING+COMMUNITY+SAFETY+2022_5.pdf" target="_blank">report</a> was published Oct. 25 by advocacy group Catalyst California and the ACLU of Southern California. It relies on county budgets' numbers and new policing data provided under the state’s Racial and Identity Profiling Act, which took effect in 2019.</p><p data-testid="paragraph-2">The law requires police to report demographic and other basic information about their work, including the duration of a stop and what actions were taken, like ordering someone out of a car.</p><p data-testid="paragraph-3">Records provided by the sheriff’s departments in Los Angeles, Sacramento, San Diego and Riverside showed the same longstanding pattern of racial disparities in police stops throughout the country for decades. Black people in San Diego were more than twice as likely than white residents to be stopped by sheriff’s deputies, for example.</p><p data-testid="paragraph-4">More notably, researchers analyzed the data to show how officers spend their time, and the patterns that emerge tell a striking story about how policing actually works. Those results, too, comport with existing research showing that U.S. police spend much of their time conducting racially biased stops and searches of minority drivers, often without reasonable suspicion, rather than “fighting crime.”</p><p data-testid="paragraph-5">Overall, sheriff patrol officers spend significantly more time on officer-initiated stops – “proactive policing” in law enforcement parlance – than they do responding to community members’ calls for help, according to the report. Research has shown that the practice is a fundamentally ineffective public safety strategy, the report pointed out.</p><p data-testid="paragraph-6">In 2019, 88% of the time L.A. County sheriff’s officers spent on stops was for officer-initiated stops rather than in response to calls. The overwhelming majority of that time – 79% – was spent on traffic violations. By contrast, just 11% of those hours was spent on stops based on reasonable suspicion of a crime.</p><p data-testid="paragraph-7">In Riverside, about 83% of deputies’ time spent on officer-initiated stops went toward traffic violations, and just 7% on stops based on reasonable suspicion.</p><p data-testid="paragraph-8">Moreover, most of the stops are pointless, other than inconveniencing citizens, or worse – “a routine practice of pretextual stops,” researchers wrote. Roughly three out of every four hours that Sacramento sheriff’s officers spent investigating traffic violations were for stops that ended in warnings, or no action, for example.</p><p data-testid="paragraph-9">Researchers calculated that more of the departments’ budgets go toward fruitless traffic stops than responses to service calls -- essentially wasting millions of public dollars.</p><p data-testid="paragraph-10">Chauncee Smith, a senior manager at Catalyst California, told me they wanted to test the dominant media and political narrative that police agencies use public funds to keep communities safe.</p><p data-testid="paragraph-11">“We found there is a significant inconsistency between their practices” and what the public might think police do, Smith said. “It begs the question of why we keep doubling down on public safety strategies that have been proven time and time again to fail.”</p><p data-testid="paragraph-12">The departments were mostly non-responsive to my questions.</p><p data-testid="paragraph-13">Riverside Sheriff Chad Bianco said the data -- which is self-reported -- is flawed. All four departments declined to answer specific questions about how officers spend their time, and didn’t provide contradictory information.</p><p data-testid="paragraph-14">The prevailing political myth about police work was echoed again in August, when President Joe Biden announced his administration’s <a data-testid="Link" href="https://www.reuters.com/legal/government/bidens-crime-prevention-plan-repeats-old-mistakes-policing-2022-08-02/">“fund the police” measure</a> to support hiring more cops around the country over the next five years.</p><p data-testid="paragraph-15">“When it comes to fighting crime, we know what works: officers on the street who know the neighborhood,” Biden said.</p><p data-testid="paragraph-16">Most of the existing research flatly contradicts that account.</p><p data-testid="paragraph-17">In 2016, a group of criminologists conducted a <a data-testid="Link" href="https://link.springer.com/article/10.1007/s11292-016-9269-8" target="_blank">systematic review</a> of 62 earlier studies of police force size and crime between 1971 and 2013. They concluded that 40 years of studies consistently show that “the overall effect size for police force size on crime is negative, small, and not statistically significant.”</p><p data-testid="paragraph-18">“This line of research has exhausted its utility,” the authors wrote. “Changing policing strategy is likely to have a greater impact on crime than adding more police.”</p><p data-testid="paragraph-19">Decades of data similarly shows that police don’t solve much serious and violent crime – the safety issues that most concern everyday people.</p><p data-testid="paragraph-20">Over the past decade, “consistently less than half of all violent crime and less than twenty-five percent of all property crime were cleared,” William Laufer and Robert Hughes wrote in a 2021 law review <a data-testid="Link" href="https://tmsnrt.rs/3DOcF3g" target="_blank">article</a>. Laufer and Hughes are professors in the Wharton School of the University of Pennsylvania’s Legal Studies and Business Ethics Department.</p><p data-testid="paragraph-21">Police “have never successfully solved crimes with any regularity, as arrest and clearance rates are consistently low throughout history,” and police have never solved even a bare majority of serious crimes, University of Utah college of law professor Shima Baradaran Baughman wrote in another 2021 law review <a data-testid="Link" href="https://tmsnrt.rs/3UbfhNO" target="_blank">article</a>, including murder, rape, burglary and robbery.</p><p data-testid="paragraph-22">Existing research also affirms the findings in the recent report on police work in California.</p><p data-testid="paragraph-23">Law “enforcement is a relatively small part of what police do every day,” Barry Friedman, a law professor at the New York University School of Law wrote in a 2021 law review <a data-testid="Link" href="https://tmsnrt.rs/3UdsxSe" target="_blank">article</a>.</p><p data-testid="paragraph-24">Studies have shown that the average police officer spent about one hour per week responding to crimes in progress, Friedman wrote.</p><p data-testid="paragraph-25">Police spend most of their time on traffic violations and routine, minor issues, like noise complaints, according to three different, recent analyses of dispatch data from <a data-testid="Link" href="https://www.latimes.com/california/story/2020-07-05/lapd-911-calls-reimagining-police" target="_blank">Los Angeles</a>, <a data-testid="Link" href="https://www.vera.org/news/most-911-calls-have-nothing-to-do-with-crime-why-are-we-still-sending-police" target="_blank">Baltimore, Detroit, New Orleans, Seattle</a>, and <a data-testid="Link" href="https://www.newhavenindependent.org/index.php/article/police_dispatch_stats" target="_blank">New Haven, Connecticut</a>.</p><p data-testid="paragraph-26">The New York Times reviewed national dispatch data from the FBI in June 2020, and found that just 4% of officers’ time is devoted to violent crime.</p><p data-testid="paragraph-27">“We hope the report helps reshape the narrative about the relationship between law enforcement and safety,” Smith told me. Californians “should understand that a reimagination of community safety is far overdue and that equitable and community-centered solutions” are more effective alternatives.</p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank">The Thomson Reuters Trust Principles.</a></p><p>Opinions expressed are those of the author. They do not reflect the views of Reuters News, which, under the Trust Principles, is committed to integrity, independence, and freedom from bias.</p><div><address><p data-testid="Body">Hassan Kanu writes about access to justice, race, and equality under law. Kanu, who was born in Sierra Leone and grew up in Silver Spring, Maryland, worked in public interest law after graduating from Duke University School of Law. After that, he spent five years reporting on mostly employment law. He lives in Washington, D.C. Reach Kanu at hassan.kanu@thomsonreuters.com</p></address></div></div></div>]]></description>
        </item>
    </channel>
</rss>