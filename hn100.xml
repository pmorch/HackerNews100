<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 19 Sep 2025 18:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A shift in developer culture is impacting innovation and creativity (206 pts)]]></title>
            <link>https://dayvster.com/blog/dev-culture-is-dying-the-curious-developer-is-gone/</link>
            <guid>45303199</guid>
            <pubDate>Fri, 19 Sep 2025 16:02:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dayvster.com/blog/dev-culture-is-dying-the-curious-developer-is-gone/">https://dayvster.com/blog/dev-culture-is-dying-the-curious-developer-is-gone/</a>, See on <a href="https://news.ycombinator.com/item?id=45303199">Hacker News</a></p>
<div id="readability-page-1" class="page"><article> <div> <div>  <!-- Responsive flex: row on md+, column on mobile --> <div> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M8 2v4"></path><path d="M16 2v4"></path><rect width="18" height="18" x="3" y="4" rx="2"></rect><path d="M3 10h18"></path></svg> <p><span>Thursday, September 18, 2025</span> </p></div>  <p> From tinkerers to metric seekers: How the shift in developer culture is impacting innovation and creativity. </p> <hr> </div>  <h2 id="when-curiosity-lead-the-way">When Curiosity Lead the Way</h2>
<p>If you have been in software development for a while, you might remember a time when developers were launching unique and innovative products and projects just for the sake of curiosity, learning or even just because they had a particular interest in a specific topic.</p>
<p>This curiosity and problem solving mindset gave us some of the best tools that we still use today such as VLC, Linux, Git, Apache HTTP Server, Docker(arguably), and many many more.</p>
<p>These tools were not created by large corporations or solopreneurs looking to increase their MMR or ARR. They were created by curious developers who wanted to solve a unique problem they had or even just wanted to learn something new.</p>
<h3 id="nights-spend-chasing-ideas-and-tinkering">Nights Spend Chasing ideas and Tinkering</h3>
<p>I still remember back in the 2000s (2003-2009) the nights I spent tinkering with new technologies, frameworks, and programming languages. I would often find myself staying up late into the night, fueled by curiosity and a desire to learn more about the craft of software development. I would make the dumbest of projects and the strangest of shortcuts just because I could and just to see if it would work. Even if it would only serve me and no one else, I would still make it because it was simply fun.</p>
<h3 id="learning-without-a-purpose">Learning Without a Purpose</h3>
<p>There is something to be said about learning without a clear purpose, goal or even a expected reward at the end of your journey. It allows you to explore new ideas and concepts without the pressure of having to deliver a specific outcome. It allows you to be creative and even tinker with suboptimal implementations and solutions or even some that are flat out insane or idiotic.</p>
<p>Because at the end of your journey, you will not be met with disappointment that you did not create a new product or service that will generate passive income or be used by hundreds of thousands of people. No, that was never your expectation going into it in the first place, you started the journey simply because you were curious and you wanted to create something even if your target demographic was just yourself.</p>
<p>This in many ways leads to a better learning journey and a more fulfilling experience as you are not bound by the constraints of having to deliver a specific outcome or meet certain expectations. You can simply explore and learn at your own pace and in your own way. Don’t get me wrong this does not apply only to new comers to the field or junior software developers, this applies to every single developer out there even the most experienced ones.</p>
<p>Personally I’d consider myself fairly experienced in the field of software development, I started learning C++ back in 2003 and my first job as a software developer was in 2008. I’ve been in the field for a while now. In fact the longer I am in the field the more I realize that I know nothing. There is always something new to learn and explore and I find myself constantly tinkering with technologies be they new or old. Learning is not only about new and shiny tech, sometimes it’s assembly or system design, microcontrollers, embedded etc.</p>
<p>This is plain and simply the <strong>tinkerers mindset</strong> and I believe that this mindset is slowly dying out in the field of software development. I find less and less like-minded people and I encounter more and more push back along the lines of “Why are you wasting your time with that? You should be focusing on X, Y or Z instead.” or “That is not going to help you in your career.” or even “You should be focusing on building products that will generate passive income or be used by hundreds of thousands of people.”</p>
<h2 id="the-era-of-metrics-and-shiny-things">The Era of Metrics and Shiny Things</h2>
<p>I find that there has been a strong shift in developer culture over the past decade or so. A very strong and worrying shift towards metrics, revenue optimization, delivering “value” and “building for the masses”. I’m not sure this is a good shift but it is one that is happening nonetheless.</p>
<p>It seems to me that the focus has shifted from curiosity, learning and a joy for creating cool things to a focus on metrics, observables, problem solving for your niche audience.</p>
<p>I see countless developers spending their free time using their free time using technologies they do not enjoy building products they do not care about for an audience they do not understand, simply because they believe that this is what they should be doing in order to be successful as a software developer or to be taken seriously in the field.</p>
<p>Many who I talk to believe that this will set them apart from the rest of the pack or that they are a temporarily embarrassed startup CTO/founder or that they are building the next big thing that will paint their name in the stars and grant them the fame and respect of their fellow developers.</p>
<p>But how can you ever hope to build something that huge if you do not even care about it? If the problem you are solving is not even a problem you yourself have or worse yet care about?</p>
<p>This is where a deeper issue shows itself, When you don’t care about what you are building you start looking elsewhere for that sense of progress, accomplishment or even identity. You become a Next.js developer, a React developer, a Rust developer etc… You start to identify yourself by the tools you use rather than the problems you solve or the things you create.</p>
<h3 id="chasing-every-new-framework-or-idea">Chasing Every new Framework or Idea</h3>
<p>If you’ve identified with anything in this article so far, then take a moment and answer this to yourself honestly. How often did you find yourself working on your product or project only get think oh but this new framework/library/module/plugin is so much better, I should be using that instead of what I am doing right now, I need to improve my stack, I need to be using the latest and greatest. Because I am building something that will eventually be used by hundreds of thousands of people, so why stunt my growth, why risk being left behind?</p>
<p>Naturally your webapp has to use the latest version of React or Next.js with it’s latest features and optimizations, a year or so back (2023-2024) that was React server components.</p>
<p>Or maybe you just had to switch to the newest version of Vue.js or Angular because they have some new feature that will make your life easier or your app faster or more scalable.</p>
<p>Or perhaps your utilities or backend are written in Go or Node or C# and really you should be using Rust because it’s just so damn fast and memory efficient. You can’t pass that up can you?</p>
<p>So you title yourself after whatever language, framework or library you are currently using. You are no longer a software developer, you are a Next.js developer, a specialist in your field.</p>
<p>you chase every new shiny thing and you write a product or service in that shiny thing optimizing for MMR, ARR, DAU, MAU, SEO rankings, conversion rates and all that jazz. Wondering why your product or service is not taking off, why no one is using it, why you are not getting any traction.</p>
<p>Weird… you used all the right things, you’ve used the technology that you should have used you’ve optimized for all the right metrics, you’ve done everything by the book. So why is it not working?</p>
<h2 id="what-we-lost-along-the-way">What we Lost Along the Way</h2>
<p>Constantly adopting the latest and greatest thing, not because it inspires you or because you care about it, but simply because you think you should be using it in order to be successful is a recipe for disaster. Not just for you for out entire developer culture as a whole.</p>
<p>I don’t want to sound overly dramatic but I do lament the loss of the curious developer, the tinkerer, the obsessed creative that just wants to build something cool even if nobody cares about it, even if it only solves their own problem.</p>
<p>I think we are slowly killing this mindset, it’s slowly disappearing from our culture wether that will be good or bad only time will tell. If you were to ask me I’d say it’s a very bad thing.</p>
<p>Don’t get me wrong we have occasional bright sparks of innovation and creativity HTMX, Bun, Astro, Zig and many other come to mind. But these are few and far between but they show that there are still curious developers out there, they are just harder to find and shrinking in numbers and being drowned out by the noise of metric seeking and revenue optimizations.</p>
<h2 id="the-world-moves-on-but-some-of-us-remember">The World Moves On, But Some of Us Remember</h2>
<p>I don’t want to sound too much like a middle aged man lamenting a world that is changing around him. I understand the world moves on, you get more and more jaded and cynical as you get older. But I assure you this is not that. This is a pattern I’ve been noticing for a while and it worries me. The tools the projects that were built by curious developers are still around and still in use but compared to before we get relatively few new ones that are truly build out of curiosity and passion.</p>
<p>There are occasional sparks but not like before.</p>
<p>Think of all the amazing software you use today, think which ones were made by insanely curious developers and think of how old that software is or when it came out and then think of more modern software and how many of those were made by massive corporations or solopreneurs or even just flat bought out or sold out.</p>
<p>I think we are losing something very important in our culture and I hope we can find it again before it’s too late. Before the curious developer is gone for good and we are left with a sea of software built with no privacy concerns, horrible monetization strategies, bloated frameworks and libraries and no ownership mindset, not for you the consumer and not by the creator.</p>
<h2 id="the-death-of-ownership-is-not-just-for-the-consumer">The Death of Ownership is not Just for the Consumer</h2>
<p>We’ve all seen the shifting tide, consumers no longer own their software, you may buy the newest Adobe suite, or JetBrains IDE, latest iPhone or Android or even the latest Windows, but you do not own it. It can be taken away from you at any time, you simply rent it, you pay a monthly fee to use it. You do not own it, you simply have a license to use it.</p>
<p>But do we ever take time to consider the loss of ownership for the creators? The developers, the curious tinkerers, the obsessed creatives that build these tools and software. Do they own it? Or do they simply rent it out to the highest bidder or sell it off to the largest corporation? Do people still want to build something that is uniquely their or do they simply want to build the latest and greatest SASS that they can rent out to the masses?</p>
<p>Do they care about the software they build or do they simply care about the metrics, the revenue, the growth?</p>
<p>You can argue that Linus Torvalds owns Linux and cares about Linux the kernel, you can argue that Jean Baptiste Kempf owns VLC and cares about VLC. Does Solomon Hykes own Docker and care about Docker? Does Daniel Ek own Spotify and care about Spotify? Does Mark Zuckerberg own Facebook and care about Facebook?</p>
<p>Are they owners, true owners of the product they built or did they simply become renters of their own creation, a slave to the metrics and revenue optimization?</p>
<p>Again I don’t want to be overly dramatic but this is an important question we all should be asking ourselves. I know I am asking myself this question more and more often as I see the world around me change and the developer culture shift towards metrics and revenue optimization.</p>
<h2 id="carving-space-for-curiosity-and-innovation">Carving Space for Curiosity and Innovation</h2>
<p>I implore you to find time in your life to be curious and creative to tinker and build something just for the sake of building it. Even if no one else cares about it, even if it only solves your own problem. Make something cool, something unique don’t care about others build it for yourself, built it because you want to and can.</p>
<p>Don’t let the world tell you what you should be doing, what you should be building, what you should be using, no matter how ambitious or dumb or idiotic it may seem to others, make it because you want to, because it makes you happy, because it makes you feel alive.</p>
<p>Software development is a unique craft, it’s equal parts creative and equal parts engineering, two opposing forces that when combined can create something truly amazing. Fight the temptation to add marketing into the mix and dilute the craft with it.</p>
<h3 id="build-what-you-cant-ship">Build what you Can’t Ship</h3>
<p>Have a project in mind that you’ve always wanted to tackle but it never made sense to you to do it because it would never be used by anyone else or it would never make you any money? Do it anyway, build it, tinker with it, learn from it. Who cares if you can’t ship it to the masses, who cares if it’s useless. Make it, create something from nothing, just because you can.</p>

<p>You might think this goes against my previous point, but it really does not. Share your work, share your creations, bring others into your world, if nobody responds who cares, you made it, you created something from nothing, maybe the value is in the journey and not the destination. Maybe the value is in the learning and not the outcome. Maybe the value is in the process and not the product.</p>
<p>And who knows maybe your unique problem will be shared by others, maybe your unique solution will inspire others to create something new, something unique. Maybe your curiosity will spark a fire in someone else and they will go on to create something truly amazing.</p>
<p>It’s not impossible it happened for Linux, it happened for VLC heck it happened for Git.</p>
<p>Just try to conceptualize what an insane idea Git was even when SVN was a well established and widely used version control system. Who in their right mind would think that a distributed version control system would be a good idea? Yet here we are, Git is the de facto standard for version control in software development.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I wrote this article to lament the loss of the curious spark in our developer culture, not to criticize or judge anyone. I understand the world moves on, I understand that we all have to make a living and that we all have to pay the bills. But I also believe that we should not lose sight of what makes software development such a unique and special craft.</p>
<p>If you’ve made it this far, thank you, sincerely thank you. It’s one of my longer articles and I appreciate you taking the time to read it. I hope it has sparked something in you, I hope it has made you think about your own journey as a software developer and I hope it has inspired you to be curious and creative again.</p>    </div> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trevor Milton's Nikola Case Dropped by SEC Following Trump Pardon (180 pts)]]></title>
            <link>https://eletric-vehicles.com/nikola/trevor-miltons-nikola-case-dropped-by-sec-following-trump-pardon/</link>
            <guid>45302220</guid>
            <pubDate>Fri, 19 Sep 2025 14:43:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eletric-vehicles.com/nikola/trevor-miltons-nikola-case-dropped-by-sec-following-trump-pardon/">https://eletric-vehicles.com/nikola/trevor-miltons-nikola-case-dropped-by-sec-following-trump-pardon/</a>, See on <a href="https://news.ycombinator.com/item?id=45302220">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
					
<p>The US Securities and Exchange Comission (SEC) has dropped its fraud case against <a href="https://eletric-vehicles.com/category/nikola/" data-type="category" data-id="236616">Nikola</a>‘s founder and former CEO Trevor Milton, after Trump’s pardon six months ago.</p>



<p>Trevor Milton, who had been convicted of securities fraud, <a href="https://eletric-vehicles.com/nikola/nikola-founder-trevor-milton-pardoned-by-donald-trump-video">received a pardon from the US President Donald Trump</a> in March, a month after the Phoenix-based electric and hydrogen truck maker <a href="https://eletric-vehicles.com/category/nikola/" data-type="category" data-id="236616">Nikola</a> filed for <a href="https://eletric-vehicles.com/nikola/ev-maker-nikola-files-for-chapter-11-bankruptcy">bankrupcy</a>.</p>



<p>The full and unconditional presidential pardon overturned a four-year prison sentence, imposed in December 2023, for deceiving investors about the company’s progress and products.</p>



<p>In an X post this Tuesday, the company’s founder said that “the SEC has dropped their case against me with prejudice.”</p>



<p>“5 years of outright lies by the media, corrupt prosecutors, former <a href="https://eletric-vehicles.com/category/nikola/" data-type="category" data-id="236616">Nikola</a> executives and short sellers is finally over,” he wrote.</p>



<p>Milton added that he comes “out of this thankful to my God for one more day in this life and for such a wonderful family and wife who never backed down against the evil men behind this.” (Read the full statement below.)</p>



<p>As the company deals with the Chapter 11 plan in a Delaware bankrupcy court, Milton <a href="https://eletric-vehicles.com/nikola/nikolas-trevor-milton-cites-trump-pardon-in-fight-over-69-million-claim/" data-type="post" data-id="81843">asked for a $69 million indemnification in legal fees against Nikola</a>, which the company has pushed back.</p>



<p>According to <a href="https://eletric-vehicles.com/category/nikola/" data-type="category" data-id="236616">Nikola</a>, Milton’s demand is invalid, as the ex-executive acted in a “grossly negligent, reckless” behaviour, engaging in “bad faith actions” during his time at the company.</p>



<p>However, last month, he argued that the bankrupcy plan does not fully reflect the presidential pardon he received.</p>



<p>&nbsp;“I don’t know him, but I was… they say it was very unfair. And they say the thing that he did wrong was he was one of the first people that supported a gentleman named Donald Trump for president,” the President said then.</p>



<p>The filing last month noted that “President Trump expressly decided here that Milton is factually innocent, the pardon did, contrary to the debtors’ assertions, wipe the slate clean.”</p>



<p>In May, <a href="https://eletric-vehicles.com/category/nikola/" data-type="category" data-id="236616">Nikola</a>‘s creditors committee asked the bankrupcy court to investigate whether Milton was dissipating personal assets that should be used to satisfy his debt to the company.</p>



<p>Milton stepped down from CEO in 2020, after allegations from Hindenburg Research that accused him of repeatedly misrepresenting <a href="https://eletric-vehicles.com/category/nikola/" data-type="category" data-id="236616">Nikola</a>‘s technological capabilities.</p>



<hr>



<p>Read Trevor Milton’s X statement below:</p>



<p>“The SEC has dropped their case against me with prejudice. 5 years of outright lies by the media, corrupt prosecutors, former Nikola executives and short sellers is finally over. </p>



<p>They falsely indicted me, silenced me, deleted my followers including here on X, stole my company, bankrupted my company, debanked me, targeted my friends and family, stole most my wealth and tried to put me in prison. </p>



<p>But it’s over now and eventually our creator will make it right. </p>



<p>I come out of this thankful to my God for one more day in this life and for such a wonderful family and wife who never backed down against the evil men behind this.”</p>




				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I regret building this $3000 Pi AI cluster (294 pts)]]></title>
            <link>https://www.jeffgeerling.com/blog/2025/i-regret-building-3000-pi-ai-cluster</link>
            <guid>45302065</guid>
            <pubDate>Fri, 19 Sep 2025 14:28:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jeffgeerling.com/blog/2025/i-regret-building-3000-pi-ai-cluster">https://www.jeffgeerling.com/blog/2025/i-regret-building-3000-pi-ai-cluster</a>, See on <a href="https://news.ycombinator.com/item?id=45302065">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://www.jeffgeerling.com/sites/default/files/images/raspberry-pi-compute-blade-10-node-ai-server.jpeg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-1c81ed77-5e91-4b90-aed5-0b13581d214a" data-insert-attach="{&quot;id&quot;:&quot;1c81ed77-5e91-4b90-aed5-0b13581d214a&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Raspberry Pi AI 10 node compute blade server"></p>

<p>I ordered a set of 10 Compute Blades in April 2023 (two years ago), and they just arrived a few weeks ago. In that time Raspberry Pi upgraded the CM4 to a CM5, so I ordered a set of 10 16GB CM5 Lite modules for my blade cluster. That should give me 160 GB of total RAM to play with.</p>

<p>This was the biggest Pi cluster I've built, and it set me back around $3,000, shipping included:</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-pricing.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-abd02693-5640-4684-8157-6fca4c656ff7" data-insert-attach="{&quot;id&quot;:&quot;abd02693-5640-4684-8157-6fca4c656ff7&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="10 Node Pi Cluster Pricing"></p>

<p>There's another Pi-powered blade computer, the <a href="https://www.kickstarter.com/projects/1907647187/small-board-big-possibilities-xerxes-pi">Xerxes Pi</a>. It's smaller and cheaper, but it just wrapped up <em>its own</em> Kickstarter. Will it ship in less than two years? Who knows, but I'm a sucker for crowdfunded blade computers, so of course I backed it!</p>

<p>But my main question, after sinking in a substantial amount of money: are Pi clusters even <em>worth</em> it anymore? I There's no way this cluster could beat the $8,000, 4-node Framework Desktop cluster in performance. But what about in <em>price per gigaflop</em>, or in efficiency or compute density?</p>

<p>There's only one way to find out.</p>

<h2>Compute Blade Cluster Build</h2>

<p>I made a video going over everything in this blog post—and the entire cluster build (and rebuild, and rebuild again) process. You can watch it here, or on YouTube:</p>

<div>
<p><iframe src="https://www.youtube.com/embed/8SiB-bNyP5E" frameborder="0" allowfullscreen=""></iframe></p>
</div>

<p>But if you're on the blog, you're probably not the type to sit through a video anyway. So moving on...</p>

<h2>Clustering means doing everything over <em>n</em> times</h2>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/compute-blade-cluster-nvme-install.jpeg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-4408b312-3e1d-4729-988d-cbae7f7bfac8" data-insert-attach="{&quot;id&quot;:&quot;4408b312-3e1d-4729-988d-cbae7f7bfac8&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Installing M.2 NVMe SSDs in Compute Blades"></p>

<p>In the course of going from 'everything's in the box' to 'running AI and HPC benchmarks reliably', I rebuilt the cluster basically three times:</p>

<ol>
<li>First, my hodgepodge of random NVMe SSDs laying around the office was unreliable. Some drives wouldn't work with the Pi 5's PCIe bus, it seems, other ones were a little flaky (there's a reason these were spares sitting around the place, and not in use!)</li>
<li>After replacing all the SSDs with <a href="https://amzn.to/4lPJ0JC">Patriot P300s</a>, they were more reliable, but the CM5s would throttle under load</li>
<li>I put <a href="https://amzn.to/45YACBV">these CM heatsinks</a> on without screwing them in... then realized they would pop off sometimes, so I took all the blades out <em>again</em> and screwed them into the CM5s/Blades so they were more secure for the long term.</li>
</ol>

<h2>Compute Blade Cluster HPL Top500 Test</h2>

<p>The first benchmark I ran was my <a href="https://github.com/geerlingguy/top500-benchmark">top500 High Performance Linpack cluster benchmark</a>. This is my favorite cluster benchmark, because it's the traditional benchmark they'd run on massive supercomputers to get on the <a href="https://www.top500.org/">top500 supercomputer list</a>.</p>

<p>Before I installed heatsinks, the cluster got 275 Gflops, which is an 8.5x speedup over a single 8 GB CM5. Not bad, but I noticed the cluster was only using 105 Watts of power during the run. Definitely more headroom available.</p>

<p>After fixing the thermals, the cluster did not throttle, and used around 130W. At full power, I got 325 Gflops, which is a 10x performance improvement (for 10x 16GB CM5s) over a single 8 GB CM5.</p>

<p>Compared to the $8,000 <a href="https://www.jeffgeerling.com/blog/2025/i-clustered-four-framework-mainboards-test-huge-llms">Framework Cluster I benchmarked last month</a>, this cluster is about 4 times slower:</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-hpl-cluster.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-4282fbe8-fd0e-4ea3-892d-548bb448a726" data-insert-attach="{&quot;id&quot;:&quot;4282fbe8-fd0e-4ea3-892d-548bb448a726&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Pi vs Framework Cluster HPL performance"></p>

<p>But the Pi cluster is <em>slightly</em> more energy efficient, on a Gflops/W basis:</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-hpl-efficiency.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-0d2634dd-ec84-4232-b303-9cbf3f1e26bf" data-insert-attach="{&quot;id&quot;:&quot;0d2634dd-ec84-4232-b303-9cbf3f1e26bf&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Pi vs Framework Cluster HPL Efficiency"></p>

<p>But what about price?</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-hpl-cost.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-ec9135e8-e6c6-4709-83ea-751dfb72a3d6" data-insert-attach="{&quot;id&quot;:&quot;ec9135e8-e6c6-4709-83ea-751dfb72a3d6&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Pi vs Framework Cluster Cost"></p>

<p>The Pi is a little less cost-effective for HPC applications than a Framework Desktop running a AMD's fastest APU. So discounting the fact we're only talking CPUs, I don't think any hyperscalers are looking to swap out a few thousand AMD EPYC systems for 10,000+ Raspberry Pis :)</p>

<p>But what about AI use cases?</p>

<h2>Compute Blade Cluster AI Test</h2>

<p>With 160 GB of total RAM, shared by the CPU and iGPU, this could be a small, efficient AI Cluster, right? Well, you'd think.</p>

<p>But no: <a href="https://github.com/ggml-org/llama.cpp/issues/9801">currently llama.cpp can't speed up AI using Vulkan on the Pi 5 iGPU</a>. That means we have 160 GB of RAM, but only CPU-powered inference. On pokey Arm Cortex A76 CPU cores with 10 GB/sec or so of memory bandwidth.</p>

<p>A small model (Llama 3.2:3B), running on a single Pi, isn't horrible; you get about 6 tokens per second. But that is pretty weak compared to even an Intel N100 (much less a single Framework Desktop):</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-ai-llama-32-3b.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-f74c7f5d-5974-4d10-b651-95c9e9f78222" data-insert-attach="{&quot;id&quot;:&quot;f74c7f5d-5974-4d10-b651-95c9e9f78222&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="AI model Llama 3.2 3B on Pi vs Framework vs N150"></p>

<p>You could have 10 nodes running 10 models, and that might be a very niche use case, but the real test would be running a <em>larger</em> AI model across all nodes. So I switched tracks to Llama 3.3:70B, which is a 40 GB model. It <em>has</em> to run across multiple Pis, since no single Pi has more than 16 GB of RAM.</p>

<p>Just as with the Framework cluster, llama.cpp RPC was <em>very</em> slow, since it splits up the model layers on all the cluster members, then goes round-robin style asking each node to perform its prompt processing, then token generation.</p>

<p>The Pi cluster couldn't even make it to token generation (tg) on my default settings, so I had to dial things back and only generate 16 tokens at a time to allow it to complete.</p>

<p>And after all that? Only 0.28 tokens per second, which is <em>25x slower</em> than the Framework Cluster, running the same model (except on AI Max iGPUs with Vulkan).</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-ai-llama-33-70b-tg.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-2006e890-bec6-4c83-bfad-723353ab509b" data-insert-attach="{&quot;id&quot;:&quot;2006e890-bec6-4c83-bfad-723353ab509b&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Pi vs Framework Cluster AI token generation"></p>

<p>I also tried <a href="https://github.com/exo-explore/exo">Exo</a> and <a href="https://github.com/b4rtaz/distributed-llama">distributed-llama</a>. Exo was having trouble even running a small 3B model on even a 2 or 3 node Pi cluster configuration, so I stopped trying to get that working.</p>

<p>Distributed llama worked, but only with up to 8 nodes for the 70B model. Doing that, I got a more useful 0.85 tokens/s, but that's still 5x slower than the Framework cluster (and it was a bit more fragile than llama.cpp RPC—the tokens were sometimes gibberish):</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-ai-llama-33-70b-tg-dllama.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-8f513004-09fa-4815-a2e5-676fce72077b" data-insert-attach="{&quot;id&quot;:&quot;8f513004-09fa-4815-a2e5-676fce72077b&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Pi vs Framework Cluster AI token generation - dllama"></p>

<p>You can find <em>all</em> my AI cluster benchmarking results in the issue <a href="https://github.com/geerlingguy/beowulf-ai-cluster/issues/6">Test various AI clustering setups on 10 node Pi 5 cluster</a> over on GitHub.</p>

<h2>Gatesworks and Conclusion</h2>

<p>Bottom line: this cluster's not a powerhouse. And dollar for dollar, if you're spending over $3k on a compute cluster, it's not the best value.</p>

<p>It <em>is</em> efficient, quiet, and compact. So if <em>density</em> is important, and if you need lots of small, physically separate nodes, this could actually make sense.</p>

<p>Like the only real world use case besides learning is for CI jobs or high security edge deployments, where you're not allowed to run multiple things on one server.</p>

<p>That's what <a href="https://unredacted.org/blog/2025/05/unredacted-labs/">Unredacted Labs</a> is building Pi clusters for: they're building Tor exit relays on blades, after they found the Pi was the most efficient way to run massive amounts of nodes. If your goal is efficiency and node density, this does win, ever so slightly.</p>

<p>But for 99% of you reading this: this is not the cluster you're looking for.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/gateworks-gblade-server.jpg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-57779fa3-1245-41f2-a134-8301fc41a298" data-insert-attach="{&quot;id&quot;:&quot;57779fa3-1245-41f2-a134-8301fc41a298&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Gateworks Gblade server"></p>

<p>Two years ago, when I originally ordered the Blades, <a href="https://www.gateworks.com/">Gateworks</a> reached out. They were selling a souped up version of the Compute Blade, made to an industrial spec. The <a href="https://www.gateworks.com/products/arm-server-blades/gblade-arm-server-blade/">GBlade</a> is around Pi 4 levels of performance, but with <em>10</em> gig networking, along with a 1 gig management interface.</p>

<p>But... it's discontinued. It doesn't look like any type of compute blade really lit the world on fire, and like the Blade movie series, the Compute Blade is more of a cult classic than a mainstream hit.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/compute-blade-cluster-front.jpeg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-3d1869ee-6403-40af-8e7d-e1521044c0b5" data-insert-attach="{&quot;id&quot;:&quot;3d1869ee-6403-40af-8e7d-e1521044c0b5&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Compute Blade 10 node Raspberry Pi Cluster mini rack 1"></p>

<p>This is a <em>bad</em> cluster. Except for maybe blade 9, which dies every time I run a benchmark. But I will keep it going, knowing it's <em>definitely</em> easier to maintain than the <a href="https://www.independent.com/2025/04/29/worlds-biggest-raspberry-pi-cluster-is-now-at-uc-santa-barbara/">1,050 node Pi cluster at UC Santa Barbera</a>, which to my knowledge is still the world's largest!</p>

<p>Before I go, I just wanted to give a special thanks to everyone who supports my on <a href="https://www.patreon.com/geerlingguy">Patreon</a>, <a href="https://github.com/sponsors/geerlingguy">GitHub</a>, <a href="https://www.youtube.com/c/JeffGeerling">YouTube Memberships</a>, and <a href="https://www.floatplane.com/channel/JeffGeerling">Floatplane</a>. It really helps when I take on these months- (or years!) long projects.</p>

<h2>Parts Used</h2>

<p>You might not want to replicate my cluster setup — but I always get asked what parts I used (especially the slim Ethernet cables... everyone asks about those!), so here's the parts list:</p>

<ul>
<li><a href="https://www.pishop.us/product/compute-blade-dev/">Compute Blade DEV</a></li>
<li><a href="https://www.pishop.us/product/fansta-1v1-0/">Compute Blade Standard Fan Unit</a></li>
<li><a href="https://github.com/Uptime-Lab/compute-blade/tree/main/models/bladerunner">Compute Blade 10" 3D Print Rackmount</a></li>
<li><a href="https://www.pishop.us/product/raspberry-pi-compute-module-5-16gb-ram-lite-cm5016000/">Raspberry Pi CM5 16GB (CM5016000)</a></li>
<li><a href="https://amzn.to/45YACBV">GLOTRENDS Aluminum CM5 Heatsink</a></li>
<li><a href="https://amzn.to/4lPJ0JC">Patriot P300 256GB NVMe SSD 10-pack</a></li>
<li><a href="https://amzn.to/3UNOwSd">GigaPlus 2.5 Gbps 10 port PoE+ switch</a></li>
<li><a href="https://www.printables.com/model/1215585-unified-10-rack-gigaplus-switch-mounting-ears">GigaPlus 10" Rack Mount 3D Print ears</a></li>
<li><a href="https://amzn.to/4fX1gzr">Monoprice Cat6A SlimRun 6" Cat6 patch cables (10 pack)</a></li>
<li><a href="https://amzn.to/47UlPKX">ioplex SFP+ Twinax DAC patch cable</a></li>
<li><a href="https://amzn.to/3UUjCHP">DeskPi RackMate TT</a></li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ants Seem to Defy Biology: They Lay Eggs That Hatch into Another Species (154 pts)]]></title>
            <link>https://www.smithsonianmag.com/smart-news/these-ant-queens-seem-to-defy-biology-they-lay-eggs-that-hatch-into-another-species-180987292/</link>
            <guid>45300865</guid>
            <pubDate>Fri, 19 Sep 2025 12:25:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/smart-news/these-ant-queens-seem-to-defy-biology-they-lay-eggs-that-hatch-into-another-species-180987292/">https://www.smithsonianmag.com/smart-news/these-ant-queens-seem-to-defy-biology-they-lay-eggs-that-hatch-into-another-species-180987292/</a>, See on <a href="https://news.ycombinator.com/item?id=45300865">Hacker News</a></p>
Couldn't get https://www.smithsonianmag.com/smart-news/these-ant-queens-seem-to-defy-biology-they-lay-eggs-that-hatch-into-another-species-180987292/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Ruby Central's Attack on RubyGems [pdf] (443 pts)]]></title>
            <link>https://pup-e.com/goodbye-rubygems.pdf</link>
            <guid>45299170</guid>
            <pubDate>Fri, 19 Sep 2025 08:09:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pup-e.com/goodbye-rubygems.pdf">https://pup-e.com/goodbye-rubygems.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45299170">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[iTerm2 Web Browser (125 pts)]]></title>
            <link>https://iterm2.com/documentation-web.html</link>
            <guid>45298793</guid>
            <pubDate>Fri, 19 Sep 2025 07:14:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://iterm2.com/documentation-web.html">https://iterm2.com/documentation-web.html</a>, See on <a href="https://news.ycombinator.com/item?id=45298793">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><a href="#" data-dropdown="drop-menu">Table of Contents</a></p>
<section>
<hr>

<h2>Overview</h2>

<p>iTerm2 includes built-in web browsing capabilities. Web browser sessions fit into iTerm2's existing window &gt; tab &gt; split pane hierarchy just like terminal sessions, allowing you to browse the web alongside your terminal work.</p>

<h2>Getting Started</h2>

<h3>Enabling the Browser</h3>

<ol>
<li>Install the <a href="https://iterm2.com/browser-plugin.html">browser plugin</a> to enable full functionality</li>
<li>Create a new profile</li>
<li>Go to <strong>Settings &gt; Profiles &gt; General</strong></li>
<li>Set <strong>Profile Type</strong> to <strong>Web Browser</strong></li>
</ol>


<p><strong>Note for Enterprise Users:</strong> Administrators can block the browser plugin by restricting bundle ID <code>com.googlecode.iterm2.iTermBrowserPlugin</code>.</p>

<h3>Disabling Browser Features</h3>

<p>If you prefer not to use browser features, you can completely hide them by setting <strong>Settings &gt; Advanced &gt; Enable browser-style profiles</strong> to <strong>No</strong>.</p>

<h2>Core Features</h2>

<h3>Navigation and Window Management</h3>

<ul>
<li>Browser profiles work within iTerm2's standard window/tab/split pane hierarchy.</li>
<li>Use hotkey windows, Open Quickly, navigation shortcuts, and per-session hotkeys as with terminal sessions.</li>
<li><strong>Exception:</strong> ⌘-[ and ⌘-] perform Back/Forward navigation instead of pane switching in browser sessions.</li>
<li><strong>Cmd+click</strong> opens links in new tabs.</li>
<li><strong>Cmd+Shift+click</strong> opens links in new vertical split panes.</li>
<li><strong>Cmd+Shift+Option+click</strong> opens links in new horizontal split panes.</li>
</ul>


<h3>Text and Selection</h3>

<ul>
<li><strong>Copy on selection</strong> works identically to terminal windows.</li>
<li><strong>Smart Selection</strong> works identically to terminal windows. Smart Selection Actions appear in the context menu.</li>
<li><strong>Copy Mode</strong> uses the same keystrokes as terminal sessions.</li>
<li><strong>Jump to Selection</strong> functions as in terminal sessions.</li>
<li><strong>Find</strong> supports regular expressions and case-sensitive search, just like in a terminal.</li>
</ul>


<h3>AI Integration</h3>

<ul>
<li>Link browser sessions to AI chat to discuss web pages.</li>
<li>Click hamburger menu → <strong>Ask AI</strong> to create a new AI chat with the reader-mode content of the current page attached.</li>
<li>Summarize, analyze, or ask questions about the current page.</li>
</ul>


<h3>Privacy and Security</h3>

<ul>
<li><strong>/dev/null mode</strong> is a mode for browsing privately that prevents any data from being saved to disk.</li>
<li>Built-in popup blocking blocks popups not initiated by user action.</li>
<li>Simple ad blocking using WebKit content blocker rules (configure via hamburger menu → <strong>Settings</strong>).</li>
<li>CONNECT proxy support for proxy-based adblockers.</li>
<li>The existing password manager has been integrated. Browser passwords are stored separately from terminal passwords.</li>
<li>Password manager integration for 1Password, and LastPass will use your existing web passwords.</li>
</ul>


<h3>Remote Access</h3>

<ul>
<li>View files on remote hosts via SSH Integration using URLs like: <code>iterm2-ssh://example.com/home/user/file.jpg</code></li>
</ul>


<h2>Advanced Features</h2>

<h3>Bookmarks and Organization</h3>

<ul>
<li><strong>Named Marks</strong> act as bookmarks for specific page sections (right-click → Add Named Mark).</li>
<li>Standard bookmarks available via hamburger menu.</li>
<li>Named Marks and Bookmarks work with Open Quickly and the Named Marks Toolbelt tool.</li>
</ul>


<h3>Recording and History</h3>

<ul>
<li><strong>Instant Replay</strong> captures browser sessions using macOS Screen Capture API.</li>
<li>Like instant replay in terminal windows, the RAM limit is respected.</li>
<li><strong>Global Search</strong> can search across browser sessions.</li>
</ul>


<h3>Automation and Customization</h3>

<ul>
<li><strong>Key Bindings</strong> work in browser profiles (some terminal-specific actions are unavailable).</li>
<li><strong>Triggers</strong> match when pages finish loading with web-specific actions.</li>
<li><strong>Pointer bindings</strong> and <strong>Actions</strong> available, minus some terminal-specific options.</li>
<li><strong>Snippets</strong> insert text into a focused form field.</li>
<li><strong>Broadcast Input</strong> works across browser sessions.</li>
<li><strong>Advanced Paste</strong> available, minus some terminal-specific features.</li>
<li><strong>Composer</strong> functions in browser sessions.</li>
</ul>


<h3>Content Management</h3>

<ul>
<li><strong>Reader Mode</strong> is avilable in the hamburger menu.</li>
<li><strong>Distraction removal mode</strong> similar to Safari's is also in the hamburger menu.</li>
<li>Right-click → <strong>Remove Element</strong> to hide cookie panels or other unwanted elements.</li>
<li><strong>Shell &gt; Save Contents</strong> saves web pages with resources.</li>
<li>Basic auto-fill of form fields is available, using your contact card information.</li>
<li>Search suggestions in URL bar.</li>
<li>Automatic audio detection and muting.</li>
</ul>


<h2>Technical Details</h2>

<p>The browser is built on WKWebView and identifies as Safari to ensure compatibility with most websites.</p>

<h2>Limitations</h2>

<ul>
<li><strong>Python API</strong> No browser-specific APIs yet. <a href="https://iterm2.com/bugs">File a feature request</a> if you have ideas.</li>
<li><strong>Passkeys</strong> Not supported due to Apple-imposed WKWebView restrictions.</li>
<li><strong>Advanced ad blocking:</strong> Limited by Apple's resource fetching API restrictions.</li>
</ul>


<h2>About This Feature</h2>

<p>This feature exists because:
- Many iTerm2 features translate well to web browsing
- It provides a unified terminal and browser experience
- A former colleague suggested this idea in 2014 and I haven't been able to stop thinking about it.
- I am maybe having a midlife crisis and this is cheaper than a sports car.</p>

<p>While not intended as a primary browser, iTerm2's web capabilities provide a useful tool for integrated terminal and web workflows.</p>


</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nostr (206 pts)]]></title>
            <link>https://nostr.com/</link>
            <guid>45298336</guid>
            <pubDate>Fri, 19 Sep 2025 05:49:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nostr.com/">https://nostr.com/</a>, See on <a href="https://news.ycombinator.com/item?id=45298336">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <!-- Header -->
    <header>
      
    </header>

    <!-- Hero -->
    <section>
      <h2>
        An open protocol with a chance of working
      </h2>
      <p>
        Nostr is an apolitical communication commons. A simple standard that defines a scalable
        architecture of clients and servers that can be used to spread information freely. Not
        controlled by any corporation or government, anyone can build on Nostr and anyone can use
        it.
        <img src="https://nostr.com/robots/nostr-robot.png" alt="Friendly robot">
      </p>
      
    </section>

    <!-- Screenshots -->
    <section>
      <!-- Terminal Mock -->
      

      <!-- Screenshot -->
      <img @click="window.open('https://coracle.social')" src="https://nostr.com/screenshots/coracle.png" title="the Coracle client">

      <!-- iframe -->
      

      <!-- iframe -->
      

      <!-- Screenshot -->
      <img @click="window.open('https://www.amethyst.social/')" src="https://nostr.com/screenshots/amethyst.png" title="the Amethyst client">

      <!-- Screenshot -->
      <img @click="window.open('https://yakihonne.com/yakihonne-mobile-app')" src="https://nostr.com/screenshots/yakihonne2.png" title="the Yakihonne client">

      <!-- Screenshot -->
      <img @click="window.open('https://nostur.com/')" src="https://nostr.com/screenshots/nostur.png" title="the Nostur client">
    </section>

    <!-- Integration Subtitle -->
    <section>
      <span>
        All Kinds of Stuff
      </span>
      <h2>
        Like the internet itself: open and chaotic
      </h2>
      <p>
        Nostr embraces the chaos of the early internet—multiple kinds of data, diverse forms of user
        interaction and different clients providing their own perspectives over the same underlying
        information.
      </p>
    </section>

    <!-- Innovation Card -->
    <div data-swap="cards">
        <div>
          <p><span>rebase</span>
          </p>
          <div>
            <h3>Many clients, many servers</h3>
            <p>
              The client is the app that is running on your computer or phone, the server is
              whatever is running on a cloud somewhere with a domain name. In centralized platforms
              and other protocols, one client talks to a single server. In Nostr clients connect to
              many.
            </p>
          </div>
        </div>
        <p><a href="https://newsletter.squishy.computer/p/natures-many-attempts-to-evolve-a" target="_blank">
          Learn how Nostr is different
          <span>›</span>
        </a>
      </p></div>

    <!-- Features Grid -->
    

    <!-- Cryptography Card -->
    <div data-swap="cards">
        <div>
          <p><span>signature</span>
          </p>
          <div>
            <h3>A new paradigm for communication</h3>
            <p>
              In Nostr, every user is represented by a secret number called a "key" and every
              message carries a digital "signature" that proves its authorship authorship and
              authenticity without the need for any authority to say so. This foundation of trust
              enables the decentralized broadcasting of information.
            </p>
          </div>
        </div>
        <p><a href="https://www.youtube.com/watch?v=GLluXpTRbqs&amp;t=169s" target="_blank">
          Watch a human-friendly explanation
          <span>›</span>
        </a>
      </p></div>

    <!-- Pro-censorship Section -->
    <div>
        <p><img :src="isDark ? '/robots/relayselection-dark.png' : '/robots/relayselection.png'"></p><div>
          <p><span>
            <span>shield</span> Pro-censorship
          </span></p><h2>
            The protocol is ownerless, relays are not
          </h2>
          <p>
            Nostr doesn't subscribe to political ideals of "free speech" — it simply recognizes that
            different people have different morals and preferences and each server, being privately
            owned, can follow their own criteria for rejecting content as they please and users are
            free to choose what to read and from where.
          </p>
          
          <div>
            <div>
              <div>
                <p>
                  handshake
                </p><p>
                Freedom of association
              </p></div>
              <p>
                When the network effect is not tied to a single organization a group of users cannot
                harm others.
              </p>
              <p><a href="https://www.youtube.com/watch?v=K5oXaW1EqbE" target="_blank">
                Watch <span>›</span>
              </a>
            </p></div>
            <div>
              <div>
                <p>
                  landscape_2
                </p><p>
                Your own piece of Nostr
              </p></div>
              <p>
                If you are a programmer or know how to run servers it is trivial to run your own
                relay with your own rules.
              </p>
              <p><a href="https://khatru.nostr.technology/" target="_blank">
                Write code <span>›</span>
              </a>
            </p></div>
          </div>
        </div>
      </div>

    <!-- Subprotocols Section -->
    <div>
        <p><img :src="isDark ? '/robots/feed-dark.png' : '/robots/feed.png'"></p><div>
          <p><span>
            <span>lightbulb_2</span> New Ideas
          </span></p><h2>Exploring the commons</h2>
          <p>
            Besides being a natural medium for a Twitter-like microblogging social network, Nostr
            can also be used for other purposes. And not only similar things like sharing
            <i>videos</i>, longform <i>articles</i>, <i>pictures</i> or <i>voice notes</i>. There
            are initiatives on Nostr for the development of sub-protocols that power
            <b>closed groups</b>, <b>decentralized wikipedia</b>, <b>couchsurfing</b>,
            <b>marketplaces</b> or <b>web annotations</b>; as well as protocols that don't use Nostr
            for the core data but as a coordination and discovery mechanism, such as
            <b>decentralized code collaboration using git</b>, <b>file hosting</b>,
            <b>torrent sharing</b> and <b>video livestreaming</b>.
          </p>
          <p><a href="https://github.com/nostr-protocol/nips" target="_blank">
            Browse the NIPs
          </a>
        </p></div>
      </div>

    <!-- Under Construction section -->
    <div>
        <p><img :src="isDark ? '/robots/underconstruction-dark.png' : '/robots/underconstruction.png'"></p><div>
          <p><span>
            <span>grass</span> Ecosystem
          </span></p><h2>Still under construction</h2>
          <p>
            Nostr is an idea with a lot of open-source software around it and a large userbase, but
            not a finished, polished product that you can buy without stress. We're still pretty
            much in the phase where new programmers and early adopters are needed to help us refine
            the protocol flows and the user experience.
          </p>
          

          <div>
            <div>
              
              <p>
                The so-called "outbox model" is the canonical way of implementing a
                censorship-resistant client, but its parameters are fluid.
              </p>
              <p><a href="https://github.com/coracle-social/how-to-nostr/blob/master/04-relays-are-repositories.md#the-outbox-model-and-friends" target="_blank">
                Learn about it <span>›</span>
              </a>
            </p></div>

            <div>
              
              <p>
                NIP-29 describes a way to do closed groups for forums or chat that can be very
                efficient by relying on a relay but are still censorship-resistant.
              </p>
              <p><a href="https://njump.me/naddr1qvzqqqr4gupzp978pfzrv6n9xhq5tvenl9e74pklmskh4xw6vxxyp3j8qkke3cezqqxnzde5xyersd33xscrwwfh5ekns6" target="_blank">
                Read the guide <span>›</span>
              </a>
            </p></div>
          </div>
        </div>
      </div>

    <!-- How Nostr Works -->
    <section>
      <span>
        <span>account_circle</span>
        Following
      </span>
      <h2>How Nostr works</h2>
      <p>
        Nostr enables true freedom by allowing users to stay connected to their audience even in
        adverse scenarios.
      </p>

      

      
    </section>

    <!-- FAQ Heading -->
    <section>
      <span>
        <span>contact_support</span>
        FAQ
      </span>
      <h2>I've got some questions!</h2>
      <p>
        It may sound like Nostr is very good, but what about these hard issues?
      </p>
    </section>

    <!-- FAQ Items -->
    <div>
        <!-- FAQ Item -->
        <details>
          <summary>
            What is a "protocol"?
          </summary>
          <div>
            <p>
              A protocol is like a common language that multiple different software can use to talk
              to each other, it's like <b>e-mail</b>, <b>HTML</b> or <b>HTTP</b>.
            </p>
            <p>
              When we say "protocol" we mean that there is no need to use a specific app in order to
              be in Nostr: there are many apps that talk the same language and can be used (mostly)
              interchangeably — and each has its own take on how to do and display things.
            </p>
          </div>
        </details>

        <!-- FAQ Item -->
        <details>
          <summary>
            How does Nostr handle spam and unwanted content?
          </summary>
          <div>
            <p>
              In the default feed you never see any spam, because clients will only fetch
              information from people that <i>you</i> follow. In that sense no one can "push" spam
              into you.
            </p>
            <p>
              It's trickier when you want to see, for example, replies to your posts, in that case a
              client might be programmed to fetch anything that <i>claims</i> to be a reply
              <i>from anyone</i>, which might include spam.
            </p>
            <p>
              The way we can deal with it on Nostr is by restricting our area of contact with the
              spam: for example, some clients may easily decide to only display replies that come
              from people followed by people you follow. More refined strategies involve announcing
              and then only reading notes from relays known to be "safe" according to your criteria
              (could be relays that require payment, relays that do screening for humans, relays
              that only accept members of certain communities or political affiliations etc).
            </p>
            <p>
              There are no perfect solutions. But these do not exist anywhere, centralized platforms
              are also full of spam. Nostr at least isn't naïve and tries to build resiliency from
              the start.
            </p>
          </div>
        </details>

        <!-- FAQ Item -->
        <details>
          <summary>
            Will Nostr scale effectively with massive user adoption?
          </summary>
          <div>
            <p>
              Yes, Nostr is just a basic client-server architecture. And the fact that users can
              naturally spread among hundreds of different relays while clients can query dozens of
              relays that they're interested in at the same time means the network has a natural
              load balancer (which doesn't prevent a single relay from having its own internal load
              balancer either).
            </p>
            <p>
              Another (almost the opposite) concern that may be raised is with problems arising from
              clients having to connect to too many relays if the profiles being followed for
              whatever reason decide to spread way too much, but this shouldn't be a problem either
              because people tend to follow many accounts with similar content and these will tend
              to share relays. Still, if it happens, it's cheap for native apps to open many
              hundreds of WebSocket connections simultaneously (as they will be getting very few
              data in each of those). For web apps that isn't so hard, but we can still go up to a
              few hundreds without big problems. Regardless of any of that, in any complete enough
              app that wants to display a "following feed" it's already necessary to store events in
              a local database, and that will make all these issues easy to deal with as you can do
              the event requests in batches instead of all at once.
            </p>
          </div>
        </details>

        <!-- FAQ Item -->
        <details>
          <summary>
            What protections does Nostr offer against online harassment?
          </summary>
          <div>
            <p>
              Harassment is similar to spam in the sense that anyone can still create the undesired
              content and publish to the relays that accept them. All the techniques mentioned in
              avoiding spam can also be applied in this case, but if we're talking about specific
              individuals with a permanent identity and not only an army of bots in this case the
              problem becomes easier, as those individuals can just be blocked by their target and
              their content will vanish. Presumably friends of such target will also block, and
              creative solutions involving shared blocklists can be created such that some people
              don't even have to click the block button directly.
            </p>
            <p>
              Other approaches involving, for example, relays with restricted read (that can emulate
              "protected account"/"only friends" features seen in centralized platforms) can further
              improve this.
            </p>
          </div>
        </details>

        <!-- FAQ Item -->
        <details>
          <summary>
            Why not just use Mastodon/Fediverse?
          </summary>
          <div>
            <p>
              There are many problems with Mastodon, mostly due to the fact that it doesn't rely on
              any cryptography. Because it cannot do the multi-master approach of Nostr due to lack
              of cryptography, identities are assumed to be "owned" by the server, which is fully
              trusted by its tenants. Mastodon server owners can do all the harm centralized
              platforms can do to their underlings, which are completely helpless in case of
              misbehavior or even in the normal case where a server owner loses their server or
              decides to shut down for whatever reason.
            </p>
            <p>
              Worse than that, for many of its purported features, such as blocking or direct
              messages, users have to also trust owners of the other servers.
            </p>
            <p>
              There are also problems with reliance on the DNS system, but we don't have to talk
              about those.
            </p>
            <p>
              The most interesting feature of Mastodon is that by its nature it creates communities
              with shared values that grow in each of its servers. Or, should I say, that should be
              a feature if it actually worked like that. In fact these are not really communities,
              but a mashup of users that may share some interests among each other, but also have
              other interests and those other interests end up polluting the supposed "community"
              with things that do not interest the other users.
            </p>
            <p>
              Nostr, on the other hand, can create real communities around relays, specifically
              because users don't have to fully belong to those relays, but can go to them only for
              some of their needs and go to other relays for other needs.
            </p>
          </div>
        </details>

        <!-- FAQ Item -->
        <details>
          <summary>
            Why not just use Bluesky/ATProto?
          </summary>
          <div>
            <p>Bluesky has many problems, the two most pronounced are:</p>
            <ol role="list">
              <li>
                Identity centralization: all accounts belong to <b>PLC</b>, a database ran by a
                central entity that can censor at will — or, alternatively, they can belong to a DNS
                domain, which is cumbersome, also censorable, risky and is not expected to be used
                by many anyway;
              </li>
              <li>
                Data centralization: because the <code>Relay-AppView-Client</code> flow assumes only
                one canonical source of data at each step (unlike Nostr multi-master architecture)
                that source is always a server that has power to censor, shadowban, reorder data and
                so on.
              </li>
            </ol>
            <p>
              <code>Clients</code> are assumed to be dumb and trust the <code>AppView</code>, and
              here you have room for all sorts of undesired shenanigans. Then AppViews also assume
              to source their data from a single <code>Relay</code>, and here you have room for the
              same effect.
            </p>
            <p>
              You could argue that Bluesky <code>Clients</code> could become smart and start
              sourcing data from multiple <code>AppViews</code>, or from multiple Relays, or that
              the <code>AppViews</code> could rely on multiple <code>Relays</code>, or that the
              <code>Clients</code> could talk directly to the <code>PDSes</code> — and all of that
              is possible and would indeed bring solutions, but notice that if those things started
              happening Bluesky would end up becoming Nostr, except with more steps.
            </p>
          </div>
        </details>

        <!-- FAQ Item -->
        <details>
          <summary>
            Are economic incentives aligned to keep relays operational?
          </summary>
          <div>
            <p>
              Yes,
              <a target="_blank" href="https://njump.me/nevent1qqsxstdgkge3k9mtezv4z4lpp88p3lnqta9k0k6n8hex0a4zv62pdmqzyqalp33lewf5vdq847t6te0wvnags0gs0mu72kz8938tn24wlfze6pfalk0">this clip</a>
              answers it well.
            </p>
            <p>
              But basically the answer is the same as the question about scale: if users can go to
              whatever relay they want we'll see relays ran by all sorts of people and entities.
              Running servers is very cheap, and a relay can run on a $5/mo server and house at
              least a few thousand users. It's not hard to imagine relays ran by communities,
              individuals who just want to be useful to others, big organizations wanting to gain
              good will with some parts of the public, but also companies, client makers, and, of
              course, dedicated entities who sell relay hosting for very cheap.
            </p>
          </div>
        </details>

        <!-- FAQ Item -->
        <details>
          <summary>
            If content is spread across multiple relays how can I be sure I'm seeing everything?
          </summary>
          <p>
              It's not a feature of the world at large to be able to see or hear everything that is
              happening everywhere at all times. Nostr inherits that property from the world, making
              it so that you can only see what you focus your attention on (and you're allowed to
              see by the relay that hosts that information).
            </p>
        </details>

        <!-- FAQ Item -->
        <details>
          <summary>
            How does search work?
          </summary>
          <div>
            <p>
              It's only possible to search on what you have seen, so search engines will always have
              to crawl some parts of the network they chose to and index those to enable public
              search. The word "chose" is employed because, as we know, there can't be a "global"
              view of the network (and no one would want such a thing anyway as it would be full of
              spam), so indexers have to choose. This is not different from Google deciding what
              websites to index.
            </p>
            <p>
              On the other hand, it's surprisingly doable for clients to store all the posts from
              people you follow, or all the posts you have seen or interacted with over time (since
              it's just text, a huge amount of notes can fit in the same space that would otherwise
              be required to store a single photo, for example) then provide local search over that.
              That kind of search will be sufficient for most of the cases you would reach out for a
              search bar in a centralized platform (which is to search for things that you have seen
              before), and perhaps even more useful since it would naturally filter out all the
              unrelated garbage.
            </p>
            <p>
              Last, niche or community-oriented relays can also provide very useful search
              capabilities by just indexing the notes they have stored locally, already filtered and
              scoped to that relay's topic or cohort (imagine searching over a Discord, Slack or
              Telegram group, for example).
            </p>
          </div>
        </details>

        <!-- FAQ Item -->
        <details>
          <summary>
            How can I discover new content from people I don't already follow if there are no
            algorithms?
          </summary>
          <div>
            <p>
              The most basic way to do that is by following the natural habits used by most
              centralized social platforms users since a long time ago: by looking at the people you
              follow and whom they're interacting with.
            </p>
            <p>
              But also it's not true that Nostr doesn't have algorithms. Nostr can have algorithms
              of all kinds: manual, automatic, AI-powered or rule-based. Some of these algorithms
              can be run entirely locally on clients (for example, surfacing posts from the times
              when you were not online, or from people that make fewer posts), while other
              algorithms can be provided by all sorts of relays, either by naturally surfacing posts
              from a community of people you don't follow or by dedicated relays that have the
              stated purpose of curating content desirable for a target audience or even by
              targeting specific users.
            </p>
          </div>
        </details>

        <!-- FAQ Item -->
        <details>
          <summary>
            Is Nostr related to Bitcoin?
          </summary>
          <div>
            <p>
              Nostr uses the same cryptographic principles of Bitcoin and was kickstarted mostly by
              a community of Bitcoiners, so it has disproportionately attracted the attention of
              Bitcoiners at the start, but aside from that it doesn't have any relationship with
              Bitcoin. It doesn't depend on Bitcoin for anything and you don't have to know or have
              or care about any Bitcoin in order to use Nostr.
            </p>
            <p>
              What about "zaps"? Zaps are a standard for tipping Nostr content using Bitcoin that is
              implemented by some Nostr clients, but it's fully and completely optional and if you
              don't care about Bitcoin you don't have to bother about it.
            </p>
          </div>
        </details>
      </div>

    

    <!-- .com Services -->
    

    <!-- Quotations -->
    <div>
        <p><span>
          <span>speaker_notes</span>
          Opinion
        </span></p><h2>What people are saying</h2>
        <p>
          Quotes from those who know better and decided to like Nostr.
        </p>
      </div>

    <!-- Footer -->
    

    

    

    

    

    <!-- Key Generation Modal -->
    
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini in Chrome (260 pts)]]></title>
            <link>https://gemini.google/overview/gemini-in-chrome/</link>
            <guid>45297331</guid>
            <pubDate>Fri, 19 Sep 2025 02:25:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gemini.google/overview/gemini-in-chrome/">https://gemini.google/overview/gemini-in-chrome/</a>, See on <a href="https://news.ycombinator.com/item?id=45297331">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="root"><main id="main"><div id="hero" aria-label="Meet Gemini in Chrome"><h2>Her er <strong>Gemini</strong> i Chrome</h2><ul><li><a href="https://support.google.com/gemini?p=mws_gic_ga" rel="nofollow" data-tracking-event="{&quot;event&quot;:&quot;gemini_in_chrome_cta&quot;,&quot;eventParams&quot;:{&quot;buttonPosition&quot;:&quot;top&quot;}}"><p>Få flere oplysninger</p></a></li></ul></div><section id="help-thats-right-with-you" aria-label="Intelligens, der arbejder sammen med dig, lige der, hvor du er."><div><h2>Intelligens, der arbejder sammen med dig, lige der, hvor du er.</h2><p>Get key takeaways, clarify concepts, and find answers based on the context of your open tabs.</p></div><gemini-5050-accordion id="see-how-it-works"><div><mws-accordion data-mode="single"><mws-accordion-item id="see-how-it-works-drawer-0" open=""><div data-slot="accordion-item-content" id="see-how-it-works-drawer-0-content" role="region" aria-labelledby="see-how-it-works-drawer-0-trigger"><p>Har du brug for et overblik i en fart? Gemini leverer præcise opsummeringer af artikler, sider og tråde direkte i din browser, så du hurtigt får et overblik over hovedpunkterne.</p></div></mws-accordion-item><mws-accordion-item id="see-how-it-works-drawer-1"><div data-slot="accordion-item-content" id="see-how-it-works-drawer-1-content" role="region" aria-labelledby="see-how-it-works-drawer-1-trigger"><p>Have a question about what you're reading? Ask Gemini. It uses the context of your open tabs to provide relevant answers and explanations, keeping you focused.</p></div></mws-accordion-item><mws-accordion-item id="see-how-it-works-drawer-2"><div data-slot="accordion-item-content" id="see-how-it-works-drawer-2-content" role="region" aria-labelledby="see-how-it-works-drawer-2-trigger"><p>Gå ud over de simple forklaringer. Når du beskæftiger dig med komplekse emner eller nye koncepter, kan du bede Gemini om ikke bare at klarlægge forvirrende dele, men også om at hjælpe dig med at engagere dig aktivt i materialet.</p></div></mws-accordion-item><mws-accordion-item id="see-how-it-works-drawer-3"><div data-slot="accordion-item-content" id="see-how-it-works-drawer-3-content" role="region" aria-labelledby="see-how-it-works-drawer-3-trigger"><p>Undersøger du produkter, eller overvejer du forskellige valgmuligheder? Bed Gemini om at finde nøgleoplysninger, specifikationer og fordele og ulemper på siden, så du kan træffe velovervejede beslutninger på en nem og overskuelig måde.</p></div></mws-accordion-item><mws-accordion-item id="see-how-it-works-drawer-4"><div data-slot="accordion-item-content" id="see-how-it-works-drawer-4-content" role="region" aria-labelledby="see-how-it-works-drawer-4-trigger"><p>Vil du brainstorme, organisere dine tanker eller dykke dybere ned i et emne? Chat naturligt med Gemini Live, og få mundtlige svar – alt sammen i Chrome.</p></div></mws-accordion-item><mws-accordion-item id="see-how-it-works-drawer-5"><div data-slot="accordion-item-content" id="see-how-it-works-drawer-5-content" role="region" aria-labelledby="see-how-it-works-drawer-5-trigger"><p>Just like on your computer, Gemini on mobile is there to answer questions about what you’re reading. On Android it works with anything on your screen—including Chrome. And coming soon to iOS, Gemini will be built right into the Chrome app.</p></div></mws-accordion-item></mws-accordion></div></gemini-5050-accordion></section><section id="your-web-your-control" aria-label="Dit net, du bestemmer"><div><h2>Dit net, du bestemmer</h2><p>Gemini i Chrome arbejder sammen med dig på dine præmisser. Den hjælper kun, når du beder den om det, så du har kontrollen.</p></div><div data-num-cards="3"><div><h3>Altid klar</h3><p>Gemini i Chrome aktiveres kun, når du vælger det ved at klikke på Gemini-ikonet eller den tastaturgenvej, du har konfigureret. Den hjælper på dine præmisser og træder kun til, når du beder om det.</p></div><div><h3>Få hjælp på din måde</h3><p>Få hjælp på din måde med Gemini i Chrome. Sig eller skriv dit spørgsmål på en naturlig måde, og Gemini bruger sidens indhold til at give dig en forståelse af emnet eller udføre kedelige opgaver.</p></div></div></section><div id="" aria-label="Nettet på en ny måde"><h2>Nettet på en ny måde</h2><p>With Gemini in Chrome, no tab switching is needed with AI assistance right in your browser, helping you quickly understand content or get tasks done using the context of your open tabs.</p><ul><li><a href="https://support.google.com/gemini?p=mws_gic_ga" rel="nofollow" data-tracking-event="{&quot;event&quot;:&quot;gemini_in_chrome_cta&quot;,&quot;eventParams&quot;:{&quot;buttonPosition&quot;:&quot;bottom&quot;}}"><p>Få flere oplysninger</p></a></li></ul></div><div id="faqs" aria-labelledby="faqs-title" aria-label="Ofte stillede spørgsmål"><p><h2 id="faqs-title">Ofte stillede spørgsmål</h2></p><mws-accordion data-mode="multiple"><mws-accordion-item id="faqs-accordion-item-0"><div data-slot="accordion-item-content" id="faqs-accordion-item-0-content" role="region" aria-labelledby="faqs-accordion-item-0-trigger"><p>With the <a href="https://support.google.com/gemini?p=mws_gic_ga">Gemini in Chrome</a> feature, you can get AI assistance from your browser to do things easily like get key takeaways, clarify concepts, find answers and more. To provide the most relevant responses, Gemini in Chrome uses the context of your open tabs.&nbsp;</p><p>Gemini i Chrome er en del af Chrome-browseren på computer og er ikke det samme som at gå til Gemini i en browser på <a href="http://gemini.google.com/">gemini.google.com</a> eller starte en chat med Gemini-webappen ved at skrive @gemini i adresselinjen i Chrome. Du kan bruge Gemini-webappen i andre browsere (eller i indholdsområdet af Chrome), men du kan hverken dele sideindhold eller bruge Live-tilstand, som du kan med Gemini i Chrome.</p></div></mws-accordion-item><mws-accordion-item id="faqs-accordion-item-1"><div data-slot="accordion-item-content" id="faqs-accordion-item-1-content" role="region" aria-labelledby="faqs-accordion-item-1-trigger"><p>You can access <a href="https://support.google.com/gemini?p=mws_gic_ga">Gemini in Chrome</a> through the Gemini icon in the Chrome toolbar or via a keyboard shortcut that you set up on a Windows or Mac desktop.</p><p>You can also activate Gemini when using Chrome on Android, and other apps, by holding the power button. And starting soon, on iOS Gemini in Chrome will be built into the app, with access through the Chrome omnibox.</p></div></mws-accordion-item><mws-accordion-item id="faqs-accordion-item-2"><div data-slot="accordion-item-content" id="faqs-accordion-item-2-content" role="region" aria-labelledby="faqs-accordion-item-2-trigger"><p><a href="https://support.google.com/gemini?p=mws_gic_ga">Gemini in Chrome</a> is&nbsp;rolling out to all eligible Mac and Windows users&nbsp;in the US who have their Chrome language set to English. We look forward to bringing this feature to more people and additional languages soon.</p><p>Gemini in Chrome on iOS is coming soon to eligible iPhone users in the US who have their Chrome language set to English.</p></div></mws-accordion-item></mws-accordion></div><div id="disclaimers"><p>Check responses. Setup required. Compatibility and availability varies. 18+</p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Playing “Minecraft” without Minecraft (2024) (137 pts)]]></title>
            <link>https://lenowo.org/viewtopic.php?t=5</link>
            <guid>45297258</guid>
            <pubDate>Fri, 19 Sep 2025 02:13:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lenowo.org/viewtopic.php?t=5">https://lenowo.org/viewtopic.php?t=5</a>, See on <a href="https://news.ycombinator.com/item?id=45297258">Hacker News</a></p>
Couldn't get https://lenowo.org/viewtopic.php?t=5: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Help Us Raise $200k to Free JavaScript from Oracle (490 pts)]]></title>
            <link>https://deno.com/blog/javascript-tm-gofundme</link>
            <guid>45297066</guid>
            <pubDate>Fri, 19 Sep 2025 01:40:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deno.com/blog/javascript-tm-gofundme">https://deno.com/blog/javascript-tm-gofundme</a>, See on <a href="https://news.ycombinator.com/item?id=45297066">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>After more than
<a href="https://javascript.tm/letter" rel="noopener noreferrer">27,000 people signed our open letter to Oracle</a>
about the “JavaScript” trademark, we filed a formal <strong>Cancellation Petition</strong>
with the US Patent and Trademark Office. Ten months in, we’re finally reaching
the crucial <strong>discovery phase</strong>.</p>
<p>Deno initiated this petition since we have legal standing as a JavaScript
runtime, but it’s really on behalf of all developers. If we win, “JavaScript”
becomes public domain – free for all developers, conferences, book authors, and
companies to use without fear of trademark threats.</p>
<p>We’re asking for your support through our
<a href="https://www.gofundme.com/f/help-us-challenge-oracles-javascript-trademark/donate" rel="noopener noreferrer">GoFundMe campaign</a>
so we can put forward the strongest case possible.</p>
<h3 id="why-200k">Why $200k?</h3>
<p>Because federal litigation is expensive. Discovery is the most
resource-intensive stage of litigation, where evidence is collected and
arguments are built.</p>
<p>We don’t want to cut corners – we want to make the best case possible by
funding:</p>
<ul>
<li><strong>Professional public surveys</strong> that carry legal weight in front of the USPTO,
proving that “JavaScript” is universally recognized as the name of a language,
not Oracle’s brand.</li>
<li><strong>Expert witnesses</strong> from academia and industry to testify on JavaScript’s
history, usage, and meaning.</li>
<li><strong>Depositions and records</strong> from standards bodies, browser vendors, and
industry leaders showing Oracle has no role in the language’s development.</li>
<li><strong>Legal filings and responses</strong> to counter Oracle’s claims at every step.</li>
</ul>
<p>If there are leftover funds, we’ll donate them to the
<a href="https://openjsf.org/governance" rel="noopener noreferrer"><strong>OpenJS</strong></a> to continue defending civil
liberties in the digital space. None of the funds will go to Deno.</p>
<h3 id="oracle-officially-denies-javascript-is-generic">Oracle officially denies “JavaScript” is generic</h3>
<p>On August 6th, 2025, Oracle for the first time addressed the validity of the
trademark.
<a href="https://ttabvue.uspto.gov/ttabvue/v?pno=92086835&amp;pty=CAN&amp;eno=16" rel="noopener noreferrer">Their response</a>
to our
<a href="https://ttabvue.uspto.gov/ttabvue/v?pno=92086835&amp;pty=CAN&amp;eno=1" rel="noopener noreferrer">petition</a>
denies that “JavaScript” is a generic term.</p>
<p>If you’re a web developer, it’s self-evident that Oracle has nothing to do with
JavaScript. The trademark system was never meant to let companies squat on
commonly-used names and rent-seek – it was designed to protect active brands in
commerce. US law makes this distinction explicit.</p>
<p>We urge you to read
<a href="https://ttabvue.uspto.gov/ttabvue/v?pno=92086835&amp;pty=CAN&amp;eno=1" rel="noopener noreferrer">our petition</a>
and <a href="https://javascript.tm/letter" rel="noopener noreferrer">open letter</a> to understand our arguments.</p>
<p>If we don’t win discovery, Oracle locks in ownership of the word “JavaScript.”
This is the decisive moment.</p>
<p>But this case is bigger than JavaScript. It’s about whether trademark law works
as written, or whether billion-dollar corporations can ignore the rule that
trademarks cannot be generic or abandoned. “JavaScript” is obviously both. If
Oracle wins anyway, it undermines the integrity of the whole system.</p>
<p>Let’s make sure the law holds.
<a href="https://www.gofundme.com/f/help-us-challenge-oracles-javascript-trademark/donate" rel="noopener noreferrer">Please donate</a>.
Please share and upvote this.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[David Lynch LA House (241 pts)]]></title>
            <link>https://www.wallpaper.com/design-interiors/david-lynch-house-los-angeles-for-sale</link>
            <guid>45296638</guid>
            <pubDate>Fri, 19 Sep 2025 00:30:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wallpaper.com/design-interiors/david-lynch-house-los-angeles-for-sale">https://www.wallpaper.com/design-interiors/david-lynch-house-los-angeles-for-sale</a>, See on <a href="https://news.ycombinator.com/item?id=45296638">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">
<p id="860c08d3-d800-4389-abae-f88720bbb782"><a data-analytics-id="inline-link" href="https://www.wallpaper.com/tag/david-lynch" data-before-rewrite-localise="https://www.wallpaper.com/tag/david-lynch">David Lynch</a>, the visionary American filmmaker behind Twin Peaks, Blue Velvet and Mulholland Dr, <a data-analytics-id="inline-link" href="https://www.wallpaper.com/art/remembering-david-lynch-obituary" data-before-rewrite-localise="https://www.wallpaper.com/art/remembering-david-lynch-obituary">passed away this January</a>, yet his creative universe endures in objects, spaces and ideas.</p><p>Among the most striking of these relics is his larger-than-life, meticulously designed Hollywood Hills home; a cinematic setting in its own right. Perched on a sweeping 2.3-acre hillside, David Lynch’s private compound, which is now listed for $15 million by Marc Silver of The Agency, unfolds like one of his own intricately plotted storylines. A showcase of <a data-analytics-id="inline-link" href="https://www.wallpaper.com/tag/midcentury-modern" data-before-rewrite-localise="https://www.wallpaper.com/tag/midcentury-modern">Mid-Century modern architecture</a>, the estate was conceived with the same care and cinematic precision that defined his work.</p><h2 id="inside-david-lynch-s-los-angeles-estate-3">Inside David Lynch's Los Angeles estate</h2><figure data-bordeaux-image-check="" id="30df4fdb-a214-4048-9ce4-bde64a5773d4"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-1080-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-1080-80.jpg.webp 1600w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-1080-80.jpg.webp 1280w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-768-80.jpg.webp 768w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-415-80.jpg.webp 415w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-360-80.jpg.webp 360w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-320-80.jpg.webp 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)">
<img src="https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ.jpg" alt="David Lynch house in LA" srcset="https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-1080-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-1080-80.jpg 1600w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-1080-80.jpg 1280w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-768-80.jpg 768w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-415-80.jpg 415w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-360-80.jpg 360w, https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ-320-80.jpg 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/qZV29f7uwbajiGKRmUgbPZ.jpg">
</picture></p></div><p>(Image credit: @hellomarcsilver)</p></figure><p id="8ef1cd5a-194e-4f1f-bea4-126f834e0241">The property, set across five contiguous parcels, reads like a storyboard in relief: three main residences and several ancillary structures stepping down the hillside, each capturing a different note in Lynch’s creative oeuvre.</p><p>The story behind this compound started in 1987, when he acquired the pink-hued Beverly Johnson House designed in the early 1960s by Lloyd Wright, son of <a data-analytics-id="inline-link" href="https://www.wallpaper.com/tag/frank-lloyd-wright" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.wallpaper.com/tag/frank-lloyd-wright">Frank Lloyd Wright</a>. The home, in fact, was recognised by Historic Places LA as an exemplary work of Mid-Century Modern residential design. Then in 1991, he commissioned Eric Lloyd Wright (Lloyd Wright’s son) to add a pool and pool house, extending the Wright imprint on his property with a new generation.</p><figure data-bordeaux-image-check="" id="5f89dbe0-f906-4a1e-be64-38ae8424c085"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-1080-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-1080-80.jpg.webp 1600w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-1080-80.jpg.webp 1280w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-768-80.jpg.webp 768w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-415-80.jpg.webp 415w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-360-80.jpg.webp 360w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-320-80.jpg.webp 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)">
<img src="https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ.jpg" alt="David Lynch house in LA" srcset="https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-1080-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-1080-80.jpg 1600w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-1080-80.jpg 1280w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-768-80.jpg 768w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-415-80.jpg 415w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-360-80.jpg 360w, https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ-320-80.jpg 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/PgcGNM2xyyeRmta3RN9UQZ.jpg">
</picture></p></div><p>(Image credit: @hellomarcsilver)</p></figure><figure data-bordeaux-image-check="" id="b1ae9c49-c048-4e87-ac49-e92aff8eaff9"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-1080-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-1080-80.jpg.webp 1600w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-1080-80.jpg.webp 1280w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-768-80.jpg.webp 768w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-415-80.jpg.webp 415w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-360-80.jpg.webp 360w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-320-80.jpg.webp 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)">
<img src="https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ.jpg" alt="David Lynch house in LA" srcset="https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-1080-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-1080-80.jpg 1600w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-1080-80.jpg 1280w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-768-80.jpg 768w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-415-80.jpg 415w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-360-80.jpg 360w, https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ-320-80.jpg 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/XozxLsJH4VQ6t24iLAZEQZ.jpg">
</picture></p></div><p>(Image credit: @hellomarcsilver)</p></figure><p id="b316f11d-f378-468f-a833-6998bd623a63">Across the years, Lynch kept expanding the plotline: in 1989, he purchased an adjoining two-bedroom Brutalist house; in 1995, a studio building; and later, more pieces of land, ultimately shaping a seven-structure sanctuary with 10 bedrooms and 11 bathrooms spread over roughly 11,000 square feet. The result was a creative campus perched above the city.</p><figure data-bordeaux-image-check="" id="cffa9a99-7c14-4250-ab1f-a342aeecc04b"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-1080-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-1080-80.jpg.webp 1600w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-1080-80.jpg.webp 1280w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-768-80.jpg.webp 768w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-415-80.jpg.webp 415w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-360-80.jpg.webp 360w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-320-80.jpg.webp 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)">
<img src="https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ.jpg" alt="David Lynch house in LA" srcset="https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-1080-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-1080-80.jpg 1600w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-1080-80.jpg 1280w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-768-80.jpg 768w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-415-80.jpg 415w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-360-80.jpg 360w, https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ-320-80.jpg 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/w2AFf2sL6TxLv6s5FJv3QZ.jpg">
</picture></p></div><p>(Image credit: @hellomarcsilver)</p></figure><p id="f022e7a7-f8e0-4648-975a-b2c9506a2a70">At the heart of the compound lies the architectural crescendo – the approximately 2,000 square feet home where light pours through generous windows and skylights to rake across organic textures and bold geometries. The facade’s cement chevrons catch the sun; inside, simple metalwork and natural woods are drenched in material honesty that often surfaced in Lynch’s films.</p><figure data-bordeaux-image-check="" id="ae17eeca-0a53-42f0-a89b-3a26b1ad4aee"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-1080-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-1080-80.jpg.webp 1600w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-1080-80.jpg.webp 1280w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-768-80.jpg.webp 768w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-415-80.jpg.webp 415w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-360-80.jpg.webp 360w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-320-80.jpg.webp 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)">
<img src="https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ.jpg" alt="David Lynch house in LA" srcset="https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-1080-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-1080-80.jpg 1600w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-1080-80.jpg 1280w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-768-80.jpg 768w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-415-80.jpg 415w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-360-80.jpg 360w, https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ-320-80.jpg 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/5kSfqfCHxxBSdi3KvCttPZ.jpg">
</picture></p></div><p>(Image credit: @hellomarcsilver)</p></figure><p id="8683a440-de58-4525-8d44-ffdcdd0ec56d">Two neighbouring addresses deepen the lore: 7029 Senalda served as the home of Asymmetrical Productions, while 7035 Senalda attained near-mythic status as both the Madison residence in the movie Lost Highway and Lynch’s own studio, complete with a library, screening room and editing suite – spaces where he refined major works, including Mulholland Drive.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-Jbf5ZvkntpGPfedcdaTi4Q"><section><p>Receive our daily digest of inspiration, escapism and design stories from around the world direct to your inbox.</p></section></div><figure data-bordeaux-image-check="" id="9f4bf3dd-9e96-457c-9728-e1fafe81c849"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-640-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-640-80.jpg.webp 1600w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-640-80.jpg.webp 1280w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-640-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-640-80.jpg.webp 768w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-415-80.jpg.webp 415w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-360-80.jpg.webp 360w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-320-80.jpg.webp 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)">
<img src="https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh.jpg" alt="David Lynch house in LA" srcset="https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-640-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-640-80.jpg 1600w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-640-80.jpg 1280w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-640-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-640-80.jpg 768w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-415-80.jpg 415w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-360-80.jpg 360w, https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh-320-80.jpg 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/kVwxPkg4ysUpwi5wFipHUh.jpg">
</picture></p></div><p>(Image credit: @barrysloanestyle)</p></figure><figure data-bordeaux-image-check="" id="4ad4f7e6-6a68-4b60-b1c5-631efabd972f"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-640-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-640-80.jpg.webp 1600w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-640-80.jpg.webp 1280w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-640-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-640-80.jpg.webp 768w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-415-80.jpg.webp 415w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-360-80.jpg.webp 360w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-320-80.jpg.webp 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)">
<img src="https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh.jpg" alt="David Lynch house in LA" srcset="https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-640-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-640-80.jpg 1600w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-640-80.jpg 1280w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-640-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-640-80.jpg 768w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-415-80.jpg 415w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-360-80.jpg 360w, https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh-320-80.jpg 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/gJmmjEKsTxBxrRsUSMcKUh.jpg">
</picture></p></div><p>(Image credit: @barrysloanestyle)</p></figure><p id="2a496d32-f12d-4fdc-8284-bb505506a735">Beyond the exemplary structures, Lynch left a personal handprint, collaborating on additional buildings: a sculptural two-storey guest house and a one-bedroom retreat finished in his favoured smooth grey plaster. Outdoors the terraces, courtyards and planted walkways offer a counterpoint to the intensity of production and everyday life.</p><figure data-bordeaux-image-check="" id="c8904453-ef00-402c-9cf2-4bfeb2affd00"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-640-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-640-80.jpg.webp 1600w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-640-80.jpg.webp 1280w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-640-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-640-80.jpg.webp 768w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-415-80.jpg.webp 415w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-360-80.jpg.webp 360w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-320-80.jpg.webp 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)">
<img src="https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh.jpg" alt="David Lynch house in LA" srcset="https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-640-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-640-80.jpg 1600w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-640-80.jpg 1280w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-640-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-640-80.jpg 768w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-415-80.jpg 415w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-360-80.jpg 360w, https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh-320-80.jpg 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/c9upqcD7Een7evvLSwAEUh.jpg">
</picture></p></div><p>(Image credit: @barrysloanestyle)</p></figure><figure data-bordeaux-image-check="" id="55d32573-66d9-4857-913c-60f9185dcbd6"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-640-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-640-80.jpg.webp 1600w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-640-80.jpg.webp 1280w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-640-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-640-80.jpg.webp 768w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-415-80.jpg.webp 415w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-360-80.jpg.webp 360w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-320-80.jpg.webp 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)">
<img src="https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh.jpg" alt="David Lynch house in LA" srcset="https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-640-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-640-80.jpg 1600w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-640-80.jpg 1280w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-640-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-640-80.jpg 768w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-415-80.jpg 415w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-360-80.jpg 360w, https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh-320-80.jpg 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/oxBq92T3bxFXv79eBWhMUh.jpg">
</picture></p></div><p>(Image credit: @barrysloanestyle)</p></figure><p id="32ad14fb-71e8-49a4-a2e6-b26b1209c6e5">As a listing note from The Agency suggests, this is a 'creative sanctuary and architectural landmark,' with provenance unlike any other in <a data-analytics-id="inline-link" href="https://www.wallpaper.com/tag/los-angeles" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.wallpaper.com/tag/los-angeles">Los Angeles</a>. For admirers of Lynch, it reads as both home and archive: a lived-in factory of ideas, meticulously composed and, at last, ready for its next act.</p><figure data-bordeaux-image-check="" id="a0e83742-6e9c-4b8a-ad5a-27ce3a13c1c6"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-1080-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-1080-80.jpg.webp 1600w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-1080-80.jpg.webp 1280w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-768-80.jpg.webp 768w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-415-80.jpg.webp 415w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-360-80.jpg.webp 360w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-320-80.jpg.webp 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)">
<img src="https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh.jpg" alt="David Lynch house in LA" srcset="https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-1080-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-1080-80.jpg 1600w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-1080-80.jpg 1280w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-768-80.jpg 768w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-415-80.jpg 415w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-360-80.jpg 360w, https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh-320-80.jpg 320w" sizes="(min-width: 710px) 670px, calc(100vw - 30px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/zEXqpshRP4DUtkUuvyoLUh.jpg">
</picture></p></div><p>(Image credit: @barrysloanestyle)</p></figure>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Llama-Factory: Unified, Efficient Fine-Tuning for 100 Open LLMs (111 pts)]]></title>
            <link>https://github.com/hiyouga/LLaMA-Factory</link>
            <guid>45296403</guid>
            <pubDate>Thu, 18 Sep 2025 23:48:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/hiyouga/LLaMA-Factory">https://github.com/hiyouga/LLaMA-Factory</a>, See on <a href="https://news.ycombinator.com/item?id=45296403">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/hiyouga/LLaMA-Factory/blob/main/assets/logo.png"><img src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/logo.png" alt="# LLaMA Factory"></a></p>
<p dir="auto"><a href="https://github.com/hiyouga/LLaMA-Factory/stargazers"><img src="https://camo.githubusercontent.com/a866b1b1a70946801d92c53cbed8f7a3ab3faff7db341e697eb4a51d952381a0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6869796f7567612f4c4c614d412d466163746f72793f7374796c653d736f6369616c" alt="GitHub Repo stars" data-canonical-src="https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social"></a>
<a href="https://github.com/hiyouga/LLaMA-Factory/commits/main"><img src="https://camo.githubusercontent.com/ff3d65656a8ef3f86e5ac227da90da84104735d095711bdf886d3ac97f933ccd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f6869796f7567612f4c4c614d412d466163746f7279" alt="GitHub last commit" data-canonical-src="https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory"></a>
<a href="https://github.com/hiyouga/LLaMA-Factory/graphs/contributors"><img src="https://camo.githubusercontent.com/2c9600a703038a041088635b552a0e70e5ff0438cd5fcad1aa0b2a38470d8ef5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6869796f7567612f4c4c614d412d466163746f72793f636f6c6f723d6f72616e6765" alt="GitHub contributors" data-canonical-src="https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange"></a>
<a href="https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml"><img src="https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg" alt="GitHub workflow"></a>
<a href="https://pypi.org/project/llamafactory/" rel="nofollow"><img src="https://camo.githubusercontent.com/816aba4a24ac2e71b3f5a5cbd3c0ecfd9403ce1ece26ee0c631cf4613b1c4697/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c6c616d61666163746f7279" alt="PyPI" data-canonical-src="https://img.shields.io/pypi/v/llamafactory"></a>
<a href="https://scholar.google.com/scholar?cites=12620864006390196564" rel="nofollow"><img src="https://camo.githubusercontent.com/a5cc965e2b198801cb90ccf04bc9a224c5607561b28911fd8515d2b0560699f3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6369746174696f6e2d3834302d677265656e" alt="Citation" data-canonical-src="https://img.shields.io/badge/citation-840-green"></a>
<a href="https://hub.docker.com/r/hiyouga/llamafactory/tags" rel="nofollow"><img src="https://camo.githubusercontent.com/69abb5bb195ec014b38ca025e9b603971b727f20af6ac5a3927e30e90fb7e141/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6869796f7567612f6c6c616d61666163746f7279" alt="Docker Pulls" data-canonical-src="https://img.shields.io/docker/pulls/hiyouga/llamafactory"></a></p>
<p dir="auto"><a href="https://twitter.com/llamafactory_ai" rel="nofollow"><img src="https://camo.githubusercontent.com/b50fda1cfeea7651440816260e5f5c190e06d54a141f4bd34a94bfdcc866ffdd/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6c6c616d61666163746f72795f6169" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/follow/llamafactory_ai"></a>
<a href="https://discord.gg/rKfvV9r9FK" rel="nofollow"><img src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/thirdparty/discord.svg" alt="Discord"></a></p>
<p dir="auto"><a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing" rel="nofollow"><img src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/thirdparty/colab.svg" alt="Open in Colab"></a>
<a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory" rel="nofollow"><img src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/thirdparty/dsw.svg" alt="Open in DSW"></a>
<a href="https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory" rel="nofollow"><img src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/thirdparty/lab4ai.svg" alt="Open in Lab4ai"></a>
<a href="https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory" rel="nofollow"><img src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/thirdparty/online.svg" alt="Open in Online"></a>
<a href="https://huggingface.co/spaces/hiyouga/LLaMA-Board" rel="nofollow"><img src="https://camo.githubusercontent.com/f302d00c36a87c4bac8741c397b6a55f1017c961cec10b0990879eda5e15a3f4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372d4f70656e253230696e2532305370616365732d626c7565" alt="Open in Spaces" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue"></a>
<a href="https://modelscope.cn/studios/hiyouga/LLaMA-Board" rel="nofollow"><img src="https://camo.githubusercontent.com/a93364cb869924b559dbdeedb28d66ba5fe6b70b4467aca043dd03a26a953fbe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6f64656c53636f70652d4f70656e253230696e25323053747564696f732d626c7565" alt="Open in Studios" data-canonical-src="https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue"></a>
<a href="https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47" rel="nofollow"><img src="https://camo.githubusercontent.com/5e329b98c5848f2215e3e24ec4f581a22c28b129faaccbebb24c4ee8ed15ec92/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e6f766974612d4465706c6f7925323054656d706c6174652d626c7565" alt="Open in Novita" data-canonical-src="https://img.shields.io/badge/Novita-Deploy%20Template-blue"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Used by <a href="https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/" rel="nofollow">Amazon</a>, <a href="https://developer.nvidia.com/rtx/ai-toolkit" rel="nofollow">NVIDIA</a>, <a href="https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory" rel="nofollow">Aliyun</a>, etc.</h3><a id="user-content-used-by-amazon-nvidia-aliyun-etc" aria-label="Permalink: Used by Amazon, NVIDIA, Aliyun, etc." href="#used-by-amazon-nvidia-aliyun-etc"></a></p>

<p dir="auto">👋 Join our <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/assets/wechat/main.jpg">WeChat</a>, <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/assets/wechat/npu.jpg">NPU</a>, <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/assets/wechat/lab4ai.jpg">Lab4AI</a>, <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/assets/wechat/online.jpg">LLaMA Factory Online</a> user group.</p>
<p dir="auto">[ English | <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md">中文</a> ]</p>
<p dir="auto"><strong>Fine-tuning a large language model can be easy as...</strong></p>
<details open="">
  <summary>
    
    <span>train_en.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/16256802/423362179-3991a3a8-4276-4d30-9cab-4cb0c4b9b99e.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgyOTI1MDEsIm5iZiI6MTc1ODI5MjIwMSwicGF0aCI6Ii8xNjI1NjgwMi80MjMzNjIxNzktMzk5MWEzYTgtNDI3Ni00ZDMwLTljYWItNGNiMGM0YjliOTllLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE5VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTcwYTNjMWFiNzIyMmVhYTNkY2UzZmUxNTNjYjRmM2M1MzJmYmFjMTEyMDRjZDY1MTY0OTZhODA3ODE4ODNkMjEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.EM-bVUh2bbP1eZF9SgJZUpiM5Aqr_0Nr99wb6RU2QLc" data-canonical-src="https://private-user-images.githubusercontent.com/16256802/423362179-3991a3a8-4276-4d30-9cab-4cb0c4b9b99e.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgyOTI1MDEsIm5iZiI6MTc1ODI5MjIwMSwicGF0aCI6Ii8xNjI1NjgwMi80MjMzNjIxNzktMzk5MWEzYTgtNDI3Ni00ZDMwLTljYWItNGNiMGM0YjliOTllLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE5VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTcwYTNjMWFiNzIyMmVhYTNkY2UzZmUxNTNjYjRmM2M1MzJmYmFjMTEyMDRjZDY1MTY0OTZhODA3ODE4ODNkMjEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.EM-bVUh2bbP1eZF9SgJZUpiM5Aqr_0Nr99wb6RU2QLc" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Choose your path:</p>
<ul dir="auto">
<li><strong>Documentation (WIP)</strong>: <a href="https://llamafactory.readthedocs.io/en/latest/" rel="nofollow">https://llamafactory.readthedocs.io/en/latest/</a></li>
<li><strong>Documentation (AMD GPU)</strong>: <a href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html" rel="nofollow">https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html</a></li>
<li><strong>Colab (free)</strong>: <a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing" rel="nofollow">https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing</a></li>
<li><strong>Local machine</strong>: Please refer to <a href="#getting-started">usage</a></li>
<li><strong>PAI-DSW (free trial)</strong>: <a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory" rel="nofollow">https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory</a></li>
<li><strong>Alaya NeW (cloud GPU deal)</strong>: <a href="https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory" rel="nofollow">https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory</a></li>
<li><strong>Official Course</strong>: <a href="https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory" rel="nofollow">https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory</a></li>
<li><strong>LLaMA Factory Online</strong>: <a href="https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory" rel="nofollow">https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory</a></li>
</ul>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#features">Features</a></li>
<li><a href="#blogs">Blogs</a></li>
<li><a href="#changelog">Changelog</a></li>
<li><a href="#supported-models">Supported Models</a></li>
<li><a href="#supported-training-approaches">Supported Training Approaches</a></li>
<li><a href="#provided-datasets">Provided Datasets</a></li>
<li><a href="#requirement">Requirement</a></li>
<li><a href="#getting-started">Getting Started</a>
<ul dir="auto">
<li><a href="#installation">Installation</a></li>
<li><a href="#data-preparation">Data Preparation</a></li>
<li><a href="#quickstart">Quickstart</a></li>
<li><a href="#fine-tuning-with-llama-board-gui-powered-by-gradio">Fine-Tuning with LLaMA Board GUI</a></li>
<li><a href="#llama-factory-online">LLaMA Factory Online</a></li>
<li><a href="#build-docker">Build Docker</a></li>
<li><a href="#deploy-with-openai-style-api-and-vllm">Deploy with OpenAI-style API and vLLM</a></li>
<li><a href="#download-from-modelscope-hub">Download from ModelScope Hub</a></li>
<li><a href="#download-from-modelers-hub">Download from Modelers Hub</a></li>
<li><a href="#use-wb-logger">Use W&amp;B Logger</a></li>
<li><a href="#use-swanlab-logger">Use SwanLab Logger</a></li>
</ul>
</li>
<li><a href="#projects-using-llama-factory">Projects using LLaMA Factory</a></li>
<li><a href="#license">License</a></li>
<li><a href="#citation">Citation</a></li>
<li><a href="#acknowledgement">Acknowledgement</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>Various models</strong>: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.</li>
<li><strong>Integrated methods</strong>: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.</li>
<li><strong>Scalable resources</strong>: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.</li>
<li><strong>Advanced algorithms</strong>: <a href="https://github.com/jiaweizzhao/GaLore">GaLore</a>, <a href="https://github.com/Ledzy/BAdam">BAdam</a>, <a href="https://github.com/zhuhanqing/APOLLO">APOLLO</a>, <a href="https://github.com/zyushun/Adam-mini">Adam-mini</a>, <a href="https://github.com/KellerJordan/Muon">Muon</a>, <a href="https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft">OFT</a>, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.</li>
<li><strong>Practical tricks</strong>: <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention-2</a>, <a href="https://github.com/unslothai/unsloth">Unsloth</a>, <a href="https://github.com/linkedin/Liger-Kernel">Liger Kernel</a>, RoPE scaling, NEFTune and rsLoRA.</li>
<li><strong>Wide tasks</strong>: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.</li>
<li><strong>Experiment monitors</strong>: LlamaBoard, TensorBoard, Wandb, MLflow, <a href="https://github.com/SwanHubX/SwanLab">SwanLab</a>, etc.</li>
<li><strong>Faster inference</strong>: OpenAI-style API, Gradio UI and CLI with <a href="https://github.com/vllm-project/vllm">vLLM worker</a> or <a href="https://github.com/sgl-project/sglang">SGLang worker</a>.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Day-N Support for Fine-Tuning Cutting-Edge Models</h3><a id="user-content-day-n-support-for-fine-tuning-cutting-edge-models" aria-label="Permalink: Day-N Support for Fine-Tuning Cutting-Edge Models" href="#day-n-support-for-fine-tuning-cutting-edge-models"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Support Date</th>
<th>Model Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>Day 0</td>
<td>Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6</td>
</tr>
<tr>
<td>Day 1</td>
<td>Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Blogs</h2><a id="user-content-blogs" aria-label="Permalink: Blogs" href="#blogs"></a></p>
<ul dir="auto">
<li>💡 <a href="https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g" rel="nofollow">Easy Dataset × LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge</a> (English)</li>
<li><a href="https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&amp;type=project&amp;utm_source=LLaMA-Factory" rel="nofollow">Fine-tune a mental health LLM using LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory" rel="nofollow">Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/" rel="nofollow">A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1</a> (Chinese)</li>
<li><a href="https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/" rel="nofollow">How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod</a> (English)</li>
</ul>
<details><summary>All Blogs</summary>
<ul dir="auto">
<li><a href="https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory" rel="nofollow">Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory" rel="nofollow">Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b" rel="nofollow">LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier</a> (Chinese)</li>
<li><a href="https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/" rel="nofollow">A One-Stop Code-Free Model Fine-Tuning &amp; Deployment Platform based on SageMaker and LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl" rel="nofollow">LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide</a> (Chinese)</li>
<li><a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory" rel="nofollow">LLaMA Factory: Fine-tuning Llama3 for Role-Playing</a> (Chinese)</li>
</ul>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Changelog</h2><a id="user-content-changelog" aria-label="Permalink: Changelog" href="#changelog"></a></p>
<p dir="auto">[25/08/22] We supported <strong><a href="https://arxiv.org/abs/2306.07280" rel="nofollow">OFT</a></strong> and <strong><a href="https://arxiv.org/abs/2506.19847" rel="nofollow">OFTv2</a></strong>. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[25/08/20] We supported fine-tuning the <strong><a href="https://huggingface.co/internlm/Intern-S1-mini" rel="nofollow">Intern-S1-mini</a></strong> models. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/8976" data-hovercard-type="pull_request" data-hovercard-url="/hiyouga/LLaMA-Factory/pull/8976/hovercard">PR #8976</a> to get started.</p>
<p dir="auto">[25/08/06] We supported fine-tuning the <strong><a href="https://github.com/openai/gpt-oss">GPT-OSS</a></strong> models. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/8826" data-hovercard-type="pull_request" data-hovercard-url="/hiyouga/LLaMA-Factory/pull/8826/hovercard">PR #8826</a> to get started.</p>
<details><summary>Full Changelog</summary>
<p dir="auto">[25/07/02] We supported fine-tuning the <strong><a href="https://github.com/THUDM/GLM-4.1V-Thinking">GLM-4.1V-9B-Thinking</a></strong> model.</p>
<p dir="auto">[25/04/28] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen3/" rel="nofollow">Qwen3</a></strong> model family.</p>
<p dir="auto">[25/04/21] We supported the <strong><a href="https://github.com/KellerJordan/Muon">Muon</a></strong> optimizer. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage. Thank <a href="https://github.com/tianshijing">@tianshijing</a>'s PR.</p>
<p dir="auto">[25/04/16] We supported fine-tuning the <strong><a href="https://huggingface.co/OpenGVLab/InternVL3-8B" rel="nofollow">InternVL3</a></strong> model. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/7258" data-hovercard-type="pull_request" data-hovercard-url="/hiyouga/LLaMA-Factory/pull/7258/hovercard">PR #7258</a> to get started.</p>
<p dir="auto">[25/04/14] We supported fine-tuning the <strong><a href="https://huggingface.co/THUDM/GLM-Z1-9B-0414" rel="nofollow">GLM-Z1</a></strong> and <strong><a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct" rel="nofollow">Kimi-VL</a></strong> models.</p>
<p dir="auto">[25/04/06] We supported fine-tuning the <strong><a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" rel="nofollow">Llama 4</a></strong> model. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/7611" data-hovercard-type="pull_request" data-hovercard-url="/hiyouga/LLaMA-Factory/pull/7611/hovercard">PR #7611</a> to get started.</p>
<p dir="auto">[25/03/31] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen2.5-omni/" rel="nofollow">Qwen2.5 Omni</a></strong> model. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/7537" data-hovercard-type="pull_request" data-hovercard-url="/hiyouga/LLaMA-Factory/pull/7537/hovercard">PR #7537</a> to get started.</p>
<p dir="auto">[25/03/15] We supported <strong><a href="https://github.com/sgl-project/sglang">SGLang</a></strong> as inference backend. Try <code>infer_backend: sglang</code> to accelerate inference.</p>
<p dir="auto">[25/03/12] We supported fine-tuning the <strong><a href="https://huggingface.co/blog/gemma3" rel="nofollow">Gemma 3</a></strong> model.</p>
<p dir="auto">[25/02/24] Announcing <strong><a href="https://github.com/hiyouga/EasyR1">EasyR1</a></strong>, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.</p>
<p dir="auto">[25/02/11] We supported saving the <strong><a href="https://github.com/ollama/ollama">Ollama</a></strong> modelfile when exporting the model checkpoints. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[25/02/05] We supported fine-tuning the <strong><a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/Qwen/Qwen2-Audio-7B-Instruct">Qwen2-Audio</a></strong> and <strong><a href="https://huggingface.co/openbmb/MiniCPM-o-2_6" rel="nofollow">MiniCPM-o-2.6</a></strong> on audio understanding tasks.</p>
<p dir="auto">[25/01/31] We supported fine-tuning the <strong><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1" rel="nofollow">DeepSeek-R1</a></strong> and <strong><a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct" rel="nofollow">Qwen2.5-VL</a></strong> models.</p>
<p dir="auto">[25/01/15] We supported <strong><a href="https://arxiv.org/abs/2412.05270" rel="nofollow">APOLLO</a></strong> optimizer. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[25/01/14] We supported fine-tuning the <strong><a href="https://huggingface.co/openbmb/MiniCPM-o-2_6" rel="nofollow">MiniCPM-o-2.6</a></strong> and <strong><a href="https://huggingface.co/openbmb/MiniCPM-V-2_6" rel="nofollow">MiniCPM-V-2.6</a></strong> models. Thank <a href="https://github.com/BUAADreamer">@BUAADreamer</a>'s PR.</p>
<p dir="auto">[25/01/14] We supported fine-tuning the <strong><a href="https://huggingface.co/collections/internlm/" rel="nofollow">InternLM 3</a></strong> models. Thank <a href="https://github.com/hhaAndroid">@hhaAndroid</a>'s PR.</p>
<p dir="auto">[25/01/10] We supported fine-tuning the <strong><a href="https://huggingface.co/microsoft/phi-4" rel="nofollow">Phi-4</a></strong> model.</p>
<p dir="auto">[24/12/21] We supported using <strong><a href="https://github.com/SwanHubX/SwanLab">SwanLab</a></strong> for experiment tracking and visualization. See <a href="#use-swanlab-logger">this section</a> for details.</p>
<p dir="auto">[24/11/27] We supported fine-tuning the <strong><a href="https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B" rel="nofollow">Skywork-o1</a></strong> model and the <strong><a href="https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT" rel="nofollow">OpenO1</a></strong> dataset.</p>
<p dir="auto">[24/10/09] We supported downloading pre-trained models and datasets from the <strong><a href="https://modelers.cn/models" rel="nofollow">Modelers Hub</a></strong>. See <a href="#download-from-modelers-hub">this tutorial</a> for usage.</p>
<p dir="auto">[24/09/19] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen2.5/" rel="nofollow">Qwen2.5</a></strong> models.</p>
<p dir="auto">[24/08/30] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen2-vl/" rel="nofollow">Qwen2-VL</a></strong> models. Thank <a href="https://github.com/simonJJJ">@simonJJJ</a>'s PR.</p>
<p dir="auto">[24/08/27] We supported <strong><a href="https://github.com/linkedin/Liger-Kernel">Liger Kernel</a></strong>. Try <code>enable_liger_kernel: true</code> for efficient training.</p>
<p dir="auto">[24/08/09] We supported <strong><a href="https://github.com/zyushun/Adam-mini">Adam-mini</a></strong> optimizer. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage. Thank <a href="https://github.com/relic-yuexi">@relic-yuexi</a>'s PR.</p>
<p dir="auto">[24/07/04] We supported <a href="https://github.com/MeetKai/functionary/tree/main/functionary/train/packing">contamination-free packed training</a>. Use <code>neat_packing: true</code> to activate it. Thank <a href="https://github.com/chuan298">@chuan298</a>'s PR.</p>
<p dir="auto">[24/06/16] We supported <strong><a href="https://arxiv.org/abs/2404.02948" rel="nofollow">PiSSA</a></strong> algorithm. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/06/07] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen2/" rel="nofollow">Qwen2</a></strong> and <strong><a href="https://github.com/THUDM/GLM-4">GLM-4</a></strong> models.</p>
<p dir="auto">[24/05/26] We supported <strong><a href="https://arxiv.org/abs/2405.14734" rel="nofollow">SimPO</a></strong> algorithm for preference learning. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/05/20] We supported fine-tuning the <strong>PaliGemma</strong> series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with <code>paligemma</code> template for chat completion.</p>
<p dir="auto">[24/05/18] We supported <strong><a href="https://arxiv.org/abs/2402.01306" rel="nofollow">KTO</a></strong> algorithm for preference learning. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/05/14] We supported training and inference on the Ascend NPU devices. Check <a href="#installation">installation</a> section for details.</p>
<p dir="auto">[24/04/26] We supported fine-tuning the <strong>LLaVA-1.5</strong> multimodal LLMs. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/04/22] We provided a <strong><a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing" rel="nofollow">Colab notebook</a></strong> for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check <a href="https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat" rel="nofollow">Llama3-8B-Chinese-Chat</a> and <a href="https://huggingface.co/zhichen/Llama3-Chinese" rel="nofollow">Llama3-Chinese</a> for details.</p>
<p dir="auto">[24/04/21] We supported <strong><a href="https://arxiv.org/abs/2404.02258" rel="nofollow">Mixture-of-Depths</a></strong> according to <a href="https://github.com/astramind-ai/Mixture-of-depths">AstraMindAI's implementation</a>. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/04/16] We supported <strong><a href="https://arxiv.org/abs/2404.02827" rel="nofollow">BAdam</a></strong> optimizer. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/04/16] We supported <strong><a href="https://github.com/unslothai/unsloth">unsloth</a></strong>'s long-sequence training (Llama-2-7B-56k within 24GB). It achieves <strong>117%</strong> speed and <strong>50%</strong> memory compared with FlashAttention-2, more benchmarks can be found in <a href="https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison">this page</a>.</p>
<p dir="auto">[24/03/31] We supported <strong><a href="https://arxiv.org/abs/2403.07691" rel="nofollow">ORPO</a></strong>. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/03/21] Our paper "<a href="https://arxiv.org/abs/2403.13372" rel="nofollow">LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</a>" is available at arXiv!</p>
<p dir="auto">[24/03/20] We supported <strong>FSDP+QLoRA</strong> that fine-tunes a 70B model on 2x24GB GPUs. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/03/13] We supported <strong><a href="https://arxiv.org/abs/2402.12354" rel="nofollow">LoRA+</a></strong>. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/03/07] We supported <strong><a href="https://arxiv.org/abs/2403.03507" rel="nofollow">GaLore</a></strong> optimizer. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/03/07] We integrated <strong><a href="https://github.com/vllm-project/vllm">vLLM</a></strong> for faster and concurrent inference. Try <code>infer_backend: vllm</code> to enjoy <strong>270%</strong> inference speed.</p>
<p dir="auto">[24/02/28] We supported weight-decomposed LoRA (<strong><a href="https://arxiv.org/abs/2402.09353" rel="nofollow">DoRA</a></strong>). Try <code>use_dora: true</code> to activate DoRA training.</p>
<p dir="auto">[24/02/15] We supported <strong>block expansion</strong> proposed by <a href="https://github.com/TencentARC/LLaMA-Pro">LLaMA Pro</a>. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this <a href="https://qwenlm.github.io/blog/qwen1.5/" rel="nofollow">blog post</a> for details.</p>
<p dir="auto">[24/01/18] We supported <strong>agent tuning</strong> for most models, equipping model with tool using abilities by fine-tuning with <code>dataset: glaive_toolcall_en</code>.</p>
<p dir="auto">[23/12/23] We supported <strong><a href="https://github.com/unslothai/unsloth">unsloth</a></strong>'s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try <code>use_unsloth: true</code> argument to activate unsloth patch. It achieves <strong>170%</strong> speed in our benchmark, check <a href="https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison">this page</a> for details.</p>
<p dir="auto">[23/12/12] We supported fine-tuning the latest MoE model <strong><a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1" rel="nofollow">Mixtral 8x7B</a></strong> in our framework. See hardware requirement <a href="#hardware-requirement">here</a>.</p>
<p dir="auto">[23/12/01] We supported downloading pre-trained models and datasets from the <strong><a href="https://modelscope.cn/models" rel="nofollow">ModelScope Hub</a></strong>. See <a href="#download-from-modelscope-hub">this tutorial</a> for usage.</p>
<p dir="auto">[23/10/21] We supported <strong><a href="https://arxiv.org/abs/2310.05914" rel="nofollow">NEFTune</a></strong> trick for fine-tuning. Try <code>neftune_noise_alpha: 5</code> argument to activate NEFTune.</p>
<p dir="auto">[23/09/27] We supported <strong><math-renderer data-run-id="ef243185fa4635c93933d839aba74530">$S^2$</math-renderer>-Attn</strong> proposed by <a href="https://github.com/dvlab-research/LongLoRA">LongLoRA</a> for the LLaMA models. Try <code>shift_attn: true</code> argument to enable shift short attention.</p>
<p dir="auto">[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[23/09/10] We supported <strong><a href="https://github.com/Dao-AILab/flash-attention">FlashAttention-2</a></strong>. Try <code>flash_attn: fa2</code> argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.</p>
<p dir="auto">[23/08/12] We supported <strong>RoPE scaling</strong> to extend the context length of the LLaMA models. Try <code>rope_scaling: linear</code> argument in training and <code>rope_scaling: dynamic</code> argument at inference to extrapolate the position embeddings.</p>
<p dir="auto">[23/08/11] We supported <strong><a href="https://arxiv.org/abs/2305.18290" rel="nofollow">DPO training</a></strong> for instruction-tuned models. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[23/07/31] We supported <strong>dataset streaming</strong>. Try <code>streaming: true</code> and <code>max_steps: 10000</code> arguments to load your dataset in streaming mode.</p>
<p dir="auto">[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (<a href="https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat" rel="nofollow">LLaMA-2</a> / <a href="https://huggingface.co/hiyouga/Baichuan-13B-sft" rel="nofollow">Baichuan</a>) for details.</p>
<p dir="auto">[23/07/18] We developed an <strong>all-in-one Web UI</strong> for training, evaluation and inference. Try <code>train_web.py</code> to fine-tune models in your Web browser. Thank <a href="https://github.com/KanadeSiina">@KanadeSiina</a> and <a href="https://github.com/codemayq">@codemayq</a> for their efforts in the development.</p>
<p dir="auto">[23/07/09] We released <strong><a href="https://github.com/hiyouga/FastEdit">FastEdit</a></strong> ⚡🩹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow <a href="https://github.com/hiyouga/FastEdit">FastEdit</a> if you are interested.</p>
<p dir="auto">[23/06/29] We provided a <strong>reproducible example</strong> of training a chat model using instruction-following datasets, see <a href="https://huggingface.co/hiyouga/Baichuan-7B-sft" rel="nofollow">Baichuan-7B-sft</a> for details.</p>
<p dir="auto">[23/06/22] We aligned the <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/src/api_demo.py">demo API</a> with the <a href="https://platform.openai.com/docs/api-reference/chat" rel="nofollow">OpenAI's</a> format where you can insert the fine-tuned model in <strong>arbitrary ChatGPT-based applications</strong>.</p>
<p dir="auto">[23/06/03] We supported quantized training and inference (aka <strong><a href="https://github.com/artidoro/qlora">QLoRA</a></strong>). See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
</details>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Models</h2><a id="user-content-supported-models" aria-label="Permalink: Supported Models" href="#supported-models"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Model size</th>
<th>Template</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/baichuan-inc" rel="nofollow">Baichuan 2</a></td>
<td>7B/13B</td>
<td>baichuan2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/bigscience" rel="nofollow">BLOOM/BLOOMZ</a></td>
<td>560M/1.1B/1.7B/3B/7.1B/176B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/THUDM" rel="nofollow">ChatGLM3</a></td>
<td>6B</td>
<td>chatglm3</td>
</tr>
<tr>
<td><a href="https://huggingface.co/CohereForAI" rel="nofollow">Command R</a></td>
<td>35B/104B</td>
<td>cohere</td>
</tr>
<tr>
<td><a href="https://huggingface.co/deepseek-ai" rel="nofollow">DeepSeek (Code/MoE)</a></td>
<td>7B/16B/67B/236B</td>
<td>deepseek</td>
</tr>
<tr>
<td><a href="https://huggingface.co/deepseek-ai" rel="nofollow">DeepSeek 2.5/3</a></td>
<td>236B/671B</td>
<td>deepseek3</td>
</tr>
<tr>
<td><a href="https://huggingface.co/deepseek-ai" rel="nofollow">DeepSeek R1 (Distill)</a></td>
<td>1.5B/7B/8B/14B/32B/70B/671B</td>
<td>deepseekr1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tiiuae" rel="nofollow">Falcon</a></td>
<td>7B/11B/40B/180B</td>
<td>falcon</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tiiuae" rel="nofollow">Falcon-H1</a></td>
<td>0.5B/1.5B/3B/7B/34B</td>
<td>falcon_h1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/google" rel="nofollow">Gemma/Gemma 2/CodeGemma</a></td>
<td>2B/7B/9B/27B</td>
<td>gemma/gemma2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/google" rel="nofollow">Gemma 3/Gemma 3n</a></td>
<td>270M/1B/4B/6B/8B/12B/27B</td>
<td>gemma3/gemma3n</td>
</tr>
<tr>
<td><a href="https://huggingface.co/zai-org" rel="nofollow">GLM-4/GLM-4-0414/GLM-Z1</a></td>
<td>9B/32B</td>
<td>glm4/glmz1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/zai-org" rel="nofollow">GLM-4.1V</a></td>
<td>9B</td>
<td>glm4v</td>
</tr>
<tr>
<td><a href="https://huggingface.co/zai-org" rel="nofollow">GLM-4.5/GLM-4.5V</a></td>
<td>106B/355B</td>
<td>glm4_moe/glm4v_moe</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openai-community" rel="nofollow">GPT-2</a></td>
<td>0.1B/0.4B/0.8B/1.5B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openai" rel="nofollow">GPT-OSS</a></td>
<td>20B/120B</td>
<td>gpt</td>
</tr>
<tr>
<td><a href="https://huggingface.co/ibm-granite" rel="nofollow">Granite 3.0-3.3</a></td>
<td>1B/2B/3B/8B</td>
<td>granite3</td>
</tr>
<tr>
<td><a href="https://huggingface.co/ibm-granite" rel="nofollow">Granite 4</a></td>
<td>7B</td>
<td>granite4</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tencent/" rel="nofollow">Hunyuan</a></td>
<td>7B</td>
<td>hunyuan</td>
</tr>
<tr>
<td><a href="https://huggingface.co/IndexTeam" rel="nofollow">Index</a></td>
<td>1.9B</td>
<td>index</td>
</tr>
<tr>
<td><a href="https://huggingface.co/internlm" rel="nofollow">InternLM 2-3</a></td>
<td>7B/8B/20B</td>
<td>intern2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/OpenGVLab" rel="nofollow">InternVL 2.5-3.5</a></td>
<td>1B/2B/4B/8B/14B/30B/38B/78B/241B</td>
<td>intern_vl</td>
</tr>
<tr>
<td><a href="https://huggingface.co/internlm/" rel="nofollow">InternLM/Intern-S1-mini</a></td>
<td>8B</td>
<td>intern_s1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/moonshotai" rel="nofollow">Kimi-VL</a></td>
<td>16B</td>
<td>kimi_vl</td>
</tr>
<tr>
<td><a href="https://github.com/facebookresearch/llama">Llama</a></td>
<td>7B/13B/33B/65B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama" rel="nofollow">Llama 2</a></td>
<td>7B/13B/70B</td>
<td>llama2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama" rel="nofollow">Llama 3-3.3</a></td>
<td>1B/3B/8B/70B</td>
<td>llama3</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama" rel="nofollow">Llama 4</a></td>
<td>109B/402B</td>
<td>llama4</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama" rel="nofollow">Llama 3.2 Vision</a></td>
<td>11B/90B</td>
<td>mllama</td>
</tr>
<tr>
<td><a href="https://huggingface.co/llava-hf" rel="nofollow">LLaVA-1.5</a></td>
<td>7B/13B</td>
<td>llava</td>
</tr>
<tr>
<td><a href="https://huggingface.co/llava-hf" rel="nofollow">LLaVA-NeXT</a></td>
<td>7B/8B/13B/34B/72B/110B</td>
<td>llava_next</td>
</tr>
<tr>
<td><a href="https://huggingface.co/llava-hf" rel="nofollow">LLaVA-NeXT-Video</a></td>
<td>7B/34B</td>
<td>llava_next_video</td>
</tr>
<tr>
<td><a href="https://huggingface.co/XiaomiMiMo" rel="nofollow">MiMo</a></td>
<td>7B</td>
<td>mimo</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openbmb" rel="nofollow">MiniCPM 1-4.1</a></td>
<td>0.5B/1B/2B/4B/8B</td>
<td>cpm/cpm3/cpm4</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openbmb" rel="nofollow">MiniCPM-o-2.6/MiniCPM-V-2.6</a></td>
<td>8B</td>
<td>minicpm_o/minicpm_v</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai" rel="nofollow">Ministral/Mistral-Nemo</a></td>
<td>8B/12B</td>
<td>ministral</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai" rel="nofollow">Mistral/Mixtral</a></td>
<td>7B/8x7B/8x22B</td>
<td>mistral</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai" rel="nofollow">Mistral Small</a></td>
<td>24B</td>
<td>mistral_small</td>
</tr>
<tr>
<td><a href="https://huggingface.co/allenai" rel="nofollow">OLMo</a></td>
<td>1B/7B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/google" rel="nofollow">PaliGemma/PaliGemma2</a></td>
<td>3B/10B/28B</td>
<td>paligemma</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft" rel="nofollow">Phi-1.5/Phi-2</a></td>
<td>1.3B/2.7B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft" rel="nofollow">Phi-3/Phi-3.5</a></td>
<td>4B/14B</td>
<td>phi</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft" rel="nofollow">Phi-3-small</a></td>
<td>7B</td>
<td>phi_small</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft" rel="nofollow">Phi-4</a></td>
<td>14B</td>
<td>phi4</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai" rel="nofollow">Pixtral</a></td>
<td>12B</td>
<td>pixtral</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen (1-2.5) (Code/Math/MoE/QwQ)</a></td>
<td>0.5B/1.5B/3B/7B/14B/32B/72B/110B</td>
<td>qwen</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen3 (MoE/Instruct/Thinking/Next)</a></td>
<td>0.6B/1.7B/4B/8B/14B/32B/80B/235B</td>
<td>qwen3/qwen3_nothink</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen2-Audio</a></td>
<td>7B</td>
<td>qwen2_audio</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen2.5-Omni</a></td>
<td>3B/7B</td>
<td>qwen2_omni</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen2-VL/Qwen2.5-VL/QVQ</a></td>
<td>2B/3B/7B/32B/72B</td>
<td>qwen2_vl</td>
</tr>
<tr>
<td><a href="https://huggingface.co/ByteDance-Seed" rel="nofollow">Seed (OSS/Coder)</a></td>
<td>8B/36B</td>
<td>seed_oss/seed_coder</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Skywork" rel="nofollow">Skywork o1</a></td>
<td>8B</td>
<td>skywork_o1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/bigcode" rel="nofollow">StarCoder 2</a></td>
<td>3B/7B/15B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Tele-AI" rel="nofollow">TeleChat2</a></td>
<td>3B/7B/35B/115B</td>
<td>telechat2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/xverse" rel="nofollow">XVERSE</a></td>
<td>7B/13B/65B</td>
<td>xverse</td>
</tr>
<tr>
<td><a href="https://huggingface.co/01-ai" rel="nofollow">Yi/Yi-1.5 (Code)</a></td>
<td>1.5B/6B/9B/34B</td>
<td>yi</td>
</tr>
<tr>
<td><a href="https://huggingface.co/01-ai" rel="nofollow">Yi-VL</a></td>
<td>6B/34B</td>
<td>yi_vl</td>
</tr>
<tr>
<td><a href="https://huggingface.co/IEITYuan" rel="nofollow">Yuan 2</a></td>
<td>2B/51B/102B</td>
<td>yuan</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">For the "base" models, the <code>template</code> argument can be chosen from <code>default</code>, <code>alpaca</code>, <code>vicuna</code> etc. But make sure to use the <strong>corresponding template</strong> for the "instruct/chat" models.</p>
<p dir="auto">Remember to use the <strong>SAME</strong> template in training and inference.</p>
<p dir="auto">*: You should install the <code>transformers</code> from main branch and use <code>DISABLE_VERSION_CHECK=1</code> to skip version check.</p>
<p dir="auto">**: You need to install a specific version of <code>transformers</code> to use the corresponding model.</p>
</div>
<p dir="auto">Please refer to <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/src/llamafactory/extras/constants.py">constants.py</a> for a full list of models we supported.</p>
<p dir="auto">You also can add a custom chat template to <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/src/llamafactory/data/template.py">template.py</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Training Approaches</h2><a id="user-content-supported-training-approaches" aria-label="Permalink: Supported Training Approaches" href="#supported-training-approaches"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Approach</th>
<th>Full-tuning</th>
<th>Freeze-tuning</th>
<th>LoRA</th>
<th>QLoRA</th>
<th>OFT</th>
<th>QOFT</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pre-Training</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Supervised Fine-Tuning</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Reward Modeling</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>PPO Training</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>DPO Training</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>KTO Training</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>ORPO Training</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>SimPO Training</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">The implementation details of PPO can be found in <a href="https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html" rel="nofollow">this blog</a>.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Provided Datasets</h2><a id="user-content-provided-datasets" aria-label="Permalink: Provided Datasets" href="#provided-datasets"></a></p>
<details><summary>Pre-training datasets</summary>
<ul dir="auto">
<li><a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/wiki_demo.txt">Wiki Demo (en)</a></li>
<li><a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb" rel="nofollow">RefinedWeb (en)</a></li>
<li><a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2" rel="nofollow">RedPajama V2 (en)</a></li>
<li><a href="https://huggingface.co/datasets/olm/olm-wikipedia-20221220" rel="nofollow">Wikipedia (en)</a></li>
<li><a href="https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered" rel="nofollow">Wikipedia (zh)</a></li>
<li><a href="https://huggingface.co/datasets/EleutherAI/pile" rel="nofollow">Pile (en)</a></li>
<li><a href="https://huggingface.co/datasets/Skywork/SkyPile-150B" rel="nofollow">SkyPile (zh)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb" rel="nofollow">FineWeb (en)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu" rel="nofollow">FineWeb-Edu (en)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI3-HQ" rel="nofollow">CCI3-HQ (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI3-Data" rel="nofollow">CCI3-Data (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-Base-v1" rel="nofollow">CCI4.0-M2-Base-v1 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-CoT-v1" rel="nofollow">CCI4.0-M2-CoT-v1 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-Extra-v1" rel="nofollow">CCI4.0-M2-Extra-v1 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/bigcode/the-stack" rel="nofollow">The Stack (en)</a></li>
<li><a href="https://huggingface.co/datasets/bigcode/starcoderdata" rel="nofollow">StarCoder (en)</a></li>
</ul>
</details>
<details><summary>Supervised fine-tuning datasets</summary>
<ul dir="auto">
<li><a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/identity.json">Identity (en&amp;zh)</a></li>
<li><a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca (en)</a></li>
<li><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-3">Stanford Alpaca (zh)</a></li>
<li><a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM">Alpaca GPT4 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2" rel="nofollow">Glaive Function Calling V2 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/GAIR/lima" rel="nofollow">LIMA (en)</a></li>
<li><a href="https://huggingface.co/datasets/JosephusCheung/GuanacoDataset" rel="nofollow">Guanaco Dataset (multilingual)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/train_2M_CN" rel="nofollow">BELLE 2M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/train_1M_CN" rel="nofollow">BELLE 1M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/train_0.5M_CN" rel="nofollow">BELLE 0.5M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M" rel="nofollow">BELLE Dialogue 0.4M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/school_math_0.25M" rel="nofollow">BELLE School Math 0.25M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M" rel="nofollow">BELLE Multiturn Chat 0.8M (zh)</a></li>
<li><a href="https://github.com/thunlp/UltraChat">UltraChat (en)</a></li>
<li><a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus" rel="nofollow">OpenPlatypus (en)</a></li>
<li><a href="https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k" rel="nofollow">CodeAlpaca 20k (en)</a></li>
<li><a href="https://huggingface.co/datasets/QingyiSi/Alpaca-CoT" rel="nofollow">Alpaca CoT (multilingual)</a></li>
<li><a href="https://huggingface.co/datasets/Open-Orca/OpenOrca" rel="nofollow">OpenOrca (en)</a></li>
<li><a href="https://huggingface.co/datasets/Open-Orca/SlimOrca" rel="nofollow">SlimOrca (en)</a></li>
<li><a href="https://huggingface.co/datasets/TIGER-Lab/MathInstruct" rel="nofollow">MathInstruct (en)</a></li>
<li><a href="https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M" rel="nofollow">Firefly 1.1M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/wiki_qa" rel="nofollow">Wiki QA (en)</a></li>
<li><a href="https://huggingface.co/datasets/suolyer/webqa" rel="nofollow">Web QA (zh)</a></li>
<li><a href="https://huggingface.co/datasets/zxbsmk/webnovel_cn" rel="nofollow">WebNovel (zh)</a></li>
<li><a href="https://huggingface.co/datasets/berkeley-nest/Nectar" rel="nofollow">Nectar (en)</a></li>
<li><a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data" rel="nofollow">deepctrl (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/HasturOfficial/adgen" rel="nofollow">Advertise Generating (zh)</a></li>
<li><a href="https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k" rel="nofollow">ShareGPT Hyperfiltered (en)</a></li>
<li><a href="https://huggingface.co/datasets/shibing624/sharegpt_gpt4" rel="nofollow">ShareGPT4 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k" rel="nofollow">UltraChat 200k (en)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/Infinity-Instruct" rel="nofollow">Infinity Instruct (zh)</a></li>
<li><a href="https://huggingface.co/datasets/THUDM/AgentInstruct" rel="nofollow">AgentInstruct (en)</a></li>
<li><a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m" rel="nofollow">LMSYS Chat 1M (en)</a></li>
<li><a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k" rel="nofollow">Evol Instruct V2 (en)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceTB/cosmopedia" rel="nofollow">Cosmopedia (en)</a></li>
<li><a href="https://huggingface.co/datasets/hfl/stem_zh_instruction" rel="nofollow">STEM (zh)</a></li>
<li><a href="https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo" rel="nofollow">Ruozhiba (zh)</a></li>
<li><a href="https://huggingface.co/datasets/m-a-p/neo_sft_phase2" rel="nofollow">Neo-sft (zh)</a></li>
<li><a href="https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered" rel="nofollow">Magpie-Pro-300K-Filtered (en)</a></li>
<li><a href="https://huggingface.co/datasets/argilla/magpie-ultra-v0.1" rel="nofollow">Magpie-ultra-v0.1 (en)</a></li>
<li><a href="https://huggingface.co/datasets/TIGER-Lab/WebInstructSub" rel="nofollow">WebInstructSub (en)</a></li>
<li><a href="https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT" rel="nofollow">OpenO1-SFT (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k" rel="nofollow">Open-Thoughts (en)</a></li>
<li><a href="https://huggingface.co/datasets/open-r1/OpenR1-Math-220k" rel="nofollow">Open-R1-Math (en)</a></li>
<li><a href="https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT" rel="nofollow">Chinese-DeepSeek-R1-Distill (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k" rel="nofollow">LLaVA mixed (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions" rel="nofollow">Pokemon-gpt4o-captions (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/oasst_de" rel="nofollow">Open Assistant (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de" rel="nofollow">Dolly 15k (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de" rel="nofollow">Alpaca GPT4 (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de" rel="nofollow">OpenSchnabeltier (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de" rel="nofollow">Evol Instruct (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/dolphin_de" rel="nofollow">Dolphin (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/booksum_de" rel="nofollow">Booksum (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de" rel="nofollow">Airoboros (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de" rel="nofollow">Ultrachat (de)</a></li>
</ul>
</details>
<details><summary>Preference datasets</summary>
<ul dir="auto">
<li><a href="https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k" rel="nofollow">DPO mixed (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized" rel="nofollow">UltraFeedback (en)</a></li>
<li><a href="https://huggingface.co/datasets/m-a-p/COIG-P" rel="nofollow">COIG-P (zh)</a></li>
<li><a href="https://huggingface.co/datasets/openbmb/RLHF-V-Dataset" rel="nofollow">RLHF-V (en)</a></li>
<li><a href="https://huggingface.co/datasets/Zhihui/VLFeedback" rel="nofollow">VLFeedback (en)</a></li>
<li><a href="https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset" rel="nofollow">RLAIF-V (en)</a></li>
<li><a href="https://huggingface.co/datasets/Intel/orca_dpo_pairs" rel="nofollow">Orca DPO Pairs (en)</a></li>
<li><a href="https://huggingface.co/datasets/Anthropic/hh-rlhf" rel="nofollow">HH-RLHF (en)</a></li>
<li><a href="https://huggingface.co/datasets/berkeley-nest/Nectar" rel="nofollow">Nectar (en)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de" rel="nofollow">Orca DPO (de)</a></li>
<li><a href="https://huggingface.co/datasets/argilla/kto-mix-15k" rel="nofollow">KTO mixed (en)</a></li>
</ul>
</details>
<p dir="auto">Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install --upgrade huggingface_hub
huggingface-cli login"><pre>pip install --upgrade huggingface_hub
huggingface-cli login</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirement</h2><a id="user-content-requirement" aria-label="Permalink: Requirement" href="#requirement"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Mandatory</th>
<th>Minimum</th>
<th>Recommend</th>
</tr>
</thead>
<tbody>
<tr>
<td>python</td>
<td>3.9</td>
<td>3.10</td>
</tr>
<tr>
<td>torch</td>
<td>2.0.0</td>
<td>2.6.0</td>
</tr>
<tr>
<td>torchvision</td>
<td>0.15.0</td>
<td>0.21.0</td>
</tr>
<tr>
<td>transformers</td>
<td>4.49.0</td>
<td>4.50.0</td>
</tr>
<tr>
<td>datasets</td>
<td>2.16.0</td>
<td>3.2.0</td>
</tr>
<tr>
<td>accelerate</td>
<td>0.34.0</td>
<td>1.2.1</td>
</tr>
<tr>
<td>peft</td>
<td>0.14.0</td>
<td>0.15.1</td>
</tr>
<tr>
<td>trl</td>
<td>0.8.6</td>
<td>0.9.6</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Optional</th>
<th>Minimum</th>
<th>Recommend</th>
</tr>
</thead>
<tbody>
<tr>
<td>CUDA</td>
<td>11.6</td>
<td>12.2</td>
</tr>
<tr>
<td>deepspeed</td>
<td>0.10.0</td>
<td>0.16.4</td>
</tr>
<tr>
<td>bitsandbytes</td>
<td>0.39.0</td>
<td>0.43.1</td>
</tr>
<tr>
<td>vllm</td>
<td>0.4.3</td>
<td>0.8.2</td>
</tr>
<tr>
<td>flash-attn</td>
<td>2.5.6</td>
<td>2.7.2</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware Requirement</h3><a id="user-content-hardware-requirement" aria-label="Permalink: Hardware Requirement" href="#hardware-requirement"></a></p>
<p dir="auto">* <em>estimated</em></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Method</th>
<th>Bits</th>
<th>7B</th>
<th>14B</th>
<th>30B</th>
<th>70B</th>
<th><code>x</code>B</th>
</tr>
</thead>
<tbody>
<tr>
<td>Full (<code>bf16</code> or <code>fp16</code>)</td>
<td>32</td>
<td>120GB</td>
<td>240GB</td>
<td>600GB</td>
<td>1200GB</td>
<td><code>18x</code>GB</td>
</tr>
<tr>
<td>Full (<code>pure_bf16</code>)</td>
<td>16</td>
<td>60GB</td>
<td>120GB</td>
<td>300GB</td>
<td>600GB</td>
<td><code>8x</code>GB</td>
</tr>
<tr>
<td>Freeze/LoRA/GaLore/APOLLO/BAdam/OFT</td>
<td>16</td>
<td>16GB</td>
<td>32GB</td>
<td>64GB</td>
<td>160GB</td>
<td><code>2x</code>GB</td>
</tr>
<tr>
<td>QLoRA / QOFT</td>
<td>8</td>
<td>10GB</td>
<td>20GB</td>
<td>40GB</td>
<td>80GB</td>
<td><code>x</code>GB</td>
</tr>
<tr>
<td>QLoRA / QOFT</td>
<td>4</td>
<td>6GB</td>
<td>12GB</td>
<td>24GB</td>
<td>48GB</td>
<td><code>x/2</code>GB</td>
</tr>
<tr>
<td>QLoRA / QOFT</td>
<td>2</td>
<td>4GB</td>
<td>8GB</td>
<td>16GB</td>
<td>24GB</td>
<td><code>x/4</code>GB</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">Installation is mandatory.</p>
</div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Install from Source</h4><a id="user-content-install-from-source" aria-label="Permalink: Install from Source" href="#install-from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &quot;.[torch,metrics]&quot; --no-build-isolation"><pre>git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
<span>cd</span> LLaMA-Factory
pip install -e <span><span>"</span>.[torch,metrics]<span>"</span></span> --no-build-isolation</pre></div>
<p dir="auto">Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Install from Docker Image</h4><a id="user-content-install-from-docker-image" aria-label="Permalink: Install from Docker Image" href="#install-from-docker-image"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest"><pre>docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest</pre></div>
<p dir="auto">This image is built on Ubuntu 22.04 (x86_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.</p>
<p dir="auto">Find the pre-built images: <a href="https://hub.docker.com/r/hiyouga/llamafactory/tags" rel="nofollow">https://hub.docker.com/r/hiyouga/llamafactory/tags</a></p>
<p dir="auto">Please refer to <a href="#build-docker">build docker</a> to build the image yourself.</p>
<details><summary>Setting up a virtual environment with <b>uv</b></summary>
<p dir="auto">Create an isolated Python environment with <a href="https://github.com/astral-sh/uv">uv</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uv sync --extra torch --extra metrics --prerelease=allow"><pre>uv sync --extra torch --extra metrics --prerelease=allow</pre></div>
<p dir="auto">Run LLaMA-Factory in the isolated environment:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml"><pre>uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml</pre></div>
</details>
<details><summary>For Windows users</summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Install PyTorch</h4><a id="user-content-install-pytorch" aria-label="Permalink: Install PyTorch" href="#install-pytorch"></a></p>
<p dir="auto">You need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the <a href="https://pytorch.org/get-started/locally/" rel="nofollow">official website</a> and the following command to install PyTorch with CUDA support:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
python -c &quot;import torch; print(torch.cuda.is_available())&quot;"><pre>pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
python -c <span><span>"</span>import torch; print(torch.cuda.is_available())<span>"</span></span></pre></div>
<p dir="auto">If you see <code>True</code> then you have successfully installed PyTorch with CUDA support.</p>
<p dir="auto">Try <code>dataloader_num_workers: 0</code> if you encounter <code>Can't pickle local object</code> error.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Install BitsAndBytes</h4><a id="user-content-install-bitsandbytes" aria-label="Permalink: Install BitsAndBytes" href="#install-bitsandbytes"></a></p>
<p dir="auto">If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of <code>bitsandbytes</code> library, which supports CUDA 11.1 to 12.2, please select the appropriate <a href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels">release version</a> based on your CUDA version.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl"><pre>pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Install Flash Attention-2</h4><a id="user-content-install-flash-attention-2" aria-label="Permalink: Install Flash Attention-2" href="#install-flash-attention-2"></a></p>
<p dir="auto">To enable FlashAttention-2 on the Windows platform, please use the script from <a href="https://huggingface.co/lldacing/flash-attention-windows-wheel" rel="nofollow">flash-attention-windows-wheel</a> to compile and install it by yourself.</p>
</details>
<details><summary>For Ascend NPU users</summary>
<p dir="auto">To install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: <code>pip install -e ".[torch-npu,metrics]"</code>. Additionally, you need to install the <strong><a href="https://www.hiascend.com/developer/download/community/result?module=cann" rel="nofollow">Ascend CANN Toolkit and Kernels</a></strong>. Please follow the <a href="https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html" rel="nofollow">installation tutorial</a> or use the following commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# replace the url according to your CANN version and devices
# install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run
bash Ascend-cann-toolkit_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run --install

# install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run
bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run --install

# set env variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh"><pre><span><span>#</span> replace the url according to your CANN version and devices</span>
<span><span>#</span> install CANN Toolkit</span>
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-<span><span>"</span><span><span>$(</span>uname -i<span>)</span></span><span>"</span></span>.run
bash Ascend-cann-toolkit_8.0.0.alpha002_linux-<span><span>"</span><span><span>$(</span>uname -i<span>)</span></span><span>"</span></span>.run --install

<span><span>#</span> install CANN Kernels</span>
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-<span><span>"</span><span><span>$(</span>uname -i<span>)</span></span><span>"</span></span>.run
bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-<span><span>"</span><span><span>$(</span>uname -i<span>)</span></span><span>"</span></span>.run --install

<span><span>#</span> set env variables</span>
<span>source</span> /usr/local/Ascend/ascend-toolkit/set_env.sh</pre></div>
<table>
<thead>
<tr>
<th>Requirement</th>
<th>Minimum</th>
<th>Recommend</th>
</tr>
</thead>
<tbody>
<tr>
<td>CANN</td>
<td>8.0.RC1</td>
<td>8.0.0.alpha002</td>
</tr>
<tr>
<td>torch</td>
<td>2.1.0</td>
<td>2.4.0</td>
</tr>
<tr>
<td>torch-npu</td>
<td>2.1.0</td>
<td>2.4.0.post2</td>
</tr>
<tr>
<td>deepspeed</td>
<td>0.13.2</td>
<td>0.13.2</td>
</tr>
<tr>
<td>vllm-ascend</td>
<td>-</td>
<td>0.7.3</td>
</tr>
</tbody>
</table>
<p dir="auto">Remember to use <code>ASCEND_RT_VISIBLE_DEVICES</code> instead of <code>CUDA_VISIBLE_DEVICES</code> to specify the device to use.</p>
<p dir="auto">If you cannot infer model on NPU devices, try setting <code>do_sample: false</code> in the configurations.</p>
<p dir="auto">Download the pre-built Docker images: <a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html" rel="nofollow">32GB</a> | <a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html" rel="nofollow">64GB</a></p>
<h4 dir="auto">Install BitsAndBytes</h4>
<p dir="auto">To use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:</p>
<ol dir="auto">
<li>Manually compile bitsandbytes: Refer to <a href="https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&amp;platform=Ascend+NPU" rel="nofollow">the installation documentation</a> for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Install bitsandbytes from source
# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch
git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git
cd bitsandbytes/

# Install dependencies
pip install -r requirements-dev.txt

# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference
apt-get install -y build-essential cmake

# Compile &amp; install  
cmake -DCOMPUTE_BACKEND=npu -S .
make
pip install ."><pre><span><span>#</span> Install bitsandbytes from source</span>
<span><span>#</span> Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch</span>
git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git
<span>cd</span> bitsandbytes/

<span><span>#</span> Install dependencies</span>
pip install -r requirements-dev.txt

<span><span>#</span> Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference</span>
apt-get install -y build-essential cmake

<span><span>#</span> Compile &amp; install  </span>
cmake -DCOMPUTE_BACKEND=npu -S <span>.</span>
make
pip install <span>.</span></pre></div>
<ol start="2" dir="auto">
<li>Install transformers from the main branch.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone -b main https://github.com/huggingface/transformers.git
cd transformers
pip install ."><pre>git clone -b main https://github.com/huggingface/transformers.git
<span>cd</span> transformers
pip install <span>.</span></pre></div>
<ol start="3" dir="auto">
<li>Set <code>double_quantization: false</code> in the configuration. You can refer to the <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/train_qlora/llama3_lora_sft_bnb_npu.yaml">example</a>.</li>
</ol>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Data Preparation</h3><a id="user-content-data-preparation" aria-label="Permalink: Data Preparation" href="#data-preparation"></a></p>
<p dir="auto">Please refer to <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README.md">data/README.md</a> for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Please update <code>data/dataset_info.json</code> to use your custom dataset.</p>
</div>
<p dir="auto">You can also use <strong><a href="https://github.com/ConardLi/easy-dataset">Easy Dataset</a></strong>, <strong><a href="https://github.com/OpenDCAI/DataFlow">DataFlow</a></strong> and <strong><a href="https://github.com/open-sciencelab/GraphGen">GraphGen</a></strong> to create synthetic data for fine-tuning.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quickstart</h3><a id="user-content-quickstart" aria-label="Permalink: Quickstart" href="#quickstart"></a></p>
<p dir="auto">Use the following 3 commands to run LoRA <strong>fine-tuning</strong>, <strong>inference</strong> and <strong>merging</strong> of the Llama3-8B-Instruct model, respectively.</p>
<div dir="auto" data-snippet-clipboard-copy-content="llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml"><pre>llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli <span>export</span> examples/merge_lora/llama3_lora_sft.yaml</pre></div>
<p dir="auto">See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples/README.md</a> for advanced usage (including distributed training).</p>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">Use <code>llamafactory-cli help</code> to show help information.</p>
<p dir="auto">Read <a href="https://github.com/hiyouga/LLaMA-Factory/issues/4614" data-hovercard-type="issue" data-hovercard-url="/hiyouga/LLaMA-Factory/issues/4614/hovercard">FAQs</a> first if you encounter any problems.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Fine-Tuning with LLaMA Board GUI (powered by <a href="https://github.com/gradio-app/gradio">Gradio</a>)</h3><a id="user-content-fine-tuning-with-llama-board-gui-powered-by-gradio" aria-label="Permalink: Fine-Tuning with LLaMA Board GUI (powered by Gradio)" href="#fine-tuning-with-llama-board-gui-powered-by-gradio"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">LLaMA Factory Online</h3><a id="user-content-llama-factory-online" aria-label="Permalink: LLaMA Factory Online" href="#llama-factory-online"></a></p>
<p dir="auto">Read our <a href="https://docs.llamafactory.com.cn/docs/documents/quickstart/getstarted/?utm_source=LLaMA-Factory" rel="nofollow">documentation</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build Docker</h3><a id="user-content-build-docker" aria-label="Permalink: Build Docker" href="#build-docker"></a></p>
<p dir="auto">For CUDA users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd docker/docker-cuda/
docker compose up -d
docker compose exec llamafactory bash"><pre><span>cd</span> docker/docker-cuda/
docker compose up -d
docker compose <span>exec</span> llamafactory bash</pre></div>
<p dir="auto">For Ascend NPU users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd docker/docker-npu/
docker compose up -d
docker compose exec llamafactory bash"><pre><span>cd</span> docker/docker-npu/
docker compose up -d
docker compose <span>exec</span> llamafactory bash</pre></div>
<p dir="auto">For AMD ROCm users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd docker/docker-rocm/
docker compose up -d
docker compose exec llamafactory bash"><pre><span>cd</span> docker/docker-rocm/
docker compose up -d
docker compose <span>exec</span> llamafactory bash</pre></div>
<details><summary>Build without Docker Compose</summary>
<p dir="auto">For CUDA users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host --gpus=all \
    -p 7860:7860 \
    -p 8000:8000 \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash"><pre>docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest <span>.</span>

docker run -dit --ipc=host --gpus=all \
    -p 7860:7860 \
    -p 8000:8000 \
    --name llamafactory \
    llamafactory:latest

docker <span>exec</span> -it llamafactory bash</pre></div>
<p dir="auto">For Ascend NPU users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=torch-npu,metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash"><pre>docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=torch-npu,metrics \
    -t llamafactory:latest <span>.</span>

docker run -dit --ipc=host \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --name llamafactory \
    llamafactory:latest

docker <span>exec</span> -it llamafactory bash</pre></div>
<p dir="auto">For AMD ROCm users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build -f ./docker/docker-rocm/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/kfd \
    --device /dev/dri \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash"><pre>docker build -f ./docker/docker-rocm/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest <span>.</span>

docker run -dit --ipc=host \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/kfd \
    --device /dev/dri \
    --name llamafactory \
    llamafactory:latest

docker <span>exec</span> -it llamafactory bash</pre></div>
</details>
<details><summary>Use Docker volumes</summary>
<p dir="auto">You can uncomment <code>VOLUME [ "/root/.cache/huggingface", "/app/shared_data", "/app/output" ]</code> in the Dockerfile to use data volumes.</p>
<p dir="auto">When building the Docker image, use <code>-v ./hf_cache:/root/.cache/huggingface</code> argument to mount the local directory to the container. The following data volumes are available.</p>
<ul dir="auto">
<li><code>hf_cache</code>: Utilize Hugging Face cache on the host machine.</li>
<li><code>shared_data</code>: The directionary to store datasets on the host machine.</li>
<li><code>output</code>: Set export dir to this location so that the merged result can be accessed directly on the host machine.</li>
</ul>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy with OpenAI-style API and vLLM</h3><a id="user-content-deploy-with-openai-style-api-and-vllm" aria-label="Permalink: Deploy with OpenAI-style API and vLLM" href="#deploy-with-openai-style-api-and-vllm"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true"><pre>API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true</pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">Download from ModelScope Hub</h3><a id="user-content-download-from-modelscope-hub" aria-label="Permalink: Download from ModelScope Hub" href="#download-from-modelscope-hub"></a></p>
<p dir="auto">If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.</p>
<div dir="auto" data-snippet-clipboard-copy-content="export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows"><pre><span>export</span> USE_MODELSCOPE_HUB=1 <span><span>#</span> `set USE_MODELSCOPE_HUB=1` for Windows</span></pre></div>
<p dir="auto">Train the model by specifying a model ID of the ModelScope Hub as the <code>model_name_or_path</code>. You can find a full list of model IDs at <a href="https://modelscope.cn/models" rel="nofollow">ModelScope Hub</a>, e.g., <code>LLM-Research/Meta-Llama-3-8B-Instruct</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download from Modelers Hub</h3><a id="user-content-download-from-modelers-hub" aria-label="Permalink: Download from Modelers Hub" href="#download-from-modelers-hub"></a></p>
<p dir="auto">You can also use Modelers Hub to download models and datasets.</p>
<div dir="auto" data-snippet-clipboard-copy-content="export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows"><pre><span>export</span> USE_OPENMIND_HUB=1 <span><span>#</span> `set USE_OPENMIND_HUB=1` for Windows</span></pre></div>
<p dir="auto">Train the model by specifying a model ID of the Modelers Hub as the <code>model_name_or_path</code>. You can find a full list of model IDs at <a href="https://modelers.cn/models" rel="nofollow">Modelers Hub</a>, e.g., <code>TeleAI/TeleChat-7B-pt</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use W&amp;B Logger</h3><a id="user-content-use-wb-logger" aria-label="Permalink: Use W&amp;B Logger" href="#use-wb-logger"></a></p>
<p dir="auto">To use <a href="https://wandb.ai/" rel="nofollow">Weights &amp; Biases</a> for logging experimental results, you need to add the following arguments to yaml files.</p>
<div dir="auto" data-snippet-clipboard-copy-content="report_to: wandb
run_name: test_run # optional"><pre><span>report_to</span>: <span>wandb</span>
<span>run_name</span>: <span>test_run </span><span><span>#</span> optional</span></pre></div>
<p dir="auto">Set <code>WANDB_API_KEY</code> to <a href="https://wandb.ai/authorize" rel="nofollow">your key</a> when launching training tasks to log in with your W&amp;B account.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use SwanLab Logger</h3><a id="user-content-use-swanlab-logger" aria-label="Permalink: Use SwanLab Logger" href="#use-swanlab-logger"></a></p>
<p dir="auto">To use <a href="https://github.com/SwanHubX/SwanLab">SwanLab</a> for logging experimental results, you need to add the following arguments to yaml files.</p>
<div dir="auto" data-snippet-clipboard-copy-content="use_swanlab: true
swanlab_run_name: test_run # optional"><pre><span>use_swanlab</span>: <span>true</span>
<span>swanlab_run_name</span>: <span>test_run </span><span><span>#</span> optional</span></pre></div>
<p dir="auto">When launching training tasks, you can log in to SwanLab in three ways:</p>
<ol dir="auto">
<li>Add <code>swanlab_api_key=&lt;your_api_key&gt;</code> to the yaml file, and set it to your <a href="https://swanlab.cn/settings" rel="nofollow">API key</a>.</li>
<li>Set the environment variable <code>SWANLAB_API_KEY</code> to your <a href="https://swanlab.cn/settings" rel="nofollow">API key</a>.</li>
<li>Use the <code>swanlab login</code> command to complete the login.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Projects using LLaMA Factory</h2><a id="user-content-projects-using-llama-factory" aria-label="Permalink: Projects using LLaMA Factory" href="#projects-using-llama-factory"></a></p>
<p dir="auto">If you have a project that should be incorporated, please contact via email or create a pull request.</p>
<details><summary>Click to show</summary>
<ol dir="auto">
<li>Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. <a href="https://arxiv.org/abs/2308.02223" rel="nofollow">[arxiv]</a></li>
<li>Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. <a href="https://arxiv.org/abs/2308.10092" rel="nofollow">[arxiv]</a></li>
<li>Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. <a href="https://arxiv.org/abs/2308.10526" rel="nofollow">[arxiv]</a></li>
<li>Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. <a href="https://arxiv.org/abs/2311.07816" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. <a href="https://arxiv.org/abs/2312.15710" rel="nofollow">[arxiv]</a></li>
<li>Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. <a href="https://arxiv.org/abs/2401.04319" rel="nofollow">[arxiv]</a></li>
<li>Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. <a href="https://arxiv.org/abs/2401.07286" rel="nofollow">[arxiv]</a></li>
<li>Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. <a href="https://arxiv.org/abs/2402.05904" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. <a href="https://arxiv.org/abs/2402.07625" rel="nofollow">[arxiv]</a></li>
<li>Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. <a href="https://arxiv.org/abs/2402.11176" rel="nofollow">[arxiv]</a></li>
<li>Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. <a href="https://arxiv.org/abs/2402.11187" rel="nofollow">[arxiv]</a></li>
<li>Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. <a href="https://arxiv.org/abs/2402.11746" rel="nofollow">[arxiv]</a></li>
<li>Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. <a href="https://arxiv.org/abs/2402.11801" rel="nofollow">[arxiv]</a></li>
<li>Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. <a href="https://arxiv.org/abs/2402.11809" rel="nofollow">[arxiv]</a></li>
<li>Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. <a href="https://arxiv.org/abs/2402.11819" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. <a href="https://arxiv.org/abs/2402.12204" rel="nofollow">[arxiv]</a></li>
<li>Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. <a href="https://arxiv.org/abs/2402.14714" rel="nofollow">[arxiv]</a></li>
<li>Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. <a href="https://arxiv.org/abs/2402.15043" rel="nofollow">[arxiv]</a></li>
<li>Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. <a href="https://arxiv.org/abs/2403.02333" rel="nofollow">[arxiv]</a></li>
<li>Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. <a href="https://arxiv.org/abs/2403.03419" rel="nofollow">[arxiv]</a></li>
<li>Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. <a href="https://arxiv.org/abs/2403.08228" rel="nofollow">[arxiv]</a></li>
<li>Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. <a href="https://arxiv.org/abs/2403.09073" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. <a href="https://arxiv.org/abs/2403.14541" rel="nofollow">[arxiv]</a></li>
<li>Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. <a href="https://arxiv.org/abs/2403.15246" rel="nofollow">[arxiv]</a></li>
<li>Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. <a href="https://arxiv.org/abs/2403.16008" rel="nofollow">[arxiv]</a></li>
<li>Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. <a href="https://arxiv.org/abs/2403.16443" rel="nofollow">[arxiv]</a></li>
<li>Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. <a href="https://arxiv.org/abs/2404.00604" rel="nofollow">[arxiv]</a></li>
<li>Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. <a href="https://arxiv.org/abs/2404.02827" rel="nofollow">[arxiv]</a></li>
<li>Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. <a href="https://arxiv.org/abs/2404.04167" rel="nofollow">[arxiv]</a></li>
<li>Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. <a href="https://arxiv.org/abs/2404.04316" rel="nofollow">[arxiv]</a></li>
<li>Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. <a href="https://arxiv.org/abs/2404.07084" rel="nofollow">[arxiv]</a></li>
<li>Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. <a href="https://arxiv.org/abs/2404.09836" rel="nofollow">[arxiv]</a></li>
<li>Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. <a href="https://arxiv.org/abs/2404.11581" rel="nofollow">[arxiv]</a></li>
<li>Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. <a href="https://arxiv.org/abs/2404.14215" rel="nofollow">[arxiv]</a></li>
<li>Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. <a href="https://arxiv.org/abs/2404.16621" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. <a href="https://arxiv.org/abs/2404.17140" rel="nofollow">[arxiv]</a></li>
<li>Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. <a href="https://arxiv.org/abs/2404.18585" rel="nofollow">[arxiv]</a></li>
<li>Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. <a href="https://arxiv.org/abs/2405.04760" rel="nofollow">[arxiv]</a></li>
<li>Dammu et al. "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. <a href="https://arxiv.org/abs/2405.05378" rel="nofollow">[arxiv]</a></li>
<li>Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. <a href="https://arxiv.org/abs/2405.09055" rel="nofollow">[arxiv]</a></li>
<li>Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. <a href="https://arxiv.org/abs/2405.12739" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. <a href="https://arxiv.org/abs/2405.13816" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. <a href="https://arxiv.org/abs/2405.20215" rel="nofollow">[arxiv]</a></li>
<li>Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. <a href="https://aclanthology.org/2024.lt4hala-1.30" rel="nofollow">[paper]</a></li>
<li>Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. <a href="https://arxiv.org/abs/2406.00380" rel="nofollow">[arxiv]</a></li>
<li>Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. <a href="https://arxiv.org/abs/2406.02106" rel="nofollow">[arxiv]</a></li>
<li>Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. <a href="https://arxiv.org/abs/2406.03136" rel="nofollow">[arxiv]</a></li>
<li>Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. <a href="https://arxiv.org/abs/2406.04496" rel="nofollow">[arxiv]</a></li>
<li>Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. <a href="https://arxiv.org/abs/2406.05688" rel="nofollow">[arxiv]</a></li>
<li>Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. <a href="https://arxiv.org/abs/2406.05955" rel="nofollow">[arxiv]</a></li>
<li>Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. <a href="https://arxiv.org/abs/2406.06973" rel="nofollow">[arxiv]</a></li>
<li>Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. <a href="https://arxiv.org/abs/2406.07115" rel="nofollow">[arxiv]</a></li>
<li>Zhu et al. Are Large Language Models Good Statisticians?. 2024. <a href="https://arxiv.org/abs/2406.07815" rel="nofollow">[arxiv]</a></li>
<li>Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. <a href="https://arxiv.org/abs/2406.10099" rel="nofollow">[arxiv]</a></li>
<li>Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. <a href="https://arxiv.org/abs/2406.10173" rel="nofollow">[arxiv]</a></li>
<li>He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. <a href="https://arxiv.org/abs/2406.12074" rel="nofollow">[arxiv]</a></li>
<li>Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. <a href="https://arxiv.org/abs/2406.14408" rel="nofollow">[arxiv]</a></li>
<li>Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. <a href="https://arxiv.org/abs/2406.14546" rel="nofollow">[arxiv]</a></li>
<li>Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. <a href="https://arxiv.org/abs/2406.15695" rel="nofollow">[arxiv]</a></li>
<li>Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. <a href="https://arxiv.org/abs/2406.17233" rel="nofollow">[arxiv]</a></li>
<li>Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. <a href="https://arxiv.org/abs/2406.18069" rel="nofollow">[arxiv]</a></li>
<li>Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. <a href="https://aclanthology.org/2024.americasnlp-1.25" rel="nofollow">[paper]</a></li>
<li>Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. <a href="https://arxiv.org/abs/2406.19949" rel="nofollow">[arxiv]</a></li>
<li>Yang et al. Financial Knowledge Large Language Model. 2024. <a href="https://arxiv.org/abs/2407.00365" rel="nofollow">[arxiv]</a></li>
<li>Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. <a href="https://arxiv.org/abs/2407.01470" rel="nofollow">[arxiv]</a></li>
<li>Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. <a href="https://arxiv.org/abs/2407.06129" rel="nofollow">[arxiv]</a></li>
<li>Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. <a href="https://arxiv.org/abs/2407.08044" rel="nofollow">[arxiv]</a></li>
<li>Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. <a href="https://arxiv.org/abs/2407.09756" rel="nofollow">[arxiv]</a></li>
<li>Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. <a href="https://scholarcommons.scu.edu/cseng_senior/272/" rel="nofollow">[paper]</a></li>
<li>Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. <a href="https://arxiv.org/abs/2407.13561" rel="nofollow">[arxiv]</a></li>
<li>Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. <a href="https://arxiv.org/abs/2407.16637" rel="nofollow">[arxiv]</a></li>
<li>Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. <a href="https://arxiv.org/abs/2407.17535" rel="nofollow">[arxiv]</a></li>
<li>Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. <a href="https://arxiv.org/abs/2407.19705" rel="nofollow">[arxiv]</a></li>
<li>Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. <a href="https://arxiv.org/abs/2408.00137" rel="nofollow">[arxiv]</a></li>
<li>Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. <a href="https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf" rel="nofollow">[paper]</a></li>
<li>Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. <a href="https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11" rel="nofollow">[paper]</a></li>
<li>Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. <a href="https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23" rel="nofollow">[paper]</a></li>
<li>Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. <a href="https://arxiv.org/abs/2408.04693" rel="nofollow">[arxiv]</a></li>
<li>Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. <a href="https://arxiv.org/abs/2408.04168" rel="nofollow">[arxiv]</a></li>
<li>Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. <a href="https://aclanthology.org/2024.finnlp-2.1/" rel="nofollow">[paper]</a></li>
<li>Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. <a href="https://arxiv.org/abs/2408.08072" rel="nofollow">[arxiv]</a></li>
<li>Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. <a href="https://dl.acm.org/doi/10.1145/3627673.3679611" rel="nofollow">[paper]</a></li>
<li>Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. <a href="https://aclanthology.org/2024.findings-acl.830.pdf" rel="nofollow">[paper]</a></li>
<li><strong><a href="https://github.com/Yu-Yang-Li/StarWhisper">StarWhisper</a></strong>: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.</li>
<li><strong><a href="https://github.com/FudanDISC/DISC-LawLLM">DISC-LawLLM</a></strong>: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.</li>
<li><strong><a href="https://github.com/X-D-Lab/Sunsimiao">Sunsimiao</a></strong>: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.</li>
<li><strong><a href="https://github.com/WangRongsheng/CareGPT">CareGPT</a></strong>: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.</li>
<li><strong><a href="https://github.com/PKU-YuanGroup/Machine-Mindset/">MachineMindset</a></strong>: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.</li>
<li><strong><a href="https://huggingface.co/Nekochu/Luminia-13B-v3" rel="nofollow">Luminia-13B-v3</a></strong>: A large language model specialized in generate metadata for stable diffusion. <a href="https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt" rel="nofollow">[demo]</a></li>
<li><strong><a href="https://github.com/BUAADreamer/Chinese-LLaVA-Med">Chinese-LLaVA-Med</a></strong>: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.</li>
<li><strong><a href="https://github.com/THUDM/AutoRE">AutoRE</a></strong>: A document-level relation extraction system based on large language models.</li>
<li><strong><a href="https://github.com/NVIDIA/RTX-AI-Toolkit">NVIDIA RTX AI Toolkit</a></strong>: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.</li>
<li><strong><a href="https://github.com/LazyAGI/LazyLLM">LazyLLM</a></strong>: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.</li>
<li><strong><a href="https://github.com/NLPJCL/RAG-Retrieval">RAG-Retrieval</a></strong>: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. <a href="https://zhuanlan.zhihu.com/p/987727357" rel="nofollow">[blog]</a></li>
<li><strong><a href="https://github.com/Qihoo360/360-LLaMA-Factory">360-LLaMA-Factory</a></strong>: A modified library that supports long sequence SFT &amp; DPO using ring attention.</li>
<li><strong><a href="https://novasky-ai.github.io/posts/sky-t1/" rel="nofollow">Sky-T1</a></strong>: An o1-like model fine-tuned by NovaSky AI with very small cost.</li>
<li><strong><a href="https://github.com/xming521/WeClone">WeClone</a></strong>: One-stop solution for creating your digital avatar from chat logs.</li>
<li><strong><a href="https://github.com/SmartFlowAI/EmoLLM">EmoLLM</a></strong>: A project about large language models (LLMs) and mental health.</li>
</ol>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This repository is licensed under the <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/LICENSE">Apache-2.0 License</a>.</p>
<p dir="auto">Please follow the model licenses to use the corresponding model weights: <a href="https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf" rel="nofollow">Baichuan 2</a> / <a href="https://huggingface.co/spaces/bigscience/license" rel="nofollow">BLOOM</a> / <a href="https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE">ChatGLM3</a> / <a href="https://cohere.com/c4ai-cc-by-nc-license" rel="nofollow">Command R</a> / <a href="https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL">DeepSeek</a> / <a href="https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt" rel="nofollow">Falcon</a> / <a href="https://ai.google.dev/gemma/terms" rel="nofollow">Gemma</a> / <a href="https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE" rel="nofollow">GLM-4</a> / <a href="https://github.com/openai/gpt-2/blob/master/LICENSE">GPT-2</a> / <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/LICENSE">Granite</a> / <a href="https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE" rel="nofollow">Index</a> / <a href="https://github.com/InternLM/InternLM#license">InternLM</a> / <a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md">Llama</a> / <a href="https://ai.meta.com/llama/license/" rel="nofollow">Llama 2</a> / <a href="https://llama.meta.com/llama3/license/" rel="nofollow">Llama 3</a> / <a href="https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE">Llama 4</a> / <a href="https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md">MiniCPM</a> / <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/LICENSE">Mistral/Mixtral/Pixtral</a> / <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/LICENSE">OLMo</a> / <a href="https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx" rel="nofollow">Phi-1.5/Phi-2</a> / <a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE" rel="nofollow">Phi-3/Phi-4</a> / <a href="https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT">Qwen</a> / <a href="https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf" rel="nofollow">Skywork</a> / <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" rel="nofollow">StarCoder 2</a> / <a href="https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf" rel="nofollow">TeleChat2</a> / <a href="https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf">XVERSE</a> / <a href="https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE" rel="nofollow">Yi</a> / <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/LICENSE">Yi-1.5</a> / <a href="https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan">Yuan 2</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If this work is helpful, please kindly cite as:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}"><pre><span>@inproceedings</span>{<span>zheng2024llamafactory</span>,
  <span>title</span>=<span><span>{</span>LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma<span>}</span></span>,
  <span>booktitle</span>=<span><span>{</span>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)<span>}</span></span>,
  <span>address</span>=<span><span>{</span>Bangkok, Thailand<span>}</span></span>,
  <span>publisher</span>=<span><span>{</span>Association for Computational Linguistics<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
  <span>url</span>=<span><span>{</span>http://arxiv.org/abs/2403.13372<span>}</span></span>
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgement</h2><a id="user-content-acknowledgement" aria-label="Permalink: Acknowledgement" href="#acknowledgement"></a></p>
<p dir="auto">This repo benefits from <a href="https://github.com/huggingface/peft">PEFT</a>, <a href="https://github.com/huggingface/trl">TRL</a>, <a href="https://github.com/artidoro/qlora">QLoRA</a> and <a href="https://github.com/lm-sys/FastChat">FastChat</a>. Thanks for their wonderful works.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Star History</h2><a id="user-content-star-history" aria-label="Permalink: Star History" href="#star-history"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/dbff38fb23c7398d1ac1b63e4685ebda2bf94c75cb36001bb964f15b4619546c/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6869796f7567612f4c4c614d412d466163746f727926747970653d44617465"><img src="https://camo.githubusercontent.com/dbff38fb23c7398d1ac1b63e4685ebda2bf94c75cb36001bb964f15b4619546c/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6869796f7567612f4c4c614d412d466163746f727926747970653d44617465" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&amp;type=Date"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jimmy kimmel should have strong odds at the Supreme Court (127 pts)]]></title>
            <link>https://www.politico.com/news/magazine/2025/09/18/jimmy-kimmel-supreme-court-first-amendment-lawsuit-00570697</link>
            <guid>45296182</guid>
            <pubDate>Thu, 18 Sep 2025 23:17:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.com/news/magazine/2025/09/18/jimmy-kimmel-supreme-court-first-amendment-lawsuit-00570697">https://www.politico.com/news/magazine/2025/09/18/jimmy-kimmel-supreme-court-first-amendment-lawsuit-00570697</a>, See on <a href="https://news.ycombinator.com/item?id=45296182">Hacker News</a></p>
Couldn't get https://www.politico.com/news/magazine/2025/09/18/jimmy-kimmel-supreme-court-first-amendment-lawsuit-00570697: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Dark patterns killed my wife's Windows 11 installation (118 pts)]]></title>
            <link>https://www.osnews.com/story/143376/dark-patterns-killed-my-wifes-windows-11-installation/</link>
            <guid>45296086</guid>
            <pubDate>Thu, 18 Sep 2025 23:05:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.osnews.com/story/143376/dark-patterns-killed-my-wifes-windows-11-installation/">https://www.osnews.com/story/143376/dark-patterns-killed-my-wifes-windows-11-installation/</a>, See on <a href="https://news.ycombinator.com/item?id=45296086">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody"><p>Last night, my wife looks up from her computer, troubled. She tells me she can’t log into her computer running Windows 11, as every time she enters the PIN code to her account, the login screen throws up a cryptic error: “Your credentials could not be verified”. She’s using the correct PIN code, so that surely isn’t it. We opt for the gold standard in troubleshooting and perform a quick reboot, but that doesn’t fix it. My initial instinct is that since she’s using an online account instead of a local one, perhaps Microsoft is having some server issues? A quick check online indicates that no, Microsoft’s servers seem to be running fine, and to be honest, I don’t even know if that would have an effect on logging into Windows in the first place.</p><p>The Windows 11 login screen does give us a link to click in case you forget your PIN code. Despite the fact the PIN code she’s entering is correct, we try to go through this process to see if it goes anywhere. This is where things really start to get weird. A few dialogs flash in and out of existence, until it’s showing us a dialog telling us to insert a security USB key of some sort, which we don’t have. Dismissing it gives us an option to try other login methods, including a basic password login. This, too, doesn’t work; just like with the PIN code, Windows 11 claims the accurate, correct password my wife is entering is invalid (just to be safe, we tested it by logging into her Microsoft account on her phone, which works just fine).</p><p>In the account selection menu in the bottom-left, an ominous new account mysteriously appears: WsiAccount.</p><p>The next option we try is to actually change the PIN code. This doesn’t work either. Windows wants us to use a second factor using my wife’s phone number, but this throws up another weird error, this time claiming the SMS service to send the code isn’t working. A quick check online once again confirms the service seems to be working just fine for everybody else. I’m starting to get really stumped and frustrated.</p><p>Of course, during all of this, we’re both searching the web to find anything that might help us figure out what’s going on. None of our searches bring up anything useful, and none of our findings seem to be related to or match up with the issue we’re having. While she’s looking at her phone and I’m browsing on my Fedora/KDE PC next to hers, she quickly mentions she’s getting a notification that OneDrive is full, which is odd, since she doesn’t use OneDrive for anything.</p><p>We take this up as a quick sidequest, and we check up on her OneDrive account on her phone. As OneDrive loads, our jaws drop in amazement: a big banner warning is telling her she’s using <em>over 5500% of her 5GB free account</em>. We look at each other and burst out laughing. We exchange some confused words, and then we realise what is going on: my wife just got a brand new Samsung Galaxy S25, and Samsung has some sort of deal with Microsoft to integrate its services into Samsung’s variant of Android. Perhaps during the process of transferring data and applications from her old to her new phone, OneDrive syncing got turned on? A quick trip to the Samsung Gallery application confirms our suspicions: the phone is synchronising over 280GB of photos and videos to OneDrive.</p><p>My wife was never asked for consent to turn this feature on, so it must’ve been turned on by default. We quickly turn it off, delete the 280GB of photos and videos from OneDrive, and move on to the real issue at hand.</p><p>Since nothing seems to work, and none of what we find online brings us any closer to what’s going on with her Windows 11 installation, we figured it’s time to bring out the big guns. For the sake of brevity, let’s run through the things we tried. Booting into safe mode doesn’t work; we get the same login problems. Trying to uninstall the latest updates, an option in <a href="https://en.wikipedia.org/wiki/Windows_Preinstallation_Environment#Windows_Recovery_Environment">WinRE</a>, doesn’t work, and throws up an unspecified error. We try to use a restore point, but despite knowing for 100% certain the feature to periodically create restore points is enabled, the only available restore point is from 2022, and is located on a drive other than her root drive (or “C:\” in Windows parlance). Using the reset option in WinRE doesn’t work either, as it also throws up an error, this time about not having enough free space. I also walk through a few more complex suggestions, like a few manual registry hacks related to the original error using cmd.exe in WinRE. None of it yields any results.</p><p>It’s now approaching midnight, and we need to get up early to drop the kids off at preschool, so I tell my wife I’ll reinstall her copy of Windows 11 tomorrow. We’re out of ideas.</p><p>The next day, I decide to give it one last go before opting for the trouble of going through a reinstallation. The one idea I still have left is to enable the hidden administrator account in Windows 11, which gives you password-free access to what is basically Windows’ root account. It <a href="https://onlinecomputertips.com/support-categories/pc-troubleshooting/enable-built-in-windows-admin-account-if-you-cant-log-on-to-your-computer/">involves</a> booting into WinRE, loading up cmd.exe, and replacing utilman.exe in system32 with cmd.exe:</p><pre><code>move c:\windows\system32\utilman.exe c:\
copy c:\windows\system32\cmd.exe c:\windows\system32\utilman.exe</code></pre><p>If you then proceed to boot into Windows 11 and click on the Accessibility icon in the bottom-right, it will open “utilman.exe”, but since that’s just cmd.exe with the utilman.exe name, you get a command prompt to work with, right on the login screen. From here, you can launch regedit, find the correct key, change a REG_BINARY, save, and reboot. At the login screen, you’ll see a new “adminstrator” account with full access to your computer.</p><p>During the various reboots, I do some more web searching, and I stumble upon <a href="https://www.reddit.com/r/WindowsHelp/comments/1irhobe/your_credentials_arent_verified_pin_isnt_working/">a post on /r/WindowsHelp</a> from 7 months ago. The user William6212 isn’t having the <em>exact</em> same issues and error messages we’re seeing, but it’s close enough that it warrants a look at the replies. The top reply by user lemonsandlimes30 contains just two words:</p><blockquote><p>storage full</p>
<cite><a href="https://www.reddit.com/r/WindowsHelp/comments/1irhobe/comment/md8e7cp/">↫ lemonsandlimes30, the real MVP</a></cite></blockquote><p>And all of a sudden all the pieces of the puzzle fall into place. I instantly figure out the course of events: my wife gets her new Galaxy S25, and transfers over the applications and data from her old phone. During this setup process, the option in the Samsung Gallery application to synchronise photos and videos to OneDrive is enabled without her consent and without informing her. The phone starts uploading the roughly 280GB of photos and videos from her phone to her 5GB OneDrive account, and she gets a warning notification that her OneDrive storage is <em>a bit</em> full.</p><p>And now her Windows 11 PC enters the scene. Despite me knowing with 100% certainty I deleted OneDrive completely off her Windows 11 PC, some recent update or whatever must’ve reinstalled it and enabled its synchronisation feature, which in turn, right as my wife’s new phone secretly started uploading her photos and videos to OneDrive, started downloading those same photos and videos to her Windows 11’s relatively small root drive. All 280GB of them.</p><p><em>Storage full.</em></p><p>The reboots were done, and indeed, the secret passwordless administrator account was now available on the login screen. I log in, wait for Windows 11’s stupid out-of-box-experience thing to run its course, immediately open Explorer, and there it is: her root drive is completely full, with a mere 25MB or so available. We go into her account’s folder, delete the OneDrive folder and its 280GB of photos and videos, and remove OneDrive from her computer once again. Hopefully this will do the trick.</p><p>It didn’t. We still can’t log in, as the original issue persists. I log back into the administrator account, open up compmgmt.msc, go to Users, and try to change my wife’s password. No luck – it’s an online account, and it turns out you can’t change the password of such an account using traditional user management tools; you have to log into your Microsoft account on the web, and change your password there. After we do this, we can finally log back into her Windows 11 account with the newly-set password.</p><p>We fixed it.</p><h5>Darkest of patterns</h5><p>My wife and I fell victim to a series of dark patterns that nearly rendered her Windows 11 installation unrecoverable. The first dark pattern is Samsung enabling the OneDrive synchronisation feature without my wife’s consent and without informing her. The second dark pattern is Microsoft reinstalling OneDrive onto my wife’s PC without my wife’s consent and without informing her. The third dark pattern is OneDrive secretely downloading 280GB of photos and videos without once realising this was way more data than her root drive could store. The fifth and final dark pattern runs through all of this like a red thread: Microsoft’s insistence on forcefully converting every local Windows 11 user account to an online Microsoft account.</p><p>This tragedy of dark patterns then neatly cascaded into a catastrophic comedy of bugs, where a full root drive apparently corrupts online Microsoft accounts on Windows 11 so hard they become essentially unrecoverable. There were no warnings and no informational popups. Ominous user accounts started to appear on the login screen. Weird suggestions to use corporate-looking security USB keys pop up. Windows wrongfully tells my wife the PIN code and password she enters are incorrect. The suggestion to change the password or PIN code breaks completely. All the well-known rescue options any average user would turn to in WinRE throw up cryptic errors.</p><p>At this point, any reasonable person would assume their Windows 11 installation was unrecoverable, or worse, that some sort of malware had taken over their machine – ominous “WsiAccount” and demands for a security USB key and all. The only course of action most Windows users would take at this point is a full reinstallation. If it wasn’t for me having just enough knowledge to piece the puzzle together – thank you lemonsandlimes30 – we’d be doing a reinstallation today, possibly running into the issue again a few days or weeks later.</p><p>No sane person would go this deep to try and fix this problem.</p><p>This cost us hours and hours of our lives, causing especially my wife a significant amount of stress, during an already very difficult time in our lives (which I won’t get into). I’m seething with rage towards Microsoft and its utter incompetence and maliciousness. Let me, for once, not mince words here: Windows 11 is a travesty, a loose collection of dark patterns and incompetence, run by people who have zero interest in lovingly crafting an operating system they can be proud of. Windows has become a vessel for subscriptions and ads, and cannot reasonably be considered anything other than a massive pile of user-hostile dark patterns designed to extract data, ad time, and subscription money from its users.</p><p>If you can switch away and ditch Windows, you should. The ship is burning, and there’s nobody left to put out the fires.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Want to piss off your IT department? Are the links not malicious looking enough? (962 pts)]]></title>
            <link>https://phishyurl.com/</link>
            <guid>45295898</guid>
            <pubDate>Thu, 18 Sep 2025 22:40:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phishyurl.com/">https://phishyurl.com/</a>, See on <a href="https://news.ycombinator.com/item?id=45295898">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <h4>What is this and what does it do?</h4>
                    <p>This is a tool that takes any link and makes it look malicious. It works on the idea of a <a href="https://en.wikipedia.org/wiki/URL_redirection" target="_blank">redirect</a>.
                        Much like <a href="https://tinyurl.com/" target="_blank">https://tinyurl.com/</a> for example.
                        Where tinyurl makes an url shorter, this site makes it look malicious.
                    </p>
                    <p>
                        Place any link in the below input, press the button and get back a fishy(phishy, heh...get, it?)
                        looking link.
                        The fishy link doesn't actually do anything, it will just redirect you to the original link you
                        provided.
                    </p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI tools are making the world look weird (192 pts)]]></title>
            <link>https://strat7.com/blogs/weird-in-weird-out/</link>
            <guid>45295794</guid>
            <pubDate>Thu, 18 Sep 2025 22:27:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://strat7.com/blogs/weird-in-weird-out/">https://strat7.com/blogs/weird-in-weird-out/</a>, See on <a href="https://news.ycombinator.com/item?id=45295794">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="04147a8" data-element_type="widget" data-widget_type="text-editor.default"> <p>In academia and the media, AI is often described as mirroring human psychology with <a href="https://spectrum.ieee.org/chain-of-thought-prompting" target="_blank" rel="noopener"><span>humanlike reasoning</span></a>, <a href="https://www.psypost.org/ai-reaches-human-level-performance-on-general-intelligence-test-what-does-it-mean/" target="_blank" rel="noopener"><span>human-level performance</span></a>, <a href="https://www.theguardian.com/technology/2025/may/14/ai-can-spontaneously-develop-human-like-communication-study-finds" target="_blank" rel="noopener"><span>human-like communication</span></a>. In these comparisons, “humans” are treated as the benchmark.</p><p>In a provocative <a href="https://osf.io/preprints/psyarxiv/5b26t_v1" target="_blank" rel="noopener"><span>2023 paper</span></a>, researchers at Harvard University asked – which humans?</p><p>The diversity of human psychologies has been a hot topic since 2010, when researchers found that many accepted psychological “truths” were often confined to so-called “WEIRD people”: Western, Educated, Industrialised, Rich, Democratic. What feel like universal beliefs for people like me and no doubt many of the readers of this blog, e.g. that I am an automonous individual, are instead only true for a thin slice of humanity.</p><p>So when we say AI tools are “human-like”, what we mean is that AI is WEIRD.</p><p>In fact, this paper found that more than that, it thinks American. The greater the cultural distance between a country and the USA, the less accurate ChatGPT got at simulating peoples’ values. For countries like Libya and Pakistan, AI results are little better than a coin toss.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta's live staged demo fails; the "AI" recording plays before the actor acts (448 pts)]]></title>
            <link>https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/</link>
            <guid>45294859</guid>
            <pubDate>Thu, 18 Sep 2025 20:50:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/">https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/</a>, See on <a href="https://news.ycombinator.com/item?id=45294859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div id="siteTable_t3_1nkbig7"><div id="thing_t1_newl6dz" onclick="click_thing(this)" data-fullname="t1_newl6dz" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="Rorviver" data-author-fullname="t2_69ucetcs" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/newl6dz/"><div id="thing_t1_nex0x4h" onclick="click_thing(this)" data-fullname="t1_nex0x4h" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="Tormint_mp3" data-author-fullname="t2_kwtzdtb0" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nex0x4h/"><p>[–]<a href="https://old.reddit.com/user/systemhost">systemhost</a><span></span> <span title="3">3 points</span><span title="4">4 points</span><span title="5">5 points</span> <time title="Thu Sep 18 18:57:12 2025 UTC" datetime="2025-09-18T18:57:12+00:00">3 hours ago</time>&nbsp;(1 child)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nexqyinty2"><div><p>Meta Ray-Ban Glasses?</p>

<p>I don't use Instagram or Facebook but I've seen videos recorded on them trickling down to Reddit. It is a nifty way to do POV recording without a silly looking camera strapped to your head. </p>

<p>Seems the video and especially audio quality could use some improvements but it's still amazing tech for the packaging.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexqyin/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#nex0x4h" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_nexkcu2" onclick="click_thing(this)" data-fullname="t1_nexkcu2" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="Silent-Hyena9442" data-author-fullname="t2_82e9vusx" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexkcu2/"><p>[–]<a href="https://old.reddit.com/user/Any_Potato_7716">Any_Potato_7716</a><span></span> <span title="1">1 point</span><span title="2">2 points</span><span title="3">3 points</span> <time title="Thu Sep 18 18:33:37 2025 UTC" datetime="2025-09-18T18:33:37+00:00">3 hours ago</time><time title="last edited 1 hour ago" datetime="2025-09-18T20:48:34+00:00">*</time>&nbsp;(2 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nexm1cybw4"><div><p>I don’t really have any distrust of Apple nor Samsung themselves stealing my data. </p>

<p>I do with certain apps for sure, but not with my phone by itself.</p>

<p>If my phone was made by Facebook or Google, i’d certainly believe it’d be siphoning every bit of my data it could by itself. They have a long history of selling off every bit of your privacy. </p>

<p>That’s why I’m not gonna trust these Facebook glasses. </p>

<p>It’s not the phones that are the problem. It’s the apps, and the companies running them.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexm1cy/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#nexkcu2" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div></div><div id="thing_t1_newlebx" onclick="click_thing(this)" data-fullname="t1_newlebx" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="Eienkei" data-author-fullname="t2_128k9d" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/newlebx/"><p>[–]<a href="https://old.reddit.com/user/ChawulsBawkley">ChawulsBawkley</a><span></span> <span title="165">165 points</span><span title="166">166 points</span><span title="167">167 points</span> <time title="Thu Sep 18 16:16:52 2025 UTC" datetime="2025-09-18T16:16:52+00:00">6 hours ago</time>&nbsp;(23 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_newt4u1a9g"><div><p>Reminds me of some of those “real gameplay” demonstrations for game announcements. The dialogue between the co-op players was always unbearable. </p>

<p>It’s time to get serious guys, let’s bring in the heavy artillery! </p>

<p>It’s too quiet. Stay frosty Steve!</p>

<p>Woah there Tyler! Calm down! Save some of the action for me!</p>

<p>Not actual quotes, but just examples. What was that one that was absolutely terrible… Anthem or something?</p>
</div></form><ul><li><a href="https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/newt4u1/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#newlebx" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_newnn31" onclick="click_thing(this)" data-fullname="t1_newnn31" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="reffob" data-author-fullname="t2_ym61l" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/newnn31/"><div id="thing_t1_nexn1j6" onclick="click_thing(this)" data-fullname="t1_nexn1j6" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="bs000" data-author-fullname="t2_562jk" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexn1j6/"><p>[–]<a href="https://old.reddit.com/user/bs000">bs000</a><span></span> <span title="10">10 points</span><span title="11">11 points</span><span title="12">12 points</span> <time title="Thu Sep 18 18:38:30 2025 UTC" datetime="2025-09-18T18:38:30+00:00">3 hours ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nexn1j6jm5"><div><p>Watch the video again.</p>

<p>"You've already combined the base ingredients, so now grate a pear to add to the sauce."</p>

<p>"You've already combined the base ingredients, so now grate the pear and gently combine it with the base sauce."</p>
</div></form><ul><li><a href="https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexn1j6/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#nex3vmc" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_nexod2q" onclick="click_thing(this)" data-fullname="t1_nexod2q" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="thefpspower" data-author-fullname="t2_glpw94" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexod2q/"><p>[–]<a href="https://old.reddit.com/user/thefpspower">thefpspower</a><span></span> <span title="1">1 point</span><span title="2">2 points</span><span title="3">3 points</span> <time title="Thu Sep 18 18:44:50 2025 UTC" datetime="2025-09-18T18:44:50+00:00">3 hours ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nexod2qru9"><div><blockquote>
<p>AIs don't normally use the exact same responses if they get asked the same question more than once.&nbsp;</p>
</blockquote>

<p>In my experience that's false, I've had multiple situations where I ask a question, it answers, then I ask a similar question but with aditional details and the response sometimes is exactly the same because it keeps using the same context.</p>

<p>When that happens I have to start a new chat because nothing I say next gets a proper answer.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexod2q/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#nex3vmc" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div></div><div id="thing_t1_newmlrl" onclick="click_thing(this)" data-fullname="t1_newmlrl" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="SmoogzZ" data-author-fullname="t2_5bwb2n6l" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/newmlrl/"><div id="thing_t1_newqtp6" onclick="click_thing(this)" data-fullname="t1_newqtp6" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="Pervasivepeach" data-author-fullname="t2_g27hl" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/newqtp6/"><div><p>[–]<a href="https://old.reddit.com/user/Pervasivepeach">Pervasivepeach</a><span></span> <span title="39">39 points</span><span title="40">40 points</span><span title="41">41 points</span> <time title="Thu Sep 18 16:05:46 2025 UTC" datetime="2025-09-18T16:05:46+00:00">6 hours ago</time>&nbsp;(8 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_newqtp6nfq"><div><p>These kinds of issues are funny, but the ai bubble isn’t going anywhere, not when it’s become the modern day arms race between China and the US. Who both see ai as a necessity to stay economically competitive </p>

<p>Meta just dropped 20 billion on a brand new data center purely for ai, openAI are investing 500 billion over the next 4 years. And AI is at the cusp of being able to self improve, to the point where nobody is saying it won’t happen, the arguments just when it will happen.</p>

<p>I despise ai like everyone else. But it’s naive to think it’s going to go away on its own, not without constant pushback.</p>

<p>And it’s only going to get so much worse</p>
</div></form><ul><li><a href="https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/newqtp6/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#newmlrl" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="siteTable_t1_newqtp6"><div id="thing_t1_nexeaqa" onclick="click_thing(this)" data-fullname="t1_nexeaqa" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="seagulls51" data-author-fullname="t2_e5eba" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexeaqa/"><p>[–]<a href="https://old.reddit.com/user/Pervasivepeach">Pervasivepeach</a><span></span> <span title="3">3 points</span><span title="4">4 points</span><span title="5">5 points</span> <time title="Thu Sep 18 19:30:21 2025 UTC" datetime="2025-09-18T19:30:21+00:00">2 hours ago</time><time title="last edited 2 hours ago" datetime="2025-09-18T19:39:01+00:00">*</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nexxu3c9q9"><div><p>But ai isn’t just some startup craze like the .com bubble was. It has a lot more in common with the nuclear arms race of the Cold War. Right now ai is being pushed by the two major world superpowers not as a product, but as a means of defense and economic warfare. The US doesn’t care how many students use ChatGPT to cheat on homework. They care about its ability to decrypt nuclear launch codes. </p>

<p>For context the US government has invested over 300 billion in AI. In addition to 20+ billion from Facebook, 500 billion from open AI, 30 billion from Microsoft. I can go on forever. And none of this is including China who’s invested very similar numbers. We are talking trillions now. At its most the .com bubble was in the 300 billion range. </p>

<p>It isn’t about the bubble, or consumer use anymore, or even profits. It’s about having a smarter Ai Than China for defense. The department of defense is currently the biggest support of AI In the world, since they are responsible for 72% of the 300+ billion from the US. And they will continue spending.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexxu3c/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#nexeaqa" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_nex3m0f" onclick="click_thing(this)" data-fullname="t1_nex3m0f" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="Informal_Tennis8599" data-author-fullname="t2_1gcdwu7num" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nex3m0f/"><p>[–]<a href="https://old.reddit.com/user/Pervasivepeach">Pervasivepeach</a><span></span> <span title="2">2 points</span><span title="3">3 points</span><span title="4">4 points</span> <time title="Thu Sep 18 19:31:23 2025 UTC" datetime="2025-09-18T19:31:23+00:00">2 hours ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nexy1qeuez"><div><p>The point is the discussion has gone from “is this possible” to “how long”</p>

<p>Yes the debate for how long is up in air all the time, but even the biggest ai skeptics are no longer saying “it won’t happen”</p>

<p>That’s what’s scary</p>
</div></form><ul><li><a href="https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexy1qe/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#nex3m0f" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div></div></div><div id="thing_t1_nexhz2e" onclick="click_thing(this)" data-fullname="t1_nexhz2e" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="Colley619" data-author-fullname="t2_cp4nx" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexhz2e/"><p>[–]<a href="https://old.reddit.com/user/Colley619">Colley619</a><span></span> <span title="1">1 point</span><span title="2">2 points</span><span title="3">3 points</span> <time title="Thu Sep 18 18:14:02 2025 UTC" datetime="2025-09-18T18:14:02+00:00">4 hours ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nexhz2e2l5"><div><p>All that's popping is peoples perceptions of current "AI" and maybe realizing that current "AI" is not real artificial intelligence, it's just advanced search algorithms and advanced emulation tools. Key word <strong>emulation</strong>.</p>

<p>AI is a fancy buzzword that does not best describe these tools, and despite them not working how people think, they ARE still revolutionizing various industries by how they <strong>actually</strong> work.</p>

<p>I am bullish on current "AI" as a whole when used correctly, I am bearish on platforms trying to use them incorrectly.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexhz2e/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#newmlrl" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div></div><div id="thing_t1_newqki3" onclick="click_thing(this)" data-fullname="t1_newqki3" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="Remic75" data-author-fullname="t2_1736xx" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/newqki3/"><p>[–]<a href="https://old.reddit.com/user/Remic75">Remic75</a><span></span> <span title="20">20 points</span><span title="21">21 points</span><span title="22">22 points</span> <time title="Thu Sep 18 16:04:32 2025 UTC" datetime="2025-09-18T16:04:32+00:00">6 hours ago</time><time title="last edited 6 hours ago" datetime="2025-09-18T16:14:27+00:00">*</time>&nbsp;(9 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_newqki3aoi"><div><p>Looking at it, the guy likely went off script.</p>

<p>AI said "I love the setup you have, I SEE that you have soy sauce and others"</p>

<p>I think he was supposed to say something like "What can I make with this? Maybe something Korean inspired" or similar. Reason why I think this is because he asked Meta the question and Meta basically responded back with redundancy (look at what he had on the table and what the AI was saying) or it was doing a general search and not waiting for its “cue”. He knew he messed up which is why he immediately interrupted the AI to go to the next step. He likely said something off script which didn't make sense in the sequence.</p>

<p>But the problem was, the AI likely included what to do to prepare FIRST with the dialogue it was giving, but he interrupted. The script got confused because the scripted dialogue was just following what it was told. This guy bumped it off of alignment.</p>

<p>Could've likely been first time jitters, or stage fright. Regardless, bad demo.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/newqki3/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div><div id="thing_t1_nexrlf3" onclick="click_thing(this)" data-fullname="t1_nexrlf3" data-type="comment" data-gildings="0" data-subreddit="LivestreamFail" data-subreddit-prefixed="r/LivestreamFail" data-subreddit-fullname="t5_38jf0" data-subreddit-type="public" data-author="OdinsOneG00dEye" data-author-fullname="t2_8tcci8eb" data-replies="0" data-permalink="/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexrlf3/"><p>[–]<a href="https://old.reddit.com/user/OdinsOneG00dEye">OdinsOneG00dEye</a><span></span> <span title="0">0 points</span><span title="1">1 point</span><span title="2">2 points</span> <time title="Thu Sep 18 19:00:12 2025 UTC" datetime="2025-09-18T19:00:12+00:00">3 hours ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nexrlf3fz8"><div><p>He’s a non technical person I believe so like my parents - the WiFi being messed up is a go to thing to say for what can be a range of issues with actual hardware. </p>

<p>He’s done his best to end the demo and hand over preventing further cringe - good job on that front. </p>

<p>The self awareness meme from this should be putting one those glasses and asking “what do I do first?” - be nice to see some self deprecating humour from a tech giant company, this is some Apple would never do allowing Meta a more human touch to exist in their marketing of this device / service once released</p>
</div></form><ul><li><a href="https://old.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/nexrlf3/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple: SSH and FileVault (472 pts)]]></title>
            <link>https://keith.github.io/xcode-man-pages/apple_ssh_and_filevault.7.html</link>
            <guid>45294440</guid>
            <pubDate>Thu, 18 Sep 2025 20:15:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://keith.github.io/xcode-man-pages/apple_ssh_and_filevault.7.html">https://keith.github.io/xcode-man-pages/apple_ssh_and_filevault.7.html</a>, See on <a href="https://news.ycombinator.com/item?id=45294440">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<section>
<h2 id="NAME"><a href="#NAME">NAME</a></h2>
<p><code>apple_ssh_and_filevault</code> —
    <span>SSH and FileVault</span></p>
</section>
<section>
<h2 id="DESCRIPTION"><a href="#DESCRIPTION">DESCRIPTION</a></h2>
<p>When FileVault is enabled, the data volume is locked and
    unavailable during and after booting, until an account has been
    authenticated using a password. The macOS version of OpenSSH stores all of
    its configuration files, both system-wide and per-account, in the data
    volume. Therefore, the usually configured authentication methods and shell
    access are not available during this time. However, when Remote Login is
    enabled, it is possible to perform password authentication using SSH even in
    this situation. This can be used to unlock the data volume remotely over the
    network. However, it does not immediately permit an SSH session. Instead,
    once the data volume has been unlocked using this method, macOS will
    disconnect SSH briefly while it completes mounting the data volume and
    starting the remaining services dependent on it. Thereafter, SSH (and other
    enabled services) are fully available.</p>
</section>
<section>
<h2 id="HISTORY"><a href="#HISTORY">HISTORY</a></h2>
<p>The capability to unlock the data volume over SSH appeared in
    macOS 26 Tahoe.</p>
</section>
<section>
<h2 id="SEE_ALSO"><a href="#SEE_ALSO">SEE
  ALSO</a></h2>
<p><a>sshd(8)</a></p>
</section>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[President Says Broadcasters Should Lose Licenses for Criticizing Him (142 pts)]]></title>
            <link>https://www.nytimes.com/live/2025/09/18/us/trump-news</link>
            <guid>45294199</guid>
            <pubDate>Thu, 18 Sep 2025 19:54:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/live/2025/09/18/us/trump-news">https://www.nytimes.com/live/2025/09/18/us/trump-news</a>, See on <a href="https://news.ycombinator.com/item?id=45294199">Hacker News</a></p>
Couldn't get https://www.nytimes.com/live/2025/09/18/us/trump-news: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. already has the critical minerals it needs, according to new analysis (240 pts)]]></title>
            <link>https://www.minesnewsroom.com/news/us-already-has-critical-minerals-it-needs-theyre-being-thrown-away-new-analysis-shows</link>
            <guid>45294058</guid>
            <pubDate>Thu, 18 Sep 2025 19:41:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.minesnewsroom.com/news/us-already-has-critical-minerals-it-needs-theyre-being-thrown-away-new-analysis-shows">https://www.minesnewsroom.com/news/us-already-has-critical-minerals-it-needs-theyre-being-thrown-away-new-analysis-shows</a>, See on <a href="https://news.ycombinator.com/item?id=45294058">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      


            
      
            <p>In new Science article, Colorado School of Mines researchers call for more research, development and policy to increase critical mineral recovery</p>
      </div><div>
      
            <p>Colorado's Climax Mine, which produces produces approximately 30 million pounds of molybdenum every year, was among the U.S. mining operations evaluated in the critical minerals analysis published today in the journal Science.</p>
      
  </div><div><p>All the critical minerals the U.S. needs annually for energy, defense and technology applications are already being mined at existing U.S. facilities, according to a <a href="http://www.science.org/doi/10.1126/science.adw8977">new analysis published today</a> in the journal <em>Science</em>.</p><p>The catch? These minerals, such as cobalt, lithium, gallium and rare earth elements like neodymium and yttrium, are currently being discarded as tailings of other mineral streams like gold and zinc, said Elizabeth Holley, associate professor of <a href="https://mining.mines.edu/">mining engineering at Colorado School of Mines</a> and lead author of the new paper.</p><p>"The challenge lies in recovery," Holley said. "It's like getting salt out of bread dough – we need to do a lot more research, development and policy to make the recovery of these critical minerals economically feasible."</p><p>To conduct the analysis, Holley and her team built a database of annual production from federally permitted metal mines in the U.S. They used a statistical resampling technique to pair these data with the geochemical concentrations of critical minerals in ores, recently compiled by the <a href="https://usgs.gov/">U.S. Geological Survey</a>, Geoscience Australia and the Geologic Survey of Canada.</p><p>Using this approach, Holley’s team was able to estimate the quantities of critical minerals being mined and processed every year at U.S. metal mines but not being recovered. Instead, these valuable minerals are ending up as discarded tailings that must be stored and monitored to prevent environmental contamination.</p><p>“This is a brand-new view of ‘low hanging fruit’ – we show where each critical mineral exists and the sites at which even 1 percent recovery of a particular critical mineral could make a huge difference, in many cases dramatically reducing or even eliminating the need to import that mineral,” Holley said.</p><p>The analysis in <em>Science</em> looks at a total of 70 elements used in applications ranging from consumer electronics like cell phones to medical devices to satellites to renewable energy to fighter jets and shows that unrecovered byproducts from other U.S. mines could meet the demand for all but two – platinum and palladium.</p><p>Among the elements included in the analysis are:</p><ul><li><strong>Cobalt (Co): </strong>The lustrous bluish-gray metal, a key component in electric car batteries, is a byproduct of nickel and copper mining. Recovering less than 10 percent of the cobalt currently being mined and processed but not recovered would be more than enough to fuel the entire U.S. battery market.</li><li><strong>Germanium (Ge):</strong> The brittle silvery-white semi-metal used for electronics and infrared optics, including sensors on missiles and defense satellites, is present in zinc and molybdenum mines. If the U.S. recovered less than 1 percent of the germanium currently mined and processed but not recovered from U.S. mines, it would not have to import any germanium to meet industry needs.</li></ul><p>The benefits of enhanced recovery are not only economic and geopolitical but also environmental, Holley said – recovering these critical minerals instead of sending them to tailings piles would reduce the environmental impact of mine waste and open more opportunities for reuse in construction and other industries.</p><p>“Now that we know which sites are low-hanging fruit, we need to conduct detailed analyses of the minerals in which these chemical elements reside and then test the technologies suitable for recovery of those elements from those specific minerals,” Holley said. “We also need policies that incentivize mine operators to incorporate additional processing infrastructure. Although these elements are needed, their market value may not be sufficient to motivate operators to invest in new equipment and processes without the right policies in place.”</p><p>Co-authors on the paper are Karlie Hadden, PhD candidate in <a href="https://geology.mines.edu/">geology</a>; Dorit Hammerling, associate professor of <a href="https://ams.mines.edu/">applied mathematics and statistics</a>; Rod Eggert, research professor of <a href="https://econbus.mines.edu/">economics and business</a>; Erik Spiller, research professor of mining engineering; and Priscilla Nelson, professor of mining engineering.</p><p>Read the full paper, <a href="http://www.science.org/doi/10.1126/science.adw8997">"Byproduct recovery from US metal mines could reduce import reliance for critical minerals,"</a> on the <em>Science</em> website. To access the data and figures before the paper appears in print, contact Mines Media Relations Specialist Erich Kirshner at <a href="mailto:erich.kirshner@mines.edu">erich.kirshner@mines.edu</a>.&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When Knowing Someone at Meta Is the Only Way to Break Out of "Content Jail" (293 pts)]]></title>
            <link>https://www.eff.org/pages/when-knowing-someone-meta-only-way-break-out-content-jail</link>
            <guid>45293273</guid>
            <pubDate>Thu, 18 Sep 2025 18:30:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/pages/when-knowing-someone-meta-only-way-break-out-content-jail">https://www.eff.org/pages/when-knowing-someone-meta-only-way-break-out-content-jail</a>, See on <a href="https://news.ycombinator.com/item?id=45293273">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p><strong>BY&nbsp;<a href="https://www.eff.org/about/staff/rindala-alajaji" target="_blank" rel="noopener noreferrer">RINDALA ALAJAJI</a></strong> | September 17, 2025</p>
<p><i><span>This is the second instalment in a ten-part blog series documenting EFF's findings from the </span></i><a href="https://www.eff.org/deeplinks/2025/02/stop-censoring-abortion-fight-reproductive-rights-digital-age"><i><span>Stop Censoring Abortion</span></i></a><i><span> campaign. You can read additional posts </span></i><a href="https://www.eff.org/pages/stop-censoring-abortion"><i><span>here</span></i></a><i><span>.</span></i><span>&nbsp;</span></p>
<p><span>During our </span><a href="https://www.eff.org/pages/stop-censoring-abortion"><span>Stop Censoring Abortion</span></a><span> campaign, we set out to collect and spotlight the growing number of stories from people and organizations that have had abortion-related content removed, suppressed, or flagged by dominant social media platforms. Our survey submissions have revealed some alarming trends, including: </span><b>if you don’t have a personal or second-degree connection at Meta, your chances of restoring your content or account are likely to drop significantly.&nbsp;</b></p>
<p><span>Through the survey, we heard from activists, clinics, and researchers whose accounts were suspended or permanently removed for allegedly violating Meta’s </span><a href="https://transparency.meta.com/policies/community-standards/restricted-goods-services/"><span>policies on promoting or selling “restricted goods,”</span></a><span> even when their posts were purely educational or informational. What the submissions also showed is a pattern of overenforcement, lack of transparency, and arbitrary moderation decisions that have specifically affected reproductive health and reproductive justice advocates.&nbsp;</span></p>
<p><span>When accounts are taken down, appeals can take days, weeks, or even months (if they're even resolved at all, or if users are even given the option to appeal). For organizations and providers, this means losing access to vital communication tools and being cut off from the communities they serve. This is highly damaging since so much of that interaction happens on Meta’s platforms. Yet we saw a disturbing pattern emerge in our survey: on several occasions, accounts are swiftly restored once someone with a connection to Meta intervenes.</span></p>
<h2><b>The</b> <b>Case Studies: An Abortion Clinic</b></h2>
<p><a href="https://www.redriverwomensclinic.com/"><span>The Red River Women's Clinic</span></a><span> is an abortion clinic in Moorhead, MN. It was originally located in Fargo, North Dakota, and for many years was the only abortion clinic in North Dakota. In early January, the clinic’s director heard from a patient that she thought they only offered procedural/surgical abortions and not medication abortion. To clarify for other patients, they posted on the clinic’s page that they offered both procedural and medication abortions—attaching an image of a box of mifepristone. When they tried to boost the post, the ad was flagged and their account was suspended.</span></p>
<p><span>They appealed the decision and initially got the ad approved, yet the page was suspended again shortly after. But this time, multiple appeals and direct emails went unanswered,&nbsp;</span><span>until they reached out to a digital rights organization that</span><span>&nbsp;was able to connect with staff at Meta that stepped in. Only then was their page restored, with Meta noting that their post did not violate the policies but warning that future violations could lead to permanent removal.</span></p>
<p><span>While this may have been a glitch in Meta’s systems or a misapplication of policy, the suspension of the clinic’s Facebook account was detrimental for them. “We were unable to update our followers about dates/times we were closed, we were unable to share important information and news about abortion that would have kept our followers up to date, there was a legislative session happening and we were unable to share events and timely asks for reaching out to legislators about issues,” shared Tammi Kromenaker, Director of Red River Women's Clinic. The clinic was also prevented from starting an Instagram page due to the suspension. “Facebook has a certain audience and Instagram has another audience,” said Kromenaker, “we are trying to cater to all of our supporters so the loss of FB and the inability to access and start an Instagram account were really troubling to us.”&nbsp;</span></p>
<h2><b>The Case Studies: RISE at Emory University</b></h2>
<p><a href="https://rise.emory.edu/"><span>RISE, a reproductive health research center at Emory University</span></a><span>, launched an Instagram account to share community-centered research and combat misinformation related to reproductive health. In January of this year, they posted educational content about mifepristone on their instagram. “Let's talk about Mifepristone + its uses + the importance of access”, read the post. Two months later, their account was suddenly suspended, flagging the account under its policy against selling illegal drugs. Their appeal was denied, which led to the account being permanently deleted.&nbsp;</span></p>
<div><p><img src="https://www.eff.org/files/2025/09/12/screenshot_2025-09-12_at_12.26.09_pm_0.png" width="1272" height="820" alt="A screenshot of an instagram post from @emory.rise that reads &quot;let's talk about mifepristone&quot; in bold black font &quot;+ its uses + the importance of access&quot; in blue" title="A screenshot of an instagram post from @emory.rise that reads &quot;let's talk about mifepristone&quot; in bold black font &quot;+ its uses + the importance of access&quot; in blue"></p><p>Screenshot submitted by RISE to EFF</p></div>
<p><span>“As a team, this was a hit to our morale” shared Sara Redd, Director of Research Translation at </span><i><span>RISE</span></i><span>. “We pour countless hours of person-power, creativity, and passion into creating the content we have on our page, and having it vanish virtually overnight took a toll on our team.” For many organizational users like RISE, their social media accounts are a repository for resources and metrics that may not be stored elsewhere. “We spent a significant amount of already-constrained team capacity attempting to recover all of the content we’d created for Instagram that was potentially going to be permanently lost. [...] We also spent a significant amount of time and energy trying to understand what options we might have available from Meta to appeal our case and/or recover our account; their support options are not easily accessible, and the time it took to navigate this issue distracted from our existing work.”&nbsp;&nbsp;</span></p>
<p><span>Meta restored the account only after RISE was able to connect with someone there. Once RISE logged back in, they confirmed that the flagged post was the one about mifepristone. The post never sold or directed people where to buy pills, it simply provided accurate information about the use and efficacy of the drug.&nbsp;</span></p>
<h2><b>This Shouldn’t Be How Content Moderation Works</b></h2>
<p><span>Meta spokespersons have admitted to instances of “overenforcement” </span><a href="https://www.theverge.com/2025/1/24/24350967/metas-instagram-facebook-abortion-access-information-blocking-banning"><span>in various</span></a> <a href="https://www.tortoisemedia.com/2025/01/27/abortion-pill-information-censored-online-1"><span>press statements</span></a><span>, noting that content is sometimes incorrectly removed or blurred even when it doesn’t actually violate policy. Meta has insisted to the public that they care about free speech, as a spokesperson mentioned </span><a href="https://www.nytimes.com/2024/06/11/business/abortion-groups-tech-platforms.html"><span>to The New York Times</span></a><span>: “We want our platforms to be a place where people can access reliable information about health services, advertisers can promote health services and everyone can discuss and debate public policies in this space [...] That’s why we allow posts and ads about, discussing and debating abortion.” In fact, </span><a href="https://www.facebook.com/business/help/263390265553560?id=434838534925385"><span>their platform policies directly mention this</span></a><span>:&nbsp;</span></p>
<blockquote><p><i><span>Note that advertisers don’t need authorization to run ads that only:</span></i></p>
<ul>
<li><i><span>Educate, advocate or give public service announcements related to prescription drugs</span></i></li>
</ul>
</blockquote>
<p><a href="https://transparency.meta.com/policies/community-standards/restricted-goods-services/"><span>Additionally</span></a><span>:&nbsp;</span></p>
<blockquote><p><i><span>Note: Debating or advocating for the legality or discussing scientific or medical merits of prescription drugs is allowed. This includes news and public service announcements.&nbsp;</span></i></p>
</blockquote>
<p><span>Meta also has policies specific to “</span><a href="https://transparency.meta.com/policies/ad-standards/restricted-goods-services/health-wellness"><span>Health and Wellness,</span></a><span>” where they state:&nbsp;</span></p>
<blockquote><p><i><span>When targeting people 18 years or older, advertisers can run ads that:</span></i></p>
<ul>
<li><i><span>Promote sexual and reproductive health and wellness products or services, as long as the focus is on health and the medical efficacy of the product or the service and not on the sexual pleasure or enhancement. And these ads must target people 18 years or older. This includes ads for: [...]</span></i></li>
<li><span>Family planning methods, such as:</span>
<ul>
<li><span>Family planning clinics</span></li>
<li><span>In Vitro Fertilization (IVF) or any other artificial insemination procedures</span></li>
<li><span>Fertility awareness</span></li>
<li><strong>Abortion medical consultation and related services</strong></li>
</ul>
</li>
</ul>
</blockquote>
<p><span>But these public commitments don’t always match users’ experiences.&nbsp;</span></p>
<p><span>Take the </span><a href="https://www.nytimes.com/2025/01/23/technology/instagram-facebook-abortion-pill-providers.html"><span>widely</span></a> <a href="https://jessica.substack.com/p/instagram-is-censoring-abortion-pill"><span>covered</span></a><span> case of </span><a href="https://aidaccess.org/en/"><span>Aid Access</span></a><span>, a group that provides medication abortion by mail. This year, several of their Instagram posts were blurred and removed on Instagram, including one with tips for feeling safe and supported at home after taking abortion medication. But only after multiple national media outlets contacted Meta for comment on the story were the posts and account restored.</span></p>
<p><span>So the question becomes: If Meta admits its enforcement isn’t perfect, why does it still take knowing someone, or having the media involved, to get a fair review? When companies like Meta claim to uphold </span><a href="https://about.fb.com/news/2025/01/meta-more-speech-fewer-mistakes/"><span>commitments to free speech</span></a><span>, those commitments </span><a href="https://www.eff.org/deeplinks/2025/01/metas-new-content-policy-will-harm-vulnerable-users-if-it-really-valued-free"><span>should materialize</span></a><span> in clear policies that are enforced equally, not only when it is escalated through leveraging relationships with Meta personnel.</span></p>
<h2><b>“Facebook Jail” Reform</b></h2>
<p><span>There is no question that the enforcement of these content moderation policies on Meta platforms and the length of time people are spending in “content jail” or “Facebook/Instagram jail” has created a </span><a href="https://www.thefire.org/research-learn/chilling-effect-overview"><span>chilling effect</span></a><span>.&nbsp;</span></p>
<p><span>“I think that I am more cautious and aware that the 6.1K followers we have built up over time could be taken away at any time based on the whims of Meta,” Tammi from Red River Women’s Clinic told us.&nbsp;</span></p>
<p><span>RISE sees it in a slightly different light, sharing that “[w]hile this experience has not affected our fundamental values and commitment to sharing our work and rigorous science, it has highlighted for us that no information posted on a third-party platform is entirely one’s own, and thus can be dismantled at any moment.”</span></p>
<p><span>At the end of the day, clinics are left afraid to post basic information, patients are left confused or misinformed, and researchers lose access to their audiences. But unless your issue catches the attention of a journalist or you know someone at Meta, you might never regain access to your account.</span></p>
<p><span>These case studies highlight the urgent need for transparent, equitable, and timely enforcement that is not dependent on insider connections, as well as accountability from platforms that claim to support open dialogue and free speech. Meta’s admitted overenforcement should, at minimum, be coupled with efficient and well-staffed review processes and policies that are transparent and easily understandable.&nbsp;</span></p>
<p><span>It’s time for Meta and other social media platforms to implement the reforms they claim to support, and for them to prove that protecting access to vital health information doesn’t hinge on who you know.</span></p>
<p><i><span>This is the second post in our blog series documenting the findings from our Stop Censoring Abortion campaign. Read more in the series: </span></i><i><span>https://www.eff.org/pages/stop-censoring-abortion</span></i><i><span>&nbsp; </span></i><span>&nbsp;</span></p>

</div>

          </article>
    </div></div>]]></description>
        </item>
    </channel>
</rss>