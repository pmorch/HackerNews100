<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 23 Oct 2024 10:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Arm is canceling Qualcomm's chip design license (341 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-10-23/arm-to-cancel-qualcomm-chip-design-license-in-escalation-of-feud</link>
            <guid>41920401</guid>
            <pubDate>Wed, 23 Oct 2024 00:48:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-10-23/arm-to-cancel-qualcomm-chip-design-license-in-escalation-of-feud">https://www.bloomberg.com/news/articles/2024-10-23/arm-to-cancel-qualcomm-chip-design-license-in-escalation-of-feud</a>, See on <a href="https://news.ycombinator.com/item?id=41920401">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Forest Service Is Losing 2,400 Jobs–Including Most of Its Trail Workers (210 pts)]]></title>
            <link>https://www.backpacker.com/news-and-events/news/us-forest-service-job-eliminations-trail-workers/</link>
            <guid>41920127</guid>
            <pubDate>Wed, 23 Oct 2024 00:13:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.backpacker.com/news-and-events/news/us-forest-service-job-eliminations-trail-workers/">https://www.backpacker.com/news-and-events/news/us-forest-service-job-eliminations-trail-workers/</a>, See on <a href="https://news.ycombinator.com/item?id=41920127">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-inject-ads="" data-inject-ads-options="{ &quot;markup&quot;: [&quot;<div class='js-ad js-hide-elements js-visible-signed-out js-scroll-load o-ad c-ad--inline' data-child-zone='in-content-leaderboard-bp' data-size='[\&quot;fluid\&quot;,[300,50],[300,100],[728,90],[320,50],[320,100],[468,60],[300,250],[1,1]]' data-size-map='[[[1145,0],[\&quot;fluid\&quot;,[728,90],[468,60],[1,1]]],[[850,0],[\&quot;fluid\&quot;,[468,60],[1,1]]],[[0,0],[\&quot;fluid\&quot;,[300,250],[320,100],[300,100],[320,50],[300,50],[1,1]]]]' data-hide-elements='.o-banner-ad__vertical .js-ad'></div>&quot;], &quot;filter&quot;: { &quot;nextExceptions&quot;: &quot;img, blockquote, div&quot;, &quot;nextContainsExceptions&quot;: &quot;img, blockquote, a.btn, a.o-button&quot;} }">
              
  <p>
      Heading out the door? Read this article on the new Outside+ app available now on iOS devices for members!
      <a href="https://outsideapp.onelink.me/wOhi/6wh1kbvw" data-analytics-event="click" data-analytics-data="{&quot;name&quot;:&quot;Element Clicked&quot;,&quot;props&quot;:{&quot;destination_url&quot;:&quot;https://outsideapp.onelink.me/wOhi/6wh1kbvw&quot;,&quot;domain&quot;:&quot;<<analytics_vars.domain>>&quot;,&quot;name&quot;:&quot;in-content-cta&quot;,&quot;type&quot;:&quot;link&quot;}}">Download the app</a>.
    </p>
                    
      
      <p>The U.S. Forest Service is a federal agency that manages 193 million acres of land, an area about the size of Texas. Next year, the agency will have to manage that land without its seasonal workforce. In September, <a data-afl-p="0" href="https://www.fs.usda.gov/inside-fs/delivering-mission/excel/september-2024-chiefs-all-employee-call-hiring-update">the agency announced</a> that it would be suspending all seasonal hiring for the 2025 season, a decision that will cut about 2,400 jobs. Nearly all of those positions are field-based jobs, ranging from biologists and timber workers to trail technicians and recreation staff. In addition, the agency is freezing all external hiring for permanent positions. The only exception to the hiring freeze are the roughly 11,300 firefighters hired by the agency every year.</p>
<p>According to the agency and its partners, the effects of these staffing cuts will be far-ranging and severe. In the September 17 all-employee call where he announced the hiring freeze, Forest Service Chief Randy Moore said “We just can’t get the same work done with fewer employees.” Though the Forest Service has been shedding jobs for decades—about 8,000 jobs in the last 20 years, Moore said—this will be the largest single-year staff cut in recent memory.</p>
<p>Seasonal employees perform vital fieldwork and research that extends beyond what many Americans consider the jurisdiction of the Forest Service. Rangers patrol whitewater rivers, rock climbing crags, and dangerous alpine summits. Biologists staff critical salmon fisheries. <a data-afl-p="0" href="https://www.backpacker.com/news-and-events/news/appalachian-trail-hurricane-helene-damage/">Recreation crews maintain forest roads</a> and clean camp latrines. Employees of all types chip in as emergency firefighters when required. According to the American Avalanche Association, the staff cuts could leave some avalanche centers, which rely on the Forest Service for funding, understaffed this winter.</p>
<p>And then there are trails. According to the Government Accountability Office, the Forest Service has had a maintenance backlog for more than a decade, and oversees more miles of trail than it can maintain. Cutting the majority of its field-going trail staff will only make the issue worse.</p>
<p>“This policy will result in a burgeoning of the <a data-afl-p="0" href="https://www.backpacker.com/stories/shortage-trail-builders-these-programs-can-help/">trail maintenance backlog</a>, both through lack of Forest Service staff attention to trail maintenance, but also through the loss of connection and relationships with partner organizations,” Mike Passo, the executive director of American Trails, a non-profit Forest Service partner, said in an email.</p>
<p><i>Backpacker </i>spoke to <a data-afl-p="0" href="https://www.backpacker.com/skills/the-12-best-jobs-in-the-outdoors/">nearly a dozen permanent and seasonal Forest Service employees</a>, most on condition of anonymity, about their experiences with the staffing cuts. Several expressed concern that trail crews would simply be unable to operate. They described crews of six seasonal employees disappearing, leaving one or two permanent crew leaders left trying to make things work. One intern in the National Pathways program, designed to automatically place successful interns into a full-time position with the agency, said she’s been told her job offer will likely be revoked. Other trail workers at conservation corps and non-profits who saw Forest Service positions as a step up the career ladder are rethinking their priorities.</p>
<p>Danica Mooney-Jones, a trail crew leader who’s been with the Forest Service since 2021, is among those out of a job next year. Where she works, the trail crew staff will go from five to two, and the broader recreation program is being cut from 13 employees to just four.</p>
<figure id="attachment_140967"><img data-lazy-load="" decoding="async" fetchpriority="high" data-src="https://cdn.backpacker.com/wp-content/uploads/2024/10/trail-crew-in-backcountry.jpg?width=730" alt="trail crew on trail" width="2400" height="1350" data-srcset="https://cdn.backpacker.com/wp-content/uploads/2024/10/trail-crew-in-backcountry.jpg?width=300 300w, https://cdn.backpacker.com/wp-content/uploads/2024/10/trail-crew-in-backcountry.jpg?width=768 768w" src="https://cdn.backpacker.com/wp-content/uploads/2024/10/trail-crew-in-backcountry.jpg?width=730"><figcaption><span>Trail workers on Cottonwood Pass in the Inyo National Forest</span> (Photo: USDA Forest Service)</figcaption></figure>
<p>“I moved across the country to work here, for a seasonal job,” she says. “We have people who have worked here for 10 years as seasonals, and made a career out of these positions. They trusted that the jobs wouldn’t go away.”</p>
<p>Now, she and her former co-workers have a tough choice to make: leave their communities to find a job in trails somewhere else, or stay put and find a new career. Mooney Jones considers herself lucky; armed with <a data-afl-p="0" href="https://www.backpacker.com/skills/outdoor-first-aid/these-wilderness-medicine-myths-need-to-disappear/">wilderness EMT training</a>, she found a local winter job as a ski patroller. Still, the idea of leaving the Forest Service behind for good is sobering.</p>
<p>“I’d be really sad if this was the end of my trail career,” says Mooney-Jones. “I really love doing the work, I love seeing the product, and I’m very proud of the work that we do.”</p>
<p>Trail maintenance is important every season, but 2025 may prove an especially difficult year to cut down on the workers who make it happen. After Hurricane Helene, southern portions of the Appalachian Trail are closed due to blowdowns, landslides, and washed out bridges. According to the Appalachian Trail Conservancy, there are more than 2,000 trees to clear from the AT in Tennessee alone, and many Forest Service access roads from Georgia to Virginia are closed due to erosion and rockfall.</p>
<p>That’s just on the AT—a popular long-distance trail supported by a non-profit organization and hundreds of trained volunteers. Elsewhere in the southern US, lesser-known trails face similar conditions but rely solely on Forest Service staff in order to re-open.</p>
<p>The cuts also left employees and partners wondering how the budget shortfall became so dire after several promising years of funding increases.</p>
<p>In 2021, the Biden administration mandated a $15 per hour minimum wage for all federal employees, which raised wages for some entry-level Forest Service jobs. Over the past several years, the agency also converted about 1,300 seasonal non-fire positions into permanent jobs. <a data-afl-p="0" href="https://www.backpacker.com/stories/issues/environment/on-the-front-lines-with-the-wildland-firefighters-protecting-the-west/">Wildland firefighters</a>, who now make up about half of the Forest Service’s workforce, received bonuses of up to $20,000 per year, which were temporarily funded through the Bipartisan Infrastructure Act. Several Forest Service employees said there was hope that pay raises for firefighters would eventually translate into raises for other field-going employees, as well.</p>
<p>But those short-term gains have all but disappeared, replaced by a sudden budget shortfall.</p>
<p>In March, the Forest Service requested $8.9 billion in funding, a $500 million jump from 2024’s $8.37 billion. By the summer, it was clear the agency was unlikely to receive it. In August, Forest Service Chief Randy Moore released a statement preparing the USFS for a reduced budget. With little evidence that Congress would pass a bill funding the government by the end of the year, Moore said in the September 17 all-employee call that “[the Forest Service] has an obligation to plan for the most conservative funding possibility.” A week later, Congress passed a continuing resolution that extended the 2024 funding levels through December 20.</p>
<p>The lowest number Moore referred to comes from the proposal from the House Interior Appropriations Committee, which sets spending limits for all federal land management agencies, including the Forest Service and National Park Service. This year’s proposal includes $8.43 billion for the Forest Service—technically a modest increase compared to 2024. But last year’s budget was boosted by an additional $945 million through pandemic-era stimulus bills, a funding source that has since dried up. And while the House proposal fully funds the firefighter pay raises, the proposed budget would still necessitate cuts elsewhere at the agency. All of these details muddy the financial picture, but compared to total funding in 2024, the agency could face a budget hole of nearly a billion dollars next year.</p>
<p>Because the Forest Service’s budget for next year is still not finalized, there is a chance the agency will fill some seasonal positions in the near future. “We are working closely with individual partners to explore creative solutions to fill gaps where we can. And we hope to have more hiring options in the coming year if additional funding becomes available,” Scott Owen, national press officer for the Forest Service, wrote in an email.</p>
<p>Even with these sobering financial details, it’s clear that the agency’s decision to balance the books by cutting seasonal jobs came as a shock to many employees.</p>
<p>“My trust has definitely taken a hit,” says Mooney-Jones. “I’d consider coming back to the Forest Service, but I’m not sure I could. It’s a balancing act between how I feel about how we’ve been treated and how much I love the forest.”</p>


      

      

  
    
      
  
      
      
                    
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How the Unchecked Power of Companies Is Destabilizing Governance (274 pts)]]></title>
            <link>https://hai.stanford.edu/news/tech-coup-new-book-shows-how-unchecked-power-companies-destabilizing-governance</link>
            <guid>41919907</guid>
            <pubDate>Tue, 22 Oct 2024 23:42:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hai.stanford.edu/news/tech-coup-new-book-shows-how-unchecked-power-companies-destabilizing-governance">https://hai.stanford.edu/news/tech-coup-new-book-shows-how-unchecked-power-companies-destabilizing-governance</a>, See on <a href="https://news.ycombinator.com/item?id=41919907">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p dir="ltr"><span>In&nbsp;</span><a href="https://press.princeton.edu/books/hardcover/9780691241173/the-tech-coup?srsltid=AfmBOorex6WwTNLG37fSSqw6a9FpDMtflkYNCSoO8sR3KfjYrV0-3HiL"><em>The Tech Coup: How to Save Democracy from Silicon Valley</em></a><em>,&nbsp;</em><span>Stanford institute for Human-Centered AI Policy Fellow</span><a href="https://hai.stanford.edu/people/marietje-schaake"><span>&nbsp;Marietje Schaake</span></a><span> aims to raise public awareness about a serious threat: the unchecked power of private companies. In addition to exerting vast economic power, tech companies are stepping into roles normally preserved for governments and wreaking havoc on the democratic rule of law in the process. From cybersecurity to systems used for policing, elections, and military defense policy, tech companies large and small are playing an outsized role.&nbsp;</span></p><p dir="ltr"><span>Her book illustrates this thesis with examples drawn from her experiences as a member of the European Parliament from 2009 to 2019 and as an engaged observer of tech culture while a fellow at</span><a href="https://cyber.fsi.stanford.edu/"><span>&nbsp;Stanford’s Cyber Policy Center</span></a><span> and&nbsp;</span><a href="https://hai.stanford.edu/"><span>Stanford HAI</span></a><span>.</span></p><p dir="ltr"><span>Here, Schaake discusses why ceding power to tech companies hurts democracy and outlines a few solutions from her book.</span></p><p dir="ltr"><strong>In what ways are private companies increasingly taking on functions normally assumed by states?</strong></p><p dir="ltr"><span>In the digital realm, companies’ control of information, unfettered agency, and power to act have almost overtaken that of governments.&nbsp;</span></p><p><img data-entity-uuid="a6752446-11f5-443e-b62b-703202bf1efa" data-entity-type="file" src="https://hai.stanford.edu/sites/default/files/inline-images/the%20tech%20coup_0.jpg" width="600" height="912" alt="cover of the book the tech coup" loading="lazy"></p><p dir="ltr"><span>For example, in the private intelligence sector, companies like NSO Group Technologies with its Pegasus spyware products are creating and selling the capability to hack into people’s devices. This means that anyone with the financial resources to purchase Pegasus spyware can access the capabilities of intelligence services and hack into the very private information of political opponents, judges, journalists, critical employees, competitors, and others.&nbsp;</span></p><p dir="ltr"><span>Another striking example is that of offensive cyber capabilities. In the name of defending their clients or their networks, companies are attacking hackers across borders, using “offense as defense.”</span></p><p dir="ltr"><span>And notice that I’m talking not only about big tech companies</span><strong>&nbsp;</strong><span>but also small ones, because there’s de facto power that comes from the development of digital technologies. Companies like NSO, or Clearview with its facial recognition software, or companies producing election technologies – these are not so-called Big Tech, but they are just as illustrative of some of the challenges that I’m pointing to, [such] as Elon Musk deciding who in Ukraine should and should not have access to Starlink internet connections.</span></p><p dir="ltr"><span>So, we see that these capabilities, these decisions, and these powers that used to be the exclusive domain of the state are now seeping into the hands of private companies, but without the checks and balances that we want in societies that function by the rule of law.</span></p><p dir="ltr"><strong>What can be done to put democratic entities back in charge?</strong></p><p dir="ltr"><span>First of all, there needs to be more awareness and understanding of the ways in which companies exert power over governance, democracy, and international law. And then we need to bring the same level of legal clarity, accountability mechanisms, and transparency measures to the digital realm that we expect around other innovations such as medicines, chemicals, foods, cars, or even processes such as the decision to engage in foreign conflict.&nbsp;</span></p><p dir="ltr"><span>Let’s compare how the U.S. responded to the Ukraine war in the physical world versus in the cyber domain. As part of NATO, the U.S. is clear: It doesn’t want to see boots on the ground. But in the cyber domain, the U.S.’s offensive activities are ongoing. That political discrepancy can continue because of the legal gray zone in the digital realm.</span></p><p dir="ltr"><span>So, I’m saying that the legal frameworks that we rely on in other industries – as well as the approaches we take to the physical side of foreign conflict – need to be on par in the digital realm and need to keep pace with where the technology has gone. It’s basically about catching up through international law, regulations, and enforcement, to make sure that the ideas that we have about what a democratic mandate looks like, what accountability looks like, and what oversight looks like are actually meaningful whenever activities happen in the digital sphere.</span></p><p dir="ltr"><strong>To</strong><span>&nbsp;</span><strong>some extent, you’re calling for implementing traditional types of laws and regulations in the digital realm, but is there also a way in which you’re calling for something new – a reinvention of democratic governments to handle the challenges of digitization?</strong></p><p dir="ltr"><span>Yes,</span><strong>&nbsp;</strong><span>there are a number of novel large- and small-scale changes I would recommend. For example, just as legislatures rely on independent legal teams to help draft legislation that will survive court challenges, they also need independent technology experts they can turn to for reliable information. Making tech expertise available to lawmakers would go a long way toward reducing lobbyists’ effectiveness and ensuring lawmakers understand how technology impacts issues like healthcare, education, justice, housing, and transportation. Because tech touches everything, lawmakers can’t get a handle on it by sitting in a single committee. They need to consult independent experts.&nbsp;</span></p><p dir="ltr"><span>Another example: Governments are increasingly outsourcing all kinds of processes to tech companies. And if a tech company operates in the name of a government, it should be as accountable as the government. I call this “the public accountability extension.” It sounds simple, but it would be a huge game changer. Right now, as governments outsource more and more critical governmental functions to tech companies, they also offload governmental accountability.</span></p><p dir="ltr"><span>For example, in many jurisdictions, the police are not allowed to hack into the devices of suspects. Instead, they engage a hacking company that will do it for them. The police can then say they don’t hack, but they actually gain access by alternate means. Similarly, state and local governments in the United States aren’t constitutionally allowed to discriminate between citizens on the basis of sensitive categories, but oftentimes the technology they use does exactly that without being held accountable. A third example: Freedom of Information Act (FOIA) requests. Journalists have the right to know what the government does in citizens’ name, but sometimes governments hire private companies to do government work or to collect and store government information, and if the companies don’t keep the information at the standard required of public entities, or if the companies are reluctant to provide information due to proprietary concerns, then the effectiveness of FOIA erodes.</span></p><p dir="ltr"><span>So, there are many examples of how governments essentially hide behind companies, including tech companies, to avoid accountability, and the accountability extension would address that.</span></p><p dir="ltr"><strong>In your book, you point out another tension: tech companies’ energy use in the communities they enter. You note that the public needs greater insight to provide greater oversight over the data centers underlying our digital lives. What do you mean?</strong></p><p dir="ltr"><span>There are no standards or reporting obligations requiring companies to say how much energy or water they’re using or plan to use. We have estimates from individual cases, but we don’t know the sum of data centers’ energy use.</span></p><p dir="ltr"><span>And often, big tech companies seeking to build data centers in a community will bid for those projects under a pseudonym – a company name that hides the fact that Amazon, Google, or Microsoft is behind the bid. The lawyers and consultants that tech companies hire to make these bids often paint a rosy picture of all the economic benefits that will accrue to communities that allow data centers to be built. They do their best to hide critical information about who they are and the data centers’ energy needs. And this complete lack of transparency regarding the use of scarce resources hinders transparent and good governance.</span></p><p dir="ltr"><span>We had some cases in the Netherlands where this was a big problem. Part-time city councilors had to make decisions about whether or not to allow a hyperscale data center in their territories. And they were up against billion-dollar companies and all their lawyers, accountants, consultants, and PR firms. The power asymmetry was enormous, and I think by standardizing the transparency and reporting requirements – including who is behind the projects and the metrics of energy use – this could become a much more fair public debate about the types of data centers a community can feasibly host.</span></p><p dir="ltr"><span>This increased transparency doesn’t begin to address whether society even wants more of these data centers using up scarce energy resources in an era of climate change. But we can’t answer that cost-benefit question if we don’t know what the cost is and all we’re told is benefit, benefit, benefit in some shady presentation.</span></p><p dir="ltr"><span>And of course, the other real issue, especially in the U.S., is the capability of the power grids. In the Netherlands, which is an advanced economy, as in the U.S. and the U.K., we’ve already seen reports that the grids are functioning at near-emergency levels: code red. The grids are stretched to their limits. They break down and outages are more frequent. And yet there are many data centers in the pipeline that were agreed to years ago. When they come online in two or three years, we may face a wave of disaster.</span></p><p dir="ltr"><strong>Can you talk about the precautionary principle that’s already part of EU law and how it might help rein in tech company power, especially AI?</strong></p><p dir="ltr"><span>Basically, the precautionary principle requires a pause to assess the societal impact of an innovation when it is new and before it is widely deployed or implemented. It’s enshrined in EU law but hasn’t been invoked by the authorities for tech innovations like AI because there’s such a push to use AI and also because the EU has now passed a separate law that deals with AI.</span></p><p dir="ltr"><span>But I think the precautionary principle is a useful concept to address situations where tech company engineers are themselves surprised by the behavior of AI models, or where society and lawmakers and citizens don’t know or understand how a particular innovation will impact their lives. So we’d like to create processes that actually serve the public by using the precautionary principle to assess and research before unintended but still preventable problems spread too widely.</span></p><p dir="ltr"><strong>Have Americans been slow to understand the threat that tech companies pose to democracy?</strong></p><p dir="ltr"><span>Around the globe, there have been signs of the harms of tech companies’ power grab for a long time now, but Americans have tended to view them as distant events. People in Myanmar used Facebook to call for genocide in 2017. In 2018, we learned that Cambridge Analytica extracted and analyzed millions of Facebook users’ data and allowed political campaigns to use it for psychological profiling in ways that may have impacted the Brexit vote in the U.K. And then in 2020, there was the storming of the U.S. Capitol, which was in part fed by disinformation on social media that has led to tens of millions of Americans losing trust in the electoral process. So, some of the problems that people were facing elsewhere have come back to Americans quite late as a boomerang.</span></p><p dir="ltr"><strong>You invite all of us as democratic citizens to help shape an agenda that puts the survival of democratic principles ahead of short-term economic benefits. What would that look like for an ordinary citizen?</strong></p><p dir="ltr"><span>There are a lot of choices that consumers can make about how they want to use technology and services, but I don’t think that’s enough because there’s a huge asymmetry between the power of the individual internet or device user and the tech companies.</span></p><p dir="ltr"><span>If people understand that the companies that offer all these entertaining, efficient, helpful digital tools are also engaged in power dynamics that affect capital and national and global politics, then they will also understand the need for independent oversight and countervailing powers, just as we have in almost every other aspect of our society.</span></p><p dir="ltr"><span>And so I hope that people will bring their concerns to the political agenda and ask more from their leaders, especially in the U.S. I think a lot of people in the United States would like to see federal data protection law; protections for kids online; better cybersecurity; and an effective means of addressing disinformation about healthcare and elections, but there is no political majority in Congress to make these things happen. And the sad result is that people get defeatist and think it’s easier to just not raise these issues.</span></p><p dir="ltr"><span>But it’s important for people to keep raising their concerns. And doing so at the state level is an interesting option because many states are now taking steps to pass laws and regulations around technology, given that the federal government can’t or won’t.</span></p><p dir="ltr"><a href="https://press.princeton.edu/books/hardcover/9780691241173/the-tech-coup?srsltid=AfmBOorex6WwTNLG37fSSqw6a9FpDMtflkYNCSoO8sR3KfjYrV0-3HiL"><em>The Tech Coup: How to Save Democracy from Silicon Valley</em></a><em> published in September from Princeton University Press.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Simone Giertz talks about invention (124 pts)]]></title>
            <link>https://spectrum.ieee.org/simone-giertz</link>
            <guid>41919717</guid>
            <pubDate>Tue, 22 Oct 2024 23:12:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/simone-giertz">https://spectrum.ieee.org/simone-giertz</a>, See on <a href="https://news.ycombinator.com/item?id=41919717">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Why Simone Giertz, the Queen of Useless Robots, Got Serious"><p>Simone Giertz came to fame in the 2010s by becoming the self-proclaimed “queen of shitty robots.” <a href="https://www.youtube.com/channel/UC3KEoMzNz8eYnwBC34RaKCQ" target="_blank">On YouTube</a> she demonstrated a hilarious series of self-built mechanized devices that worked perfectly for ridiculous applications, such as a headboard-mounted alarm clock with a rubber hand <a href="https://www.youtube.com/watch?v=mXLzfAHl4-k&amp;list=PLFONC7byEU96gsPcOaWCp3CQxtEdOvgH4&amp;index=5" target="_blank">to slap the user awake</a>. </p><p>But <a href="https://spectrum.ieee.org/tag/simone-giertz" target="_blank">Giertz</a> has parlayed her Internet renown into <a href="https://yetch-shop.fourthwall.com/" target="_blank">Yetch</a>, a design company that makes commercial consumer products. (The company name comes from how Giertz’s Swedish name is properly pronounced.) Her first release, a daily <a href="https://yetch-shop.fourthwall.com/products/every-day-goal-calendar" target="_blank">habit-tracking calendar</a>, was picked up by prestigious outlets such as the <a href="https://store.moma.org/" target="_blank">Museum of Modern Art design store</a> in New York City. She has continued to make commercial products since, as well as one-off strange inventions for her online audience.  </p><p><strong>Where did the motivation for your useless robots come from?</strong></p><p><strong>Simone Giertz:</strong> I just thought that <a href="https://spectrum.ieee.org/darpa-robotics-challenge-robots-falling" target="_blank">robots that failed</a> were really funny. It was also a way for me to get out of creating from a place of performance anxiety and perfection. Because if you set out to do something that fails, that gives you a lot of creative freedom. </p><p><strong>You built up a big online following. A lot of people would be happy with that level of success. But you moved into inventing commercial products. Why?</strong></p><p><strong>Giertz: </strong>I like torturing myself, I guess! I’d been creating things for YouTube and for social media for a long time. I wanted to try something new and also find longevity in my career. I’m not super motivated to constantly try to get people to give me attention. That doesn’t feel like a very good value to strive for. So I was like, “Okay, what do I want to do for the rest of my career?” And developing products is something that I’ve always been really, really interested in. And yeah, it is tough, but I’m so happy to be doing it. I’m enjoying it thoroughly, as much as there’s a lot of face-palm moments. </p><p><img id="11444" data-rm-shortcode-id="b06fca5d370fd3feca45654908f84f06" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-graphical-illustration-of-a-calendar.jpg?id=53806043&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/a-graphical-illustration-of-a-calendar.jpg?id=53806043&amp;width=980" width="2000" height="2000" alt="A graphical illustration of a calendar."><small placeholder="Add Photo Caption...">Giertz’s every day goal calendar was picked up by the Museum of Modern Art’s design store. </small><small placeholder="Add Photo Credit...">Yetch </small></p><p><strong>What role does failure play in your invention process?</strong></p><p><strong>Giertz: </strong>I think it’s inevitable. Before, obviously, I wanted something that failed in the most unexpected or fun way possible. And now when I’m developing products, it’s still a part of it. You make so many different versions of something and each one fails because of something. But then, hopefully, what happens is that you get smaller and smaller failures. Product development feels like you’re going in circles, but you’re actually going in a spiral because the circles are taking you somewhere.  </p><p><strong>What advice do you have for aspiring inventors?</strong></p><p><strong>Giertz: </strong>Make things that <em><em>you</em></em> want. A lot of people make things that they think that other people want, but the main target audience, at least for myself, is me. I trust that if I find something interesting, there are probably other people who do too. And then just find good people to work with and collaborate with. There is no such thing as the lonely genius, I think. I’ve worked with a lot of different people and some people made me really nervous and anxious. And some people, it just went easy and we had a great time. You’re just like, “Oh, what if we do this? What if we do <em><em>this</em></em>?” Find those people.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Several Russian developers lose kernel maintainership status (104 pts)]]></title>
            <link>https://lwn.net/Articles/995186/</link>
            <guid>41919670</guid>
            <pubDate>Tue, 22 Oct 2024 23:03:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/995186/">https://lwn.net/Articles/995186/</a>, See on <a href="https://news.ycombinator.com/item?id=41919670">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
Perhaps one of the more surprising changes in the 6.12-rc4 development
kernel was <a href="https://git.kernel.org/linus/6e90b675cf94">the removal
of several entries</a> from the kernel's </p><tt>MAINTAINERS</tt><p> file.  The <a href="https://lore.kernel.org/all/2024101835-tiptop-blip-09ed@gregkh/">patch</a>
performing the removal was sent (by Greg Kroah-Hartman) only to the
patches@lists.linux.dev mailing list; the change was included in <a href="https://lwn.net/ml/linux-kernel/ZxUH2J0BL3FCV6Hr@kroah.com/">a char-misc drivers
pull request</a> with no particular mention.
</p><p>
The explanation for the removal is simply "<q>various compliance
requirements</q>".  Given that the developers involved all appear to be of
Russian origin, it is not too hard to imagine what sort of compliance is
involved here.  There has, however, been no public posting of the policy
that required the removal of these entries.<br clear="all"></p><hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Bans Accounts Tracking Private Jets for Zuckerberg, Musk (243 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-10-22/meta-bans-accounts-tracking-private-jets-for-zuckerberg-musk</link>
            <guid>41919173</guid>
            <pubDate>Tue, 22 Oct 2024 22:00:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-10-22/meta-bans-accounts-tracking-private-jets-for-zuckerberg-musk">https://www.bloomberg.com/news/articles/2024-10-22/meta-bans-accounts-tracking-private-jets-for-zuckerberg-musk</a>, See on <a href="https://news.ycombinator.com/item?id=41919173">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Elderly dementia patients are unwittingly fueling political campaigns (114 pts)]]></title>
            <link>https://www.cnn.com/interactive/2024/10/politics/political-fundraising-elderly-election-invs-dg/</link>
            <guid>41918383</guid>
            <pubDate>Tue, 22 Oct 2024 20:29:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/interactive/2024/10/politics/political-fundraising-elderly-election-invs-dg/">https://www.cnn.com/interactive/2024/10/politics/political-fundraising-elderly-election-invs-dg/</a>, See on <a href="https://news.ycombinator.com/item?id=41918383">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        
        
        

<header>
  </header>
        

        
              
  <p><!-- HTML_TAG_START --><strong>(CNN) —</strong> The 80-year-old communications engineer from Texas had saved for decades, driving around in an old car and buying clothes from thrift stores so he’d have enough money to enjoy his retirement years.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->But as dementia robbed him of his reasoning abilities, he began making online political donations over and over again — eventually telling his son he believed he was part of a network of political operatives communicating with key Republican leaders. In less than two years, the man became one of the country’s largest grassroots supporters of the Republican Party, ultimately giving away nearly half a million dollars to former President Donald Trump and other candidates. Now, the savings account he spent his whole life building is practically empty.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->The story of this unlikely political benefactor is one of many playing out across the country.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->More than 1,000 reports filed with government agencies and consumer advocacy groups reviewed by CNN, along with an analysis of campaign finance data and interviews with dozens of contributors and their family members, show how deceptive political fundraisers have victimized hundreds of elderly Americans and misled those battling dementia or other cognitive impairments into giving away millions of dollars — far more than they ever intended. Some unintentionally joined the ranks of the top grassroots political donors in the country as they tapped into retirement savings and went into debt, contributing six-figure sums through thousands of transactions.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->To provide a snapshot of who these vulnerable donors are and how much money they have lost to increasingly aggressive fundraising campaigns, reporters reached out to more than 300 of the biggest and most frequent small-dollar political donors and their family members. Through these interviews and consumer complaints, reporters collected the accounts of more than 50 unwitting elderly donors and traced the path of where their money went.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Often coming in $5 or $10 at a time, contributions from this small sampling of donors alone added up to more than $6 million over the last five years — the majority of which ended up with Trump and a long list of other Republican candidates, CNN found.<!-- HTML_TAG_END --></p>      
        
  <p><!-- HTML_TAG_START -->While this is a small fraction of the billions raised by political campaigns, for many of the individuals who made the donations, the sums represented huge portions of their life savings.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Deceptive fundraising <a target="_blank" href="https://www.nytimes.com/2021/04/03/us/politics/trump-donations.html" title="tactics,">tactics,</a> including those that trick elderly donors, were <a target="_blank" href="https://www.nytimes.com/2021/06/26/us/politics/recurring-donations-seniors.html" title="exposed">exposed</a> in the wake of the 2020 election. While <a target="_blank" href="https://www.pewresearch.org/politics/2024/04/09/age-generational-cohorts-and-party-identification/" title="studies show">studies show</a> that older Americans tend to lean more Republican, both parties have continued to rake in donations from elderly voters. And mainstream Republican candidates have only doubled down on this strategy, using more aggressive and predatory tactics than those used by Democrats, according to donor complaints, interviews with experts and a review of solicitations. The Republican fundraising machine has been subject to more than 800 complaints to the Federal Trade Commission since 2022 — nearly seven times more than the number of complaints lodged against the other side.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->“The fact there are lots of complaints means this is likely a huge problem,” said Prentiss Cox, a law professor at the University of Minnesota and former manager of consumer protection at the Minnesota Attorney General's Office, when presented with CNN’s reporting. “From a consumer protection standpoint, this raises red flag level concerns about consumer misinformation and deception.”<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Donors identified by CNN were often in their 80s and 90s. They included retired public workers, house cleaners and veterans, widows living alone, nursing home residents and people who donated more money than they paid for their homes, according to records and interviews.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->The money they gave came from pensions, Social Security payments and retirement savings accounts meant to last decades. Donors took out new credit cards and mortgages to pay for the contributions. In some cases, they gave away most of their life savings. Their cell phones and email inboxes were so full of pleas for money that they missed photos of their grandkids and other important messages.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->At least one person continued to be charged for contributions after his death.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Donors or their families often learned of the extent of their donations from CNN reporters. The family members of some said their loved ones had also been the targets of common elder scams. Most asked CNN not to name them out of concern that they would be further victimized or ashamed.<!-- HTML_TAG_END --></p>      
  <figure><header><h3>WinRed had nearly seven times more FTC complaints than ActBlue</h3>
    <h4>Number of Federal Trade Commission complaints filed against WinRed and ActBlue, from January 2022 through June 2024.</h4>
</header>
        <ul><li><h4>WinRed</h4>
                <p><span>803</span></p>
            </li><li><h4>ActBlue</h4>
                <p><span>120</span></p>
            </li></ul>
    
</figure>      
  <p><!-- HTML_TAG_START -->One 82-year-old woman, who wore pajamas with holes in them because she didn’t want to spend money on new ones, didn’t realize she had given Republicans more than $350,000 while living in a 1,000 square-foot Baltimore condo since 2020.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->By the time a Taiwanese immigrant from California passed away from lung cancer this year at age 80, she had given away more than $180,000 to Trump’s campaign and a litany of other Republican candidates – writing letters to candidates apologizing for not getting donations to them on time because she was going into heart surgery. She had only $250 in her bank account when she died, leaving her family scrambling to cover the cost of her funeral.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->And a 78-year-old, a widow who limited showers to save on her water bill and canceled her long-term care insurance, didn’t understand why the retirement savings her husband had left her was dwindling so quickly. After CNN reached out to her family, they learned that the woman gave more than $200,000 in donations to Democratic political groups and candidates.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Have you or a loved one had an experience that you would like to share about WinRed, ActBlue or a political group soliciting donations? CNN wants to hear your story. Email us at <a href="mailto:watchdog@cnn.com">watchdog@cnn.com</a>.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->The federal government has gone after non-political companies for similar tactics, such as making false statements in ads or making them seem as if they were written directly to the recipient. But regulators have done little to stop fundraisers from using misleading and deceptive advertisements to target vulnerable donors. And the lawmakers who experts say would need to act to protect consumers at both the state and federal levels are the same ones benefiting from the current fundraising machines.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->The biggest beneficiary of the small-dollar donations from unwitting donors identified by CNN was Donald Trump. His current and former campaigns and affiliated political committees brought in more than $400,000 from these elderly consumers between July 2019 and June 2024, which included multiple election cycles. The national committees which raise money to support House and Senate races across the nation also received hundreds of thousands of dollars from such donors, according to CNN’s analysis, and in all, the long list of Republican candidates and causes took in nearly $4 million.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->A spokesperson for the Trump campaign and the Republican National Committee noted that fundraising efforts over the years have varied and said Trump ads were designed to be respectful, including such language as “don’t sweat it” if donors couldn’t afford to contribute on a regular basis.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->On the Democratic side, much of the money went to left-leaning political action committees known as PACs, not mainstream Democratic candidates. The Progressive Turnout Project and its affiliated group, Stop Republicans, took in the most, with roughly $150,000 coming from the vulnerable donors identified by CNN. President Joe Biden’s presidential campaign was not among the top recipients, taking in only about 10% of the $400,000 Trump brought in from those elderly donors.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Vice President Kamala Harris had not yet launched her presidential bid during the fundraising period covered in CNN’s analysis, though more recent campaign finance reports show her campaign has taken in several thousand dollars from this snapshot of donors. The Harris campaign did not respond to a request for comment.<!-- HTML_TAG_END --></p>      
        
  <p><!-- HTML_TAG_START -->The fundraising operations that have solicited money from vulnerable senior citizens use either WinRed or ActBlue, two juggernaut digital platforms that unite hundreds of political groups and campaigns under a single umbrella. Each platform charges a fee of about 4% of every transaction, although WinRed is a for-profit company and ActBlue is a nonprofit. Campaign finance records show that the two groups took in more than $100 million in fees from federal campaigns and political committees in the last two years alone.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->WinRed, founded by a former Trump staffer in 2019, and ActBlue, which has been around for two decades, say it is the campaigns themselves that are choosing the frequency and tenor of the pleas for donations. WinRed did not respond to multiple requests for comment. A spokesperson for ActBlue said the platform is designed to give “donors full control” over their contributions, saying donors or their families can contact their in-house support team with any concerns.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->But it is the fundraising platforms that have made it easier than ever for campaigns to bring in as much money as possible from their bases. Both have boasted of record-breaking hauls this campaign cycle, with WinRed saying this summer it was processing more than 2,700 donations per minute — a jump of more than 40% from 2020.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->The controversial feature that fools many donors is a pre-checked box campaigns use to automatically authorize recurring donations. Donors often don’t realize they need to uncheck that box, so while attempting to make a one-time small donation, they are unknowingly signing up for weekly or monthly recurring donations. Sometimes it takes months or years before they realize a campaign has been regularly charging their credit card or taking money out of their bank account.<!-- HTML_TAG_END --></p>      
  <figure>
    <video src="https://cdn.cnn.com/cnn/interactive/2024/10/politics/political-fundraising-elderly-election-invs-dg/video/vignette-b-sm.mp4" controls="" poster="https://cdn.cnn.com/cnn/interactive/2024/10/politics/political-fundraising-elderly-election-invs-dg/video/vignette-b-lg.jpg" preload="none"></video>
    
<figcaption><!-- HTML_TAG_START -->Richard Benjamin’s family tells CNN he was “manipulated” into donating his life savings to Republican causes while suffering from dementia.<!-- HTML_TAG_END -->
    
</figcaption>
</figure>      
  <p><!-- HTML_TAG_START -->While many mainstream Democratic candidates have backed away from the practice, both the Trump and Harris campaigns have recently been using donation pages with pre-checked recurring boxes to raise money, a CNN analysis of fundraising emails and Facebook and Instagram ads found.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Trump’s campaign said two of his current fundraising committees only returned to the use of pre-checked boxes in September after pausing the practice in January 2023. The spokesperson, however, did not address whether another Trump-affiliated committee still receiving donations this year had been using the tactic. Trump Campaign Senior Advisor Brian Hughes said "we strive to be direct and transparent by informing them immediately when they have registered for recurring donations, sending a notice three days before processing, notifying them when the processing is complete, and having staff on-hand to assist with refunds and cancellations.”<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Recurring donations only multiply as confirmed donors become valuable political currency — their names and contact information quickly swapped and sold. And once WinRed or ActBlue has a donor’s financial information, donations can be triggered by actions including a response to an online survey, an order of campaign merchandise or a one-word reply to a text message.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->As a result, records show, some donors ended up being charged in excess of 100 times in a single day.<!-- HTML_TAG_END --></p>      
  <h3>‘Lonely and isolated’
</h3>      
  <p><!-- HTML_TAG_START -->Richard Benjamin, an 81-year-old from Arizona, believed he had been in personal communication with former president Trump through all the messages he was receiving.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->At one point, he told his children the former president invited him to a luxurious reception at Mar-a-Lago. He had grown up on a farm and worried he would feel out of his element at such a fancy venue. But when he received what he described to his children as an invitation to be a VIP at a rally in Arizona, he was thrilled he would finally meet the former president himself. He started making travel plans and asking his sister-in-law if she would like to accompany him, since his wife had passed away in 2018.<!-- HTML_TAG_END --></p>      
  <figure><p><img src="https://www.cnn.com/interactive/2024/10/politics/political-fundraising-elderly-election-invs-dg/img/image-sm.jpg" alt=""></p>

    
<figcaption><!-- HTML_TAG_START -->Excerpts from a WinRed ad inviting donors to Mar-a-Lago<!-- HTML_TAG_END -->
    <span>Source: Trump Save America Joint Fundraising Committee</span>
</figcaption>
    
</figure>      
  <p><!-- HTML_TAG_START -->Later, he told his son how angry he was that Donald Trump Jr. wouldn't call him back even though the former president’s son had sent Benjamin so many nice messages.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->“He was old, lonely and isolated,” his son, Jason Benjamin, told CNN, saying the pandemic only compounded that isolation. “’Save America, help save America,’ that was the constant message. He would get thanked for helping to save America.”<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Richard Benjamin, who now lives in a memory care unit at an assisted living facility, would look forward to the emails and texts, and especially to the ones thanking him for being a true American and patriot when he donated his money. This eventually led him to give about $80,000, leaving him tens of thousands of dollars in debt and his children angry at the campaigns who they say tricked their dad and took advantage of his compromised state of mind. “He really, in his heart, believed that Donald Trump and Donald Trump Jr. and other politicians were personally reaching out to him,” Jason Benjamin said.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Richard Benjamin and his family showed CNN that he continues to be inundated with text messages and phone calls from politicians to this day. Yet he often couldn’t identify the politicians he financially supported.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->“There’s no excuse for them to allow something like this to happen,” Jason Benjamin said of the campaigns behind the many solicitations. An employee from Richard Benjamin’s bank even lodged a complaint to the Federal Trade Commission about deceptive practices, saying Benjamin had never intended to donate close to the amount he did, according to FTC records.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Forensic geriatrician Kathryn Locatell said what Richard Benjamin felt each time he received a “thank you” message or made a donation is the same “dopamine hit” a lot of elderly Americans are seeking. And the solicitations are crafted in a way that intentionally suck elderly donors into their web, providing “a feeling of belonging to a thrilling, special club.”<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->“You and I could call these demands for money laughable but to a person who’s lost their capacity to judge reality, and who’s been soaking up all the toxic misinformation out there, on the internet and TV, it’s a perfectly coherent reality and they’re happy to join in and become a part of it,” she said. “That’s how all their money will be drained until it’s gone.”<!-- HTML_TAG_END --></p>      
  <figure><header>Watch the full investigation:</header>
    <video src="https://cdn.cnn.com/cnn/interactive/2024/10/politics/political-fundraising-elderly-election-invs-dg/video/21060694-cen-1-sm.mp4" controls="" poster="https://cdn.cnn.com/cnn/interactive/2024/10/politics/political-fundraising-elderly-election-invs-dg/video/21060694-cen-1-lg.jpg" preload="none"></video>
    
<figcaption><!-- HTML_TAG_START -->CNN’s Kyung Lah reveals how deceptive political fundraisers have misled hundreds of elderly Americans into unwittingly giving away millions of dollars.<!-- HTML_TAG_END -->
    
</figcaption>
</figure>      
  <p><!-- HTML_TAG_START -->Because the text informing donors they are enrolling in recurring donations is often so tiny, particularly on WinRed donation pages, it would be very easy for someone who isn’t actually processing what they’re reading to miss it, according to Locatell and other experts interviewed by CNN. Elderly donors who have short-term memory issues could be making one-time donations again and again, not remembering that they made a donation even an hour earlier, they added.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->“These ads are a form of misleading, undue influence,” Locatell said. “One simple rule could be enacted: Ads can’t contain these pre-checked boxes; the person has to actively choose recurring donation.”<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Behavioral symptoms such as getting hooked by solicitations like these and making poor financial decisions often happen in the early stages of dementia, before family members even realize their loved ones are experiencing any sort of cognitive decline.<!-- HTML_TAG_END --></p>      
  <div>
                  
  <section><p><!-- HTML_TAG_START -->CNN compared examples of pages donors use to make contributions to presidential candidates, former President Donald Trump and Vice President Kamala Harris.<!-- HTML_TAG_END --></p>
</section>      
  <section><p><!-- HTML_TAG_START -->Some donors believe political candidates are speaking to them directly, with an ad like this one indicating that Trump requested to be their friend on social media.<!-- HTML_TAG_END --></p>
</section>      
  <section><p><!-- HTML_TAG_START -->Invitations to join phony boards or focus groups are another common fundraising tactic that experts told CNN are used to make people feel special and are more likely to entrap elderly donors who may have diminished reasoning abilities.<!-- HTML_TAG_END --></p>
</section>      
  <section><p><!-- HTML_TAG_START -->Polls like this one — with 11 questions — can rile up voters before asking them for money.<!-- HTML_TAG_END --></p>
</section>      
  <section><p><!-- HTML_TAG_START -->The yellow box automatically signs donors up for weekly donations, after asking if Trump can count on their “sustained support.”<!-- HTML_TAG_END --></p>
</section>      
  <section><p><!-- HTML_TAG_START -->The way this small text appears to be hyperlinked makes it seem like donors should click there to sign up for recurring donations. But they are actually already signed up by the pre-checked box above.<!-- HTML_TAG_END --></p>
</section>      
  <section><p><!-- HTML_TAG_START -->These boxes, which claim to be messages from Trump, are known as upsells. A click on both of these results in two additional donations.<!-- HTML_TAG_END --></p>
</section>      
  <section><p><!-- HTML_TAG_START -->Fundraising pages for the Harris campaign, like this one, are clearer than those used by Trump.<!-- HTML_TAG_END --></p>
</section>      
  <section><p><!-- HTML_TAG_START -->This headline tells donors they will be signing up for weekly donations.<!-- HTML_TAG_END --></p>
</section>      
  <section><p><!-- HTML_TAG_START -->Harris fundraising pages tend to be short and relatively simple.<!-- HTML_TAG_END --></p>
</section>      
  <section><p><!-- HTML_TAG_START -->Like Trump, the Harris campaign sometimes uses pre-checked boxes that automatically sign donors up for recurring donations unless they opt out.<!-- HTML_TAG_END --></p>
</section></div>      
  <p><!-- HTML_TAG_START -->Calls from CNN reporters asking about donations prompted some family members to rush over to their loved ones’ homes to help look through statements or to begin the long process of taking over their finances. Other families CNN spoke with said it was the unusually large number of political donations – which took precedence over expenses as important as electricity bills and taxes -- that alerted them something was wrong.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->In the case of the 80-year-old retired communications engineer who gave away close to half a million dollars, one of the man’s sons discovered his savings account was nearly drained from all his donations. The son said he took him to a neurologist where he was diagnosed with dementia and spent weeks canceling credit cards and disputing charges, attempting to turn off recurring donations one by one. He ultimately received refunds from WinRed for his father’s most recent donations. Still, his father remains out roughly $300,000.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->“When I found this out and started showing Dad, he was shocked, he had no idea,” his son said. “He's not a man to give anyone what he considered his life savings.”<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->It often falls on families to try to help their loved ones.<!-- HTML_TAG_END --></p>      
  <div><header>Watch to learn how to search the FEC website</header>
    <figure>
    <video src="https://cdn.cnn.com/cnn/interactive/2024/10/politics/political-fundraising-elderly-election-invs-dg/video/search-sm.mp4" controls="" poster="https://cdn.cnn.com/cnn/interactive/2024/10/politics/political-fundraising-elderly-election-invs-dg/video/search-lg.jpg" preload="none"></video>
    
<figcaption>
    
</figcaption>
</figure>
    <h4>Look up political donations on the FEC site</h4>
    <p>Enter a name and state to be redirected to the Federal Election Commission (FEC) website where you can view contributions to political campaigns through ActBlue or WinRed.</p><p>Information entered is not stored by CNN.</p>

    

    </div>      
  <p><!-- HTML_TAG_START -->The daughter of an 81-year-old woman with Alzheimer’s disease told CNN that when she visited her mother at her home last month, her mom got a call from Ted Cruz’s campaign and a text with an ActBlue link within minutes of each other. She said her mother was going to donate to each before she stopped her. Her mother’s cognitive decline has gotten so bad, she said, that she believed when someone asked her for money, she needed to give it. The daughter, who lives in another city, said she has been trying to coordinate her mother’s medical care while also sorting through the credit card debt she accrued after giving away more than $100,000.<!-- HTML_TAG_END --></p>      
  <h3>No oversight
</h3>      
  <p><!-- HTML_TAG_START -->When an elderly man discovered thousands of dollars' worth of political charges on his credit card he said he never authorized, he turned to the agency that oversees campaign donations – the Federal Election Commission – for help.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->“I am an 89-year-old widower, a retired architect who lives alone,” he wrote in a 2022 complaint. He said he had been plagued by robocalls and scam mail and it appeared ActBlue was “rife with fraud.” By the time the FEC completed its investigation, he had died, collapsing "on his way to the post office to mail postcards to voters in Texas,” according to his obituary. But the agency hadn’t planned on doing anything about his complaint anyway, writing in a response that the matter was “rated as low priority” for an enforcement action based on criteria including the severity of the allegation and the dollar amount involved.<!-- HTML_TAG_END --></p>      
        
  <p><!-- HTML_TAG_START -->The agency shut down another complaint made about WinRed by the daughters of an elderly woman who said their mother had lost more than $6,700 to “elder abuse” with the same boiler plate response, noting the “low dollar amount involved.”<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->The FEC would not comment on WinRed or ActBlue specifically but said in a <a target="_blank" href="https://www.fec.gov/resources/cms-content/documents/legrec2023.pdf" title="2023 report">2023 report</a> on legislative recommendations that it had been regularly hearing from donors who had signed up for recurring donations without their knowledge or consent. The report noted  consumers had often attempted to cancel the donations without success before contacting agency staff for help. Agency officials said the outcry from donors "strongly suggests that many contributors are unaware of the ‘pre-checked boxes’ and are surprised by the already completed transactions appearing on account statements." But it said it didn’t have the authority to ban the practice, and that Congress should introduce legislation to change this — a recommendation agency officials first made <a target="_blank" href="https://www.nytimes.com/2021/05/06/us/politics/fec-trump-donations.html" title="after">after</a> the New York Times <a target="_blank" href="https://www.nytimes.com/2021/04/03/us/politics/trump-donations.html" title="reported">reported</a> on the issue in 2021.<!-- HTML_TAG_END --></p>      
  <div><header><h3>Read what some victims and their families are saying to government watchdogs</h3>
    
</header>
    <div><p>"<!-- HTML_TAG_START -->I'm a retired cleaning lady. I just can't believe anyone would do this to me.<!-- HTML_TAG_END -->"</p><p>"<!-- HTML_TAG_START -->I gave Trump $20 6 years ago that was just once. Since then I've been hounded by REPUBLICANS BEGGING FOR MONEY. On the RNC Convention day I received 77 text messages of more begging.<!-- HTML_TAG_END -->"</p><p>"<!-- HTML_TAG_START -->My father is elderly and isn't able to manage his finances anymore, so this went on undetected for a while until we realized that he was running out of money and didn't know why.<!-- HTML_TAG_END -->"</p>
      </div><div><p>"<!-- HTML_TAG_START -->I was recently helping my 93 year old father, who suffers from a cognitive impairment, sort through a pile of mail and saw a massive fraud that someone in ActBlue is perpetrating on my father.... these fraudsters need to stop defrauding older people, especially ones with mental impairments.<!-- HTML_TAG_END -->"</p><p>"<!-- HTML_TAG_START -->I can't afford you taking all my Social security payments. You drained my bank account, before I can even get food or pay my bills.<!-- HTML_TAG_END -->"</p>
      </div><div><p>"<!-- HTML_TAG_START -->I am distressed by what appears to be outright fraud on behalf of the Republicans.<!-- HTML_TAG_END -->"</p><p>"<!-- HTML_TAG_START -->I may never contribute again. You are greedy and illegally fool people into taking our money. Not American. not what I thought the party stood for.<!-- HTML_TAG_END -->"</p><p>"<!-- HTML_TAG_START -->My grandmother, utterly confused and frustrated by the situation has paid her bills and been draining her financial accounts... she was absolutely floored and clearly did not understand the gravity of the deception.<!-- HTML_TAG_END -->"</p>
      </div>

    
    
</div>      
  <p><!-- HTML_TAG_START -->Dan Weiner, director of the Brennan Center’s Elections &amp; Government Program, said federal campaign finance law is primarily intended to police political corruption but offers few protections for donors. He said seeking recurring donations “from someone who is not of sound mind” is “unscrupulous behavior [that] falls into a legal gray area.”<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->The Federal Trade Commission criticized pre-checked boxes in a <a target="_blank" href="https://www.ftc.gov/system/files/ftc_gov/pdf/P214800%20Dark%20Patterns%20Report%209.14.2022%20-%20FINAL.pdf" title="2022 staff report">2022 staff report</a> as an example of a “trick” and “psychological tactic” used by retailers and direct marketers “to get consumers to part with their money.” But the agency — which enforces laws governing deceptive advertising, among other regulations — told CNN it did not have jurisdiction over ads used by political campaigns, or over the operations of WinRed and ActBlue, despite fielding hundreds of complaints from consumers.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Currently, pre-checked boxes for recurring donations are allowed in almost every state despite widespread condemnation of the practice from consumer advocates. Federal legislation introduced in recent years that would have prevented their use died in committee without gaining traction.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->And while four Democratic attorneys general have been <a target="_blank" href="https://www.cnn.com/2021/07/09/politics/political-fundraising-winred-actblue/index.html" title="investigating">investigating</a> the fundraising tactics of WinRed and ActBlue, no action has been taken to date. A number of Republican lawmakers have specifically targeted ActBlue with investigations after right-wing influencers claimed the large volume of transactions from some donors suggested money laundering. CNN has found no evidence of money laundering, instead finding that many elderly donors simply didn’t realize how much they were giving.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Individual complaints made to state attorneys general around the country, meanwhile, have not resulted in any apparent action taken against the fundraising platforms or campaigns, according to records obtained by CNN. And in what some donors and experts have pointed to as a potential conflict of interest, the same attorneys general receiving complaints about these platforms have also used WinRed and ActBlue for their own fundraising efforts.<!-- HTML_TAG_END --></p>      
  <h3>Making it stop
</h3>      
  <p><!-- HTML_TAG_START -->In Utah, a 69-year-old woman, who had a tube surgically placed in her brain to drain fluid from it and had such severe dementia that she required around-the-clock home care, donated nearly $20,000 to Republican candidates in less than four months.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->WinRed continued to charge her even after the attorney overseeing her financial affairs informed the company of her condition and sent a cease-and-desist letter. He filed a lawsuit in 2021 against the platform, which remains ongoing. But it still took a year to make the donations stop. Just seven days after she made her last donation to the Republicans, public records show that more donations were charged to her accounts. This time, to a political group supporting Democrats.<!-- HTML_TAG_END --></p>      
        
  <p><!-- HTML_TAG_START -->Matthew Hurtt, the chairman of Virginia-based Arlington County Republican Committee, said he has fielded more than 200 calls and emails from frustrated donors located all over the country. Many of them are elderly donors who say they have struggled to stop their recurring contributions and receive refunds. Hurtt is not affiliated with WinRed, but said he receives the angry inquiries because WinRed is registered in Arlington, so charges made through the platform can sometimes show up with “Arlington” on credit card statements.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Hurtt, like other experts, told CNN he believes such aggressive fundraising tactics are short-sighted and will turn off voters in the long run.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->“We don't have to trick people into giving money to our candidates and to our causes,” Hurtt said. “Any platform that tricks older donors into giving money unwittingly seems like a scheme to me.”<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Both <a target="_blank" href="https://donors.winred.com/en/articles/3773508-requesting-a-refund" title="WinRed">WinRed</a> and <a target="_blank" href="https://support.actblue.com/donors/contributions/i-am-a-caregiver-or-have-been-granted-power-of-attorney-for-someone-else-how-do-i-seek-a-refund-or-support-on-behalf-of-that-person/" title="ActBlue">ActBlue</a> have said they notify donors of each recurring contribution and have guides on their websites explaining how donors or their families can request refunds and cancel recurring donations.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->ActBlue told CNN it has trained its customer service staff to be on the lookout for those donors who use keywords that indicate confusion or cognitive issues and escalate them to a higher level of service. The platform said this situation is rare and that employees do what they can to accommodate refunds beyond the standard 90-day refund window.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->WinRed’s website meanwhile says it “is happy to refund any donation made in error,” but notes that it can only provide refunds for those contributions made in the last 60 days.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Some of the candidates and PACs who CNN identified as receiving the most money from elderly, unwitting donors said they try to work with contributors to refund mistaken donations.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->But many of those interviewed by CNN said it was hard to determine which campaigns to contact, especially since donations were often spread out among hundreds of groups. They usually contacted WinRed or ActBlue instead and found the process confusing, difficult and frequently unsuccessful.<!-- HTML_TAG_END --></p>      
        
  <p><!-- HTML_TAG_START -->In the case of WinRed specifically, many said they couldn’t even figure out how to reach a representative on the phone. One frustrated son who filed a complaint with the Massachusetts Attorney General’s Office told CNN he resorted to tracking down a company intern he found on LinkedIn in the hopes of finally turning off his mother’s recurring donations. He said she had lost more than $20,000 and that after contacting WinRed and even cancelling her debit card, she continued to be charged.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->In Washington state, meanwhile, WinRed was quick to dismiss an 80-year-old woman’s plea for the state to investigate the platform for “senior abuse” -- saying the woman had agreed to the company’s terms of use, which make clear it provides refunds only as required by law. By the time she had filed her complaint, her money was long gone. It was up to her to reach out to the campaigns for any refunds.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->“With all due respect,” WinRed’s attorney wrote, “there is nothing here to investigate.”<!-- HTML_TAG_END --></p>      
        
  <section>      
  <h3>How CNN reported this story
</h3>      
  <p><!-- HTML_TAG_START -->CNN reviewed Federal Election Commission reports on hundreds of millions of contributions made through WinRed and ActBlue, the main donation platforms for Republican and Democratic campaigns. Reporters identified the top small-dollar donors who also gave the most frequently on each platform in recent years, and then reached out to more than 150 of those top donors and their family members for each platform.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Based on these interviews, as well as an analysis of hundreds of consumer complaints submitted to the Federal Trade Commission and state Attorneys General, reporters identified more than 50 elderly donors who gave more than they intended, many of whom suffered from cognitive decline or dementia. Reporters also reviewed complaints lodged on consumer advocacy websites TrustPilot.com and PissedConsumer.com.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->Reporters reviewed data on donations made by those vulnerable donors reported in WinRed and ActBlue’s FEC filings over five years, from July 2019 through the end of June 2024. The donation amounts reported in the story do not include any refunds the donors later received, which were typically only a small fraction of the total they had given.<!-- HTML_TAG_END --></p>      
  <p><!-- HTML_TAG_START -->To identify ads with pre-checked recurring donation boxes, reporters downloaded data on fundraising ads on Facebook and Instagram from Meta’s <a target="_blank" href="https://www.facebook.com/ads/library/?active_status=active&amp;ad_type=political_and_issue_ads&amp;country=US&amp;media_type=all&amp;source=nav-header" title="Ad Library">Ad Library</a>, and also reviewed fundraising emails sent by Trump and Harris’ campaigns provided by the <a target="_blank" href="https://politicalemails.org/" title="Archive of Political Emails">Archive of Political Emails</a>, which collects emails from numerous campaigns.<!-- HTML_TAG_END --></p>
</section>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Tragedy of Google Books (2017) (271 pts)]]></title>
            <link>https://www.theatlantic.com/technology/archive/2017/04/the-tragedy-of-google-books/523320/</link>
            <guid>41917016</guid>
            <pubDate>Tue, 22 Oct 2024 18:11:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/technology/archive/2017/04/the-tragedy-of-google-books/523320/">https://www.theatlantic.com/technology/archive/2017/04/the-tragedy-of-google-books/523320/</a>, See on <a href="https://news.ycombinator.com/item?id=41917016">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header data-event-module="hero"><div><div><figure><div data-flatplan-lead_figure_media="true"><picture><source media="(prefers-reduced-motion)" srcset="https://cdn.theatlantic.com/thumbor/iiAux28ZM7sOGwZWZJQZiRVtWp4=/0x100:2000x1225/640x360/filters:still()/media/img/2017/04/17/GoogleBooks_Library/original.gif 640w, https://cdn.theatlantic.com/thumbor/u6AQYdnn3Qg2IFRtRH5lSAR13Nk=/0x100:2000x1225/750x422/filters:still()/media/img/2017/04/17/GoogleBooks_Library/original.gif 750w, https://cdn.theatlantic.com/thumbor/WJAvZaSdJEQbgQ2IKepHP6ehFvc=/0x100:2000x1225/850x478/filters:still()/media/img/2017/04/17/GoogleBooks_Library/original.gif 850w, https://cdn.theatlantic.com/thumbor/Mh-eJ_7EzF2xMfe-0qCG1Ymoarw=/0x100:2000x1225/1536x864/filters:still()/media/img/2017/04/17/GoogleBooks_Library/original.gif 1536w, https://cdn.theatlantic.com/thumbor/yJFyqPFP-UbPSV0qScOzsNt07Qo=/0x100:2000x1225/1920x1080/filters:still()/media/img/2017/04/17/GoogleBooks_Library/original.gif 1920w" sizes="(min-width: 1920px) 1920px, 100vw"><img alt="" sizes="(min-width: 1920px) 1920px, 100vw" srcset="https://cdn.theatlantic.com/thumbor/6U4wlcnJQyGBFD4kElex8s8paeE=/0x100:2000x1225/640x360/media/img/2017/04/17/GoogleBooks_Library/original.gif 640w, https://cdn.theatlantic.com/thumbor/vC49xt8MOqxiRsbEV-b0bXZseM4=/0x100:2000x1225/750x422/media/img/2017/04/17/GoogleBooks_Library/original.gif 750w, https://cdn.theatlantic.com/thumbor/wKIVksczXCTb_D4Nd9JA9hTgymI=/0x100:2000x1225/850x478/media/img/2017/04/17/GoogleBooks_Library/original.gif 850w, https://cdn.theatlantic.com/thumbor/KvmvriUDno1JEL8BAwxIQyJ7zlo=/0x100:2000x1225/1536x864/media/img/2017/04/17/GoogleBooks_Library/original.gif 1536w, https://cdn.theatlantic.com/thumbor/WavqmK--qsDCR7Cx2Orxghyu7E0=/0x100:2000x1225/1920x1080/media/img/2017/04/17/GoogleBooks_Library/original.gif 1920w" src="https://cdn.theatlantic.com/thumbor/iT9oCAiCW4aXa65-LS0bCgLMVF0=/0x100:2000x1225/1440x810/media/img/2017/04/17/GoogleBooks_Library/original.gif" id="article-lead-image" width="1440" height="810"></picture></div><figcaption data-flatplan-lead_figure_caption="true">meshaphoto / Getty / Konstantin Orlov / Shutterstock / Katie Martin / The Atlantic</figcaption></figure></div><div><p>“Somewhere at Google there is a database containing 25 million books and nobody is allowed to read them.”</p></div></div><gpt-ad format="injector" sizes-at-0="mobile-wide" targeting-pos="injector-article-start" sizes-at-976="desktop-wide"></gpt-ad></header><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true" data-flatplan-dropcap="true">Y<span>ou were going</span> to get one-click access to the full text of nearly every book that’s ever been published. Books still in print you’d have to pay for, but everything else—a collection slated to grow larger than the holdings at the Library of Congress, Harvard, the University of Michigan, at any of the great national libraries of Europe—would have been available for free at terminals that were going to be placed in every local library that wanted one.</p><p data-flatplan-paragraph="true">At the terminal you were going to be able to search tens of millions of books and read every page of any book you found. You’d be able to highlight passages and make annotations and share them; for the first time, you’d be able to pinpoint an idea somewhere inside the vastness of the printed record, and send somebody straight to it with a link. Books would become as instantly available, searchable, copy-pasteable—as alive in the digital world—as web pages.</p><p data-flatplan-paragraph="true">It was to be the realization of a long-held dream. “The universal library has been talked about for millennia,” Richard Ovenden, the head of Oxford’s Bodleian Libraries, has said. “It was possible to think in the Renaissance that you might be able to amass the whole of published knowledge in a single room or a single institution.” In the spring of 2011, it seemed we’d amassed it in a terminal small enough to fit on a desk.</p><p data-flatplan-paragraph="true">“This is a watershed event and can serve as a catalyst for the reinvention of education, research, and intellectual life,” one eager observer wrote at the time.</p><p data-flatplan-paragraph="true">On March 22 of that year, however, the legal agreement that would have unlocked a century’s worth of books and peppered the country with access terminals to a universal library was rejected under Rule 23(e)(2) of the Federal Rules of Civil Procedure by the U.S. District Court for the Southern District of New York.</p><p data-flatplan-paragraph="true">When the library at Alexandria burned it was said to be an “international catastrophe.” When the most significant humanities project of our time was dismantled in court, the scholars, archivists, and librarians who’d had a hand in its undoing breathed a sigh of relief, for they believed, at the time, that they had narrowly averted disaster.</p><p><b>* * *</b></p><p data-flatplan-paragraph="true" data-flatplan-dropcap="true">G<span>oogle’s secret effort</span> to scan every book in the world, codenamed “Project Ocean,” began in earnest in 2002 when Larry Page and Marissa Mayer sat down in the office together with a 300-page book and a metronome. Page wanted to know how long it would take to scan more than a hundred-million books, so he started with one that was lying around. Using the metronome to keep a steady pace, he and Mayer paged through the book cover-to-cover. It took them 40 minutes.</p><p data-flatplan-paragraph="true">Page had always wanted to digitize books. Way back in 1996, the student project that eventually became Google—a “crawler” that would ingest documents and rank them for relevance against a user’s query—was actually conceived as part of an effort “to develop the enabling technologies for a single, integrated and universal digital library.” The idea was that in the future, once all books were digitized, you’d be able to map the citations among them, see which books got cited the most, and use that data to give better search results to library patrons. But books still lived mostly on paper. Page and his research partner, Sergey Brin, developed their popularity-contest-by-citation idea using pages from the World Wide Web.</p><p data-flatplan-paragraph="true">By 2002, it seemed to Page like the time might be ripe to come back to books. With that 40-minute number in mind, he approached the University of Michigan, his alma mater and a world leader in book scanning, to find out what the state of the art in mass digitization looked like. Michigan told Page that at the current pace, digitizing their entire collection—7 million volumes—was going to take about a thousand years. Page, who’d by now given the problem some thought, replied that he thought Google could do it in six.</p><p data-flatplan-paragraph="true">He offered the library a deal: You let us borrow all your books, he said, and we’ll scan them for you. You’ll end up with a digital copy of every volume in your collection, and Google will end up with access to one of the great untapped troves of data left in the world. Brin put Google’s lust for library books this way: “You have thousands of years of human knowledge, and probably the highest-quality knowledge is captured in books.” What if you could feed all the knowledge that’s locked up on paper to a search engine?</p><p data-flatplan-paragraph="true">By 2004, Google had started scanning. In just over a decade, after making deals with Michigan, Harvard, Stanford, Oxford, the New York Public Library, and dozens of other library systems, the company, outpacing Page’s prediction, had scanned about 25 million books. It cost them an estimated $400 million. It was a feat not just of technology but of logistics.</p><p data-flatplan-paragraph="true" data-flatplan-dropcap="true">E<span>very weekday, semi trucks</span> full of books would pull up at designated Google scanning centers. The one ingesting Stanford’s library was on Google’s Mountain View campus, in a converted office building. The books were unloaded from the trucks onto the kind of carts you find in libraries and wheeled up to human operators sitting at one of a few dozen brightly lit scanning stations, arranged in rows about six to eight feet apart.</p><p data-flatplan-paragraph="true">The stations—which didn’t so much scan as photograph books—had been custom-built by Google from the sheet metal up. Each one could digitize books at a rate of 1,000 pages per hour. The book would lie in a specially designed motorized cradle that would adjust to the spine, locking it in place. Above, there was an array of lights and at least $1,000 worth of optics, including four cameras, two pointed at each half of the book, and a range-finding LIDAR that overlaid a three-dimensional laser grid on the book’s surface to capture the curvature of the paper. The human operator would turn pages by hand—no machine could be as quick and gentle—and fire the cameras by pressing a foot pedal, as though playing at a strange piano.</p><p data-flatplan-paragraph="true">What made the system so efficient is that it left so much of the work to software. Rather than make sure that each page was aligned perfectly, and flattened, before taking a photo, which was a major source of delays in traditional book-scanning systems, cruder images of curved pages were fed to de-warping algorithms, which used the LIDAR data along with some clever mathematics to artificially bend the text back into straight lines.</p><p data-flatplan-paragraph="true">At its peak, the project involved about 50 full-time software engineers. They developed optical character-recognition software for turning raw images into text; they wrote de-warping and color-correction and contrast-adjustment routines to make the images easier to process; they developed algorithms to detect illustrations and diagrams in books, to extract page numbers, to turn footnotes into real citations, and, per Brin and Page’s early research, to rank books by relevance. “Books are not part of a network,” Dan Clancy, who was the engineering director on the project during its heyday, has said. “There is a huge research challenge, to understand the relationship between books.”</p><p data-flatplan-paragraph="true">At a time when the rest of Google was obsessed with making apps more “social”—Google Plus was released in 2011—Books was seen by those who worked on it as one of those projects from the old era, like Search itself, that made good on the company’s mission “to organize the world’s information and make it universally accessible and useful.”</p><p data-flatplan-paragraph="true">It was the first project that Google ever called a “moonshot.” Before the self-driving car and Project Loon—their effort to deliver Internet to Africa via high-altitude balloons—it was the idea of digitizing books that struck the outside world as a wide-eyed dream. Even some Googlers themselves thought of the project as a boondoggle. “There were certainly lots of folks at Google that while we were doing Google Book Search were like, Why are we spending all this money on this project?,” Clancy said to me. “Once Google started being a little more conscious about how it was spending money, it was like, wait, you have $40 million a year, $50 million a year on the cost of scanning? It’s gonna cost us $300 to $400 million before we’re done? What are you thinking? But Larry and Sergey were big supporters.”</p><p data-flatplan-paragraph="true">In August 2010, Google put out a blog post announcing that there were 129,864,880 books in the world. The company said they were going to scan them all.</p><p data-flatplan-paragraph="true">Of course, it didn’t quite turn out that way. This particular moonshot fell about a hundred-million books short of the moon. What happened was complicated but how it started was simple: Google did that thing where you ask for forgiveness rather than permission, and forgiveness was not forthcoming. Upon hearing that Google was taking millions of books out of libraries, scanning them, and returning them as if nothing had happened, authors and publishers filed suit against the company, alleging, as the authors put it simply in their initial complaint, “massive copyright infringement.”</p><p><b>* * *</b></p><p data-flatplan-paragraph="true" data-flatplan-dropcap="true"><span>When Google started</span> scanning, they weren’t actually setting out to build a digital library where you could read books in their entirety; that idea would come later. Their original goal was just to let you <i>search</i> books. For books in copyright, all they would show you were “snippets,” just a few sentences of context around your search terms. They likened their service to a card catalog.</p><p data-flatplan-paragraph="true">Google thought that creating a card catalog was protected by “fair use,” the same doctrine of copyright law that lets a scholar excerpt someone’s else’s work in order to talk about it. “A key part of the line between what’s fair use and what’s not is transformation,” Google’s lawyer, David Drummond, has said. “Yes, we’re making a copy when we digitize. But surely the ability to find something because a term appears in a book is not the same thing as reading the book. That’s why Google Books is a different product from the book itself.”</p><p data-flatplan-paragraph="true">It was important for Drummond to be right. Statutory damages for “willful infringement” of a copyright can run as high as $150,000 for each work infringed. Google’s potential liability for copying tens of millions of books could have run into the trillions of dollars. “Google had some reason to fear that it was betting the firm on its fair-use defense,” Pamela Samuelson, a law professor at UC Berkeley, wrote in 2011. Copyright owners pounced.</p><p data-flatplan-paragraph="true">They had good reason to. Instead of asking for anyone’s permission, Google had plundered libraries. This seemed obviously wrong: If you wanted to copy a book, you had to have the <i>right</i> to <i>copy</i> it—you had to have the damn copyright. Letting Google get away with the wholesale copying of every book in America struck them as setting a dangerous precedent, one that might well render their copyrights worthless. An advocacy group called the Authors Guild, and several book authors, filed a class action lawsuit against Google on behalf of everyone with a U.S. copyright interest in a book. (A group of publishers filed their own lawsuit but joined the Authors Guild class action shortly thereafter.)</p><p data-flatplan-paragraph="true">There’s actually a long tradition of technology companies disregarding intellectual-property rights as they invent new ways to distribute content. In the early 1900s, makers of the “piano rolls” that control player pianos ignored copyrights in sheet music and were sued by music publishers. The same thing happened with makers of vinyl records and early purveyors of commercial radio. In the 60s, cable operators re-aired broadcast TV signals without first getting permission and found themselves in costly litigation. Movie studios sued VCR makers. Music labels sued KazaA and Napster.</p><p data-flatplan-paragraph="true">As Tim Wu pointed out in a <a data-event-element="inline link" href="https://papers.ssrn.com/sol3/papers2.cfm?abstract_id=2060694">2003 law review article</a>, what usually becomes of these battles—what happened with piano rolls, with records, with radio, and with cable—isn’t that copyright holders squash the new technology. Instead, they cut a deal and start making money from it. Often this takes the form of a “compulsory license” in which, for example, musicians are <i>required</i> to license their work to the piano-roll maker, but in exchange, the piano-roll maker has to pay a fixed fee, say two cents per song, for every roll they produce. Musicians get a new stream of income, and the public gets to hear their favorite songs on the player piano. “History has shown that time and market forces often provide equilibrium in balancing interests,” Wu writes.</p><p data-flatplan-paragraph="true">But even if everyone typically ends up ahead, each new cycle starts with rightsholders fearful they’re being displaced by the new technology. When the VCR came out, film executives lashed out. “I say to you that the VCR is to the American film producer and the American public as the Boston strangler is to the woman home alone,” Jack Valenti, then the president of the MPAA, testified before Congress. The major studios sued Sony, arguing that with the VCR, the company was trying to build an entire business on intellectual property theft. But <i>Sony Corp. of America v. Universal City Studios, Inc.</i> became famous for its holding that as long as a copying device was capable of “substantial noninfringing uses”—like someone watching home movies—its makers couldn’t be held liable for copyright infringement.</p><p data-flatplan-paragraph="true">The Sony case forced the movie industry to accept the existence of VCRs. Not long after, they began to see the device as an opportunity. “The VCR turned out to be one of the most lucrative inventions—for movie producers as well as hardware manufacturers—since movie projectors,” one commentator put it in 2000.</p><p data-flatplan-paragraph="true">It only took a couple of years for the authors and publishers who sued Google to realize that there was enough middle ground to make everyone happy. This was especially true when you focused on the back catalog, on out-of-print works, instead of books still on store shelves. Once you made that distinction, it was possible to see the whole project in a different light. Maybe Google wasn’t plundering anyone’s work. Maybe they were giving it a new life. Google Books could turn out to be for out-of-print books what the VCR had been for movies out of the theater.</p><p data-flatplan-paragraph="true">If that was true, you wouldn’t actually want to stop Google from scanning out-of-print books—you’d want to encourage it. In fact, you’d want them to go beyond just showing snippets to actually selling those books as digital downloads. Out-of-print books, almost by definition, were commercial dead weight. If Google, through mass digitization, could make a new market for them, that would be a real victory for authors and publishers. “We realized there was an opportunity to do something extraordinary for readers and academics in this country,” Richard Sarnoff, who was then Chairman of the American Association of Publishers, said at the time. “We realized that we could light up the out-of-print backlist of this industry for two things: discovery and consumption.”</p><p data-flatplan-paragraph="true">But once you had that goal in mind, the lawsuit itself—which was about whether Google could keep scanning and displaying snippets—began to seem small time. Suppose the Authors Guild won: they were unlikely to recoup anything more than the statutory minimum in damages; and what good would it do to stop Google from providing snippets of old books? If anything those snippets might drive demand. And suppose Google won: Authors and publishers would get nothing, and all readers would get for out-of-print books would be snippets—not access to full texts.</p><p data-flatplan-paragraph="true">The plaintiffs, in other words, had gotten themselves into a pretty unusual situation. They didn’t want to lose their own lawsuit—but they didn’t want to win it either.</p><p><b>* * *</b></p><p data-flatplan-paragraph="true" data-flatplan-dropcap="true">T<span>he basic problem</span> with out-of-print books is that it’s unclear who owns most of them. An author might have signed a book deal with their publisher 40 years ago; that contract stipulated that the rights revert to the author after the book goes out of print, but required the author to send a notice to that effect, and probably didn’t say anything about digital rights; and all this was recorded on some pieces of paper that nobody has.</p><p data-flatplan-paragraph="true">It’s been estimated that about half the books published between 1923 and 1963 are actually in the public domain—it’s just that no one knows which half. Copyrights back then had to be renewed, and often the rightsholder wouldn’t bother filing the paperwork; if they did, the paperwork could be lost. The cost of figuring out who owns the rights to a given book can end up being greater than the market value of the book itself. “To have people go and research each one of these titles,” Sarnoff said to me, “It’s not just Sisyphean—it’s an impossible task economically.” Most out-of-print books are therefore locked up, if not by copyright then by inconvenience.</p><p data-flatplan-paragraph="true">The tipping point toward a settlement of <i>Authors Guild v. Google</i> was the realization that it offered a way to skirt this problem entirely. <i>Authors Guild</i> was a class action lawsuit, and the class included everyone who held an American copyright in one or more books. In a class action, the named plaintiffs litigate <i>on behalf of</i> the whole class (though anyone who wants to can opt out).</p><p data-flatplan-paragraph="true">So a settlement of the <i>Authors Guild</i> case could theoretically bind just about every author and publisher with a book in an American library. In particular, you could craft a deal in which copyright owners, as a class, agreed to release any claims against Google for scanning and displaying their books, in exchange for a cut of the revenue on sales of those books.</p><p data-flatplan-paragraph="true">“If you have a kind of an institutional problem,” said Jeff Cunard, a partner at Debevoise &amp; Plimpton who represented the publishers in the case, “you can address the issue through a class-action settlement mechanism, which releases all past claims and develops a solution on a going-forward basis. And I think the genius here was of those who saw this as a way of addressing the problem of out-of-print books and liberating them from the dusty corners to which they’d been consigned.”</p><p data-flatplan-paragraph="true">It was a kind<span> </span>of hack. If you could get the class on board with your settlement, and if you could convince a judge to approve it—a step required by law, because you want to make sure the class representatives are acting in the class’s best interests—then you could in one stroke cut the Gordian knot of ambiguous rights to old books. With the class action settlement, authors and publishers who stayed in the class would in effect be saying to Google, “go ahead.”</p><p data-flatplan-paragraph="true">Naturally, they’d have to get something in return. And that was the clever part. At the heart of the settlement was a collective licensing regime for out-of-print books. Authors and publishers could opt out their books at any time. For those who didn’t, Google would be given wide latitude to display and sell their books, but in return, 63 percent of the revenues would go into escrow with a new entity called the Book Rights Registry. The Registry’s job would be to distribute funds to rightsholders as they came forward to claim their works; in ambiguous cases, part of the money would be used to figure out who actually owned the rights.</p><p data-flatplan-paragraph="true">“Book publishing isn’t the healthiest industry in the world, and individual authors don’t make any money out of out-of-print books,” Cunard said to me. “Not that they would have made gazillions of dollars” with Google Books and the Registry, “but they would at least have been paid something for it. <i>And</i> most authors actually want their books to be read.”</p><p data-flatplan-paragraph="true">What became known as the Google Books Search Amended Settlement Agreement came to 165 pages and more than a dozen appendices. It took two and a half years to hammer out the details. Sarnoff described the negotiations as “four-dimensional chess” between the authors, publishers, libraries, and Google. “Everyone involved,” he said to me, “and I mean everyone—on all sides of this issue—thought that if we were going to get this through, this would be the single most important thing they did in their careers.” Ultimately the deal put Google on the hook for about $125 million, including a one-time $45 million payout to the copyright holders of books it had scanned—something like $60 per book—along with $15.5 million in legal fees to the publishers, $30 million to the authors, and $34.5 million toward creating the Registry.</p><p data-flatplan-paragraph="true">But it also set the terms for how out-of-print books, newly freed, would be displayed and sold. Under the agreement, Google would be able to preview up to 20 percent of a given book to entice individual users to buy, and it would be able to offer downloadable copies for sale, with the prices determined by an algorithm or by the individual rightsholder, in price bins initially ranging from $1.99 to $29.99. All the out-of-print books would be packaged into an “institutional subscription database” that would be sold to universities, where students and faculty could search and read the full collection for free. And in §4.8(a), the agreement describes in bland legalese the creation of an incomparable public utility, the “public-access service” that would be deployed on terminals to local libraries across the country.</p><p data-flatplan-paragraph="true">Sorting out the details had taken years of litigation and then years of negotiation, but now, in 2011, there was a plan—a plan that seemed to work equally well for everyone at the table. As Samuelson, the Berkeley law professor, put it in a paper at the time, “The proposed settlement thus looked like a win-win-win: the libraries would get access to millions of books, Google would be able to recoup its investment in GBS, and authors and publishers would get a new revenue stream from books that had been yielding zero returns. And legislation would be unnecessary to bring about this result.”</p><p data-flatplan-paragraph="true">In this, she wrote, it was “perhaps the most adventuresome class action settlement ever attempted.” But to her way of thinking, that was the very reason it should fail.</p><p><b>* * *</b></p><p data-flatplan-paragraph="true" data-flatplan-dropcap="true">T<span>he publication of</span> the Amended Settlement Agreement to the <i>Authors Guild</i> case was headline news. It was quite literally a big deal—a deal that would involve the shakeup of an entire industry. Authors, publishers, Google’s rivals, legal scholars, librarians, the U.S. government, and the interested public paid attention to the case’s every move. When the presiding judge, Denny Chin, put out a call for responses to the proposed settlement, responses came in droves.</p><p data-flatplan-paragraph="true">Those who had been at the table crafting the agreement had expected some resistance, but not the “parade of horribles,” as Sarnoff described it, that they eventually saw. The objections came in many flavors, but they all started with the sense that the settlement was handing to Google, and Google alone, an awesome power. “Did we want the greatest library that would ever exist to be in the hands of one giant corporation, which could really charge almost anything it wanted for access to it?”, Robert Darnton, then president of Harvard’s library, has said.</p><p data-flatplan-paragraph="true">Darnton had initially been supportive of Google’s scanning project, but the settlement made him wary. The scenario he and many others feared was that the same thing that had happened to the academic journal market would happen to the Google Books database. The price would be fair at first, but once libraries and universities became dependent on the subscription, the price would rise and rise until it began to rival the usurious rates that journals were charging, where for instance by 2011 a yearly subscription to the Journal of Comparative Neurology could cost as much as $25,910.</p><p data-flatplan-paragraph="true">Although academics and library enthusiasts like Darnton were thrilled by the prospect of opening up out-of-print books, they saw the settlement as a kind of deal with the devil. Yes, it would create the greatest library there’s ever been—but at the expense of creating perhaps the largest <i>bookstore</i>, too, run by what they saw as a powerful monopolist. In their view, there had to be a better way to unlock all those books. “Indeed, most elements of the GBS settlement would seem to be in the public interest, except for the fact that the settlement restricts the benefits of the deal to Google,” the Berkeley law professor Pamela Samuelson wrote.</p><p data-flatplan-paragraph="true">Certainly Google’s competitors felt put out by the deal. Microsoft, predictably, argued that it would further cement Google’s position as the world’s dominant search engine, by making it the only one that could legally mine out-of-print books. By using those books in results for user’s long-tail queries, Google would have an unfair advantage over competitors. Google’s response to this objection was simply that anyone could scan books and show them in search results if they wanted—and that doing so was fair use. (Earlier this year, a Second Circuit court ruled finally that Google’s scanning of books and display of snippets was, in fact, fair use.)</p><p data-flatplan-paragraph="true">“There was this hypothesis that there was this huge competitive advantage,” Clancy said to me, regarding Google’s access to the books corpus. But he said that the data never ended up being a core part of any project at Google, simply because the amount of information on the web itself dwarfed anything available in books. “You don’t need to go to a book to know when Woodrow Wilson was born,” he said. The books data was helpful, and interesting for researchers, but “the degree to which the naysayers characterized this as being the strategic motivation for the whole project—that was malarkey.”</p><p data-flatplan-paragraph="true">Amazon, for its part, worried that the settlement allowed Google to set up a bookstore that no one else could. Anyone else who wanted to sell out-of-print books, they argued, would have to clear rights on a book-by-book basis, which was as good as impossible, whereas the class action agreement gave Google a license to all of the books at once.</p><p data-flatplan-paragraph="true">This objection got the attention of the Justice Department, in particular the Antitrust division, who began investigating the settlement. In a statement filed with the court, the DOJ argued that the settlement would give Google a de facto monopoly on out-of-print books. That’s because for Google’s competitors to get the same rights to those books, they’d basically have to go through the exact same bizarre process: scan them en masse, get sued in a class action, and try to settle. “Even if there were reason to think history could repeat itself in this unlikely fashion,” the DOJ wrote, “it would scarcely be sound policy to encourage deliberate copyright violations and additional litigation.”</p><p data-flatplan-paragraph="true">Google’s best defense was that the whole point of antitrust law was to protect consumers, and, as one of their lawyers put it, “From the perspective of consumers, one way to get something is unquestionably better than no way to get it at all.” Out-of-print books had been totally inaccessible online; now there’d be a way to buy them. How did that hurt consumers? A person closely involved in the settlement said to me, “Each of the publishers would go into the Antitrust Division and say well but look, Amazon has 80 percent of the e-book market. Google has 0 percent or 1 percent. This is allowing someone else to compete in the digital books space against Amazon. And so you should be regarding this as pro-competitive, not anti-competitive. Which seemed also very sensible to me. But it was like they were talking to a brick wall. And that reaction was shameful.”</p><p data-flatplan-paragraph="true">The DOJ held fast. In some ways, the parties to the settlement didn’t have a good way out: no matter how “non-exclusive” they tried to make the deal, it was in effect a deal that only Google could get—because Google was the only defendant in the case. For a settlement in a class action titled <i>Authors Guild v. Google</i> to include not just Google but, say, every company that wanted to become a digital bookseller, would be to stretch the class action mechanism past its breaking point.</p><p data-flatplan-paragraph="true">This was a point that the DOJ kept coming back to. The settlement was <i>already</i> a stretch, they argued: the original case had been about whether Google could show snippets of books it had scanned, and here you had a settlement agreement that went way beyond that question to create an elaborate online marketplace, one that depended on the indefinite release of copyrights by authors and publishers who might be difficult to find, particularly for books long out of print. “It is an attempt,” they wrote, “to use the class-action mechanism to implement forward-looking business arrangements that go far beyond the dispute before the Court in this litigation.”</p><p data-flatplan-paragraph="true">The DOJ objections left the settlement in a double bind: Focus the deal on Google and you get accused of being anticompetitive. Try to open it up and you get accused of stretching the law governing class actions.</p><p data-flatplan-paragraph="true">The lawyers who had crafted the settlement tried to thread the needle. The DOJ acknowledged as much. “The United States recognizes that the parties to the ASA are seeking to use the class action mechanism to overcome legal and structural challenges to the emergence of a robust and diverse marketplace for digital books,” they wrote. “Despite this worthy goal, the United States has reluctantly concluded that use of the class-action mechanism in the manner proposed by the ASA is a bridge too far.”</p><p data-flatplan-paragraph="true">Their argument was compelling, but the fact that the settlement was ambitious didn’t mean it was illegal—just unprecedented. Years later, another class-action settlement that involved opt-out, “forward-looking business arrangements” very similar to the kind set up by the Google settlement was approved by another district court. That case involved the prospective exploitation of publicity rights of retired NFL players; the settlement made those rights available to an entity that would license them and distribute the proceeds. “What was interesting about it,” says Cunard, who was also involved in that litigation, “was that not a single opponent of the settlement ever raised Judge Chin’s decision or any of the oppositions to it with respect to that settlement being ‘beyond the scope of the pleadings.’” Had that case been decided ten years ago, Cunard said, it would have been “a very important and substantial precedent,” significantly undercutting the “bridge too far” argument against the <i>Authors Guild</i> agreement. “It demonstrates that the law is a very fluid thing,” he said. “Somebody’s got to be first.”</p><p data-flatplan-paragraph="true">In the end, the DOJ’s intervention likely spelled the end of the settlement agreement. No one is quite sure why the DOJ decided to take a stand instead of remaining neutral. Dan Clancy, the Google engineering lead on the project who helped design the settlement, thinks that it was a particular brand of objector—not Google’s competitors but “sympathetic entities” you’d think would be in favor of it, like library enthusiasts, academic authors, and so on—that ultimately flipped the DOJ. “I don’t know how the settlement would have transpired if those naysayers hadn’t been so vocal,” he told me. “It’s not clear to me that if the libraries and the Bob Darntons and the Pam Samuelsons of the world hadn’t been so active that the Justice Department <i>ever</i> would have become involved, because it just would have been Amazon and Microsoft bitching about Google. Which is like yeah, tell me something new.”</p><p data-flatplan-paragraph="true">Whatever the motivation, the DOJ said its piece and that seemed to carry the day. In his ruling concluding that the settlement was not “fair, adequate, and reasonable” under the rules governing class actions, Judge Denny Chin recited the DOJ’s objections and suggested that to fix them, you’d either have to change the settlement to be an opt-in arrangement—which would render it toothless—or try to accomplish the same thing in Congress.</p><p data-flatplan-paragraph="true">“While the digitization of books and the creation of a universal digital library would benefit many,” Chin wrote in his decision, “the ASA would simply go too far.”</p><p><b>* * *</b></p><p data-flatplan-paragraph="true" data-flatplan-dropcap="true">A<span>t the close</span> of the “fairness hearing,” where people spoke for and against the settlement, Judge Chin asked, as if merely out of curiosity, just how many objections had there been? And how many people had opted out of the class? The answers were more than 500, and more than 6,800.</p><p data-flatplan-paragraph="true">Reasonable people could disagree about the legality of the settlement; there were strong arguments on either side, and it was by no means obvious to observers which side Judge Chin was going to come down on. What seemed to turn the tide against the settlement was the reaction of the class itself. “In my more than twenty-five years of practice in class action litigation, I’ve never seen a settlement reacted to that way, with that many objectors,” said Michael Boni, who was the lead negotiator for the authors class in the case. That strong reaction was what likely led to the DOJ’s intervention; it turned public opinion against the agreement; and it may have led Chin to look for ways to kill it. After all, the question before him was whether the agreement was fair to class members. The more class members came out of the woodwork, and the more upset they seemed to be, the more reason he’d have to think that the settlement didn’t represent their interests.</p><p data-flatplan-paragraph="true">The irony is that so many people opposed the settlement in ways that suggested they fundamentally believed in what Google was trying to do. One of Pamela Samuelson’s main objections was that Google was going to be able to <i>sell</i> books like hers, whereas she thought they should be made available for free. (The fact that she, like any author under the terms of the settlement, could set her own books’ price to zero was not consolation enough, because “orphan works” with un-findable authors would still be sold for a price.) In hindsight, it looks like the classic case of perfect being the enemy of the good: surely having the books made available at all would be better than keeping them locked up—even if the price for doing so was to offer orphan works for sale. In her paper concluding that the settlement went too far, Samuelson herself even wrote, “It would be a tragedy not to try to bring this vision to fruition, now that it is so evident that the vision is realizable.”</p><p data-flatplan-paragraph="true">Many of the objectors indeed thought that there would be some other way to get to the same outcome without any of the ickiness of a class action settlement. A refrain throughout the fairness hearing was that releasing the rights of out-of-print books for mass digitization was more properly “a matter for Congress.” When the settlement failed, they pointed to proposals by the U.S. Copyright Office recommending legislation that seemed in many ways inspired by it, and to similar efforts in the Nordic countries to open up out-of-print books, as evidence that Congress could succeed where the settlement had failed.</p><p data-flatplan-paragraph="true">Of course, nearly a decade later, nothing of the sort has actually happened. “It has got no traction,” Cunard said to me about the Copyright Office’s proposal, “and is not going to get a lot of traction now I don’t think.” Many of the people I spoke to who were in favor of the settlement said that the objectors simply weren’t practical-minded—they didn’t seem to understand how things actually get done in the world. “They felt that if not for us and this lawsuit, there was some other future where they could unlock all these books, because Congress would pass a law or something. And that future... as soon as the settlement with <i>Guild</i>, nobody gave a shit about this anymore,” Clancy said to me.</p><p data-flatplan-paragraph="true">It certainly seems unlikely that someone is going to spend political capital—especially today—trying to change the licensing regime for <i>books</i>, let alone old ones. “This is not important enough for the Congress to somehow adjust copyright law,” Clancy said. “It’s not going to get anyone elected. It’s not going to create a whole bunch of jobs.” It’s no coincidence that a class action against Google turned out to be perhaps the only plausible venue for this kind of reform: Google was the only one with the initiative, and the money, to make it happen. “If you want to look at this in a raw way,” Allan Adler, in-house counsel for the publishers, said to me, “a deep pocketed, private corporate actor was going to foot the bill for something that everyone wanted to see.” Google poured resources into the project, not just to scan the books but to dig up and digitize old copyright records, to negotiate with authors and publishers, to foot the bill for a Books Rights Registry. Years later, the Copyright Office has gotten nowhere with a proposal that re-treads much the same ground, but whose every component would have to be funded with Congressional appropriations.</p><p data-flatplan-paragraph="true">I asked Bob Darnton, who ran Harvard’s library during the Google Books litigation and who spoke out against the settlement, whether he had any regrets about what ended up happening. “Insofar as I have a regret, it is that the attempts to out-Google Google are so limited by copyright law,” he said. He’s been working on another project to scan library books; the scanning has been limited to books in the public domain. “I’m in favor of copyright, don’t get me wrong, but really to leave books out of the public domain for more than a century—to keep most American literature behind copyright barrier,” he said, “I think is crazy.”</p><p data-flatplan-paragraph="true">The first copyright statute in the United States, passed in 1790, was called An Act for the Encouragement of Learning. Copyright terms were to last fourteen years, with the option to renew for another fourteen, but only if the author was alive at the end of the first term. The idea was to strike a “pragmatic bargain” between authors and the reading public. Authors would get a limited monopoly on their work so they could make a living from it; but their work would retire quickly into the public domain.</p><p data-flatplan-paragraph="true">Copyright terms have been radically extended in this country largely to keep pace with Europe, where the standard has long been that copyrights last for the life of the author plus 50 years. But the European idea, “It’s based on natural law as opposed to positive law,” Lateef Mtima, a copyright scholar at Howard University Law School, said. “Their whole thought process is coming out of France and Hugo and those guys that like, you know, ‘My work is my <i>enfant</i>,’” he said, “and the <i>state</i> has absolutely no right to do anything with it—kind of a Lockean point of view.” As the world has flattened, copyright laws have converged, lest one country be at a disadvantage by freeing its intellectual products for exploitation by the others. And so the American idea of using copyright primarily as a vehicle, per the constitution, “to promote the Progress of Science and useful Arts,” not to protect authors, has eroded to the point where today we’ve locked up nearly every book published after 1923.</p><p data-flatplan-paragraph="true">“The greatest tragedy is we are still exactly where we were on the orphan works question. That stuff is just sitting out there gathering dust and decaying in physical libraries, and with very limited exceptions,” Mtima said, “<i>nobody</i> can use them. So everybody has lost and no one has won.”</p><p data-flatplan-paragraph="true" data-flatplan-dropcap="true">A<span>fter the settlement </span>failed, Clancy told me that at Google “there was just this air let out of the balloon.” Despite eventually winning <i>Authors Guild v. Google</i>, and having the courts declare that displaying snippets of copyrighted books was fair use, the company all but shut down its scanning operation.</p><p data-flatplan-paragraph="true">It was strange to me, the idea that somewhere at Google there is a database containing 25-million books and nobody is allowed to read them. It’s like that scene at the end of the first <i>Indiana Jones</i> movie where they put the Ark of the Covenant back on a shelf somewhere, lost in the chaos of a vast warehouse. It’s <i>there</i>. The books are <i>there</i>. People have been trying to build a library like this for ages—to do so, they’ve said, would be to erect one of the great humanitarian artifacts of all time—and here we’ve done the work to make it real and we were about to give it to the world and now, instead, it’s 50 or 60 petabytes on disk, and the only people who can see it are half a dozen engineers on the project who happen to have access because they’re the ones responsible for locking it up.</p><p data-flatplan-paragraph="true">I asked someone who used to have that job, what would it take to make the books viewable in full to everybody? I wanted to know how hard it would have been to unlock them. What’s standing between us and a digital public library of 25 million volumes?</p><p data-flatplan-paragraph="true">You’d get in a lot of trouble, they said, but all you’d have to do, more or less, is write a single database query. You’d flip some access control bits from off to on. It might take a few minutes for the command to propagate.</p></section><div data-event-module="footer"><p><h3>About the Author</h3></p><div><address id="article-writer-0" data-event-element="author" data-event-position="1" data-flatplan-bio="true"><div><p><a href="https://www.theatlantic.com/author/james-somers/" data-event-element="image"><img alt="" loading="lazy" src="https://cdn.theatlantic.com/thumbor/dAWiCuHMQIN5yuJ84kZJUGUhIUY=/32x556:3000x3524/120x120/media/None/headshot-15/original.jpg" width="60" height="60"></a></p><div><p><a href="https://www.theatlantic.com/author/james-somers/" data-label="https://www.theatlantic.com/author/james-somers/" data-action="click author - name">James Somers</a> is a former contributing editor at <em>The Atlantic</em>.</p></div></div></address></div></div><gpt-ad format="injector" sizes-at-0="mobile-wide,native,house" targeting-pos="injector-most-popular" sizes-at-976="desktop-wide,native,house"></gpt-ad></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[USGS uses machine learning to show large lithium potential in Arkansas (282 pts)]]></title>
            <link>https://www.usgs.gov/news/national-news-release/unlocking-arkansas-hidden-treasure-usgs-uses-machine-learning-show-large</link>
            <guid>41916322</guid>
            <pubDate>Tue, 22 Oct 2024 17:05:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.usgs.gov/news/national-news-release/unlocking-arkansas-hidden-treasure-usgs-uses-machine-learning-show-large">https://www.usgs.gov/news/national-news-release/unlocking-arkansas-hidden-treasure-usgs-uses-machine-learning-show-large</a>, See on <a href="https://news.ycombinator.com/item?id=41916322">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>The study’s novel methodology, carried out collaboratively by the USGS and the Arkansas Department of Energy and Environment’s Office of the State Geologist, made it possible to quantify the amount of lithium present in brines located in a geological unit known as the Smackover Formation. Extracting lithium from brines co-produced during oil and gas operations provides an opportunity to extract a valuable commodity from what would otherwise be considered a waste stream.&nbsp; &nbsp;</p>
<p>“Lithium is a critical mineral for the energy transition, and the potential for increased U.S. production to replace imports has implications for employment, manufacturing and supply-chain resilience. This study illustrates the value of science in addressing economically important issues,” said <strong>David Applegate, USGS Director</strong>.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>
<p>The Smackover Formation is a relic of an ancient sea that left an extensive, porous, and permeable limestone geologic unit that extends under parts of Arkansas, Louisiana, Texas, Alabama, Mississippi, and Florida. It dates to the Jurassic geological time period and is known for its rich deposits of oil and bromine. In recent years, the Smackover Formation has also gained attention for potential lithium in brines—high-salinity waters associated with deep salt deposits.&nbsp;</p>
<p>“Our research was able to estimate total lithium present in the southwestern portion of the Smackover in Arkansas for the first time.&nbsp; We estimate there is enough dissolved lithium present in that region to replace U.S. imports of lithium and more.&nbsp; It is important to caution that these estimates are an in-place assessment. We have not estimated what is technically recoverable based on newer methods to extract lithium from brines,” said <strong>Katherine Knierim, a hydrologist and the study’s principal researcher</strong>.&nbsp; &nbsp;</p>
<p>Global demand for lithium, a critical mineral essential for battery production, has increased substantially in recent years. This trend is projected to persist as the transition from fossil fuels to electric and hybrid vehicles intensifies, underscoring the mineral's growing significance in energy technology advancements.&nbsp;</p>
<p>The U.S. relies on imports for more than 25% of its lithium. The USGS estimates there is enough lithium brought to the surface in the oil and brine waste streams in southern Arkansas to cover current estimated U.S. &nbsp;lithium consumption. &nbsp;The low-end estimate of 5 million tons of lithium present in Smackover brines is also equivalent to more than nine times the International Energy Agency’s projection of global lithium demand for electric vehicles in 2030.&nbsp;</p>
<p>The USGS predictive model provides the first estimate of total lithium present in Smackover Formation brines in southern Arkansas, using machine learning, which is a type of artificial intelligence. Samples from Arkansas were analyzed by the USGS Brine Research Instrumentation and Experimental lab in Reston, VA, and then compared with data from historic samples within the <a href="https://www.usgs.gov/tools/us-geological-survey-national-produced-waters-geochemical-database-viewer"><u>USGS Produced Waters Database</u></a> of water from hydrocarbon production.&nbsp; The machine learning model was then used to combine lithium concentrations in brines with geological data to create maps that predict total lithium concentrations across the region, even in areas lacking lithium samples.&nbsp;&nbsp;</p>
<p>"The USGS – and science as well-- works best as a partnership, and this important research was possible because of our strong partnership with the Office of the Arkansas State Geologist,” said <strong>Dr. Knierim</strong>.&nbsp;</p>
<p>Since 1879, the USGS has been the nation’s primary source of impartial scientific information on geologic, energy and mineral resources. The USGS also tracks lithium production, demand and imports in the U.S. as part of its role mandated by the Energy Act of 2020 to maintain the whole of government List of Critical Minerals.&nbsp; &nbsp;</p>
<figure role="group">
  <div>
    
  

    <figcaption><em>This map of the U.S. shows an inset area displaying highlighted areas for the Smackover Formation and sampling area. The Smackover formation (highlighted in yellow) covers the southern to eastern portion of Texas, southern portion of Arkansas, the upper half of Louisiana and some eastern areas, southern half of Mississippi, southwest area of Alabama, and portions of the Florida panhandle. The sampling area is located in the lower portion of Arkansas (highlighted with red stripes).</em></figcaption>
  </div>
</figure>
<p>The study, which was published in Science Advances, can be found at <a href="https://www.science.org/doi/10.1126/sciadv.adp8149"><u>https://www.science.org/doi/10.1126/sciadv.adp8149</u></a>.&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Open-source Counter-Strike-like game (298 pts)]]></title>
            <link>https://github.com/solcloud/Counter-Strike</link>
            <guid>41915958</guid>
            <pubDate>Tue, 22 Oct 2024 16:27:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/solcloud/Counter-Strike">https://github.com/solcloud/Counter-Strike</a>, See on <a href="https://news.ycombinator.com/item?id=41915958">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Competitive multiplayer FPS game where two football fan teams fight with the goal of winning more rounds than the opponent team.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/74121353/271781704-dfca8ed0-4624-4199-8d4c-336e101e0922.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk2MjkzMDIsIm5iZiI6MTcyOTYyOTAwMiwicGF0aCI6Ii83NDEyMTM1My8yNzE3ODE3MDQtZGZjYThlZDAtNDYyNC00MTk5LThkNGMtMzM2ZTEwMWUwOTIyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDIwMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE4MDdlMGIyZjZlMzk0MmU1YmY3NDc4YmJiOGMwMWFjMDVlZjJlYjc5YTc5MDcxNzY4YTU3MjkwZjUyZGVhN2YmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.QPAsfIgYn2p8cVjvgMG8qtTdXfhgOuXNAff3J2dMfNc"><img src="https://private-user-images.githubusercontent.com/74121353/271781704-dfca8ed0-4624-4199-8d4c-336e101e0922.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk2MjkzMDIsIm5iZiI6MTcyOTYyOTAwMiwicGF0aCI6Ii83NDEyMTM1My8yNzE3ODE3MDQtZGZjYThlZDAtNDYyNC00MTk5LThkNGMtMzM2ZTEwMWUwOTIyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDIwMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE4MDdlMGIyZjZlMzk0MmU1YmY3NDc4YmJiOGMwMWFjMDVlZjJlYjc5YTc5MDcxNzY4YTU3MjkwZjUyZGVhN2YmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.QPAsfIgYn2p8cVjvgMG8qtTdXfhgOuXNAff3J2dMfNc" alt="promo"></a></p>
<p dir="auto">Teams are Attackers and Defenders. The Defenders team has a goal of protecting their fan base sanctuary every round from desecrating by attackers using their graffiti bomb ball.</p>
<p dir="auto">Defenders win round by:</p>
<ul dir="auto">
<li>eliminated all attackers players before bomb planted (touchdown)</li>
<li>defusing bomb before it blows (graffiti fireworks)</li>
<li>at least one player survive round time and no bomb is planted</li>
</ul>
<p dir="auto">Attackers win round by:</p>
<ul dir="auto">
<li>eliminated all defenders players before round time ends</li>
<li>planted bomb (touchdown) before round time ends and don't allow defenders to defuse it</li>
</ul>
<p dir="auto">If attackers deploy graffiti bomb before round time ends, then round clock countdown is set to 40 sec and defenders has 30 sec (or 35 sec in case of using defuse kit) to defuse bomb.</p>
<p dir="auto"><em>This is low violence game so there is no red blood, animal killing or similar violence visuals.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Client</h3><a id="user-content-client" aria-label="Permalink: Client" href="#client"></a></p>
<p dir="auto">Download executable asset for your OS platform from the <a href="https://github.com/solcloud/Counter-Strike/releases/latest">latest release</a>. Or build by yourself locally from the project source folder.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd electron/
npm install
npm run dev"><pre><span>cd</span> electron/
npm install
npm run dev</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Server</h3><a id="user-content-server" aria-label="Permalink: Server" href="#server"></a></p>
<p dir="auto">Currently, there is no official public server available (as match making service is also WIP), but you can run server yourself (or somebody can host it for you).</p>
<div dir="auto" data-snippet-clipboard-copy-content="composer install -a --no-dev
php cli/server.php 2 # will start server waiting for 2 players to connect"><pre>composer install -a --no-dev
php cli/server.php 2 <span><span>#</span> will start server waiting for 2 players to connect</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Help us</h2><a id="user-content-help-us" aria-label="Permalink: Help us" href="#help-us"></a></p>
<p dir="auto">If you know html/css/js/php languages or 3D modeling/texturing/animation you can join the project and help us improve this game quicker by sending a pull request.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The PlanetScale vectors public beta (104 pts)]]></title>
            <link>https://planetscale.com/blog/announcing-planetscale-vectors-public-beta</link>
            <guid>41915724</guid>
            <pubDate>Tue, 22 Oct 2024 16:06:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://planetscale.com/blog/announcing-planetscale-vectors-public-beta">https://planetscale.com/blog/announcing-planetscale-vectors-public-beta</a>, See on <a href="https://news.ycombinator.com/item?id=41915724">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header><p>You can now use the vector data type for vector search and storage in your PlanetScale MySQL database.</p></header><div><p><img alt="Announcing the PlanetScale vectors public beta" fetchpriority="high" width="1280" height="720" decoding="async" data-nimg="1" sizes="100vw" srcset="https://planetscale.com/_next/image?url=%2Fassets%2Fblog%2Fcontent%2Fannouncing-planetscale-vectors-public-beta%2Fvectors-blog.png&amp;w=640&amp;q=90 640w, https://planetscale.com/_next/image?url=%2Fassets%2Fblog%2Fcontent%2Fannouncing-planetscale-vectors-public-beta%2Fvectors-blog.png&amp;w=750&amp;q=90 750w, https://planetscale.com/_next/image?url=%2Fassets%2Fblog%2Fcontent%2Fannouncing-planetscale-vectors-public-beta%2Fvectors-blog.png&amp;w=828&amp;q=90 828w, https://planetscale.com/_next/image?url=%2Fassets%2Fblog%2Fcontent%2Fannouncing-planetscale-vectors-public-beta%2Fvectors-blog.png&amp;w=1080&amp;q=90 1080w, https://planetscale.com/_next/image?url=%2Fassets%2Fblog%2Fcontent%2Fannouncing-planetscale-vectors-public-beta%2Fvectors-blog.png&amp;w=1200&amp;q=90 1200w, https://planetscale.com/_next/image?url=%2Fassets%2Fblog%2Fcontent%2Fannouncing-planetscale-vectors-public-beta%2Fvectors-blog.png&amp;w=1920&amp;q=90 1920w, https://planetscale.com/_next/image?url=%2Fassets%2Fblog%2Fcontent%2Fannouncing-planetscale-vectors-public-beta%2Fvectors-blog.png&amp;w=2048&amp;q=90 2048w, https://planetscale.com/_next/image?url=%2Fassets%2Fblog%2Fcontent%2Fannouncing-planetscale-vectors-public-beta%2Fvectors-blog.png&amp;w=3840&amp;q=90 3840w" src="https://planetscale.com/_next/image?url=%2Fassets%2Fblog%2Fcontent%2Fannouncing-planetscale-vectors-public-beta%2Fvectors-blog.png&amp;w=3840&amp;q=90"></p></div><p><label for="toc">Table of contents</label></p><section><p>We're excited to announce that PlanetScale vector search and storage is now available in open beta! With PlanetScale vector support, you can store your vector data alongside your application's relational MySQL data — eliminating the need for a separate specialized vector database.</p><h2 id="what-sets-planetscale-vector-search-and-storage-apart-"><a href="#what-sets-planetscale-vector-search-and-storage-apart-">What sets PlanetScale vector search and storage apart?</a></h2><p>When we decided to add vector support to PlanetScale's MySQL fork, we knew it would be a long journey to ensure the solution meets our high standards for performance and scalability.</p><p>A crucial piece of this was architecting a search algorithm that allows for both fast performance and large scale. PlanetScale's vector search is based on two innovative research papers from Microsoft Research: SPANN (Space-Partitioned Approximate Nearest Neighbors) and SPFresh. We did additional work to fully integrate our solution with InnoDB and Vitess, which allows us to support transactional operations, ensure data consistency, and efficiently manage vector indexes at terabyte-scale. This makes our vector search solution ideal for large-scale databases.</p><p>On top of that, we wanted to make sure our implementation supports the following:</p><ul><li>Pre-filtering and post-filtering</li><li>Full SQL syntax — including <code>JOIN</code>, <code>WHERE</code>, and subqueries</li><li>ACID compliance</li></ul><p>Our base implementation checks all of these boxes, and we will continue to improve performance leading up to GA.</p><h3 id="choosing-a-vector-search-algorithm"><a href="#choosing-a-vector-search-algorithm">Choosing a vector search algorithm</a></h3><p>There are a few different algorithms commonly used to implement vector search: <a href="https://planetscale.com/docs/vectors/terminology-and-concepts#hierarchical-navigable-small-world-hnsw-">Hierarchical Navigable Small Worlds (HNSW)</a> and <a href="https://planetscale.com/docs/vectors/terminology-and-concepts#diskann">DiskANN</a> being two of the most popular. These algorithms, however, make technical trade-offs that we deemed inadequate for our implementation.</p><p>HNSW has very good query performance, but struggles to scale because it needs to fit its whole dataset in RAM. Most importantly, HNSW indexes cannot be updated incrementally, so they require periodically re-building the index with the underlying vector data. This is just not a good fit for a relational database. DiskANN scales well, but suffers from worse query performance, and while it can be modified to allow incremental updates, these are not particularly efficient and are hard to map to transactional SQL semantics.</p><p>Because PlanetScale is designed to support incredible performance for databases at massive scale, we knew these implementations wouldn't work for us. So we set out to find a better solution.</p><p>PlanetScale vector search is based on a novel implementation of two state-of-the art papers from Microsoft Research: <a href="https://arxiv.org/abs/2111.08566">SPANN</a> (Space-Partitioned Approximate Nearest Neighbors) and <a href="https://dl.acm.org/doi/10.1145/3600006.3613166">SPFresh</a>. <a href="https://planetscale.com/docs/vectors/terminology-and-concepts#space-partitioned-approximate-nearest-neighbors-spann-">SPANN is a hybrid vector indexing and search algorithm</a> that uses both graph and tree structures, and was specifically designed to work well for larger-than-RAM indexes that require SSD usage. SPFresh extends the design of SPANN with a set of concurrent background maintenance operations that allow the index to be continuously updated without losing recall or query performance.</p><p>For our implementation, we have extended SPFresh by adding transactional support to all its operations and fully integrating it inside InnoDB, MySQL's default storage engine. This means that inserts, updates, and deletes of vector data are immediately reflected on the vector index as part of committing your SQL transaction, and follow the same transactional semantics, including support for batch commits and rollbacks.</p><p>Since the indexes are fully managed and stored on-disk by InnoDB, they are always in-sync with the vector data in your tables, they survive process crashes with strong consistency guarantees, they do <em>not</em> need to be periodically rebuilt, and they scale all the way into terabytes, just like any other MySQL table. Together with Vitess, PlanetScale's sharding layer, this allows the construction and efficient querying of huge vector indexes that are fully integrated with all the relational data in your database and can be used with <code>JOIN</code>s and <code>WHERE</code> clauses while the underlying vector data is continuously updated.</p><h2 id="how-to-enable-vector-support"><a href="#how-to-enable-vector-support">How to enable vector support</a></h2><p>To get started, go to your database settings page, click "Beta features", find Vectors and click "Enroll". Vector support is enabled at the branch level, so choose the branch you wish to enroll into the vectors beta. Click the gear icon on the branches page, and click the toggle next to "Enable vectors".</p><h2 id="more-resources"><a href="#more-resources">More resources</a></h2><p>To learn more about vectors embeddings, checkout our YouTube video:</p><p>You can check out the following documentation:</p><ul><li><a href="https://planetscale.com/docs/vectors/overview">PlanetScale vectors overview</a></li><li><a href="https://planetscale.com/docs/vectors/terminology-and-concepts#space-partitioned-approximate-nearest-neighbors-spann-">Vector database terminology and concepts</a></li><li><a href="https://planetscale.com/docs/vectors/use-cases">Common use cases for vector search</a></li><li><a href="https://planetscale.com/docs/vectors/using-with-an-orm">PlanetScale vector usage with ORMs</a></li><li><a href="https://planetscale.com/docs/vectors/reference">Vector type and index reference</a></li></ul><p>Your feedback is extremely valuable during this beta period, so don’t hesitate to reach out. You can <a href="https://planetscale.com/contact">submit a support ticket</a> to relay any feedback or issues. We also have a vectors channel in <a href="https://discord.gg/pDUGAAFEJx">our Discord</a> where you can ask questions, share feedback, or chat about use cases.</p></section></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FTC's rule banning fake online reviews goes into effect (488 pts)]]></title>
            <link>https://abcnews.go.com/US/wireStory/ftcs-rule-banning-fake-online-reviews-effect-115009298</link>
            <guid>41915187</guid>
            <pubDate>Tue, 22 Oct 2024 15:21:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abcnews.go.com/US/wireStory/ftcs-rule-banning-fake-online-reviews-effect-115009298">https://abcnews.go.com/US/wireStory/ftcs-rule-banning-fake-online-reviews-effect-115009298</a>, See on <a href="https://news.ycombinator.com/item?id=41915187">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="prism-article-body"><p><span>WASHINGTON -- </span>A federal rule <a data-testid="prism-linkbase" href="https://apnews.com/article/small-business-fake-reviews-ftc-9858b1b84c2e54c70e55179b10cca3e5">banning fake online reviews</a> is now in effect. </p><p>The Federal Trade Commission <a data-testid="prism-linkbase" href="https://www.ftc.gov/news-events/news/press-releases/2024/08/federal-trade-commission-announces-final-rule-banning-fake-reviews-testimonials">issued the rule</a> in August banning the sale or purchase of online reviews. The rule, which went into effect Monday, allows the agency to seek civil penalties against those who knowingly violate it.</p><p>“Fake reviews not only waste people’s time and money, but also pollute the marketplace and divert business away from honest competitors,” FTC Chair Lina Khan said about the rule in August. She added that the rule will “protect Americans from getting cheated, put businesses that unlawfully game the system on notice, and promote markets that are fair, honest, and competitive.”</p><p>Specifically, the rule bans reviews and testimonials attributed to people who don’t exist or are generated by artificial intelligence, people who don’t have experience with the business or product/services, or misrepresent their experience.</p><p>It also bans businesses from creating or selling reviews or testimonials. Businesses that knowingly buy fake reviews, procure them from company insiders or disseminate fake reviews will be penalized. It also prohibits businesses from using “unfounded or groundless legal threats, physical threats, intimidation, or certain false public accusations.”</p><p>People can report violations at <a data-testid="prism-linkbase" href="https://t.co/k0O6juZImU">https://reportfraud.ftc.gov</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku (1176 pts)]]></title>
            <link>https://www.anthropic.com/news/3-5-models-and-computer-use</link>
            <guid>41914989</guid>
            <pubDate>Tue, 22 Oct 2024 15:02:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/3-5-models-and-computer-use">https://www.anthropic.com/news/3-5-models-and-computer-use</a>, See on <a href="https://news.ycombinator.com/item?id=41914989">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Today, we’re announcing an <strong>upgraded Claude 3.5 Sonnet</strong>, and a new model, <strong>Claude 3.5 Haiku</strong>. The upgraded Claude 3.5 Sonnet delivers across-the-board improvements over its predecessor, with particularly significant gains in coding—an area where it already led the field. Claude 3.5 Haiku matches the performance of Claude 3 Opus, our prior largest model, on many evaluations for the same cost and similar speed to the previous generation of Haiku.</p><p>We’re also introducing a groundbreaking new capability in public beta: <strong>computer use</strong>. Available <a href="https://docs.anthropic.com/en/docs/build-with-claude/computer-use">today on the API</a>, developers can direct Claude to use computers the way people do—by looking at a screen, moving a cursor, clicking buttons, and typing text. Claude 3.5 Sonnet is the first frontier AI model to offer computer use in public beta. At this stage, it is still <a href="https://www.anthropic.com/news/developing-computer-use">experimental</a>—at times cumbersome and error-prone. We're releasing computer use early for feedback from developers, and expect the capability to improve rapidly over time.</p><p>Asana, Canva, Cognition, DoorDash, Replit, and The Browser Company have already begun to explore these possibilities, carrying out tasks that require dozens, and sometimes even hundreds, of steps to complete. For example, Replit is using Claude 3.5 Sonnet's capabilities with computer use and UI navigation to develop a key feature that evaluates apps as they’re being built for their Replit Agent product.</p><p>The upgraded Claude 3.5 Sonnet is now available for all users. Starting today, developers can build with the computer use beta on the Anthropic API, Amazon Bedrock, and Google Cloud’s Vertex AI. The new Claude 3.5 Haiku will be released later this month.</p><h3>Claude 3.5 Sonnet: Industry-leading software engineering skills</h3><p>The updated <a href="https://www.anthropic.com/claude/sonnet">Claude 3.5 Sonnet</a> shows wide-ranging improvements on industry benchmarks, with particularly strong gains in agentic coding and tool use tasks. On coding, it improves performance on <a href="https://www.swebench.com/">SWE-bench Verified</a> from 33.4% to 49.0%, scoring higher than all publicly available models—including reasoning models like OpenAI o1-preview and specialized systems designed for agentic coding. It also improves performance on <a href="https://github.com/sierra-research/tau-bench">TAU-bench</a>, an agentic tool use task, from 62.6% to 69.2% in the retail domain, and from 36.0% to 46.0% in the more challenging airline domain. The new Claude 3.5 Sonnet offers these advancements at the same price and speed as its predecessor.</p><p>Early customer feedback suggests the upgraded Claude 3.5 Sonnet represents a significant leap for AI-powered coding. GitLab, which tested the model for DevSecOps tasks, found it delivered stronger reasoning (up to 10% across use cases) with no added latency, making it an ideal choice to power multi-step software development processes. Cognition uses the new Claude 3.5 Sonnet for autonomous AI evaluations, and experienced substantial improvements in coding, planning, and problem-solving compared to the previous version. The Browser Company, in using the model for automating web-based workflows, noted Claude 3.5 Sonnet outperformed every model they’ve tested before.</p><p>As part of our continued effort to partner with external experts, joint pre-deployment testing of the new Claude 3.5 Sonnet model was conducted by the US AI Safety Institute (US AISI) and the UK Safety Institute (UK AISI).</p><p>We also evaluated the upgraded Claude 3.5 Sonnet for catastrophic risks and found that the ASL-2 Standard, as outlined in our <a href="https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy">Responsible Scaling Policy</a>, remains appropriate for this model.</p><h3>Claude 3.5 Haiku: State-of-the-art meets affordability and speed</h3><p><a href="https://www.anthropic.com/claude/haiku">Claude 3.5 Haiku</a> is the next generation of our fastest model. For the same cost and similar speed to Claude 3 Haiku, Claude 3.5 Haiku improves across every skill set and surpasses even Claude 3 Opus, the largest model in our previous generation, on many intelligence benchmarks. Claude 3.5 Haiku is particularly strong on coding tasks. For example, it scores 40.6% on SWE-bench Verified, outperforming many agents using publicly available state-of-the-art models—including the original Claude 3.5 Sonnet and GPT-4o.</p><p>With low latency, improved instruction following, and more accurate tool use, Claude 3.5 Haiku is well suited for user-facing products, specialized sub-agent tasks, and generating personalized experiences from huge volumes of data—like purchase history, pricing, or inventory records.</p><p>Claude 3.5 Haiku will be made available later this month across our first-party API, Amazon Bedrock, and Google Cloud’s Vertex AI—initially as a text-only model and with image input to follow.</p><h3>Teaching Claude to navigate computers, responsibly</h3><p>With computer use, we're trying something fundamentally new. Instead of making specific tools to help Claude complete individual tasks, we're teaching it <em>general</em> computer skills—allowing it to use a wide range of standard tools and software programs designed for people. Developers can use this nascent capability to automate repetitive processes, <a href="https://www.youtube.com/watch?v=vH2f7cjXjKI">build and test software</a>, and <a href="https://youtu.be/jqx18KgIzAE">conduct open-ended tasks like research</a>.</p><p>To make these general skills possible, we've built an API that allows Claude to perceive and interact with computer interfaces. Developers can integrate this API to enable Claude to translate instructions (e.g., “use data from my computer and online to fill out this form”) into computer commands (e.g. check a spreadsheet; move the cursor to open a web browser; navigate to the relevant web pages; fill out a form with the data from those pages; and so on). On <a href="https://os-world.github.io/">OSWorld</a>, which evaluates AI models' ability to use computers like people do, Claude 3.5 Sonnet scored 14.9% in the screenshot-only category—notably better than the next-best AI system's score of 7.8%. When afforded more steps to complete the task, Claude scored 22.0%.</p><p>While we expect this capability to improve rapidly in the coming months, Claude's current ability to use computers is imperfect. Some actions that people perform effortlessly—scrolling, dragging, zooming—currently present challenges for Claude and we encourage developers to begin exploration with low-risk tasks. Because computer use may provide a new vector for more familiar threats such as spam, misinformation, or fraud, we're taking a proactive approach to promote its safe deployment. We've developed new classifiers that can identify when computer use is being used and whether harm is occurring. You can read more about the research process behind this new skill, along with further discussion of safety measures, in our post on <a href="http://anthropic.com/news/developing-computer-use">developing computer use</a>.</p><h3>Looking ahead</h3><p>Learning from the initial deployments of this technology, which is still in its earliest stages, will help us better understand both the potential and the implications of increasingly capable AI systems.</p><p>We’re excited for you to explore <a href="https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf">our new models</a> and the public beta of computer use—and welcome you to <a href="mailto:feedback@anthropic.com">share your feedback</a> with us. We believe these developments will open up new possibilities for how you work with Claude, and we look forward to seeing what you'll create.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We built a new powerful JSON data type for ClickHouse (313 pts)]]></title>
            <link>https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse</link>
            <guid>41914845</guid>
            <pubDate>Tue, 22 Oct 2024 14:47:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse">https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse</a>, See on <a href="https://news.ycombinator.com/item?id=41914845">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><a href="https://www.json.org/json-en.html">JSON</a> has become the lingua franca for handling semi-structured and unstructured data in modern data systems. Whether it’s in logging and <a href="https://clickhouse.com/blog/the-state-of-sql-based-observability">observability</a> scenarios, real-time data streaming, mobile app storage, or machine learning pipelines, JSON’s flexible structure makes it the go-to format for capturing and transmitting data across distributed systems.</p>
<p>At ClickHouse, we’ve long <a href="https://github.com/ClickHouse/ClickHouse/issues/23516">recognized</a> the importance of seamless JSON support. But as simple as JSON seems, leveraging it effectively at scale presents unique challenges, which we briefly describe below.</p>

<p>ClickHouse <a href="https://benchmark.clickhouse.com/">is</a> amongst the <a href="https://www.vldb.org/pvldb/vol17/p3731-schulze.pdf">fastest</a> analytical databases on the market. Such a level of performance can only be achieved with the right data “orientation”. ClickHouse is a <a href="https://clickhouse.com/docs/en/about-us/distinctive-features#true-column-oriented-database-management-system">true</a> <a href="https://clickhouse.com/docs/en/faq/general/columnar-database">column-oriented</a> database that stores tables as a collection of column data files on disk. This enables optimal <a href="https://clickhouse.com/docs/en/data-compression/compression-in-clickhouse">compression</a> and hardware-efficient, blazing-fast, <a href="https://clickhouse.com/docs/en/development/architecture">vectorized</a> column operations such as filters or <a href="https://clickhouse.com/blog/clickhouse_vs_elasticsearch_mechanics_of_count_aggregations">aggregations</a>.</p>
<p>To enable the same level of performance for JSON data, we needed to implement <a href="https://clickhouse.com/docs/en/about-us/distinctive-features#true-column-oriented-database-management-system">true</a> column-oriented storage for JSON, such that JSON paths can be compressed and processed (e.g., filtered and aggregated in a vectorized way) as efficiently as all other column types like numerics.</p>
<p>Therefore, instead of blindly dumping (and later <a href="https://clickhouse.com/docs/en/sql-reference/functions/json-functions">parsing</a>) JSON documents into a string column, as sketched in the following diagram:</p>
<p><img src="https://clickhouse.com/uploads/JSON_01_1b40b01231.png" alt="JSON-01.png" node="[object Object]"></p>
<p>We wanted to store the values of each unique JSON path in a true columnar fashion:</p>
<p><img src="https://clickhouse.com/uploads/JSON_02_c5811c3a53.png" alt="JSON-02.png" node="[object Object]"></p>

<p>If we can store JSON paths in a true columnar fashion, the next challenge is that JSON allows values with different data types for the same JSON paths. For ClickHouse, such different data types are possibly incompatible and not known beforehand. Furthermore, we needed to find a way to preserve all data types instead of unifying them into the least common type. For example, if we have two integers and a float as values for the same JSON path <code>a</code>, we don’t want to store all three as float values on disk, as shown in this figure:</p>
<p><img src="https://clickhouse.com/uploads/JSON_03_d34ab653ae.png" alt="JSON-03.png" node="[object Object]"></p>
<p>As such an approach would not preserve the integrity of mixed-type data and would also not support more complex scenarios, for example, if the next stored value under the same path <code>a</code> is an array:</p>
<p><img src="https://clickhouse.com/uploads/JSON_04_85b5392a1b.png" alt="JSON-04.png" node="[object Object]"></p>

<p>Storing JSON paths in a true columnar fashion has advantages for data compression and vectorized data processing. However, blindly creating a new column file per new unique JSON path can end up in an avalanche of column files on disk in scenarios with a high number of unique JSON keys:</p>
<p><img src="https://clickhouse.com/uploads/JSON_05_8c3f9a39b4.png" alt="JSON-05.png" node="[object Object]"></p>
<p>This can create performance issues, as this requires a high number of <a href="https://en.wikipedia.org/wiki/File_descriptor">file descriptors</a> (requiring space in memory each) and affects the performance of merges due to a large number of files to process.  As a result, we needed to introduce limits on column creation. This enables JSON storage to scale effectively, ensuring high-performance analytics over petabyte-scale datasets.</p>

<p>In scenarios with a high number of unique but sparse JSON keys, we wanted to avoid having to redundantly store (and process) NULL or default values for rows that don’t have a real value for a specific JSON path, as sketched in this diagram:</p>
<p><img src="https://clickhouse.com/uploads/JSON_06_81504bcb8e.png" alt="JSON-06.png" node="[object Object]"></p>
<p>Instead, we wanted to store the values of each unique JSON path in a dense, non-redundant way. Again, this allows JSON storage to scale for high-performance analytics over PB datasets.</p>

<p>We’re excited to introduce our new and significantly enhanced <a href="https://clickhouse.com/docs/en/sql-reference/data-types/newjson">JSON data type</a>, which is purpose-built to deliver high-performance handling of JSON data without the bottlenecks that traditional implementations often face.</p>
<p>In this first post, we’ll dive into how we built this feature, addressing all of the aforementioned challenges (and <a href="https://github.com/ClickHouse/ClickHouse/issues/54864">past limitations</a>) while showing you why our implementation stands out as the best possible implementation of JSON on top of columnar storage featuring support for:</p>
<ul>
<li>
<p><strong>Dynamically changing data</strong>: allow values with different data types (possibly incompatible and not known beforehand) for the same JSON paths without unification into a least common type, preserving the integrity of mixed-type data.</p>
</li>
<li>
<p><strong>High performance and dense, true column-oriented storage:</strong> store and read any inserted JSON key path as a native, dense subcolumn, allowing high data compression and maintaining query performance seen on classic types.</p>
</li>
<li>
<p><strong>Scalability</strong>: allow limiting the number of subcolumns that are stored separately, to scale JSON storage for high-performance analytics over PB datasets.</p>
</li>
<li>
<p><strong>Tuning</strong>: allow hints for JSON parsing (explicit types for JSON paths, paths that should be skipped during parsing, etc).</p>
</li>
</ul>
<p>The rest of this post will explain how we developed our new JSON type by first building foundational components with broader applications beyond JSON.</p>

<p>The <a href="https://clickhouse.com/docs/en/sql-reference/data-types/variant">Variant data type</a> is the first building block in implementing our new JSON data type. It was designed as a completely separate feature that <a href="https://clickhouse.com/blog/clickhouse-release-24-01#variant-type">can be used outside of JSON</a>, and allows to efficiently store (and read) values with different data types within the same table column. Without any unification into a least common type. This solves our <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#challenge-1-true-column-oriented-storage">first</a> and <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#challenge-2-dynamically-changing-data-without-type-unification">second</a> challenge.</p>

<p>Without the new variant data type, the columns of a ClickHouse table all have a fixed type, and all inserted values must either be in the correct data type of the targeted column or are implicitly coerced into the required type.</p>
<p>As a preparation to better understand how the Variant type works, the following diagram shows how ClickHouse traditionally stores the data of a <a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family">MergeTree</a> family table with fixed data type columns on disk (per <a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree#mergetree-data-storage">data part</a>):</p>
<p><img src="https://clickhouse.com/uploads/JSON_07_c5da8ca64b.png" alt="JSON-07.png" node="[object Object]"></p>
<p>The SQL code to reproduce the example table in the diagram above is <a href="https://gist.github.com/tom-clickhouse/558b82bb6e7dbb00dbbf0f669012b64a">here</a>. Note that we have annotated each column with its data type, e.g. column <code>C1</code> has the type <code>Int64</code>. As ClickHouse is a <a href="https://clickhouse.com/docs/en/faq/general/columnar-database">columnar database</a>, the values from each table column are stored on disk in separate (highly <a href="https://clickhouse.com/docs/en/data-compression/compression-in-clickhouse">compressed</a>) column files. Because column <code>C2</code> is <a href="https://clickhouse.com/docs/en/sql-reference/data-types/nullable">Nullable</a>, ClickHouse <a href="https://clickhouse.com/docs/en/sql-reference/data-types/nullable#storage-features">uses</a> a separate file with NULL masks in addition to the normal column file with values, to differentiate between NULL and empty (default) values. For table column <code>C3</code>, the diagram above shows how ClickHouse natively supports storing <a href="https://clickhouse.com/docs/en/sql-reference/data-types/array">Arrays</a> by using a separate file on disk storing the size of each array from each table row. These size values are used to calculate the corresponding offsets for accessing array elements in the data file.</p>

<p>With the new Variant data type, we can store all values from all columns of the table above inside a single column. The next figure (you can click on it to enlarge it) sketches how such a column works and how it is implemented on top of ClickHouse’s columnar storage on disk (per data part):</p>
<p><a href="https://clickhouse.com/uploads/JSON_08_c04e3510ad.png" target="_blank"><img src="https://clickhouse.com/uploads/JSON_08_c04e3510ad.png" alt="Markdown Image" node="[object Object]"></a></p>
<p><a href="https://gist.github.com/tom-clickhouse/c4f3da235843252b7b5c38472bdeba5d">Here</a> is the SQL code to recreate the example table shown in the diagram above. We annotated the ClickHouse table column <code>C</code> with its Variant type, <a href="https://clickhouse.com/docs/en/sql-reference/data-types/variant">specifying</a> that we want to store a mix of integers, strings, and integer arrays as values for <code>C</code>. For such a column, ClickHouse stores all values with the same concrete data type in separate subcolumns (type variant column data files, which by themselves look almost identical to the column data files in the <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#traditional-data-storage-in-clickhouse">previous</a> example). For example, all integer values are stored in the <code>C.Int64 .bin</code> file, all String values are stored in <code>C.String .bin</code>, and so on.</p>

<p>To know which type is used per row of the ClickHouse table, ClickHouse assigns a discriminator value to each data type and stores a corresponding additional (<code>UInt8</code>) column data file with these discriminators ( <code>C .variant _discr .bin</code> in the figure above).  Each discriminator value represents an index into a list of sorted used type names. Discriminator 255 is reserved for <code>NULL</code> values, which means that by design, a Variant can have a maximum of 255 different concrete types.</p>
<p>Note that we don’t need a separate <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#traditional-data-storage-in-clickhouse">NULL mask file</a> to differentiate between NULL and default values.</p>
<p>Further, note that there is a <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#one-more-detail---compact-discriminator-serialization">special compact form</a> of discriminator serialization (to optimize for typical JSON scenarios).</p>

<p>The separate type variant column data files are dense. We don’t store <code>NULL</code> values in these files. In scenarios with many unique but sparse JSON keys, we don't store default values for rows that don’t have a real value for a specific JSON path, as sketched in <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#challenge-4-dense-storage">this</a> diagram above (as a counter-example). This solves our <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#challenge-4-dense-storage">fourth</a> challenge.</p>
<p>Because of this dense storage of type variants, we also need a mapping from a row in the discriminators column to the row in the corresponding type variant column data file. For this purpose, we use an additional <code>UInt64</code> offsets column (see <code>offsets</code> in the figure above) that only exists in memory but is not stored on disk (the in-memory representation can be created on-the-fly from the discriminators column file).</p>
<p>As an example, to get the value in the ClickHouse table row 6 in the diagram above, ClickHouse inspects row 6 in the discriminators column to identify the type variant column data file that contains the requested value:  <code>C.Int64 .bin</code>. Additionally, ClickHouse knows the concrete offset of the requested value within the  <code>C.Int64 .bin</code> file, by inspecting row 6 of the <code>offsets</code> file: offset 2. Therefore the requested value for ClickHouse table row 6 is 44.</p>

<p>The order of types nested in a Variant column doesn't matter: <code>Variant(T1, T2)</code> = <code>Variant(T2, T1)</code>. Furthermore, the Variant type allows arbitrary nesting, e.g. you can use the Variant type as one of the type variants inside a Variant type. We demonstrate this with another diagram (you can click on it to enlarge it):</p>
<p><a href="https://clickhouse.com/uploads/JSON_09_ceb9570915.png" target="_blank"><img src="https://clickhouse.com/uploads/JSON_09_ceb9570915.png" alt="Markdown Image" node="[object Object]"></a></p>
<p>The SQL code to replicate the example table from the diagram above can be found <a href="https://gist.github.com/tom-clickhouse/56b56271239eb2b7c9a8ca970f62611f">here</a>. This time, we specified that we want to use Variant column <code>C</code> to store integers, strings, and arrays that contain Variant values—a mix of integers and strings. The figure above sketches how ClickHouse uses the Variant storage approach that we explained above, nested within the array column data file, to implement a nested Variant type.</p>

<p>The Variant type <a href="https://clickhouse.com/docs/en/sql-reference/data-types/variant#reading-variant-nested-types-as-subcolumns">supports</a> reading the values of a single nested type from a Variant column using the type name as a subcolumn. For example, you can read all integer values of the <code>Int64</code> <code>C</code>-subcolumn from the table above using the syntax <code>C.Int64</code>:</p>
<pre><code><span>SELECT</span> C.Int64
<span>FROM</span> test;

   ┌─C.Int64─┐
<span>1.</span> │      <span>42</span> │
<span>2.</span> │    ᴺᵁᴸᴸ │
<span>3.</span> │    ᴺᵁᴸᴸ │
<span>4.</span> │      <span>43</span> │
<span>5.</span> │    ᴺᵁᴸᴸ │
<span>6.</span> │    ᴺᵁᴸᴸ │
<span>7.</span> │      <span>44</span> │
<span>8.</span> │    ᴺᵁᴸᴸ │
<span>9.</span> │    ᴺᵁᴸᴸ │
   └─────────┘
</code></pre>

<p>The next step after the Variant type was implementing the <a href="https://clickhouse.com/docs/en/sql-reference/data-types/dynamic">Dynamic</a> type on top of it. Like the Variant type, the Dynamic type is implemented as a standalone feature that <a href="https://clickhouse.com/blog/clickhouse-release-24-05#dynamic-data-type">can be used on its own</a> outside of a JSON context.</p>
<p>The Dynamic type can be seen as an enhancement of the Variant type, introducing two key new features:</p>
<ol>
<li>
<p>Storing values of any data type inside a single table column without knowing and having to specify all the types in advance.</p>
</li>
<li>
<p>Possibility to limit the number of types that are stored as separate column data files. This (<a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#preventing-an-avalanche-of-column-files">partially</a>) solves our <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#challenge-3-prevention-of-avalanche-of-column-data-files-on-disk">third</a> challenge.</p>
</li>
</ol>
<p>We will briefly describe these two new features in the following.</p>

<p>The next diagram (you can click on it to enlarge it) shows a ClickHouse table with a single Dynamic column and its storage on disk (per data part):</p>
<p><a href="https://clickhouse.com/uploads/JSON_10_92f1907815.png" target="_blank"><img src="https://clickhouse.com/uploads/JSON_10_92f1907815.png" alt="Markdown Image" node="[object Object]"></a></p>
<p>You can use <a href="https://gist.github.com/tom-clickhouse/cba68ca35a5926d2145a186bec695d73">this</a> SQL code to recreate the table depicted in the diagram above. We can insert values of any type into the Dynamic column <code>C</code> without specifying the types in advance as we do in the Variant type.</p>
<p>Internally, a Dynamic column stores data on disk in the <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#storage-extension-for-dynamically-changing-data">same</a> way as a Variant column, plus some additional information about the types stored in a particular column. The figure above shows that the storage differs from the Variant column only in that it has one additional file, <code>C.dynamic_structure.bin</code> that contains information about the list of types stored as subcolumns plus statistics of the sizes of the type variant column data files. This metadata is used for subcolumns reading and data part merging.</p>

<p>The Dynamic type also supports limiting the number of types that are stored as separate column data files by specifying the <code>max_types</code> parameter in the type declaration: <code>Dynamic(max_types=N)</code> where 0 &lt;= N &lt; 255. The default value of <code>max_types</code> is 32. When this limit is reached, all remaining types are stored in a single column data file with a special structure. The following diagram shows an example of this (you can click on it to enlarge it):</p>
<p><a href="https://clickhouse.com/uploads/JSON_11_c3698916bb.png" target="_blank"><img src="https://clickhouse.com/uploads/JSON_11_c3698916bb.png" alt="Markdown Image" node="[object Object]"></a></p>
<p><a href="https://gist.github.com/tom-clickhouse/c30b287d0a4a514b1019fcbed1584467">Here</a>’s the SQL script to generate the example table illustrated in the diagram above. This time we use a Dynamic column <code>C</code> with the <code>max_types</code> parameter set to 3.</p>
<p>Therefore only the first three used types are stored in separate column data files (which is efficient for compression and analytical queries). All values from additionally used types (marked with green highlights in the example table above) are stored together in a single column data file (<code>C.SharedVariant.bin</code>) that has type <code>String</code>. Each row in SharedVariant contains a string value that contains the following data: &lt;<a href="https://clickhouse.com/docs/en/sql-reference/data-types/data-types-binary-encoding">binary_encoded_data_type</a>&gt;&lt;binary_value&gt;. Using this structure, we can store (and retrieve) values of different types inside a single column.</p>

<p>Like the Variant type, the Dynamic type <a href="https://clickhouse.com/docs/en/sql-reference/data-types/dynamic#reading-dynamic-nested-types-as-subcolumns">supports</a> reading the values of a single nested type from a Dynamic column using the type name as a subcolumn:</p>
<pre><code><span>SELECT</span> C.Int64
<span>FROM</span> test;

   ┌─C.Int64─┐
<span>1.</span> │      <span>42</span> │
<span>2.</span> │    ᴺᵁᴸᴸ │
<span>3.</span> │    ᴺᵁᴸᴸ │
<span>4.</span> │      <span>43</span> │
<span>5.</span> │    ᴺᵁᴸᴸ │
<span>6.</span> │    ᴺᵁᴸᴸ │
<span>7.</span> │      <span>44</span> │
<span>8.</span> │    ᴺᵁᴸᴸ │
<span>9.</span> │    ᴺᵁᴸᴸ │
   └─────────┘
</code></pre>

<p>After implementing the Variant and the Dynamic type, we had all the required building blocks to implement a new powerful JSON type on top of ClickHouse’s columnar storage, overcoming all of our <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#introduction">challenges</a> with support for:</p>
<ul>
<li>
<p><strong>Dynamically changing data</strong>: allow values with different data types  (possibly incompatible and not known beforehand)  for the same JSON paths without unification into a least common type, preserving the integrity of mixed-type data.</p>
</li>
<li>
<p><strong>High performance and dense, true column-oriented storage:</strong> store and read any inserted JSON key path as a native, dense subcolumn, allowing high data compression and maintaining query performance seen on classic types.</p>
</li>
<li>
<p><strong>Scalability</strong>: allow limiting the number of subcolumns that are stored separately, to scale JSON storage for high-performance analytics over PB datasets.</p>
</li>
<li>
<p><strong>Tuning</strong>: allow hints for JSON parsing (explicit types for JSON paths, paths that should be skipped during parsing, etc).</p>
</li>
</ul>
<p>Our new <a href="https://clickhouse.com/docs/en/sql-reference/data-types/newjson">JSON type</a> allows for the storage of JSON objects with any structure and the reading of every JSON value from it using the JSON path as a subcolumn.</p>

<p>The new type has several optional parameters and hints in its declaration:</p>
<pre><code><span>&lt;</span>column_name<span>&gt;</span> JSON(
  max_dynamic_paths<span>=</span>N, 
  max_dynamic_types<span>=</span>M, 
  some.path TypeName, 
  <span>SKIP</span> path.to.skip, 
  <span>SKIP</span> REGEXP <span>'paths_regexp'</span>)
</code></pre>
<p>Where:</p>
<ul>
<li>
<p><code>max_dynamic_paths</code> (default value <code>1024</code>) specifies how many JSON key paths are stored separately as subcolumns. If this limit is exceeded, all other paths will be stored together in a single subcolumn with a special structure.</p>
</li>
<li>
<p><code>max_dynamic_types</code> (default value <code>32</code>) is between <code>0</code> and <code>254</code> and specifies how many different data types are stored as separate column data files for a single JSON key path column with type <code>Dynamic</code> . If this limit is exceeded, all new types will be stored together in a single column data file with a special structure.</p>
</li>
<li>
<p><code>some.path TypeName</code> is a type hint for a particular JSON path. Such paths are always stored as subcolumns with the specified type, providing performance guarantees.</p>
</li>
<li>
<p><code>SKIP path.to.skip</code> is a hint for particular JSON path that should be skipped during JSON parsing. Such paths will never be stored in the JSON column. If the specified path is a nested JSON object, the whole nested object will be skipped.</p>
</li>
<li>
<p><code>SKIP REGEXP 'path_regexp'</code> is a hint with a regular expression that is used to skip paths during JSON parsing. All paths that match this regular expression will never be stored in the JSON column.</p>
</li>
</ul>

<p>The following diagram (you can click on it to enlarge it) shows a ClickHouse table with a single JSON column and how the JSON data of that column is efficiently implemented on top of ClickHouse’s columnar storage on disk (per data part):</p>
<p><a href="https://clickhouse.com/uploads/JSON_12_f4326293fb.png" target="_blank"><img src="https://clickhouse.com/uploads/JSON_12_f4326293fb.png" alt="Markdown Image" node="[object Object]"></a></p>
<p>Use <a href="https://gist.github.com/tom-clickhouse/c52ab757aca15723427032f305c73656">this</a> SQL code below to recreate the table as illustrated in the diagram above. Column <code>C</code> of our example table is of type <code>JSON</code>, and we provided two type hints specifying the types for JSON paths <code>a.b</code> and <code>a.c</code>.</p>
<p>Our table column contains 6 JSON documents, and the leaf values of each unique JSON key path are stored on disk either as regular column data files (for typed JSON paths–paths with a type hint, see <code>C.a.b</code> and <code>C.a.c</code> in the diagram above) or as a dynamic subcolumn (for dynamic JSON paths - paths with potentially dynamically changing data, see <code>C.a.d</code>, <code>C.a.d.e</code>, and <code>C.a.e</code> in the diagram above). For the latter, ClickHouse uses the <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#building-block-2---dynamic-type">Dynamic data type</a>.</p>
<p>In addition, the JSON type uses a special file (<code>object_structure</code>) containing metadata information about the dynamic paths and statistics of non-null values for each dynamic path (calculated during column serialization). This metadata is used for reading subcolumns and merging data parts.</p>

<p>To prevent an explosion of the number of column files on disk in scenarios with (1) a large number of dynamic types within a single JSON key path, and (2) a huge number of unique dynamic JSON key paths, the JSON type allows to:</p>
<p>(1) restrict how many different data types are stored as separate column data files for a single JSON key path with the <code>max_dynamic_types</code> (default value <code>32</code>) parameter.</p>
<p>(2) restrict how many JSON key paths are stored separately as subcolumns with the <code>max_dynamic_paths</code> (default value <code>1024</code>) parameter.</p>
<p>This solves our <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#challenge-3-prevention-of-avalanche-of-column-data-files-on-disk">third</a> challenge.</p>
<p>We gave an example for (1) further <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#preventing-column-file-avalanche">above</a>. And we demonstrate (2) with another diagram (you can click on it to enlarge it):</p>
<p><a href="https://clickhouse.com/uploads/JSON_13_846ce6ca7c.png" target="_blank"><img src="https://clickhouse.com/uploads/JSON_13_846ce6ca7c.png" alt="Markdown Image" node="[object Object]"></a></p>
<p><a href="https://gist.github.com/tom-clickhouse/c02b49fc5ec275aaa6e9d463311048ba">This</a> is the SQL code to reproduce the table from the above diagram. As in the previous example, Column <code>C</code> of our ClickHouse table is of type <code>JSON,</code> and we provided the same two type hints specifying the types for JSON paths <code>a.b</code> and <code>a.c</code>.</p>
<p>Additionally, we set the <code>max_dynamic_paths</code> parameter to 3. This causes ClickHouse to store only the leaf values of the first three dynamic JSON paths as dynamic subcolumns (using the Dynamic type).</p>
<p>The dynamic JSON paths with their type information and values (marked with green highlights in the example table above) are stored as shared data -  see the files <code>C .object_ shared_ data .size0.bin</code>, <code>C .object_shared_data.paths.bin</code> and <code>C.object_shared_data.values.bin</code> in the figure above. Note that the shared data file (<code>object_shared_data.values</code>) has type <code>String</code>. Each entry is a string value that contains the following data: &lt;<a href="https://clickhouse.com/docs/en/sql-reference/data-types/data-types-binary-encoding">binary_encoded_data_type</a>&gt;&lt;binary_value&gt;.</p>
<p>With shared data, we also store additional statistics (used for reading subcolumns and merging data parts) in the <code>object_structure.bin</code> file. We store statistics for non-null values for (the first 10000) paths stored in the shared data column. Note that we currently store the statistics only for the first 10000 paths stored in the shared data column.</p>

<p>The JSON type <a href="https://clickhouse.com/docs/en/sql-reference/data-types/newjson#reading-json-paths-as-subcolumns">supports</a> reading the leave values of every path using the pathname as a subcolumn. For example, all values for JSON path <code>a.b</code> in our example table above can be read using the syntax <code>C.a.b</code>:</p>
<pre><code><span>SELECT</span> C.a.b
<span>FROM</span> test;

   ┌─C.a.b─┐
<span>1.</span> │    <span>10</span> │
<span>2.</span> │    <span>20</span> │
<span>3.</span> │    <span>30</span> │
<span>4.</span> │    <span>40</span> │
<span>5.</span> │    <span>50</span> │
<span>6.</span> │    <span>60</span> │
   └───────┘
</code></pre>
<p>If the type of the requested path was not specified in the JSON type declaration by a type hint, the path values will always have the type Dynamic:</p>
<pre><code><span>SELECT</span>
    C.a.d,
    toTypeName(C.a.d)
<span>FROM</span> test;

   ┌─C.a.d───┬─toTypeName(C.a.d)─┐
<span>1.</span> │ <span>42</span>      │ <span>Dynamic</span>           │
<span>2.</span> │ <span>43</span>      │ <span>Dynamic</span>           │
<span>3.</span> │ ᴺᵁᴸᴸ    │ <span>Dynamic</span>           │
<span>4.</span> │ foo     │ <span>Dynamic</span>           │
<span>5.</span> │ [<span>23</span>,<span>24</span>] │ <span>Dynamic</span>           │
<span>6.</span> │ ᴺᵁᴸᴸ    │ <span>Dynamic</span>           │
   └─────────┴───────────────────┘
</code></pre>
<p>It is also possible to read subcolumns of a Dynamic type using special JSON syntax <code>JSON_column.some.path.:TypeName</code>:</p>
<pre><code><span>SELECT</span> C.a.d.:Int64
<span>FROM</span> test;


   ┌─C.a.d.:`Int64`─┐
<span>1.</span> │             <span>42</span> │
<span>2.</span> │             <span>43</span> │
<span>3.</span> │           ᴺᵁᴸᴸ │
<span>4.</span> │           ᴺᵁᴸᴸ │
<span>5.</span> │           ᴺᵁᴸᴸ │
<span>6.</span> │           ᴺᵁᴸᴸ │
   └────────────────┘
</code></pre>
<p>Additionally, the JSON type <a href="https://clickhouse.com/docs/en/sql-reference/data-types/newjson#reading-json-sub-objects-as-subcolumns">supports</a> reading nested JSON objects as subcolumns with type JSON using the special syntax <code>JSON_column.^some.path</code>:</p>
<pre><code><span>SELECT</span> C.<span>^</span>a
<span>FROM</span> test;

   ┌─C.<span>^</span>`a`───────────────────────────────────────┐
<span>1.</span> │ {"b":<span>10</span>,"c":"str1","d":"42"}                 │
<span>2.</span> │ {"b":<span>20</span>,"c":"str2","d":"43"}                 │
<span>3.</span> │ {"b":<span>30</span>,"c":"str3","e":"44"}                 │
<span>4.</span> │ {"b":<span>40</span>,"c":"str4","d":"foo","e":"baz"}      │
<span>5.</span> │ {"b":<span>50</span>,"c":"str5","d":["23","24"]}          │
<span>6.</span> │ {"b":<span>60</span>,"c":"str6","d":{"e":"bar"},"e":"45"} │
   └──────────────────────────────────────────────┘
</code></pre>
<pre><code><span>SELECT</span> <span>toTypeName</span>(C.^a)
<span>FROM</span> test
<span>LIMIT</span> <span>1</span>;

   ┌─<span>toTypeName</span>(C.^<span>`a`</span>)───────┐
<span>1.</span> │ <span>JSON</span>(b <span>UInt32</span>, c <span>String</span>) │
   └──────────────────────────┘
</code></pre>
<blockquote>
<p>Currently, the dot syntax doesn’t read nested objects for performance reasons. The data is stored so that reading literal values by paths is very efficient, but reading all subobjects by path requires reading much more data and can sometimes be slower. Therefore, when we want to return an object, we need to use .^ instead. We are <a href="https://github.com/ClickHouse/ClickHouse/issues/68428">planning</a> to unify the two different <code>.</code> syntaxes.</p>
</blockquote>

<p>In many scenarios, dynamic JSON paths will have values of mostly the same type. In this case, the Dynamic type’s <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#discriminator-column-for-switching-between-subtypes">discriminators file</a> will mainly contain the same numbers (type discriminators).</p>
<p>Similarly, when storing a high number of unique but sparse JSON paths, the discriminators file for each path will mainly contain the value 255 (indicating a NULL value).</p>
<p>In both cases the discriminators file will be compressed well but still can be quite redundant when all the rows have the same values.</p>
<p>To optimize this, we implemented a special compact format of the discriminators serialization. Instead of just writing the discriminators as the <a href="https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse#discriminator-column-for-switching-between-subtypes">usual</a> <code>UInt8</code> values, if all discriminators are the same in the <a href="https://clickhouse.com/docs/en/optimize/sparse-primary-indexes#data-is-organized-into-granules-for-parallel-data-processing">target granule</a>, we serialize only 3 values (instead of <a href="https://clickhouse.com/docs/en/operations/settings/merge-tree-settings#index_granularity">8192</a> values):</p>
<ol>
<li>an indicator of compact granule format</li>
<li>an indicator of the amount of values in this granule</li>
<li>the discriminator value</li>
</ol>
<p>This optimization can be controlled by the MergeTree setting <code>use_compact_variant_discriminators_serialization</code> (enabled by default).</p>

<p>In this post, we outlined how we developed our new JSON type from scratch by first creating foundational building blocks that also have broader applications beyond JSON.</p>
<p>This new JSON type was designed to replace the now deprecated <a href="https://clickhouse.com/docs/en/sql-reference/data-types/object-data-type">Object('json')</a> data type, addressing its limitations and improving overall functionality.</p>
<p>The new implementation is currently <a href="https://clickhouse.com/blog/clickhouse-release-24-08#json-data-type">released</a> as experimental for testing purposes, and we’re not finished yet with the feature set. Our <a href="https://github.com/ClickHouse/ClickHouse/issues/68428">JSON roadmap</a> includes some powerful enhancements, such as using JSON key paths inside a table’s primary key or in data-skipping indices.</p>
<p>Last but not least, the building blocks we created to finally implement the new JSON type have paved the way for extending ClickHouse to support additional semi-structured types like XML, YAML, and more.</p>
<p>Stay tuned for upcoming posts, in which we’ll showcase the new JSON type's main query features with real-world data along with benchmarks for data compression and query performance. We’ll also dive deeper into the inner workings of JSON’s implementation to uncover how data is efficiently merged and processed in memory.</p>
<p>If you are using ClickHouse Cloud and want to test our new JSON data type, please <a href="https://clickhouse.com/docs/en/cloud/support">contact our support</a> to get private preview access.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Rust Web Framework (420 pts)]]></title>
            <link>https://github.com/levkk/rwf</link>
            <guid>41914544</guid>
            <pubDate>Tue, 22 Oct 2024 14:15:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/levkk/rwf">https://github.com/levkk/rwf</a>, See on <a href="https://news.ycombinator.com/item?id=41914544">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Rwf ‐ Rust Web Framework</h2><a id="user-content-rwf--rust-web-framework" aria-label="Permalink: Rwf ‐ Rust Web Framework" href="#rwf--rust-web-framework"></a></p>
<p dir="auto">Rwf is a comprehensive framework for building web applications in Rust. Written using the classic MVC  pattern (model-view-controller), Rwf comes standard with everything you need to easily build fast and secure web apps.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">📘 The documentation <strong><a href="https://levkk.github.io/rwf/" rel="nofollow">is available here</a></strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features overview</h2><a id="user-content-features-overview" aria-label="Permalink: Features overview" href="#features-overview"></a></p>
<ul dir="auto">
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/examples/quick-start">HTTP server</a></li>
<li>✔️ User-friendly <a href="https://github.com/levkk/rwf/blob/main/examples/orm">ORM</a> to build PostgreSQL queries easily</li>
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/examples/dynamic-templates">Dynamic templates</a></li>
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/examples/auth">Authentication</a> &amp; built-in user sessions</li>
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/examples/middleware">Middleware</a></li>
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/examples/background-jobs">Background jobs</a> and <a href="https://github.com/levkk/rwf/blob/main/examples/scheduled-jobs">scheduled jobs</a></li>
<li>✔️ Database migrations</li>
<li>✔️ Built-in <a href="https://github.com/levkk/rwf/blob/main/examples/rest">REST framework</a> with JSON serialization</li>
<li>✔️ WebSockets support</li>
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/examples/static-files">Static files</a> hosting</li>
<li>✔️ Tight integration with <a href="https://turbo.hotwired.dev/" rel="nofollow">Hotwired Turbo</a> for building <a href="https://github.com/levkk/rwf/blob/main/examples/turbo">backend-driven SPAs</a></li>
<li>✔️ Environment-specific configuration</li>
<li>✔️ Logging and metrics</li>
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/rwf-cli">CLI</a></li>
<li>✔️ WSGI server for <a href="https://github.com/levkk/rwf/blob/main/examples/django">migrating</a> from Django/Flask apps</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<p dir="auto">To add Rwf to your stack, create a Rust binary application and add <code>rwf</code> and <code>tokio</code> to your dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo add rwf
cargo add tokio@1 --features full"><pre>cargo add rwf
cargo add tokio@1 --features full</pre></div>
<p dir="auto">Building an app is then as simple as:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use rwf::prelude::*;
use rwf::http::Server;

#[derive(Default)]
struct IndexController;

#[async_trait]
impl Controller for IndexController {
    async fn handle(&amp;self, request: &amp;Request) -> Result<Response, Error> {
        Ok(Response::new().html(&quot;<h1>Hey Rwf!</h1>&quot;))
    }
}

#[tokio::main]
async fn main() {
    Server::new(vec![
        route!(&quot;/&quot; => IndexController),
    ])
    .launch(&quot;0.0.0.0:8000&quot;)
    .await
    .unwrap();
}"><pre><span>use</span> rwf<span>::</span>prelude<span>::</span><span>*</span><span>;</span>
<span>use</span> rwf<span>::</span>http<span>::</span><span>Server</span><span>;</span>

<span>#<span>[</span>derive<span>(</span><span>Default</span><span>)</span><span>]</span></span>
<span>struct</span> <span>IndexController</span><span>;</span>

<span>#<span>[</span>async_trait<span>]</span></span>
<span>impl</span> <span>Controller</span> <span>for</span> <span>IndexController</span> <span>{</span>
    <span>async</span> <span>fn</span> <span>handle</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>request</span><span>:</span> <span>&amp;</span><span>Request</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>Response</span><span>,</span> <span>Error</span><span>&gt;</span> <span>{</span>
        <span>Ok</span><span>(</span><span>Response</span><span>::</span><span>new</span><span>(</span><span>)</span><span>.</span><span>html</span><span>(</span><span>"&lt;h1&gt;Hey Rwf!&lt;/h1&gt;"</span><span>)</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>#<span>[</span>tokio<span>::</span>main<span>]</span></span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>Server</span><span>::</span><span>new</span><span>(</span><span>vec</span><span>!</span><span>[</span>
        route!<span>(</span><span>"/"</span> =&gt; <span>IndexController</span><span>)</span>,
    <span>]</span><span>)</span>
    <span>.</span><span>launch</span><span>(</span><span>"0.0.0.0:8000"</span><span>)</span>
    <span>.</span><span>await</span>
    <span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">See <a href="https://github.com/levkk/rwf/blob/main/examples">examples</a> for common use cases.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚧 Status 🚧</h2><a id="user-content-construction-status-construction" aria-label="Permalink: :construction: Status :construction:" href="#construction-status-construction"></a></p>
<p dir="auto">Rwf is in early development and not ready for production. Many features and documentation are incomplete. Contributions are welcome. Please see <a href="https://github.com/levkk/rwf/blob/main/CONTRIBUTING.md">CONTRIBUTING</a> for guidelines, <a href="https://github.com/levkk/rwf/blob/main/ARCHITECTURE.md">ARCHITECTURE</a> for a tour of the code, and <a href="https://github.com/levkk/rwf/blob/main/ROADMAP.md">ROADMAP</a> for a non-exhaustive list of desired features.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rustls Outperforms OpenSSL and BoringSSL (102 pts)]]></title>
            <link>https://www.memorysafety.org/blog/rustls-performance-outperforms/</link>
            <guid>41914064</guid>
            <pubDate>Tue, 22 Oct 2024 13:29:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.memorysafety.org/blog/rustls-performance-outperforms/">https://www.memorysafety.org/blog/rustls-performance-outperforms/</a>, See on <a href="https://news.ycombinator.com/item?id=41914064">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><p>Josh Aas, Joe Birr-Pixton, and Daniel McCarney<br>Oct 22, 2024</p></header><article><p>ISRG has been investing heavily in the <a href="https://github.com/rustls/rustls">Rustls TLS library</a> over the past few years. Our goal is to create a library that is both memory safe and a leader in performance.</p><p>Back in January of this year we published a <a href="https://www.memorysafety.org/blog/rustls-performance/">post</a> about the start of our performance journey. We've come a long way since then and we're excited to share an update on Rustls performance today.</p><h2 id="what-is-rustls">What is Rustls?</h2><p>Rustls is a memory safe TLS implementation with a focus on performance. It is production ready and used in a wide range of applications. You can read more about its history on <a href="https://en.wikipedia.org/wiki/Rustls">Wikipedia</a>.</p><p>It comes with a C API and <a href="https://www.memorysafety.org/blog/rustls-with-aws-crypto-back-end-and-fips/">FIPS support</a> so that we can bring both memory safety and performance to a broad range of existing programs. This is important because OpenSSL and its derivatives, widely used across the Internet, have a long history of memory safety vulnerabilities with more being found this year. It's time for the Internet to move away from C-based TLS.</p><h2 id="handshake-performance">Handshake Performance</h2><p>The first metric we'll look at is the number of handshakes that can be completed per second on the same hardware with the same resource constraints. These tests connect one client to one server over a memory buffer, and then measure the time elapsed in client and server processing — therefore, they give an upper bound on performance given no network latency or system call overhead.</p><p><img src="https://www.memorysafety.org/images/blog/blog-2024-10-22-chart1.png" alt=""></p><p><img src="https://www.memorysafety.org/images/blog/blog-2024-10-22-chart2.png" alt=""></p><p>Rustls leads in every scenario tested.</p><h2 id="throughput-performance">Throughput Performance</h2><p>The next metric we'll look at is throughput on the same hardware with the same resource constraints, in terms of megabytes per second:</p><p><img src="https://www.memorysafety.org/images/blog/blog-2024-10-22-chart3.png" alt=""></p><p>Rustls leads across the board in throughput as well.</p><h2 id="testing-methodology">Testing Methodology</h2><p>Tests were performed using Debian Linux on a bare-metal Intel Xeon E-2386G CPU with hyper-threading disabled, dynamic frequency scaling disabled, and the CPU scaling governor set to performance for all cores. More details are available <a href="https://gist.github.com/ctz/deaab7601f20831d0f9d4bf5f3ac734a">here</a>.</p><h2 id="try-rustls">Try Rustls!</h2><p>Rustls is ready for production use today and we encourage folks to <a href="https://github.com/rustls/rustls">try it out</a>. In addition to memory safety and great performance, it offers:</p><ul><li>C and Rust APIs</li><li>FIPS Support</li><li>Post-quantum key exchange (updated algorithms coming soon)</li><li>Encrypted Client Hello (client side)</li><li>OS trust verifier support</li></ul><h2 id="thank-you">Thank You</h2><p>Rustls uses the <a href="https://github.com/aws/aws-lc-rs">aws-lc-rs</a> cryptographic library by default. We'd like to thank the aws-lc-rs team at AWS for helping us reach our performance goals, and for being generally helpful with our adoption of their library. We couldn't have asked for better partners in this.</p><p>We'd also like to thank Intel for helping with AVX-512 optimizations for aws-lc-rs recently. This was an important part of achieving our performance goals.</p><p>We would not be able to do this work without our funders. Thank you to Sovereign Tech Fund, Alpha-Omega, Google, Fly.io, and Amazon Web Services for their support.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Against /Tmp (220 pts)]]></title>
            <link>https://dotat.at/@/2024-10-22-tmp.html</link>
            <guid>41913610</guid>
            <pubDate>Tue, 22 Oct 2024 12:36:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dotat.at/@/2024-10-22-tmp.html">https://dotat.at/@/2024-10-22-tmp.html</a>, See on <a href="https://news.ycombinator.com/item?id=41913610">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <p>I commented on Lobsters that <a href="https://lobste.rs/s/bzg0kq/how_do_you_deploy_10_seconds#c_ihmo1o"><code>/tmp</code> is usually a bad idea</a>,
which caused some surprise. I suppose <code>/tmp</code> security bugs were common
in the 1990s when I was learning Unix, but they are pretty rare now so
I can see why less grizzled hackers might not be familiar with the
problems.</p>
<p>I guess that’s some kind of success, but sadly the fixes have left
behind a lot of scar tissue because they didn’t address the underlying
problem: <code>/tmp</code> should not exist.</p>
<blockquote>
<p>It’s a bad idea because it’s shared global mutable state that
crosses security boundaries. There’s a ton of complexity at all
levels of unix (filesystems, kernel APIs, libc, shell, admin
scripts) that only exists as a workaround for the dangers caused by
making <code>/tmp</code> shared.</p>
</blockquote>
<h2><a name="sticky-bit" href="#sticky-bit">sticky bit</a></h2>
<p>I think the earliest and lowest-level workaround is the sticky bit.</p>
<p>The sticky bit is mode bit 01000 in <a href="https://pubs.opengroup.org/onlinepubs/9799919799/functions/chmod.html">unix file permissions</a>. It
is printed as the <code>t</code> instead of <code>x</code> in <code>rwt</code>.</p>
<pre><code>    drwxrwxrwt  5 root  wheel  160 Oct 22 10:22 /tmp/
</code></pre>
<p>Originally in the 1970s the sticky bit was invented to speed up
frequently-used programs such as the shell: the sticky bit indicated
to the kernel that the executable file should stick in core. This
functionality was made obsolete in the 1980s by filesystem page
caches.</p>
<p>The sticky bit was re-used to mean something else on directories, to
fix a security problem with <code>/tmp</code>.</p>
<p>On unix, permission to delete a file depends on write access to the
directory containing the file, independent of the file’s own
permissions.</p>
<p>So if a directory (such as <code>/tmp</code>) is world-writable, anyone can delete
any file in it.</p>
<p>This means <code>/tmp</code> was vulnerable to all sorts of accidental or malicious
trouble caused by users deleting each others’ files.</p>
<p>To fix this, <a href="https://man.freebsd.org/cgi/man.cgi?query=sticky">a file in a sticky directory may only be removed or
renamed by a user if the user has write permission for the directory
and the user is the owner of the file, the owner of the directory, or
the super-user</a>.</p>
<h2><a name="tmp-stupidity-in-c" href="#tmp-stupidity-in-c">tmp stupidity in C</a></h2>
<p>POSIX used to provide 5 (five!) ways to create a temporary file, three
of which (most of them!) you must never use and which have
subsequently been deprecated – except the do-not-use footgun <code>tmpnam</code>
which is still part of the C standard. Two more safe functions have
subsequently been added to support more complicated situations.</p>
<ul>
<li>bad: <a href="https://pubs.opengroup.org/onlinepubs/007908799/xsh/mktemp.html">mktemp</a>, <a href="https://pubs.opengroup.org/onlinepubs/007908799/xsh/tempnam.html">tempnam</a>, <a href="https://pubs.opengroup.org/onlinepubs/007908799/xsh/tmpnam.html">tmpnam</a></li>
<li>ok: <a href="https://pubs.opengroup.org/onlinepubs/007908799/xsh/mkstemp.html">mkstemp</a>, <a href="https://pubs.opengroup.org/onlinepubs/007908799/xsh/tmpfile.html">tmpfile</a></li>
<li>new: <a href="https://pubs.opengroup.org/onlinepubs/9799919799/functions/mkstemp.html">mkdtemp, mkostemp, mkstemp</a></li>
</ul>
<p>There are so many dangerous API design problems in these functions!
I’m not going to waste time roasting them in detail because I can
cover the important points by discussing …</p>
<h2><a name="mkstemp-and-mkdtemp" href="#mkstemp-and-mkdtemp">mkstemp and mkdtemp</a></h2>
<p>The purpose of all these functions is to create a temporary file (or
directory) that doesn’t collide with other concurrent activity.</p>
<p>When you are creating a file in <code>/tmp</code> the risk is that another
malicious user can make you open a file under their control. To avoid
this vulnerability, you (or rather <code>mkstemp</code>) must:</p>
<ul>
<li>
<p>Generate an <em>unpredictable</em> filename.</p>
<p>It isn’t enough for the filename to be unique, it must contain
sufficient <em>secure</em> randomness that an adversary can’t win a race
and interfere.</p>
</li>
<li>
<p>Open the file with <code>O_CREAT</code> | <code>O_EXCL</code>.</p>
<p>This ensures the file has not already been created by another
friendly or malicious process, without any time-of-check /
time-of-use vulnerabilities. And it ensures that the file isn’t a
symlink pointing somewhere an attacker wants you to accidentally
corrupt.</p>
</li>
<li>
<p>Retry if open fails with <code>EEXIST</code>.</p>
<p>Together with the randomness in the name, this avoids
denial-of-service vulnerabilites.</p>
</li>
</ul>
<p>This is intricate and not entirely obvious, as you can tell from all
the previous failed attempts at functions that didn’t create temporary
files safely.</p>
<h2><a name="mktemp-in-shell" href="#mktemp-in-shell">mktemp in shell</a></h2>
<p>The <a href="https://man.freebsd.org/cgi/man.cgi?query=mktemp">mktemp(1)</a> command is a wrapper around mkstemp(3). (Slightly
confusingly it isn’t a wrapper around mktemp(3) because that would be
unsafe.) It was introduced by OpenBSD and it’s widely supported though
it isn’t yet in POSIX. Its manual page includes a nice rationale:</p>
<blockquote>
<p>The <code>mktemp</code> utility is provided to allow shell scripts to safely use
temporary files. Traditionally, many shell scripts take the name of
the program with the pid as a suffix and use that as a temporary
file name. This kind of naming scheme is predictable and the race
condition it creates is easy for an attacker to win. A safer, though
still inferior, approach is to make a temporary directory using
the same naming scheme. While this does allow one to guarantee that
a temporary file will not be subverted, it still allows a simple
denial of service attack. For these reasons it is suggested that
<code>mktemp</code> be used instead.</p>
</blockquote>
<p>Although shell scripts can’t avoid using the temporary file by name,
mktemp(1) is safe because it creates the file securely and an attacker
can’t interfere with it after that point. It’s OK to re-open a file
that you know is yours.</p>
<h2><a name="tmp-cleanup" href="#tmp-cleanup">tmp cleanup</a></h2>
<p>The last item on my list of regrets is “admin scripts”, an oblique
reference to <code>/tmp</code> cleanup jobs.</p>
<p>By its nature <code>/tmp</code> tends to accumulate junk, so it was common to have
cron jobs that would delete old files. (Less common now that computers
are much bigger.)</p>
<p>These scripts tended to have problems with time-of-check / time-of-use
vulnerabilities, careless handling of symlinks, and pulling the rug
out from under long-running programs that foolishly used <code>/tmp</code>. (Lots
more reasons these scripts are now less common!)</p>
<h2><a name="tmp-remedy" href="#tmp-remedy">tmp remedy</a></h2>
<p>So I’ve spent dozens of paragraphs outlining bugs and complications
related to <code>/tmp</code>. All of them could have been avoided if <code>/tmp</code> did not
exist, and everything would have been simpler as a result.</p>
<p>So where should temporary files go, if not in <code>/tmp</code>?</p>
<p>There should be per-user temporary directories. In fact, on modern
systems there <em>are</em> per-user temporary directories! But this solution
came several decades too late.</p>
<p>If you have per-user <code>$TMPDIR</code> then temporary filenames can safely be
created using the simple mechanisms described in the mktemp(1)
rationale or used by the old deprecated C functions. There’s no need
to defend against an attacker who doesn’t have sufficient access to
mount an attack! There’s no need for sticky directories because there
aren’t any world-writable directories.</p>
<p>There’s a minor wrinkle that setuid programs would have to be more
careful about how they create temporary files, but setuid programs
have to be more careful about everything.</p>
<h2><a name="tmp-rationale" href="#tmp-rationale">tmp rationale</a></h2>
<p>So why wasn’t per-user <code>$TMPDIR</code> a thing back in the day?</p>
<p>Probably the main reason was path-dependence: <code>/tmp</code> was created and in
wide use before its problems became apparent, at which point it was
difficult to deprecate.</p>
<p>There are reasons 1990-ish-you didn’t want <code>$TMPDIR</code> to be in your
home directory:</p>
<ul>
<li>
<p><code>$HOME</code> might be on NFS so a local <code>$TMPDIR</code> might be faster</p>
</li>
<li>
<p><code>$TMPDIR</code> can be a way to get workspace beyond your disk quota</p>
</li>
<li>
<p><code>$HOME</code> might be on a filesystem that doesn’t support named pipes,
so your X11 and SSH agent sockets live in <code>$TMPDIR</code> instead</p>
</li>
</ul>
<p>The fix, way back when, should have been for login(8) to create a
per-user temporary directory in a sensible place before it drops
privilege, and set <code>$TMPDIR</code> so the user’s shell and child processes
can find it.</p>
<p>Oh well, think of all the excitement we would have missed if insecure
temporary file vulnerabilites had not been a thing!</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tog's Paradox (231 pts)]]></title>
            <link>https://www.votito.com/methods/togs-paradox/</link>
            <guid>41913437</guid>
            <pubDate>Tue, 22 Oct 2024 12:05:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.votito.com/methods/togs-paradox/">https://www.votito.com/methods/togs-paradox/</a>, See on <a href="https://news.ycombinator.com/item?id=41913437">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto">
	
	<p><em>Tog’s Paradox</em> (also known as <em>The Complexity Paradox</em> or <em>Tog’s Complexity
Paradox</em>) is an observation that products aiming to simplify a task for users tend to
inspire new, more complex tasks. It’s one of the key reasons for the
symptom of requirements changing after delivery in enterprise software
products, and for feature creep in consumer products. Tog’s Paradox also
explains why it’s futile to try to completely nail down requirements for a
software product, as the product itself will have an impact on the users,
causing them to demand new functions.</p>

<blockquote>
  <p>[when] we reduce the complexity people experience in a given task, people will take on a more challenging task.</p>

  <p>– Bruce Tognazzini, <a rel="nofollow noopener noreferrer" href="https://www.asktog.com/columns/011complexity.html">The Complexity Paradox</a></p>
</blockquote>

<ul id="markdown-toc">
  <li><a href="#loophole-in-teslers-law" id="markdown-toc-loophole-in-teslers-law">Loophole in Tesler’s law</a></li>
  <li><a href="#examples-of-togs-paradox" id="markdown-toc-examples-of-togs-paradox">Examples of Tog’s Paradox</a></li>
  <li><a href="#togs-paradox-explains-why-requirements-always-change" id="markdown-toc-togs-paradox-explains-why-requirements-always-change">Tog’s paradox explains why requirements always change</a></li>
</ul>

<h2 id="loophole-in-teslers-law">Loophole in Tesler’s law</h2>

<p>Bruce Tognazzini formulated the paradox as a loophole in Tesler’s Law, which
states that inherent complexity in a user task does not change (but it can be
shifted from a user to an application or the other way around). Tognazzini
suggested instead that the task complexity does not stay the same, but increases.</p>

<figure>
<picture><source type="image/webp" srcset="https://www.votito.com/assets/20241005/togs-paradox.webp">
<img loading="lazy" src="https://www.votito.com/assets/20241005/togs-paradox.png" width="1340" height="1500" alt="Tog's Paradox" title="Tog's Paradox"></picture>
<figcaption>A product trying to simplify user tasks invariantly makes users want to perform more complex tasks</figcaption></figure>

<p>The argument follows Tognazzini’s previous observation, called <em>Tog’s Law of
Commuting</em> (published in the 1995 book <a rel="nofollow noopener noreferrer" href="https://amzn.to/3YzNnjT">Tog on Software Design</a>),
which suggests that users have a specific amount of time to complete a task,
and if they can finish the work sooner, they will take on more work to fill the
available time.</p>

<blockquote>
  <p>People will strive to experience an equal or increasing level of complexity in their lives no matter what is done to reduce it.</p>

  <p>– Bruce Tognazzini, <a rel="nofollow noopener noreferrer" href="https://www.asktog.com/columns/011complexity.html">The Complexity Paradox</a></p>
</blockquote>

<p>In spirit, Tog’s Paradox is similar to Jevon’s Paradox (which loosely states
that technological advances which improve the efficiency of using a resource
tend to lead to increased demand) and Parkinson’s law (which states that work
expands so to fill the time available for its completion).</p>

<h2 id="examples-of-togs-paradox">Examples of Tog’s Paradox</h2>

<p>Tog’s Paradox is highly visible in enterprise software development, where
attempts to streamline workflows often lead to more complex requirements over
time. For instance, a CRM system designed to automate customer interactions
might initially focus on basic functions like managing contacts and tracking
communication history. Once users experience the efficiency gains from these
core features, they begin requesting more sophisticated tools—such as
integrations with other software, advanced reporting, or analytics to
further optimize their work. Each new feature brings added complexity to the
system, requiring not only more robust infrastructure but also additional
training and support. This mirrors Tognazzini’s idea that making tasks more
efficient encourages the demand for additional use cases, driving the
complexity of the software.</p>

<p>Tog’s observation is also evident when software increases user productivity but
leads to expanded scope in responsibilities. Consider an HR platform that
automates payroll and performance management, freeing up HR staff from routine
tasks. HR teams will need to justify what they do the rest of the time, and may
use this newfound capacity to take on more strategic roles—such as employee
engagement or talent development initiatives—which eventually demands
additional software functionalities. The software that initially saved time
ends up accommodating these new, more complex tasks, reinforcing the idea that
saved time often gets filled with more work, creating an ongoing cycle of
increasing complexity. This phenomenon is common in enterprise software, where
solving one problem often leads to the creation of new, more intricate
challenges.</p>

<p>The law also plays out in consumer applications. Social media platforms like
Instagram or Twitter/X are another clear example. Initially designed to provide
simple ways to share photos or short messages, these platforms quickly expanded
as users sought additional capabilities, such as live streaming, integrated
shopping, or augmented reality filters. Each of these features added new layers
of complexity to the app, requiring more sophisticated algorithms, larger
databases, and increased development efforts. What began as a relatively
straightforward tool for sharing personal content has transformed into a
multi-faceted platform requiring constant updates to handle new features and
growing user expectations. Tog’s Paradox is evident here, as simplifying one
aspect of the communication and sharing often drives demand for other, more
complex ways of sharing content.</p>

<h2 id="togs-paradox-explains-why-requirements-always-change">Tog’s paradox explains why requirements always change</h2>

<p>Tog’s Paradox reveals why attempts to finalize design requirements are often doomed to fail. The moment a product begins to
solve its users’ core problems efficiently, it sparks a natural progression of
second-order effects. As users save time and effort, they inevitably find new,
more complex tasks to address, leading to feature requests that expand the
scope far beyond what was initially anticipated. This cycle shows that the
product itself actively influences users’ expectations and demands, making it
nearly impossible to fully define design requirements upfront.</p>

<p>This evolving complexity highlights the futility of attempting to lock down
requirements before the product is deployed. No matter how thorough the initial
planning, the reality is that users’ experiences with the product will inspire
new use cases and deeper needs. The product’s initial efficiency gains create a
feedback loop, where users seek more capabilities and push for new features and
workflows. These second-order effects suggest that software development must
embrace flexibility and continuous iteration.</p>

<p>Ultimately, Tog’s Paradox underscores that requirements gathering and user
research are not one-time events, but part of an ongoing process. Attempting to
fully specify requirements beforehand fails to account for how the product
itself will influence user behavior and expectations. To truly meet users’
needs, products must be built with the understanding that the needs will change
due to user interactions with those same products, the complexity will grow, and user
demands will evolve, often in unpredictable ways. Recognizing this dynamic is
key to building successful products that remain responsive and relevant over
time.</p>

<p>Tog’s Paradox has significant implications for user experience design,
particularly in how complexity impacts usability and user satisfaction over
time. As products evolve and add features in response to user demands, the
original simplicity and ease of use can degrade, leading to a more complex and
sometimes overwhelming interface. This presents a core UX challenge: balancing
the need for added functionality with maintaining an intuitive and accessible
user experience.</p>

<p><a rel="nofollow noopener noreferrer" href="https://news.ycombinator.com/item?id=41913437">Comments/Discussion on HN</a></p>
 



<h2>Learn more about the Tog's paradox</h2>
<ul>

	<li>
	
	<a rel="nofollow noopener noreferrer" href="https://www.asktog.com/columns/011complexity.html">The Complexity Paradox</a> by Bruce Tognazzini (1998)
	
	</li>

	<li>
	
	<a rel="nofollow noopener noreferrer" href="https://amzn.to/3YzNnjT">Tog on Software Design</a>, ISBN 978-0201489170 (1995)
	
	</li>

</ul>







<hr>

<nav>
	<span><b>Next article</b>: <a href="https://www.votito.com/methods/umux/">Usability Metric for User Experience</a></span>
	<span><a href="https://www.votito.com/methods/" rel="nofollow">All articles</a></span>
</nav>



	
	
</div></div>]]></description>
        </item>
    </channel>
</rss>