<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 23 Dec 2024 20:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Fogus: Things and Stuff of 2024 (148 pts)]]></title>
            <link>https://blog.fogus.me/2024/12/23/the-best-things-and-stuff-of-2024/</link>
            <guid>42495077</guid>
            <pubDate>Mon, 23 Dec 2024 15:30:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.fogus.me/2024/12/23/the-best-things-and-stuff-of-2024/">https://blog.fogus.me/2024/12/23/the-best-things-and-stuff-of-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=42495077">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
                 <h3><a href="https://blog.fogus.me/2024/12/23/the-best-things-and-stuff-of-2024/" title="The best things and stuff of 2024">The best things and stuff of 2024</a></h3>
                 <p><span>Dec 23, 2024 </span></p><p>Great things and people that I discovered, learned, read, met, etc. in 2024.  No particular ordering is implied.  Not everything is new.</p>

<p><em>also: see the lists from <a href="https://blog.fogus.me/2023/12/18/the-best-things-and-stuff-of-2023/">2023</a>, <a href="http://blog.fogus.me/2022/12/13/the-best-things-and-stuff-of-2022/">2022</a>, <a href="https://blog.fogus.me/2021/12/27/the-best-things-and-stuff-of-2021/">2021</a>, <a href="http://blog.fogus.me/2020/12/31/the-best-things-and-stuff-of-2020/">2020</a>, <a href="http://blog.fogus.me/2019/12/30/the-best-things-and-stuff-of-2019/">2019</a>, <a href="http://blog.fogus.me/2019/01/02/the-best-things-and-stuff-of-2018/">2018</a>, <a href="http://blog.fogus.me/2018/01/02/the-best-things-and-stuff-of-2017/">2017</a>, <a href="http://blog.fogus.me/2016/12/29/the-best-things-and-stuff-of-2016/">2016</a>, <a href="http://blog.fogus.me/2015/12/29/the-best-things-and-stuff-of-2015/">2015</a>, <a href="http://blog.fogus.me/2014/12/29/the-best-things-and-stuff-of-2014/">2014</a>, <a href="http://blog.fogus.me/2013/12/27/the-best-things-and-stuff-of-2013/">2013</a>, <a href="http://blog.fogus.me/2012/12/26/the-best-things-and-stuff-of-2012/">2012</a>, <a href="http://blog.fogus.me/2011/12/31/the-best-things-and-stuff-of-2011/">2011</a> and <a href="http://blog.fogus.me/2010/12/30/the-best-things-in-2010/">2010</a></em></p>

<h2>Great posts | articles | talks read/watched</h2>

<ul>
<li><em><a href="https://www.youtube.com/watch?v=lC4YLMLar5I">ELITE: The game that couldn’t be written</a></em> from Alexander the ok – <em>Elite was one of my favorite games on my Commodore 64 1,000,000 years ago and so I’m a sucker for articles on this gem. If you’re interested, also check out <a href="https://elite.bbcelite.com/c64/">the annotated C64 source code</a>.</em> <sup id="fnref:elite"><a href="#fn:elite" rel="footnote">1</a></sup></li>
<li><em><a href="https://thereader.mitpress.mit.edu/the-rich-history-of-ham-radio-culture/">The Rich History of Ham Radio Culture</a></em> by Kristen Haring – <em>I missed out on the Ham radio craze and only recently learned about its rich history. This article is a good overview and starting point if you’re interested in learning too.</em></li>
<li><em><a href="https://www.atlasobscura.com/articles/japans-bathroom-ghosts">Get to Know Your Japanese Bathroom Ghosts</a></em> by Eric Grundhauser – <em>Describes the interesting Japanese cultural folklore around bathroom ghosts.</em></li>
<li><em><a href="https://www.abortretry.fail/p/arrogant-difficult-powerful">The History of WordStar</a></em> by Abort Retry Fail LLC – <em>A great historical article about one of the most influential software suites ever created. Additionally, the comments are a goldmine of additional information and corrections and should not be skipped.</em></li>
<li><em><a href="https://blog.zdsmith.com/series/combinatory-programming.html">Combinatory Programming</a></em> by zdsmith – <em>Describes combinatorial programming using motivated examples — a technique that’s surprisingly scarce in articles about the topic.</em></li>
<li><em><a href="https://www.openculture.com/2014/05/philip-k-dicks-favorite-classical-music.html">Philip K. Dick’s Favorite Classical Music</a></em> by Open Culture – <em>Discusses PKD’s love for classical music and the references to composers and their works in his fiction. The post also, includes an <a href="https://open.spotify.com/playlist/1RsnkX0bQWd2CVWW8jcxBR">11-hour classical music playlist</a> for your listening pleasure.</em></li>
<li><em><a href="https://new.wunderland.com/2024/11/20/goodbye-kory/">Goodbye, Kory</a></em> by Andy Looney – <em>The world lost Kory Heath, a game designer whom I admire immensely. I’ve talked about his magnum opus <a href="https://blog.fogus.me/2014/10/23/games-of-interest-zendo/">Zendo</a> on this blog before and have run numerous play sessions over the years. He was single-handedly responsible for hundreds of hours of enjoyment around my home and within my group of friends. The world is much the poorer without him in it. RIP.</em> <sup id="fnref:chalker"><a href="#fn:chalker" rel="footnote">2</a></sup></li>
</ul>

<h2>Most viewed blog posts by me</h2>

<ul>
<li><em><a href="https://blog.fogus.me/2024/08/19/on-method-values-part-1/">On method values, part 1</a></em> – <em>We released Clojure 1.12.0 this year and so I wanted to write about one of the features that I worked on. Method values are symbolic references to Java methods that can be used in value contexts and the design and implementation of this feature was interesting enough to talk about. The feature has been generally well received by the Clojure community.</em></li>
</ul>

<h2>Favorite technical (and technical-adjacent) books discovered (and read)</h2>

<ul>
<li><em><a href="https://books.google.com/books/about/And_So_FORTH.html?id=iqUZAQAAIAAJ">And so FORTH</a></em> by Timothy Huang – <em>I found this long out of print Forth tome via inter library loan and enjoyed it immensely. It’s a nice blend of the ideas in Brodie’s <a href="https://thinking-forth.sourceforge.net/">Thinking Forth</a> and something like Geere’s <a href="https://archive.org/details/forth-the-next-step-ron-geere">Forth: The Next Step</a>. It was a sad day when I had to return this beauty back to the library because I could have used another read or two at least.</em></li>
<li><em><a href="https://www.amazon.com/BASIC-FORTH-Parallel-S-J-Wainwright/dp/0859341135?tag=fogus-20">BASIC and FORTH in Parallel</a></em> by S.J. Wainwright – <em>This style of book is exactly the kind of book that I would one day like to write. While the specifics of any such book would be different, the central conceit is perfect. That is, this book uses BASIC to create a simple stack machine and Forth interpreter and then presents simple Forth programs exercising them.</em></li>
</ul>

<h2>Favorite non-technical books read</h2>

<ul>
<li><em><a href="https://www.amazon.com/Butchers-Crossing-Review-Books-Classics/dp/1590171985/?tag=fogus-20">Butcher’s Crossing</a></em> by John Williams – <em>Follows Harvard drop-out Will Andrews as he escapes to the American frontier with a wad of cash to find adventure and “an original relation to nature”. Andrews eventually finds Miller who is more than happy to help the young man part with his money in an attempt to find a hidden Colorado valley filled with buffalo that may or may not still (if it ever did) exist. The book follows Miller and Andrews’ (plus a skinner Schneider and driver Hoge) trek throw the frontier and describes in harrowing detail their tribulations. I could not stop reading and finished the book in a weekend. This one demands multiple reads to really absorb the nuance.</em></li>
<li><em><a href="https://www.amazon.com/Spectral-Link-Thomas-Ligotti-ebook/dp/B00LE52256/?tag=fogus-20">The Spectral Link</a></em> by Thomas Ligotti – <em>Contains two stories by Ligotti: “Metaphysica Morum” and “The Small People”. The first is quite different than most of Ligotti’s work that I’ve read so far. It follows a self-described “metaphysical mutant” and blends overtly dark humor with an underlying pessimistic philosophy centered on a theme of euthanasia. “The Small People” is a dream-like exploration of paranoia and isolation. Both stories are a good introduction to the range in Ligotti’s work if you’re interested in checking him out.</em></li>
<li><em><a href="https://www.amazon.com/Corvo-Cult-History-Obsession-2014-10-09/dp/B01FIY47AQ/?tag=fogus-20">The Corvo Cult</a></em> by Robert Scoble – <em>Frederick Rolfe (aka Baron Corvo) was an little-known Edwardian author who is often remembered more for his bombastic personality than his fictional works. This book talks about the rise and growth of the still active “Corvo Cult” — an obscure literary fandom. In many cases, Rolfe’s fervid devotees matched the controversial author in eccentricity, but the true fascination lies in the broad range of people drawn to his eclectic works.</em></li>
</ul>

<h2>Number of books written or published</h2>

<p>0</p>

<h2>Number of programming languages designed</h2>

<p>0.5</p>

<h2>Favorite music discovered</h2>

<ul>
<li><em><a href="https://www.youtube.com/watch?v=o6TI2FfqGJ8&amp;pp=ygUOInRoZSBwYXJhZ29ucyI%3D">The Paragons</a></em> – <em>At some point I became interested in the roots of ska and The Paragons were the best group that I discovered during my explorations.</em></li>
<li><em><a href="https://en.wikipedia.org/wiki/That%27s_All!">That’s All!</a> by Sammy Davis Jr. – *A fantastic performance from a master of the vocal form. The songs are brilliant but the banter between songs will keep me listening into the distant future.</em></li>
</ul>

<h2>Favorite films discovered</h2>

<ul>
<li><em><a href="https://en.wikipedia.org/wiki/Withnail_and_I">Withnail &amp; I</a></em> – <em><a href="http://sam.aaron.name/">Sam Aaron</a> recommended this film to me years ago but I only managed to watch it in 2024. It’s a great example of a dry comedy following a couple of screw-ups and their misadventures.</em></li>
<li><em><a href="https://en.wikipedia.org/wiki/Jodorowsky%27s_Dune">Jodorowsky’s Dune</a></em> – <em>A documentary about the most influential film that never was.</em></li>
<li><em><a href="https://en.wikipedia.org/wiki/Requiem_for_a_Dream">Requiem for a Dream</a></em> – <em>I’m probably the last person in the world to watch this relentless survey of despair. Not for the faint of heart.</em></li>
</ul>

<h2>Favorite podcasts</h2>

<ul>
<li><em><a href="https://www.youtube.com/@WilliamEByrd">Will Radio</a></em> – <em>Will Byrd started the year promising a KiloTube of videos (i.e. 1024 videos) in 2024 and it’s been a blast following along! There’s no one quite like Will and so any chance that I can get to experience more of him I will jump on.</em></li>
<li><em><a href="https://cinepunx.com/podcast-episodes/eros-massacre/">Eros + Massacre</a></em> – <em>Another podcast triumph by Samm Deighan surveying the weird world of psychotronic cinema.</em></li>
</ul>

<h2>Favorite programming languages (or related) I hacked on/with on my own time</h2>

<ul>
<li><em><a href="https://hypercubed.github.io/joy/joy.html">Joy</a></em> – <em>Joy is a mindfrak of a programming language in the concatenative functional language family. The core of Joy is beautiful and among the foundational programming languages in my opinion.</em></li>
<li><em><a href="https://www.forth.com/forth/">Forth</a></em> – <em>Sticking with the concatenative family in 2024, I continued to explore Forth. Interestingly the language is incredibly rich in history and conducive to a wide range of techniques and paradigms. I’m unsure if I’ll ever find the opportunity to use Forth in anger, but I will say that I should come out of my explorations a stronger programmer and program designer.</em></li>
</ul>

<h2>Programming languages used for work-related projects</h2>

<ul>
<li><a href="https://mail.openjdk.org/pipermail/amber-spec-experts/2023-December/003959.html">Java</a> – <em>Working deep in the Clojure compiler means that much of my work in 2024 was in Java.</em></li>
<li><a href="http://www.clojure.org/">Clojure</a> – <em>2024 marks the 15th year<sup id="fnref:15th"><a href="#fn:15th" rel="footnote">3</a></sup> as a full-time Clojure programmer and the 1st year as a full-time Clojure core developer.</em></li>
<li><a href="http://www.clojurescript.org/">ClojureScript</a> – <em>Less-so now than when I was consulting full-time but I occasionally dig into explore the implications of changes to Clojure on CLJS.</em></li>
<li><a href="http://www.datomic.com/">Datalog</a> – <em>The <a href="https://www.datomic.com/">Datomic</a> flavor of Datalog is the flavor of choice for database access, be it in-process or in the cloud. Again, my day-to-day usage is limited, but I have my share of personal databases hosted on Datomic.</em></li>
</ul>

<h2>Programming languages (and related) that I hope to explore more deeply</h2>

<ul>
<li><em><a href="https://hypercubed.github.io/joy/joy.html">Joy</a></em> – <em>There’s a mountain of deep information on Joy that I would like to devour in 2025.<sup id="fnref:joy"><a href="#fn:joy" rel="footnote">4</a></sup></em></li>
<li><em><a href="https://en.wikipedia.org/wiki/Mouse_(programming_language)">Mouse</a></em> – <em>Yet another concatenative language to explore that’s long-dead but still has some lessons to teach one such as myself.</em></li>
<li><em><a href="https://poplogarchive.getpoplog.org/poplog.info.html">POP-11</a></em> – <em>Another dead language that was designed to support AI applications in the 70s and 80s. I love the idea of exploring the language and the suite of applications that built up around it.</em></li>
</ul>

<h2>Favorite papers discovered (and read)</h2>

<ul>
<li><em><a href="https://hypercubed.github.io/joy/html/j05cmp.html">Recursion Theory and Joy</a></em> by Manfred von Thun – <em>Joy’s underlying reliance on combanatory programming manifests deep in the language even to the degree that recursion in the language is implemented in userspace via recursive combinators. This paper describes the “Joy Way” and its relationship to recursion.</em></li>
<li><em><a href="https://www.cs.tufts.edu/~nr/cs257/archive/dominique-clement/applicative.pdf">A Simple Applicative Language: Mini-ML</a> (PDF) by D. Clement and J. Despeyroux and T. Despeyroux and G. Kahn</em> – <em>Presents a beautiful definition of ML language and its compilation to an abstract machine.</em></li>
</ul>

<h2>Still haven’t read…</h2>

<p>I Ching, A Fire upon the Deep, Don Quixote, and <strong><a href="http://blog.fogus.me/2012/09/21/the-amazing-colossal-science-fiction-ketchup/">a boat-load of sci-fi</a></strong></p>

<h2>Favorite technical conference attended</h2>

<ul>
<li><em><a href="https://2024.clojure-conj.org/">Clojure/conj 2024</a></em> – <em>This was the first Clojure conference that I played a somewhat active part in organizing. Let me be clear, my part in the matter was minimal at best, but it did provide me a window into the complexities of organizing a conference. The conference itself was a blast and it was great to meet old and new Clojure friends as well as <a href="https://www.nubank.com/">colleagues</a>!</em></li>
</ul>

<h2>Favorite code read</h2>

<ul>
<li><em><a href="https://buttondown.com/tensegritics-curiosities/archive/restrained-datalog-in-39loc/">Restrained Datalog in 39loc</a></em> by Christophe Grande – <em>I’ve learned over the years that if Christophe writes a technical article then it behooves me to study it deeply. The highlight of the year from Christophe was his simple, yet rich, Datalog implementation in 39 lines of Clojure code. It’s clear that 39 lines of Clojure goes a long way and especially so when a master of the language plays in it.</em></li>
<li><em><a href="https://zserge.com/posts/post-apocalyptic-programming/">Post-Apocalyptic Programming</a></em> by Serge Zaitsev – <em>I love the central conceit of the post, summarized as “what technology could/should we create in the absence of modern computing niceties?” The post starts with a CPU emulator, builds a language for it, and motives its decisions along the way. There’s a brilliant hard science fiction story in here somewhere, I can feel it.</em></li>
<li><em><a href="https://github.com/monsonite/MINT">MINT</a></em> – <em>MINT is highly inspirational to me as a lesson in minimal programming language design. Based on Forth, MINT makes various design decisions and trade-offs to remain small and fast.</em></li>
</ul>

<h2>Life-changing technology “discovered”</h2>

<p>Nothing this year.</p>

<h2>State of plans from 2023</h2>

<ul>
<li><em>Clojure 1.12</em> – Released in <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0">early September</a> and one of the biggest releases in years as far as feature additions go.</li>
<li><em>Go much deeper down the concatenative rabbit-hole</em> – An unmitigated success!</li>
<li><em>Publish even more non-technical writing</em> – My research into the Corvo-related archives stored at Georgetown University was a success. However, my efforts in writing up my findings has stalled.</li>
</ul>

<h2>Plans for 2025</h2>

<ul>
<li><em><a href="https://www.clojure.org/">Clojure 1.13</a></em> – <em>Thinking around the 1.13 release is ongoing and we’d like to get it out sooner rather than later. Stay tuned.</em></li>
<li><em><a href="https://github.com/clojure/core.async">clojure.core.async next</a></em> – <em>We’ve laid the groundwork for a new version of core.async and released it as version 1.7.701. We’d love to leverage JDK 21+ virtual threads to vastly simplify core.async’s implementation and have started along this path in earnest.</em></li>
<li><em><a href="">Simplify my blog</a></em> – <em>I’d love to move away from WordPress in 2025.</em></li>
<li><em><a href="https://gist.github.com/fogus/6d716276678b0698c96dd13e040c71eb">Juxt</a></em> – <em>Juxt is my exploration in functional concatenative language design built on the JVM. It’s not yet clear to me if or when I would ever release this into the wild, but the explorations have been great fun and I’ve used Juxt as a vehicle for finding relevant books and papers.<sup id="fnref:juxtbib"><a href="#fn:juxtbib" rel="footnote">5</a></sup> That said, most of my programming time is spent maintaining and evolving Clojure, but there are rare moments of time that I can spend on Juxt, and I plan to continue to do so in 2025.</em></li>
</ul>

<center><a href="https://blog.fogus.me/wp-content/uploads/2024/12/juxt.jpg"><img decoding="async" src="https://blog.fogus.me/wp-content/uploads/2024/12/juxt-274x300.jpg" alt="" width="274" height="300" srcset="https://blog.fogus.me/wp-content/uploads/2024/12/juxt-274x300.jpg 274w, https://blog.fogus.me/wp-content/uploads/2024/12/juxt-768x840.jpg 768w, https://blog.fogus.me/wp-content/uploads/2024/12/juxt.jpg 816w" sizes="(max-width: 274px) 100vw, 274px"></a></center>

<h2>2024 Tech Radar</h2>

<ul>
<li>try: <a href="https://www.amazon.com/BOOX-Tablet-Go-10-3-ePaper/dp/B0D4DFT3W3/?tag=fogus-20">Boox Go 10.3 tablet</a> – recommended by many colleagues</li>
<li>adopt: <a href="https://apps.apple.com/us/app/blank-spaces-launcher/id1570856853">Blank Spaces app</a> – helps to avoid phone brain-drain </li>
<li>assess: <a href="https://www.typescriptlang.org/">TypeScript</a> – What does it buy <em>me</em> over JS?</li>
<li>hold: <a href="https://ziglang.org/">Zig</a> – This looks like a dead-end for me</li>
<li>stop: <a href="https://www.amazon.com/Joy-Clojure-Michael-Fogus/dp/1617291412/?tag=fogus-20">Joy of Clojure</a> 3rd edition – Another edition is unlikely but hopefully something else may come of this work… this is an evolving situation.</li>
</ul>

<h2>People who inspired me in 2024 (in no particular order)</h2>

<p>Yuki, Keita, Shota, Craig Andera, Carin Meier, Justin Gehtland, Rich Hickey, Nick Bentley, Paula Gearon, Zeeshan Lakhani, Brian Goetz, David Nolen, Jeb Beich, Paul Greenhill, Kristin Looney, Andy Looney, Kurt Christensen, Samm Deighan, David Chelimsky, Chas Emerick, Stacey Abrams, Paul deGrandis, Nada Amin, Michiel Borkent, Alvaro Videla, Slava Pestov, Yoko Harada, Mike Fikes, Dan De Aguiar, Christian Romney, Russ Olsen, Alex Miller, Adam Friedman, Tracie Harris, Alan Kay, Janet A. Carr, Wayne Applewhite, Naoko Higashide, Zach Tellman, Nate Prawdzik, Bobbi Towers, JF Martel, Phil Ford, Nate Hayden, Sean Ross, Tim Good, Chris Redinger, Steve Jensen, Jordan Miller, Tim Ewald, Stu Halloway, Jack Rusher, Michael Berstein, Benoît Fleury, Rafael Ferreira, Robert Randolph, Joe Lane, Renee Lee, Pedro Matiello, Jarrod Taylor, Jaret Binford, Ailan Batista, Matheus Machado, Quentin S. Crisp, John Cooper, Conrad Barski, Amabel Holland, Ben Kamphaus, Barry Malzberg (RIP), Kory Heath (RIP).</p>

<p>Onward to 2025!</p>

<p>:F</p>



            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Commercial tea bags release microplastics, entering human cells (268 pts)]]></title>
            <link>https://medicalxpress.com/news/2024-12-commercial-tea-bags-millions-microplastics.html</link>
            <guid>42494746</guid>
            <pubDate>Mon, 23 Dec 2024 14:47:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2024-12-commercial-tea-bags-millions-microplastics.html">https://medicalxpress.com/news/2024-12-commercial-tea-bags-millions-microplastics.html</a>, See on <a href="https://news.ycombinator.com/item?id=42494746">Hacker News</a></p>
Couldn't get https://medicalxpress.com/news/2024-12-commercial-tea-bags-millions-microplastics.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Xerox to acquire Lexmark (174 pts)]]></title>
            <link>https://newsroom.lexmark.com/2024-12-23-Xerox-to-Acquire-Lexmark</link>
            <guid>42494067</guid>
            <pubDate>Mon, 23 Dec 2024 12:57:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsroom.lexmark.com/2024-12-23-Xerox-to-Acquire-Lexmark">https://newsroom.lexmark.com/2024-12-23-Xerox-to-Acquire-Lexmark</a>, See on <a href="https://news.ycombinator.com/item?id=42494067">Hacker News</a></p>
Couldn't get https://newsroom.lexmark.com/2024-12-23-Xerox-to-Acquire-Lexmark: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Can AI do maths yet? Thoughts from a mathematician (213 pts)]]></title>
            <link>https://xenaproject.wordpress.com/2024/12/22/can-ai-do-maths-yet-thoughts-from-a-mathematician/</link>
            <guid>42493464</guid>
            <pubDate>Mon, 23 Dec 2024 10:50:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xenaproject.wordpress.com/2024/12/22/can-ai-do-maths-yet-thoughts-from-a-mathematician/">https://xenaproject.wordpress.com/2024/12/22/can-ai-do-maths-yet-thoughts-from-a-mathematician/</a>, See on <a href="https://news.ycombinator.com/item?id=42493464">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
<p>So the big news this week is that o3, OpenAI’s new language model, got 25% on FrontierMath. Let’s start by explaining what this means.</p>



<h2>What is o3? What is FrontierMath?</h2>



<p>A language model, as probably most people know, is one of these things like ChatGPT where you can ask it a question and it will write some sentences which are an attempt to give you an answer. There were language models before ChatGPT, and on the whole they couldn’t even write coherent sentences and paragraphs. ChatGPT was really the first public model which was coherent. There have been many other models since. Right now they’re still getting better really fast. How much longer this will go on for nobody knows, but there are lots of people pouring lots of money into this game so it would be a fool who bets on progress slowing down any time soon. o3 is a new language model. </p>



<p>FrontierMath is a secret dataset of “hundreds” of hard maths questions, curated by Epoch AI, and announced last month. “Hundreds” is a quote from <a href="https://arxiv.org/abs/2411.04872" target="_blank" rel="noreferrer noopener">the paper</a> (first line of the abstract), but I’ve heard a rumour that when the paper came out there were under 200 questions, although I’ve heard another rumour that apparently more are have been added since. As an academic mathematician who spent their entire life collaborating openly on research problems and sharing my ideas with other people, it frustrates me a little that already in this paragraph we’ve seen more questions than answers — I am not even to give you a coherent description of some basic facts about this dataset, for example, its size. However there is a good reason for the secrecy. Language models train on large databases of knowledge, so you moment you make a database of maths questions public, the language models will train on it. And then if you ask such a model a question from the database they’ll probably just rattle off the answer which they already saw.</p>



<h2>How hard is the FrontierMath dataset?</h2>



<p>So what are the questions in the FrontierMath dataset like? Here’s what we know. They’re not “prove this theorem!” questions, they’re “find this number!” questions. More precisely, the paper says “Problems had to possess definitive, computable answers that could be automatically verified”, and in the five sample problems which were made public from the dataset (Appendix A of the paper, pages 14 to 23) the solutions are all positive whole numbers (one answer is 9811, another is 367707, and the final three solutions are even larger — clearly these questions are designed in such a way that random guesswork is extremely unlikely to succeed). The sample questions are nontrivial, even to a research mathematician. I understood the statements of all five questions. I could do the third one relatively quickly (I had seen the trick before that the function mapping a natural n to alpha^n was p-adically continuous in n iff the p-adic valuation of alpha-1 was positive) and I knew exactly how to do the 5th one (it’s a standard trick involving the Weil conjectures for curves) but I didn’t bother doing the algebra to work out the exact 13-digit answer. The first and second question I knew I couldn’t do, and I figured I might be able to make progress on the 4th one if I put some real effort in, but ultimately I didn’t attempt it, I just read the solution. I suspect that a typical smart mathematics undergraduate would struggle to do even one of these questions. To do the first one you would, I imagine, have to be at least a PhD student in analytic number theory. The FrontierMath paper contains some quotes from mathematicians about the difficulty level of the problems. Tao (Fields Medal) says “These are extremely challenging” and suggests that they can only be tackled by a domain expert (and indeed the two sample questions which I could solve are in arithmetic, my area of expertise; I failed to do all of the ones outside my area). Borcherds (also Fields Medal) however is quoted in the paper as saying that machines producing numerical answers “aren’t quite the same as coming up with original proofs”.</p>



<p>So why make such a dataset? The problem is that grading solutions to “hundreds” of answers to “prove this theorem!” questions is expensive (one would not trust a machine to do grading at this level, at least in 2024, so one would have to pay human experts), whereas checking whether hundreds of numbers in one list correspond to hundreds of numbers in another list can be done in a fraction of a second by a computer. As Borcherds pointed out, mathematics researchers spend most of the time trying to come up with proofs or ideas, rather than numbers, however the FrontierMath dataset is still extremely valuable because the area of AI for mathematics is desperately short of hard datasets, and creating a dataset such as this is very hard work (or equivalently very expensive). </p>



<p>So there was an article about the dataset in <a href="https://www.science.org/content/article/brutal-math-test-stumps-ai-not-human-experts" target="_blank" rel="noreferrer noopener">Science</a> and I was quoted in it as saying “If you have a system that can ace that database, then it’s game over for mathematicians.” Just to be clear: I had nothing to do with the dataset, I’ve only seen the five public questions, and was basing my comments on those. I also said “In my opinion, currently, AI is a long way away from being able to do those questions … but I’ve been wrong before”. And then this week there’s an announcement that the language model o3 got a score of 25 percent on the dataset. I was shocked.</p>



<h2>What exactly has happened here?</h2>



<p>Why was I shocked? Because my mental model on where “AI” is currently, when it comes to doing mathematics, is “undergrad or pre-undergrad”. It’s getting very good at “Olympiad-style” problems of the sort given to bright high-schoolers. Within a year it’s absolutely clear that AI systems will be passing undergraduate mathematics exams (not least because when you’re setting an undergraduate mathematics exam you ideally need to make sure that you don’t fail 50 percent of the class, so you throw in a couple of standard questions which are very similar to questions that the students have seen already, to ensure that those with a basic understanding of the course will pass the exam. Machines will easily be able to ace such questions). But the jump from that to having innovative ideas at advanced undergrad/early PhD level beyond recycling standard ideas seems <em>to me</em> to be quite a big one. For example I was very unimpressed by the ChatGPT answers to the recent Putnam exam posted <a href="https://x.com/danhendrycks/status/1866191952531845547?s=46">here</a> — as far as I can see only question B4 was answered adequately by the machine, most other answers are worth one or two out of 10 at most. So I was expecting this dataset to remain pretty unattackable for a couple of years. </p>



<p>My initial excitement was tempered however by a post from Elliot Glazer from Epoch AI on <a href="https://www.reddit.com/r/OpenAI/comments/1hiq4yv/comment/m30yfqp/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button" target="_blank" rel="noreferrer noopener">Reddit</a> where he claimed that in fact 25 percent of the problems in the dataset were “IMO/undergrad style problems”. This claim is a little confusing because I would be hard pressed to apply such adjectives to any of the five publically-released problems in the dataset; even the simplest one used the Weil conjectures for curves (or a brute force argument which is probably just about possible but would be extremely painful, as it involves factoring 10^12 degree 3 polynomials over a finite field, although this could certainly be parallelised). This of course raises questions in my mind about what the actual level of the problems in this secret dataset is (or equivalently whether the five public questions are actually a representative sample), but this is not knowledge which we’re likely to have access to. Given this new piece of information that 25 percent of the problems are undergraduate level, perhaps I will revert to being unsurprised again, but will look forward to being surprised when AI is getting nearer 50 percent on the dataset, because performance at “qual level” (as Elliot describes it — the next 50 percent of the questions) is exactly what I’m waiting to see from these systems — for me this would represent a big breakthrough.</p>



<h2>Prove this theorem!</h2>



<p>However, as Borcherds points out, even if we ended up with a machine which was super-human at “find this number!” questions, it would still have limited applicability in many areas of research mathematics, where the key question of interest is usually how to “prove this theorem!”.  In my mind, the biggest success story in 2024 is DeepMind’s <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/" target="_blank" rel="noreferrer noopener">AlphaProof</a>, which solved four out of the six 2024 IMO (International Mathematics Olympiad) problems. These were either “prove this theorem!” or “find a number and furthermore prove that it’s the right number” questions and for three of them, the output of the machine was a fully formalized Lean proof. <a href="https://lean-lang.org/" target="_blank" rel="noreferrer noopener">Lean</a> is an interactive theorem prover with a solid mathematics library <a href="https://github.com/leanprover-community/mathlib4" target="_blank" rel="noreferrer noopener">mathlib</a> containing many of the techniques needed to solve IMO problems and a lot more besides; DeepMind’s system’s solutions were human-checked and verified to be “full marks” solutions. However, we are back at high school level again; whilst the questions are extremely hard, the solutions use only school-level techniques. In 2025 I’m sure we’ll see machines performing at gold level standard in the IMO. However this now forces us to open up the “grading” can of worms which I’ve already mentioned once, and I’ll finish this post by talking a little more about it.</p>



<h2>Who is marking the machines?</h2>



<p>July 2025. I can envisage the following situation. As well as hundreds of the world’s smartest schoolchildren entering the IMO, there will be machines entering. Hopefully not too many though. Because the systems will be of two types. There will be systems submitting answers in the language of a computer proof checker like Lean (or Rocq, Isabelle, or many others). And there will be language models submitting answers in human language. The big difference between these two submissions are that: if a marker verifies that the <em>statement</em> of the question has been correctly translated into the computer proof checker, then all they need to do is to check that the proof compiles and then they basically know that it is a “full marks” solution. For the language models we will have a situation like the poor Putnam solutions above — the computer will write something, it will look convincing, but a human is going to have to read it carefully and grade it, and there is certainly no guarantee that it will be a “full marks” solution. Borcherds is right to remind the AI community that “prove this theorem!” is what we really want to see as mathematicians, and language models are currently at least an order of magnitude less accurate than expert humans when it comes to logical reasoning. I am dreading the inevitable onslaught in a year or two of language model “proofs” of the Riemann hypothesis which will just contain claims which are vague or inaccurate in the middle of 10 pages of correct mathematics which the human will have to wade through to find the line which doesn’t hold up. On the other hand, theorem provers are at least an order of magnitude more accurate: every time I’ve seen Lean not accept a human argument in the mathematical literature, the human has been wrong. </p>



<p>In fact, as mathematicians, we would like to see more than “prove this theorem!”. We would like to see “prove this theorem, correctly, and explain what makes the proof work in a way which we humans understand”. With the language model approach I worry (a lot) about “correctly” and with the theorem prover approach I worry about “in a way which we humans understand”. There is still a huge amount to be done. Progress is currently happening really quickly. But we are a long way away. When will we “beat the undergraduate barrier”? Nobody knows.</p>

			
														</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I built a platform for discovering and sharing Chrome extension collections (113 pts)]]></title>
            <link>https://webextension.net/collections</link>
            <guid>42492753</guid>
            <pubDate>Mon, 23 Dec 2024 08:15:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webextension.net/collections">https://webextension.net/collections</a>, See on <a href="https://news.ycombinator.com/item?id=42492753">Hacker News</a></p>
Couldn't get https://webextension.net/collections: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Litestack: All your data infrastructure, in one Ruby gem (141 pts)]]></title>
            <link>https://github.com/oldmoe/litestack</link>
            <guid>42491196</guid>
            <pubDate>Mon, 23 Dec 2024 02:32:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/oldmoe/litestack">https://github.com/oldmoe/litestack</a>, See on <a href="https://news.ycombinator.com/item?id=42491196">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/oldmoe/litestack/blob/master/assets/litestack_logo_teal_large.png?raw=true"><img src="https://github.com/oldmoe/litestack/raw/master/assets/litestack_logo_teal_large.png?raw=true" alt="litestack"></a></p>
<p dir="auto"><a href="https://badge.fury.io/rb/litestack" rel="nofollow"><img height="21" src="https://camo.githubusercontent.com/799fb43754d9e4aae16af0ca0b65a088daca4e67af89ab5bd8707333509109c6/68747470733a2f2f62616467652e667572792e696f2f72622f6c697465737461636b2e737667" alt="Gem Version" data-canonical-src="https://badge.fury.io/rb/litestack.svg"></a>
<a href="https://rubygems.org/gems/litestack" rel="nofollow"><img height="21" src="https://camo.githubusercontent.com/9315d4493fe6a8508be2ac3c7d25e51fbd4c3dacdb9358270538a90e42abbcf9/68747470733a2f2f696d672e736869656c64732e696f2f67656d2f64742f6c697465737461636b3f636f6c6f723d627269676874677265656e266c6162656c3d5275627967656d73253230446f776e6c6f616473" alt="RubyGems Downloads" data-canonical-src="https://img.shields.io/gem/dt/litestack?color=brightgreen&amp;label=Rubygems%20Downloads"></a></p>
<p dir="auto">All your data infrastructure, in a gem!</p>
<p dir="auto">Litestack is a Ruby gem that provides both Ruby and  Ruby on Rails applications an all-in-one solution for web application data infrastructure. It exploits the power and embeddedness of SQLite to deliver a full-fledged SQL database, a fast cache , a robust job queue, a reliable message broker, a full text search engine and a metrics platform all in a single package.</p>
<p dir="auto">Compared to conventional approaches that require separate servers and databases, Litestack offers superior performance, efficiency, ease of use, and cost savings. Its embedded database and cache reduce memory and CPU usage, while its simple interface streamlines the development process. Overall, Litestack sets a new standard for web application development and is an excellent choice for those who demand speed, efficiency, and simplicity.</p>
<p dir="auto">You can read more about why litestack can be a good choice for your next web application <strong><a href="https://github.com/oldmoe/litestack/blob/master/WHYLITESTACK.md">here</a></strong>, you might also be interested in litestack <strong><a href="https://github.com/oldmoe/litestack/blob/master/BENCHMARKS.md">benchmarks</a></strong>.</p>
<p dir="auto">With litestack you only need to add a single gem to your app which would replace a host of other gems and services, for example, a typical Rails app using litestack will no longer need the following services:</p>
<ul dir="auto">
<li>Database Server (e.g. PostgreSQL, MySQL)</li>
<li>Cache Server (e.g. Redis, Memcached)</li>
<li>Job Processor (e.g. Sidekiq, Goodjob)</li>
<li>Pubsub Server (e.g. Redis, PostgreSQL)</li>
<li>Fulltext Search Server (e.g. Elasticsearch, Meilisearch)</li>
</ul>
<p dir="auto">To make it even more efficient, litestack will detect the presence of Fiber based IO frameworks like Async (e.g. when you use the Falcon web server) or Polyphony. It will then switch its background workers for caches and queues to fibers (using the semantics of the existing framework). This is done transparently and will generally lead to lower CPU and memory utilization.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/oldmoe/litestack/blob/master/assets/litestack_advantage.png?raw=true"><img src="https://github.com/oldmoe/litestack/raw/master/assets/litestack_advantage.png?raw=true" alt="litestack"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Add the <code>litestack</code> gem line to your application's Gemfile:</p>

<p dir="auto">To configure a Rails application to run the full litestack, run:</p>
<div data-snippet-clipboard-copy-content="$ rails generate litestack:install"><pre><code>$ rails generate litestack:install
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">litestack currently offers six main components</p>
<ul dir="auto">
<li>litedb</li>
<li>litecache</li>
<li>litejob</li>
<li>litecable</li>
<li>litesearch</li>
<li>litemetric</li>
</ul>
<blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/oldmoe/litestack/blob/master/assets/litedb_logo_teal.png?raw=true"><img src="https://github.com/oldmoe/litestack/raw/master/assets/litedb_logo_teal.png?raw=true" alt="litedb"></a></p>
</blockquote>
<p dir="auto">litedb is a wrapper around SQLite3, offering a better default configuration that is tuned for concurrency and performance. Out of the box, litedb works seamlessly between multiple processes without database locking errors. litedb can be used in multiple ways, including:</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Direct litedb usage</h4><a id="user-content-direct-litedb-usage" aria-label="Permalink: Direct litedb usage" href="#direct-litedb-usage"></a></p>
<p dir="auto">litedb can be used exactly as the SQLite3 gem, since litedb inherits from SQLite3</p>
<div dir="auto" data-snippet-clipboard-copy-content="require 'litestack'
db = Litedb.new(path_to_db)
db.execute(&quot;create table users(id integer primary key, name text)&quot;)
db.execute(&quot;insert into users(name) values (?)&quot;, &quot;Hamada&quot;)
db.query(&quot;select count(*) from users&quot;) # => [[1]]"><pre><span>require</span> <span>'litestack'</span>
<span>db</span> <span>=</span> <span>Litedb</span><span>.</span><span>new</span><span>(</span><span>path_to_db</span><span>)</span>
<span>db</span><span>.</span><span>execute</span><span>(</span><span>"create table users(id integer primary key, name text)"</span><span>)</span>
<span>db</span><span>.</span><span>execute</span><span>(</span><span>"insert into users(name) values (?)"</span><span>,</span> <span>"Hamada"</span><span>)</span>
<span>db</span><span>.</span><span>query</span><span>(</span><span>"select count(*) from users"</span><span>)</span> <span># =&gt; [[1]]</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">ActiveRecord</h4><a id="user-content-activerecord" aria-label="Permalink: ActiveRecord" href="#activerecord"></a></p>
<p dir="auto">litedb provides tight Rails/ActiveRecord integration and can be configured as follows</p>
<p dir="auto">In database.yml</p>
<div dir="auto" data-snippet-clipboard-copy-content="adapter: litedb
# normal sqlite3 configuration follows"><pre><span>adapter</span>: <span>litedb</span>
<span><span>#</span> normal sqlite3 configuration follows</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Sequel</h4><a id="user-content-sequel" aria-label="Permalink: Sequel" href="#sequel"></a></p>
<p dir="auto">litedb offers integration with the Sequel database toolkit and can be configured as follows</p>
<div dir="auto" data-snippet-clipboard-copy-content="DB = Sequel.connect(&quot;litedb://path_to_db_file&quot;)"><pre><span>DB</span> <span>=</span> <span>Sequel</span><span>.</span><span>connect</span><span>(</span><span>"litedb://path_to_db_file"</span><span>)</span></pre></div>
<blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/oldmoe/litestack/blob/master/assets/litecache_logo_teal.png?raw=true"><img src="https://github.com/oldmoe/litestack/raw/master/assets/litecache_logo_teal.png?raw=true" alt="litecache"></a></p>
</blockquote>
<p dir="auto">litecache is a high speed, low overhead caching library that uses SQLite as its backend. litecache can be accessed from multiple processes on the same machine seamlessly. It also has features like key expiry, LRU based eviction and increment/decrement of integer values.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Direct litecache usage</h4><a id="user-content-direct-litecache-usage" aria-label="Permalink: Direct litecache usage" href="#direct-litecache-usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="require 'litestack'
cache = Litecache.new(path: &quot;path_to_file&quot;)
cache.set(&quot;key&quot;, &quot;value&quot;)
cache.get(&quot;key&quot;) #=> &quot;value&quot;"><pre><span>require</span> <span>'litestack'</span>
<span>cache</span> <span>=</span> <span>Litecache</span><span>.</span><span>new</span><span>(</span><span>path</span>: <span>"path_to_file"</span><span>)</span>
<span>cache</span><span>.</span><span>set</span><span>(</span><span>"key"</span><span>,</span> <span>"value"</span><span>)</span>
<span>cache</span><span>.</span><span>get</span><span>(</span><span>"key"</span><span>)</span> <span>#=&gt; "value"</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">ActiveResource::Cache</h4><a id="user-content-activeresourcecache" aria-label="Permalink: ActiveResource::Cache" href="#activeresourcecache"></a></p>
<p dir="auto">In your desired environment file (e.g. production.rb)</p>
<div dir="auto" data-snippet-clipboard-copy-content="config.cache_store = :litecache, {path: './path/to/your/cache/file'}"><pre><span>config</span><span>.</span><span>cache_store</span> <span>=</span> <span>:litecache</span><span>,</span> <span>{</span><span>path</span>: <span>'./path/to/your/cache/file'</span><span>}</span></pre></div>
<p dir="auto">This provides a transparent integration that uses the Rails caching interface</p>
<p dir="auto">litecache spawns a background thread for cleanup purposes. In case it detects that the current environment has <em>Fiber::Scheduler</em> or <em>Polyphony</em> loaded it will spawn a fiber instead, saving on both memory and CPU cycles.</p>
<blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/oldmoe/litestack/blob/master/assets/litejob_logo_teal.png?raw=true"><img src="https://github.com/oldmoe/litestack/raw/master/assets/litejob_logo_teal.png?raw=true" alt="litejob"></a></p>
</blockquote>
<p dir="auto">More info about Litejob can be found in the <a href="https://github.com/oldmoe/litestack/wiki/Litejob-guide">litejob guide</a></p>
<p dir="auto">litejob is a fast and very efficient job queue processor for Ruby applications. It builds on top of SQLite as well, which provides transactional guarantees, persistence and exceptional performance.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Direct litejob usage</h4><a id="user-content-direct-litejob-usage" aria-label="Permalink: Direct litejob usage" href="#direct-litejob-usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="require 'litestack'
# define your job class
class MyJob
  include ::Litejob
      
  queue = :default
      
  # must implement perform, with any number of params
  def perform(params)
    # do stuff
  end
end
    
#schedule a job asynchronusly
MyJob.perform_async(params)
    
#schedule a job at a certain time
MyJob.perform_at(time, params)
    
#schedule a job after a certain delay
MyJob.perform_after(delay, params)"><pre><span>require</span> <span>'litestack'</span>
<span># define your job class</span>
<span>class</span> <span>MyJob</span>
  <span>include</span> ::<span>Litejob</span>
      
  <span>queue</span> <span>=</span> <span>:default</span>
      
  <span># must implement perform, with any number of params</span>
  <span>def</span> <span>perform</span><span>(</span><span>params</span><span>)</span>
    <span># do stuff</span>
  <span>end</span>
<span>end</span>
    
<span>#schedule a job asynchronusly</span>
<span>MyJob</span><span>.</span><span>perform_async</span><span>(</span><span>params</span><span>)</span>
    
<span>#schedule a job at a certain time</span>
<span>MyJob</span><span>.</span><span>perform_at</span><span>(</span><span>time</span><span>,</span> <span>params</span><span>)</span>
    
<span>#schedule a job after a certain delay</span>
<span>MyJob</span><span>.</span><span>perform_after</span><span>(</span><span>delay</span><span>,</span> <span>params</span><span>)</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">ActiveJob</h4><a id="user-content-activejob" aria-label="Permalink: ActiveJob" href="#activejob"></a></p>
<p dir="auto">In your desired environment file (e.g. production.rb)</p>
<div dir="auto" data-snippet-clipboard-copy-content="config.active_job.queue_adapter = :litejob"><pre><span>config</span><span>.</span><span>active_job</span><span>.</span><span>queue_adapter</span> <span>=</span> <span>:litejob</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Configuration file</h4><a id="user-content-configuration-file" aria-label="Permalink: Configuration file" href="#configuration-file"></a></p>
<p dir="auto">You can add more configuration in litejob.yml (or config/litejob.yml if you are integrating with Rails)</p>
<div dir="auto" data-snippet-clipboard-copy-content="queues:
    - [default, 1]
    - [urgent, 5]
    - [critical, 10, &quot;spawn&quot;]"><pre><span>queues</span>:
    - <span>[default, 1]</span>
    - <span>[urgent, 5]</span>
    - <span>[critical, 10, "spawn"]</span></pre></div>
<p dir="auto">The queues need to include a name and a priority (a number between 1 and 10) and can also optionally add the token "spawn", which means every job will run it its own concurrency context (thread or fiber)</p>
<blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/oldmoe/litestack/blob/master/assets/litecable_logo_teal.png?raw=true"><img src="https://github.com/oldmoe/litestack/raw/master/assets/litecable_logo_teal.png?raw=true" alt="litecable"></a></p>
</blockquote>
<p dir="auto"><h4 tabindex="-1" dir="auto">ActionCable</h4><a id="user-content-actioncable" aria-label="Permalink: ActionCable" href="#actioncable"></a></p>
<p dir="auto">This is a drop in replacement adapter for actioncable that replaces <code>async</code> and other production adapters (e.g. PostgreSQL, Redis). This adapter is currently only tested in local (inline) mode.</p>
<p dir="auto">Getting up and running with litecable requires configuring your cable.yaml file under the config/ directory</p>
<p dir="auto">cable.yaml</p>
<div dir="auto" data-snippet-clipboard-copy-content="development:
  adapter: litecable

test:
  adapter: test

staging:
  adapter: litecable

production:
  adapter: litecable"><pre><span>development</span>:
  <span>adapter</span>: <span>litecable</span>

<span>test</span>:
  <span>adapter</span>: <span>test</span>

<span>staging</span>:
  <span>adapter</span>: <span>litecable</span>

<span>production</span>:
  <span>adapter</span>: <span>litecable</span></pre></div>
<blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/oldmoe/litestack/blob/master/assets/litesearch_logo_teal.png?raw=true"><img src="https://github.com/oldmoe/litestack/raw/master/assets/litesearch_logo_teal.png?raw=true" alt="litesearch"></a></p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Litesearch</h3><a id="user-content-litesearch" aria-label="Permalink: Litesearch" href="#litesearch"></a></p>
<p dir="auto">Litesearch adds full text search capabilities to Litedb, you can use it in standalone mode as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="require 'litestack/litedb'
db = Litedb.new(&quot;:memory:&quot;)
# create the index
idx = db.search_index('index_name') do |schema|
    schema.fields [:sender, :receiver, :body]
    schema.field :subject, weight: 10
    schema.tokenizer :trigram
end
# add documents
idx.add({sender: 'Kamal', receiver: 'Laila', subject: 'Are the girls awake?', body: 'I got them the new phones they asked for, are they awake?'})
# search the index, all fields
idx.search('kamal')
# search the index, specific field, partial workd (trigram)
idx.search('subject: awa') "><pre><span>require</span> <span>'litestack/litedb'</span>
<span>db</span> <span>=</span> <span>Litedb</span><span>.</span><span>new</span><span>(</span><span>":memory:"</span><span>)</span>
<span># create the index</span>
<span>idx</span> <span>=</span> <span>db</span><span>.</span><span>search_index</span><span>(</span><span>'index_name'</span><span>)</span> <span>do</span> |<span>schema</span>|
    <span>schema</span><span>.</span><span>fields</span> <span>[</span><span>:sender</span><span>,</span> <span>:receiver</span><span>,</span> <span>:body</span><span>]</span>
    <span>schema</span><span>.</span><span>field</span> <span>:subject</span><span>,</span> <span>weight</span>: <span>10</span>
    <span>schema</span><span>.</span><span>tokenizer</span> <span>:trigram</span>
<span>end</span>
<span># add documents</span>
<span>idx</span><span>.</span><span>add</span><span>(</span><span>{</span><span>sender</span>: <span>'Kamal'</span><span>,</span> <span>receiver</span>: <span>'Laila'</span><span>,</span> <span>subject</span>: <span>'Are the girls awake?'</span><span>,</span> <span>body</span>: <span>'I got them the new phones they asked for, are they awake?'</span><span>}</span><span>)</span>
<span># search the index, all fields</span>
<span>idx</span><span>.</span><span>search</span><span>(</span><span>'kamal'</span><span>)</span>
<span># search the index, specific field, partial workd (trigram)</span>
<span>idx</span><span>.</span><span>search</span><span>(</span><span>'subject: awa'</span><span>)</span> </pre></div>
<p dir="auto">Litesearch integrates tightly with ActiveRecord and Sequel, here are integration examples</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">ActiveRecord</h4><a id="user-content-activerecord-1" aria-label="Permalink: ActiveRecord" href="#activerecord-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="class Author < ActiveRecord::Base
    has_many :books
end

class Book < ActiveRecord::Base
    belongs_to :author

    include Litesearch::Model

    litesearch do |schema|
        schema.fields [:title, :description]
        schema.field :author, target: 'authors.name'
        schema.tokenizer :porter
    end
end
# insert records
Author.create(name: 'Adam A. Writer') 
Book.create(title: 'The biggest stunt', author_id: 1, description: 'a description') 
# search the index, the search method integrates with AR's query interface
Book.search('author: writer').limit(1).all"><pre><span>class</span> <span>Author</span> &lt; <span>ActiveRecord</span>::<span>Base</span>
    <span>has_many</span> <span>:books</span>
<span>end</span>

<span>class</span> <span>Book</span> &lt; <span>ActiveRecord</span>::<span>Base</span>
    <span>belongs_to</span> <span>:author</span>

    <span>include</span> <span>Litesearch</span>::<span>Model</span>

    <span>litesearch</span> <span>do</span> |<span>schema</span>|
        <span>schema</span><span>.</span><span>fields</span> <span>[</span><span>:title</span><span>,</span> <span>:description</span><span>]</span>
        <span>schema</span><span>.</span><span>field</span> <span>:author</span><span>,</span> <span>target</span>: <span>'authors.name'</span>
        <span>schema</span><span>.</span><span>tokenizer</span> <span>:porter</span>
    <span>end</span>
<span>end</span>
<span># insert records</span>
<span>Author</span><span>.</span><span>create</span><span>(</span><span>name</span>: <span>'Adam A. Writer'</span><span>)</span> 
<span>Book</span><span>.</span><span>create</span><span>(</span><span>title</span>: <span>'The biggest stunt'</span><span>,</span> <span>author_id</span>: <span>1</span><span>,</span> <span>description</span>: <span>'a description'</span><span>)</span> 
<span># search the index, the search method integrates with AR's query interface</span>
<span>Book</span><span>.</span><span>search</span><span>(</span><span>'author: writer'</span><span>)</span><span>.</span><span>limit</span><span>(</span><span>1</span><span>)</span><span>.</span><span>all</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Sequel</h4><a id="user-content-sequel-1" aria-label="Permalink: Sequel" href="#sequel-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="class Author < Sequel::Model
    one_to_many :books
end

class Book < Sequel::Model
    many_to_one :author

    include Litesearch::Model
    litesearch do |schema|
        schema.fields [:title, :description]
        schema.field :author, target: 'authors.name'
        schema.tokenizer :porter
    end
end
# insert records
Author.create(name: 'Adam A. Writer') 
Book.create(title: 'The biggest stunt', author_id: 1, description: 'a description') 
# search the index, the search method integrates with Sequel's query interface
Book.search('author: writer').limit(1).all"><pre><span>class</span> <span>Author</span> &lt; <span>Sequel</span>::<span>Model</span>
    <span>one_to_many</span> <span>:books</span>
<span>end</span>

<span>class</span> <span>Book</span> &lt; <span>Sequel</span>::<span>Model</span>
    <span>many_to_one</span> <span>:author</span>

    <span>include</span> <span>Litesearch</span>::<span>Model</span>
    <span>litesearch</span> <span>do</span> |<span>schema</span>|
        <span>schema</span><span>.</span><span>fields</span> <span>[</span><span>:title</span><span>,</span> <span>:description</span><span>]</span>
        <span>schema</span><span>.</span><span>field</span> <span>:author</span><span>,</span> <span>target</span>: <span>'authors.name'</span>
        <span>schema</span><span>.</span><span>tokenizer</span> <span>:porter</span>
    <span>end</span>
<span>end</span>
<span># insert records</span>
<span>Author</span><span>.</span><span>create</span><span>(</span><span>name</span>: <span>'Adam A. Writer'</span><span>)</span> 
<span>Book</span><span>.</span><span>create</span><span>(</span><span>title</span>: <span>'The biggest stunt'</span><span>,</span> <span>author_id</span>: <span>1</span><span>,</span> <span>description</span>: <span>'a description'</span><span>)</span> 
<span># search the index, the search method integrates with Sequel's query interface</span>
<span>Book</span><span>.</span><span>search</span><span>(</span><span>'author: writer'</span><span>)</span><span>.</span><span>limit</span><span>(</span><span>1</span><span>)</span><span>.</span><span>all</span></pre></div>
<blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/oldmoe/litestack/blob/master/assets/litemetric_logo_teal.png?raw=true"><img src="https://github.com/oldmoe/litestack/raw/master/assets/litemetric_logo_teal.png?raw=true" alt="litemetric"></a></p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Litemetric</h3><a id="user-content-litemetric" aria-label="Permalink: Litemetric" href="#litemetric"></a></p>
<p dir="auto">Litestack comes with a module that can collect useful metrics for its different components, in each component, you need to add the following to the respective .yml file (database.yml in case of Litedb)</p>
<div dir="auto" data-snippet-clipboard-copy-content="    metrics: true # default is false"><pre>    <span>metrics</span>: <span>true </span><span><span>#</span> default is false</span></pre></div>
<p dir="auto">If you have the metrics enabled, it will start collecting data from the various modules and will store them in a database file called metric.db located in the Litesupport.root folder</p>
<p dir="auto">Litemetric has an API that would enable collecting arbitrary metrics for non-litestack classes. The metrics will be in the database but currently the Liteboard is only able to show correct data for Litestack modules, displaying arbitrary metrics for other components will be included later.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Liteboard</h3><a id="user-content-liteboard" aria-label="Permalink: Liteboard" href="#liteboard"></a></p>
<p dir="auto">Liteboard is a simple web server that provides a web interface for the collected metrics, it should be available globally, for usage instructions type</p>

<p dir="auto">It allows you to point to a specific metrics database file or a config file and then it will display the data in that metrics database</p>
<p dir="auto">Example metrics views:</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Litedb</h4><a id="user-content-litedb" aria-label="Permalink: Litedb" href="#litedb"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/oldmoe/litestack/blob/master/assets/litedb_metrics.png?raw=true"><img src="https://github.com/oldmoe/litestack/raw/master/assets/litedb_metrics.png?raw=true" alt="litedb"></a></p>
<ul dir="auto">
<li>Database size, number of tables &amp; indexes</li>
<li>Number of read/write queries</li>
<li>Read/Write query ratio over time</li>
<li>Read/Write query time over time</li>
<li>Slowest queries</li>
<li>Most expensive queries (total run time = frequency * cost)</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Litecache</h4><a id="user-content-litecache" aria-label="Permalink: Litecache" href="#litecache"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/oldmoe/litestack/blob/master/assets/litecache_metrics.png?raw=true"><img src="https://github.com/oldmoe/litestack/raw/master/assets/litecache_metrics.png?raw=true" alt="litecache"></a></p>
<ul dir="auto">
<li>Cache size, % of size limit</li>
<li>Number of entries</li>
<li>Reads/Writes over time</li>
<li>Read hits/misses over time</li>
<li>Most written entries</li>
<li>Most read entries</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Bug reports and pull requests are welcome on GitHub at <a href="https://github.com/oldmoe/litestack">https://github.com/oldmoe/litestack</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The gem is available as open source under the terms of the <a href="https://opensource.org/licenses/MIT" rel="nofollow">MIT License</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Google Sheet on half of 13.6" MacBook Air screen is fully covered by popups (172 pts)]]></title>
            <link>https://imgur.com/a/NQskWzI</link>
            <guid>42490948</guid>
            <pubDate>Mon, 23 Dec 2024 01:43:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://imgur.com/a/NQskWzI">https://imgur.com/a/NQskWzI</a>, See on <a href="https://news.ycombinator.com/item?id=42490948">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA['United Healthcare' using DMCA against Luigi Mangione images (356 pts)]]></title>
            <link>https://abovethelaw.com/2024/12/united-healthcare-using-dmca-against-luigi-mangione-images-which-is-bizarre-wildly-inappropriate/</link>
            <guid>42490453</guid>
            <pubDate>Mon, 23 Dec 2024 00:26:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abovethelaw.com/2024/12/united-healthcare-using-dmca-against-luigi-mangione-images-which-is-bizarre-wildly-inappropriate/">https://abovethelaw.com/2024/12/united-healthcare-using-dmca-against-luigi-mangione-images-which-is-bizarre-wildly-inappropriate/</a>, See on <a href="https://news.ycombinator.com/item?id=42490453">Hacker News</a></p>
Couldn't get https://abovethelaw.com/2024/12/united-healthcare-using-dmca-against-luigi-mangione-images-which-is-bizarre-wildly-inappropriate/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Predictions for 2025? (238 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42490343</link>
            <guid>42490343</guid>
            <pubDate>Mon, 23 Dec 2024 00:08:06 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42490343">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=42490343: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Classified fighter jet specs leaked on War Thunder forums (186 pts)]]></title>
            <link>https://ukdefencejournal.org.uk/classified-fighter-jet-specs-leaked-on-war-thunder-again/</link>
            <guid>42490191</guid>
            <pubDate>Sun, 22 Dec 2024 23:46:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ukdefencejournal.org.uk/classified-fighter-jet-specs-leaked-on-war-thunder-again/">https://ukdefencejournal.org.uk/classified-fighter-jet-specs-leaked-on-war-thunder-again/</a>, See on <a href="https://news.ycombinator.com/item?id=42490191">Hacker News</a></p>
Couldn't get https://ukdefencejournal.org.uk/classified-fighter-jet-specs-leaked-on-war-thunder-again/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[MI300X vs. H100 vs. H200 Benchmark Part 1: Training – CUDA Moat Still Alive (195 pts)]]></title>
            <link>https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/</link>
            <guid>42489844</guid>
            <pubDate>Sun, 22 Dec 2024 22:47:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/">https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/</a>, See on <a href="https://news.ycombinator.com/item?id=42489844">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2 id="intro">Intro</h2>



<p>SemiAnalysis has been on a five-month long quest to settle the reality of MI300X. In theory, the MI300X should be at a huge advantage over Nvidia’s H100 and H200 in terms of specifications and Total Cost of Ownership (TCO). However, the reality is that the on paper specs as given below are not representative of performance that can be expected in a real-world environment. If AMD could deliver the below marketed performance with this memory, it would be a very strong competitor in the market.&nbsp;</p>



<figure><img data-recalc-dims="1" decoding="async" width="2184" height="1088" data-attachment-id="150418455" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/screenshot-3/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/10-H100-vs-H200-vs-MI300X-Basic-Specs-initial-1.jpg?fit=2184%2C1088&amp;ssl=1" data-orig-size="2184,1088" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;Screenshot&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;Screenshot&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="Screenshot" data-image-description="" data-image-caption="<p>Screenshot</p>
" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/10-H100-vs-H200-vs-MI300X-Basic-Specs-initial-1.jpg?fit=300%2C149&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/10-H100-vs-H200-vs-MI300X-Basic-Specs-initial-1.jpg?fit=1024%2C510&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/10-H100-vs-H200-vs-MI300X-Basic-Specs-initial-1.jpg?resize=2184%2C1088&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/10-H100-vs-H200-vs-MI300X-Basic-Specs-initial-1.jpg?w=2184&amp;ssl=1 2184w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/10-H100-vs-H200-vs-MI300X-Basic-Specs-initial-1.jpg?resize=300%2C149&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/10-H100-vs-H200-vs-MI300X-Basic-Specs-initial-1.jpg?resize=1024%2C510&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/10-H100-vs-H200-vs-MI300X-Basic-Specs-initial-1.jpg?resize=768%2C383&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/10-H100-vs-H200-vs-MI300X-Basic-Specs-initial-1.jpg?resize=1536%2C765&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/10-H100-vs-H200-vs-MI300X-Basic-Specs-initial-1.jpg?resize=2048%2C1020&amp;ssl=1 2048w" sizes="(max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis, Nvidia, AMD</figcaption></figure>



<p>Today we are going to talk through our five-month journey conducting independent analysis and training-focused benchmarking of the MI300X, the H100 and the H200, engaging with both NVIDIA and AMD. We will do a detailed overview of the numerous low-level benchmarks that we ran, see the table of contents for summary. Furthermore, we will compare the total cost of ownership of Nvidia and AMD GPUs and factor in performance. Ultimately much of what we are doing is openly giving a comprehensive public recommendation to AMD on what they need to do to be competitive and fix their software issues after five months of submitting and squashing bugs. It’s not just that it’s immature software, they need to change how they do development.&nbsp;</p>



<p>In short, when comparing Nvidia’s GPUs to AMD’s MI300X, we found that the potential on paper advantage of the MI300X was not realized due to a lack within AMD public release software stack and the lack of testing from AMD.</p>



<p>AMD’s software experience is riddled with bugs rendering out of the box training with AMD is impossible. We were hopeful that AMD could emerge as a strong competitor to NVIDIA in training workloads, but, as of today, this is unfortunately not the case. The CUDA moat has yet to be crossed by AMD due to AMD’s weaker-than-expected software Quality Assurance (QA) culture and its challenging out of the box experience.&nbsp;<strong>As fast as AMD tries to fill in the CUDA moat, NVIDIA engineers are working overtime to deepen said moat with new features, libraries, and performance updates.</strong></p>



<p>We shared benchmark source code and intermediate test results for GEMM benchmark and Single Node Training with both Nvidia and AMD and held calls and discussions to solicit feedback and implement improvements to the benchmarks, and we worked with AMD to implement bug fixes for the software stacks.&nbsp;</p>



<p>Our goal with this highly iterative interaction was to ensure that our tests are an unbiased evaluation of what real-world users would experience.&nbsp;</p>



<p>We initially planned to publish this article a few months ago but wanted to take the extra time to engage with the AMD team and explore possible fixes or development work. We spent a considerable time identifying and fixing AMD software bugs so that we could give AMD every chance to show MI300X unhindered by AMD software stack bugs as opposed to only showing problematic performance out of the box. To give a fair impression, we also explain the considerable amount of work on tuning and bug-squashing that it took to get there. We think this approach provides users with the best possible level of transparency.&nbsp;&nbsp;</p>



<p><strong>We wanted to contribute in any way we could to try to improve the AMD ecosystem.</strong>&nbsp;<strong>Though</strong>&nbsp;<strong>AMD software is much better now due to&nbsp;our bug reports and tire-kicking,&nbsp;its public software stack still falls short</strong>. We have open-sourced many of the benchmarks and created simple one-liner commands to reproduce them.</p>



<p>If Lisa Su and the AMD Leadership redouble their investment with a focus on their software and testing stack, they have a chance to be competitive with Nvidia on training. We think the engineers at AMD are extremely capable and are doing their best to advance the AMD ecosystem – and indeed support from these engineers in the form of bug fixes, configuration help and custom images improved the results we were able to get from the MI300X.&nbsp;</p>



<p>To bring our benchmarking process to a coda, on November 15<sup>th</sup>, 2024 we sent Nvidia and AMD a draft of most of our major GEMM and single node benchmarking code and results for comments, verification, and fine-tuning. We asked that any final comments, fixes, feedback and any performance improvements be submitted by November 25<sup>th</sup>. We set this time frame to crystallize test results to allow time to write an in-depth analysis and commentary and carry out multiple rounds of internal and external reviews, all steps that can take a variable and often unknowable amount of time, typically from 2-4 weeks.&nbsp;</p>



<p>A few days ago, after we informed both that we had confirmed an article publication date of December 20<sup>th</sup>, AMD requested that we delay publication to include results based on a beta WIP development build on an AMD developer’s branch. All of our benchmarking on Nvidia was conducted on publicly available stable release builds. In the spirit of transparency and fairness, we include these results as well as updated testing harness results on as the original November 25<sup>th</sup>&nbsp;deadline image and the latest publicly available software. However, we believe that the correct way to interpret the results is to look at the performance of the public stable release of AMD/Nvidia software.&nbsp;</p>



<p><strong>Below are the list of software builds that we have used for benchmarking:</strong></p>



<ul>
<li>H100 Public Stable Release – Out of Box experience for Nvidia H100.</li>



<li>H200 Public Stable Release – Out of Box experience for Nvidia H200.</li>



<li>MI300X Nov 25<sup>th</sup>&nbsp;Custom Build – This is a custom VIP docker image hand-crafted that builds all dependencies from source code written by AMD principal engineers.</li>



<li>MI300X Stable Public Release PyTorch 2.5.1 – Out of Box experience for AMD MI300X.</li>



<li>MI300X Public Nightly Dec 19<sup>th&nbsp;</sup>– This can indicate where AMD performance can be by January 2025, when PyTorch 2.6 is released, over 1 year after launch.</li>



<li>MI300X Dec 21<sup>st</sup>&nbsp;WIP dev build – This is the image that AMD submitted to us after we agreed to delay publication of the article. It is an experimental development build that has not yet been merged into AMD’s internal main branch, and it does not use the native PyTorch flash attention API. Performance with this image can indicate where AMD public stable release performance will be in 1-2 quarters from now.</li>
</ul>



<p>We are very thankful for the technical support provided by AMD and Nvidia throughout this process, but we maintain our independence in the results we publish. We want to shout out to and thank our AMD counterparties, Anush Elangovan (AMD VP of AI), Hui Liu and many dozens of amazing AMD Principal/Senior engineers, AMD VPs of Engineering, AMD Engineering Fellows, AMD CVPs of Engineering and AMD Directors of Engineering, AMD Software Library Leads, for triaging and fixing our various bug reports. On the Nvidia side, we are grateful to Kedar Potdar, Ian Buck, Sylvain Jeaugey and the NCCL team from NVIDIA for their amazing support.&nbsp;</p>



<p>Thank you to&nbsp;<a href="https://crusoe.ai/cloud">Crusoe</a>,&nbsp;<a href="https://tensorwave.com/">TensorWave</a>&nbsp;(<em>AMD Ventures Portco</em>),&nbsp;<a href="https://nebius.com/">Nebius</a>,&nbsp;<a href="https://lambdalabs.com/">Lambda</a>,&nbsp;<a href="https://hotaisle.xyz/">Hot Aisle</a>&nbsp;<a>and&nbsp;</a><a href="https://smc.co/">Sustainable Metal Cloud (SMC)</a>&nbsp;/&nbsp;<a href="https://firmus.co/">Firmus</a>&nbsp;for the compute and for being supporters of open-source benchmarking. Crusoe, Nebius, SMC / Firmus and Lambda support managed SLURM and shared home directories out of the box. TensorWave currently has managed SLURM in beta and this feature will come to general availability (GA) at the start of next year. Sustainable Metal Cloud is one of the few neoclouds that has&nbsp;<a href="https://mlcommons.org/benchmarks/training/">official MLPerf GPT-3 175B Training results</a>.</p>



<p><strong>We will be releasing a follow up article on inferencing for the H100, H200 and MI300X. We may also release a follow-up article in a few months to follow up on AMD training performance to see if out of box experience has improved and test other models such as LlaVa &amp; Mamba.</strong></p>



<figure><img data-recalc-dims="1" decoding="async" width="2354" height="1244" data-attachment-id="150418456" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/12-thank-you-for-compute-firmus-2/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/12-thank-you-for-compute-FIRMUS-1.png?fit=2354%2C1244&amp;ssl=1" data-orig-size="2354,1244" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="12 thank you for compute FIRMUS" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/12-thank-you-for-compute-FIRMUS-1.png?fit=300%2C159&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/12-thank-you-for-compute-FIRMUS-1.png?fit=1024%2C541&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/12-thank-you-for-compute-FIRMUS-1.png?resize=2354%2C1244&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/12-thank-you-for-compute-FIRMUS-1.png?w=2354&amp;ssl=1 2354w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/12-thank-you-for-compute-FIRMUS-1.png?resize=300%2C159&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/12-thank-you-for-compute-FIRMUS-1.png?resize=1024%2C541&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/12-thank-you-for-compute-FIRMUS-1.png?resize=768%2C406&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/12-thank-you-for-compute-FIRMUS-1.png?resize=1536%2C812&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/12-thank-you-for-compute-FIRMUS-1.png?resize=2048%2C1082&amp;ssl=1 2048w" sizes="(max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<h2 id="key-findings">Key Findings</h2>



<ol>
<li>Comparing on paper FLOP/s and HBM Bandwidth/Capacity is akin to comparing cameras by merely examining megapixel count. The only way to tell the actual performance is to run benchmarking.</li>



<li>Nvidia’s Out of the Box Performance &amp; Experience is amazing, and we did not run into any Nvidia specific bugs during our benchmarks. Nvidia tasked a single engineer to us for technical support, but we didn’t run into any Nvidia software bugs as such we didn’t need much support.</li>



<li>AMD’s Out of the Box Experience is very difficult to work with and can require considerable patience and elbow grease to move towards a usable state.&nbsp;<strong>On most of our benchmarks, Public AMD stable releases of AMD PyTorch is still broken and we needed workarounds</strong>.</li>



<li>If we weren’t supported by multiple teams of AMD engineers triaging and fixing bugs in AMD software that we ran into, AMD’s results would have been much lower than Nvidia’s.</li>



<li>We ran unofficial MLPerf Training GPT-3 175B on 256 H100 in collaboration with Sustainable Metal Cloud to test the effects of different VBoost setting</li>



<li>For AMD, Real World Performance on public stable released software is nowhere close to its on paper marketed TFLOP/s. Nvidia’s real world performance also undershoots its marketing TFLOP/s, but not by nearly as much.</li>



<li>The MI300X has a lower total cost of ownership (TCO) compared to the H100/H200, but training performance per TCO is higher on the MI300X on public stable releases of AMD software. This changes if one uses custom development builds of AMD software.&nbsp;</li>



<li>Training performance is weaker, as demonstrated by the MI300X ‘s matrix multiplication micro-benchmarks, and AMD public release software on single-node training throughput still lags that of Nvidia’s H100 and H200.</li>



<li><strong>MI300X performance is held back by AMD software</strong>.&nbsp;<strong>AMD MI300X software on&nbsp;BF16&nbsp;development branches have better performance</strong>&nbsp;but has not yet merged into the main branch of AMD’s internal repos. By the time it gets merged into the main branch and into the PyTorch stable release, Nvidia Blackwell will have already been available to everyone.</li>



<li>AMD’s training performance is also held back as the MI300X does not deliver strong scale out performance. This is due to its weaker ROCm Compute Communication Library (RCCL) and AMD’s lower degree of vertical integration with networking and switching hardware compared to Nvidia’s strong integration of its Nvidia Collective Communications Library (NCCL), InfiniBand/Spectrum-X network fabric and switches.</li>



<li>Many of AMD AI Libraries are forks of NVIDIA AI Libraries, leading to suboptimal outcomes and compatibility issues.</li>



<li>AMD customers tend to use hand crafted kernels only for inference, which means their performance outside of very narrow well defined use cases is poor, and their flexibility to rapidly shifting workloads is non-existent.</li>
</ol>



<h2 id="executive-recommendation-to-amd">Executive Recommendation to AMD</h2>



<p><strong>We&nbsp;genuinely&nbsp;want&nbsp;to see&nbsp;another effective competitor to Nvidia and want to help AMD get to that spot</strong>, but, unfortunately, there is still much work to be done on that front. At the bottom of this article, we have a detailed list of feedback for the Lisa Su and the AMD Leadership Team, but provide a summary here:</p>



<ol>
<li>Give AMD Engineers more compute and engineering resources to fix and improve the AMD ecosystem, they have very few internal gpu boxes relative to what Nvidia provides to their engineers. Tensorwave, the largest AMD GPU Cloud has given GPU time for free to a team at AMD to fix software issues, which is insane given they paid for the GPUs.</li>



<li>AMD needs to hook up thousands more of MI300X, MI325X to PyTorch CI/CD for automated testing to ensure there is no AMD performance regressions &amp; functional AMD bugs. Nvidia has given thousands of GPUs for PyTorch CI/CD to ensure an amazing out of box experience</li>



<li>The AMD Executive Team should personally and intensively internally test (i.e., “dogfood”) products that are being shipped to the public rather than focus on testing internal builds. Preferably dogfood during livestream (twitch.tv) to show the authentic out of box experience. This is like how geohotz livestreams</li>



<li>AMD should collaborate with Meta to get production LLM training workloads working as soon as possible on PyTorch ROCm, AMD’s answer to CUDA, as commonly, PyTorch code paths that Meta isn’t using have numerous bugs.</li>



<li>Move away from over-reliance on properly setting numerous environment flags (up to dozens) to make an AMD deployment usable. Instead, bake these settings into the default configuration. Make the out of the box experience usable!</li>



<li>Focus on making out of box experience good instead of over-reliance on custom VIP images that build all dependencies from source code main@specificcommit and take 5 hours to build.</li>



<li>Stop expecting end users to use PYTORCH_TUNABLE_OPS which is a prototype buggy feature and is not respectful of the end users time as it takes ~1 hour for the end user to tune every time an end user wants to make any changes to their code.&nbsp;</li>



<li>AMD should submit MLPerf Training GPT-3 175B results. MLPerf is an apples-to-apples benchmarking methodology that uses time to convergence as the north star.</li>



<li>We want AMD to be competitive and are open to meet with more detailed feedback on how to fix the AMD Datacenter GPU Ecosystem for the better.</li>
</ol>



<h2 id="a-summary-of-the-amd-vs-nvidia-narrative">A Summary of the AMD vs Nvidia Narrative</h2>



<p>Before we dive into various facets of AMD’s software stack that hold AMD back, we will discuss the MI300X’s basic specifications, its comparative total cost of ownership, and how most analysts and investors have evaluated its competitiveness.</p>



<p>The MI300X launched in late 2023 with an exciting set of on paper specifications—featuring 1,307 TFLOP/s of FP16 compute (stronger than the H100’s 989 TFLOP/s), 5.3 TB/s of memory bandwidth, and 192GB of HBM3, 3.35 TB/s of memory bandwidth, and 80GB of HBM3. These specs outstrip those of the H200, which itself is, effectively, a memory-spec bumped version of the H100, delivering 4.8TB/s of memory bandwidth and 141GB of HBM3e.&nbsp;</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="2184" height="1088" data-attachment-id="150418457" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/screenshot-4/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/20-H100-vs-H200-vs-MI300X-Basic-Specs-second-appearance-1.jpg?fit=2184%2C1088&amp;ssl=1" data-orig-size="2184,1088" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;Screenshot&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;Screenshot&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="Screenshot" data-image-description="" data-image-caption="<p>Screenshot</p>
" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/20-H100-vs-H200-vs-MI300X-Basic-Specs-second-appearance-1.jpg?fit=300%2C149&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/20-H100-vs-H200-vs-MI300X-Basic-Specs-second-appearance-1.jpg?fit=1024%2C510&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/20-H100-vs-H200-vs-MI300X-Basic-Specs-second-appearance-1.jpg?resize=2184%2C1088&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/20-H100-vs-H200-vs-MI300X-Basic-Specs-second-appearance-1.jpg?w=2184&amp;ssl=1 2184w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/20-H100-vs-H200-vs-MI300X-Basic-Specs-second-appearance-1.jpg?resize=300%2C149&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/20-H100-vs-H200-vs-MI300X-Basic-Specs-second-appearance-1.jpg?resize=1024%2C510&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/20-H100-vs-H200-vs-MI300X-Basic-Specs-second-appearance-1.jpg?resize=768%2C383&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/20-H100-vs-H200-vs-MI300X-Basic-Specs-second-appearance-1.jpg?resize=1536%2C765&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/20-H100-vs-H200-vs-MI300X-Basic-Specs-second-appearance-1.jpg?resize=2048%2C1020&amp;ssl=1 2048w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis, Nvidia, AMD</figcaption></figure>



<p>On paper total cost of ownership for an MI300X deployment is extremely compelling, not only due to the lower ASP of the MI300X, but also because it is typically deployed using cheaper Ethernet networking. Comparing a cluster of 16k H200s vs a 16k MI300X ethernet cluster leads to nearly 40% of the cost savings coming from networking alone, with the remainder of the savings from a lower accelerator cost. The use of Whitebox Ethernet switches is a substantial cost savings compared to using Nvidia’s Quantum-2 switches, but the real difference is cheaper transceivers, as Nvidia branded transceivers cost as much as 2-3x over what a typical transceiver OEM charges.</p>



<p>At face value, the MI300X seems the best of both worlds: higher performance and lower total cost of ownership. At the time of its launch, it was logical to expect share gains to the underdog AMD from this compelling combination. The table below shows total upfront cluster capex – we present a more detailed breakdown of cluster capex components as well as a detailed networking BoM analysis in the sections at near the bottom of the article.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1819" height="619" data-attachment-id="150418458" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/30-h100-vs-h200-vs-mi300x-capex-2/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/30-H100-vs-H200-vs-MI300X-Capex-1.jpg?fit=1819%2C619&amp;ssl=1" data-orig-size="1819,619" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="30 H100 vs H200 vs MI300X Capex" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/30-H100-vs-H200-vs-MI300X-Capex-1.jpg?fit=300%2C102&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/30-H100-vs-H200-vs-MI300X-Capex-1.jpg?fit=1024%2C348&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/30-H100-vs-H200-vs-MI300X-Capex-1.jpg?resize=1819%2C619&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/30-H100-vs-H200-vs-MI300X-Capex-1.jpg?w=1819&amp;ssl=1 1819w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/30-H100-vs-H200-vs-MI300X-Capex-1.jpg?resize=300%2C102&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/30-H100-vs-H200-vs-MI300X-Capex-1.jpg?resize=1024%2C348&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/30-H100-vs-H200-vs-MI300X-Capex-1.jpg?resize=768%2C261&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/30-H100-vs-H200-vs-MI300X-Capex-1.jpg?resize=1536%2C523&amp;ssl=1 1536w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source:&nbsp;<a href="https://www.semianalysis.com/p/ai-cloud-tco-model">SemiAnalysis AI TCO Model</a></figcaption></figure>



<p>As orders solidified, excitement built up for potential of the MI300X, helped along by bullish commentary and guidance from AMD. With a compelling spec advantage, it was easy to argue for further upside to AMD’s guidance, which most investors assumed management was sandbagging. AMD had a strong hand, in theory. After all they have mid-single digit market share in datacenter GPUs for 2024 and, logically, a glide path towards even 10-12% market share by 2027 could be conservative while offering considerable earnings upside for AMD.</p>



<p><br>However, over from late 2023 and through most of 2024, guidance for full year 2024 datacenter GPU sales repeatedly underperformed those lofty expectations. From its 1Q24 earnings through its 3Q24 earnings, AMD only raised guidance from $4B to $5B, well under the $6-8B investor bogey based on&nbsp;<a href="https://www.semianalysis.com/p/accelerator-model">CoWoS and HBM supply agreements</a>. Our demand view in the&nbsp;<a href="https://semianalysis.com/accelerator-industry-model/">Accelerator Model</a>&nbsp;tracked&nbsp;<a href="https://semianalysis.com/accelerator-industry-model/">Microsoft’s disappointment early in the year and lack of follow on orders</a>.</p>



<p>The earlier bullish line of reasoning was like purchasing a certain car model from a magazine without a test drive or soliciting feedback from owners of that model or reading any reviews. But fear not – SemiAnalysis has put the MI300X, H100, and H200 through their paces at scale and can show why AMD’s current software stack issues decisively disprove this line of reasoning.&nbsp;</p>



<h2 id="general-matrix-multiply-gemm-performance">General Matrix Multiply (GEMM) Performance</h2>



<p>Most FLOPS in a transformer-based architecture (i.e. ChatGPT, Llama, etc.) go towards matrix multiplication, also known as GEMMs. For this reason,&nbsp;<strong>GEMM performance is a good proxy for how well frontier transformers, such as ChatGPT, Llama, Claude, Grok, etc. will train on the hardware</strong>.&nbsp;</p>



<p>GEMMs take two input matrices, Matrix A and Matrix B, with Matrix A having the shape of (M, K), M rows and K columns, and Matrix B having the shape of (K,N) to produce an output matrix of shape (M,N).&nbsp;</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1499" height="1404" data-attachment-id="150418459" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/60-matmul-k-n-overviewgimp-2/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/60-matmul-K-N-overviewGIMP-1.png?fit=1499%2C1404&amp;ssl=1" data-orig-size="1499,1404" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="60 matmul K N overviewGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/60-matmul-K-N-overviewGIMP-1.png?fit=300%2C281&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/60-matmul-K-N-overviewGIMP-1.png?fit=1024%2C959&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/60-matmul-K-N-overviewGIMP-1.png?resize=1499%2C1404&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/60-matmul-K-N-overviewGIMP-1.png?w=1499&amp;ssl=1 1499w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/60-matmul-K-N-overviewGIMP-1.png?resize=300%2C281&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/60-matmul-K-N-overviewGIMP-1.png?resize=1024%2C959&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/60-matmul-K-N-overviewGIMP-1.png?resize=768%2C719&amp;ssl=1 768w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: Nvidia</figcaption></figure>



<p>Conceptually, each element of the resulting matrix is a sum of element-wise multiplications along the “K” dimension of the inputs. For this matter, the K dimension is also known as the reduction dimension.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1875" height="684" data-attachment-id="150418460" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/61-matmul-overview-2/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/61-matmul-overview-1.png?fit=1875%2C684&amp;ssl=1" data-orig-size="1875,684" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="61 matmul overview" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/61-matmul-overview-1.png?fit=300%2C109&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/61-matmul-overview-1.png?fit=1024%2C374&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/61-matmul-overview-1.png?resize=1875%2C684&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/61-matmul-overview-1.png?w=1875&amp;ssl=1 1875w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/61-matmul-overview-1.png?resize=300%2C109&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/61-matmul-overview-1.png?resize=1024%2C374&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/61-matmul-overview-1.png?resize=768%2C280&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/61-matmul-overview-1.png?resize=1536%2C560&amp;ssl=1 1536w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>Below, we have tested the following real-world shapes, given in the form (M,N,K)—which is short for multiplying a matrix of dimensions (M,K) and (K,N) together.&nbsp;</p>



<p>These following matrix shapes were actually used in&nbsp;<a href="https://github.com/pytorch-labs/float8_experimental/blob/fe6e08c867abf56b1acd0f34473c69cde624f0a3/benchmarks/bench_matmul.py#L57">Meta’s Llama 70B</a>&nbsp;production training:</p>



<ul>
<li>(16384, 8192, 1280) – Fused QKV Projection GEMM shape</li>



<li>(16384, 1024, 8192) – Attention Output Projection shape</li>



<li>(16384, 8192, 7168) – FFN GEMM shape</li>



<li>(16384, 3584, 8192) – FFN GEMM shape</li>



<li>(8192, 8192, 8192) – Standard GEMM shape for benchmarking</li>
</ul>



<p>We used OpenAI’s do_bench function for the benchmark setup, an industry standard method of benchmarking PyTorch. The do_bench function provides cache clearing between runs as a default and provides ways to warmup and execute the benchmark multiple times, taking the median result as the given accuracy.&nbsp;&nbsp;We used warmup=30 and rep=200 for these tests. Both input tensor A and B were randomly initialized with a normal distribution with mean 0 and variance 1. This is because a normal distribution comes the closest to matching the actual distribution of weights and activations in modern neural networks. The distribution of the input tensors will affect the results of the TFLOP/s performance benchmark. We will discuss the reasons why the input distribution effects TFLOP/s performance later in the article.</p>



<p>For BF16, we can see that the H100 and H200 achieves roughly 720 TFLOP/s against their marketed 989.5 TFLOP/s, while the MI300X reaches a mere ~620 TFLOP/s compared with their marketed 1,307 TFLOP/s.&nbsp;</p>



<p>This means that, despite a much higher marketed BF16 TFLOP/s, the MI300X is 14% slower than the H100 and H200. This AMD result used a custom docker image that was hand crafted by an AMD principal engineer yet still achieved slower performance than Nvidia’s GPUs. For our out of the box testing of the MI300X, the TFLOP/s throughput even slower than this! In addition to a custom image, AMD also requires the user to set numerous environment flags that aren’t set by default to reach these performance results.&nbsp;</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1489" height="1084" data-attachment-id="150418461" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/71-bf16-gemm-perf-for-real-world-shapes-w-amd-images/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/71-bf16-gemm-perf-for-real-world-shapes-w-amd-images.png?fit=1489%2C1084&amp;ssl=1" data-orig-size="1489,1084" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="71 bf16 gemm perf for real world shapes w amd images" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/71-bf16-gemm-perf-for-real-world-shapes-w-amd-images.png?fit=300%2C218&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/71-bf16-gemm-perf-for-real-world-shapes-w-amd-images.png?fit=1024%2C745&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/71-bf16-gemm-perf-for-real-world-shapes-w-amd-images.png?resize=1489%2C1084&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/71-bf16-gemm-perf-for-real-world-shapes-w-amd-images.png?w=1489&amp;ssl=1 1489w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/71-bf16-gemm-perf-for-real-world-shapes-w-amd-images.png?resize=300%2C218&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/71-bf16-gemm-perf-for-real-world-shapes-w-amd-images.png?resize=1024%2C745&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/71-bf16-gemm-perf-for-real-world-shapes-w-amd-images.png?resize=768%2C559&amp;ssl=1 768w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>Unfortunately, the story is worse for FP8. The H100/H200 achieves ~1,280 TFLOP/s out of the marketed 1979 TFLOP/s. The MI300X, in comparison, only reaches ~990 TFLOP/s. Thus, for FP8, the MI300X is 22% slower than H100. This is for both inputs being of the e4m3 FP8 (<a href="https://semianalysis.com/2024/01/11/neural-network-quantization-and-number/">i.e. 4 exponent bits and 3 mantissa bits</a>) datatype.&nbsp;</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1514" height="1152" data-attachment-id="150418462" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/81-fp8-gemm-tflops-real-world-shapes-3/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/81-fp8-gemm-tflops-real-world-shapes-2.png?fit=1514%2C1152&amp;ssl=1" data-orig-size="1514,1152" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="81 fp8 gemm tflops real world shapes" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/81-fp8-gemm-tflops-real-world-shapes-2.png?fit=300%2C228&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/81-fp8-gemm-tflops-real-world-shapes-2.png?fit=1024%2C779&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/81-fp8-gemm-tflops-real-world-shapes-2.png?resize=1514%2C1152&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/81-fp8-gemm-tflops-real-world-shapes-2.png?w=1514&amp;ssl=1 1514w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/81-fp8-gemm-tflops-real-world-shapes-2.png?resize=300%2C228&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/81-fp8-gemm-tflops-real-world-shapes-2.png?resize=1024%2C779&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/81-fp8-gemm-tflops-real-world-shapes-2.png?resize=768%2C584&amp;ssl=1 768w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>It is important to note that calling GEMM is a simple task, and we shouldn’t expect to run into AMD software bugs. Unfortunately, a&nbsp;<strong>major bug</strong>&nbsp;that we encountered is that the torch.matmul and F.Linear APIs have been delivering different performances on AMD for a couple of months during the summer. One would expect the torch.matmul and F.Linear APIs to have the same performance, but, surprisingly, F.Linear is much slower!</p>



<p>This is a strange bug as torch.matmul and F.Linear are both wrappers around the hardware vendor GEMM libraries, so they should achieve the same level of performance. F.Linear, in particular, is important, as this is the way most end users in PyTorch launch the GEMM kernels.&nbsp;</p>



<p>When we started testing AMD five months ago, the public AMD PyTorch still had this bug. The root cause was that AMD in fact has two different underlying GEMM libraries, rocBLAS and hipBLASLt, with HipBLASLt being more optimized for the MI300X. The bug was that torch.matmul uses the optimized hipBLASLt, but AMD had not changed F.Linear by default, leaving it to use the unoptimized rocBLAS library.&nbsp;</p>



<p>This major bug was ultimately fixed by AMD a few months ago after our bug reports, and we hope it doesn’t reappear due to a lack of proper regression testing. AMD’s usability could improve considerably if it boosted its testing efforts instead of waiting for users to discover these critical issues.</p>



<p>We have open sourced the GEMM benchmark used in our tests into a simple three liner that anyone can easily run:</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="821" height="571" data-attachment-id="150418463" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/90-amd-vs-nvidia-matmulgimp-v2/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/90-amd-vs-nvidia-matmulGIMP-v2.png?fit=821%2C571&amp;ssl=1" data-orig-size="821,571" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="90 amd vs nvidia matmulGIMP v2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/90-amd-vs-nvidia-matmulGIMP-v2.png?fit=300%2C209&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/90-amd-vs-nvidia-matmulGIMP-v2.png?fit=821%2C571&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/90-amd-vs-nvidia-matmulGIMP-v2.png?resize=821%2C571&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/90-amd-vs-nvidia-matmulGIMP-v2.png?w=821&amp;ssl=1 821w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/90-amd-vs-nvidia-matmulGIMP-v2.png?resize=300%2C209&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/90-amd-vs-nvidia-matmulGIMP-v2.png?resize=768%2C534&amp;ssl=1 768w" sizes="auto, (max-width: 821px) 100vw, 821px"><figcaption>Source:&nbsp;<a href="https://www.ray.so/#theme=prisma&amp;darkMode=false&amp;code=IyBBTUQgVklQIGltYWdlCmFsaWFzIGRydW49InN1ZG8gZG9ja2VyIHJ1biAtLXByaXZpbGVnZWQgLS1uZXR3b3JrPWhvc3QgLS1kZXZpY2U9L2Rldi9rZmQgLS1kZXZpY2U9L2Rldi9kcmkgLS1ncm91cC1hZGQgdmlkZW8gLS1jYXAtYWRkPVNZU19QVFJBQ0UgLS1zZWN1cml0eS1vcHQgc2VjY29tcD11bmNvbmZpbmVkIC0taXBjPWhvc3QgLS1zaG0tc2l6ZT0xOTI2IC0tcm0gLWl0IgpkcnVuIHNlbWlhbmFseXNpc3dvcmsvYW1kLW1hdG11bDpsYXRlc3QKRElTQUJMRV9BREROX0hJUF9MVD0wIFBZVE9SQ0hfVFVOQUJMRV9PUF9FTkFCTEVEPTEgcHl0aG9uIG1hdG11bC5weQoKI0FNRCBweXBpIG5pZ2h0bHkKZHJ1biBhbWQtbGF0ZXN0LXB5cGktbmlnaHRseS1tYXRtdWwKUFlUT1JDSF9UVU5BQkxFX09QX0VOQUJMRUQ9MSBweXRob24gbWF0bXVsLnB5CgojIEFNRCBweXBpIHN0YWJsZSBQeVRvcmNoIDIuNS4xCmRydW4gc2VtaWFuYWx5c2lzd29yay9hbWQtbGF0ZXN0LXB5cGktc3RhYmxlLW1hdG11bApQWVRPUkNIX1RVTkFCTEVfT1BfRU5BQkxFRD0xIHB5dGhvbiBtYXRtdWwucHkKCiMgTnZpZGlhIHN0YWJsZSAyNC4wOQphbGlhcyBkcnVuPSJkb2NrZXIgcnVuIC0tcm0gLWl0IC0tZ3B1cyBhbGwgLS1pcGM9aG9zdCAtLW5ldD1ob3N0IC0tc2htLXNpemU9MTkyNiIKZHJ1biBzZW1pYW5hbHlzaXN3b3JrL252aWRpYS1tYXRtdWw6bGF0ZXN0CnB5dGhvbiBtYXRtdWwucHkKCg&amp;language=shell">SemiAnalysis</a></figcaption></figure>



<h2 id="popular-gemm-benchmark-isnt-accurate">Popular GEMM Benchmark Isn’t Accurate</h2>



<p>Recently, a benchmark has been floating around the internet that claims that, on GEMMs, AMD MI300X’s performance is close to that of the H100. </p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="548" data-attachment-id="150418464" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/100-mamf-comparison-tablegimp-2/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/100-mamf-comparison-tableGIMP-1.png?fit=2343%2C1254&amp;ssl=1" data-orig-size="2343,1254" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="100 mamf comparison tableGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/100-mamf-comparison-tableGIMP-1.png?fit=300%2C161&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/100-mamf-comparison-tableGIMP-1.png?fit=1024%2C548&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/100-mamf-comparison-tableGIMP-1.png?resize=1024%2C548&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/100-mamf-comparison-tableGIMP-1.png?resize=1024%2C548&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/100-mamf-comparison-tableGIMP-1.png?resize=300%2C161&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/100-mamf-comparison-tableGIMP-1.png?resize=768%2C411&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/100-mamf-comparison-tableGIMP-1.png?resize=1536%2C822&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/100-mamf-comparison-tableGIMP-1.png?resize=2048%2C1096&amp;ssl=1 2048w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: Github</figcaption></figure>



<p>There are two main issues with the benchmark: it isn’t properly carrying out L2 Cache clearing and also is simply taking the max performance, instead of the median/mean TFLOP/s over the course of the iterations for a specific shape. Without L2 Cache clearing between iterations, the benchmark does not accurately reflect real-world GEMM performance. Furthermore, since the TFLOP/s change based on which iteration it is on, you need to use a mean/median over at least 100 iterations as the basis for an accurate GEMM benchmark. OpenAI’s do_bench provides L2 cache and mean/median out of the box by default, so we recommend that engineers use it for micro-benchmarking. Below, we have simplified the benchmark into pseudocode and have commented on the issues mentioned above.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1470" height="880" data-attachment-id="150418550" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/110-replacement-for-bm-test/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/110-replacement-for-bm-test.png?fit=1470%2C880&amp;ssl=1" data-orig-size="1470,880" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="110 replacement for bm test" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/110-replacement-for-bm-test.png?fit=300%2C180&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/110-replacement-for-bm-test.png?fit=1024%2C613&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/110-replacement-for-bm-test.png?resize=1470%2C880&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/110-replacement-for-bm-test.png?w=1470&amp;ssl=1 1470w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/110-replacement-for-bm-test.png?resize=300%2C180&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/110-replacement-for-bm-test.png?resize=1024%2C613&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/110-replacement-for-bm-test.png?resize=768%2C460&amp;ssl=1 768w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<h2 id="hbm-memory-bandwidth-performance">HBM Memory Bandwidth Performance</h2>



<p>It is widely known that AMD MI300X has better memory bandwidth than the Nvidia H100 and H200, offering 5.3 TB/s of bandwidth vs 4.8 TB/s for the H200 and 3.35 TB/s for the H100. Improved HBM memory bandwidth is very useful in inferencing and is sometimes useful in training. In training, users can set a larger batch size if they have more HBM memory capacity and memory bandwidth. Although if a larger global batch size is used, after a certain size, the model will take longer to convergence. It is easy to run fast with big global batch size but at a high level, it will hurt time to convergence.</p>



<p>From our HBM memory bandwidth benchmarking, we see that that MI300X indeed has way better memory bandwidth than both the H200 and the H100. We tested memory bandwidth in Pytorch with Tensor.copy_ &amp; used the industry standard OpenAI do_bench to ensure accuracy.</p>



<p><strong>As you will see in our upcoming H100 vs H200 vs MI300X inference article, memory bandwidth is very important for inferencing.</strong></p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1485" height="1047" data-attachment-id="150418466" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/115-hbm-copy-bandwidth-chart/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/115-HBM-Copy-Bandwidth-Chart.png?fit=1485%2C1047&amp;ssl=1" data-orig-size="1485,1047" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="115 HBM Copy Bandwidth Chart" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/115-HBM-Copy-Bandwidth-Chart.png?fit=300%2C212&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/115-HBM-Copy-Bandwidth-Chart.png?fit=1024%2C722&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/115-HBM-Copy-Bandwidth-Chart.png?resize=1485%2C1047&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/115-HBM-Copy-Bandwidth-Chart.png?w=1485&amp;ssl=1 1485w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/115-HBM-Copy-Bandwidth-Chart.png?resize=300%2C212&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/115-HBM-Copy-Bandwidth-Chart.png?resize=1024%2C722&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/115-HBM-Copy-Bandwidth-Chart.png?resize=768%2C541&amp;ssl=1 768w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="829" height="665" data-attachment-id="150418467" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/117-code-for-bw-test/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/117-code-for-BW-test.png?fit=829%2C665&amp;ssl=1" data-orig-size="829,665" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="117 code for BW test" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/117-code-for-BW-test.png?fit=300%2C241&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/117-code-for-BW-test.png?fit=829%2C665&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/117-code-for-BW-test.png?resize=829%2C665&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/117-code-for-BW-test.png?w=829&amp;ssl=1 829w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/117-code-for-BW-test.png?resize=300%2C241&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/117-code-for-BW-test.png?resize=768%2C616&amp;ssl=1 768w" sizes="auto, (max-width: 829px) 100vw, 829px"><figcaption>Source:&nbsp;<a href="https://ray.so/#code=aW1wb3J0IHRvcmNoCmltcG9ydCB0cml0b24KCiMgVGVuc29yIHNpemUgKDE2R0IpCnRlbnNvcl9zaXplID0gMTYgKiAxMDI0KiozICAjIDE2IEdCIGluIGJ5dGVzCmR0eXBlID0gdG9yY2guZmxvYXQzMiAgIyBBc3N1bWluZyBmbG9hdDMyICg0IGJ5dGVzIHBlciBlbGVtZW50KQpudW1fZWxlbWVudHMgPSB0ZW5zb3Jfc2l6ZSAvLyA0ICAjIENhbGN1bGF0ZSBudW1iZXIgb2YgZWxlbWVudHMKCiMgQWxsb2NhdGUgdGVuc29ycyBvbiB0aGUgZGV2aWNlCmEgPSB0b3JjaC5yYW5kbihudW1fZWxlbWVudHMsIGRldmljZT0iY3VkYSIsIGR0eXBlPWR0eXBlKQpiID0gdG9yY2guZW1wdHlfbGlrZShhKQoKIyBGdW5jdGlvbiB0byBiZW5jaG1hcmsKZGVmIGNvcHlfdGVuc29yKCk6CiAgICBiLmNvcHlfKGEpCgojIEJlbmNobWFyayB1c2luZyBUcml0b24KdGltZV9tcyA9IHRyaXRvbi50ZXN0aW5nLmRvX2JlbmNoKGNvcHlfdGVuc29yKQoKIyBDYWxjdWxhdGUgYmFuZHdpZHRoCmJhbmR3aWR0aF9nYnBzID0gKHRlbnNvcl9zaXplICogMikgLyAodGltZV9tcyAqIDFlLTMpIC8gMWU5ICAjIE11bHRpcGx5IGJ5IDIgZm9yIHJlYWQgKyB3cml0ZQoKIyBQcmludCByZXN1bHRzCnByaW50KGYiQ29weSBCYW5kd2lkdGg6IHtiYW5kd2lkdGhfZ2JwczouMmZ9IEdCL3MgKHRpbWU6IHt0aW1lX21zOi4yZn0gbXMpIikK&amp;darkMode=false">SemiAnalysis</a></figcaption></figure>



<h2 id="amd-hand-crafted-vip-custom-builds-and-wip-development-builds">AMD Hand-Crafted VIP Custom Builds and WIP Development Builds</h2>



<p>The only reason we have been able to get AMD performance within 75% of H100/H200 performance is because we have been supported by multiple teams at AMD in fixing numerous AMD software bugs. To get AMD to a usable state with somewhat reasonable performance, a giant ~60 command Dockerfile that builds dependencies from source, hand crafted by an AMD principal engineer, was specifically provided for us, since the Pytorch Nightly and public PyTorch AMD images functioned poorly and had version differences. This docker image requires ~5 hours to build from source and installs dependencies and sub-dependencies (hipBLASLt, Triton, PyTorch, TransformerEngine), a huge difference compared to Nvidia, which offers a pre-built, out of the box experience and takes but a single line of code.&nbsp;<strong>Most users do not build Pytorch, hipBLASLt from source code but instead use the stable release.</strong></p>



<p>When using public PyTorch, users have the choice of working with the latest stable images or a nightly PyTorch upload.&nbsp;<strong>So, although a nightly PyTorch upload may have the latest commits that could potentially lead to better performance or could fix some bugs, but users must accept that the upload may not be fully tested and could contain new bugs</strong>&nbsp;from Meta/AMD/Nvidia or other PyTorch contributors that have not been discovered yet.&nbsp;<strong>Note that most end users are using the stable release of PyTorch.</strong></p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="3680" height="7420" data-attachment-id="150418469" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/160-amd-pytorchgimp2/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/160-amd-pytorchGIMP2.png?fit=3680%2C7420&amp;ssl=1" data-orig-size="3680,7420" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="160 amd pytorchGIMP2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/160-amd-pytorchGIMP2.png?fit=149%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/160-amd-pytorchGIMP2.png?fit=508%2C1024&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/160-amd-pytorchGIMP2.png?resize=3680%2C7420&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/160-amd-pytorchGIMP2.png?w=3680&amp;ssl=1 3680w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/160-amd-pytorchGIMP2.png?resize=149%2C300&amp;ssl=1 149w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/160-amd-pytorchGIMP2.png?resize=508%2C1024&amp;ssl=1 508w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/160-amd-pytorchGIMP2.png?resize=768%2C1549&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/160-amd-pytorchGIMP2.png?resize=762%2C1536&amp;ssl=1 762w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/160-amd-pytorchGIMP2.png?resize=1016%2C2048&amp;ssl=1 1016w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/160-amd-pytorchGIMP2.png?w=2000&amp;ssl=1 2000w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/160-amd-pytorchGIMP2.png?w=3000&amp;ssl=1 3000w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis, AMD</figcaption></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="453" data-attachment-id="150418470" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/170-nvidia-pytorchgimp/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/170-nvidia-pytorchGIMP.png?fit=2124%2C940&amp;ssl=1" data-orig-size="2124,940" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="170 nvidia pytorchGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/170-nvidia-pytorchGIMP.png?fit=300%2C133&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/170-nvidia-pytorchGIMP.png?fit=1024%2C453&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/170-nvidia-pytorchGIMP.png?resize=1024%2C453&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/170-nvidia-pytorchGIMP.png?resize=1024%2C453&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/170-nvidia-pytorchGIMP.png?resize=300%2C133&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/170-nvidia-pytorchGIMP.png?resize=768%2C340&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/170-nvidia-pytorchGIMP.png?resize=1536%2C680&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/170-nvidia-pytorchGIMP.png?resize=2048%2C906&amp;ssl=1 2048w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: Nvidia</figcaption></figure>



<p>Delightfully, Nvidia’s Docker images contain the complete set of developer tools needed for profiling and debugging, like Nsight Compute and Nsight Systems. AMD, in contrast, does not include their OmniTrace developer tool out of the box.&nbsp;</p>



<p>Until a couple weeks ago, the AMD docker images only supported PyTorch 2.3, which released 8 months ago. Mainline PyTorch 2.4 and PyTorch 2.5 have also since released and PyTorch 2.6 is about to come out in Q1 2025. We recommended to an AMD Principal Engineer and to AMD’s VP of AI that AMD should have the latest AMD PyTorch version – AMD has since started publishing containers for some of these AMD PyTorch versions. Docker image for AMD PyTorch 2.5 is still missing.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="2057" height="1098" data-attachment-id="150418471" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/180-contents-of-the-pytorch-containergimp/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/180-contents-of-the-pytorch-containerGIMP.png?fit=2057%2C1098&amp;ssl=1" data-orig-size="2057,1098" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="180 contents of the pytorch containerGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/180-contents-of-the-pytorch-containerGIMP.png?fit=300%2C160&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/180-contents-of-the-pytorch-containerGIMP.png?fit=1024%2C547&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/180-contents-of-the-pytorch-containerGIMP.png?resize=2057%2C1098&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/180-contents-of-the-pytorch-containerGIMP.png?w=2057&amp;ssl=1 2057w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/180-contents-of-the-pytorch-containerGIMP.png?resize=300%2C160&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/180-contents-of-the-pytorch-containerGIMP.png?resize=1024%2C547&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/180-contents-of-the-pytorch-containerGIMP.png?resize=768%2C410&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/180-contents-of-the-pytorch-containerGIMP.png?resize=1536%2C820&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/180-contents-of-the-pytorch-containerGIMP.png?resize=2048%2C1093&amp;ssl=1 2048w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: Nvidia</figcaption></figure>



<h2 id="dec-21st-amd-development-builds">Dec 21st AMD Development Builds</h2>



<p>Below is AMD’s December 21st development build docker image. As you can see, it uses a number of non stable devlopment branches for dependencies such as hipBLASLt, AOTriton, ROCm Attention and installs everything including PyTorch from source code, taking upwards of 5 hours to build. These versions of the dependencies haven’t even been merged into AMD’s own main branch yet.&nbsp;&nbsp;<strong>99.9% of users will not be installing PyTorch from source code and all of its dependencies from source code on development branches but will instead use the public stable PyPi PyTorch.</strong></p>



<p>Furthermore, instead of using Flash Attention through the PyTorch native user friendly&nbsp;<a href="https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html">torch. scaled_dot_product_attention</a>&nbsp;API, this AMD Development build imports another library (development branch as well) attention implementation. We have seen more users use Flash Attention through PyTorch native&nbsp;<a href="https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html">torch. scaled_dot_product_attention</a>&nbsp;API since it is more user friendly and bundled into out of box PyTorch.&nbsp;<a href="https://rocm.blogs.amd.com/artificial-intelligence/flash-attention/README.html#benchmarking-attention">Even AMD’s own public documentation recommends using Flash Attention through torch.scaled_dot_product_attention API</a>. We hope that these kernels get merged into PyTorch flash attention instead of making the end user install a separate library taking hours of their time to build. This is not a user-friendly experience. Furthermore, AMD must support FlexAttention as it has quickly become the go to in the industry.&nbsp;</p>



<p>AMD’s December 21<sup>st</sup>&nbsp;Dev build is on a hanging development branch. That means it is a branch that has not been fully QA’ed and is at use only at a risk branch. There are many concerns about the validity of the results from using a development build and branches and building from source code, as most users are not doing this in real life. Most users will be installing AMD/Nvidia PyTorch from PyPI stable release mostly so we recommend readers keep this in mend when analyzing these results.</p>



<p>That being said, we are including these development build results as it is an indication of where AMD public stable release software will be 1-2 quarters from now. However, at the same time, when it comes to compete, 1-2 quarters from now, Nvidia Blackwell will already be widely deployed, while AMD MI355X will not commence shipments until H2 2025.&nbsp;</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="3680" height="5640" data-attachment-id="150418472" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/190-amd-dec-21th-development-build/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/190-AMD-Dec-21th-Development-Build.png?fit=3680%2C5640&amp;ssl=1" data-orig-size="3680,5640" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="190 AMD Dec 21th Development Build" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/190-AMD-Dec-21th-Development-Build.png?fit=196%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/190-AMD-Dec-21th-Development-Build.png?fit=668%2C1024&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/190-AMD-Dec-21th-Development-Build.png?resize=3680%2C5640&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/190-AMD-Dec-21th-Development-Build.png?w=3680&amp;ssl=1 3680w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/190-AMD-Dec-21th-Development-Build.png?resize=196%2C300&amp;ssl=1 196w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/190-AMD-Dec-21th-Development-Build.png?resize=668%2C1024&amp;ssl=1 668w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/190-AMD-Dec-21th-Development-Build.png?resize=768%2C1177&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/190-AMD-Dec-21th-Development-Build.png?resize=1002%2C1536&amp;ssl=1 1002w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/190-AMD-Dec-21th-Development-Build.png?resize=1336%2C2048&amp;ssl=1 1336w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/190-AMD-Dec-21th-Development-Build.png?w=2000&amp;ssl=1 2000w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/190-AMD-Dec-21th-Development-Build.png?w=3000&amp;ssl=1 3000w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis, AMD</figcaption></figure>



<h2 id="training-testing-methodology-gpt1-5b-llama-8b-llama-70b-mistral">Training Testing Methodology (GPT1.5B, Llama 8B, Llama 70B, Mistral)</h2>



<p>There are many ways to test training performance. The most accurate way is to take a medium-sized AI startup model’s internal codebases and run them on a 512-1024 GPU cluster. This way, the test run has all the optimizations that a typical user would have. Everything else is just a proxy for the performance of these training runs. Training performance takes into account HBM bandwidth, HBM capacity, TFLOP/s, networking, and system architecture.&nbsp;<strong>Comparing on paper HBM bandwidth/capacity is just like comparing on paper camera megapixels.</strong></p>



<p>MLPerf GPT3 175B Training is also a good proxy to measure the time it takes to train to a specific convergence. MLPerf benchmark considers global batch sizes and whether a mixed precision implementation incurs a convergence penalty. Unfortunately, MLPerf is quite difficult to run due to a lack of user-friendly documentation and instructions, and the performance is often min-maxed via a custom tuned configuration specifically concocted for MLPerf that an average user would not adopt. Note that Nvidia has submitted MLPerf Training results with over 11k H100s, while AMD runs MLPerf Training internally. AMD’s results are likely weak, so they have never submitted any MLPerf Training, let alone the MLPerf GPT3 175B benchmark.</p>



<p>When designing our SemiAnalysis benchmark, we wanted to reflect the average user’s model implementation, and so opted for&nbsp;<a href="https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html">torch. scaled_dot_product_attention</a>&nbsp;API (which uses flash attention backend), PyTorch Distributed Data Parallel (DDP) and/or Fully Sharded Data Parallel (FSDP) with&nbsp;<a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">torch.compile</a>.&nbsp;<a href="https://rocm.blogs.amd.com/artificial-intelligence/flash-attention/README.html#benchmarking-attention">Also note that AMD recommends users use torch.scaled_dot_product_attention in their own documentation</a>. We believe this is the most representative of a typical user workload. Further, we used a generic PyTorch native implementation of these models to keep it close to a typical ML Scientist user and make it easy to run with a single line of code. In contrast to MLPerf, the goal of our benchmark is to be as simple to run as possible, while still being a good proxy for performance. Note, since we don’t take into account time to convergence, this benchmark has a slight bias towards AMD as we set the micro batch size higher on AMD vs on Nvidia. When taking time to convergence into account, AMD results will be worse than what is stated.</p>



<p>As an aside, many AI practitioners have said they are not using Megatron or NeMo or 3D Parallelism due to the high level of complexity and lack of flexibility associated with those libraries, whose rigidity and complexity make their usage for ML Research effectively impossible. Note that in terms of 3D Parallelism, both Nvidia and AMD will get higher performance, assuming their software stack works, which is a big assumption for AMD. AMD Megatron is a fork of Nvidia Megatron and has less than 10 stars which means that it is probably not dogfooded well. Submitting bug reports would take&nbsp;<strong>extra months</strong>&nbsp;to get AMD Megatron working for simple models.&nbsp;</p>



<p>For our SemiAnalysis model training benchmark, we will test four models, with the first being a simple GPT 1.5B DDP, as we believe this is representative of what small-scale experiments/ablations would look like before scale-out to bigger model sizes. DDP is a much simpler and less network-intensive form of parallelism. Next, we tested the standard Llama3 8B and Llama3 70B 4 Layer Proxy as a baseline for a popular model’s performance. Third, we tested Mistral 7B v0.1, which evaluates if hardware will perform well when adding a bit of complexity, as Mistral uses sliding window attention instead of the standard causal attention. Modern models such as ChatGPT, Claude, Genimi, o1, o3 do not use standard causal attention &amp; use a complex attention mechanism.</p>



<p>A Modern GPT/Llama/Transformer model is built by stacking the same transformer layer over &amp; over again. As such, measuring the performance of just 4 layers is a great proxy for the overall performance of the model.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="502" height="1141" data-attachment-id="150418473" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/195-modern-gpt-transformer-next-token-probabilities/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/195-Modern-GPT-transformer-Next-Token-Probabilities.png?fit=502%2C1141&amp;ssl=1" data-orig-size="502,1141" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="195 Modern GPT transformer Next Token Probabilities" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/195-Modern-GPT-transformer-Next-Token-Probabilities.png?fit=132%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/195-Modern-GPT-transformer-Next-Token-Probabilities.png?fit=451%2C1024&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/195-Modern-GPT-transformer-Next-Token-Probabilities.png?resize=502%2C1141&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/195-Modern-GPT-transformer-Next-Token-Probabilities.png?w=502&amp;ssl=1 502w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/195-Modern-GPT-transformer-Next-Token-Probabilities.png?resize=132%2C300&amp;ssl=1 132w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/195-Modern-GPT-transformer-Next-Token-Probabilities.png?resize=451%2C1024&amp;ssl=1 451w" sizes="auto, (max-width: 502px) 100vw, 502px"><figcaption>Source: Imgur</figcaption></figure>



<p>Furthermore, in modern LLM training for all frontier LLM models, pipeline parallelism is used which means that a couple of transformer layers are placed in each GPU server. Never in modern pretraining is a whole model placed on a single node.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="2150" height="735" data-attachment-id="150418474" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/196-node1-node2-node3-node-m/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/196-Node1-Node2-Node3-Node-M.png?fit=2150%2C735&amp;ssl=1" data-orig-size="2150,735" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="196 Node1 Node2 Node3 Node M" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/196-Node1-Node2-Node3-Node-M.png?fit=300%2C103&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/196-Node1-Node2-Node3-Node-M.png?fit=1024%2C350&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/196-Node1-Node2-Node3-Node-M.png?resize=2150%2C735&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/196-Node1-Node2-Node3-Node-M.png?w=2150&amp;ssl=1 2150w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/196-Node1-Node2-Node3-Node-M.png?resize=300%2C103&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/196-Node1-Node2-Node3-Node-M.png?resize=1024%2C350&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/196-Node1-Node2-Node3-Node-M.png?resize=768%2C263&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/196-Node1-Node2-Node3-Node-M.png?resize=1536%2C525&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/196-Node1-Node2-Node3-Node-M.png?resize=2048%2C700&amp;ssl=1 2048w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>The model FLOP for each token trained is defined by the following formula:</p>



<p>6 * non_input_embedding_params + 12 * num_layers * num_heads * head_dim * max_seq_len * density</p>



<p>With density being how sparse the attention is relative to a full mask. Causal attention has, for example, a 50% sparsity, while sliding window attention has even lower sparsity.</p>



<p>Note that originally our testing harness used 6 * params instead of 6 * non_input_embedding_params which is the wrong way of calculating model FLOP per token. Furthermore, there was another bug in regard to the way we used FSDP.&nbsp;<strong>We have since updated our testing harness and retroactively retested as well as updated all of benchmark results across all versions of software for both H100, H200, MI300X, public stable, public nightly, VIP images and AMD development builds. All results listed below are with the updated testing harness.</strong></p>



<h2 id="single-node-training-performance">Single Node Training Performance</h2>



<p>Note that the H100/H200 performance we present in this report reflects an out of the box performance without any hand-crafted tuning from Nvidia engineers, while the results for the MI300X comes after many months of tuning and bug fixes from AMD’s engineers. We did not run into any Nvidia-specific bugs compared to AMD training, which was comparatively bug-filled. Five months ago, many models couldn’t run at more than 150 TFLOP/s on the AMD MI300X due to an AMD software bug in attention backwards and torch compile, which forced the user to manually mark a region of the model as non-compliable instead of having a full graph compile.</p>



<p>We see that, for all models, the H100/H200 wins relative to MI300X public releases/public nightly releases/Nov 25<sup>th</sup>build from source VIP image. It is interesting that the MI300X does not perform well on smaller models such as GPT 1.5B or on any model that uses a non-causal attention layer, like Mistral 7B v0.1. This is due to FlexAttention not being fully operational at the time of the deadline, while, on Nvidia GPUs, it has been working since August 2024. As such, the H100/H200 beats MI300X by more than 2.5x in terms of TFLOP/s for MI300X public release/public nightly release/Nov25th VIP build.</p>



<p>For the Dec 21<sup>st</sup>&nbsp;MI300X internal WIP development branches build, we still see it perform worse than H100/H200 on GPT 1.5B. Furthermore, it performs slightly worse than H100 on Mistral 7B. For Llama3 8B and Llama3 70B Proxy, the Dec 21<sup>st</sup>&nbsp;MI300X WIP development build performs better than H100/H200, but note that this is due to MI300X WIP development using an AMD engineer’s development branch that has not even been merged to the AMD main branch.&nbsp;</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1491" height="1180" data-attachment-id="150418475" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/121-bf16-single-node-8gpu-training-perf-with-new-amd-images/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/121-bf16-single-node-8gpu-training-perf-with-new-AMD-images.png?fit=1491%2C1180&amp;ssl=1" data-orig-size="1491,1180" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="121 bf16 single node 8gpu training perf – with new AMD images" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/121-bf16-single-node-8gpu-training-perf-with-new-AMD-images.png?fit=300%2C237&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/121-bf16-single-node-8gpu-training-perf-with-new-AMD-images.png?fit=1024%2C810&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/121-bf16-single-node-8gpu-training-perf-with-new-AMD-images.png?resize=1491%2C1180&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/121-bf16-single-node-8gpu-training-perf-with-new-AMD-images.png?w=1491&amp;ssl=1 1491w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/121-bf16-single-node-8gpu-training-perf-with-new-AMD-images.png?resize=300%2C237&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/121-bf16-single-node-8gpu-training-perf-with-new-AMD-images.png?resize=1024%2C810&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/121-bf16-single-node-8gpu-training-perf-with-new-AMD-images.png?resize=768%2C608&amp;ssl=1 768w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>Three months ago, attempting to do FP8 Training on AMD led to segfaults and hard errors. On the off chance it did work, it was, in fact, slower than the same run using BF16. We worked with AMD’s FP8 team to fix this issue, as well as the AMD hipBLASLt team, which created&nbsp;<a href="https://github.com/ROCm/hipBLASLt/pull/1378">tuning</a>&nbsp;for fixing MI300X FP8 performance. FP8 Training is important as it speeds up training compared to BF16 &amp; most frontier labs use FP8 Training.</p>



<p>After many fixes, we can see that the MI300X’s Nov 25th throughput for Llama3 8B and GPT 1.5B is somewhat competitive with H100’s. As usual, H200 wins in this category. However, for Llama3 70B 4 Layer Proxy, AMD Nov 25th’s results are sorely beaten.</p>



<p>For Mistral 7B which has a non-causal attention layer, AMD Nov 25th performance is close to half that of an H100. This shows that, for anything that isn’t a simple model, even after months of tuning, AMD is still not competitive due to a slight tweak in the model structure. Many frontier models and AI training startups are using complex attention layers for long context spans and efficient attention, but, AMD is still far behind on those.</p>



<p>Unfortunately, FP8 training on AMD only works on custom images such as our November 25<sup>th</sup>&nbsp;VIP image and December 21<sup>st</sup>&nbsp;WIP development branch image. When we first started trying AMD FP8 Training, it was slower than AMD BF16 Training on public releases.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1491" height="1181" data-attachment-id="150418476" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/131-fp8-single-node-8gpu-training-perf-with-new-amd-image-v2/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/131-fp8-single-node-8gpu-training-perf-with-new-amd-Image-v2.png?fit=1491%2C1181&amp;ssl=1" data-orig-size="1491,1181" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="131 fp8 single node 8gpu training perf – with new amd Image v2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/131-fp8-single-node-8gpu-training-perf-with-new-amd-Image-v2.png?fit=300%2C238&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/131-fp8-single-node-8gpu-training-perf-with-new-amd-Image-v2.png?fit=1024%2C811&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/131-fp8-single-node-8gpu-training-perf-with-new-amd-Image-v2.png?resize=1491%2C1181&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/131-fp8-single-node-8gpu-training-perf-with-new-amd-Image-v2.png?w=1491&amp;ssl=1 1491w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/131-fp8-single-node-8gpu-training-perf-with-new-amd-Image-v2.png?resize=300%2C238&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/131-fp8-single-node-8gpu-training-perf-with-new-amd-Image-v2.png?resize=1024%2C811&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/131-fp8-single-node-8gpu-training-perf-with-new-amd-Image-v2.png?resize=768%2C608&amp;ssl=1 768w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>For AMD’s WIP development builds, we see that on Llama3 8B, it wins against H100 but is still slower than H200’s public stable software release. H200 performance completely beats MI300X even on their Dec 21<sup>st</sup>&nbsp;WIP development branches.</p>



<p>It is interesting that the MI300X does not perform well on non-causal attention layer, like Mistral 7B v0.1 even for their internal builds. Mistral using sliding window attention which some of the frontier models uses. It seems that if you want to train a model that doesn’t use causal attention, AMD MI300X will automatically lose.</p>



<p>While a lot of people putting out performance comparisons between hardware, most do not open source their testing code and they do not make easily reproducible. We took an open source approach, and we have open-sourced our single node training benchmark and made it easy to run with only a couple of lines:</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="836" height="941" data-attachment-id="150418477" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/140-run-single-node-nv-vs-amdgimp-v2/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/140-run-single-node-nv-vs-amdGIMP-v2.png?fit=836%2C941&amp;ssl=1" data-orig-size="836,941" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="140 run single node nv vs amdGIMP v2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/140-run-single-node-nv-vs-amdGIMP-v2.png?fit=267%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/140-run-single-node-nv-vs-amdGIMP-v2.png?fit=836%2C941&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/140-run-single-node-nv-vs-amdGIMP-v2.png?resize=836%2C941&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/140-run-single-node-nv-vs-amdGIMP-v2.png?w=836&amp;ssl=1 836w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/140-run-single-node-nv-vs-amdGIMP-v2.png?resize=267%2C300&amp;ssl=1 267w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/140-run-single-node-nv-vs-amdGIMP-v2.png?resize=768%2C864&amp;ssl=1 768w" sizes="auto, (max-width: 836px) 100vw, 836px"><figcaption>Source:&nbsp;<a href="https://ray.so/#darkMode=false&amp;code=IyBBbGwgb2YgdGhlIEluc3RydWN0aW9ucyBiZWxvdyBoYXZlIHRoZSB1cGRhdGVkIHRlc3QgaGFybmVzcwojIFJ1biBTaW5nbGUgTm9kZQoKIyBOdmlkaWEKYWxpYXMgZHJ1bj0iZG9ja2VyIHJ1biAtLXJtIC1pdCAtLWdwdXMgYWxsIC0taXBjPWhvc3QgLS1uZXQ9aG9zdCAtLXNobS1zaXp">SemiAnalysis</a></figcaption></figure>



<h2 id="multi-node-training-performance">Multi-Node Training Performance</h2>



<p>For multi-node, we benchmarked two nodes of H100 and two nodes of MI300X. Unfortunately, we didn’t get access to a multi-node H200 deployment in time for the article.</p>



<p>H100 wins again by a big margin in this benchmark compared to MI300X, with the H100 ranging from 10-25% faster. This gap widens as you add more nodes working together into a single training workload. This is a known problem, which AMD is attempting to fix next year by deploying their new in house 400G AI focused NIC.</p>



<h2 id="amd-pytorch_tunable_ops-flag-is-a-bad-user-experience">AMD PYTORCH_TUNABLE_OPS FLAG is a Bad User Experience</h2>



<p>In order to get AMD training working decently, users need to use PYTORCH_TUNABLE_OPS which is an AMD specific prototype flag for the end user to tune GEMMs. Since this is a prototype feature (i.e. not stable), in the past a lot of bugs with this feature cropped up including but not limited to&nbsp;<a href="https://github.com/pytorch/pytorch/issues/139116">seg faults</a>, HBM memory leaks, and a&nbsp;<a href="https://github.com/pytorch/pytorch/pull/139137">whole</a>&nbsp;<a href="https://github.com/pytorch/pytorch/pull/143507">host</a>&nbsp;of&nbsp;<a href="https://github.com/pytorch/pytorch/pull/140673">other</a>issues such as many&nbsp;<a href="https://www.torch-ci.com/failure?failureCaptures=%5B%22test_linalg.py%3A%3ATestLinalgCUDA%3A%3Atest_matmul_small_brute_force_tunableop_cuda_float16%22%5D">unit tests being disabled</a>. These known tunable ops bugs have been fixed now but there are likely a many more unknown AMD software bugs.&nbsp;</p>



<p>Furthermore, even if users do not encounter any bugs and thus the runway is clear for this prototype AMD flag to work, it still takes users anywhere from 1-2 hours to tune any modern LLM model. Although these GEMMs can be cached by the end user, any minor changes to the end user’s code results in the need for the user to spend another 1-2 hours tuning. As you can imagine, this will slow down an ML Scientist’s iteration cycle speed when trying to conduct model R&amp;D and ablations experiments.&nbsp;</p>



<p>On Nvidia, this flag isn’t needed as their GEMM library (cuBLASLt) comes tuned out of the box and cuBLASLt’s heuristic model out of the box picks the correct algorithm for most shapes on H100/H200. In contrast, AMD hipBLASLt/rocBLAS’s heuristic model picks the wrong algorithm for most shapes out of the box, which is why so much time-consuming tuning is required by the end user.</p>



<p>We recommend that AMD to fix their GEMM libraries’ heuristic model such that it picks the correct algorithm out of the box instead of wasting the end user’s time doing tuning on their end. Users often iterate quickly when doing research and therefore rerunning tunable ops will slow down research velocity significantly.<br></p>



<h2 id="scale-up-nvlinkxgmitopology">Scale Up NVLink/xGMI&nbsp;&nbsp;Topology</h2>



<p>Scale up fabric is extremely important for GPU Clusters, as it provides an extremely fast path for tensor and expert parallelism used in frontier model training. For this reason, we have conducted benchmarks to measure scale up fabric performance.</p>



<p>The scale up fabric on H100 and H200 is called NVLink and provides 450GByte/s of bandwidth per GPU and connects 8 GPUs together. On the MI300X, the scale up fabric is called xGMI and, on paper, it connects 8 GPUs, providing 448GByte/s of bandwidth per GPU. On the surface, MI300X’s scale up network is extremely similar and close in performance to that of the H100/H200, providing just 0.5% less on paper bandwidth. Unfortunately, the reality of the situation differs sharply.</p>



<p>First, MI300X’s xGMI is a point-to-point fabric, which means that it isn’t&nbsp;<em>actually</em>&nbsp;providing 448GByte/s of bandwidth between GPUs pairs. Instead, each GPU can only talk to one another at 64GByte/s. A GPU can only reach the stated 448GByte/s if one GPU addresses all 7 other GPUs simultaneously. That means that, for Tensor Parallelism TP=2, the maximum bandwidth is 64GByte/s and 189GByte/s for TP=4.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1455" height="1147" data-attachment-id="150418478" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/201-amd-mi300x-xgmi-topology/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/201-amd-mi300x-xgmi-topology.png?fit=1455%2C1147&amp;ssl=1" data-orig-size="1455,1147" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="201 amd mi300x xgmi topology" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/201-amd-mi300x-xgmi-topology.png?fit=300%2C236&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/201-amd-mi300x-xgmi-topology.png?fit=1024%2C807&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/201-amd-mi300x-xgmi-topology.png?resize=1455%2C1147&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/201-amd-mi300x-xgmi-topology.png?w=1455&amp;ssl=1 1455w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/201-amd-mi300x-xgmi-topology.png?resize=300%2C236&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/201-amd-mi300x-xgmi-topology.png?resize=1024%2C807&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/201-amd-mi300x-xgmi-topology.png?resize=768%2C605&amp;ssl=1 768w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>In contrast, since Nvidia’s NVLink uses a switched topography, one GPU can talk to another GPU at the full 450GByte/s. Furthermore, the four NVSwitches in H100/H200 support in-network reduction (referred to as NVLink SHARP (NVLS), enabled by default), a technique to reduce data movements by carrying out collectives/reductions inside the switch itself.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="2172" height="743" data-attachment-id="150418479" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/202-h100_200-nvlink-topology/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/202-h100_200-nvlink-topology.png?fit=2172%2C743&amp;ssl=1" data-orig-size="2172,743" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="202 h100_200 nvlink topology" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/202-h100_200-nvlink-topology.png?fit=300%2C103&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/202-h100_200-nvlink-topology.png?fit=1024%2C350&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/202-h100_200-nvlink-topology.png?resize=2172%2C743&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/202-h100_200-nvlink-topology.png?w=2172&amp;ssl=1 2172w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/202-h100_200-nvlink-topology.png?resize=300%2C103&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/202-h100_200-nvlink-topology.png?resize=1024%2C350&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/202-h100_200-nvlink-topology.png?resize=768%2C263&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/202-h100_200-nvlink-topology.png?resize=1536%2C525&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/202-h100_200-nvlink-topology.png?resize=2048%2C701&amp;ssl=1 2048w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<h2 id="all-reduceall-to-allreduce-scatterall-gather-collectives-overview">All Reduce/All to All/Reduce Scatter/All Gather Collectives Overview</h2>



<p>We will showcase benchmarks across scale-up and scale-out networks for both the Nvidia H100/H200 and AMD’s MI300. The collectives that we will be testing are the main set of collectives used in frontier LLM training: all_reduce, all_gather, reduce_scatter, and all to all. All reduce is for data parallelism and tensor parallelism, all gather is used for ZeRO/FSDP parallelism (as well as for tensor parallelism), and Reduce Scatter is used for ZeRO/FSDP parallelism.&nbsp;</p>



<p>Due to the way that compute-communication overlapping works, real-world message sizes range from 16MiB to 256MiB, with the default PyTorch DDP size being 25MiB (NVIDIA’s MLPerf 11,000 H100 GPT-3 175B run used a&nbsp;<a href="https://github.com/mlcommons/training_results_v4.1/blob/b87b9e396f771345d4ef122ba33456304f15228d/NVIDIA/benchmarks/gpt3/implementations/eos-dfw_n1452_ngc24.04_nemo/config_common.sh#L69">message size of max 200MiB</a>). We also test 8GiB and 16GiB just to see what the peak bus bandwidth is, though these message sizes are not used in the real world. All these collectives discussed above are used during 3D Parallelism and FSDP/ZeRO Parallelism, which are common techniques for training frontier models.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="2259" height="1357" data-attachment-id="150418480" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/210-3d-parallelismgimp/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/210-3d-parallelismGIMP.png?fit=2259%2C1357&amp;ssl=1" data-orig-size="2259,1357" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="210 3d-parallelismGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/210-3d-parallelismGIMP.png?fit=300%2C180&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/210-3d-parallelismGIMP.png?fit=1024%2C615&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/210-3d-parallelismGIMP.png?resize=2259%2C1357&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/210-3d-parallelismGIMP.png?w=2259&amp;ssl=1 2259w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/210-3d-parallelismGIMP.png?resize=300%2C180&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/210-3d-parallelismGIMP.png?resize=1024%2C615&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/210-3d-parallelismGIMP.png?resize=768%2C461&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/210-3d-parallelismGIMP.png?resize=1536%2C923&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/210-3d-parallelismGIMP.png?resize=2048%2C1230&amp;ssl=1 2048w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: DeepSpeed</figcaption></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="2206" height="1165" data-attachment-id="150418481" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/220-fully-sharded-data-parallel-traininggimp/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/220-fully-sharded-data-parallel-trainingGIMP.png?fit=2206%2C1165&amp;ssl=1" data-orig-size="2206,1165" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="220 fully sharded data parallel trainingGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/220-fully-sharded-data-parallel-trainingGIMP.png?fit=300%2C158&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/220-fully-sharded-data-parallel-trainingGIMP.png?fit=1024%2C541&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/220-fully-sharded-data-parallel-trainingGIMP.png?resize=2206%2C1165&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/220-fully-sharded-data-parallel-trainingGIMP.png?w=2206&amp;ssl=1 2206w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/220-fully-sharded-data-parallel-trainingGIMP.png?resize=300%2C158&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/220-fully-sharded-data-parallel-trainingGIMP.png?resize=1024%2C541&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/220-fully-sharded-data-parallel-trainingGIMP.png?resize=768%2C406&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/220-fully-sharded-data-parallel-trainingGIMP.png?resize=1536%2C811&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/220-fully-sharded-data-parallel-trainingGIMP.png?resize=2048%2C1082&amp;ssl=1 2048w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: Meta</figcaption></figure>



<h2 id="single-node-nccl-collective">Single Node NCCL Collective</h2>



<p>We see that Nvidia does much better than AMD across all the real-world messages for every single collective. This is not surprising due to the H100/H200’s superior 450GByte/s NVLink switched topology with in-network reduction (NVLS), compared to MI300X’s 7x64GByte/s xGMI point-to-point topology.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1725" height="1216" data-attachment-id="150418482" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/240-single-node-8-gpu-allreduce-busbw/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/240-single-node-8-gpu-allreduce-BusBW.png?fit=1725%2C1216&amp;ssl=1" data-orig-size="1725,1216" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="240 single node 8 gpu allreduce BusBW" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/240-single-node-8-gpu-allreduce-BusBW.png?fit=300%2C211&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/240-single-node-8-gpu-allreduce-BusBW.png?fit=1024%2C722&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/240-single-node-8-gpu-allreduce-BusBW.png?resize=1725%2C1216&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/240-single-node-8-gpu-allreduce-BusBW.png?w=1725&amp;ssl=1 1725w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/240-single-node-8-gpu-allreduce-BusBW.png?resize=300%2C211&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/240-single-node-8-gpu-allreduce-BusBW.png?resize=1024%2C722&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/240-single-node-8-gpu-allreduce-BusBW.png?resize=768%2C541&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/240-single-node-8-gpu-allreduce-BusBW.png?resize=1536%2C1083&amp;ssl=1 1536w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1592" height="1147" data-attachment-id="150418483" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/250-single-node-8-gpu-all-to-all/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/250-single-node-8-gpu-all-to-all.png?fit=1592%2C1147&amp;ssl=1" data-orig-size="1592,1147" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="250 single node 8 gpu all to all" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/250-single-node-8-gpu-all-to-all.png?fit=300%2C216&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/250-single-node-8-gpu-all-to-all.png?fit=1024%2C738&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/250-single-node-8-gpu-all-to-all.png?resize=1592%2C1147&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/250-single-node-8-gpu-all-to-all.png?w=1592&amp;ssl=1 1592w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/250-single-node-8-gpu-all-to-all.png?resize=300%2C216&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/250-single-node-8-gpu-all-to-all.png?resize=1024%2C738&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/250-single-node-8-gpu-all-to-all.png?resize=768%2C553&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/250-single-node-8-gpu-all-to-all.png?resize=1536%2C1107&amp;ssl=1 1536w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1593" height="1134" data-attachment-id="150418484" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/260-single-node-8gpu-all-gather/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/260-single-node-8gpu-all-gather.png?fit=1593%2C1134&amp;ssl=1" data-orig-size="1593,1134" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="260 single node 8gpu all gather" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/260-single-node-8gpu-all-gather.png?fit=300%2C214&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/260-single-node-8gpu-all-gather.png?fit=1024%2C729&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/260-single-node-8gpu-all-gather.png?resize=1593%2C1134&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/260-single-node-8gpu-all-gather.png?w=1593&amp;ssl=1 1593w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/260-single-node-8gpu-all-gather.png?resize=300%2C214&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/260-single-node-8gpu-all-gather.png?resize=1024%2C729&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/260-single-node-8gpu-all-gather.png?resize=768%2C547&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/260-single-node-8gpu-all-gather.png?resize=1536%2C1093&amp;ssl=1 1536w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1594" height="1168" data-attachment-id="150418485" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/270-single-node-8gpu-reduce-scatter/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/270-single-node-8gpu-reduce-scatter.png?fit=1594%2C1168&amp;ssl=1" data-orig-size="1594,1168" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="270 single node 8gpu reduce scatter" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/270-single-node-8gpu-reduce-scatter.png?fit=300%2C220&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/270-single-node-8gpu-reduce-scatter.png?fit=1024%2C750&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/270-single-node-8gpu-reduce-scatter.png?resize=1594%2C1168&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/270-single-node-8gpu-reduce-scatter.png?w=1594&amp;ssl=1 1594w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/270-single-node-8gpu-reduce-scatter.png?resize=300%2C220&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/270-single-node-8gpu-reduce-scatter.png?resize=1024%2C750&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/270-single-node-8gpu-reduce-scatter.png?resize=768%2C563&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/270-single-node-8gpu-reduce-scatter.png?resize=1536%2C1126&amp;ssl=1 1536w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>To reproduce this test, you can use our open source ClusterMax-NCCL/RCCL benchmark, which we developed to be easily run with one line of Bash. ClusterMax is our upcoming evaluation quantitative performance and qualitative user experience for ranking H100/B200/GB200/MI300X Neocloud clusters. Look forward to our upcoming&nbsp;<em>“ClusterMax Neocloud Evaluation | How to Rent GPUs”</em>&nbsp;article.&nbsp;</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="3496" height="1568" data-attachment-id="150418486" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/280-nv-vs-amd-docker-rungimp/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/280-nv-vs-amd-docker-runGIMP.png?fit=3496%2C1568&amp;ssl=1" data-orig-size="3496,1568" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="280 nv vs amd docker runGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/280-nv-vs-amd-docker-runGIMP.png?fit=300%2C135&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/280-nv-vs-amd-docker-runGIMP.png?fit=1024%2C459&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/280-nv-vs-amd-docker-runGIMP.png?resize=3496%2C1568&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/280-nv-vs-amd-docker-runGIMP.png?w=3496&amp;ssl=1 3496w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/280-nv-vs-amd-docker-runGIMP.png?resize=300%2C135&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/280-nv-vs-amd-docker-runGIMP.png?resize=1024%2C459&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/280-nv-vs-amd-docker-runGIMP.png?resize=768%2C344&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/280-nv-vs-amd-docker-runGIMP.png?resize=1536%2C689&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/280-nv-vs-amd-docker-runGIMP.png?resize=2048%2C919&amp;ssl=1 2048w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/280-nv-vs-amd-docker-runGIMP.png?w=3000&amp;ssl=1 3000w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<h2 id="multi-node-rcclnccl-collectives-and-scale-out-network-benchmarks">Multi Node RCCL/NCCL Collectives and Scale Out Network Benchmarks</h2>



<p>On both Nvidia’s H100/H200 and the MI300X, each GPU is connected to other nodes over the scale out network using a 400G Network Interface Card (NIC), connected directly every GPU. The H100/H200 reference design typically uses ConnectX-7 NICs for InfiniBand NDR or BlueField-3 for Spectrum-X Ethernet. Spectrum-X is NVIDIA’s custom Ethernet solution purpose-built for AI workloads. On the MI300X, the reference design recommends using RoCEv2 Ethernet with Broadcom Thor-2 NIC.&nbsp;</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1100" height="624" data-attachment-id="150418487" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/290-trees-on-dgx-h100gimp/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/290-trees-on-dgx-h100GIMP.png?fit=1100%2C624&amp;ssl=1" data-orig-size="1100,624" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="290 trees on dgx h100GIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/290-trees-on-dgx-h100GIMP.png?fit=300%2C170&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/290-trees-on-dgx-h100GIMP.png?fit=1024%2C581&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/290-trees-on-dgx-h100GIMP.png?resize=1100%2C624&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/290-trees-on-dgx-h100GIMP.png?w=1100&amp;ssl=1 1100w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/290-trees-on-dgx-h100GIMP.png?resize=300%2C170&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/290-trees-on-dgx-h100GIMP.png?resize=1024%2C581&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/290-trees-on-dgx-h100GIMP.png?resize=768%2C436&amp;ssl=1 768w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: Nvidia</figcaption></figure>



<p>A typical GPU cluster almost always requires more layers than a single tier network, as a single-tier network can only support 128 GPUs (in the case of Broadcom Ethernet or Nvidia Spectrum X Ethernet) and 64 GPUs (for H100/H200 InfiniBand). In such a multi-tier network, deployments typically use an 8-rail optimized fat tree, where each one of the 8 GPU is connected to a separate switch (such a connection is called a “rail”).&nbsp;<a href="https://semianalysis.com/2024/10/03/ai-neocloud-playbook-and-anatomy/#cluster-level-networking-bill-of-materials">In our AI Neocloud&nbsp;Playbook and Anatomy article, we explained in detail how a rail optimized network works</a>.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1614" height="781" data-attachment-id="150418488" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/300-1024-h100-compute-fabric-topology/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/300-1024-H100-compute-fabric-topology.png?fit=1614%2C781&amp;ssl=1" data-orig-size="1614,781" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="300 1024 H100 compute fabric topology" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/300-1024-H100-compute-fabric-topology.png?fit=300%2C145&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/300-1024-H100-compute-fabric-topology.png?fit=1024%2C496&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/300-1024-H100-compute-fabric-topology.png?resize=1614%2C781&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/300-1024-H100-compute-fabric-topology.png?w=1614&amp;ssl=1 1614w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/300-1024-H100-compute-fabric-topology.png?resize=300%2C145&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/300-1024-H100-compute-fabric-topology.png?resize=1024%2C496&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/300-1024-H100-compute-fabric-topology.png?resize=768%2C372&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/300-1024-H100-compute-fabric-topology.png?resize=1536%2C743&amp;ssl=1 1536w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>Just as Nvidia’s NVLink offers NVLS for its scale-up network, Nvidia’s H100/H200 InfiniBand scale out network also offers InfiniBand SHARP In-network Reduction which is, again, exclusive to Nvidia. AMD does not have an analogous product for the MI300X. InfiniBand SHARP works similarly to NVLink SHARP In-network Reduction as they both provide a way to reduce the amount of traffic going through the network, with the reductions carried out inside of Quantum-2 InfiniBand switches in the case of InfiniBand SHARP.&nbsp;</p>



<p>Unfortunately, unlike NVLink SHARP, which is enabled by default, InfiniBand SHARP is not enabled by default in the UFM/IB subnet manager. We have spoken to many Neoclouds, H100 cluster operators, and AI frontier labs, and most have said that they have not enabled SHARP due to increased NCCL_TIMEOUT rates and difficulties installing and configuring the network. We asked NVIDIA which AI customers use InfiniBand SHARP, but they declined to answer in specifics. One could speculate that if InfiniBand SHARP was useful in AI production workloads, NVIDIA marketing would shout at the top of their lungs to promote its successful deployment. Given the apparently limited adoption of InfiniBand SHARP for now, we show here collective performance for Nvidia both when SHARP is and is not enabled.</p>



<p>For some of the benchmarks, we have also collected Nvidia Spectrum-X Ethernet data on an Nvidia internal cluster called Israel-1. Nvidia Spectrum-X is used in xAI’s 200k H100/H200 cluster and can support clusters up to 100k GPUs in the Spectrum-X reference architecture version 1.2, but could potentially support up to 512k GPUs with a non-reference custom design.</p>



<p>We are also in the process of testing Google Cloud (GCP) H100’s in-house ethernet, as well as AWS’ H100 and H200s that are deployed on AWS’s in-house Ethernet (called EFAv2/EFAv3). We will be sharing the results in our upcoming “Collective Deep Dive” article, which will provide visualizations of&nbsp;&nbsp;the different types of collectives, explain the different NCCL protocols (SIMPLE, LL, LL128), different NCCL algorithms (NVLS, NVLSTREE, RING, TREE, COLNETDIRECT, COLNETCHAIN, PAT), and how collectives run on GCP H100 Ethernet, AWS H100/H200 EFA, InfiniBand H100, Spectrum-X, etc.</p>



<p>Below we show a 32 GPU all reduce collective test. You can see that MI300X RoCEv2 is in last place compared to normal InfiniBand H100 and InfiniBand H100 with SHARP enabled. Simply put, poor all reduce performance leads to poor scale-out training.&nbsp;&nbsp;</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1594" height="1203" data-attachment-id="150418489" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/311-32gpu-non-blokcing-all-reduce/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/311-32gpu-non-blokcing-all-reduce.png?fit=1594%2C1203&amp;ssl=1" data-orig-size="1594,1203" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="311 32gpu non blokcing all reduce" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/311-32gpu-non-blokcing-all-reduce.png?fit=300%2C226&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/311-32gpu-non-blokcing-all-reduce.png?fit=1024%2C773&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/311-32gpu-non-blokcing-all-reduce.png?resize=1594%2C1203&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/311-32gpu-non-blokcing-all-reduce.png?w=1594&amp;ssl=1 1594w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/311-32gpu-non-blokcing-all-reduce.png?resize=300%2C226&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/311-32gpu-non-blokcing-all-reduce.png?resize=1024%2C773&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/311-32gpu-non-blokcing-all-reduce.png?resize=768%2C580&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/311-32gpu-non-blokcing-all-reduce.png?resize=1536%2C1159&amp;ssl=1 1536w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>The MI300X’s performance decreases if you scale out (i.e. increase) the number of GPUs participating in a collective. As you can imagine, modern frontier training is carried out on clusters of at least 100,000 GPUs. MI300X RoCEv2 runs at half the speed for all the real-world message sizes of 16MiB to 256MiB when compared to the baseline of InfiniBand Non-SHARP. As per the chart below, Nvidia Spectrum-X Ethernet performance is quite close to InfiniBand Non-SHARP’s performance, due to Spectrum-X’s vertical integration with the NCCL collective library as well as its use of good congestion control and adaptive routing. AMD is attempting to vertically integrate next year with their upcoming Pollara 400G NIC, which supports Ultra Ethernet, hopefully making AMD competitive with Nvidia. As always, Nvidia is not standing still and by late next year, it will be ready to go into production with its 800G ConnectX-8 NICs, which provide a line rate twice as fast as AMD’s Pollara NIC.&nbsp;</p>



<p>AMD RCCL is a fork of Nvidia NCCL. AMD’s RCCL Team and many other teams at AMD are resource limited and don’t have enough of either compute or headcount to improve the AMD ecosystem. AMD’s RCCL Team currently has stable access to less than&nbsp;<em>32 MI300Xs for R&amp;D</em>, which is ironic, as improving collective operations is all about having access to many GPUs. This is frankly silly, AMD should spend more on their software teams having access to more GPUs.</p>



<p>This contrasts with Nvidia’s NCCL team, which has access to R&amp;D resources on Nvidia’s 11,000 H100 internal EOS cluster. Furthermore, Nvidia has Sylvain Jeaugey, who is&nbsp;the&nbsp;subject matter expert on collective communication. There are a lot of other world class collective experts working at Nvidia as well, and, unfortunately, AMD has largely failed to attract collective library talent due to less attractive compensation and resources – as opposed to engineers at Nvidia, where it is not uncommon to see engineers make greater than a million dollars per year thanks to appreciation in the value of RSUs.&nbsp;</p>



<p>To help alleviate these issues, TensorWave and SemiAnalysis are currently working with the AMD RCCL Team to improve collective performance. TensorWave has generously sponsored AMD a medium-sized cluster in order help the RCCL Team have greater resources to do their jobs. The fact that Tensorwave after buying many GPUs has to give AMD GPUs for them to fix their software is insane.</p>



<p>Another trend to notice is that for non-SHARP networks, all reduce collective’s speed will reduce logarithmically as you double the number of GPUs. In contrast, with SHARP, the speed/completion time stays the same. We have results for up to 1,024 H100s showing that IB SHARP all reduce is constant time across any number of GPUs in a collective. We will publish this in our upcoming&nbsp;<em>“Collective Deep Dive”&nbsp;</em>article.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1473" height="1153" data-attachment-id="150418490" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/331-128gpu-with-400g-non-blocking-allreduce/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/331-128gpu-with-400g-non-blocking-allreduce.png?fit=1473%2C1153&amp;ssl=1" data-orig-size="1473,1153" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="331 128gpu with 400g non blocking allreduce" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/331-128gpu-with-400g-non-blocking-allreduce.png?fit=300%2C235&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/331-128gpu-with-400g-non-blocking-allreduce.png?fit=1024%2C802&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/331-128gpu-with-400g-non-blocking-allreduce.png?resize=1473%2C1153&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/331-128gpu-with-400g-non-blocking-allreduce.png?w=1473&amp;ssl=1 1473w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/331-128gpu-with-400g-non-blocking-allreduce.png?resize=300%2C235&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/331-128gpu-with-400g-non-blocking-allreduce.png?resize=1024%2C802&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/331-128gpu-with-400g-non-blocking-allreduce.png?resize=768%2C601&amp;ssl=1 768w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>For all gather, all to all, and reduce scatter collectives, MI300X is anywhere from 2-4 times slower than InfiniBand. Unfortunately, we did not have access to Spectrum-X or InfiniBand SHARP benchmark data for all gather or reduce scatter.&nbsp;</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1670" height="1250" data-attachment-id="150418545" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/screenshot-9/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/340-128gpu-400g-non-blocking-allgather.jpg?fit=1670%2C1250&amp;ssl=1" data-orig-size="1670,1250" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;Screenshot&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;Screenshot&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="Screenshot" data-image-description="" data-image-caption="<p>Screenshot</p>
" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/340-128gpu-400g-non-blocking-allgather.jpg?fit=300%2C225&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/340-128gpu-400g-non-blocking-allgather.jpg?fit=1024%2C766&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/340-128gpu-400g-non-blocking-allgather.jpg?resize=1670%2C1250&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/340-128gpu-400g-non-blocking-allgather.jpg?w=1670&amp;ssl=1 1670w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/340-128gpu-400g-non-blocking-allgather.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/340-128gpu-400g-non-blocking-allgather.jpg?resize=1024%2C766&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/340-128gpu-400g-non-blocking-allgather.jpg?resize=768%2C575&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/340-128gpu-400g-non-blocking-allgather.jpg?resize=1536%2C1150&amp;ssl=1 1536w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1720" height="1221" data-attachment-id="150418492" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/350-128gpu-reduce-scatter/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/350-128gpu-reduce-scatter.png?fit=1720%2C1221&amp;ssl=1" data-orig-size="1720,1221" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="350 128gpu reduce scatter" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/350-128gpu-reduce-scatter.png?fit=300%2C213&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/350-128gpu-reduce-scatter.png?fit=1024%2C727&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/350-128gpu-reduce-scatter.png?resize=1720%2C1221&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/350-128gpu-reduce-scatter.png?w=1720&amp;ssl=1 1720w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/350-128gpu-reduce-scatter.png?resize=300%2C213&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/350-128gpu-reduce-scatter.png?resize=1024%2C727&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/350-128gpu-reduce-scatter.png?resize=768%2C545&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/350-128gpu-reduce-scatter.png?resize=1536%2C1090&amp;ssl=1 1536w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1723" height="1219" data-attachment-id="150418493" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/360-128gpu-all-to-all/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/360-128gpu-all-to-all.png?fit=1723%2C1219&amp;ssl=1" data-orig-size="1723,1219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="360 128gpu all to all" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/360-128gpu-all-to-all.png?fit=300%2C212&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/360-128gpu-all-to-all.png?fit=1024%2C724&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/360-128gpu-all-to-all.png?resize=1723%2C1219&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/360-128gpu-all-to-all.png?w=1723&amp;ssl=1 1723w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/360-128gpu-all-to-all.png?resize=300%2C212&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/360-128gpu-all-to-all.png?resize=1024%2C724&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/360-128gpu-all-to-all.png?resize=768%2C543&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/360-128gpu-all-to-all.png?resize=1536%2C1087&amp;ssl=1 1536w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>Below, we provide our nccl/rccl benchmarking script. Unfortunately, due to the nature of cluster-specific setups, it is not as simple as a one-liner. It does require you to follow the README.md of nccl/rccl and nccl-tests/rccl-tests to run properly. On AWS and Google Cloud, there may also be custom nccl adapters that you will need to install.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="814" height="747" data-attachment-id="150418494" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/370-installing-ccl-testsgimpo-v2/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/370-installing-ccl-testsGIMPO-v2.png?fit=814%2C747&amp;ssl=1" data-orig-size="814,747" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="370 installing ccl testsGIMPO v2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/370-installing-ccl-testsGIMPO-v2.png?fit=300%2C275&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/370-installing-ccl-testsGIMPO-v2.png?fit=814%2C747&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/370-installing-ccl-testsGIMPO-v2.png?resize=814%2C747&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/370-installing-ccl-testsGIMPO-v2.png?w=814&amp;ssl=1 814w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/370-installing-ccl-testsGIMPO-v2.png?resize=300%2C275&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/370-installing-ccl-testsGIMPO-v2.png?resize=768%2C705&amp;ssl=1 768w" sizes="auto, (max-width: 814px) 100vw, 814px"><figcaption>Source:&nbsp;<a href="https://ray.so/#code=CiMhL2Jpbi9iYXNoCiNTQkFUQ0ggLS1qb2ItbmFtZT1jb2xsZWN0aXZlX3BlcmYKI1NCQVRDSCAtLW5vZGVzPTE2CiNTQkFUQ0ggLS1udGFza3MtcGVyLW5vZGU9OAojU0JBVENIIC0tZ3Jlcz1ncHU6OAojU0JBVENIIC0tY3B1cy1wZXItdGFzaz0yMgojU0JBVENIIC0tdGltZT0wODowMDowMAoKIyBJbnN0YW">SemiAnalysis</a></figcaption></figure>



<h2 id="amds-user-experience-is-suboptimal-and-the-mi300x-is-not-usable-out-of-the-box">AMD’s User Experience is Suboptimal and the MI300X is Not Usable Out of the Box</h2>



<p>Due to poor internal testing (i.e. “dogfooding”) and a lack of automated testing on AMD’s part, the MI300 is not usable out of the box and requires considerable amounts of work and tuning.&nbsp;<a href="https://www.youtube.com/live/vJ8aEO6ggOs?si=ViPmlckQNmDYCayJ&amp;t=3416">In November 2024 at AMD’s “Advancing AI”, AMD’s SVP of AI</a>&nbsp;stated that are over 200k tests running every evening internally at AMD. However, this seems to have done little to ameliorate the many AMD software bugs we ran into, and we doubt AMD is doing proper CI/CD tests include proper performance regression, or functional and convergence/numerics testing. We will outline a few examples here for readers to understand the nature of the AMD software bugs we have encountered and why we feel they have been very obstructive to a good user experience on AMD.&nbsp;</p>



<p><a href="https://rocm.blogs.amd.com/artificial-intelligence/flash-attention/README.html#benchmarking-attention">Although AMD’s own documentation recommends using PyTorch native Flash Attention</a>, for a couple months this summer, AMD’s PyTorch native Flash Attention kernel ran at less than 20 TFLOP/s, meaning that a modern CPU would have calculated the attention backwards layer&nbsp;<em>faster than an MI300X GPU</em>. For a time, basically all Transformer/GPT model training using PyTorch on the MI300X ran at a turtle’s pace. Nobody at AMD noticed this until a bug report was filed following deep PyTorch/Perfetto profiling showing the backwards pass (purple/brown kernels) took up far more time than the forward pass (dark green section). Normally, the backwards section should take up just ~2x as much time as the forward pass (slightly more if using activation checkpointing).&nbsp;</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1481" height="209" data-attachment-id="150418495" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/380-github-picture-lolgimp/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/380-github-picture-lolGIMP.png?fit=1481%2C209&amp;ssl=1" data-orig-size="1481,209" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="380 github picture lolGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/380-github-picture-lolGIMP.png?fit=300%2C42&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/380-github-picture-lolGIMP.png?fit=1024%2C145&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/380-github-picture-lolGIMP.png?resize=1481%2C209&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/380-github-picture-lolGIMP.png?w=1481&amp;ssl=1 1481w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/380-github-picture-lolGIMP.png?resize=300%2C42&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/380-github-picture-lolGIMP.png?resize=1024%2C145&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/380-github-picture-lolGIMP.png?resize=768%2C108&amp;ssl=1 768w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>Another issue we encountered was that the AMD PyTorch attention layer led to a hard error when used with&nbsp;<a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">torch.compile</a>&nbsp;due to the rank of the longsumexp Tensor being incorrect. What was frustrating is that this had already been fixed in internal builds of AMD PyTorch on May 30<sup>th</sup>, but did not reach any AMD PyTorch distributions or even any PyTorch nightly builds until October when it was pointed out to them that there was a bug. This demonstrates a lack of testing and dogfooding on the packages AMD puts out to the public. Another core reason for this problem is that the lead maintainer of PyTorch (Meta) does not currently use MI300X internally for production LLM training, leading to code paths not used internally at Meta being buggy and not dogfooded properly. We believe AMD should partner with Meta to get their internal LLM training working on MI300X.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="890" height="453" data-attachment-id="150418496" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/390-rocm-internal-testinggimp/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/390-rocm-internal-testingGIMP.png?fit=890%2C453&amp;ssl=1" data-orig-size="890,453" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="390 rocm internal testingGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/390-rocm-internal-testingGIMP.png?fit=300%2C153&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/390-rocm-internal-testingGIMP.png?fit=890%2C453&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/390-rocm-internal-testingGIMP.png?resize=890%2C453&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/390-rocm-internal-testingGIMP.png?w=890&amp;ssl=1 890w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/390-rocm-internal-testingGIMP.png?resize=300%2C153&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/390-rocm-internal-testingGIMP.png?resize=768%2C391&amp;ssl=1 768w" sizes="auto, (max-width: 890px) 100vw, 890px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<p>On August 8<sup>th</sup>, Horace He and the Meta PyTorch Team released&nbsp;<a href="https://pytorch.org/blog/flexattention/">FlexAttention</a>, a critical API for creating non-causal attention layers without losing speed. To previously use attention variants like document masking, sliding window attention, softcap, and Alibi, a user would need to spend weeks handcrafting their own kernel in CUDA/HIP language, and subsequently pybinding it to PyTorch. However, with FlexAttention, a user can quickly generate all the attention variants using the API. FlexAttention achieves great performance by using block sparsity by only calculating the blocks of the mask where needed, ignoring the rest.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="437" height="357" data-attachment-id="150418497" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/400-block-maskgimp/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/400-block-maskGIMP.png?fit=437%2C357&amp;ssl=1" data-orig-size="437,357" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="400 block maskGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/400-block-maskGIMP.png?fit=300%2C245&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/400-block-maskGIMP.png?fit=437%2C357&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/400-block-maskGIMP.png?resize=437%2C357&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/400-block-maskGIMP.png?w=437&amp;ssl=1 437w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/400-block-maskGIMP.png?resize=300%2C245&amp;ssl=1 300w" sizes="auto, (max-width: 437px) 100vw, 437px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1600" height="1459" data-attachment-id="150418498" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/410-attention-variant-supportgimp/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/410-attention-variant-supportGIMP.jpg?fit=1600%2C1459&amp;ssl=1" data-orig-size="1600,1459" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="410 attention variant supportGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/410-attention-variant-supportGIMP.jpg?fit=300%2C274&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/410-attention-variant-supportGIMP.jpg?fit=1024%2C934&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/410-attention-variant-supportGIMP.jpg?resize=1600%2C1459&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/410-attention-variant-supportGIMP.jpg?w=1600&amp;ssl=1 1600w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/410-attention-variant-supportGIMP.jpg?resize=300%2C274&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/410-attention-variant-supportGIMP.jpg?resize=1024%2C934&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/410-attention-variant-supportGIMP.jpg?resize=768%2C700&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/410-attention-variant-supportGIMP.jpg?resize=1536%2C1401&amp;ssl=1 1536w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: Meta</figcaption></figure>



<p>With sliding window attention, FlexAttention can improve performance by 10-20x! This is amazing for the end user, but unfortunately, MI300X FlexAttention was in a poor state and suffers from numerous AMD software bugs (including convergence issues) until but a couple days ago. While the latest PyTorch nightly now fixes for convergence issues, this contrasts starkly with FlexAttention on Nvidia, which has been available since August. That means a ~6 month gap exists between the availability of these fantastic Pytorch features on Nvidia and AMD’s platforms. For frontier AI labs, six months is a lifetime, with OpenAI, Anthropic, and Google having released numerous models in such a span.&nbsp;</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1475" height="1009" data-attachment-id="150418499" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/415-sliding-window-attention-both/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/415-sliding-window-attention-both.png?fit=1475%2C1009&amp;ssl=1" data-orig-size="1475,1009" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="415 sliding window attention – both" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/415-sliding-window-attention-both.png?fit=300%2C205&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/415-sliding-window-attention-both.png?fit=1024%2C700&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/415-sliding-window-attention-both.png?resize=1475%2C1009&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/415-sliding-window-attention-both.png?w=1475&amp;ssl=1 1475w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/415-sliding-window-attention-both.png?resize=300%2C205&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/415-sliding-window-attention-both.png?resize=1024%2C700&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/415-sliding-window-attention-both.png?resize=768%2C525&amp;ssl=1 768w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<h2 id="exploring-ideas-for-better-performance-on-amd">Exploring Ideas for Better Performance on AMD</h2>



<p>AMD recommended we try PYTORCH_ TUNABLE_OPS to improve GEMM performance by sweeping through GEMM algorithms at runtime. However, as we mentioned earlier, this API works poorly because GEMMs should be tuned when compiling the hipBLASLt/RoCBLAS/cuBLASLt and not during the users’ runtime. Users of Nvidia H100s do not need to use PYTORCH_ TUNABLE_OPS for most shapes because cuBLAS heuristic model will pick the correct algorithmn. This contrasts with AMD’s heuristic model, which never seems to pick the correct algorithm for most shapes. We recommend that AMD stop suggesting that users try tunable ops and instead focus on properly tuning their GEMM libraries internally.&nbsp;</p>



<p>When we tried PYTORCH_ TUNABLE_OPS on AMD, it led to an HBM memory leak of over 25 GByte out of the total MI300X capacity of 192GBytes, essentially wiping out the MI300’s HBM capacity advantage over the H100. The fix for this is to set a default hipBLASLt and rocBLAS workspace to prevent memory leaks.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="540" data-attachment-id="150418500" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/430-cublas-and-cudagimp/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/430-cublas-and-cudaGIMP.png?fit=1038%2C547&amp;ssl=1" data-orig-size="1038,547" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="430 cublas and cudaGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/430-cublas-and-cudaGIMP.png?fit=300%2C158&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/430-cublas-and-cudaGIMP.png?fit=1024%2C540&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/430-cublas-and-cudaGIMP.png?resize=1024%2C540&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/430-cublas-and-cudaGIMP.png?resize=1024%2C540&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/430-cublas-and-cudaGIMP.png?resize=300%2C158&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/430-cublas-and-cudaGIMP.png?resize=768%2C405&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/430-cublas-and-cudaGIMP.png?w=1038&amp;ssl=1 1038w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: PyTorch/AMD</figcaption></figure>



<p>As we mentioned earlier in this article, another issue we ran into was that there was a plethora of environment flags needed on MI300X to make it actually usable. We recommend to AMD that they stop putting users in the position of having to set these environment flags themselves and, instead, set default flags that lead to a usable environment. It is not simply their number, but also the complex interactions between the flags, making troubleshooting difficult. Getting reasonable training performance out of AMD MI300X is an NP-Hard problem.&nbsp;</p>



<p>Another issue is that certain AMD ROCm libraries could not be installed inside Docker due to AMD software CMake bugs leading to hard errors. This has since been fixed. On AMD GPUs, you need to pass in a convoluted set of flags to get the GPUs to be able to work inside a container, whereas with docker, getting GPUs to work is as simple as passing in “—gpus=all”. We recommend to AMD that they partner with Docker and ensure that Docker can autodetect GPUs for AMD as well, making the workflow as streamlined as when working with Nvidia GPUs.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="3496" height="1300" data-attachment-id="150418501" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/440-nv-vs-amd-rocker-run-gpus-allgimp/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/440-nv-vs-amd-rocker-run-gpus-allGIMP.png?fit=3496%2C1300&amp;ssl=1" data-orig-size="3496,1300" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="440 nv vs amd rocker run gpus allGIMP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/440-nv-vs-amd-rocker-run-gpus-allGIMP.png?fit=300%2C112&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/440-nv-vs-amd-rocker-run-gpus-allGIMP.png?fit=1024%2C381&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/440-nv-vs-amd-rocker-run-gpus-allGIMP.png?resize=3496%2C1300&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/440-nv-vs-amd-rocker-run-gpus-allGIMP.png?w=3496&amp;ssl=1 3496w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/440-nv-vs-amd-rocker-run-gpus-allGIMP.png?resize=300%2C112&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/440-nv-vs-amd-rocker-run-gpus-allGIMP.png?resize=1024%2C381&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/440-nv-vs-amd-rocker-run-gpus-allGIMP.png?resize=768%2C286&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/440-nv-vs-amd-rocker-run-gpus-allGIMP.png?resize=1536%2C571&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/440-nv-vs-amd-rocker-run-gpus-allGIMP.png?resize=2048%2C762&amp;ssl=1 2048w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/440-nv-vs-amd-rocker-run-gpus-allGIMP.png?w=3000&amp;ssl=1 3000w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<h2 id="amd%e2%80%99s-forked-libraries">AMD’s Forked Libraries</h2>



<p>Many of AMD’s libraries are forked off Nvidia’s open-source or ecosystem libraries. AMD uses a tool called Hipify to carry out source-to-source translation of Nvidia CUDA to AMD HIP. While the motivation is understandable,&nbsp;<strong>they arenevertheless building on top of their competitor’s platform</strong>&nbsp;and cannot expect to match or surpass Nvidia’s user experience with this software development strategy. They need to contribute their software to the AMD ecosystem. For example, instead of supporting FP8 training by forking Nvidia/TransformerEngine and source-to-source translation, they should attempt PyTorch native FP8 training to work well on their own hardware. Currently, AMD PyTorch native FP8 training recipes don’t work on AMD and the unit tests don’t even pass yet, there is no CI/CD for AMD PyTorch native FP8 training.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="330" data-attachment-id="150418502" data-permalink="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/450-amd-forks-of-nv-libraries/" data-orig-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/450-amd-forks-of-NV-libraries.png?fit=2211%2C712&amp;ssl=1" data-orig-size="2211,712" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="450 amd forks of NV libraries" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/450-amd-forks-of-NV-libraries.png?fit=300%2C97&amp;ssl=1" data-large-file="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/450-amd-forks-of-NV-libraries.png?fit=1024%2C330&amp;ssl=1" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/450-amd-forks-of-NV-libraries.png?resize=1024%2C330&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/450-amd-forks-of-NV-libraries.png?resize=1024%2C330&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/450-amd-forks-of-NV-libraries.png?resize=300%2C97&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/450-amd-forks-of-NV-libraries.png?resize=768%2C247&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/450-amd-forks-of-NV-libraries.png?resize=1536%2C495&amp;ssl=1 1536w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/450-amd-forks-of-NV-libraries.png?resize=2048%2C660&amp;ssl=1 2048w" sizes="auto, (max-width: 1000px) 100vw, 1000px"><figcaption>Source: SemiAnalysis</figcaption></figure>



<h2 id="detailed-recommendations-to-amd-on-how-to-fix-their-software">Detailed Recommendations to AMD on How to Fix Their Software</h2>



<p>First, AMD needs to focus on attracting more software engineering resources and improving compensation for current engineers. The current compensation gap between AMD and Nvidia means that top talent is lured to Nvidia over AMD. This top talent is also attracted to Nvidia as it has far more compute/resources for engineers. AMD should procure more GPUs for their in-house development work and submit an MLPerf GPT3 175B result as soon as possible. Even if the result is not competitive with Nvidia right now, submitting such a benchmark will kick off the process for iterative improvement.&nbsp;</p>



<p>We also notice that AMD frequently gives their customers custom images, and, in fact, AMD developers themselves often work on top of such bespoke images. This is not best practice, as this means that AMD engineers have a different experience vs. images available to the public. AMD should instead lift the standard of public images by using these images internally and with its customers, and the AMD executive team should personally internally test (i.e. “dogfood”) what is getting shipped publicly.</p>



<p>We recommend that AMD create a public dashboard that runs every night, showing the performance of their hardware on benchmarks such as MLPerf or TorchBench. This dashboard should also include H100/H200 performance as a baseline.</p>



<p>Finally, AMD needs to completely transform its approach to environmental flags. Instead of setting a myriad of flags to get running out of the box, it should set them to recommended defaults so users can get started quickly.&nbsp;</p>



<p>AMD should collaborate with Meta to get production training workloads working on ROCm, as it is well-known amongst PyTorch users that PyTorch code paths tend to have tons of bugs unless Meta uses it internally. Meta currently hand writes HIP Kernels for their production MI300X inferencing but does not use MI300X for real training. It would be a fantastic improvement for the AMD ecosystem, and a marketing victory, if a smaller version of the next Llama is trained on AMD. Not to mention that this would open the door to AMD progressively moving towards larger models/clusters with Meta. Meta using AMD GPUs for actual model training would be a win-win for both companies as Meta is also looking for alternative training chips to Nvidia.</p>



<p>Currently Nvidia offers well over 1,000 GPUs for Continuous improvement and development of Pytorch externally and many more internally. AMD doesn’t. AMD needs to work with an AMD focused GPU Neocloud to have ~10,000 GPUs of each generation for internal development purposes and Pytorch. This will still be 1/8<sup>th</sup>&nbsp;that of Nvidia with their coming huge Blackwell clusters, but it’s a start. These can be dedicated to internal development and CICD for Pytorch.</p>



<p><strong>Lisa, we are open to a meeting on how to fix AMD’s Datacenter GPU User Experience for the better!</strong></p>



<h2 id="h100h200mi300x-networking-bom-analysis-and-performance-per-tco">H100/H200/MI300X Networking BoM Analysis and Performance per TCO</h2>



<p>In addition to our benchmarking of collectives and GEMM throughput, we have conducted several experiments exploring insightful topics for conducting further benchmarks and running real-world workloads on clusters. These experiments cover benchmarking warmup and repeat effects, VBoost Power Shifting, MLPerf Training GPT-3, BF16 vs FP16 throughput, throughput by GEMM input distribution, power per FLOP, and throughput for the PyTorch PyPi distribution vs Nvidia NGC Stable PyTorch images.&nbsp;</p>



<p>We also present a detailed networking bill of materials (BoM) analysis for the 1k GPU Ethernet, 1k GPU InfiniBand, 16k GPU Ethernet, and 16k GPU InfiniBand clusters. We also discuss the impact of using 51.2T Radix vs. 25.6T Radix switches for back-end networking.</p>



<p>Lastly – we present a performance per TCO analysis that shows how the H100/H200/MI300X stacks up in terms of $/hr per effective training petaflop. These items are available below to all SemiAnalysis subscribers and will be of great interest to datacenter operators, ML scientists, and investors.</p>



<div id="semianalysis-blocks-login-to-view-content">
<h5 id="subscribe-for-full-access-to-this-article">Subscribe for full access to this article</h5>



<p>With a SemiAnalysis subscription you get full access to all articles, Data Explorer graphs, article discussions, and further insight into deep dives.</p>







<div>







<p>Please verify your email address to proceed.</p>



</div>



<p>By subscribing, you agree to the&nbsp;<a href="https://semianalysis.com/privacy-policy/">Privacy Policy</a>&nbsp;and&nbsp;<a href="https://semianalysis.com/terms-of-service/">Terms and Conditions</a>.</p>
</div>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[German watchdog orders Sam Altman's biometric ID project World to delete data (124 pts)]]></title>
            <link>https://www.euronews.com/next/2024/12/19/german-watchdog-orders-sam-altmans-biometric-id-project-world-to-delete-data</link>
            <guid>42489072</guid>
            <pubDate>Sun, 22 Dec 2024 21:02:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.euronews.com/next/2024/12/19/german-watchdog-orders-sam-altmans-biometric-id-project-world-to-delete-data">https://www.euronews.com/next/2024/12/19/german-watchdog-orders-sam-altmans-biometric-id-project-world-to-delete-data</a>, See on <a href="https://news.ycombinator.com/item?id=42489072">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Twtxt is a decentralised, minimalist microblogging service for hackers (296 pts)]]></title>
            <link>https://twtxt.readthedocs.io/en/latest/index.html</link>
            <guid>42488983</guid>
            <pubDate>Sun, 22 Dec 2024 20:51:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twtxt.readthedocs.io/en/latest/index.html">https://twtxt.readthedocs.io/en/latest/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=42488983">Hacker News</a></p>
Couldn't get https://twtxt.readthedocs.io/en/latest/index.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tokenisation Is NP-Complete (107 pts)]]></title>
            <link>https://arxiv.org/abs/2412.15210</link>
            <guid>42488853</guid>
            <pubDate>Sun, 22 Dec 2024 20:32:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2412.15210">https://arxiv.org/abs/2412.15210</a>, See on <a href="https://news.ycombinator.com/item?id=42488853">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="labstabs"><p>
    <label for="tabone">Bibliographic Tools</label></p><div>
      <h2>Bibliographic and Citation Tools</h2>
      <div>
          <p><label>
              
              <span></span>
              <span>Bibliographic Explorer Toggle</span>
            </label>
          </p>
          
        </div>
        
        
        
        
    </div>


    <p>
    <label for="tabtwo">Code, Data, Media</label></p><div>
      <h2>Code, Data and Media Associated with this Article</h2>
      

      
      
      
      
      
      
      
      
    </div>


      <p>
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label></p><div>
        <h2>Demos</h2>
        
        
        
        
      </div>
      <p>
      <label for="tabfour">Related Papers</label></p><div>
        <h2>Recommenders and Search Tools</h2>
        
        
        
        
        
      </div>

      <p>
      <label for="tabfive">
        About arXivLabs
      </label></p><div>
            <h2>arXivLabs: experimental projects with community collaborators</h2>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>

    </div></div>]]></description>
        </item>
    </channel>
</rss>