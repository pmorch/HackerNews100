<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 22 Nov 2024 20:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Phased Array Microphone (2023) (198 pts)]]></title>
            <link>https://benwang.dev/2023/02/26/Phased-Array-Microphone.html</link>
            <guid>42215552</guid>
            <pubDate>Fri, 22 Nov 2024 17:10:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://benwang.dev/2023/02/26/Phased-Array-Microphone.html">https://benwang.dev/2023/02/26/Phased-Array-Microphone.html</a>, See on <a href="https://news.ycombinator.com/item?id=42215552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article>

  

  <div>
    <p>A 192-channel phased array microphone, with FPGA data acquisition and beamforming/visualization on the GPU.
Phased arrays allow for applications not possible with traditional directional microphones, as the directionality
can be changed instantly, after the recording is made, or even be focused at hundreds of thousands of points 
simultaneously in real time.</p>

<p>All designs are open source:</p>

<ul>
  <li><a href="https://github.com/kingoflolz/mic_host">Host software</a></li>
  <li><a href="https://github.com/kingoflolz/mic_gateware">FPGA gateware</a></li>
  <li><a href="https://github.com/kingoflolz/mic_hardware">PCB layout and schematics, mechanical components</a></li>
</ul>

<p><img src="https://benwang.dev/assets/mic%20block%20diagram.png" alt=""></p>
<p> Block diagram </p>

<p><img src="https://benwang.dev/assets/mic%20overall.jpg" alt=""></p>
<p> Glamor shot </p>

<h2 id="hardware">Hardware</h2>

<p>To create a phased array microphone, a large number of microphones needs to be placed in an arrangement with
 a wide distribution of spacing. For a linear array, exponential spacing between microphones is found to be optimal for
 broadband signals. To create a 2d array, these symmetrical linear arrays (“arms”) are be placed radially, which allows
the central (“hub”) board to be compact. The total cost for the array is approximately $700.</p>

<h3 id="arms">Arms</h3>

<p>The length of each arm is dictated by the limits of PCB manufacturing and assembly. These boards were made at JLCPCB,
where the maximum length for manufacturing and assembly of 4 layer PCBs was 570mm.</p>

<p>The microphones chosen were the <a href="https://www.lcsc.com/product-detail/MEMS-Microphones_MEMS-MSM261D4030H1CPM_C966942.html">cheapest digital output MEMS microphone</a>
(because there are a lot of them!), which were about $0.5. At this bottom of the barrel price
range, there is little differentiation in the performance characteristics between different microphones. Most have 
decent performance up to 10khz and unspecified matching of phase delay and volume.</p>

<p>These microphones output data using pulse density modulation (PDM), which provides a one bit output at a frequency
significantly higher than the audible range (up to 4 MHz), with the high sampling rate compensating for quantization noise. 
These microphones also support latching the data either on the rising or falling edge of the clock (DDR), which allows
two microphones to be multiplexed on a single wire, reducing the amount of connections required.</p>

<p>Each arm contains 8 microphones sharing 4 output lines, as well as an output buffer on the clock input line.
This ensures the rise times are reasonable, even with hundreds of microphones sharing the same clock signal.</p>

<p>For some reason (likely the low rigidity of the panel and some suboptimal solder paste stencil patterns combined with 
the LGA microphone footprints), the yields on the arm PCBs are not very good, with only 50% of them working out of the 
box. The most common fault was the clock line being shorted to either 3V3 or ground, which unfortunately requires trial
and error of removing microphones from the PCB until the short is resolved. Next time some series resistors on the
clock line would speed this process up a lot, and improving the panelization and paste stencil would likely 
improve yields so extensive rework isn’t required.</p>

<p>Even with rework, there are still some microphones which produce bogus data. These are just masked out in the code, as
there are enough working ones to make up for it (and it’s too much work to remove a bunch of arms to do more rework…)</p>

<p><img src="https://benwang.dev/assets/mic%20arm%20panel.png" alt=""></p>

<h3 id="hub">Hub</h3>

<p>An FPGA is used to collect all the data, due to the large number of low latency IOs available combined with the ability 
to communicate using high speed interfaces (e.g. Gigabit Ethernet). Specifically, the <a href="https://www.colorlight-led.com/product/colorlight-i5-led-display-receiver-card.html">Colorlight i5</a>
card is used, as it has enough IOs, is cheap and readily available, and has two integrated ethernet PHYs 
(only one is used for this project). The card is originally designed as an ethernet interface for LED panels, but has 
been <a href="https://github.com/wuxx/Colorlight-FPGA-Projects">fully reverse engineered</a>. About 100 GPIOs are broken out over 
the DDR2 connector, which is much easier to fan out than the BGA of the original FPGA.</p>

<p><img src="https://benwang.dev/assets/mic%20hub%20board.png" alt=""></p>

<p>Other than the FPGA, the hub contains some simple power management circuitry, and connectors for the arm boards as well
as an Ethernet connector with integrated magnetics.</p>

<h3 id="mechanical-design">Mechanical Design</h3>

<p>The arms are attached with M3 screws to the hub using <a href="https://www.lcsc.com/product-detail/Nuts_Sinhoo-SMTSO3080CTJ_C2916369.html">PCB mounted standoffs/nuts</a> 
, which conveniently can be assembled with SMD processes. The connections from each arm to the hub is made with 8 pin,
2mm pitch connectors.</p>

<p><img src="https://benwang.dev/assets/mic%20hub%20attachment.png" alt=""></p>

<p>The original mechanical design consists of slots on the arm PCBs which interlock with circumferential structural PCBs,
however the low torsional rigidity of the arms means the whole structure deformed too easily.</p>

<p><img src="https://benwang.dev/assets/mic%20structural%20pcb.png" alt=""></p>

<p>The final mechanical design consists of pieces of laser cut 1/4th inch MDF around the outer edge of the array, with each
arm attached to the MDF with some zip ties.</p>

<p><img src="https://benwang.dev/assets/mic%20arm%20attachment.jpg" alt=""></p>

<p>As the microphone array is mounted on the wall (which is very susceptible to reflections), a layer of acoustic foam is
used to attenuate the reflections to make calibration easier.</p>

<h2 id="gateware">Gateware</h2>

<p>The main objective for the gateware is to reliably transmit the raw acquired data losslessly to the computer for 
further processing, while keeping it as simple as possible. Performing decimation and filtering on the
FPGA would reduce the data rate, but sending the raw PDM data is achievable with Gigabit Ethernet. This
reduces the complexity of the FPGA code and allowing faster iteration. Compiling is much faster than place and route,
and it’s much easier to use a debugger in code than in gateware!</p>

<p>There are three major components to the gateware, a module for interfacing with the PDM interfaces, a module for 
creating fixed size packets from those readings, and a UDP streamer to write the packets to the Ethernet interface.</p>

<h3 id="pdm-interface">PDM Interface</h3>

<p>The PDM input module is a relatively simple piece of logic, which divides the 50 MHz system clock by a factor of 16 to 
output a 3.125MHz PDM clock, latches all 96 of the input pins after each clock edge, and then shifts out 32 bits of the 
data on each clock cycle. Each chunk of 192 bits is has a header added which is a 32 bit incrementing integer.</p>

<p>The PDM interface receives data at a rate of 3.125Mhz * 96 (input pins) * 2 (DDR), which is 600Mbps. With the header,
the data rate output from this module is 700Mbps, or approximately 40% utilization of the 32 bit output data path.</p>

<h3 id="packetizer">Packetizer</h3>

<p>The packetizer is essentially a FIFO buffer with a special interface on the input. A standard FIFO marks the output as available
whenever there is at least one item in the queue, but this would lead to smaller packets than requested as the ethernet
interface operates faster than the PDM output (leading to buffer underruns).
Thus, the packetizer waits until there is at least a full packet worth of 
data in the queue before starting a packet, which ensures constant sized packets.</p>

<p>48 PDM output blocks at 224 bits (192 bits of data with a 32 bit header) are placed into each packet, which totals
1344 bytes of data per packet, plus a 20 byte IPv4 header and an 8 byte UDP header, at a rate of approximately 65k pps.</p>

<p>This leads to a wire rate of 715 Mbps, or about 70% utilization of Gigabit Ethernet.</p>

<h3 id="udp-streamer">UDP Streamer</h3>

<p>The LiteEth project made this module very easy, as it abstracts out the lower level complexities of UDP and IP 
encapsulation, ARP tables and the like, and provides a convenient interface for simply hooking up a FIFO to a UDP 
stream. Occasionally there is some latency, but there is enough slack in the bus and buffer in the
packetizer FIFO to absorb any hiccups.</p>

<h3 id="utilization-and-timing">Utilization and Timing</h3>

<p>The FPGA on the Colorlight i5 is a <code>LFE5U-25F-6BG381C</code>, which has 25k LUTs. The design is 
placed and routed with the open source Project Trellis toolchain. By keeping the gateware very simple, 
the utilization on the device is quite low, and there is lots of room for additional functionality.</p>

<div><pre><code>Info: Device utilisation:
Info:                 DP16KD:    16/   56    28%
Info:                EHXPLLL:     1/    2    50%
Info:             TRELLIS_FF:  1950/24288     8%
Info:           TRELLIS_COMB:  3701/24288    15%
Info:           TRELLIS_RAMW:    49/ 3036     1%

Info: Max frequency for clock                   '$glbnet$crg_clkout': 73.17 MHz (PASS at 50.00 MHz)
Warning: Max frequency for clock '$glbnet$eth_clocks1_rx$TRELLIS_IO_IN': 124.07 MHz (FAIL at 125.00 MHz)
</code></pre></div>

<p>(Timing violations on eth rx clock is due to <a href="https://github.com/litex-hub/litex-boards/issues/40#issuecomment-1108817182">false positive from gray counter in liteeth</a>)</p>

<h2 id="software">Software</h2>

<h3 id="cic-filter">CIC Filter</h3>

<p>Each microphone produces a 1 bit signal at 3.125Mhz, and needs to be reduced to a more reasonable sample rate
and bit depth for further
processing. This is done very efficiently with a CIC filter, which only requires a few arithmetic operations to 
process each sample. For understanding more about CIC filters, <a href="https://tomverbeure.github.io/2020/09/30/Moving-Average-and-CIC-Filters.html">this series</a>
of blog posts from Tom Verbeure provides an excellent introduction. Following the nice graphs from there, 
I decided on a 4 stage, 16x decimation CIC filter which reduced the sample rate to a much reasonable 195kHz, at 32 bits.</p>

<p>To ingest the data at 3.125Mhz, the filter must be able to process each set of samples in 320ns. A naive implementation
in Rust wasn’t fast enough on a single core, but an implementation with some less abstraction (and a hence some more 
autovectorization) got there, and is what was used at the end. I also experimented with a version using SIMD 
intrinsics which was much faster, but ended up running into alignment issues when using it in together with other code.</p>

<p>Even with close to a billion bits per second of data to process, a single CPU core can do quite a few operations on 
each individual bit!</p>

<div><pre><code>test cic::bench_cic       ... bench: 574 ns/iter (+/- 79) = 41 MB/s
test cic::bench_fast_cic  ... bench: 181 ns/iter (+/- 24) = 132 MB/s
test cic::bench_simd_cic  ... bench:  36 ns/iter (+/- 0)  = 666 MB/s
</code></pre></div>

<h3 id="calibration">Calibration</h3>

<p>To perform array calibration, a speaker playing white noise is moved around the room in front of the array. An FFT based
cross correlation is performed between all pairs of microphones to compute relative delays.</p>

<p>A cross correlation can be
performed by computing the FFT of both signals (cached and computed once for each signal), and then computing the 
inverse FFT of the complex multiplication of the two. This is quite compute intensive, as there are over 18 thousand
pairs! For the window sizes used of 16-64k, the FFTs are memory bound, and thus the IFFT and peak finding is fused to
avoid writing the results to memory, which results in a 15x speedup. On a 7950X, this process runs in realtime.</p>

<p>Then the positions of the source at each timestep 
and the positions of each microphone is optimized using gradient descent (when you know PyTorch, all optimization 
problems look like gradient descent…). The loss function tries to minimize the difference between the measured
correlations and the ideal correlations, while trying to minimize the deviation of the microphone positions from
the initial positions as well as the jerk of the source trajectory.</p>

<p>As part of the calibration, the speed of sound is also a parameter which is optimized to obtain the best model of the
system, which allows this whole procedure to act as a ridiculously overengineered thermometer.</p>

<p>After a few hundred iterations, it converges to a reasonable solution for both
the source positions and the microphone positions, as well as constants such as the speed of sound. Fortunately this
problem vectorizes well for GPU, and converges in a few seconds.</p>

<p>The final mean position error is on the order of 1mm, and is able to correct for large scale systematic distortions
such as concavity from the lack of structural rigidity. The largest position error between the calibrated positions
and the designed positions is on the order of 5mm, which is enough to introduce significant phase errors to high 
frequency sound if uncorrected, although perhaps not strictly necessary (10khz sound has a wavelength of ~3.4cm).</p>

<p><img src="https://benwang.dev/assets/mic%20calibration.png" alt=""></p>

<h3 id="beamforming">Beamforming</h3>

<p>Beamforming is how the raw microphone inputs are processed to produce directional responses. The simplest method of
beamforming is delay-and-sum (DAS), where each signal is delayed according to its distance from the source. This is 
the type of beamforming implemented for this process, with the beamforming happening in the frequency domain.</p>

<p>In the frequency domain, a delay can be implemented by the complex multiplication of the signal with a linear phase term
proportional to the delay required, which also nicely handles delays which are not integer multiples of the sampling 
period.</p>

<p>Multiple nested subarrays of the original array are used for different frequency ranges. This reduces the processing
required for beamforming, as each frequency does not need to be beamformed with all the microphone. This also ensures
that the beamforming gains of all the frequencies are matched.</p>

<p><img src="https://benwang.dev/assets/mic%20subarray.png" alt=""></p>

<p>Two different types of beamforming visualizations are implemented, a 3d near field beamformer and a 2d far field
beamformer. When the audio source is far away, the wavefront is essentially a flat plane, and how far away the
source is does not meaningfully change the signals at the array. On the other hand, if the source is close to the 
array, the wavefront will have a significant curvature which allows the 3d location of the source to be determined.</p>

<p>The beamformer is implemented as a <a href="https://github.com/openai/triton/">Triton kernel</a>, a Python DSL which compiles to 
run on Nvidia GPUs. When beamforming to hundreds of thousands of points, the massive parallelism provided
by GPUs allows for results to be produced in real time. Some <a href="https://github.com/openai/triton/issues/974">current limitations</a>
with the Triton language around 
support to indexing with shared memory arrays lead to slightly suboptimal performance, but writing CUDA C++ doesn’t 
seem very fun…</p>

<h4 id="near-field-3d-beamforming">Near Field 3D Beamforming</h4>

<p>Near field 3D beamforming is performed a 5cm voxel grid with size 64x64x64. An update rate of 12hz is achieved on a
RTX 4090 with higher update rates limited by overhead of suboptimal CPU-GPU synchronization with the smaller work units. 
The voxel grid is then visualized using <a href="https://vispy.org/">VisPy</a>,
a high performance data visualization library which uses OpenGL. Modern games have millions of polygons, so rendering
a quarter million semi-transparent voxels at interactive framerates is no issue.</p>

<p>A quick demo of the voxel visualization below, note the reflection off the roof!</p>

<video controls="">
<source src="https://benwang.dev/assets/mic%203d%20demo.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>

<h4 id="far-field-2d-beamforming">Far Field 2D Beamforming</h4>

<p>Far field beamforming works similarly, but can be performed in higher resolution as there is no depth dimension
required. A 512x512 pixel grid is used, and the same 12hz update rate achieved. (The far field beamforming uses an 
approximation of just putting the points far away instead of actually assuming planar wavefront due to laziness…)</p>

<p>A demo of the 2d visualization here, but it’s not very exciting due to the poor acoustic environment of my room around 
the array, with lots of reflections and multipath.</p>

<video controls="">
<source src="https://benwang.dev/assets/mic%202d%20demo.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>

<h4 id="directional-audio">Directional Audio</h4>

<p>The previous two beamforming implementations compute the energy of sound from each location, but never materializes
the beamformed audio in memory. A time domain delay and sum beamformer is implemented to allow for directional audio 
recording. It takes a 3D coordinate relative from array center and outputs audio samples. 
An interesting aspect about this beamformer is that it is differentiable with regard to the location from the output. 
This means the location of the audio sources can be optimized
based on some differentiable loss function (like neural network), which might allow for some interesting applications 
such as using a forced alignment model of a multi-party transcript to determine the physical location of each speaker.</p>

<p>A speaker playing some audio is placed in front of the array, with another speaker placed approximately 45 degrees away
at the same distance from array center, playing white noise. The effectiveness of the beamforming can be demonstrated
by comparing the raw audio from a single microphone with the output from the beamforming.</p>

<p>Raw audio from a single microphone:</p>

<p><audio controls="">
<source src="https://benwang.dev/assets/mic%20raw.wav" type="audio/wav">
Your browser does not support the video tag.
</audio></p><p>Beamformed audio:</p>

<p><audio controls="">
<source src="https://benwang.dev/assets/mic%20beamformed.wav" type="audio/wav">
Your browser does not support the video tag.
</audio></p><h3 id="recording">Recording</h3>

<p>As the data from the microphone array is just UDP packets, it can be recorded with tools like <code>tcpdump</code>, and the 
packet capture file can be read to reinject the packets back into the listener. All the programs in the
previous sections are designed to work at real time, but can also work on recorded data using this process.</p>

<p>The tradeoff with this recording implementation is that the output data rate is quite high (due to faithfully recording
everything, even the quantization noise). At 87.5 MBps, a 1-hour recording would be 315 GB! A more optimized
implementation would do some compression, and do the recording after the CIC filter at a lower sample rate.</p>

<h2 id="next-steps">Next Steps</h2>

<p>I consider this project essentially complete, and don’t plan to work on it any further for the foreseeable future, but
there are still lots of possible cool extensions if you’d like to build one!</p>
<ul>
  <li>Using more advanced beamforming algorithms (<a href="https://ntrs.nasa.gov/api/citations/20080015889/downloads/20080015889.pdf">DAMAS</a> etc.)</li>
  <li>Better GUI to combine all existing functions (e.g. See where sound is coming from, and record audio from there)</li>
  <li>Combine differentiable beamforming and neural models (e.g. forced alignment example mentioned above)</li>
</ul>

  </div>

</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon to invest another $4B in Anthropic, OpenAI's biggest rival (282 pts)]]></title>
            <link>https://www.cnbc.com/2024/11/22/amazon-to-invest-another-4-billion-in-anthropic-openais-biggest-rival.html</link>
            <guid>42215126</guid>
            <pubDate>Fri, 22 Nov 2024 16:25:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/11/22/amazon-to-invest-another-4-billion-in-anthropic-openais-biggest-rival.html">https://www.cnbc.com/2024/11/22/amazon-to-invest-another-4-billion-in-anthropic-openais-biggest-rival.html</a>, See on <a href="https://news.ycombinator.com/item?id=42215126">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ArticleBody-InlineImage-107408832" data-test="InlineImage"><p>Anadolu | Anadolu | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Friday announced it would invest an additional $4 billion in Anthropic, the artificial intelligence startup founded by ex-OpenAI research executives.</p><p>The new funding brings the tech giant's total investment to $8 billion, though Amazon will retain its position as a minority investor, according to Anthropic, the San Francisco-based company behind the Claude chatbot and AI model.</p><p>Amazon Web Services will also become Anthropic's "primary cloud and training partner," according to a blog post. From now on, Anthropic will use AWS Trainium and Inferentia chips&nbsp;to train and deploy its largest AI models.</p><p>Anthropic is the company behind Claude — one of the chatbots that, like OpenAI's ChatGPT and&nbsp;Google's Gemini, has exploded in popularity. Startups like Anthropic and OpenAI, alongside tech giants such as&nbsp;<a href="https://www.cnbc.com/quotes/GOOG/">Google</a>,&nbsp;<a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a>,&nbsp;<a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a>&nbsp;and&nbsp;<a href="https://www.cnbc.com/quotes/META/">Meta</a>, are all part of a generative AI arms race to ensure they don't fall behind in a market&nbsp;<a href="https://www.bloomberg.com/professional/insights/data/generative-ai-races-toward-1-3-trillion-in-revenue-by-2032/#:~:text=Generative%20AI%20is%20poised%20to,our%20proprietary%20market%2Dsizing%20model." target="_blank">predicted to top $1 trillion</a>&nbsp;in revenue within a decade. </p><p>Some, like Microsoft and Amazon, are <a href="https://www.cnbc.com/2024/03/30/fomo-drives-tech-heavyweights-to-invest-billions-in-generative-ai-.html">backing generative AI startups with hefty investments</a> as well as working on in-house generative AI.</p><p>The partnership announced Friday will also allow AWS customers "early access" to an Anthropic feature: the ability for an AWS customer to do fine-tuning with their own data on Anthropic's Claude. It's a unique benefit for AWS customers, according to a company blog post.</p><p>In March, Amazon's $2.75 billion investment in Anthropic was the company's largest outside investment in its three-decade history. The companies announced an&nbsp;<a href="https://www.cnbc.com/2023/09/25/amazon-to-invest-up-to-4-billion-in-anthropic-a-rival-to-chatgpt-developer-openai.html">initial $1.25 billion investment</a>&nbsp;in September 2023.</p><p>Amazon does not have a seat on Anthropic's board.</p><p>News of Amazon's additional investment comes one month after <a href="https://www.cnbc.com/2024/10/22/anthropic-announces-ai-agents-for-complex-tasks-racing-openai.html">Anthropic announced</a> a significant milestone for the company: AI agents that can use a computer to complete complex tasks like a human would.</p><p>Anthropic's new Computer Use capability, part of its two newest AI models, allows its tech to interpret what's on a computer screen, select buttons, enter text, navigate websites, and execute tasks through any software and real-time internet browsing.</p><p>The tool can "use computers in basically the same way that we do," Jared Kaplan, Anthropic's chief science officer, told CNBC in an interview last month, adding it can do tasks with "tens or even hundreds of steps."</p><p>Amazon had early access to the tool, Anthropic told CNBC at the time, and early customers and beta testers included Asana, Canva and Notion. The company had been working on the tool since early this year, according to Kaplan.</p><p>In September, Anthropic&nbsp;<a href="https://www.cnbc.com/2024/09/04/amazon-backed-anthropic-rolls-out-claude-enterprise-ai-for-big-business.html">rolled out Claude Enterprise</a>, its biggest new product since its chatbot's debut, designed for businesses looking to integrate Anthropic's AI. In&nbsp;<a href="https://www.cnbc.com/2024/06/20/anthropic-claude-3point5-sonnet-ai-announced.html">June</a>, the company debuted its more powerful AI model, Claude 3.5 Sonnet, and in May, it rolled out its&nbsp;<a href="https://www.cnbc.com/2024/05/01/anthropic-iphone-ai-app-business-plan-to-compete-with-openai-announced.html">"Team" plan for smaller businesses</a>.</p><p>Last year, Google&nbsp;<a href="https://www.cnbc.com/2023/10/27/google-commits-to-invest-2-billion-in-openai-competitor-anthropic.html">committed</a>&nbsp;to invest $2 billion in Anthropic, after previously confirming it had taken a 10% stake in the startup alongside a large cloud contract between the two companies.</p></div><div id="SpecialReportArticle-RelatedContent-1"><h2>Don’t miss these insights from CNBC PRO</h2><div><ul><li><a href="https://www.cnbc.com/2024/11/14/warren-buffetts-berkshire-hathaway-takes-a-stake-in-dominos-pizza.html">Warren Buffett's Berkshire Hathaway takes a stake in Domino's Pizza</a></li><li><a href="https://www.cnbc.com/2024/11/17/wall-street-gears-up-for-ma-boom-these-names-could-be-attractive-targets.html">Wall Street is gearing up for an M&amp;A boom under Trump. These companies could be targets</a></li><li><a href="https://www.cnbc.com/2024/11/13/inflation-report-shows-market-could-have-a-recipe-for-disaster-heading-into-new-year-says-economist.html">Inflation report shows market could have a 'recipe for disaster' heading into new year, says economist</a></li><li><a href="https://www.cnbc.com/2024/11/18/morningstar-strategist-picks-2-stocks-from-a-sector-he-is-betting-on.html">Morningstar names cheap stocks in a sector that ‘deserves a place in everybody’s portfolio’</a></li><li><a href="https://www.cnbc.com/2024/11/18/these-2-active-etfs-have-outperformed-the-sp-500-this-year-last-year-and-over-5-years.html">These 2 active ETFs have outperformed the S&amp;P 500 this year, last year and over 5 years</a><br></li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Salmon return to lay eggs in historic habitat after dam removal project (244 pts)]]></title>
            <link>https://www.opb.org/article/2024/11/17/salmon-return-to-lay-eggs-in-historic-habitat-after-largest-dam-removal-project-in-us-history/</link>
            <guid>42213663</guid>
            <pubDate>Fri, 22 Nov 2024 13:27:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.opb.org/article/2024/11/17/salmon-return-to-lay-eggs-in-historic-habitat-after-largest-dam-removal-project-in-us-history/">https://www.opb.org/article/2024/11/17/salmon-return-to-lay-eggs-in-historic-habitat-after-largest-dam-removal-project-in-us-history/</a>, See on <a href="https://news.ycombinator.com/item?id=42213663">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h3><a href="https://www.opb.org/science_environment/">Science &amp; Environment</a></h3><h2>Less than a month after four towering dams on the Klamath River were demolished, hundreds of salmon made it into waters they have been cut off from for decades to spawn in cool creeks</h2></div><div><p>A giant female Chinook salmon flips on her side in the shallow water and wriggles wildly, using her tail to carve out a nest in the riverbed as her body glistens in the sunlight. In another moment, males butt into each other as they jockey for a good position to fertilize eggs.</p><p>These are scenes local tribes have dreamed of seeing for decades as they fought to bring down four hydroelectric dams blocking passage for struggling salmon along more than 400 miles (644 kilometers) of the Klamath River and its tributaries along the Oregon-California border.</p><p>Now, less than a month after those dams came down in the largest dam removal project in U.S. history, salmon are once more returning to spawn in cool creeks that have been cut off to them for generations. Video shot by the Yurok Tribe show that hundreds of salmon have made it to tributaries between the former Iron Gate and Copco dams, a hopeful sign for the <a href="https://apnews.com/article/klamath-dams-removal-california-oregon-river-salmon-44fefba145d74383aa70a68d50597299">newly freed waterway</a>.</p><p>“Seeing salmon spawning above the former dams fills my heart,” said Joseph L. James, chairman of the Yurok Tribe. “Our salmon are coming home. Klamath Basin tribes fought for decades to make this day a reality because our future generations deserve to inherit a healthier river from the headwaters to the sea.”</p><figure><picture><img src="https://opb-opb-prod.cdn.arcpublishing.com/resizer/v2/L4PZ6DW7WVFY7CAPQQYK4LFYBY.jpg?auth=dbf1036c45044667d8c89bc083e002747fbe8424784231f2f62fa4f20e53863d&amp;width=150" alt="FILE - Excess water spills over the top of a dam on the Lower Klamath River known as Copco 1 near Hornbrook, Calif."></picture><figcaption><p>FILE - Excess water spills over the top of a dam on the Lower Klamath River known as Copco 1 near Hornbrook, Calif.</p><p><em>Gillian Flaccus / AP</em></p></figcaption></figure><p>The Klamath River flows from its headwaters in southern Oregon and across the mountainous forests of northern California before it reaches the Pacific Ocean.</p><p>The completion of the hydroelectric dam removal project on Oct. 2 marked a <a href="https://apnews.com/article/klamath-dam-removal-completed-tribes-435b955f5bfdeaca82de66bfc6551ba1">major victory for local tribes</a>. Through protests, testimony and lawsuits, the tribes showcased the environmental devastation caused by the dams, especially to salmon, which were cut off from their historic habitat and dying in alarming numbers because of poor water-quality.</p><p>There have been lower concentrations of harmful algae blooms since the dam removal, Toz Soto, fisheries program manager with the Karuk Tribe, said during a press conference after the dams came down. In October, the water temperature during the day was an average of 8 degrees Celsius (14 degrees Fahrenheit) cooler compared to the same month over the last nine years, according to the Klamath River Renewal Corporation, the nonprofit entity created to oversee the project.</p><p>“All in all, the fish that came up this year were really healthy,” Soto said. “I didn’t see fish with bacterial infections and things like that, so water temperature’s already having an impact on the fishes’ health.”</p><p>The number of salmon that have quickly made it into previously inaccessible tributaries has also been encouraging. Experts have counted 42 redds, or salmon egg nests, and have tallied as many as 115 Chinook salmon in one day in Spencer Creek, which is above the former J.C. Boyle dam, the furthest upstream of the four removed dams, said Mark Hereford with the Oregon Department of Fish and Wildlife.</p><p>“They’re showing us where the good habitat is; they’re showing us where there’s a lack of habitat,” said Barry McCovey Jr, director of the Yurok Tribal Fisheries department. “So we can use these fish to inform us as river managers, as scientists, where restoration needs to take place.”</p><figure><picture><img src="https://opb-opb-prod.cdn.arcpublishing.com/resizer/v2/LGNPDXOGT5BVVFRWUYS2JOUJZM.jpg?auth=4044d38dcd2939dc685d1e8ed46985f104509dd24221fb95319420e09aed3ad1&amp;width=150" alt="FILE - A view shows the Copco 1 Dam in Hornbrook, Calif., Sunday, Sept. 17, 2023."></picture><figcaption><p>FILE - A view shows the Copco 1 Dam in Hornbrook, Calif., Sunday, Sept. 17, 2023.</p><p><em>Haven Daley / AP</em></p></figcaption></figure><p>Power company PacifiCorp built <a href="https://apnews.com/article/klamath-dam-california-removal-restoration-473a570024584c2e02837434e05693da">the dams</a> to generate electricity between 1918 and 1962. But the structures halted the natural flow of the waterway that was once known as the third-largest salmon-producing river on the West Coast. They disrupted the lifecycle of the region’s salmon, which spend most of their life in the Pacific Ocean but return to the chilly mountain streams to lay eggs.</p><p>At the same time, the dams only produced a fraction of PacifiCorp’s energy at full capacity, enough to power about 70,000 homes. They also didn’t provide irrigation, drinking water or flood control, according to Klamath River Renewal Corporation.</p><p>McCovey said the return of so many salmon happened faster than he had expected and makes him hopeful for the future of the river.</p><p>“Out of all the milestones that we’ve had, this one to me is the most significant,” he said. “It feels like catharsis. It feels like the right path.”</p><p>___</p><p><i>Associated Press reporter Sophie Austin contributed to this report.</i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A “meta-optics” camera that is the size of a grain of salt (178 pts)]]></title>
            <link>https://cacm.acm.org/news/a-camera-the-size-of-a-grain-of-salt-could-change-imaging-as-we-know-it/</link>
            <guid>42212992</guid>
            <pubDate>Fri, 22 Nov 2024 11:39:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cacm.acm.org/news/a-camera-the-size-of-a-grain-of-salt-could-change-imaging-as-we-know-it/">https://cacm.acm.org/news/a-camera-the-size-of-a-grain-of-salt-could-change-imaging-as-we-know-it/</a>, See on <a href="https://news.ycombinator.com/item?id=42212992">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en"><section id="sec1"><p id="p-1">When it comes to cameras, size matters, but not in the way you think.</p><p id="p-2">Any time a new smartphone is released, it is easy to drool over the latest, greatest, and biggest features that allow you to take even more stunning selfies composed of even more megapixels. However, in the world of cameras, smaller cameras could end up having a far greater impact on the world at large—and enable a ton of positive applications in society—than the next iPhone camera. Work from researchers at Princeton University and the University of Washington is pointing the way.</p><p id="p-3">A team of researchers from both institutions has published work that uses innovative methods and materials to create a “meta-optics” camera that is the size of a single grain of salt.</p><figure id="attachment_762770" aria-describedby="caption-attachment-762770"><a data-fslightbox="https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg" data-type="image" data-caption="" href="https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg">
				<img decoding="async" src="https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg?w=1024" alt="ultracompact camera system" width="1024" height="576" srcset="https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg 1200w, https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg?resize=300,169 300w, https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg?resize=768,432 768w, https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg?resize=1024,576 1024w" sizes="(max-width: 1024px) 100vw, 1024px">
			</a><figcaption id="caption-attachment-762770">The ultracompact camera system developed by researchers at Princeton University and the University of Washington relies on a technology called a metasurface, which is studded with 1.6 million cylindrical posts and can be produced much like a computer chip.</figcaption></figure><p id="p-4">The meta-optics camera is the first device of its kind to produce full-color images that are equal in quality to those produced by conventional cameras, which are an order of magnitude larger. In fact, the meta-optics camera is 500,000 times smaller than conventional cameras that capture the same level of image quality.</p><p id="p-5">The approach the researchers used to create this meta-optics camera’s small form factor is a huge deal.</p><p id="p-6">They used nano-structures called “metasurfaces” and novel approaches to hardware design to build a meta-optics camera far superior to past efforts, as well as implementing unique AI-powered image post-processing to create high-quality images from the camera.</p><p id="p-7">Their work is impressive on its own for breaking through past limitations of meta-optics imaging devices. Yet it is also notable because it opens the door to the creation of extremely small cameras that can create high-fidelity images for a range of industries and use-cases (for instance, by enabling the use of less-invasive medical imaging without compromising image quality).</p><p id="p-8">This work also unlocks the science-fiction-like possibilities of turning entire surfaces into cameras made up of thousands of such devices, and launching high-quality, ultra-light telescopes into space.</p><p id="p-9">Here’s how they did it—and why it could change the world of imaging as we know it.</p></section><section id="sec2"><h2>From conventional lenses to metasurfaces</h2><p id="p-10">All camera designers and engineers, no matter the type(s) of cameras they design, share the same challenge: they want to make their cameras as compact as possible while still allowing it to record as much light as possible.</p><p id="p-11">Smartphone cameras present a great example of the trade-offs inherent in solving this challenge. Each new smartphone packs more computational firepower into smaller and thinner frames, to the point where the newest generations of smartphones look positively futuristic. However, smartphone cameras are still obviously large and obtrusive on otherwise sleek smartphone frames because camera designers are packing more and more lenses into them so they can take higher-quality pictures.</p><p id="p-12">This means researchers are always on the hunt for ways to compress more optical power into smaller form factors, said Ethan Tseng, a researcher at Princeton who was part of the team that produced the salt-grain-sized meta-optics camera.</p><p id="p-13">“Metasurfaces have emerged as a promising candidate for performing this task,” Tseng said.</p><p id="p-14">A metasurface, Tseng explained, is an artificial, man-made material that allows us to affect light in unique ways. It is an ultrathin, flat surface just half a millimeter wide and studded with millions of cylindrical posts, which are called “nano-antennas.” These nano-antennas can be individually tuned by researchers to shape light in certain ways so that, together, they are capable of producing images just like standard refractive glass lenses—but in a device that is much, much smaller.</p><p id="p-15">“Using metasurfaces enables us to open a large design space of optics that we only hardly were able to access before with conventional refractive optics,” said Felix Heide, a Princeton professor who is the senior author of the study that produced the salt-grain-sized meta-optics camera.</p><p id="p-16">With a standard refractive lens, you can only really shape the surface of the lens and vary the material to get better results. However, with metasurfaces, researchers are able to modulate light at the sub-wavelength level, said Heide.</p><p id="p-17">In the salt-grain-sized camera, the research team was able to create a single metasurface that has more light-steering power than a traditional lens, dramatically reducing the overall size of the camera while still achieving similar results. The meta-optic lens itself is 0.5 millimeters in size, while the sensor is 1 millimeter in size, making the entire camera much, much smaller than traditional lenses.</p><p id="p-18">The researchers did not invent the concept of using metasurfaces for cameras, but they did determine how to make the approach work in a way that was actually useful in the real world. Meta-optics cameras have been designed before, but none of them can produce images that are of sufficient quality to deploy for imaging use cases.</p><p id="p-19">“Existing approaches have been unable to design a meta-optics camera that can capture crisp, wide-field-of-view full-color images,” said Tseng.</p><p id="p-20">The research team’s work changed that. Their meta-optics camera is the first high-quality, polarization-insensitive nano-optic imager for full-color, wide field-of-view imaging.</p><p id="p-21">“We addressed the shortcomings of previous meta-optics imaging systems through advances in both hardware design and software post-processing,” said Tseng. To do that, the researchers used artificial intelligence to address two challenges: lens design and image processing.</p><p id="p-22">First, the team used novel AI optimization algorithms to design the nano-antennas on the actual metasurface. Simulating the optical response of a metasurface and calculating the corresponding gradients can be quite computationally expensive, Tseng said, so the team created essentially fast “proxies” for metasurface physics that allowed them to compute how to design the metasurface very quickly.</p><p id="p-23">Then, a physics-based neural network was used to process the images captured by the meta-optics camera. Because the neural network was trained on metasurface physics, it can remove aberrations produced by the camera.</p><p id="p-24">“We were the first to treat the metasurface as an optimizable, differentiable layer that can perform computation with light,” said Heide. “This made it possible to effectively treat metasurfaces like layers in optical neural networks and piggyback on the large toolbox of AI to optimize these layers.”</p><p id="p-25">Finally, the metasurface physics simulator and the post-processing algorithm were combined into a single pipeline to fabricate the actual meta-optic camera, and then to reconstruct the images it captures into high-quality, full-color images.</p><p id="p-26">This innovative combination of hardware and software means that the researchers’ meta-optics camera produces images that could actually be used in real-world contexts, like medical imaging.</p><p id="p-27">“Only combined with computation were we able to explore this design space and make our lenses work for broadband applications,” said Heide.</p></section><section id="sec3"><h2>Better endoscopes, smartphone cameras, telescopes</h2><p id="p-28">The potential real-world applications of the research are vast.</p><p id="p-29">The most obvious one is medical imaging, which directly benefits from cameras that are as small as possible so as not to be invasive. “We are very excited about miniaturized optics in endoscopes, which could allow for novel non-invasive diagnosis and surgery,” said Heide.</p><p id="p-30">Ultra-compact endoscopes powered by a meta-optics camera could even image regions of the body that are difficult to reach with today’s technology.</p><p id="p-31">Another major area of interest for using meta-optics cameras—or cameras that incorporate meta-optics techniques—is consumer hardware. The ability to design cameras and lenses that are an order of magnitude smaller than those in devices today opens up exciting possibilities across smartphones, wearables, and augmented reality (AR) and virtual reality (VR) headsets.</p><p id="p-32">Your smartphone screen or the back of your phone itself could become a camera, says Heide. Wearables could bake high-quality cameras right into the surfaces of, say, eyeglasses. Or, VR headsets could become dramatically lighter and sleeker, leading to higher adoption and greater use of these devices on the go.</p><p id="p-33">Drones also could benefit from significantly smaller cameras. All drones require cameras of some type to perform their work, whether for military purposes like reconnaissance or civilian ones like order delivery. Much smaller cameras would result in far lighter drones that consume far less battery power, said Tseng.</p><p id="p-34">In fact, with a breakthrough like the meta-optics camera, the very nature of cameras can be rethought entirely.</p><p id="p-35">“Our tiny cameras have also recently allowed us to rethink large cameras as flat arrays of salt-grain cameras—effectively turning surfaces into cameras,” said Heide. Larger metasurfaces could even replace the lenses needed for telescopes, making it not only easier to build them but also to send more powerful lenses into space.</p><p id="p-36">While researchers are still in the early stages of brainstorming and engineering potential real-world applications for meta-optics cameras, the way in which metasurfaces are produced has them excited.</p><p id="p-37">“Metasurfaces are especially interesting because they can be made using the same mature technology used to produce computer chips,” said Tseng. Today’s computer chips are produced on wafers, and each wafer contains hundreds of identical copies of the chip. Metasurfaces are produced in an identical way, which holds the promise of greatly reducing the individual cost per metasurface produced, he said.</p><p id="p-38">Not to mention, while the exact materials used to make metasurfaces vary, the researchers used a silica wafer for their mounting surface and silicon nitride for their nano-antennas. Both materials are compatible with today’s semiconductor manufacturing techniques that pump out computer chips.</p><p id="p-39">This means going from sophisticated computer chips to meta-optics cameras might be easier than we think. If so, the picture for how to use these devices in many different industries could get much, much clearer.</p><h2 id="FurtherReading">Further Reading:</h2><ul id="reflist-1"><li></li><li></li><li></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A Marble Madness-inspired WebGL game we built for Netlify (340 pts)]]></title>
            <link>https://5-million-devs.netlify.com/</link>
            <guid>42212644</guid>
            <pubDate>Fri, 22 Nov 2024 10:31:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://5-million-devs.netlify.com/">https://5-million-devs.netlify.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42212644">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Rebels in the sky – Terminal game about space pirates (117 pts)]]></title>
            <link>https://github.com/ricott1/rebels-in-the-sky</link>
            <guid>42212071</guid>
            <pubDate>Fri, 22 Nov 2024 08:21:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ricott1/rebels-in-the-sky">https://github.com/ricott1/rebels-in-the-sky</a>, See on <a href="https://news.ycombinator.com/item?id=42212071">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Rebels in the Sky</h2><a id="user-content-rebels-in-the-sky" aria-label="Permalink: Rebels in the Sky" href="#rebels-in-the-sky"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description demo_v1.0.18.mp4">demo_v1.0.18.mp4</span>
    <span></span>
  </summary>

  <video src="https://github.com/user-attachments/assets/aaa02f04-06db-4da5-8fa4-732b60083e66" data-canonical-src="https://github.com/user-attachments/assets/aaa02f04-06db-4da5-8fa4-732b60083e66" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">It's the year 2101. Corporations have taken over the world.
The only way to be free is to join a pirate crew and start plundering the galaxy. The only mean of survival is to play basketball.</p>
<p dir="auto">Now it's your turn to go out there and make a name for yourself. Create your crew and start wandering the galaxy in search of worthy basketball opponents.</p>
<p dir="auto">The game is under heavy development and breaking changes are often introduced. If you can't continue an old game because the save file is invalid, you probably need to start a new one or open an issue to check if the save file can be migrated.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Just try it out!</h2><a id="user-content-just-try-it-out" aria-label="Permalink: Just try it out!" href="#just-try-it-out"></a></p>
<p dir="auto">Connect via SSH to try the game.</p>
<p dir="auto"><code>ssh rebels.frittura.org -p 3788</code></p>
<p dir="auto">Save files are deleted after 2 days of inactivity.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build</h3><a id="user-content-build" aria-label="Permalink: Build" href="#build"></a></p>
<p dir="auto">You need to have the rust toolchain installed --&gt; <a href="https://www.rust-lang.org/tools/install" rel="nofollow">https://www.rust-lang.org/tools/install</a>. Then you can clone the repo and build the game with</p>
<p dir="auto"><code>cargo build --release</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With cargo</h3><a id="user-content-with-cargo" aria-label="Permalink: With cargo" href="#with-cargo"></a></p>
<p dir="auto"><code>cargo install rebels</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">From the latest release page</h3><a id="user-content-from-the-latest-release-page" aria-label="Permalink: From the latest release page" href="#from-the-latest-release-page"></a></p>
<ul dir="auto">
<li>Download the latest release asset for your platform from <a href="https://rebels.frittura.org/" rel="nofollow">https://rebels.frittura.org</a>;</li>
<li>Give execution permissions to the executable with <code>chmod +x rebels</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Distro Packages</h3><a id="user-content-distro-packages" aria-label="Permalink: Distro Packages" href="#distro-packages"></a></p>
<details>
  <summary>Packaging status</summary>
<p dir="auto"><a href="https://repology.org/project/rebels-in-the-sky/versions" rel="nofollow"><img src="https://camo.githubusercontent.com/215eb0238498ff1dd9cb718b7e49c7e1ab1b8c2ea204728011c1ccb821c79ef2/68747470733a2f2f7265706f6c6f67792e6f72672f62616467652f766572746963616c2d616c6c7265706f732f726562656c732d696e2d7468652d736b792e737667" alt="Packaging status" data-canonical-src="https://repology.org/badge/vertical-allrepos/rebels-in-the-sky.svg"></a></p>
</details>
<p dir="auto"><h4 tabindex="-1" dir="auto">Arch Linux</h4><a id="user-content-arch-linux" aria-label="Permalink: Arch Linux" href="#arch-linux"></a></p>
<p dir="auto"><code>rebels-in-the-sky</code> can be installed from the <a href="https://archlinux.org/packages/extra/x86_64/rebels-in-the-sky/" rel="nofollow">official repositories</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pacman -S rebels-in-the-sky"><pre>pacman -S rebels-in-the-sky</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Run</h2><a id="user-content-run" aria-label="Permalink: Run" href="#run"></a></p>
<p dir="auto">This game runs as a terminal application, meaning that you just need to run the executable from your terminal with</p>
<p dir="auto"><code>./rebels</code></p>
<p dir="auto">Suggested minimal terminal size: 160x48. Not all terminals support the game colors nicely, so you might need to try different ones. Here is a list of tested terminals:</p>
<ul dir="auto">
<li>Linux: whatever the default terminal is, it should work</li>
<li>MacOs: <a href="https://iterm2.com/" rel="nofollow">iTerm2</a>, <a href="https://tabby.sh/" rel="nofollow">tabby</a>, <a href="https://wezfurlong.org/wezterm/index.html" rel="nofollow">WezTerm</a></li>
<li>Windows: <a href="https://tabby.sh/" rel="nofollow">tabby</a></li>
</ul>
<p dir="auto"><strong>Important</strong>: currently local bot teams are generated by default to make the game more enjoyable. This behaviour can be disabled by passing the <code>-f</code> flag to the executable. In the future, when more players will be available, the game will default to online teams only.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Music</h2><a id="user-content-music" aria-label="Permalink: Music" href="#music"></a></p>
<p dir="auto">Previous versions had the option to play music directly in the game, but this was removed to reduce the binary size and now music is streamed from internet radios. Nevertheless, you can still listen to the game soundtrack directly by connecting to <code>https://radio.frittura.org/rebels.ogg</code>!</p>
<p dir="auto">You can add more radio stations by including them in <code>assets/data/stream_data.json</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<ul dir="auto">
<li>Planet gifs were generated using the <a href="https://deep-fold.itch.io/pixel-planet-generator" rel="nofollow">pixel planet generator</a> by <a href="https://deep-fold.itch.io/" rel="nofollow">Deep Fold</a>.</li>
<li>Special thanks to <a href="https://www.ildeposito.org/" rel="nofollow">Il Deposito</a> for inspiration and the great musical archive.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contribution</h2><a id="user-content-contribution" aria-label="Permalink: Contribution" href="#contribution"></a></p>
<p dir="auto">Join the <a href="https://discord.gg/ebjp33UrrV" rel="nofollow">discord</a>! There is no fixed roadmap for the game yet, anyone is welcome to participate with ideas.</p>
<p dir="auto">It is almost guaranteed that you will encounter bugs along your journey. If you do, please open an issue and describe what happened. If you are a developer and want to contribute, feel free to open a pull request.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This software is released under the <a href="https://www.gnu.org/licenses/gpl-3.0.en.html" rel="nofollow">GPLv3</a> license.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Story of the two thousand stolen Playdate handhelds (172 pts)]]></title>
            <link>https://podcast.play.date/episodes/s01e31/</link>
            <guid>42211689</guid>
            <pubDate>Fri, 22 Nov 2024 06:53:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://podcast.play.date/episodes/s01e31/">https://podcast.play.date/episodes/s01e31/</a>, See on <a href="https://news.ycombinator.com/item?id=42211689">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<article id="s01e31">
							<header>
							
							<p><time datetime="2024-11-19T07:30:00.000Z">
									Tuesday, November 19, 2024
								</time>

								•

								54 minutes
							</p>
						</header>


	


	<p><audio controls="">
			<source src="https://download.panic.com/playdate_podcast/PlaydatePodcast-s01e31-True-Crime-Edition.mp3" type="audio/mpeg">

			Your browser does not support the <code>audio</code> element.
		</audio>
	</p>

	<p>Earlier this year, our Financial Controller, Jen, realized our Playdate inventory was 2,000 units short. How did that eventually lead us to a Circle K in North Las Vegas, and just how much should you tip for a roofing consultation, anyway?
Buckle up, because we are going for a ride—in Magnum P.I.'s cool car.</p>
<h2 id="show-notes" tabindex="-1">Show Notes</h2>
<ul>
<li><a href="https://gdcvault.com/play/1034707/The-Playdate-Story-What-Was">Cabel’s GDC Talk</a></li>
<li><a href="https://en.wikipedia.org/wiki/Magnum%2C_P.I.">Magnum, P.I.</a></li>
<li><a href="https://en.wikipedia.org/wiki/Froster">Circle K Froster</a></li>
<li><a href="https://podcast.play.date/episodes/s01e31/">Episode page with photos</a></li>
<li><a href="https://podcast.play.date/episodes/s01e31/transcript/">Episode transcript</a></li>
</ul>



	


	
	
	

<a href="https://podcast.play.date/episodes/s01e31/">Listen Now</a></article>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple will now be treated like a bank (102 pts)]]></title>
            <link>https://9to5mac.com/2024/11/21/apple-will-now-be-treated-like-a-bank-says-us-consumer-financial-protection-bureau/</link>
            <guid>42211525</guid>
            <pubDate>Fri, 22 Nov 2024 06:06:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5mac.com/2024/11/21/apple-will-now-be-treated-like-a-bank-says-us-consumer-financial-protection-bureau/">https://9to5mac.com/2024/11/21/apple-will-now-be-treated-like-a-bank-says-us-consumer-financial-protection-bureau/</a>, See on <a href="https://news.ycombinator.com/item?id=42211525">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1500" height="750" src="https://9to5mac.com/wp-content/uploads/sites/6/2024/11/Apple-will-now-be-treated-like-a-bank-says-US-Consumer-Financial-Protection-Bureau.webp?w=1500" alt="Apple will now be treated like a bank, says US&nbsp;Consumer Financial Protection Bureau | In-store Apple Pay transaction on Square terminal" srcset="https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2024/11/Apple-will-now-be-treated-like-a-bank-says-US-Consumer-Financial-Protection-Bureau.webp?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2024/11/Apple-will-now-be-treated-like-a-bank-says-US-Consumer-Financial-Protection-Bureau.webp?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2024/11/Apple-will-now-be-treated-like-a-bank-says-US-Consumer-Financial-Protection-Bureau.webp?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2024/11/Apple-will-now-be-treated-like-a-bank-says-US-Consumer-Financial-Protection-Bureau.webp?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>The popularity of <a href="https://9to5mac.com/guides/apple-pay/" target="_blank" rel="noreferrer noopener">Apple Pay</a> will now see the Cupertino company regulated by the US&nbsp;Consumer Financial Protection Bureau (CFPB), a watchdog whose role is normally limited to banks and financial services companies.</p>



<p>The decision means that the bureau will have the power to monitor and regulate <a href="https://9to5mac.com/guides/aapl/" target="_blank" rel="noreferrer noopener">Apple’s</a> policies and practices in regard to its mobile wallet services …</p>



<h2 id="h-the-consumer-financial-protection-bureau">The&nbsp;Consumer Financial Protection Bureau</h2>



<p>The CFPB is a US agency responsible for enforcing federal consumer financial law, but also has a broader role as a regulator to ensure that consumer financial products are “fair, transparent, and competitive.”</p>



<blockquote>
<p>We aim to make consumer financial markets work for consumers, responsible providers, and the economy as a whole. We protect consumers from unfair, deceptive, or abusive practices and take action against companies that break the law. We arm people with the information, steps, and tools that they need to make smart financial decisions.</p>
</blockquote>



<p>It was always able to ensure that mobile wallet services like Apple Pay and Google Pay complied with the law, but last year proposed that these services be treated much more like banks, giving the CFPB broader powers to enforce fairness and deal with consumer complaints.</p>



<h2 id="h-apple-pay-will-be-regulated-from-next-month">Apple Pay will be regulated from next month</h2>



<p><em><a href="https://www.bloomberg.com/news/articles/2024-11-21/apple-pay-other-tech-firms-come-under-cfpb-regulatory-oversight?embedded-checkout=true" target="_blank" rel="noreferrer noopener">Bloomberg</a></em> reports that the proposal has been finalized, and will take effect from next month.</p>



<blockquote>
<p>The top US consumer watchdog will supervise&nbsp;<a href="https://www.bloomberg.com/quote/AAPL:US" target="_blank" rel="noreferrer noopener">Apple Inc.</a>&nbsp;and other major technology firms that offer digital wallets and payment apps, finalizing a proposal from last year with several changes.</p>



<p>The US&nbsp;Consumer Financial Protection Bureau&nbsp;will now treat those companies more like banks as long as they handle more than 50 million transactions a year, conducted in US dollars, according to a statement Thursday.&nbsp;</p>
</blockquote>



<p>The agency’s director says the decision was made because mobile wallet services are now an integral part of people’s financial lives.</p>



<blockquote>
<p>“Digital payments have gone from novelty to necessity and our oversight must reflect this reality,” CFPB Director&nbsp;Rohit Choprasaid in the statement.</p>
</blockquote>



<p>More than 60% of the US population now uses a mobile wallet, with Apple Pay the most popular choice.</p>



<h2 id="h-9to5mac-s-take">9to5Mac’s Take</h2>



<p>Apple typically doesn’t change its policies to address legislative concerns until it is forced to do so in each of the countries and regions in which it operates, but on this occasion chose to act ahead of time.</p>



<p>The European Union required Apple to open up access to the NFC payment chip to banks and payment card companies, and it was likely that the CFPB would have imposed the same requirement on the company. Instead of limiting the change to the EU, the iPhone maker made the change globally, getting ahead of the game.</p>



<p>It’s more than a decade <a href="https://9to5mac.com/2013/09/17/why-touch-id-is-bigger-news-than-any-of-us-appreciated/" target="_blank" rel="noreferrer noopener">since I first speculated</a> that <a href="https://9to5mac.com/2015/06/22/opinion-apple-bank/" target="_blank" rel="noreferrer noopener">Apple may end up becoming a bank</a>. While that hasn’t happened yet, we have seen <a href="https://9to5mac.com/2023/05/02/apple-bank/" target="_blank" rel="noreferrer noopener">significant movement in this direction</a>. It already had to obtain banking licenses to <a href="https://9to5mac.com/guides/apple-pay-later/" target="_blank" rel="noreferrer noopener">launch Apple Pay Later</a>, though it later <a href="https://9to5mac.com/2024/06/17/apple-pay-later-united-states-ending/" target="_blank" rel="noreferrer noopener">withdrew the service</a> when it seemed likely to be subject to <a href="https://9to5mac.com/2024/06/19/apple-pay-later-withdrawal-reason/" target="_blank" rel="noreferrer noopener">even more regulation</a>. Today’s CFPB announcement means that whatever labels Apple chooses to use, Apple Pay will now be subject to bank-like regulatory oversight.</p>



<p><em>Photo by&nbsp;<a href="https://unsplash.com/@christiannkoepke?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" target="_blank" rel="noreferrer noopener">Christiann Koepke</a>&nbsp;on&nbsp;<a href="https://unsplash.com/photos/person-browsing-on-white-monitor-WiE01mC9AtY?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" target="_blank" rel="noreferrer noopener">Unsplash</a></em></p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBggKMLOFATDAGg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Mac to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon S3 now supports the ability to append data to an object (187 pts)]]></title>
            <link>https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3-express-one-zone-append-data-object/</link>
            <guid>42211280</guid>
            <pubDate>Fri, 22 Nov 2024 04:46:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3-express-one-zone-append-data-object/">https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3-express-one-zone-append-data-object/</a>, See on <a href="https://news.ycombinator.com/item?id=42211280">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="aws-page-content" data-page-alert-target="true"> 
   <main id="aws-page-content-main" role="main" tabindex="-1"> 
    <div data-eb-tpl-root="" data-reactroot="" data-eb-tpl-n="awsm-whats-new/whats-new-post" data-eb-tpl-v="1.0.0" data-eb-ce="" data-eb-c-scope="d105f9bc-63d3-11ee-8c99-0242ac120002" data-eb-d-scope="DIRECTORIES" data-eb-locale="en-US" data-eb-1e70fe18="" data-eb-ssr-ce="" data-eb-tpl-ns="awsmWhatsNew" data-eb-slot="d105f9bc-63d3-11ee-8c99-0242ac120002" data-eb-slot-meta="{'version':'1.0','slotId':'d105f9bc-63d3-11ee-8c99-0242ac120002','experienceId':'d105f9bc-63d3-11ee-8c99-0242ac120002','allowBlank':false,'hasAltExp':false,'isRTR':false,'filters':{'limit':1,'query':'id \u003d \'v1578391091\''}}"> 
         <main> 
           
           
          <div><p>Amazon S3 Express One Zone now supports the ability to append data to an object. For the first time, applications can add data to an existing object in S3.</p><p>  Applications that continuously receive data over a period of time need the ability to add data to existing objects. For example, log-processing applications continuously add new log entries to the end of existing log files. Similarly, media-broadcasting applications add new video segments to video files as they are transcoded and then immediately stream the video to viewers. Previously, these applications needed to combine data in local storage before copying the final object to S3. Now, applications can directly append new data to existing objects and then immediately read the object, all within S3 Express One Zone.</p><p>  You can append data to objects in S3 Express One Zone in all AWS Regions where the storage class is <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-Endpoints.html" target="_blank">available</a>. You can get started using the AWS SDK, the AWS CLI, or Mountpoint for Amazon S3 (version 1.12.0 or higher). To learn more, visit the <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-buckets-objects-append.html" target="_blank">S3 User Guide</a>.</p></div> 
         </main> 
        </div> 
   </main> 
  </div><div data-lb-comp="modal" data-lb-modal-id="ie-deprecation-msg" data-ie10-deprecation-msg="You are using an outdated browser. Please upgrade to a modern browser to improve your experience."> 
      
     <p>
       AWS support for Internet Explorer ends on 07/31/2022. Supported browsers are Chrome, Firefox, Edge, and Safari. 
      <a href="https://aws.amazon.com/blogs/aws/heads-up-aws-support-for-internet-explorer-11-is-ending/" rel="noopener">Learn more »</a> 
     </p> 
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Autoflow, a Graph RAG based and conversational knowledge base tool (193 pts)]]></title>
            <link>https://github.com/pingcap/autoflow</link>
            <guid>42210689</guid>
            <pubDate>Fri, 22 Nov 2024 02:42:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pingcap/autoflow">https://github.com/pingcap/autoflow</a>, See on <a href="https://news.ycombinator.com/item?id=42210689">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">autoflow</h2><a id="user-content-autoflow" aria-label="Permalink: autoflow" href="#autoflow"></a></p>
  <p><a href="https://www.pingcap.com/tidb-cloud-serverless/?utm_source=tidb.ai&amp;utm_medium=community" rel="nofollow">
    <img src="https://raw.githubusercontent.com/pingcap/tidb.ai/main/frontend/app/public/nextra/icon-dark.svg" alt="TiDB.AI" width="100" height="100">
  </a>
</p></div>
<p dir="auto"><a href="https://hub.docker.com/r/tidbai/backend" rel="nofollow"><img src="https://camo.githubusercontent.com/d98265459cd0fa737dff9bcbe4f5790f3f775a570ed79391f941dfe644ad60bb/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f762f7469646261692f6261636b656e643f736f72743d73656d76657226617263683d616d643634266c6162656c3d7469646261692532466261636b656e6426636f6c6f723d626c7565266c6f676f3d66617374617069" alt="Backend Docker Image Version" data-canonical-src="https://img.shields.io/docker/v/tidbai/backend?sort=semver&amp;arch=amd64&amp;label=tidbai%2Fbackend&amp;color=blue&amp;logo=fastapi"></a>
<a href="https://hub.docker.com/r/tidbai/frontend" rel="nofollow"><img src="https://camo.githubusercontent.com/c12a65b263504840fe27d881a4f115e82c5dfbc2a0c6bf6823c68dfcb7e7602a/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f762f7469646261692f66726f6e74656e643f736f72743d73656d76657226617263683d616d643634266c6162656c3d74696462616925324666726f6e74656e642626636f6c6f723d626c7565266c6f676f3d6e6578742e6a73" alt="Frontend Docker Image Version" data-canonical-src="https://img.shields.io/docker/v/tidbai/frontend?sort=semver&amp;arch=amd64&amp;label=tidbai%2Ffrontend&amp;&amp;color=blue&amp;logo=next.js"></a>
<a href="https://tidb-ai-playwright.vercel.app/" rel="nofollow"><img src="https://camo.githubusercontent.com/93eef9bffb5e2de940b77726ad4647576bc184b08cace8b639643438108e1418/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636865636b2d72756e732f70696e676361702f746964622e61692f6d61696e3f6e616d6546696c7465723d45324525323054657374266c6162656c3d653265" alt="E2E Status" data-canonical-src="https://img.shields.io/github/check-runs/pingcap/tidb.ai/main?nameFilter=E2E%20Test&amp;label=e2e"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">An open source GraphRAG (Knowledge Graph) built on top of <a href="https://www.pingcap.com/ai?utm_source=tidb.ai&amp;utm_medium=community" rel="nofollow">TiDB Vector</a> and <a href="https://github.com/run-llama/llama_index">LlamaIndex</a> and <a href="https://github.com/stanfordnlp/dspy">DSPy</a>.</p>
<ul dir="auto">
<li><strong>Live Demo</strong>: <a href="https://tidb.ai/" rel="nofollow">TiDB.AI</a></li>
<li><strong>Documentation</strong>: <a href="https://tidb.ai/docs/?utm_source=github&amp;utm_medium=tidb.ai" rel="nofollow">Docs</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Perplexity-style Conversational Search page</strong>: Our platform features an advanced built-in website crawler, designed to elevate your browsing experience. This crawler effortlessly navigates official and documentation sites, ensuring comprehensive coverage and streamlined search processes through sitemap URL scraping.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/1237528/341611735-9cc87d32-14ac-47c6-b664-efa7ec53e751.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIyNzUzMDIsIm5iZiI6MTczMjI3NTAwMiwicGF0aCI6Ii8xMjM3NTI4LzM0MTYxMTczNS05Y2M4N2QzMi0xNGFjLTQ3YzYtYjY2NC1lZmE3ZWM1M2U3NTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjMyMWVhMmUyOTExYTMyYzk5YjA5NGQ2YWIyZDNmMjNiMDBiNGJhZDZmYzRlYmIxYjM1MzU4M2RmMTBlNWEyYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.FenzZ_CnBmWquNEcDABbTau2CGViA66cBxkARP7wQIk"><img src="https://private-user-images.githubusercontent.com/1237528/341611735-9cc87d32-14ac-47c6-b664-efa7ec53e751.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIyNzUzMDIsIm5iZiI6MTczMjI3NTAwMiwicGF0aCI6Ii8xMjM3NTI4LzM0MTYxMTczNS05Y2M4N2QzMi0xNGFjLTQ3YzYtYjY2NC1lZmE3ZWM1M2U3NTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjMyMWVhMmUyOTExYTMyYzk5YjA5NGQ2YWIyZDNmMjNiMDBiNGJhZDZmYzRlYmIxYjM1MzU4M2RmMTBlNWEyYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.FenzZ_CnBmWquNEcDABbTau2CGViA66cBxkARP7wQIk" alt="out-of-box-conversational-search" title="Image Title"></a></p>
<p dir="auto">You can even edit the Knowledge Graph to add more information or correct any inaccuracies. This feature is particularly useful for enhancing the search experience and ensuring that the information provided is accurate and up-to-date.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/1237528/341612004-7bc57b34-99b7-4c4b-a098-9ad33dd0dfdc.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIyNzUzMDIsIm5iZiI6MTczMjI3NTAwMiwicGF0aCI6Ii8xMjM3NTI4LzM0MTYxMjAwNC03YmM1N2IzNC05OWI3LTRjNGItYTA5OC05YWQzM2RkMGRmZGMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmRkYjQ0MzY4ZTU4YzBmODQ4ZTczNmYxZjUzMDI3MGM2OTZkMGY3YjA3NTJiZTVjZmNmOGZiZTcwM2M4NmM2OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.BSZKuaDcOlT0SAT00rvvKyikxrqG3qqC0YBIYwSdKJM"><img src="https://private-user-images.githubusercontent.com/1237528/341612004-7bc57b34-99b7-4c4b-a098-9ad33dd0dfdc.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIyNzUzMDIsIm5iZiI6MTczMjI3NTAwMiwicGF0aCI6Ii8xMjM3NTI4LzM0MTYxMjAwNC03YmM1N2IzNC05OWI3LTRjNGItYTA5OC05YWQzM2RkMGRmZGMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmRkYjQ0MzY4ZTU4YzBmODQ4ZTczNmYxZjUzMDI3MGM2OTZkMGY3YjA3NTJiZTVjZmNmOGZiZTcwM2M4NmM2OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.BSZKuaDcOlT0SAT00rvvKyikxrqG3qqC0YBIYwSdKJM" alt="out-of-box-conversational-search" title="Image Title"></a></p>
</li>
<li>
<p dir="auto"><strong>Embeddable JavaScript Snippet</strong>: Integrate our conversational search window effortlessly into your website by copying and embedding a simple JavaScript code snippet. This widget, typically placed at the bottom right corner of your site, facilitates instant responses to product-related queries.</p>
</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/1237528/322688872-5a445231-a27a-4ae6-8287-a4f8cf7b64d0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIyNzUzMDIsIm5iZiI6MTczMjI3NTAwMiwicGF0aCI6Ii8xMjM3NTI4LzMyMjY4ODg3Mi01YTQ0NTIzMS1hMjdhLTRhZTYtODI4Ny1hNGY4Y2Y3YjY0ZDAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NjZhNDRkYThhMDI4M2E3YzljZTA4YTE5MGRmMTE5MjE2NzYxMGY5NDM4ODM4NGUyNWQxZGE4NWEyODM1MWIxYiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.azwCKcbHBK2ZfPrOUUshHWXB7LTg8-T1EpjssOpl_qo"><img src="https://private-user-images.githubusercontent.com/1237528/322688872-5a445231-a27a-4ae6-8287-a4f8cf7b64d0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIyNzUzMDIsIm5iZiI6MTczMjI3NTAwMiwicGF0aCI6Ii8xMjM3NTI4LzMyMjY4ODg3Mi01YTQ0NTIzMS1hMjdhLTRhZTYtODI4Ny1hNGY4Y2Y3YjY0ZDAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NjZhNDRkYThhMDI4M2E3YzljZTA4YTE5MGRmMTE5MjE2NzYxMGY5NDM4ODM4NGUyNWQxZGE4NWEyODM1MWIxYiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.azwCKcbHBK2ZfPrOUUshHWXB7LTg8-T1EpjssOpl_qo" alt="embeddable-javascript-snippet" title="Image Title"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deploy</h2><a id="user-content-deploy" aria-label="Permalink: Deploy" href="#deploy"></a></p>
<ul dir="auto">
<li><a href="https://tidb.ai/docs/deploy-with-docker" rel="nofollow">Deploy with Docker Compose</a> (with: 4 CPU cores and 8GB RAM)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tech Stack</h2><a id="user-content-tech-stack" aria-label="Permalink: Tech Stack" href="#tech-stack"></a></p>
<ul dir="auto">
<li><a href="https://www.pingcap.com/ai?utm_source=tidb.ai&amp;utm_medium=community" rel="nofollow">TiDB</a> – Database to store chat history, vector, json, and analytic</li>
<li><a href="https://www.llamaindex.ai/" rel="nofollow">LlamaIndex</a> - RAG framework</li>
<li><a href="https://github.com/stanfordnlp/dspy">DSPy</a> - The framework for programming—not prompting—foundation models</li>
<li><a href="https://nextjs.org/" rel="nofollow">Next.js</a> – Framework</li>
<li><a href="https://ui.shadcn.com/" rel="nofollow">shadcn/ui</a> - Design</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact Us</h2><a id="user-content-contact-us" aria-label="Permalink: Contact Us" href="#contact-us"></a></p>
<p dir="auto">You can reach out to us on <a href="https://twitter.com/TiDB_Developer" rel="nofollow">@TiDB_Developer</a> on Twitter.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome contributions from the community. If you are interested in contributing to the project, please read the <a href="https://github.com/pingcap/autoflow/blob/main/CONTRIBUTING.md">Contributing Guidelines</a>.</p>
<a href="https://next.ossinsight.io/widgets/official/compose-last-28-days-stats?repo_id=752946440" rel="nofollow">
  <themed-picture data-catalyst-inline="true"><picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/7688e1e5672b8d311d3d1627d88f4d75c78c16cd81bc2192dc4d5b8046ee909e/68747470733a2f2f6e6578742e6f7373696e73696768742e696f2f776964676574732f6f6666696369616c2f636f6d706f73652d6c6173742d32382d646179732d73746174732f7468756d626e61696c2e706e673f7265706f5f69643d37353239343634343026696d6167655f73697a653d6175746f26636f6c6f725f736368656d653d6461726b" width="655" height="auto" data-canonical-src="https://next.ossinsight.io/widgets/official/compose-last-28-days-stats/thumbnail.png?repo_id=752946440&amp;image_size=auto&amp;color_scheme=dark">
    <img alt="Performance Stats of pingcap/autoflow - Last 28 days" src="https://camo.githubusercontent.com/aaab798344e1bc436f9d385a67ad0cf4c10b4491ea69b2e1b632042848c559b4/68747470733a2f2f6e6578742e6f7373696e73696768742e696f2f776964676574732f6f6666696369616c2f636f6d706f73652d6c6173742d32382d646179732d73746174732f7468756d626e61696c2e706e673f7265706f5f69643d37353239343634343026696d6167655f73697a653d6175746f26636f6c6f725f736368656d653d6c69676874" width="655" height="auto" data-canonical-src="https://next.ossinsight.io/widgets/official/compose-last-28-days-stats/thumbnail.png?repo_id=752946440&amp;image_size=auto&amp;color_scheme=light">
  </picture></themed-picture>
</a>

<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">TiDB.AI is open-source under the Apache License, Version 2.0. You can <a href="https://github.com/pingcap/autoflow/blob/main/LICENSE.txt">find it here</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tailwind CSS v4.0 Beta 1 (157 pts)]]></title>
            <link>https://tailwindcss.com/blog/tailwindcss-v4-beta</link>
            <guid>42210553</guid>
            <pubDate>Fri, 22 Nov 2024 02:12:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tailwindcss.com/blog/tailwindcss-v4-beta">https://tailwindcss.com/blog/tailwindcss-v4-beta</a>, See on <a href="https://news.ycombinator.com/item?id=42210553">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>About eight months ago we <a href="https://tailwindcss.com/blog/tailwindcss-v4-alpha">open-sourced our progress</a> on Tailwind CSS v4.0. Hundreds of hours of fixing bugs, soul-crushing backward compatibility work, and troubleshooting Windows CI failures later, I’m excited to finally tag the first public beta release.</p>
<p>As I talked about when we published the first alpha, Tailwind CSS v4.0 is an all-new engine built for performance, and designed for the modern web.</p>
<ul role="list">
<li><strong>Built for performance</strong> — full builds in the new engine are up to 5x faster, and incremental builds are over 100x faster — and measured in microseconds.</li>
<li><strong>Unified toolchain</strong> — built-in import handling, vendor prefixing, and syntax transforms, with no additional tooling required.</li>
<li><strong>CSS-first configuration</strong> — a reimagined developer experience where you customize and extend the framework directly in CSS instead of a JavaScript configuration file.</li>
<li><strong>Designed for the modern web</strong> — built on native cascade layers, wide-gamut colors, and including first-class support for modern CSS features like container queries, <code>@starting-style</code>, popovers, and more.</li>
</ul>
<p>There’s so much more to say, but everything you need to get started is in the new beta documentation we published today:</p>
<p><a href="https://tailwindcss.com/docs/v4-beta">Get started with Tailwind CSS v4.0 Beta 1 →</a></p>
<p>Start building and help us bullet-proof this thing for the stable release early in the new year.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mechanically strong yet metabolizable plastic breaks down in seawater (108 pts)]]></title>
            <link>https://www.science.org/doi/abs/10.1126/science.ado1782?af=R</link>
            <guid>42210528</guid>
            <pubDate>Fri, 22 Nov 2024 02:07:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/doi/abs/10.1126/science.ado1782?af=R">https://www.science.org/doi/abs/10.1126/science.ado1782?af=R</a>, See on <a href="https://news.ycombinator.com/item?id=42210528">Hacker News</a></p>
Couldn't get https://www.science.org/doi/abs/10.1126/science.ado1782?af=R: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[What's Next for WebGPU (176 pts)]]></title>
            <link>https://developer.chrome.com/blog/next-for-webgpu</link>
            <guid>42209272</guid>
            <pubDate>Thu, 21 Nov 2024 22:18:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.chrome.com/blog/next-for-webgpu">https://developer.chrome.com/blog/next-for-webgpu</a>, See on <a href="https://news.ycombinator.com/item?id=42209272">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  
    




<div translate="no">
  
    
    
      <div>
        
          <p><img alt="François Beaufort" src="https://web.dev/images/authors/beaufortfrancois.jpg" decoding="async" height="64" loading="lazy" width="64"></p>
      </div>
    
  
    
    
      <div>
        
          <p><img alt="Corentin Wallez" src="https://web.dev/images/authors/cwallez.jpg" decoding="async" height="64" loading="lazy" width="64"></p>
      </div>
    
  
</div>

<p>
  Published: November 21, 2024
</p>


<p>The <a href="https://gpuweb.github.io/gpuweb/">WebGPU specification</a> is always evolving, with major companies like Google, Mozilla, Apple, Intel, and Microsoft meeting weekly to discuss its development. The most recent GPU for the Web working group meeting offered a glimpse into the main objectives and features planned for the next iteration of WebGPU. This blog post explores some of the key takeaways from the meeting.</p>

<h2 id="reaching_candidate_recommendation_status" data-text="Reaching candidate recommendation status" tabindex="-1">Reaching candidate recommendation status</h2>

<p>A major focus of the meeting was to discuss the progress of <a href="https://github.com/gpuweb/gpuweb/milestone/2">Milestone 0</a> and to finalize the issues that need to be addressed before it can reach candidate recommendation status for the W3C. This is the next step in the standardization process, and it comes with stronger guarantees of stability and intellectual property protection.</p>

<p>There was general agreement among the meeting participants that they are no blockers and that these issues can be resolved in a timely manner, paving the way for the W3C candidate recommendation of WebGPU.</p>

<h2 id="prioritizing_new_features" data-text="Prioritizing new features" tabindex="-1">Prioritizing new features</h2>

<p>Participants at the meeting also prioritized new features. They started with a list of feature requests compiled from feedback from developers, implementers, and stakeholders.</p>

<figure>
  <img src="https://developer.chrome.com/static/blog/next-for-webgpu/image/whiteboard.jpg" alt="A whiteboard with a handwritten list of terms related to computer graphics APIs, including 'Bindless,' 'Subgroups', 'MDI,' 'Push Constants,' 'UMA,' 'Subgroup Matrix,' and others." width="1600" height="899" srcset="https://developer.chrome.com/static/blog/next-for-webgpu/image/whiteboard_36.jpg 36w,https://developer.chrome.com/static/blog/next-for-webgpu/image/whiteboard_48.jpg 48w,https://developer.chrome.com/static/blog/next-for-webgpu/image/whiteboard_72.jpg 72w,https://developer.chrome.com/static/blog/next-for-webgpu/image/whiteboard_96.jpg 96w,https://developer.chrome.com/static/blog/next-for-webgpu/image/whiteboard_480.jpg 480w,https://developer.chrome.com/static/blog/next-for-webgpu/image/whiteboard_720.jpg 720w,https://developer.chrome.com/static/blog/next-for-webgpu/image/whiteboard_856.jpg 856w,https://developer.chrome.com/static/blog/next-for-webgpu/image/whiteboard_960.jpg 960w,https://developer.chrome.com/static/blog/next-for-webgpu/image/whiteboard_1440.jpg 1440w,https://developer.chrome.com/static/blog/next-for-webgpu/image/whiteboard_1920.jpg 1920w,https://developer.chrome.com/static/blog/next-for-webgpu/image/whiteboard_2880.jpg 2880w" sizes="(max-width: 840px) 100vw, 856px">
</figure>

<p>After discussion, the following key WebGPU features for AI were identified:</p>

<ul>
<li><p><strong>Subgroups and subgroup matrices</strong>: Let application use fast local communication between GPU threads, and take advantage of fixed-size matrix multiplication hardware next to shader cores. See <a href="https://github.com/gpuweb/gpuweb/blob/main/proposals/subgroups.md">the subgroups proposal</a>.</p></li>
<li><p><strong>Texel buffers</strong>: Provide a more efficient way to store and access small data types, like 16-bit or 8-bit values, in a portable way. This is important for some ML image processing algorithms. See <a href="https://docs.google.com/presentation/d/1XR6TbSuJpZ04UA6Q7cwllJNabSagpUE7lZC3dJAkx-0/edit#slide=id.g30fa23859c2_0_0">the texel buffer slides</a>.</p></li>
<li><p><strong>UMA buffer mapping</strong>: Improve data upload performance by reducing or eliminating copies and synchronization overhead. See <a href="https://github.com/gpuweb/gpuweb/issues/2388">the spec issue 2388</a>.</p></li>
</ul>

<p>Also under consideration and prioritization are the following WebGPU features to unlock new kinds of rendering algorithms:</p>

<ul>
<li><p><strong>Bindless</strong>: This highly anticipated <a href="https://hackmd.io/PCwnjLyVSqmLfTRSqH0viA?view">feature proposal</a> is a prerequisite for most leading-edge rendering algorithms because they need scene-wide information. Bindless lets shaders use an unlimited number of resources, including textures, when compared to the relatively strict limits currently.</p></li>
<li><p><strong>Multi-draw indirect</strong>: Lets previous computations on the GPU created multiple draws instead of just one with <code translate="no" dir="ltr">drawIndirect</code> previously. It is an important capability for GPU-driven rendering like for GPU culling of objects. See <a href="https://github.com/gpuweb/gpuweb/pull/2315">the pull request 2315</a>.</p></li>
<li><p><strong>64-bit atomics</strong>: Either in buffers or textures, it is necessary for doing "software rasterization" on the GPU, by bundling the depth-test and writing of a 32-bit payload in a single <code translate="no" dir="ltr">atomicMax</code> operation. See <a href="https://github.com/gpuweb/gpuweb/issues/4329">the issue 4329</a>.</p></li>
</ul>

<p>To enhance WebGPU's capabilities and integration with the broader web platform, the following WebGPU features have been discussed:</p>

<ul>
<li><p><strong>Compatibility mode</strong>: This mode aims to enable WebGPU to run on a wider range of devices, including those that only support OpenGL ES 3.1. See <a href="https://github.com/gpuweb/gpuweb/blob/main/proposals/compatibility-mode.md">the compatibility mode proposal</a>.</p></li>
<li><p><strong>WebXR</strong>: Allows the existing WebXR Layers module to interface with WebGPU by providing WebGPU swapchains for each layer type. See <a href="https://docs.google.com/presentation/d/1XEzrBrxJ_p9XTEUojibG2Ct2Ft1f7ul4ggzulCu-jpg/edit#slide=id.g30fae2e5bd8_0_0">the WebGPU/WebXR Integration slides</a>.</p></li>
<li><p><strong>Canvas2D</strong>: Creates better interoperability between Canvas 2D and WebGPU, addressing both performance and ergonomics problems. This <a href="https://github.com/fserb/canvas2D/blob/master/spec/webgpu.md">WebGPU Transfer proposal</a> would allow having access to text and path drawing in WebGPU, and being able to apply WebGPU rendering to Canvas 2D.</p></li>
</ul>

<p>The meeting also featured presentations and discussions on efforts to improve WGSL tooling and libraries. One notable initiative is the development of <a href="https://docs.google.com/presentation/d/1rBSSPT6X67IJQogS_GNd6niQEt65dytN49Punv3p8RQ/edit#slide=id.p">WESL</a> (WGSL Extended Shading Language) which aims to provide a community-driven set of extensions to WGSL.</p>

<p>You can find more information in the <a href="https://docs.google.com/document/d/1FlVeiqRzx5t-9z03Ocx7_gw-lPpysUaw_83xofJyxQQ/edit?tab=t.0">raw meeting notes</a>.</p>

<h2 id="thoughts" data-text="Thoughts" tabindex="-1">Thoughts</h2>

<p>This meeting highlighted the importance of collaboration between the WebGPU working group, developers, and the broader graphics community in shaping the future of WebGPU. The working group is actively seeking feedback on the proposed features and is committed to working with developers to ensure that WebGPU meets their needs.</p>

<p>The next evolutions of WebGPU promise to be a significant step forward, unlocking new possibilities for web graphics and empowering developers to create even more immersive and engaging web experiences for AI.</p>

  

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WhisperNER: Unified Open Named Entity and Speech Recognition (107 pts)]]></title>
            <link>https://arxiv.org/abs/2409.08107</link>
            <guid>42208964</guid>
            <pubDate>Thu, 21 Nov 2024 21:41:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2409.08107">https://arxiv.org/abs/2409.08107</a>, See on <a href="https://news.ycombinator.com/item?id=42208964">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2409.08107">View PDF</a>
    <a href="https://arxiv.org/html/2409.08107v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Integrating named entity recognition (NER) with automatic speech recognition (ASR) can significantly enhance transcription accuracy and informativeness. In this paper, we introduce WhisperNER, a novel model that allows joint speech transcription and entity recognition. WhisperNER supports open-type NER, enabling recognition of diverse and evolving entities at inference. Building on recent advancements in open NER research, we augment a large synthetic dataset with synthetic speech samples. This allows us to train WhisperNER on a large number of examples with diverse NER tags. During training, the model is prompted with NER labels and optimized to output the transcribed utterance along with the corresponding tagged entities. To evaluate WhisperNER, we generate synthetic speech for commonly used NER benchmarks and annotate existing ASR datasets with open NER tags. Our experiments demonstrate that WhisperNER outperforms natural baselines on both out-of-domain open type NER and supervised finetuning.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Aviv Navon [<a href="https://arxiv.org/show-email/172bfea7/2409.08107" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 12 Sep 2024 15:00:56 UTC (1,796 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Security researchers identify new malware targeting Linux (128 pts)]]></title>
            <link>https://www.welivesecurity.com/en/eset-research/unveiling-wolfsbane-gelsemiums-linux-counterpart-to-gelsevirine/</link>
            <guid>42208580</guid>
            <pubDate>Thu, 21 Nov 2024 20:57:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.welivesecurity.com/en/eset-research/unveiling-wolfsbane-gelsemiums-linux-counterpart-to-gelsevirine/">https://www.welivesecurity.com/en/eset-research/unveiling-wolfsbane-gelsemiums-linux-counterpart-to-gelsevirine/</a>, See on <a href="https://news.ycombinator.com/item?id=42208580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>ESET researchers have identified multiple samples of Linux backdoor, which we have named WolfsBane, that we attribute with high confidence to the Gelsemium advanced persistent threat (APT) group. This China-aligned threat actor has a known history dating back to 2014 and until now, there have been no public reports of Gelsemium using Linux malware. Additionally, we discovered another Linux backdoor, which we named FireWood. However, we cannot definitively link FireWood to other Gelsemium tools, and its presence in the analyzed archives might be coincidental. Thus, we attribute FireWood to Gelsemium with low confidence, considering it could be a tool shared among multiple China-aligned APT groups.</p>
<p>The most notable samples we found in archives uploaded to VirusTotal are two backdoors resembling known Windows malware used by Gelsemium. WolfsBane is the Linux counterpart of Gelsevirine, while FireWood is connected to Project Wood. We also discovered other tools potentially related to Gelsemium’s activities. The goal of the backdoors and tools discovered is cyberespionage targeting sensitive data such as system information, user credentials, and specific files and directories. These tools are designed to maintain persistent access and execute commands stealthily, enabling prolonged intelligence gathering while evading detection.</p>
<p>The trend of APT groups focusing on Linux malware is becoming more noticeable. We believe this shift is due to improvements in Windows email and endpoint security, such as the widespread use of endpoint detection and response (EDR) tools and Microsoft’s decision to disable Visual Basic for Applications (VBA) macros by default. Consequently, threat actors are exploring new attack avenues, with a growing focus on exploiting vulnerabilities in internet-facing systems, most of which run on Linux.</p>
<p>In this blogpost, we provide technical analysis of the Linux malware, mainly focusing on the two different backdoors.</p>
<blockquote>
<p><strong>Key points of the blogpost:</strong></p>
<ul>
<li>ESET researchers found archives with multiple Linux samples, containing two previously unknown backdoors.</li>
<li>The first backdoor, WolfsBane, is a Linux version of Gelsevirine, a Windows backdoor used by Gelsemium.</li>
<li>Its dropper is the equivalent of the Gelsemine dropper, and features a hider based on an open-source userland rootkit.</li>
<li>The second backdoor, which we have named FireWood, is connected to Project Wood. The Windows version of the Project Wood backdoor was previously used by the Gelsemium group in Operation TooHash.</li>
<li>Alongside the backdoors, we found additional tools, mainly web shells based on publicly available code.</li>
</ul>
</blockquote>
<h2>Overview</h2>
<p>In 2023, we found these samples in archives uploaded to VirusTotal from Taiwan, the Philippines, and Singapore, probably originating from an incident response on a compromised server. Gelsemium has <a href="https://www.welivesecurity.com/2021/06/09/gelsemium-when-threat-actors-go-gardening/" target="_blank" rel="noopener">previously</a> targeted entities in Eastern Asia and the Middle East.</p>
<p>The first backdoor is a part of a simple loading chain consisting of the dropper, launcher, and backdoor. We named this malware WolfsBane. As explained in the <a href="#Attribution and connection"><em>Attribution and connection</em></a> and <em><a href="#Technical analysis">Technical analysis</a></em> sections, WolfsBane is a Linux equivalent of Gelsemium’s Gelsevirine backdoor and the WolfsBane dropper is analogous to the Gelsemine dropper. Our name for Gelsemium comes from one possible translation of the name we found in the report from <a href="https://www.venustech.com.cn/uploads/2018/08/231401512426.pdf" target="_blank" rel="noopener">VenusTech</a>, who dubbed the group 狼毒草. It’s the name of a genus of flowering plants in the family Gelsemiaceae, and <em>Gelsemium elegans</em> is the species that contains toxic compounds like Gelsemine, Gelsenicine, and Gelsevirine, which we chose as names for the three components of this malware family. We previously analyzed Gelsevirine and Gelsemine in <a href="https://web-assets.esetstatic.com/wls/2021/06/eset_gelsemium.pdf" target="_blank" rel="noopener">this white paper</a>. Part of the analyzed WolfsBane attack chain is also a modified open-source userland rootkit, a type of software that exists in the user space of an operating system and hides its activities.</p>
<p>The second backdoor, which we named FireWood, is connected to a backdoor tracked by ESET researchers under the name Project Wood, previously analyzed in the <em>Project Wood</em> section of <a href="https://www.welivesecurity.com/en/eset-research/nspx30-sophisticated-aitm-enabled-implant-evolving-since-2005/" target="_blank" rel="noopener">this blogpost</a>. We have traced it back to 2005 and observed it evolving into more sophisticated versions.</p>
<p>The archives we analyzed also contain several additional tools, mostly webshells, that allow remote control to a user once they are installed on a compromised server, and simple utility tools.</p>
<h2>Attribution and connection<a id="Attribution and connection"></a></h2>
<p>In this section, we explain the similarities that led us to attribute the WolfsBane malware to the Gelsemium APT group and establish a connection between the FireWood backdoor and the Project Wood malware.</p>
<h3>WolfsBane links to Windows Gelsevirine</h3>
<p>Based on the following similarities, we assess that the WolfsBane backdoor is the Linux version of <a href="https://www.welivesecurity.com/2021/06/09/gelsemium-when-threat-actors-go-gardening/" target="_blank" rel="noopener">Gelsevirine</a>. Therefore, we attribute WolfsBane to the Gelsemium APT group with high confidence:</p>
<ul>
<li><strong>Custom libraries for network communication:</strong> Both the Linux and Windows versions load an embedded custom library for network communication, with a different library for each communication protocol used. The backdoor accesses the library’s functions by calling its <span>create_seesion</span> export/symbol; notably, the typo <span>seesion</span> is the same in both versions (as shown in Figure 1).</li>
</ul>
<figure><img title="Figure 1. Accessing the create_seesion export in Linux (left) and Windows (right) versions of backdoor" src="https://web-assets.esetstatic.com/wls/2024/11-2024/wolfsbane/figure-1.jpeg" alt="Figure 1. Accessing the create_seesion export in Linux and Windows versions of backdoor" width="" height="">
<figcaption><em>Figure 1. Accessing the </em><span>create_seesion</span><em> export in Linux (left) and Windows (right) versions of backdoor</em></figcaption>
</figure>
<ul>
<li><strong>Command execution mechanism:</strong> Both versions use the same mechanism for executing commands received from the C&amp;C server. The backdoor creates a table with hashes (derived from the command name) and corresponding pointers to functions that handle those commands (Figure 2). We provide more details in the <em><a href="#Technical analysis">Technical analysis</a></em> section.</li>
</ul>
<figure><img title="Figure 2. Comparison of plugin command names found in the Linux Wolfsbane (left) and Windows Gelsevirine (right) backdoors" src="https://web-assets.esetstatic.com/wls/2024/11-2024/wolfsbane/figure-2.jpeg" alt="Figure 2. Comparison of plugin command names" width="" height="">
<figcaption><em>Figure 2. Comparison of plugin command names found in the Linux Wolfsbane (left) and Windows Gelsevirine (right) backdoors</em></figcaption>
</figure>
<ul>
<li><strong>Configuration structure:</strong> Both backdoors use a very similar configuration structure. While the Linux version has some omitted fields and some extra ones, most of the field names are consistent. For example, the value of <span>pluginkey</span> found in the configuration is the same as in all Windows Gelsevirine samples from 2019. Additionally, the <span>controller_version</span> values in the Linux version configuration match those in the Gelsevirine samples.</li>
<li><strong>Domain Usage: </strong>The domain <span>dsdsei[.]com</span>, used by the Linux version, was previously flagged by ESET researchers as an indicator of compromise (IoC) associated with the Gelsemium APT group.</li>
</ul>
<h3>FireWood connection to Project Wood</h3>
<p>We have found code similarities between the FireWood sample and the backdoor used in Operation TooHash (SHA-1: <span>ED5342D9788392C6E854AAEFA655C4D3B4831B6B</span>), as <a href="https://paper.seebug.org/papers/APT/APT_CyberCriminal_Campagin/2014/GDATA_TooHash_CaseStudy_102014_EN_v1.pdf" target="_blank" rel="noopener">described by G&nbsp;DATA</a>, who consider it to be a part of the DirectsX rootkit. ESET researchers later named this backdoor <a href="https://www.welivesecurity.com/en/eset-research/nspx30-sophisticated-aitm-enabled-implant-evolving-since-2005/" target="_blank" rel="noopener">Project Wood</a>. Those similarities include:</p>
<ul>
<li><strong>Naming conventions:</strong> Both use the "Wood" string in naming. For example, the FireWood backdoor configuration structure is referenced by the symbol <span>WoodConf</span>, and Win32 versions use the mutex name <span>IMPROVING CLIENT Want Wood To Exit?</span>.</li>
<li><strong>File extensions:</strong> Both samples share specific filename extensions such as <span>.k2</span> and <span>.v2</span>.</li>
<li><strong>TEA encryption algorithm:</strong> The implementation of the TEA encryption algorithm with a variable number of rounds is the same in both samples.</li>
<li><strong>C&amp;C communication strings:</strong> Both samples use the same strings in the code responsible for C&amp;C communications, XORed with the same single-byte key (<span>0x26</span>).</li>
<li><strong>Networking code:</strong> The networking code in both samples is very similar.</li>
</ul>
<p>Based on these findings, we assess with high confidence that the FireWood backdoor is the Linux continuation of the Project Wood backdoor. A connection between the FireWood backdoor to other Gelsemium tools cannot be proved and its presence in the archives analyzed could be coincidental. So, we make our attribution to Gelsemium only with low confidence and acknowledge the possibility that it is a tool shared by multiple Chinese APT groups, perhaps through a common digital quartermaster as we have seen with other China-aligned groups.</p>
<h2>Technical analysis<a id="Technical analysis"></a></h2>
<p>The <a href="https://www.virustotal.com/gui/file/3aa8a5afa686e6b21fcc268760ea1f344560607abe9a3edb3f23d14a6032597b">first archive</a> was uploaded to VirusTotal on March 6<sup>th</sup>, 2023, from Taiwan. Subsequent archives were uploaded also from the Philippines and Singapore. Based on the folder structure (Figure 3), the target was probably an Apache Tomcat webserver running an unidentified Java web application.</p>
<figure><img title="Figure 3. Example of archive structure" src="https://web-assets.esetstatic.com/wls/2024/11-2024/wolfsbane/figure-3-1.png" alt="Figure 3. Example of archive structure" width="" height="">
<figcaption><em>Figure 3. Example of archive structure</em></figcaption>
</figure>
<h4>Initial access</h4>
<p>Although we lack concrete evidence regarding the initial access vector, the presence of multiple webshells (as shown in Table 1 and described in the <em><a href="#Webshells">Webshells</a> </em>section) and the tactics, techniques, and procedures (TTPs) used by the Gelsemium APT group in recent years, we conclude with medium confidence that the attackers exploited an unknown web application vulnerability to gain server access.</p>
<p><em>Table 1. Webshells found in analyzed archives</em></p>
<table>
<thead>
<tr>
<td><strong>SHA-1</strong></td>
<td><strong>Filename</strong></td>
<td><strong>Description</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td><span>238C8E8EB7A732D85D8A<wbr>7F7CA40B261D8AE4183D</span></td>
<td><span>login.jsp</span></td>
<td>Modified <a href="https://github.com/AntSwordProject/AntSword-JSP-Template/blob/master/web/shell.jsp" target="_blank" rel="noopener">AntSword JSP</a> webshell.</td>
</tr>
<tr>
<td><span>9F7790524BD759373AB5<wbr>7EE2AAFA6F5D8BCB918A</span></td>
<td><span>yy1.jsp</span></td>
<td><a href="https://github.com/tennc/webshell/blob/master/jsp/icesword.jsp" target="_blank" rel="noopener">icesword</a> webshell.</td>
</tr>
<tr>
<td><span>FD601A54BC622C041DF0<wbr>242662964A7ED31C6B9C</span></td>
<td><span>a.jsp</span></td>
<td>Obfuscated JSP webshell.</td>
</tr>
</tbody>
</table>
<h3>Toolset</h3>
<h4>WolfsBane</h4>
<p>WolfsBane components and chain of execution are depicted in Figure 4.</p>
<figure><img title="Figure 4. WolfsBane execution chain" src="https://web-assets.esetstatic.com/wls/2024/11-2024/wolfsbane/figure-4.jpeg" alt="Figure 4. WolfsBane execution chain" width="" height="">
<figcaption><em>Figure 4. WolfsBane execution chain</em></figcaption>
</figure>
<h5>Stage 1: WolfsBane dropper</h5>
<p>The dropper for WolfsBane was found in a file named <span>cron</span>, mimicking the <a href="https://en.wikipedia.org/wiki/Cron">legitimate command scheduling tool</a>. Upon execution, it first places the launcher and the primary backdoor in the <span>$HOME/.Xl1</span> hidden directory (note the use of the letter l), created by the dropper. The directory is most likely deliberately named to resemble X11 – a commonly used folder name in the <a href="https://en.wikipedia.org/wiki/X_Window_System">X Window System</a>.</p>
<p>The dropper then establishes persistence based on the system’s configuration and execution context:</p>
<p>If executed as <span>root</span>:</p>
<ul>
<li>Checks for the presence of the <span>systemd</span> suite.</li>
<li>If <span>systemd</span> is present, writes the file <span>/lib/systemd/system/display-managerd.service</span> with the path to the next stage (WolfsBane launcher) as the <span>ExecStart</span> entry (see Figure 5). This ensures the launcher runs as a system service, because <span>.service</span> files in this folder are parsed during system startup.</li>
<li>Disables the <a href="https://www.redhat.com/en/topics/linux/what-is-selinux">SELinux</a> &nbsp;security module by changing the <span>SELINUX</span> entry in the SELinux configuration file from <span>enforcing to disabled</span>.</li>
</ul>
<pre><code>[Unit]
Description=Display-Manager
[Service]
Type=simple
ExecStart=&lt;PATH_TO_LAUNCHER_EXECUTABLE&gt;
[Install]
WantedBy=multi-user.targetComment</code></pre>
<p><em>Figure 5. Content of the </em><span>display-managerd.service</span><em> file</em></p>
<p>If <span>systemd</span> is not present, the dropper writes a simple bash script that executes the launcher (Figure 6), to a file named <span>S60dlump</span> into all <span>rc[1-5].d</span> startup folders.</p>
<pre><code>#!/bin/bash
/usr/bin/.Xl1/kde</code></pre>
<p><em>Figure 6. Script executing WolfsBane launcher</em></p>
<p>If executed as an unprivileged user on a Debian-based system, it:</p>
<ul>
<li>writes a similar bash script to the <span>profile.sh</span> file, and</li>
<li>adds the command <span>/home/www/.profile.sh 2&gt;/dev/null</span> to <span>.bashrc</span> and <span>.profile</span> files in the user’s home folder, ensuring that the Wolfsbane launcher starts automatically after the victim logs in.</li>
</ul>
<p>For other Linux distributions it creates the same <span>profile.sh</span> file but adds its path only to <span>.bashrc</span>.</p>
<p>Additionally, if the dropper is executed with root privileges, it drops the WolfsBane Hider rootkit as <span>/usr/lib/libselinux.so</span> and adds this command to <span>/etc/ld.so.preload</span>, ensuring that the rootkit library loads into all processes.</p>
<p>Finally, the dropper removes itself from the disk and executes the next stage – the launcher.</p>
<h5>Stage 2: WolfsBane launcher</h5>
<p>A small binary named <span>kde</span> is used to maintain persistence, cleverly disguised as a legitimate <a href="https://en.wikipedia.org/wiki/KDE">KDE desktop component</a> to avoid detection and maintain persistence. Regardless of establishment method, the aim is to execute this binary, whose main function is to parse its embedded configuration and initiate the next stage – the WolfsBane backdoor – from the specified file in the configuration.</p>
<h5>Stage 3: WolfsBane backdoor</h5>
<p>The WolfsBane backdoor, stored in a file named <span>udevd</span>, begins by loading an embedded library and calling its <span>main_session</span> export, which contains the main backdoor functionalities. This library, named by its authors as <span>libMainPlugin.so</span>, is analogous to the <span>MainPlugin.dll</span> used in the Windows version of the Gelsevirine backdoor.</p>
<p>Similar to its Windows version, the WolfsBane backdoor uses other embedded libraries for network communication. In the samples we’ve collected, they are named <span>libUdp.so</span> and <span>libHttps.so</span>, and both export the symbol <span>create_seesion</span> (the spelling mistake is exactly the same as in the Windows version of the Gelsevirine TCP module). These shared libraries provide C&amp;C communications via UDP and HTTPS protocols, respectively.</p>
<p>The backdoor encrypts the <span>libMainPlugin.so</span> library using the RC4 algorithm (with the key obtained from the <span>pluginkey</span> value in the configuration) and saves it to <span>&lt;work_directory&gt;/X1l/data/gphoto2</span>. On subsequent executions, the backdoor first checks for this file: if it exists, the file is decrypted and loaded instead of the embedded <span>libMainPlugin.so</span>. This mechanism allows the backdoor to be updated by overwriting the file.</p>
<p>The WolfsBane backdoor uses a similar approach to its Windows counterpart for executing commands received from its C&amp;C server.</p>
<h5>WolfsBane Hider rootkit</h5>
<p>WolfsBane backdoor uses a modified open-source <a href="https://github.com/unix-thrust/beurk/tree/dev" target="_blank" rel="noopener">BEURK</a> userland rootkit to hide its activities. Located in <span>/usr/lib/libselinux.so</span>, this rootkit abuses the operating system’s preload mechanism to load into new processes before other libraries by adding its path to the <span>/etc/ld.so.preload</span> file, thus enabling its functions to hook the original ones.</p>
<p>The WolfsBane Hider rootkit hooks many basic standard C library functions such as <span>open</span>, <span>stat</span>, <span>readdir</span>, and <span>access</span>. While these hooked functions invoke the original ones, they filter out any results related to the WolfsBane malware. Unlike the original BEURK rootkit, which uses an embedded configuration file for filtering, the WolfsBane developers retained the default configuration but modified the source code to exclude information related to the hardcoded filenames of the malware executables <span>udevd</span> and <span>kde</span>. Additionally, the original BEURK rootkit’s network traffic-hiding features are absent.</p>
<h4>FireWood backdoor</h4>
<p>The FireWood backdoor, in a file named <span>dbus</span>, is the Linux OS continuation of the Project Wood malware, as noted in the <a href="#Attribution and connection"><em>Attribution and connection</em></a> section. The analyzed code suggests that the file <span>usbdev.ko</span> is a kernel driver module working as a rootkit to hide processes. The FireWood backdoor communicates with the kernel drivers using the <a href="https://en.wikipedia.org/wiki/Netlink">Netlink protocol</a>.</p>
<p>FireWood uses a configuration file named <span>kdeinit</span> that is XOR encrypted with the single-byte key <span>0x26</span>. The configuration file’s structure is detailed in Table 2.</p>
<p><em>Table 2. Selected offsets and their corresponding values from the FireWood backdoor configuration file</em></p>
<table>
<thead>
<tr>
<td><strong>Offset</strong></td>
<td><strong>Value</strong></td>
<td><strong>Meaning</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td><span>0x00</span></td>
<td><span>20190531110402</span></td>
<td>Unknown timestamp.</td>
</tr>
<tr>
<td><span>0x28</span></td>
<td><span>AAAAAAAAAA</span></td>
<td>Placeholder for backdoor working directory.</td>
</tr>
<tr>
<td><span>0x3C</span></td>
<td><span>0.0.0.0</span></td>
<td>C&amp;C IP address (if 0.0.0.0, the backdoor uses the C&amp;C domain).</td>
</tr>
<tr>
<td><span>0x66</span></td>
<td><span>asidomain[.]com</span></td>
<td>C&amp;C domain.</td>
</tr>
<tr>
<td><span>0xCC</span></td>
<td><span>[scsi_eh_7]</span></td>
<td>Spoofed process name.</td>
</tr>
<tr>
<td><span>0x164</span></td>
<td><span>0x072BA1E6</span></td>
<td>TEA encryption key.</td>
</tr>
<tr>
<td><span>0x1E0</span></td>
<td><span>4</span></td>
<td>Connection day (backdoor connects every fourth day of the month).</td>
</tr>
<tr>
<td><span>0x1E4</span></td>
<td><span>5</span></td>
<td>Delay time.</td>
</tr>
<tr>
<td><span>0x1E8</span></td>
<td><span>0x0474</span></td>
<td>Connection time (in minutes).</td>
</tr>
</tbody>
</table>
<p>FireWood renames its process based on the value in the configuration.</p>
<p>To establish persistence on the system, it creates a file named <span>/.config/autostart/gnome-control.desktop</span>. During startup, all files with a <span>.desktop</span> extension in the <span>/.config/autostart/ directory</span> are parsed, and any commands listed in the <span>Exec</span> entry are executed. The contents of the <span>gnome-control.desktop</span> file can be seen in Figure&nbsp;7.</p>
<pre><code>[Desktop Entry]
Type=Application
Exec=&lt;PATH/TO/OWN/EXECUTABLE&gt;
Hidden=false
NoDisplay=false
X-GNOME-Autostart-enabled=true
Name[en_US]=gnome-calculator
Name=gnome-control
Comment[en_US]=</code></pre>
<p><em>Figure 7. Contents of the </em><span>gnome-control.desktop</span><em> file used for persistence by the FireWood backdoor</em></p>
<p>FireWood communicates with its C&amp;C server via TCP, as specified in its configuration. All data is encrypted using the TEA encryption algorithm with a variable number of rounds. The encryption key and number of rounds are provided in the FireWood configuration file, as shown back in Table&nbsp;2.</p>
<p>The structure of sent and received messages is shown in Figure 8. The outcome of executing a command varies depending on the command type, but typically, <span>0x10181</span> indicates success, while <span>0x10180</span> denotes an error.</p>
<pre><code>struct data{
    DWORD commandID_or_return_code_value ; 
    BYTE  data [];
}</code></pre>
<p><em>Figure 8. Data. structure for C&amp;C communications used by FireWood backdoor</em></p>
<p>This backdoor is capable of executing several commands, as described in Table 3.</p>
<p><em>Table 3. FireWood backdoor commands</em></p>
<table>
<thead>
<tr>
<td><strong>Command ID</strong></td>
<td><strong>Description</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td><span>0x105</span></td>
<td>Download an executable file from the C&amp;C to <span>&lt;PATH&gt;/tmpWood</span> and execute it with the ‌<span>‑UPDATE</span> parameter.</td>
</tr>
<tr>
<td><span>0x110</span></td>
<td>Execute a shell command using the <span>popen</span> function.</td>
</tr>
<tr>
<td><span>0x111</span></td>
<td>Change connection time value in the configuration.</td>
</tr>
<tr>
<td><span>0x112</span></td>
<td>Hide a process using the <span>usbdev.ko</span> kernel module.</td>
</tr>
<tr>
<td><span>0x113</span></td>
<td>Change delay time in configuration.</td>
</tr>
<tr>
<td><span>0x114</span></td>
<td>Change connection day value in configuration.</td>
</tr>
<tr>
<td><span>0x132</span></td>
<td>Clean up and exit.</td>
</tr>
<tr>
<td><span>0x181</span></td>
<td>List contents of the specified directory.</td>
</tr>
<tr>
<td><span>0x182</span></td>
<td>Exfiltrate specified file to C&amp;C server.</td>
</tr>
<tr>
<td><span>0x183</span></td>
<td>Delete specified file.</td>
</tr>
<tr>
<td><span>0x184</span></td>
<td>Rename specified file.</td>
</tr>
<tr>
<td><span>0x185</span></td>
<td>Execute specified file using the <span>system</span> function.</td>
</tr>
<tr>
<td><span>0x186</span></td>
<td>Download file from C&amp;C server.</td>
</tr>
<tr>
<td><span>0x189</span></td>
<td>Exfiltrate specified folder to C&amp;C server.</td>
</tr>
<tr>
<td><span>0x193</span></td>
<td>Load specified kernel module or shared library.</td>
</tr>
<tr>
<td><span>0x194</span></td>
<td>Unload specified kernel module or shared library.</td>
</tr>
<tr>
<td><span>0x19F</span></td>
<td>Modify specified file timestamp.</td>
</tr>
<tr>
<td><span>0x200</span></td>
<td>Delete specified directory.</td>
</tr>
<tr>
<td><span>0x201</span></td>
<td>Read content of the specified file and send it to the C&amp;C server.</td>
</tr>
<tr>
<td><span>0x1018F</span></td>
<td>Search for the specified file in the folder defined in the command.</td>
</tr>
</tbody>
</table>
<h4>Other tools</h4>
<p>We discovered two additional tools in the archives, which could be related to Gelsemium activity: the SSH password stealer and a small privilege escalation tool.</p>
<p>The SSH password stealer is an SSH client based on the open-source <a href="https://www.openssh.com/">OpenSSH</a> software, modified to collect users’ SSH credentials necessary for authenticating the user’s access to a server. The adversaries replaced the original SSH client binary in <span>/usr/bin/ssh</span> with a trojanized version. While it functions as a normal SSH client, it saves all login data in the format <span>&lt;USERNAME&gt;@&lt;HOST&gt;\t&lt;PASSWORD&gt;</span> into the file <span>/tmp/zijtkldse.tmp</span>.</p>
<p>The privilege escalation tool is a small binary, named <span>ccc</span>, that just escalates user privileges by setting UID and GUID of the execution context to <span>0</span> and executes a program at a path received as an argument. To perform this technique, the user must have root privileges to add SUID permission to this executable in advance, making it a tool for maintaining privileges rather than for obtaining them.</p>
<h4>Webshells<a id="Webshells"></a></h4>
<p>The <span>login.jsp</span> is a modified <a href="https://github.com/AntSwordProject/AntSword-JSP-Template">AntSword JSP</a> webshell that executes Java bytecode from attackers. The payload, a Java class file, is base64 encoded in the <span>tiger</span> parameter of an HTTP POST request. The original webshell also supports remote terminal, file operations, and database operations.</p>
<p>The <span>yy1.jsp</span> webshell, which we identified as icesword JSP, is sourced from internet forums, primarily those in Chinese. The icesword JSP webshell features a complete graphical user interface within its server-side code, allowing it to render a GUI in the attacker’s browser. It is not obfuscated and collects system information, executes system commands, and performs file operations. It also connects to SQL databases on the compromised host and executes SQL queries.</p>
<p>The <span>a.jsp</span> webshell, similar to <span>login.jsp</span> but obfuscated, carries a binary Java payload that is AES encrypted with the key <span>6438B9BD2AB3C40A</span> and then base64 encoded. The payload is provided in the <span>Tas9er</span> parameter. The obfuscation includes garbage comments, \u-escaped Unicode strings (which are made harder to read), and random string variables and function names. The result, base64 encoded and inserted into the string <span>1F2551A37335B564&lt;base64_encoded_result&gt;8EF53BE997851B95</span>, is sent to the attackers in the response body.</p>
<h2>Conclusion</h2>
<p>This report describes the Linux malware toolset and its connections with Windows malware samples utilized by the Gelsemium APT group. We have focused on capabilities of WolfsBane and FireWood backdoors, and analyzed WolfsBane execution chain and its utilization of the userland rootkit. This is the first public report documenting Gelsemium’s use of Linux malware, marking a notable shift in their operational strategy.</p>
<p>The trend of malware shifting towards Linux systems seems to be on the rise in the APT ecosystem. From our perspective, this development can be attributed to several advancements in email and endpoint security. The ever-increasing adoption of EDR solutions, along with Microsoft’s default strategy of disabling VBA macros, are leading to a scenario where adversaries are being forced to look for other potential avenues of attack.</p>
<p>As a result, the vulnerabilities present in internet-facing infrastructure, particularly those systems that are Linux-based, are becoming increasingly targeted. This means that these Linux systems are becoming the new preferred targets for these adversaries.</p>
<blockquote>
<p><em>For any inquiries about our research published on WeLiveSecurity, please contact us at <a href="mailto:threatintel@eset.com?utm_source=welivesecurity.com&amp;utm_medium=referral&amp;utm_campaign=autotagging&amp;utm_content=eset-research&amp;utm_term=en">threatintel@eset.com</a>.&nbsp;</em></p>
<p><em>ESET Research offers private APT intelligence reports and data feeds. For any inquiries about this service, visit the <a href="https://www.eset.com/int/business/services/threat-intelligence/?utm_source=welivesecurity.com&amp;utm_medium=referral&amp;utm_campaign=wls-research&amp;utm_content=unveiling-wolfsbane-gelsemiums-linux-counterpart-to-gelsevirine&amp;sfdccampaignid=7011n0000017htTAAQ" target="_blank" rel="noopener">ESET Threat Intelligence</a> page.</em></p>
</blockquote>
<h2>IoCs</h2>
<p>A comprehensive list of indicators of compromise (IoCs) and samples can be found in <a href="https://github.com/eset/malware-ioc/tree/master/gelsemium">our GitHub repository</a>.</p>
<h3>Files</h3>

<table>
<thead>
<tr>
<td><strong>SHA-1</strong></td>
<td><strong>Filename</strong></td>
<td><strong>Detection</strong></td>
<td><strong>Description</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td><span> </span> <span>0FEF89711DA11C550D39<wbr>14DEBC0E663F5D2FB86C</span><br><span> </span></td>
<td><span> </span> <span>dbus</span><br><span> </span></td>
<td>Linux/Agent.WF</td>
<td>FireWood backdoor.</td>
</tr>
<tr>
<td><span> </span> <span>44947903B2BC760AC2E7<wbr>36B25574BE33BF7AF40B</span><br><span> </span></td>
<td><span> </span> <span>libselinux.so</span><br><span> </span></td>
<td>Linux/Rootkit.Agent.EC</td>
<td>WolfsBane Hider rootkit.</td>
</tr>
<tr>
<td><span> </span> <span>0AB53321BB9699D354A0<wbr>32259423175C08FEC1A4</span><br><span> </span></td>
<td><span> </span> <span>udevd</span><br><span> </span></td>
<td>Linux/Agent.WF</td>
<td>WolfsBane backdoor.</td>
</tr>
<tr>
<td><span> </span> <span>8532ECA04C0F58172D80<wbr>D8A446AE33907D509377</span><br><span> </span></td>
<td><span> </span> <span>kde</span><br><span> </span></td>
<td>Linux/Agent.WF</td>
<td>WolfsBane launcher.</td>
</tr>
<tr>
<td><span> </span> <span>B2A14E77C96640914399<wbr>E5F46E1DEC279E7B940F</span><br><span> </span></td>
<td><span> </span> <span>cron</span><br><span> </span></td>
<td>Linux/Agent.WF</td>
<td>WolfsBane dropper.</td>
</tr>
<tr>
<td><span> </span> <span>209C4994A42AF7832F52<wbr>6E09238FB55D5AAB34E5</span><br><span> </span></td>
<td><span> </span> <span>ccc</span><br><span> </span></td>
<td>Linux/Agent.WF</td>
<td>Privilege escalation helper tool.</td>
</tr>
<tr>
<td><span> </span> <span>F43D4D46BAE9AD963C2E<wbr>B05EF43E90AA3A5D88E3</span><br><span> </span></td>
<td><span> </span> <span>ssh</span><br><span> </span></td>
<td>Linux/SSHDoor.IC</td>
<td>Trojanized SSH client.</td>
</tr>
<tr>
<td><span> </span> <span>FD601A54BC622C041DF0<wbr>242662964A7ED31C6B9C</span><br><span> </span></td>
<td><span> </span> <span>a.jsp</span><br><span> </span></td>
<td>Java/Agent.BP</td>
<td>JSP webshell.</td>
</tr>
<tr>
<td><span> </span> <span>9F7790524BD759373AB5<wbr>7EE2AAFA6F5D8BCB918A</span><br><span> </span></td>
<td><span> </span> <span>yy1.jsp</span><br><span> </span></td>
<td>Java/JSP.J</td>
<td>icesword webshell.</td>
</tr>
<tr>
<td><span> </span> <span>238C8E8EB7A732D85D8A<wbr>7F7CA40B261D8AE4183D</span><br><span> </span></td>
<td><span> </span> <span>login.jsp</span><br><span> </span></td>
<td>Java/Webshell.AM</td>
<td>Modified AntSword JSP webshell.</td>
</tr>
<tr>
<td><span> </span> <span>F1DF0C5A74C9885CB593<wbr>4E3EEE5E7D3CF4D291C0</span><br><span> </span></td>
<td><span> </span> <span>virus.tgz</span><br><span> </span></td>
<td>Linux/Agent.WF</td>
<td>VirusTotal archive.</td>
</tr>
<tr>
<td><span> </span> <span>B3DFB40336C2F17EC740<wbr>51844FFAF65DDB874CFC</span><br><span> </span></td>
<td><span> </span> <span>virus-b.tgz</span><br><span> </span></td>
<td>Linux/Agent.WF</td>
<td>VirusTotal archive.</td>
</tr>
<tr>
<td><span> </span> <span>85528EAC10090AE743BC<wbr>F102B4AE7007B6468255</span><br><span> </span></td>
<td><span> </span> <span>CHINA-APT-Trojan.zip</span><br><span> </span></td>
<td>Java/Agent.BP</td>
<td>VirusTotal archive.</td>
</tr>
<tr>
<td><span> </span> <span>CDBBB6617D8937D17A1A<wbr>9EF12750BEE1CDDF4562</span><br><span> </span></td>
<td><span> </span> <span>CHINA-APT-Trojan.zip</span><br><span> </span></td>
<td>Linux/Rootkit.Agent.EC</td>
<td>VirusTotal archive.</td>
</tr>
<tr>
<td><span> </span> <span>843D6B0054D066845628<wbr>E2D5DB95201B20E12CD2</span><br><span> </span></td>
<td><span> </span> <span>CHINA-APT-Trojan.zip</span><br><span> </span></td>
<td>Linux/Rootkit.Agent.EC</td>
<td>VirusTotal archive.</td>
</tr>
<tr>
<td><span> </span> <span>BED9EFB245FAC8CFFF83<wbr>33AE37AD78CCFB7E2198</span><br><span> </span></td>
<td><span> </span> <span>Xl1.zip</span><br><span> </span></td>
<td>Linux/Rootkit.Agent.EC</td>
<td>VirusTotal archive.</td>
</tr>
<tr>
<td><span> </span> <span>600C59733444BC8A5F71<wbr>D41365368F3002465B10</span><br><span> </span></td>
<td><span> </span> <span>CHINA-APT-Trojan.zip</span><br><span> </span></td>
<td>Linux/Rootkit.Agent.EC</td>
<td>VirusTotal archive.</td>
</tr>
<tr>
<td><span> </span> <span>72DB8D1E3472150C1BE9<wbr>3B68F53F091AACC2234D</span><br><span> </span></td>
<td><span> </span> <span>virus.tgz</span><br><span> </span></td>
<td>Linux/Agent.WF</td>
<td>VirusTotal archive.</td>
</tr>
</tbody>
</table>
<h3>Network</h3>
<table>
<thead>
<tr>
<td><strong>IP</strong></td>
<td><strong>Domain</strong></td>
<td><strong>Hosting provider</strong></td>
<td><strong>First seen</strong></td>
<td><strong>Details</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td>N/A</td>
<td><span>dsdsei[.]com</span></td>
<td>N/A</td>
<td>2020⁠-⁠08⁠-⁠16</td>
<td>WolfsBane backdoor C&amp;C server.</td>
</tr>
<tr>
<td>N/A</td>
<td><span>asidomain[.]com</span></td>
<td>N/A</td>
<td>2022⁠-⁠01⁠-⁠26</td>
<td>FireWood backdoor C&amp;C server.</td>
</tr>
</tbody>
</table>
<h2>MITRE ATT&amp;CK techniques</h2>
<p>This table was built using <a href="https://attack.mitre.org/resources/versions/">version 15</a> of the MITRE ATT&amp;CK framework.</p>
<table>
<thead>
<tr>
<td>
<p><strong>Tactic</strong></p>
</td>
<td>
<p><strong>ID</strong></p>
</td>
<td>
<p><strong>Name</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="3">
<p><strong>Resource Development</strong></p>
</td>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1583/001">T1583.001</a></p>
</td>
<td>
<p>Acquire Infrastructure: Domains</p>
</td>
<td>
<p>Gelsemium has registered domains through commercial providers.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1583/004">T1583.004</a></p>
</td>
<td>
<p>Acquire Infrastructure: Server</p>
</td>
<td>
<p>Gelsemium most likely acquires VPS from commercial providers.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1587/001">T1587.001</a></p>
</td>
<td>
<p>Develop Capabilities: Malware</p>
</td>
<td>
<p>Gelsemium develops its own custom malware.</p>
</td>
</tr>
<tr>
<td>
<p><strong>Execution</strong></p>
</td>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1059/004">T1059.004</a></p>
</td>
<td>
<p>Command-Line Interface: Unix Shell</p>
</td>
<td>
<p>Gelsemium malware is capable of executing Linux shell commands.</p>
</td>
</tr>
<tr>
<td rowspan="4">
<p><strong>Persistence</strong></p>
</td>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1037/004">T1037.004</a></p>
</td>
<td>
<p>Boot or Logon Initialization Scripts: RC Scripts</p>
</td>
<td>
<p>The WolfsBane launcher remains persistent on the system by using RC startup scripts.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1543/002">T1543.002</a></p>
</td>
<td>
<p>Create or Modify System Process: Systemd Service</p>
</td>
<td>
<p>The WolfsBane dropper can create a new system service for persistence.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1574/006">T1574.006</a></p>
</td>
<td>
<p>Hijack Execution Flow: Dynamic Linker Hijacking</p>
</td>
<td>
<p>The WolfsBane Hider rootkit abuses the <span>ld.so.preload</span> preload technique.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1547/013">T1547.013</a></p>
</td>
<td>
<p>Boot or Logon Autostart Execution: XDG Autostart Entries</p>
</td>
<td>
<p>The FireWood backdoor persists on the system by creating the <span>gnome-control.desktop</span> autostart file.</p>
</td>
</tr>
<tr>
<td rowspan="2">
<p><strong>Privilege Escalation</strong></p>
</td>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1546/004">T1546.004</a></p>
</td>
<td>
<p>Event Triggered Execution: .bash_profile and .bashrc</p>
</td>
<td>
<p>The WolfsBane dropper tampers with various shell configuration files to achieve persistence.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1548/001">T1548.001</a></p>
</td>
<td>
<p>Abuse Elevation Control Mechanism: Setuid and Setgid</p>
</td>
<td>
<p>Gelsemium uses a simple tool abusing setuid and setguid for keeping escalated privileges.</p>
</td>
</tr>
<tr>
<td rowspan="8">
<p><strong>Defense Evasion</strong></p>
</td>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1070/004">T1070.004</a></p>
</td>
<td>
<p>Indicator Removal: File Deletion</p>
</td>
<td>
<p>The WolfsBane dropper removes itself.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1070/006">T1070.006</a></p>
</td>
<td>
<p>Indicator Removal: Timestomp</p>
</td>
<td>
<p>The FireWood backdoor has a command for modifying the MAC time of files.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1070/009">T1070.009</a></p>
</td>
<td>
<p>Indicator Removal: Clear Persistence</p>
</td>
<td>
<p>The WolfsBane dropper removes itself from disk.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1564/001">T1564.001</a></p>
</td>
<td>
<p>Hide Artifacts: Hidden Files and Directories</p>
</td>
<td>
<p>Both the WolfsBane and FireWood backdoors are located/installed in hidden folders.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1222/002">T1222.002</a></p>
</td>
<td>
<p>File Permissions Modification: Linux and Mac File and Directory Permissions Modification</p>
</td>
<td>
<p>The WolfsBane dropper uses Linux chmod commands to modify permissions of dropped executables.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1027/009">T1027.009</a></p>
</td>
<td>
<p>Obfuscated Files or Information: Embedded Payloads</p>
</td>
<td>
<p>The WolfsBane dropper has all its payloads compressed and embedded.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1014">T1014</a></p>
</td>
<td>
<p>Rootkit</p>
</td>
<td>
<p>Both WolfsBane and FireWood malware utilize rootkits for evasion.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1036/005">T1036.005</a></p>
</td>
<td>
<p>Masquerading: Match Legitimate Name or Location</p>
</td>
<td>
<p>Gelsemium often names its malware to match legitimate files and folders.</p>
</td>
</tr>
<tr>
<td rowspan="2">
<p><strong>Discovery</strong></p>
</td>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1082">T1082</a></p>
</td>
<td>
<p>System Information Discovery</p>
</td>
<td>
<p>The WolfsBane dropper enumerates system information.</p>
</td>
</tr>
<tr>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1083">T1083</a></p>
</td>
<td>
<p>File and Directory Discovery</p>
</td>
<td>
<p>The FireWood backdoor is capable of searching in the machine file system for specified files and folders.</p>
</td>
</tr>
<tr>
<td>
<p><strong>Collection</strong></p>
</td>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1056">T1056</a></p>
</td>
<td>
<p>Input Capture</p>
</td>
<td>
<p>The SSH password stealer captures user credentials.</p>
</td>
</tr>
<tr>
<td>
<p><strong>Exfiltration</strong></p>
</td>
<td>
<p><a href="https://attack.mitre.org/versions/v15/techniques/T1041">T1041</a></p>
</td>
<td>
<p>Exfiltration Over C2 Channel</p>
</td>
<td>
<p>The FireWood backdoor exfiltrates collected data utilizing C&amp;C communications.</p>
</td>
</tr>
</tbody>
</table>
<p><a href="https://www.eset.com/int/business/services/threat-intelligence/?utm_source=welivesecurity.com&amp;utm_medium=referral&amp;utm_campaign=wls-research&amp;utm_content=unveiling-wolfsbane-gelsemiums-linux-counterpart-to-gelsevirine&amp;sfdccampaignid=7011n0000017htTAAQ" target="_blank" rel="noopener"><img src="https://web-assets.esetstatic.com/wls/2023/2023-12/welivesecurity-eset-threat-intelligence.jpeg" alt="" width="915" height="296"></a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Llama 3.2 Interpretability with Sparse Autoencoders (509 pts)]]></title>
            <link>https://github.com/PaulPauls/llama3_interpretability_sae</link>
            <guid>42208383</guid>
            <pubDate>Thu, 21 Nov 2024 20:37:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/PaulPauls/llama3_interpretability_sae">https://github.com/PaulPauls/llama3_interpretability_sae</a>, See on <a href="https://news.ycombinator.com/item?id=42208383">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Llama 3 Interpretability with Sparse Autoencoders</h2><a id="user-content-llama-3-interpretability-with-sparse-autoencoders" aria-label="Permalink: Llama 3 Interpretability with Sparse Autoencoders" href="#llama-3-interpretability-with-sparse-autoencoders"></a></p>
<p dir="auto"><a href="https://github.com/PaulPauls/llama3_interpretability_sae"><img src="https://camo.githubusercontent.com/d8ab014b7c4fddd9e1be09aad531f865d6c10714656e450bd25916bee9184faf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d302e322d627269676874677265656e2e737667" alt="Version" data-canonical-src="https://img.shields.io/badge/Version-0.2-brightgreen.svg"></a>
<a href="https://www.python.org/downloads/release/python-3120/" rel="nofollow"><img src="https://camo.githubusercontent.com/301b3c09bd84634def9aedd025140e5490068ad6d3c1dbf3ee7e438bc18ecc10/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e31322d626c75652e737667" alt="Python 3.12" data-canonical-src="https://img.shields.io/badge/Python-3.12-blue.svg"></a>
<a href="https://pytorch.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/5f73ee33476fce4b75d5433466697168447f41344deba992dfd8ca022f87898d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5079546f7263682d322e342e312d2532334545344332432e7376673f7374796c653d666c6174266c6f676f3d7079746f726368266c6f676f436f6c6f723d7768697465" alt="PyTorch 2.4.1" data-canonical-src="https://img.shields.io/badge/PyTorch-2.4.1-%23EE4C2C.svg?style=flat&amp;logo=pytorch&amp;logoColor=white"></a>
<a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" rel="nofollow"><img src="https://camo.githubusercontent.com/cc7eb8b5b76db2770a1185619fe6e84884eae824a79af854d418438ef3cce6e5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541362539392532304c6c616d612d332e322d626c756576696f6c65742e737667" alt="Llama 3.2" data-canonical-src="https://img.shields.io/badge/%F0%9F%A6%99%20Llama-3.2-blueviolet.svg"></a>
<a href="https://huggingface.co/datasets/PaulPauls/openwebtext-sentences" rel="nofollow"><img src="https://camo.githubusercontent.com/6bf69a74cc81c38011dc538b7169597928a2644fb726da1eee000e8ea0ebc089/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446174617365742d4f70656e5765625465787453656e74656e6365732d6f72616e67652e737667" alt="Dataset" data-canonical-src="https://img.shields.io/badge/Dataset-OpenWebTextSentences-orange.svg"></a>
<a href="https://api.wandb.ai/links/paulpauls/pi9dpi2a" rel="nofollow"><img src="https://camo.githubusercontent.com/b3032f9b6ba38ec405b1fbb5e410082ce6acaeea33a5af19f9bfc831ffb0c9ae/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f57656967687473253230262532304269617365732d4c6f67732d79656c6c6f772e737667" alt="Weights &amp; Biases" data-canonical-src="https://img.shields.io/badge/Weights%20&amp;%20Biases-Logs-yellow.svg"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/sparse-autoencoder_light.webp"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/sparse-autoencoder_light.webp" alt="Sparse Autoencoder Animation"></a>
  <br>
  <em>Source: <a href="https://openai.com/index/extracting-concepts-from-gpt-4/" rel="nofollow">OpenAI - Extracting Concepts from GPT-4</a></em>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Overview</h2><a id="user-content-project-overview" aria-label="Permalink: Project Overview" href="#project-overview"></a></p>
<p dir="auto">Modern LLMs encode concepts by superimposing multiple features into the same neurons and then interpeting them by taking into account the linear superposition of all neurons in a layer. This concept of giving each neuron multiple interpretable meanings they activate depending on the context of other neuron activations is called <em>superposition</em>. Sparse Autoencoders (SAEs) are models that are inserted into a trained LLM for the purpose of projecting the activations into a very large but very sparsely activated latent space. By doing so they attempt to untangle these superimposed representations into separate, clearly interpretable features for each neuron activation that each represent one clear concept - which in turn would make these neurons <em>monosemantic</em>. Such a mechanistic interpretability has proven very valuable for understanding model behavior, detecting hallucinations, analyzing information flow through models for optimization, etc.</p>
<p dir="auto">This project attempts to recreate this great research into mechanistic LLM Interpretability with Sparse Autoencoders (SAE) to extract interpretable features that was very successfully conducted and published by Anthropic, OpenAI and Google DeepMind a few months ago. The project aims to provide a full pipeline for capturing training data, training the SAEs, analyzing the learned features, and then verifying the results experimentally. Currently, the project provides all code, data, and models that were created by running the whole project pipeline once and creating a functional and interpretable Sparse Autoencoder for the Llama 3.2-3B model.</p>
<p dir="auto">Such a research project obviously requires a lot of computational resources (meaning money) and time that I don't necessarily have at my full disposal for a non-profit side project of mine. Therefore, the project - as I am releasing it now with version 0.2 - is in a good, efficient, and scalable state, but it is not final and will hopefully be updated and improved upon over time. Please feel free to contribute code or feedback or just let me know if you found a bug - thank you!</p>
<p dir="auto">This project is based primarily on the following research papers:</p>
<ul dir="auto">
<li><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html" rel="nofollow">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a> (Anthropic, May 2024)</li>
<li><a href="https://arxiv.org/abs/2406.04093" rel="nofollow">Scaling and Evaluating Sparse Autoencoders</a> (OpenAI, June 2024)</li>
<li><a href="https://arxiv.org/abs/2408.05147" rel="nofollow">Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2</a> (Google DeepMind, July 2024)</li>
</ul>
<p dir="auto">And the Open Source LLM Llama 3.2 that was used for the current state of the project:</p>
<ul dir="auto">
<li><a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" rel="nofollow">Llama 3.2: Revolutionizing edge AI and vision with open, customizable models</a></li>
<li><a href="https://github.com/meta-llama/llama-models">Llama Models</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Features</h3><a id="user-content-core-features" aria-label="Permalink: Core Features" href="#core-features"></a></p>
<p dir="auto">A complete end-to-end pipeline from activation capture to Sparse AutoEncoder (SAE) training, feature interpretation, and verification, written in pure PyTorch with minimal dependencies. Specifically:</p>
<ul dir="auto">
<li>Captures residual activations from large language models as SAE training dataset, using a custom sentence-split OpenWebText dataset variant</li>
<li>Preprocesses training data (prebatching, stat calculation) for efficient training</li>
<li>Supports distributed (multi-GPU, single-node) large-scale and efficient training of SAEs with captured and preprocessed activation data</li>
<li>Implements SAE training with auxiliary loss to prevent and revive dead latents, and gradient projection to stabilize training dynamics</li>
<li>Provides comprehensive logging, visualization, and checkpointing of SAE training through Weights &amp; Biases and console logging, including detailed logs for:
<ul dir="auto">
<li>Training progress (main/aux loss)</li>
<li>Validation results (main/aux loss)</li>
<li>Dead latent monitoring for debugging and analysis</li>
</ul>
</li>
<li>Offers interpretability analysis tools for feature extraction and semantic analysis of learned features by:
<ul dir="auto">
<li>Capturing inputs that maximally activate the sparse autoencoder latents</li>
<li>Cost-effectively analyzing them at scale using a Frontier LLM</li>
</ul>
</li>
<li>Provides a pure PyTorch implementation of Llama 3.1/3.2 chat and text completion without external dependencies (e.g., Fairscale) for general usage and result verification</li>
<li>Verifies SAE's impact on model behavior and enables feature steering of extracted semantic features through:
<ul dir="auto">
<li>Text and chat completion tasks</li>
<li>Optional Gradio interface for ease of use</li>
</ul>
</li>
<li><strong>All components are designed and implemented with scalability, efficiency, and maintainability in mind</strong></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Published Resources</h2><a id="user-content-published-resources" aria-label="Permalink: Published Resources" href="#published-resources"></a></p>
<p dir="auto">The following resource are available to reproduce the results of the current state of the project or to provide insight into the training:</p>
<ol dir="auto">
<li>
<p dir="auto"><a href="https://huggingface.co/datasets/PaulPauls/openwebtext-sentences" rel="nofollow"><strong>OpenWebText Sentence Dataset</strong></a>:</p>
<ul dir="auto">
<li>Custom Version of the OpenWebText Dataset used for activation capture</li>
<li>All text from the original OpenWebText dataset</li>
<li>Sentences are stored individually now in parquet format for faster access</li>
<li>Maintains all original OpenWebText text and the order thereof</li>
<li>Sentences were split using NLTK 3.9.1 pre-trained "Punkt" tokenizer</li>
</ul>
</li>
<li>
<p dir="auto"><a href="https://drive.google.com/drive/folders/1GqMQz3d0z40TRg0codbpt_qacWSPftTZ?usp=sharing" rel="nofollow"><strong>Captured Llama 3.2-3B Activations</strong></a>:</p>
<ul dir="auto">
<li>25 million sentences worth of Llama 3.2-3B layer 23 residual activations</li>
<li>Size: 3.2 TB (compressed from 4 TB raw)</li>
<li>Split into 100 archives for more manageable download</li>
</ul>
</li>
<li>
<p dir="auto"><a href="https://api.wandb.ai/links/paulpauls/pi9dpi2a" rel="nofollow"><strong>SAE Training Log</strong></a>:</p>
<ul dir="auto">
<li>Weights &amp; Biases visualized log of training, validation and debug metrics</li>
<li>10 epochs of training, 10,000 logged steps</li>
<li>Includes train/val main losses, auxiliary losses, and dead latent stats during training</li>
</ul>
</li>
<li>
<p dir="auto"><a href="https://drive.google.com/file/d/1HEty3WPThHethMsRqS8duhdKrmdawxeu/view?usp=sharing" rel="nofollow"><strong>Trained 65,536 latents SAE Model</strong></a>:</p>
<ul dir="auto">
<li>Final Trained SAE model after 10 epochs with training log specified configuration</li>
<li>Trained on 6.5 Billion activations from Llama 3.2-3B layer 23</li>
</ul>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Structure</h2><a id="user-content-project-structure" aria-label="Permalink: Project Structure" href="#project-structure"></a></p>
<p dir="auto">The project is organized into four main components:</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">1. Data Capture</h4><a id="user-content-1-data-capture" aria-label="Permalink: 1. Data Capture" href="#1-data-capture"></a></p>
<ul dir="auto">
<li><code>capture_activations.py</code> - Captures LLM residual activations</li>
<li><code>openwebtext_sentences_dataset.py</code> - Custom dataset for sentence-level processing</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">2. SAE Training</h4><a id="user-content-2-sae-training" aria-label="Permalink: 2. SAE Training" href="#2-sae-training"></a></p>
<ul dir="auto">
<li><code>sae.py</code> - Core SAE model implementation</li>
<li><code>sae_preprocessing.py</code> - Data preprocessing for SAE training</li>
<li><code>sae_training.py</code> - Distributed SAE training implementation</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">3. Interpretability</h4><a id="user-content-3-interpretability" aria-label="Permalink: 3. Interpretability" href="#3-interpretability"></a></p>
<ul dir="auto">
<li><code>capture_top_activating_sentences.py</code> - Identifies sentences that maximize feature activation</li>
<li><code>interpret_top_sentences_send_batches.py</code> - Builds and sends batches for interpretation</li>
<li><code>interpret_top_sentences_retrieve_batches.py</code> - Retrieves interpretation results</li>
<li><code>interpret_top_sentences_parse_responses.py</code> - Parses and analyzes interpretations</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">4. Verification and Testing</h4><a id="user-content-4-verification-and-testing" aria-label="Permalink: 4. Verification and Testing" href="#4-verification-and-testing"></a></p>
<ul dir="auto">
<li><code>llama_3_inference.py</code> - Core inference implementation</li>
<li><code>llama_3_inference_text_completion_test.py</code> - Text completion testing</li>
<li><code>llama_3_inference_chat_completion_test.py</code> - Chat completion testing</li>
<li><code>llama_3_inference_text_completion_gradio.py</code> - Gradio interface for interactive testing</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install Poetry if not already installed
curl -sSL https://install.python-poetry.org | python3.12 -

# Clone the repository
git clone https://github.com/PaulPauls/llama3_interpretability_sae
cd llama3_interpretability_sae

# Install exact dependencies the project has been run with
poetry install --sync"><pre><span><span>#</span> Install Poetry if not already installed</span>
curl -sSL https://install.python-poetry.org <span>|</span> python3.12 -

<span><span>#</span> Clone the repository</span>
git clone https://github.com/PaulPauls/llama3_interpretability_sae
<span>cd</span> llama3_interpretability_sae

<span><span>#</span> Install exact dependencies the project has been run with</span>
poetry install --sync</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Implementation Details &amp; Results</h2><a id="user-content-implementation-details--results" aria-label="Permalink: Implementation Details &amp; Results" href="#implementation-details--results"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">1. Custom Llama Implementation</h4><a id="user-content-1-custom-llama-implementation" aria-label="Permalink: 1. Custom Llama Implementation" href="#1-custom-llama-implementation"></a></p>
<p dir="auto">The basis of this research is the custom Llama 3.1/3.2 transformer model implementation in <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/llama_3/model_text_only.py"><code>llama_3/model_text_only.py</code></a>. It is based on the reference implementation from the llama models repository, however I've made several significant modifications to better suit this project. I rewrote the implementation to remove the heavy dependency on the Fairscale library - simply because I am not familiar with it and much more comfortable working directly with PyTorch, thereby hoping to avoid bugs or performance bottlenecks stemming from working with a library I am not familiar with. Similarly, I stripped out the multimodal functionality since investigating image interpretability would have added unnecessary complexity for this initial release.
The modified Llama 3.1/3.2 model implementation includes new constructor options that enable either activation capture or the injection of a trained Sparse Autoencoder (SAE) model at specific layers during inference. This functionality is implemented through the following constructor parameters:</p>
<div dir="auto" data-snippet-clipboard-copy-content="class Transformer(nn.Module):
    def __init__(
        self,
        params: ModelArgs,
        store_layer_activ: list[int] | None = None,
        sae_layer_forward_fn: dict[int, callable] | None = None,
    ):"><pre><span>class</span> <span>Transformer</span>(<span>nn</span>.<span>Module</span>):
    <span>def</span> <span>__init__</span>(
        <span>self</span>,
        <span>params</span>: <span>ModelArgs</span>,
        <span>store_layer_activ</span>: <span>list</span>[<span>int</span>] <span>|</span> <span>None</span> <span>=</span> <span>None</span>,
        <span>sae_layer_forward_fn</span>: <span>dict</span>[<span>int</span>, <span>callable</span>] <span>|</span> <span>None</span> <span>=</span> <span>None</span>,
    ):</pre></div>
<p dir="auto">For the supporting code in the <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/llama_3"><code>llama_3/</code></a> directory, I made a pragmatic decision to keep most of the auxiliary files from the original Llama models repository unchanged. 95% of this auxiliary code is unused and only needed for the chat formatter, which is very dependent on interconnected imports. However, since it wasn't research critical did I decide not to rewrite it and simply carry over the files over.
The actual inference implementation is custom and contained in my <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/llama_3_inference.py"><code>llama_3_inference.py</code></a> module, which provides streaming capabilities for both chat and text completion tasks and will be primarily of use when testing and validating the results. The implementation allows for batched inference and features configurable temperature and top-p sampling parameters, with an automatic fallback to greedy sampling when the temperature is set to 0.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">2. Data Capture</h4><a id="user-content-2-data-capture" aria-label="Permalink: 2. Data Capture" href="#2-data-capture"></a></p>
<p dir="auto">For the data capture I created a custom variant of the <a href="https://huggingface.co/datasets/Skylion007/openwebtext" rel="nofollow">OpenWebText</a> dataset that processes text at the sentence level, capturing 25 million sentences with a maximum length of 192 tokens each. This extensive data collection resulted in 4TB of raw activation data, which compresses to 3.2TB of tar.gz archives. In total I captured approximately 700 million activations from these 25 million contexts, with sentences averaging 27.3 tokens in length.</p>
<p dir="auto">While this dataset is roughly an order of magnitude smaller than those used by Anthropic or Google DeepMind (who both utilized around 8 billion unique activations), it still provides a substantial foundation for training an initial SAE model in my opinion. To compensate for the smaller dataset size I trained the SAE for 10 epochs, effectively processing the same number of total activations as Anthropic and Google DeepMind - the key difference being that my SAE encounters each activation 10 times instead of only once. This approach was simply a monetary constraints for a non-profit side project, as scaling to match their single-epoch approach would have increased my GCP bucket costs from approximately $80/month (for 3.2TB w/ traffic) to $800/month (for 32TB w/ traffic), not to mention the significantly higher costs for instance SSDs during training.</p>
<p dir="auto">The decision to process data at the sentence level was deliberate and grounded in several key considerations. Sentences represent natural linguistic units that contain complete thoughts and concepts, which I hope would lead to more interpretable and semantically meaningful features. This approach avoids artificial truncation of context and prevents <em>contextual bleed</em> of meanings across sentence boundaries, while still capturing essential contextual relationships within grammatically complete units. My thinking was that this makes it easier to attribute discovered features to specific linguistic or semantic phenomena. This approach was also chosen so that the training dataset contains activations adapted to the same linguistic units that I later intended to use for the interpretability analysis.</p>
<p dir="auto">Furthermore I specifically chose to process sentences without the 'beginning-of-sequence' (bos) token to avoid position-specific patterns, as my goal was to interpret features based on their meaning alone, independent of their position in a sequence. Since sentences represent natural semantic units that provide sufficient context for meaningful interpretation while remaining specific enough to identify distinct concepts, this aligns well with the goal of having a LLM analyze the semantic content that activates specific latents.</p>
<p dir="auto">From a technical implementation perspective, I captured residual stream activations after layer normalization from Llama 3.2-3B, specifically from layer 23 out of 28 (positioned just short of 5/6th through the model's depth, following OpenAI's implementation). The capture process utilizes a distributed implementation using NCCL for single-node multi-GPU inference, with asynchronous disk I/O handled through a separate process to prevent GPU processing bottlenecks. The entire data capture process took approximately 12 hours using 4x Nvidia RTX4090 GPUs.</p>
<p dir="auto">OpenAI's implementation also uses residual stream activations after layer normalization, with a context length of 64 tokens and sampling 5/6th-way throughout the GPT-4 model (though they sample more toward the middle layers for smaller models). Anthropic's implementation differs more substantially, using 8 billion MLP activation vectors from 40 million contexts (250 tokens per context) and applying them to residual stream activations at the middle layer of the Claude 3 Sonnet model. In hindsight, there could be a potential for improvement when capturing earlier layers in the next experiments for a relatively small 3B parameter model.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><em>Data Preprocessing</em></h5><a id="user-content-data-preprocessing" aria-label="Permalink: Data Preprocessing" href="#data-preprocessing"></a></p>
<p dir="auto">I am personally a big fan of data preprocessing if complicated batching is necessary. In this case, the challenge was to create batches of 1024 activations each, while handling activations of variable sequence lengths that necessitate carryover handling, potentially in a multiprocessing context. Given the high risk of batching bugs or I/O-related performance bottlenecks, I made the decision to implement a preprocessing phase rather than handling these complexities during training.</p>
<p dir="auto">Since preprocessing was already necessary, I also took the opportunity to calculate the mean tensor across all activations using Welford's algorithm. This algorithm was specifically chosen for its numerical stability and memory efficiency when processing very large datasets. The calculated mean serves as the initial value for the <code>b_pre</code> bias term in the SAE model. While OpenAI's implementation uses the geometric median instead of the mean for initializing <code>b_pre</code>, they only compute it for the first ~30,000 samples of their dataset rather than the complete set and given that the <code>b_pre</code> is optimizable anyway, I decided that using the mean as an initial approximation would be sufficient.</p>
<p dir="auto">The entire preprocessing pipeline is implemented with full CPU parallelization through multiprocessing, ensuring efficient processing of the large activation dataset. This approach simplifies the training process by providing clean, pre-batched data.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">3. SAE Implementation</h4><a id="user-content-3-sae-implementation" aria-label="Permalink: 3. SAE Implementation" href="#3-sae-implementation"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><em>SAE DESIGN</em></h5><a id="user-content-sae-design" aria-label="Permalink: SAE DESIGN" href="#sae-design"></a></p>
<p dir="auto">The core of the Sparse Autoencoder implementation follows a straightforward encoder-decoder architecture with design choices following mainly the choices made by OpenAI. The full forward pass of the TopK Autoencoder includes two key bias terms: <code>b_pre</code> for both encoder and decoder (initialized as the mean calculated in the preprocessing as outlined in the prior section), and <code>b_enc</code> specifically for the encoder (initialized randomly). The complete forward pass can be described as:</p>
<p dir="auto">Encoder: <code>h = TopK(W_enc(x - b_pre) + b_enc)</code></p>
<p dir="auto">Decoder: <code>x^ = W_dec * h (+ h_bias) + b_pre</code></p>
<p dir="auto">Sparsity in the latent space is enforced through the TopK activation function, which retains only the k largest activations and sets the rest to zero. This approach directly controls sparsity without requiring an L1 penalty term for sparsity in the loss function as required by Anthropic's approach. The model includes an optional <code>h_bias</code> parameter that remains disabled during training but can be activated afterwards for feature steering, allowing dynamic manipulation of the latent space post-training.</p>
<p dir="auto">For numerical precision I chose to work with float32 dtype due to its quick and precise conversion compatibility with Llama's required bfloat16 dtype. Both formats share the same 1 sign bit and 8 exponent bits structure, differing only in their mantissa bits (23 vs 7), making conversions quick and accurate.</p>
<p dir="auto">My implementation differs from both Anthropic's and OpenAI's approaches in several ways. Anthropic uses a one-hidden-layer MLP with ReLU activation and enforces sparsity through an L1 penalty instead of TopK. They also worked with massively larger latent sizes (~1M, ~4M, and ~34M features), though their average number of active features per token remained below 300 for all SAEs. OpenAI's architecture is more similar to my implementation, but they experimented with latent sizes from 2^11 (2,048) up to 2^24 (16.7M) for GPT-4. Their experiments showed that larger latent sizes generally produced better loss and feature explainability, while lower k values (fewer active latents) led to more interpretable features.</p>
<p dir="auto">For this project - working with the 3B parameter Llama 3.2 model with a residual stream dimension of 3,072 - I chose a latent dimension of 2^16 (65,536) and a k value of 64. This decision aims to strike a balance between several factors: providing sufficient feature capacity at approximately 21x the residual stream dimension, maintaining computational efficiency as suggested by the OpenAI and Google DeepMind papers, and staying within the project's monetary constraints when training on ~8 billion activations for comparability. The k value of 64 was selected to achieve a good balance between reconstruction power and the strong sparsity needed for interpretable features.</p>
<p dir="auto">In hindsight however, as I outline in <a href="#6-future-work--improvements">section 6</a>, I would considerably increase the latent size and decrease the k value for future experiments to improve the variety and interpretability of the features and attempt to find efficiency improvements to stay within budget constraints. As a first full run of the project, I am however very happy with the chosen hyperparameters and results.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><em>SAE TRAINING METHODOLOY</em></h5><a id="user-content-sae-training-methodoloy" aria-label="Permalink: SAE TRAINING METHODOLOY" href="#sae-training-methodoloy"></a></p>
<p dir="auto">The training configuration of the Sparse Autoencoder was chosen to balance efficiency and feature interpretability. The core hyperparameters reflect both the model architecture and the training dynamics:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Set up configuration
d_model = 3072
n_latents = 2**16  # 65536
k = 64
k_aux = 2048
aux_loss_coeff = 1 / 32
dead_steps_threshold = 80_000  # ~1 epoch in training steps
sae_normalization_eps = 1e-6
batch_size = 1024
num_epochs = 10
early_stopping_patience = 10  # disabled
learning_rate = 5e-5
learning_rate_min = learning_rate / 5
optimizer_betas = (0.85, 0.9999)
optimizer_eps = 6.25e-10
dtype = torch.float32
dataloader_num_workers = 8
logs_per_epoch = 1000
train_val_split = 0.95"><pre><span># Set up configuration</span>
<span>d_model</span> <span>=</span> <span>3072</span>
<span>n_latents</span> <span>=</span> <span>2</span><span>**</span><span>16</span>  <span># 65536</span>
<span>k</span> <span>=</span> <span>64</span>
<span>k_aux</span> <span>=</span> <span>2048</span>
<span>aux_loss_coeff</span> <span>=</span> <span>1</span> <span>/</span> <span>32</span>
<span>dead_steps_threshold</span> <span>=</span> <span>80_000</span>  <span># ~1 epoch in training steps</span>
<span>sae_normalization_eps</span> <span>=</span> <span>1e-6</span>
<span>batch_size</span> <span>=</span> <span>1024</span>
<span>num_epochs</span> <span>=</span> <span>10</span>
<span>early_stopping_patience</span> <span>=</span> <span>10</span>  <span># disabled</span>
<span>learning_rate</span> <span>=</span> <span>5e-5</span>
<span>learning_rate_min</span> <span>=</span> <span>learning_rate</span> <span>/</span> <span>5</span>
<span>optimizer_betas</span> <span>=</span> (<span>0.85</span>, <span>0.9999</span>)
<span>optimizer_eps</span> <span>=</span> <span>6.25e-10</span>
<span>dtype</span> <span>=</span> <span>torch</span>.<span>float32</span>
<span>dataloader_num_workers</span> <span>=</span> <span>8</span>
<span>logs_per_epoch</span> <span>=</span> <span>1000</span>
<span>train_val_split</span> <span>=</span> <span>0.95</span></pre></div>
<p dir="auto">The loss function combines a main reconstruction loss resulting from the reconstruction error with a complex auxiliary loss designed to prevent and revive dead latents in the following way: <code>total_loss = main_loss + aux_loss_coeff * aux_loss</code>. Following OpenAI's approach, I set <code>aux_loss_coeff = 1/32</code>. Both losses are computed in normalized space to ensure equal contribution from all features regardless of their original scale, which helps maintain numerical stability throughout training.</p>
<p dir="auto">The auxiliary loss was proposed by OpenAI and plays a crucial role in preventing dead latents through a clever mechanism: it calculates the MSE between the main reconstruction residual (the difference between input and main reconstruction) and a special auxiliary reconstruction. This auxiliary reconstruction uses the same pre-activation latent as the main reconstruction, but it takes only the top-(aux-k) activation values from latents that haven't fired recently (which is tracked through the shared <code>stats_last_nonzero</code> tensor) and sends them through the decoder again to get this 'auxiliary reconstruction'. This gives these <code>k_aux = 2048</code> inactive latents that haven't even activated during the actual training where only the top <code>k</code> latents are used for the reconstruction a dedicated learning signal to capture information missed by these main latents. This makes the dead latents more likely to activate in future forward passes and thereby allows for reviving dead latents, keeping all latents alive and useful.</p>
<p dir="auto">The training only consider dead latents for the auxiliary loss. A latent is considered dead if it hasn't been activated in <code>dead_steps_threshold</code> training steps (set to 80,000 steps, approximately one epoch in my setup), which equates to no activation in the reconstruction of the last ~650M activations given a batch size of 8192. This threshold serves two purposes: it ensures the main loss has sufficient warm-up time before receiving an auxiliary loss signal, and it guarantees that we only attempt to revive latents that haven't fired once after seeing all unique training data activations.</p>
<p dir="auto">The training infrastructure employs distributed training using the NCCL backend for a single-node multi-GPU setup. Using 8x Nvidia RTX4090 GPUs for 10 epochs with a per-GPU batch size of 1024 (effective batch size 8192), I processed approximately 7 billion activations over a span of a little over 7 days. The number of epochs was chosen to match the total number of processed activations in Anthropic's and Google DeepMind's experiments. All training progress, including losses and debugging statistics about dead latents, was tracked comprehensively via Weights &amp; Biases.</p>
<p dir="auto">The parameters of the optimzier were carefully tuned for the specific challenges of training a sparse autoencoder where some features activate extremely rarely. I chose a base learning rate of 5e-5 after comparative testing showed it achieved similar optimization speed to higher rates while promising better fine-tuning potential for sparse features in later training stages. The learning rate follows a cosine annealing schedule down to a minimum of 1e-5 (1/5 of initial).
The AdamW configuration required special consideration for the sparse nature of the autoencoder:</p>
<ul dir="auto">
<li><code>beta_1 = 0.85</code> (lower than the typical 0.9 to make individual updates more meaningful given the large effective batch size of 8192 and sparse nature of the autoencoder)</li>
<li><code>beta_2 = 0.9999</code> (accommodates sparse activation patterns where some features might activate very rarely and therefore need longer momentum preservation)</li>
<li><code>eps = 6.25e-10</code> (provides sufficient numerical stability for float32 precision while allowing precise parameter updates needed for optimizing rare activation patterns)</li>
</ul>
<p dir="auto">Weight initialization and normalization were implemented with particular attention to training stability as recommended by the OpenAI paper. The encoder and decoder weights are initialized orthogonally (with the decoder as the transpose of the encoder) to ensure balanced, independent initial feature directions. Input features are normalized with a small epsilon term for training robustness. Following empirical findings from both the OpenAI paper and <a href="https://transformer-circuits.pub/2023/monosemantic-features" rel="nofollow">Bricken et al. [2023]</a>, the decoder weights are explicitly normalized to unit norm after initialization and each training step, as this improves MSE performance.</p>
<p dir="auto">A key implementation detail is the gradient projection via <code>project_decoder_grads()</code>, which maintains the unit-norm constraint on decoder weights by removing gradient components parallel to existing dictionary vectors. This projection helps stabilize training and prevents the autoencoder from learning redundant or degenerate features when identifying sparse patterns in the data:</p>
<div dir="auto" data-snippet-clipboard-copy-content="def project_decoder_grads(self):
    &quot;&quot;&quot;Project out gradient information parallel to dict vectors.&quot;&quot;&quot;
    # Compute dot product of decoder weights and their grads, then subtract the projection from the grads
    # in place to save memory
    proj = torch.sum(self.decoder.weight * self.decoder.weight.grad, dim=1, keepdim=True)
    self.decoder.weight.grad.sub_(proj * self.decoder.weight)"><pre><span>def</span> <span>project_decoder_grads</span>(<span>self</span>):
    <span>"""Project out gradient information parallel to dict vectors."""</span>
    <span># Compute dot product of decoder weights and their grads, then subtract the projection from the grads</span>
    <span># in place to save memory</span>
    <span>proj</span> <span>=</span> <span>torch</span>.<span>sum</span>(<span>self</span>.<span>decoder</span>.<span>weight</span> <span>*</span> <span>self</span>.<span>decoder</span>.<span>weight</span>.<span>grad</span>, <span>dim</span><span>=</span><span>1</span>, <span>keepdim</span><span>=</span><span>True</span>)
    <span>self</span>.<span>decoder</span>.<span>weight</span>.<span>grad</span>.<span>sub_</span>(<span>proj</span> <span>*</span> <span>self</span>.<span>decoder</span>.<span>weight</span>)</pre></div>
<p dir="auto">My implementation differs from both Anthropic's and OpenAI's approaches in several ways. Anthropic uses a combination of L2 reconstruction error and L1 regularization for sparsity, implements periodic neuron resampling for dead neurons and modifies gradient updates for decoder weight normalization. While they used the same batch size of 8192, their approach to maintaining sparsity and handling dead neurons is quite different from my TopK implementation.</p>
<p dir="auto">OpenAI's implementation is much closer to mine but uses different Adam optimizer settings (<code>beta_1 = 0.9</code>, <code>beta_2 = 0.999</code>), maintains a constant learning rate, implements gradient clipping for stability and considers neurons dead much earlier (no activation after 10M tokens). They also use an EMA of weights instead of raw optimization weights and opted for an extremely large batch size of 131,072 tokens.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><em>SAE TRAINING RESULTS</em></h5><a id="user-content-sae-training-results" aria-label="Permalink: SAE TRAINING RESULTS" href="#sae-training-results"></a></p>
<p dir="auto">The training process ran for ~7 days on 8x Nvidia RTX4090 GPUs, demonstrating stable and efficient convergence throughout. The training progression showed a nice logarithmic decay in the loss function, ultimately achieving a final total normalized loss of approximately 0.144:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/sae_training_2024-10-28_train-total-loss.png"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/sae_training_2024-10-28_train-total-loss.png" alt="SAE Training 2024-10-28, Train Total Loss"></a>
  <br>
  <em>Fig 1: SAE Training - Total Loss</em>
</p>
<p dir="auto">The validation loss was computed on a held-out 5% of the training data and exhibited a similar logarithmic decay pattern, though predictably less steep than the training loss:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/sae_training_2024-10-28_val-total-loss.png"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/sae_training_2024-10-28_val-total-loss.png" alt="SAE Training 2024-10-28, Val Total Loss"></a>
  <br>
  <em>Fig 2: SAE Training - Validation Loss</em>
</p>
<p dir="auto">A particularly interesting aspect of the training dynamics emerged after the initial warm-up period of 80,000 training steps. At this point, about 40% of the latents were identified as "dead" - meaning they hadn't activated once so far. However, the auxiliary loss mechanism proved remarkably effective at reviving these dead latents quickly:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/sae_training_2024-10-28_dead-latents-ratio.png"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/sae_training_2024-10-28_dead-latents-ratio.png" alt="SAE Training 2024-10-28, Dead Latents Ratio"></a>
  <br>
  <em>Fig 3: Dead Latents Ratio - Rapid decrease after warm-up period, stabilizing at minimum threshold</em>
</p>
<p dir="auto">The auxiliary loss started quite high but also showed quick decay as it successfully revived dead latents:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/sae_training_2024-10-28_train-aux-loss.png"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/sae_training_2024-10-28_train-aux-loss.png" alt="SAE Training 2024-10-28, Train Aux Loss"></a>
  <br>
  <em>Fig 4: SAE Training - Aux Loss</em>
</p>
<p dir="auto">An interesting implementation detail emerged regarding the auxiliary loss calculation: it only triggers when at least k_aux (2,048) latents are dead, effectively establishing a soft lower bound of dead latents at approximately 3% (2,048/65,536), which is very visible in Figure 3. I initially implemented this condition as an optimization to avoid unnecessary auxiliary loss calculations when few latents were dead. Surprisingly to me, the auxiliary loss mechanism was so effective at reviving dead latents that it consistently drove the dead latent count toward this lower bound, particularly in the later stages of training where the auxiliary loss was frequently zero due to insufficient dead latents to trigger the calculation.</p>
<p dir="auto">One reason why I was surprised by such an effective revival of dead latents was that I expected a much higher percentage of dead latents. Anthropic and OpenAI both reported up to 65% dead latents in certain configurations, though admittedly their latent size was 1 to 2 orders of magnitude larger than mine. The effectiveness of the auxiliary loss implementation, combined with the gradient projection technique for stabilizing training dynamics appears to create a very robust training. For future experiments though, removing the minimum dead latents threshold for auxiliary loss calculation could potentially allow for even fewer dead latents, though I am happy with the results of the current implementation.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">4. Interpretability Analysis</h4><a id="user-content-4-interpretability-analysis" aria-label="Permalink: 4. Interpretability Analysis" href="#4-interpretability-analysis"></a></p>
<p dir="auto">The interpretability analysis approach builds upon methods established in Anthropic's research on scaling monosemanticity, but with a key difference in granularity. While Anthropic primarily focused on single-token analysis, this implementation captures and analyzes complete sentences - specifically the top 50 sentences that most strongly activate each latent. The activation strength is calculated using a mean and last-token aggregation across all tokens in a sentence, which is intended to hopefully provide a more holistic view of semantic activation patterns in Llama 3.2-3B's intermediate layers.</p>
<p dir="auto">As I already addressed in <a href="#2-data-capture">section 2</a>, the decision to use sentence-level analysis instead of token-level analysis was deliberate and based on the hopes of combining linguistic principles with a simple approach for a first release. Sentences represent natural linguistic units that contain complete thoughts and in my opinion provide a great balance between context and specificity. This approach prevents both the artificial truncation of context and the potential mixing of meanings across sentence boundaries (<em>contextual bleed</em>). To aggregate all latent activations in a sequence I primarily chose to rely on 2 methods:</p>
<ul dir="auto">
<li><code>mean</code> aggregation to hopefully reveal features that maintain a consistent activation throughout a sentence, highlighting a sustained semantic theme</li>
<li><code>last</code> aggregation (simply taking the last token's activations) to hopefully leverage an LLMs autoregressive nature and capture the final representation that has seen the whole sentence through self-attention</li>
</ul>
<p dir="auto">For the semantic analysis itself, I utilized the most advanced frontier LLM that is available to me at the time of this project: Claude 3.5 (specifically <code>claude-3-5-sonnet-20241022</code>) with a structured chain-of-thought prompt. I employed an automated approach that allows for scalable interpretation while hopefully maintaining semantic specificity. The prompt is supposed to guide Claude through specific analysis steps for all 50 supplied sentences:</p>
<ol dir="auto">
<li>Identify key words and phrases</li>
<li>Group thematic elements</li>
<li>Consider potential outliers</li>
<li>Provide a final semantic interpretation with a confidence score</li>
</ol>
<p dir="auto">This analysis pipeline is implemeneted in the three stages of <em>sending analysis requests in cost-effective batches</em>, <em>retrieving responses</em>, and <em>parsing and processing the semantic interpretations</em>. All intermediate data is preserved for reproducibility and further analysis:</p>
<ul dir="auto">
<li>Raw sentences, their activation aggregations, and OpenWebText indices are stored in <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/captured_top_sentences"><code>capture_top_sentences/</code></a></li>
<li>Unprocessed semantic analysis responses are saved in <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/top_sentences_last_responses"><code>top_sentences_last_responses/</code></a> and <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/top_sentences_mean_responses"><code>top_sentences_mean_responses/</code></a></li>
<li>Processed results mapping latents to their identified 'common_semantic' and 'certainty score' are stored in <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/latent_index_meaning"><code>latent_index_meaning/</code></a></li>
</ul>
<p dir="auto">To illustrate the analysis process let's take a look at latent #896, which was identified as representing "References to United Nations institutions, personnel, operations, or official documentation using formal institutional terminology". Claude's detailed analysis included:</p>
<div data-snippet-clipboard-copy-content="<semantic_analysis>
1. Key Word Analysis:
    - Frequent terms: &quot;UN&quot;, &quot;United Nations&quot;, &quot;Secretary-General&quot;
    - Official titles: &quot;Special Rapporteur&quot;, &quot;Under-Secretary-General&quot;, &quot;Coordinator&quot;
    - Department names: &quot;UNDP&quot;, &quot;UNHCR&quot;, &quot;OCHA&quot;, &quot;UNODC&quot;

2. Thematic Grouping:
    - UN organizational structure references
    - UN personnel and positions
    - UN reports and documentation
    - UN agencies and bodies
    - UN operations and activities

3. Pattern Analysis:
    - All sentences reference UN entities, personnel, or activities
    - Formal institutional language
    - Heavy use of official titles and department names
    - References to official documents and reports

4. Strength Assessment:
    - 50 out of 50 sentences contain direct UN references
    - Mix of department names, personnel titles, and activities
    - Consistent institutional terminology
    - No outliers identified

5. Certainty Calculation:
    - 100% of sentences contain UN references
    - Very strong institutional terminology consistency
    - Clear organizational focus
    - Direct and explicit connections

The analysis reveals that every sentence relates to United Nations organizations, personnel, operations, or documentation, with extremely consistent institutional terminology and structure. The commonality is both obvious and comprehensive.
</semantic_analysis>

{
    &quot;common_semantic&quot;: &quot;References to United Nations institutions, personnel, operations, or official documentation using formal institutional terminology&quot;,
    &quot;certainty&quot;: 1.0
}"><pre lang="text"><code>&lt;semantic_analysis&gt;
1. Key Word Analysis:
    - Frequent terms: "UN", "United Nations", "Secretary-General"
    - Official titles: "Special Rapporteur", "Under-Secretary-General", "Coordinator"
    - Department names: "UNDP", "UNHCR", "OCHA", "UNODC"

2. Thematic Grouping:
    - UN organizational structure references
    - UN personnel and positions
    - UN reports and documentation
    - UN agencies and bodies
    - UN operations and activities

3. Pattern Analysis:
    - All sentences reference UN entities, personnel, or activities
    - Formal institutional language
    - Heavy use of official titles and department names
    - References to official documents and reports

4. Strength Assessment:
    - 50 out of 50 sentences contain direct UN references
    - Mix of department names, personnel titles, and activities
    - Consistent institutional terminology
    - No outliers identified

5. Certainty Calculation:
    - 100% of sentences contain UN references
    - Very strong institutional terminology consistency
    - Clear organizational focus
    - Direct and explicit connections

The analysis reveals that every sentence relates to United Nations organizations, personnel, operations, or documentation, with extremely consistent institutional terminology and structure. The commonality is both obvious and comprehensive.
&lt;/semantic_analysis&gt;

{
    "common_semantic": "References to United Nations institutions, personnel, operations, or official documentation using formal institutional terminology",
    "certainty": 1.0
}
</code></pre></div>
<p dir="auto">From a cost perspective, this interpretability analysis proved remarkably efficient compared to the dataset capture, storage, and SAE training phases. Processing 24,828,558 input tokens and generating 3,920,044 output tokens with Claude 3.5 in batch mode cost only $66.74.</p>
<p dir="auto">While this approach to semantic analysis is relatively straightforward, it was chosen as a solid initial method for both feature interpretation and potential feature steering. The sentence-level analysis helps avoid uncertainties around when specific latents should be activated, though this simplicity does certainly come at the cost to result quality. I considered developing more sophisticated interpretation methods, but this seemed like a complex research challenge that could potentially be debated and refined for months. Anthropic for example is not only publishing great papers on this topic but also consistently high quality blog posts about it for years at <a href="https://transformer-circuits.pub/" rel="nofollow">transformer-circuits.pub</a>. So for this initial release, I opted for a simpler approach that validates my full pipeline first before potentially making improvements on it in the future.</p>
<p dir="auto">So as outlined, my approach differs intentionally quite considerably from both Anthropic and OpenAI in several key aspects:</p>
<p dir="auto">Anthropic's approach to interpretability focuses on analyzing individual token activations in Claude 3 Sonnet, working with sparse autoencoders containing up to 34M features. Their methodology combines manual feature inspection with comprehensive validation through steering experiments and ablation studies. The validation protocol specifically examines feature interactions, activation patterns, and the impact of feature manipulation on model behavior. This multi-faceted approach allows them to verify both the existence and significance of identified features while providing insights into how these features contribute to the model's overall behavior.</p>
<p dir="auto">OpenAI's implementation similarly focuses on individual token activations but takes a different approach to analysis, examining both specific activations and random activating examples. Their methodology emphasizes automated interpretability at scale utilizing a lot of technical metrics. These include probe loss measurements, N2G (Neuron to Graph) pattern matching for feature identification, and multiple quality assessment metrics such as downstream loss, ablation sparsity, and explanation precision/recall. Furthermore OpenAI is also very systemic in assessing the quality of the detected features, using multiple quantitative metrics to evaluate the reliability and usefulness of identified features.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">5. Verification and Testing</h4><a id="user-content-5-verification-and-testing" aria-label="Permalink: 5. Verification and Testing" href="#5-verification-and-testing"></a></p>
<p dir="auto">The verification and testing infrastructure consists of three main components designed to analyze and validate the SAE's impact on model behavior:</p>
<ul dir="auto">
<li><code>llama_3_inference_chat_completion_test.py</code></li>
<li><code>llama_3_inference_text_completion_test.py</code></li>
<li><code>llama_3_inference_text_completion_gradio.py</code></li>
</ul>
<p dir="auto">These scripts enable both feature activation analysis and feature steering through text and chat completion tasks. Each implementation supports batched inference (considering each line a separate batch element), configurable temperature and top-p parameters, and most importantly the ability to inject a trained SAE model for feature analysis and steering.</p>
<p dir="auto">The semantic meanings and certainty scores for each latent - derived in the earlier interpretability analysis in <a href="#4-interpretability-analysis">section 4</a> - are stored in the <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/latent_index_meaning"><code>latent_index_meaning/</code></a> directory. These processed results serve as the basis for both feature activation analysis and steering experiments. To demonstrate the practical application of these tools, let's look at a concrete example using four sample input prompts, text completed in the terminal-UI with settings <code>max_new_tokens=128, temperature=0.7, top_p=0.9, seed=42</code>, demonstrated in figure 5:</p>
<div data-snippet-clipboard-copy-content="The delegates gathered at the
Foreign officials released a statement
Humanitarian staff coordinated their efforts
Senior diplomats met to discuss"><pre lang="text"><code>The delegates gathered at the
Foreign officials released a statement
Humanitarian staff coordinated their efforts
Senior diplomats met to discuss
</code></pre></div>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/testing_terminal.webp"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/testing_terminal.webp" alt="Inference Testing in Terminal"></a>
  <br>
  <em>Fig 5: Terminal UI inference without feature steering</em>
</p>
<p dir="auto">Aside from feature activation analysis it is also possible to perform feature steering experiments with the same sample sentences and configuration. While this is also possible in the terminal UI does figure 6 show such a feature steering using the gradio UI for the sake of demonstration. In this example the latent #896 is targeted, which earlier analysis identified as representing "References to United Nations institutions, personnel, operations, or official documentation using formal institutional terminology". By increasing this latent's activation value by 20 through the dynamically adjustable <code>h_bias</code> (see <a href="#_sae-design_">section 3.1</a> for a reminder of the placement of <code>h_bias</code>) can the model's text completion successfully be steered toward UN-related content.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/feature_steering_gradio.webp"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/feature_steering_gradio.webp" alt="Feature Steering in Gradio UI"></a>
  <br>
  <em>Fig 6: Gradio UI inference with feature steering towards UN-related content</em>
</p>
<p dir="auto">The feature steering is not particularly strong in this first beta version of the project, hence why in the example above only the second and third sentence are flipping over to UN related content. Because of this the sample prompts were also chosen so that the start of the text completion provides a chance that the completion can be steered towards the United Nations, as for example feature steering towards the UN for a sentence that starts with "For any n, if 2n - 1 is odd" would certainly fail.</p>
<p dir="auto">This limitation of feature steering stems from the focus on feature extraction rather than steering optimization during interpretability analysis. However while this means that the steering capabilities produce inconsistent results, is it worth emphasizing that feature extraction alone provides valuable insights into the model. Therefore I just consider the ability to also perform feature steering a nice additional showcase and interesting demonstration in this first project release.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">6. Potential Future Work &amp; Improvements</h4><a id="user-content-6-potential-future-work--improvements" aria-label="Permalink: 6. Potential Future Work &amp; Improvements" href="#6-potential-future-work--improvements"></a></p>
<ul dir="auto">
<li>Expanding the latent dimension to at least 2^18 (262,144) features while reducing k to 32. This would provide more capacity for discovering unique features while maintaining stronger sparsity, potentially leading to more interpretable and monosemantic features. The increased computational demands would need to be offset somehow, by potentially increasing efficiency or implementing things like gradient accumulation.</li>
<li>Implementing comprehensive activation tracking of the latents, e.g. by often logging the state of the <code>latent_last_nonzero</code> tensor throughout training, rather than just using the basic debug logs I used so far. This would provide deeper insights into how and when latents become active or die and how their activation is distributed.</li>
<li>Adding support for analyzing feature interactions despite sparsity by tracking co-activation patterns in the latent space. Understanding how features work together could provide insights into more complex semantic representations and could potentially improve both interpretability and steering capabilities.</li>
<li>Developing more sophisticated interpretability analysis methods, particularly in grouping high-activating sentences and n-grams. While the current sentence-level analysis provides a good foundation, more nuanced approaches to pattern recognition could reveal subtler semantic features and improve our understanding of how the model represents information.</li>
<li>Similarly, also performing not only feature extraction interpretability analysis but also feature steering interpretability analysis, though admittedly in sufficiently sophisticated methods this would both coincide.</li>
<li>Extending the research to include Llama 3.1-8B model activations. Since it shares the same codebase as Llama 3.2, this would be a very straightforward extension that would essentially only require an adaptation of the hyperparameters and a lot of compute power.</li>
<li>Experimenting with different activation capture points, varying from depth into the model (particularly earlier layers) to using different capture points inside the transformer block (e.g. using the Attention Head Outputs or MLP outputs as experimented on by Google DeepMind)</li>
<li>Further optimize the auxiliary loss mechanism based on the surprisingly effective results in preventing dead latents. The current implementation already shows strong performance, but investigating the relationship between the minimum dead latents threshold and feature quality could lead to even better training dynamics.</li>
<li>Experimenting with modifications to the SAE architecture, particularly around the bias terms and main loss function. Given the success of the current implementation, targeted adjustments to these components could potentially improve both training stability and feature interpretability while maintaining the benefits of the current design.</li>
<li>Adding proper docstrings throughout the codebase. While I added inline documentation everywhere throughout the codebase would a proper addition of docstrings be very beneficial. This is not how I'd normally deliver production code though I simple didn't find the time to add proper docstrings to the codebase and I considered it sufficient for a first release of this sideproject.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use this code in your research or project, please cite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{pauls2024sae,
    title = {Llama 3 Interpretability with Sparse Autoencoders},
    author = {Paul Pauls},
    year = {2024},
    publisher = {GitHub},
    howpublished = {\url{https://github.com/PaulPauls/llama3_interpretability_sae}}
}"><pre><span>@misc</span>{<span>pauls2024sae</span>,
    <span>title</span> = <span><span>{</span>Llama 3 Interpretability with Sparse Autoencoders<span>}</span></span>,
    <span>author</span> = <span><span>{</span>Paul Pauls<span>}</span></span>,
    <span>year</span> = <span><span>{</span>2024<span>}</span></span>,
    <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
    <span>howpublished</span> = <span><span>{</span>\url{https://github.com/PaulPauls/llama3_interpretability_sae}<span>}</span></span>
}</pre></div>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>