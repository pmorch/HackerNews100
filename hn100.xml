<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 22 Mar 2024 11:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: Mapping almost every law, regulation and case in Australia (115 pts)]]></title>
            <link>https://umarbutler.com/mapping-almost-every-law-regulation-and-case-in-australia/</link>
            <guid>39788322</guid>
            <pubDate>Fri, 22 Mar 2024 07:53:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://umarbutler.com/mapping-almost-every-law-regulation-and-case-in-australia/">https://umarbutler.com/mapping-almost-every-law-regulation-and-case-in-australia/</a>, See on <a href="https://news.ycombinator.com/item?id=39788322">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>What if you could take every law, regulation and case in Australia and project them onto a two-dimensional map such that their distance from one another was proportional to their similarity in meaning? What would that look like?</p><p>Perhaps something like this.</p><div id="oalm-mobile"><figure> <img decoding="async" src="https://umarbutler.com/wp-content/uploads/oalm/map.oalm.svg" alt="The first ever map of Australian law." data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure><p> <em>Note: since you’re on a mobile device, the map has been replaced with a screenshot. Hop on a computer to experience the map in all its interactive glory.</em></p></div><p>This is the first ever map of Australian law. Each point represents a unique law, regulation or case in the <a href="https://huggingface.co/datasets/umarbutler/open-australian-legal-corpus">Open Australian Legal Corpus</a>, the world’s largest open source database of Australian law (you can learn about how I built that corpus <a href="https://umarbutler.com/how-i-built-the-largest-open-database-of-australian-law/">here</a>).</p><p>The closer any two documents are on the map, the more similar they are in meaning.</p><p>If you’re on a computer and hover over a document, you’ll see its title, type, jurisdiction and category. You can open a document by clicking on it.</p><p>Documents are coloured by category. The legend on the right shows what colour each category correspond to. Click on a category and you’ll exclude it from the map. Double click and you’ll only see documents from that category.</p><p>Over the course of this article, I’ll cover exactly what the map can teach us about Australian law as well as give you a behind-the-scenes look at how I built it, providing code examples along the way. Those more interested in the technology powering the map can skip straight to that section <a href="#so-howd-you-do-it" data-type="internal" data-id="#so-howd-you-do-it">here</a>.</p><h2>What can we learn from it?</h2><p>While it might not look like much at first, the map gives us a rare look into some of the many hidden ways Australian laws, regulations and cases are both connected to and disconnected from one another.</p><h3><em>The invisible barrier between cases and legislation</em></h3><p>It is readily observable, for example, that there is a sort of invisible barrier separating cases on the one hand from legislation on the other. This barrier corresponds roughly with the map’s north and south poles.</p><figure><img decoding="async" src="https://umarbutler.com/wp-content/uploads/2024/02/types.map_.oalm_-2.svg" alt="" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption><em>An annotated version of the map where cases and legislation are enclosed in two shapes corresponding with the map’s north and south poles, respectively.</em></figcaption></figure><p>The presence of this barrier tells us that documents of the same type will tend to share more in common with each other than they will with documents of the same subject matter.</p><p>Although they may often focus on the same topics, cases and legislation are, after all, written in different styles, towards different ends.</p><h3>The absence of borders between documents of different jurisdictions</h3><p>Interestingly, however, we find no such borders between documents of different jurisdictions; although, it is worth noting, due to copyright restrictions, the Open Australian Legal Corpus only contains decisions from the Commonwealth and New South Wales and is missing legislation from Victoria, the Northern Territory and the Australian Capital Territory.</p><figure><img decoding="async" src="https://umarbutler.com/wp-content/uploads/2024/02/jurisdictions.annotated.map_.oalm_.svg" alt="" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption><em>An alternate view of the map where documents are coloured by jurisdiction, illustrating the lack of boundaries between documents of different jurisdictions.</em></figcaption></figure><p>The absence of borders between cases and legislation of different jurisdictions indicates that Australian state and federal law is relatively homogenous. There are no differences between the style, principles of interpretation or general jurisprudence of state and federal law that appear to be reflected in the map. Of what borders do exist between state and federal law, they correspond better with differences in subject matter than they do with the jurisprudence of their jurisdictions.</p><p>This conforms with fact that state and federal courts and legislatures operate within a single legal framework, under which they have jurisdiction over matters prescribed by the Constitution in their territory, with a single court, the High Court of Australia, arbitrating on disputes between governments over the precise limits of those constitutional rights and powers.</p><h3>The judicial and legislative mainlands and islands</h3><p>Turning back to the barrier between cases and legislation, we also observe that, within the map’s north and south poles, each pole has a ‘mainland’ of sorts that most documents belong to, and then there are a range of ‘islands’ that orbit those mainlands, typically consisting of documents of the same subject matter.</p><figure><img decoding="async" src="https://umarbutler.com/wp-content/uploads/2024/02/islands.map_.oalm_.svg" alt="" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption><em>An annotated version of the map where ‘islands’ of documents of the same subject matter are enclosed in shapes that orbit ‘mainlands’ of cases and legislation.</em></figcaption></figure><p>The fact that there are judicial and legislative mainlands suggests that most cases and legislation draw from and feed into a single, interconnected pool of knowledge.</p><p>This is not particularly surprising. What is surprising is that there are large islands of legislation and judgments that are entirely cut off from their respective mainlands.</p><p><a href="https://www.abf.gov.au/importing-exporting-and-manufacturing/tariff-concessions-system/tariff-concession-order">Tariff concession orders</a>, for example, form their very own unique archipelago, perhaps because each order is centred around regulating a distinct, often quite technical class of importable goods, from <a href="https://www.legislation.gov.au/F2007L04317/latest/text">magazine holders</a> to <a href="https://www.legislation.gov.au/F2007L00538/latest/text">forklifts</a>.</p><p>There is also quite a sizeable island of <a href="https://en.wikipedia.org/wiki/Airworthiness_Directive">airworthiness directives</a> primarily focused on regulating aircraft components, another highly technical domain.</p><p>Somewhat unexpectedly, the largest island by surface area consists almost entirely of migration cases. Furthermore, of all 19 possible branches of law, migration and family law are the only two to be found more often outside a mainland than inside one.</p><p>Migration and family law are, in effect, the most isolated areas of Australian law on the map.</p><p>Funnily enough, while researching why that might be, I stumbled upon this rather pertinent quote from <a href="https://en.wikipedia.org/wiki/Jonathan_Sumption,_Lord_Sumption">Lord Sumption</a>:</p><blockquote><p>Courts exercising family jurisdiction do not occupy a <em>desert island</em> in which general legal concepts are suspended or mean something different. If a right of property exists, it exists in every division of the High Court and in every jurisdiction of the county courts. If it does not exist, it does not exist anywhere.</p><p><a href="https://www.supremecourt.uk/cases/uksc-2013-0004.html"><em>Prest v Petrodel Resources Ltd</em></a> [2013] UKSC 34, [37] (emphasis added)</p></blockquote><p>I also discovered that <a href="https://en.wikipedia.org/wiki/James_Munby">Munby LJ</a>, later President of the UK Family Division, had likewise once quipped:</p><blockquote><p>The Family Division is part of the High Court. It is not some legal <a href="https://en.wikipedia.org/wiki/Whitefriars,_London#Alsatia">Alsatia</a> where the common law and equity do not apply. The rules of agency apply there as much as elsewhere. But in applying those rules one must have regard to the context …</p><p><em><a href="https://www.bailii.org/ew/cases/EWCA/Civ/2011/79.html">Richardson v Richardson</a></em> [2011] EWCA Civ 79, [53]</p></blockquote><p>It would seem that there was already a <a href="https://www.iclr.co.uk/blog/commentary/family-law-no-island-entire-unto-itself/">perception</a> that family law is somewhat isolated from the rest of the law, which the map appears to support.</p><p>As for migration law, although I was unable to locate equally apropos quotes, from my own review of a selection of cases on the map, they appear relatively self-contained in that they tend to reference legislation and cases particular to migration law. It also makes sense that migration law would be a little distant from other areas of law given its unique subject matter.</p><p>While not as insular as family and migration law, it is also worth addressing the relatively large hexagram-shaped island of criminal law, which features a tail of transport and administrative law cases coming out of it.</p><p>That island appears to consist mostly of substantive criminal law cases (along with certain punitive transport and administrative law cases focused on the suspension of various types of licences), whereas the criminal law cases connected to the judicial mainland tend to concern criminal procedure.</p><figure><img decoding="async" src="https://umarbutler.com/wp-content/uploads/2024/03/criminal.annotated.map_.oalm_.svg" alt="" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption><em>An annotated version of criminal law cases with the island of substantive criminal law cases and the cluster of procedural criminal law cases connected to the judicial mainland enclosed in their own shapes. Only light blue data points are criminal cases.</em></figcaption></figure><p>This supports the broad division of substantive law into criminal law and civil law while also conforming well with the fact that criminal procedure law and civil procedure law share a number of common principles of natural justice.</p><h3>The most and least legislative areas of judicial law</h3><p>Fascinatingly, migration, family and substantive criminal law also all tend to cluster closely together latitudinally, hinting at potential hidden connection. They are all known to overlap in certain ways and they all share a special focus on regulating the lives of individuals, and not merely the property rights of <a href="https://en.wikipedia.org/wiki/Legal_person">legal persons</a>.</p><p>Migration, family and substantive criminal law cases also all happen to be the most distant types of cases from legislation on the map. Of course, this does not mean that they never cite legislation, but it may be that they rely on precedent more often than other areas of case law. It might also be the result of the inherent difficulty in attempting to represent highly complex and multidimensional relationships in a simple two-dimensional map.</p><p>Conversely, the class of cases closest to legislation is development cases, which makes sense since they can often deal quite intimately with local planning laws and regulations.</p><h3>The case law continuum</h3><p>If we start at the bottom of the cases mainland and make our way up, we can also see that Australian case law is a continuum of sorts.</p><figure><img decoding="async" src="https://umarbutler.com/wp-content/uploads/2024/03/judicial-mainland.annotated-1.svg" alt="" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption><em>An annotated version of the case law mainland where select branches of law are pointed out, illustrating the continuum of case law.</em></figcaption></figure><p>Development cases connect with environmental cases, which then link with land cases.</p><p>Land cases border contract cases which in turn have procedural cases to their north, intellectual property cases to their west and commercial cases to their east.</p><p>Moving further north of procedural law brings you to criminal law and defamation.</p><p>Heading west from intellectual property law takes you through administrative law, health and social services law, employment law, negligence and finally transport law.</p><p>Going east of commercial law, you’ll find equity and a subset of family law.</p><figure><img fetchpriority="high" decoding="async" width="2236" height="1258" src="https://umarbutler.com/wp-content/uploads/2024/03/compressed_animation.webp" alt="" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://umarbutler.com/wp-content/uploads/2024/03/compressed_animation.webp"><figcaption><em>An animation of branches of law appearing on the map sequentially, illustrating the continuum of case law.</em></figcaption></figure><p>This continuum corresponds well with our pre-existing understandings of the relationships between the various branches of the law.</p><p>It makes sense, for example, that development, environment and land law would all be intertwined given their similar subject matter. Likewise, it is not at all unexpected that negligence would cluster closely with transport and employment law when a great many negligence cases centre around motor and workplace accident claims.</p><p>The map, in effect, crystallises our own mental models of the law.</p><p>It also shows us that the borders between various areas of the law can often be quite porous. We notice, for instance, that there is a streak of land law judgments that overlaps with commercial and procedural law cases and is disconnected from most other land law cases. Interestingly, cases in this streak tend to focus on mortgage disputes often involving defaults which would explain why they overlap with commercial and procedural law cases.</p><p>We can also see that there are some transport law judgments that are connected to the cases mainland and then there are others that are connected to the island of substantive criminal law cases. Transport judgments connected to that island often centre around the suspension of transport licences, whereas judgments connected to the cases mainland tend to focus on transport accidents. Although disconnected from one another, however, both clusters of transport cases are still relatively close to each other, reflecting their shared subject matter.</p><h3>Final thoughts</h3><p>By now, we’ve covered how the map reflects already known distinctions between cases and legislation, while also revealing potential new divisions and hidden connections between various areas of the law. We’ve also seen how Australian case law can be more of a continuum than a rigidly defined structure and how the borders between branches of case law can often be quite porous.</p><p>Other specific insights we’ve been able to find are that:</p><ul><li>Migration, family and substantive criminal law are the most isolated branches of case law on the map;</li><li>Migration, family and substantive criminal law are the most distant branches of case law from legislation on the map;</li><li>Development law is the closest branch of case law to legislation on the map; and</li><li>The map does not reveal any noticeable distinctions between Australian state and federal law, whether it be in style, principles of interpretation or general jurisprudence.</li></ul><p>These are but a selection of the most readily observable insights to be gained from attempting to map Australian law. There are no doubt countless others waiting to be uncovered. Producing a three-dimensional map of Australian laws, cases and regulations could, for example, reveal new hidden relationships that are almost impossible to represent in two dimensions. Adding cases and legislation from other states and territories might also give us a sharper, higher resolution image of the map, deepening our understanding of the geography of Australian law. One could even imagine adding legal documents from other common law countries such as the UK, Canada and New Zealand to, in a sense, photograph the historic and continued interactions between our legal systems.</p><p>Nevertheless, for a first attempt, the map already has a lot to teach us. Perhaps you’ve even identified patterns in the map that I could not.</p><p>The greatest thing about this exercise is that it can be applied to virtually any domain, not just Australian law. Semantic mapping is particularly useful for very quickly developing an understanding of the underlying composition and structure of a dataset without having to manually scour through hundreds of examples to build your own much noisier and less persistent mental model of that data.</p><p>Since finishing the map, I’ve already been able to reuse this technique to study countless other seemingly unstructured large datasets, and you can too. It doesn’t take an expert in clustering and mapping to pull it off. Far from it. Prior to starting this project, I didn’t know the first thing about semantic mapping and now I’m about to teach you how to do it yourself.</p><h2 id="so-howd-you-do-it">So how’d you do it?</h2><p>At a high level, the process for mapping any arbitrary set of data points, whether they be PDFs, YouTube videos, TikToks or anything else, can be broken down into six stages, illustrated below.</p><figure><img decoding="async" src="https://umarbutler.com/wp-content/uploads/2024/03/process.oalm_.svg" alt="" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption><em>An illustration of the process of semantically mapping data.</em></figcaption></figure><p>In brief, we try to represent the meaning of data in the form of sets of numbers (<em>vectorisation</em>), after which we group those sets into clusters based on their similarity (<em>clustering</em>) and subsequently label those clusters based on whatever unique patterns we can find in them (<em>labelling</em>). Finally, we project the numerical representations of the data into two-dimensional coordinates (<em>dimensionality reduction</em>) which we then plot on a map (<em>visualisation</em>).</p><p>Through this next section, we’ll take a deeper look at exactly how every step of the semantic mapping process works in practice. Before that though, I’d like to express my gratitude to the creators of <a href="https://github.com/MaartenGr/BERTopic" target="_blank" rel="noreferrer noopener">BERTopic</a>, a topic modelling technique which this process was loosely based on, as well as <a href="https://www.kaggle.com/mikedelong">Dr Mike DeLong</a> whose <a href="https://www.kaggle.com/code/mikedelong/australian-law-topics">topic map</a> of the Open Australian Legal Corpus served as the inspiration for this entire project.</p><h3>Vectorisation</h3><p>The first step in semantically mapping a dataset is to vectorise its data points.</p><p>In this context, <em>vectorisation</em> refers to the process of converting information into a set of numbers intended to represent its underlying meaning, known as a <em>vector</em> or <em>embedding</em>. By calculating how similar vectors are to one another, we can also get a rough idea of how similar they are in meaning. This principle is what allows us to later group data points into clusters and project them onto a two-dimensional map.</p><p>To vectorise a data point, we can use an <em>embedding model</em>, a model specially trained for the task of representing the meaning of information as vectors. Thankfully, for my uses and probably yours too, it isn’t necessary to train a custom embedding model or pay someone to use theirs. At least for text vectorisation, most of the world’s best models are already available for free and under open-source licences.</p><p>Hugging Face helpfully maintains a ranked list of the most accurate text embedding models as benchmarked against hundreds of datasets, known as the <a href="https://huggingface.co/spaces/mteb/leaderboard">Massive Text Embedding Benchmark (MTEB) Leaderboard</a>. When I built the map, <code><a href="https://huggingface.co/BAAI/bge-small-en-v1.5">BAAI/bge-small-en-v1.5</a></code> was one of the best open-source models available for its size, so that’s what I went with. Nowadays, <code><a href="https://huggingface.co/avsolatorio/GIST-small-Embedding-v0">avsolatorio/GIST-small-Embedding-v0</a></code> (a finetune of that model) ranks higher, but its worth checking out the leaderboard yourself as new models are released every day.</p><p>One constraint of contemporary text embedding models worth keeping in mind is that they can only vectorise a fixed number of <em>tokens</em>, known as a <em>context window</em>. If you don’t know what a token is, you can think of it as the most basic unit of input a text model can take. There are roughly 0.75 tokens in a word. So, if a text embedding model’s context window is 512 tokens like <code>GIST-small-Embedding-v0</code>‘s is, then you can only vectorise roughly 384 words at a time.</p><p>To get around this, we can split text into chunks up to 512 tokens long, vectorise each chunk and then average those vectors to produce a mean text embedding that represents the average meaning of the text. This process can also be applied to vectorise videos and audio clips longer than what an embedding model can take as input or really any other type of embeddable data.</p><figure><img decoding="async" src="https://umarbutler.com/wp-content/uploads/2024/03/OALM-Vectorisation-1.svg" alt="" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption><em>An illustration of the process of producing a mean embedding.</em></figcaption></figure><p>In splitting up our long-form data, however, it is essential that we do so in as meaningful a way as possible. Simply breaking up text at every 512th-token or, if we’re working with audiovisual data, 512th-second, could result in the loss of semantically important information. Imagine if we ended up splitting the sentence ‘I love to eat kangaroo gummies, they’re my favourite snack’ at the word ‘kangaroo’, resulting in the chunks ‘I love to eat kangaroo’ and ‘gummies, they’re my favourite snack’. The final embedding would no doubt be quite dissimilar from the text’s actual meaning.</p><p>Ideally, we’d like for our data to have already been divided into semantically meaningful sections that are all under our model’s context window. Realistically though, our data may not have any sections at all or, if it does have sections, not all may fit within the model’s context window. In such cases, we can first split our data into whatever parts we do have and then use a <em>semantic chunker</em> to bring whatever data is over the context window, under it.</p><p>For text data, I’d recommend <code><a href="https://github.com/umarbutler/semchunk">semchunk</a></code>, an extremely fast and lightweight Python library I developed to split millions of tokens worth of text into chunks as semantically meaningful as possible in a matter of seconds. It works by searching for sequences of characters known to indicate semantic breaks in text such as consecutive newlines and tabs, and then recursively splitting at those sequences until a text is under the given chunk size.</p><p>The code snippet below demonstrates exactly how you can use <code>semchunk</code> to split a dataset of documents into chunks with any given Hugging Face text embedding model of your choice. Just make sure you have <code><a href="https://github.com/umarbutler/semchunk">semchunk</a></code> and <code><a href="https://github.com/huggingface/transformers" target="_blank" rel="noreferrer noopener">transformers</a></code> installed beforehand.</p> <p>After chunking our data, we still need to vectorise it and then average those vectors such that<code> [1,</code> <code>2]</code> and <code>[3, 4]</code> becomes <code>[2, 3]</code> (not <code>[7]</code>). This is how’d you do that in practice, keeping in mind this code also requires <code><a href="https://github.com/pytorch/pytorch" target="_blank" rel="noreferrer noopener">torch</a></code> and <code><a href="https://github.com/tqdm/tqdm" target="_blank" rel="noreferrer noopener">tqdm</a></code>:</p> <h3>Dimensionality reduction</h3><p>Now that we’ve vectorised our data, the next step is to reduce its dimensionality. Dimensionality reduction is where you take a really long vector like <code>[4, 2, 1, 5, ..]</code> and turn it into lower-dimension coordinates like <code>[4, 2]</code>. This is how we map our data. It also makes our data easier to cluster later on, since high-dimensional data can often be quite tricky to cluster due to the ‘<a href="https://www.nature.com/articles/s41592-018-0019-x">curse of dimensionality</a>‘.</p><p>To reduce the data’s dimensionality, we use a <em>dimensionality reduction model</em>, a model capable of projecting high-dimensional data into low-dimensional spaces while preserving as much meaningful information as possible.</p><p>The model I used was <a href="https://github.com/YingfanWang/PaCMAP">PaCMAP</a>, which benchmarks as one of the fastest and most accurate dimensionality reduction models, capable of preserving both global and local structures in underlying datasets. The visualisation below, courtesy of their GitHub repository, shows what it looks like when you attempt to reduce a three-dimensional model of a mammoth down to two dimensions with PaCMAP (visible on the far right) and other popular dimensionality reduction models.</p><figure><img decoding="async" width="936" height="720" src="https://umarbutler.com/wp-content/uploads/2024/03/PaCMAP-Mammoth.png" alt="" srcset="https://umarbutler.com/wp-content/uploads/2024/03/PaCMAP-Mammoth.png 936w, https://umarbutler.com/wp-content/uploads/2024/03/PaCMAP-Mammoth-300x231.png 300w, https://umarbutler.com/wp-content/uploads/2024/03/PaCMAP-Mammoth-768x591.png 768w" sizes="(max-width: 936px) 100vw, 936px" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://umarbutler.com/wp-content/uploads/2024/03/PaCMAP-Mammoth.png"><figcaption><em>A visualisation of the reduction of a three-dimensional model of a mammoth down to two dimensions with the most popular dimensionality reduction models, courtesy of the Apache-2.0 licensed <a href="https://github.com/YingfanWang/PaCMAP">PaCMAP</a> GitHub repository.</em></figcaption></figure><p>Now, because we’re using PaCMAP for two different purposes, namely, to map the data and to make it easier to cluster, we can reduce our vectors to two different dimensions.</p><p>For mapping, two dimensions is what I went with but it’s also possible to visualise three.</p><p>For clustering, I choose 80 because my clusters seemed to benefit from high-dimensional data and 80 was the most dimensions I could use without slowing my PC down too much. What worked for me, however, may not work for you. With another dataset of ~400 data points, much lower than the Open Australian Legal Corpus’ 200k documents, I had found that 2 dimensions worked considerably better than 80. It is worth testing a range of dimensions to see what yields the best clusters for your data.</p><p>After installing the <code><a href="https://github.com/YingfanWang/PaCMAP">pacmap</a></code> Python package, you can use the following code to reduce the dimensionality of your data for both mapping and later clustering it:</p> <h3>Clustering</h3><p>Once the dimensionality of your data has been reduced, we can use a <em>clustering model</em> to group it into clusters of data points that are close together in our vector space. These clusters tend to correlate with the broad set of topics and themes present in a dataset.</p><p>There are a range of clustering models to choose from, each with their own unique advantages and drawbacks. I ended up settling on <a href="https://github.com/scikit-learn-contrib/hdbscan">HDBSCAN</a>, which is generally <a href="https://pberba.github.io/stats/2020/07/08/intro-hdbscan/">well-regarded</a>. They have a <a href="https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html">page</a> in their documentation which covers their differences with other popular clustering methods. The most notable differentiator is that, unlike older algorithms such as <em>k</em>-means, HDBSCAN does not force every data point into a cluster, which is quite reasonable. There will always be data points that don’t quite fit into a known box. Forcing them into boxes just makes those boxes noisier.</p><p>The only problem with HDBSCAN is that it can sometimes be overzealous in refusing to cluster data points. In my case, there were 218,336 legal texts in the Open Australian Legal Corpus at the time that I produced the map and 84,780 (38.8%) could not be clustered. A further 10,100 (4.6%) were placed in clusters that did not appear to have any meaningful unifying features. In total, there were 94,880 (43.4%) documents that could not be assigned to a meaningful cluster and so were excluded from the map.</p><p>This is what the map would have looked like if I had included them.</p><figure><img decoding="async" width="2240" height="1260" src="https://umarbutler.com/wp-content/uploads/2024/03/unassigned.map_.oalm_.svg" alt="" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption><em>A version of the map with documents without meaningful clusters included.</em></figcaption></figure><p>It is clear that there were documents that HDBSCAN could and should have clustered, such as those part of the criminal and family law islands. This is likely the result of the curse of dimensionality but it may also be because I used <code><a href="https://github.com/TutteInstitute/fast_hdbscan">fast_hdbscan</a></code>, a faster implementation of HDBSCAN that I later discovered has a tendency to produce patchier clusters than regular HDBSCAN.</p><p>Accordingly, I’ll be using regular HDBSCAN in my code example.</p><p>You will notice that there are two hyperparameters that may be tuned, <code>min_cluster_size</code> and <code>min_samples</code>. Thanks to <a href="https://www.reddit.com/r/datascience/comments/5sfj0y/comment/ddfabms/?utm_source=share&amp;utm_medium=web2x&amp;context=3">this</a> Reddit comment and my own experimentation, I have found that <code>min_samples</code> should approach <code>log(n)</code> the noisier your dataset is, where <code>n</code> is the number of data points in the dataset. For clean data, it is appropriate to set <code>min_samples</code> to 1, which is what I used.</p><p><code>min_cluster_size</code> refers to the minimum number of data points that should be in a cluster. It is possible to yield meaningful clusters with both low and high minimum cluster sizes. It may be thought of as a control on how generalised clusters should be. For granular clusters reflecting specific topics in your data, a low minimum cluster size is preferable. For a couple broad clusters reflecting general themes in your data, a higher minimum cluster size is advised.</p><p>I ended up going with a cluster size of 50, which, in proportion to the size of my database, was minuscule. I only did this because I wanted to manually merge clusters myself in order to ensure that the final clusters were both as broad as I wanted them to be and were as precise as they could be. This resulted in 507 unique clusters (excluding the unassigned cluster), which I manually whittled down to 19 branches of law. I’ll get into how I pulled that off in the next section but for now, here is the code for how HDBSCAN can be used to cluster vectors:</p> <h3>Labelling</h3><p>After clustering your data, you’ll want to assign some meaningful labels to those clusters. This means figuring out exactly what it is that they share in common.</p><p>There are a number of techniques to choose from for identifying meaningful labels for clusters, including the use of tf-idf, generative AI-based labellers and of course hand labelling. I won’t get into all the options available, I’ll just cover what worked for me.</p><p>First, I identified the top tokens in each cluster by their tf-idf, which is a measure of a token’s frequency in a cluster weighted by its overall frequency in a dataset, such that tokens that are extremely common in only one cluster will have a higher tf-idf for that cluster than tokens that are extremely common in all clusters. This served as an easy way to quickly associate clusters with labels reflecting their unique composition.</p><p>With the top tokens by tf-idf in hand, I merged any clusters whose top four tokens were the same, which got rid of just 2 of the 507 clusters. Next, I manually reviewed the clusters in order to produce a set of 337 rules on how to merge them based on their top tokens.</p><p>In manually merging clusters, I tried my best to be as agnostic as possible on what the final set of categories should look like. The idea was to let the data guide me, rather than me guiding the data. As the number of clusters began to dwindle, however, I soon found myself forced to make increasingly difficult decisions about what categories to include and exclude, such as whether it would better to roll up health and social services law into a single area of law rather than creating a commercial law category out of tax, finance and insolvency law. I originally wanted to include many more areas of law than the 19 branches you see on the map now, but I was ultimately constrained by the fact that even visualising 19 easily distinguishable categories on a map with contiguous continents is no small feat (it was only thanks to a colour <a href="https://sashamaps.net/docs/resources/20-colors/">palette</a> published by fellow data scientist Sasha Trubetskoy that I managed to pull it off!).</p><p>Eventually, I ended up settling on a set of clusters that I felt were a reasonable way of dividing up my map, although I recognise it may not necessarily be the most optimal way, if such an optimum even exists.</p><p>This merging process was one of the most difficult components of building the map, second only to writing this article, but it also taught me a lot not only about the broad makeup of Australian law but also about the many different ways law in general can be sliced up.</p><p>I would only recommend manually merging clusters if you also want to develop an intimate understanding of the composition of your data or if it is particularly important to you that the final product be as precise and accurate as possible. Otherwise, it would be much more beneficial to tune your clustering model to produce a more manageable amount of clusters. You can then automatically label those clusters by either taking their top three tokens by tf-idf as their label or using a large language model to generate more coherent labels from those tokens.</p><p>In the code snippet below, I show how you can identify the top tokens in a cluster by tf-idf. Please note that the code relies on <code><a href="https://www.nltk.org/">nltk</a></code>.</p> <p>If you wanted to take your automated labelling a step further, you could also use the following code to get GPT-4 (or another OpenAI API-compatible model) to generate labels for you, keeping in mind that this code requires the <code><a href="https://github.com/openai/openai-python">openai</a></code> and <code><a href="https://github.com/umarbutler/semchunk">semchunk</a></code> Python libraries.</p> <h3>Visualisation</h3><p>At this point, the only piece of the puzzle left is to visualise your map. There are a number of Python libraries capable of producing two- and three-dimensional scatter plots, but none of them are particularly impressive, including the library I eventually settled on, which was <a href="https://plotly.com/">Plotly</a>.</p><p>My main grievance with Plotly is that it does not let you expand the size of data points when zooming into a map. This really becomes an issue where you have hundreds of thousands of data points and you find that they either overlap with each other or, if you reduce their size, they become almost impossible to identify when zoomed in. There is a 3-year-old <a href="http://GitHub issue">GitHub issue</a> concerning this problem but it doesn’t look like it will get solved anytime soon.</p><p>There were other less severe issues I experienced with Plotly that I was able to work around with custom CSS and Javascript. I won’t provide that code at the moment as it is not particularly pretty, but I will share a code snippet illustrating how Plotly can be used to visualise mapped data:</p> <p>With that done, you should now have your very semantic map. The next step is to analyse it. Look for patterns in the map’s geography, inspect outliers, search for islands, get a sense of the underlying structure of your data.</p><p>As my own analysis has shown, there is a lot you can learn just by mapping a dataset. And, once you get the ball rolling, it can quickly spiral into an addition. I have hundreds of ideas for how to expand my map to uncover new relationships. The real power of semantic mapping comes out when you apply it against very large datasets. Imagine applying these techniques on the <a href="https://commoncrawl.org/">Common Crawl</a> corpus, for example. You would be able to produce a first-of-its-kind high-resolution map of the internet.</p><p>If you do end up publishing your own semantic map, be sure to cite this article to enable others to learn about the power of semantic mapping.</p><p>Otherwise, happy mapping!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gitlab confirms it's removed Suyu, a fork of Nintendo Switch emulator Yuzu (138 pts)]]></title>
            <link>https://www.theverge.com/2024/3/21/24108191/gitlab-suyu-nintendo-switch-emulator-takedown</link>
            <guid>39787728</guid>
            <pubDate>Fri, 22 Mar 2024 05:36:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/3/21/24108191/gitlab-suyu-nintendo-switch-emulator-takedown">https://www.theverge.com/2024/3/21/24108191/gitlab-suyu-nintendo-switch-emulator-takedown</a>, See on <a href="https://news.ycombinator.com/item?id=39787728">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Nintendo might not need <a href="https://www.theverge.com/24098640/nintendo-emulator-yuzu-lawsuit-switch-aftermath">to individually sue emulators out of existence</a> to drive them deeper underground. Today, <a href="https://gitlab.com/suyu-emu/suyu">GitLab cut off access to Nintendo Switch emulator Suyu</a>, and disabled the accounts of its developers, after receiving what appears to be a scary email in the form of a DMCA takedown request. </p></div><p>“GitLab received a DMCA takedown notice from a representative of the rightsholder and&nbsp;followed our standard process <a href="https://handbook.gitlab.com/handbook/legal/dmca/">outlined here</a>,” spokesperson Kristen Butler tells <em>The Verge</em>. </p><p>Suyu was a fork of Yuzu, the emulator that Nintendo successfully sued, but this isn’t about Nintendo now having the rights to Yuzu’s code — or maybe even Nintendo at all? Nintendo didn’t necessarily win the rights to Yuzu’s code <a href="https://www.theverge.com/2024/3/4/24090357/nintendo-yuzu-emulator-lawsuit-settlement">in its settlement</a>, and GitLab didn’t tell <em>The Verge</em> that Nintendo is behind the takedown. </p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="One of the emails received by a Suyu contributor." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/376x193/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/384x197/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/415x212/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/480x246/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/540x276/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/640x328/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/750x384/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/828x424/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/1080x553/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/1200x614/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/1440x737/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/1920x983/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/2048x1049/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/2400x1229/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1000x512/2400x1229/filters:focal(500x256:501x257):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348883/image.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>One of the emails received by a Suyu contributor.</em></figcaption></p></div><p>Instead, as you can see in the email above — one of several being shared in Suyu’s Discord and <a href="https://overkill.wtf/suyu-emulator-removed-from-gitlab/">published earlier by Overkill.wtf</a> — whoever sent the takedown request is trying to piggyback on how Yuzu allegedly violated DMCA 1201 by circumventing Nintendo’s technical protection measures. Oh, and maybe also subtly threatening GitLab with unlawful trafficking (also part of DMCA 1201) while they’re at it.</p><p>I’m not a lawyer, but a couple of lawyers told me two years ago that a <em>valid</em> DMCA takedown request <a href="https://www.law.cornell.edu/uscode/text/17/512">should technically contain</a> “Identification of the copyrighted work claimed to have been infringed,” and that DMCA 1201 is not the same thing as DMCA 512, which covers takedown requests.</p><p>Also, <a href="https://arstechnica.com/gaming/2024/03/heres-how-the-makers-of-the-suyu-switch-emulator-plan-to-avoid-getting-sued/">Suyu has claimed</a> it does not include the same circumvention measures as Yuzu.</p><p>But those lawyers also told me that valid or invalid, it doesn’t necessarily matter all that much, since a platform like GitLab doesn’t have to host anything that it doesn’t want to host. It may not be worth the time and effort to push back on an invalid DMCA takedown request to protect something you might not even care to protect — particularly if the alternative might be Nintendo coming at you with an actual lawsuit.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="What Suyu’s GitLab page looks like now." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/376x252/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/384x257/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/415x278/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/480x322/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/540x362/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/640x429/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/750x503/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/828x555/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/1080x724/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/1200x804/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/1440x965/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/1920x1287/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/2048x1372/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/2400x1608/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1055x707/2400x1608/filters:focal(528x354:529x355):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25348879/chrome_0WztcTGYBn.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>What Suyu’s GitLab page looks like now.</em></figcaption></p></div><p>GitLab didn’t immediately answer a question about whether it’s company policy to disable user’s accounts <em>before</em> giving them the opportunity to delete their projects or file a DMCA counter-notice. The company’s online handbook does not say why GitLab might decide to block or ban a user from its platform; only that “we may, in appropriate circumstances, disable access or terminate the account(s) of the reported user(s).”</p><p>Suyu appears to have already found a new home. About an hour ago, its leader wrote “I’m most certainly going to host a copy of the code.” By that point, another member had already cloned the repository to <a href="https://git.suyu.dev/">git.suyu.dev.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Users say Glassdoor added real names to user profiles without their consent (234 pts)]]></title>
            <link>https://techcrunch.com/2024/03/20/glassdoor-added-real-names-profiles-without-consent/</link>
            <guid>39787559</guid>
            <pubDate>Fri, 22 Mar 2024 04:48:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/03/20/glassdoor-added-real-names-profiles-without-consent/">https://techcrunch.com/2024/03/20/glassdoor-added-real-names-profiles-without-consent/</a>, See on <a href="https://news.ycombinator.com/item?id=39787559">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary"><span>Users of the</span> popular site Glassdoor, which lets anyone anonymously sign up to review companies they have worked for, say Glassdoor collected and added their names to their user profiles without their consent.</p>
<p>One user, who goes by Monica, wrote in a post on her personal blog that Glassdoor added her name and the city where she lives to her Glassdoor profile following an email exchange with Glassdoor customer support, despite having never provided her name during the sign-up process some years earlier. Monica, whose last name we’re not publishing to protect her privacy, accused Glassdoor of getting her full name from the email she sent to customer support, which she says they added to her Glassdoor profile.</p>
<p>“My email ‘from’ line contains my full name — never thought that would be a problem!” Monica told TechCrunch in an email. “They then added my name to my Glassdoor profile.”</p>
<p>Monica repeatedly protested Glassdoor, telling customer support that the company did not have her consent or permission to do this. But Glassdoor said Monica was “required” to have her name added to her profile, adding that this would not compromise her anonymity of past reviews she gave. Monica said that her anonymity might not last if Glassdoor was to experience a hack or a data breach and compromise users’ data. It also means this information can be obtained by legal process, such as a lawsuit or police demanding access to Glassdoor user data.</p>
<p>As Monica explained, Glassdoor will add a user’s real name (and potentially other information) to the user’s account without their permission if Glassdoor learns it.</p>
<p>And the only other option is to delete your account, Monica said.</p>
<p>Glassdoor users expressed alarm at Monica’s story, which has been widely shared on social media and news-sharing sites, for fear that their anonymity could be compromised by having data collected about them and added to their profiles, as well.</p>
<div>
		<blockquote>
			“It’s not clear to me how they got this information.”							<cite>Josh Simmons, Glassdoor user</cite>
					</blockquote>
	</div>
	
<p>Glassdoor has long allowed users to sign up anonymously. In 2021, <a href="https://techcrunch.com/2021/09/14/glassdoor-fishbowl-linkedin/" target="_blank" rel="noopener">Glassdoor bought Fishbowl</a>, a semi-anonymous professional social network site that allows users to “ask questions without disclosing your name.” Ars Technica, which <a href="https://arstechnica.com/tech-policy/2024/03/glassdoor-adding-users-real-names-job-info-to-profiles-without-consent/" target="_blank" rel="noopener">first reported</a> Monica’s story, explained that Fishbowl requires users to verify their identities before using the site. As part of the acquisition deal, Glassdoor signed every user up for a Fishbowl account, meaning Glassdoor would have to change its terms of service so that every Glassdoor user could also be verified.</p>
<p>Aaron Mackey, an attorney with the digital rights group Electronic Frontier Foundation, told TechCrunch that Glassdoor has been an “industry leader” in defending its users’ anonymity. Mackey previously defended an anonymous Glassdoor user in court whose employer <a href="https://www.eff.org/press/releases/eff-fights-protect-anonymity-glassdoor-commenter" target="_blank" rel="noopener">tried to unmask and identify their identity</a>.</p>
<p>“We hope that Glassdoor will continue to defend its users’ anonymity in court,” said Mackey. “But the latest news regarding Glassdoor’s policies raises concerns about whether users may be identified even if their information is never sought by an employer or law enforcement. Those policies also appear to conflict with, or at least be in tension with, Glassdoor’s goal of encouraging employees to candidly review their employers.”</p>
<p>In some cases, the data added to the user’s profile did not completely line up.</p>
<p>Josh Simmons said Glassdoor added information about him to his profile without his consent, describing it as a “breach of trust.” Simmons told TechCrunch that he did not know how Glassdoor got his personal data.</p>
<p>“It’s not clear to me how they got this information,” Simmons told TechCrunch. “I didn’t have any social accounts connected to Glassdoor, and I hadn’t used the service in several years,” suggesting that the data may have been scraped or come from a data broker.</p>
<p>Simmons said his supplemented Glassdoor profile had an “incoherent mix of details, but each detail was correct in isolation,” describing how Glassdoor got the name of his consultancy correct but jumbled his location in California with his main client based in London.</p>
<p>“Taken together, it signaled to me that it was the result of an automated process,” Simmons said.</p>
<p>By Glassdoor’s own admission, the company <a href="https://help.glassdoor.com/s/article/Does-Glassdoor-verify-employees?language=en_US" target="_blank" rel="noopener">says on its website</a> that it is “unable to fully confirm our users’ identities, the truthfulness of their contributions, or their employment status.” It’s not clear what the goal of Glassdoor’s data collection is if the information is not accurate.</p>
<p>When reached for comment, Glassdoor spokesperson Amanda Livingood would not answer TechCrunch’s specific questions, including how — if at all — Glassdoor verifies the accuracy of the information it receives, or how it can be used or obtained. Glassdoor does not publish a transparency report detailing the number of requests for user data it receives from law enforcement.</p>
<p>Instead, the company provided a boilerplate statement:</p>
<blockquote><p>Glassdoor is committed to providing a platform for people to share their opinions and experiences about their jobs and companies, anonymously – without fear of intimidation or retaliation. User reviews on Glassdoor have always and will always be anonymous. In the Glassdoor community, users always have the choice to post with their name or post anonymously with their company name or job title. Glassdoor has never and will never reveal a user’s name alongside their content, unless that is what the user chooses.</p></blockquote>
<p>Mackey said that the risk of data breaches or legal demands are magnified because Glassdoor is now collecting more information about users that could identify them. “But because Glassdoor now collects that information, including from email addresses and headers, Glassdoor now has data that directly identifies their users,” Mackey said.</p>
<p>That leaves users like Monica with no choice but to delete their account if they are not willing to have their name on their profile. And so Monica did.</p>
<p><em>According to Monica, closing your account just deactivates it. If you want to fully delete your Glassdoor account, you can head over to <strong><a href="https://help.glassdoor.com/s/privacyrequest" target="_blank" rel="noopener">this specific Glassdoor privacy request page</a></strong> and fill out the data request form with the appropriate selection, such as “Delete my personal data.”</em></p>
<hr>
<p><em>To contact this reporter, get in touch on Signal and WhatsApp at +1 646-755-8849, or <a href="mailto:zack.whittaker@techcrunch.com" target="_blank" rel="noopener">by email</a>. You can also send files and documents via&nbsp;<a href="https://techcrunch.com/tips" target="_blank" rel="noopener">SecureDrop</a>.</em></p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Picotron Is a Fantasy Workstation (309 pts)]]></title>
            <link>https://www.lexaloffle.com/picotron.php</link>
            <guid>39786984</guid>
            <pubDate>Fri, 22 Mar 2024 02:48:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lexaloffle.com/picotron.php">https://www.lexaloffle.com/picotron.php</a>, See on <a href="https://news.ycombinator.com/item?id=39786984">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main_div">


	<p><img src="https://www.lexaloffle.com/gfx/picotron.png"></p><a href="https://www.lexaloffle.com/games.php?page=updates">
	<div><center>
	<table><tbody><tr><td>
		<img src="https://www.lexaloffle.com/gfx/filedisk.png" width="20" height="20">
		</td><td>
		<span>v0.1.0b</span>
		</td></tr>
	</tbody></table>
	</center></div></a>


	<div>

	<p><a href="https://www.lexaloffle.com/dl/wip/picotron_desktop.png">
	<img src="https://www.lexaloffle.com/dl/wip/picotron_desktop.png">
	</a></p><div>

			<h3>Welcome to Picotron</h3>

			<p><a href="https://www.lexaloffle.com/picotron.php">Picotron</a> is a <b>Fantasy Workstation</b> for making pixelart games, animations, music, demos and other curiosities.

			It has a toy operating system designed to be a cosy creative space, but runs on top of <b>Windows</b>, <b>MacOS</b> or <b>Linux</b>.

			Picotron apps can be made with built-in tools, and shared with other users in a special 256k png cartridge format.

	</p></div>

</div>


<div>


<p><a href="https://www.lexaloffle.com/dl/wip/picotron_desktop3.png">
	<img src="https://www.lexaloffle.com/dl/wip/picotron_desktop3.png">
</a></p><div>

		<h3>Specifications</h3>

		<p><span>Display:</span> 	<span>480x270 / 240x135 64 definable colours</span><br>
		<span>Graphics:</span>  <span> 	Blend tables, tline3d, stencil clipping</span><br>
		<span>Audio:</span> <span>	64-node synth and 8-channel tracker</span><br>
		<span>Code:</span> 	<span>Lua 5.4 w/ <a href="https://www.lexaloffle.com/pico-8.php">PICO-8</a> compat features</span><br>
		<span>CPU:</span> <span>8M Lua VM insts / second</span><br>
		<span>Cart:</span> 	<span>.p64.png (256k) / .p64 (unlimited) </span><br>

</p></div>

</div>


<div><p>

	For more technical details and design documents, see the 
	</p><a href="https://www.lexaloffle.com/picotron.php?page=faq"><p>Picotron FAQ</p></a>

</div>

<div>

	<p><a href="https://www.lexaloffle.com/dl/wip/picotron_desktop2.png">
	<img src="https://www.lexaloffle.com/dl/wip/picotron_desktop2.png">
	</a></p><div>

		<h3>Customize your Machine</h3><p>

		Make your own live wallpapers, screensavers, widgets, custom tools, and set up workflows just the way you like! 

		Or have a look around <a href="https://www.lexaloffle.com/bbs/?cat=8"> the BBS </a> to see what other cartridge authors are up to.

	</p></div>

	
	
</div>



	<h2>Status: Alpha</h2>

	<p>
		Picotron is brand new! It is still mostly held together by duct tape, but the runtime and API are fairly complete, 
		and the current binaries allow basic editing code, graphics, maps and sound, and exporting 
		to .p64.png to share on the BBS. Web exporters are coming soon! 
	</p>


	<a href="https://www.lexaloffle.com/picotron.php?page=roadmap"><p>Picotron Roadmap</p></a>



	<h2>Get Picotron</h2>

	<p>
		Until the end of March, Picotron is available for just $11.99. After that it will cost $19.99, or $11.99 when purchased
		with other fantasy consoles. All future updates are included, and the binaries are DRM-free, for Windows, Mac and Linux.
	</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Chain-of-Thought Reasoning Helps Neural Networks Compute (108 pts)]]></title>
            <link>https://www.quantamagazine.org/how-chain-of-thought-reasoning-helps-neural-networks-compute-20240321/</link>
            <guid>39786666</guid>
            <pubDate>Fri, 22 Mar 2024 01:50:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/how-chain-of-thought-reasoning-helps-neural-networks-compute-20240321/">https://www.quantamagazine.org/how-chain-of-thought-reasoning-helps-neural-networks-compute-20240321/</a>, See on <a href="https://news.ycombinator.com/item?id=39786666">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Your grade school teacher probably didn’t show you how to add 20-digit numbers. But if you know how to add smaller numbers, all you need is paper and pencil and a bit of patience. Start with the ones place and work leftward step by step, and soon you’ll be stacking up quintillions with ease.</p>
<p>Problems like this are easy for humans, but only if we approach them in the right way. “How we humans solve these problems is not ‘stare at it and then write down the answer,’” said <a href="https://www.eranmalach.com/">Eran Malach</a>, a machine learning researcher at Harvard University. “We actually walk through the steps.”</p>
<p>That insight has inspired researchers studying the large language models that power chatbots like ChatGPT. While these systems might ace questions involving a few steps of arithmetic, they’ll often flub problems involving many steps, like calculating the sum of two large numbers. But in 2022, a team of Google researchers <a href="https://arxiv.org/abs/2201.11903">showed</a> that asking language models to generate step-by-step solutions enabled the models to solve problems that had previously seemed beyond their reach. Their technique, called chain-of-thought prompting, soon became widespread, even as researchers struggled to understand what makes it work.</p>
<p>Now, several teams have explored the power of chain-of-thought reasoning by using techniques from an arcane branch of theoretical computer science called computational complexity theory. It’s the latest chapter in a line of research that uses complexity theory to study the intrinsic capabilities and limitations of language models. These efforts clarify where we should expect models to fail, and they might point toward new approaches to building them.</p>
<p>“They remove some of the magic,” said <a href="https://directory.engr.wisc.edu/ece/Faculty/Papailiopoulos_Dimitris/">Dimitris Papailiopoulos</a>, a machine learning researcher at the University of Wisconsin, Madison. “That’s a good thing.”</p>
<h2><strong>Training Transformers</strong></h2>
<p>Large language models are built around mathematical structures called artificial neural networks. The many “neurons” inside these networks perform simple mathematical operations on long strings of numbers representing individual words, transmuting each word that passes through the network into another. The details of this mathematical alchemy depend on another set of numbers called the network’s parameters, which quantify the strength of the connections between neurons.</p>

<p>To train a language model to produce coherent outputs, researchers typically start with a neural network whose parameters all have random values, and then feed it reams of data from around the internet. Each time the model sees a new block of text, it tries to predict each word in turn: It guesses the second word based on the first, the third based on the first two, and so on. It compares each prediction to the actual text, then tweaks its parameters to reduce the difference. Each tweak only changes the model’s predictions a tiny bit, but somehow their collective effect enables a model to respond coherently to inputs it has never seen.</p>
<p>Researchers have been training neural networks to process language for 20 years. But the work really took off in 2017, when researchers at Google introduced a <a href="https://arxiv.org/abs/1706.03762">new kind of network</a> called a transformer.</p>
<p>“This was proposed seven years ago, which seems like prehistory,” said <a href="https://pbarcelo.ing.uc.cl/">Pablo Barceló</a>, a machine learning researcher at the Pontifical Catholic University of Chile.</p>
<p>What made transformers so transformative is that it’s easy to scale them up — to increase the number of parameters and the amount of training data — without making training prohibitively expensive. Before transformers, neural networks had at most a few hundred million parameters; today, the largest transformer-based models have more than a trillion. Much of the improvement in language-model performance over the past five years comes from simply scaling up.</p>
<p>Transformers made this possible by using special mathematical structures called attention heads, which give them a sort of bird’s-eye view of the text they’re reading. When a transformer reads a new block of text, its attention heads quickly scan the whole thing and identify relevant connections between words — perhaps noting that the fourth and eighth words are likely to be most useful for predicting the 10th. Then the attention heads pass words along to an enormous web of neurons called a feedforward network, which does the heavy number crunching needed to generate the predictions that help it learn.</p>
<p>Real transformers have multiple layers of attention heads separated by feedforward networks, and only spit out predictions after the last layer. But at each layer, the attention heads have already identified the most relevant context for each word, so the computationally intensive feedforward step can happen simultaneously for every word in the text. That speeds up the training process, making it possible to train transformers on increasingly large sets of data. Even more important, it allows researchers to spread the enormous computational load of training a massive neural network across many processors working in tandem.</p>
<p>To get the most out of massive data sets, “you have to make the models really large,” said <a href="https://www3.nd.edu/~dchiang/">David Chiang</a>, a machine learning researcher at the University of Notre Dame. “It’s just not going to be practical to train them unless it’s parallelized.”</p>

<p>However, the parallel structure that makes it so easy to train transformers doesn’t help after training — at that point, there’s no need to predict words that already exist. During ordinary operation, transformers output one word at a time, tacking each output back onto the input before generating the next word, but they’re still stuck with an architecture optimized for parallel processing.</p>
<p>As transformer-based models grew and certain tasks continued to give them trouble, some researchers began to wonder whether the push toward more parallelizable models had come at a cost. Was there a way to understand the behavior of transformers theoretically?</p>
<h2><strong>The Complexity of Transformers</strong></h2>
<p>Theoretical studies of neural networks face many difficulties, especially when they try to account for training. Neural networks use a well-known procedure to tweak their parameters at each step of the training process. But it can be difficult to understand why this simple procedure converges on a good set of parameters.</p>
<p>Rather than consider what happens during training, some researchers study the intrinsic capabilities of transformers by imagining that it’s possible to adjust their parameters to any arbitrary values. This amounts to treating a transformer as a special type of programmable computer.</p>
<p>“You’ve got some computing device, and you want to know, ‘Well, what can it do? What kinds of functions can it compute?’” Chiang said.</p>
<p>These are the central questions in the formal study of computation. The field dates back to 1936, when Alan Turing first imagined a <a href="https://www.quantamagazine.org/alan-turings-most-important-machine-was-never-built-20230503/">fanciful device</a>, now called a Turing machine, that could perform any computation by reading and writing symbols on an infinite tape. Computational complexity theorists would later build on Turing’s work by proving that computational problems naturally fall into different <a href="https://www.quantamagazine.org/a-short-guide-to-hard-problems-20180716/">complexity classes</a> defined by the resources required to solve them.</p>
<p>In 2019, Barceló and two other researchers <a href="http://arxiv.org/abs/1901.03429">proved</a> that an idealized version of a transformer with a fixed number of parameters could be just as powerful as a Turing machine. If you set up a transformer to repeatedly feed its output back in as an input and set the parameters to the appropriate values for the specific problem you want to solve, it will eventually spit out the correct answer.</p>
<p>That result was a starting point, but it relied on some unrealistic assumptions that would likely overestimate the power of transformers. In the years since, researchers have worked to develop more realistic theoretical frameworks.</p>
<p>One such effort began in 2021, when <a href="https://lambdaviking.com/">William Merrill</a>, now a graduate student at New York University, was leaving a two-year fellowship at the Allen Institute for Artificial Intelligence in Seattle. While there, he’d analyzed other kinds of neural networks using techniques that seemed like a poor fit for transformers’ parallel architecture. Shortly before leaving, he struck up a conversation with the Allen Institute for AI researcher <a href="https://allenai.org/team/ashishs">Ashish Sabharwal</a>, who’d studied complexity theory before moving into AI research. They began to suspect that complexity theory might help them understand the limits of transformers.</p>

<p>“It just seemed like it’s a simple model; there must be some limitations that one can just nail down,” Sabharwal said.</p>
<p>The pair analyzed transformers using a branch of computational complexity theory, called circuit complexity, that is often used to study parallel computation and had <a href="https://arxiv.org/abs/2204.06618">recently been applied</a> to simplified versions of transformers. Over the following year, they refined several of the unrealistic assumptions in previous work. To study how the parallel structure of transformers might limit their capabilities, the pair considered the case where transformers didn’t feed their output back into their input — instead, their first output would have to be the final answer. They <a href="http://arxiv.org/abs/2207.00729">proved</a> that the transformers in this theoretical framework couldn’t solve any computational problems that lie outside a specific complexity class. And many math problems, including relatively simple ones like solving linear equations, are thought to lie outside this class.</p>
<p>Basically, they showed that parallelism did come at a cost — at least when transformers had to spit out an answer right away. “Transformers are quite weak if the way you use them is you give an input, and you just expect an immediate answer,” Merrill said.</p>
<h2><strong>Thought Experiments</strong></h2>
<p>Merrill and Sabharwal’s results raised a natural question — how much more powerful do transformers become when they’re allowed to recycle their outputs? Barceló and his co-authors had studied this case in their 2019 analysis of idealized transformers, but with more realistic assumptions the question remained open. And in the intervening years, researchers had discovered chain-of-thought prompting, giving the question a newfound relevance.</p>
<p>Merrill and Sabharwal knew that their purely mathematical approach couldn’t capture all aspects of chain-of-thought reasoning in real language models, where the wording in the prompt <a href="https://arxiv.org/abs/2212.10001">can be very important</a>. But no matter how a prompt is phrased, as long as it causes a language model to output step-by-step solutions, the model can in principle reuse the results of intermediate steps on subsequent passes through the transformer. That could provide a way to evade the limits of parallel computation.</p>

<p>Meanwhile, a team from Peking University had been thinking along similar lines, and their preliminary results were positive. In a May 2023 paper, they identified some math problems that should be impossible for ordinary transformers in Merrill and Sabharwal’s framework, and <a href="http://arxiv.org/abs/2305.15408">showed</a> that intermediate steps enabled the transformers to solve these problems.</p>
<p>In October, Merrill and Sabharwal followed up their earlier work with a <a href="http://arxiv.org/abs/2310.07923">detailed theoretical study</a> of the computational power of chain of thought. They quantified how that extra computational power depends on the number of intermediate steps a transformer is allowed to use before it must spit out a final answer. In general, researchers expect the appropriate number of intermediate steps for solving any problem to depend on the size of the input to the problem. For example, the simplest strategy for adding two 20-digit numbers requires twice as many intermediate addition steps as the same approach to adding two 10-digit numbers.</p>
<p>Examples like this suggest that transformers wouldn’t gain much from using just a few intermediate steps. Indeed, Merrill and Sabharwal proved that chain of thought only really begins to help when the number of intermediate steps grows in proportion to the size of the input, and many problems require the number of intermediate steps to grow much larger still.</p>
<p>The thoroughness of the result impressed researchers. “They really pinned this down,” said <a href="https://www.cs.columbia.edu/~djhsu/">Daniel Hsu</a>, a machine learning researcher at Columbia University.</p>
<p>Merrill and Sabharwal’s recent work indicates that chain of thought isn’t a panacea — in principle, it can help transformers solve harder problems, but only at the cost of a lot of computational effort.</p>
<p>“We’re interested in different ways of getting around the limitations of transformers with one step,” Merrill said. “Chain of thought is one way, but this paper shows that it might not be the most economical way.”</p>
<h2><strong>Back to Reality </strong></h2>
<p>Still, researchers caution that this sort of theoretical analysis can only reveal so much about real language models. Positive results — proofs that transformers can in principle solve certain problems — don’t imply that a language model will actually learn those solutions during training.</p>

<p>And even results that address the limitations of transformers come with caveats: They indicate that no transformer can solve certain problems perfectly in all cases. Of course, that’s a pretty high bar. “There might be special cases of the problem that it could handle just fine,” Hsu said.</p>
<p>Despite these caveats, the new work offers a template for analyzing different kinds of neural network architectures that might eventually replace transformers. If a complexity theory analysis suggests that certain types of networks are more powerful than others, that would be evidence that those networks might fare better in the real world as well.</p>
<p>Chiang also stressed that research on the limitations of transformers is all the more valuable as language models are increasingly used in a wide range of real-world applications, making it easy to overestimate their abilities.</p>
<p>“There’s actually a lot of things that they don’t do that well, and we need to be very, very cognizant of what the limitations are,” Chiang said. “That’s why this kind of work is really important.”</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Employer Review Site Glassdoor Deanonymized Users Without Consent (102 pts)]]></title>
            <link>https://restoreprivacy.com/employer-review-site-glassdoor-deanonymized-users-without-consent/</link>
            <guid>39785815</guid>
            <pubDate>Thu, 21 Mar 2024 23:46:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restoreprivacy.com/employer-review-site-glassdoor-deanonymized-users-without-consent/">https://restoreprivacy.com/employer-review-site-glassdoor-deanonymized-users-without-consent/</a>, See on <a href="https://news.ycombinator.com/item?id=39785815">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<figure><img fetchpriority="high" decoding="async" width="950" height="534" src="https://cdn-resprivacy.pressidium.com/wp-content/uploads/2024/03/Employer-Review-Site-Glassdoor-Deanonymized-Users-Without-Consent.jpeg" alt="Employer Review Site Glassdoor Deanonymized Users Without Consent" srcset="https://cdn-resprivacy.pressidium.com/wp-content/uploads/2024/03/Employer-Review-Site-Glassdoor-Deanonymized-Users-Without-Consent-300x169.jpeg 300w, https://cdn-resprivacy.pressidium.com/wp-content/uploads/2024/03/Employer-Review-Site-Glassdoor-Deanonymized-Users-Without-Consent-768x432.jpeg 768w, https://cdn-resprivacy.pressidium.com/wp-content/uploads/2024/03/Employer-Review-Site-Glassdoor-Deanonymized-Users-Without-Consent.jpeg 950w" sizes="(max-width: 950px) 100vw, 950px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20950%20534'%3E%3C/svg%3E" data-lazy-srcset="https://cdn-resprivacy.pressidium.com/wp-content/uploads/2024/03/Employer-Review-Site-Glassdoor-Deanonymized-Users-Without-Consent-300x169.jpeg 300w, https://cdn-resprivacy.pressidium.com/wp-content/uploads/2024/03/Employer-Review-Site-Glassdoor-Deanonymized-Users-Without-Consent-768x432.jpeg 768w, https://cdn-resprivacy.pressidium.com/wp-content/uploads/2024/03/Employer-Review-Site-Glassdoor-Deanonymized-Users-Without-Consent.jpeg 950w" data-lazy-src="https://cdn-resprivacy.pressidium.com/wp-content/uploads/2024/03/Employer-Review-Site-Glassdoor-Deanonymized-Users-Without-Consent.jpeg"></figure>



<p>Glassdoor users have been reporting lately that the platform has introduced changes in its privacy policy, which include publishing people’s real names and locations on their profiles without securing consent.</p>



<p>Glassdoor is a website where current and former employees can anonymously submit reviews about their experiences working for a company, including details such as their employer’s culture, management practices, salaries, work-life balance, and more. Job seekers can read those reviews to help them make job-related decisions, and employees can use anonymous feedback to improve on key areas.</p>



<p>While Glassdoor is built on the principle of transparency, it wouldn’t work without securing anonymity for reviewers, as exposing internal practices could put reviewers in legal trouble, blacklisting, firing, and other forms of retaliation. Yet, this is precisely what is happening to the platform right now, according to some user reports.</p>



<p>A Glassdoor account holder named ‘Monica’ posted about her recent experience explaining that she created an account about ten years ago when the platform only required an email address. She specifically chose not to link her Glassdoor account with Facebook or Google.</p>



<p>After responding to a support email from Glassdoor, Monica noticed that her profile had been updated to include her real name and location without the platform asking for her consent.</p>



<p>This discovery led to a series of email exchanges with Glassdoor’s support team, during which the author was informed of a new policy requiring all users to verify their identity for full platform access. The support agents explain that this was happening in the context of a transition to a “verified network,” including the introduction of ‘Fishbowl’ accounts linked to Glassdoor profiles.</p>



<p>Fishbowl is a semi-anonymous social networking platform designed for professionals to connect, share insights, and engage in discussions. The platform was bought by Glassdoor in 2021, and its engineers are working on introducing some level of integration that would offer users a combination of Glassdoor’s insights and Fishbowl’s networking opportunities.</p>



<p>Monica strongly objected to the whole idea, citing privacy concerns and the potential risks of such information being exposed, especially in relation to their employment and the security of their data online. Glassdoor’s support agents didn’t yield and insisted that real names cannot be removed, so the only option for Monica would be to delete the account entirely.</p>



<p>Monica concludes by advising all Glassdoor users to delete their accounts. According to the <a href="https://help.glassdoor.com/s/privacyrequest?language=en_US" target="_blank" rel="noreferrer noopener">platform’s privacy policy</a>, deletion merely deactivates the account, and to completely erase personal data, one must specifically request the deletion of their personal data through a form provided at the bottom of Glassdoor’s data policy page.</p>



<p>A report that appeared in <a href="https://techcrunch.com/2024/03/20/glassdoor-added-real-names-profiles-without-consent/" target="_blank" rel="noreferrer noopener">TechCrunch</a> yesterday attempts to fill in some of the gaps left in <a href="https://cellio.dreamwidth.org/2024/03/12/glassdoor-violates-privacy.html" target="_blank" rel="noreferrer noopener">Monica’s blog post</a>, such as how exactly Glassdoor got to know the user’s name without them giving it. Another Glassdoor user, Josh Simmons, who also got his name exposed on his public profile without previously sharing it with the platform, speculates that the info was either scraped or acquired from a data broker, both very shady scenarios. </p>





<p>Aaron Mackey from the Electronic Frontier Foundation highlighted Glassdoor’s previous unwavering stance in protecting its users’ anonymity in courts and called the recent developments highly concerning.</p>



<p>RestorePrivacy has contacted Glassdoor for a comment on the allegations and clarifications on its policies on user anonymity, and we will update this post as soon as we hear back.</p>



<p>In the meantime, if you have a Glassdoor account and have posted anything in the past, make sure to log in and check the information included on your public profile to avoid damaging exposure.</p>



<h2 id="h-further-reading">Further reading:</h2>



<ul>
<li><a href="https://restoreprivacy.com/qualcomm-denies-unlawful-user-location-data-collection-on-phones/">Qualcomm Denies Unlawful User Location Data Collection on Phones</a></li>



<li><a href="https://restoreprivacy.com/23andme-accounts-hijacked-and-data-put-up-for-sale-on-hacker-forum/">23andMe Accounts Hijacked and Data Put Up for Sale on Hacker Forum</a></li>



<li><a href="https://restoreprivacy.com/data-removal/delete-yourself-from-internet/">How to Delete Yourself from the Internet (Easy Steps)</a></li>



<li><a href="https://restoreprivacy.com/att-investigating-breach-following-leak-of-73-4-million-records/">AT&amp;T Investigating Potential Breach Following Leak of 73.4 Million Records</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FDA says marijuana has a legitimate medicinal purpose (140 pts)]]></title>
            <link>https://www.abcactionnews.com/news/national/fda-says-marijuana-has-a-legitimate-medicinal-purpose</link>
            <guid>39785660</guid>
            <pubDate>Thu, 21 Mar 2024 23:27:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.abcactionnews.com/news/national/fda-says-marijuana-has-a-legitimate-medicinal-purpose">https://www.abcactionnews.com/news/national/fda-says-marijuana-has-a-legitimate-medicinal-purpose</a>, See on <a href="https://news.ycombinator.com/item?id=39785660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <main>
      <article itemprop="mainEntity" itemscope="" itemtype="http://schema.org/Article">
        <div>
          
            
              
            
          
          <div>
                <h2>Actions</h2>
                
              </div>
          <div>
            
              
            
            
              <p>As a Schedule 1 drug, marijuana is currently in the same category as some of the hardest drugs, like heroin and LSD.</p>
            
          </div></div>
  <div>
    
      <div>
  <div>
        <p><img alt="" src="https://x-default-stgec.uplynk.com/ause/slices/6a1/45bf940c346f431c9be273b8942ab6eb/6a167d65bb0b4921b0070f5948dd05c2/poster_5b3668d1bd3c4618be863597229897e7.jpg" data-caption="The FDA released a report saying that marijuana does have a legitimate use for medical purposes and recommended the US Drug Enforcement Agency to change its classification from Schedule 1 to Schedule 3.  “The definition of a schedule 1 drug says it has no health benefits to it, and, so, obviously, there’s been plenty of research that has documented the multitudes of ways that cannabis can be helpful,” said Dr. David Berger with Wholistic ReLeaf." data-old-src="/styleguide/assets/Blank.gif" data-src="https://x-default-stgec.uplynk.com/ause/slices/6a1/45bf940c346f431c9be273b8942ab6eb/6a167d65bb0b4921b0070f5948dd05c2/poster_5b3668d1bd3c4618be863597229897e7.jpg">
    









        </p>
      </div>
  <div>
      
      
      <p>The FDA released a report saying that marijuana does have a legitimate use for medical purposes and recommended the US Drug Enforcement Agency to change its classification from Schedule 1 to Schedule 3.  “The definition of a schedule 1 drug says it has no health benefits to it, and, so, obviously, there’s been plenty of research that has documented the multitudes of ways that cannabis can be helpful,” said Dr. David Berger with Wholistic ReLeaf.</p>
      
    </div>

  
  
  
  
</div>
    
    
      
    

    
      <p><span>Posted at</span> 5:43 AM, Mar 21, 2024 </p>
    
    
      <p><span>and last updated</span> <span>2024-03-21 20:13:30-04</span></p>
    

    

    
      <div itemprop="articleBody"><p>TAMPA, Fla. — The FDA released a report saying that marijuana does have a legitimate use for medical purposes and recommended the US Drug Enforcement Agency change its classification from Schedule 1 to Schedule 3.</p><p>“The definition of a schedule 1 drug says it has no health benefits to it, and, so, obviously, there’s been plenty of research that has documented the multitudes of ways that cannabis can be helpful,” said Dr. David Berger with Wholistic ReLeaf.</p><p>Though not all in the medical community agree, many people swear by the medicinal effects of marijuana to help treat symptoms of cancer, anxiety, PTSD and epilepsy. </p><p>“It’s no longer appropriate to say that there’s no medical benefit when there are hundreds if not thousands of medical studies that show the opposite,” explained Dr. Berger.</p><p>As a Schedule 1 drug, marijuana is in the same category as some of the hardest drugs like heroin and LSD, which means it’s classified as being more dangerous than fentanyl and methamphetamine. </p><p>“What happens after this is the federal government has more decisions to make as to what they’re going to do next,” said Dr. Berger.</p><p>The Department of Health and Human Services formally recommended that the DEA classify marijuana as Schedule 3 in August of last year after the Biden Administration asked them to review how the drug is classified under federal law. </p><p>Earlier this year, Senate Democrats urged Biden to deschedule the drug entirely, meaning you would not need a doctor’s authorization to use marijuana.</p>
    

</div>
    
    
    <p>Copyright 2024 Scripps Media, Inc. All rights reserved. This material may not be published, broadcast, rewritten, or redistributed.</p>


    
    <div>

  <p id="news-sign">
    <h3>Sign up for the <span>Morning Headlines Newsletter</span> and receive up to date information.</h3>
    
  </p>

  

  </div>


  </div>
  
    



  </article>
  </main>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A type of bacteria that causes dental plaque was found in 50% of colon cancers (119 pts)]]></title>
            <link>https://www.nbcnews.com/health/cancer/aggressive-colon-cancer-newly-identified-bacteria-found-half-tumors-ma-rcna144164</link>
            <guid>39784148</guid>
            <pubDate>Thu, 21 Mar 2024 20:43:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nbcnews.com/health/cancer/aggressive-colon-cancer-newly-identified-bacteria-found-half-tumors-ma-rcna144164">https://www.nbcnews.com/health/cancer/aggressive-colon-cancer-newly-identified-bacteria-found-half-tumors-ma-rcna144164</a>, See on <a href="https://news.ycombinator.com/item?id=39784148">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A type of bacteria that causes dental plaque may be behind a treatment-resistant form of colorectal cancer, a <a href="https://www.nature.com/articles/s41586-024-07182-w" target="_blank">study</a> published Wednesday in the journal Nature found.</p><figure><picture data-testid="picture"><source media="(min-width: 1000px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_avif,q_auto:eco,dpr_2/rockcms/2024-03/240320-colon-model-doctor-stock-vl-945a-b0e8a3.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:best/rockcms/2024-03/240320-colon-model-doctor-stock-vl-945a-b0e8a3.jpg 1x"><source srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_avif,q_auto:eco,dpr_2/rockcms/2024-03/240320-colon-model-doctor-stock-vl-945a-b0e8a3.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2024-03/240320-colon-model-doctor-stock-vl-945a-b0e8a3.jpg 1x"><img src="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2024-03/240320-colon-model-doctor-stock-vl-945a-b0e8a3.jpg" alt="Doctor holding a colon anatomy model for study" height="1333" width="2000"></picture><figcaption data-testid="caption"><span data-testid="caption__source">Sasirin Pamai / Getty Images</span></figcaption></figure><p>The particular bacterium, which appears to shield tumor cells from cancer-fighting drugs, was found in 50% of the tumors tested in the study. The discovery, experts say, could pave the way for new treatments and possibly new methods of screening.</p><p>Colon cancer is the second-leading cause of cancer deaths in the United States and is expected to kill more than 53,000 people in the nation in 2024, according to the <a href="https://www.cancer.org/cancer/types/colon-rectal-cancer/about/key-statistics.html" target="_blank">American Cancer Society</a>.&nbsp;</p><p>Rates are rising sharply among younger people: The percentage of people younger than 55 diagnosed with colon cancer <a href="https://acsjournals.onlinelibrary.wiley.com/doi/full/10.3322/caac.21772" target="_blank">almost doubled</a> between 1995 and 2019, leaping from 11% to 20% of cases. What’s more, these cases are often diagnosed at <a href="https://www.nbcnews.com/health/health-news/colon-cancer-deaths-younger-men-women-report-rcna134084" target="_blank">later, more aggressive stages</a>.</p><p>Experts are still struggling to explain the shift.</p><p>“Colorectal cancer is very treatable when caught early, but the cases in younger people are increasing and we don’t know why,” said Dr. Flavio Rocha, a surgical oncologist and physician in chief at the Oregon Health &amp; Science University’s Knight Cancer Institute, who was not involved in the study.&nbsp;</p><p>The new research doesn’t answer that question; it’s far too early to implicate this bacteria in the rise in <a href="https://www.nbcnews.com/health/health-news/colon-cancer-early-signs-symptoms-young-adults-rcna82629" target="_blank">cases in younger people</a>. What’s more, most of the patients in the study were over the age of 50.</p><p>But the findings raise “the question as to whether there are elevated levels of this bacterium in young onset colorectal cancer which is on the rise globally for unknown reasons,” said co-lead study author Susan Bullman, an assistant professor of human biology at the Fred Hutchinson Cancer Center in Seattle.</p><h2><strong>Bacteria with a secret</strong></h2><p>Scientists have suspected a link between the bacteria, called Fusobacterium nucleatum, and colorectal cancer growth for almost a decade. The bacteria is usually only found in the mouth, far from the colon.&nbsp;</p><p>In the mouth, it’s one of the most common types of <a href="https://www.nbcnews.com/health/health-news/bad-breath-certain-types-probiotic-bacteria-may-help-rcna62671" target="_blank">disease-causing bacteria</a>, linked to gum disease and plaque buildup. But it was unclear how it could withstand the journey through the gut and eventually invade tumor cells in places in the body where these types of bacteria usually don’t survive.</p><p>In the study, Bullman and her colleagues looked at the bacterial makeup of almost 200 colorectal tumors, as well as stool samples from more than 1,200 people, half of whom did not have cancer.&nbsp;</p><p>What they learned was that the bacteria was a bit more complicated than once thought. Namely, it has two distinct subspecies, one of which appears to shield colorectal tumors from cancer-fighting drugs.&nbsp;</p><p>“It acts like a cloak,” Bullman said.&nbsp;</p><p>Normally, <a href="https://www.nbcnews.com/health/health-news/two-early-trials-blood-cancer-treatment-appears-promising-deadly-brain-rcna143216" target="_blank">immune cells called T-cells</a> recognize and attack tumor cells. But this bacteria recruits another type of immune cell into the cancer cells, one that lets them escape the T-cells. </p><p>The stealthy subspecies was present in 50% of the colorectal tumors collected in the study. The corresponding stool samples also had elevated amounts of the subspecies, compared to their healthy counterparts.&nbsp;</p><p>“Patients who have high levels of this bacteria in their colorectal tumors have a <a href="https://www.nbcnews.com/health/cancer/colon-cancer-advanced-younger-symptoms-rcna72983" target="_blank">far worse prognosis</a>,” Bullman said. “They don’t respond as well to chemotherapy and they have an increased risk of recurrence.”&nbsp;</p><p>The subspecies may also cause cancer to form in the first place.&nbsp;</p><p>When Bullman and her team transplanted the subspecies to mice, they appeared to cause precancerous polyps to form, one of the first warning signs of colorectal cancer, though she added that this causation hasn’t yet been proven in humans.</p><p>The researchers also found clues that may answer the question as to how Fusobacterium nucleatum can get to the colon in the first place: The bacterium appears to be able to survive the journey through the stomach, withstanding what scientists previously thought would be a toxic dose of stomach acid.&nbsp;</p><h2><strong>New targets for treatment</strong></h2><p>The discovery of the subspecies has huge consequences for targeted therapies that are already underway, said Dr. Michael White, an assistant professor of colorectal surgery at The University of Texas MD Anderson Cancer Center in Houston.</p><p>“There is evidence that if you clear these bacteria, there is more response during treatment,” said White, who was not involved with the new research. Clinical trials are slated to soon test whether treating a patient with antibiotics prior to chemotherapy will induce a better response, he said.</p><p>Knowing more about which bacterial subspecies, including Fusobacterium nucleatum, are dangerous will allow for a more targeted approach, he said.&nbsp;</p><p>That could potentially include prevention.</p><p>It’s possible that scientists could identify the subspecies while it’s still in the mouth and give a person antibiotics at that point, wiping it out before it could&nbsp; travel&nbsp; to the colon, Bullman said. Even if antibiotics can’t successfully eliminate the bacteria from the mouth, its presence there could serve as an indication that someone is at higher risk for aggressive colon cancer, she added.&nbsp;</p><p>Rocha agreed. In the future, part of <a href="https://www.nbcnews.com/health/health-news/colon-cancer-screening-blood-test-coming-approval-rcna143066" target="_blank">colorectal cancer screening</a> could be as simple as a mouth swab, he said.&nbsp;</p><p>Understanding the newly identified subspecies could also lead to the development of new antibiotics that would specifically target this bacterial subtype, rather than wiping out both forms of the bacteria or all of the bacteria in the mouth.</p><p>There’s also the possibility of harnessing the bacteria to do the cancer-fighting work.&nbsp;</p><p>The subtype has already proven that it can enter cancer cells quite easily, so it might be possible to genetically modify the bacteria to carry cancer-fighting drugs directly into the tumors, Bullman said.</p><p>Researchers are just beginning to scratch the surface of the ways a person’s microbiome plays a role in the individual’s cancer risk, Rocha said, but it’s one of the most important concepts being explored in cancer research today.</p></div><div data-activity-map="expanded-byline-article-bottom"><p><span data-testid="byline-thumbnail"></span><span data-testid="byline-name">Kaitlin Sullivan</span><span></span></p><p>Kaitlin Sullivan is a contributor for NBCNews.com who has worked with NBC News Investigations.&nbsp;She reports on health, science and the environment and&nbsp;is a graduate of the&nbsp;Craig Newmark Graduate School of Journalism at City University of New York.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Memories – FOSS Google Photos alternative built for high performance (624 pts)]]></title>
            <link>https://memories.gallery/</link>
            <guid>39783223</guid>
            <pubDate>Thu, 21 Mar 2024 19:25:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://memories.gallery/">https://memories.gallery/</a>, See on <a href="https://news.ycombinator.com/item?id=39783223">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="main">
              <article>
                
<div>
    <div>
        <div>
            <h2>
                Privacy First
            </h2>
            <p>
                Your data should be owned by you! Memories is a self-hosted app, which means that
                your photos are stored on your own Nextcloud instance and not on someone else's
                servers. You can even encrypt your data with a secret key. And it's completely free
                and open source!
            </p>
        </div>
        <p><img src="https://memories.gallery/assets/nclogo.webp" alt="Nextcloud" draggable="false">
    </p></div>

    <div>
            <h2>
                Auto Upload
            </h2>
            <p>
                With the offical Nextcloud mobile apps for
                <a target="_blank" href="https://play.google.com/store/apps/details?id=com.nextcloud.client">Android</a>
                and
                <a target="_blank" href="https://itunes.apple.com/us/app/nextcloud/id1125420102">iOS</a>,
                you can automatically upload photos and videos to your Nextcloud server.
                Memories will automatically extract EXIF metadata from your photos as they are uploaded.
            </p>
        </div>

    <div>
        <div>
            <h2>
                Photo Timeline
            </h2>
            <p>
                Trying to relive memories from your birthday party 10 years ago?
                Memories shows your photos in a familiar timeline view, which lets you instantly
                jump to any point of time in your photo library, even if it has hundreds of
                thousands of photos.
            </p>
        </div>
        <p><img src="https://memories.gallery/assets/timeline-sample.webp" alt="Timeline" draggable="false">
    </p></div>

    <div>
            <h2>
                Albums
            </h2>
            <p>
                Create albums to organize your photos. You can also share albums with other
                users on your Nextcloud server or with anyone on the internet. Multiple users
                on the same Nextcloud server can also collaborate on albums together.
            </p>
        </div>

    <div>
        <div>
            <h2>
                Automatic Tagging
            </h2>
            <p>
                Memories integrates with the
                <a target="_blank" href="https://github.com/nextcloud/recognize">Recognize</a>
                and
                <a target="_blank" href="https://github.com/matiasdelellis/facerecognition">Face Recognition</a>
                apps to automatically tag your photos with keywords and faces using artificial intelligence.
                You can also manually curate your library by assigning tags and faces to photos.
            </p>
        </div>
        <p><img src="https://memories.gallery/assets/ai-sample.webp" alt="Automatic Tagging" draggable="false">
    </p></div>

    <div>
            <h2>
                Metadata Editing
            </h2>
            <p>
                You can edit the EXIF metadata of your photos, such as the title, description, GPS location,
                date and time, and tags. You can also edit the metadata of multiple photos at once!
            </p>
        </div>

    <div>
            <h2>
                Video Transcoding
            </h2>
            <p>
                To support a wide range of video formats and adaptive streaming, Memories is bundled with an
                on-demand video transcoder. You can also give the transcoding process a boost by using
                VA-API or NVENC hardware acceleration.
            </p>
        </div>

    <div>
        <div>
            <h2>
                Map of Photos
            </h2>
            <p>
                Zoom in on your vacations around the world with the map view. Memories will automatically
                extract GPS data from your photos and plot them on a map. You can also find all
                photos at a location with its name with accurate reverse geocoding.
            </p>
        </div>
        <p><img src="https://memories.gallery/assets/map-sample.webp" alt="Map" draggable="false">
    </p></div>

    <div>
            <h2>
                Fast &amp; Battle Tested
            </h2>
            <p>
                Memories is built with performance in mind. It is designed and highly optimized for
                handling large photo libraries even when running on modest hardware such as a Raspberry Pi.
                It also relies on the battle tested Nextcloud platform as the underlying storage layer,
                which is used by thousands of organizations around the world.
            </p>
        </div>

    <div>
            <h2>
                No Lock-In
            </h2>
            <p>
                Memories stores most of the metadata in the EXIF headers of your photos, which means
                that you can easily migrate to other solutions without losing your data. It also utilizes
                your existing filesystem structure for organization without converting it to any
                specialized format.
            </p>
        </div>

              </div></article>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jan: An open source alternative to ChatGPT that runs on the desktop (145 pts)]]></title>
            <link>https://jan.ai/</link>
            <guid>39782876</guid>
            <pubDate>Thu, 21 Mar 2024 18:56:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jan.ai/">https://jan.ai/</a>, See on <a href="https://news.ycombinator.com/item?id=39782876">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__docusaurus_skipToContent_fallback"><main><div><p>Turn your computer into an AI machine</p><p>500K+ Downloads | Free &amp; Open Source</p><p><img src="https://jan.ai/img/homepage/app-frame-light.webp" alt="App screenshots"><img src="https://jan.ai/img/homepage/app-frame-dark.webp" alt="App screenshots"></p></div><div><h2>Built with love</h2><p>Jan is entirely open-source. We build it transparently, guided by the belief <br> that AI's future should be open and shared with everyone.</p></div><div><div><h2>People say nice things</h2><p>...despite our bugs and fast moving releases<!-- --> <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_1810_7276)"><path d="M24.0001 11.4301H22.8601V19.4326H24.0001V11.4301Z"></path><path d="M22.8599 19.4325H21.7124V20.5725H22.8599V19.4325Z"></path><path d="M22.8599 9.14246H21.7124V11.43H22.8599V9.14246Z"></path><path d="M21.7125 20.5725H20.5725V21.72H21.7125V20.5725Z"></path><path d="M21.7125 6.86255H20.5725V9.14255H21.7125V6.86255Z"></path><path d="M20.5726 21.72H19.4326V22.86H20.5726V21.72Z"></path><path d="M20.5726 5.71497H19.4326V6.86247H20.5726V5.71497Z"></path><path d="M19.4324 2.28748H18.2849V5.71498H19.4324V2.28748Z"></path><path d="M12.5701 22.86V21.72H14.8576V20.5725H11.4301V21.72H5.71509V22.86H11.4301V24H19.4326V22.86H12.5701Z"></path><path d="M18.285 1.14746H17.145V2.28746H18.285V1.14746Z"></path><path d="M17.1449 11.4301H14.8574V12.5701H17.1449V11.4301Z"></path><path d="M16.0049 19.4325H14.8574V20.5725H16.0049V19.4325Z"></path><path d="M17.145 0H13.7175V1.1475H17.145V0Z"></path><path d="M14.8575 18.285H13.7175V19.4325H14.8575V18.285Z"></path><path d="M14.8575 12.5699H13.7175V13.7174H14.8575V12.5699Z"></path><path d="M16.005 2.28748H13.7175V4.57498H16.005V2.28748Z"></path><path d="M13.7176 13.7175H12.5701V18.285H13.7176V13.7175Z"></path><path d="M13.7176 1.14746H12.5701V2.28746H13.7176V1.14746Z"></path><path d="M12.5699 2.28748H11.4299V4.57498H12.5699V2.28748Z"></path><path d="M11.43 19.4325H10.29V20.5725H11.43V19.4325Z"></path><path d="M10.2901 16.005H9.14258V19.4325H10.2901V16.005Z"></path><path d="M11.4301 4.57495H9.14258V5.71495H11.4301V4.57495Z"></path><path d="M9.14244 14.8575H8.00244V16.005H9.14244V14.8575Z"></path><path d="M9.14244 6.86255H8.00244V8.00255H9.14244V6.86255Z"></path><path d="M9.14244 2.28748H8.00244V4.57498H9.14244V2.28748Z"></path><path d="M8.00255 1.14746H6.86255V2.28746H8.00255V1.14746Z"></path><path d="M8.00245 8.00244H4.57495V9.14244H8.00245V8.00244Z"></path><path d="M5.71495 20.5725H4.57495V21.72H5.71495V20.5725Z"></path><path d="M6.86249 0H3.42749V1.1475H6.86249V0Z"></path><path d="M1.1475 22.86V21.72H0V24H5.715V22.86H1.1475Z"></path><path d="M8.0026 13.7175H2.2876V14.8575H8.0026V13.7175Z"></path><path d="M5.71499 2.28748H3.42749V4.57498H5.71499V2.28748Z"></path><path d="M3.42746 20.5725H4.57496V19.4325H2.28746V20.5725H1.14746V21.72H3.42746V20.5725Z"></path><path d="M3.4276 5.71497H2.2876V8.00247H3.4276V5.71497Z"></path><path d="M3.4276 1.14746H2.2876V2.28746H3.4276V1.14746Z"></path><path d="M2.28746 18.285H1.14746V19.4325H2.28746V18.285Z"></path><path d="M2.28746 8.00244H1.14746V10.2899H2.28746V8.00244Z"></path><path d="M2.28746 2.28748H1.14746V5.71498H2.28746V2.28748Z"></path><path d="M2.2875 16.005V14.8575H1.1475V10.29H0V18.285H1.1475V16.005H2.2875Z"></path></g><defs><clipPath id="clip0_1810_7276"><rect width="24" height="24" fill="white"></rect></clipPath></defs></svg></p></div><div data-theme="light"><div><p><iframe width="100%" height="260" src="https://www.youtube.com/embed/ZCiEQVOjH5U" title="Install Jan to Run LLM Offline and Local First" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" loading="lazy"></iframe></p><p><iframe width="100%" height="260" src="https://www.youtube.com/embed/7JpzE-_cKo4" title="Install Jan to Run LLM Offline and Local First" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" loading="lazy"></iframe></p></div><div><p><iframe width="100%" height="260" src="https://www.youtube.com/embed/QpMQgJL4AZA" title="Install Jan to Run LLM Offline and Local First" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" loading="lazy"></iframe></p><p><iframe width="100%" height="260" src="https://www.youtube.com/embed/9ta2S425Zu8" title="Install Jan to Run LLM Offline and Local First" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" loading="lazy"></iframe></p></div><div><p><iframe width="100%" height="260" src="https://www.youtube.com/embed/zkafOIyQM8s" title="Install Jan to Run LLM Offline and Local First" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" loading="lazy"></iframe></p></div><div><p><iframe width="100%" height="260" src="https://www.youtube.com/embed/9ta2S425Zu8" title="Install Jan to Run LLM Offline and Local First" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" loading="lazy"></iframe></p><p><iframe width="100%" height="260" src="https://www.youtube.com/embed/ES021_sY6WQ" title="Install Jan to Run LLM Offline and Local First" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" loading="lazy"></iframe></p><p><iframe width="100%" height="260" src="https://www.youtube.com/embed/CbJGxNmdWws" title="Install Jan to Run LLM Offline and Local First" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" loading="lazy"></iframe></p></div></div></div><div><div><div><h2>0<!-- -->1</h2><div><h6>Run local AI or connect to remote APIs</h6><p>Choose between running AI models locally for privacy, like Llama or Mistral, or connect to remote APIs, like ChatGPT or Claude.</p></div></div><div><h2>0<!-- -->2</h2><div><h6>Browse and download models</h6><p>With Jan's Hub, instantly download popular AI models or import your own to expand your toolkit without hassle.</p></div></div><div><h2>0<!-- -->3</h2><div><h6>Use Jan in your natural workflows</h6><p>Jan works with your workflow, ready to assist at a moment's notice without interrupting your work.</p></div></div><div><h2>0<!-- -->4</h2><div><h6>Customize and add features with Extensions</h6><p>Tailor Jan exactly to your needs with Extensions, making your experience truly your own.</p></div></div></div><p><img src="https://jan.ai/img/homepage/features01.webp" alt="App screenshots"><img src="https://jan.ai/img/homepage/features01dark.webp" alt="App screenshots"></p></div><div><h2>Our Philosophy</h2><p>Jan is opinionated software on what AI should be<!-- --> <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.5 0H15V1.5H13.5V3H18V4.5H16.5V6H15V7.5H13.5V9H12V10.5H13.5V12H15V13.5H10.5V16.5H9V12H7.5V10.5H6V9H7.5V7.5H9V6H10.5V4.5H12V1.5H10.5V0Z"></path><path d="M21 0H22.5V1.5H21V0Z"></path><path d="M9 1.5H10.5V4.5H9V1.5Z"></path><path d="M18 1.5H21V3H18V1.5Z"></path><path d="M4.5 10.5H6V12H4.5V10.5Z"></path><path d="M3 12H4.5V13.5H3V12Z"></path><path d="M0 13.5H3V15H0V13.5Z"></path><path d="M15 13.5H16.5V15H21V16.5H15V13.5Z"></path><path d="M7.5 16.5H9V18H12V19.5H7.5V21H3V19.5H6V18H7.5V16.5Z"></path><path d="M12 16.5H15V18H12V16.5Z"></path><path d="M21 16.5H22.5V18H21V16.5Z"></path><path d="M15 18H16.5V19.5H15V18Z"></path><path d="M19.5 18H21V19.5H19.5V18Z"></path><path d="M16.5 19.5H19.5V21H16.5V19.5Z"></path><path d="M1.5 21H3V22.5H1.5V21Z"></path><path d="M7.5 21H9V22.5H12V24H3V22.5H7.5V21Z"></path><path d="M12 21H16.5V22.5H12V21Z"></path></svg></p><div><div><svg width="32" height="32" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M28.96 13.72V9.14H27.43V10.67H24.39V7.62H25.91V6.1H22.86V3.05H21.34V0H4.58V1.52H1.53V13.72H0V32H32V13.72H28.96ZM22.86 7.62V12.19H27.43V13.72H15.24V7.62H22.86ZM3.05 3.05H4.58V13.72H3.05V3.05ZM9.15 18.29H7.62V19.81H4.58V18.29H3.05V16.76H9.15V18.29ZM9.15 13.72H6.1V1.52H19.81V3.05H9.15V13.72ZM10.67 4.57H21.34V6.1H13.72V13.72H10.67V4.57ZM30.48 30.48H12.2V15.24H30.48V30.48Z" fill="#4377E9"></path><path d="M28.9601 22.86H27.4301V24.38H28.9601V22.86Z" fill="#4377E9"></path><path d="M28.9601 27.43H19.8101V28.95H28.9601V27.43Z" fill="#4377E9"></path><path d="M27.43 7.62H25.91V9.14H27.43V7.62Z" fill="#4377E9"></path><path d="M25.91 24.38H22.86V25.91H25.91V24.38Z" fill="#4377E9"></path><path d="M21.3401 22.86H19.8101V24.38H21.3401V22.86Z" fill="#4377E9"></path></svg><h5>Local-first</h5><p>We believe your conversations and files should remain yours alone. That's why we prioritize local-first AI, running open-source models directly on your computer.</p></div><div><svg width="29" height="32" viewBox="0 0 29 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M28.9495 22.8602H24.3795V21.3302H25.8995V19.8103H3.04994V21.3302H4.57491V22.8602H0V32H28.9495V22.8602ZM6.08989 21.3302H22.8496V22.8602H6.08989V21.3302ZM27.4295 30.4801H1.51997V24.3802H27.4295V30.4801Z" fill="#4377E9"></path><path d="M27.4294 1.52042H25.8994V19.8101H27.4294V1.52042Z" fill="#4377E9"></path><path d="M24.3795 25.8996H16.7596V27.4296H24.3795V25.8996Z" fill="#4377E9"></path><path d="M18.2796 10.6697H16.7596V12.1897H18.2796V10.6697Z" fill="#4377E9"></path><path d="M18.2796 7.62H16.7596V9.13997H18.2796V7.62Z" fill="#4377E9"></path><path d="M16.7596 12.1889H12.1897V13.7089H16.7596V12.1889Z" fill="#4377E9"></path><path d="M12.1896 10.6697H10.6597V12.1897H12.1896V10.6697Z" fill="#4377E9"></path><path d="M12.1896 7.62H10.6597V9.13997H12.1896V7.62Z" fill="#4377E9"></path><path d="M4.57483 18.2895H24.3845V3.04974H4.57483V18.2895ZM6.0898 4.56972H22.8495V16.7595H6.0898V4.56972Z" fill="#4377E9"></path><path d="M7.61977 25.8996H4.56982V28.9495H7.61977V25.8996Z" fill="#4377E9"></path><path d="M25.8995 0H3.04993V1.51997H25.8995V0Z" fill="#4377E9"></path><path d="M3.04987 1.52042H1.5199V19.8101H3.04987V1.52042Z" fill="#4377E9"></path></svg><h5>User-owned</h5><p>Your data, your rules. Jan stores everything on your device in universal formats, giving you total freedom to move your data without tricks or traps.</p></div><div><svg width="28" height="32" viewBox="0 0 28 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M27.715 9.90503H26.185V28.195H27.715V9.90503Z" fill="#4377E9"></path><path d="M26.185 28.195H24.665V29.715H26.185V28.195Z" fill="#4377E9"></path><path d="M26.185 8.38501H24.665V9.90501H26.185V8.38501Z" fill="#4377E9"></path><path d="M24.665 29.715H3.33496V31.235H24.665V29.715Z" fill="#4377E9"></path><path d="M24.665 6.85501H18.565V8.38501H24.665V6.85501Z" fill="#4377E9"></path><path d="M18.565 3.80502H17.045V6.85502H18.565V3.80502Z" fill="#4377E9"></path><path d="M17.045 2.285H15.525V3.805H17.045V2.285Z" fill="#4377E9"></path><path d="M15.525 0.765015H12.475V2.28501H15.525V0.765015Z" fill="#4377E9"></path><path d="M12.4749 2.285H10.9449V3.805H12.4749V2.285Z" fill="#4377E9"></path><path d="M10.945 3.80502H9.42499V6.85502H10.945V3.80502Z" fill="#4377E9"></path><path d="M7.905 17.525H6.375V20.575H7.905V17.525Z" fill="#4377E9"></path><path d="M6.37498 20.575H4.85498V22.095H6.37498V20.575Z" fill="#4377E9"></path><path d="M6.37498 15.995H4.85498V17.525H6.37498V15.995Z" fill="#4377E9"></path><path d="M9.42496 6.85501H3.33496V8.38501H9.42496V6.85501Z" fill="#4377E9"></path><path d="M1.80497 14.475V9.90503H0.284973V15.995H4.85497V14.475H1.80497Z" fill="#4377E9"></path><path d="M3.33499 28.195H1.80499V29.715H3.33499V28.195Z" fill="#4377E9"></path><path d="M3.33499 8.38501H1.80499V9.90501H3.33499V8.38501Z" fill="#4377E9"></path><path d="M1.80497 23.615H4.85497V22.095H0.284973V28.195H1.80497V23.615Z" fill="#4377E9"></path></svg><h5>Fully Customizable</h5><p>You can endlessly customize the experience with 3rd party extensions. You can adjust alignment, moderation, and censorship levels to your needs.</p></div></div><div><div><p title="Ownership"><h6>Ownership</h6></p><p title="Openness"><h6>Openness</h6></p><p title="Your Role"><h6>Your Role</h6></p><p title="Approach"><h6>Approach</h6></p><p title="Data Handling"><h6>Data Handling</h6></p><p title="Privacy"><h6>Privacy</h6></p><p title="Transparency"><h6>Transparency</h6></p><p title="Outage Resilience"><h6>Outage Resilience</h6></p><p title="Philosophy"><h6>Philosophy</h6></p></div><div><div><p><img src="https://jan.ai/img/logo.svg" alt="logo-mark" width="20" height="20"></p><h6>Jan</h6></div><p>Fully owned by you</p><p>Open-source (AGPLv3)</p><p>Creator</p><p>Local-first</p><p>Stored locally, openly accessible</p><p>Private and offline</p><p>Open-source and customizable</p><p>Continues to work on your device</p><p>Empower users with the right to repair</p></div></div></div><div><p><img src="https://jan.ai/img/homepage/mac-system-black.svg" alt="App screenshots"><img src="https://jan.ai/img/homepage/mac-system-white.svg" alt="App screenshots"></p><h2>The Soul of a New Machine</h2><p>Follow our AI research and journey in building Jan</p></div><div><p><img src="https://jan.ai/img/homepage/mapbase-light.webp" alt="App screenshots"><img src="https://jan.ai/img/homepage/mapbase.webp" alt="App screenshots"></p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DuckDB as the New jq (272 pts)]]></title>
            <link>https://www.pgrs.net/2024/03/21/duckdb-as-the-new-jq/</link>
            <guid>39782356</guid>
            <pubDate>Thu, 21 Mar 2024 18:19:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pgrs.net/2024/03/21/duckdb-as-the-new-jq/">https://www.pgrs.net/2024/03/21/duckdb-as-the-new-jq/</a>, See on <a href="https://news.ycombinator.com/item?id=39782356">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
        <header>
          
          

  <p>
    
      
      <span>
        
        
        <time datetime="2024-03-21T00:00:00-07:00">March 21, 2024</time>
      </span>
    

    <span></span>

    
      
      

      <span>
        
        
          2 minute read
        
      </span>
    
  </p>


        </header>
      

      <section itemprop="text">
        
        <p>Recently, I’ve been interested in the <a href="https://duckdb.org/">DuckDB</a> project (like a <a href="https://www.sqlite.org/">SQLite</a> geared towards data applications). And one of the amazing features is that it has many data importers included without requiring extra dependencies. This means it can natively read and parse JSON as a database table, among many other formats.</p>

<p>I work extensively with JSON day to day, and I often reach for <a href="https://jqlang.github.io/jq/">jq</a> when exploring documents. I love <code>jq</code>, but I find it hard to use. The syntax is super powerful, but I have to study the docs anytime I want to do anything beyond just selecting fields.</p>

<!-- I love [jq](https://jqlang.github.io/jq/) for wrangling JSON data, but I find it hard to use. The syntax is super powerful, but I have to read the docs anytime I want to do anything beyond just selecting fields. -->

<p>Once I learned DuckDB could read JSON files directly into memory, I realized that I could use it for many of the things where I’m currently using <code>jq</code>. In contrast to the complicated and custom <code>jq</code> syntax, I’m very familiar with SQL and use it almost daily.</p>

<p>Here’s an example:</p>

<p>First, we fetch some sample JSON to play around with. I used the GitHub API to grab the repository information from the golang org:</p>

<div><pre><code>% curl <span>'https://api.github.com/orgs/golang/repos'</span> <span>&gt;</span> repos.json
</code></pre></div>

<p>Now, as a sample question to answer, let’s get some stats on the types of open source licenses used.</p>

<p>The JSON structure looks like this:</p>

<div><pre><code><span>[</span><span>
  </span><span>{</span><span>
    </span><span>"id"</span><span>:</span><span> </span><span>1914329</span><span>,</span><span>
    </span><span>"name"</span><span>:</span><span> </span><span>"gddo"</span><span>,</span><span>
    </span><span>"license"</span><span>:</span><span> </span><span>{</span><span>
      </span><span>"key"</span><span>:</span><span> </span><span>"bsd-3-clause"</span><span>,</span><span>
      </span><span>"name"</span><span>:</span><span> </span><span>"BSD 3-Clause </span><span>\"</span><span>New</span><span>\"</span><span> or </span><span>\"</span><span>Revised</span><span>\"</span><span> License"</span><span>,</span><span>
      </span><span>...</span><span>
    </span><span>},</span><span>
    </span><span>...</span><span>
  </span><span>},</span><span>
  </span><span>{</span><span>
    </span><span>"id"</span><span>:</span><span> </span><span>11440704</span><span>,</span><span>
    </span><span>"name"</span><span>:</span><span> </span><span>"glog"</span><span>,</span><span>
    </span><span>"license"</span><span>:</span><span> </span><span>{</span><span>
      </span><span>"key"</span><span>:</span><span> </span><span>"apache-2.0"</span><span>,</span><span>
      </span><span>"name"</span><span>:</span><span> </span><span>"Apache License 2.0"</span><span>,</span><span>
      </span><span>...</span><span>
    </span><span>},</span><span>
    </span><span>...</span><span>
  </span><span>},</span><span>
  </span><span>...</span><span>
</span><span>]</span><span>
</span></code></pre></div>

<p>This might not be the best way, but here is what I cobbled together after searching and reading some docs for how to do this in <code>jq</code>:</p>

<div><pre><code> % <span>cat </span>repos.json | jq <span>\</span>
  <span>'group_by(.license.key)
  | map({license: .[0].license.key, count: length})
  | sort_by(.count)
  | reverse'</span>
<span>[</span>
  <span>{</span>
    <span>"license"</span>: <span>"bsd-3-clause"</span>,
    <span>"count"</span>: 23
  <span>}</span>,
  <span>{</span>
    <span>"license"</span>: <span>"apache-2.0"</span>,
    <span>"count"</span>: 5
  <span>}</span>,
  <span>{</span>
    <span>"license"</span>: null,
    <span>"count"</span>: 2
  <span>}</span>
<span>]</span>
</code></pre></div>

<p>And here is what it looks like in DuckDB using SQL:</p>

<div><pre><code>% duckdb <span>-c</span> <span>\</span>
  <span>"select license-&gt;&gt;'key' as license, count(*) as count </span><span>\</span><span>
  from 'repos.json' </span><span>\</span><span>
  group by 1 </span><span>\</span><span>
  order by count desc"</span>
┌──────────────┬───────┐
│   license    │ count │
│   varchar    │ int64 │
├──────────────┼───────┤
│ bsd-3-clause │    23 │
│ apache-2.0   │     5 │
│              │     2 │
└──────────────┴───────┘
</code></pre></div>

<p>For me, this SQL is much simpler and I was able to write it without looking at any docs. The only tricky part is querying nested JSON with the <code>-&gt;&gt;</code> operator. The syntax is the same as the <a href="https://www.postgresql.org/docs/current/functions-json.html">PostgreSQL JSON Functions</a>, however, so I was familiar with it.</p>

<p>And if we do need the output in JSON, there’s a DuckDB flag for that:</p>

<div><pre><code>% duckdb <span>-json</span> <span>-c</span> <span>\</span>
  <span>"select license-&gt;&gt;'key' as license, count(*) as count </span><span>\</span><span>
  from 'repos.json' </span><span>\</span><span>
  group by 1 </span><span>\</span><span>
  order by count desc"</span>
<span>[{</span><span>"license"</span>:<span>"bsd-3-clause"</span>,<span>"count"</span>:23<span>}</span>,
<span>{</span><span>"license"</span>:<span>"apache-2.0"</span>,<span>"count"</span>:5<span>}</span>,
<span>{</span><span>"license"</span>:null,<span>"count"</span>:2<span>}]</span>
</code></pre></div>

<p>We can still even pretty print with <code>jq</code> at the end, after using DuckDB to do the heavy lifting:</p>

<div><pre><code>% duckdb <span>-json</span> <span>-c</span> <span>\</span>
  <span>"select license-&gt;&gt;'key' as license, count(*) as count </span><span>\</span><span>
  from 'repos.json' </span><span>\</span><span>
  group by 1 </span><span>\</span><span>
  order by count desc"</span> <span>\</span>
  | jq
<span>[</span>
  <span>{</span>
    <span>"license"</span>: <span>"bsd-3-clause"</span>,
    <span>"count"</span>: 23
  <span>}</span>,
  <span>{</span>
    <span>"license"</span>: <span>"apache-2.0"</span>,
    <span>"count"</span>: 5
  <span>}</span>,
  <span>{</span>
    <span>"license"</span>: null,
    <span>"count"</span>: 2
  <span>}</span>
<span>]</span>
</code></pre></div>

<p>JSON is just one of the many ways of importing data into DuckDB. This same approach would work for CSVs, parquet, Excel files, etc.</p>

<p>And I could choose to create tables and persist locally, but often I’m just interrogating data and don’t need the persistence.</p>

<p>Read more about DuckDB’s great JSON support in this blog post: <a href="https://duckdb.org/2023/03/03/json.html">Shredding Deeply Nested JSON, One Vector at a Time</a></p>

        
      </section>

      

      


      
  

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Launch HN: Soundry AI (YC W24) – Music sample generator for music creators (142 pts)]]></title>
            <link>https://soundry.ai/</link>
            <guid>39782213</guid>
            <pubDate>Thu, 21 Mar 2024 18:09:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://soundry.ai/">https://soundry.ai/</a>, See on <a href="https://news.ycombinator.com/item?id=39782213">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p><img src="https://soundry.ai/assets/star.png" alt="image of a star logo">
            <span>Better than sample libraries</span>
            <span>Say goodbye to generic sounds. Soundry AI offers superior
              flexibility and versatility compared to standard sample
              libraries.</span>
          </p>
          <p><img src="https://soundry.ai/assets/down.png" alt="image of right-pointing arrow">
            <span>Faster than sound design</span>
            <span>Cut down on production time. Soundry AI helps you create
              high-quality samples faster than traditional sound design
              methods.</span>
          </p>
          <p><img src="https://soundry.ai/assets/share.png" alt="image of a circle with 3 dots on it">
            <span>Unlimited variations</span>
            <span>Experiment non-stop with Soundry AI, adjusting a sound endlessly
              until you discover the perfect variation for your project.</span>
          </p>
          <p><img src="https://soundry.ai/assets/heart.png" alt="image of a heart">
            <span>Super easy to use</span>
            <span>Our user-friendly interface makes it accessible for both novice
              and experienced creators to enjoy the process.</span>
          </p>
          <p><img src="https://soundry.ai/assets/file.png" alt="image of a folder">
            <span>Glossary for inspiration</span>
            <span>Get inspiration from our extensive glossary. It's a treasure
              trove that will spark your creativity every time.</span>
          </p>
          <p><img src="https://soundry.ai/assets/tick.png" alt="image of a checkmark circled">
            <span>Completely unique results</span>
            <span>Craft music that stands out. With Soundry AI, you'll generate
              completely unique results that resonate with your style.</span>
          </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Silent Running: 1970s Environmental Fable Remains Depressingly All Too Relevant (125 pts)]]></title>
            <link>https://reactormag.com/silent-running-a-1970s-environmental-fable-remains-depressingly-all-too-relevant/</link>
            <guid>39781776</guid>
            <pubDate>Thu, 21 Mar 2024 17:35:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reactormag.com/silent-running-a-1970s-environmental-fable-remains-depressingly-all-too-relevant/">https://reactormag.com/silent-running-a-1970s-environmental-fable-remains-depressingly-all-too-relevant/</a>, See on <a href="https://news.ycombinator.com/item?id=39781776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p><strong><em>Silent Running </em>(1972) </strong>Directed by Douglas Trumbull. Starring Bruce Dern. Screenplay by Deric Washburn, Michael Cimino, and Steven Bochco.</p>
<hr>
<p>For just a moment, let’s time travel back to the summer of 1962. In the U.S., John F. Kennedy is president and committed to sending astronauts to the moon. Dodger Stadium is brand new. The Supreme Court issues rulings declaring mandatory prayer in schools unconstitutional and decriminalizing nude photographs of men. The Cuban Missile Crisis is lurking just a few months in the future. And, starting in June as a lead-up to a September publication, <em>The New Yorker</em> begins serializing a little book called <em>Silent Spring, </em>in which marine biologist and conservationist Rachel Carson documents research into the harmful effects of man-made pesticides on the natural world.</p>
<p><em>Silent Spring</em> had a huge and immediate impact on the American public, which Carson and her publisher, Houghton Mifflin, had very much expected and prepared for. There was somewhat panicked pushback from the U.S. government and chemical industry giants like DuPont and Monsanto, but all of that only made <em>Silent Spring</em> more influential in public opinion. A culture of unchecked growth and technological development had dominated life in the U.S. since the end of WWII, and Carson’s book was one of the many catalysts that prompted people to seriously ask if all that progress was doing more harm than good.</p>
<p>The environmental movement picked up momentum over the next decade. By the end of the 1960s, the Environmental Defense Fund, founded as a direct response to <em>Silent Spring</em>, was actively filing lawsuits to end use of the pesticide DDT. The first Earth Day was declared in 1970; the U.S. Environmental Protection Agency was established that same year. Environmental concerns were in the news, in the halls of government, in the courtrooms, and in the public consciousness. It’s no surprise that those themes showed up in movie theaters as well.</p>
<p>Humanity’s impact on the natural world—whether our own Earth or other worlds—has always been a part of science fiction, but <em>Silent Running</em> was not initially meant to be an environmental story at all. Douglas Trumbull had recently finished working with Stanley Kubrick, creating the special effects for <em>2001: A Space Odyssey</em>, so he had big, thoughtful, serious science fiction on his mind when he first starting putting <em>Silent Running</em> together. His earliest conception of the film was about first contact with an alien civilization.</p>
<p>But it evolved, as stories tend to do, and what he ended up with is an environmental fable that simply—and not remotely subtly—calls out the dysfunction in humanity’s relationship with nature. Trumbull had never directed a film before, and in fact he would only direct one more feature in his life. (That would be <em>Brainstorm</em> (1983), which is largely remembered now for being Natalie Wood’s last movie, as she died during production.) What Trumbull is mostly known for is his absolutely legendary special effects work on some of Hollywood’s most influential sci fi films: <em>2001</em>, <em>Star Trek: The Motion Picture, Close Encounters of the Third Kind</em>, <em>Blade Runner.</em> So much of how we imagine science fiction to look comes from Trumbull’s special effects work.</p>
<p>Here are some fun details about how <em>Silent Running </em>was made, which I share for no real reason other than the fact that I love learning about this stuff and hope you do too:</p>
<ul>
<li>The exterior shots of <em>Valley Forge</em> and the other ships use a 25-foot model built from wood, metal, and plastic, with much of the mechanical detailing coming from literally hundreds of model kits of WWII airplanes and tanks.</li>
<li>For the interior, Trumbull made a deal with the U.S. Navy to film inside the aircraft carrier <em>USS Valley Forge</em>, which was waiting to be decommissioned and scrapped at the Long Beach Naval Shipyard.</li>
<li>The forests were filmed inside a hangar at the Van Nuys Airport, and many scenes with the stars and domes were done “in-camera,” that is, using projected footage in the live scene, rather than added afterward during processing.</li>
<li>And, finally, the three robotic drones—Huey, Dewey, and Louie—are not mechanical at all, but are actors in costume: Mark Persons, Cheryl Sparks, Steven Brown, and Larry Whisenhunt, all double amputees. (Additional fun fact: A few years later, George Lucas would direct Ralph McQuarrie to consider the <em>Silent Running</em> drones as an example of what he wanted R2-D2 to look like.)</li>
</ul>
<p>On that note, if the business and craft of making sci fi movies in ’70s Hollywood interests you, I recommend Trumbull’s lengthy <a href="https://archive.org/details/Fantastic_Films_003_August_1978_vol1_no3_c2c-Tranzor-HQS/page/n5/mode/2up" target="_blank" rel="noreferrer noopener">1978 interview with <em>Fantastic Films Magazine</em></a>.</p>
<p>Trumbull was inspired by working on <em>2001</em>, but he wasn’t working with <em>2001 </em>money. As a result the practical effects in <em>Silent Running </em>are a great example of doing a lot on a comparatively limited budget, just by looking around Los Angeles and getting creative with what was available. (We are talking about more than a million dollars; this is Hollywood budgeting, not normal people budgeting.) And the film still looks really, really good. The ship exterior is visually interesting, the sense of interior space is convincing, and the images of the darkness of space outside the forest domes are hauntingly effective.</p>
<p>But let’s be real. The most important special effect in <em>Silent Running</em> is Bruce Dern’s crazy eyes.</p>
<p><iframe title="YouTube video player" src="https://www.youtube.com/embed/IAeTg6nGQEk?si=hGaFt5a8eA2ztgby" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p>Dern plays Freeman Lowell, a man who has spent the better part of a decade aboard the spaceship <em>Valley Forge, </em>caring for the last remnants of Earth’s flora and forests. The film doesn’t explain why the last pieces of nature have been enclosed in domes and launched into space aboard American Airlines spacecraft—and I have questions about what the hell American Airlines was thinking with this sponsorship, because it does not make them look good. We learn that there are no trees or plants left on Earth, no parks or wild areas, no places where kids can dig in the dirt or run through the grass. There is also, apparently, “…hardly any disease. No poverty. And everybody has a job.” Everything has been replaced by absolute uniformity: everywhere is 75 degrees Fahrenheit and everybody looks and acts the same. (Feel free to insert your own joke about Hollywood here.)</p>
<p>The film doesn’t dig deeply into any of this; these details emerge when the characters are arguing. I’m skeptical enough to doubt fictional characters when they claim the world has no poverty or disease, nor am I entirely convinced the audience is supposed to buy it. (The whole world? Or just the world of white guys working corporate space jobs? I have questions.) It’s vague and muddled worldbuilding anyway, so we won’t dwell on it. Whatever the intent, as a person who likes trees and seasons a lot more than I like being an anonymous cog in the grinding wheels of capitalism, I agree with Lowell that this future Earth sounds pretty bleak. But even bleaker is the fact that of the few characters we meet in the film, only Lowell believes it’s a situation than can be changed.</p>
<p>When <em>Valley Forge </em>and its fleet receive orders to destroy the precious forests and return to Earth, with no explanation except that it’s time they get on with the business of commercial shipping, Lowell is crushed, but the other members of the expedition are happy to be going home. They acknowledge that it’s kinda sad to destroy the last of Earth’s forests, but to them it’s inevitable. They don’t even question that they’re doing it just so their bosses can make more money using the ships for something else. There’s no point in trying to fight it, or dream about a different world. One of them says, “The fact is, Lowell, if people were interested, something would have been done a long time ago.”</p>
<p>I’ll pause there to let everybody wince before we move on.</p>
<p>Lowell decides to do something about it. What he decides to do is murder: he kills his three crewmates to stop them from jettisoning the last forest dome, then stages an explosion so the other ships in the fleet think he’s suffering some sort of catastrophic mechanical failure. He sets course for Saturn, hoping to run away far enough that the other ships don’t follow.</p>
<p>Because Lowell is the only character on screen for the majority of the film, so much of the movie depends on how Dern plays him. Contemporaneous reviews of the film had some mixed opinions about Dern’s portrayal, but I come down on the side of loving it. Even before things start to go wrong, he’s wide-eyed and strident, soft-spoken but intense, and more than a little sanctimonious. His crew uniform has a prominent Smokey the Bear patch on it, but when he’s working in the forest dome he wears a loose robe to talk to rabbits and birds, like some sort of space-age St. Francis of Assisi, complete with Joan Baez on the soundtrack. (The music was composed by Peter Schickele, better known as musical satirist P.D.Q. Bach; Diane Lampert wrote the lyrics to the songs Baez sings.) He’s a hippie, a wild man of the woods, a bedraggled mystic, a wise hermit. He’s committed to his counterculture perspective. He’s an insufferable dinner companion.</p>
<p>The fact that he’s right about what a terrible decision it is to destroy the forests has nothing to do with how he comes across; there is no effort here to artificially link righteousness or morality with likability. Lowell’s crewmates are good-natured and affable—they’re also the ones who laugh while blowing up cute little bunnies with nuclear bombs.</p>
<p>But they are Lowell’s fellow humans and almost-friends, and their deaths weigh on his conscience, even though he believes he made the right choice. The way he unravels as the film goes on is fascinating, because he uses the robotic drones to replace the crew. He reprograms them to follow his lead when it comes to work, but also to keep him company in poker games. He even has them bury the corpse of a former human crewmate, just in case the symbolism wasn’t clear enough. (About the poker: A computer scientist by the name of Nicholas Findler was programming computers to play poker in the 1970s—there may have been others, but his research articles were the ones that came up when I dug around—so this element wasn’t actually very futuristic at the time, just a few years on the early side.)</p>
<p>This progression grows more unsettling when Lowell teaches the drones to care for the forest, recalling the children of Earth who will never have a chance to climb a tree or play in grass. It’s interesting to me that the drones are never actually proven to have any personality or human characteristics; Lowell’s perception is what anthropomorphizes them. And when we get to the end of the film, Lowell uses the last drone to replace himself as the final caretaker of Earth’s forests. Humans may be willing to save themselves—after all, the other ship shows up to rescue Lowell, even after he assumed they would abandon him—but they can’t be trusted with the last scrap of Earth’s forests. That’s up to one little robot with a watering can.</p>
<p><iframe loading="lazy" title="YouTube video player" src="https://www.youtube.com/embed/gsNBcmD3gnA?si=ukbqBOowD2jjDltP" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p><em>Silent Running</em> is far from a perfect movie. If we started listing the scientific inaccuracies we would be here all day. In a <a href="https://www.nytimes.com/1978/05/28/archives/growing-up-with.html" target="_blank" rel="noreferrer noopener">1978 <em>New York Times</em> piece about science fiction</a>, Carl Sagan wrote, “Trumbull’s characters are able to build interplanetary cities but have forgotten <a href="https://en.wikipedia.org/wiki/Inverse-square_law" target="_blank" rel="noreferrer noopener">the inverse‐square law</a>. I was willing to overlook the portrayal of the rings of Saturn as pastel‐colored gases, but not this.” And, really, that about sums it up. Science fiction often has very silly science.</p>
<p>But as a fable about man’s relationship to the natural world, the film is anything but silly. It’s heavy and melancholy, even more so now, fifty-two years of escalating climate crisis later, than it was upon release. <em>Silent Running</em> was a modest success at the time, sandwiched as it was in an era of some of the biggest, flashiest, most genre-defining sci fi films to come out of Hollywood, but it’s easy to see why it’s maintained a long-lasting cult status, even as its style of earnest and heavy-handed moral commentary has fallen out of style.</p>
<p>There are a lot of climate crisis stories in modern sci fi, but a great many of them focus, intentionally or not, on the natural world’s utility to humans: we must preserve it or else we doom ourselves. <em>Silent Running </em>argues that we should preserve the natural world even if we can live without it, even if it serves no purpose in feeding the hungry or curing the ill, even if we can find a way to get along just fine. That’s a less common philosophy in environmental sci fi, and it’s one of the reasons I find this film so interesting.</p>
<p>One last note: On the wall besides Lowell’s bunk is a copy of something called the “Conservation Pledge,” which dates back to 1946, when the magazine <a href="https://www.outdoorlife.com/blogs/out-there/outdoor-life-archives-70-year-anniversary-ol-conservation-pledge/" target="_blank" rel="noreferrer noopener"><em>Outdoor Life</em></a> held a contest to encourage outdoors enthusiasts to dedicate themselves to the preservation of the America’s natural resources. The winning entry, the one that adorns Lowell’s wall aboard <em>Valley Forge</em>, was submitted by L.L. Foreman, a former ranch hand turned author of pulpy adventure Westerns.</p>
<p>The second-place winner of that 1946 contest? Rachel Carson.</p>
<p>What are your thoughts on <em>Silent Running</em> and its place in the subgenre of environmental sci fi? Do the cute little drones succeed in emotionally manipulating you even when you’re fully aware you’re being emotionally manipulated? Share your thoughts below!</p>
<hr>
<p><strong>Next week:</strong> We’re traveling back into deep space with another (loose) adaptation of a Stanislaw Lem novel, traveling to a distant planet aboard the spaceship <em>Ikarie XB-1</em>. Watch it on <a href="https://www.criterionchannel.com/ikarie-xb-1" target="_blank" rel="noreferrer noopener">Criterion</a>, <a href="https://www.cultpix.com/movie/ikarie-xb-1/1069" target="_blank" rel="noreferrer noopener">Cultpix</a> (some locations), <a href="https://player.bfi.org.uk/subscription/film/watch-ikarie-xb-1-1963-online" target="_blank" rel="noreferrer noopener">British Film Institute</a> (UK only), and I suspect you all know how to poke around the internet for other options, if you need to. If the version you stumble across is the American dubbed release titled <em>Voyage to the End of the Universe</em>, take note that it has a different cut and ending.<svg width="31" height="13" viewBox="0 0 31 13" fill="none" xmlns="http://www.w3.org/2000/svg" aria-labelledby="icon-paragraph-end">
<title id="icon-paragraph-end">icon-paragraph-end</title>
<path d="M15.0891 10.4122C15.138 10.2591 15.186 10.128 15.221 9.99444C15.256 9.86009 15.278 9.72248 15.3114 9.55963C15.1217 9.57836 14.9621 9.59546 14.8025 9.6093C13.871 9.69154 12.9387 9.73796 12.0039 9.68096C11.8313 9.67037 11.7222 9.7119 11.6261 9.86824C11.5463 9.9977 11.4177 10.1068 11.2906 10.1948C11.1498 10.2925 10.9853 10.2794 10.82 10.2241C10.7484 10.2004 10.6555 10.198 10.5847 10.2216C10.3722 10.2933 10.1645 10.3804 9.9569 10.4651C9.50336 10.6491 8.98712 10.4521 8.82427 10.1093C8.80799 10.0759 8.73389 10.0555 8.68667 10.0547C8.62234 10.0547 8.55801 10.0816 8.49369 10.093C8.2543 10.1361 8.06865 10.0457 7.9343 9.85195C7.77877 9.62722 7.73643 9.37398 7.77959 9.10772C7.8032 8.96523 7.8944 8.87811 8.03282 8.83169C8.20137 8.77551 8.20056 8.76492 8.16392 8.59393C8.14437 8.50355 8.12972 8.41154 8.12646 8.31953C8.11669 8.0093 8.22906 7.83993 8.5173 7.73653C8.86906 7.6095 9.22407 7.49144 9.57583 7.3636C9.63364 7.34243 9.69389 7.3009 9.72972 7.25205C9.82173 7.12665 9.94224 7.06884 10.0855 7.11525C10.3233 7.19261 10.4446 7.13072 10.5814 6.9011C10.7076 6.69021 10.6881 6.54283 10.5936 6.35963C10.5309 6.2383 10.4813 6.10884 10.4056 5.99647C10.3176 5.86782 10.2215 5.73754 10.1051 5.63657C9.65562 5.24736 9.19639 4.86955 8.73959 4.48848C8.67934 4.43799 8.61501 4.39239 8.55231 4.34435C8.56127 4.32888 8.56941 4.31423 8.57837 4.29876C8.64595 4.27107 8.71354 4.2442 8.7803 4.21652C10.3396 3.57407 11.8973 2.92756 13.4582 2.29163C14.2203 1.98058 14.9898 1.68338 15.8024 1.52704C16.2519 1.44073 16.7013 1.40165 17.1427 1.58567C17.1809 1.60195 17.2501 1.57264 17.2925 1.54577C17.9406 1.14272 18.601 0.75839 19.3208 0.500273C20.372 0.123274 21.4574 -0.0859885 22.577 0.0337065C23.4971 0.132231 24.3333 0.491316 25.0759 1.03686C25.4497 1.31127 25.7745 1.65325 26.1198 1.96755C26.2338 2.07096 26.347 2.17682 26.4512 2.29C26.6955 2.55707 27.013 2.63035 27.3444 2.5758C28.1522 2.44308 28.955 2.28104 29.7603 2.13203C29.8686 2.11168 29.9769 2.09295 30.0868 2.08236C30.1186 2.07911 30.1536 2.10679 30.1878 2.12063C30.1699 2.1532 30.1609 2.20124 30.1332 2.21671C30.042 2.26883 29.9468 2.31687 29.8474 2.35269C29.2221 2.57743 28.5943 2.79646 27.9681 3.02038C27.8232 3.07249 27.6807 3.13112 27.5382 3.18975C27.2768 3.29723 27.0961 3.48288 26.9853 3.74506C26.4276 5.05845 25.5384 6.09092 24.3341 6.85143C24.1257 6.98253 23.9034 7.09082 23.6892 7.21378C23.6469 7.2382 23.5915 7.27566 23.5818 7.31637C23.493 7.68686 23.2553 7.95393 22.9866 8.20146C22.6641 8.49948 22.2668 8.67129 21.8816 8.86671C20.946 9.34141 19.9356 9.55475 18.9137 9.73551C17.7607 9.93907 16.6085 10.1516 15.4564 10.3592C15.3407 10.3804 15.2243 10.3934 15.0883 10.4138L15.0891 10.4122ZM23.6819 1.54414C23.6078 1.53193 23.55 1.51646 23.4906 1.51239C22.5925 1.45132 21.7041 1.52134 20.8247 1.71269C19.8883 1.91626 18.9764 2.19636 18.0913 2.56359C18.0497 2.58069 17.9602 2.57743 17.9488 2.55463C17.9105 2.48216 17.9463 2.42028 18.0253 2.38852C18.0457 2.38038 18.0652 2.37061 18.0856 2.36084C18.93 1.97244 19.8118 1.70048 20.7197 1.51076C20.955 1.4619 21.1936 1.42445 21.4509 1.37804C21.1667 0.944854 21.3133 0.576812 21.6235 0.234827C21.582 0.221799 21.5698 0.216099 21.5575 0.215285C21.5266 0.213656 21.4957 0.212842 21.4655 0.21447C20.854 0.238898 20.2588 0.354522 19.6799 0.549128C18.9747 0.786075 18.3079 1.11096 17.6809 1.50424C16.8381 2.03351 16.0182 2.59778 15.1901 3.14985C15.1323 3.18812 15.0867 3.24593 15.0134 3.31514C15.072 3.31107 15.0932 3.31025 15.1144 3.307C15.5679 3.24104 16.019 3.1645 16.4742 3.10995C17.4334 2.99514 18.3942 2.90639 19.3607 3.01712C19.8843 3.07656 20.3972 3.17672 20.8646 3.43728C20.9151 3.46496 20.9607 3.5016 21.025 3.54639C20.9656 3.56756 20.9338 3.57977 20.9013 3.59117C20.0951 3.85906 19.2833 4.11148 18.4846 4.39891C16.6191 5.06904 14.7341 5.67647 12.8206 6.19433C12.6097 6.25133 12.4005 6.31566 12.1863 6.37754C12.1383 6.27087 12.1008 6.17235 12.0503 6.08034C11.9876 5.96716 11.9168 5.95169 11.8118 6.02578C11.7116 6.09744 11.6155 6.17479 11.5203 6.25377C11.197 6.52248 10.8843 6.80502 10.6897 7.18446C10.6164 7.32777 10.5822 7.49958 10.5692 7.66161C10.557 7.81144 10.6083 7.82935 10.7451 7.76502C11.0366 7.62904 11.3257 7.48818 11.6188 7.35708C13.8075 6.37835 16.0613 5.56655 18.329 4.79382C19.4429 4.41438 20.5699 4.07809 21.7318 3.88023C21.8767 3.8558 22.029 3.86476 22.1772 3.87534C22.2195 3.8786 22.257 3.94211 22.2969 3.97875C22.2529 4.00237 22.2122 4.03982 22.1658 4.04715C21.025 4.23524 19.9071 4.51453 18.8078 4.87117C16.6997 5.55515 14.6217 6.31891 12.5723 7.16166C11.9616 7.41245 11.3615 7.69011 10.7557 7.95312C10.649 7.99953 10.6409 8.06385 10.6783 8.16401C10.7207 8.278 10.7825 8.23566 10.8534 8.20553C11.5764 7.90182 12.2897 7.5753 13.0242 7.30171C14.6584 6.69428 16.2983 6.10069 17.9447 5.52746C19.5105 4.98273 21.1146 4.57153 22.7439 4.26619C22.8253 4.25071 22.8384 4.21977 22.8408 4.14649C22.8489 3.8558 22.8498 3.5643 22.8823 3.27606C22.9515 2.65234 23.1616 2.08643 23.6184 1.63371C23.6388 1.61335 23.6526 1.58567 23.6803 1.54414H23.6819ZM14.5713 8.99373C14.5761 9.01164 14.581 9.03037 14.5859 9.04828C14.6315 9.05154 14.6779 9.05887 14.7235 9.05643C14.9043 9.04991 15.0842 9.04096 15.265 9.03119C16.4001 8.97012 17.5351 8.90986 18.6588 8.72747C19.0936 8.65663 19.526 8.57358 19.9144 8.34477C20.3223 8.10457 20.6871 7.81876 20.9281 7.40187C20.9867 7.3009 21.012 7.18121 21.0527 7.06965C21.0397 7.06151 21.0275 7.05337 21.0144 7.04523C20.9298 7.06558 20.8443 7.08187 20.7612 7.10711C19.6888 7.42385 18.6173 7.74141 17.5457 8.0606C16.6769 8.31871 15.8073 8.57765 14.9401 8.83984C14.8131 8.87811 14.6934 8.94243 14.5713 8.99373ZM26.0343 2.38526C25.5368 2.05305 25.01 1.69722 24.3455 1.63208C24.0605 1.6044 23.8236 1.68419 23.6371 1.90079C23.3456 2.24033 23.1877 2.64257 23.1014 3.07575C23.0769 3.19789 23.1062 3.21906 23.2276 3.17997C24.0361 2.92104 24.8683 2.83147 25.7135 2.83717C25.8079 2.83717 25.9016 2.83717 25.9708 2.83717C25.9903 2.69387 26.0091 2.56114 26.0335 2.38526H26.0343ZM11.0521 5.29947C11.1473 5.26364 11.2288 5.23352 11.3094 5.20176C12.3956 4.77916 13.485 4.36552 14.6266 4.11229C14.7895 4.07646 14.9548 4.04389 15.1119 3.99097C15.1657 3.97306 15.2341 3.89244 15.2316 3.8444C15.2292 3.79718 15.1478 3.72145 15.0948 3.71575C14.9572 3.7011 14.8131 3.69865 14.6779 3.72471C13.9109 3.87534 13.1797 4.14323 12.4501 4.41519C12.0121 4.57886 11.5878 4.76939 11.1978 5.03077C11.1115 5.08858 11.0358 5.15046 11.0504 5.29866L11.0521 5.29947ZM25.8625 3.93967C25.8576 3.92909 25.8527 3.91769 25.8478 3.9071C25.7811 3.91524 25.711 3.91361 25.6483 3.93397C25.0205 4.13509 24.3944 4.3411 23.7666 4.54222C23.6705 4.57316 23.651 4.63504 23.6836 4.71402C23.7275 4.81906 23.7772 4.92247 23.8374 5.01937C23.9262 5.16268 23.9865 5.18548 24.1273 5.09754C24.6566 4.76614 25.1801 4.42741 25.7045 4.08786C25.764 4.04959 25.8104 3.99015 25.8625 3.94048V3.93967ZM9.18336 8.61347C8.81287 8.73724 8.45786 8.84961 8.10855 8.97663C7.98478 9.02141 7.94732 9.16798 7.98234 9.31699C8.00351 9.40655 8.05318 9.43587 8.13542 9.40818C8.50427 9.28116 8.87313 9.15169 9.25257 9.01897C9.22977 8.88706 9.2086 8.76004 9.18336 8.61347ZM9.56606 7.59077C9.20534 7.70965 8.8544 7.82365 8.5059 7.94172C8.40168 7.97673 8.33979 8.05245 8.33328 8.16726C8.32677 8.28777 8.36585 8.32197 8.47903 8.2837C8.76565 8.18518 9.05064 8.0834 9.33644 7.98243C9.51883 7.9181 9.57909 7.82039 9.56687 7.59077H9.56606ZM22.1047 0.752691C22.3026 0.702207 22.4874 0.665566 22.6641 0.605311C22.7219 0.585769 22.805 0.49783 22.7968 0.457117C22.7838 0.396048 22.7097 0.319509 22.647 0.303224C22.5053 0.265768 22.3539 0.242969 22.2073 0.247855C22.1202 0.251112 22.0225 0.302409 21.9525 0.360221C21.867 0.431875 21.8059 0.552385 21.8694 0.652538C21.9069 0.711978 22.0241 0.720935 22.1039 0.751876L22.1047 0.752691ZM9.76636 8.18843C9.82336 8.35454 9.88443 8.5638 9.96993 8.76248C9.98947 8.80727 10.0921 8.84472 10.1515 8.83821C10.2394 8.82925 10.2541 8.74375 10.2419 8.66233C10.2378 8.63627 10.2346 8.61022 10.2305 8.58416C10.1971 8.38141 10.1686 8.17785 10.1279 7.97673C10.1132 7.90263 10.0855 7.82202 10.0383 7.76502C9.96667 7.67871 9.88606 7.69581 9.85104 7.80248C9.81603 7.90996 9.80138 8.02396 9.76636 8.18843ZM26.6686 3.09773C26.7671 3.06924 26.8697 3.04969 26.9634 3.00817C27.0016 2.99107 27.0407 2.92837 27.0407 2.88684C27.0407 2.85672 26.9813 2.80868 26.9406 2.79972C26.7997 2.76878 26.6564 2.74435 26.5131 2.73295C26.4219 2.72562 26.3836 2.79728 26.382 2.88196C26.3779 3.03911 26.4325 3.07575 26.6694 3.09773H26.6686ZM23.208 1.14923C23.5028 1.19401 23.5663 1.16552 23.6469 0.964396C23.6762 0.891113 23.721 0.814574 23.6111 0.764904C23.5052 0.716049 23.3375 0.744548 23.3049 0.828416C23.2658 0.929383 23.2422 1.03524 23.2088 1.14923H23.208Z" fill="#CA3624"></path>
<path d="M7.43167 10.071C7.46669 10.0995 7.51473 10.1223 7.53101 10.1589C7.53834 10.176 7.49111 10.2363 7.45773 10.2493C7.21101 10.3438 6.96348 10.4342 6.7135 10.5197C4.56876 11.2517 2.42321 11.9821 0.27766 12.7124C0.219034 12.732 0.161222 12.7564 0.100967 12.7662C0.0692112 12.7711 0.0333843 12.7499 0 12.7401C0.0154708 12.7027 0.0211704 12.6522 0.048855 12.6302C0.087939 12.5984 0.143308 12.5846 0.192978 12.5675C1.97945 11.9584 3.76673 11.3502 5.55401 10.7411C6.07024 10.5653 6.58648 10.3886 7.1019 10.2086C7.20938 10.1712 7.31198 10.1207 7.43167 10.0702V10.071Z" fill="#CA3624"></path>
<path d="M0.537992 12.0546C0.528221 12.0196 0.509494 11.9846 0.511936 11.9504C0.514379 11.9162 0.527407 11.8673 0.552649 11.8527C0.603947 11.8217 0.666644 11.8079 0.724456 11.7875C2.77718 11.0832 4.8291 10.3781 6.88182 9.67374C6.93964 9.65339 6.99501 9.61593 7.05445 9.60942C7.10086 9.60453 7.15134 9.63466 7.2002 9.6485C7.16926 9.68758 7.14564 9.74703 7.10574 9.76168C6.67012 9.91639 6.23205 10.0638 5.79561 10.2144C4.11418 10.7942 2.43357 11.3739 0.752955 11.9536C0.699214 11.9724 0.644659 11.9895 0.590104 12.0074C0.573005 12.0229 0.555906 12.0383 0.538807 12.0546H0.537992Z" fill="#CA3624"></path>
<path d="M7.47241 8.79712C7.49277 8.8134 7.53755 8.83213 7.53755 8.85249C7.53755 8.88994 7.52452 8.94368 7.49684 8.96323C7.45205 8.9958 7.39261 9.00801 7.33806 9.02511C5.94569 9.48272 4.5525 9.94033 3.16013 10.3988C3.06405 10.4305 2.97041 10.4696 2.87433 10.4997C2.82059 10.5168 2.74161 10.5632 2.72695 10.4663C2.72125 10.4297 2.77906 10.3597 2.82141 10.3442C3.14385 10.2261 3.46874 10.1154 3.79444 10.0071C4.9637 9.6187 6.1346 9.23193 7.30468 8.84516C7.35434 8.82888 7.40564 8.81585 7.4716 8.79712H7.47241Z" fill="#CA3624"></path>
<path d="M8.55146 10.4764C8.50586 10.5098 8.46515 10.557 8.41385 10.5749C7.74046 10.8078 7.06545 11.0358 6.39125 11.2662C6.03786 11.3875 5.68529 11.5121 5.33191 11.6334C5.28712 11.6489 5.23827 11.6611 5.19186 11.6595C5.17394 11.6595 5.13649 11.6082 5.14219 11.5968C5.16091 11.5577 5.1886 11.5089 5.22443 11.495C5.42555 11.4177 5.62992 11.3485 5.83349 11.2776C6.60702 11.0114 7.38056 10.7459 8.1541 10.4788C8.23716 10.4503 8.31451 10.4023 8.39838 10.3836C8.44316 10.3738 8.4969 10.408 8.54657 10.4226C8.5482 10.4406 8.54983 10.4585 8.55146 10.4756V10.4764Z" fill="#CA3624"></path>
<path d="M4.88158 5.90776C4.86203 5.89391 4.82376 5.88089 4.81969 5.86053C4.81318 5.83122 4.82051 5.78399 4.84086 5.76689C4.87995 5.73351 4.93043 5.71071 4.97847 5.69279C5.96534 5.31661 6.95303 4.94205 7.9399 4.56668C7.94805 4.56343 7.957 4.5561 7.96433 4.55773C8.01563 4.56587 8.06611 4.57646 8.1166 4.58623C8.0881 4.63182 8.06937 4.70267 8.02866 4.71976C7.81858 4.81015 7.6028 4.88669 7.38866 4.9673C6.60616 5.26287 5.82448 5.55844 5.04198 5.85402C4.99313 5.87274 4.94264 5.88821 4.88076 5.90857L4.88158 5.90776Z" fill="#CA3624"></path>
<path d="M7.25558 5.73729C7.23197 5.721 7.18963 5.70472 7.19044 5.69006C7.19207 5.64609 7.19696 5.58747 7.22545 5.56222C7.26942 5.52233 7.33131 5.50115 7.38912 5.47998C7.7596 5.33993 8.1309 5.20151 8.5022 5.06472C8.56083 5.04355 8.64144 4.99306 8.66424 5.09077C8.67319 5.13067 8.61701 5.21454 8.57141 5.23327C8.28235 5.35133 7.98759 5.45556 7.69528 5.56629C7.55115 5.62085 7.40866 5.67785 7.25558 5.7381V5.73729Z" fill="#CA3624"></path>
<path d="M23.6822 1.54485C23.6537 1.58557 23.6406 1.61325 23.6203 1.63442C23.1635 2.08715 22.9534 2.65386 22.8842 3.27677C22.8524 3.56501 22.8516 3.85651 22.8427 4.1472C22.8402 4.22048 22.8272 4.25143 22.7458 4.2669C21.1156 4.57306 19.5124 4.98344 17.9466 5.52817C16.3002 6.10141 14.6602 6.69499 13.026 7.30243C12.2916 7.5752 11.5783 7.90253 10.8553 8.20625C10.7844 8.23556 10.7225 8.27871 10.6802 8.16472C10.6427 8.06375 10.6509 7.99942 10.7575 7.95383C11.3633 7.69082 11.9634 7.41398 12.5741 7.16237C14.6236 6.31962 16.7016 5.55586 18.8097 4.87189C19.9089 4.51524 21.0269 4.23596 22.1676 4.04786C22.2141 4.04053 22.2556 4.00308 22.2987 3.97947C22.2588 3.94364 22.2214 3.87931 22.179 3.87606C22.0317 3.86547 21.8786 3.85651 21.7337 3.88094C20.5709 4.07962 19.444 4.41509 18.3309 4.79453C16.0632 5.56726 13.8102 6.37906 11.6206 7.35779C11.3275 7.48889 11.0376 7.62894 10.747 7.76574C10.6102 7.83006 10.5589 7.81215 10.5711 7.66233C10.5849 7.50029 10.6183 7.32767 10.6916 7.18517C10.8854 6.80573 11.1981 6.52319 11.5221 6.25448C11.6174 6.1755 11.7135 6.09815 11.8136 6.02649C11.9187 5.9524 11.9895 5.96787 12.0522 6.08105C12.1027 6.17306 12.1401 6.27158 12.1882 6.37825C12.4023 6.31637 12.6116 6.25204 12.8225 6.19504C14.736 5.67799 16.621 5.06975 18.4864 4.39962C19.2852 4.11219 20.097 3.86059 20.9031 3.59188C20.9357 3.5813 20.9674 3.56908 21.0269 3.5471C20.9617 3.50231 20.9161 3.46567 20.8665 3.43799C20.3991 3.17743 19.8861 3.07809 19.3626 3.01783C18.396 2.9071 17.4352 2.99585 16.476 3.11066C16.0217 3.16521 15.5698 3.24175 15.1162 3.30771C15.0951 3.31097 15.0731 3.31097 15.0153 3.31585C15.0885 3.24664 15.1341 3.18883 15.192 3.15056C16.02 2.59931 16.8408 2.03422 17.6828 1.50496C18.3097 1.11167 18.9766 0.786786 19.6817 0.549838C20.2607 0.355232 20.8559 0.238794 21.4674 0.215181C21.4983 0.214367 21.5293 0.215181 21.5594 0.215995C21.5716 0.215995 21.5838 0.223324 21.6254 0.235537C21.3151 0.577523 21.1686 0.945565 21.4527 1.37875C21.1954 1.42516 20.9569 1.46261 20.7215 1.51147C19.8136 1.70119 18.9318 1.97396 18.0874 2.36155C18.0671 2.3705 18.0475 2.38028 18.0272 2.38923C17.9482 2.42099 17.9124 2.48287 17.9506 2.55534C17.9629 2.57814 18.0516 2.5814 18.0931 2.5643C18.9782 2.19707 19.8902 1.91697 20.8266 1.7134C21.706 1.52205 22.5943 1.45203 23.4924 1.5131C23.5511 1.51717 23.6097 1.53183 23.6838 1.54485H23.6822ZM20.8111 0.933351C20.7891 0.895081 20.7696 0.845412 20.737 0.807956C20.6018 0.654877 20.2941 0.696404 20.1605 0.882053C20.0604 1.02129 20.1133 1.17274 20.2778 1.22241C20.3926 1.25661 20.7272 1.1247 20.7834 1.0221C20.7956 1.00012 20.7989 0.972435 20.8111 0.934165V0.933351ZM19.0092 1.59615C19.2217 1.59778 19.3821 1.49111 19.3862 1.34455C19.3894 1.22404 19.3145 1.15157 19.1867 1.1532C19.0116 1.15483 18.8292 1.29569 18.8317 1.4276C18.8333 1.52938 18.9025 1.59534 19.0092 1.59615Z" fill="white"></path>
<path d="M14.5713 8.99404C14.6942 8.94192 14.8131 8.87841 14.9401 8.84014C15.8081 8.57714 16.6769 8.31902 17.5457 8.0609C18.6173 7.74253 19.6889 7.42497 20.7612 7.10742C20.8443 7.08299 20.9298 7.06589 21.0145 7.04553C21.0275 7.05367 21.0397 7.06182 21.0527 7.06996C21.012 7.18151 20.9868 7.30121 20.9281 7.40217C20.6871 7.81826 20.3215 8.10487 19.9144 8.34508C19.526 8.57388 19.0936 8.65775 18.6588 8.72778C17.5352 8.91017 16.4001 8.97042 15.265 9.03149C15.0851 9.04126 14.9043 9.05022 14.7236 9.05673C14.678 9.05836 14.6315 9.05103 14.5859 9.04859C14.5811 9.03068 14.5762 9.01195 14.5713 8.99404Z" fill="white"></path>
<path d="M26.0348 2.38542C26.0103 2.5613 25.9916 2.69484 25.9721 2.83733C25.9029 2.83733 25.8092 2.83733 25.7148 2.83733C24.8696 2.83082 24.0374 2.9212 23.2289 3.18013C23.1075 3.21922 23.0782 3.19805 23.1027 3.07591C23.189 2.64273 23.3469 2.24049 23.6384 1.90094C23.8249 1.68435 24.0618 1.60456 24.3468 1.63224C25.0113 1.69738 25.5381 2.05321 26.0356 2.38542H26.0348Z" fill="white"></path>
<path d="M11.0518 5.29969C11.0364 5.15149 11.1121 5.08961 11.1992 5.0318C11.5892 4.77042 12.0135 4.57908 12.4515 4.41623C13.1811 4.14427 13.9123 3.87556 14.6793 3.72574C14.8145 3.69968 14.9586 3.70131 15.0962 3.71678C15.1491 3.72248 15.2297 3.79821 15.233 3.84543C15.2354 3.89348 15.1671 3.97409 15.1133 3.992C14.9562 4.04574 14.7909 4.0775 14.628 4.11332C13.4856 4.36574 12.397 4.7802 11.3108 5.20279C11.2301 5.23455 11.1487 5.26386 11.0535 5.3005L11.0518 5.29969Z" fill="white"></path>
<path d="M25.8625 3.93866C25.8104 3.98833 25.7639 4.04695 25.7045 4.08604C25.1809 4.42558 24.6566 4.76431 24.1273 5.09571C23.9873 5.18365 23.927 5.16166 23.8374 5.01754C23.778 4.92065 23.7275 4.81724 23.6835 4.7122C23.651 4.63321 23.6705 4.57133 23.7666 4.54039C24.3944 4.33927 25.0205 4.13326 25.6483 3.93214C25.711 3.91179 25.781 3.91342 25.8478 3.90527C25.8527 3.91586 25.8576 3.92726 25.8625 3.93784V3.93866Z" fill="white"></path>
<path d="M9.18325 8.61365C9.20849 8.76021 9.22966 8.88724 9.25246 9.01914C8.8722 9.15187 8.50416 9.28133 8.1353 9.40836C8.05307 9.43686 8.0034 9.40754 7.98223 9.31716C7.94721 9.16897 7.98467 9.0224 8.10843 8.9768C8.45775 8.84978 8.81276 8.73741 9.18325 8.61365Z" fill="white"></path>
<path d="M9.56532 7.59131C9.57754 7.82093 9.5181 7.91782 9.33489 7.98296C9.04909 8.08393 8.7641 8.18571 8.47748 8.28424C8.36512 8.32251 8.32522 8.28831 8.33173 8.1678C8.33825 8.05299 8.40013 7.97726 8.50435 7.94225C8.85367 7.82418 9.2038 7.711 9.56451 7.59131H9.56532Z" fill="white"></path>
<path d="M22.1045 0.752621C22.0247 0.721679 21.9075 0.712723 21.87 0.653282C21.8065 0.553129 21.8676 0.43262 21.9531 0.360966C22.0223 0.303154 22.1208 0.251042 22.2079 0.248599C22.3545 0.243714 22.5051 0.265698 22.6476 0.303968C22.7103 0.320253 22.7844 0.396793 22.7975 0.457862C22.8056 0.49776 22.7234 0.586514 22.6647 0.606056C22.488 0.66631 22.3032 0.702952 22.1053 0.753435L22.1045 0.752621Z" fill="white"></path>
<path d="M9.76562 8.18845C9.80064 8.02397 9.81448 7.90998 9.85031 7.8025C9.88532 7.69583 9.96593 7.67873 10.0376 7.76504C10.0848 7.82204 10.1117 7.90265 10.1272 7.97675C10.1671 8.17787 10.1964 8.38143 10.2297 8.58418C10.2338 8.61023 10.2371 8.63629 10.2411 8.66235C10.2534 8.74377 10.2387 8.82927 10.1508 8.83822C10.0913 8.84474 9.98792 8.80728 9.96919 8.7625C9.88451 8.56382 9.82262 8.35537 9.76562 8.18845Z" fill="white"></path>
<path d="M26.6685 3.09762C26.4315 3.07563 26.377 3.03899 26.3811 2.88184C26.3835 2.79797 26.4218 2.72632 26.5122 2.73283C26.6555 2.74423 26.7988 2.76948 26.9396 2.7996C26.9795 2.80856 27.0398 2.8566 27.0398 2.88673C27.0398 2.92825 27.0007 2.99095 26.9624 3.00805C26.8696 3.04876 26.767 3.06912 26.6677 3.09762H26.6685Z" fill="white"></path>
<path d="M23.2078 1.14955C23.2412 1.03555 23.2657 0.928887 23.3039 0.828734C23.3365 0.744867 23.505 0.717182 23.6101 0.765223C23.72 0.815706 23.6752 0.892246 23.6459 0.964714C23.5661 1.16502 23.5018 1.19433 23.207 1.14955H23.2078Z" fill="white"></path>
<path d="M20.8109 0.934089C20.7995 0.973173 20.7954 1.00004 20.7832 1.02203C20.727 1.12544 20.3923 1.25653 20.2775 1.22233C20.1122 1.17266 20.0601 1.02203 20.1603 0.881977C20.293 0.695514 20.6016 0.654801 20.7368 0.80788C20.7701 0.845336 20.7889 0.895819 20.8109 0.933275V0.934089Z" fill="#CA3624"></path>
<path d="M19.0076 1.5963C18.9009 1.5963 18.8317 1.52953 18.8301 1.42775C18.8277 1.29584 19.0101 1.15579 19.1851 1.15335C19.3138 1.15172 19.3887 1.22337 19.3846 1.3447C19.3805 1.49045 19.2201 1.59711 19.0076 1.5963Z" fill="#CA3624"></path>
</svg></p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VitePress 1.0 (111 pts)]]></title>
            <link>https://blog.vuejs.org/posts/vitepress-1.0</link>
            <guid>39780284</guid>
            <pubDate>Thu, 21 Mar 2024 15:58:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.vuejs.org/posts/vitepress-1.0">https://blog.vuejs.org/posts/vitepress-1.0</a>, See on <a href="https://news.ycombinator.com/item?id=39780284">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://vitepress.dev/vitepress-logo-large.webp" alt="vitepress logo"></p><p>Today we are happy to announce the (long overdue) 1.0 release of <a href="https://vitepress.dev/" target="_blank" rel="noreferrer">VitePress</a>!</p><p>VitePress is a Static Site Generator (SSG) designed for building fast, content-centric websites. In a nutshell, VitePress takes your source content written in Markdown, applies a theme to it, and generates static HTML pages that can be easily deployed anywhere.</p><p>VitePress is built on top of <a href="https://vitejs.dev/" target="_blank" rel="noreferrer">Vite</a> and <a href="https://vuejs.org/" target="_blank" rel="noreferrer">Vue</a>, and is the spiritual successor and modern replacement of <a href="https://vuepress.vuejs.org/" target="_blank" rel="noreferrer">VuePress</a>.</p><hr><h2 id="what-can-i-use-vitepress-for" tabindex="-1">What can I use VitePress for? <a href="#what-can-i-use-vitepress-for" aria-label="Permalink to &quot;What can I use VitePress for?&quot;">​</a></h2><ul><li><p><strong>Documentation</strong></p><p>VitePress ships with a default theme designed for technical documentation. It powers the documentation for <a href="https://vitejs.dev/" target="_blank" rel="noreferrer">Vite</a>, <a href="https://rollupjs.org/" target="_blank" rel="noreferrer">Rollup</a>, <a href="https://pinia.vuejs.org/" target="_blank" rel="noreferrer">Pinia</a>, <a href="https://vueuse.org/" target="_blank" rel="noreferrer">VueUse</a>, <a href="https://vitest.dev/" target="_blank" rel="noreferrer">Vitest</a>, <a href="https://d3js.org/" target="_blank" rel="noreferrer">D3</a>, <a href="https://unocss.dev/" target="_blank" rel="noreferrer">UnoCSS</a>, <a href="https://iconify.design/" target="_blank" rel="noreferrer">Iconify</a> and many more.</p><p>The <a href="https://vuejs.org/" target="_blank" rel="noreferrer">official Vue.js documentation</a> is also based on VitePress, but uses a custom theme shared between multiple translations.</p></li><li><p><strong>Blogs, Portfolios, and Marketing Sites</strong></p><p>VitePress supports <a href="https://vitepress.dev/guide/custom-theme" target="_blank" rel="noreferrer">fully customized themes</a>, with the developer experience of a standard Vite + Vue application. Being built on Vite also means you can directly leverage Vite plugins from its rich ecosystem. In addition, VitePress provides flexible APIs to <a href="https://vitepress.dev/guide/data-loading" target="_blank" rel="noreferrer">load data</a> (local or remote) and <a href="https://vitepress.dev/guide/routing#dynamic-routes" target="_blank" rel="noreferrer">dynamically generate routes</a> at build time. You can use it to build almost anything as long as the data can be determined at build time.</p><p><a href="https://github.com/vuejs/blog" target="_blank" rel="noreferrer">This very blog</a> is built with VitePress' custom theme and data loading API.</p></li></ul><h2 id="developer-experience" tabindex="-1">Developer experience <a href="#developer-experience" aria-label="Permalink to &quot;Developer experience&quot;">​</a></h2><p>VitePress aims to provide a great Developer Experience (DX) when working with Markdown content.</p><ul><li><p><strong><a href="https://vitejs.dev/" target="_blank" rel="noreferrer">Vite-Powered:</a></strong> instant server start, with edits always instantly reflected (&lt;100ms) without page reload.</p></li><li><p><strong><a href="https://vitepress.dev/guide/markdown" target="_blank" rel="noreferrer">Built-in Markdown Extensions:</a></strong> Frontmatter, tables, syntax highlighting... you name it. Specifically, VitePress provides many advanced features for working with code blocks, making it ideal for highly technical documentation.</p></li><li><p><strong><a href="https://vitepress.dev/guide/using-vue" target="_blank" rel="noreferrer">Vue-Enhanced Markdown:</a></strong> each Markdown page is also a Vue <a href="https://vuejs.org/guide/scaling-up/sfc.html" target="_blank" rel="noreferrer">Single-File Component</a>, thanks to Vue template's 100% syntax compatibility with HTML. You can embed interactivity in your static content using Vue templating features or imported Vue components.</p></li></ul><h2 id="performance" tabindex="-1">Performance <a href="#performance" aria-label="Permalink to &quot;Performance&quot;">​</a></h2><p>Unlike many traditional SSGs where each navigation results in a full page reload, a website generated by VitePress serves static HTML on the initial visit, but becomes a <a href="https://en.wikipedia.org/wiki/Single-page_application" target="_blank" rel="noreferrer">Single Page Application</a> (SPA) for subsequent navigation within the site. This model, in our opinion, provides an optimal balance for performance:</p><ul><li><p><strong>Fast Initial Load</strong></p><p>The initial visit to any page will be served the static, pre-rendered HTML for fast loading speed and optimal SEO. The page then loads a JavaScript bundle that turns the page into a Vue SPA ("hydration"). Contrary to common assumptions of SPA hydration being slow, this process is actually extremely fast thanks to Vue 3's raw performance and compiler optimizations. On <a href="https://pagespeed.web.dev/report?url=https%3A%2F%2Fvitepress.dev%2F" target="_blank" rel="noreferrer">PageSpeed Insights</a>, typical VitePress sites achieve near-perfect performance scores even on low-end mobile devices with a slow network.</p></li><li><p><strong>Fast Post-load Navigation</strong></p><p>More importantly, the SPA model leads to better user experience <strong>after</strong> the initial load. Subsequent navigation within the site will no longer cause a full page reload. Instead, the incoming page's content will be fetched and dynamically updated. VitePress also automatically pre-fetches page chunks for links that are within viewport. In most cases, post-load navigation will feel instant.</p></li><li><p><strong>Interactivity Without Penalty</strong></p><p>To be able to hydrate the dynamic Vue parts embedded inside static Markdown, each Markdown page is processed as a Vue component and compiled into JavaScript. This may sound inefficient, but the Vue compiler is smart enough to separate the static and dynamic parts, minimizing both the hydration cost and payload size. For the initial page load, the static parts are automatically eliminated from the JavaScript payload and skipped during hydration.</p></li></ul><hr><p>The path to VitePress 1.0 wouldn't have been possible without the hard work of <a href="https://github.com/vuejs/vitepress/graphs/contributors" target="_blank" rel="noreferrer">our contributors</a>. In particular, shout out to <a href="https://github.com/kiaking" target="_blank" rel="noreferrer">Kia King Ishii (@kiaking)</a> for creating the beautiful design for the default theme, and <a href="https://github.com/brc-dd" target="_blank" rel="noreferrer">Divyansh Singh (@brc-dd)</a> for tirelessly leading the maintenance and pushing new features.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ragas – Open-source library for evaluating RAG pipelines (103 pts)]]></title>
            <link>https://github.com/explodinggradients/ragas</link>
            <guid>39780114</guid>
            <pubDate>Thu, 21 Mar 2024 15:48:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/explodinggradients/ragas">https://github.com/explodinggradients/ragas</a>, See on <a href="https://news.ycombinator.com/item?id=39780114">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/explodinggradients/ragas/blob/main/docs/_static/imgs/logo.png"><img height="200" src="https://github.com/explodinggradients/ragas/raw/main/docs/_static/imgs/logo.png"></a>
</h2><a id="user-content---" aria-label="Permalink: " href="#--"></a></div>
<p dir="auto">
  <i>Evaluation framework for your Retrieval Augmented Generation (RAG) pipelines</i>
</p>
<p dir="auto">
    <a href="https://github.com/explodinggradients/ragas/releases">
        <img alt="GitHub release" src="https://camo.githubusercontent.com/b0363594278d3be7fa131530e2f3ae7dd1b9eecd1c493cb6431bc414d092cc5d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6578706c6f64696e676772616469656e74732f72616761732e737667" data-canonical-src="https://img.shields.io/github/release/explodinggradients/ragas.svg">
    </a>
    <a href="https://www.python.org/" rel="nofollow">
            <img alt="Build" src="https://camo.githubusercontent.com/739db8b875292148bfeb7ac6f277e83e07251132d00a177c85d25f4be780adec/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d616465253230776974682d507974686f6e2d3166343235662e7376673f636f6c6f723d707572706c65" data-canonical-src="https://img.shields.io/badge/Made%20with-Python-1f425f.svg?color=purple">
    </a>
    <a href="https://github.com/explodinggradients/ragas/blob/master/LICENSE">
        <img alt="License" src="https://camo.githubusercontent.com/c1e2e7f623faa7ddd4d181a7d599ceff928611e88069814c00916ad832a25033/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6578706c6f64696e676772616469656e74732f72616761732e7376673f636f6c6f723d677265656e" data-canonical-src="https://img.shields.io/github/license/explodinggradients/ragas.svg?color=green">
    </a>
    <a href="https://colab.research.google.com/github/explodinggradients/ragas/blob/main/docs/quickstart.ipynb" rel="nofollow">
        <img alt="Open In Colab" src="https://camo.githubusercontent.com/f5e0d0538a9c2972b5d413e0ace04cecd8efd828d133133933dfffec282a4e1b/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg">
    </a>
    <a href="https://discord.gg/5djav8GGNZ" rel="nofollow">
        <img alt="discord-invite" src="https://camo.githubusercontent.com/87c4f083b2dee938b2ad6b076306bd4ef2329640b8e3aaf2ecc4d91dc27718af/68747470733a2f2f646362616467652e76657263656c2e6170702f6170692f7365727665722f35646a61763847474e5a3f7374796c653d666c6174" data-canonical-src="https://dcbadge.vercel.app/api/server/5djav8GGNZ?style=flat">
    </a>
    <a href="https://github.com/explodinggradients/ragas/">
        <img alt="Downloads" src="https://camo.githubusercontent.com/196e8b088932bdaf1b820ca7ab78e04307999f5bd44130bbb392053d8d07a9c8/68747470733a2f2f6261646765732e66726170736f66742e636f6d2f6f732f76312f6f70656e2d736f757263652e7376673f763d313033" data-canonical-src="https://badges.frapsoft.com/os/v1/open-source.svg?v=103">
    </a>
</p>

<blockquote>
<p dir="auto">🚀 Dedicated solutions to evaluate, monitor and improve performance of LLM &amp; RAG application in production including custom models for production quality monitoring.<a href="https://calendly.com/shahules/30min" rel="nofollow">Talk to founders</a></p>
</blockquote>
<p dir="auto">Ragas is a framework that helps you evaluate your Retrieval Augmented Generation (RAG) pipelines. RAG denotes a class of LLM applications that use external data to augment the LLM’s context. There are existing tools and frameworks that help you build these pipelines but evaluating it and quantifying your pipeline performance can be hard. This is where Ragas (RAG Assessment) comes in.</p>
<p dir="auto">Ragas provides you with the tools based on the latest research for evaluating LLM-generated text to give you insights about your RAG pipeline. Ragas can be integrated with your CI/CD to provide continuous checks to ensure performance.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛡️ Installation</h2><a id="user-content-shield-installation" aria-label="Permalink: :shield: Installation" href="#shield-installation"></a></p>

<p dir="auto">if you want to install from source</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/explodinggradients/ragas &amp;&amp; cd ragas
pip install -e ."><pre>git clone https://github.com/explodinggradients/ragas <span>&amp;&amp;</span> <span>cd</span> ragas
pip install -e <span>.</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔥 Quickstart</h2><a id="user-content-fire-quickstart" aria-label="Permalink: :fire: Quickstart" href="#fire-quickstart"></a></p>
<p dir="auto">This is a small example program you can run to see ragas in action!</p>
<div dir="auto" data-snippet-clipboard-copy-content="
from datasets import Dataset 
import os
from ragas import evaluate
from ragas.metrics import faithfulness, answer_correctness

os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;

data_samples = {
    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],
    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],
    'contexts' : [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'], 
    ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],
    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']
}

dataset = Dataset.from_dict(data_samples)

score = evaluate(dataset,metrics=[faithfulness,answer_correctness])
score.to_pandas()"><pre><span>from</span> <span>datasets</span> <span>import</span> <span>Dataset</span> 
<span>import</span> <span>os</span>
<span>from</span> <span>ragas</span> <span>import</span> <span>evaluate</span>
<span>from</span> <span>ragas</span>.<span>metrics</span> <span>import</span> <span>faithfulness</span>, <span>answer_correctness</span>

<span>os</span>.<span>environ</span>[<span>"OPENAI_API_KEY"</span>] <span>=</span> <span>"your-openai-key"</span>

<span>data_samples</span> <span>=</span> {
    <span>'question'</span>: [<span>'When was the first super bowl?'</span>, <span>'Who won the most super bowls?'</span>],
    <span>'answer'</span>: [<span>'The first superbowl was held on Jan 15, 1967'</span>, <span>'The most super bowls have been won by The New England Patriots'</span>],
    <span>'contexts'</span> : [[<span>'The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'</span>], 
    [<span>'The Green Bay Packers...Green Bay, Wisconsin.'</span>,<span>'The Packers compete...Football Conference'</span>]],
    <span>'ground_truth'</span>: [<span>'The first superbowl was held on January 15, 1967'</span>, <span>'The New England Patriots have won the Super Bowl a record six times'</span>]
}

<span>dataset</span> <span>=</span> <span>Dataset</span>.<span>from_dict</span>(<span>data_samples</span>)

<span>score</span> <span>=</span> <span>evaluate</span>(<span>dataset</span>,<span>metrics</span><span>=</span>[<span>faithfulness</span>,<span>answer_correctness</span>])
<span>score</span>.<span>to_pandas</span>()</pre></div>
<p dir="auto">Refer to our <a href="https://docs.ragas.io/" rel="nofollow">documentation</a> to learn more.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🫂 Community</h2><a id="user-content--community" aria-label="Permalink: 🫂 Community" href="#-community"></a></p>
<p dir="auto">If you want to get more involved with Ragas, check out our <a href="https://discord.gg/5djav8GGNZ" rel="nofollow">discord server</a>. It's a fun community where we geek out about LLM, Retrieval, Production issues, and more.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔍 Open Analytics</h2><a id="user-content--open-analytics" aria-label="Permalink: 🔍 Open Analytics" href="#-open-analytics"></a></p>
<p dir="auto">We track very basic usage metrics to guide us to figure out what our users want, what is working, and what's not. As a young startup, we have to be brutally honest about this which is why we are tracking these metrics. But as an Open Startup, we open-source all the data we collect. You can read more about this <a href="https://github.com/explodinggradients/ragas/issues/49" data-hovercard-type="issue" data-hovercard-url="/explodinggradients/ragas/issues/49/hovercard">here</a>. <strong>Ragas does not track any information that can be used to identify you or your company</strong>. You can take a look at exactly what we track in the <a href="https://github.com/explodinggradients/ragas/blob/main/src/ragas/_analytics.py">code</a></p>
<p dir="auto">To disable usage-tracking you set the <code>RAGAS_DO_NOT_TRACK</code> flag to true.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hackers found a way to open any of 3M hotel keycard locks in seconds (274 pts)]]></title>
            <link>https://www.wired.com/story/saflok-hotel-lock-unsaflok-hack-technique/</link>
            <guid>39779291</guid>
            <pubDate>Thu, 21 Mar 2024 14:57:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/saflok-hotel-lock-unsaflok-hack-technique/">https://www.wired.com/story/saflok-hotel-lock-unsaflok-hack-technique/</a>, See on <a href="https://news.ycombinator.com/item?id=39779291">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>When thousands of security researchers descend on Las Vegas every August for what's come to be known as “hacker summer camp,” the back-to-back <a href="https://www.wired.com/tag/black-hat/">Black Hat</a> and <a href="https://www.wired.com/tag/defcon/">Defcon</a> hacker conferences, it's a given that some of them will experiment with hacking the infrastructure of Vegas itself, the city's elaborate array of <a href="https://www.wired.com/story/card-shuffler-hack/">casino</a> and <a href="https://www.wired.com/story/elevator-phone-phreaking-defcon/">hospitality</a> technology. But at one private event in 2022, a select group of researchers were actually <em>invited</em> to hack a Vegas hotel room, competing in a suite crowded with their laptops and cans of Red Bull to find digital vulnerabilities in every one of the room's gadgets, from its TV to its bedside VoIP phone.</p><p>One team of hackers spent those days focused on the lock on the room's door, perhaps its most sensitive piece of technology of all. Now, more than a year and a half later, they're finally bringing to light the results of that work: a technique they discovered that would allow an intruder to open any of millions of hotel rooms worldwide in seconds, with just two taps.</p><p>Today, Ian Carroll, Lennert Wouters, and a team of other security researchers are revealing a hotel keycard hacking technique they call <a data-offer-url="https://unsaflok.com/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://unsaflok.com/&quot;}" href="https://unsaflok.com/" rel="noopener" target="_blank">Unsaflok</a>. The technique is a collection of security vulnerabilities that would allow a hacker to almost instantly open several models of Saflok-brand RFID-based keycard locks sold by the Swiss lock maker Dormakaba. The Saflok systems are installed on 3 million doors worldwide, inside 13,000 properties in 131 countries.</p><p>By exploiting weaknesses in both Dormakaba's encryption and the underlying RFID system Dormakaba uses, known as MIFARE Classic, Carroll and Wouters have demonstrated just how easily they can open a Saflok keycard lock. Their technique starts with obtaining any keycard from a target hotel—say, by booking a room there or grabbing a keycard out of a box of used ones—then reading a certain code from that card with a $300 RFID read-write device, and finally writing two keycards of their own. When they merely tap those two cards on a lock, the first rewrites a certain piece of the lock's data, and the second opens it.</p><p>“Two quick taps and we open the door,” says Wouters, a researcher in the Computer Security and Industrial Cryptography group at the KU Leuven University in Belgium. “And that works on every door in the hotel.”</p><figure><p><span>A video of the researchers demonstrating their lock-hacking technique. (The pattern of lights shown on the lock is redacted at one point at the researchers’ request to avoid revealing a detail of their technique they agreed with Dormakaba not to make public.)</span><span>Video: Ian Carroll</span></p></figure><p>Wouters and Carroll, an independent security researcher and founder of travel website Seats.aero, shared the full technical details of their hacking technique with Dormakaba in November 2022. Dormakaba says that it's been working since early last year to make hotels that use Saflok aware of their security flaws and to help them fix or replace the vulnerable locks. For many of the Saflok systems sold in the last eight years, there's no hardware replacement necessary for each individual lock. Instead, hotels will only need to update or replace the front desk management system and have a technician carry out a relatively quick reprogramming of each lock, door by door.</p><p>Wouters and Carroll say they were nonetheless told by Dormakaba that, as of this month, only 36 percent of installed Safloks have been updated. Given that the locks aren't connected to the internet and some older locks will still need a hardware upgrade, they say the full fix will still likely take months longer to roll out, at the very least. Some older installations may take years.</p><p>“We have worked closely with our partners to identify and implement an immediate mitigation for this vulnerability, along with a longer-term solution,” Dormakaba wrote to WIRED in a statement, though it declined to detail what that “immediate mitigation” might be. “Our customers and partners all take security very seriously, and we are confident all reasonable steps will be taken to address this matter in a responsible way.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The technique to hack Dormakaba's locks that Wouters and Carroll's research group discovered involves two distinct kinds of vulnerabilities: One that allows them to write to its keycards, and one that allows them to know <em>what</em> data to write to the cards to successfully trick a Saflok lock into opening. When they analyzed Saflok keycards, they saw that they use the MIFARE Classic RFID system, which has been known for more than a decade to have vulnerabilities that allow hackers to write to keycards, though the brute-force process can take as long as 20 seconds. They then cracked a part of Dormakaba's own encryption system, its so-called key derivation function, which allowed them to write to its cards far faster. With either of those tricks, the researchers could then copy a Saflok keycard at will, but still not generate one for a different room.</p><p>The researchers' more crucial step required them to obtain one of the lock programming devices that Dormakaba distributes to hotels, as well as a copy of its front desk software for managing keycards. By reverse engineering that software, they were able to understand all the data stored on the cards, pulling out a hotel property code as well as a code for each individual room, then create their own values and encrypt them just as Dormakaba's system would, allowing them to spoof a working master key that opens any room on the property. “You can make a card that really looks as if it was created by the software from Dormakaba, essentially,” says Wouters.</p><p>And how did Carroll and Wouters obtain Dormakaba's front desk software? “We nicely asked a few people,” Wouters says. “Manufacturers assume that no one will sell their equipment on eBay, and that no one will make a copy of their software, and those assumptions, I think everyone knows, are not really valid.”</p><p>Once they'd managed all that reverse engineering work, the final version of their attack could be pulled off with little more than a $300 Proxmark RFID read-write device and a couple of blank RFID cards, an Android phone, or a <a href="https://www.wired.com/story/what-is-flipper-zero-tiktok/">Flipper Zero radio hacking tool</a>.</p><figure><p><span><p>A Saflok branded lock.</p>
</span><span>Photograph: Dormakaba</span></p></figure></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The biggest caveat to the hackers' Unsaflok technique is that it still requires that they have a keycard—even an expired one—for a room somewhere in the same hotel as the room they're targeting. That's because each card has a property-specific code they need to read and then duplicate on their spoofed card, as well as a room-specific one.</p><p>Once they have that property code, the technique also requires using an RFID read-write device to write two cards—one card that reprograms a target lock as well as the second spoofed card that unlocks it. (An Android phone or a Flipper Zero could also be used to emit one signal after another instead of the two cards, the researchers say.) The researchers hint that the first card allows them to open a target room without guessing its unique identifier in the hotel's system, but declined to say exactly what that first card does. They're holding that element of the technique in confidence to avoid giving too clear a set of instructions to would-be intruders or thieves.</p><p>By contrast, one security researcher presented a similar hotel keycard hack that <a href="https://www.forbes.com/sites/andygreenberg/2012/07/23/hacker-will-expose-potential-security-flaw-in-more-than-four-million-hotel-room-keycard-locks/?sh=78d51b9eeb85">opened locks sold by the firm Onity</a> at the Black Hat conference in 2012 with no such obfuscation, and allowed any hacker to build a device that opened any of Onity's 10 million locks worldwide. When Onity refused to pay for the hardware upgrades necessary to solve the problem and instead put the onus on its customers, the issue remained unfixed in many hotels—and eventually was exploited in at least <a href="https://www.wired.com/2017/08/the-hotel-hacker/">one hacker's cross-country burglary spree</a>.</p><p>Carroll and Wouters say that they're trying to avoid that scenario by taking a more cautious approach, while still warning the public about their technique, given that hundreds of properties will likely remain vulnerable to it even now that Dormakaba has offered its fix. “We're trying to find the middle ground of helping Dormakaba to fix it quickly, but also telling the guests about it," says Carroll. “If someone else reverse engineers this today and starts exploiting it before people are aware, that might be an even bigger problem.”</p><p>To that end, Carroll and Wouters point out that hotel guests can recognize the vulnerable locks most often—but not always—by their distinct design: a round RFID reader with a wavy line cutting through it. They suggest that if hotel guests do have a Saflok on their door, they can determine if it's been updated by checking their keycard with the NFC Taginfo app by NXP, available for <a data-offer-url="https://apps.apple.com/us/app/nfc-taginfo-by-nxp/id1246143596" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://apps.apple.com/us/app/nfc-taginfo-by-nxp/id1246143596&quot;}" href="https://apps.apple.com/us/app/nfc-taginfo-by-nxp/id1246143596" rel="noopener" target="_blank">iOS</a> or <a href="https://play.google.com/store/apps/details?id=com.nxp.taginfolite&amp;hl=en_US&amp;gl=US">Android</a>. If the lock is manufactured by Dormakaba, and that app shows that the keycard is still a MIFARE Classic card, it's likely still vulnerable.</p><p>If that's the case, the two researchers say, there's not much to do other than avoid leaving valuables in the room and, when you're inside, bolt the chain on the door. They warn that the deadbolt on the room is also controlled by the keycard lock, so it doesn't provide an extra safeguard. “If someone locks the deadbolt, they’re still not protected,” says Carroll.</p><p>Even without a perfect or fully implemented fix, Wouters and Carroll argue, it's better for hotel guests to know the risks than to have a false sense of security. After all, they point out, the Saflok brand has been sold for more than three decades, and may have been vulnerable for much or all of those years. Though Dormakaba says it's not aware of any past use of Wouters and Carroll's technique, the researchers point out that doesn't mean it never happened in secret.</p><p>“We think the vulnerability has been there for a long time,” says Wouters. “It's unlikely that we are the first to find this.”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GoFetch: New side-channel attack using data memory-dependent prefetchers (210 pts)]]></title>
            <link>https://gofetch.fail</link>
            <guid>39779195</guid>
            <pubDate>Thu, 21 Mar 2024 14:51:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gofetch.fail">https://gofetch.fail</a>, See on <a href="https://news.ycombinator.com/item?id=39779195">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<section id="abstract">
				<h4><i></i>Overview of <span>GoFetch Attack</span></h4>
				<div><p>
						GoFetch is a microarchitectural side-channel attack 
						that can extract secret keys from 
						constant-time cryptographic implementations via 
						data memory-dependent prefetchers (DMPs).
						</p><p>
						
						We show that DMPs are present in many Apple CPUs 
						and pose a real threat to multiple cryptographic implementations, 
						allowing us to extract keys from OpenSSL Diffie-Hellman, 
						Go RSA, as well as CRYSTALS Kyber and Dilithium.
					</p></div>
			</section>

			<section id="demos">
				<h4><i></i>Demo <span>Videos.</span></h4>
				<div>
							<h5>Go's RSA-2048 Key Extraction on Apple m1</h5>
							
						</div>
			</section>

			<section id="people">
				<h4><i></i>People
					<span>Behind GoFetch</span>
				</h4>
				<div>
						<ul>
							<li><a href="https://bluechen8.github.io/">Boru Chen </a><span><a href="https://illinois.edu/">University of Illinois Urbana-Champaign</a></span></li>
							<li><a href="https://www.cs.utexas.edu/~yingchen/">Yingchen Wang</a> <span><a href="https://www.utexas.edu/">University of Texas at Austin</a></span></li>
							<li><a href="https://pradyumnashome.com/">Pradyumna Shome</a> <span><a href="https://www.gatech.edu/">Georgia Institute of Technology</a></span></li>
							<li><a href="https://cwfletcher.github.io/">Christopher W. Fletcher</a> <span><a href="https://www.berkeley.edu/">University of California, Berkeley</a></span></li>
							<li><a href="https://homes.cs.washington.edu/~dkohlbre/">David Kohlbrenner</a> <span><a href="https://www.washington.edu/">University of Washington</a></span></li>
							<li><a href="https://www.cs.cmu.edu/~rpaccagn/">Riccardo Paccagnella</a> <span><a href="https://www.cmu.edu/">Carnegie Mellon University</a></span></li>
							<li><a href="https://faculty.cc.gatech.edu/~genkin/">Daniel Genkin</a> <span><a href="https://www.gatech.edu/">Georgia Institute of Technology</a></span></li>
						</ul>
					</div>
			</section>

			<section id="qa">
				<h4><i></i>Frequently Asked <span>Questions</span></h4>

				<div id="all-question-answer">
					<div id="panelsStayOpen-collapseAffected" aria-labelledby="panelsStayOpen-headingAffected">
								<p>
									The GoFetch attack is based on a CPU feature 
									called data memory-dependent prefetcher (DMP), 
									which is present in the latest Apple processors. 
									We reverse-engineered DMPs on Apple m-series CPUs 
									and found that the DMP activates (and attempts to dereference) 
									data loaded from memory that "looks like" a pointer. 
									This explicitly violates a requirement 
									of the constant-time programming paradigm, 
									which forbids mixing data and memory access patterns. 
								</p>
								<p>
									To exploit the DMP, we craft chosen inputs to cryptographic operations, 
									in a way where pointer-like values only appear 
									if we have correctly guessed some bits of the secret key. 
									We verify these guesses by monitoring whether the DMP performs 
									a dereference through cache-timing analysis. 
									Once we make a correct guess, we proceed to guess the next batch of key bits. 
									Using this approach, we show end-to-end key extraction attacks on popular 
									constant-time implementations of classical 
									(OpenSSL Diffie-Hellman Key Exchange, Go RSA decryption) 
									and post-quantum cryptography (CRYSTALS-Kyber and CRYSTALS-Dilithium).
								</p>
							</div>
					<div id="panelsStayOpen-collapseDifference" aria-labelledby="panelsStayOpen-headingDifference">
							<p>
								We have mounted end-to-end GoFetch attacks on Apple hardware equipped with m1 processors. 
								We also tested DMP activation patterns on other Apple processors 
								and found that m2 and m3 CPUs also exhibit similar exploitable DMP behavior. 
								While we have not tested other m-series variants (e.g., m2 Pro, etc), 
								we hypothesize that since these parts have the same microarchitecture as their simpler counterparts, 
								they are likewise equipped with exploitable DMPs. 
								Finally, we found that Intel's 13th Gen Raptor Lake microarchitecture also features a DMP. 
								However, its activation criteria are more restrictive, making it robust to our attacks.
							</p>
						</div>
					<div id="panelsStayOpen-collapseCacheAttack" aria-labelledby="panelsStayOpen-headingCacheAttack">
								<p>
									The Apple m-series DMP was first discovered by <a href="https://www.prefetchers.info/">Augury</a>, 
									which suggested that DMPs might mix data and addresses under some conditions. 
									However, we found that the DMP activation criteria outlined by Augury are overly restrictive. 
									This prevents Augury's findings from being sufficient to mount attacks on real-world constant-time cryptography. 
								</p>
								<p>
									GoFetch shows that the DMP is significantly more aggressive than previously thought, 
									and thus poses a much greater security risk. 
									Specifically, we find that any value loaded from memory is a candidate for being dereferenced (literally!). 
									This allows us to sidestep many of Augury's limitations and demonstrate end-to-end attacks on real constant-time code.
								</p>
							</div>
					<div id="panelsStayOpen-collapseConstantTime" aria-labelledby="panelsStayOpen-headingConstantTime">
							<p>
								Modern processors use caches to reduce a program's memory access latency. 
								If data has been accessed before, it gets cached, which makes subsequent accesses to it faster. 
								Since the cache is shared by processes running on the same machine, 
								attackers co-located to the same machine can monitor the cache's state to deduce a victim's access pattern.
							</p>
						</div>
					<div id="panelsStayOpen-collapseDMP" aria-labelledby="panelsStayOpen-headingDMP">
								<p>
									Constant-time programming is a paradigm that aims to harden code against side-channel attacks by ensuring that all operations take the same amount of time, regardless of their operands. In particular, constant-time code cannot contain secret-dependent branches, loops, or other control structures. Moreover, as the CPU caches different addresses with attacker-observable latency, constant-time code cannot mix data and addresses in any way and prohibits the use of secret-dependent memory accesses or array indices.
								</p>
								<p>
									We show that even if a victim correctly separates data from addresses by following the constant-time paradigm, the DMP will generate secret-dependent memory access on the victim's behalf, resulting in variable-time code susceptible to our key-extraction attacks. 
								</p>
							</div>
					<div id="panelsStayOpen-collapseOther" aria-labelledby="panelsStayOpen-headingOther">
							<p>
								Prefetchers are a hardware optimization that predicts memory addresses accessed in the near future and fetch the data into the cache accordingly from the main memory. To make a prediction, classical prefetchers use the address trace of previous demand accesses. This strategy performs poorly when it comes to irregular access patterns like linked-list traversals. Aiming to handle such irregular patterns, data memory-dependent prefetchers (DMPs) also consider the content of memory to determine what to fetch, which is capable of capturing those indirect access patterns. Unfortunately, this behavior inherently mixes data and memory addresses at the hardware level, making the entire compute stack non-constant-time, enabling our attack.
							</p>
						</div>
					<div id="panelsStayOpen-collapseOthers" aria-labelledby="panelsStayOpen-headingOthers">
							<p>
								We don't know. Our attack relies on the fact that it is possible to craft inputs to control specific intermediate states, making them contain memory addresses in a key-dependent way. The DMP then serves as an oracle, allowing us to learn if the intermediate state indeed looks like a pointer and thus leaks secret key bits. Unfortunately, to assess if an implementation is vulnerable, cryptanalysis and code inspection are required to understand when and how intermediate values can be made to look like pointers in a way that leaks secrets. This process is manual and slow and does not rule out other attack approaches. 
							</p>
						</div>
					<div id="panelsStayOpen-collapseDisable" aria-labelledby="panelsStayOpen-headingDisable">
							<p>
								Yes, but only on some processors.
								We observe that the <a href="https://developer.apple.com/documentation/xcode/writing-arm64-code-for-apple-platforms#Enable-DIT-for-constant-time-cryptographic-operations">DIT bit</a>
								set on m3 CPUs effectively disables the DMP.
								This is not the case for the m1 and m2.
								Also, Intel's counterpart, <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/software-security-guidance/best-practices/data-operand-independent-timing-isa-guidance.html">DOIT bit</a>, 
								can be used to disable DMP on the Raptor Lake processors.
							</p>
						</div>
					<div id="panelsStayOpen-collapseCountermeasure" aria-labelledby="panelsStayOpen-headingCountermeasure">
							<p>
								For users, we recommend using the latest versions of software, 
								as well as performing updates regularly. 
								Developers of cryptographic libraries can either set the 
								<a href="https://www.intel.com/content/www/us/en/developer/articles/technical/software-security-guidance/best-practices/data-operand-independent-timing-isa-guidance.html">DOIT bit</a>
								and 
								<a href="https://developer.apple.com/documentation/xcode/writing-arm64-code-for-apple-platforms#Enable-DIT-for-constant-time-cryptographic-operations">DIT bit</a> 
								bits, which disable the DMP on some CPUs. 
								Additionally, input blinding can help some cryptographic schemes avoid having attacker-controlled intermediate values, 
								avoiding key-dependent DMP activation. 
								Finally, preventing attackers from measuring DMP activation in the first place, 
								for example by avoiding hardware sharing, can further enhance the security of cryptographic protocols.
							</p>
						</div>
					<div id="panelsStayOpen-collapsePoC" aria-labelledby="panelsStayOpen-headingPoC">
							<p>
								Yes, we will release it soon.
							</p>
						</div>
					<div id="panelsStayOpen-collapseLogo" aria-labelledby="panelsStayOpen-headingLogo">
							<p>
								Yes, <a href="https://gofetch.fail/img/gofetch.svg">SVG</a> and <a href="https://gofetch.fail/img/gofetch.png">PNG</a> versions 
								of GoFetch logo are free to use under a <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC0</a> liscence.
							</p>
						</div>
					<div id="panelsStayOpen-collapseNotify" aria-labelledby="panelsStayOpen-headingNotify">
							<p>
								We discloused our findings to Apple on December 5, 2023 (107 days before public release).
							</p>
						</div>
				</div>
			</section>

			<section id="news">
				<h4><i></i><span>GoFetch</span> in the News</h4>
				
			</section>

			<section id="acknowledgments">
				<h4><i></i>Acknowledgments</h4>
				<p>
						This work was partially supported by
						the Air Force Office of Scientific Research (AFOSR) under award number FA9550-20-1-0425;
						the Defense Advanced Research Projects Agency (DARPA) under contract
						numbers W912CG-23-C-0022 and HR00112390029;
						the National Science Foundation (NSF) under grant numbers 1954712, 1954521, 2154183, 2153388, and 1942888;
						the Alfred P. Sloan Research Fellowship;
						and gifts from Intel, Qualcomm,
						and Cisco.
					</p>
			</section>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introducing pgzx: create PostgreSQL extensions using Zig (137 pts)]]></title>
            <link>https://xata.io/blog/introducing-pgzx</link>
            <guid>39779024</guid>
            <pubDate>Thu, 21 Mar 2024 14:39:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xata.io/blog/introducing-pgzx">https://xata.io/blog/introducing-pgzx</a>, See on <a href="https://news.ycombinator.com/item?id=39779024">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>As part of the <a href="https://xata.io/launch-week-unleash-the-elephant">Xata launch week</a>, we are introducing <a href="https://github.com/xataio/pgzx">pgzx</a>, an open-source framework for developing PostgreSQL extensions written in Zig. It provides a set of utilities (e.g. error handling, memory allocators, wrappers) as well as a build and development environment to simplify integrating with the Postgres codebase.</p>

<p><a href="https://ziglang.org/">Zig</a> is described on its website as a general-purpose programming language and toolchain for maintaining <strong>robust</strong>, <strong>optimal</strong> and <strong>reusable</strong> software. Zig is a new language (pre 1.0), but it has been gaining popularity in the systems programming community. It is fair to think of it as a "modern C", providing safer memory management, compilation time code execution (comptime), and a rich standard library. We'll show some of these features in the context of Postgres extensions below.</p>
<p>A major reason to use Zig for Postgres development is its ability to interoperate with C code. Zig supports the C ABI, works with C pointers and data types, including NULL terminated strings, and can even translate C header files into Zig code. Zig's automatic translation of C macros to Zig code is not yet perfect, but it's still helpful. This makes Zig a great choice for working with very large C codebases, like Postgres happens to be.</p>

<p>Because Zig can call any C function and translate C macros to inline Zig functions, you can write Postgres extensions in Zig without any additional tooling by following the same steps you would for a C extension. However, a framework like pgzx simplifies the process significantly by providing a development environment, a set of utilities and wrappers for Postgres APIs, common error handling, and more.</p>

<p>pgzx has at the moment 2 sample extensions that you can take inspiration from. <a href="https://github.com/xataio/pgzx/tree/main/examples/char_count_zig">char_count_zig</a> is a minimal extension, while <a href="https://github.com/xataio/pgzx/tree/main/examples/pgaudit_zig">pg_audit_zig</a> is more complex and shows more of the features of pgzx.</p>
<p>Let's look at the <code>char_count_zig</code>, which is only slightly more than a "Hello, World!" of Postgres extensions. It adds a function that counts how many times a character appears in a string. This was inspired by <a href="https://www.highgo.ca/2019/10/01/a-guide-to-create-user-defined-extension-modules-to-postgres/">this tutorial</a>, which shows how to do this in plpqsql and C.</p>
<div role="group"><pre tabindex="0"><code><span><span>select</span><span> char_count_zig(</span><span>'hi hii'</span><span>, </span><span>'i'</span><span>);</span></span>
<span></span>
<span><span> char_count_zig</span></span>
<span><span>----------------</span></span>
<span><span>              3</span></span>
<span><span>(</span><span>1</span><span> row</span><span>)</span></span>
<span></span></code></pre></div>
<p>Here is the <code>char_count</code> extension written in Zig and in C.</p>
<div><div><p>char_count written in Zig</p><div role="group"><pre tabindex="0"><code><span><span>const</span><span> std = </span><span>@import</span><span>(</span><span>"std"</span><span>);</span></span>
<span><span>const</span><span> pgzx = </span><span>@import</span><span>(</span><span>"pgzx"</span><span>);</span></span>
<span></span>
<span><span>comptime</span><span> {</span></span>
<span><span>    pgzx.</span><span>PG_MODULE_MAGIC</span><span>();</span></span>
<span></span>
<span><span>    pgzx.</span><span>PG_FUNCTION_V1</span><span>(</span><span>"char_count_zig"</span><span>, char_count_zig);</span></span>
<span><span>}</span></span>
<span></span>
<span><span>fn</span><span> char_count_zig</span><span>(input_text: []</span><span>const</span><span> u8</span><span>, target_char: []</span><span>const</span><span> u8</span><span>) </span><span>!u32</span><span> {</span></span>
<span><span>    if</span><span> (target_char.len &gt; </span><span>1</span><span>) {</span></span>
<span><span>        return</span><span> pgzx.elog.</span><span>Error</span><span>(</span><span>@src</span><span>(), </span><span>"Target char is more than one byte"</span><span>, .{});</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    pgzx.elog.</span><span>Info</span><span>(</span><span>@src</span><span>(), </span><span>"input_text: {s}\n"</span><span>, .{input_text});</span></span>
<span><span>    pgzx.elog.</span><span>Info</span><span>(</span><span>@src</span><span>(), </span><span>"target_char: {s}\n"</span><span>, .{target_char});</span></span>
<span><span>    pgzx.elog.</span><span>Info</span><span>(</span><span>@src</span><span>(), </span><span>"Target char len: {}\n"</span><span>, .{target_char.len});</span></span>
<span></span>
<span><span>    var</span><span> count: </span><span>u32</span><span> = </span><span>0</span><span>;</span></span>
<span><span>    for</span><span> (input_text) </span><span>|</span><span>char</span><span>|</span><span> {</span></span>
<span><span>        if</span><span> (char </span><span>==</span><span> target_char[</span><span>0</span><span>]) {</span></span>
<span><span>            count </span><span>+=</span><span> 1</span><span>;</span></span>
<span><span>        }</span></span>
<span><span>    }</span></span>
<span><span>    return</span><span> count;</span></span>
<span><span>}</span></span>
<span></span></code></pre></div></div><div><p>char_count written in C</p><div role="group"><pre tabindex="0"><code><span><span>#include</span><span> "postgres.h"</span></span>
<span><span>#include</span><span> "fmgr.h"</span></span>
<span><span>#include</span><span> "utils/builtins.h"</span></span>
<span></span>
<span><span>PG_MODULE_MAGIC;</span></span>
<span></span>
<span><span>PG_FUNCTION_INFO_V1</span><span>(char_count_c);</span></span>
<span></span>
<span><span>Datum</span></span>
<span><span>char_count_c</span><span>(PG_FUNCTION_ARGS)</span></span>
<span><span>{</span></span>
<span><span>    int</span><span> charCount </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>    int</span><span> i </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>    text </span><span>*</span><span> inputText </span><span>=</span><span> PG_GETARG_TEXT_PP(</span><span>0</span><span>)</span><span>;</span></span>
<span><span>    text </span><span>*</span><span> targetChar </span><span>=</span><span> PG_GETARG_TEXT_PP(</span><span>1</span><span>)</span><span>;</span></span>
<span></span>
<span><span>    int</span><span> inputText_sz </span><span>=</span><span> VARSIZE(inputText)</span><span>-</span><span>VARHDRSZ;</span></span>
<span><span>    int</span><span> targetChar_sz </span><span>=</span><span> VARSIZE(targetChar)</span><span>-</span><span>VARHDRSZ;</span></span>
<span><span>    char</span><span> *</span><span> cp_inputText </span><span>=</span><span> NULL</span><span>;</span></span>
<span><span>    char</span><span> *</span><span> cp_targetChar </span><span>=</span><span> NULL</span><span>;</span></span>
<span></span>
<span><span>    if</span><span> (targetChar_sz </span><span>&gt;</span><span> 1</span><span> )</span></span>
<span><span>    {</span></span>
<span><span>        elog(ERROR</span><span>,</span><span> "arg1 must be 1 char long"</span><span>)</span><span>;</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    cp_inputText </span><span>=</span><span> (</span><span>char</span><span> *</span><span>) </span><span>palloc(inputText_sz </span><span>+</span><span> 1</span><span>)</span><span>;</span></span>
<span><span>    cp_targetChar </span><span>=</span><span> (</span><span>char</span><span> *</span><span>) </span><span>palloc(targetChar_sz </span><span>+</span><span> 1</span><span>)</span><span>;</span></span>
<span><span>    memcpy(cp_inputText</span><span>,</span><span> VARDATA(inputText)</span><span>,</span><span> inputText_sz)</span><span>;</span></span>
<span><span>    memcpy(cp_targetChar</span><span>,</span><span> VARDATA(targetChar)</span><span>,</span><span> targetChar_sz)</span><span>;</span></span>
<span></span>
<span><span>    elog(INFO</span><span>,</span><span> "arg0 length is </span><span>%d</span><span>, value </span><span>%s</span><span>"</span><span>,</span><span> (</span><span>int</span><span>)strlen(cp_inputText)</span><span>,</span><span> cp_inputText)</span><span>;</span></span>
<span><span>    elog(INFO</span><span>,</span><span> "arg1 length is </span><span>%d</span><span>, value </span><span>%s</span><span>"</span><span>,</span><span> (</span><span>int</span><span>)strlen(cp_targetChar)</span><span>,</span><span> cp_targetChar)</span><span>;</span></span>
<span></span>
<span><span>    while</span><span> ( i </span><span>&lt;</span><span> strlen(cp_inputText)</span><span> )</span></span>
<span><span>    {</span></span>
<span><span>        if</span><span>( cp_inputText[i] </span><span>==</span><span> cp_targetChar[</span><span>0</span><span>] )</span></span>
<span><span>            charCount</span><span>++</span><span>;</span></span>
<span><span>        i</span><span>++</span><span>;</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    pfree(cp_inputText)</span><span>;</span></span>
<span><span>    pfree(cp_targetChar)</span><span>;</span></span>
<span><span>    PG_RETURN_INT32(charCount)</span><span>;</span></span>
<span><span>}</span></span>
<span></span></code></pre></div></div></div>
<p>Whilst they're similar, the Zig version is actually a bit more concise. This is partially because Zig is more expressive, but also because pgzx does a bit more of the work.</p>
<p>Registered SQL functions receive their arguments serialized and some code is needed to deserialize them. In C you do that with the help of the <code>G_GETARG_*</code> macros, however with pgzx you simply receive them as normal parameters already deserialized. How? By having a <code>comptime</code> function that generates the necessary boilerplate code at compile time. If you are curious, check out the <a href="https://github.com/xataio/pgzx/blob/9825dde752ee4ace2cbc594332eba100292aafd5/src/pgzx/fmgr.zig#L77-L102">pgCall</a> function, it's a nice example for the power of Zig's <code>comptime</code> execution.</p>
<p>The <code>PG_MODULE_MAGIC</code> and <code>PG_FUNCTION_INFO_V1</code> functions are a second example of <code>comptime</code> usage. They export the required symbols needed for Postgres to recognize this as an extension and to register the function as a SQL function. In this case, <code>comptime</code> acts pretty much like the corresponding C macros.</p>

<p>If you looked carefully at the code above, you might have noticed that it contains a bug. It checks for the <code>target_char</code> to not have more than 1 character, but it doesn't check if it has 0 characters. Later, the code accesses <code>target_char[0]</code>, so if the string is the empty string, there will be a out of bounds access error. We left that bug intentionally in so you can see what happens when bugs like that happen in the extension.</p>
<p>You can trigger the bug with this SQL:</p>
<div role="group"><pre tabindex="0"><code><span><span>select</span><span> char_count_zig(</span><span>'hi hii'</span><span>, </span><span>''</span><span>);</span></span>
<span></span></code></pre></div>
<p>Which responds:</p>
<div role="group"><pre tabindex="0"><code><span><span>server closed the connection unexpectedly</span></span>
<span><span>        This probably means the server terminated abnormally</span></span>
<span><span>        before or while processing the request.</span></span>
<span><span>The connection to the server was lost. Attempting reset: Failed.</span></span>
<span><span></span></span></code></pre></div>
<p>In C code, this type of bug could trigger a segmentation fault or even a security vulnerability. If you try this with the <code>char_count_zig</code> extension, the Postgres process still crashes (not the full server, just the process serving the connection), but if you check the logs you'll see an error message like this:</p>
<div role="group"><pre tabindex="0"><code><span><span>thread 70501513 panic: index out of bounds: index 0, len 0</span></span>
<span><span>/Users/tsg/src/xataio/pgzx/examples/char_count_zig/src/main.zig:21:32: 0x103aaedff in char_count_zig (char_count_zig)</span></span>
<span><span>        if (char == target_char[0]) {</span></span>
<span><span>                               ^</span></span>
<span><span>/Users/tsg/src/xataio/pgzx/src/pgzx/fmgr.zig:95:5: 0x103aaf20f in call (char_count_zig)</span></span>
<span><span>    const value = @call(.no_async, impl, callArgs) catch |e| elog.throwAsPostgresError(src, e);</span></span>
<span><span>    ^</span></span>
<span><span>???:?:?: 0x10316045b in _ExecInterpExpr (???)</span></span>
<span><span>???:?:?: 0x10315fbef in _ExecInterpExprStillValid (???)</span></span>
<span><span>???:?:?: 0x10326ceef in _evaluate_expr (???)</span></span>
<span><span>???:?:?: 0x10326da67 in _simplify_function (???)</span></span>
<span><span>???:?:?: 0x10326bacf in _eval_const_expressions_mutator (???)</span></span>
<span><span></span></span></code></pre></div>
<p>It points exactly where the error happened! This happens because Zig has runtime checks depending on the <a href="https://ziglang.org/documentation/master/#Build-Mode">build mode</a>. The <code>ReleaseSafe</code> mode, for example, trades a bit of performance for more safety checks.</p>
<p>Note that this stacktrace worked so well because the error was in Zig code. When building Postgres extensions, you often have to call Postgres APIs which will still segmentation fault if you use them incorrectly.</p>

<p>Postgres uses <a href="https://en.wikipedia.org/wiki/Region-based_memory_management">allocator arenas</a> to manage memory. In the Postgres source code, the arenas are called <a href="https://github.com/postgres/postgres/blob/master/src/backend/utils/mmgr/README">memory contexts</a>. Memory allocated in a "context" can be freed all at once (for example, when a query execution is finished), which simplifies memory management significantly, because you only need to track contexts, not individual allocations. Contexts are also hierarchical, so you can create a context that is a child of another context, and when the parent context is freed, all children are freed as well. This makes memory leaks rather unlikely.</p>
<p>Another advantage of memory contexts is that they improve <a href="https://www.postgresql.org/docs/current/view-pg-backend-memory-contexts.html">memory monitoring</a>, because context have names and you can see how much memory is used by each context. This is useful for debugging large memory usage.</p>
<p>This model of using an arena/context allocator happens to match really good with Zig. One reason is Zig's convention to have all functions/objects that allocate memory receive an allocator as a parameter. This makes allocations <a href="https://notes.eatonphil.com/2024-03-15-zig-rust-and-other-languages.html">more explicit</a> but also makes it easy to use a custom allocator. pgzx defines custom allocators that wrap the Postgres memory contexts and make them available to Zig code.</p>
<p>Here is an example that creates a new context as a child of the current context and gets the allocator for it:</p>
<div role="group"><pre tabindex="0"><code><span><span>var</span><span> memctx = </span><span>try</span><span> pgzx.mem.</span><span>createAllocSetContext</span><span>(</span><span>"zig_context"</span><span>, .{ .parent = pg.CurrentMemoryContext });</span></span>
<span><span>const</span><span> allocator = memctx.</span><span>allocator</span><span>();</span></span>
<span></span></code></pre></div>

<p>Another Postgres API that you will very likely need to know about in a more complex extension is the error handling. Postgres implements "exceptions" in C via <code>setjmp/longjmp</code> and provides a set of macros to throw and catch them (<a href="https://github.com/postgres/postgres/blob/master/src/include/utils/elog.h#L318">PG_TRY/PG_CATCH</a>).</p>
<p>The issue is that long jumps could bypass the Zig control flow, for example <code>errdefer</code> blocks might not be executed. This means that if your extension calls into Postgres APIs, and those APIs can throw errors, long jumps might skip your <code>defer</code> and <code>errdefer</code> blocks!</p>
<p>Luckily, pgzx is here to help. It provides a set of functions that allow you to catch the Postgres exceptions and convert them into Zig errors. For example:</p>
<div role="group"><pre tabindex="0"><code><span><span>var</span><span> errctx = pgzx.err.Context.</span><span>init</span><span>();</span></span>
<span><span>defer</span><span> errctx.</span><span>deinit</span><span>();</span></span>
<span><span>if</span><span> (errctx.</span><span>pg_try</span><span>()) {</span></span>
<span><span>    // Call Postgres C functions.</span></span>
<span><span>} </span><span>else</span><span> {</span></span>
<span><span>    return</span><span> errctx.</span><span>errorValue</span><span>();</span></span>
<span><span>}</span></span>
<span></span></code></pre></div>

<p>pgzx comes with a <a href="https://nixos.wiki/wiki/Flakes">Nix flakes</a> based development environment for developing extensions as well as pgzx itself. It also comes with a project template which you can use to set up this environment in a new repository. To use it, <a href="https://github.com/DeterminateSystems/nix-installer">install Nix</a> and then run:</p>
<div role="group"><pre tabindex="0"><code><span><span>mkdir</span><span> my_extension</span></span>
<span><span>cd</span><span> my_extension</span></span>
<span><span>nix</span><span> flake</span><span> init</span><span> -t</span><span> github:xataio/pgzx</span></span>
<span></span></code></pre></div>
<p>Then load the nix shell with:</p>

<p>The dev environment includes commands to relocate the Postgres binaries in your development environment, to start the server, and so on. The template also comes with a minimal extension and a <code>build.zig</code> file with a few common tasks. See the template <a href="https://github.com/xataio/pgzx/tree/main/nix/templates/init">README</a> for how to build the extension from this point.</p>

<p>Postgres extensions are typically tested via a tool called <code>pg_regress</code>. This is supported by pgzx as well, simply call <code>zig build pg_regress</code>.</p>
<p>But we also wanted to have unit tests. This is a bit tricky because the tests need to be compiled and run in the <strong>context of a Postgres instance</strong>. Otherwise, they won't be able to interact with Postgres' APIs.</p>
<p>In order to solve this, pgzx registers a custom <code>run_tests</code> function. This function can be called from SQL (<code>SELECT run_tests();</code>) and it will run the unit tests. A test suite is a Zig struct with functions starting with <code>test</code>. To register a test suite, you would typically do something like this:</p>
<div role="group"><pre tabindex="0"><code><span><span>comptime</span><span> {</span></span>
<span><span>    pgzx.testing.</span><span>registerTests</span><span>(</span><span>@import</span><span>(</span><span>"build_options"</span><span>).testfn, .{Tests});</span></span>
<span><span>}</span></span>
<span></span></code></pre></div>
<p>The <a href="https://github.com/xataio/pgzx/blob/9825dde752ee4ace2cbc594332eba100292aafd5/src/pgzx/testing.zig#L39">registerTests</a> function is another example of <code>comptime</code> usage. It iterates over all the fields of a struct and generates the call to run the tests when the <code>run_tests()</code> function is called in SQL.</p>

<p>This walkthrough covered some of the more interesting functionality exposed by pgzx, but there's more on offer: wrappers for the Postgres data structures (<a href="https://xataio.github.io/pgzx/#A;pgzx:list">lists</a>, hashtables), <a href="https://xataio.github.io/pgzx/#A;pgzx:spi">SPI</a>, <a href="https://xataio.github.io/pgzx/#A;pgzx:shmem">shared memory access</a>, <a href="https://xataio.github.io/pgzx/#A;pgzx:pq">connection management</a>, and more...</p>

<p>At Xata, we have been working on a new Postgres project, which doesn't really have a name yet, so let's call it <em>"Xata's take on distributed Postgres"</em>. It's in its very early stages, however we'll be open sourcing it soon and we're planning to <a href="https://mailchi.mp/xata/2zoy27tx2e">build it in public</a>. It will be somewhat similar to Citus, but some different choices, especially around usage and DX, based on what we learned running Xata for the past couple of years.</p>
<p>For this project, we considered three potential implementation directions:</p>
<ol role="list"><li>As an external proxy, like Vitess does for MySQL.</li><li>As a Postgres extension, like Citus.</li><li>As a fork of Postgres, like Greenplum.</li></ol>
<p>We have some experience with the first option because that's how our current Xata Proxy works, see more details in this <a href="https://xata.io/blog/serverless-postgres-platform">blog post</a>, so we were almost biased towards it. We know we can speak the Postgres wire protocol, parse the incoming queries and understand them at a deep level, as well as create distributed transactions via 2PC. However, we also know that leveraging existing Postgres code it will open up more options and avoid us having to reimplement some very difficult parts. Given the long term vision of the project, and that we didn't want to maintain a fork, we decided to go with the second option.</p>
<p>The next node in our decision tree was the <strong>programming language</strong>. The main options we considered were C, Rust, and Zig. While there are pros and cons to each of these options, which we might cover in detail a future blog post, we decided to go with Zig. The main "pros" for Zig were:</p>
<ul role="list"><li>It allows us to call into the Postgres APIs almost directly, so we have the same power as using C.</li><li>It offers more memory safety than C, it's a bit more expressive, and a bit more fun (we think).</li><li>It fits well with the Postgres codebase, for example when it comes to memory management and string handling.</li></ul>
<p>As we started working, we realized that an equivalent for Rust's <a href="https://github.com/pgcentralfoundation/pgrx">pgrx</a> would be needed for Zig as well, so we started pgzx.</p>
<p>While Zig and pgzx might not be the best choice for every Postgres extension, we think it's a reasonable choice for our project and perhaps for some others as well.</p>

<p>pgzx is new and should be considered "alpha" as of now. However, if you want to build a Postgres extension and you want to use Zig, it's going to be a lot easier with pgzx than without it. A status of the covered functionality is in the <a href="https://github.com/xataio/pgzx">README</a>. If you need help or you'd like to contribute, please join us on the <a href="https://xata.io/discord">Xata Discord</a>.</p>
<p>Also, if this blog post has sparked your interest in Zig and you want to give it a try, why not develop a Postgres extension? Let us know about it and we'll include it in the list of examples!</p>

<p>Learn more about the development of this project from the folks that built it, and watch a quick demo to see it in action. Check out our latest meet the makers session here:</p>
<div><p><iframe src="https://www.youtube.com/embed/oUvAfia7gjE?si=P5ApXa2moZR3NfL8" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p></div>
<p>Want to keep up with pgzx, the distributed Postgres project, or other open-source projects from Xata? We have set up an email newsletter just for that. You can subscribe <a href="https://mailchi.mp/xata/2zoy27tx2e">here</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. sues Apple, accusing it of maintaining an iPhone monopoly (2231 pts)]]></title>
            <link>https://www.nytimes.com/2024/03/21/technology/apple-doj-lawsuit-antitrust.html</link>
            <guid>39778999</guid>
            <pubDate>Thu, 21 Mar 2024 14:37:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/03/21/technology/apple-doj-lawsuit-antitrust.html">https://www.nytimes.com/2024/03/21/technology/apple-doj-lawsuit-antitrust.html</a>, See on <a href="https://news.ycombinator.com/item?id=39778999">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/03/21/technology/apple-doj-lawsuit-antitrust.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The Reddits (421 pts)]]></title>
            <link>https://www.ycombinator.com/blog/the-reddits</link>
            <guid>39778590</guid>
            <pubDate>Thu, 21 Mar 2024 14:00:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ycombinator.com/blog/the-reddits">https://www.ycombinator.com/blog/the-reddits</a>, See on <a href="https://news.ycombinator.com/item?id=39778590">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Dropflow, a CSS layout engine for node or <canvas> (350 pts)]]></title>
            <link>https://github.com/chearon/dropflow</link>
            <guid>39778570</guid>
            <pubDate>Thu, 21 Mar 2024 13:58:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/chearon/dropflow">https://github.com/chearon/dropflow</a>, See on <a href="https://news.ycombinator.com/item?id=39778570">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">dropflow</h2><a id="user-content-dropflow" aria-label="Permalink: dropflow" href="#dropflow"></a></p>
<p dir="auto">Dropflow is a CSS layout engine created to explore the reaches of the foundational CSS standards (that is: inlines, blocks, floats, positioning and eventually tables, but not flexbox or grid). It has a high quality text layout implementation and is capable of displaying many of the languages of the world. You can use it to generate PDFs or images on the backend with Node and <a href="https://github.com/Automattic/node-canvas">node-canvas</a> or render rich, wrapped text to a canvas in the browser.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Supports over 30 properties including complex ones like <code>float</code></li>
<li>Bidirectional and RTL text</li>
<li>Hyperscript (<code>h()</code>) API with styles as objects in addition to accepting HTML and CSS</li>
<li>Any OpenType/TrueType buffer can (and must) be registered</li>
<li>Font fallbacks at the grapheme level</li>
<li>Colored diacritics</li>
<li>Desirable line breaking (e.g. carries starting padding to the next line)</li>
<li>Optimized shaping</li>
<li>Inherited and cascaded styles are never calculated twice</li>
<li>Handles as many CSS layout edge cases as I can find</li>
<li>Fully typed</li>
<li>Lots of tests</li>
<li>Fast</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported CSS rules</h2><a id="user-content-supported-css-rules" aria-label="Permalink: Supported CSS rules" href="#supported-css-rules"></a></p>
<p dir="auto">Following are rules that work or will work soon. Shorthand properties are not listed. If you see all components of a shorthand (for example, <code>border-style</code>, <code>border-width</code>, <code>border-color</code>) then the shorthand is assumed to be supported (for example <code>border</code>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inline formatting</h2><a id="user-content-inline-formatting" aria-label="Permalink: Inline formatting" href="#inline-formatting"></a></p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Values</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>color</code></td>
<td><code>rgba()</code>, <code>rgb()</code>, <code>#rrggbb</code>, <code>#rgb</code>, <code>#rgba</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>direction</code></td>
<td><code>ltr</code>, <code>rtl</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>font-‍family</code></td>
<td></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>font-‍size</code></td>
<td><code>em</code>, <code>px</code>, <code>smaller</code> etc, <code>small</code> etc, <code>cm</code> etc</td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>font-‍stretch</code></td>
<td><code>condensed</code> etc</td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>font-‍style</code></td>
<td><code>normal</code>, <code>italic</code>, <code>oblique</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>font-‍variant</code></td>
<td></td>
<td>🚧‍&nbsp;Planned</td>
</tr>
<tr>
<td><code>font-‍weight</code></td>
<td><code>normal</code>, <code>bolder</code>, <code>lighter</code> <code>light</code>, <code>bold</code>, <code>100</code>-<code>900</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>line-‍height</code></td>
<td><code>normal</code>, <code>px</code>, <code>em</code>, <code>%</code>, <code>number</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>tab-‍size</code></td>
<td></td>
<td>🚧‍&nbsp;Planned</td>
</tr>
<tr>
<td><code>text-‍align</code></td>
<td><code>start</code>, <code>end</code>, <code>left</code>, <code>right</code>, <code>center</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>text-‍decoration</code></td>
<td></td>
<td>🚧‍&nbsp;Planned</td>
</tr>
<tr>
<td><code>unicode-‍bidi</code></td>
<td></td>
<td>🚧‍&nbsp;Planned</td>
</tr>
<tr>
<td><code>vertical-‍align</code></td>
<td><code>baseline</code>, <code>middle</code>, <code>sub</code>, <code>super</code>, <code>text-top</code>, <code>text-bottom</code>, <code>%</code>, <code>px</code> etc, <code>top</code>, <code>bottom</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>white-‍space</code></td>
<td><code>normal</code>, <code>nowrap</code>, <code>pre</code>, <code>pre-wrap</code>, <code>pre-line</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Block formatting</h2><a id="user-content-block-formatting" aria-label="Permalink: Block formatting" href="#block-formatting"></a></p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Values</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>clear</code></td>
<td><code>left</code>, <code>right</code>, <code>both</code>, <code>none</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>float</code></td>
<td><code>left</code>, <code>right</code>, <code>none</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>writing-‍mode</code></td>
<td><code>horizontal-tb</code>, <code>vertical-lr</code>, <code>vertical-rl</code></td>
<td>🏗 Partially done<sup>1</sup></td>
</tr>
</tbody>
</table>
<p dir="auto"><sup>1</sup>Implemented for BFCs but not IFCs yet</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Boxes and positioning</h2><a id="user-content-boxes-and-positioning" aria-label="Permalink: Boxes and positioning" href="#boxes-and-positioning"></a></p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Values</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>background-‍clip</code></td>
<td><code>border-box</code>, <code>content-box</code>, <code>padding-box</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>background-‍color</code></td>
<td><code>rgba()</code>, <code>rgb()</code>, <code>#rrggbb</code>, <code>#rgb</code>, <code>#rgba</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>border-‍color</code></td>
<td><code>rgba()</code>, <code>rgb()</code>, <code>#rrggbb</code>, <code>#rgb</code>, <code>#rgba</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>border-‍style</code></td>
<td><code>solid</code>, <code>none</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>border-‍width</code></td>
<td><code>em</code>, <code>px</code>, <code>cm</code> etc</td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>top</code>, <code>right</code>, <code>bottom</code>, <code>left</code></td>
<td><code>em</code>, <code>px</code>, <code>%</code>, <code>cm</code> etc</td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>box-‍sizing</code></td>
<td><code>border-box</code>, <code>content-box</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>display</code></td>
<td><code>block</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>display</code></td>
<td><code>inline</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>display</code></td>
<td><code>inline-block</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>display</code></td>
<td><code>flow-root</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>display</code></td>
<td><code>none</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>display</code></td>
<td><code>table</code></td>
<td>🚧‍&nbsp;Planned</td>
</tr>
<tr>
<td><code>height</code></td>
<td><code>em</code>, <code>px</code>, <code>%</code>, <code>cm</code> etc, <code>auto</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>margin</code></td>
<td><code>em</code>, <code>px</code>, <code>%</code>, <code>cm</code> etc, <code>auto</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>max-height</code>, <code>max-width</code>,<br><code>min-height</code>, <code>min-width</code></td>
<td><code>em</code>, <code>px</code>, <code>%</code>, <code>cm</code> etc, <code>auto</code></td>
<td>🚧‍&nbsp;Planned</td>
</tr>
<tr>
<td><code>padding</code></td>
<td><code>em</code>, <code>px</code>, <code>%</code>, <code>cm</code> etc</td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>position</code></td>
<td><code>absolute</code></td>
<td>🚧‍&nbsp;Planned</td>
</tr>
<tr>
<td><code>position</code></td>
<td><code>fixed</code></td>
<td>🚧‍&nbsp;Planned</td>
</tr>
<tr>
<td><code>position</code></td>
<td><code>relative</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>transform</code></td>
<td></td>
<td>🚧‍&nbsp;Planned</td>
</tr>
<tr>
<td><code>overflow</code></td>
<td></td>
<td>🚧‍&nbsp;Planned</td>
</tr>
<tr>
<td><code>width</code></td>
<td><code>em</code>, <code>px</code>, <code>%</code>, <code>cm</code> etc, <code>auto</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
<tr>
<td><code>z-index</code></td>
<td><code>number</code>, <code>auto</code></td>
<td>✅‍&nbsp;Works</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Dropflow works off of a DOM with inherited and calculated styles, the same way
that browsers do. You create the DOM with the familiar <code>h()</code> function, and
specify styles as plain objects.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import * as flow from 'dropflow';
import {createCanvas} from 'canvas';
import fs from 'node:fs';

// Register fonts before layout. This is a required step.
// It is only async when you don't pass an ArrayBuffer
await flow.registerFont(new URL('fonts/Roboto-Regular.ttf', import.meta.url));
await flow.registerFont(new URL('fonts/Roboto-Bold.ttf', import.meta.url));

// Always create styles at the top-level of your module if you can
const divStyle = {
  backgroundColor: {r: 28, g: 10, b: 0, a: 1},
  color: {r: 179, g: 200, b: 144, a: 1},
  textAlign: 'center'
};

// Since we're creating styles directly, colors have to be defined numerically
const spanStyle = {
  color: {r: 115, g: 169, b: 173, a: 1},
  fontWeight: 700
};

// Create a DOM
const rootElement = flow.h('div', {style: divStyle}, [
  'Hello, ',
  flow.h('span', {style: spanStyle}, ['World!'])
]);

// Layout and paint into the entire canvas (see also renderToCanvasContext)
const canvas = createCanvas(250, 50);
flow.renderToCanvas(rootElement, canvas, /* optional density: */ 2);

// Save your image
canvas.createPNGStream().pipe(fs.createWriteStream(new URL('hello.png', import.meta.url)));
"><pre><span>import</span> <span>*</span> <span>as</span> <span>flow</span> <span>from</span> <span>'dropflow'</span><span>;</span>
<span>import</span> <span>{</span><span>createCanvas</span><span>}</span> <span>from</span> <span>'canvas'</span><span>;</span>
<span>import</span> <span>fs</span> <span>from</span> <span>'node:fs'</span><span>;</span>

<span>// Register fonts before layout. This is a required step.</span>
<span>// It is only async when you don't pass an ArrayBuffer</span>
<span>await</span> <span>flow</span><span>.</span><span>registerFont</span><span>(</span><span>new</span> <span>URL</span><span>(</span><span>'fonts/Roboto-Regular.ttf'</span><span>,</span> <span>import</span><span>.</span><span>meta</span><span>.</span><span>url</span><span>)</span><span>)</span><span>;</span>
<span>await</span> <span>flow</span><span>.</span><span>registerFont</span><span>(</span><span>new</span> <span>URL</span><span>(</span><span>'fonts/Roboto-Bold.ttf'</span><span>,</span> <span>import</span><span>.</span><span>meta</span><span>.</span><span>url</span><span>)</span><span>)</span><span>;</span>

<span>// Always create styles at the top-level of your module if you can</span>
<span>const</span> <span>divStyle</span> <span>=</span> <span>{</span>
  <span>backgroundColor</span>: <span>{</span><span>r</span>: <span>28</span><span>,</span> <span>g</span>: <span>10</span><span>,</span> <span>b</span>: <span>0</span><span>,</span> <span>a</span>: <span>1</span><span>}</span><span>,</span>
  <span>color</span>: <span>{</span><span>r</span>: <span>179</span><span>,</span> <span>g</span>: <span>200</span><span>,</span> <span>b</span>: <span>144</span><span>,</span> <span>a</span>: <span>1</span><span>}</span><span>,</span>
  <span>textAlign</span>: <span>'center'</span>
<span>}</span><span>;</span>

<span>// Since we're creating styles directly, colors have to be defined numerically</span>
<span>const</span> <span>spanStyle</span> <span>=</span> <span>{</span>
  <span>color</span>: <span>{</span><span>r</span>: <span>115</span><span>,</span> <span>g</span>: <span>169</span><span>,</span> <span>b</span>: <span>173</span><span>,</span> <span>a</span>: <span>1</span><span>}</span><span>,</span>
  <span>fontWeight</span>: <span>700</span>
<span>}</span><span>;</span>

<span>// Create a DOM</span>
<span>const</span> <span>rootElement</span> <span>=</span> <span>flow</span><span>.</span><span>h</span><span>(</span><span>'div'</span><span>,</span> <span>{</span><span>style</span>: <span>divStyle</span><span>}</span><span>,</span> <span>[</span>
  <span>'Hello, '</span><span>,</span>
  <span>flow</span><span>.</span><span>h</span><span>(</span><span>'span'</span><span>,</span> <span>{</span><span>style</span>: <span>spanStyle</span><span>}</span><span>,</span> <span>[</span><span>'World!'</span><span>]</span><span>)</span>
<span>]</span><span>)</span><span>;</span>

<span>// Layout and paint into the entire canvas (see also renderToCanvasContext)</span>
<span>const</span> <span>canvas</span> <span>=</span> <span>createCanvas</span><span>(</span><span>250</span><span>,</span> <span>50</span><span>)</span><span>;</span>
<span>flow</span><span>.</span><span>renderToCanvas</span><span>(</span><span>rootElement</span><span>,</span> <span>canvas</span><span>,</span> <span>/* optional density: */</span> <span>2</span><span>)</span><span>;</span>

<span>// Save your image</span>
<span>canvas</span><span>.</span><span>createPNGStream</span><span>(</span><span>)</span><span>.</span><span>pipe</span><span>(</span><span>fs</span><span>.</span><span>createWriteStream</span><span>(</span><span>new</span> <span>URL</span><span>(</span><span>'hello.png'</span><span>,</span> <span>import</span><span>.</span><span>meta</span><span>.</span><span>url</span><span>)</span><span>)</span><span>)</span><span>;</span></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/chearon/dropflow/blob/master/assets/images/hello.png"><img src="https://github.com/chearon/dropflow/raw/master/assets/images/hello.png" alt="Hello world against a dark background, with &quot;world&quot; bolded and colored differently"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">HTML</h2><a id="user-content-html" aria-label="Permalink: HTML" href="#html"></a></p>
<p dir="auto">This API is only recommended if performance is not a concern, or for learning
purposes. Parsing adds extra time (though it is fast thanks to @fb55) and
increases bundle size significantly.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import * as flow from 'dropflow/with-parse.js';
import {createCanvas} from 'canvas';
import fs from 'node:fs';

await flow.registerFont(new URL('fonts/Roboto-Regular.ttf', import.meta.url));
await flow.registerFont(new URL('fonts/Roboto-Bold.ttf', import.meta.url));

const rootElement = flow.parse(`
  <div style=&quot;background-color: #1c0a00; color: #b3c890; text-align: center;&quot;>
    Hello, <span style=&quot;color: #73a9ad; font-weight: bold;&quot;>World!</span>
  </div>
`);

const canvas = createCanvas(250, 50);
flow.renderToCanvas(rootElement, canvas, 2);

canvas.createPNGStream().pipe(fs.createWriteStream(new URL('hello.png', import.meta.url)));"><pre><span>import</span> <span>*</span> <span>as</span> <span>flow</span> <span>from</span> <span>'dropflow/with-parse.js'</span><span>;</span>
<span>import</span> <span>{</span><span>createCanvas</span><span>}</span> <span>from</span> <span>'canvas'</span><span>;</span>
<span>import</span> <span>fs</span> <span>from</span> <span>'node:fs'</span><span>;</span>

<span>await</span> <span>flow</span><span>.</span><span>registerFont</span><span>(</span><span>new</span> <span>URL</span><span>(</span><span>'fonts/Roboto-Regular.ttf'</span><span>,</span> <span>import</span><span>.</span><span>meta</span><span>.</span><span>url</span><span>)</span><span>)</span><span>;</span>
<span>await</span> <span>flow</span><span>.</span><span>registerFont</span><span>(</span><span>new</span> <span>URL</span><span>(</span><span>'fonts/Roboto-Bold.ttf'</span><span>,</span> <span>import</span><span>.</span><span>meta</span><span>.</span><span>url</span><span>)</span><span>)</span><span>;</span>

<span>const</span> <span>rootElement</span> <span>=</span> <span>flow</span><span>.</span><span>parse</span><span>(</span><span>`</span>
<span>  &lt;div style="background-color: #1c0a00; color: #b3c890; text-align: center;"&gt;</span>
<span>    Hello, &lt;span style="color: #73a9ad; font-weight: bold;"&gt;World!&lt;/span&gt;</span>
<span>  &lt;/div&gt;</span>
<span>`</span><span>)</span><span>;</span>

<span>const</span> <span>canvas</span> <span>=</span> <span>createCanvas</span><span>(</span><span>250</span><span>,</span> <span>50</span><span>)</span><span>;</span>
<span>flow</span><span>.</span><span>renderToCanvas</span><span>(</span><span>rootElement</span><span>,</span> <span>canvas</span><span>,</span> <span>2</span><span>)</span><span>;</span>

<span>canvas</span><span>.</span><span>createPNGStream</span><span>(</span><span>)</span><span>.</span><span>pipe</span><span>(</span><span>fs</span><span>.</span><span>createWriteStream</span><span>(</span><span>new</span> <span>URL</span><span>(</span><span>'hello.png'</span><span>,</span> <span>import</span><span>.</span><span>meta</span><span>.</span><span>url</span><span>)</span><span>)</span><span>)</span><span>;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance characteristics</h2><a id="user-content-performance-characteristics" aria-label="Permalink: Performance characteristics" href="#performance-characteristics"></a></p>
<p dir="auto">Performance is a top goal and is second only to correctness. Run the performance examples in the <code>examples</code> directory to see the numbers for yourself.</p>
<ul dir="auto">
<li>8 paragraphs with several inline spans of different fonts can be turned from HTML to image in <strong>9ms</strong> on a 2019 MacBook Pro and <strong>13ms</strong> on a 2012 MacBook Pro (<code>perf-1.ts</code>)</li>
<li>The Little Prince (over 500 paragraphs) can be turned from HTML to image in under <strong>160ms</strong> on a 2019 MacBook Pro and under <strong>250ms</strong> on a 2012 MacBook Pro (<code>perf-2.ts</code>)</li>
<li>A 10-letter word can be generated and laid out (not painted) in under <strong>25µs</strong> on a 2019 MacBook Pro and under <strong>50µs</strong> on a 2012 MacBook Pro (<code>perf-3.ts</code>)</li>
</ul>
<p dir="auto">The fastest performance can be achieved by using the hyperscript API, which creates a DOM directly and skips the typical HTML and CSS parsing steps. Take care to re-use style objects to get the most benefits. Reflows at different widths are faster than recreating the layout tree.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">API</h2><a id="user-content-api" aria-label="Permalink: API" href="#api"></a></p>
<p dir="auto">The first two steps are:</p>
<ol dir="auto">
<li><a href="#fonts">Register fonts</a></li>
<li><a href="#hyperscript">Create a DOM via the Hyperscript or Parse API</a></li>
</ol>
<p dir="auto">Then, you can either render the DOM into a canvas using its size as the viewport:</p>
<ol dir="auto">
<li><a href="#render-dom-to-canvas">Render DOM to canvas</a></li>
</ol>
<p dir="auto">Or, you can use the lower-level functions to retain the layout, in case you want to re-layout at a different size, choose not to paint (for example if the layout isn't visible) or get intrinsics:</p>
<ol dir="auto">
<li><a href="#generate">Generate a tree of layout boxes from the DOM</a></li>
<li><a href="#layout">Layout the box tree</a></li>
<li><a href="#paint">Paint the box tree to a target like canvas</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Fonts</h2><a id="user-content-fonts" aria-label="Permalink: Fonts" href="#fonts"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>registerFont</code></h3><a id="user-content-registerfont" aria-label="Permalink: registerFont" href="#registerfont"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="async function registerFont(url: URL, options?: {paint: boolean}): Promise<void>;
async function registerFont(buffer: ArrayBuffer, url: URL, options?: {paint: boolean}): Promise<void>;"><pre><span>async</span> <span>function</span> <span>registerFont</span><span>(</span><span>url</span>: <span>URL</span><span>,</span> <span>options</span>?: <span>{</span><span>paint</span>: <span>boolean</span><span>}</span><span>)</span>: <span>Promise</span><span>&lt;</span><span><span>void</span></span><span>&gt;</span><span>;</span>
<span>async</span> <span>function</span> <span>registerFont</span><span>(</span><span>buffer</span>: <span>ArrayBuffer</span><span>,</span> <span>url</span>: <span>URL</span><span>,</span> <span>options</span>?: <span>{</span><span>paint</span>: <span>boolean</span><span>}</span><span>)</span>: <span>Promise</span><span>&lt;</span><span><span>void</span></span><span>&gt;</span><span>;</span></pre></div>
<p dir="auto">Registers a font to be selected by the <code>font</code> properties. Dropflow <strong>does not search system fonts</strong>, so you must do this with at least one font.</p>
<p dir="auto">When a URL is passed, don't forget to <code>await</code> this. If an <code>ArrayBuffer</code> is passed, there is no need to <code>await</code>. In that function signature, the <code>URL</code> is only used to provide a unique name for the font.</p>
<p dir="auto">The <code>URL</code> must always be unique.</p>
<p dir="auto">In the browser, make sure the font is also loaded into page so that the paint backend can reference it with <code>ctx.font</code>. In <code>node-canvas</code>, you should either use <code>registerFont</code> from <code>canvas</code> for this font, or pass <code>{paint: true}</code> for <code>options</code>, which will try to load <code>node-canvas</code> and call its <code>registerFont</code>.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">This will soon be replaced with an API that looks more like the <code>document.fonts</code> API in the browser.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>unregisterFont</code></h3><a id="user-content-unregisterfont" aria-label="Permalink: unregisterFont" href="#unregisterfont"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="function unregisterFont(url: URL): void;"><pre><span>function</span> <span>unregisterFont</span><span>(</span><span>url</span>: <span>URL</span><span>)</span>: <span><span>void</span></span><span>;</span></pre></div>
<p dir="auto">Removes a font from the internal list so that it won't be picked by the <code>font</code> properties. This does not remove it from the paint target.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hyperscript</h2><a id="user-content-hyperscript" aria-label="Permalink: Hyperscript" href="#hyperscript"></a></p>
<p dir="auto">The hyperscript API is the fastest way to generate a DOM.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>h</code></h3><a id="user-content-h" aria-label="Permalink: h" href="#h"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="type HsChild = HTMLElement | TextNode | string;

interface HsData {
  style?: DeclaredPlainStyle;
  attrs?: {[k: string]: string};
}

function h(tagName: string): HTMLElement;
function h(tagName: string, data: HsData): HTMLElement;
function h(tagName: string, children: HsChild[]): HTMLElement;
function h(tagName: string, text: string): HTMLElement;
function h(tagName: string, data: HsData, children: HsChild[] | string): HTMLElement;"><pre><span>type</span> <span>HsChild</span> <span>=</span> <span>HTMLElement</span> <span>|</span> <span>TextNode</span> <span>|</span> <span>string</span><span>;</span>

<span>interface</span> <span>HsData</span> <span>{</span>
  <span>style</span>?: <span>DeclaredPlainStyle</span><span>;</span>
  <span>attrs</span>?: <span>{</span><span>[</span><span>k</span>: <span>string</span><span>]</span>: <span>string</span><span>}</span><span>;</span>
<span>}</span>

<span>function</span> <span>h</span><span>(</span><span>tagName</span>: <span>string</span><span>)</span>: <span>HTMLElement</span><span>;</span>
<span>function</span> <span>h</span><span>(</span><span>tagName</span>: <span>string</span><span>,</span> <span>data</span>: <span>HsData</span><span>)</span>: <span>HTMLElement</span><span>;</span>
<span>function</span> <span>h</span><span>(</span><span>tagName</span>: <span>string</span><span>,</span> <span>children</span>: <span>HsChild</span><span>[</span><span>]</span><span>)</span>: <span>HTMLElement</span><span>;</span>
<span>function</span> <span>h</span><span>(</span><span>tagName</span>: <span>string</span><span>,</span> <span>text</span>: <span>string</span><span>)</span>: <span>HTMLElement</span><span>;</span>
<span>function</span> <span>h</span><span>(</span><span>tagName</span>: <span>string</span><span>,</span> <span>data</span>: <span>HsData</span><span>,</span> <span>children</span>: <span>HsChild</span><span>[</span><span>]</span> <span>|</span> <span>string</span><span>)</span>: <span>HTMLElement</span><span>;</span></pre></div>
<p dir="auto">Creates an HTMLElement. Styles go on <code>data.style</code> (see <code>style.ts</code> for supported values and their types).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>dom</code></h3><a id="user-content-dom" aria-label="Permalink: dom" href="#dom"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="function dom(el: HTMLElement | HTMLElement[]): HTMLElement"><pre><span>function</span> <span>dom</span><span>(</span><span>el</span>: <span>HTMLElement</span> <span>|</span> <span>HTMLElement</span><span>[</span><span>]</span><span>)</span>: <span>HTMLElement</span></pre></div>
<p dir="auto">Calculates styles and wraps with <code>&lt;html&gt;</code> if the root <code>tagName</code> is not <code>"html"</code>.</p>
<p dir="auto">The entire <code>h</code> tree to render must be passed to this function before rendering.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Parse</h2><a id="user-content-parse" aria-label="Permalink: Parse" href="#parse"></a></p>
<p dir="auto">This part of the API brings in a lot more code due to the size of the HTML and CSS parsers. Import it like so:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import flow from 'dropflow/with-parse.js';"><pre><span>import</span> <span>flow</span> <span>from</span> <span>'dropflow/with-parse.js'</span><span>;</span></pre></div>
<p dir="auto">Note that only the <code>style</code> HTML attribute is supported at this time. <code>class</code> does not work yet.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>parse</code></h3><a id="user-content-parse-1" aria-label="Permalink: parse" href="#parse-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="function parse(str: string): HTMLElement;"><pre><span>function</span> <span>parse</span><span>(</span><span>str</span>: <span>string</span><span>)</span>: <span>HTMLElement</span><span>;</span></pre></div>
<p dir="auto">Parses HTML. If you don't specify a root <code>&lt;html&gt;</code> element, content will be wrapped with one.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Render DOM to canvas</h2><a id="user-content-render-dom-to-canvas" aria-label="Permalink: Render DOM to canvas" href="#render-dom-to-canvas"></a></p>
<p dir="auto">This is only for simple use cases. For more advanced usage continue on to the next section.</p>
<div dir="auto" data-snippet-clipboard-copy-content="function renderToCanvas(rootElement: HTMLElement, canvas: Canvas): void;"><pre><span>function</span> <span>renderToCanvas</span><span>(</span><span>rootElement</span>: <span>HTMLElement</span><span>,</span> <span>canvas</span>: <span>Canvas</span><span>)</span>: <span><span>void</span></span><span>;</span></pre></div>
<p dir="auto">Renders the whole layout to the canvas, using its width and height as the viewport size.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Generate</h2><a id="user-content-generate" aria-label="Permalink: Generate" href="#generate"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>generate</code></h3><a id="user-content-generate-1" aria-label="Permalink: generate" href="#generate-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="function generate(rootElement: HTMLElement): BlockContainer"><pre><span>function</span> <span>generate</span><span>(</span><span>rootElement</span>: <span>HTMLElement</span><span>)</span>: <span>BlockContainer</span></pre></div>
<p dir="auto">Generates a box tree for the element tree. Box trees roughly correspond to DOM trees, but usually have more boxes (like for anonymous text content between block-level elements (<code>div</code>s)) and sometimes fewer (like for <code>display: none</code>).</p>
<p dir="auto"><code>BlockContainer</code> has a <code>repr()</code> method for logging the tree.</p>
<p dir="auto">Hold on to the return value so you can lay it out many times in different sizes, paint it or don't paint it if it's off-screen, or get intrinsics to build a higher-level logical layout (for example, spreadsheet column or row size even if the content is off screen).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Layout</h2><a id="user-content-layout" aria-label="Permalink: Layout" href="#layout"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>layout</code></h3><a id="user-content-layout-1" aria-label="Permalink: layout" href="#layout-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="function layout(root: BlockContainer, width = 640, height = 480);"><pre><span>function</span> <span>layout</span><span>(</span><span>root</span>: <span>BlockContainer</span><span>,</span> <span>width</span> <span>=</span> <span>640</span><span>,</span> <span>height</span> <span>=</span> <span>480</span><span>)</span><span>;</span></pre></div>
<p dir="auto">Position boxes and split text into lines so the layout tree is ready to paint. Can be called over and over with a different viewport size.</p>
<p dir="auto">In more detail, layout involves:</p>
<ul dir="auto">
<li>Margin collapsing for block boxes</li>
<li>Passing text to HarfBuzz, iterating font fallbacks, wrapping, reshaping depending on break points</li>
<li>Float placement and <code>clear</code>ing</li>
<li>Positioning shaped text spans and backgrounds according to <code>direction</code> and text direction</li>
<li>Second and third pass layouts for intrinsics of <code>float</code>, <code>inline-block</code>, and <code>absolute</code>s</li>
<li>Post-layout positioning (<code>position</code>)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Paint</h2><a id="user-content-paint" aria-label="Permalink: Paint" href="#paint"></a></p>
<p dir="auto">This step paints the layout to a target. Painting can be done as many times as needed (for example, every time you clear and render all of your scene to the canvas).</p>
<p dir="auto">Canvas is currently the only seriously supported target, but other targets will be added, like pdf.js and SVG. There is also a toy HTML target that was used early on in development, and kept around for fun.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>paintToCanvas</code></h3><a id="user-content-painttocanvas" aria-label="Permalink: paintToCanvas" href="#painttocanvas"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="function paintToCanvas(root: BlockContainer, ctx: CanvasRenderingContext2D): void;"><pre><span>function</span> <span>paintToCanvas</span><span>(</span><span>root</span>: <span>BlockContainer</span><span>,</span> <span>ctx</span>: <span>CanvasRenderingContext2D</span><span>)</span>: <span><span>void</span></span><span>;</span></pre></div>
<p dir="auto">Paints the layout to a browser canvas, node-canvas, or similar standards-compliant context.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>paintToHtml</code></h3><a id="user-content-painttohtml" aria-label="Permalink: paintToHtml" href="#painttohtml"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="function paintToHtml(root: BlockContainer): string;"><pre><span>function</span> <span>paintToHtml</span><span>(</span><span>root</span>: <span>BlockContainer</span><span>)</span>: <span>string</span><span>;</span></pre></div>
<p dir="auto">Paint to HTML! Yes, this API can actually be used to go from HTML to HTML. It generates a flat list of a bunch of absolutely positioned elements. Probably don't use this, but it can be useful in development and is amusing.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other</h2><a id="user-content-other" aria-label="Permalink: Other" href="#other"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>staticLayoutContribution</code></h3><a id="user-content-staticlayoutcontribution" aria-label="Permalink: staticLayoutContribution" href="#staticlayoutcontribution"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="function staticLayoutContribution(box: BlockContainer): number;"><pre><span>function</span> <span>staticLayoutContribution</span><span>(</span><span>box</span>: <span>BlockContainer</span><span>)</span>: <span>number</span><span>;</span></pre></div>
<p dir="auto">Returns the inline size in CSS pixels taken up by the layout, not including empty space after lines or the effect of any <code>width</code> properties. <code>layout</code> must be called before this.</p>
<p dir="auto">The intended usage is this: after laying out text into a desired size, use <code>staticLayoutContribution</code> to get the size without any remaining empty space at the end of the lines, then <code>layout</code> again into that size to get a tightly fitting layout.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">HarfBuzz</h2><a id="user-content-harfbuzz" aria-label="Permalink: HarfBuzz" href="#harfbuzz"></a></p>
<p dir="auto">Glyph layout is performed by <a href="https://github.com/harfbuzz/harfbuzz">HarfBuzz</a> compiled to WebAssembly. This allows for a level of correctness that isn't possible by using the <code>measureText</code> API to position spans of text. If you color the "V" in the text "AV" differently in Google Sheets, you will notice kerning is lost, and the letters appear further apart than they should be. That's because two <code>measureText</code> and <code>fillText</code> calls were made on the letters, so contextual glyph advances were lost. Dropflow uses HarfBuzz on more coarse shaping boundaries (not when color is changed) so that the font is more correctly supported.</p>
<p dir="auto">HarfBuzz compiled to WebAssembly can achieve performance metrics similar to <code>CanvasRenderingContext2D</code>'s <code>measureText</code>. It's not as fast as <code>measureText</code>, but it's not significantly slower (neither of them are the dominators in a text layout stack) and <code>measureText</code> has other correctness drawbacks. For example, a <code>measureText</code>-based text layout implementation must use a word cache to be quick, and this is what GSuite apps do. But a word cache is not able to support fonts with effects across spaces, and to support such a font would have to involve a binary search on the paragraph's break indices, which is far slower than passing the whole paragraph to HarfBuzz. Colored diacritics are not possible in any way with <code>measureText</code> either.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Shout-outs</h2><a id="user-content-shout-outs" aria-label="Permalink: Shout-outs" href="#shout-outs"></a></p>
<p dir="auto">dropflow doesn't have any <code>package.json</code> dependencies, but the work of many others made it possible. Javascript dependencies have been checked in and modified to varying degrees to fit this project, maintain focus, and rebel against dependency-of-dependency madness. Here are the projects I'm grateful for:</p>
<ul dir="auto">
<li><a href="https://github.com/harfbuzz/harfbuzz">harfbuzz</a> does font shaping and provides essential font APIs (C++)</li>
<li><a href="https://github.com/Tehreer/SheenBidi">Tehreer/SheenBidi</a> calculates bidi boundaries (C++)</li>
<li><a href="https://github.com/foliojs/linebreak">foliojs/linebreak</a> provides Unicode break indices (JS, modified)</li>
<li><a href="https://github.com/foliojs/grapheme-breaker">foliojs/grapheme-breaker</a> provides Unicode grapheme boundaries (JS, modified)</li>
<li><a href="https://github.com/peggyjs/peggy">peggyjs/peggy</a> builds the CSS parser (JS, dev dependency)</li>
<li><a href="https://github.com/fb55/htmlparser2">fb55/htmlparser2</a> parses HTML (JS, modified)</li>
<li><a href="https://github.com/google/emoji-segmenter">google/emoji-segmenter</a> segments emoji (C++)</li>
<li><a href="https://github.com/foliojs/unicode-trie">foliojs/unicode-trie</a> is used for fast unicode data (JS, heavily modified to remove unused parts)</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What is a pig butchering scam? (2023) (128 pts)]]></title>
            <link>https://www.wired.com/story/what-is-pig-butchering-scam/</link>
            <guid>39778486</guid>
            <pubDate>Thu, 21 Mar 2024 13:50:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/what-is-pig-butchering-scam/">https://www.wired.com/story/what-is-pig-butchering-scam/</a>, See on <a href="https://news.ycombinator.com/item?id=39778486">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>Digital swindles like</span> <a href="https://www.wired.com/story/business-email-compromise-scams/">business email compromises</a> and <a href="https://www.wired.com/2015/10/online-dating-made-woman-pawn-global-crime-plot/">romance scams</a> generate billions of dollars for criminals. And they all start with a little bit of “social engineering” to trick a victim into doing something disadvantageous, whether that's trusting someone they shouldn't or sending money into the void. Now, a new variation of these schemes, known as “pig butchering,” is on the rise, ensnaring unsuspecting targets to steal all of their money and operating at a massive scale thanks in large part to forced labor.</p><figure data-testid="IframeEmbed"></figure><p>Pig butchering scams originated in China, where they came to be known by the Chinese version of the phrase <em>shāzhūpán</em> because of an approach in which attackers essentially fatten victims up and then take everything they’ve got. These scams are typically cryptocurrency schemes, though they can involve other types of financial trading as well.</p><p>Scammers cold-contact people on SMS texting or other social media, dating, and communication platforms. Often they’ll simply say “Hi” or something like “Hey Josh, it was fun catching up last week!” If the recipient responds to say that the attacker has the wrong number, the scammer seizes the opportunity to strike up a conversation and guide the victim toward feeling like they’ve hit it off with a new friend. After establishing a rapport, the attacker will introduce the idea that they have been making a lot of money in cryptocurrency investing and suggest the target consider getting involved while they can.</p><p>Next, the scammer gets the target set up with a malicious app or web platform that appears trustworthy and may even impersonate the platforms of legitimate financial institutions. Once inside the portal, victims can often see curated real-time market data meant to show the potential of the investment. And once the target funds their “investment account,” they can start watching their balance “grow.” Crafting the malicious financial platforms to look legitimate and refined is a hallmark of pig butchering scams, as are other touches that add verisimilitude, like letting victims do a video call with their new “friend” or allowing them to withdraw a little bit of money from the platform to reassure them. The latter is a tactic that scammers also use in traditional Ponzi schemes.</p><p>Though the swindle has some new twists, you can still see where it's going. Once the victim has deposited all the money they have and everything the scammers can get them to borrow, the attackers shut down the account and disappear.</p><p>“That’s the whole pig butchering thing—they are going for the whole hog,” says Sean Gallagher, a senior threat researcher at the security firm Sophos who has been tracking pig butchering as it has emerged over the past three years. “They go after people who are vulnerable. Some of the victims are people who have had long-term health problems, who are older, people who feel isolated. They want to get every last bit of oink, and they are persistent.”&nbsp;</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Though carrying off pig butchering scams takes a lot of communication and relationship building with victims over time, researchers say that crime syndicates in China developed scripts and playbooks that allowed them to offload the work at scale onto inexperienced scammers or even forced laborers who are <a href="https://www.vice.com/en/article/n7zb5d/pig-butchering-scam-cambodia-trafficking">victims of human trafficking</a>.&nbsp;</p><p>“We can already see the damage and the human cost both to scam victims and to forced laborers,” says Michael Roberts, founder of Rexxfield Cyber Investigations, who has been working with victims of pig butchering attacks. “That’s why we need to start educating people about this threat so we can disrupt the cycle and reduce the demand for these kidnappings and forced labor.”</p><p>The concept is similar to that of ransomware attacks and digital extortion in which law enforcement encourages victims not to pay hackers’ ransom demands so they will be disincentivized to keep trying.</p><p>The Chinese government <a href="https://www.reuters.com/world/china/china-arrests-over-1100-suspects-crackdown-crypto-related-money-laundering-2021-06-10/">cracked down</a> on cryptocurrency scams beginning in 2021, but criminals have been able to move their pig butchering operations to Southeast Asian countries including Cambodia, Laos, Malaysia, and Indonesia. Governments around the world have <a href="https://www.fbi.gov/contact-us/field-offices/portland/news/press-releases/fbi-oregon-tech-tuesday-building-a-digital-defense-against-a-new-cryptocurrency-scam-pig-butchering">increasingly</a> been <a data-offer-url="https://www.ic3.gov/Media/Y2022/PSA221003" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ic3.gov/Media/Y2022/PSA221003&quot;}" href="https://www.ic3.gov/Media/Y2022/PSA221003" rel="noopener" target="_blank">warning</a> about the threat. In 2021, the FBI’s Internet Crime Complaint Center <a data-offer-url="https://www.ic3.gov/Media/PDF/AnnualReport/2021_IC3Report.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ic3.gov/Media/PDF/AnnualReport/2021_IC3Report.pdf&quot;}" href="https://www.ic3.gov/Media/PDF/AnnualReport/2021_IC3Report.pdf" rel="noopener" target="_blank">received</a> more than 4,300 submissions related to pig butchering scams, totaling more than $429 million in losses. And at the end of November, the US Department of Justice <a href="https://www.justice.gov/usao-edva/pr/court-authorizes-seizure-domains-used-furtherance-cryptocurrency-pig-butchering-scheme">announced</a> that it had seized seven domain names used in pig butchering scams in 2022.</p><p>“In this scheme, fraudsters, posing as highly successful traders in cryptocurrency, entice victims to make purported investments in cryptocurrency providing fictitious returns to encourage additional investments,” the FBI <a data-offer-url="https://www.ic3.gov/Media/Y2022/PSA221003" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ic3.gov/Media/Y2022/PSA221003&quot;}" href="https://www.ic3.gov/Media/Y2022/PSA221003" rel="noopener" target="_blank">said</a> in an October alert.</p><p>Government officials and researchers emphasize that public education is a key component of helping people avoid becoming the victim of a pig butchering scheme. If people know the telltale signs and understand the concepts underlying the scams, they are less likely to be ensnared. The challenge, they say, is reaching the wider public and getting people who learn about pig butchering to pass on the information to others in their families and social circles.&nbsp;</p><p>As with romance scams and other highly personal and exploitative attacks, researchers say that pig butchering scams take an enormous psychological toll on victims in addition to their financial toll. And the <a href="https://www.propublica.org/article/human-traffickers-force-victims-into-cyberscamming">use of forced labor</a> to carry out pig butchering schemes adds yet another layer of trauma and creates even more urgency to addressing the threat.</p><p>“Some of the stories you hear from victims—it eats you up,” says Ronnie Tokazowski, a longtime business email compromise and pig butchering researcher and principal threat advisor at the cybersecurity firm Cofense. “It eats you up really freaking bad.”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Surgeons transplant pig kidney into a patient (101 pts)]]></title>
            <link>https://www.nytimes.com/2024/03/21/health/pig-kidney-organ-transplant.html</link>
            <guid>39778415</guid>
            <pubDate>Thu, 21 Mar 2024 13:42:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/03/21/health/pig-kidney-organ-transplant.html">https://www.nytimes.com/2024/03/21/health/pig-kidney-organ-transplant.html</a>, See on <a href="https://news.ycombinator.com/item?id=39778415">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/03/21/health/pig-kidney-organ-transplant.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Difftastic, a structural diff tool that understands syntax (875 pts)]]></title>
            <link>https://difftastic.wilfred.me.uk/</link>
            <guid>39778412</guid>
            <pubDate>Thu, 21 Mar 2024 13:42:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://difftastic.wilfred.me.uk/">https://difftastic.wilfred.me.uk/</a>, See on <a href="https://news.ycombinator.com/item?id=39778412">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h2>
          a structural diff tool that understands syntax
        </h2>
        
      </div><div>
      <p>
        Difftastic is a CLI diff tool that compares files based on their syntax,
        not line-by-line. Difftastic produces accurate diffs that are easier for
        humans to read.
      </p>

      <div>
        <div>
            <h2>
              <span>Understand</span>
              What Actually Changed
            </h2>

            <p><img src="https://difftastic.wilfred.me.uk/home_img/add_wrap.png" alt="screenshot">
            </p>
            <p>
              Difftastic parses your code with
              <a href="https://tree-sitter.github.io/tree-sitter/">tree-sitter</a>. Unlike a line-oriented text diff, difftastic understands that
              the inner expression hasn't changed here.
            </p>
          </div>

        <div>
            <h2><span>Ignore</span> Formatting Changes</h2>

            <p><img src="https://difftastic.wilfred.me.uk/home_img/reformat.png" alt="screenshot">
            </p>
            <p>
              Has your code formatter decided to split something over multiple
              lines? Difftastic can show what has actually changed.
            </p>
          </div>

        <div>
            <h2><span>Visualise</span> Wrapping Changes</h2>

            <p><img src="https://difftastic.wilfred.me.uk/home_img/wrap_struct.png" alt="screenshot">
            </p>
            <p>
              Have you added a wrapper? Difftastic can match the delimiters
              exactly.
            </p>

            <p><img src="https://difftastic.wilfred.me.uk/home_img/change_wrap.png" alt="screenshot">
            </p>
            <p>
              Even if you change the inner content, difftastic can still show
              you the additional wrapper.
            </p>
          </div>

        <div>
            <h2>
              <span>Real</span>
              Line Numbers
            </h2>

            <p><img src="https://difftastic.wilfred.me.uk/home_img/line_numbers.png" alt="screenshot">
            </p>
            <p>
              Do you know how to read
              <code>@@ -5,6 +5,7 @@</code> syntax? Difftastic shows the actual
              line numbers from your files, both before and after.
            </p>
          </div>
      </div>

      <div>
          <h2>60 Second <span>Demo</span></h2>

          
        </div>

      <!-- https://devicon.dev/ provides SVG logos. -->
      <div>
            <h2>Programming Languages</h2>

            <div>
              <div>
                <h3>
                  C++
                </h3>
              </div>

              <div>
                <p>
                  <h3>
                    C#
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Clojure
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Dart
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Erlang
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Go
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Haskell
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Java
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    JavaScript
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Kotlin
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Lisp
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Lua
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    OCaml
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    PHP
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Python
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    R
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Ruby
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Rust
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    Scala
                  </h3>
                </p>
              </div>

              <div>
                <p>
                  <h3>
                    TypeScript
                  </h3>
                </p>
              </div>

              <!-- end languages -->
            </div>

            <p>
              And more! See the full
              <a href="https://difftastic.wilfred.me.uk/languages_supported.html">list of supported languages</a>
              in the manual.
            </p>
          </div>

      

      <div>
          <h2>Fully <span>Open Source</span></h2>
          <p>
            Difftastic is
            <a href="https://github.com/Wilfred/difftastic/blob/master/LICENSE">MIT licensed</a>. Download it, modify it, share it with your friends!
          </p>
        </div>

      <p>
        <em>Made with Emacs and coffee by
          <a href="https://github.com/wilfred/">Wilfred Hughes</a>.</em>
      </p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Research shows plant-based polymers can disappear within seven months (178 pts)]]></title>
            <link>https://today.ucsd.edu/story/biodegradable-microplastics</link>
            <guid>39777898</guid>
            <pubDate>Thu, 21 Mar 2024 12:50:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://today.ucsd.edu/story/biodegradable-microplastics">https://today.ucsd.edu/story/biodegradable-microplastics</a>, See on <a href="https://news.ycombinator.com/item?id=39777898">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">



		
	
		


	<section id="feature-detail-hero">
		<p>   
			
					
			
			
			<h2>Research shows plant-based polymers can disappear within seven months</h2>
			
		</p>
		
		
		
			
				<!-- This hero holds a single image -->
				<div id="slideshow">
					<figure data-slideshow-item="" data-slideshow-image-src="https://today.ucsd.edu/news_uploads/microplastics_teaser.jpg" data-slideshow-image-alt="tiny colorful bits of plastic floating against a blue background" data-slideshow-image-caption="Microplastics can take anywhere from 100 to 1,000 years to break down. Much is still unknown about their impacts on the environment and human health. (cr: Whitehoune/iStock)">
						<p><img data-src="https://today.ucsd.edu/news_uploads/_social/microplastics_teaser.jpg" alt="tiny colorful bits of plastic floating against a blue background" width="1200" height="628" src="https://today.ucsd.edu/news_uploads/_social/microplastics_teaser.jpg">
						</p>
						
					   <figcaption>
					     Microplastics can take anywhere from 100 to 1,000 years to break down. Much is still unknown about their impacts on the environment and human health. (cr: Whitehoune/iStock)
					   </figcaption>
					   
					</figure>
				</div>
				
			
			
			
			
			
			
	</section>
	
		<section id="wysiwyg">
    
  
  
    
	 <!-- START DATE STORIES IN NEW FORMAT -->
	 
	 <!-- START OF AUTHORS-BLOCK FOR MOBILE  -->
	 
	 <!-- END OF AUTHORS-BLOCK FOR MOBILE -->
	 
	 

		  <!-- START NEW CONTENT BLOCK -->
		  
		    <!-- START IF COPY -->
		   
		      <div>
							
							
							
						   <p><span><span>Microplastics are tiny, nearly indestructible fragments shed from everyday plastic products. As we learn more about microplastics, the news keeps getting worse. Already well-documented in our oceans and soil, we’re now discovering them in the unlikeliest of places: our arteries, lungs and even placentas. </span></span></p>

<p><span><span>Microplastics can take anywhere from 100 to 1,000 years to break down and, in the meantime, our planet and bodies are becoming more polluted with these materials every day.</span></span></p>

<p><span><span>Finding viable alternatives to traditional petroleum-based plastics and microplastics has never been more important. New research from scientists at the University of California San Diego and materials-science company Algenesis shows that their plant-based polymers biodegrade — even at the microplastic level — in under seven months. The paper, whose authors are all UC San Diego professors, alumni or former research scientists, appears in </span></span><a href="https://doi.org/10.1038/s41598-024-56492-6"><span><span><span><span>Nature Scientific Reports</span></span></span></span></a><span><span>.</span></span></p>

<p><span><span>“</span></span><span><span>We're just starting to understand the implications of microplastics. We've only scratched the surface of knowing the environmental and health impacts,” stated Professor of Chemistry and Biochemistry Michael Burkart, one of the paper’s authors and an Algenesis co-founder. “We're trying to find replacements for materials that already exist, and make sure these replacements will biodegrade at the end of their useful life instead of collecting in the environment. That's not easy.”</span></span></p>
											
						</div>
		      
		    <!-- END IF COPY -->
		    
		    <!-- START OPTIONAL STAND ALONE IMAGE -->
		    
		    <!-- END OPTIONAL STAND ALONE IMAGE -->
		  
		    <!-- START IF COPY -->
		   
		      <div>
									<p><span><span>“</span></span><span><span>When we first created these algae-based polymers about six years ago, our intention was always that it be completely biodegradable,” said another of the paper’s authors, Robert Pomeroy, who is also a professor of chemistry and biochemistry and an Algenesis co-founder. “We had plenty of data to suggest that our material was disappearing in the compost, but this is the first time we’ve measured it at the microparticle level.”</span></span></p>

<h3><span><span>Putting it to the test</span></span></h3>

<p><span><span>To test its biodegradability, the team ground their product into fine microparticles, and used three different measurement tools to confirm that, when placed in a compost, the material was being digested by microbes.</span></span></p>

<p><span><span>The first tool was a respirometer. When the microbes break down compost material, they release carbon dioxide (CO2), which the respirometer measures. These results were compared to the breakdown of cellulose, which is considered the industry standard of 100% biodegradability. The plant-based polymer matched the cellulose at almost one hundred percent.</span></span></p>
													
								</div>
		      
		    <!-- END IF COPY -->
		    
		    <!-- START OPTIONAL STAND ALONE IMAGE -->
		    
		    <!-- END OPTIONAL STAND ALONE IMAGE -->
		  
		    <!-- START IF COPY -->
		   
		      
		      
		    <!-- END IF COPY -->
		    
		    <!-- START OPTIONAL STAND ALONE IMAGE -->
		    
		    	
		    		
							<!-- MEDIUM WIDTH IMAGE -->
							<div>
										<!-- 
  This figure is meant to be part of a Article/Feature Detail page. It provides 
  data attributes for the slideshow carousel script to target the image and
  caption for dynamically pulling into the slideshow modal
  - Supported variables
  -- image-src
  -- image-alt
  -- image-caption
  -- image-size
-->

	<figure data-slideshow-item="" data-image-size="lg" data-slideshow-image-src="https://today.ucsd.edu/news_uploads/microplastics_inset_2.jpg" data-slideshow-image-alt="graph showing decomposition of plastic and biodegradable plastic" data-slideshow-image-caption="<div>Particle counts of petroleum-based (EVA) and plant-based (TPU-FC1) microplastics show that, over time, EVAs exhibit virtually no biodegradation, while the TPUs have mostly disappeared by day 200.&nbsp;</div>">
	  <img data-src="https://today.ucsd.edu/news_uploads/microplastics_inset_2.jpg" alt="graph showing decomposition of plastic and biodegradable plastic" width="705" height="470" src="https://today.ucsd.edu/news_uploads/microplastics_inset_2.jpg">
	  <figcaption>
	    <p>Particle counts of petroleum-based (EVA) and plant-based (TPU-FC1) microplastics show that, over time, EVAs exhibit virtually no biodegradation, while the TPUs have mostly disappeared by day 200.&nbsp;</p>
	  </figcaption>
	</figure>
	
									</div>
			      
		      
		    
		    <!-- END OPTIONAL STAND ALONE IMAGE -->
		  
		    <!-- START IF COPY -->
		   
		      <div>
							
						   <p><span><span>Next the team used water flotation. Since plastics are not water soluble and they float, they can easily be scooped off the surface of water. At intervals of 90 and 200 days, almost 100% of the petroleum-based microplastics were recovered, meaning none of it had biodegraded. On the other hand, after 90 days, only 32% of the algae-based microplastics were recovered, showing that more than two thirds of it had biodegraded. After 200 days, only 3% was recovered indicating that 97% of it had disappeared.</span></span></p>

<p><span><span>The last measurement involved chemical analysis via gas chromatography/mass spectrometry (GCMS), which detected the presence of the monomers used to make the plastic, indicating that the polymer was being broken to its starting plant materials. Scanning-electron microscopy further showed how microorganisms colonize the biodegradable microplastics during composting.&nbsp;&nbsp;</span></span></p>

<p><span><span>“This material is the first plastic demonstrated to&nbsp;</span></span><span><span>not</span></span><span><span>&nbsp;create microplastics as we use it,” said Stephen Mayfield, a paper coauthor, School of Biological Sciences professor and co-founder of Algenesis. “This is more than just a sustainable solution for the end-of-product life cycle and our crowded landfills. This is actually plastic that is&nbsp;</span></span><span><span>not</span></span><span><span>&nbsp;going to make us sick.”</span></span></p>

<p><span><span>Creating an eco-friendly alternative to petroleum-based plastics is only one part of the long road to viability. The ongoing challenge is to be able to use the new material on pre-existing manufacturing equipment that was originally built for traditional plastic, and here Algenesis is making progress. They have partnered with several companies to make products that use the plant-based polymers developed at UC San Diego, including Trelleborg for use in coated fabrics and RhinoShield for use in the production of cell phone cases.&nbsp;</span></span></p>

<p><span><span>“When we started this work, we were told it was impossible,” stated Burkart. “Now we see a different reality. There's a lot of work to be done, but we want to give people hope. It </span></span><span><span>is</span></span><span><span> possible.”&nbsp;</span></span></p>

<p><span><span>Full list of authors: Robert S. Pomeroy, Michael D. Burkart, Steven P. Mayfield (all UC San Diego), Marco N. Allemann, Marissa Tessman, Jaysen Reindel, Gordon B. Scofield, Payton Evans, Ryan Simkovsky (all Algenesis).</span></span></p>

<p><span><span>This research was supported by funding from the Department of Energy (DE-SC0019986 and DE-EE0009295).</span></span></p>

<p><em><span><span>Disclosure: Burkart, Mayfield and Pomeroy are co-founders of and hold equity positions in Algenesis Corporation.&nbsp;</span></span></em></p>
											
						</div>
		      
		    <!-- END IF COPY -->
		    
		    <!-- START OPTIONAL STAND ALONE IMAGE -->
		    
		    <!-- END OPTIONAL STAND ALONE IMAGE -->
		    
		  <!-- END CONTENT BLOCK -->
		  
	
	  
      
  

  <!-- START TOPICS & SHARE MOBILE  -->
  <div>
    <!--
  This renders topics, share this, and optional text copy
  - Supported variables
  -- topics
-->

	
	<!-- begin new story format -->
	
		
		
		
		<h2>Share This:</h2>
		
	<!-- end new story format -->	
		

  </div>
  <!-- END TOPICS & SHARE MOIBILE -->
</section>

	

	<section>
        <div>
    <h2>
      You May Also Like
    </h2>
    
  </div>
        
        <div>
                     
                     
                     
	                     <p><a href="https://today.ucsd.edu/story/san-diego-supercomputer-center-interns-create-app-for-uc-san-diegos-stuart-collection" alt="San Diego Supercomputer Center Interns Create App for UC San Diego’s Stuart Collection">
	                             <img srcset="https://today.ucsd.edu/news_uploads/_special-lead-mobile/PR20240318_ABLE_Stuart_Collection.jpg 350w, https://today.ucsd.edu/news_uploads/_special-lead-desk/PR20240318_ABLE_Stuart_Collection.jpg 864w" sizes="(min-width: 768px) 864px, 350px" data-src="https://today.ucsd.edu/news_uploads/_special-lead-desk/PR20240318_ABLE_Stuart_Collection.jpg" alt="San Diego Supercomputer Center Interns Create App for UC San Diego’s Stuart Collection" width="750" height="488">
	                         </a>
	                     </p>
                     
                     
                 </div>
        
        
        
        
        <div>
                        
                        
                        
	                        <p><a href="https://today.ucsd.edu/story/san-diego-supercomputer-center-interns-create-app-for-uc-san-diegos-stuart-collection" alt="San Diego Supercomputer Center Interns Create App for UC San Diego’s Stuart Collection">
	                                <img srcset="https://today.ucsd.edu/news_uploads/_special-lead-mobile/PR20240318_ABLE_Stuart_Collection.jpg 350w, https://today.ucsd.edu/news_uploads/_special-lead-desk/PR20240318_ABLE_Stuart_Collection.jpg 864w" sizes="(min-width: 768px) 864px, 350px" data-src="https://today.ucsd.edu/news_uploads/_special-lead-desk/PR20240318_ABLE_Stuart_Collection.jpg" alt="San Diego Supercomputer Center Interns Create App for UC San Diego’s Stuart Collection" width="750" height="488">
	                            </a>
	                        </p>
                        
                    </div>
    </section>
	<div id="subscribe">
    <div>
        <h2>Stay in the Know</h2>
        <p>Keep up with all the latest from UC San Diego. Subscribe
          to the newsletter today.
        </p>
      </div>
    <div>
        <form novalidate="" data-subscribe-form="" action="subscribe.html" method="post" data-form_type="newsletter_signup">
          <div>
            <p><label for="subscriber-email">
              Email
            </label>
            </p><div data-validation-message="email">
              <p>Please provide a valid email address.</p></div>
          </div>
          
        </form>
      </div>
  </div>
<!-- START Subscribe Modal -->

<!-- STOP Subscribe Modal -->
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ikigai: What We Got Wrong and How to Find Meaning in Life (182 pts)]]></title>
            <link>https://nesslabs.com/ikigai</link>
            <guid>39777896</guid>
            <pubDate>Thu, 21 Mar 2024 12:50:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nesslabs.com/ikigai">https://nesslabs.com/ikigai</a>, See on <a href="https://news.ycombinator.com/item?id=39777896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
			
<p>I lived in Japan for seven months when I was younger. For all of the challenges I faced there as a woman and a foreigner, I still learned a lot from Japanese culture.</p>



<p>Because Japan experienced a long period of relative isolation from the outside world — caused by <em>sakoku</em> (literally “closed country”), the isolationist foreign policy of the military government during the Edo period — Japanese people have developed their own unique set of values and beliefs.</p>



<p>One unique Japanese concept is the idea of <em>ikigai</em>, which can be roughly translated to <em>reason for being</em> (or “raison d’être” in my native French)<em> </em>. Each person’s ikigai is personal to them, reflective of their inner self, and creating a mental state in which they feel at ease.</p>



<p>What makes it such a powerful idea in today’s age of constant change and uncertainty is that ikigai doesn’t limit someone’s value in life to career and financial status. In fact, in a survey of 2,000 Japanese people <a href="https://www.crs.or.jp/backno/No636/6362.htm">conducted</a> by Central Research Services, only a third of respondents considered work as their ikigai.</p>



<p>Rather, ikigai is about feeling your life makes a difference in people’s lives — the idea that you can contribute to other people’s lives simply by living a fulfilling life. And this idea can unlock many benefits.</p>



<h2>The Health Benefits of Ikigai</h2>



<p>Because your ikigai is less of a theory and more of a way of living, it can have a profound impact on your mental and physical health.</p>



<ul>
<li><strong>Ikigai reduces anxiety.</strong> Research <a href="http://www.ccsenet.org/journal/index.php/gjhs/article/view/18478/12980">shows</a> that the feeling of ikigai contributes to a well-balanced secretion of neurotransmitters such as serotonin, dopamine, and endorphins, which in turn reduces the feeling of stress.</li>



<li><strong>Ikigai is good for your heart.</strong> A seven-year long study with more than 40,000 Japanese adults <a href="https://www.ncbi.nlm.nih.gov/pubmed/18596247">found</a> evidence that people with a low sense of ikigai had a higher overall mortality risk, mostly due to higher cardiovascular disease.</li>



<li><strong>Ikigai increases your self-authorship.</strong> Research <a href="https://www.autonomicneuroscience.com/article/S1566-0702(09)00479-2/abstract">suggests</a> that people without ikigai have a strong need for approval from others, while those with ikigai tend to perform tasks for their own satisfaction.</li>



<li><strong>Ikigai makes you more resilient.</strong> There’s <a href="https://link.springer.com/article/10.1023/A:1021747419204">evidence</a> that ikigai may help you go through times of hardship more easily, making you feel like it’s worthwhile to continue living. For example, it helped many Japanese people cope <a href="http://www.scirp.org/fileOperation/downLoad.aspx?path=PSYCH20110800005_56761305.pdf&amp;type=journal">during</a> the earthquake that occurred in Japan in March 2011 and <a href="https://www.researchgate.net/publication/349725406_Health_Benefits_of_Ikigai_A_Review_of_Literature">during</a> the COVID-19 pandemic.</li>



<li><strong>Ikigai helps you live longer.</strong> Another study <a href="https://www.ncbi.nlm.nih.gov/pubmed/19539820">identified</a> ikigai as a positive psychological factor contributing to longevity, with men and women with a sense of ikigai showing decreased risks of mortality from all causes.</li>
</ul>



<p>In short, there’s quite a bit of research suggesting that a sense of ikigai will contribute to your overall well-being. So, how can you leverage the power of ikigai?</p>



<h2>Leverage the Actual Power of Ikigai</h2>



<p>The concept of ikigai has often been misunderstood in the Western world largely due to the popularity of the below Venn diagram:</p>


<div>
<figure><img fetchpriority="high" decoding="async" width="1024" height="574" src="https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-1-1024x574.png" alt="Ikigai Venn Diagram 1" srcset="https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-1-1024x574.png 1024w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-1-300x168.png 300w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-1-768x431.png 768w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-1-1536x861.png 1536w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-1-2048x1148.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20574'%3E%3C/svg%3E" data-lazy-srcset="https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-1-1024x574.png 1024w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-1-300x168.png 300w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-1-768x431.png 768w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-1-1536x861.png 1536w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-1-2048x1148.png 2048w" data-lazy-src="https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-1-1024x574.png"></figure></div>


<p>This diagram was created in 2011 by astrologer Andres Zuzunaga, who designed it to help people find their purpose in life, and was then adapted by blogger Marc Winn, who replaced the word “purpose” with “ikigai” (you can read the whole story <a href="https://ikigaitribe.com/ikigai/ikigai-misunderstood/">here</a>).</p>



<p>However, the concept of ikigai is not about finding the intersection of what you love, what you’re good at, what the world needs, and what you can get paid for. It’s just about finding pleasure in life and being happy to get up in the morning.</p>



<p>“Japanese people don’t view ikigai as a lofty goal, a destination, or something to achieve,” explains Nicholas Kemp, author of the book <em>Ikigai-Kan</em>. Similarly, in The Little Book of Ikigai, Ken Mogi wrote: “Japanese do not need grandiose motivational frameworks to keep going, but rely more on the little rituals in their daily routines.”</p>


<div>
<figure><img decoding="async" width="1024" height="575" src="https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-2-1024x575.png" alt="Ikigai Venn Diagram 2" srcset="https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-2-1024x575.png 1024w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-2-300x168.png 300w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-2-768x431.png 768w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-2-1536x862.png 1536w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-2-2048x1150.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20575'%3E%3C/svg%3E" data-lazy-srcset="https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-2-1024x575.png 1024w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-2-300x168.png 300w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-2-768x431.png 768w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-2-1536x862.png 1536w, https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-2-2048x1150.png 2048w" data-lazy-src="https://nesslabs.com/wp-content/uploads/2024/03/ikigai-venn-diagram-2-1024x575.png"></figure></div>


<p>Your ikigai can be found in small daily rituals, side projects, and deep conversations. It can be found in moments of silence and idleness, or in moments of creative flow. To find your ikigai, forget about the westernized version and instead follow these principles:</p>



<ol>
<li><strong>Stop seeking your One True Passion.</strong> Many of us think that finding our passion will magically give our life a purpose. Instead, find meaning in your daily experiences and interactions. Explore the world around and inside you. Learn something new everyday, including about yourself. Play with uncertainty instead of chasing the <a href="https://nesslabs.com/the-paradox-of-goals">next milestone</a>.</li>



<li><strong>Embrace lifelong learning.</strong> The concept of ikigai never mentions being good at what you do. There is joy in being a beginner all over again, learning through mistakes, and growing outside of your comfort zone. Don’t try to be the expert in the room. Keep <a href="https://nesslabs.com/generative-questions">asking questions</a>. Never stop learning.</li>



<li><strong>Let go of lofty financial goals.</strong> Ikigai also doesn’t have anything to do with money. Of course, we all need enough money to live a comfortable life, and money can help explore projects and ideas that bring you pleasure in life, but beyond the point of comfort, financial success should be seen as a potential byproduct of living a meaningful life.</li>



<li><strong>Don’t try to save the world.</strong> Instead, focus on the positive impact you can have on your friends, family, colleagues, and community. Ask yourself how you can connect with people in meaningful ways and which changes you want to bring to life. This is how we save the world — when everyone contributes at their own human scale.</li>
</ol>



<p>As psychiatrist Mieko Kamiya <a href="https://www.amazon.co.jp/%E7%94%9F%E3%81%8D%E3%81%8C%E3%81%84%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6-%E7%A5%9E%E8%B0%B7%E7%BE%8E%E6%81%B5%E5%AD%90%E3%82%B3%E3%83%AC%E3%82%AF%E3%82%B7%E3%83%A7%E3%83%B3-%E7%A5%9E%E8%B0%B7-%E7%BE%8E%E6%81%B5%E5%AD%90/dp/4622081814">puts it</a>, ikigai is closer in Japanese to the “power necessary to live in this world” or the “happiness to be alive”, which unfortunately is often translated to “a life worth living” in English, when the original concept doesn’t ascribe measurable value to our lives.</p>



<p>Instead of pursuing a grand life purpose, optimize for wanting to wake up in the morning. Live a life of curiosity and connection. Trust that success will be a byproduct of the meaning you find in daily experiences.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Response from Flipper Zero to the Canada Ban (113 pts)]]></title>
            <link>https://twitter.com/flipper_zero/status/1770459769452589468</link>
            <guid>39777746</guid>
            <pubDate>Thu, 21 Mar 2024 12:35:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/flipper_zero/status/1770459769452589468">https://twitter.com/flipper_zero/status/1770459769452589468</a>, See on <a href="https://news.ycombinator.com/item?id=39777746">Hacker News</a></p>
Couldn't get https://twitter.com/flipper_zero/status/1770459769452589468: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: An AI-Powered WordPress Site Builder That We Are Open-Sourcing Today (170 pts)]]></title>
            <link>https://themeisle.com/blog/we-are-open-sourcing-our-ai-site-builder/</link>
            <guid>39777528</guid>
            <pubDate>Thu, 21 Mar 2024 12:11:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://themeisle.com/blog/we-are-open-sourcing-our-ai-site-builder/">https://themeisle.com/blog/we-are-open-sourcing-our-ai-site-builder/</a>, See on <a href="https://news.ycombinator.com/item?id=39777528">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-99801">
						
<p>A couple of weeks back, we released the prototype of <a href="https://quickwp.ai/" target="_blank" rel="noopener">QuickWP</a>, an AI-powered WordPress site builder that uses <strong>OpenAI</strong>, an <strong>FSE theme</strong>, and <strong>WordPress Playground</strong> to generate a personalized theme for the user based on the topic and description of your website.</p>



<span id="more-99801"></span>



<p>If you haven’t checked it out yet, you can see <a href="https://twitter.com/HardeepAsrani/status/1760336196264251746" target="_blank" rel="noopener">the preview of QuickWP</a> on Twitter (aka X).</p>



<figure><img decoding="async" width="1024" height="585" src="https://mllj2j8xvfl0.i.optimole.com/cb:jC7e.37109/w:1024/h:585/q:90/f:best/https://themeisle.com/blog/wp-content/uploads/2024/03/QuickWP-Preview.webp" alt="QuickWP - an AI-powered WordPress site builder" data-opt-src="https://mllj2j8xvfl0.i.optimole.com/cb:jC7e.37109/w:1024/h:585/q:90/f:best/https://themeisle.com/blog/wp-content/uploads/2024/03/QuickWP-Preview.webp" data-old-src="https://mllj2j8xvfl0.i.optimole.com/cb:jC7e.37109/w:1024/h:585/q:eco/f:best/https://themeisle.com/blog/wp-content/uploads/2024/03/QuickWP-Preview.webp"></figure>



<p>Building QuickWP has been a challenging and learning experience for us, and today, we are open-sourcing the code base for the project so you can also learn from it and maybe even build something awesome upon it.</p>



<p>In this article, I will discuss the ideas, challenges, and things we learned by working on QuickWP. I hope this helps you if you ever face similar challenges.</p>



		
			






<h2 id="h-the-idea">The idea</h2>



<p>While we have thought of experimenting with AI and OpenAI APIs for a while, we never planned to create an AI website builder. Previously, we tried integrating AI with the <a href="https://themeisle.com/plugins/otter-blocks/?utm_source=themeisle&amp;utm_medium=themeisle_blog&amp;utm_campaign=we-are-open-sourcing-our-ai-site-builder">Otter Blocks</a> plugin to generate layouts from available patterns using AI prompt, but that implementation was quite primitive. The results were very generic and did not consider user context much in the provided result.</p>



<p>Given that patterns in Block Editor are easy to break even with minor changes, we could not simply ask GPT to create patterns on the fly or even ask it to replace content.</p>



<p>It all changed when we thought of this idea based on wireframes. It is simple: we create an FSE theme with wireframes and extensive color palettes. And then, with AI, we pick the patterns based on user prompts.</p>



<p>In FSE themes, using the <a href="https://developer.wordpress.org/themes/global-settings-and-styles/" target="_blank" rel="noopener">theme.json</a> file properties, we can easily modify the styling of the entire website from one place. And the same is applied to our patterns so that we have uniformity across the website without worrying about different patterns having different settings that need to be modified separately.</p>



<p>Here, we also use a CC0 image directory to populate the website with images to give a better starting point to the user.</p>



<p>While the idea sounds simple enough, it required some trials and errors for us to reach the point where it could generate results that were good enough for the user. The goal was to spend as little time as possible to create a prototype that users can use as a SaSS from the product website.</p>






<h2 id="h-overview-of-project-stack">Overview of project stack</h2>



<p>The project required more than one part, so we used a number of stacks, i.e., whatever made it easier for us to prototype as quickly as possible.</p>



<p>Here are the various parts of the project:</p>



<ul>
<li><a href="https://github.com/Codeinwp/quickwp-theme" target="_blank" rel="noopener">FSE Theme</a>: The base of the project. It includes various patterns and a comprehensive theme.json file.</li>



<li><a href="https://github.com/Codeinwp/quickwp" target="_blank" rel="noopener">Base Plugin</a>: This plugin has all the functionality and UI required to make the project work.</li>



<li><a href="https://github.com/Codeinwp/quickwp-api" target="_blank" rel="noopener">API Endpoint</a>: An API endpoint communicating between the user website and OpenAI API.</li>
</ul>



<figure><img decoding="async" width="972" height="793" src="https://mllj2j8xvfl0.i.optimole.com/cb:jC7e.37109/w:972/h:793/q:90/f:best/https://themeisle.com/blog/wp-content/uploads/2024/03/QuickWP-Diagram.png" alt="QuickWP Diagram" data-opt-src="https://mllj2j8xvfl0.i.optimole.com/cb:jC7e.37109/w:972/h:793/q:90/f:best/https://themeisle.com/blog/wp-content/uploads/2024/03/QuickWP-Diagram.png" data-old-src="https://mllj2j8xvfl0.i.optimole.com/cb:jC7e.37109/w:972/h:793/q:eco/f:best/https://themeisle.com/blog/wp-content/uploads/2024/03/QuickWP-Diagram.png"></figure>



<p>Here is a simplified diagram to show the entire workflow.</p>






<h2 id="h-fse-theme">FSE theme</h2>



<p>The FSE theme works as the base of the entire project. To make prototyping easier, we started with a fork of the Twenty Twenty-Four theme. We pretty much removed all the patterns and customized the theme.json properties as per our needs.</p>



<p>FSE theme best practices are changing very quickly, and with each version of WordPress, we have a new way of doing things. Starting with the fork of the default theme allows us to build upon a solid foundation with minimal work.</p>



<p>In terms of code, most of the things are as you would expect in an FSE theme. The only difference you will notice is <a href="https://github.com/Codeinwp/quickwp-theme/blob/main/patterns/hero-cover.php" target="_blank" rel="noopener">how we use strings and images in patterns</a>.</p>



<p>Here, we add default text, template-specific namespace for the strings, and a default preview namespace to each string.</p>



<p>The default text is the text that will appear in the patterns when used normally, in case someone is adding a pattern inside the editor or using the theme without QuickWP AI.</p>



<p>The template-specific namespace is an identifier for that particular string. And the default preview namespace is a shared namespace that we use for all the strings in context. We will come back to this later.</p>






<h2 id="h-ai-prompt-generation">AI prompt generation</h2>



<p>As it was a quick prototype, we wanted to explore easier testing and implementation methods. We experimented with various AI models but ended up with the most popular option, which is OpenAI. During the development phase, we used GPT-4 as the results were much better with OpenAI’s latest model offering, but it was too costly, so we decided to shift to using GPT-3.5 Turbo for most tasks. I say most of the tasks as we are still using GPT-4 for color palette generation as the color variety was not great with GPT-3.5</p>



<p>For making requests, we tried different options that OpenAI offers but found the Assistant API best suited for our needs. To avoid some bad-faith actors, we also used OpenAI’s Moderation API to prevent processing the requests if they do not align with OpenAI’s content policies. As we can see after the release, people have tried to experiment with all sorts of prompts that could have landed our OpenAI account in trouble, so adding the moderation was worth the time. And yes, it is free to use!</p>






<h2 id="h-image-generation">Image generation</h2>



<p>When we were imagining this project, one of the issues was how to generate images. We could, of course, use Dall-E or other models to do it, but they’re slow, low-quality, and quite expensive. It turned out that we were thinking in the wrong direction. Why generate images when there are millions and millions of CC0 images available on the internet?</p>



<p>After some consideration, we chose <a href="https://www.pexels.com/" target="_blank" rel="noopener">Pexels</a>. The reason behind choosing Pexels was that it has more liberal request limits and a good catalog of images. And, of course, we link back to the original image on our app.</p>






<h2 id="h-how-do-you-maintain-context-site-wide">How do you maintain context site-wide?</h2>



<p>The first problem we needed to solve to make this project work was to see how we could maintain context site-wide when generating content for the user. Different patterns have different numbers and types of strings, and we can’t just randomly add content there and hope it will be relevant for the website.</p>



<p>And this is where our great friend JSON came to the rescue. With some creative prompts (found in the source code) and a consistent JSON schema, we could maintain context throughout the website and have strings that complement each other, rather than random gibberish.</p>



<p>If you look at one of our templates, you will see how we list each pattern with a description to let the API know its purpose and what strings it contains.</p>



<p>For example, here’s the first pattern from that template:</p>



<pre><code>
{
    "order": 1,
    "slug": "quickwp/hero-centered",
    "name": "Hero Centered",
    "description": "Hero sections are used to introduce the product or service. They are the first and primary section of the website. This is a centered hero section with a large title, a subtitle and two buttons.",
    "category": "heroes_page_titles",
    "strings": [
        {
            "slug": "hero-centered/title",
            "description": "Main title of the hero section"
        },
        {
            "slug": "hero-centered/subtitle",
            "description": "Subtitle of the hero section"
        },
        {
            "slug": "hero-centered/button-primary",
            "description": "Primary button text of the hero section"
        },
        {
            "slug": "hero-centered/button-secondary",
            "description": "Secondary button text of the hero section"
        }
    ],
    "images": [
        {
            "slug": "hero-centered/image",
            "description": "Background image of the hero section"
        }
    ]
}</code></pre>



<p>Each string, along with the namespace, also describes its connection to the rest of the pattern. This allows us to make sure that GPT does not repeat the same thing in multiple places and, for example, keeps the subtitle related to the title of the pattern.</p>



<p>When we get the request back on the site, we use the string slug to <a href="https://github.com/Codeinwp/quickwp-theme/blob/main/patterns/hero-centered.php#L17" target="_blank" rel="noopener">replace it in the pattern</a>.</p>



<p>While our current implementation is primitive, you can use this approach to give even more context to the string, such as the length and tone of the string. This way, we only exchange the data and not the markup.</p>






<h2 id="h-we-need-wordpress-instances-for-each-user">We need WordPress instances for each user</h2>



<p>Another problem we needed to solve was to have an instance of WordPress for each user session. In our implementation, we are making changes live on the WordPress instance of the current user and then using existing WordPress functionality to export the FSE theme. Only if there was a solution to create WordPress instances without pretty much building a small web hosting solution…</p>



<p>Let me introduce you to <a href="https://wordpress.org/playground/" target="_blank" rel="noopener">WordPress Playground</a>. Playground allows you to run WordPress in your browser with zero clicks. If you have not used the WP Playground, you will be surprised at how awesome it is!</p>






<h2 id="h-what-will-you-be-building-with-wordpress">What will you be building with WordPress?</h2>



<p>Now that we have walked you through some of the challenges we faced, what will you be building with these tools? We hope the article inspired you to use some of the tools we discussed, like OpenAI API, FSE themes, and WordPress Playground, and build something awesome. If you do, let us know because we would love to try it!</p>



<p>Once again, all the source code is available <a href="https://github.com/Codeinwp/quickwp" target="_blank" rel="noopener">on our GitHub</a>, so feel free to use it in any way it can help you!</p>


<div>
		<h5>Free guide</h5>
		<h3>4 Essential Steps to Speed Up<br>Your WordPress Website</h3>
		<h4>Follow the simple steps in our 4-part mini series<br>and reduce your loading times by 50-80%.&nbsp;🚀</h4>
		<p><a href="#cb2c164e38">Free Access</a>
	</p></div>

    					</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The baffling intelligence of a single cell: The story of E. coli chemotaxis (412 pts)]]></title>
            <link>https://jsomers.net/e-coli-chemotaxis/</link>
            <guid>39777229</guid>
            <pubDate>Thu, 21 Mar 2024 11:29:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jsomers.net/e-coli-chemotaxis/">https://jsomers.net/e-coli-chemotaxis/</a>, See on <a href="https://news.ycombinator.com/item?id=39777229">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        <p>
          I want to tell the story of a beautiful phenomenon in biology. In some
          sense it’s the prototype of much of the activity of life. The
          phenomenon is the way in which an individual cell of
          <em>E. coli</em> forages for nutrients. This process, known as
          “chemotaxis”—the “chemo-” for chemical and the “taxis” from the Greek
          τάξις, for tactics—is intelligence in one of
          its most elemental forms. An individual <em>E. coli</em> has no brain,
          obviously, and is even many orders of magnitude simpler than a human
          cell, and yet already it possesses something like a sense of smell,
          drive, even a memory. Chemotaxis recasts <em>E. coli</em> not as some
          aimless gut-pest but rather as an exquisitely sophisticated
          <a href="https://yalebooks.yale.edu/book/9780300167849/wetware/#:~:text=This%20book%20offers%20a%20startling,is%20a%20form%20of%20computation.">physical computer</a>.
        </p>
        <p>
          I’m also telling this story because
          <a href="https://jsomers.net/i-should-have-loved-biology">I never liked the way biology was taught in high school</a>. It was too much about the names of things. A subject so vast is
          spoiled by a textbook, which can only point at the endless parade of
          stuff-there-is-to-know. It’s better approached with questions—like
          “what’s happening when you smell?” or ”what is a fever,
          actually?”—that contemplate narrow, deep slices.
        </p>

        <p>
          Chemotaxis is a great slice: it’s a triumph of systems biology—we
          understand it holistically but also in fine detail at almost every
          level. It acquaints you with many of the most important motifs in
          biology, including the way in which protein structure determines
          function; how membranes control the information flow into cells; and
          how chemical modifications store and communicate state. It involves
          one of the most sophisticated and beautiful pieces of molecular
          nanotechnology, the flagellar motor. And it helps give an intuition
          for how a bag of unthinking chemicals could possibly give rise to a
          being.
        </p>
        <h3>The 30,000-foot view</h3>
        <figure>
          
          <figcaption>
            Even with simple rules, the E. coli finds food more often than not
          </figcaption>
        </figure>
        <p>
          The basic idea is this: <em>E. coli</em> “smells” chemicals it’s
          attracted to with a set of nose-like receptors and decides how to
          swim. Depending on what it senses, it can either use its flagellar
          tails to swim forward—this is known as a “run”—or it can spin in a
          random direction (a “tumble”). By running when the getting is good and
          tumbling when it isn’t, the <em>E. coli</em> takes a meandering path
          toward the attractant.
        </p>
        <p>
          A little more detail now: there are half a dozen or so rotors on the
          <em>E. coli’s</em> body, each controlling a long whip-like tail that
          flows behind it. When all the rotors are spinning in the same
          direction, the tails join together into a coil that torques the cell
          forward into a run. When even one rotor is spinning against the
          others, the coil unbundles and <em>E. coli</em> spins into a tumble.
        </p>
        <p>
          In a uniform chemical environment, the <em>E. coli</em> swims in a
          random walk by balancing runs with periodic tumbles. By default, a run
          lasts about a second, or ten times longer than a tumble. The rate of
          runs versus tumbles, and their relative duration, is carefully tuned
          to balance “exploitation” and “exploration”: if runs happened too
          often or lasted too long, <em>E. coli</em> would range too widely and
          zip past its food; too seldom or too short, and it’d likely never find
          food in the first place.
        </p>
        <p>
          But how is this balance achieved? The crux of it is a signaling
          molecule called CheY (pronounced “KEY-why”). CheY is constantly
          bouncing around in the cytoplasm of the <em>E. coli</em>, interacting
          with both the receptor complex (the “nose”) and the rotors, carrying
          information between them. In the steady state, when CheY encounters
          the receptor complex, it gets chemically modified, or
          “phosphorylated,” at a certain rate to become CheY-p. Unlike the
          unmodified version, CheY-p has a strong affinity for the rotors, and
          when enough copies bind to one, it reverses its spin, causing a
          tumble.
        </p>

        <p>
          The trick is that when the nose detects an <em>increase</em> in the
          concentration of attractant, that steady turning of CheYs into CheY-ps
          is interrupted. As a result, fewer CheY-ps bind to the rotors; fewer
          reversals take place; and so the <em>E. coli</em> runs more and
          tumbles less. In other words, the all-important relative rate of runs
          versus tumbles is determined entirely by how often the
          CheY→CheY-p process churns—and this, in turn, is determined by
          how much attractant is detected by the nose.
        </p>
        <p>
          You can see this process in action in the interactive illustration.
          Try altering the ratio of CheY-p (white) to CheY (blue) by adding some
          attractant (pink). You‘ll end up inducing a long run.
        </p>
        
        <div>
          <p>Less attractant</p>
          <p>More attractant</p>
        </div>
        <p>
          Why do you need all this complexity? You could imagine a system in
          which the motors themselves responded directly to attractant. We’ll
          see later on that the stream of CheY-ps acts as a kind of adaptable,
          tunable chemical amplifier. “<a href="https://www.ks.uiuc.edu/Research/chemotaxis/#:~:text=In%20particular%2C%20bacterial,per%20cell%20volume!">Bacterial cells can amplify</a>
          signals more than 50-fold; that is to say, a 2% change in receptor
          occupancy can bring about a 100% change in the output of the system at
          the flagellar motors. This feature allows cells to sense minute
          changes in concentration—less than three molecules per cell volume!”
        </p>
        <h3>
          The story gets more complicated: adaptation
        </h3>
        <p>
          If the system were as described above, then
          <em>E. coli</em> wouldn’t have much dynamic range. Imagine: if the
          cell has a huge reaction to just three molecules of attractant,
          wouldn’t a thousand times as many just completely overwhelm it?
        </p>
        <p>
          In reality, the <em>E. coli</em> is able to respond sensitively across
          <a href="https://www.youtube.com/watch?t=58&amp;v=cT855rpX8bc&amp;feature=youtu.be">five orders of magnitude</a>
          of attractant concentration. The cell learns to treat whatever
          concentration it stumbles into as the new normal, so that the
          slightest increase triggers the same hypersensitive response as
          always.
        </p>
        <p>
          The mechanism powering this adaptation is extremely clever. You can
          think of each receptor as being equipped with “struts” that have
          pockets in them. When the receptor is bound to attractant, its struts
          change shape so that these pockets open up, and become the targets for
          little molecules known as methyl groups. Methyl groups are ubiquitous
          in biochemistry: for instance, they help determine which parts of your
          DNA get expressed. Methyl groups bind to the structural proteins your
          DNA strands coil around, called histones; the “methylated” histone can
          kink the DNA strand into or out of view of your transcription
          machinery, turning it on or off.
        </p>
        <p>
          In this case, methylation serves to fill up the strut’s pockets,
          causing it to become more rigid. (I’m simplifying the actual physical
          details somewhat, as we’ll see later.) With more rigid struts the
          receptor’s signaling power is dampened: it takes more attractant to
          elicit the same response. Because there are many methylation sites per
          strut and many struts per receptor, there’s a wide range of possible
          dampening values—as if those pockets were really the holes of an
          elaborate wind instrument.
          <span>[<a id="bray-maybe-referent" href="#bray-maybe">Bray</a>]</span>
          This wide dynamic range is what allows the bacteria not just to find a
          favorable environment but to keenly and speedily nose its way up a
          chemical gradient. No wonder a similar mechanism is used by cells in
          your immune system to track and hunt down invaders.
        </p>
        <p>
          Methylation of the receptors gives <em>E. coli</em> a “<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2899694/#:~:text=The%20receptor%20methylation%20level%20also%20provides%20a%20simple%20chemical%20memory%20used%20to%20ascertain%20whether%20the%20current%20direction%20of%20swimming%20is%20favorable%20or%20unfavorable.">simple chemical memory</a>.” This is a powerful and somewhat profound idea: individual bacteria
          can model their environment and remember important features of it by
          encoding that information in internal chemical modifications.
          <em>E. coli</em> “knows” whether attractant has become more or less
          concentrated in its surroundings going back
          <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2899694">several seconds</a>; that helps it determine whether it’s swimming in a good or bad
          direction. Which is not that different in principle from what brains
          do. In fact one reason that it requires an artificial neural network
          of about a
          <a href="https://www.sciencedirect.com/science/article/pii/S0896627321005018">thousand</a>
          elements just to model the computational capabilities of a single real
          neuron is that the real neuron stores so much “state” in its internal
          chemistry.
        </p>
        <p>
          (Here‘s an aside: should we be surprised at how resilient people can
          be, given the mechanisms available to a single cell for accepting
          previously extreme conditions as “a new normal”? No doubt our macro
          resilience is in some cases actually underwritten by similar cellular
          mechanisms.)
        </p>
        <h3>
          The full picture: a complex signaling network
        </h3>
        <figure>
          <iframe src="https://www.youtube.com/embed/LgPDOSou1tw?controls=0&amp;autoplay=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
        </figure>
        <p>
          The video above is a very legible overview of
          <em>E. coli</em> chemotaxis, from a popular textbook. It layers in
          even more detail, including not just the proteins that phosphorylate
          CheY but those that dephosphorylate it; and not just the proteins that
          methylate the receptors but those that demethylate it. What you come
          to see is that these doers and undoers define a sort of equilibrated
          circuit whose activity can be conveniently dialed up or down.
        </p>
        <p>
          Dennis Bray describes these sorts of circuits nicely in his book,
          <em>Wetware: A Computer in Every Living Cell</em>:
        </p>
        <blockquote>
          <p>
            In a typical signaling pathway, proteins are continually being
            modified and demodified. Kinases and phosphatases work ceaselessly
            like ants in a nest, adding phosphate groups to proteins and
            removing them again. It seems a pointless exercise, especially when
            you consider that each cycle of addition and removal costs the cell
            one molecule of ATP—one unit of precious energy. Indeed, cyclic
            reactions of this kind were originally labeled “futile.” But the
            adjective is misleading. The addition of phosphate groups to
            proteins is the single most common reaction in cells and underpins a
            large proportion of the computations they perform. Far from being
            futile, this cyclic reaction provides the cell with an essential
            resource: a flexible and rapidly tunable device.
          </p>
          <p>
            If the cell really needs to change the concentration of the modified
            protein very quickly, it can. All it has to do is to switch on or
            shut off the phosphate-adding reaction and the concentration will
            fall precipitously—at the speed of the spinning cycle. There is no
            buildup of products or depletion of substrates to slow down the
            process, as there would be in a linear chain of enzyme reactions.
          </p>
        </blockquote>
        <p>
          This is a clever way to regulate the level of some protein or
          metabolite. Rather than producing the thing you want via a lengthy
          chain reaction, you just have this running cycle that activates and
          then de-activates it, for example via phosphorylation and
          de-phosphorylation. When you want more of the active version, you just
          tamp down the de-activating reaction in the cycle, as if sliding down
          the volume on a stereo.
        </p>
        <p>
          Regulation in this manner via phosphorylation and dephosphorylation
          (by “kinases” and “phosphatases” respectively) is an extremely general
          feature of life. “About 30–50% of human proteins contain covalently
          attached phosphate. [. . .] A typical mammalian cell makes use of
          hundreds of distinct types of protein kinases at any moment.”
          <span>[<a id="alberts-referent" href="#alberts">Alberts</a>]</span>
        </p>
        <p>
          In the interactive figure above, phosphorylation is represented by the
          blue dots becoming white, and de-phosphorylation happens when they
          turn blue again. This cycle is constantly running. The speed of the
          cycle determines how quickly the cell can react to levels of
          attractant. Notice that when you add some, the blue→white
          reaction stops happening as much. But the blue←white reaction
          keeps going at the same rate. So blue CheY proliferates, and the cell
          runs more. (If the cycle spun more slowly, the blues wouldn‘t take
          over so quickly.)
        </p>
        <h3>Down the rabbit-hole…</h3>
        <p>
          One thing I don’t love in presentations of chemotaxis—and of
          biological concepts generally—is that they often prominently feature
          flowcharts and network diagrams. In the case of chemotaxis, as you can
          gather from the video above, there are many players with nearly
          indistinguishable names: CheA phosphorylates CheY to become CheY-p,
          and CheZ dephosphorylates it back to CheY; CheW couples CheA to the
          receptors, and CheR methylates those receptors’ struts; CheB,
          meanwhile, “clips off” the methyl groups added to the struts by CheR.
          A network diagram is no doubt useful for organizing this sea of names
          but in a sense it foregrounds the most abstract view of the process.
          I’d rather try to get a sense of the parts as a living whole or in
          their individual physical detail.
        </p>
        <p>When you do that, it’s amazing what you find.</p>
        <h3>
          What does it mean for a receptor to detect attractant?
        </h3>
        <p>
          Almost every action in a cell depends on proteins changing shape and
          binding to each other. It’s no different in the
          <em>E. coli</em> receptor complex.
        </p>
        <p>
          The way it works is that there are stimulus-specific proteins embedded
          in the <em>E. coli’s</em> cell membrane, protruding into what’s known
          as the periplasm. These proteins are “stimulus-specific” in the
          literal sense that they are shaped so as to bind favorably with
          individual molecules of attractant. <em>E. coli</em> has five or six
          of these, for instance one that detects a crucial amino acid called
          aspartate. This sensor protein has little clefts in it that are shaped
          just so for molecules of aspartate to fit snugly into them.
          <span>[<a id="falke-referent" href="#falke">Falke</a>]</span>
          In schematic form the aspartate receptor looks like this:
        </p>
        <figure>
          <img loading="lazy" width="2032" height="3280" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/transmembrane.jpg">
          <figcaption>
            Source: Falke,
            <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2899694/">“The Two-Component Signaling Pathway of Bacterial Chemotaxis: A
              Molecular View of Signal Transduction by Receptors, Kinases, and
              Adaptation Enzymes”</a>
          </figcaption>
        </figure>
        <p>
          You can see that the sensory part—up top, where the aspartate binds—is
          connected to the signaling proteins CheW and CheA by a columnar
          structure that straddles the cell’s membrane. What does this protein
          complex “actually” look like?
        </p>
        <p>
          An individual protein is small enough—<a href="http://book.bionumbers.org/how-big-is-the-average-protein/">like a few nanometers wide</a>—that it can’t really be seen through a regular light microscope.
          This receptor from top to bottom measures about 350 angstroms, or 35
          nanometers. But modern biology is all about seeing the unseeable.
          Nowadays, we try to find out what nanostructures look like by
          <a href="https://en.wikipedia.org/wiki/X-ray_crystallography">X-ray diffraction</a>
          or, more and more often, by
          <a href="https://en.wikipedia.org/wiki/Cryogenic_electron_microscopy">cryo-freezing them</a>
          in an electron microscope. Once we determine a protein’s structure
          it’s usually rendered using ribbon diagrams, a style
          <a href="https://stories.duke.edu/sciences-mother-of-ribbon-diagrams-celebrates-50-years-at-duke">invented by the biochemist Jane Richardson</a>
          in the late 1970s. Here’s a ribbon diagram for the
          <em>E. coli</em> serine receptor (really it’s a “trimer of dimers,” or
          a complex of six receptors):
        </p>
        <figure>
          <img loading="lazy" width="1079" height="1373" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/image10.png">
          <figcaption>
            Source:
            <a href="https://ckcassidycom.files.wordpress.com/2015/12/full_trimer.jpg">Keith Cassidy</a>
          </figcaption>
        </figure>
        <p>
          This whole thing is the receptor. (Those parts just inside the
          membrane, with little yellow methyl groups lingering stuck to them,
          are the struts.) How exactly it works is quite complex, and the
          subject of current research. But in simplified terms it acts like one
          big piston: when the asparate binds to the part in the periplasm, the
          columnar structure it’s attached to changes shape—a real biologist
          would call these subtle allosteric effects; to me it looks like
          dipping and tilting—in such a way to lock the thing that’s supposed to
          be phosphorylating CheY, the CheA kinase, into an inactive state.
        </p>
        <figure>
          <video loading="lazy" width="480" height="480" src="https://jsomers.net/e-coli-chemotaxis/articleImages/CheA-conformational-change.mp4" autoplay="" muted="" loop="" playsinline=""></video>
          <figcaption>
            Adapted from:
            <a href="https://www.youtube.com/watch?v=MCobqYrE67w">CheA conformation change, posted by Keith Cassidy</a>
          </figcaption>
        </figure>
        <p>
          When I think of a cell I imagine a Rube Goldberg–type contraption
          where an arm swings here, which drops a ball into a slide there, which
          rolls down and opens a trap door, which… eventually turns on or off
          some important cellular function. Indeed, <em>E. coli</em>’s “sense of
          smell” rests ultimately in a series of physical lock-and-key
          mechanisms, starting with literal molecules of e.g. aspartate nuzzling
          into a protein and transmitting that physical shape-change across the
          membrane.
        </p>
        <p>
          This piston-shaped receptor complex is just one of a huge array
          arranged near the front of <em>E. coli</em>’s body. In cross section
          they appear almost to have been laid down through a lithography
          process, in a neat hexagonal pattern:
        </p>
        <figure>
          <img loading="lazy" width="1862" height="598" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/image2.png">
          <figcaption>
            Source:
            <a href="https://www.ks.uiuc.edu/Research/chemotaxis/">Theoretical and Computational Biophysics Group, UIUC</a>
          </figcaption>
        </figure>
        <figure>
          <img loading="lazy" width="1116" height="762" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/image13.png">
          <figcaption>
            Source:
            <a href="https://www.pnas.org/doi/10.1073/pnas.0610106104">Direct visualization of Escherichia coli chemotaxis receptor
              arrays using cryo-electron microscopy</a>
          </figcaption>
        </figure>
        <p>
          Calling <em>E. coli’s</em> receptor complex its “nose” is no mere
          metaphor. Our own noses operate on a similar principle: when you smell
          a flower, it means that actual flower-molecules—possibly only a tiny
          number of them—have reached the inside of your nose and bound to some
          protein with a specific affinity for that very molecule. This signal
          is then transmitted via nerves to your brain. The human nose has
          several hundred receptor proteins for smell; a dog has
          <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5884888/#:~:text=The%20canine%20OR%20repertoire%20is%20composed%20of%201%2C094%20genes%2C%20approximately%20three%20times%20more%20than%20a%20human.">more than a thousand</a>.
        </p>

        <p>
          Every one of our senses works like this. Touch is underwritten by
          proteins that get “squished” by tactile forces into cell membranes,
          triggering a set of downstream responses. Sight is my favorite
          example. There’s a protein called opsin that lives in the cells of our
          retina. What’s so cool about it is that the thing that changes its
          shape is a literal photon. That is,
          <a href="https://pdb101.rcsb.org/motm/147#:~:text=Rhodopsin%2C%20shown%20here,to%20the%20brain.">opsin converts the electromagnetic force of an incoming photon into
            a biomechanical / biochemical signal</a>. This is why I tend to think of molecular biology as the science of
          shapes bumping into each other.
        </p>
        <p>
          I think of <em>E. coli’s</em> receptor complex as a protoversion of
          our own sensory apparatus. Its nose has only
          <a href="https://www.youtube.com/watch?t=2596&amp;v=cT855rpX8bc&amp;feature=youtu.be">five or six</a>
          attractant-specific sensory proteins, but their signals are
          integrated, as if different sets of receptor-protein activations were
          playing different “chords” on the
          <em>E. coli’s</em> sensorium. “In short, the chemosensory array
          functions as an ultrasensitive, ultrastable biological integrated
          circuit or sensory chip.”
          <span>[<a id="falke-referent-2" href="#falke-2">Falke</a>]</span>
        </p>
        <h3>How the signal is carried</h3>
        <p>
          So a bit of attractant binds one of the receptors, and lo, the
          equilibrium inside the cell begins to shift. Because the CheA kinase
          is now inactive, CheYs are no longer getting phosphorylated as
          quickly; the process that <em>de</em>-phosphorylates existing CheY-ps
          starts winning out. Recall that this is a response that is dynamic, a
          flow that is tuned. The net number of CheY-ps in the cell is carefully
          faded down. And then what?
        </p>
        <p>
          The CheY-ps had been binding to the flagellar rotors, flipping them,
          causing tumbles. That now no longer happens as much, because the
          unphosphorylated CheY doesn’t have the same affinity for the rotor as
          CheY-p. As a result, the cell tumbles less, runs more, and biases its
          random walk toward the attractant.
        </p>
        <p>
          There’s something really important worth dwelling on here. When we say
          that CheY-p has an “affinity” for the rotor protein, it’s not like it
          gets directed there; nor does it have some long-acting magnetic
          attraction for it. What this really means is that it has a strong
          inclination to bind to the rotor protein when it gets really really
          close to it. (And CheY, without the -p, doesn't have such an
          inclination.) Given how small a single CheY-p is in the scheme of the
          whole cell’s cytoplasm, it might seem improbable that it’ll somehow
          sidle up right next to one of these rotor proteins somewhere on the
          other end of the <em>E. coli’s</em> body. But that gets at the heart
          of
          <a href="https://www.newyorker.com/magazine/2022/03/07/a-journey-to-the-center-of-our-cells">the crazy kinetic chaos inside our cells</a>.
        </p>
        <figure>
          <img loading="lazy" width="1272" height="832" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/image19.png">
          <img loading="lazy" width="1376" height="1829" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/image7.jpg">
          <figcaption>
            Source: David Goodsell, <em>The Machinery of Life</em>
          </figcaption>
        </figure>
        <p>
          Cells are dense with stuff, but everything in it is also
          <em>extremely</em> fast-moving:
        </p>
        <blockquote>
          <p>
            To get an idea of how fast this motion is, imagine a typical
            bacterial cell, and place an enzyme at one end and a sugar molecule
            at the other. They will bump around and wander through the whole
            cell, encountering many molecules along the way. On average, though,
            it will only take about a second for those two molecules to bump
            into each other at least once. This is truly remarkable: this means
            that any molecule in a typical bacterial cell, during its chaotic
            journey through the cell, will encounter almost every other molecule
            in a matter of seconds.
            <span>[<a id="goodsell-referent" href="#goodsell">Goodsell</a>]</span>
          </p>
        </blockquote>
        <p>
          Just to put this in perspective: imagine you took an
          <em>E. coli</em> cell and scaled it up so that it was the length of a
          football field. And imagine you kept all the physics the same. A water
          molecule would be about an inch wide; a protein would be about the
          size of a basketball.
          <span>[<a href="#bionumbers">BioNumbers</a>]</span>
          The proteins would be juddering violently due to the thermal motion of
          the water particles bombarding them—so violently in fact that if left
          unchecked they’d be moving at 500 meters per second. But they aren’t
          left unchecked: if you were in such an environment it would be so
          crowded as to be nearly impossible to see. What you really get, then,
          is an incredible ceaseless shaking and bouncing-into-each-other of all
          the component parts.
        </p>
        <p>
          This is why shape changes that lead to different bonding affinities
          are so important in biology. It’s as if inside a cell everyone is
          constantly going up to everyone else, seeing if they fit together.
          Proteins sample the space of interactions with other proteins so
          quickly that for a long time, most biologists didn’t really
          contemplate where in the cytoplasm two reactants lived; they knew that
          you never had to wait too long for them to meet each other. In fact it
          was a
          <a href="https://www.quantamagazine.org/a-newfound-source-of-cellular-order-in-the-chemistry-of-life-20210107/">relatively recent discovery</a>
          that inside the cytoplasm certain proteins that share functional
          relationships do seem to keep especially close together, inside little
          oil drops known as “phase-separated liquids.” Weak interactive forces
          between the floppy tails of different proteins cause them to
          spontaneously “phase separate” into these more viscous pools, and this
          biases certain proteins to interact more frequently.
        </p>
        <p>
          The rate-limiting step in <em>E. coli’s</em> reaction to attractant is
          the time it takes for CheY-p to diffuse from the nose to the motor. It
          takes about a tenth of a second. The journey has actually been tracked
          on camera, using a fluorescent version of the protein:
        </p>
        <figure>
          <img loading="lazy" width="1626" height="660" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/electroporated-dye-labelled-CheY.png">
          <figcaption>
            Source:
            <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5052738/">Single-molecule imaging of electroporated dye-labelled CheY in
              live Escherichia coli</a>
          </figcaption>
        </figure>
        <h3>
          What happens when the signal reaches the motor?
        </h3>
        <p>
          Let’s talk about these motors. These things are so intricate and
          beautiful and seem so reminiscent of machines we’d engineer ourselves
          that they’re sometimes cited as evidence for intelligent design.
        </p>
        <p>
          The flagellar motor operates with close to 100% energy efficiency. It
          spins at about 1,500 rotations per second. And the craziest part is
          that like all molecular nanomachines it is entirely self-assembled.
          There’s an amazing 30-minute documentary
          <a href="https://www.youtube.com/watch?v=uw0-MHI_248">available on YouTube</a>
          that details the mechanics of the self-assembly process—and,
          refreshingly, profiles some of the scientists who figured it out,
          describing the methods they used to make their discoveries.
        </p>
        <figure>
          <video loading="lazy" width="320" height="240" src="https://jsomers.net/e-coli-chemotaxis/articleImages/self-assembly.mp4" autoplay="" muted="" loop="" playsinline=""></video>
          <figcaption>
            From:
            <a href="https://www.youtube.com/watch?v=uw0-MHI_248">Self-Assembling NanoMachine: a film about flagellar biosynthesis
            </a>
          </figcaption>
        </figure>
        <p>
          My favorite part of the self-assembly process is that after building a
          base for the rotor, a sort of tunnel is built and the proteins that
          comprise the whip-like “hook” of the flagellum are extruded through
          it—as if the flagellum were built by vomiting forth parts of itself.
        </p>
        <p>
          Anyway, at the base of each rotor there are a series of proteins
          called FliG, FliM, and FliN—pronounced like “Fly G,” “Fly M,” “Fly
          N”—to which CheY-p, our Frodo-esque bearer of the message from the
          nose, attaches once it finally arrives. CheY-p has a strong affinity
          for FliG and will readily glom onto it. We’ll see how that actually
          affects the flagellum in a second. But for now it’s worth noting that
          there’s a thresholding mechanism here: just one CheY-p attaching to
          FliG won’t be enough to flip the motor from counter-clockwise to
          clockwise (thereby causing a tumble)—it actually takes a handful of
          CheY-ps conspiring to make that happen. In fact the motor has
          something like seven states, from rotating quickly counterclockwise at
          three discrete levels of decreasing speed—as if stepping through three
          gears on a bike—to stalling entirely, to starting back up again in the
          clockwise direction, also with three speeds.
        </p>
        <p>
          Even as the motor is in the process of changing direction, any CheY-ps
          that do attach to FliG are under constant threat of being removed by
          yet another player, CheZ. That is, the proteins that would reverse the
          motors are subject to removal by other proteins that un-reverse it.
          Again we have a responsive regulatory circuit reminiscent of the one
          that phosphorylated and de-phosphorylated CheY in the first place
          upstream at the receptor. The idea is that every effect is reversible,
          and in fact is reversed at a regular rate. This means that in the
          absence of further signal the cell will quickly return to baseline.
        </p>
        <h3>
          How does the motor actually change directions?
        </h3>
        <p>
          As a matter of pure mechanics this might be the most ingenious part of
          the story. It took quite a long time to figure out and even still it
          seems that we’re not entirely confident with our explanation. But one
          mechanism that’s been proposed is that CheY-p binds to a protein
          called FliM (“Fly EM”) embedded in that ring that defines the base of
          the rotor. This tilts it and causes a 90-degree rotation in an
          attached protein called FliGc. That protein sits at the interface
          between the rotating part of the motor and the so-called “stator,”
          which drives it from the part that’s anchored solidly in the cell
          membrane.
        </p>
        <figure>
          <p><img loading="lazy" width="438" height="539" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/flagellar-motor-a.png">
            <img loading="lazy" width="449" height="699" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/flagellar-motor-b-c.png">
          </p>
          <figcaption>
            Source:
            <a href="https://www.pnas.org/doi/10.1073/pnas.1110111108">A molecular mechanism of direction switching in the flagellar
              motor of Escherichia coli</a>
          </figcaption>
        </figure>
        <p>
          When FliGc changes orientation, the stepper-motor-like cycle that
          normally drives the motor counterclockwise starts driving it clockwise
          instead. In the illustration below, Figure A shows the stator, i.e.,
          the driving mechanism of the motor. It works by stepping back and
          forth between the “open” and “closed” states, schematized by the
          <img loading="lazy" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/image14.png">
          and
          <img loading="lazy" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/image3.png">
          symbols respectively. In Figure B you can see how, in the normal CCW
          direction, the repeated cycling between these two states drives the
          “teeth” of the motor—the crucial FliGc proteins, here tilted
          left-to-right.
        </p>
        <figure>
          <img loading="lazy" width="1328" height="1068" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/image9.png">
          <figcaption>
            Source: Supplementary figure S9,
            <a href="https://www.pnas.org/doi/10.1073/pnas.1110111108">A molecular mechanism of direction switching in the flagellar
              motor of Escherichia coli</a>
          </figcaption>
        </figure>
        <p>
          When the CheY-p arrives at the rotor it has the effect of flipping the
          FliGc proteins so that now they tilt right-to-left. In that
          orientation the step-drive action works the opposite way, and the
          motor rotates clockwise:
        </p>
        <figure>
          <img loading="lazy" width="1512" height="816" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/image6.png">
        </figure>
        <p>
          You can see this more clearly below in the interactive version of
          those figures. Click “Step” to drive the motor in one direction, and
          “Reverse” to flip the orientation of the FliGc proteins; Step again
          and you’ll see it run in the opposite direction.
        </p>
        <div>
          <p><img id="stepReverse-image" src="https://jsomers.net/e-coli-chemotaxis/articleImages/rotor/rotor-0.png">
          </p>
          
        </div>
        <p>
          It would be nice if the original paper presenting this theory included
          an illustration like this. But even this crude version took me many
          hours to make. As Bret Victor argues in
          <a href="https://www.youtube.com/watch?v=ZfytHvgHybA">Stop Drawing Dead Fish</a>, making moving pictures shouldn’t be so hard. If it were easier,
          such animations would spread everywhere in scientific communication,
          because so often what a paper describes is some kind of dynamic
          process.
        </p>

        <p>
          Dynamic illustrations would help readers grasp proposed mechanisms
          more quickly. As it is, someone who understands a complex mechanism
          usually has to explain it in patient detail to someone
          <em>else</em> who’s good at animating; this costs time and money; and
          most people simply opt not to go through with it. Perhaps someday the
          process will be democratized by better tools, or by a multimodal AI
          system.
        </p>
        <h3>
          How the motor changing directions causes the <em>E. coli</em> to
          tumble
        </h3>
        <p>
          The final part of the story—for me, anyway; there’s a lot more to
          explore!—is why exactly the clockwise rotation of just one of the
          flagellar motors would send the whole cell a-tumbling. It helps to
          understand how the thing works in “run” mode, when all the flagella
          are oriented the same way.
        </p>
        <figure>
          <img loading="lazy" width="1398" height="797" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/image4.png">
          <figcaption>
            Traced from:
            <a href="https://www.nsf.gov/news/mmg/mmg_disp.jsp?med_id=66718&amp;from=">How Escherichia coli Move | National Science Foundation</a>
          </figcaption>
        </figure>
        <p>
          Even though this bundle of flagella sort of looks like a propeller,
          when you actually think about it, that’s not really what it is. It’s
          more like a pig’s curly tail that spins with a whip-y sort of motion.
          How exactly does that propel the entire cell? A wonderful book called
          <em>Random Walks in Biology</em> gets into the physics in some detail:
        </p>

        <blockquote>
          <figure>
            <img loading="lazy" width="734" height="366" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/berg-random-walks-in-biology.png">
            <figcaption>
              <p>Source: Howard C. Berg, Random Walks in Biology.</p>
            </figcaption>
          </figure>
          <p>
            “Fig. 6.3. Analysis of viscous drag on two segments of a flagellar
            filament moving slowly to the right and turning rapidly
            counterclockwise. The velocity of each segment, <i>v</i>, is
            decomposed into velocities normal and parallel to the segment,
            <i>v<sub>n</sub></i> and <i>v<sub>p</sub></i>, respectively. The segment shown on the left is moving upward in
            front of the plane of the paper; the one shown on the right (denoted
            by primes) is moving downward behind the plane of the paper. The
            frictional drags normal and parallel to each segment,
            <i>F<sub>n</sub></i> and <i>F<sub>p</sub></i>, act in directions opposite to <i>v<sub>n</sub></i> and
            <i>v<sub>p</sub></i>, respectively. Note that their magnitudes are in the ratio
            <i>F<sub>n</sub></i>/<i>F<sub>p</sub></i> = 2<i>v<sub>n</sub></i>/<i>v<sub>p</sub></i>. <i>F<sub>n</sub></i> and <i>F<sub>p</sub></i> are decomposed into
            components normal and parallel to the helical axis,
            <i>F<sub>Ω</sub></i> and <i>F<sub>v</sub></i>, respectively. <i>F<sub>Ω</sub></i> and <i>F'<sub>Ω</sub></i> act
            in opposite directions and form a couple that contributes to the
            torque. <i>F<sub>v</sub></i> and <i>F'<sub>v</sub></i> act in the
            same direction and contribute to the thrust.”
          </p>
        </blockquote>
        <p>
          To get a grip on things like this, it helps to have a model of some
          kind that you can hold in your hand. And actually the question of how
          separate filaments running in phase near each other would come to
          bundle was explored nicely in
          <a href="https://www.pnas.org/doi/10.1073/pnas.2633596100">this paper</a>. The authors used physical models of the flagella by wrapping hollow
          Tygon tubes around a mandrel and filling them with epoxy. They then
          used a couple stepper motors to drive the counter-clockwise rotation.
          “The flow field generated by each helix tilts the other helix, causing
          the helices to roll around each other and form a right-handed
          wrapping”:
        </p>

        <figure>
          <video loading="lazy" width="720" height="480" src="https://jsomers.net/e-coli-chemotaxis/articleImages/bundling.mp4" autoplay="" muted="" loop="" playsinline=""></video>
          <figcaption>
            From:
            <a href="https://www.youtube.com/watch?v=25FtMdIFtXM">Macro Scale Model E.Coli Flagella Bundling
            </a>
          </figcaption>
        </figure>

        <h3>
          Individuality in the bacterial population
        </h3>
        <p>
          We tend to think of a colony of something like <em>E. coli</em> as an
          undifferentiated evil goo, each bacterium identical to its neighbors.
          But people who’ve studied these organisms under the microscope observe
          a surprising amount of individual personality.
          <a href="https://www.nature.com/articles/262467a0">A 1976 <em>Nature</em> paper</a>, “Non-genetic individuality: chance in the single cell,” explores
          variation in the context of chemotaxis using strains of Salmonella and
          Enterobacter bacteria.
        </p>
        <p>
          The paper came out before the exact mechanism behind chemotactic
          regulation was well-understood; all the authors knew was that “control
          of tumbling can be rationalised as caused by changes in the levels of
          a tumble regulator.” They hypothesized that although bacteria of the
          same strain would all share the exact same DNA, there might be a
          relatively small number of copies of that “tumble regulator,” and
          natural variation in the transcription, translation, and destruction
          of these regulator proteins could account for differences in behavior.
          Their experiments were mostly at the behavioral level. They observed
          how different individuals—including those in a particularly “tumbly”
          mutant strain (I love that word)—reacted to environments with and
          without attractant, and found plenty of variance.
        </p>
        <p>
          Their theory was spot-on. We talked above about how
          <em>E. coli</em> adapt to higher and higher concentrations of
          attractant via a clever methylation mechanism. Well, it turns out that
          the methylation of the receptor struts is governed by only about
          <a href="https://www.youtube.com/watch?t=3032&amp;v=cT855rpX8bc&amp;feature=youtu.be">100 CheR proteins</a>
          in the cell. The number of those proteins—along with CheB, which
          un-methylates the struts—determines the speed of the “futile cycle”
          that reacts to changes in attractant concentration. That is, it
          affects how quickly the bacterium adapts when the concentration goes
          up and refracts when it goes down.
          <span>[<a href="#gore">Gore</a>
            <a href="https://www.youtube.com/watch?v=cT855rpX8bc&amp;t=3989s">1:06:30</a>]</span>
          Because 100 copies of CheR is so extraordinarily tiny in the context
          of the full cell buzzing with something like ten million proteins,
          variation by just a handful can have a relatively large effect on the
          cell’s behavior.
          <span>[<a href="#gore">Gore</a>
            <a href="https://www.youtube.com/watch?v=cT855rpX8bc&amp;t=4390s">1:13:20</a>]</span>
          That helps account for why different <em>E. coli</em> with the exact
          same genetic sequence will tumble and adapt at different frequencies.
        </p>
        <p>
          Recent
          <a href="https://elifesciences.org/articles/27455">experiments</a>
          have used fluorescent microscopy to quantify the individuality of
          different E. coli cells, individuality that arises not from
          differences in gene expression but from the dynamics of signaling
          networks.
        </p>
        <h3>How did we figure all this stuff out?</h3>
        <p>
          We don’t yet have the technology to just observe all of the activity
          inside a living cell. That Goodsell painting above that shows the
          crowded cytoplasm packed with proteins is an artistic composite—backed
          by rigorous research to be sure—because there’s no way to capture all
          the different players in situ at once. And obviously it’s a “still
          life,” not a video. So how could we possibly know all this detail
          about what exactly a given protein looks like, and how and when it
          interacts with others to kick off some particular part of the
          chemotaxis process?
        </p>
        <p>
          There seem to be three or four major kinds of experiment. Probably the
          most common is genetic: you can selectively disrupt one gene at a time
          and, by observing how the mutant <em>E. coli</em> behaves, begin to
          get a grip on each gene’s function. All of the proteins “CheY,”
          “CheZ,” “CheW,” and so on are named simply because they are the
          products of genes that, when excised, “cause a general defect in
          chemotaxis.”
          <span>[<a href="#blair">Blair</a>]</span> As you
          can imagine, identifying all of these is painstaking work, and
          involves a considerable amount of clever inference. For instance you
          might observe that without gene X the bacteria never seems to tumble;
          is that because that gene is involved in recognizing attractant or in
          forcing the rotor to run clockwise?
        </p>
        <p>
          Once you have a hypothesis, a second kind of experiment involves
          purifying some subset of these proteins-of-interest in vitro to see
          how they work together to form a particular signaling pathway. For
          example you could put CheA and CheY along with some phosphate groups
          and other necessary reactants and observe whether and how much
          phosphorylation takes place. That’s what the authors did in
          <a href="https://doi.org/10.1016/0092-8674(90)90429-I">this paper</a>
          in <em>Cell</em>, in 1990. They used a radioactive version of
          phosphate as a tracer. “Incorporation of [32P]phosphate into CheA or
          CheY was determined by excising the radioactive band out of the dried
          gel and quantitating in scintillation fluid or by analysis of the
          intact gel using a Phosphorimager (Molecular Dynamics, Sunnyvale, CA)
          and compare with known radioactive standards.” Another common method
          for observing in vitro dynamics is to genetically modify proteins to
          fluoresce; or to “find” a protein in solution using an antibody that
          recognizes some part of it—you attach that antibody to another
          protein, and that one you fluoresce, so you can find the hidden one.
        </p>
        <p>
          To understand the literal lock-and-key mechanics at a particular
          binding site—for instance how exactly a molecule of aspartate causes a
          receptor to deform, kicking off a signaling cascade—involves
          “structural” biology work, i.e., taking pictures of individual
          proteins or, increasingly, ensembles of them in situ. For this you can
          use X-ray crystallography, nuclear magnetic resonance imaging,
          cryo-electron microscopy, super-resolution light microscopy, or some
          combination.
        </p>
        <p>
          A <a href="https://www.ks.uiuc.edu/Research/chemotaxis/">group</a> at
          University of Illinois at Urbana-Champagne uses atomic-scale molecular
          dynamics simulations, in software, to understand structural
          details—like the exact
          <a href="https://www.youtube.com/watch?v=MCobqYrE67w">way</a> that
          CheA changes shape to kick off a downstream signaling process—that
          wouldn’t be apparent from high-resolution imaging alone. (Keith
          Cassidy, whose figures appeared above, now has a
          <a href="https://ckcassidy.mufaculty.umsystem.edu/">lab at the University of Missouri-Columbia</a>
          that’s studying the molecular dynamics of the receptor signaling
          complex.)
        </p>
        <p>
          Sometimes you can’t get a direct picture. It may require deduction to
          understand, say, how exactly a protein fits in.
          <a href="https://pubmed.ncbi.nlm.nih.gov/1326408/">One experiment</a>
          found that CheA didn’t bind to a receptor except in the presence of
          CheW; that plus the fact that adding too much CheW into the mixture
          actually led to a decrease in the ability of CheA–CheW complexes to
          bind receptor suggested that CheW competed with that complex for the
          binding site on the receptor and that therefore it must sit between
          the receptor and CheW in the receptor–CheW–CheA trimer
          <span>[via <a href="#blair">Blair</a>]</span>.
          Biology is lousy with heroic inferences like that. It’s a world that’s
          hard to see; sometimes you just have to imagine what’s going on down
          there, and back up those imaginings with the right experiments.
        </p>
        <p>
          The very idea that bacteria run and tumble came from experiments
          <a href="https://www.nature.com/articles/239500a0">published in 1972</a>
          by Howard Berg and Douglas Brown, who used a special three-dimensional
          tracking microscope of their own design to watch the little suckers in
          action. (A fun fact is that they called the non-runs “twiddles”
          instead of “tumbles.”) Some of the physics of flagellar
          propulsion—like how much force the little tails generate—<a href="https://www.google.com/books/edition/An_Introduction_to_Systems_Biology/tcxCkIxzCO4C?hl=en&amp;gbpv=1&amp;pg=PAfrontcover&amp;printsec=frontcover">was discovered</a>
          later by tethering the flagella to a microscope slide: because it’s
          anchored, the “tail wags the dog“ and you can measure how fast the
          <em>E. coli’s</em> body spins. We know that bacterial flagellar motors
          are powered by the
          <a href="https://link.springer.com/referenceworkentry/10.1007/978-3-642-11274-4_141">proton motive force</a>
          from a
          <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC431412/">1977 paper</a>
          that measured how cells ran or “twiddled” in the presence or absence
          of an electrical potential. But the research has become even more
          refined than that. Just by observing the strength of the rotation
          under various conditions—different viscosities, temperatures, and so
          on—we know that “rotation is tightly coupled to proton flow, with a
          fixed number of protons (~500) used to drive each revolution.”
          <span>[<a href="#blair">Blair</a>]</span>
          Think of how detailed an understanding we’ve gotten!
        </p>
        <p>
          One reason I’m particularly attracted to studies of
          <em>E. coli</em> chemotaxis is that it’s an early star of what’s been
          called “in silico” biology. It’s been the subject of many computer
          models. Dennis Bray, the author of that book that put me onto this
          stuff in the first place, made one of the more
          <a href="https://pubmed.ncbi.nlm.nih.gov/17208180/">popular models</a>. Here’s a nice screenshot of the model in action:
        </p>
        <figure>
          <img loading="lazy" width="562" height="425" alt="" src="https://jsomers.net/e-coli-chemotaxis/articleImages/image16.jpg">
        </figure>
        <p>
          Maybe the chief role of a computer model is that to get it working in
          the first place you have to explicitly articulate every one of your
          assumptions. In much the same way that writing tends to clarify your
          thinking (or at least reveal how unclear it really is), a computer
          model forces you to synthesize what you know. If anything it’s even
          more exacting than a blank page.
        </p>
        <p>
          Once you have a model, you can use it to explore variations on those
          assumptions. “The program gives the correct phenotype of over 60
          mutants in which chemotaxis-pathway components are deleted or
          overexpressed,” Bray writes. At best, a good enough model lets you
          discover things you didn’t already know, or suggests your next
          experiment. “In order to match the impulse response to a brief
          stimulus [. . .] we also had to increase the activities of the
          adaptational enzymes CheR and CheB at least an order of magnitude
          greater than published values.”
        </p>
        <h3>So what?</h3>
        <p>
          Why should you care about <em>E. coli</em> chemotaxis? A typical
          answer to that sort of question—and I’m sure the answer given in many
          of the grant applications supporting the work cited here—is that there
          are medical and practical uses. For instance: if you understand the
          signaling pathways of bacterial chemotaxis you can disrupt them; that
          work might lead to a new kind of antibiotic, which, in an era of
          increasing resistance, is direly needed. Or you might hijack
          chemotaxis pathways to create “intelligent sniffers” (Keith Cassidy’s
          phrase) that could home in on cancer cells or environmental waste.
        </p>
        <p>
          More generally you might say—and in fact I led with this up top—that
          understanding this specific phenomenon equips you to understand all
          kinds of others. “Bacterial two-component pathways
          <span>[<a href="https://en.wikipedia.org/wiki/Two-component_regulatory_system">def’n</a>]</span>
          control a dazzling array of functions including cell division,
          virulence, antibiotic resistance, metabolite fixation and utilization,
          response to environmental stress, sporulation, and taxis.”
        </p>
        <p>
          But I don’t know, to me the real reason is that it’s neat. It’s just
          fun to find out about. “To learn, and at due times to repeat what one
          has learnt, is that not after all a pleasure?”
        </p>
        <p><em>Please send feedback or corrections to <a href="https://jsomers.net/">James</a>.</em></p>
      </article><div>
        
        <p>
          <strong>Bray, Dennis.</strong>
          <em>Wetware: A Computer in Every Living Cell</em>. 2009. This is the
          book that kicked off my interest in E. coli chemotaxis. Bray built
          computer models of the network of interacting parts that give rise to
          E. coli’s sophisticated foraging behavior—an early example of in
          silico “systems biology.” This is a very evocative book, giving you a
          newfound appreciation for the computational power of an individual
          cell.
        </p>
        <p>
          “<a href="https://www.youtube.com/watch?v=LgPDOSou1tw">Chemotaxis: Molecular Events</a>.” A superb video that gives an overview of everything you need to
          know about chemotaxis. I think this was prepared for Molecular Biology
          of the Cell, the textbook cited below. Is it sufficient to just watch
          this video and be done with it? Maybe, but for some reason I had a
          bunch of questions I still wanted answered that led me to all this
          other stuff.
        </p>
        <p>
          <strong>Webre, Daniel J. et al.</strong> “<a href="https://www.cell.com/current-biology/pdf/S0960-9822(02)01424-0.pdf">Bacterial chemotaxis</a>.” Current Biology,
          <a href="https://www.sciencedirect.com/journal/current-biology/vol/13/issue/2">Vol 13 No 2</a>, 2003. Good short overview article, like an encyclopedia entry.
        </p>
        <p id="gore">
          <strong>Gore, Jeff.</strong> “<a href="https://www.youtube.com/watch?v=cT855rpX8bc">Robustness and Bacterial Chemotaxis.</a>” 2014. This is a truly awesome lecture that gives you a sense of the
          majesty of bacterial chemotaxis and also of the weird physics of the
          microscopic world (which Gore calls “Life at low Reynold’s number”).
          This lecture is part of Gore’s
          <a href="https://ocw.mit.edu/courses/physics/8-591j-systems-biology-fall-2014/syllabus/">MIT Systems Biology course</a>, available via MIT’s amazing OpenCourseWare initiative.
        </p>
        <p id="alberts">
          <strong><a href="#alberts-referent">Alberts, Bruce et al.</a></strong>
          <a href="https://wwnorton.com/books/9780393884821">Molecular Biology of the Cell</a>, 7th edition, 2022. The meme bio textbook. It’s good, and I actually
          in my adulthood paid for a digital copy of this thing. I refer to it
          every so often—actually maybe just twice since I paid for it. It feels
          handy, the kind of known-good and deep source it’s helpful to have for
          answering your own and others’ basic questions. One thing that strikes
          me about this book, excellent as it is, is that it’d probably be a
          tough way to actually learn biology. Instead, try the stuff cited in
          the “Reading list” at the bottom of
          <a href="https://jsomers.net/i-should-have-loved-biology/">this post</a>.
        </p>
        <p id="falke">
          <strong><a href="#falke-referent">Falke, Joseph J. et al.</a></strong>
          “<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2899694/">The Two-Component Signaling Pathway of Bacterial Chemotaxis: A
            Molecular View of Signal Transduction by Receptors, Kinases, and
            Adaptation Enzymes.</a>” 1997. A really nice paper that tells the basic story, if in
          somewhat forbidding technical language.
        </p>
        <p id="falke-2">
          <strong><a href="#falke-referent-2">Falke, Joseph J. et al.</a></strong>
          “<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4268382/">Architecture and signal transduction mechanism of the bacterial
            chemosensory array: Progress, controversies, and challenges.</a>” 2014. Another good review-ish paper.
        </p>
        <p>
          <strong>Bray, Dennis et al.</strong>
          <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC300951/pdf/mbc00099-0029.pdf">“Computer Simulation of the Phosphorylation Cascade Controlling
            Bacterial Chemotaxis.”</a>
          1993. A good example of in silico biology.
        </p>
        <p>
          <strong>Bray, Dennis et al.</strong> “<a href="https://www.cell.com/current-biology/fulltext/S0960-9822(06)02508-5?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0960982206025085%3Fshowall%3Dtrue">The chemotactic behavior of computer-based surrogate bacteria.</a>” 2007. Another, later paper by Bray with a more detailed computer
          model.
        </p>
        <p>
          “<a href="https://www.ks.uiuc.edu/Research/chemotaxis/">Bacterial Chemotaxis</a>.” A nice page from the Theoretical and Computational Biophysics
          group at the University of Illinois at Urbana–Champaign.
        </p>
        <p>
          <strong>Cassidy, C. Keith.</strong>
          <a href="https://ckcassidy.com/">Personal website.</a> This fellow
          used to work at the UIUC group in the previous reference. He is a
          leading expert on the structure of the chemotaxis signaling complex.
          He now directs a
          <a href="https://ckcassidy.mufaculty.umsystem.edu/">lab at the University of Missouri-Columbia</a>
        </p>
        <p>
          <strong>Beniaguev, David et al.</strong> “<a href="https://www.sciencedirect.com/science/article/pii/S0896627321005018">Single cortical neurons as deep artificial neural networks.</a>” 2021. A paper showing that “cortical neurons are well approximated
          by a deep neural network (DNN) with 5–8 layers.”
        </p>
        <p>
          <strong>Koushik, Paul et al.</strong> “<a href="https://www.pnas.org/doi/10.1073/pnas.1110111108">A molecular mechanism of direction switching in the flagellar motor
            of Escherichia coli.</a>” 2011. This is where all those great diagrams showing a hypothesized
          mechanism for the rotor switching come from. I only wish they’d
          animated them! (Maybe they felt that they were already out on a limb,
          and making a video of a proposed mechanism would make it seem realer
          than it deserve to be? Or maybe they just thought it was too hard.)
        </p>
        <p>
          <strong>Berg, Howard.</strong> “<a href="https://web.archive.org/web/20220810172710/http://www.rowland.harvard.edu/labs/bacteria/movies/ecoli.php">Swimming </a><a href="https://web.archive.org/web/20220810172710/http://www.rowland.harvard.edu/labs/bacteria/movies/ecoli.php">Escherichia coli</a><a href="https://web.archive.org/web/20220810172710/http://www.rowland.harvard.edu/labs/bacteria/movies/ecoli.php">.</a>” Archived web page with lots of videos of <em>E. coli</em> in
          motion. Berg was one of the OG chemotaxis people, and also wrote the
          amazing Random Walks in Biology, cited below.
        </p>
        <p>
          <strong>Alon, U. et al.</strong> “<a href="https://www.nature.com/articles/16483">Robustness in bacterial chemotaxis.</a>” 1999. I don’t know if I understood this paper all that well, except
          to get the point that the chemotaxis system doesn’t rely on there
          being some precise concentration of any of the component parts, e.g.
          CheY. Like many biological systems it works in the face of lots of
          gotchas and what-have-yous.
        </p>
        <p>
          “<a href="https://www.youtube.com/watch?v=uw0-MHI_248">Self-Assembling NanoMachine: a film about flagellar biosynthesis</a>.” A whole documentary (!) about the way that the bacterial flagellar motor
          is made, and the various discoveries that elaborated the process.
          Pretty hard to believe that this documentary exists, but there it is.
          It’s awesome.
        </p>
        <p>
          <strong>Di Paolo, Diana et al.</strong> “<a href="https://royalsocietypublishing.org/doi/10.1098/rstb.2015.0492">Single-molecule imaging of electroporated dye-labelled CheY in live
            E coli.</a>” A neat paper showing the tracking of CheY through the cytoplasm.
        </p>
        <p id="goodsell">
          <strong>Goodsell, David.</strong> “<a href="https://ccsb.scripps.edu/goodsell/machinery-of-life/"><em>The Machinery of Life</em></a>” 1993. One of my favorite biology books, maybe even one of my
          favorite books of all time. Simple, clear, magically illustrated. This
          should be <em>the</em> high school biology textbook.
        </p>
        <p>
          <strong>MunJu, Kim et al.</strong>
          <a href="https://www.youtube.com/watch?v=25FtMdIFtXM">Macro Scale Model E. Coli Flagella Bundling</a>. YouTube video from
          <a href="https://www.pnas.org/doi/10.1073/pnas.2633596100">this paper</a>. 2003. Uses the drill and tubing to simulate flagellar bundling.
        </p>
        <p>
          <strong>Sarkar, Mayukh K. et al.</strong> “<a href="https://www.pnas.org/doi/10.1073/pnas.1000935107">Chemotaxis signaling protein CheY binds to the rotor protein FliN
            to control the direction of flagellar rotation in Escherichia
            coli.</a>” Pretty clear title!
        </p>
        <p>
          <strong>Vladimirov, Nikita.</strong> “<a href="https://github.com/nvladimus/rapidcell">Multiscale modeling of E.coli chemotaxis.</a>” Github project page. Here’s a model I tried to actually run—and
          shockingly, it worked out of the box. Sadly it didn’t show little
          virtual E. colis running and tumbling around on my screen, but still
          pretty cool.
        </p>
        <p id="blair">
          <strong>Blair, D F.</strong> “<a href="https://pubmed.ncbi.nlm.nih.gov/8561469/">How bacteria sense and swim.</a>” 1995. A really well-written review article.
        </p>
        <p id="bionumbers">
          <strong>Milo, Ron and Phillips, Rob.</strong> “<a href="https://pubmed.ncbi.nlm.nih.gov/8561469/">Cell Biology by the Numbers</a>” 2015. The dimensions above were adapted by me (maybe erroneously)
          from a conversation/slides by Drew Endy. He got some of his numbers
          from this BioNumbers book, which is a very neat and totally necessary
          compendium of facts that help you understand life at cell scale.
        </p>
        <p id="bray-maybe">
          <strong>Bray, Dennis / Noble, Denis.</strong> This “wind instrument”
          analogy is not original, I don’t think. I’m guessing it originated
          with either Dennis Bray or Denis Noble (maybe in
          <em>The Music of Life</em>).
        </p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: How to onboard yourself to a new product/industry in a new job? (181 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39777223</link>
            <guid>39777223</guid>
            <pubDate>Thu, 21 Mar 2024 11:28:45 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39777223">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="39777223">
      <td><span></span></td>      <td><center><a id="up_39777223" href="https://news.ycombinator.com/vote?id=39777223&amp;how=up&amp;goto=item%3Fid%3D39777223"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=39777223">Ask HN: How to onboard yourself to a new product/industry in a new job?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_39777223">109 points</span> by <a href="https://news.ycombinator.com/user?id=sujdes">sujdes</a> <span title="2024-03-21T11:28:45"><a href="https://news.ycombinator.com/item?id=39777223">5 hours ago</a></span> <span id="unv_39777223"></span> | <a href="https://news.ycombinator.com/hide?id=39777223&amp;goto=item%3Fid%3D39777223">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20How%20to%20onboard%20yourself%20to%20a%20new%20product%2Findustry%20in%20a%20new%20job%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=39777223&amp;auth=5dfdba3df5c7a7c16c12e741210701218c883f62">favorite</a> | <a href="https://news.ycombinator.com/item?id=39777223">53&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>E.g. I've never been strong at finance (always at the bottom of the class) but I've joined a startup 2 months back that makes a b2b saas fintech product, to lead marketing (first mktg hire in in a total 12 member team). I've worked with the founder before and I like the company. The founder has mentioned that it'll take time to learn the nuances of the product/industry but since I'm a somewhat senior person (9 years exp.), I wanted to see if there are any best practices out there to increase the speed of knowledge transfer/onboarding so that I can connect the dots faster, so to speak.</p><p>We do have a set of required reading material that I've gone through, along with product demos, and even 2 VC books that have been recommended on the subject. But unfortunately, I still feel left behind. I need repetition to understand something deeply. Kind of like rote.</p><p>The lack of knowledge is obviously going to affect my growth somewhere down the line as well as in identifying opportunities for the company's growth.</p><p>I'm looking for advice on what can I do more from folks who joined a company/product/industry that they knew nothing about and how long it took you to get comfortable with it?</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
    </channel>
</rss>