<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 11 Aug 2025 06:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Vanishing from Hyundai’s data network (178 pts)]]></title>
            <link>http://techno-fandom.org/~hobbit/cars/ev/offnet.html</link>
            <guid>44860139</guid>
            <pubDate>Mon, 11 Aug 2025 01:55:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://techno-fandom.org/~hobbit/cars/ev/offnet.html">http://techno-fandom.org/~hobbit/cars/ev/offnet.html</a>, See on <a href="https://news.ycombinator.com/item?id=44860139">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="85%">
<tbody><tr><td>&nbsp;</td><td>
The Yuppie Button page talks about making lots of light.&nbsp;
Now I needed to do the opposite, by "going dark" -- to vanish completely
from Hyundai's data network, and avoid having the car being tracked
or actively interfered with outside of my control.&nbsp;
See, this is one of the showstopping problems I have with Tesla -- they
*insist* that you have your car online all the time, talking to Tesla's
cloud and sending telematic data.&nbsp;
Thank you, NO.&nbsp;
The <a href="https://www.hyundaiusa.com/bluelink/index.aspx">
range of things</a>
that Hyundai's BlueLink setup is able to
do remotely to someone's car given only a VIN is totally scary.&nbsp;
Not only did I want no parts of that, we all have every right to not
participate in that nonsense if we so choose.
<p>

As a first step I refused to let the dealer sign me for BlueLink, telling
them that I could handle signup later myself if I wanted to.&nbsp;
But I knew there was more to it, since the car as it came was still able
to make a cellular data connection and send information about itself.&nbsp;
The obvious question was to find and disable the cellular communication
facility, or "telematics unit" as it is implemented in many vehicles.
</p></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla remotely deactivates rapper's vehicle for singing about the Cybertruck (205 pts)]]></title>
            <link>https://www.threads.com/@brittainforsenate/post/DNMcEZ9yOxk</link>
            <guid>44859807</guid>
            <pubDate>Mon, 11 Aug 2025 00:56:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.threads.com/@brittainforsenate/post/DNMcEZ9yOxk">https://www.threads.com/@brittainforsenate/post/DNMcEZ9yOxk</a>, See on <a href="https://news.ycombinator.com/item?id=44859807">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[I tried coding with AI, I became lazy and stupid (117 pts)]]></title>
            <link>https://thomasorus.com/i-tried-coding-with-ai-i-became-lazy-and-stupid</link>
            <guid>44858641</guid>
            <pubDate>Sun, 10 Aug 2025 21:54:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thomasorus.com/i-tried-coding-with-ai-i-became-lazy-and-stupid">https://thomasorus.com/i-tried-coding-with-ai-i-became-lazy-and-stupid</a>, See on <a href="https://news.ycombinator.com/item?id=44858641">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            

<p>Around April 2025, my boss at $dayjob insisted we try AI tools for coding. It wasn't toxic pressure or anything like <em>"20% of your code needs to be AI"</em>, just a concern from him that we could miss on something. I understand why he asked that and I don't blame him. We are in difficult economic period even for software, and we have salaries to pay. If AI can increase productivity or our margins, it should be at least put on the table of negotiations. I am not happy about this coming, but <em>I get it</em>.</p>

<details><summary>My personal stance of AI</summary> I have personal reasons to dislike LLMs. My partner lost their writing job due to chatGPT convincing their manager writers were now useless. A lot of artist friends struggle because of LLMs. We recently had an intern who lost her translator role due to LLMs. And even outside my personal experience, LLMs are based on stolen content, don't respect consent, waste huge amount of electricity and water, and are overall a new weapon for the capitalists in the <a href="https://danmcquillan.org/ai_thatcherism.html">class warfare</a>. </details>

<p>The other reason why I folded comes from a toxic relationship I built with my job when I became a developer. I detailed in a previous blog post how choosing this career came with very high stakes which triggered a shift in my brain that hasn't left me since:</p>

<figure><blockquote cite="/every-web-stack-is-a-product-now">When I started web development seven years ago, I was in survival mode after years of low paying wages and unemployment. It <em>had</em> to work, and for it to work, I <em>had</em> to always learn more, read and listen about web development all the time, monitor the field, socialize as much as possible with my peers. This way, I would not get disposable and lose my job. I would build a network. I would be safe.</blockquote><figcaption>— I, <a href="https://thomasorus.com/every-web-stack-is-a-product-now">In a blog post from 2022</a></figcaption></figure>

<p>10 years and 3 burnouts later, one can tell this mindset, even if it worked out for a while, wasn't sane or desirable. I had managed to put aside this fear of being disposable, but LLMs triggered it back big time. What if AI vendors were right? What if a future company I apply to requires you to use it? Am I going to lose my job? I'm almost 40, what will I do?</p>

<p>So I tried AI. First at my day job, because I wanted answers. But outside fixing TypeScript types errors, generate inaccessible template code, or review my code for errors, I couldn't find a <em>life changing</em> use out of it that all AI influencers talk about. I asked my colleagues about their own experiments, and lots of them came to the same conclusion: it doesn't seem to help me help our clients achieve their goals.</p>

<p>When July came I was starting the image processing part of my new CMS that powers this website. Still stressed I couldn't get a real shot at coding with an LLM, and very tired by different personal events that fogged my brain, I decided it was the right task to try it seriously and get answers.</p>

<p>After setting up everything in VS Code, opening the AI panel, giving access to the codebase and detailing my needs in a prompt, the LLM produced around 200 lines of code. Mostly functions using dependencies to convert, resize, process images. It wasn't perfect but after a few changes, the task was done and it had taken around 30 minutes, far less than if I had made it by hand.</p>

<p>I was impressed. It really felt like I had superpowers! But then I had the idea to audit the code the LLM just produced, like I did at my $dayjob for a Vue application. Feeling that uploading files could be a source of security issues, I asked the same LLM to focus on this specific topic.</p>

<p>It found several dangers: directory traversal attacks, file size limits, system file overwrite, etc. I had no idea the initial code was this unsafe. I had reviewed the code, but without enough experience in backend development, how could I identify issues I didn't know existed? And why, if it knew about all those dangers, did the LLM produced unsafe code in the first place?</p>

<p>When I tried to fix the security issues, I quickly realized how this whole thing was a trap. Since I didn't wrote it, I didn't have a good bird's eye view of the code and what it did. I couldn't make changes quickly, which started to frustrated me. The easiest route was asking the LLM to do the fixes for me, so I did. More code was changed and added. It worked, but again I could not tell if it was good or not.</p>

<p>That's when I stopped the experiment.</p>

<p>I was shocked by how easily I had slipped into this slacker way of programming. The LLM had given me shitty code, made me ignorant about my own code base, and too lazy to try to fix it myself. And at the same time, the whole experience felt smooth, frictionless, empowering. On the moment I felt smarter, more productive, in control. But it was all an illusion.</p>

<p>I knew about this as we had studies showing <a href="https://www.researchgate.net/publication/392560878_Your_Brain_on_ChatGPT_Accumulation_of_Cognitive_Debt_when_Using_an_AI_Assistant_for_Essay_Writing_Task">LLM use makes us dumb</a>, and that <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">self-reported productivity gains are false</a>. But experiencing it for myself was a totally different feeling.</p>

<p>It gave me a whole different perspective and answered my initial question: will I get replaced by AI soon?</p>

<p>The answer is no. I don't think AI will take my job anytime soon because it's smarter and more productive than I am. I also don't think AI will make me <a href="https://colton.dev/blog/curing-your-ai-10x-engineer-imposter-syndrome/">10 times more productive</a>. If I lose my job due to AI, it will be because I used it so much it made me lazy and stupid to the point another human has to replace me and I become unemployable.</p>

<p>I shouldn't invest time in AI. I should invest more time studying new things that interest me. That's probably the only way to keep doing this job and, you know, <em>be safe</em>.</p>


            
    

        </article><p><small><strong>Initially published: </strong>08 Aug 2025 at 20:03</small><br>
        <small><strong>Modified and/or rebuilt: </strong>08 Aug 2025 at 20:17</small>
    </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[1910: The year the modern world lost its mind (253 pts)]]></title>
            <link>https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost</link>
            <guid>44858154</guid>
            <pubDate>Sun, 10 Aug 2025 20:48:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost">https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost</a>, See on <a href="https://news.ycombinator.com/item?id=44858154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><p><em>“Automobilism is an illness, a mental illness. This illness has a pretty name: speed... [Man] can no longer stand still, he shivers, his nerves tense like springs, impatient to get going once he has arrived somewhere because it is not somewhere else, somewhere else, always somewhere else.” </em></p><p><em>- Octave Mirbeau, French novelist, 1910</em></p></div><p><em><strong>About today’s piece: When we hear about technological change and social crisis in the 21st century, it is easy to imagine that we are living through a special period of history. But many eras have grappled with the problems that seem to uniquely plague our own. The beginning of the 20th century was a period of speed and technological splendor (the automobile! the airplane! the bicycle!), shattered nerves, mass anxiety, and a widespread sense that the world had been forever knocked off its historical axis: a familiar stew of ideas. I think we can learn a lot about the present by studying historical periods whose challenges rhyme with our own.</strong></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!QkGQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!QkGQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 424w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 848w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg" width="580" height="394" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:394,&quot;width&quot;:580,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;1910 Model T ad&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="1910 Model T ad" title="1910 Model T ad" srcset="https://substackcdn.com/image/fetch/$s_!QkGQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 424w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 848w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Welcome back to The Sunday Morning Post!</p><p><span>My favorite period of history is the 30- to 40-year span between the end of the 19th century and the early innings of the 20th century. It was an era of incredible change. From </span><em>Abundance</em><span>:</span></p><blockquote><p>Imagine going to sleep in 1875 in New York City and waking up thirty years later. As you shut your eyes, there is no electric lighting, Coca-Cola, basketball, or aspirin. There are no cars or “sneakers.” The tallest building in Manhattan is a church. </p><p>When you wake up in 1905, the city has been remade with towering steel-skeleton buildings called “skyscrapers.” The streets are filled with novelty: automobiles powered by new internal combustion engines, people riding bicycles in rubber-soled shoes—all recent innovations. The Sears catalog, the cardboard box, and aspirin are  new arrivals. People have enjoyed their first sip of Coca-Cola and their first bite of what we now call an American hamburger. The Wright brothers have flown the first airplane. When you passed into slumber, nobody had taken a picture with a Kodak camera or used a machine that made motion pictures, or bought a device to play recorded music. By 1905, we have the first commercial versions of all three—the simple box camera, the cinematograph,  and the phonograph. </p></blockquote><p><span>No book on turn-of-the-century history has influenced me more, or brought me more joy, than </span><em><a href="https://www.amazon.com/Vertigo-Years-Europe-1900-1914/dp/0465020291" rel="">The Vertigo Years: Europe 1900-1914</a></em><span> by Philipp Blom. I think it might be the most underrated history book ever written.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-170457512" href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost#footnote-1-170457512" target="_self" rel="">1</a></span><span> In my favorite chapters focusing on the years around 1910</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-170457512" href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost#footnote-2-170457512" target="_self" rel="">2</a></span><span>, Blom describes how turn-of-the-century technology changed the way people thought about art and human nature and how it contributed to a nervous breakdown across the west. Disoriented by the speed of modern times, Europeans and Americans suffered from record-high rates of anxiety and a sense that our inventions had destroyed our humanity. Meanwhile, some artists channeled this disorientation to create some of the greatest art of all time.</span></p><p><span>In today’s TSMP, I want to share with you my favorite passages and lessons from </span><em>The Vertigo Years</em><span>, most of which come from the chapter on the year 1910. Great history books remind us that while history never repeats itself, its themes never stop rhyming, and we would all do well to listen with open ears. I’ve tried to limit my summary to areas of overlap between the early 1900s and the 2020s, but I’m not going to press the similarities too hard throughout the piece. You’re going to have to recognize them for yourself.</span></p><p>Transportation technology remade the west in a few short decades between the 1880s and 1910. A “bicycle craze” swept America in the 1890s. The Wright Brothers took flight in 1903. The first Model Ts rolled off Ford’s production lines in 1908. In Europe, cars quickly transformed the physical environment. The number of automobiles in France increased from about 3,000 in 1900 to 100,000 by 1914. That year, Ford's factory in Detroit produced and sold more than 300,000 Model Ts. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3gex!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3gex!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 424w, https://substackcdn.com/image/fetch/$s_!3gex!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 848w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!3gex!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg" width="1456" height="978" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:978,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!3gex!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 424w, https://substackcdn.com/image/fetch/$s_!3gex!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 848w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Model T, Wikimedia Commons</figcaption></figure></div><p><span>Speed was a physical experience, Blom writes, and cultural critics of the early 1900s were confident that it was unnatural for people to move so quickly through space—women, in particular. A woman on a bicycle was a thing to be feared. She signified a high-velocity freedom that was often associated with moral and sexual deviancy. Physicians warned that </span><a href="https://pessimistsarchive.org/list/bicycle/clippings/1897/sc-178" rel="">"diseases of the wheel"</a><span> came by "the almost universal use of the bicycle" and that </span><a href="https://pessimistsarchive.org/list/bicycle/clippings/1897/m-sc-192-184" rel="">"serious evils"</a><span> might befall the youth who rode without restraint.  Moralists condemned women who “pedaled along gleefully, having discarded their corsets and put on more practical clothing, including trousers.” </span></p><p><span>Critics and novelists considered technological speed to be a vice, and they warned that our lust for celerity might turn into literal lust; that cars and bicycles would beckon us into carnal sin. In </span><em>Le surmale</em><span> (1902), the book’s hero wins a 100,000-mile bike race and then celebrates with an act of love-making that makes one character exclaim, “This is not a man, but a machine!” The idea that cars, planes, and bicycles were turning people into “machines” was most entertainingly summarized by a 1905 article in the journal </span><em>Je sais tout</em><span> (“I know all”), which calculated just how tall a human being would have to be to naturally walk at the pace that our new machines traveled. To equal the speed of a bicycle, for example, it was calculated that a person have to be more than 40 feet tall. Blom:</span></p><blockquote><p>Comparisons with other forms of transportation showed that in a fast train, a voyager would be effectively 51 meters tall, while the chauffeur of a racing car would almost dwarf Notre Dame Cathedral in Paris. Technology had created a new race of giants — in both senses of the term — and it changed the experience of space and time itself.</p></blockquote><p>“The growing speed of daily life, of news and work and play was a fetish of artists and industrialists alike,” Blom writes. “Never before had so much social change occurred so quickly.” As daily life sped up, people in the west started to break down.</p><p><span>Around the turn of the century, a nervous disorder first diagnosed in the U.S. gradually made its way across the Atlantic. The doctor George Miller Beard had called it “neurasthenia,” or nervous exhaustion. Europeans sometimes referred to it as “American Nervousness.” According to Beard, the affliction was most common among “the in-door classes of civilized countries” and the sufferers could be found “in nearly every brain-working household.”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-170457512" href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost#footnote-3-170457512" target="_self" rel="">3</a></span><span> </span></p><p>As Blom points out, those afflicted tended to be white-collar workers working at the “frontiers of technology,” as “telephone operators, typesetters on new, faster machines, railway workers, engineers, [or] factory workers handling fast machines. One 1893 hospital survey of neurasthenia found that among nearly 600 cases, “there were almost 200 businessmen, 130 civil servants, 68 teachers, 56 students and eleven farmers.” Notably, no manual workers were counted at the clinic. Neurasthenia seemed to disproportionately affect white-collar workers, who were “overwhelmed” by their labor. “Overwork was a common theme in patients’ histories,” Blom writes.</p><p>It is tempting to write off this phenomenon as just another case of the “worried well.” But the scale of the west’s mental health distress in this period was striking. Blom: </p><blockquote><p><strong>In Germany, 40,375 patients were registered in mental hospitals in 1870. The number rose to 115,882 in 1900 and 220,881 in 1910</strong><span>. Over the same period, the proportion of patients admitted to general hospitals for illnesses of the nervous system rose from 44 to 60 percent. While these numbers include those suffering from many and varied mental conditions, not just neurasthenia, they do not include the huge number of sufferers who preferred going for cures or long stays in private sanatoriums, spas or other paramedical establishments in which a doctor would look after the guests — as in the one described by Thomas Mann in </span><em>The Magic Mountain</em><span>. </span></p></blockquote><p>“Artists were fascinated by this accelerated reality and its possibilities,” Blom writes. The novelists, painters, and musicians of the era could not stop talking about the changes they saw around them and their duty to use art to enter into a dialogue with those changes. Blom: </p><blockquote><p>Their view of things was shaped by reading about races in fast machines and in children’s magazines, by over-hearing adult whispers about nervous breakdowns and fast women … their imagination was alert to the fact that an age had ended and a new one — by turns a promise and a menace — was busting onto the scene, visible as yet only in flashes and fragmented visions. </p></blockquote><p>Blom deeply considers three artistic icons of the era: the composer Igor Stravinsky and the painters Vassily Kandinsky and Pablo Picasso. Each sought to make art that felt simultaneously cutting-edge and primal. Each responded to the modern age by reaching for inspiration in the past.</p><p><span>Blom begins with Stravinsky, whose famous orchestral work </span><em>The Rite of Spring</em><span> was inspired by ancient Russian dance rituals. A melange of old folk music and arresting dissonance, the piece’s first performance in Paris 1913 triggered one of the most infamously violent reactions of any concert-hall audience in history. As Blom puts it bluntly, “all hell broke loose”: </span></p><blockquote><p>“During the first two minutes the public remained quiet,' Monteux [a musician] later recalled, “then there were boos and hissing from the upper circle, soon after from the stalls. People sitting next to one another began to hit one another on the head with fists and walking sticks, or whatever else they had to hand. Soon, their anger was turned against the dancers and especially against the orchestra... Everything to hand was thrown at them, but we continued playing. The chaos was complete when members of the audience turned on one another, on anyone supporting the other side. A heavily bejewelled lady was seen slapping her neighbour before storming off, while another one spat in her detractor's face. Fights broke out everywhere and challenges to duels were issued.”</p></blockquote><p><span>Some music critics now consider The Rite of Spring </span><a href="https://www.pittsburgh-theater.com/shows/heinz-hall/pittsburgh-symphony-orchestra-the-rite-of-spring" rel="">“undoubtedly the most famous composition of the early 20th century.”</a></p><p>As classical music disintegrated in the concert halls, visual art was undergoing its own revolution, which may have been technological in origin. For thousands of years before the turn-of-the-century, the ability to perfectly represent nature been a rare skill possessed only by the most talented painters and drawers among us. But the Kodak camera (invented in 1888, with sales accelerating into the 1900s) turned the ability to capture realist images into a consumerist trifle. It cannot be a coincidence that the rise of abstract art coincided so perfectly with the proliferation of cheap camera technology that debased the value of perfect renderings of the natural world. </p><p>In the early 1900s, Vassily Kandinsky, one of the great pioneers of abstract art, pushed back against the mind-blurring speed of modernity. Kandinsky drew inspiration from the shamans of the Ural Mountains and the sound of their drums, according to Blom, and his abstraction sought to capture their primary music in images. By turning sound into image, Kandinsky’s art sought to achieve an act of synesthesia that no Kodak machine could ever match. Other art historians are less certain about what inspired Kandinsky’s first abstract watercolors, which he painted around 1910. All that is certain is that paintings like this one reject any effort to depict the natural world as it might be seen through a retina or camera lens. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!EFhk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!EFhk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 424w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 848w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!EFhk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg" width="1456" height="1151" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1151,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Untitled First Abstract Watercolor, 1910–1913, Centre Pompidou, Paris[19]&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Untitled First Abstract Watercolor, 1910–1913, Centre Pompidou, Paris[19]" title="Untitled First Abstract Watercolor, 1910–1913, Centre Pompidou, Paris[19]" srcset="https://substackcdn.com/image/fetch/$s_!EFhk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 424w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 848w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Untitled </span><em>First Abstract Watercolor</em><span>, 1910–1913</span></figcaption></figure></div><p>Kandinsky is one of my favorite artists. But the critical response to the dawn of abstract painting was about as brutal as it gets. One German review that Blom cites includes the following passage:</p><blockquote><p>Looked at as painting they are the end of art, a prank. But they show a more nefarious side. The modern phrase that the object of art is indifferent, if abused here in a truly malevolent way... What is presented to us breathes the poison breath of the darkest places of vice of the big city and shows the constitution of the artists, which can only be understood in terms of pathology.</p></blockquote><p><span>Around the same time that Kandinsky was putting his mark on abstraction, Pablo Picasso was pioneering his own rejection of purely representative art, with primitivism. Drawing inspiration from African masks and carvings from West Africa, Picasso’s art “did everything to hide its underlying technical and compositional virtuosity,” Blom writes. Picasso’s 1907 classic </span><em>Les Demoiselles d'Avignon</em><span> is “a large canvas of brutal and disturbing bluntness.” While Picasso was indifferent to the actual “significance and symbolism” of the African styles he drew on, Blom writes, critics have said his aim was to represent the “unchanging structure of the human condition” in the face of civilizational change.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!9OXU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!9OXU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 424w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 848w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!9OXU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg" width="1280" height="1326" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1326,&quot;width&quot;:1280,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;undefined&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="undefined" title="undefined" srcset="https://substackcdn.com/image/fetch/$s_!9OXU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 424w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 848w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Independent of one another, Stravinsky, Kandinsky, and Picasso each reacted to the modern world and “the alienation of the human mind from its own emotions” by pulling pre-modern styles and atavistic images into their art. What we call Modernism today was in most cases a reaction to modernity. It was an effort to excavate something ancient and honest about humanity in an age obsessed with and overrun by novelty.</p><p>Blom closes his chapter “1910: Human Nature Changed” by considering two intellectual giants of the time: the sociologist Max Weber and the psychoanalyst Sigmund Freud, whose International Psychoanalytic Association was founded in 1910. The tension between their theories of human nature are profoundly relevant today.</p><p><span>In his famous work </span><em>The Protestant Ethic and the Spirit of Capitalism</em><span>, Weber, a German sociologist, argued that certain Protestant—especially Calvinist—traditions supported habits that aligned with the development of modern capitalism. He argued that the Protestant tradition of northern European worshippers cultivated a disciplined approach to work, savings, and investment that proved valuable in commerce, while the Calvinist doctrine of divine grace “could lead believers to read worldly success as a possible sign of God’s favor,” as Blom summarizes. Weber believed that Protestantism not only encouraged followers to pour their energies into labor (hence the allusion to </span><em>Work Ethic</em><span> in the book’s title) but also helped create a culture of trade and investment that supported the rise of modern capitalism.</span></p><p><span>“It is easy to see how Freud’s analysis follows on from Weber’s,” Blom writes. To Freud, human nature was at risk of being fully dissolved by capitalism and modern society, like chalk dropped in acid. Beneath the polite masks demanded by modern society, he said, there lurked a more atavistic and instinctual self. Freud saw our psyche as a tug-of-war between the id (our animal urges) and superego (the voice in our head that internalizes society’s rules), with the ego stuck in the middle trying to negotiate an authentic identity in the face of mass inauthenticity. One of Freud’s most fantastic insights was that some people can channel or redirect their most raw and unacceptable urges toward productive and acceptable work. His name for this bit of psychological alchemy was </span><em>sublimation</em><span>. </span></p><p>Modern capitalism, in Freudian terms, was the sublimation of self-interest—or, one might even say, the sublimation of greed. “The suppression of natural urges is a necessary precondition for capitalist success,” Blom writes in summary, “but while it is productive for the group and its wealth, such an approach will eventually exact its revenge on the individual.” By this interpretation, the mass anxiety of the early 1900s—whether you call it neurasthenia, American Nervousness, or Newyorkitis—was price of modernity, technological development, and even capitalism itself.</p><p><span>There is little evidence that Freud and Weber ever debated one another. Yet when you set their theories side by side, it’s hard not to hear a conversation that still shapes much modern commentary. Weber wrote that modern capitalism evolved from religious doctrines that fit our nature, while Freud argued that human nature is unfit for a modern world that distorts and represses our basic urges. </span><em>Are our most impressive inventions the ultimate expression of our humanity, or are they the ultimate threat to it? </em><span>This is the question that every generation must answer for itself, including our own. It is a question equally worthy of the automobile and artificial intelligence. The troubling answer—for Weber and for Freud; for 1910 and for 2025—is: perhaps, both.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[One Million Screenshots (177 pts)]]></title>
            <link>https://onemillionscreenshots.com/?q=random</link>
            <guid>44858067</guid>
            <pubDate>Sun, 10 Aug 2025 20:30:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onemillionscreenshots.com/?q=random">https://onemillionscreenshots.com/?q=random</a>, See on <a href="https://news.ycombinator.com/item?id=44858067">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><a href="https://onemillionscreenshots.com/"><h2><span>One</span><span>Million</span><span>Screenshots</span></h2></a><p>Zoom into the web's top homepages</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Bolt – A super-fast, statically-typed scripting language written in C (193 pts)]]></title>
            <link>https://github.com/Beariish/bolt</link>
            <guid>44856935</guid>
            <pubDate>Sun, 10 Aug 2025 17:53:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Beariish/bolt">https://github.com/Beariish/bolt</a>, See on <a href="https://news.ycombinator.com/item?id=44856935">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">⚡ Bolt</h2><a id="user-content--bolt" aria-label="Permalink: ⚡ Bolt" href="#-bolt"></a></p>
<p dir="auto">A <em>lightweight</em>, <strong>lightning-fast</strong>, type-safe embeddable language for real-time applications.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import print, error, Error from core
import abs, epsilon from math

// The return type of safe_divide is inferred to be `Error | number`
fn safe_divide(a: number, b: number) {
    if abs(b) < epsilon {
        return error(&quot;Cannot divide by zero!&quot;)
    }

    return a / b
}

match let result = safe_divide(10, 5) {
    is Error {
        // The type of result is narrowed in this branch!
        print(&quot;Failed to divide:&quot;, result.what)
    }

    is number {
        print(&quot;The answer is&quot;, result)
    }
}"><pre><span>import</span> <span>print</span><span>,</span> <span>error</span><span>,</span> <span>Error</span> <span>from</span> <span>core</span>
<span>import</span> <span>abs</span><span>,</span> <span>epsilon</span> <span>from</span> <span>math</span>

<span>// The return type of safe_divide is inferred to be `Error | number`</span>
<span>fn</span> <span>safe_divide</span><span>(</span><span>a</span>: <span>number</span><span>,</span> <span>b</span>: <span>number</span><span>)</span><span></span> <span>{</span>
    <span>if</span> <span>abs</span><span>(</span><span>b</span><span>)</span> <span>&lt;</span> <span>epsilon</span> <span>{</span>
        <span>return</span> <span>error</span><span>(</span><span>"Cannot divide by zero!"</span><span>)</span>
    <span>}</span>

    <span>return</span> <span>a</span> <span>/</span> <span>b</span>
<span>}</span>

<span>match</span> <span>let</span> <span>result</span> <span>=</span> <span>safe_divide</span><span>(</span><span>10</span><span>,</span> <span>5</span><span>)</span><span></span> <span>{</span>
    <span>is</span> <span>Error</span> <span>{</span>
        <span>// The type of result is narrowed in this branch!</span>
        <span>print</span><span>(</span><span>"Failed to divide:"</span><span>,</span> <span>result</span><span>.</span><span>what</span><span>)</span>
    <span>}</span>

    <span>is</span> <span>number</span> <span>{</span>
        <span>print</span><span>(</span><span>"The answer is"</span><span>,</span> <span>result</span><span>)</span>
    <span>}</span>
<span>}</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><a href="https://github.com/Beariish/bolt/blob/main/doc/Bolt%20Performance.md">Lightning-fast performance</a>, outperforming other languages in its class</li>
<li>Compact implementation, leaving a minimal impact on build size while remaining consise enough to browse.</li>
<li>Blazingly quick compilation, plow through code at over 500kloc/thread/second. That's 50'000 lines in the blink of an eye.</li>
<li>Ease of embedding, only a handful of lines to get going</li>
<li>Rich type system to catch errors before code is ran, with plenty of support for extending it from native code</li>
<li>Embed-first design, prioritizing inter-language performance and agility</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Links</h2><a id="user-content-links" aria-label="Permalink: Links" href="#links"></a></p>
<ul dir="auto">
<li><strong><a href="https://github.com/Beariish/bolt/blob/main/doc/Bolt%20Programming%20Guide.md">Bolt programming guide</a></strong></li>
<li><strong><a href="https://github.com/Beariish/bolt/tree/main/doc/Bolt%20Standard%20Library">Bolt standard library reference</a></strong></li>
<li><strong><a href="https://github.com/Beariish/bolt/tree/main/doc/Bolt%20Embedding%20Guide.md">Bolt embedding and API reference</a></strong></li>
<li><strong><a href="https://github.com/Beariish/bolt/blob/main/doc/Bolt%20Performance.md">Bolt performance</a></strong></li>
<li><strong><a href="https://github.com/Beariish/bolt/blob/main/doc/Bolt%20Users.md">Notable Bolt users</a></strong></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dependencies</h2><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<p dir="auto">Bolt only depends on the C standard library as well as <code>libm</code> on Unix-based systems.
Some standard library modules include things like file and system IO, but these can be disabled easily.
By default, Bolt sets up an environment that uses <code>malloc</code>/<code>realloc</code>/<code>free</code>, but this is also easy to configure.
Bolt also embeds my other library <a href="https://github.com/Beariish/picomatch">picomatch</a> for regex parsing</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Minimal embedding example</h2><a id="user-content-minimal-embedding-example" aria-label="Permalink: Minimal embedding example" href="#minimal-embedding-example"></a></p>
<p dir="auto">The <a href="https://github.com/Beariish/bolt/blob/main/bolt-cli/main.c">bolt-cli</a> program provides a very consice example of how to embed bolt an an application, see the <a href="https://github.com/Beariish/bolt/tree/main/doc/Bolt%20Embedding%20Guide.md">Bolt embedding guide</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Language examples</h2><a id="user-content-language-examples" aria-label="Permalink: Language examples" href="#language-examples"></a></p>
<p dir="auto">The <a href="https://github.com/Beariish/bolt/tree/main/examples">examples</a> folder contains a few short examples of ideomatically written bolt code. Check out the <a href="https://github.com/Beariish/bolt/tree/main/tests">tests</a> and <a href="https://github.com/Beariish/bolt/tree/main/benchmarks">benchmarks</a> folders as wel for some more in-depth language overview.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">Bolt currently only builds on x64. 32-bit architectures are explicitly not supported, arm and riscv are untested.
Running <code>cmake</code> in the root directory of the project will generate a static library for the language, as well as the CLI tool.
For more information and options regarding embedding Bolt in your application, see <code>bt_config.h</code>.
See below for the status of Bolt on each relevant compiler.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compiler Status</h2><a id="user-content-compiler-status" aria-label="Permalink: Compiler Status" href="#compiler-status"></a></p>
<p dir="auto">Please note that Bolt is <strong>not</strong> yet stable, expect to encounter compiler bugs and crashes. If you do, opening an issue with replicable Bolt code would be much appreciated 😊</p>
<p dir="auto"><a href="https://github.com/Beariish/bolt/actions/workflows/cmake-multi-platform.yml"><img src="https://github.com/Beariish/bolt/actions/workflows/cmake-multi-platform.yml/badge.svg" alt="Build Status"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Compiler</th>
<th>Status</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>MSVC</td>
<td>✅</td>
<td>no issues</td>
</tr>
<tr>
<td>GCC</td>
<td>✅🟨</td>
<td>all functional, some warnings</td>
</tr>
<tr>
<td>Clang</td>
<td>✅🟨</td>
<td>all functional, some warnings</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Bolt is a very opinionated project, and any contributions should take the vision into account.</p>
<p dir="auto">Bugfixes are likely to be accepted as long as they're within reason and don't change any expected behaviour. Adding tests in case of regression is very much appreciated as well. A clean run of <code>/tests/all</code> is expected of course.</p>
<p dir="auto">Optimizations may also be accepted for minor versions under similar criteria. A before/after run of <code>/benchmarks/all</code> is expected to evaluate the impact and make sure nothing else regresses. If the specific optimization isn't captured in any existing benchmark, adding one is required.</p>
<p dir="auto">Feature additions will need a lot of consideration, Bolt is very intentionally minimal in its' design and featureset. I highly suggest you submit some kind of proposal or plan before starting any significant work on a feature to review. Use cases, performance, and implementation cost will all be expected to be justified.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Bolt is licensed under MIT. See LICENSE for more information.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fight Chat Control (956 pts)]]></title>
            <link>https://fightchatcontrol.eu/</link>
            <guid>44856426</guid>
            <pubDate>Sun, 10 Aug 2025 16:50:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fightchatcontrol.eu/">https://fightchatcontrol.eu/</a>, See on <a href="https://news.ycombinator.com/item?id=44856426">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <!-- Hero Section -->
        <div>
                    <h2>
                        The EU (still) wants to scan <br> your private messages and photos
                    </h2>
                    <p>
                        The "Chat Control" proposal would mandate scanning of <strong>all</strong> private digital communications,
                        including encrypted messages and photos. This threatens <strong>fundamental privacy rights</strong> and digital security
                        for all EU citizens.
                    </p>
                    <div>
                        
                        <div>
                            <h3 id="support-count">15</h3>
                            <p>Member States Supporting</p>
                        </div>
                        <div>
                            <h3 id="undecided-count">9</h3>
                            <p>Member States Undecided</p>
                        </div>
                    </div>
                    
                </div>

        <!-- Overview Section -->
        <div id="overview">
                <h2>You Will Be Impacted</h2>
                <p>
                    Every photo, every message, every file you send will be automatically scanned—without your consent or suspicion. This is not about catching criminals. It is mass surveillance imposed on all 450 million citizens of the European Union.
                </p>
                <div>
                    <div>
                        <p>📱</p>
                        <h3>Mass Surveillance</h3>
                        <p>Every private message, photo, and file scanned automatically: no suspicion required, no exceptions*, even encrypted communications.</p>
                    </div>
                    <div>
                        <p>🔓️</p>
                        <h3>Breaking Encryption</h3>
                        <p>Weakening or breaking end-to-end encryption exposes everyone’s communications—including sensitive financial, medical, and private data—to hackers, criminals, and hostile actors.</p>
                    </div>
                    <div>
                        <p>⚖️</p>
                        <h3>Fundamental Rights</h3>
                        <p>Undermines your fundamental rights to privacy and data protection, as guaranteed by Articles 7 and 8 of the EU Charter—rights considered core to European democratic values.</p>
                    </div>
                    <div>
                        <p>🎯</p>
                        <h3>False Positives</h3>
                        <p>Automated scanners routinely misidentify innocent content, such as vacation photos or private jokes, as illegal, putting ordinary people at risk of false accusations and damaging investigations.</p>
                    </div>
                    <div>
                        <p>👨‍👩‍👧‍👦</p>
                        <h3>Ineffective Child Protection</h3>
                        <p>Child protection experts and organisations, including the UN, warn that mass surveillance fails to prevent abuse and actually makes children less safe—by weakening security for everyone and diverting resources from proven protective measures.</p>
                    </div>
                    <div>
                        <p>🌍</p>
                        <h3>Global Precedent</h3>
                        <p>Creates a dangerous global precedent enabling authoritarian governments, citing EU policy, to roll out intrusive surveillance at home, undermining privacy and free expression worldwide.</p>
                    </div>
                </div>
                
                <p>
                    *EU politicians exempt themselves from this surveillance under "professional secrecy" rules.
                    They get privacy.<br> You and your family do not. Demand fairness.
                </p>
            </div>

        <!-- Member States Section -->
        <div id="member-states">
                <h2>Member State Positions</h2>
                <br>
                
                
            </div>

        <!-- Delegates Section -->
        <div id="delegates">
                <h2>Find Your Representatives</h2>
                <br>
                
                
            </div>

        <!-- Contact Tool Section -->
        <div id="contact-tool">
                <h2>Take Action!<br> Contact Your MEPs</h2>
                <h3>
                    Your privacy and freedoms are at risk.
                    These policies will impact every European—your messages, photos, and private conversations will be scanned without your consent.
                    But we have the power to stop this.
                    Contact your MEPs now with a clear message: NO to mass surveillance.
                    Your voice matters. Make it heard today.
                </h3>
                
            </div>

        <!-- Timeline Section -->
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diffusion language models are super data learners (168 pts)]]></title>
            <link>https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac</link>
            <guid>44856101</guid>
            <pubDate>Sun, 10 Aug 2025 16:04:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac">https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac</a>, See on <a href="https://news.ycombinator.com/item?id=44856101">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[AOL closes its dial up internet service (158 pts)]]></title>
            <link>https://www.ispreview.co.uk/index.php/2025/08/after-34-years-aol-finally-closes-its-dial-up-internet-service.html</link>
            <guid>44856090</guid>
            <pubDate>Sun, 10 Aug 2025 16:02:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ispreview.co.uk/index.php/2025/08/after-34-years-aol-finally-closes-its-dial-up-internet-service.html">https://www.ispreview.co.uk/index.php/2025/08/after-34-years-aol-finally-closes-its-dial-up-internet-service.html</a>, See on <a href="https://news.ycombinator.com/item?id=44856090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<div id="news_thumbpicture"><picture id="primaryimage"><img width="600" height="600" src="https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-600x600.webp" alt="aol uk homepage from the past" decoding="async" fetchpriority="high" srcset="https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-600x600.webp 600w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-300x300.webp 300w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-768x768.webp 768w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-150x150.webp 150w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657.webp 1000w" sizes="(max-width: 600px) 100vw, 600px"></picture></div><!-- Article Start --><p>In a somewhat surprising development, mainly because almost everybody assumed it had died a long time ago, <a href="https://www.aol.co.uk/" target="_blank" rel="noopener">AOL</a> (America Online) – one of the very first consumer ISPs in both the USA and UK – recently caused a stir again by announcing that it had “<em>decided to discontinue Dial-up Internet</em>” on 30th September 2025.<span id="more-42747"></span></p>
<p>According to <a href="https://help.aol.com/articles/dial-up-internet-to-be-discontinued" target="_blank" rel="noopener">AOL’s website</a>: “<em>AOL routinely evaluates its products and services and has decided to discontinue Dial-up Internet. This service will no longer be available in AOL plans. As a result, on September 30, 2025 this service and the associated software, the AOL Dialer software and AOL Shield browser, which are optimized for older operating systems and dial-up internet connections, will be discontinued</em>.” But their email service will continue.</p>
<p><strong>NOTE:</strong> Many <a href="https://www.ispreview.co.uk/index.php/link/dialup" target="_blank" rel="" title="dialup" data-chref="https://www.ispreview.co.uk/broadband_dialup.php">dialup</a> ISPs in the UK during 1995 – like AOL – used expensive premium rate numbers, although this did soon gravitate to local call rates and then unmetered via FRIACO. The v90 <a href="https://www.ispreview.co.uk/index.php/link/dialup" target="_blank" rel="" title="dialup" data-chref="https://www.ispreview.co.uk/broadband_dialup.php">dialup</a> standard (56Kbps capable or 0.056Mbps) didn’t arrive until 1998 and by then <a href="https://www.ispreview.co.uk/index.php/link/adsl" target="_blank" rel="" title="digital subscriber line" data-chref="https://www.ispreview.co.uk/broadband_DSL.php">ADSL</a> and cable broadband were just around the corner – ready to revolutionise the market.</p>
<p>The change appears to have been announced within the past few weeks, although it wasn’t picked up more widely until journalist <a href="https://tedium.co/" target="_blank" rel="noopener">Ernie Smith</a> noted it in a post on <a href="https://bsky.app/profile/ernie.tedium.co/post/3lvwjugziec2f" target="_blank" rel="noopener">Bluesky</a>. Just to be clear, the announcement above refers to the USA and Canada. However, we’re fairly confident that what remains of AOL UK (aka – <a href="https://www.ispreview.co.uk/index.php/go/tt" target="_blank" rel="nofollow" title="talktalk">TalkTalk</a>) doesn’t have any legacy dial-up customers left, although we would have said the same about the USA and Canada too, until that announcement dropped (dial-up speeds in 2025 would be practically unusable). ISPreview is currently checking, just to be sure.</p>

<p>In case anybody has forgotten. The original AOL UK experience was somewhat of a walled-garden way of accessing the internet, which forced you to use the company’s own software and restricted your ability to access certain internet services. This had the benefit of simplifying the experience, but AOL later fell behind the curve and ended up being overtaken by rivals.</p>
<p>The <a title="carphone warehouse" href="https://www.ispreview.co.uk/index.php/go/cpw" target="_blank" rel="nofollow">Carphone Warehouse</a> (CPW) ultimately won the auction to buy AOL UK’s Internet access business in 2006 for £370m (note: AOL’s content division became a separate business). At the time, AOL were the UK’s third-largest ISP with around <strong>2.1 million customers</strong> (600,000 on dial-up and 1.5 million with broadband) and were later re-branded to AOL Broadband.</p>
<p>A second big change occurred on 29th March 2010, when CPW and <a href="https://www.ispreview.co.uk/index.php/go/tt" target="_blank" rel="nofollow" title="talktalk">TalkTalk</a> separated (demerged) – the latter became a separate business, which included customers from CPW’s prior acquisitions (e.g. AOL Broadband, Tiscali etc.). Several more years passed until May 2014, when TalkTalk confirmed that AOL Broadband (formerly AOL UK) had stopped taking on new internet and phone customers (<a href="https://www.ispreview.co.uk/index.php/2014/05/uk-isp-aol-broadband-longer-available-new-customers.html">here</a>), although no mention was made of the dial-up base.</p>
<p>We’re certain that plenty of our readers (those now of a certain age group) will have stories to share of the early AOL UK days. Yours truly only used the original service briefly, before promptly switching away as the UK’s then dialup (narrowband) internet market became more competitive, affordable and less restrictive. It’s a service I was glad to forget, but it played an important role.</p>

<p><iframe title="AOL (Sign On - Dial Up)" width="500" height="281" src="https://www.youtube.com/embed/D1UY7eDRXrs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<!-- CONTENT END 1 -->
<!-- Article End -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zig's Lovely Syntax (212 pts)]]></title>
            <link>https://matklad.github.io/2025/08/09/zigs-lovely-syntax.html</link>
            <guid>44855881</guid>
            <pubDate>Sun, 10 Aug 2025 15:33:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2025/08/09/zigs-lovely-syntax.html">https://matklad.github.io/2025/08/09/zigs-lovely-syntax.html</a>, See on <a href="https://news.ycombinator.com/item?id=44855881">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article>
        <h2>
          Zig’s Lovely Syntax <time datetime="2025-08-09">Aug 9, 2025</time>
        </h2>
        <p>
          It’s a bit of a silly post, because syntax is the least interesting
          detail about the language, but, still, I can’t stop thinking how Zig
          gets this detail just right for the class of curly-braced languages,
          and, well, now you’ll have to think about that too.
        </p>
        <p>
          On the first glance, Zig looks almost exactly like Rust, because Zig
          borrows from Rust liberally. And I think that Rust has great syntax,
          considering all the semantics it needs to express (see
          <a href="https://matklad.github.io/2023/01/26/rusts-ugly-syntax.html">“Rust’s Ugly Syntax”</a>). But Zig improves on that, mostly by
          leveraging simpler language semantics, but also through some purely
          syntactical tasteful decisions.
        </p>
        <section id="Integer-Literals">
          <h2>
            <a href="#Integer-Literals">Integer Literals </a>
          </h2>
          <p>
            How do you spell a number ninety-two? Easy, <code>92</code>. But
            what type is that? Statically-typed languages often come with
            several flavors of integers: <code>u32</code>, <code>u64</code>,
            <code>u8</code>. And there’s often a syntax for literals of a
            particular types: <code>92u8</code>, <code>92l</code>, <code>92z</code>.
          </p>
          <p>
            Zig doesn’t have suffixes, because, in Zig, all integer literals
            have the same type: <code>comptime_int</code>:
          </p>

          <figure>
            <pre><code><span><span>const</span> an_integer = <span>92</span>;</span>
<span>assert(<span>@TypeOf</span>(an_integer) <span>==</span> <span>comptime_int</span>);</span></code></pre>
          </figure>
          <p>
            The value of an integer literal is known at compile time and is
            coerced to a specific type on assignment
            <span><code>const x: i32 = 92;</code></span>
            or ascription:
            <span><code>@as(i32, 92)</code></span>
          </p>
          <p>
            To emphasize, this is <em>not</em> type inference, this is implicit
            comptime coercion. This does mean that code like
            <span><code>var x = 92;</code></span>
            generally doesn’t work, and requires an explicit type.
          </p>
        </section>
        <section id="String-Literals">
          <h2>
            <a href="#String-Literals">String Literals </a>
          </h2>
          <p>Raw or multiline strings are spelled like this:</p>

          <figure>
            <pre><code><span><span>const</span> raw =</span>
<span>    <span>\\Roses are red</span></span>
<span>    <span>\\  Violets are blue,</span></span>
<span>    <span>\\Sugar is sweet</span></span>
<span>    <span>\\  And so are you.</span></span>
<span>    <span>\\</span></span>
<span>;</span></code></pre>
          </figure>
          <p>
            This syntax doesn’t require a special form for escaping <code>\\</code> itself:
          </p>

          <figure>
            <pre><code><span><span>const</span> still_raw =</span>
<span>    <span>\\const raw =</span></span>
<span>    <span>\\    <span>\\</span>Roses are red</span></span>
<span>    <span>\\    <span>\\</span>  Violets are blue,</span></span>
<span>    <span>\\    <span>\\</span>Sugar is sweet</span></span>
<span>    <span>\\    <span>\\</span>  And so are you.</span></span>
<span>    <span>\\    <span>\\</span></span></span>
<span>    <span>\\;</span></span>
<span>    <span>\\</span></span>
<span>;</span></code></pre>
          </figure>
          <p>
            It nicely dodges indentation problems that plague every other
            language with a similar feature. And, the best thing ever:
            lexically, each line is a separate token. As Zig has only
            line-comments, this means that <code>\n</code> is <em>always</em>
            whitespace. Unlike most other languages, Zig can be correctly lexed
            in a line-by-line manner.
          </p>
          <p>
            Raw strings is perhaps the biggest improvement of Zig over Rust.
            Rust brute-forces the problem with
            <code>r##""##</code> syntax, which does the required job,
            technically, but suffers from the mentioned problems: indentation is
            messy, nesting quotes requires adjusting hashes, unclosed raw
            literal breaks the following lexical structure completely, and
            rustfmt’s formatting of raw strings tends to be rather ugly. On the
            plus side, this syntax at least cannot be expressed by a
            context-free grammar!
          </p>
        </section>
        <section id="Record-Literals">
          <h2>
            <a href="#Record-Literals">Record Literals </a>
          </h2>
          <p>For the record, Zig takes C syntax (not that C would notice):</p>

          <figure>
            <pre><code><span><span>const</span> p: Point = .{</span>
<span>    .x = <span>1</span>,</span>
<span>    .y = <span>2</span>,</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The <code>.{</code> feels weird! It will make sense by the end of
            the post. Here, I want only to note <code>.x = 1</code>
            part, which matches the assignment syntax <code>obj.x = 1</code>.
            This is great! This means that grepping for
            <code>".x ="</code> gives you <em>all</em> instances where a field
            is written to. This is hugely valuable: most of usages are reads,
            but, to understand the flow of data, you only need to consider
            writes. Ability to mechanically partition the entire set of usages
            into majority of boring reads and a few interesting writes does
            wonders for code comprehension.
          </p>
        </section>
        <section id="Prefix-Types">
          <h2>
            <a href="#Prefix-Types">Prefix Types </a>
          </h2>
          <p>
            Where Zig departs from C the most is the syntax for types. C uses a
            needlessly confusing spiral rule. In Zig, all types are prefix:
          </p>

          <figure>
            <pre><code><span><span>u32</span>      <span>// An integer</span></span>
<span>[<span>3</span>]<span>u32</span>   <span>// An array of three integers</span></span>
<span>?[<span>3</span>]<span>u32</span>  <span>// An array of three integers or null</span></span>
<span></span>
<span><span>// A pointer to...</span></span>
<span><span>*</span><span>const</span> ?[<span>3</span>]<span>u32</span></span></code></pre>
          </figure>
          <p>
            While pointer type is prefix, pointer dereference is postfix, which
            is a more natural subject-verb order to read: <span><code>ptr.* = 92;</code></span>
          </p>
        </section>
        <section id="Identifiers">
          <h2>
            <a href="#Identifiers">Identifiers </a>
          </h2>
          <p>
            Zig has general syntax for “raw” identifiers:
            <span><code>@"a name which a space"</code></span>
            It is useful to avoid collisions with keywords, or for exporting a
            symbol whose name is otherwise not a valid Zig identifier. It is a
            bit more to type than Kotlin’s delightful
            <span><code>`a name with a space`</code>,</span> but
            manages to re-use Zig’s syntax for built-ins (<code>@TypeOf</code>)
            and strings.
          </p>
        </section>
        <section id="Functions">
          <h2>
            <a href="#Functions">Functions </a>
          </h2>
          <p>
            Like, Rust, Zig goes for <code>fn foo</code> function declaration
            syntax. This is such a massive improvement over C/Java style
            function declarations: it puts <code>fn</code> token (which is
            completely absent in traditional C family) and function name next to
            each other, which means that textual search for <code>fn name</code>
            allows you to quickly find the function. Then Zig adds a little
            twist. While in Rust we write
          </p>

          <figure>
            <pre><code><span><span>fn</span> <span>add</span>(x: <span>i32</span>, <span>i32</span>) <span>-&gt;</span> <span>i32</span></span></code></pre>
          </figure>
          <p>Zig is</p>

          <figure>
            <pre><code><span><span>fn</span><span> add</span>(x: <span>i32</span>, <span>i32</span>) <span>i32</span></span></code></pre>
          </figure>
          <p>
            The arrow is gone! Now that I’ve used this for some time, I find
            arrow very annoying to type, and adding to the visual noise. Rust
            needs the arrow: Rust has lambdas with an inferred return type, and,
            in a lambda, the return type is optional. So you need some sort of
            an explicit syntax to tell the parser if there is return type:
          </p>

          <figure>
            <pre><code><span>|| expression;</span>
<span>|| <span>-&gt;</span> Type { }</span></code></pre>
          </figure>
          <p>
            And its understandable that lambdas and functions would want to use
            compatible syntax. But Zig doesn’t have lambdas, so it just makes
            the type mandatory. So the main is
          </p>

          <figure>
            <pre><code><span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> {}</span></code></pre>
          </figure>
          <p>
            Related small thing, but, as name of the type, I think I like <code>void</code> more than <code>()</code>.
          </p>
        </section>
        <section id="Locals">
          <h2>
            <a href="#Locals">Locals </a>
          </h2>
          <p>
            Zig is using <code>const</code> and <code>var</code> for binding
            values to names:
          </p>

          <figure>
            <pre><code><span><span>const</span> mid = lo <span>+</span> <span>@divFloor</span>(hi <span>-</span> lo, <span>2</span>);</span></code></pre>
          </figure>
          <p>
            This is ok, a bit weird after Rust’s, whose <code>const</code> would
            be <code>comptime</code> in Zig, but not really noticeable after
            some months. I do think this particular part is not great, because
            <code>const</code>, the more frequent one, is longer. I think Kotlin
            nails it: <code>val</code>, <code>var</code>, <code>fun</code>. Note
            all three are monosyllable, unlike <code>const</code> and <code>fn</code>! Number of syllables matters more than the number of
            letters!
          </p>
          <p>Like Rust, Zig uses</p>

          <figure>
            <pre><code><span><span>'name'</span> (<span>':'</span> Type)?</span></code></pre>
          </figure>
          <p>syntax for ascribing types, which is better than</p>

          <figure>
            <pre><code><span>Type <span>'name'</span></span></code></pre>
          </figure>
          <p>
            because optional suffixes are easier to parse visually and
            mechanically than optional prefixes.
          </p>
        </section>
        <section id="Conjunction-Is-Control-Flow">
          <h2>
            <a href="#Conjunction-Is-Control-Flow">Conjunction Is Control Flow
            </a>
          </h2>
          <p>
            Zig doesn’t use <code>&amp;&amp;</code> and <code>||</code> and
            spells the relevant operators as <code>and</code> and <code>or</code>:
          </p>

          <figure>
            <pre><code><span><span>while</span> (count &gt; <span>0</span> <span>and</span> ascii.isWhitespace(buffer[count <span>-</span> <span>1</span>])) {</span></code></pre>
          </figure>
          <p>
            This is easier to type and much easier to read, but there’s also a
            deeper reason why they are not sigils. Zig marks any control flow
            with a keyword. And, because boolean operators short-circuit, they
            <em>are</em> control flow! Treating them as normal binary operator
            leads to an entirely incorrect mental model. For bitwise operations,
            Zig of course uses <code>&amp;</code> and <code>|</code>.
          </p>
        </section>
        <section id="Explicit-return">
          <h2>
            <a href="#Explicit-return">Explicit return </a>
          </h2>
          <p>
            Both Zig and Rust have statements and expressions. Zig is a bit more
            statement oriented, and requires explicit returns:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> add</span>(x: <span>i32</span>, y: <span>i32</span>) <span>i32</span> {</span>
<span>  <span>return</span> x <span>+</span> y;</span>
<span>}</span></code></pre>
          </figure>
          <p>
            Furthermore, because there are no lambdas, scope of return is always
            clear.
          </p>
          <p>
            Relatedly, the value of a block expression is void. A block is a
            list of statements, and doesn’t have an optional expression at the
            end. This removes the semicolon problem — while Rust rules around
            semicolons are sufficiently clear (until you get to macros), there’s
            some constant mental overhead to getting them right all the time.
            Zig is more uniform and mechanical here.
          </p>
          <p>
            If you need a block that yields a value, Zig supports a general
            syntax for breaking out of a labeled block:
          </p>

          <figure>
            <pre><code><span><span>const</span> header_oldest = blk: {</span>
<span>    <span>var</span> oldest: ?<span>usize</span> = <span>null</span>;</span>
<span>    <span>for</span> (headers.slice, <span>0</span>..) <span>|</span><span>*</span>header, i<span>|</span> {</span>
<span>        <span>switch</span> (Headers.dvc_header_type(header)) {</span>
<span>            .blank =&gt; assert(i &gt; <span>0</span>),</span>
<span>            .valid =&gt; oldest = i,</span>
<span>        }</span>
<span>    }</span>
<span>    <span>break</span> :blk <span>&amp;</span>headers.slice[oldest.?];</span>
<span>};</span></code></pre>
          </figure>
        </section>
        <section id="If">
          <h2>
            <a href="#If">If </a>
          </h2>
          <p>
            Rust makes pedantically correct choice regarding <code>if</code>s:
            braces are mandatory:
          </p>

          <figure>
            <pre><code><span><span>if</span> cond1 {</span>
<span>  case_a</span>
<span>} <span>else</span> {</span>
<span>  <span>if</span> cond2 {</span>
<span>    case_b</span>
<span>  } <span>else</span> {</span>
<span>    case_c</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            This removes the dreaded “dangling else” grammatical ambiguity.
            While theoretically nice, it makes
            <code>if</code>-expression one-line feel too heavy. It’s not the
            braces, it’s the whitespace around them:
          </p>

          <figure>
            <pre><code><span>if (a) b else c</span>
<span>if a { b } else { c }</span></code></pre>
          </figure>
          <p>
            But the ternary is important! Exploding a simple choice into
            multi-line condition <em>hurts</em>
            readability. Zig goes with traditional choice of making parentheses
            required and braces optional:
          </p>

          <figure>
            <pre><code><span>  .direction = <span>if</span> (prng.boolean()) .ascending <span>else</span> .descending,</span></code></pre>
          </figure>
          <p>
            By itself, this does create a risk of <code>goto: fail;</code> style
            bugs. But in Zig formatter (non-configurable, user-directed) is a
            part of the compiler, and formatting errors that can mask bugs are
            caught during compilation. For example, <code>1 -2</code> is an
            error due to inconsistent whitespace around the minus sign, which
            signals a plausible mixup of infix and binary minus. No such errors
            are currently produced for incorrect indentation (the value add
            there is relatively little, given <code>zig fmt</code>), but this is
            planned.
          </p>
          <p>
            NB: because Rust requires <code>if</code> branches to be blocks, it
            is forced to make <code>{ expr }</code> synonym with
            <code>(expr)</code>. Otherwise, the ternary <code>if</code> would be
            even more unusable! Syntax design is tricky! Whether you need <code>return</code>s and whether you make <code>()</code> or <code>{}</code> mandatory in ifs are not orthogonal!
          </p>
        </section>
        <section id="Loops">
          <h2>
            <a href="#Loops">Loops </a>
          </h2>
          <p>
            Like Python, Zig allows <code>else</code> on loops. Unlike Python,
            loops are expressions, which leads to a nicely readable imperative
            searches:
          </p>

          <figure>
            <pre><code><span><span>pub</span> <span>const</span> Word = <span>for</span> (.{ <span>u8</span>, <span>u16</span>, <span>u32</span>, <span>u64</span>, <span>u128</span>, <span>u256</span> }) <span>|</span>W<span>|</span> {</span>
<span>    <span>if</span> (<span>@bitSizeOf</span>(W) &gt;= bitset_capacity) <span>break</span> W;</span>
<span>} <span>else</span> <span>unreachable</span>;</span></code></pre>
          </figure>
          <p>
            Zig doesn’t have syntactically-infinite loop like Rust’s <code>loop
              {</code> or Go’s <code>for {</code>. Normally I’d consider that a
            drawback, because these loops produce different control flow,
            affecting reachability analysis in the compiler, and I don’t think
            it’s great to make reachability dependent on condition being visibly
            constant. But! As Zig places <code>comptime</code> semantics front
            and center, and the rules for what is and isn’t a comptime constant
            are a backbone of every feature, “anything equivalent to
            <code>while (true)</code>” becomes sufficiently precise.
            Incidentally, these days I tend to write “infinite” loops as
          </p>

          <figure>
            <pre><code><span><span>for</span> (<span>0</span>..safety_bound) <span>|</span>_<span>|</span> {</span>
<span></span>
<span>} <span>else</span> <span>@panic</span>(<span>"loop safety counter exceeded"</span>);</span></code></pre>
          </figure>
          <p>
            Almost always there is an up-front bound for the number of
            iterations until the break, and its worth asserting this bound,
            because debugging crashes is easier than debugging hangs.
          </p>
          <p>
            <code>for</code>, <code>while</code>, <code>if</code>, <code>switch</code>, and <code>catch</code> all use the same Ruby/Rust
            inspired syntax for naming captured values:
          </p>

          <figure>
            <pre><code><span><span>for</span> (slice) <span>|</span>element<span>|</span> {</span>
<span>  use(element);</span>
<span>}</span>
<span></span>
<span><span>while</span> (iterator.next()) <span>|</span>element<span>|</span> {</span>
<span>  use(element);</span>
<span>}</span></code></pre>
          </figure>
          <p>
            I like how the iterator comes first, and then the name of an item
            follows, logically and syntactically.
          </p>
        </section>
        <section id="Clarity-of-Names">
          <h2>
            <a href="#Clarity-of-Names">Clarity of Names </a>
          </h2>
          <p>
            I have a very strong opinion about variable shadowing. It goes both
            ways: I spent hours debugging code which incorrectly tried to use a
            variable that was shadowed by something else, but I also spent hours
            debugging code that accidentally used a variable that should have
            been shadowed! I really don’t know whether on balance it is better
            to forbid or encourage shadowing!
          </p>
          <p>
            Zig of course forbids shadowing, but what’s curious is that it’s
            just one episode of the large crusade against any complexity in name
            resolution. There’s no “prelude”, if you want to use anything from
            std, you need to import it:
          </p>

          <figure>
            <pre><code><span><span>const</span> std = <span>@import</span>(<span>"std"</span>);</span></code></pre>
          </figure>
          <p>
            There are no glob imports, if you want to use an item from std, you
            need to import it:
          </p>

          <figure>
            <pre><code><span><span>const</span> ArrayList = std.ArrayList;</span></code></pre>
          </figure>
          <p>
            Zig doesn’t have inheritance, mixins, argument-dependent lookup,
            extension functions, implicit or traits, so, if you see <code>x.foo()</code>, that <code>foo</code> is guaranteed to be a boring
            method declared on <code>x</code>
            type. Similarly, while Zig has powerful comptime capabilities, it
            <a href="https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html">intentionally disallows</a>
            declaring methods at compile time.
          </p>
          <p>
            Like Rust, Zig used to allow a method and a field to share a name,
            because it actually is syntactically clear enough at the call site
            which is which. But then this feature got removed from Zig.
          </p>
          <p>
            More generally, Zig doesn’t have namespaces. There can be only one
            kind of <code>foo</code> in scope, while Rust allows things like
          </p>

          <figure>
            <pre><code><span><span>struct</span> <span>Point</span> { x: <span>i32</span>, y: <span>i32</span> }</span>
<span><span>fn</span> <span>Point</span>(x: <span>i32</span>, y: <span>i32</span>) <span>-&gt;</span> Point { Point { x, y } }</span></code></pre>
          </figure>
          <p>
            I am astonished at the relative lack of inconvenience in Zig’s
            approach. Turns out that <code>foo.bar.baz</code>
            is all the syntax you’ll ever need for accessing things? For the
            historically inclined, see “The module naming situation” thread in
            the
            <a href="https://github.com/brson/rust-dev-archives">rust mailing list archive</a>
            to learn the story of how rust got its <code>std::vec</code> syntax.
          </p>
        </section>
        <section id="Everything-Is-an-Expression">
          <h2>
            <a href="#Everything-Is-an-Expression">Everything Is an Expression
            </a>
          </h2>
          <p>
            The lack of namespaces touches on the most notable (by its absence)
            feature of Zig syntax, which deeply relates to the most profound
            aspect of Zig’s semantics. Everything is an expression. By which I
            mean, there’s no separate syntactic categories of values, types, and
            patterns. Values, types, and patterns are of course different
            things. And usually in the language grammar it is <em>syntactically</em>
            obvious whether a particular text fragment refers to a type or a
            value:
          </p>

          <figure>
            <pre><code><span><span>let</span> <span>PATTERN</span>: TYPE = VALUE;</span></code></pre>
          </figure>
          <p>
            So the standard way is to have separate syntax families for the
            three categories, which need to be internally unambiguous, but <em>can</em> be ambiguous across the categories because the place in
            the grammar dictates the category: when parsing <code>let</code>,
            everything until <code>:</code> is a pattern, stuff between
            <code>:</code> and <code>=</code> is a type, and after <code>=</code> we have a value.
          </p>
          <p>
            There are two problems here. First, there’s a combinatorial
            explosion of sorts in the syntax, because, while three categories
            describe different things, it turns out that they have the same
            general tree-ish shape.
          </p>
          <p>
            The second problem is that it might be hard to maintain category
            separation in the grammar. Rust
            <em>started</em> with the three categories separated by a bright
            line. But then, changes happen. Originally, Rust only allowed
            <span><code>VALUE = VALUE;</code></span>
            syntax for assignment. But today you can also write
            <span><code>PATTERN = VALUE;</code></span>
            to do unpacking like
            <span><code>(a, b) = (b, a);</code></span>
          </p>
          <p>
            Similarly, the turbofish used to move the parser from the value to
            the type mode, but now const parameters are values that can be found
            in the type position!
          </p>
          <p>
            The alternative is not to pick this fight at all. Rather than trying
            to keep the categories separately in the syntax, use the same
            surface syntax to express all three, and categorize later, during
            semantic analysis. In fact, this is already happens in the <span><code>VALUE = VALUE</code></span>
            example — these are different things! One is a place (lvalue) and
            another is a “true” value (rvalue), but we use the same syntax for
            both.
          </p>
          <p>
            I don’t think such syntactic unification necessarily implies
            semantic unification, but Zig does treat everything uniformly, as a
            value with comptime and runtime behavior (for some values, runtime
            behavior may be missing, for others — comptime):
          </p>

          <figure>
            <pre><code><span><span>const</span> E = <span>enum</span> { a, b };</span>
<span></span>
<span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> {</span>
<span>    <span>const</span> e: <span>if</span> (<span>true</span>) E <span>else</span> <span>void</span> = .a;</span>
<span>    _ = <span>switch</span> (e) {</span>
<span>        (<span>if</span> (<span>true</span>) .a <span>else</span> .b) =&gt; .a,</span>
<span>        (<span>if</span> (<span>true</span>) .b <span>else</span> .a) =&gt; .b,</span>
<span>    };</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The fact that you can write an <code>if</code> where a type goes is
            occasionally useful. But the fact that simple types look like simple
            values syntactically consistently make the language feel
            significantly less busy.
          </p>
        </section>
        <section id="Generics">
          <h2>
            <a href="#Generics">Generics </a>
          </h2>
          <p>
            As a special case of everything being an expression, instances of
            generic types look like this:
            <span><code>ArrayList(u32)</code></span>
          </p>
          <p>
            Just a function call! Though, there’s some resistance to trickery
            involved to make this work. Usually, languages rely on type
            inference to allow eliding generic arguments. That in turn requires
            making argument <em>syntax</em> optional, and that in turn leads to
            separating generic and non-generic arguments into separate parameter
            lists and some introducer sigil for generics, like <code>::&lt;&gt;</code> or
            <code>!()</code>.
          </p>
          <p>
            Zig solves this syntactic challenge in the most brute-force way
            possible. Generic parameters are never inferred, if a function takes
            3 comptime arguments and 2 runtime arguments, it will always be
            called with 5 arguments syntactically. Like with the (absence of)
            importing flourishes, a reasonable reaction would be “wait, does
            this mean that I’ll have to specify the types all the time?” And,
            like with import, in practice this is a non-issue. The trick are
            comptime closures. Consider a generic
            <code>ArrayList</code>:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> ArrayListType</span>(<span>comptime</span> T: <span>type</span>) <span>type</span> {</span>
<span>    <span>return</span> <span>struct</span> {</span>
<span>        <span>const</span> ArrayList = <span>@This</span>();</span>
<span></span>
<span>        <span>fn</span><span> init</span>(gpa: Allocator) ArrayList {}</span>
<span>        <span>fn</span><span> deinit</span>(list: <span>*</span>ArrayList, gpa: Allocator) <span>void</span> {}</span>
<span>        <span>fn</span><span> push</span>(list: <span>*</span>ArrayList, item: T) <span>!</span><span>void</span> {}</span>
<span>    };</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> usage</span>(gpa: Allocator) <span>!</span><span>void</span> {</span>
<span>    <span>var</span> xs: ArrayListType(<span>u32</span>) = .init(gpa);</span>
<span>    <span>defer</span> xs.deinit(gpa);</span>
<span></span>
<span>    <span>try</span> xs.push(<span>92</span>);</span>
<span>}</span></code></pre>
          </figure>
          <p>
            We have to specify type <code>T</code> when creating an instance of
            an <code>ArrayList</code>. But subsequently, when we are <em>using</em> the array list, we don’t have to specify the type
            parameter again, because the type of
            <code>xs</code> variable already closes over <code>T</code>. This is
            the major truth of object-orienting programming, the truth so
            profound that no one even notices it: in real code, 90% of functions
            are happiest as (non-virtual) methods. And, because of that, the
            annotation burden in real-world Zig programs is low.
          </p>
        </section>
        <section id="Declaration-Literals">
          <h2>
            <a href="#Declaration-Literals">Declaration Literals </a>
          </h2>
          <p>
            While Zig doesn’t have Hindley-Milner constraint-based type
            inference, it relies heavily on one specific way to propagate types.
            Let’s revisit the first <code>comptime_int</code> example:
          </p>

          <figure>
            <pre><code><span><span>const</span> x = <span>if</span> (condition()) <span>1</span> <span>else</span> <span>2</span>;</span></code></pre>
          </figure>
          <p>
            This doesn’t compile: <code>1</code> and <code>2</code> are
            different <code>comptime</code> values, we can’t select between two
            at runtime because they are different. We need to coerce the
            constants to a specific runtime type:
          </p>

          <figure>
            <pre><code><span><span>const</span> x: <span>u32</span> = <span>if</span> (condition()) <span>1</span> <span>else</span> <span>2</span>;</span>
<span></span>
<span><span>const</span> x = <span>@coerceTo</span>(</span>
<span>  <span>u32</span>,</span>
<span>  <span>if</span> (condition()) <span>1</span> <span>else</span> <span>2</span>,</span>
<span>);</span></code></pre>
          </figure>
          <p>
            But this doesn’t kick the can sufficiently far enough and
            essentially reproduces the <code>if</code> with two incompatible
            branches. We need to sink coercion down the branches:
          </p>

          <figure>
            <pre><code><span><span>const</span> x = <span>if</span> (condition())</span>
<span>    <span>@coerceTo</span>(<span>u32</span>, <span>1</span>)</span>
<span><span>else</span></span>
<span>    <span>@coerceTo</span>(<span>u32</span>, <span>2</span>);</span></code></pre>
          </figure>
          <p>
            And that’s exactly how Zig’s “Result Location Semantics” works. Type
            “inference” runs a simple left-to-right tree-walking algorithm,
            which resembles interpreter’s <code>eval</code>. In fact, <code>eval</code> is
            <em>exactly</em> what happens. Zig is not a compiler, it is an
            interpreter. When <code>zig</code> evaluates an expression, it gets:
          </p>
          <ul>
            <li>
              expression’s type (as a Zig value),
            </li>
            <li>
              expression’s value (if it can be evaluated at comptime),
            </li>
            <li>
              code to compute expression’s value otherwise.
            </li>
          </ul>

          <figure>
            <pre><code><span>eval("1 + 2") =</span>
<span>  3</span>
<span></span>
<span>eval("f() + g()") =</span>
<span>  $1 = call 'f'</span>
<span>  $2 = call 'g'</span>
<span>  $3 = add $1, $2</span>
<span></span>
<span>eval("f() + 2") =</span>
<span>  $1 = call 'f'</span>
<span>  $2 = add $1,  imm 2</span></code></pre>
          </figure>
          <p>When interpreting code like</p>

          <figure>
            <pre><code><span>obj.field = if (condition()) 1 else 2;</span></code></pre>
          </figure>
          <p>
            the interpreter passes the result location (<code>obj.field</code>)
            and type down the tree of subexpressions. If branches store result
            directly into object field (there’s a <code>store</code> inside each
            branch, as opposed to one <code>store</code> after the <code>if</code>), and each coerces its comptime constant to the
            appropriate runtime type of the result.
          </p>
          <p>
            This mechanism enables concise <code>.variant</code> syntax for
            specifying enums:
          </p>

          <figure>
            <pre><code><span><span>const</span> E = <span>enum</span> { a, b };</span>
<span></span>
<span><span>fn</span><span> example</span>(e: E) <span>u32</span> {</span>
<span>    <span>return</span> <span>switch</span> (e) {</span>
<span>        .a =&gt; <span>1</span>,</span>
<span>        (<span>if</span> (<span>true</span>) .b <span>else</span> .a) =&gt; <span>2</span>,</span>
<span>    };</span>
<span>}</span></code></pre>
          </figure>
          <p>
            When <code>zig</code> evaluates the switch, it first evaluates the
            scrutinee, and realizes that it has type
            <code>E</code>. When evaluating <code>switch</code> arm, it sets
            result type to <code>E</code> for the condition, and a literal <code>.a</code>
            gets coerced to <code>E</code>. The same happens for the second arm,
            where result type further sinks down the
            <code>if</code>.
          </p>
          <p>
            Result type semantics also explains the leading dot in the record
            literal syntax:
          </p>

          <figure>
            <pre><code><span><span>const</span> p: Point = .{</span>
<span>    .x = <span>1</span>,</span>
<span>    .y = <span>2</span>,</span>
<span>};</span></code></pre>
          </figure>
          <p>
            Syntactically, we just want to disambiguate records from blocks.
            But, semantically, we want to coerce the literal to whatever type we
            want to get out of this expression. In Zig, <code>.whatever</code>
            is a shorthand for <code>@ResultType().whatever</code>.
          </p>
          <p>
            I must confess that <code>.{}</code> did weird me out a lot at first
            during <em>writing</em> code (I don’t mind reading the dot). It’s
            not the easiest thing to type! But that was fixed once I added <code>..</code> snippet, expanding to <code>.{$0}</code>.
          </p>
          <p>
            The benefits to lightweight record literal syntax are huge, as they
            allow for some pretty nice APIs. In particular, you get named and
            default arguments for free:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> exec</span>(argv: []<span>const</span> <span>u8</span>, options: <span>struct</span> {</span>
<span>    working_directory: ?[]<span>const</span> <span>u8</span> = <span>null</span></span>
<span>}) <span>!</span><span>void</span> {</span>
<span>    <span>// ...</span></span>
<span>}</span>
<span></span>
<span><span>fn</span><span> usage</span>() <span>!</span><span>void</span> {</span>
<span>    <span>try</span> exec(<span>&amp;</span>.{ <span>"git"</span>, <span>"status"</span>}, .{});</span>
<span></span>
<span>    <span>try</span> exec(<span>&amp;</span>.{ <span>"git"</span>, <span>"status"</span>}, .{</span>
<span>        .working_directory = <span>"./src"</span>,</span>
<span>    });</span>
<span>}</span></code></pre>
          </figure>
          <p>
            I don’t really miss the absence of named arguments in Rust, you can
            always design APIs without them. But they are free in Zig, so I use
            them liberally. Syntax wise, we get two features (calling functions
            and initializing objects) for the price of one!
          </p>
        </section>
        <section id="Built-ins">
          <h2>
            <a href="#Built-ins">Built-ins </a>
          </h2>
          <p>
            Finally, the thing that weirds out some people when they see Zig
            code, and makes others reconsider their choice GitHub handles, even
            when they haven’t seen any Zig: <code>@divExact</code> syntax for
            built-in functions.
          </p>
          <p>
            Every language needs to glue “userspace” code with primitive
            operations supported by the compiler. Usually, the gluing is
            achieved by making the standard library privileged and allowing it
            to define intrinsic functions without bodies, or by adding ad-hoc
            operators directly to the language (like Rust’s <code>as</code>).
            And Zig does have a fair amount of operators, like <code>+</code> or
            <code>orelse</code>. But the release valve for a lot of
            functionality are built-in functions in distinct syntactic
            namespace, so Zig separates out <code>@bitCast</code>, <code>@addrSpaceCast</code>, <code>@alignCast</code>, <code>@constCast</code>, <code>@ptrCast</code>, <code>@intCast</code>,
            <code>@floatCast</code>, <code>@volatileCast</code>, <code>@ptrFromInt</code>, and <code>@intFromPtr</code>. There’s no need
            to overload casting when you can give each variant a name.
          </p>
          <p>
            There’s also <span><code>@as(i32, 92)</code></span>
            for type ascription. The types goes first, because the mechanism
            here is result type semantics: <code>@as</code> evaluates the first
            argument as a type, and then uses that as the type for the second
            argument. Curiously, <code>@as</code> I think actually can be
            implemented in the userspace:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> as</span>(<span>comptime</span> T: <span>type</span>, value: T) T {</span>
<span>    <span>return</span> value;</span>
<span>}</span></code></pre>
          </figure>
          <p>
            In Zig, a type of function parameter may depend on values of
            preceding (comptime) ones!
          </p>
          <p>
            My favorite builtin is <code>@import()</code>. First, it’s the most
            obvious way to import code:
            <span><code>const foo =
                @import("./foo.zig")</code></span>
            Its crystal clear where the file comes from.
          </p>
          <p>
            But, second, it is an instance of reverse syntax sugar. You see,
            import isn’t really a function. You can’t do
          </p>

          <figure>
            <pre><code><span><span>const</span> name = <span>"./foo.zig"</span>;</span>
<span><span>const</span> foo = <span>@import</span>(name);</span></code></pre>
          </figure>
          <p>
            The argument of <code>@import</code> has to be a string,
            syntactically. It really is
            <span><code>import "./path.zig"</code></span>
            syntax, except that the function-call form is re-used, because it
            already has the right shape.
          </p>
          <hr>
          <p>
            So, this is it. Just a bunch of silly syntactical decisions, which
            add up to a language which is positively enjoyable to read. As for
            big lessons, obviously, the less features your language has, the
            less syntax you’ll need. And less syntax is generally good, because
            varied syntactic constructs tend to step on each other toes.
            Languages are not combinations of orthogonal aspects. Features tug
            and pull the language in different directions and their combinations
            might turn to be miraculous features in their own right, or might
            drag the language down.
          </p>
          <p>
            Even with a small feature-set fixed, there’s still a lot of work to
            pick a good concrete syntax: unambiguous to parse, useful to grep,
            easy to read and not to painful to write. A smart thing is of course
            to steal and borrow solutions from other languages, not because of
            familiarity, but because the ruthless natural selection tends to
            weed out poor ideas. But there’s a lot of inertia in languages, so
            there’s no need to fear innovation. If an odd-looking syntax is
            actually good, people will take to it.
          </p>
          <p>
            Is there anything about Zig’s syntax I don’t like? I thought no,
            when starting this post. But in the process of writing it I did
            discover one form that annoys me. It is the while with the increment
            loop:
          </p>

          <figure>
            <pre><code><span><span>var</span> i: <span>u32</span> = <span>0</span>;</span>
<span><span>while</span> (i &lt; <span>10</span>) : (i<span>+=</span><span>1</span>) {</span>
<span>    print(<span>"{d}"</span>, .{i});</span>
<span>}</span></code></pre>
          </figure>
          <p>
            This is two-thirds of a C-style <code>for</code> loop (without the
            declarator), and it sucks for the same reason: control flow jumps
            all other the place and is unrelated to the source code order. We go
            from condition, to the body, to the increment. But in the source
            order the increment is between the condition and the body. In Zig,
            this loop sucks for one additional reason: that <code>:</code>
            separating the increment I think is the single example of control
            flow in Zig that is expressed by a sigil, rather than a keyword.
          </p>
          <p>
            This form used to be rather important, as Zig lacked a counting
            loop. It has
            <span><code>for(0..10) |i|</code></span>
            form now, so I am tempted to call the while-with-increment
            redundant.
          </p>
          <p>Annoyingly,</p>

          <figure>
            <pre><code><span><span>while</span> (condition) {</span>
<span>    <span>defer</span> increment;</span>
<span></span>
<span>    body</span>
<span>}</span></code></pre>
          </figure>
          <p>is <em>almost</em> equivalent to</p>

          <figure>
            <pre><code><span><span>while</span> (condition) : (increment) {</span>
<span>  body</span>
<span>}</span></code></pre>
          </figure>
          <p>
            But not exactly: if <code>body</code> contains a <code>return</code>, <code>break</code> or <code>try</code>, the <code>defer</code> version would run the
            <code>increment</code> one extra time, which is useless and might be
            outright buggy. Oh well.
          </p>
        </section>
      </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-OSS vs. Qwen3 and a detailed look how things evolved since GPT-2 (367 pts)]]></title>
            <link>https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the</link>
            <guid>44855690</guid>
            <pubDate>Sun, 10 Aug 2025 15:06:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the">https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the</a>, See on <a href="https://news.ycombinator.com/item?id=44855690">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>OpenAI just released their new open-weight LLMs this week: gpt-oss-120b and gpt-oss-20b, their first open-weight models since GPT-2 in 2019. And yes, thanks to some clever optimizations, they can run locally (but more about this later).</p><p>This is the first time since GPT-2 that OpenAI has shared a large, fully open-weight model. Earlier GPT models showed how the transformer architecture scales. The 2022 ChatGPT release then made these models mainstream by demonstrating concrete usefulness for writing and knowledge (and later coding) tasks. Now they have shared some long-awaited weight model, and the architecture has some interesting details.</p><p>I spent the past few days reading through the code and technical reports to summarize the most interesting details. (Just days after, OpenAI also announced GPT-5, which I will briefly discuss in the context of the gpt-oss models at the end of this article.)</p><p>Below is a quick preview of what the article covers. For easier navigation, I recommend using the Table of Contents on the left of on the article page.</p><ul><li><p>Model architecture comparisons with GPT-2</p></li><li><p>MXFP4 optimization to fit gpt-oss models onto single GPUs</p></li><li><p>Width versus depth trade-offs (gpt-oss vs Qwen3)</p></li><li><p>Attention bias and sinks</p></li><li><p>Benchmarks and comparisons with GPT-5</p></li></ul><p>I hope you find it informative!</p><p>Before we discuss the architecture in more detail, let's start with an overview of the two models, gpt-oss-20b and gpt-oss-120b, shown in Figure 1 below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!rlW0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!rlW0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 424w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 848w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1272w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png" width="1456" height="681" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:681,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:243817,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!rlW0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 424w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 848w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1272w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Figure 1: The two gpt-oss models side by side.</figcaption></figure></div><p><span>If you have looked at recent LLM architecture diagrams before, or read my previous </span><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="">Big Architecture Comparison</a><span> article, you may notice that there is nothing novel or unusual at first glance. </span></p><div data-component-name="DigestPostEmbed"><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!83ox!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd72e5a99-1a11-42b7-8831-8f5785ed2bc1_1600x1116.png"><img src="https://substackcdn.com/image/fetch/$s_!83ox!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd72e5a99-1a11-42b7-8831-8f5785ed2bc1_1600x1116.png" sizes="100vw" alt="The Big LLM Architecture Comparison" width="140" height="140"></picture></div></a></div><p>This is not surprising, since leading LLM developers tend to use the same base architecture and then apply smaller tweaks. This is pure speculation on my part, but I think this is because</p><ul><li><p>There is significant rotation of employees between these labs.</p></li><li><p><span>We still have not found anything better than the transformer architecture. Even though state space models and text diffusion models exist, as far as I know no one has shown that they perform as well as transformers at this scale. (Most of the comparisons I found focus only on benchmark performance. It is still unclear how well the models handle real-world, multi-turn writing and coding tasks. At the time of writing, the highest-ranking non-purely-transformer-based model on the </span><a href="https://lmarena.ai/leaderboard/text" rel="">LM Arena</a><span> is Jamba, which is a transformer–state space model hybrid, at rank 96.)</span></p></li><li><p>Most of the gains likely come from data and algorithm tweaks rather than from major architecture changes.</p></li></ul><p>That being said, there are still many interesting aspects of their design choices. Some are shown in the figure above (while others are not, but we will discuss them later as well). In the rest of this article, I will highlight these features and compare them to other architectures, one at a time.</p><p>I should also note that I am not affiliated with OpenAI in any way. My information comes from reviewing the released model code and reading their technical reports. If you want to learn how to use these models locally, the best place to start is OpenAI's official model hub pages:</p><ul><li><p><a href="https://huggingface.co/openai/gpt-oss-20b" rel="">https://huggingface.co/openai/gpt-oss-20b</a></p></li><li><p><a href="https://huggingface.co/openai/gpt-oss-120b" rel="">https://huggingface.co/openai/gpt-oss-120b</a></p></li></ul><p>The 20B model can run on a consumer GPU with up to 16 GB of RAM. The 120B model can run on a single H100 with 80 GB of RAM or newer hardware. I will return to this later, as there are some important caveats.</p><p>Before we jump into comparisons between gpt-oss and a more recent architecture, let's hop into the time machine and take a side-by-side look at GPT-2 (Figure 2) to see just how far things have come.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!AsnD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AsnD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 424w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 848w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1272w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png" width="1456" height="788" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:788,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:267271,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!AsnD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 424w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 848w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1272w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 2: A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B.</figcaption></figure></div><p><span>Both gpt-oss and GPT-2 are decoder-only LLMs built on the transformer architecture introduced in the </span><a href="https://arxiv.org/abs/1706.03762" rel="">Attention Is All You Need (2017)</a><span> paper. Over the years, many details have evolved.</span></p><p><span>However, these changes are not unique to gpt-oss. And as we will see later, they appear in many other LLMs. Since I discussed many of these aspects in the previous </span><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="">Big Architecture Comparison</a><span> article, I will try to keep each subsection brief and focused.</span></p><p><a href="https://arxiv.org/abs/1207.0580" rel="">Dropout (2012)</a><span> is a traditional technique to prevent overfitting by randomly "dropping out" (i.e., setting to zero) a fraction of the layer activations or attention scores (Figure 3) during training. However, dropout is rarely used in modern LLMs, and most models after GPT-2 have dropped it (no pun intended).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!BS-w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!BS-w!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 424w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 848w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1272w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png" width="554" height="557.2781065088758" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:850,&quot;width&quot;:845,&quot;resizeWidth&quot;:554,&quot;bytes&quot;:130475,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!BS-w!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 424w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 848w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1272w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 3: An illustration of dropout applied to the attention score matrix.</figcaption></figure></div><p>I assume that dropout was originally used in GPT-2 because it was inherited from the original transformer architecture. Researchers likely noticed that it does not really improve LLM performance (I observed the same in my small-scale GPT-2 replication runs). This is likely because LLMs are typically trained for only a single epoch over massive datasets, which is in contrast to the multi-hundred-epoch training regimes for which dropout was first introduced. So, since LLMs see each token only once during training, there is little risk of overfitting.</p><p><span>Interestingly, while Dropout is kind of ignored in LLM architecture design for many years, I found a </span><a href="https://arxiv.org/abs/2505.24788" rel="">2025 research paper</a><span> with small scale LLM experiments (Pythia 1.4B) that confirms that Dropout results in worse downstream performance in these single-epoch regimes.</span></p><p>In transformer-based LLMs, positional encoding is necessary because of the attention mechanism. By default, attention treats the input tokens as if they have no order. In the original GPT architecture, absolute positional embeddings addressed this by adding a learned embedding vector for each position in the sequence (Figure 4), which is then added to the token embeddings.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!YCov!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!YCov!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 424w, https://substackcdn.com/image/fetch/$s_!YCov!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 848w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1272w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png" width="1195" height="533" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:533,&quot;width&quot;:1195,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123823,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!YCov!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 424w, https://substackcdn.com/image/fetch/$s_!YCov!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 848w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1272w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 4: Illustration of absolute positional embeddings.</figcaption></figure></div><p><span>RoPE (</span><a href="https://arxiv.org/abs/2104.09864" rel="">Rotary Position Embedding</a><span>) introduced a different approach: instead of adding position information as separate embeddings, it encodes position by rotating the query and key vectors in a way that depends on each token's position. (RoPE is an elegant idea but also a bit of a tricky topic to explain. I plan to cover separately in more detail one day.)</span></p><p>While first introduced in 2021, RoPE became widely adopted with the release of the original Llama model in 2023 and has since become a staple in modern LLMs.</p><p>Early GPT architectures used GELU. Why now use Swish over GELU? Swish is considered computationally slightly cheaper, and in my opinion, that all there is to it. Depending on which paper you look at, you will find that one is slightly better than the other in terms of modeling performance. In my opinion, these small differences are probably within a standard error, and your mileage will vary based on hyperparameter sensitivity.</p><p>Activation functions used to be a hot topic of debate until the deep learning community largely settled on ReLU more than a decade ago. Since then, researchers have proposed and tried many ReLU-like variants with smoother curves, and GELU and Swish (Figure 5) are the ones that stuck.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!WIz6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!WIz6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 424w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 848w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1272w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png" width="1407" height="775" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:775,&quot;width&quot;:1407,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:237022,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!WIz6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 424w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 848w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1272w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 5: Comparison between Swish and GELU activations, which are both smoother versions or ReLU.</figcaption></figure></div><p><span>Early GPT architectures used GELU, which is defined as </span><code>0.5x * [1 + erf(x / sqrt(2))]</code><span>. Here, </span><code>erf</code><span> (short for error function) is the integral of a Gaussian and it is computed using polynomial approximations of the Gaussian integral, which makes it more computationally expensive than simpler functions like the sigmoid used in Swish, where Swish is simply </span><code>x * sigmoid(x)</code><span>.</span></p><p>In practice, Swish is computationally slightly cheaper than GELU, and that's probably the main reason it replaced GELU in most newer models. Depending on which paper we look at, one might be somewhat better in terms of modeling performance. But I'd say these gains are often within standard error, and the winner will depend heavily on hyperparameter tuning.</p><p>Swish is used in most architectures today. However, GELU is not entirely forgotten; for example, Google's Gemma models still use GELU.</p><p><span>What's more notable, though, is that the feed forward module (a small multi-layer perceptron) is replaced by a gated "GLU" counterpart, where GLU stands for gated linear unit and was proposed in a </span><a href="https://arxiv.org/pdf/2002.05202" rel="">2020 paper</a><span>. Concretely, the 2 fully connected layers are replaced by 3 fully connected layers that are used as shown in Figure 6 below.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!8gzt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8gzt!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 424w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 848w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1272w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png" width="655" height="550.0696517412936" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:844,&quot;width&quot;:1005,&quot;resizeWidth&quot;:655,&quot;bytes&quot;:190423,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8gzt!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 424w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 848w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1272w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 6: A comparison between Swish and GELU and their gated counterparts, SwiGLU and GEGLU.</figcaption></figure></div><p><span>At first glance, it may appear that the GEGLU/SwiGLU variants may be better than the regular feed forward layers because there are simply more parameters due to the extra layer. But this is deceiving because in practice, the </span><code>W</code><span> and </span><code>V</code><span> weight layers in SwiGLU/GEGLU are usually chosen to be half the size each of the </span><code>W_1</code><span> layer in a traditional feed forward layer.</span></p><p>To illustrate this better, consider the concrete code implementations of the regular and GLU variants:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!_JVz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_JVz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 424w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 848w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1272w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png" width="687" height="513.1010587102984" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:776,&quot;width&quot;:1039,&quot;resizeWidth&quot;:687,&quot;bytes&quot;:267148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_JVz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 424w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 848w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1272w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 7: Regular feed forward module (top) and SwiGLU variant (bottom) next to each other.</figcaption></figure></div><p>So, suppose we have an embedding dimension of 1024. In the regular feed forward case, this would then be</p><ul><li><p>fc1: 1024 × 4096 = 4,194,304</p></li><li><p>fc2: 1024 × 4096 = 4,194,304</p></li></ul><p>That is fc1 + fc2 = 8,388,608 parameters.</p><p>For the GLU variant, we have</p><ul><li><p>fc1: 1024 × 2048 = 2,097,152</p></li><li><p>fc2: 1024 × 2048 = 2,097,152</p></li><li><p>fc3: 2048 × 1024 = 2,097,152</p></li></ul><p>I.e., 3 × 2,097,152 = 6,291,456 weight parameters.</p><p>So, overall, using the GLU variants results in fewer parameters, and they perform better as well. The reason for this better performance is that these GLU variants provide an additional multiplicative interaction, which improves expressivity (the same reason deep &amp; slim neural nets perform better than shallow &amp; wide neural nets, provided they are trained well).</p><p>In addition to upgrading the feed forward module to a SwiGLU, as discussed in the previous section, gpt-oss replaces the single feed forward module with multiple feed forward modules, using only a subset for each token generation step. This approach is known as a Mixture-of-Experts (MoE) and illustrated in Figure 8 below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!SYqb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!SYqb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 424w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 848w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1272w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png" width="1307" height="640" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:1307,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:120915,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!SYqb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 424w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 848w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1272w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 8: The feed forward module is replaced by a Mixture-of-Expert (MoE) module.</figcaption></figure></div><p><span>So, replacing </span><em>a single</em><span> feed forward module with </span><em>multiple</em><span> feed forward modules (as done in a MoE setup) substantially increases the model's total parameter count. However, the key trick is that we don't use ("activate") all experts for every token. Instead, a router selects only a small subset of experts per token.</span></p><p><span>Because only a few experts are active at a time, MoE modules are often referred to as </span><em>sparse</em><span>, in contrast to </span><em>dense</em><span> modules that always use the full parameter set. However, the large total number of parameters via an MoE increases the capacity of the LLM, which means it can take up more knowledge during training. The sparsity keeps inference efficient, though, as we don't use all the parameters at the same time.</span></p><p>(Fun fact: In most MoE models, expert weights account for more than 90% of the total model parameters.)</p><p>As mentioned in my previous articles, Grouped Query Attention (GQA) has emerged in recent years as a more compute- and parameter-efficient alternative to Multi-Head Attention (MHA).</p><p>In MHA, each head has its own set of keys and values. GQA reduces memory usage by grouping multiple heads to share the same key and value projections.</p><p>For example, as shown in Figure 9, if there are 2 key–value groups and 4 attention heads, heads 1 and 2 might share one set of keys and values, while heads 3 and 4 share another. This grouping decreases the total number of key and value computations, leading to lower memory usage and improved efficiency — without noticeably affecting modeling performance, according to ablation studies.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Kohq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Kohq!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 424w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 848w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1272w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png" width="637" height="302.7938561034762" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:588,&quot;width&quot;:1237,&quot;resizeWidth&quot;:637,&quot;bytes&quot;:83420,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Kohq!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 424w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 848w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1272w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 9: A comparison between MHA and GQA. Here, the group size is 2, where a key and value pair is shared among 2 queries.</figcaption></figure></div><p>So, the core idea behind GQA is to reduce the number of key and value heads by sharing them across multiple query heads. This (1) lowers the model's parameter count and (2) reduces the memory bandwidth usage for key and value tensors during inference since fewer keys and values need to be stored and retrieved from the KV cache.</p><p><span>(If you are curious how GQA looks in code, see my</span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb" rel=""> GPT-2 to Llama 3 conversion guide</a><span> for a version without KV cache and my KV-cache variant </span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/pkg/llms_from_scratch/llama3.py" rel="">here</a><span>.)</span></p><p><span>While GQA is mainly a computational-efficiency workaround for MHA, ablation studies (such as those in the</span><a href="https://arxiv.org/abs/2305.13245" rel=""> original GQA paper</a><span> and the </span><a href="https://arxiv.org/abs/2307.09288" rel="">Llama 2 paper</a><span>) show it performs comparably to standard MHA in terms of LLM modeling performance.</span></p><p><span>Sliding-window attention (Figure 10 below) was first introduced in the </span><a href="https://arxiv.org/abs/2004.05150" rel="">LongFormer paper (2020)</a><span> and later popularized by Mistral. Interestingly, gpt-oss applies it in every second layer. You can think of it as a variation of multi-head attention, or in this case grouped query attention (GQA), where the attention context is restricted to a smaller window, reducing both memory usage and compute costs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wwFe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wwFe!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg" width="1456" height="721" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:721,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:225815,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!wwFe!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 10: Comparison between regular attention (left) and sliding window attention (right).</figcaption></figure></div><p>Concretely, gpt-oss alternates between GQA layers that attend to the full context and GQA layers with a sliding window limited to 128 tokens.</p><p><span>As I discussed in my </span><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="">previous article</a><span>, </span><a href="https://arxiv.org/abs/2408.00118" rel="">Gemma 2 (2024)</a><span> used a similar 1:1 ratio. </span><a href="https://arxiv.org/abs/2503.19786" rel="">Gemma 3</a><span> earlier this year went much further and shifted to a 5:1 ratio, which means only one full-attention layer for every five sliding-window (local) attention layers.</span></p><p>According to the Gemma ablation studies, sliding-window attention has minimal impact on modeling performance, as shown in the figure below. Note that the window size in Gemma 2 was 4096 tokens, which Gemma 3 reduced to 1024. In gpt-oss, the window is just 128 tokens, which is remarkably small.</p><p><span>And as a fun fact, the </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">official announcement article</a><span> notes that sliding-window attention was apparently already used in GPT-3:</span></p><blockquote><p>The models use alternating dense and locally banded sparse attention patterns, similar to GPT-3</p></blockquote><p><span>Who knew!? I went back to the original </span><a href="https://arxiv.org/abs/2005.14165" rel="">GPT-3 paper</a><span>, and it was indeed mentioned there:</span></p><blockquote><p>We use the same model and architecture as GPT-2 [ RWC+19 ], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [ CGRS19 ]. </p></blockquote><p><span>Finally, the last small tweak, coming from GPT-2, is replacing </span><a href="https://arxiv.org/abs/1607.06450" rel="">LayerNorm (2016)</a><span> by </span><a href="https://arxiv.org/abs/1910.07467" rel="">RMSNorm (2019)</a><span>, which has been a common trend in recent years.</span></p><p>Akin to swapping GELU with Swish and SwiGLU, RMSNorm is one of these smaller but sensible efficiency improvements. RMSNorm is similar to LayerNorm in its purpose to normalize layer activations, as shown in Figure 11 below.</p><p>You might recall that not too long ago, BatchNorm was the go-to choice for this task. It has since fallen out of favor, largely because it is harder to parallelize efficiently (due to the mean and variance batch statistics) and performs poorly with small batch sizes.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!H32R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!H32R!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 424w, https://substackcdn.com/image/fetch/$s_!H32R!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 848w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1272w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png" width="1367" height="599" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:599,&quot;width&quot;:1367,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:274255,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!H32R!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 424w, https://substackcdn.com/image/fetch/$s_!H32R!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 848w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1272w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 11: A comparison between LayerNorm (left) and RMSNorm (right) for a small linear layer.</figcaption></figure></div><p>As we can see in Figure 11 above, both LayerNorm and RMSNorm scale the layer outputs to be in a reasonable range.</p><p>LayerNorm subtracts the mean and divides by the standard deviation such that the layer outputs have a zero mean and unit variance (variance of 1 and standard deviation of one).</p><p>RMSNorm divides the inputs by the root-mean-square. This doesn't force zero mean and unit variance, but the mean and variance are in a reasonable range: -1 to 1 for the mean and 0 to 1 for the variance. In this particular example shown in Figure 11, the mean is 0.77 and the variance is 0.41.</p><p>Both LayerNorm and RMSNorm stabilize activation scales and improve optimization, but RMSNorm is often preferred in large-scale LLMs because it is cheaper to compute. Unlike LayerNorm, RMSNorm has no bias (shift) term and reduces the expensive mean and variance computations to a single root-mean-square operation. This reduces the number of cross-feature reductions from two to one, which lowers communication overhead on GPUs and improving training efficiency.</p><p>Figure 12 shows what this looks like in code:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!m5aM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!m5aM!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 424w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 848w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1272w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png" width="589" height="442.23068552774754" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de968991-5068-40d9-87bb-98b887f5f384_919x690.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:690,&quot;width&quot;:919,&quot;resizeWidth&quot;:589,&quot;bytes&quot;:259430,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!m5aM!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 424w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 848w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1272w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 12: Code implementations of LayerNorm and RMSNorm showing that RMSNorm is computationally simpler.</figcaption></figure></div><p>I still think that GPT-2 is an excellent beginner architecture when learning about LLMs. It's simple enough to understand without getting lost in layers of optimization tricks, but still complex enough to give you a solid grasp of how modern transformer models work.</p><p>By starting with GPT-2, you can focus on the fundamentals (attention mechanisms, positional embeddings, normalization, and the overall training pipeline) without being overwhelmed by the extra features and tweaks found in newer architectures.</p><p>In fact, I think it's worth the time to learn about and even implement GPT-2 first before trying to stack newer changes on top. You will not only have an easier time understanding those changes, but you will likely also appreciate them more, because you will get a better understanding of what limitations or problems they try to solve.</p><p><span>For instance, starting with my GPT-2 code I recently implemented the </span><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3" rel="">Qwen3 architecture from scratch</a><span>, which is super similar to gpt-oss, which brings us to the next topic: Comparing gpt-oss to a more recent architecture.</span></p><p>Now that we have walked through the evolution from GPT-2 to GPT OSS, we can take the next step and compare GPT OSS to a more recent architecture, Qwen3, which was released three months earlier in May 2025.</p><p>The reason I am selecting Qwen3 here is that it is among the top open-weight models as of the time of writing. Additionally, one of the Qwen3 MoE models is more or less directly comparable to GPT OSS due to its relatively similar overall size in terms of trainable parameters.</p><p>Figure 13 below compares gpt-oss-20b to a Qwen3 model of comparable size.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!5K75!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5K75!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 424w, https://substackcdn.com/image/fetch/$s_!5K75!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 848w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1272w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png" width="1456" height="741" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:741,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:268927,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!5K75!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 424w, https://substackcdn.com/image/fetch/$s_!5K75!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 848w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1272w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 13: A gpt-oss and Qwen3 model of comparable size side by side.</figcaption></figure></div><p>As we can see, gpt-oss 20B and Qwen3 30B-A3B are very similar in their architecture components. The primary difference here, aside from the dimensions, is that gpt-oss employs sliding window attention, as discussed earlier in section 1.6 (not shown in this figure), whereas Qwen3 does not.</p><p>Let's walk through the noteworthy details one by one in the following subsections.</p><p>If we look at the two models closely, we see that Qwen3 is a much deeper architecture with its 48 transformer blocks instead of 24 (Figure 14).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!G1hj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!G1hj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 424w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 848w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1272w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png" width="1456" height="696" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:696,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:307435,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!G1hj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 424w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 848w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1272w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 14: Qwen3 has twice as many transformer blocks as gpt-oss-20b.</figcaption></figure></div><p>On the other hand, gpt-oss is a much wider architecture:</p><ul><li><p>An embedding dimension of 2880 instead of 2048</p></li><li><p>An intermediate expert (feed forward) projection dimension of 5760 instead of 768</p></li></ul><p>It's also worth noting that gpt-oss uses twice as many attention heads, but this doesn't directly increase the model's width. The width is determined by the embedding dimension.</p><p>Does one approach offer advantages over the other given a fixed number of parameters? As a rule of thumb, deeper models have more flexibility but can be harder to train due to instability issues, due to exploding and vanishing gradients (which RMSNorm and shortcut connections aim to mitigate).</p><p>Wider architectures have the advantage of being faster during inference (with a higher tokens/second throughput) due to better parallelization at a higher memory cost.</p><p><span>When it comes to modeling performance, there's unfortunately no good apples-to-apples comparison I am aware of (where parameter size and datasets are kept constant) except for an ablation study in the </span><a href="https://arxiv.org/abs/2408.00118" rel="">Gemma 2 paper (Table 9)</a><span>, which found that for a 9B parameter architecture, a wider setup is slightly better than a deeper setup. Across 4 benchmarks, the wider model achieved a 52.0 average score, and the deeper model achieved a 50.8 average score.</span></p><p>As shown in Figure 14 above, it's also noteworthy that gpt-oss has a surprisingly small number of experts (32 instead of 128), and only uses 4 instead of 8 active experts per token. However, each expert is much larger than the experts in Qwen3.</p><p>This is interesting because the recent trends and developments point towards more, smaller models as being beneficial. This change, at a constant total parameter size, is nicely illustrated in Figure 15 below from the DeepSeekMoE paper.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!qYc3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!qYc3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 424w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 848w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1272w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png" width="1131" height="609" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:609,&quot;width&quot;:1131,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:219481,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!qYc3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 424w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 848w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1272w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 15: An annotated figure from "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models", </span><a href="https://arxiv.org/abs/2401.06066" rel="">https://arxiv.org/abs/2401.06066</a></figcaption></figure></div><p>Notably, unlike DeepSeek's models, neither gpt-oss nor Qwen3 uses shared experts, though.</p><p>To be fair, the small number of experts in gpt-oss could be a side effect of the 20B size. Looking at the 120B mode below, they indeed increased the number of experts (and transformer blocks) while keeping everything else fixed, as shown in Figure 16 below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!w8-R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!w8-R!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 424w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 848w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1272w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png" width="1456" height="726" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:291088,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!w8-R!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 424w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 848w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1272w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 16: The two gpt-oss architectures side by side, where the larger 120B model only scales the number of transformer blocks and number of experts.</figcaption></figure></div><p>The boring explanation for the fact that the 20B and 120B models are so similar is probably that the 120B model was the main focus. And the easiest way to create a smaller model was to make it a bit shorter (fewer transformer blocks) and to reduce the number of experts, because that's where most of the parameters are. However, one might speculate whether they started training the 120B model, and then chopped some of the transformer blocks and experts for continued pre-training (instead of starting from random weights).</p><p>In any case, it's because it's quite unusual to only scale those two (transformer blocks and number of experts). For instance, when looking at Qwen3 MoE models of multiple sizes (Figure 17 below), they were scaled more proportionally to each other over many more aspects..</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!0h6T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!0h6T!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 424w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 848w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1272w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png" width="1120" height="903" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:903,&quot;width&quot;:1120,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:210100,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!0h6T!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 424w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 848w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1272w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 17: Architecture differences in the various Qwen3 models.</figcaption></figure></div><p>Both gpt-oss and Qwen3 use grouped query attention. The main difference is that gpt-oss restricts the context size via sliding window attention in each second layer, as mentioned earlier.</p><p>However, there's one interesting detail that caught my eye. It seems that gpt-oss uses bias units for the attention weights, as shown in the figure below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!U3bl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!U3bl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 424w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 848w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1272w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png" width="1456" height="441" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:441,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:176606,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!U3bl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 424w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 848w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1272w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 18: gpt-oss models use bias units in the attention layers. See code example </span><a href="https://github.com/huggingface/transformers/blob/369c99d0cea403b77bd0aef818527106453fd9fc/src/transformers/models/gpt_oss/modular_gpt_oss.py#L228-L243" rel="">here</a><span>.</span></figcaption></figure></div><p><span>I haven't seen these bias units being used since the GPT-2 days, and they are commonly regarded as redundant. Indeed, I found a </span><a href="https://arxiv.org/abs/2302.08626" rel="">recent paper</a><span> that shows mathematically that this is at least true for the key transformation (k_proj). Furthermore, the empirical results show that there is little difference between with and without bias units (see Figure 19 below).</span></p><p><span>Another detail you may have noticed is the definition of </span><code>sinks</code><span> in the code screenshot in Figure 18. In general models, attention sinks are special "always-attended" tokens placed at the start of the sequence to stabilize attention, which is especially useful in long-context scenarios. I.e., if the context gets very long, this special attended token at the beginning is still attended to, and it can learn to store some generally useful information about the entire sequence. (I think it was originally proposed in the </span><a href="https://arxiv.org/abs/2309.17453" rel="">Efficient Streaming Language Models with Attention Sinks</a><span> paper.)</span></p><p><span>In the gpt-oss implementation, </span><em>attention sinks</em><span> are not actual tokens in the input sequence. Instead, they are learned per-head bias logits that are appended to the attention scores (Figure 20). The goal is the same as with the above-mentioned attention sinks, but without modifying the tokenized inputs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Qwo6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Qwo6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 424w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 848w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1272w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png" width="988" height="684" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:684,&quot;width&quot;:988,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:202184,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Qwo6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 424w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 848w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1272w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 20: The use of attention sinks in gpt-oss; based on the Hugging Face code </span><a href="https://github.com/huggingface/transformers/blame/369c99d0cea403b77bd0aef818527106453fd9fc/src/transformers/models/gpt_oss/modular_gpt_oss.py" rel="">here</a><span>.</span></figcaption></figure></div><p>Lastly, and similar to Qwen3, the gpt-oss models are Apache 2.0 open-source license, which is great (it's the same license that I prefer for my own open-source projects). This means that the models can be distilled into other models or used in commercial products without restriction.</p><p><strong>Open-weight vs. open-source LLMs.</strong><span> This distinction has been debated for years, but it is worth clarifying to avoid confusion about this release and its artifacts. Some model developers release only the model weights and inference code (for example, Llama, Gemma, gpt-oss), while others (for example, OLMo) release everything including training code, datasets, and weights as true open source.</span></p><p><span>By that stricter definition, gpt-oss is an </span><em>open-weight</em><span> model (just like Qwen3) because it includes the weights and inference code but not the training code or datasets. However, the terminology is used inconsistently across the industry.</span></p><p><span>I assume the "oss" in "gpt-oss" stands for </span><em>open source software</em><span>; however, I am positively surprised that OpenAI itself clearly describes gpt-oss as an open-weight model in their official </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">announcement article</a><span>.</span></p><p>While the previous sections described how the architecture has evolved since GPT-2 and discussed its similarities to Qwen3 (and most other recent models), there are still a few additional but noteworthy details I have not mentioned, yet. These are points that did not fit neatly into the earlier sections but are still worth mentioning.</p><p><span>Unfortunately, there is not much information about the training set sizes and algorithms available. I added the most interesting puzzle pieces from the </span><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf" rel="">model card report</a><span> (1) and </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">announcement post</a><span> (2) below:</span></p><blockquote><p>The gpt-oss models were trained using our most advanced pre-training and post-training techniques [...] (1)</p><p>[...] required 2.1million H100-hours to complete, with gpt-oss-20b needing almost 10x fewer. (1)</p><p>[...] including a supervised fine-tuning stage and a high-compute RL stage [...] (2)</p><p>We trained the models on a mostly English, text-only dataset, with a focus on STEM, coding, and general knowledge. (2)</p></blockquote><p><span>So, we know that the gpt-oss models are reasoning models. The training compute of 2.1 million H100 GPU hours is roughly on par with the 2.788 million H800 GPU hours that the ~5.6x larger </span><a href="https://arxiv.org/abs/2412.19437" rel="">DeepSeek V3</a><span> model was trained for. Unfortunately, there is no information about the Qwen3 training time available yet.</span></p><p>Interestingly, the GPT-oss training hour estimate includes both the supervised learning for instruction following and the reinforcement learning for reasoning, whereas DeepSeek V3 is just a pre-trained base model on top of which DeepSeek R1 was trained separately.</p><p>As mentioned in the previous section, the gpt-oss models are reasoning models. However, what's particularly interesting is that they were trained so that users can easily control the degree of reasoning via inference time scaling.</p><p>Concretely, gpt-oss models can receive "Reasoning effort: low/medium/high" instructions as part of their system prompt, which directly affects the response length and accuracy, as shown in Figure 21.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!LsLL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LsLL!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 424w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 848w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1272w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png" width="1219" height="548" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:548,&quot;width&quot;:1219,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:175317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!LsLL!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 424w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 848w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1272w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 21: Response length and quality of gpt-oss models under different reasoning efforts (annotated figure from the </span><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf" rel="">model card</a><span>)</span></figcaption></figure></div><p>This level of adjustability is useful because it lets us balance cost, compute, and accuracy. For example, if the task is simple, such as answering a straightforward knowledge question or fixing a small typo, we can skip extended reasoning. This saves time and resources while avoiding unnecessarily long responses and verbose reasoning traces.</p><p>It is somewhat unfortunate that OpenAI did not release the base models prior to reinforcement learning-based reasoning training, unlike Qwen3 or OLMo. Base models are particularly valuable starting points for researchers working on reasoning methods (which is one reason I currently like working with Qwen3 Base). My guess is that OpenAI's decision was driven more by industry and production use cases than by research considerations.</p><p><span>Note that the original Qwen3 models also have a toggle for enabling/disabling thinking (reasoning) modes (via a </span><code>enable_thinking=True/False</code><span> setting in the tokenizer that simply adds &lt;think&gt;&lt;/think&gt; tags to disable the reasoning behavior). However, the Qwen3 team updated their models in the last few weeks and moved away from the hybrid model towards dedicated Instruct/Thinking/Coder variants.</span></p><p>The reason was that the hybrid mode resulted in lower performance compared to the individual models:</p><blockquote><p><span>After discussing with the community and reflecting on the matter, we have decided to abandon the hybrid thinking mode. We will now train the Instruct and Thinking models separately to achieve the best possible quality. </span><a href="https://www.actuia.com/en/news/alibaba-launches-qwen3-235b-a22b-instruct-2507-and-breaks-away-from-hybrid-reasoning/?utm_source=chatgpt.com" rel="">Source</a></p></blockquote><p>One interesting surprise is that OpenAI released the gpt-oss models with an MXFP4 quantization scheme for the MoE experts.</p><p>Quantization formats used to be a niche topic, mostly relevant to mobile or embedded AI, but that's changed with the push toward bigger models. In this case, the MXFP4 optimization allows the model to run on single GPU devices.</p><p>Here’s what that looks like in practice:</p><ul><li><p>The large model (think 120B) fits on a single 80GB H100 or newer GPU. Not consumer hardware, but hey, it's much cheaper to rent a 1-H100 machine than a multi-H100 machine. Plus, we don't have to worry about distributing the model across GPUs and adding communication overhead. It's really nice that AMD MI300X cards are supported from day 1 as well!</p></li><li><p>The smaller 20B model even fits into 16 GB of VRAM; the caveat is that it has to be a RTX 50-series GPU or newer to support MXFP4.</p></li></ul><p>Note that the models will also run on older hardware but without MXFP4 support and will thus consume more RAM. Without MXFP4 optimization, the models in bfloat16 will consume more like 48 GB (gpt-oss-20b) and 240 GB (gpt-oss-120b).</p><p>By the way, I can run the gpt-oss-20b model comfortably on my Mac Mini using ollama. It uses about 13.5 Gb or memory, which is really reasonable.</p><p><span>The models are still a bit too new for independent benchmarks. Checking the </span><a href="https://lmarena.ai/leaderboard" rel="">LM Arena leaderboard</a><span>, I found that gpt-oss is not listed, yet. So, Qwen3-Instruct remains the top open-weight model, according to users on the LM Arena, for now (Figure 22).</span></p><p>Looking at a reasoning benchmarks provide in the gpt-oss announcement post, we can see that the gpt-ossmodels are on par with OpenAI's proprietary models as well as Qwen3 (Figure 23).</p><p>However, this should be caveated by the fact that gpt-oss-120b is almost half the size of the Qwen3 A235B-A22B-Thinking-2507 model and can run on a single GPU.</p><p>Benchmark performance, however, does not always reflect real-world usability. In my limited use over the past few days, I have found gpt-oss to be quite capable. That said, as others have observed, it does seem to have a relatively high tendency to hallucinate (a point also mentioned in its model card).</p><p>This may stem from its heavy training focus on reasoning tasks such as math, puzzles, and code, which could have led to some "general knowledge forgetting." Still, because gpt-oss was designed with tool use in mind, this limitation may become less relevant over time. Tool integration in open-source LLMs is still in its early stages, but as it matures, I expect that we increasingly let models consult external sources (like search engines) when answering factual or knowledge-based queries.</p><p>If that happens, it could be sensible to prioritize reasoning capacity over memorization. This is much like in human learning in school (or in life in general), where problem-solving skills often matter more than memorizing facts.</p><p>OpenAI had a busy week and released the long-awaited GPT-5 model shortly after gpt-oss. The GPT-5 release was interesting. And if there's one thing I have to say here, it's that I am really surprised by how good their open-source models really are compared to their best product offering in terms of benchmark performance (Figure 24).</p><p>All in all, even though some people called the release overhyped, I am glad that we have a new set of really strong open weight models that are not too far behind the best proprietary ones. Of course, benchmarks often do not accurately reflect real-world use, and it is still too early to tell based on the limited usage. But I think these are good times for people who like to work with open-weight and local (or privately hosted) models.</p><p><em>This magazine is a personal passion project, and your support helps keep it alive. If you would like to contribute, there are a few great ways:</em></p><ul><li><p><em><strong><a href="https://amzn.to/4fqvn0D" rel="">Grab a copy of my book</a></strong><span>. Build a Large Language Model (From Scratch) walks you through building an LLM step by step, from tokenizer to training.</span></em></p></li></ul><ul><li><p><em><strong><a href="https://www.manning.com/livevideo/master-and-build-large-language-models" rel="">Check out the video course</a></strong><span>. There’s now a 17-hour video course based on the book, available from Manning. It follows the book closely, section by section, and works well both as a standalone or as a code-along resource. The video course is ad-free (unlike the YouTube version) and has a cleaner, more structured format. It also contains 5 additional hours of pre-requisite video material created by Abhinav Kimothi.</span></em></p></li></ul><ul><li><p><em><strong><a href="https://magazine.sebastianraschka.com/subscribe" rel="">Subscribe</a></strong><span>. A paid subscription helps to make my writing sustainable and gives you access to additional contents.</span></em></p></li></ul><p><em>Thanks for reading, and for helping support independent research!</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wVLk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg" width="1456" height="878" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:878,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Engineering.fyi – Search across tech engineering blogs in one place (344 pts)]]></title>
            <link>https://engineering.fyi/</link>
            <guid>44855157</guid>
            <pubDate>Sun, 10 Aug 2025 13:44:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fyi/">https://engineering.fyi/</a>, See on <a href="https://news.ycombinator.com/item?id=44855157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div data-slot="card"><p>How Airbnb upgrades tens of thousands of pods on dozens of Kubernetes clusters to new Istio versions</p></div><div data-slot="card"><p>MCP UI extends the Model Context Protocol to enable AI agents to return fully interactive UI components. It solves the critical challenge that commerce experiences require visual and interactive elements like product selectors, image galleries, and cart flows. This open-source protocol allows agents to embed commerce components while maintaining control through an intent-based messaging system, delivering shopping experiences that go far beyond traditional text-only AI interactions.</p></div><div data-slot="card"><p>What if you could control any device using only subtle hand movements? New research from Meta’s Reality Labs is pointing even more firmly toward wrist-worn devices using surface electromyography (s…</p></div><div data-slot="card"><div data-slot="card-header"><p>The Google Developer Program is rolling out major updates to make its tools and community more accessible and powerful. These enhancements include a new flexible monthly subscription tier, a centralized GDP Forum for collaboration, and increased Gemini CLI access for all members.</p></div><div data-slot="card-content"><p><span>Chris Demeke, Kevin Flores</span></p></div></div><div data-slot="card"><p>Working Together to Accelerate AI Adoption</p></div><div data-slot="card"><p>FlashList v2 is a complete rewrite, delivering faster load times, improved scrolling performance, and precise rendering without requiring item size estimates. It powers thousands of lists in the Shopify mobile app and is now production-ready.</p></div><div data-slot="card"><p>Google introduces Veo 3 Fast, an optimized model for speed and price, along with new image-to-video capabilities for both Veo 3 and Veo 3 Fast, enabling developers to efficiently create high-quality video content from text or still images, with varying pricing based on the model and audio inclusion, now available in the Gemini API.</p></div><div data-slot="card"><div data-slot="card-header"><p>The Gemini Embedding model enhances AI applications, particularly through context engineering, which is being successfully adopted by various organizations across industries to power context-aware systems, leading to significant improvements in performance, accuracy, and efficiency.</p></div><div data-slot="card-content"><p><span>Vishal Dharmadhikari, Janie Zhang</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>LangExtract is a new open-source Python library powered by Gemini models for extracting structured information from unstructured text, offering precise source grounding, reliable structured outputs using controlled generation, optimized long-context extraction, interactive visualization, and flexible LLM backend support.</p></div><div data-slot="card-content"><p><span>Akshay Goel, Atilla Kiraly</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>Max's journey introduces LQRax, a JAX-native LQR solver, which exemplifies the growing JAX robotics ecosystem that includes tools like Brax, MJX, and JaxSim, highlighting the benefits of JAX for computational efficiency in optimal control and simulation, and for seamlessly integrating model-based and learning-based approaches.</p></div><div data-slot="card-content"><p><span>Srikanth Kilaru, Max Muchen Sun</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>ExecuTorch is the PyTorch inference framework for edge devices developed by Meta with support from industry leaders like Arm, Apple, and Qualcomm.&nbsp; Running machine learning (ML) models on-device is…</p></div><div data-slot="card-content"><p><span>PyTorch Edge Team in collaboration with Family of Apps</span></p></div></div><div data-slot="card"><p>How to achieve high availability with distributed databases on Kubernetes</p></div><div data-slot="card"><div data-slot="card-header"><p>Co-hosted by Ashley Oldacre and Christina Warren, People of AI podcast's Season 5 will focus on the builders in the space of AI, highlighting the unique journeys, challenges, and triumphs of these innovators.</p></div><div data-slot="card-content"><p><span>Ashley Oldacre, Christina Warren</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>Opal is a new experimental tool from Google Labs that helps you compose prompts into dynamic, multi-step mini-apps using natural language, removing the need for code, allowing users to build and deploy shareable AI apps with powerful features and seamless integration with existing Google tools.</p></div><div data-slot="card-content"><p><span>Ali Modarres, Bill Byrne, Paul Lewis</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>Apigee helps enterprises integrate large language models (LLMs) into existing API ecosystems securely and scalably, addressing challenges like authentication and authorization not fully covered by the evolving Model Context Protocol (MCP), and offering an open-source MCP server example that demonstrates how to implement enterprise-ready API security for AI agents.</p></div><div data-slot="card-content"><p><span>Antony Arul, Ruben Gonzalez</span></p></div></div><div data-slot="card"><p>Automatically review your PRs with Bugbot</p></div><div data-slot="card"><div data-slot="card-header"><p>New AI capabilities for popular frameworks in Firebase Studio include AI-optimized templates, streamlined integration with Firebase backend services, and the ability to fork workspaces for experimentation and collaboration, making AI-assisted app development more intuitive and faster for developers worldwide.</p></div><div data-slot="card-content"><p><span>Jeanine Banks, Vikas Anand</span></p></div></div><div data-slot="card"><p><span>Lavanya Verma, Ryan Hang, Sung Whang, Joseph Wang</span></p></div><div data-slot="card"><div data-slot="card-header"><p>Gemini 2.5 Flash-Lite, previously in preview, is now stable and generally available. This cost-efficient model is ~1.5x faster than 2.0 Flash-Lite and 2.0 Flash, offers high quality, and includes 2.5 family features like a 1 million-token context window and multimodality.</p></div><div data-slot="card-content"><p><span>Logan Kilpatrick, Zach Gleicher</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>Gemini's advanced capability for conversational image segmentation allows intuitive interaction with visual data by understanding complex phrases, conditional logic, and abstract concepts, streamlining developer experience and opening doors for new applications in media editing, safety monitoring, and damage assessment.</p></div><div data-slot="card-content"><p><span>Paul Voigtlaender, Valentin Gabeur, Rohan Doshi</span></p></div></div><div data-slot="card"><p><span>Austin Harrison, Eddie Huang, Spencer Garth, Tim Ross, Taya Yusuf</span></p></div><div data-slot="card"><p>ChatGPT now thinks and acts, proactively choosing from a toolbox of agentic skills to complete tasks for you using its own computer.</p></div><div data-slot="card"><div data-slot="card-header"><p>Veo 3, Google’s latest AI video generation model, is now available in paid preview via the Gemini API and Google AI Studio. Unveiled at Google I/O 2025, Veo 3 can generate both video and synchronized audio, including dialogue, background sounds, and even animal noises. This model delivers realistic visuals, natural lighting, and physics, with accurate lip syncing and sound that matches on-screen action.</p></div><div data-slot="card-content"><p><span>Alisa Fortin, Luciano Martins, Seth Odoom</span></p></div></div><div data-slot="card"><p>Meta has developed an open-source AI tool to design concrete mixes that are stronger, more sustainable, and ready to build with faster—speeding up construction while reducing environmental impact. …</p></div><div data-slot="card"><p>Shopify’s Global Catalogue demonstrates the impact of multimodal LLMs on one of commerce’s hardest problems: building a unified, structured, and continuously evolving understanding of billions of product listings created by millions of merchants.</p></div><div data-slot="card"><div data-slot="card-header"><p>The Marin project aims to expand the definition of 'open' in AI to include the entire scientific process, not just the model itself, by making the complete development journey accessible and reproducible. This effort, powered by the JAX framework and its Levanter tool, allows for deep scrutiny, trust in, and building upon foundation models, fostering a more transparent future for AI research.</p></div><div data-slot="card-content"><p><span>Srikanth Kilaru, David Hall</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>The updated Agent Development Kit (ADK) simplifies and accelerates the process of building AI agents by providing the CLI with a deep, cost-effective understanding of the ADK framework, allowing developers to quickly ideate, generate, test, and improve functional agents through conversational prompts, eliminating friction and keeping them in a productive "flow" state.</p></div><div data-slot="card-content"><p><span>Julia Wiesinger, Hangfei Lin</span></p></div></div><div data-slot="card"><p>The `logprobs` feature has been officially introduced in the Gemini API on Vertex AI, provides insight into the model's decision-making by showing probability scores for chosen and alternative tokens. This step-by-step guide will walk you through how to enable and interpret this feature and apply it to powerful use cases such as confident classification, dynamic autocomplete, and quantitative RAG evaluation.</p></div><div data-slot="card"><p>Microsoft’s AI-powered code review assistant has transformed pull request workflows by automating routine checks, suggesting improvements, and enabling conversational Q&amp;A, leading to faster PR completion, improved code quality, and enhanced developer onboarding.</p></div><div data-slot="card"><p>The Gemini Embedding text model is now generally available in the Gemini API and Vertex AI. This versatile model has consistently ranked #1 on the MTEB Multilingual leaderboard since its experimental launch in March, supports over 100 languages, has a 2048 maximum input token length, and is priced at $0.15 per 1M input tokens.</p></div><div data-slot="card"><div data-slot="card-header"><p>The Apigee API hub and Developer Portals are distinct but interconnected parts of the Apigee platform that help organizations discover and manage APIs for different personas, unlocking API potential and accelerating innovation.</p></div><div data-slot="card-content"><p><span>Venkat Sadras, David Rush</span></p></div></div><div data-slot="card"><p>Our new jurisdiction resolution system (JRS) is a faster, less resource-intensive solution to the challenging problem of determining tax obligations in places with complicated, overlapping tax jurisdictions.</p></div><div data-slot="card"><div data-slot="card-header"><p>GenAI Processors is a new open-source Python library from Google DeepMind designed to simplify the development of AI applications, especially those handling multimodal input and requiring real-time responsiveness, by providing a consistent "Processor" interface for all steps from input handling to model calls and output processing, for seamless chaining and concurrent execution.</p></div><div data-slot="card-content"><p><span>Andre Elisseeff, Alexey Guseynov, Oskar Bunyan, Shrestha Basu Mallick</span></p></div></div><div data-slot="card"><p>Updates in Firebase Studio include new Agent modes, foundational support for the Model Context Protocol (MCP), and Gemini CLI integration, all designed to redefine AI-assisted development allow developers to create full-stack applications from a single prompt and integrate powerful AI capabilities directly into their workflow.</p></div><div data-slot="card"><div data-slot="card-header"><p>T5Gemma is a new family of encoder-decoder LLMs developed by converting and adapting pretrained decoder-only models based on the Gemma 2 framework, offering superior performance and efficiency compared to its decoder-only counterparts, particularly for tasks requiring deep input understanding, like summarization and translation.</p></div><div data-slot="card-content"><p><span>Biao Zhang, Paul Suganthan, Ben Hora</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>The new batch mode in the Gemini API is designed for high-throughput, non-latency-critical AI workloads, simplifying large jobs by handling scheduling and processing, and making tasks like data analysis, bulk content creation, and model evaluation more cost-effective and scalable, so developers can process large volumes of data efficiently.</p></div><div data-slot="card-content"><p><span>Lucia Loher, Vishal Dharmadhikari</span></p></div></div><div data-slot="card"><p>Commerce is a dynamic ecosystem where our mission is to empower every merchant to succeed. We optimize each step of their journey—from product creation to customer delivery—using advanced tools, infrastructure, and partnerships to solve a complex optimization challenge.</p></div><div data-slot="card"><p>How the new Pro plan works and why we changed our pricing.</p></div><div data-slot="card"><p><span>Prateek Jain, Soheil Sadeghi, Mehrdad Bakhtiari</span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Try and (502 pts)]]></title>
            <link>https://ygdp.yale.edu/phenomena/try-and</link>
            <guid>44855079</guid>
            <pubDate>Sun, 10 Aug 2025 13:32:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ygdp.yale.edu/phenomena/try-and">https://ygdp.yale.edu/phenomena/try-and</a>, See on <a href="https://news.ycombinator.com/item?id=44855079">Hacker News</a></p>
<div id="readability-page-1" class="page"><div property="content:encoded"><p><strong><br>
		I'm gonna try and change the course of hip hop again.<br>
	</strong>
</p>
<p>
		(Dr. Dre)
	</p>

<p>
	Typically, <i>try</i> can be followed by three kinds of phrases: a noun phrase (1a), an infinitival verb phrase with <i>to</i> (1b), or a verb phrase with -<i>ing</i> (1c).
</p>
<blockquote><p>
		1)	a. I'll try the salad.
	</p>
<p id="indented">
		b. I'll try to eat this horrible salad.
	</p>
<p id="indented">
		c. I'll try adding vinegar to the salad, to improve the taste.
	</p>
</blockquote>
<p>
	However, <i>try</i> can also combine with the conjunction <i>and</i>, followed by a bare verb form:
</p>
<blockquote><p>
		2)	I’ll try and eat the salad.
	</p>
</blockquote>
<p>
	This usage is very similar in meaning to <i>try to</i>, if not identical, but is deemed prescriptively incorrect (Routledge 1864:579 in D. Ross 2013a:120; Partridge 1947:338, Crews et al. 1989:656 in Brook &amp; Tagliamonte 2016:320). In the next few sections, we will see that it has a number of interesting properties.
</p>


<h2>Who says this?</h2>
<p>
	<i>Try and</i> is described as more prevalent in British English than American English, but is common in both varieties (Hommerberg &amp; Tottie 2007). Brook &amp; Tagliamonte (2016) show that Canadian speakers pattern with American speakers in their usage of the construction.
</p>
<p>
	<i>Try and</i> is not a recent innovation – it first emerged in the late 1500s, although the earliest textual attestion is from 1390 (Tottie 2012, D. Ross 2013a). Tottie (2012) provides some examples of <i>try and</i> from EEBO-TCP corpus, including this one:
</p>
<blockquote><p>
		3) ...howe and by what certaine and generall rule I mighte <b>trye and</b> throughly discerne the veritie of the catholike faithe, from the falsehood of wicked heresye... (1554)<br>
		4) You maie (saide I) <b>trie and</b> bring him in, and shewe him to her. (1569)
	</p>
</blockquote>
<p>
	<i>Webster’s Dictionary</i> (1989:919) suggests that <i>try and</i> in fact predates <i>try to</i>, and this conclusion is supported by Hommerberg &amp; Tottie (2007:60), Tottie &amp; Hoffman (2011) and Tottie (2012). However, D. Ross (2013a) disputes this, saying that “<i>[t]ry and</i> and <i>try to</i> developed simultaneously and independently”. What is clear is that <i>try and</i> has been around for at least as long as <i>try to</i>.
</p>

<h2>Syntactic Properties</h2>
<p>
	Carden &amp; Pesetsky (1977:86) note that <i>try and</i> does not behave like a regular case of coordination. </p>
<h3>Question words are allowed</h3>
<p>One property of ‘true’ coordination is that it is subject to the <i>Coordinate Structure Constraint</i> (J. Ross 1967), which states that a <i>wh</i>-word cannot move out of one of the conjuncts. This is shown in (5).
</p>
<blockquote><p>
		5)	a. Mary [met Bill and ignored Susie].
	</p>
<p id="indented">
		b. *Who did Mary [meet Bill and ignore __]?
	</p>
</blockquote>
<p>
	However, a <i>wh</i>-word can happily be moved out of a <i>try and structure:
</i></p>
<blockquote><p>
		6) Who did Mary [try and talk to __]?
	</p>
</blockquote>
<h3>No reordering</h3>
<p>
	A second property of pseudo-coordination that distinguishes it from regular coordination is that the two conjuncts cannot be reordered. In (6), we see that regular coordination permits the order of conjuncts to be changed, while in (7) we see that the same is not possible with <i>try and</i> (De Vos 2005:59).</p>
<blockquote><p>
		7)	a. John will wash the bathroom and kill mosquitos.
	</p>
<p id="indented">
		b.	John will kill mosquitos and wash the bathroom.
</p></blockquote>
<blockquote><p>
		8)	a. John will try and kill mosquitos.
	</p>
<p id="indented">
		b.	*John will kill mosquitos and try.
</p></blockquote>
<h3><em>Both</em> is not possible</h3>
<p>
	Another piece of evidence that <i>try and</i> is not regular coordination structure comes from the unavailability of <i>both</i>. Usually, coordinated verb phrases can be preceded by <i>both</i>:</p>
<blockquote><p>
		9)	<i>Reality is Broken</i> will both [stimulate your brain and stir your soul]. [<a href="https://janemcgonigal.com/my-book/">source</a>, February 28 2017]
	</p>
</blockquote>
<p>
However, De Vos (2005:59) points out that <i>try and</i> may not be preceded by <i>both</i>:
</p>
<blockquote><p>
		10)	a. John will try and kill mosquitos.
	</p>
<p id="indented">
		b. *John will both try and kill mosquitos.
	</p>
</blockquote>
<h3>Bare form only</h3>
<p>
Unlike with regular coordination, <i>try and</i> is available only when both <i>try</i> and the verb following <i>and</i> are uninflected, which means it must occur in its bare form. Carden &amp; Pesetsky (1977)  call this the <i>bare form condition</i>. The following examples are adapted from D. Ross (2013a:111):
</p>
<blockquote><p>
		11)	a. I will try and finish the assignment.
	</p>
<p id="indented">
		b. I try and finish an assignment every day.
	</p>
<p id="indented">
		c. *I tried and finish(ed) the assignment.
	</p>
<p id="indented">
		d. *He tries and finish(es) an assignment every day.
	</p>
<p id="indented">
		e. *It’s tough when you’re trying and finish(ing) an assignment under pressure.
	</p>
</blockquote>
<h3>Dialect variation in the Bare Form Condition</h3>
<p>
	Is the bare form condition universal? D. Ross (2013a:124-5) notes that it has weakened in some dialects, though not necessarily in the same way. In dialects of Northeastern Canada, parallel inflected forms are acceptable:
</p>
<blockquote><p>
		12)	They tries and does that.
	</p>
</blockquote>
<p>
	In South African English, on the other hand, <i>try</i> may be inflected while the second verb remains a bare form (examples from D. Ross 2013a:125):
</p>
<blockquote><p>
		13)	a. Noeleen tries and find answers and solutions. [<a href="http://www.tvsa.co.za/default.asp?blogname=coming_up_on_3Talk&amp;ArticleID=2903">source</a>, August 2006]
	</p>
<p id="indented">
		b. We’re trying and get across that nature is harsh but not necessarily full of malice and cruelty. (Dereck Joubert on “Wild about Africa,” Carte Blanche: March 18, 2007)
	</p>
</blockquote>
<h3>No separation of <i>try</i> and <i>and</i></h3>
<p>
	There are some other restrictions on the distribution of try and. Unlike with <i>try to</i>, <i>try</i> may not be separated from <i>and</i> by an adverb (Webster’s Dictionary 1989:919):
</p>
<blockquote><p>
		14)	a. Try always to tell the truth.
	</p>
<p id="indented">
		b. *Try always and tell the truth.
	</p>
</blockquote>
<p>
	Similarly, <i>try</i> may not be separated from <i>and</i> by negation (Brook &amp; Tagliamonte 2016:308):
</p>
<blockquote><p>
		15)	a. You try not to let it bother you.
	</p>
<p id="indented">
		b. *You try not and let it bother you.
	</p>
</blockquote>
<h3>No ellipsis allowed</h3>
<p>
	<i>Try and</i> is incompatible with ellipsis of the following verb phrase (Brook &amp; Tagliamonte 2016):
</p>
<blockquote><p>
		16)	a. Sure, I'll try to.
	</p>
<p id="indented">
		b. *Sure, I'll try and.
</p></blockquote>

<h2>Other instances of pseudocoordination</h2>
<p>
	Infinitival <i>to</i> can be replaced by <i>and</i> in several other cases, subject to dialectal and individual variation. Brook &amp; Tagliamonte (2016:302) state that the best candidate for a verb phrase that behaves like <i>try</i> is <i>be sure</i>:
</p>
<blockquote><p>
		17)	Be sure and visit Harry tomorrow. (Carden &amp; Pesetsky 1977:84)
	</p>
</blockquote>
<p>
	D. Ross (2013a:122) provides several examples of other verb phrases in which infinitival <i>to</i> has been replaced with <i>and</i>:
</p>
<blockquote><p>
		18)	a. <b>Mind and</b> get all right for next Saturday. (Poutsma 1905:361)
	</p>
<p id="indented">
		b. You know I go to all these different schools and I <b>start and</b> get mixed up after a while. (Hopper 2002:162)
	</p>
<p id="indented">
		c. <b>Remember and</b> wash your hair. (BNC: KE4 636, 1992)
	</p>
</blockquote>
<p>
	Another instance of pseudocoordination is found with motion verbs, such as <i>come</i> and <i>go</i>:
</p>
<blockquote><p>
		19)	a. Can you come and pick me up from the station?
	</p>
<p id="indented">
		b. I’ll go and get the mop.
	</p>
</blockquote>
<p>
	D. Ross (2013b) argues that motion verb pseudocoordination has a different syntax and semantics from <i>try and</i> pseudocoordination. Syntactically, we can see that motion verb pseudocoordination is <i>not</i> subject to the bare form condition:
</p>
<blockquote><p>
		20)	a. He came and <b>picked</b> me up from the station.
	</p>
<p id="indented">
		b. She goes and <b>gets</b> lunch every day at noon.
	</p>
</blockquote>
<p>
	Semantically, <i>go and</i> entails that the event was completed, so in (21) below it is strange to use <i>go and</i> if the book was not acquired. In contrast, its non-pseudocoordination equivalent <i>go to</i> does not have this entailment.
</p>
<blockquote><p>
		21)	The man will go to/*and buy the book, even if it is sold out.
	</p>
</blockquote>
<p><em>Page contributed by Matthew Tyler on Feb 23, 2018.</em></p>
<p><em>Updates/revisions: June 27, 2018 (Katie Martin)</em></p>
<p><b>Please cite this page as:</b> Tyler, Matthew. 2018. <em>Try and</em>. <i>Yale Grammatical Diversity Project: English in North America</i>. (Available online at <a href="http://ygdp.yale.edu/phenomena/try-and">http://ygdp.yale.edu/phenomena/try-and</a>. Accessed on YYYY-MM-DD). Updated by Katie Martin (2018).</p>
<h2>References</h2>
</div><div><p>Phenomenon Dialect:&nbsp;</p><div><p>Widespread American English</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inside OS/2 (1987) (113 pts)]]></title>
            <link>https://gitpi.us/article-archive/inside-os2/</link>
            <guid>44854989</guid>
            <pubDate>Sun, 10 Aug 2025 13:15:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gitpi.us/article-archive/inside-os2/">https://gitpi.us/article-archive/inside-os2/</a>, See on <a href="https://news.ycombinator.com/item?id=44854989">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>by Vaughn Vernon</p>
<p>from the December 1987 issue of Computer Language</p>
<p>OS/2, Microsoft’s latest addition to its operating system line, could well become the operating system of the next decade for Intel 80286/80386 microcomputers. Its multitasking capabilities, full-featured application programming interface (API), and extendability to future hardware almost guarantee its success.</p>
<p>Microsoft sees microcomputing as a platform for office automation hardware and software: The office of the future (regardless of a company’s structure and line of business) is envisioned as a place of personal and group productivity. Personal productivity is to be achieved through multitasking, common graphical user interfaces, and the sharing of resources such as data and powerful hardware. Group productivity will occur through individual use of wide and local area networks (LANs).</p>
<p>This article introduces you to the variety of system services provided in OS/2, allowing you to investigate the opportunities offered by OS/2 without a substantial investment of time and money. First I will overview the operating system, then I will delve into the details of the system services.</p>
<h2 id="os2-software-development-kit">OS/2 Software Development Kit</h2>
<p>The OS/2 system architecture has three layers. The main layer is the OS/2 kernel and system services. The second layer is the Windows Presentation Manager (WPM), Microsoft’s new Windows interface specifically for OS/2. The third layer is the OS/2 LAN Manager, the control software for local and wide area networking. This article focuses on the OS/2 kernel and is based on the OS/2 Software Development Kit (SDK), first beta test release. Some features may be changed or added by the time this article is printed.</p>
<p>The base OS/2 kernel (low-level control process) provides the needed multitasking and related system services without requiring any other special interfaces. A user can run many applications and/or utilities concurrently.</p>
<p>To use OS/2, developers at most will have to learn the OS/2 API, a high-level, call-based programming interface. The API is especially useful for programming languages such as C since it is a natural function call interface. Although Microsoft is stressing the use of WPM, it is not necessary. The LAN Manager also is not a requirement. Applications can take advantage of OS/2 multitasking by simply using the plain vanilla OS/2 system.</p>
<p>The OS/2 SDK comes complete with an optimizing C compiler, macro assembler, and a host of other programming and operating system utilities and tools: object librarian, linker, CodeView source debugger, and full-featured window-based programmer’s editor. The WPM developer’s kit and LAN Manager software will be shipped shortly. The SDK has an array of technical manuals for system call interface specifications and device drivers.</p>
<p>OS/2 is based on a preemptive scheduler. Thus on given intervals the OS/2 kernel is interrupted by the real-time clock in the PC. When it is interrupted, the kernel gains total control of the CPU (even over any currently running task). The kernel reschedules all runnable tasks and executes the task that is next in line.</p>
<p>The scheduler uses time slicing to give a task time to run. Time slicing is based on the priority of each task. Each task is given an allotted time to run, after which the kernel does a context switch. Context switching preserves the current task’s registers and modes of operation and switches in the context of the next task in line to be executed.</p>
<p>The API is implemented in a set of dynamic link (dyn-link) libraries. Routine addresses are far, 32-bit addresses, and arguments are passes on the caller’s stack.</p>
<p>Dyn-link libraries save disk space because program file size is reduced. When the program is built by linking, OS/2 system call function addresses are not resolved by the linker. Instead, the linker knows (because of a special object header) that the routines will be resolved at run time when the program is loaded and does not generate errors.</p>
<p>Not only are the dyn-link routines loaded at run time, they may also be shared by as many concurrently executing tasks as require their use. This ability also saves internal real memory or RAM.</p>
<p>Dyn-linking plays an important part in Microsoft’s view of microcomputers. Since all applications call the API, the API or dyn-link routines may be replaced to support future hardware without having to distribute a new version of the application to fit it. Also, if a dyn-link routine has a bug, your application’s user could be directed to Microsoft for a new dyn-link library. Thus your application should never have to be changed. Code sharing is exploited not only with dyn-link libraries but also with multiple occurrences of the same application. OS/2 knows when an application has been loaded two or more times and just reuses the existing code segment, only allocating memory for the other process sections such as data, heap, and stack.</p>
<p>OS/2 has the ability to run in two modes: protected and real. Protected mode is for multitasking, and real mode facilitates the running of MS-DOS applications. Protected mode is used by the 80286 and 80386 microprocessor to keep one process from being affected by another, ill-behaved process. In other words, a process may not access memory outside its process environment. In real mode, an application can access all machine resources (such as the video memory map) directly.</p>
<p>From its inception, one requirement for OS/2 was the ability to run existing MS-DOS applications. This feature allows users to migrate to the new multitasking system and still be able to run their favorite MS-DOS applications at the same time as OS/2 multitasking applications.</p>
<p>MS-DOS applications are run in real mode when OS/2 dynamically changes the processor’s mode settings. OS/2 continues to execute protected mode processes in the background while the MS-DOS compatibility box runs in the foreground. If the user switches to a protected-mode application, the 286/386 processor is changed back to protected mode and the MS-DOS application is frozen from execution in the background. The MS-DOS compatibility box may be completely disabled if it is not needed, giving OS/2 more memory for protected mode.</p>
<p>The user loads and runs applications in screen groups. Screen groups are a logical division for nontechnical users to exploit multitasking. These groups also help manage and organize the division of completely different applications.</p>
<p><img src="https://gitpi.us/img/os2-table1.jpg" alt="Table 1"></p>
<p>Screen groups are controlled by a process called the session manager. The session manager allows the user to invoke new protected mode command processors and one individual MS-DOS session. As is seen in Figure 1, the user presses a hot key to switch back to the session manager or go directly to another screen group.</p>
<p>Screen groups are implemented as multiple virtual terminals or sessions. Each group has a virtual screen buffer, keyboard, and mouse. When you create a screen group, the session manager invokes a new command processor for it. From there, you may run multiple processes in each group. A process may be spawned in either the foreground or background.</p>
<p>When a process is running in a foreground screen group, it writes directly to the video display. When a process is running in a background screen group and needs to display information, the data is written to its virtual screen buffer. Thus when you switch to a new screen group, any information that was written to the virtual display is flushed to the physical display. The focus of keyboard and mouse events is concentrated on the current foreground screen group.</p>
<p>If you are familiar with programming under MS-DOS, you will notice that OS/2 has a superset of the MS-DOS facilities for most device and system capabilities. For instance, the OS/2 video interface is a superset of the INT 10H ROM BIOS calls offered on IBM PCs and compatibles. The mouse and keyboard interfaces are also much easier to use. The file system interface includes all the read, write, and file search mechanisms and even implements new asynchronous file reads and writes. Thus an application can do something else while the disk drive is being accessed.</p>
<p>A Family API (FAPI) subset of OS/2 system calls supports MS-DOS 2.x and 3.x as well as OS/2. This subset excludes the various multitasking features and interprocess communications (IPCs) but allows programs to be completely portable between the two operating systems. Each major OS/2 system interface subsystem, such as file and video, has a set of system calls reserved for use under both operating systems. A programming utility called “binding” makes a program compatible with both operating systems.</p>
<p>Binding a program places a set of compatibility routine stubs at the end of the program file. If the program is loaded under MS-DOS, the stub routines are loaded with the program and used during execution. When the program is run under OS/2, the stubs are not loaded; the dyn-link libraries are used instead. Binding a program costs the application in both program size and execution speed, but FAPI is useful when you need the compatibility.</p>
<p>The base OS/2 system is a rich programming environment. You don’t need to use the WPM to create applications. This fact is important to the success of OS/2 because the learning curve involved in programming for WPM is tremendous. If all companies had to use WPM, many might look to another operating system forum to carry their next generation software products.</p>
<p>Not having to use WPM will allow existing large multitasking. applications under systems such as UNIX to be readily ported to OS/2 with minimal conceptual revisions. Not using WPM will also allow many applications to become available very soon, adding to the momentum of OS/2. Early products will give OS/2 a reasonably strong foundation and give WPM developers time to complete new software without a lot of pressure to deliver before the product’s time.</p>
<p>Now that I have explained the basic foundation of OS/2, let’s look at its subsystems in detail. The areas I will concentrate on are:</p>
<ul>
<li>Process creation and execution</li>
<li>Memory management</li>
<li>Device services</li>
<li>File management</li>
<li>Interprocess communications</li>
<li>Miscellaneous OS/2 services</li>
</ul>
<p>System routine names are referenced in this article by capitalizing the first letter of each word in the descriptive name, as in <em>DosWrite()</em>. The actual OS/2 symbol names are in all caps, so a program would call the <em>DosWrite()</em> routine as <em>DOSWRITE()</em>.</p>
<p>All of the OS/2 API routines use the Pascal extended keyword for their calling convention so that arguments are pushed on the stack in the opposite order of C. The Pascal keyword does not allow a system routine to receive a variable number of arguments, but the code generated using the Pascal convention is smaller and faster than the standard C convention. Also, the stack is-restored by the called procedure rather than the caller.</p>
<h2 id="process-creation-and-execution">Process creation and execution</h2>
<p>Multiple processes have been discussed to some degree, but it is still unclear how they are implemented and how they execute. A process is an environment in machine memory that contains code, data, heap, and stack segments.</p>
<p>A major difference between OS/2 and other popular operating systems such as UNIX is the way tasks are scheduled and executed. Under OS/2, threads are executed, not processes. A process is simply an instance of program execution that owns resources such as code, data, and allocated memory. A thread is the actual dispatchable entity.</p>
<p>You might say, “That’s just semantics, what’s the big deal?” But the idea behind threads is not just verbal organization. A process may own several asynchronously executing threads that share the data and other resources possessed by their owning process. When a process is first created, it owns one thread, the primary thread. This thread can request the OS/2 kernel to spawn another thread to perform some task. The new thread runs on its own time-slice schedule and priorities, separate from the primary thread. Both the primary and secondary threads can themselves spawn other threads.</p>
<p>Threads are much faster to load and run than additional processes because OS/2 does not have to create another process environment and load another program file from disk to execute that process. When a thread finishes its job, it can terminate without affecting any other thread in the entire process.</p>
<p>A process is created with a call to <em>DosExecPgm()</em>, which is similar to the MS-DOS EXEC function but more powerful. A flag passed in the call to <em>DosExecPgm()</em> tells the kernel to execute the new process either synchronously or asynchronously to its parent process. If synchronous execution is selected, the parent process will be suspended until the child completes. With asynchronous execution, the parent will continue to execute while the child process runs.</p>
<p>Like MS-DOS and UNIX, OS/2 allows the child to inherit its parent’s environment, file descriptors, and other vital resource handles. The parent may boost its child’s priority or even terminate the child if necessary.</p>
<p>A new thread may be created by calling <em>DosCreateThread()</em>. The kernel is given a dynamically allocated stack segment for the new thread and the far address of a routine to be set up for task scheduling. The kernel returns the newly created thread’s ID code to the primary thread for use in task control.</p>
<p>Threads should be used instead of new processes when the task to be performed is local to the current process. Doing so allows the sharing of process data and heap, making task performance much faster than if the duties were given to a new process.</p>
<p>Figure 2 is an example in C of the primary thread creating a secondary thread to do some calculations for it. This example is not extremely useful (all it really does is waste CPU time), but it does demonstrate the calling convention and necessary arguments for thread creation. The example also shows that the thread has a stack to use for local (automatic) variable creation. Here, the only thing that makes <em>thread_func()</em> a thread is that the primary thread passes its far address to the OS/2 scheduler to create a context for it and run it asynchronously. The primary thread could call <em>thread_func()</em> as a synchronous C function call instead of a thread.</p>
<p>Imagine you are developing a spreadsheet package. The primary thread handles the user interactions but passes spreadsheet calculations off to a secondary thread. The user is now free to continue working on another spreadsheet while the calculations are being performed asynchronously on the first spreadsheet. This example should give you an idea of how threads can complement your programming effort.</p>
<p>Two functions handle setting thread priorities: <em>DosSetPrty()</em> alters priorities for the caller or another thread, and <em>DosGetPrty()</em> is used in conjunction with <em>DosSetPrty()</em> to inquire about a thread’s current priority.</p>
<p>Priorities have three classes: idle-time, regular, and time-critical. Within each class is a priority level ranging from 0 to 31 for each class. Idle-time, level 0 is the lowest possible priority, and time-critical, level 31 is the highest possible priority.</p>
<p><img src="https://gitpi.us/img/os2-table2.jpg" alt="Table 2"></p>
<p>The user of OS/2 and your application ultimately. can control priority modification. The file CONFIG.SYS is used at boot time to set priority permissions. Three configuration keywords are used in CONFIG.SYS to control the use of priorities: <em>PRIORITY = ABSOLUTE</em> or <em>DYNAMIC</em>, <em>MAXWAIT=x</em>, and <em>TIMESLICE=x|,y|</em>.</p>
<p>The <em>PRIORITY= ABSOLUTE</em> or <em>DYNAMIC</em> keyword and arguments determine whether or not dynamic priority modification is allowed. If <em>PRIORITY</em> is equal to <em>ABSOLUTE</em>, OS/2 will run all threads at the same priority and refuse any request for modification. The *DYNAMIC8 configuration allows the full span of task priority usage.</p>
<p><em>MAXWAIT=x</em> tells OS/2 not to allow a thread to sit idle for more than <em>x</em> seconds. If a thread is idle for more than the specified number of seconds, it will receive a boost in priority for one time slice so that the scheduler will run it. The default number of idle seconds is 3.</p>
<p>Finally, <em>TIMESLICE =x|,y|</em> allows the user to set the range of milliseconds the OS/2 scheduler will dedicate to a given thread. The <em>x</em> parameter is the lowest time slice any given thread will be granted, and <em>y</em> is the maximum allowable time slice.</p>
<p><em>TIMESLICE</em> helps OS/2 reduce the amount of context switches necessary to execute a CPU bound thread. If a thread has executed for <em>x</em> milliseconds but has not completed a computation or is not blocking for I/O, then its subsequent time slices will each be incremented by one millisecond, up to <em>y</em> milliseconds, until the computation is done.</p>
<p>These configuration settings are important to users of OS/2. It is possible and very easy to boost your own processes’ threads to a time-critical, level 31 priority and stay there. But doing so will turn OS/2 into a single-tasking system, which is no less than pure abuse of this OS/2 feature.</p>
<p>Legitimate time-critical applications do exist. However, most tasks should be run at regular priority. If a thread must use time-critical priority, it should only do so for a few milliseconds. This area will require education and documentation to prevent users from being battered by CPU selfishness.</p>
<p>The last area in process tasking control to be covered is process and thread termination. If you again look at Figure 2, you can see the last OS/2 system call in each function is to <em>DosExit()</em>. This call takes two arguments: an action code and the thread’s result code.</p>
<p>An action code of 1 tells OS/2 to terminate all threads in the process. An action code of 0 only terminates the caller. The result code is a value that is returned to the parent thread waiting for the child process to terminate execution. The result code is analogous to the code that is passed to the standard C library functions <em>exit()</em> and <em>_exit()</em>.</p>
<p>A thread can wait on a process to terminate in two ways. If the process is spawned synchronously, the thread will automatically block until its child process terminates, returning the result code. When a process is created to execute asynchronously, the system routine <em>DosCWait()</em> must be used to obtain the result codes of the child. By using <em>DosCWait</em>, a thread can wait on just its direct child’s termination or, optionally, the termination of any grandchildren and great-grandchildren, and so on.</p>
<p>Another extremely useful OS/2 routine is <em>DosExitList()</em>. This routine maintains a list of code addresses that should be called when a process terminates normally or abnormally, although the main purpose is to service abnormal terminations. Those of you who will be writing such tools as data base interface libraries will want to make use of this routine.</p>
<p>Suppose an application uses a data base interface for transaction processing. Any form of abnormal termination-could leave.the data base file in an undefined state. By using <em>DosExitList()</em>, the data base manager could be alerted to flush its cache buffers and close files before the process terminates.</p>
<h2 id="memory-management">Memory management</h2>
<p>Memory management under OS/2 is much more than merely dynamic memory allocation. All of the common memory allocation primitives (and then some) are present.</p>
<p>The most important feature of memory management is the support of virtual memory. Virtual memory allows a process to obtain more memory than physically exists. The OS/2 memory manager works in conjunction with three other OS/2 resources: the swapper, LDT (Local Descriptor Table), and GDT (Global Descriptor Table). The LDT and GDT are used to implement not only virtual memory but also memory protection and shared memory. With memory protection, a process is not allowed to use memory it doesn’t own.</p>
<p>Each process has an LDT. There is one system GDT. A process’s memory is mapped from virtual addresses to physical addresses using the process’s LDT. The GDT contains virtual-to-physical memory mapping for systemwide resources used by all applications. If you try to access an address not in your process’s LDT, OS/2 will abort your process because of a hardware trap. Figure 3 shows how physical memory is mapped to virtual addresses in the LDT.</p>
<p>A process can request the allocation and use of memory in a size that doesn’t physically exist on the machine. The OS/2 memory manager calls on the swapper to create room in physical memory for the request. The swapper operates with a least-used algorithm to yank segments of data from RAM to disk, freeing the needed memory for the requesting process.</p>
<p><img src="https://gitpi.us/img/os2-table3.jpg" alt="Table 3"></p>
<p>A swapped segment is marked as such in the LDT with a special flag. If a thread attempts to access data that has been swapped to disk, OS/2 blocks the thread. The swapper is then interrupted by hardware to create room to read the segment back from disk. Swapping in a segment often requires that another nonused segment be swapped out first. When the new physical segment is allocated, the LDT virtual-to-physical mapping must be updated and the segment marked as nonswapped. The thread will then complete its time slice.</p>
<p>Theoretically, this swapping scheme facilitates the use of 1 gigabyte of virtual memory within the 286’s environment that is physically limited to addressing 16MB. Realistically, total memory is limited to the lump sum of physical RAM and the maximum size your hard disk can be formatted to.</p>
<p>Unfortunately, even with this large amount of virtual memory, processes that totally exploit the limitations of physical memory can perform sluggishly due to the swapper constantly “hitting” the disk to accommodate the needed memory. This sluggishness occurs under any virtual-memory-based operating system.</p>
<p>With the OS/2 memory manager, you are no longer limited to using memory in chunks restricted to 64K. A very large memory allocation routine is supplied in <em>DosAllocHuge()</em>. Allocating segments of memory smaller than 64K is supported, as is the reallocation of both large and small segments. <em>DosAllocSeg()</em> is used to allocate memory in chunks smaller than 64K.</p>
<p>Shared memory is arranged by calling <em>DosAllocShrSeg()</em>, Shared memory is memory that can be accessed by one or more processes and can technically be called an IPC facility. One process may call <em>DosAllocShrSeg()</em> to create the shared memory; other processes wanting to share the memory must call <em>DosGetShrSeg()</em>.</p>
<p>A path identifier references the segment used for creation and sharing. Both <em>DosAllocShrSeg()</em> and <em>DosGetShrSeg()</em> are returned a segment selector to the memory that is commonly referenced by a logical path beginning with the directory \SHAREMEM\ as in \SHAREMEMNDATABASE.REC. Although this procedure is like opening a file, shared memory is not associated with the OS/2 file system.</p>
<p>You may share a block of memory obtained by <em>DosAllocSeg()</em> with another process by calling <em>DosGiveSeg()</em>. However, <em>DosAllocShrSeg()</em> and <em>DosGetShrSeg()</em> are easier to use and more versatile because the caller of <em>DosGiveSeg()</em> must pass the memory recipient the returned selector by some other means of IPC. Also, the caller of <em>DosGiveSeg()</em> must know the recipient’s process ID.</p>
<p>OS/2 maps the shared memory segment onto the LDT of one or more processes. Therefore, two or more processes may use a block of physical memory by referencing the virtual addresses found in the LDT.</p>
<h2 id="device-services">Device services</h2>
<p>Among the variety of devices supported, the most commonly used directly by an application are the video display (the terminal), keyboard, and mouse. The disk drive is also used frequently by most applications, but I will focus on disk access when I discuss file management.</p>
<p>The video interface to OS/2 is a refreshing change from using the IBM PC memory map and TTY device drivers of other operating systems. The video interface is a superset of the IBM PC BIOS services, but don’t let the thought of INT 10H frighten you. It, like the other OS/2 dyn-link interfaces, is implemented through high-level <em>CALL</em>s. You won’t find the slow performance of the BIOS in OS/2’s video subsystem. OS/2 does not call the ROM BIOS since software interrupts are not permitted in protected mode. The superset actually uses IOPL to directly access the video hardware, so it’s at least as fast as writing directly to the memory map yourself.</p>
<p>Listing some of the function calls an application can make will give you an idea of services available in OS/2. You can write a string with an attribute using <em>VioWrtCharStrAtt()</em>. Repositioning the cursor is handled by <em>VioSetCursorPos()</em>. And no-flicker, smooth scrolling is handled by <em>VioScrollxx()</em>, where <em>xx</em> is either <em>Dn</em>, <em>Lf</em>, <em>Rt</em>, or <em>Up</em>.</p>
<p>Although you can get a pointer to the physical video buffer for both character and graphics modes, doing so is highly discouraged. With the high-level interface to character-mode video, accessing the physical pointer is not really necessary. You can use the physical buffer for graphics, but doing so is strictly device dependent. Graphics is fast becoming the job of sophisticated interfaces such as WPM.</p>
<p>OS/2 keyboard support is a superset of the IBM ROM BIOS INT 16H interface. You can either read or peek at the keyboard. Reading one character from the keyboard is done through <em>KbdCharIn()</em>. The keystroke information is returned with character and scan code information and shift statuses, so you can interpret function and other special keys. Shift and press statuses are independently accessible through <em>KbdGetStatus()</em>. With <em>KbdStringIn()</em>, you can read a string of data requesting a maximum number of characters and receive the actual number input.</p>
<p>Mouse support is also a tremendous improvement over the MS-DOS interrupt interface. To initialize the mouse, just call <em>MouOpen()</em>. Instead of having to use interrupts to access mouse movement and button press information, just call <em>MouReadEventQue()</em>. The mouse device driver and subsystem implement an event queue that contains mouse events generated by the user. The event queue is a circular buffer and has a default length of 10 events. This default may be changed in the CONFIG.SYS system file.</p>
<p>CONFIG.SYS is also used to install the mouse driver of your choice. Different mouse drivers are available for the serial, bus, import, and Mouse Systems mouse device. A new keyword for the mouse driver tells which serial port the device is installed on. The default port is COM1, but by using the <em>SERIAL=</em> keyword, you can set up use of the mouse on COM2.</p>
<p>Here is how you would install the mouse driver on COM2 for the Microsoft serial mouse for use with both protected mode and real mode:</p>
<pre tabindex="0"><code>device=mouse02.sys serial=com2
    mode=b
</code></pre><p><em>mode=b</em> says to use both protected mode and real mode. To use only protected mode, you would enter <em>mode=p</em>.</p>
<p>All three of these device subsystems (video, keyboard, and mouse) can be replaced by application software. By using <em>VioRegister()</em>, <em>KbdRegister()</em>, or <em>MouRegister()</em>, you can replace one or all of the routines the subsystem uses by mapping the default routine(s) to a replacement in your application.</p>
<p>One of the most valuable features of these device services is that outside of the mouse routines they are primarily FAPI calls and therefore bindable to MS-DOS. The VIO interface under MS-DOS uses direct memory mapping of video character mode output for optimum speed. The mouse interface is not portable or bindable to MS-DOS because the mouse interface is interrupt driven when using the MS-DOS driver.</p>
<h2 id="file-management">File management</h2>
<p>The file system that OS/2 uses is basically the same as that offered under MS-DOS. In fact, all normal files are completely compatible between the two systems. The file system is based on a hierarchical directory structure similar to that of UNIX. Currently file permissions are passed off to the OS/2 LAN manager, but in a future release of OS/2 they will be featured as a standard part of the operating system interface.</p>
<p>The hard disk size is limited under OS/2. Actually, the limitation is not directly related to disk size but to partition size. The maximum size of any partition is 32MB, but this size is not entirely restrictive to larger storage media. With a larger disk drive, you can partition the disk into multiple 32MB (or smaller) partitions and reference each partition as a logically separate disk drive. Thus you don’t completely lose the storage capacity of the disk, but each logical drive may be only as large as 32MB. The use of a non-IBM standard hard drive (or even a 32-inch floppy) requires creating a new disk device driver since OS/2 only supports standard devices.</p>
<p>There is good news for people that cannot tolerate the thought of copy protection. Copy protection schemes will not work under OS/2. Software cannot access absolute disk locations; therefore, OS/2 will not be able to read a file that was saved in this way. (Sounds like a good limitation!)</p>
<p><em>DosOpen()</em> and <em>DosClose()</em> are used to open and close files, respectively. As under MS-DOS and UNIX, each process inherits its parents open file descriptors, including <em>stdin</em>, <em>stdout</em>, and <em>stderr</em> (if they are open). By default, a process may have as many as 20 open file descriptors. By calling <em>DosSetMaxFH()</em>, this limit may be dynamically adjusted up to a limit of 255 open handles per process. The old <em>files=xx</em> of MS-DOS is a no-op for CONFIG.SYS under OS/2 protected mode.</p>
<p>The file system API is complete with the read and write primitives <em>DosRead()</em> and <em>DosWrite()</em>. In addition, two new read and write functions, <em>DosReadAsync()</em> and <em>DosWriteAsync()</em>, are implemented through threads. These routines allow the programmer to do asynchronous reads and writes so that the application is free to continue processing in another area while the disk file is being accessed.</p>
<p>Directory query functions are provided by using <em>DosFindFirst()</em>, <em>DosFindNext()</em>, and <em>DosFindClose()</em>. The information passed to <em>DosFindFirst()</em> may be in wild card format. The output of both <em>DosFindFirst()</em> and <em>DosFindNext()</em> contains full file name, attribute, and status (date, time, and access) information.</p>
<p>You can change the size of a writable file with DosNewSize(). However, <em>DosNewSize()</em> is slow and thus is not used by the OS/2 swapper to shrink the swap file after a segment is yanked back from disk to memory. The swapper’s disk swap file can grow (and eventually will unless you remove it yourself) to the limit of a disk partition. Consequently, the swap file should reside in its own partition so it doesn’t steal space from your standard file system.</p>
<h2 id="interprocess-communication">Interprocess communication</h2>
<p>IPC is a familiar resource to those who have used a multitasking system before. OS/2 supplies the primary IPC mechanisms in pipes, queues, and semaphores. (Though shared memory is also a form of IPC, it is discussed in more detail in the section on memory management.)
Most MS-DOS and UNIX programmers have used pipes at one time or another. The shell command:</p>
<p><code>sort customer.dat  more</code></p>
<p>sorts the file customer.dat and writes the output to <em>stdout</em> (standard output—video display). by default. However, the | syntax tells the command interpreter to send the standard output of <em>sort</em> to <em>stdin</em> (standard input—usually the keyboard) of the
more file pager. This process is known as a pipe.</p>
<p>The <em>DosMakePipe()</em> and <em>DosDupHandle()</em> system calls set up a pipe between two processes. A pipe has a read end and a write end. The thread that reads from the pipe cannot also write to the pipe. The very nature of how this form of pipe IPC is implemented requires that sharers of the pipe be closely related. Generally, a parent process will set itself up to read from or write to the pipe and create a child process to do the opposite. Having named pipes (provided with the OS/2 LAN manager) allows two processes to use pipes and be remotely related, similar to using files.</p>
<p>OS/2 automatically synchronizes a pipe’s reader and writer. If a pipe’s reader runs out of data, an attempt to read the pipe will cause the reader to block (an operating-system-enforced wait state). Likewise, if a pipe’s writer is filling faster than data can be drained, the writer will block.</p>
<p>OS/2 message queues facilitate the passing of messages from several servers to a client. The queue reader process creates a message queue by calling <em>DosCreateQueue()</em>. The queue path
identifier to be used to create the queue and attach to the queue by the servers is in the form \QUEUES, as in \QUEUES\DATABASE.MSG. Message queues, like shared memory, are not truly related to the OS/2 file system.</p>
<p>Messages can enter and leave the queue in one of three ways: through FIFO, LIFO, or a server priority number (priority is O—15; 15 is the highest priority). A server adds messages to the queue using <em>DosWriteQueue()</em>, and the client reads from the queue using <em>DosReadQueue()</em>. <em>DosPeekQueue()</em> is used by the client to search for a particular type of message.</p>
<p>Each message has an associated data buffer: the message itself. A special request word attached to each message is to be understood by both the servers and client. Using the request word makes quéues more powerful and versatile since the request can be a message in itself. When the queue is read, the request word is accompanied by the process ID of the writer of the message. OS/2 makes no special use of the request word, it only queues the value along with the rest of the message.</p>
<p>The data buffer associated with message queues must be a pointer to a shared memory block if the message will be passed to a thread not in the sender’s process. You will also need to implement a queue buffer array. The queue buffer array allows multiple messages to be placed on the queue without destroying messages that haven’t been read yet. This extra work is needed because the kernel doesn’t manage a kernel buffer area for each message buffer, it only retains a far pointer to the message data.</p>
<p>A semaphore is used to serialize the access of reusable resources such as data, as well as physical devices between two or more asynchronously executing threads. The semaphore model asserts that only one thread can own a shared resource at any given time. A thread calls the OS/2 API to obtain ownership of a common resource-representing semaphore. The resource itself is not really owned. Also, owning a semaphore does not guarantee that another thread will refrain from accessing the shared resource. However, the ownership of the resource-representing semaphore is adequate resource ownership for all threads playing by the rules.</p>
<p>OS/2 has two kinds of semaphores: RAM and system. Your choice depends on how you intend to use the semaphore. RAM semaphores are typically used by threads within the same process to coordinate globally accessible data owned by the process. System semaphores are used on a systemwide basis to allow threads from two or more processes to share a common resource such as a disk file or the keyboard (if two asynchronous processes within the same screen group are contending for use of the keyboard).</p>
<p><img src="https://gitpi.us/img/os2-table4.jpg" alt="Table 4"></p>
<p>RAM semaphores fit so well into the environment of one process because of how they are implemented. A RAM semaphore is just a double word of storage (C long type). The semaphore is initially set to zero, meaning that it is not owned, and used by all threads that want to share a resource. The variable, if declared as a global, can be used by all threads within a process.</p>
<p>System semaphores, like queues and shared memory, are created and opened by the threads that require their use. The identifying directory for system semaphores begins with the \SEM\ path as in \SEM\DATABASE.SEM. Only one process may create and own a particular system semaphore. Multiple threads may open a system semaphore. The OS/2 APIs for accessing this semaphore type are <em>DosCreateSem()</em> and <em>DosOpenSem()</em>.</p>
<p>The <em>DosSemRequest()</em> call is used to get ownership of the semaphore. <em>DosSemClear()</em> relinquishes ownership of the semaphore. These calls work on both RAM and system semaphores.</p>
<p>A thread may request one of three action types if it requests semaphore ownership and another thread already owns it. The thread may block indefinitely until the semaphore is cleared. The thread may also set a time-out limit for waiting on the semaphore or request an immediate time-out. An immediate time-out tells the <em>DosSemRequest()</em> routine to return right away if the semaphore is already owned so that the thread can do something else in the meantime.</p>
<p>A thread should not retain ownership of a semaphore for an indefinite period. Instead, it should access the common resource and release the semaphore as soon as possible so that processing is efficient. Figure 4 illustrates three threads in two different processes requesting semaphore ownership for video access privileges.</p>
<p>OS/2 also allows threads to use semaphores as signal-triggering mechanisms. Suppose thread A sets a semaphore as a trigger to thread B, which blocks until thread A clears the semaphore. Thread A does not clear the semaphore until an event occurs. The event could be related to an auto-answer modem being called. When thread A detects that the modem has been called, it would clear the semaphore so that thread B would answer and handshake with the caller.</p>
<p><em>DosSemSet()</em>, <em>DosSemWait()</em>, and <em>DosMuxSemWait()</em> can be added to the list of OS/2 routines that support a semaphore signaling environment. Most notably, <em>DosMuxSemWait()</em> allows a thread to wait on one of up to 16 events with time-out specifications.</p>
<h2 id="miscellaneous-os2-services">Miscellaneous OS/2 services</h2>
<p>Among the hundreds of OS/2 API services, some of the more commonly used routines are in the areas of task timing and process signaling. Your application may also need sound, whether for music or just to get some attention.</p>
<p>The OS/2 task-timing API supports an interval timer and asynchronous timer delay interface. You may start the interval timer using <em>DosTimerStart()</em>. The interval timer clears a set semaphore based on the interval duration specified by the caller. The semaphore should be reset before each interval is reached by the timer. The semaphore will continue to be cleared until the timer is stopped by calling <em>DosTimerStop()</em>.</p>
<p>The <em>DosTimerAsync()</em> routine works similarly to DosTimerStop() but only waits on one elapsed timing event and then clears the semaphore. These calls mesh well with the signaling semaphore environment so that a blocking thread may be dispatched on timed intervals.</p>
<p>A thread may give up its time slice for a number of milliseconds by calling <em>DosSleep()</em>. Threads may need to wait on some kind of event. <em>DosSleep()</em> causes a thread to forfeit the CPU instead of wasting CPU time in an empty loop.</p>
<p>Signals are caused by events that occur in hardware or software that directly affect a process or group of processes. <em>SIGTERM</em> is a common OS/2 signal generated when another process calls <em>DosKillProcess()</em>, a tasking call, to terminate another process. Usually a process is aborted immediately when it receives this signal. A process may disable signal processing by using <em>DosHoldSignal()</em> so that the <em>SIGTERM</em> event would not affect the signaled process.</p>
<p><em>DosSetSigHandler()</em> records the routine address within a process to be called by OS/2 when a given signal occurs. Supporting a signal handler allows a process to properly recover from a signal or terminate gracefully, if necessary.</p>
<p>To use the speaker attached to your PC, call the <em>DosBeep()</em> service routine. <em>DosBeep()</em> takes as arguments a frequency and time duration. The speaker will generate the given sound frequency (between 25H and 7FFFH) for the number of specified milliseconds.</p>
<h2 id="what-the-future-holds">What the future holds</h2>
<p>Will OS/2 be the operating system of the future for 286/386 microcomputers? Nothing. is certain, but it does have the required substance and definitely enough momentum and corporate support. The investment is high, not only in dollars but also in time and complexity. OS/2 is no toy. It was designed to serve the purpose of a platform for the next generation of 286/386 software.</p>
<p>OS/2 will receive a few enhancements in the future. The file system will most likely become installable, and security mechanisms will be added.</p>
<p>And what about 80386 support? Although OS/2 runs on a 386, it doesn’t take full advantage of it. With the 386, a full 32-bit linear address space is available; 32-bit machine operations and multiple concurrent MS-DOS screen groups can run along with protected-mode applications. In addition, much of the code now in OS/2 will disappear because the 386 itself supports virtual memory and paging. The next two years may well prove to be the most important for OS/2.</p>
<hr>
<p>Vaughn Vernon is president of Aspen Scientific, a company that specializes in programmer’s tools for cross development between UNIX/Xenix, OS/2, and MS-DOS. Vaughn is coauthor of The Advanced C Programmer’s Guide to OS/2 published by Microsoft Press.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MCP: An (Accidentally) Universal Plugin System (156 pts)]]></title>
            <link>https://worksonmymachine.ai/p/mcp-an-accidentally-universal-plugin</link>
            <guid>44854860</guid>
            <pubDate>Sun, 10 Aug 2025 12:53:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://worksonmymachine.ai/p/mcp-an-accidentally-universal-plugin">https://worksonmymachine.ai/p/mcp-an-accidentally-universal-plugin</a>, See on <a href="https://news.ycombinator.com/item?id=44854860">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>There's this thing about USB-C that nobody really talks about. Not the part where we all had to buy new dongles (RIP my dongle drawer, 2010-2023). The other part.</p><p>See, we all thought USB-C was just going to be about charging things and moving files around like the other USBs. Very serious. Very purposeful. But because of the way it is it can do... other things.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!kwfs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kwfs!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!kwfs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png" width="1024" height="1024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1960400,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!kwfs!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>My friend Rex connected his toaster to his monitor last week. I don't know why. The toaster doesn't know why. But it </span><em>worked</em><span>, and now Rex's toast has HDMI output.</span></p><p>Remember car cigarette lighters? Nobody uses them for cigarettes anymore. They're just universal power outlets that happen to be shaped like something from 1952. Your car doesn't care if you're charging a phone or running a personal pizza oven. The hole is the same size. The power is there.</p><p><em>The protocol doesn't judge your life choices.</em></p><p>This brings me to something I discovered about MCP (Model Context Protocol) while trying to make my calendar app order takeout. Stay with me here.</p><p>Everyone thinks MCP is for making AI assistants smarter. You know, "Claude, please read my files and understand my soul." And sure, it does that. But here's what they put in the documentation that made me spit out my morning tea:</p><blockquote><p>"MCP provides a standardized way to connect AI models to different data sources and tools."</p></blockquote><p><span>Okay but. </span><em>But</em><span>. What if you just... removed the AI part?</span></p><p><span>What if it's just "a standardized way to connect </span><s>AI models</s><span> </span><strong>literally anything</strong><span> to different data sources and tools"?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!MBI9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!MBI9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 424w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 848w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 1272w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!MBI9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png" width="1456" height="954" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:954,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:554311,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!MBI9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 424w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 848w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 1272w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Or remember when someone looked at NFTs—which were supposed to just </span><em>point</em><span> at images—and thought "what if the pointer... WAS the image?"</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!C2qU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!C2qU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 424w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 848w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 1272w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!C2qU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png" width="1456" height="475" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:475,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:417583,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!C2qU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 424w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 848w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 1272w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>For those of you who don’t get the idea, copy and paste this into your url bar: data:application/json;base64,eyJuYW1lIjogIkJhZyAjNzQ4IiwgImRlc2NyaXB0aW9uIjogIkxvb3QgaXMgcmFuZG9taXplZCBhZHZlbnR1cmVyIGdlYXIgZ2VuZXJhdGVkIGFuZCBzdG9yZWQgb24gY2hhaW4uIFN0YXRzLCBpbWFnZXMsIGFuZCBvdGhlciBmdW5jdGlvbmFsaXR5IGFyZSBpbnRlbnRpb25hbGx5IG9taXR0ZWQgZm9yIG90aGVycyB0byBpbnRlcnByZXQuIEZlZWwgZnJlZSB0byB1c2UgTG9vdCBpbiBhbnkgd2F5IHlvdSB3YW50LiIsICJpbWFnZSI6ICJkYXRhOmltYWdlL3N2Zyt4bWw7YmFzZTY0LFBITjJaeUI0Yld4dWN6MGlhSFIwY0RvdkwzZDNkeTUzTXk1dmNtY3ZNakF3TUM5emRtY2lJSEJ5WlhObGNuWmxRWE53WldOMFVtRjBhVzg5SW5oTmFXNVpUV2x1SUcxbFpYUWlJSFpwWlhkQ2IzZzlJakFnTUNBek5UQWdNelV3SWo0OGMzUjViR1UrTG1KaGMyVWdleUJtYVd4c09pQjNhR2wwWlRzZ1ptOXVkQzFtWVcxcGJIazZJSE5sY21sbU95Qm1iMjUwTFhOcGVtVTZJREUwY0hnN0lIMDhMM04wZVd4bFBqeHlaV04wSUhkcFpIUm9QU0l4TURBbElpQm9aV2xuYUhROUlqRXdNQ1VpSUdacGJHdzlJbUpzWVdOcklpQXZQangwWlhoMElIZzlJakV3SWlCNVBTSXlNQ0lnWTJ4aGMzTTlJbUpoYzJVaVBsTm9iM0owSUZOM2IzSmtQQzkwWlhoMFBqeDBaWGgwSUhnOUlqRXdJaUI1UFNJME1DSWdZMnhoYzNNOUltSmhjMlVpUGtScGRtbHVaU0JTYjJKbElHOW1JSFJvWlNCR2IzZzhMM1JsZUhRK1BIUmxlSFFnZUQwaU1UQWlJSGs5SWpZd0lpQmpiR0Z6Y3owaVltRnpaU0krU0c5dlpEd3ZkR1Y0ZEQ0OGRHVjRkQ0I0UFNJeE1DSWdlVDBpT0RBaUlHTnNZWE56UFNKaVlYTmxJajVRYkdGMFpXUWdRbVZzZER3dmRHVjRkRDQ4ZEdWNGRDQjRQU0l4TUNJZ2VUMGlNVEF3SWlCamJHRnpjejBpWW1GelpTSStSR2wyYVc1bElGTnNhWEJ3WlhKelBDOTBaWGgwUGp4MFpYaDBJSGc5SWpFd0lpQjVQU0l4TWpBaUlHTnNZWE56UFNKaVlYTmxJajVEYUdGcGJpQkhiRzkyWlhNOEwzUmxlSFErUEhSbGVIUWdlRDBpTVRBaUlIazlJakUwTUNJZ1kyeGhjM005SW1KaGMyVWlQazVsWTJ0c1lXTmxQQzkwWlhoMFBqeDBaWGgwSUhnOUlqRXdJaUI1UFNJeE5qQWlJR05zWVhOelBTSmlZWE5sSWo1VWFYUmhibWwxYlNCU2FXNW5QQzkwWlhoMFBqd3ZjM1puUGc9PSJ9</figcaption></figure></div><p>The protocol meant for storing references became a protocol for storing reality. It's like using a library card as the actual book.</p><p><span>Here's where it gets even better. The more MCP servers people build for AI, the more capabilities </span><em>every</em><span> app can have. It's like:</span></p><ol><li><p>Someone builds an MCP server for their AI to access Spotify</p></li><li><p>Your workout app can now generate playlists</p></li><li><p>You didn't write any Spotify code</p></li><li><p>The Spotify MCP developer doesn't know your app exists</p></li><li><p>Everyone wins?</p></li></ol><p>It's like a potluck where everyone brings their specialty dish, but instead of food, it's functionality. And instead of eating, you're... actually, this metaphor is falling apart. But you get it.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Laga!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Laga!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 424w, https://substackcdn.com/image/fetch/$s_!Laga!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 848w, https://substackcdn.com/image/fetch/$s_!Laga!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 1272w, https://substackcdn.com/image/fetch/$s_!Laga!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Laga!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png" width="1456" height="1132" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1132,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:629338,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Laga!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 424w, https://substackcdn.com/image/fetch/$s_!Laga!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 848w, https://substackcdn.com/image/fetch/$s_!Laga!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 1272w, https://substackcdn.com/image/fetch/$s_!Laga!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The beautiful chaos is that every MCP server built for Claude or ChatGPT or whatever becomes a free plugin for </span><em>anything</em><span> that speaks MCP. It's accidentally creating a universal plugin ecosystem. Nobody planned this (I don’t think). It's just happening.</span></p><p>They keep saying MCP is like USB-C for AI. But what does that actually mean?</p><p><span>USB-C isn't special because it's a port. It's special because it's a </span><em>possibility space</em><span>. It's a hole that says "put something here and we'll figure it out." Power? Sure. Data? Why not. Video? Apparently yes. Toaster control protocols? Rex says absolutely.</span></p><p>MCP is the same thing but for functionality. It's not saying "I'm for AI." It's saying "I'm a well-designed hole. Put something here."</p><p><span>So we’re building this thing called </span><strong>APM</strong><span> (</span><a href="https://actionsperminute.io/" rel="">Actions Per Minute</a><span>). On paper, it's a task management app. In reality? It's a shape-shifter that becomes whatever you plug into it.</span></p><p>The entire plugin system? Just MCP servers.</p><p>Want spell check? MCP server.  </p><p>Want it to order coffee when you complete 10 tasks? MCP server.  </p><p>Want your AI agents to respond like peons from Warcraft 3 when you assign them a task? Of course you do, and that MCP server is already written and ready to use.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!cfIp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!cfIp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 424w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 848w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 1272w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!cfIp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png" width="1456" height="1443" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e3871464-0023-4b56-8235-fad217a9a232_3045x3018.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1443,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1310775,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!cfIp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 424w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 848w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 1272w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Every great protocol gets used for something its creators never imagined:</p><ul><li><p>HTTP was for academic papers. Now it runs civilization.</p></li><li><p>Bluetooth was for hands-free calling. Now it unlocks your front door.</p></li><li><p>USB was for keyboards and mice. Now it charges your emotional support portable fan.</p></li></ul><p>MCP thinks it's for giving context to AI models.</p><p>But really? It's just a really good protocol for making things talk to other things.</p><p>And in a world where Rex's toast has HDMI output, maybe that's exactly what we need.</p><p>---</p><p><strong>P.S.</strong><span> If you build an MCP server that makes your computer emit the smell of fresh bread, we need to talk.</span></p><p><strong>P.P.S.</strong><span> We’ve just opened up early access for APM. Build something weird. Build something useful. Build something that makes us question our life choices. I believe in you.</span></p><p><em>(Somewhere, a protocol is being used exactly as intended. This is deeply suspicious.)</em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Booting 5000 Erlangs on Ampere One 192-core (193 pts)]]></title>
            <link>https://underjord.io/booting-5000-erlangs-on-ampere-one.html</link>
            <guid>44854525</guid>
            <pubDate>Sun, 10 Aug 2025 11:41:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://underjord.io/booting-5000-erlangs-on-ampere-one.html">https://underjord.io/booting-5000-erlangs-on-ampere-one.html</a>, See on <a href="https://news.ycombinator.com/item?id=44854525">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        
        <small>2025-08-05</small>
        <p>
            Underjord is an artisanal consultancy doing consulting in Elixir, Nerves with an accidental speciality in marketing and outreach. If
            you
            like
            the writing you should really <a href="https://underjord.io/services.html">try the pro version</a>.</p>
        <p>In the previous post on <a href="https://underjord.io/500-virtual-linux-devices-on-arm64.html">500 virtual linux devices on ARM64</a> I hinted that I expected serious improvements if we got KVM working. Well. We’re there. Let’s see what we got going on.</p>
<p><em>Disclosure: I am running a conference called <a href="https://goatmire.com/">Goatmire Elixir</a> which Ampere is a sponsor of. This post is not part of the sponsorship exchange as such. It is prep for my talk for the conference which uses the hardware they lent me. So this is your transparency notice, but fundamentally I am not making comparisons on whether they are better or not. I’m learning and sharing about qemu and virtual Linux machines. Now I’d love if they paid me to shill them a bit later and I’d be transparent about that too. But this is not that :)</em></p>
<p>To recap. We have an Ampere One 192-core machine with 1 TB of RAM. The goal is to run as many virtual Linux IoT devices <em>using the Nerves framework</em>. We got 500 of them last time before I tried pushing any further. I also got a bit further on the same setup when I tried. Maybe 1000, I don’t recall exactly. But there have been developments, so read on!</p>
<p>Briefly on Nerves: the framework treats the BEAM virtual machine like the OS and essentially only uses Linux for a kernel, drivers and the like. This means we can write much if not all of the embedded device in a productive high-level language running on a provenly robust and reliable environment with memory safety and solid recovery strategies. And it means your cloud integration developer doesn’t risk seg-faulting the entire device while mangling JSON back and forth. Nerves also brings some best-practice tooling and conventions. Your init process is <a href="https://github.com/nerves-project/erlinit">erlinit</a>, your updates use <a href="https://github.com/fwup-home/fwup">fwup</a> to provide A/B partitions and factory reset, auto-failback, validation of firmware viability, disk encryption, delta updates, streaming updates and a bunch more.</p>
<p>The most interesting development is the thing you can probably learn the most from. Frank Hunleth who has been my co-conspirator and a massive help saved me from fighting u-boot by .. writing another bootloader. Introducing <a href="https://github.com/fhunleth/little_loader">little_loader</a>. This adorable tractor will load up your ARM64 qemu device, consult the uboot environment that Nerves uses, find a Linux kernel from information in that and then boot. Consequently it enables the A/B upgrade features and everything else that makes Nerves great.</p>
<p>Writing a boot loader is a little bit ridiculous. Frank knows his way around C and apparently ChatGPT knows a fair bit about ARM and qemu. Enough to be dangerous. And where it was wrong he could rummage around until he found the way. How he does what he does is beyond me but the result is a very small boot loader that you can probably read through and understand. So if you are curious about booting ARM64 or about how qemu starts things the code should be a worthwhile read.</p>
<p>We got a bit tangled up in EL1 vs EL2 when we only ever needed EL1 to work. EL2 on ARM is what you’d run under if you want to be able to run VMs in your VMs so you can VM while you VM. And the version of qemu + KVM I got from Ubuntu doesn’t seem to support that. We weren’t interested in it either. At some point we might explore EL3 for secure boot and whatnot. Only time will tell.</p>
<p>One of the weirder challenges and something we haven’t disentangled yet is that we have some compilation issue where using the toolchains I was using the non-debug build would hang while the debug one runs fine. For now I run the debug build of the bootloader. I think it was fine from GCC 15? Anyway, hopefully we pin that down at some point. But it tripped us up a few times when the bootloader would hang due that issue rather than any actual problems with the implementation.</p>
<p>KVM didn’t really require anything extra aside from making sure we didn’t go to EL2. And when we tried it on MacOS it worked great with HVF as well. Host-based ARM64 VMs are ridiculously fast and practical. As in booting to the full IEx prompt in single-digit seconds instead of double-digit. And they use about 500Mb less memory. And see, that’s important. Because we want to shove as many as we can into this server I got access to.</p>
<h2 id="accelerated-on-host">Accelerated on host</h2>
<p>My very hacky project for running this stuff is <a href="https://github.com/lawik/amproj">available here</a>. This code is cribbed from <code>simple.sh</code>:</p>

  <div data-file="simple.sh">
    <p><span>shell</span>
      <span>simple.sh</span>
    </p>
    <div><pre tabindex="0"><code data-lang="shell">  qemu-system-aarch64 <span>\
</span><span></span>	-machine virt,accel<span>=</span>kvm <span>\
</span><span></span>	-cpu host <span>\
</span><span></span>	-smp <span>1</span> <span>\
</span><span></span>	-m 150M <span>\
</span><span></span>	-kernel ../little_loader/little_loader.elf <span>\
</span><span></span>	-netdev user,id<span>=</span>eth0 <span>\
</span><span></span>	-device virtio-net-device,netdev<span>=</span>eth0,mac<span>=</span>de:ad:be:ef:00:01 <span>\
</span><span></span>	-global virtio-mmio.force-legacy<span>=</span>false <span>\
</span><span></span>	-drive <span>if</span><span>=</span>none,file<span>=</span>/space/disks/special.img,format<span>=</span>raw,id<span>=</span>vdisk <span>\
</span><span></span>	-device virtio-blk-device,drive<span>=</span>vdisk,bus<span>=</span>virtio-mmio-bus.0 <span>\
</span><span></span>	-nographic</code></pre></div>
  </div>

<p>To go through it. We use <code>qemu-system-aarch64</code> to emulate an ARM64 machine. <code>aarch64</code> is the common shortname for ARM64, except sometimes on MacOS where I hear it can be <code>arm64</code>. We specify the <code>machine</code> to be <a href="https://www.qemu.org/docs/master/system/arm/virt.html">virt</a>. Previously we’d leave it there but now we use an <a href="https://www.qemu.org/docs/master/system/introduction.html">accelerator</a> named KVM (<a href="https://linux-kvm.org/page/Main_Page">Kernel-based Virtual Machine</a>). It is the virtualization mechanism included with Linux and qemu can integrate with that to accelerate the execution. This also requires <code>-cpu host</code>, meaning, we are no longer trying to emulate a <code>cortex-a53</code> processor. We are trying to run on the host processor, whatever that is. <code>host</code> means that we emulate the host CPU, or at least as much as qemu and the KVM accelerator can support of what the host can do. This is where we drop about 500Mb of memory overhead. We no longer have to have a pretend ARM chip in memory because we have an actual ARM chip to run on. That’s my understanding at least. Would love notes on that.</p>
<p>We only give it 1 core via <code>-smp 1</code> and we give it 150Mb memory with <code>-m 150</code>. Skipping ahead we give it virtual Ethernet and a <a href="https://docs.kernel.org/driver-api/virtio/virtio.html">virtio</a> block storage drive. You’ll see a lot of virt and virtio when doing this stuff. And with <code>-nographic</code> we tell it to not bother trying to pop up a GUI window, so we get our console in the terminal. I’ve done all the work over SSH so that’s definitely my preference.</p>
<p>The disk we provide runs from the raw disk image file <code>special.img</code> which was generated using <code>fwup</code> based on the Nerves project <code>amproj</code> I mentioned earlier. If you build that project with <code>mix firmware</code> you can then run:</p>

  <div>
    <p><span>shell</span>
      
    </p>
    <div><pre tabindex="0"><code data-lang="shell">fwup -a -i amproj.fw -d special.img -t complete</code></pre></div>
  </div>

<p>That’ll give you an image file that contains a full Nerves system. The stuff written to disk is:</p>
<ul>
<li>A uboot env formatted chunk of data. We don’t use uboot this time but we used that format.</li>
<li>A linux kernel, not on a filesystem. Just written to the disk. RAW!</li>
<li>An MBR and some partitions:
<ul>
<li>Root filesystem A (squashfs, read-only)</li>
<li>Root filesystem B (squashfs, read-only)</li>
<li>Application data partition (f2fs, read/write)</li>
</ul>
</li>
</ul>
<p>The uboot env is used to tell the bootloader important things about the A/B upgrade process as well as where to find the kernel to load as well as what <a href="https://www.kernel.org/doc/html/v4.14/admin-guide/kernel-parameters.html">kernel cmdline</a> to use which is how we tell it what root filesystem to use.</p>
<p>The only config I put into the loader is to set the offset where it can expect the uboot-env and I set that at build-time.</p>
<h2 id="promising-results">Promising results</h2>
<p>NervesCloud received 3389 simultaneous connected devices before the server hit me with the OOM killer. It was probably running a few more but around there. So each VM is:</p>
<ul>
<li>Bootloader</li>
<li>Linux</li>
<li>erlinit</li>
<li>BEAM/ERTS</li>
<li>Nerves base functionality</li>
<li><a href="https://github.com/nerves-hub/nerves_hub_link">NervesHubLink</a> for connecting to NervesHub</li>
</ul>
<p>I have had 3000 devices running stable and then I started to see “fun” challenges. For one thing, our NervesCloud hosts were looking a bit tight on memory because all these devices connect from the US west coast and we were only running a single node in that region. I scaled that up a smidge to make sure I didn’t bother any paying customers.</p>
<p>The VMs are super well-behaved, the Ampere CPU just works. The memory usage is roughly where I’d expect it. 150-250 total I think. There are probably things I can do to make it behave a little more tighter. Will explore that if time allows.</p>
<p>Then I ran my first demo workloads. As the purveyor of the finest Over-the-Air updates for embedded devices we here at NervesCloud.. I kid. But I wanted to shove lots of updates at them and see what that did. The updates process is a lot of compression, decompression and IO. Probably mostly IO-bound but if the devices would be struggling for CPU that’d be noticeable. If the memory usage exploded, that’d be noticed very quickly.</p>
<p>It worked. Not really any problems. I limited the concurrency of the update to 1000 and it couldn’t hand out the updates faster than they completed so it tended to hover around 200-300 concurrent updates happening. Or at least that’s my understanding of what happened. Did I mention the KVM setup is pretty fast?</p>
<p>I logged some issues about UI behavior as I was watching things live and trying to adjust things. It seems like good guy Nate Shoemaker already has a fix in flight for this. There may be more details. When you get a lot of progress reports the LiveView UI perhaps shouldn’t try to refresh all the things all the time.</p>
<h2 id="memory-tuning">Memory tuning</h2>
<p>Frank gave me some tips about tuning Linux memory usage and tuning BEAM memory usage. When I looked into his advice I ended up doing a few things:</p>
<p>For BEAM VM, we <a href="https://github.com/lawik/amproj/commit/cb5919eee5b5c58321cf67aaff388c5db05a1ccc#diff-b9fbf8080c62b37068a3fefe69eeed4d0b7e801049c87b9aaaf721c2d5ed47afR38">change the allocators</a>. This should use less memory and probably trades off in raw performance. Which is fine for this purpose.</p>
<p>Erlang release, <a href="https://github.com/lawik/amproj/commit/cb5919eee5b5c58321cf67aaff388c5db05a1ccc#diff-b9fbf8080c62b37068a3fefe69eeed4d0b7e801049c87b9aaaf721c2d5ed47afR26">use default mode</a> instead of embedded. Which probably makes it boot a bit faster, makes it use less memory but it could lead to surprising delays and growing memory usage later if it loads code ad-hoc. The use of embedded mode is helpful in making the release behave much more consistently, is my understanding.</p>
<p>Made <a href="https://github.com/nerves-project/nerves_system_qemu_aarch64/commit/149a0545312df0d422e44975babe9c9b15247ce7">a bunch of adjustments to Linux memory usage</a>. Using <code>zram</code> was suggested by Frank. Then I checked with (famous ML model) Claude to get hints about what knobs were available to tune on Linux because Frank hinted that it might be caching a bit much and I never know where to start when it comes to what I can do to the Linux kernel. It had some suggestions, I looked those up, found articles that matched the claims that this might reduce memory usage. Changing swappiness, dirty ratios and <code>vfs_cache_pressure</code> like I knew what I was doing and it sure seems to have improved things.</p>
<p>I know I could play with different allocators, my co-founder Josh has been doing that for NervesCloud recently. I think I could also do something with virtual balloons to reclaim memory and essentially over-provision VMs but I haven’t got there yet.</p>
<p>This memory tuning led to some interesting further runs where we ran a solid 5100 devices and I could have pushed it a bit further. I just didn’t have time and could be bothered to do more math at the time. The VMs are now started with 110 MB of RAM on the inside and they seem to run steady around 160 MB RES according to htop. The people I’ve talked to at Ampere indicate that I’m probably running the most VMs anyone has ever ran on their hardware. Which is fun. I’m not even running tiny VMs. I could make a Buildroot system that does nothing and run another gajillion probably. But this is much closer to a real device and workload.</p>
<h2 id="the-utility-of-it-all">The utility of it all</h2>
<p>Honestly, getting a chance to run significant, not massive, but significant workloads against a SaaS is pretty useful. But the work we’ve put in now means we can tidy up this Nerves system and make it part of supported Nerves tooling. This would make it easy to run stuff “on device” without physical hardware. It would make running more detailed tests of Nerves functionality much more feasible as well. Essentially you’d need an ARM64 Linux box with KVM or an Apple Silicon Mac and you’d get the blazing fast ones. Or you can absolutely get by with the emulated, more demanding things from the x86 side of things. There is a lot we can do with a full-featured qemu-system for ARM devices.</p>
<p>While my experimentation is a bit of a stunt and mostly for the joy of experimentation and Frank’s bootloader is mostly about learning the end result is still that we have produced something we should get good mileage out of.</p>
<p>Heck that MacOS thing. I just tried the <code>DELAY=1 COUNT=200 CHUNK=10 ./run.exs</code> after modifying the script to use <code>hvf</code>instead of <code>kvm</code> on my M2 MacBook Air. I think I had 50 VMs when I ran out of disk. Solvable problem, but not throwing out my photo library right this minute.</p>
<h2 id="further-work">Further work</h2>
<p>I need to look at how KVM and <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA</a> interact and if/how I can pin. I don’t think I’ll hit problems where caches and pinning matter all that much but it would feel better. When the VMs are at rest after booting the overall system CPU usage is generally less than 20% running thousands of VMs. Mostly idle, yes, but there are things happening in all of them.</p>
<p>Should run the workload with some graphs to see what is actually happening big picture. Right now I’m mostly going “hey, it is STILL running, eh!?”. Which is fine enough when figuring out if it fits in memory.</p>
<h2 id="tidying-up">Tidying up</h2>
<p>We are in the process of tidying up <a href="https://github.com/nerves-project/nerves_system_qemu_aarch64">nerves_system_qemu_aarch64</a> and then it should get a proper release and some docs. It has a mix task for generating an appropriate qemu command for you. So this all becomes a part of Nerves. Over time we should be able to build some really nice tooling based off of this. And if you have ideas you should be able to pick it up and run with it already.</p>
<p>Really enjoying this deeper dive into things I’ve only been at the periphery of. Learning a lot of Linux, getting to really get into it with qemu, performance tuning for both the BEAM, Linux and virtualization. It is a ton of fun to see how far you can push the hardware.</p>
<p>Alright, that’s enough words. Let me know what you think and if there is anything in particular you’d like me try in and around this. Thanks for reading, hit me up on <a href="mailto:lars@underjord.io">lars@underjord.io</a> or <a href="https://hachyderm.io/@lawik">@lawik@hachyderm.io</a> or wherever you find me.</p>

        <p>
            Underjord is an artisanal consultancy doing consulting in Elixir, Nerves with an accidental speciality in marketing and outreach. If
            you
            like
            the writing you should really <a href="https://underjord.io/services.html">try the pro version</a>.</p>
        <p>
            Note: Or try the videos on <a href="https://youtube.com/c/underjord">the YouTube
                channel</a>.
        </p>
        
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open Lovable (159 pts)]]></title>
            <link>https://github.com/mendableai/open-lovable</link>
            <guid>44854120</guid>
            <pubDate>Sun, 10 Aug 2025 10:10:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mendableai/open-lovable">https://github.com/mendableai/open-lovable</a>, See on <a href="https://news.ycombinator.com/item?id=44854120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">Open Lovable</h2><a id="user-content-open-lovable" aria-label="Permalink: Open Lovable" href="#open-lovable"></a></p>
<p dir="auto">Chat with AI to build React apps instantly.</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ee68ebd089163b65192416765f028cb913aa1cc4b475f4f4b94e54b0809ebeb9/68747470733a2f2f6d65646961312e67697068792e636f6d2f6d656469612f76312e59326c6b505463354d4749334e6a4578626d5a746148466c654752734d544e6c61574e7964476469616e49344e475134644868795a6a42306432566b636a52796558427563435a6c634431324d563970626e526c636d35686246396e61575a66596e6c666157516d593351395a772f5a46564c574d61366456736b5158307175312f67697068792e676966"><img src="https://camo.githubusercontent.com/ee68ebd089163b65192416765f028cb913aa1cc4b475f4f4b94e54b0809ebeb9/68747470733a2f2f6d65646961312e67697068792e636f6d2f6d656469612f76312e59326c6b505463354d4749334e6a4578626d5a746148466c654752734d544e6c61574e7964476469616e49344e475134644868795a6a42306432566b636a52796558427563435a6c634431324d563970626e526c636d35686246396e61575a66596e6c666157516d593351395a772f5a46564c574d61366456736b5158307175312f67697068792e676966" alt="Open Lovable Demo" width="100%" data-animated-image="" data-canonical-src="https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExbmZtaHFleGRsMTNlaWNydGdianI4NGQ4dHhyZjB0d2VkcjRyeXBucCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ZFVLWMa6dVskQX0qu1/giphy.gif"></a>
</p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<ol dir="auto">
<li><strong>Clone &amp; Install</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/mendableai/open-lovable.git
cd open-lovable
npm install"><pre>git clone https://github.com/mendableai/open-lovable.git
<span>cd</span> open-lovable
npm install</pre></div>
<ol start="2" dir="auto">
<li><strong>Add <code>.env.local</code></strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Required
E2B_API_KEY=your_e2b_api_key  # Get from https://e2b.dev (Sandboxes)
FIRECRAWL_API_KEY=your_firecrawl_api_key  # Get from https://firecrawl.dev (Web scraping)

# Optional (need at least one AI provider)
ANTHROPIC_API_KEY=your_anthropic_api_key  # Get from https://console.anthropic.com
OPENAI_API_KEY=your_openai_api_key  # Get from https://platform.openai.com (GPT-5)
GROQ_API_KEY=your_groq_api_key  # Get from https://console.groq.com (Fast inference - Kimi K2 recommended)"><pre><span><span>#</span> Required</span>
<span>E2B_API_KEY</span><span>=</span><span>your_e2b_api_key<span>  <span>#</span> Get from https://e2b.dev (Sandboxes)</span></span>
<span>FIRECRAWL_API_KEY</span><span>=</span><span>your_firecrawl_api_key<span>  <span>#</span> Get from https://firecrawl.dev (Web scraping)</span></span>

<span><span>#</span> Optional (need at least one AI provider)</span>
<span>ANTHROPIC_API_KEY</span><span>=</span><span>your_anthropic_api_key<span>  <span>#</span> Get from https://console.anthropic.com</span></span>
<span>OPENAI_API_KEY</span><span>=</span><span>your_openai_api_key<span>  <span>#</span> Get from https://platform.openai.com (GPT-5)</span></span>
<span>GROQ_API_KEY</span><span>=</span><span>your_groq_api_key<span>  <span>#</span> Get from https://console.groq.com (Fast inference - Kimi K2 recommended)</span></span></pre></div>
<ol start="3" dir="auto">
<li><strong>Run</strong></li>
</ol>

<p dir="auto">Open <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing simple tab-completions for Bash and Zsh (232 pts)]]></title>
            <link>https://mill-build.org/blog/14-bash-zsh-completion.html</link>
            <guid>44854035</guid>
            <pubDate>Sun, 10 Aug 2025 09:50:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mill-build.org/blog/14-bash-zsh-completion.html">https://mill-build.org/blog/14-bash-zsh-completion.html</a>, See on <a href="https://news.ycombinator.com/item?id=44854035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article>

<div id="preamble">
<p><em>Li Haoyi, 7 August 2025</em></p>
<p>Shell tab-completions can be very handy, but setting them up is complicated by the fact
that half your users would be using Bash-on-Linux, while the other half will be
using Zsh-on-OSX, each of which has different tab-completion APIs. Furthermore, most
users exploring an unfamiliar CLI tool using tab completion appreciate showing a
description along with each completion so they can read what it is, but that’s
normally only available on Zsh and not on Bash.</p>
<p>But with some work, you can make your tab-completions work on both shells, including
nice quality-of-life features like completion descriptions. This blog post will explore how it
can be done, based on our recent experience implementing this in the <a href="https://mill-build.org/">Mill build tool</a>
version <a href="https://github.com/com-lihaoyi/mill/blob/main/changelog.adoc#103">1.0.3</a>,
providing the great tab-completion experience you see below in a way that works across
both common shells. Hopefully based on this, you will know enough and have enough reference
examples to set up Bash and Zsh completions for your own command-line tooling.</p>
<div>
<p><img src="https://mill-build.org/blog/_images/CompletionDescriptions.png" alt="CompletionDescriptions">
</p>
</div>
<div>
<p><img src="https://mill-build.org/blog/_images/CompletionDescriptions2.png" alt="CompletionDescriptions2">
</p>
</div>
</div>
<div>
<h2 id="_basic_tab_completion"><a href="#_basic_tab_completion"></a>Basic Tab Completion</h2>
<div>
<p>The basic way tab-completion works in shells like Bash or Zsh is to register a handler
function that is called when a user presses <code>&lt;TAB&gt;</code> at the command line. This handler
function is then given the words currently written, and the index of the word the
user’s cursor is currently over. From this information, the completion function generates
a list of strings that are possible completions for the word at that index, and
return it to the shell. At a glance, this looks something like:</p>
<div>
<pre><code data-lang="bash">_generate_foo_completions() {
  local idx=$1; shift
  local words=( "$@" )
  local current_word=${words[idx]}

  local array=(apple apricot banana cherry durian)
  for elem in "${array[@]}"; do
    if [[ $elem == "$current_word"* ]]; then echo "$elem"; fi
  done
}

_complete_foo_bash() {
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  COMPREPLY=( "${raw[@]}" )
}

_complete_foo_zsh() {
  local -a raw
  raw=($(_generate_foo_completions "$CURRENT" "${words[@]}"))
  compadd -- $raw
}

if [ -n "${ZSH_VERSION:-}" ]; then
  autoload -Uz compinit
  compinit
  compdef _complete_foo_zsh foo
elif [ -n "${BASH_VERSION:-}" ]; then
  complete -F _complete_foo_bash foo
fi</code></pre>
</div>
<div>
<ul>
<li>
<p><code>_generate_foo_completions</code> is a dummy function used
for demonstration purposes that prints out a hardcoded set of completions,
but in a real scenario would be the logic that generates completions for
your specific app or CLI tool.</p>
</li>
<li>
<p><code>_complete_foo_bash</code> and <code>_complete_foo_zsh</code> are the shell-specific
completion functions that pass the current word to <code>_generate_foo_completions</code>
and wire up the results to each shell’s unique completion APIs. Bash completion
functions need to set the <code>COMPREPLY</code> environment variable, while Zsh completion
functions need to call <code>compadd</code> (or one of the other similar functions)</p>
</li>
<li>
<p>This example snippet would typically be put (or <code>source</code>ed) in your
<code>~/.bashrc</code>, <code>~/.bash_profile</code>, and <code>~/.zshrc</code> so the <code>if</code>/<code>elif</code>/<code>fi</code> block at
the bottom registers the relevant hooks when the shell starts.
These hook into tab-completion whenever <code>foo</code> is the
first word at the prompt.</p>
</li>
</ul>
</div>
<p>For example, the Mill build tool provides a <code>./mill mill.tabcomplete/install</code>
builtin that automatically updates these files and instructs the user to
restart the shell or <code>source</code> the relevant script to begin using completions:</p>
<div>
<pre><code data-lang="console">$ ./mill mill.tabcomplete/install
[1/1] mill.tabcomplete.TabCompleteModule.install
Writing to /Users/lihaoyi/.cache/mill/download/mill-completion.sh
Writing to /Users/lihaoyi/.bash_profile
Writing to /Users/lihaoyi/.zshrc
Writing to /Users/lihaoyi/.bashrc
Please restart your shell or `source ~/.cache/mill/download/mill-completion.sh` to enable completions</code></pre>
</div>
<p>Although the Shell syntax can be very finnicky, e.g. passing arrays to as
function arguments via <code>"${words[@]}"</code>, the actual underlying logic here isn’t
too complicated. <code>_complete_foo_bash</code> and <code>_complete_foo_zsh</code> take the
local variables from the shell, pass it to <code>_generate_foo_completions</code>
that uses them to return the possible completions, and passes the completions
back to the shell via <code>COMPREPLY</code> or <code>compadd</code>.</p>
<p>You can try this out live by pasting it into your Bash or Zsh shell and
typing <code>foo &lt;TAB&gt;</code> or <code>foo a&lt;TAB&gt;</code>. Note that you don’t
actually need a <code>foo</code> command installed:</p>
<div>
<pre><code data-lang="console">$ foo &lt;TAB&gt;
apple    apricot  banana   cherry   durian

$ foo a&lt;TAB&gt;
apple    apricot</code></pre>
</div>
<p>That’s all you need to get a basic tab-completer working. In real usage"</p>
<div>
<ul>
<li>
<p><code>foo</code> would be the name of the command the user would invoke your CLI program with
(e.g. <code>mill</code>)</p>
</li>
<li>
<p><code>_generate_foo_completions</code> would be your bespoke logic
to print out a line-separated list of completions. This could be a hard-coded list
for programs that change infrequently, or it could actually invoke your binary and
ask it what completions are available for the given input (what <code>mill</code> does).</p>
</li>
<li>
<p>While this example only looks up <code>words[idx]</code> to try and find a prefix
match for the current word, the completer is allowed to use the entirety of <code>words</code>
to decide what completions to offer, e.g. based on what flags or command-names are present in that array</p>
</li>
</ul>
</div>
<p>Note that when you register completion hooks for <code>foo</code> in Bash and Zsh, they apply
to commands like <code>./foo</code> as well. This is handy for programs like Mill, Maven, or Gradle
which typically use a <code>./mill</code> <a href="https://mill-build.org/mill/cli/installation-ide.html#_bootstrap_scripts" class="page">Bootstrap Script</a>
to run:</p>
<div>
<pre><code data-lang="console">$ ./foo a&lt;TAB&gt;
apple    apricot</code></pre>
</div>
</div>
</div>
<div>
<h2 id="_zsh_completion_descriptions"><a href="#_zsh_completion_descriptions"></a>Zsh Completion Descriptions</h2>
<div>
<p>The completions above work and provide a basic level of assistance for users of your CLI, but
it would be nice for users if they could also see a description of each command they could
complete in the terminal, as is done in the Mill build tool:</p>
<div>
<p><img src="https://mill-build.org/blog/_images/CompletionDescriptions.png" alt="CompletionDescriptions">
</p>
</div>
<p>To do this, we can make <code>_generate_foo_completions</code> generate an array of
longer strings containing both the completion and a description. Bash does not support
completion descriptions by default so we trim off the description,
but in Zsh we pass both the <code>trimmed</code> completion-words as well as the <code>raw</code> words and
descriptions to <code>compadd -d raw — $trimmed</code> as two parallel arrays.</p>
<div>
<pre><code data-lang="bash">_generate_foo_completions() {
  local idx=$1; shift
  local words=( "$@" )
  local current_word=${words[idx]}

  local array=(
    "apple: a common fruit"
    "apricot: sour fruit with a large stone"
    "banana: starchy and high in potassium"
    "cherry: small and sweet with a large pit"
    "durian: stinky spiky fruit"
  )
  for elem in "${array[@]}"; do
    if [[ $elem == "$current_word"* ]]; then echo "$elem"; fi
  done
}

_complete_foo_bash() {
  local IFS=$'\n'
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  local trimmed=()
  for d in "${raw[@]}"; do trimmed+=( "${d%%:*}" ); done

  COMPREPLY=( "${trimmed[@]}" )
}

_complete_foo_zsh() {
  local -a raw trimmed
  local IFS=$'\n'
  raw=($(_generate_foo_completions "$CURRENT" "${words[@]}"))

  for d in $raw; do trimmed+=( "${d%%:*}" ); done
  compadd -d raw -- $trimmed
}

if [ -n "${ZSH_VERSION:-}" ]; then
  autoload -Uz compinit
  compinit
  compdef _complete_foo_zsh foo
elif [ -n "${BASH_VERSION:-}" ]; then
  complete -F _complete_foo_bash foo
fi</code></pre>
</div>
<p>Zsh would then display the <code>raw</code> lines including both the completion-word as well
as the descriptions when displaying the completion options, but use the <code>trimmed</code>
lines which only contain the completion-words when completing the line</p>
<div>
<pre><code data-lang="console">$ foo a&lt;TAB&gt;
$ foo ap

$ foo ap&lt;TAB&gt;
apple: a common fruit                          apricot: sour fruit with a large stone

$ foo app&lt;TAB&gt;
$ foo apple</code></pre>
</div>
<p>However in this scenario the descriptions are entirely ignored by Bash. Because Bash
does not have a concept of tab-complete descriptions, in Bash we only pass the <code>trimmed</code>
word-completions to <code>COMPREPLY</code> and discard the <code>raw</code> lines containing the descriptions.</p>
</div>
</div>
<div>
<h2 id="_hacking_bash_completion_descriptions"><a href="#_hacking_bash_completion_descriptions"></a>Hacking Bash Completion Descriptions</h2>
<div>
<p>To make Bash show completion "descriptions", we can take advantage of the fact
that the completions are generated dynamically every time we call
<code>_generate_foo_completions</code>, and Bash and Zsh only inserts text
that is a common prefix to all completion options</p>

<p>Therefore, if we have multiple differing word-completions, we can actually append
whatever we want to the right of those words in <code>_generate_foo_completions</code>!
This "appended text" will be shown to users if there are multiple completions
available, but since the word-completions differ, Bash will never insert the entire word,
and thus never insert the appended text either.</p>
<p>The code below implements this: if there is only one completion we trim off the description
following the <code>:</code> off as normal, but if there’s more than one completion we leave the
description intact for the user to see</p>
<div>
<pre><code data-lang="bash">_complete_foo_bash() {
  local IFS=$'\n'
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  local trimmed=()
  if (( ${#raw[@]} == 1 )); then
    trimmed=( "${raw[0]%%:*}" )
  else
    trimmed=( "${raw[@]}" )
  fi

  COMPREPLY=( "${trimmed[@]}" )
}</code></pre>
</div>
<p>Now when I use autocomplete in Bash, I can see the descriptions for each item, but when
the tab-completion actually completes the token it only completes the word itself and
does not include the description!</p>
<div>
<pre><code data-lang="console">$ foo &lt;TAB&gt;
apple: a common fruit                     cherry: small and sweet with a large pit
apricot: sour fruit with a large stone    durian: stinky spiky fruit
banana: starchy and high in potassium

$ foo a&lt;TAB&gt;
$ foo ap

$ foo ap&lt;TAB&gt;
apple: a common fruit                   apricot: sour fruit with a large stone


$ foo app&lt;TAB&gt;
$ foo apple</code></pre>
</div>
<p>In this section, we only needed to make changes to the <code>_complete_foo_bash</code> function,
as the Zsh completion logic in <code>_complete_foo_zsh</code> is completely unchanged.</p>
</div>
</div>
<div>
<h2 id="_showing_single_completion_descriptions"><a href="#_showing_single_completion_descriptions"></a>Showing Single-Completion Descriptions</h2>
<div>
<p>The last quality of life feature we will add is the ability to show completion
descriptions when tabbing on a complete word:</p>

<p>For example, the Mill build tool does this so if you’re not sure what a flag or command
does, you can press <code>&lt;TAB&gt;</code> on it to see more details:</p>
<div>
<p><img src="https://mill-build.org/blog/_images/CompletionSingleDescription.png" alt="CompletionSingleDescription">
</p>
</div>
<p>Tab-completion is a common way to explore unfamiliar APIs, and just because someone
finished writing a flat or command doesn’t mean they aren’t curious about what
it does! But while Zsh tab-completion displays descriptions when multiple
options match the prefix, and we managed to hack Bash tab-completion to do the same
thing, neither displays any information if the word you are tab-completing is already
complete.</p>
<p>This behavior can be annoying, if the user wants to see the description, they will
need to first:</p>
<div>
<ul>
<li>
<p>Delete enough characters to make the token match multiple completions</p>
</li>
<li>
<p>Press <code>&lt;TAB&gt;</code></p>
</li>
<li>
<p>Visually scan the multiple completions printed to find the word description
they care about</p>
</li>
<li>
<p>Type back in all the missing characters so they can run the command</p>
</li>
</ul>
</div>
<p>To solve this, we can hack Bash and Zsh to print tab-completion descriptions even
if the token is already a complete word. We do this by checking if the token
is a complete word, and if so adding a second "dummy" completion: this makes
the tab-completion ambiguous, which cases Bash and Zsh to print out the completions
and descriptions for the user to see.</p>
<p>Doing this in <code>_complete_foo_bash</code> looks like the following:</p>
<div>
<pre><code data-lang="bash">_complete_foo_bash() {
  local IFS=$'\n'
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  local trimmed=()
  trimmed+=( "${raw[@]}" )

  if (( ${#raw[@]} == 1 )); then
    trimmed+=( "${raw[0]%%:*}" )
  fi

  COMPREPLY=( "${trimmed[@]}" )
}</code></pre>
</div>
<p>Instead of checking the length of <code>raw</code> to decide whether we add a trimmed
and non-trimmed lines to <code>trimmed</code>, we now instead <em>always</em> add the non-trimmed lines
that contain the completion descriptions, and in the case where there’s only
one line we then add an additional word-only completion with the description
trimmed off.</p>
<p>This means that all completions are ambiguous and will print the description -
even completions with a single real choice - but the additional trimmed line
when there is only 1 real choice ensures that the description text never gets
inserted into the user’s command</p>
<p>In Zsh, this can be similarly done via:</p>
<div>
<pre><code data-lang="bash">_complete_foo_zsh() {
  local -a raw trimmed
  local IFS=$'\n'
  raw=($(_generate_foo_completions "$CURRENT" "${words[@]}"))

  for d in $raw; do trimmed+=( "${d%%:*}" ); done
  if (( ${#raw} == 1 )); then
    trimmed+=( "${raw[1]}" )
    raw+=( "${trimmed[1]}" )
  fi

  compadd -d raw -- $trimmed
}</code></pre>
</div>
<p>The change here is similar to the Bash snippet above: when the number of completions is 1,
we add an additional completion to make it ambiguous so Zsh prints the description. But
because Zsh expects to pass two parallel arrays of descriptions and tokens to <code>compadd</code>,
our <code>if</code> block needs to append items to both <code>trimmed</code> and <code>raw</code>.</p>
<p>Using this, it now looks like</p>
<div>
<pre><code data-lang="console">$ foo apple&lt;TAB&gt;
apple                  apple: a common fruit</code></pre>
</div>
<p>Although the UI is not quite perfect - the word <code>apple</code> gets duplicated twice -
this nevertheless achieves the original goal of letting users <code>&lt;TAB&gt;</code> on an
already-completed flag or command to see the description or documentation for that word.</p>
</div>
</div>
<div>
<h2 id="_conclusion"><a href="#_conclusion"></a>Conclusion</h2>
<div>
<p>At this point, our final code looks like this:</p>
<div>
<pre><code data-lang="bash">_generate_foo_completions() {
  local idx=$1; shift
  local words=( "$@" )
  local current_word=${words[idx]}

  local array=(
    "apple: a common fruit"
    "apricot: sour fruit with a large stone"
    "banana: starchy and high in potassium"
    "cherry: small and sweet with a large pit"
    "durian: stinky spiky fruit"
  )
  for elem in "${array[@]}"; do
    if [[ $elem == "$current_word"* ]]; then echo "$elem"; fi
  done
}

_complete_foo_bash() {
  local IFS=$'\n'
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  local trimmed=()
  trimmed+=( "${raw[@]}" )

  if (( ${#raw[@]} == 1 )); then
    trimmed+=( "${raw[0]%%:*}" )
  fi

  COMPREPLY=( "${trimmed[@]}" )
}

_complete_foo_zsh() {
  local -a raw trimmed
  local IFS=$'\n'
  raw=($(_generate_foo_completions "$CURRENT" "${words[@]}"))

  for d in $raw; do trimmed+=( "${d%%:*}" ); done
  if (( ${#raw} == 1 )); then
    trimmed+=( "${raw[1]}" )
    raw+=( "${trimmed[1]}" )
  fi

  compadd -d raw -- $trimmed
}

if [ -n "${ZSH_VERSION:-}" ]; then
  autoload -Uz compinit
  compinit
  compdef _complete_foo_zsh foo
elif [ -n "${BASH_VERSION:-}" ]; then
  complete -F _complete_foo_bash foo
fi</code></pre>
</div>
<p>And can be used in both Bash or Zsh to provide an identical user experience:</p>
<div>
<ul>
<li>
<p>Showing possible tab-completions when there are multiple available</p>
</li>
<li>
<p>Showing command or flag descriptions (even though this is not natively supported by Bash)</p>
</li>
<li>
<p>Performing partial or entire-word completions</p>
</li>
<li>
<p>Showing the description or documentation when <code>&lt;TAB&gt;</code>ing on an already-completed word</p>
</li>
</ul>
</div>
<div>
<pre><code data-lang="console">$ foo &lt;TAB&gt;
apple: a common fruit                     banana: starchy and high in potassium     durian: stinky spiky fruit
apricot: sour fruit with a large stone    cherry: small and sweet with a large pit

$ foo a&lt;TAB&gt;
$ foo ap

$ foo ap&lt;TAB&gt;
apple: a common fruit                   apricot: sour fruit with a large stone

$ foo app&lt;TAB&gt;
$ foo apple

$ foo apple&lt;TAB&gt;
apple                  apple: a common fruit</code></pre>
</div>
<p>The actual docs for each shell’s tab-completion system contains a lot more detail (e.g.
<a href="https://zsh.sourceforge.io/Doc/Release/Completion-System.html">72 pages</a> for Zsh!), and
there are definitely many different ways you can set up your tab-completion scripts.
This blog post just aims to provide the simplest working example that works in both
Bash and Zsh, so hopefully you can understand it well enough to integrate into
your own projects.</p>
</div>
</div>

</article>
  </div></div>]]></description>
        </item>
    </channel>
</rss>