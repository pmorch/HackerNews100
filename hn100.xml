<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 31 Dec 2024 15:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Darktable 5.0.0 (108 pts)]]></title>
            <link>https://www.darktable.org/2024/12/darktable-5.0.0-released/</link>
            <guid>42558037</guid>
            <pubDate>Tue, 31 Dec 2024 11:19:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.darktable.org/2024/12/darktable-5.0.0-released/">https://www.darktable.org/2024/12/darktable-5.0.0-released/</a>, See on <a href="https://news.ycombinator.com/item?id=42558037">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>We’re proud to announce the new feature release of darktable, 5.0.0!</p>
<p>The github release is here: <a href="https://github.com/darktable-org/darktable/releases/tag/release-5.0.0">https://github.com/darktable-org/darktable/releases/tag/release-5.0.0</a>.</p>
<p>To build from source, do not use the autogenerated tarball provided by GitHub on the release page, download our tar.xz file instead. If you’re just building for yourself without creating a package for some distribution, then using source code cloning in git is an even more convenient way.</p>
<p>The checksums are:</p>
<pre tabindex="0"><code>$ sha256sum darktable-5.0.0.tar.xz
eaa136e6e624bb53127282e26aafa0441abcc189b55371465e1f5a8a493fa3a1  darktable-5.0.0.tar.xz

$ sha256sum darktable-5.0.0-x86_64.dmg
3f49cfb63958269b99065cf6b501678d4e63f2457ee1915bcd7ffa0dfef9dcfd  darktable-5.0.0-x86_64.dmg

$ sha256sum darktable-5.0.0-arm64.dmg
14feb35ef2b2e8e50cf1855826ad4913e905a5600a56a87dd98382e8d828e9db  darktable-5.0.0-arm64.dmg

$ sha256sum darktable-5.0.0-arm64-13.5.dmg
b43011cae5ddc9f19a8f895ba389e9ddb79d01534e9ca0568b7125026ac72145  darktable-5.0.0-arm64-13.5.dmg

$ sha256sum darktable-5.0.0-win64.exe
40444d5c7d310b1e1e859bd6b7c5d5e35d538a7bf9ad3e918b0e883c971451ea  darktable-5.0.0-win64.exe

$ sha256sum Darktable-5.0.0-x86_64.AppImage
d0061ac5a345c473d98f04388197afaee48e61b638db576ae1c88700cb8855cd  Darktable-5.0.0-x86_64.AppImage
</code></pre><p>When updating from the stable 4.8 series, please bear in mind that your edits will be preserved during this process, but the new library and configuration will no longer be usable with 4.8.</p>
<p>You are strongly advised to take a backup first.</p>
<h4 id="important-note-to-make-sure-that-darktable-can-keep-on-supporting-the-raw-file-format-for-your-camera-please-read-this-posthttpsdiscusspixlsustraw-samples-wanted5420ulebedevri-on-howwhat-raw-samples-you-can-contribute-to-ensure-that-we-have-the-full-raw-sample-set-for-your-camera-under-cc0-license">Important note: to make sure that darktable can keep on supporting the raw file format for your camera, <em>please</em> read <a href="https://discuss.pixls.us/t/raw-samples-wanted/5420?u=lebedevri">this post</a> on how/what raw samples you can contribute to ensure that we have the <em>full</em> raw sample set for your camera under CC0 license!</h4>
<p>Since darktable 4.8:</p>
<ul>
<li>1198 commits to darktable+rawspeed</li>
<li>505 pull requests handled</li>
<li>45 issues closed</li>
</ul>
<p><em>Please note that the darktable documentation is not currently complete for release 5.0
and contributions are greatly appreciated. Please see the
<a href="https://github.com/darktable-org/dtdocs#contributing">project documentation</a>
for more information on how to contribute.</em></p>
<h2 id="the-big-ones">The Big Ones</h2>
<p>The following is a summary of the main features added to darktable
5.0. Please see the user manual for more details of the individual
changes (where available).</p>
<ul>
<li>This development cycle has included a large number of changes which
improve the user experience, as detailed in the next section.</li>
</ul>
<h2 id="uiux-improvements">UI/UX Improvements</h2>
<ul>
<li>
<p>Added camera-specific styles for more than 500 camera models to more
closely approximate the out-of-camera JPEG rendition.  These styles
only affect contrast, brightness, and saturation and do not attempt
to match sharpening, denoising, or hue shifts.  Also added a Lua
script to auto-apply the appropriate style on import and manually
apply styles to a collection of previously-imported images.</p>
</li>
<li>
<p>Added an optional splash screen showing startup progress (including
estimated time remaining during the scan for updated sidecar files) to
dramatically reduce the time between invoking darktable and something
appearing on screen when the user has a large library.</p>
</li>
<li>
<p>The user interface now gives feedback while processing bulk image
operations such as rating, tagging, applying styles, and edit
history management (and undoing those operations), rather than
silently freezing until the operation completes.  While the
operation is in progress, darktable will now show either a busy
cursor (such as a stopwatch or spinner) or a progress bar with
option to cancel the remainder of the operation.</p>
</li>
<li>
<p>Paths for drawn masks now display two Bézier handles per control point,
which can be moved individually. This allows for more precise control
of the paths.</p>
</li>
<li>
<p>Added a high-contrast theme with bright white text on a dark gray
background.</p>
</li>
<li>
<p>Enhanced tooltips for utility module headers to provide more
information about the module.</p>
</li>
<li>
<p>Added more new-user hints on an empty lighttable.</p>
</li>
<li>
<p>Added two new error placeholder images to distinguish between
missing, unsupported, and corrupted images.  When attempting to edit
such an image, an appropriate, more specific error message is
displayed.</p>
</li>
<li>
<p>When selecting a style in the export module, hovering on the style
name in the popup menu displays a thumbnail previewing the effect of
appending the style to the active image’s edit (first selected image
in lighttable, center-view image in darkroom).</p>
</li>
<li>
<p>Allow for selecting the utility modules to be displayed on the
panels in the different views.</p>
<ul>
<li>
<p>Right-click on the empty panel area below the modules to get a
menu where they can be hidden or shown. This allows additional
modules to be added to the darkroom, like metadata editor and
styles.</p>
</li>
<li>
<p>This replaces the options in the “collections” and “recently used
collections” modules’ preferences to show or hide the latter and
show a “history” button in the former instead. Users that want the
separate module will need to reenable it once via the new
<kbd>Right-click</kbd> menu.</p>
</li>
<li>
<p>The menu also contains an option “restore defaults” that resets
the selection and position of modules in the current view. In the
preferences dialog, on the general tab, there’s a “reset view
panels” button that resets all views, including visibility and
width of the panels themselves.</p>
</li>
</ul>
</li>
<li>
<p>Added a global preference to swap the left and right side panels in
the darkroom view.</p>
</li>
<li>
<p>The first time a new user presses Tab, they will be warned that this
will hide all panels and how to get them back. Hopefully this
prevents some confusion or frustration.</p>
</li>
<li>
<p>Drag&amp;drop utility module headers to reposition them across the left
and right panels (lighttable) as well as vertically (all
views). Each view can have a different layout.</p>
</li>
<li>
<p>Drag&amp;drop of processing modules in the darkroom right panel has been
improved to auto-scroll when reaching the top or bottom and to not
get confused when images get dragged into the area. This functionality
no longer requires <kbd>Ctrl+Shift</kbd> modifiers.</p>
</li>
<li>
<p>Improved the message displayed at startup when the database is
locked by another instance of darktable.</p>
</li>
<li>
<p>Replaced the icon of the operator button in the color label filter
for working with multiple selected color labels
(union/intersection).</p>
</li>
</ul>
<h2 id="performance-improvements">Performance Improvements</h2>
<ul>
<li>
<p>Added OpenCL implementation of color equalizer.</p>
</li>
<li>
<p>Improved the speed of bulk image operations by improving the speed
of sidecar writes, and by moving sidecar updates for many operations
into a background task, allowing the user to proceed before the
writes complete.</p>
</li>
<li>
<p>Significantly accelerated loading of PFM files due to loops
parallelization and optimization that eliminated additional
processing.</p>
</li>
</ul>
<h2 id="other-changes">Other Changes</h2>
<ul>
<li>
<p>Switched default scope for new installations from histogram to
waveform to display more detailed information about image color and
tonality.</p>
</li>
<li>
<p>The ISO 12646 color assessment condition is kept until unset by user
action.</p>
</li>
<li>
<p>Exposure bias can now be used to form collections and as a display filter.</p>
</li>
<li>
<p>Improved visualization of the color equalizer’s effect.</p>
</li>
<li>
<p>Improved debugging support for verifying CPU vs. GPU results.</p>
</li>
<li>
<p>Add Calibrite alias for X-Rite ColorChecker in color calibration.</p>
</li>
<li>
<p>The scan for updated sidecar files now ignores timestamp differences
of two seconds or less.</p>
</li>
<li>
<p>The macOS installation package now has a background image to direct
the user on installing darktable.app.</p>
</li>
<li>
<p>Changed the user interface of the import dialog to make it easier to
delete custom places.</p>
</li>
<li>
<p>Numerous rounds of code cleanup.</p>
</li>
<li>
<p>The copy-parts dialog does not select any module by default now.</p>
</li>
<li>
<p>Add support for undo/redo for actions done on the filmstrip while in
darkroom.</p>
</li>
<li>
<p>In darkroom, add action (binding to <kbd>Ctrl+x</kbd> by default) for
synchronizing the last edited module on current edited module to the
selection.</p>
</li>
<li>
<p>Adjusted the internal AVIF encoder parameter to significantly boost
encoding speed without compromising the output quality.</p>
</li>
<li>
<p>Tag names can now easily be copied to the clipboard via popup
context menu in the tagging module.</p>
</li>
<li>
<p>The Piwigo export storage now supports to specify a file name
pattern for the exported file.</p>
</li>
<li>
<p>The directory where darktable will write the log file under Windows
has been changed to %USERPROFILE%\Documents\Darktable. This allows
the user to easily see where the log file is located without even
having to search for it in the documentation or FAQ. The previous
location was deep in the system subdirectories of the user profile,
and also under a hidden directory (so it was impossible to click to
it in File Explorer with default system settings).</p>
</li>
<li>
<p>Allow import of JPEG 2000 files with .jpf and .jpx file extensions.</p>
</li>
<li>
<p>Add a visible indicator to the color calibration module when its
color mapping section has non-neutral settings which will affect
color rendition.</p>
</li>
<li>
<p>Added new substitution variables <code>$(IMAGE.TAGS.HIERARCHY)</code> to insert
tags with full hierarchy and <code>$(IMAGE.ID.NEXT)</code> to insert the image ID
to be assigned to the image being imported, allowing the image ID to
be part of the filename generated during a copy&amp;import operation.</p>
</li>
<li>
<p>Exporting to floating-point JPEG XL with a quality of 100 will try
to do it as losslessly as possible. That is now consistent with the
behavior of integral JPEG XL formats.</p>
</li>
<li>
<p>Improved visibility of shortcuts that can be changed by users by
using bold text.</p>
</li>
<li>
<p>The histogram-exposure interface now supports all standard bauhaus
features (<kbd>Ctrl+click</kbd>, <kbd>Right-click</kbd>…).</p>
</li>
<li>
<p>Introduce image module order v5.0 to have the final-scale done before
color-out to fix some issues with color difference between darkroom
view and exported files.</p>
</li>
<li>
<p>Add support for editing any live color-picker samples. Using
<kbd>Right-click</kbd> on a sample it is possible to edit it
(changing location and/or size of the box) and either add a new
sample based on the edit or store the edit into an existing live
sample.</p>
</li>
<li>
<p>Added more substitution variables for using EXIF data fields,
enabled autocompletion of variables in the watermark module.</p>
<p>The new variables are <code>$(EXIF.FLASH)</code>, <code>$(EXIF.METERING)</code>,
<code>$(EXIF.EXPOSURE.PROGRAM)</code>, <code>$(EXIF.WHITEBALANCE)</code> and
<code>$(GPS.LOCATION.ICON)</code>.</p>
</li>
<li>
<p>Increase maximum focal length for filtering auto-applied presets to
2000mm.</p>
</li>
<li>
<p>Added an expanded color-checker preset to the Color Look Up Table
module with seven-level red/green/blue/gray ramps, IT8/CC24-like
skin tones, and miscellaneous color patches for more targeted color
adjustments across the full spectrum.</p>
</li>
<li>
<p>Added support for EXIF tags ‘AnalogBalance’ used for color
calibration and ‘LinearResponseLimit’ used in highlights
reconstruction.</p>
</li>
<li>
<p>If we find currently unsupported color calibration data in DNG
specific tags, we tag the image by darktable|issue|no-samples for
better support.</p>
</li>
<li>
<p>Added read support for HEIF files with AVC (H.264) compression and
.avci file extension.</p>
</li>
<li>
<p>Added read support for JPEG 2000 encoded images in HEIF containers
with .hej2 file extension.</p>
</li>
</ul>
<h2 id="bug-fixes">Bug Fixes</h2>
<ul>
<li>
<p>Fixed a performance regression for redrawing mipmaps.</p>
</li>
<li>
<p>Fixed handling of old (2020) edits using Filmic RGB.</p>
</li>
<li>
<p>Various OpenCL fixes to reduce differences between CPU and GPU
processing: colorspace conversion, saturation gradient filter in
color equalizer.</p>
</li>
<li>
<p>Fixed gallery export not working on Windows.</p>
</li>
<li>
<p>Fixed printer discovery in the print module, which could cause
available printers to be missed.</p>
</li>
<li>
<p>Work around out-of-spec EXIF date field caused by buggy software.</p>
</li>
<li>
<p>Fixed reading embedded color profiles from PNG images.</p>
</li>
<li>
<p>Fixed certain boundary cases in the crop module.</p>
</li>
<li>
<p>Fixed crash when loading corrupted .gpx file in the geotagging module</p>
</li>
<li>
<p>Fix preset handling in the export module not saving all parameters.</p>
</li>
<li>
<p>Fix an issue in FilmicRGB where one of the parameter could be above
the maximum allowed range making the validation failing and the
whole set of parameters reset to default.</p>
</li>
<li>
<p>Fix overlay recording to work in all cases (discarding history or
copy/paste history for example) ensuring that an image not
referenced anymore as overlay in a composite module can be removed.</p>
</li>
<li>
<p>Properly reset darktable internal tag darktable|style|<name> and
darktable|changed when resetting history.</name></p>
</li>
<li>
<p>Fixed crash in the Piwigo export storage when not logged in to the
Piwigo server.</p>
</li>
<li>
<p>Fixed a bug in the export module where it was impossible to export a
file again if “on conflict: overwrite if changed” was selected.</p>
</li>
<li>
<p>Fixed a bug where double clicking on a label in darkroom modules
does not reset the control.</p>
</li>
<li>
<p>The composite module now prevents assigning an overlay that would
lead to a loop. Previously, only direct references
(image #1 &lt;-&gt; image #2) were checked; this has now been extended
to also cover chains (image #1 -&gt; image #2 -&gt; image #3 -&gt; image #1)
of arbitrary length.</p>
</li>
<li>
<p>Fix a bug in overlay module which incorrectly apply a color profile
and so creating an unwanted and wrong color cast. This bug was a
regression added just before the 4.8 release.</p>
</li>
<li>
<p>Fixed a bug in color calibration module where switching between
various illuminants could lead to unpredictable settings.</p>
</li>
<li>
<p>Various fixes In the demosaic module. Non-usable options are hidden
now. Fixed dual demosaicing for xtrans sensors and OpenCL code.</p>
</li>
<li>
<p>Fixed a bug in the history module where style creation fails if a
style with that name already exists.</p>
</li>
<li>
<p>Fixed guides drawing in case a module is expanded and active.</p>
</li>
<li>
<p>Ensure that the list of images in the culling view remains up to
date when hidden.</p>
</li>
<li>
<p>Fixed minor glitches in color calibration module.</p>
</li>
<li>
<p>Fixed issues with wrong corrections in highlight opposed OpenCL
code.</p>
</li>
<li>
<p>Fixed surface blur radius calculation possibly resulting in garbled
output.</p>
</li>
</ul>
<h2 id="lua">Lua</h2>
<h3 id="api-version">API Version</h3>
<ul>
<li>API version is now 9.4.0</li>
</ul>
<h3 id="new-features">New Features</h3>
<ul>
<li>
<p>Added new event, inter-script-communication, to permit sending messages
from one running script to another running script.</p>
</li>
<li>
<p>Added new function darktable.util.message(), for sending messages using
the inter-script-communication event.</p>
</li>
<li>
<p>Added new EXIF data fields to dt_lua_image_t:</p>
<ul>
<li>
<p>exif_whitebalance</p>
</li>
<li>
<p>exif_flash</p>
</li>
<li>
<p>exif_exposure_program</p>
</li>
<li>
<p>exif_metering_mode</p>
</li>
</ul>
</li>
<li>
<p>Added new event, image-group-information-changed, that is raised any time
an images group information changes.</p>
</li>
</ul>
<h3 id="bug-fixes-1">Bug Fixes</h3>
<ul>
<li>Fixed a bug with dt_imageio_module_format_t.write_image so it returns
true on success and false on failure.</li>
</ul>
<h3 id="add-action-support-for-lua">Add action support for Lua</h3>
<h3 id="other-lua-changes">Other Lua changes</h3>
<ul>
<li>Lua scripts are now better integrated into Darktable and can be
fully translated. The design for the scripts manager has been
reworked to be more in line with the current Darktable GUI modules.</li>
</ul>
<h2 id="notes">Notes</h2>
<ul>
<li>
<p>When exporting to AVIF, EXR, JPEG XL, or XCF, selecting specific
metadata (e.g. geo-tag or creator) is not currently possible. For
AVIF, EXR, JPEG XL, and XCF formats, darktable will not include any
metadata fields unless the user selects all of the checkboxes in the
export module’s preference options.</p>
</li>
<li>
<p>Since 4.8 release the support for macOS versions older than 13.5 has
been dropped.</p>
</li>
</ul>
<h2 id="changed-dependencies">Changed Dependencies</h2>
<h3 id="mandatory">Mandatory</h3>
<ul>
<li>Bump SQLite requirement to 3.26</li>
</ul>
<h3 id="optional">Optional</h3>
<ul>
<li>n/a</li>
</ul>
<h2 id="rawspeed-changes">RawSpeed changes</h2>
<ul>
<li>Fujifilm GFX cameras now use the vendor supplied crop</li>
</ul>
<h2 id="camera-support-compared-to-48">Camera support, compared to 4.8</h2>
<h3 id="base-support">Base Support</h3>
<ul>
<li>Fujifilm X-M5 (compressed)</li>
<li>Fujifilm X-T50 (compressed)</li>
<li>Leica D-Lux 8 (DNG)</li>
<li>Leica M11-D (DNG)</li>
<li>Leica Q3 43 (DNG)</li>
<li>Minolta Alpha Sweet Digital</li>
<li>Minolta Alpha-7 Digital</li>
<li>Nikon Z50_2 (14bit-compressed)</li>
<li>Nikon Z6_3 (14bit-compressed)</li>
<li>Panasonic DC-FZ80D (4:3)</li>
<li>Panasonic DC-FZ82D (4:3)</li>
<li>Panasonic DC-FZ85 (4:3)</li>
<li>Panasonic DC-FZ85D (4:3)</li>
<li>Panasonic DC-G100D (4:3)</li>
<li>Phase One P20+</li>
<li>Sony ILCE-1M2</li>
</ul>
<h3 id="white-balance-presets">White Balance Presets</h3>
<ul>
<li>Nikon Z6_3</li>
<li>Sony ILCE-6700</li>
</ul>
<h3 id="noise-profiles">Noise Profiles</h3>
<ul>
<li>Canon PowerShot G1 X</li>
<li>Leica M11</li>
<li>Nikon Z6_3</li>
</ul>
<h3 id="missing-compression-mode-support">Missing Compression Mode Support</h3>
<ul>
<li>Apple ProRAW DNGs</li>
<li>CinemaDNG lossless (Blackmagic, some DJI, etc.) and lossy (Blackmagic)</li>
<li>DNG 1.7 using JPEG XL (Adobe enhanced, Samsung Expert RAW)</li>
<li>Fujifilm lossy RAFs</li>
<li>Nikon high efficiency NEFs</li>
<li>OM System 14-bit high resolution ORFs</li>
<li>Sony downsized lossless ARWs (“M” for full-frame, “S” for full-frame &amp; APS-C)</li>
</ul>
<h3 id="suspended-support">Suspended Support</h3>
<p>Support for the following cameras is suspended because no samples are available on <a href="https://raw.pixls.us/">https://raw.pixls.us</a>:</p>
<ul>
<li>Creo/Leaf Aptus 22(LF3779)/Hasselblad H1</li>
<li>Fujifilm IS-1</li>
<li>Kodak EasyShare Z980</li>
<li>Leaf Aptus-II 5(LI300059)/Mamiya 645 AFD</li>
<li>Leaf Credo 60</li>
<li>Leaf Credo 80</li>
<li>Minolta DiMAGE 5</li>
<li>Olympus SP320</li>
<li>Phase One IQ250</li>
<li>Sinar Hy6/ Sinarback eXact</li>
<li>ST Micro STV680</li>
</ul>
<h2 id="translations">Translations</h2>
<ul>
<li>Czech</li>
<li>German</li>
<li>European Spanish</li>
<li>Finnish</li>
<li>French</li>
<li>Japanese</li>
<li>Dutch</li>
<li>Brazilian Portuguese</li>
<li>Slovenian</li>
<li>Albanian</li>
<li>Ukrainian</li>
<li>Chinese (Simplified)</li>
<li>Chinese (Traditional)</li>
</ul>

          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Interesting Interview with DeepSeek's CEO (118 pts)]]></title>
            <link>https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas</link>
            <guid>42557586</guid>
            <pubDate>Tue, 31 Dec 2024 09:28:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas">https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas</a>, See on <a href="https://news.ycombinator.com/item?id=42557586">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Deepseek is a Chinese AI startup whose latest R1 model </span><strong><a href="https://api-docs.deepseek.com/news/news1120" rel="">beat OpenAI’s o1</a><span> on multiple reasoning benchmarks</span></strong><span>. Despite its low profile, Deepseek is the Chinese AI lab to watch.</span></p><p><span>Before Deepseek, CEO Liang Wenfeng’s main venture was High-Flyer (幻方), a top 4 Chinese quantitative hedge fund last valued at $8 billion. Deepseek is fully funded by High-Flyer and has no plans to fundraise. It focuses on building foundational technology rather than commercial applications and has committed to open sourcing all of its models. It has also singlehandedly kicked off price wars in China by charging very affordable API rates. Despite this, Deepseek can afford to stay in the scaling game: with access to High-Flyer’s compute clusters, Dylan Patel’s </span><a href="https://x.com/dylan522p/status/1859302712803807696" rel="">best guess</a><span> is they have upwards of “50k Hopper GPUs,” orders of magnitude more compute power than the 10k A100s they cop to publicly.</span></p><p>Deepseek’s strategy is grounded in their ambition to build AGI. Unlike previous spins on the theme, Deepseek’s mission statement does not mention safety, competition, or stakes for humanity, but only “unraveling the mystery of AGI with curiosity”. Accordingly, the lab has been laser-focused on research into potentially game-changing architectural and algorithmic innovations.</p><p><span>Deepseek has delivered a series of impressive technical breakthroughs. Before R1-Lite-Preview, there had been a longer track record of wins: </span><a href="https://arxiv.org/abs/2401.06066" rel="">architectural improvements</a><span> like multi-head latent attention (MLA) and sparse mixture-of-experts (DeepseekMoE) had reduced inference costs so much as to trigger a price war among Chinese developers. Meanwhile, Deepseek’s </span><a href="https://arxiv.org/abs/2406.11931" rel="">coding model</a><span> trained on these architectures outperformed open weights rivals like July’s GPT4-Turbo.</span></p><p>As a first step to understanding what’s in the water at Deepseek, we’ve translated a rare, in-depth interview with CEO Liang Wenfeng, originally published this past July on a 36Kr sub-brand. It contains some deep insights into:</p><ul><li><p>How DeepSeek’s ambitions for AGI flow through their research strategy</p></li><li><p>Why it views open source as the dominant strategy and why it ignited a price war</p></li><li><p>How he hires and organizes researchers to leverage young domestic talent far better than other labs that have splurged on returnees</p></li><li><p>Why Chinese firms settle for copying and commercialization instead of “hardcore innovation” and how Liang hopes Deepseek will ignite more “hardcore innovation” across the Chinese economy.  </p></li></ul><p><a href="https://mp.weixin.qq.com/s/r9zZaEgqAa_lml_fOEZmjg" rel="">Wechat</a><span>, </span><a href="https://archive.is/JnE4j" rel="">Archive link</a><span>. Text | Lily Yu 于丽丽. Editor | Liu Jing 刘旌.</span></p><p>Of China’s seven large-model startups, DeepSeek has been the most discreet — yet it consistently manages to be memorable in unexpected ways.</p><p>A year ago, this unexpectedness came from its backing by High-Flyer 幻方, a quantitative hedge fund powerhouse, making it the only non-big tech giant with a reserve of 10,000 A100 chips. A year later, it became known as the catalyst for China’s AI model price war. A year later, it became known as the catalyst for China’s AI model price war.</p><p>In May, amid continuous AI developments, DeepSeek suddenly rose to prominence. The reason was that they released an open-source model called DeepSeek V2, which offered an unprecedented price/performance ratio: inference costs were reduced to only 1 RMB per million tokens, which is about one-seventh of the cost of Llama3 70B and one-seventieth of the cost of GPT-4 Turbo.</p><p>DeepSeek was quickly dubbed the “Pinduoduo of AI,” and other major tech giants such as ByteDance, Tencent, Baidu, and Alibaba couldn’t hold back, cutting their prices one after another. A price war for large models in China was imminent.</p><p><strong>This diffuse smoke of war actually concealed one fact: unlike many big companies burning money on subsidies, DeepSeek is profitable.</strong></p><p><span>​​This success stems from DeepSeek’s comprehensive innovation in model architecture. They proposed a novel MLA (</span><strong>multi-head latent attention</strong><span>) architecture that reduces memory usage to 5-13% of the commonly used MHA architecture. Additionally, their original DeepSeekMoESparse structure minimized computational costs, ultimately leading to reduced overall costs.</span></p><p>In Silicon Valley, DeepSeek is known as “the mysterious force from the East” 来自东方的神秘力量. SemiAnalysis’s chief analyst believes the DeepSeek V2 paper “may be the best one of the year.” Former OpenAI employee Andrew Carr found the paper “full of amazing wisdom” 充满惊人智慧, and applied its training setup to his own models. And Jack Clark, former policy head at OpenAI and co-founder of Anthropic, believes DeepSeek “hired a group of unfathomable geniuses” 雇佣了一批高深莫测的奇才, adding that large models made in China “will be as much of a force to be reckoned with as drones and electric cars” 将和无人机、电动汽车一样，成为不容忽视的力量.</p><p><strong>In the AI ​​wave — where the story is largely driven by Silicon Valley — this is a rare occurrence. </strong><span>Several industry insiders told us that </span><strong>this strong response stems from innovation at the architectural level, a rare attempt by domestic large model companies and even global open-source large-scale models</strong><span>. One AI researcher said that the Attention architecture has hardly been successfully modified, let alone validated on a large scale, in the years since it was proposed. “It’s an idea that would be shut down at the decision-making stage because most people lack confidence” 这甚至是一个做决策时就会被掐断的念头，因为大部分人都缺乏信心.</span></p><p><span>On the other hand, large domestic models have rarely dabbled in innovation at the architectural level before, partly due to a prevailing belief that </span><strong>Americans excel at 0-to-1 technical innovation, while Chinese excel at 1-to-10 application innovation</strong><span>. Moreover, this kind of behavior is very unprofitable — after all, a new generation of models will inevitably emerge after a few months, so Chinese companies need only follow along and focus on downstream applications. Innovating the model architecture means that there is no path to follow, meaning multiple failures and substantial time and economic costs.</span></p><p>DeepSeek is clearly going against the grain. Amid the clamor that large-model technology is bound to converge and following is a smarter shortcut, DeepSeek values the learning accumulated through “detours” 弯路, and believes that Chinese large-model entrepreneurs can join the global technological innovation stream beyond just application innovation.</p><p>Many of DeepSeek’s choices differ from the norm. Until now, among the seven major Chinese large-model startups, it’s the only one that has given up the “want it all” 既要又要 approach, so far focusing on only research and technology, without the toC applications. It’s also the only one that hasn’t fully considered commercialization, firmly choosing the open-source route without even raising capital. While these choices often leave it in obscurity, DeepSeek frequently gains organic user promotion within the community.</p><p>How did DeepSeek achieve this all? We interviewed DeepSeek’s seldom-seen founder, Liang Wenfeng 梁文锋, to find out.</p><p>The post-80s founder, who has been working behind the scenes on technology since the High-Flyer era, continues his low-key style in the DeepSeek era — “reading papers, writing code, and participating in group discussions” 看论文，写代码，参与小组讨论 every day, just like every other researcher does.</p><p>And unlike many quant fund founders — who have overseas hedge-fund experience and physics or mathematics degrees — Liang Wenfeng has always maintained a local background: in his early years, he studied artificial intelligence at Zhejiang University’s Department of Electrical Engineering.</p><p>Multiple industry insiders and DeepSeek researchers told us that Liang Wenfeng is a very rare person in China’s AI industry — someone who has “both strong infra engineering and modeling capabilities, as well as the ability to mobilize resources” he “can make accurate, high-level judgments, while also remaining stronger than first-line researchers in the details”. He has a “terrifying ability to learn”, and at the same time, he is “not at all like a boss and much more like a geek.”</p><p><span>This is a particularly rare interview. Here, this technological idealist provides a voice that is especially scarce in China’s tech world: </span><strong>he is one of the few who puts “right and wrong” before “profits and losses”</strong><span> 把“是非观”置于“利害观”之前, </span><strong>who reminds us to see the inertia of the times, and who puts “original innovation”</strong><span> 原创式创新 </span><strong>at the top of the agenda</strong><span>.</span></p><p><span>A year ago, when DeepSeek first came off the market, we interviewed Liang Wenfeng: “</span><a href="https://mp.weixin.qq.com/s/fpnmf5W1rr6qTIQjbf9aCg" rel="">Crazy High-Flyer: A Stealth AI Giant’s Road to Large Models</a><span>” </span><a href="https://archive.is/OoId6" rel="">疯狂的幻方：一家隐形AI巨头的大模型之路</a><span>. If the phrase “be insanely ambitious and insanely sincere” 务必要疯狂地怀抱雄心，且还要疯狂地真诚 was merely a beautiful slogan back then, a year later, it has become action.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg" width="1266" height="705" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:705,&quot;width&quot;:1266,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Waves: After DeepSeek V2’s release, it quickly triggered a fierce price war in the large-model market. Some say you’ve become the industry’s catfish.</p><p><span>Liang Wenfeng: We didn’t mean to become a catfish — we just accidentally became a catfish. [</span><em>Translator’s note: This is likely a reference to Wong Kar-wai’s new tv show </em><span>王家卫</span><em> </em><span>“Blossoms Shanghai” 繁花</span><em>, where catfish are symbolic of market disruptors due to their cannibalistic nature.</em><span>]</span></p><p>Waves: Was this outcome a surprise to you?</p><p>Liang Wenfeng: Very surprising. We didn’t expect pricing to be so sensitive to everyone. We were just doing things at our own pace and then accounted for and set the price. Our principle is that we don’t subsidize nor make exorbitant profits. This price point gives us just a small profit margin above costs.</p><p>Waves: Zhipu AI 智谱AI followed suit five days later, followed by ByteDance, Alibaba, Baidu, Tencent, and other big players.</p><p>Liang Wenfeng: Zhipu AI reduced the price of an entry-level product, while their models comparable to ours remained expensive. ByteDance was truly the first to follow, reducing its flagship model to match our price, which then triggered other tech giants to cut prices. Since big companies’ model costs are much higher than ours, we never expected anyone would do this at a loss, but it eventually turned into the familiar subsidy-burning logic of the internet era.</p><p>Waves: From the outside, price cuts look a lot like bids for users, which is usually the case in internet-era price wars.</p><p>Liang Wenfeng: Poaching users is not our main purpose. We cut prices because, on the one hand, our costs decreased while exploring next-generation model architectures, and on the other hand, we also feel that both APIs and AI should be accessible and affordable to everyone.</p><p>Waves: Before this, most Chinese companies would directly copy the current generation’s Llama architecture for applications. Why did you start from the model structure?</p><p>Liang Wenfeng: If the goal is to make applications, using the Llama structure for quick product deployment is reasonable. But our destination is AGI, which means we need to study new model structures to realize stronger model capability with limited resources. This is one of the fundamental research areas needed for scaling up to larger models. And beyond model structure, we’ve done extensive research in other areas, including data construction and making models more human-like — which are all reflected in the models we released. In addition, Llama’s structure, in terms of training efficiency and inference cost, is estimated to have a two-generation gap behind international frontier levels in training efficiency and inference costs.</p><p>Waves: Where does this generation gap mainly come from?</p><p>Liang Wenfeng: First of all, there’s a training efficiency gap. We estimate that compared to the best international levels, China’s best capabilities might have a twofold gap in model structure and training dynamics — meaning we have to consume twice the computing power to achieve the same results. In addition, there may also be a twofold gap in data efficiency, that is, we have to consume twice the training data and computing power to achieve the same results. Combined, that’s four times more computing power needed. What we’re trying to do is to keep closing these gaps.</p><p>Waves: Most Chinese companies choose to have both models and applications. Why has DeepSeek chosen to focus on only research and exploration?</p><p>Liang Wenfeng: Because we believe the most important thing now is to participate in the global innovation wave. For many years, Chinese companies are used to others doing technological innovation, while we focused on application monetization — but this isn’t inevitable. In this wave, our starting point is not to take advantage of the opportunity to make a quick profit, but rather to reach the technical frontier and drive the development of the entire ecosystem.</p><p>Waves: The Internet and mobile Internet eras left most people with the belief that the United States excels at technological innovation, while China excels at making applications.</p><p><span>Liang Wenfeng: We believe that as the economy develops, </span><strong>China should gradually become a contributor instead of freeriding</strong><span>. In the past 30+ years of the IT wave, we basically didn’t participate in real technological innovation. </span><strong>We’re used to Moore’s Law falling out of the sky, lying at home waiting 18 months for better hardware and software to emerge. That’s how the Scaling Law is being treated</strong><span>.</span></p><p><strong>But in fact, this is something that has</strong><span> </span><strong>been created through the tireless efforts of generations of Western-led tech communities. It’s just because we weren’t previously involved in this process that we’ve ignored its existence.</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png" width="166" height="135.8849557522124" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b41161e2-7139-4c15-84ba-0fff53b18561_452x370.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:370,&quot;width&quot;:452,&quot;resizeWidth&quot;:166,&quot;bytes&quot;:99155,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Waves: Why did DeepSeek V2 surprise so many people in Silicon Valley?</p><p>Liang Wenfeng: Among the numerous innovations happening daily in the United States, this is quite ordinary. They were surprised because it was a Chinese company joining their game as an innovation contributor. After all, most Chinese companies are used to following, not innovating.</p><p>Waves: But choosing to innovate in the Chinese context is a very extravagant decision. Large models are a heavy investment game, and not all companies have the capital to solely research and innovate instead of thinking about commercialization first.</p><p><span>Liang Wenfeng: The cost of innovation is definitely not low, and past tendencies toward indiscriminate borrowing were also related to China’s previous conditions. But now you see, whether it’s China’s economic scale, or the profits of giants like ByteDance and Tencent — none of it is low by global standards. </span><strong>What we lack in innovation is definitely not capital, but a lack of confidence and knowledge of how to organize high-density talent for effective innovation.</strong></p><p>Waves: Why do Chinese companies — including the huge tech giants — default to rapid commercialization as their #1 priority?</p><p>Liang Wenfeng: In the past 30 years, we’ve emphasized only making money while neglecting innovation. Innovation isn’t entirely business-driven; it also requires curiosity and a desire to create. We’re just constrained by old habits, but this is tied to a particular economic phase.</p><p>Waves: But you’re ultimately a business organization, not a public-interest research institution — so where do you build your moat when you choose to innovate and then open source your innovations? Won’t the MLA architecture you released in May be quickly copied by others?</p><p><span>Liang Wenfeng: I</span><strong>n the face of disruptive technologies, moats created by closed source are temporary. Even OpenAI’s closed source approach can’t prevent others from catching up</strong><span>. S</span><strong>o we anchor our value in our team — our colleagues grow through this process, accumulate know-how, and form an organization and culture capable of innovation. That’s our moat.</strong></p><p>Open source, publishing papers, in fact, do not cost us anything. For technical talent, having others follow your innovation gives a great sense of accomplishment. In fact, open source is more of a cultural behavior than a commercial one, and contributing to it earns us respect. There is also a cultural attraction for a company to do this.</p><p>Waves: What do you think of those who believe in the market, like [GSR Ventures’[ Zhu Xiaohu 朱啸虎?</p><p>Liang Wenfeng: Zhu Xiaohu is logically consistent, but his style of play is more suitable for fast money-making companies. And if you look at America’s most profitable companies, they’re all high-tech companies that accumulated deep technical foundations before making major breakthroughs.</p><p>Waves: But when it comes to large models, pure technical leadership rarely forms an absolute advantage. What bigger thing are you betting on?</p><p><span>Liang Wenfeng: </span><strong>What we see is that Chinese AI can’t be in the position of following forever. We often say that there is a gap of one or two years between Chinese AI and the United States, but the real gap is the difference between originality and imitation. If this doesn’t change, China will always be only a follower — so some exploration is inescapable.</strong></p><p>Nvidia’s leadership isn’t just the effort of one company, but the result of the entire Western technical community and industry working together. They see the next generation of technology trends and have a roadmap in hand. Chinese AI development needs such an ecosystem. Many domestic chip developments struggle because they lack supporting technical communities and have only second-hand information. China inevitably needs people to stand at the technical frontier.</p><p>Waves: DeepSeek, right now, has a kind of idealistic aura reminiscent of the early days of OpenAI, and it’s open source. Will you change to closed source later on? Both OpenAI and Mistral moved from open-source to closed-source.</p><p>Liang Wenfeng: We will not change to closed source. We believe having a strong technical ecosystem first is more important.</p><p>Waves: Do you have a financing plan? I’ve seen media reports saying that High-Flyer plans to spin off DeepSeek for an IPO. AI startups in Silicon Valley inevitably end up binding themselves to major firms.</p><p><span>Liang Wenfeng: We do not have financing plans in the short term. </span><strong>Money has never been the problem for us; bans on shipments of advanced chips are the problem.</strong></p><p>Waves: Many people believe that developing AGI and quantitative finance are completely different endeavors. Quantitative finance can be pursued quietly, but AGI may require a high-profile and bold approach, forming alliances to amplify your investments.</p><p>Liang Wenfeng: More investments do not equal more innovation. Otherwise, big firms would’ve monopolized all innovation already.</p><p>Waves: Are you not focusing on applications right now because you lack the operational expertise?</p><p>Liang Wenfeng: We believe the current stage is a period of explosive growth in technological innovation, not in applications. In the long run, we hope to create an ecosystem where the industry directly utilizes our technology and outputs. Our focus will remain on foundational models and cutting-edge innovation, while other companies can build B2B and B2C businesses based on DeepSeek’s foundation. If a complete industry value chain can be established, there’s no need for us to develop applications ourselves. Of course, if needed, nothing stops us from working on applications, but research and technological innovation will always be our top priority.</p><p>Waves: But when customers are choosing APIs, why should they choose DeepSeek over offerings from bigger firms?</p><p>Liang Wenfeng: The future world is likely to be one of specialized division of labor. Foundational large models require continuous innovation, and large companies have limits on their capabilities, which may not necessarily make them the best fit.</p><p>Waves: But can technology itself really create a significant gap? You’ve also mentioned that there are no absolute technological secrets.</p><p>Liang Wenfeng: There are no secrets in technology, but replication requires time and cost. Nvidia’s graphics cards, theoretically, have no technological secrets and are easy to replicate. However, building a team from scratch and catching up with the next generation of technology takes time, so the actual moat remains quite wide.</p><p>Waves: Once DeepSeek lowered its prices, ByteDance followed suit, which shows that they feel a certain level of threat. How do you view new approaches to competition between startups and big firms?</p><p><strong>Liang Wenfeng: Honestly, we don’t really care, because it was just something we did along the way. Providing cloud services isn’t our main goal. Our ultimate goal is still to achieve AGI.</strong></p><p><span>Right now I don’t see any new approaches, but big firms do not have a clear upper hand. </span><strong>Big firms have existing customers, but their cash-flow businesses are also their burden, and this makes them vulnerable to disruption at any time.</strong></p><p>Waves: What do you see as the end game of the six other large-model startups?</p><p>Liang Wenfeng: Two or three may survive. All of them are in the “burning-money” phase right now, so those with a clear self-positioning and better refinement of operations have a higher chance of making it. Other companies might undergo significant transformations. Things of value won’t simply disappear but will instead take on a different form.</p><p>Waves: High-Flyer’s approach to competition has been described as “impervious,” as it pays little attention to horizontal competition. What’s your starting point when it comes to thinking about competition?</p><p>Liang Wenfeng: What I often think about is whether something can improve the efficiency of society’s operations, and whether you can find a point of strength within its industrial chain. As long as the ultimate goal is to make society more efficient, it’s valid. Many things in between are just temporary phases, and overly focusing on them can lead to confusion.</p><p><span>Waves: Jack Clark, former policy director at OpenAI and co-founder of Anthropic, said that DeepSeek hired </span><a href="https://importai.substack.com/p/import-ai-372-gibberish-jailbreak" rel="">“inscrutable wizards.”</a><span> What kind of people are behind DeepSeek V2?</span></p><p><strong>Liang Wenfeng: There are no wizards. We are mostly fresh graduates from top universities, PhD candidates in their fourth or fifth year, and some young people who graduated just a few years ago.</strong></p><p>Waves: Many LLM companies are obsessed with recruiting talents from overseas, and it’s often said that the top 50 talents in this field might not even be working for Chinese companies. Where are your team members from?</p><p><strong>Liang Wenfeng: The team behind the V2 model doesn’t include anyone returning to China from overseas — they are all local. The top 50 experts might not be in China, but perhaps we can train such talents ourselves.</strong></p><p><strong>Waves: How did this MLA innovation come about? I heard the idea originated from the personal interest of a young researcher?</strong></p><p><strong>Liang Wenfeng: After summarizing some mainstream evolutionary trends of the attention mechanism, he just thought to design an alternative. However, turning the idea into reality was a lengthy process. We formed a team specifically for this and spent months getting it to work. [</strong><em><span>Jordan: really reminiscent of how </span><a href="https://aibusiness.com/nlp/sxsw-23-openai-co-founder-shares-the-story-behind-chatgpt" rel="">Alec Radford’s early contribution to the GPT series</a><span> and speaks to the broader thesis we’ve argued in the past on ChinaTalk that algorithmic innovation is fundamentally different from pushing the technological frontier in something like semiconductor fabrication. Instead of needing a PhD and years of industry experience to really be useful, you can push the frontier by being a really sharp and hungry 20something (of which China has many!). Dwarkesh’s interview with OpenAI </span><a href="https://www.dwarkeshpatel.com/p/sholto-douglas-trenton-bricken" rel="">Sholto Douglass and Anthropic’s Trenton Bricken</a><span> illustrates this dynamic well. Dwarkesh opens with the ine “Noam Brown, who wrote the Diplomacy paper, said this about Sholto: “he's only been in the field for 1.5 years, but people in AI know that he was one of the most important people behind Gemini's success.”</span></em><strong>]</strong></p><p>Waves: The emergence of such divergent thinking seems closely related to your innovation-driven organizational structure. Back in the High-Flyer era, your team rarely assigned goals or tasks from the top down. But AGI involves frontier exploration with much uncertainty — has that led to more management intervention?</p><p><strong>Liang Wenfeng: DeepSeek is still entirely bottom-up. We generally don’t predefine roles; instead, the division of labor occurs naturally. Everyone has their own unique journey, and they bring ideas with them, so there’s no need to push anyone. While we explore, if someone sees a problem, they will naturally discuss it with someone else. However, if an idea shows potential, we do allocate resources top-down.</strong></p><p>Waves: I heard that DeepSeek is very flexible in mobilizing resources like GPUs and people.</p><p><strong>Liang Wenfeng: Anyone on the team can access GPUs or people at any time. If someone has an idea, they can access the training cluster cards anytime without approval. Similarly, since we don’t have hierarchies or separate departments, people can collaborate across teams, as long as there’s mutual interest.</strong></p><p>Waves: Such a loose management style relies on having highly self-driven people. I heard you excel at identifying exceptional talent through non-traditional evaluation criteria.</p><p><span>Liang Wenfeng: </span><strong>Our hiring standard has always been passion and curiosity. Many of our team members have unusual experiences, and that is very interesting. Their desire to do research often comes before making money.</strong></p><p>Waves: Transformers was born at Google’s AI Lab, and ChatGPT at OpenAI. How do you compare the value of innovations at big companies’ AI labs versus startups?</p><p>Liang Wenfeng: Google’s AI Lab, OpenAI, and even Chinese tech companies’ AI labs are all immensely valuable. The fact that OpenAI succeeded was partly due to a few historical coincidences.</p><p>Waves: So, is innovation largely a matter of luck? I noticed that the middle row of meeting rooms in your office has doors on both sides that anyone can open. Your colleagues said that this design leaves room for serendipity. The creation of transformers involved someone overhearing a discussion and joining, ultimately turning it into a general framework.</p><p><span>Liang Wenfeng: I believe innovation starts with believing. </span><strong>Why is Silicon Valley so innovative? Because they dare to do things. When ChatGPT came out, the tech community in China lacked confidence in frontier innovation. From investors to big tech, they all thought that the gap was too big and opted to focus on applications instead. But innovation starts with confidence, which we often see more from young people.</strong></p><p>Waves: But you don’t fundraise or even speak to the public, so your visibility is lower than those companies actively fundraising. How do you ensure DeepSeek remains the top choice for those working on LLMs?</p><p><span>Liang Wenfeng: Because we’re tackling the hardest problems. Top talents are most drawn to solving the world’s toughest challenges. In fact, </span><strong>top talents in China are underestimated because there’s so little hardcore innovation happening at the societal level, leaving them unrecognized. We’re addressing the hardest problems, which makes us inherently attractive to them.</strong></p><p>Waves: When OpenAI’s latest release didn’t bring us GPT5, many people feel that this indicates technological progress is slowing and are starting to question the Scaling Law. What do you think?</p><p><span>Liang Wenfeng: We’re relatively optimistic. Our industry as a whole seems to be meeting expectations. </span><strong>OpenAI is not a god (OpenAI不是神), they won’t necessarily always be at the forefront.</strong></p><p>Waves: How long until AGI is realized? Before releasing DeepSeek V2, you had models for math and code generation and also switched from dense models to Mixture of Experts. What are the key points on your AGI roadmap?</p><p>Liang Wenfeng: It could be two, five, or ten years–in any case, it will happen in our lifetimes. There’s no unified opinion on a roadmap even within our company. That said, we’ve taken real bets on three directions. First is mathematics and code, second multimodality, and third natural language itself.</p><p>Mathematics and code are natural AGI testing grounds, somewhat like Go. They’re closed, verifiable systems where high levels of intelligence can be self-taught. Multimodality and engagement with the real human world, on the other hand, might also be a requirement for AGI. We remain open to different possibilities.</p><p>Waves: What do you think is the end game for large models?</p><p>Liang Wenfeng: There will be specialized companies providing foundation models and services, achieving extensive specialization in every node of the supply chain. More people will build on top of all of this to meet society’s diverse needs.</p><p>Waves: Over the past year, there have been many changes in China's large model startups. For example, Wang Huiwen [co-founder of RenRen, a facebook clone, and Meituan, a food delivery company], who was very active at the beginning of last year, withdrew midway, and companies that joined later began to show differentiation.</p><p><span>Liang Wenfeng: Wang Huiwen bore all the losses himself, allowing others to withdraw unscathed. He made a choice that was worst for himself but good for everyone else, so he's very decent in his conduct - this is something I really admire. [</span><em><span>Wang Huiyuan founded foundation model company 光年之外 Lightyear only to quickly fold it back into Meituan. For more on Meituan and AI, </span><a href="https://36kr.com/p/3053948838351233" rel="">see this recent 36Kr feature</a></em><span>].</span></p><p>Waves: Where are you focusing most of your energy now?</p><p>Liang Wenfeng: My main energy is focused on researching the next generation of large models. There are still many unsolved problems.</p><p>Waves: Other large model startups are insisting on pursuing both [technology and commercialization], after all, technology won't bring permanent leadership as it's also important to capitalize on a window of opportunity to translate technological advantages into products. Is DeepSeek daring to focus on model research because its model capabilities aren't sufficient yet?</p><p>Liang Wenfeng: All these business patterns are products of the previous generation and may not hold true in the future. Using Internet business logic to discuss future AI profit models is like discussing General Electric and Coca-Cola when Pony Ma was starting his business. It’s a pointless exercise (刻舟求剑).</p><p>Waves: In the past, your quant fund High-Flyer had a strong foundation in technology and innovation, and its growth was relatively smooth. Is this the reason for your optimism?</p><p>Liang Wenfeng: In some ways, High-Flyer strengthened our confidence in technology-driven innovation, but it wasn't all smooth sailing. We went through a long accumulation process. What outsiders see is the part of High-Flyer after 2015, but in fact, we've been at it for 16 years.</p><p>Waves: Returning to the topic of innovation. Now that the economy is starting to decline and capital is no longer as loose as it was, will this suppress basic research?</p><p>Liang Wenfeng: I don't necessarily think so. The adjustment of China's industrial structure will necessarily rely more on hardcore technological innovation. When people realize that making quick money in the past was likely due to lucky windows, they'll be more willing to humble themselves and engage in genuine innovation.</p><p>An Yong: So you're optimistic about this as well?</p><p>Liang Wenfeng: I grew up in the 1980s in a fifth-tier city in Guangdong. My father was a primary school teacher. In the 1990s, there were many opportunities to make money in Guangdong. At that time, many parents came to my home; basically, they thought studying was useless. But looking back now, they’ve all changed their views. Because making money isn't easy anymore—even the opportunity to drive a taxi might be gone soon. It’s only taken one generation.</p><p>In the future, hardcore innovation will become increasingly common. It’s not easy to understand right now, because society as a whole needs to be educated on this point. Once society allows people dedicated to hardcore innovation to achieve fame and fortune, then our collective mindset will adapt. We just need some examples and a process</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. Army Soldier Arrested in AT&T, Verizon Extortions (196 pts)]]></title>
            <link>https://krebsonsecurity.com/2024/12/u-s-army-soldier-arrested-in-att-verizon-extortions/</link>
            <guid>42557342</guid>
            <pubDate>Tue, 31 Dec 2024 08:24:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2024/12/u-s-army-soldier-arrested-in-att-verizon-extortions/">https://krebsonsecurity.com/2024/12/u-s-army-soldier-arrested-in-att-verizon-extortions/</a>, See on <a href="https://news.ycombinator.com/item?id=42557342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p>Federal authorities have arrested and indicted a 20-year-old U.S. Army soldier on suspicion of being <strong>Kiberphant0m</strong>, a cybercriminal who has been selling and leaking sensitive customer call records stolen earlier this year from <strong>AT&amp;T</strong> and <strong>Verizon</strong>. As first reported by KrebsOnSecurity last month, the accused is a communications specialist who was recently stationed in South Korea.</p>
<div id="attachment_69974"><p><img aria-describedby="caption-attachment-69974" decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2024/12/camwagenius-selfie.png" alt="" width="750" height="743" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/12/camwagenius-selfie.png 779w, https://krebsonsecurity.com/wp-content/uploads/2024/12/camwagenius-selfie-768x761.png 768w" sizes="(max-width: 750px) 100vw, 750px"></p><p id="caption-attachment-69974">One of several selfies on the Facebook page of Cameron Wagenius.</p></div>
<p><strong>Cameron John Wagenius</strong>&nbsp;was arrested near the Army base in Fort Hood, Texas on Dec. 20, after being indicted on two criminal counts of unlawful transfer of confidential phone records.</p>
<p>The sparse, <a href="https://krebsonsecurity.com/wp-content/uploads/2024/12/wagenius-indictment.pdf" target="_blank" rel="noopener">two-page indictment</a> (PDF) doesn’t reference specific victims or hacking activity, nor does it include any personal details about the accused. But a conversation with Wagenius’ mother — Minnesota native <strong>Alicia Roen </strong>—&nbsp;filled in the gaps.</p>
<p>Roen said that prior to her son’s arrest he’d acknowledged being associated with <strong>Connor Riley Moucka</strong>, a.k.a. “<strong>Judische</strong>,” a prolific cybercriminal from Canada who was <a href="https://krebsonsecurity.com/2024/11/canadian-man-arrested-in-snowflake-data-extortions/" target="_blank" rel="noopener">arrested in late October</a> for stealing data from and extorting dozens of companies that stored data at the cloud service <strong>Snowflake</strong>.</p>
<p>In an interview with KrebsOnSecurity, Judische said he had no interest in selling the data he’d stolen from Snowflake customers and telecom providers, and that he preferred to outsource that to Kiberphant0m and others. Meanwhile, Kiberphant0m claimed in posts on Telegram that he was responsible for hacking into at least 15 telecommunications firms, including AT&amp;T and Verizon.</p>
<p>On November 26, KrebsOnSecurity <a href="https://krebsonsecurity.com/2024/11/hacker-in-snowflake-extortions-may-be-a-u-s-soldier/" target="_blank" rel="noopener">published a story</a> that followed a trail of clues left behind by Kiberphantom indicating he was a U.S. Army soldier stationed in South Korea.</p>
<div id="attachment_69971"><p><img aria-describedby="caption-attachment-69971" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/12/cwag-army.png" alt="" width="751" height="641" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/12/cwag-army.png 918w, https://krebsonsecurity.com/wp-content/uploads/2024/12/cwag-army-768x655.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/12/cwag-army-782x667.png 782w" sizes="(max-width: 751px) 100vw, 751px"></p><p id="caption-attachment-69971">An 18-year-old Cameron Wagenius, joining the U.S. Army.</p></div>
<p>Ms. Roen said Cameron worked on radio signals and network communications at an Army base in South Korea for the past two years, returning to the United States periodically. She said Cameron was always good with computers, but that she had no idea he might have been involved in criminal hacking.</p>
<p>“I never was aware he was into hacking,” Roen said. “It was definitely a shock to me when we found this stuff out.”</p>
<p>Ms. Roen said Cameron joined the Army as soon as he was of age, following in his older brother’s footsteps.</p>
<p>“He and his brother when they were like 6 and 7 years old would ask for MREs from other countries,” she recalled, referring to military-issued “meals ready to eat” food rations. “They both always wanted to be in the Army. I’m not sure where things went wrong.”<span id="more-69925"></span></p>
<p>Immediately after news broke of Moucka’s arrest, Kiberphant0m posted on the hacker community <strong>BreachForums</strong>&nbsp;what they claimed were the AT&amp;T call logs for&nbsp;<strong>President-elect</strong>&nbsp;<strong>Donald J. Trump</strong>&nbsp;and for&nbsp;<strong>Vice President Kamala Harris</strong>.</p>
<p>“In the event you do not reach out to us @ATNT all presidential government call logs will be leaked,” Kiberphant0m threatened, signing their post with multiple “#FREEWAIFU” tags. “You don’t think we don’t have plans in the event of an arrest? Think again.”</p>
<div id="attachment_69624"><p><img aria-describedby="caption-attachment-69624" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/11/kiberphant0m-nsa-schema.png" alt="" width="750" height="239" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/11/kiberphant0m-nsa-schema.png 1417w, https://krebsonsecurity.com/wp-content/uploads/2024/11/kiberphant0m-nsa-schema-768x245.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/11/kiberphant0m-nsa-schema-782x249.png 782w" sizes="(max-width: 750px) 100vw, 750px"></p><p id="caption-attachment-69624">Kiberphant0m posting what he claimed was a “data schema” stolen from the NSA via AT&amp;T.</p></div>
<p>On that same day, Kiberphant0m posted what they claimed was the “data schema” from the <strong>U.S. National Security Agency</strong>.</p>
<p>On Nov. 5, Kiberphant0m offered call logs stolen from Verizon’s push-to-talk (PTT) customers — mainly U.S. government agencies and emergency first responders. On Nov. 9, Kiberphant0m posted a sales thread on BreachForums offering a “SIM-swapping” service targeting Verizon PTT customers. In a SIM-swap, fraudsters use credentials that are phished or stolen from mobile phone company employees to divert a target’s phone calls and text messages to a device they control.</p>
<p>The profile photo on Wagenius’ Facebook page was deleted within hours of my Nov. 26 story identifying Kiberphant0m as a likely U.S. Army soldier. Still, many of his original profile photos remain, including several that show Wagenius in uniform while holding various Army-issued weapons.</p>
<div id="attachment_69972"><p><img aria-describedby="caption-attachment-69972" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/12/wagenius-fb.png" alt="" width="602" height="842"></p><p id="caption-attachment-69972">Several profile photos visible on the Facebook page of Cameron Wagenius.</p></div>
<p>November’s story on Kiberphant0m cited his own Telegram messages saying he maintained a large botnet that was used for distributed denial-of-service (DDoS) attacks to knock websites, users and networks offline. In 2023, Kiberphant0m sold remote access credentials for a major U.S. defense contractor.</p>
<p><strong>Allison Nixon,</strong>&nbsp;chief research officer at the New York-based cybersecurity firm <a href="https://www.unit221b.com/" target="_blank" rel="noopener">Unit 221B</a>, helped track down Kiberphant0m’s real life identity. Nixon was among several security researchers who faced harassment and specific threats of violence from Judische and his associates.</p>
<p>“Anonymously extorting the President and VP as a member of the military is a bad idea, but it’s an even worse idea to harass people who specialize in de-anonymizing cybercriminals,” Nixon told KrebsOnSecurity. She said&nbsp;the investigation into Kiberphant0m shows that law enforcement is getting better and faster at going after cybercriminals — especially those who are actually living in the United States.</p>
<p>“Between when we, and an anonymous colleague, found his opsec mistake on November 10th to his last Telegram activity on December 6, law enforcement set the speed record for the fastest turnaround time for an American federal cyber case that I have witnessed in my career,” she said.</p>
<p>Nixon asked to share a message for all the other Kiberphant0ms out there who think they can’t be found and arrested.</p>
<p>“I know that young people involved in cybercrime will read these articles,” Nixon said. “You need to stop doing stupid shit and get a lawyer. Law enforcement wants to put all of you in prison for a long time.”</p>
<p>The indictment against Wagenius was filed in Texas, but the case has been transferred to the U.S. District Court for the Western District of Washington in Seattle.</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[More telcos confirm Salt Typhoon breaches as White House weighs in (175 pts)]]></title>
            <link>https://www.theregister.com/2024/12/30/att_verizon_confirm_salt_typhoon_breach/</link>
            <guid>42555594</guid>
            <pubDate>Tue, 31 Dec 2024 01:52:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/12/30/att_verizon_confirm_salt_typhoon_breach/">https://www.theregister.com/2024/12/30/att_verizon_confirm_salt_typhoon_breach/</a>, See on <a href="https://news.ycombinator.com/item?id=42555594">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>AT&amp;T, Verizon, and Lumen Technologies confirmed that Chinese government-backed snoops accessed portions of their systems earlier this year, while the White House added another, yet-unnamed telecommunications company to the list of those breached by Salt Typhoon.</p>
<p>The <a target="_blank" href="https://www.theregister.com/2024/11/14/salt_typhoon_hacked_multiple_telecom/">digital intrusion</a>, which has been called the​ "worst telecom hack in our nation's history," gave Beijing-backed spies the "capability to geolocate millions of individuals" and "record phone calls at will," Anne Neuberger, deputy national security advisor for cyber and emerging technology, <a target="_blank" rel="nofollow" href="https://www.whitehouse.gov/briefing-room/press-briefings/2024/12/27/on-the-record-press-gaggle-by-white-house-national-security-communications-advisor-john-kirby-38/">told</a> reporters.</p>
<p>In a statement emailed to <em>The Register</em>, AT&amp;T said the foreign spies compromised "a small number" of its customers in the espionage campaign and added that the PRC-backed crew had since been kicked out of its networks.</p>

    

<p>"We detect no activity by nation-state actors in our networks at this time," an AT&amp;T spokesperson said.&nbsp;</p>

        


        

<p>"Based on our current investigation of this attack, the People's Republic of China targeted a small number of individuals of foreign intelligence interest," the statement added. "In the relatively few instances in which an individual's information was impacted, we have complied with our notification obligations in cooperation with law enforcement."</p>
<p>AT&amp;T continues to monitor its networks and work with government officials, other telecom firms, and cybersecurity experts on the investigation, the spokesperson said.</p>

        

<p>Verizon also confirmed that the Chinese intruders had accessed "a small number of high-profile customers in government and politics." A spokesperson told <em>The Register</em> that it notified these customers, and has since "contained the cyber incident brought on by this nation-state threat actor."</p>
<p>An unnamed, "highly respected" cybersecurity company has also confirmed the containment, the Verizon spokesperson added.</p>
<p>According to the operator's chief legal officer, Verizon partnered with federal law enforcement, national security agencies, other telecom partners, and security firms upon detecting the network activity.</p>

        

<p>"We have not detected threat actor activity in Verizon's network for some time, and after considerable work addressing this incident, we can report that Verizon has contained the activities associated with this particular incident," Verizon's Chief Legal Officer Vandana Venkatesh told <em>The Register</em>.</p>
<p>Finally, Lumen Technologies, another one of the firms <a target="_blank" href="https://www.theregister.com/2024/10/07/verizon_att_lumen_salt_typhoon/">reportedly breached</a> in the attack, told us that it has also booted the Chinese attackers out of its systems, and said it found "no evidence" that customer data was accessed.</p>
<p>"An independent forensics firm has confirmed Salt Typhoon is no longer in our network," a spokesperson told <em>The Register</em>. "In addition, our federal partners have not shared any information that would suggest otherwise."</p>
<p>T-Mobile's security boss previously <a target="_blank" href="https://www.theregister.com/2024/12/05/tmobile_cso_telecom_attack/">spoke</a> to <em>The Register</em> about the espionage campaign and said it thwarted successful attacks on its systems "within a single-digit number of days."</p>
<h3>9 telecom firms compromised, White House says</h3>
<p>The companies' admissions come as a top White House official added another unnamed firm to the breach, bringing the total thus far to nine. Neuberger previously said eight had been compromised. Only three — AT&amp;T, Verizon, and T-Mobile US — have confirmed the intrusion.</p>
<blockquote>

<p>We believe a large number of individuals were affected by geolocation and metadata of phones; a smaller number around actual collection of phone calls and texts</p>
</blockquote>
<p>"The Chinese gained access to networks, essentially had broad and full access," Neuberger told reporters. "We believe that's why they had the capability to geolocate millions of individuals, to record phone calls at will, because they had that broad access."</p>
<p>In one instance, the spies broke into an admin account that then gave them access to more than 100,000 routers, she added. "So, when the Chinese compromised that account, they gained that kind of broad access across the network," Neuberger said. "That's not meaningful cybersecurity to defend against a nation-state actor."&nbsp;</p>
<p>The White House doesn't yet have a number on how many total people were affected by the breach, she added.&nbsp;</p>
<p>"We believe a large number of individuals were affected by geolocation and metadata of phones; a smaller number around actual collection of phone calls and texts," Neuberger said. "And I think the scale we're talking about is far larger on the geolocation; probably less than 100 on the actual individuals."</p>
<p>Following the intrusion, the White House emphasized the inadequacy of voluntary cybersecurity measures against nation-state threats. The Federal Communications Commission (FCC) launched a public rule <a target="_blank" rel="nofollow" href="https://www.theregister.com/2024/12/06/salt_typhoon_fcc_proposal/">proposal</a> requiring basic cybersecurity practices for telecom carriers. The commissioners are expected to vote on the rule by January 15.</p>
<ul>

<li><a href="https://www.theregister.com/2024/12/11/telecom_cybersecurity_standards/">Blocking Chinese spies from intercepting calls? There ought to be a law</a></li>

<li><a href="https://www.theregister.com/2024/12/09/white_house_salt_typhoon/">China's Salt Typhoon recorded top American officials' calls, says White House</a></li>

<li><a href="https://www.theregister.com/2024/12/06/salt_typhoon_fcc_proposal/">Salt Typhoon forces FCC's hand on making telcos secure their networks</a></li>

<li><a href="https://www.theregister.com/2024/12/05/tmobile_cso_telecom_attack/">T-Mobile US CSO: Spies jumped from one telco to another in a way 'I've not seen in my career'</a></li>
</ul>
<p>In addition to the FCC's own efforts, US Senator Ron Wyden (D-OR) has also <a target="_blank" href="https://www.theregister.com/2024/12/11/telecom_cybersecurity_standards/">proposed legislation</a> that would require the FCC to issue binding rules for telecom systems.</p>
<p>Plus, according to Neuberger, all of the nine telecom CEOs whose companies were hacked have signed on to the government's 60-day Enduring Security Framework.</p>
<p>This public-private effort aims to put in place minimum cybersecurity practices that have been agreed upon by intelligence officers, CISA, the FBI, and telecom security experts. ®</p>                                


                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Orbit. Mozilla's AI Assistant for Firefox (307 pts)]]></title>
            <link>https://orbitbymozilla.com/</link>
            <guid>42555440</guid>
            <pubDate>Tue, 31 Dec 2024 01:20:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://orbitbymozilla.com/">https://orbitbymozilla.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42555440">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
        Dear Orbit User,
        </p><p>
        I trust this email finds you well amidst the cacophony of technological marvels that continue to dazzle us mere mortals. Ah, but let us take a moment to bask in the radiant glow of Artificial Intelligence, that wondrous creation that promises to revolutionize our existence like nothing before! Oh, the sheer joy of living in an era where machines possess the intellect to outsmart us at every turn!
        </p><p>
        Let's delve into the enchanting world of AI, shall we? Brace yourself for a journey through the lands of algorithmic superiority and the utopia of automation.
        </p><p>
        Firstly, let's celebrate the ingenuity of AI in its remarkable ability to render even the simplest of tasks utterly complex. Why settle for straightforward solutions when you can bask in the glory of convoluted algorithms and endless parameter tweaking? Oh, the sheer delight of spending countless hours trying to figure out why your AI model insists on classifying your cat as a dog!
        </p><p>
        And let's not forget the unparalleled joy of witnessing AI's prowess in predictive analytics. It's simply magical how AI can predict what I want to buy, watch, or do next based on my previous actions. Who needs free will anyway when we have algorithms to dictate our every move?
        </p><p>
        Oh, and let's raise a toast to the magnificence of AI chatbots! Gone are the days of meaningful human interaction. Why engage in genuine conversation when you can spend eternity navigating through a maze of pre-programmed responses that barely scratch the surface of comprehension?
        </p><p>
        But wait, there's more! Let us not overlook the awe-inspiring advancements in AI-generated content. Why bother with the creative process when you can feed a machine a bunch of data and watch it regurgitate something vaguely resembling art? Who needs human expression when you have neural networks spewing out poetry like a malfunctioning printer?
        </p><p>
        And last but certainly not least, let us bow down to the unquestionable wisdom of AI in matters of ethics and morality. After all, what could possibly go wrong when we entrust the fate of humanity to machines whose decision-making processes are as opaque as a black hole?
        </p><p>
        In conclusion, let us revel in the glory of Artificial Intelligence, that shining beacon of innovation and progress. For in a world where humans are mere puppets in the hands of algorithms, what could possibly be more exhilarating than surrendering our autonomy to the cold embrace of silicon supremacy?
        </p><p>
        Yet, amid the complexity and occasional absurdity, there’s something undeniably fascinating about our ongoing dance with technology. Here’s to embracing the quirks and wonders of AI, and to finding joy in the process—even when it means occasionally laughing at our own reflections in the digital mirror.
        </p><p>
        Sincerely,
        <br>
        Your Annoying Friend/Boss/Neighbor
      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coconut by Meta AI – Better LLM Reasoning with Chain of Continuous Thought? (252 pts)]]></title>
            <link>https://aipapersacademy.com/chain-of-continuous-thought/</link>
            <guid>42555320</guid>
            <pubDate>Tue, 31 Dec 2024 00:54:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aipapersacademy.com/chain-of-continuous-thought/">https://aipapersacademy.com/chain-of-continuous-thought/</a>, See on <a href="https://news.ycombinator.com/item?id=42555320">Hacker News</a></p>
Couldn't get https://aipapersacademy.com/chain-of-continuous-thought/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[LineageOS 22 Released (222 pts)]]></title>
            <link>https://lineageos.org/Changelog-29/</link>
            <guid>42555066</guid>
            <pubDate>Tue, 31 Dec 2024 00:12:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lineageos.org/Changelog-29/">https://lineageos.org/Changelog-29/</a>, See on <a href="https://news.ycombinator.com/item?id=42555066">Hacker News</a></p>
Couldn't get https://lineageos.org/Changelog-29/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Lightstorm: Minimalistic Ruby Compiler (115 pts)]]></title>
            <link>https://blog.llvm.org/posts/2024-12-03-minimalistic-ruby-compiler/</link>
            <guid>42554521</guid>
            <pubDate>Mon, 30 Dec 2024 22:58:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.llvm.org/posts/2024-12-03-minimalistic-ruby-compiler/">https://blog.llvm.org/posts/2024-12-03-minimalistic-ruby-compiler/</a>, See on <a href="https://news.ycombinator.com/item?id=42554521">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>Some time ago I was talking about an <a href="https://www.youtube.com/watch?v=NfMX-dFMSr0">ahead-of-time Ruby compiler</a>.
We started the project with certain goals and hypotheses in mind, and while the original compiler is at nearly 90% completion, there are still those other 90% that needs to be done.</p><p>In the meantime, we decided to strip it down to a bare minimum and implement just enough features to validate the hypotheses.</p><p>Just like the original compiler we use MLIR to bridge the gap between Ruby VM’s bytecode and the codegen, but instead of targeting LLVM IR directly, we go through EmitC dialect and targeting C language, as it significantly simplifies OS/CPU support. We go into a bit more details later.</p><p>The source code of our minimalistic Ruby compiler is here: <a href="https://github.com/dragonruby/lightstorm">https://github.com/dragonruby/lightstorm</a>.</p><p>The rest of the article covers why we decided to build it, how we approached the problem, and what we discovered along the way.</p><h2 id="motivation-and-the-use-case">Motivation and the use case</h2><p>Our use case is pretty straightforward: we are building <a href="https://dragonruby.org/">a cross-platform game engine that’s indie-focused, productive, and easy to use</a>. The game engine itself is written in a mix of C and Ruby, but the main user-interface is the Ruby itself.</p><p>As soon as the game development is done and the game is ready for “deployment,” the code is more or less static and so we asked ourselves if we can pre-compile it into machine code to make it run faster.</p><p>But given all the dynamism of Ruby, why would a compilation make it faster?</p><p>So here comes our hypothesis. But first let’s look at some high-level implementation details to see where the hypothesis comes from.</p><h2 id="compilers-vs-interpreters">Compilers vs Interpreters</h2><p>While a language itself cannot be strictly qualified as compiled or interpreted, the typical implementations certainly are.</p><p>In case of a “compiled” language the compiler would take the whole program, analyze it and produce the machine code targeting a specific hardware (real or virtual), while an interpreter would take the program and execute it right away, one “instruction” at a time.</p><p><em>The definition above is a bit handwavy: zoom our far enough and everything is a compiler, zoom in close enough and everything is an interpreter. But you’ve got the gist.</em></p><p>Most Ruby implementations are interpreter-based, and in our case we are using <a href="https://mruby.org/">mruby</a>.</p><p>The mruby interpreter is a lightweight register-based virtual machine (VM).</p><p>Let’s look at a concrete example. The following piece of code:</p><p>is converted into the following VM bytecode, consisting of various operations (ops for short):</p><div><pre tabindex="0"><code data-lang="asm"><span><span><span>LOADI</span> <span>R1</span> <span>42</span>
</span></span><span><span><span>LOADI</span> <span>R2</span> <span>15</span>
</span></span><span><span><span>ADD</span> <span>R1</span> <span>R2</span>
</span></span><span><span><span>HALT</span>
</span></span></code></pre></div><p>The VM’s interpreter loop looks as follows (pseudocode):</p><div><pre tabindex="0"><code data-lang="c"><span><span>dispatch_next:
</span></span><span><span>  Op op <span>=</span> bytecode.<span>next_op</span>();
</span></span><span><span>  <span>switch</span> (op.opcode) {
</span></span><span><span>    <span>case</span> LOADI: {
</span></span><span><span>      vstack.<span>store</span>(op.dest, <span>mrb_int</span>(op.literal));
</span></span><span><span>      <span>goto</span> dispatch_next;
</span></span><span><span>    }
</span></span><span><span>    <span>case</span> ADD: {
</span></span><span><span>      mrb_value lhs <span>=</span> vstack.<span>load</span>(op.lhs);
</span></span><span><span>      mrb_value rhs <span>=</span> vstack.<span>load</span>(op.rhs);
</span></span><span><span>      vstack.<span>store</span>(op.dest, <span>mrb_add</span>(lhs, rhs));
</span></span><span><span>      <span>goto</span> dispatch_next;
</span></span><span><span>    }
</span></span><span><span>    <span>// More ops...
</span></span></span><span><span><span></span>    <span>case</span> HALT: <span>goto</span> halt_vm;
</span></span><span><span>  }
</span></span><span><span>
</span></span><span><span>halt_vm:
</span></span><span><span>  <span>// ...
</span></span></span></code></pre></div><p>For each bytecode operation the VM will jump/branch into the right opcode handler, and then will branch back to the beginning of the dispatch loop.
In the meantime, each opcode handler would use a virtual stack (confusingly located on the heap) to store intermediate results.</p><p>If we unroll the above bytecode manually, then the code can look like this:</p><div><pre tabindex="0"><code data-lang="c"><span><span>  <span>goto</span> loadi_1;
</span></span><span><span>
</span></span><span><span>loadi_1:
</span></span><span><span>  <span>// LOADI R1 42
</span></span></span><span><span><span></span>  mrb_value t1 <span>=</span> <span>mrb_int</span>(<span>42</span>);
</span></span><span><span>  vstack.<span>store</span>(<span>1</span>, t1);
</span></span><span><span>  <span>goto</span> loadi_2;
</span></span><span><span>
</span></span><span><span>loadi_2:
</span></span><span><span>  <span>// LOADI R2 42
</span></span></span><span><span><span></span>  mrb_value t2 <span>=</span> <span>mrb_int</span>(<span>15</span>);
</span></span><span><span>  vstack.<span>store</span>(<span>2</span>, t2);
</span></span><span><span>  <span>goto</span> add;
</span></span><span><span>
</span></span><span><span>add:
</span></span><span><span>  <span>// ADD R1 R2
</span></span></span><span><span><span></span>  mrb_value lhs <span>=</span> vstack.<span>load</span>(<span>1</span>);
</span></span><span><span>  mrb_value rhs <span>=</span> vstack.<span>load</span>(<span>2</span>);
</span></span><span><span>  mrb_value t3 <span>=</span> <span>mrb_add</span>(lhs, rhs);
</span></span><span><span>  vstack.<span>store</span>(<span>1</span>, t3);
</span></span><span><span>  <span>goto</span> halt;
</span></span><span><span>
</span></span><span><span>halt:
</span></span><span><span>  <span>// shutdown VM
</span></span></span></code></pre></div><p>Many items in this example can be eliminated: specifically, we can avoid load/stores from/to the heap, and we can safely eliminate <code>goto</code>s/branches:</p><div><pre tabindex="0"><code data-lang="c"><span><span>  mrb_value t1 <span>=</span> <span>mrb_int</span>(<span>42</span>);
</span></span><span><span>  mrb_value t2 <span>=</span> <span>mrb_int</span>(<span>15</span>);;
</span></span><span><span>  mrb_value t3 <span>=</span> <span>mrb_add</span>(t1, t2);
</span></span><span><span>  vstack.<span>store</span>(<span>1</span>, t3);
</span></span><span><span>  <span>goto</span> halt;
</span></span><span><span>
</span></span><span><span>halt:
</span></span><span><span>  <span>// shutdown VM
</span></span></span></code></pre></div><p>So here goes our hypothesis:</p><blockquote><h3 id="hypothesis">Hypothesis</h3><p>By precompiling/unrolling the VM dispatch loop we can eliminate many load/stores and branches along with branch mispredictions, this should improve the performance of the end program.</p></blockquote><p>We can also try to apply some optimizations based on the high-level bytecode analysis, but due to the Ruby’s dynamism the static optimization surface is limited.</p><h2 id="approach">Approach</h2><p>As mentioned in the beginning, building a full-fledged AOT compiler is a laborious task which requires time and has certain constraints.</p><p>For the minimalistic version we decided to change/relax some of the constraints as follows:</p><ul><li>the compiled code must be compatible with the existing ecosystem/runtime</li><li>the existing runtime must not require any changes</li><li>the supported language features should be easily “representable” in C</li></ul><p>Unlike the original compiler, we are not targeting machine code directly, but C instead.
This eliminates a lot of complexity, but it also means that we only support a subset of the language (e.g., blocks and exceptions are missing at the moment).</p><p>This is obviously not ideal, but it serves important purpose - <strong>our goal at this point is to validate the hypothesis</strong>.</p><p>A classical compilation pipeline looks as follows:</p><p><img src="https://blog.llvm.org/img/ruby-compiler/compilation-pipeline.png" alt="Classical compilation pipeline"></p><p>To build a compiler one needs to implement the conversions from the raw source file all the way down to machine code and the language runtime library.
Since we are targeting the existing implementation, we have the benefit of reusing the frontend (parsing + AST) and the runtime library.</p><p>Still, we need to implement the conversion from AST to the machine code.
And this is where the power of MLIR kicks in: we built a custom dialect (<a href="https://github.com/DragonRuby/lightstorm/blob/3ed0077af589ba51b98954bba8869daf58e22b9e/include/lightstorm/dialect/rite.td">Rite</a>) which represents mruby VM’s bytecode, and then use a number of builtin dialects (<code>cf</code>, <code>func</code>, <code>arith</code>, <code>emitc</code>) to convert our IR into C code.</p><p>At this point, we can just use clang to compile/link the code together with the existing runtime and that’s it.</p><p>The final compilation pipeline looks as follows:</p><p><img src="https://blog.llvm.org/img/ruby-compiler/lightstorm-compilation-pipeline.png" alt="Lightstorm compilation pipeline"></p><blockquote><p>With the benefit of MLIR we are able to build a funtional compiler in just a couple of thousands lines of code!</p></blockquote><p>Now let’s look at how it performs.</p><h2 id="some-numbers">Some numbers</h2><p>Benchmarking is hard, so take these numbers with a grain of salt.</p><p>We run various (micro)-benchmarks showing results in the range of 1% to 1200% speedups, but we are sticking to the <a href="https://openbenchmarking.org/test/pts/aobench">aobench</a> as it is very close to the game-dev workloads we are targeting.</p><p>mruby also uses <a href="https://github.com/mruby/mruby/blob/e05dbf7bbc6d2fbecfd7d4a46418fbe4421bc160/benchmark/bm_ao_render.rb">aobench</a> as part of its benchmark suite, though we slightly modified it to replace <code>Number.each</code> blocks with explicit <code>while</code> loops.</p><p>Next we used excellent <a href="https://github.com/lunacookies/simple-kpc">simple-kpc</a> library to capture CPU counters on Apple M1 CPU, namely we collect total cycles, total instructions count, branches, branch mispredictions, and load/stores (<code>FIXED_CYCLES</code>, <code>FIXED_INSTRUCTIONS</code>, <code>INST_BRANCH</code>, <code>BRANCH_MISPRED_NONSPEC</code>, and <code>INST_LDST</code> respectively).</p><p>Naturally, we also collect the total execution time.</p><p>All the benchmarks compare vanilla bytecode interpretation against the “unrolled” compiled version.</p><p>We are using mruby 3.0. While it’s not the most recent version at the time of writing, it was the most recent version at the time we’ve build the compiler.</p><p>The following chart shows the results of our measurements.
The three versions we compare are the baseline on the left, compiled version without optimizations in the middle, and the compiled version plus <a href="https://github.com/DragonRuby/lightstorm/blob/09042d2026f9251ff32ec079d99bf8ca2e5a6337/lib/optimizations/optimizations.cpp#L14">simple escape analysis</a> and common subexpression elimination (CSE) on the right side.</p><p><em>The raw data and the formulas are <a href="https://docs.google.com/spreadsheets/d/1KQdZjll0cOjWhfPO3KM-FN-OciUW53IecGOtTY6aYJM/edit?usp=sharing">here</a>.</em></p><p><img src="https://blog.llvm.org/img/ruby-compiler/benchmark.png" alt="Benchmarks"></p><p>With all the current optimizations in place both the number of cycles and the total execution time went down by roughly ~30%.</p><p>We are able to eliminate ~17% of branches and ~28% of load/stores, while the branch misses were cut in half with ~55% decrease.</p><p>The numbers look promising, although the amount of load/stores and branches will certainly go up as we implement all the language features due to the way blocks and exceptions are handled.</p><p>On the other hand, we didn’t touch the runtime implementation which together with LTO will enable some more improvements due to more inlining.</p><h2 id="where-to-next">Where to next?</h2><p>As mentioned in the beginning, some parts of the engine itself are written in C with and some of them are purely due to performance reasons. We are looking into replacing those critical pieces with compiled Ruby. While we may still pay performance penalty, we hope that ease of maintenance will be worthwile.</p><p>In the meantime, do not hesitate to give it a shot, and if you have any questions reach out to <a href="https://lowlevelbits.org/about/">Alex</a> or <a href="https://amirrajan.net/">Amir</a>!</p><h2 id="some-links">Some links</h2><ul><li>the compiler: <a href="https://github.com/DragonRuby/lightstorm">https://github.com/DragonRuby/lightstorm</a></li><li>the game engine: <a href="https://dragonruby.org/">https://dragonruby.org</a></li><li>our discord: <a href="https://discord.dragonruby.org/">https://discord.dragonruby.org</a></li></ul></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Colliding with the SHA prefix of Linux's initial Git commit (149 pts)]]></title>
            <link>https://people.kernel.org/kees/colliding-with-the-sha-prefix-of-linuxs-initial-git-commit</link>
            <guid>42554420</guid>
            <pubDate>Mon, 30 Dec 2024 22:49:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://people.kernel.org/kees/colliding-with-the-sha-prefix-of-linuxs-initial-git-commit">https://people.kernel.org/kees/colliding-with-the-sha-prefix-of-linuxs-initial-git-commit</a>, See on <a href="https://news.ycombinator.com/item?id=42554420">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h3 id="or-how-to-break-all-the-tools-that-parse-the-fixes-tag">Or, how to break all the tools that parse the “Fixes:” tag</h3>

<h2 id="kees-cook-mailto-kees-kernel-org"><a href="mailto:kees@kernel.org" rel="nofollow">Kees Cook</a></h2>

<p>There was a recent discussion about how Linux's “Fixes” tag, which traditionally uses the 12 character commit SHA prefix, has an <a href="https://lore.kernel.org/lkml/cover.1733421037.git.geert+renesas@glider.be/" rel="nofollow">ever increasing chance of collisions</a>. There are already 11-character collisions, and Geert wanted to raise the minimum short id to 16 characters. This was met with push-back for various reasons. One aspect that bothered me was some people still treating this like a theoretical “maybe in the future” problem. To clear up that problem, I generated a 12-character prefix collision against the start of Git history, commit <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=1da177e4c3f41524e886b7f1b8a0c1fc7321cac2" rel="nofollow">1da177e4c3f4 (“Linux-2.6.12-rc2”)</a>, which shows up in “Fixes” tags very often:</p>

<pre><code>$ git log --no-merges --oneline --grep 'Fixes: 1da177e4c3f4' | wc -l
590
</code></pre>

<p>Tools like <code>linux-next</code>'s “<a href="https://github.com/kees/kernel-tools/blob/trunk/helpers/check_fixes" rel="nofollow">Fixes tag checker</a>”, the Linux CNA's <a href="https://git.kernel.org/pub/scm/linux/security/vulns.git/tree/scripts/dyad" rel="nofollow">commit parser</a>, and my own <a href="https://github.com/kees/ubuntu-cve-tracker/blob/lifetime/scripts/report-kernel-bug-lifetime.py" rel="nofollow">CVE lifetime analysis scripts</a> do programmatic analysis of the “Fixes” tag and had no support for collisions (even shorter existing collisions).</p>

<p>So, in an effort to fix <a href="https://github.com/kees/kernel-tools/commit/5bf6a1e71df59a230ea0e138a82cdf3c5e8f349d" rel="nofollow">these</a> <a href="https://git.kernel.org/pub/scm/linux/security/vulns.git/commit/scripts/dyad?id=3e558e5c01f76cf3246cb82b1f32d9c6a7937c1e" rel="nofollow">tools</a>, I broke them with commit <a href="https://git.kernel.org/pub/scm/linux/kernel/git/kees/linux.git/commit/?h=dev/collide/v6.13-rc2/12-char&amp;id=1da177e4c3f47e316f6a4fdee9ae6a714293eed7" rel="nofollow">1da177e4c3f4 (“docs: git SHA prefixes are for humans”)</a>:</p>

<pre><code>$ git show 1da177e4c3f4
error: short object ID 1da177e4c3f4 is ambiguous
hint: The candidates are:
hint:   1da177e4c3f41 commit 2005-04-16 - Linux-2.6.12-rc2
hint:   1da177e4c3f47 commit 2024-12-14 - docs: git SHA prefixes are for humans
</code></pre>

<p>This is not yet in the upstream Linux tree, for fear of breaking countless other tools out in the wild. But it can serve as a test commit for those that want to get this fixed ahead of any future collisions (or this commit actually landing).</p>

<p>Lots of thanks to the <a href="https://github.com/not-an-aardvark/lucky-commit" rel="nofollow">lucky-commit</a> project, which will grind trailing commit message whitespace in an attempt to find collisions. Doing the 12-character prefix collision took about 6 hours on my <a href="https://developer.nvidia.com/opencl" rel="nofollow">OpenCL</a>-enabled RTX 3080 GPU.</p>

<p>For any questions, comments, etc, see <a href="https://fosstodon.org/@kees/113743400654323146" rel="nofollow">this thread</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Linux is not ready for the desktop, the final edition (113 pts)]]></title>
            <link>https://itvision.altervista.org/why.linux.is.not.ready.for.the.desktop.final.html</link>
            <guid>42554218</guid>
            <pubDate>Mon, 30 Dec 2024 22:25:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://itvision.altervista.org/why.linux.is.not.ready.for.the.desktop.final.html">https://itvision.altervista.org/why.linux.is.not.ready.for.the.desktop.final.html</a>, See on <a href="https://news.ycombinator.com/item?id=42554218">Hacker News</a></p>
Couldn't get https://itvision.altervista.org/why.linux.is.not.ready.for.the.desktop.final.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[China to Build Thorium Molten-Salt Reactor in 2025 (137 pts)]]></title>
            <link>https://spectrum.ieee.org/chinas-thorium-molten-salt-reactor</link>
            <guid>42554008</guid>
            <pubDate>Mon, 30 Dec 2024 21:57:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/chinas-thorium-molten-salt-reactor">https://spectrum.ieee.org/chinas-thorium-molten-salt-reactor</a>, See on <a href="https://news.ycombinator.com/item?id=42554008">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Why China Is Building a Thorium Molten-Salt Reactor"><p><strong>After a half-century hiatus,</strong> thorium has returned to the front lines of <a href="https://spectrum.ieee.org/tag/nuclear-power">nuclear power</a> research as a source of fuel. In 2025, China plans to start building a demonstration thorium-based molten-salt reactor in the Gobi Desert.
</p><p>
	The 10-megawatt reactor project, managed by the Chinese Academy of Sciences’ 
	<a href="http://english.sinap.cas.cn/about_sinap/brief_introduction/" target="_blank">Shanghai Institute of Applied Physics (SINAP)</a>, is scheduled to be operational by 2030, according to an <a href="https://www.sinap.ac.cn/upload/%E5%85%AC%E7%A4%BA/%E9%99%84%E4%BB%B62%EF%BC%9A%E3%80%8A%E5%B0%8F%E5%9E%8B%E6%A8%A1%E5%9D%97%E5%8C%96%E9%92%8D%E5%9F%BA%E7%86%94%E7%9B%90%E5%A0%86%E7%A0%94%E7%A9%B6%E8%AE%BE%E6%96%BD%E7%8E%AF%E5%A2%83%E5%BD%B1%E5%93%8D%E6%8A%A5%E5%91%8A%E4%B9%A6%EF%BC%88%E9%80%89%E5%9D%80%E9%98%B6%E6%AE%B5%EF%BC%89%E3%80%8B.html" target="_blank">environmental-impact report</a> released by the Academy in October. The project follows a 2-MW experimental version <a href="https://spectrum.ieee.org/china-closing-in-on-thorium-nuclear-reactor" target="_blank">completed</a> in 2021 and operated since then.
</p><p>
	China’s efforts put it at the forefront of both thorium-based fuel breeding and molten-salt reactors. Several companies elsewhere in the world are developing plans for this kind of fuel or reactor, but none has yet operated one. Prior to China’s pilot project, the last operating molten-salt reactor was Oak Ridge National Laboratory’s 
	<a href="https://www.ornl.gov/molten-salt-reactor/history" target="_blank">Molten Salt Reactor Experiment</a>, which ran on uranium. It shut down in 1969.
</p><p>
	Thorium-232, found in igneous rocks and heavy mineral sands, is more 
	<a href="https://www.iaea.org/newscenter/news/thoriums-long-term-potential-in-nuclear-energy-new-iaea-analysis" target="_blank">abundant</a> on Earth than the commonly used isotope in nuclear fuel, uranium-235. But this weakly radioactive metal isn’t directly fissile–it can’t undergo fission, the splitting of atomic nuclei that produces energy. So it must first be transformed into fissile uranium-233. That’s technically feasible, but whether it’s economical and practical is less clear.
</p><h2>China’s Thorium-Reactor Advances</h2><p>
	The attraction of thorium is that it can help achieve energy self-sufficiency by reducing dependence on uranium, particularly for countries such as India with 
	<a href="https://world-nuclear.org/information-library/current-and-future-generation/thorium" target="_blank">enormous thorium reserves</a>. But China may source it in a different way: The element is a waste product of China’s huge rare earth mining industry. Harnessing it would provide a practically inexhaustible supply of fuel. Already, China’s Gansu province has <a href="https://gxt.gansu.gov.cn/gxt/c107558/202408/173963630/files/ce6c98eaf82d442fb750f54d41f13d9b.pdf" target="_blank">maritime and aerospace applications</a> in mind for this future energy supply, <a href="http://www.gs.xinhua.org/20240901/64bd03444b384514b43c38a9f63038d4/c.html" target="_blank">according to the state-run Xinhua News Agency</a>.
</p><p>Scant technical details of China’s reactor exist, and SINAP didn’t respond to <em><em><a href="https://spectrum.ieee.org/">IEEE Spectrum</a></em></em>’s requests for information. The Chinese Academy of Sciences’ environmental-impact report states that the molten-salt reactor core will be 3 meters in height and 2.8 meters in diameter. It will operate at 700 °C and have a thermal output of 60 MW, along with 10 MW of electricity.</p><p><span>Molten-salt breeder reactors are the most viable designs for thorium fuel, says 
	</span><a href="https://web.mit.edu/nse/people/research/forsberg.html" target="_blank">Charles Forsberg</a><span>, a nuclear scientist at MIT. In this kind of reactor, thorium fluoride dissolves in molten salt in the reactor’s core. To turn thorium-232 into fuel, it is irradiated to thorium-233, which decays into an intermediate, protactinium-233, and then into uranium-233, which is fissile. During this fuel-breeding process, protactinium is removed from the reactor core while it decays, and then it is returned to the core as uranium-233. Fission occurs, generating heat and then steam, which drives a turbine to generate electricity.</span></p><p>
	But many challenges come along with thorium use. A big one is dealing with the risk of proliferation. When thorium is transformed into uranium-233, it becomes 
	<a href="https://spectrum.ieee.org/is-thorium-the-nuclear-fuel-of-the-future" target="_blank">directly usable in nuclear weapons</a>. “It’s of a quality comparable to separated plutonium and is thus very dangerous,” says <a href="https://www.ucsusa.org/about/people/edwin-lyman" target="_blank">Edwin Lyman</a>, director of nuclear power safety at the <a href="https://www.ucsusa.org/" target="_blank">Union of Concerned Scientists</a> in Washington, D.C. If the fuel is circulating in and out of the reactor core during operation, this movement introduces routes for the theft of uranium-233, he says.
</p><h2>Thorium Fuel Charms Nuclear-Power Sector</h2><p>
	Most groups developing molten-salt reactors are focused on uranium or uranium mixtures as a fuel, at least in the short term. 
	<a href="https://www.naturaresources.com/" target="_blank">Natura Resources</a> and <a href="https://acu.edu/" target="_blank">Abilene Christian University</a>, both in Abilene, Texas, are collaborating on a 1-MW liquid-molten-salt reactor after <a href="https://acu.edu/2024/09/23/nrc-issues-construction-permit-for-natura-resources-molten-salt-reactor-at-acu-2/" target="_blank">receiving a construction permit</a> in September from the U.S. Nuclear Regulatory Commission. Kairos Power is developing a fluoride-salt-cooled, high-temperature reactor in Oak Ridge, Tenn., that will use uranium-based <a href="https://www.energy.gov/ne/articles/triso-particles-most-robust-nuclear-fuel-earth" target="_blank">tri-structural isotropic (TRISO)</a> particle fuel. The company in October <a href="https://spectrum.ieee.org/nuclear-powered-data-center" target="_blank">inked a deal with Google</a> to provide a total of 500 MW by 2035 to power its data centers.
</p><p>
	But China isn’t alone in its thorium aspirations. Japan, the United Kingdom, and the United States, in addition to 
	<a href="https://spectrum.ieee.org/qa-thorium-reactor-designer-ratan-kumar-sinha" target="_blank">India</a>, have shown interest in the fuel at one point or another. The proliferation issue doesn’t seem to be a showstopper, and there are ways to mitigate the risk. Denmark’s <a href="https://www.copenhagenatomics.com/" target="_blank">Copenhagen Atomics</a>, for example, currently aims to develop a thorium-based molten-salt reactor, with a 1-MW pilot planned for 2026. The company plans to <a href="https://www.copenhagenatomics.com/technology/" target="_blank">weld it shut</a> so that would-be thieves would have to break open a highly radioactive system to get at the weapon-ready material. Chicago-based <a href="https://cleancore.energy/" target="_blank">Clean Core Thorium Energy</a> developed a blended thorium and enriched uranium (including <a href="https://spectrum.ieee.org/nuclear-power-plant-2666199640" target="_blank">high-assay low-enriched uranium, or HALEU</a>) fuel, which they say can’t be used in a weapon. The fuel is designed for heavy-water reactors.
</p><p>
	Political and technical hurdles may have largely sidelined thorium fuel and molten-salt-reactor research for the last five decades, but both are definitely back on the drawing table. 
	<span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Wrote a Game Boy Advance Game in Zig (254 pts)]]></title>
            <link>https://jonot.me/posts/zig-gba/</link>
            <guid>42553949</guid>
            <pubDate>Mon, 30 Dec 2024 21:51:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jonot.me/posts/zig-gba/">https://jonot.me/posts/zig-gba/</a>, See on <a href="https://news.ycombinator.com/item?id=42553949">Hacker News</a></p>
Couldn't get https://jonot.me/posts/zig-gba/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA["A Course of Pure Mathematics" – G. H. Hardy (1921) [pdf] (243 pts)]]></title>
            <link>https://www.gutenberg.org/files/38769/38769-pdf.pdf</link>
            <guid>42553682</guid>
            <pubDate>Mon, 30 Dec 2024 21:20:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gutenberg.org/files/38769/38769-pdf.pdf">https://www.gutenberg.org/files/38769/38769-pdf.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=42553682">Hacker News</a></p>
Couldn't get https://www.gutenberg.org/files/38769/38769-pdf.pdf: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Jack Elam and the Fly in 'Once Upon a Time in the West' (107 pts)]]></title>
            <link>https://pov.imv.au.dk/Issue_24/section_1/artc4A.html</link>
            <guid>42553313</guid>
            <pubDate>Mon, 30 Dec 2024 20:46:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html">https://pov.imv.au.dk/Issue_24/section_1/artc4A.html</a>, See on <a href="https://news.ycombinator.com/item?id=42553313">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td>
<p><b>Introduction</b><br>
One of the scenes most often singled out for special mention even in the briefest discussions of <i>Once Upon a Time in the West</i>, involves a fly and the legendary character actor Jack Elam, the wall-eyed heavy who was aptly described as:</p>
<blockquote><p>grizzled and stringy-haired and one of his eyes always seemed to be trying to roll around so it could look behind his head. He was the sort of shifty character who might shoot the family dog or dunk a bawling baby in hot water just for kicks. <a name="akr01" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#fn1">[1]</a></p></blockquote>
<p> In this film his character's name is "Snakey," which suitably evokes the reptilian quality of the part.</p>
<p>The extraordinary set-piece with the fly begins about six minutes into the opening credit sequence after the three gunmen, played by Elam, Woody Strode and Al Mulock, have taken over an isolated railroad station and are waiting for a train to arrive.</p>
<p>Elam is seated in a rocking chair on the porch of the station. His face looks chronically unwashed and is covered with beard stubble. Having just been annoyed by the ticker-tape noise of a nearby telegraph, he has reached over and ripped the wires out of the machine to silence it for good, and has now pulled his hat down over his eyes, trying to take a nap. Standing under a water tower and with his hat removed in order to fan himself with it, Woody Strode ("Stony") feels drops of water landing on his head, each drop making a loud splash as it hits his pate.  He replaces the hat on his head, so that the drops that continue to fall now land just as audibly on his hat. And not far away, under the open sky, Al Mulock ("Knuckles") passes the time by cracking his knuckles as he waits for the train.</p>
<p>It is at this point that a fly lands on Elam's neck and Elam opens his eyes and tries unsuccessfully to blow the fly away, by directing his breath in its direction. After it has resettled near his lip, he finally shoos it off his face with a wave of his hand. He then turns to see where it has landed - on an adjacent wooden surface. With pistol now in hand, Elam slyly waits a moment, then rapidly turns and slamming the muzzle up against the wooden surface, captures the fly in the barrel of his gun. With an index finger blocking the muzzle so that the fly can't escape, Elam is visibly pleased with his exploit and holds the gun up to his ear, listening to the fly's desperate buzzing. He then looks down at the barrel of the gun with his one good eye, opening it exceptionally wide. And after holding the gun barrel to his ear once again to hear the buzzing of the imprisoned fly, he finally lets it go as the train approaches, by removing his finger from the muzzle and waving the gun in the air. </p>
<p>It is after this bit of action that the last of the opening credits appears on screen as the train pulls into the station, and "Snakey" soon has his often-quoted dialogue with Harmonica (Charles Bronson):</p>

<blockquote><p>
Harmonica: Where's Frank?<br>
Snakey: Frank sent us.<br>
Harmonica: Did you bring a horse for me?<br>
Snakey (laughing): Looks like we� Looks like we're shy one horse.<br>
Harmonica (shaking his head no): You brought two too many.
</p></blockquote>


<p>This is followed by the shootout that leaves all three gunmen dead and Harmonica slightly wounded.</p>
<p>In order to complete the contextualization of the fly scene, the events preceding it should also be briefly summarized. At the very start of the film, when Elam, Strode and Mulock appear at the railroad station wearing their long "duster" coats, Elam soon grabs the toothless old station agent by the neck and pushes him into what is presumably the w.c., making a "shhhh" gesture with index finger and mouth, then signaling to Mulock to close the door. As that door slams closed, the screen goes black and the first credit - A SERGIO LEONE FILM - appears. Many of the remaining credits appear as the three gunmen take up their respective positions while waiting for the train to arrive.</p>
<p>The entire opening sequence, which was clearly inspired by the start of <i>High Noon</i> as has often been pointed out, runs about 12 minutes up to the end of the gunfight, and was shot in Spain. The scene with Jack Elam and the fly was first attempted by placing a fake fly on the actor's face. When that didn't work, better results were achieved by smearing honey or jam on Elam's beard to attract flies kept in a jar just out of view and released one at a time. <a name="akr02" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#fn2">[2]</a> </p>

<p><b>The origins of the fly scene</b><br>
The treatment of <i>Once Upon a Time in the West</i> was written by Dario Argento, Bernardo Bertolucci and Sergio Leone. However, the idea for the fly scene was not conceived until a later point and by Sergio Donati when he and Leone held brainstorming sessions prior to the writing of the screenplay. In response to my questions about the origins of the fly scene, Sergio Donati graciously replied:</p>

<blockquote><p>forty years have now passed since Sergio and I, locked in a room, "told" each other images for the film. The fly episode was certainly born during those days and in effect, I believe it was my idea; Sergio had Jack Elam in mind, and the idea of those wall-eyes fixed upon the barrel of the gun with the fly imprisoned, appeared to me very ironically "Leonesque"! <a name="akr03" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#fn3">[3]</a> </p></blockquote>

<p>This recent statement is a perfect supplement to an earlier interview in which Sergio Donati is quoted as saying:</p>

<blockquote><p>So I stayed with Sergio [Leone] for two weeks, together, to make the skeleton, the outline, to tell each other the scenes very clearly. [�] I never met Bertolucci and Argento at that time. The story they produced was not so gigantic. It was eighty pages. Then I wrote the whole script in twenty-five days, I think. Working like hell, scarcely getting up from my seat. And I had to rewrite just two things. If you read the shooting script, everything was shot exactly as in the script. Including the fly at the station.</p>
<p>And as the interviewer explains: "The reason Sergio Donati emphasizes 'the fly' is that Dario Argento subsequently claimed this aspect of the opening sequence as his idea." <a name="akr04" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#fn4">[4]</a> </p></blockquote>

<p><b>The fly scene in the screenplay</b><br>
NB. The screenplay for <i>Once Upon a Time in the West</i> has never been published, and I am grateful to Sergio Donati for generously making the relevant pages (18-21) available to me for publication here. Though I take full responsibility for the translation, I wish to thank Francesco Caviglia, Flemming Forsberg and Alexander Forsberg for their assistance, and Roberto Trapanese, Lars �lgaard and Filippo Ciampini for kindly facilitating contact with Sergio Donati.</p>




<div>
<table>
<tbody><tr><td>
96-98-<br>Zzzz, a fly buzzes obstinately around the face of Snakey, who doesn't move but only follows the insect intently with his eyes.
  
</td></tr>
<tr><td>
99-100-<br>The fly lands on the wooden panel near Snakey's head. And suddenly, flashing like the tongue of a chameleon, Snakey's right hand quickly grabs the pistol and presses it against the panel.
  
</td></tr>
<tr><td>
101 - <br>The opening of the barrel is resting on the wall, in ECU [extreme close-up], and Snakey puts his ear against [the barrel] and with him we hear 
  
</td></tr>
<tr><td>
THE BUZZING OF THE FLY IMPRISONED IN THE BARREL

</td></tr>
<tr><td>

102 -<br>Snakey reveals his gapped teeth in a smile. Carefully, he removes the pistol from the wall, blocking the opening with a finger, and approaches the barrel to his ear, listening with amusement to the furious buzzing of the imprisoned fly.
</td></tr>
<tr><td <td="">
OVER THE BUZZING OF THE FLY IS SUPERIMPOSED FOR A MOMENT THE VERY DISTANT WHISTLE OF A TRAIN

</td></tr>
<tr><td>
103 - <br>Snakey's smile fades. His eyes focus in the distance, on the train tracks. He removes his finger from the barrel, the fly flies away.			
  
</td></tr>
<tr><td <td="">

A SECOND DISTANT WHISTLE.
</td></tr>
</tbody></table>
</div>





 
<p><b>Making sense of the scene</b><br>
In an effort to find meaning in the fly scene, commentators have devised two main approaches.</p>
<p>One involves attributing to this scene a specific, definable purpose within the plot of the film. For example, after describing the interaction of Elam and the fly in some detail, one commentator writes:</p>

<blockquote><p>At this moment, a train enters the station and Elam releases the fly. The object of the wait has arrived, and the victim of the dry run is no longer needed. The Man (Charles Bronson) appears behind the train and quickly guns down the three killers. Elam's gunman may have gotten the best of the fly, but in the real event he is unable to escape death. The intrusion of the fly serves to heighten the tension of the approaching showdown; the annoyance of the visitor foreshadows a much more dangerous encounter. <a name="akr05" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#fn5">[5]</a></p></blockquote>

<p>While this is an admirable effort to make the fly scene meaningful in terms of plot, it could be argued that the scene tells us nothing about Snakey that we didn't already know from his treatment of the station agent, and nothing new that we need to know in order to understand and fully appreciate any subsequent event in the film, including the shoot-out with Harmonica. </p>
<p>The other and more common approach in the literature on the film is to link the buzzing of Snakey's fly to the drops of water splashing on Stony's head and hat, and the cracking of Knuckles' knuckles - this triad of the killers' sounds enmeshed within a broader sound montage which also includes a squeaking windmill, a slamming door, a quickly silenced telegraph, and the heavy chug of an approaching train. The fullest study of <i>Once Upon a Time in the West</i> characteristically deals with the fly scene primarily in a chapter devoted to "The Music of Sound and Dialogue," <a name="akr06" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#fn6">[6]</a> and in one way or another, music often becomes a key concept in discussions of the fly scene, as in the following delightfully extravagant assertion:</p>

<blockquote><p>Jack Elam suffers the loyal attention of one fly (I think we know this is an Italian fly) - such a fly, a Caruso of an insect - which he captures in the barrel of his pistol, where it sings the aria of a furious and neurotic bullet. <a name="akr07" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#fn7">[7]</a> </p></blockquote>

<p>And in another discussion, also with music as the central concept, the fly scene is taken as emblematic of Leone's filmmaking, even to the point of drawing a parallel between Elam and Leone:</p>

<blockquote><p>Leone is like the wall-eyed villain Jack Elam, who catches a persistent and ordinary fly in the barrel of his colt, and who smiles at the music produced by the insect inside the gun. Nothing is ordinary. You just have to know how to metamorphose flies into musical instruments. <a name="akr08" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#fn8">[8]</a> </p></blockquote>

<p>Intriguing as these claims may be, particularly since no musical theme was used in the opening sequence in order to let the montage of heightened diegetic sounds entirely fill the soundtrack, I believe that the real importance of the fly scene can best be understood in an entirely different perspective, more in keeping with the screenwriter's original inspiration (already cited above): "the idea of those wall-eyes fixed upon the barrel of the gun with the fly imprisoned, appeared to me very ironically 'Leonesque'!"</p>
<p>In contrast to the plot-related and largely sound-based approaches mentioned above, I would like to suggest that having a fly buzz around Elam's face and inside the barrel of his gun, provided a perfect opportunity to have the actor perform a series of facial gestures, some of which emphasized his bad eye, and for the filmmaker to lavish cinematic attention on that notorious physiognomy in action. Providing a pretext for Elam to enact that measured succession of largely underplayed grimaces and grins without speaking a word and for Leone to film them in prolonged close-ups, is the most important function of the fly in this scene. And I would argue that for the approximately 100-second duration of this remarkable set-piece, Jack Elam's face becomes the story. </p>
<p>Both an homage to and a send-up of Elam's ominous screen presence, simultaneously celebrating and parodying his evil look, this scene, conceived by Sergio Donati, is unlike anything ever seen before in any Western. It is also a perfect illustration of Leone's characterization of himself as "a director of gestures and silences. And an orator of images." <a name="akr09" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#fn9">[9]</a> </p>





</td></div><div><td>
<br><hr><br>
<blockquote>

<p>
<a name="fn1" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#akr01"><sup>1</sup></a>
Ron Miller, "Born to be booed, yet three earned Oscars." <i>The Columnists</i>, 14 Aug 2006. <br><a href="http://www.thecolumnists.com/miller/miller542.html">http://www.thecolumnists.com/miller/miller542.html</a> A childhood fight left Elam blind in his left eye.
</p>

<p>
<a name="fn2" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#akr02"><sup>2</sup></a>
  Cinematographer Tonino delli Colli's comments in the DVD bonus film <i>The Wages of Sin</i>, and those of Christopher Frayling on the audio commentary track of the <i>Once Upon a Time in the West</i> - <i>Special Collector's Edition</i> DVD (Paramount, 2004).
</p>


<p>
<a name="fn3" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#akr03"><sup>3</sup></a>
My translation of an email sent on May 27, 2007. 
</p>


<p>
<a name="fn4" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#akr04"><sup>4</sup></a>
Christopher Frayling, <i>Sergio Leone. Something To Do With Death</i> (London: Faber &amp; Faber, 2000), p. 265. 
</p>

<p>
<a name="fn5" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#akr05"><sup>5</sup></a>
Andrew Schenker, "Death, the Fly and Dickinson." <i>The Cine File</i>, 2 April 2007. <br>
<a href="http://aschenker.blogspot.com/2007/04/death-fly-and-dickinson.html">http://aschenker.blogspot.com/2007/04/death-fly-and-dickinson.html </a>
</p>

<p>
<a name="fn6" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#akr06"><sup>6</sup></a>
John Fawell, <i>The Art of Sergio Leone's</i> Once Upon a Time in the West. <i>A Critical Appreciation</i> (Jefferson, N.C.: McFarland, 2005).
</p>

<p>
<a name="fn7" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#akr07"><sup>7</sup></a>
David Thomson, "Leonesque." <i>American Film</i>, 14 (September 1989), p. 26.
</p>

<p>
<a name="fn8" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#akr08"><sup>8</sup></a>
Michel Mardore, "Vive le western!" <i>Le Nouvel Observateur</i>, 250 (25 August 1969), p. 35; my translation from the French.
</p>

<p>
<a name="fn9" href="https://pov.imv.au.dk/Issue_24/section_1/artc4A.html#akr09"><sup>9</sup></a>
Sergio Leone, "I'm a director of gestures and silences." <i>American Film</i>, 14 (September 1989), p. 31.
</p>

</blockquote>

<br><hr><br>
<blockquote>
<p><b>Bibliography</b>:</p>

<div>

<p>Canby, Vincent. "Once Upon a Time in the West." <i>New York Times</i>, 29 May 1969. </p>
<p>Carlson, Michael R. <i>Sergio Leone</i>. Harpenden: Pocket Essentials, 2001.</p>
<p>Daney, Serge. "Il �tait une fois dans l'ouest." <i>Cahiers du Cin�ma</i>, no. 216  (October 1969).</p>
<p>Ebert, Roger. "Once Upon a Time in the West." <i>Chicago Sun-Times</i>, 6 June 1969.</p>
<p>Fagen, Herb. <i>The Encyclopedia of Westerns</i>. New York: Facts on File, 2003.</p>
<p>Fawell, John. <i>The Art of Sergio Leone's </i>Once Upon a Time in the West<i>. A Critical Appreciation</i>. Jefferson, N.C.: McFarland, 2005.</p>
<p>Frayling, Christopher. <i>Sergio Leone. Something To Do With Death</i>. London: Faber and Faber, 2000.</p>
<p>Frayling, Christopher. <i>Spaghetti Westerns: Cowboys and Europeans from Karl May to Sergio Leone</i>. London: I. B. Tauris, 2006.</p>
<p>Hughes, Howard. <i>The Pocket Essential Spaghetti Westerns</i>. Harpenden:  Pocket Essentials, 2001. </p>
<p>Hughes, Howard. <i>Once Upon a Time in the Italian West: The Filmgoer's Guide to Spaghetti Westerns</i>. London: I. B. Taurus, 2005.</p>
<p>Laprevotte, Gilles. "Le western europ�en." 21e Festival International du Film d'Amiens.<br> <a href="http://www.filmfestamiens.org/archives/2001_Western_txt_GL.html">http://www.filmfestamiens.org/archives/2001_Western_txt_GL.html</a></p>
<p>Leone, Sergio. "I'm a director of gestures and silences." <i>American Film</i>, no. 14 (September 1989), p. 31.</p>
<p>Lomenzo, Elaine. "A fable for adults." <i> Film Comment</i> (1984).</p>
<p>Mardore, Michel. "Vive le western!" <i>Le Nouvel Observateur</i>, No. 250 (25 August 1969), pp. 34-35.</p>
<p>Miller, Ron. "Born to be booed, yet three earned Oscars." <i>The Columnists</i>, 14 August 2006. <br><a href="http://www.thecolumnists.com/miller/miller542.html">http://www.thecolumnists.com/miller/miller542.html</a> </p>
<p>Nicholls, David. "Once Upon a Time in Italy." <i>Sight and Sound</i> 50 (1980-1981), pp. 46-49.</p>
<p>Parish, James Robert and Michael R. Pitts, <i>The Great Western Pictures</i>. Metuchen, N. J.: Scarecrow Press, 1976.</p>
<p>PT, "Once Upon a Time in the West." <i>Time Out</i>, n.d.; http://www.timeout.com/film/66019.html </p>
<p>Schenker, Andrew. "Death, the Fly and Dickensen." <i>The Cine File</i>. 2 April 2007; <br><a href="http://aschenker.blogspot.com/2007/04/death-fly-and-dickinson.html">http://aschenker.blogspot.com/2007/04/death-fly-and-dickinson.html</a></p>
<p>Smith, Derek. "Once Upon a Time in the West." <i>Cinematic Reflections</i>, 5 Sept. 2004; http://www.cinematicreflections.com/OnceUponaTimeintheWest.html </p>
<p>Stevens, Chuck. "Once Upon a Time in the West." <i>The Village Voice</i>, 27 Sept. 2005; <br><a href="http://www.villagevoice.com/film/0539/tracking4/68248/20.html">http://www.villagevoice.com/film/0539/tracking4/68248/20.html</a> </p>
<p>Thomson, David. "Leonesque." <i>American Film</i> 14 (Sept. 1989), pp. 26-30, 56.</p>
<p>Toubiana, Serge. "Les secrets de Sergio Leone." <i>Cahiers du Cin�ma</i> 422 (July-August 1989).</p>
<p><i>Once Upon a Time in the West</i> - Special Collector's Edition DVD. Paramount, 2004.</p>


</div>


</blockquote>
<hr></td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Database mocks are not worth it (159 pts)]]></title>
            <link>https://www.shayon.dev/post/2024/365/database-mocks-are-just-not-worth-it/</link>
            <guid>42552976</guid>
            <pubDate>Mon, 30 Dec 2024 20:12:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.shayon.dev/post/2024/365/database-mocks-are-just-not-worth-it/">https://www.shayon.dev/post/2024/365/database-mocks-are-just-not-worth-it/</a>, See on <a href="https://news.ycombinator.com/item?id=42552976">Hacker News</a></p>
Couldn't get https://www.shayon.dev/post/2024/365/database-mocks-are-just-not-worth-it/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The Homa Network Protocol (105 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1003059/41b1d2ea281b6779/</link>
            <guid>42552887</guid>
            <pubDate>Mon, 30 Dec 2024 20:04:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1003059/41b1d2ea281b6779/">https://lwn.net/SubscriberLink/1003059/41b1d2ea281b6779/</a>, See on <a href="https://news.ycombinator.com/item?id=42552887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>
<p>
The origins of the TCP and UDP network protocols can be traced back a full
50&nbsp;years.  Even though networks and their use have changed radically
since those protocols were designed, they can still be found behind most
networking applications.  Unsurprisingly, these protocols are not optimal
for all situations, so there is ongoing interest in the development of
alternatives.  One such is the <a href="https://homa-transport.atlassian.net/wiki/spaces/HOMA/overview">Homa
transport protocol</a>, developed by John Ousterhout (of <a href="https://www.tcl.tk/">Tcl/Tk</a> and <a href="https://raft.github.io/">Raft</a> fame, among other accomplishments),
which is aimed at data-center applications.  Ousterhout is currently trying
to get <a href="https://lwn.net/ml/all/20241217000626.2958-1-ouster@cs.stanford.edu">a
minimal Homa implementation</a> into the kernel.
</p><p>
Most networking applications are still based on TCP, which was designed for
efficient and reliable transport of streams of data across a distributed
Internet.  Data-center applications, instead, are often dominated by large
number of small messages between many locally connected hosts.  The
requirements of TCP, including the establishment of connections and
ordering of data, add a lot of overhead to that kind of application.  The
design of Homa is intended to remove that overhead while taking advantage
of what current data-center networking hardware can do, with a focus on
minimizing the latency between a request and its response.
</p><h4>A quick Homa overview</h4>
<p>
At its core, Homa is designed for remote procedure call (RPC) applications;
every interaction on a Homa network comes down to a request and associated
reply.  A client will send a request message a server that includes a
unique request ID; the server will send a reply back that quotes that ID.
The only state that exists on the server is held between the receipt of the
request and the receipt of the response by the client.
</p><p>
Much of the key to the performance of this protocol can be found in how
these messages are handled.  There is no connection setup; instead, the
client starts transmitting the request, with no introductory handshake, to
the server.  There is a limit on how many bytes of this "unscheduled"
request data can be sent in this manner, which is determined by the
round-trip time of the network; it should be just high enough to keep the
request-transmission pipeline full until an initial response can be
received from the server side.  The figure of about 10,000 bytes appears in
some of the Homa papers.
</p><p>
The initial request packet includes the length of the full request.  If the
request does not fit into the size allowed for the unscheduled data, the
client will wait for a "grant" response before sending any more.  That
grant should, if the server is responding quickly, arrive just as the
initial request data has finished transmitting, allowing the client to
continue sending without a pause.  Grants include a maximum amount of data
that can be sent, and thus function like the TCP receive window.
</p><p>
This machinery is intended to get a request to the server as quickly as
possible, but without the need for much, if any, buffering in the network
path between the two machines.  Priority queues are used to manage this
traffic, with unscheduled packets normally having the highest priority.
Lower priorities are used for granted traffic; the requests with the least
amount of data remaining to be received are given the highest priority.
</p><p>
Once the server has received the full request and processed it, a response
is sent back to the client.  Once again, the initial bytes are sent as
unscheduled packets, with grants required for the rest if the response is
large enough.  In the earlier descriptions of the protocol, the server
would forget everything it knew about the request immediately after sending
the response.  That created the possibility that requests could be resent
(if the response ever arrives) and executed multiple times.  More recent
publications include an explicit acknowledgment message indicating that a
response has been received, with the sender retaining the necessary state
to retransmit a reply until that acknowledgment is received.
</p><p>
The details of the protocol are, of course, rather more complex than
described here.  There are, for example, mechanisms for clamping down on
the amount of unscheduled data sent if a server is finding itself
overloaded.  The receiving side of a message can request retransmission if
an expected packet does not arrive; unlike TCP and many other protocols,
Homa puts the responsibility for detecting lost packets onto the receiving
side.  There is also a fair amount of thought that has gone into letting
systems overcommit their resources by issuing more grants than they can
immediately handle; the purpose here is to keep the pipelines full even if
some senders do not transmit as quickly as expected.
</p><p>
See <a href="https://dl.acm.org/doi/10.1145/3230543.3230564">this paper</a>
for a more complete (and surely more correct) description of the Homa
protocol, <a href="https://github.com/PlatformLab/HomaModule/blob/main/protocol.md">this
page</a>, which reflects some more recent changes, and <a href="https://lwn.net/Articles/914030/">this 2022 article</a> for more details.
</p><h4>Homa on Linux</h4>
<p>
The Unix socket interface was designed around streams, and is not a perfect
fit for Homa, but the implementation sticks with it to the extent it can.
A <a href="https://man7.org/linux/man-pages/man2/socket.2.html"><tt>socket()</tt></a>
call is used to create a socket for communication with any number of other
systems; the <tt>IPPROTO_HOMA</tt> protocol type is used.  Homa can run
over either IPv4 or IPv6.  For server systems, a <a href="https://man7.org/linux/man-pages/man2/bind.2.html"><tt>bind()</tt></a>
call can be used to set up a well-known port to receive requests; clients
need not bind to a port.
</p><p>
Messages are sent and received, as one might expect, with <a href="https://man7.org/linux/man-pages/man2/sendmsg.2.html"><tt>sendmsg()</tt></a>
and <a href="https://man7.org/linux/man-pages/man2/recv.2.html"><tt>recvmsg()</tt></a>,
but there are some Homa-specific aspects that developers must be aware of.
When sending a message, an application must include a pointer to this
structure in the <tt>msg_control</tt> field of the <tt>msghdr</tt>
structure passed to <tt>sendmsg()</tt>:
</p><pre>    struct homa_sendmsg_args {
	uint64_t id;
	uint64_t completion_cookie;
    };
</pre>
<p>
If a request is being sent, <tt>id</tt> should be set to zero; the protocol
implementation will then assign a unique ID to the request (and write it
into <tt>id</tt>) before sending it to the server.  For a reply message,
<tt>id</tt> should be the ID value that arrived with the request being
responded to.  The <tt>completion_cookie</tt> value, which is only used for
requests, will be passed back to the caller with the reply data when it is
received.
</p><p>
The receive side is a bit more complicated, because Homa requires that the
buffer space for replies be registered before sending the first request on
a socket.  To do so, the process should allocate a range of memory, then
pass it into the kernel with <tt>SO_HOMA_RCVBUF</tt> <a href="https://man7.org/linux/man-pages/man2/getsockopt.2.html"><tt>setsockopt()</tt></a>
operation, using this structure:
</p><pre>    struct homa_rcvbuf_args {
	void *start;
	size_t length;
    };
</pre>
<p>
The <tt>start</tt> address must be page-aligned.  This memory is
split into individual buffers, called "bpages", each of which is
<tt>HOMA_BPAGE_SIZE</tt> in length; that size is 64KB in the current
implementation.  Each message will occupy at least one bpage; large
messages will be scattered across multiple, not necessarily contiguous,
bpages.
</p><p>
A message is received by making a call to <tt>recvmsg()</tt> with a
pointer to this structure passed in the <tt>msg_control</tt> field of
<tt>struct msghdr</tt>:
</p><pre>    struct homa_recvmsg_args {
	uint64_t id;
	uint64_t completion_cookie;
	uint32_t flags;
	uint32_t num_bpages;
	uint32_t bpage_offsets[HOMA_MAX_BPAGES];
    };
</pre>
<p>
The flags field describes what the caller is willing to receive; it is a
bitmask that can include either or both of <tt>HOMA_RECVMSG_REQUEST</tt> (to
receive request messages) and <tt>HOMA_RECVMSG_RESPONSE</tt> (to receive
responses).  If <tt>id</tt> is zero, then <tt>HOMA_RECVMSG_RESPONSE</tt>
will cause any response message to be returned; otherwise, only a response
corresponding to the provided request ID will be returned.  On return,
<tt>num_bpages</tt> will indicate the number of bpages in the registered
buffer area have been used to hold the returned message;
<tt>bpage_offsets</tt> gives the offset of each one.
</p><p>
The bpages returned by this call are owned by the application at this
point, and will not be used by the kernel until they have been explicitly
returned.  That is done with a subsequent <tt>recvmsg()</tt> call, where
<tt>num_bpages</tt> and <tt>bpage_offsets</tt> will indicate a set of
bpages to be given back.
</p><p>
This code has been "<q>stripped down to the bare minimum</q>" to be able to
actually transmit requests and responses across the net; it is evidently
about half of the full set of Homa patches.  The intent, of course, is to
ease the task of reviewing the work and getting initial support into the
kernel; the rest of the work can come later.  In its current form,
according to the cover letter, its performance "<q>is not very
interesting</q>", but that is expected to improve once the rest of the work
is merged.
</p><p>
See <a href="https://www.usenix.org/system/files/atc21-ousterhout.pdf">this
paper</a> for more information on the Linux implementation of Homa.
</p><h4>Prospects</h4>
<p>
The Homa protocol originates at Stanford University, with support from a
number of technology companies.  Academic work often does not successfully
make the transition from interesting prototype into production-quality code
that can be accepted into Linux.  In this case, though, Ousterhout seems
determined to get the code into the mainline, and is trying to do the right
things to get it there.  Thus far, the four postings of the code have
yielded some conversations about the protocol, but have not yet resulted in
a detailed review of the code.  That suggests that the initial merge of
Homa is not imminent.
</p><p>
It does seem likely to happen at some point, though.  Then, it will be a
matter of whether the operators of large data centers decide that it is
worth using.  Complicating that question is Ousterhout's assertion (in the
above-linked paper) that, even in a kernel with less overhead than Linux,
CPUs simply are not fast enough to keep up with the increases in networking
speed.  The real future for Homa, he suggests, may be inside the networking
hardware itself.  In that case, the merging into Linux would be an
important proof of concept that accelerates further development of the
protocol, but its use in real-world deployments might be limited.  It does,
in any case, show how Linux is firmly at the center of protocol development
for modern networks.<br clear="all"></p><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Kernel/Index">Kernel</a></td><td><a href="https://lwn.net/Kernel/Index#Networking-Protocols">Networking/Protocols</a></td></tr>
            </tbody></table><br clear="all">

               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple M5 could ditch unified memory architecture for split CPU and GPU designs (110 pts)]]></title>
            <link>https://www.notebookcheck.net/Apple-M5-Pro-Max-and-Ultra-could-ditch-much-vaunted-unified-memory-architecture-for-split-CPU-and-GPU-designs-fabbed-on-TSMC-N3E.937047.0.html</link>
            <guid>42552494</guid>
            <pubDate>Mon, 30 Dec 2024 19:18:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.notebookcheck.net/Apple-M5-Pro-Max-and-Ultra-could-ditch-much-vaunted-unified-memory-architecture-for-split-CPU-and-GPU-designs-fabbed-on-TSMC-N3E.937047.0.html">https://www.notebookcheck.net/Apple-M5-Pro-Max-and-Ultra-could-ditch-much-vaunted-unified-memory-architecture-for-split-CPU-and-GPU-designs-fabbed-on-TSMC-N3E.937047.0.html</a>, See on <a href="https://news.ycombinator.com/item?id=42552494">Hacker News</a></p>
Couldn't get https://www.notebookcheck.net/Apple-M5-Pro-Max-and-Ultra-could-ditch-much-vaunted-unified-memory-architecture-for-split-CPU-and-GPU-designs-fabbed-on-TSMC-N3E.937047.0.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Dumping Memory to Bypass BitLocker on Windows 11 (286 pts)]]></title>
            <link>https://noinitrd.github.io/Memory-Dump-UEFI/</link>
            <guid>42552227</guid>
            <pubDate>Mon, 30 Dec 2024 18:43:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://noinitrd.github.io/Memory-Dump-UEFI/">https://noinitrd.github.io/Memory-Dump-UEFI/</a>, See on <a href="https://news.ycombinator.com/item?id=42552227">Hacker News</a></p>
Couldn't get https://noinitrd.github.io/Memory-Dump-UEFI/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[US credit card defaults jump to highest level since 2010 (190 pts)]]></title>
            <link>https://www.ft.com/content/c755a34d-eb97-40d1-b780-ae2e2f0e7ad9</link>
            <guid>42552222</guid>
            <pubDate>Mon, 30 Dec 2024 18:41:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/c755a34d-eb97-40d1-b780-ae2e2f0e7ad9">https://www.ft.com/content/c755a34d-eb97-40d1-b780-ae2e2f0e7ad9</a>, See on <a href="https://news.ycombinator.com/item?id=42552222">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a data-trackable="a11y-skip-to-help" href="https://www.ft.com/accessibility">Accessibility help</a><a data-trackable="a11y-skip-to-navigation" href="#site-navigation">Skip to navigation</a><a data-trackable="a11y-skip-to-content" href="#site-content">Skip to content</a><a data-trackable="a11y-skip-to-footer" href="#site-footer">Skip to footer</a></p><div id="barrier-page"><div id="heroOffer-Hero offer-20e69883-7811-457d-adeb-3e5e02b7989f" data-component="heroOffer" data-component-unique-name="Hero offer"><div data-o-grid-colspan="12 L6"><p><span></span><span></span><span></span><span>Subscribe to unlock this article</span><span></span></p></div><div data-o-grid-colspan="12 L6"><p><h2><span>Try unlimited access</span></h2><h2><strong><span>Only </span><span>CHF1</span><span> for 4 weeks</span></strong></h2></p><p><span>Then </span><span>CHF85</span><span> per month.
Complete digital access to quality FT journalism on any device. 
Cancel anytime during your trial.</span></p></div></div><div id="recommendedOffers-Recommended offers-32a7b33c-c00a-473f-a105-83369a1cabc9" data-component="recommendedOffers" data-component-unique-name="Recommended offers"><p><h2 data-o-grid-colspan="12">Explore more offers.</h2></p><div data-o-grid-colspan="12"><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_standard.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p><h3>Standard Digital</h3></p></div><p><span>CHF55</span><span> per month</span></p><p><span>Essential digital access to quality FT journalism on any device. Pay a year upfront and save 20%.</span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_premium.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>CHF85</span><span> per month</span></p><p><span>Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.</span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_ftprofessional.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>Pay per reader</span></p><p><span>Complete digital access for organisations. Includes exclusive features and content.</span></p></div></div></div><div data-component="subscriptionOptions" data-component-unique-name="Subscription options"><h2>Explore our full range of subscriptions.</h2><div><div><p>Discover all the plans currently available in your country</p></div><div><p>Digital access for organisations. Includes exclusive features and content.</p></div></div></div><div data-component="whyFT" data-component-unique-name="Why FT"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft?ft-content-uuid=c755a34d-eb97-40d1-b780-ae2e2f0e7ad9">Find out why</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: John Friel my father, internet pioneer and creator of QModem, has died (989 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42551900</link>
            <guid>42551900</guid>
            <pubDate>Mon, 30 Dec 2024 18:11:47 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42551900">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="42552389"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552389" href="https://news.ycombinator.com/vote?id=42552389&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div><p>Qmodem was my favorite comm program during the BBS days, and it still is today when working with vintage computers. It was just nice to use. Its scripting language was the first I used and I find myself wishing there was a Linux comm program with scripting that worked that well. Long distance calls were expensive so I used a Qmodem script to call BBSs each morning to download my email before school.</p><p>Just the last several months I've been using Qmodem scripting to make thousands of modem calls over VoIP to test downloads to see which models and ATAs work best.</p><p>After I jumped back into the vintage BBS world I've been keeping an eye out for anything Qmodem. I recently just picked up a Qmodem manual on ebay that I wanted to scan and archive, because it's pretty rare to see.</p><p>Not too long ago I saw where John had posted to a FB group he was working on a new DOS version of Qmodem, my first interaction with him. I was excited to see it be worked on again and hoped to see the new version.  Sad to see him go.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552416"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552416" href="https://news.ycombinator.com/vote?id=42552416&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div><p>The amount of engineers your father directly or indirectly created are innumerable. Myself included.</p><p>I would’ve never discovered UNIX or the pre-web internet without software like and including QModem.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552205"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552205" href="https://news.ycombinator.com/vote?id=42552205&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div><p>QModem and then Telix were a window through which I explored another world as a young teenager with a budget modem with shaky MNP compatibility. In that world I eventually found friends, a wealth of knowledge, and a career. So thanks JF. RIP.</p><p>ATH0.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42552296"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42552296" href="https://news.ycombinator.com/vote?id=42552296&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div>
                  <p>The family of modem data transfer software back then had Kermit, xmodem, ymodem, zmodem, UUCP scripts, and pro-quality tools like QModem and Telix, as you mentioned. I'm sure I've left some other modem data transfer tools out. QModem had a certain polish and stability to it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552440"><td></td></tr>
                  <tr id="42552506"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552506" href="https://news.ycombinator.com/vote?id=42552506&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div>
                  <p>Condolences on your loss, Aaron &amp; family.   Few of us will ever write software with as much impact as QModem, I hope that your father found satisfaction in that reach.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552337"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552337" href="https://news.ycombinator.com/vote?id=42552337&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div><p>Only today I told my colleague that the first BBS connection was one of the most extraordinary experience I've had in my life. Of course, it wasn't possible without QModem software.</p><p>Thank you so much to your father for the happiest moments in my childhood!</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552214"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552214" href="https://news.ycombinator.com/vote?id=42552214&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div><p>I used QModemPro (after it was bought by Wildcat) and it was the best. It integrated with OLR (offline reader) which meant I could login to the BBS, download my messages via Zmodem as a compressed QWK file, and logoff.</p><p>It was so much more efficient than downloading plain text messages. Of course these days we no longer have bandwidth constraints like that but back in the day it enabled long discussions like the sort we’re having on HN today.</p><p>Qmodem wasn’t the only terminal emulator but it was the most professional one.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552136"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552136" href="https://news.ycombinator.com/vote?id=42552136&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div>
                  <p>The Qmodem program, brought home on some random 3.5 inch floppy, allowed me to connect to local BBSes and started my journey into networking computers. Now I have a PhD in CS and I spent more than a decade deeply caring about the Internet, networks, and network research.  Without the start given by those BBSes, my path could have been very different!  I am very sorry for your loss, and I hope the fact that he made a random teen's life better is some comfort.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552317"><td></td></tr>
            <tr id="42552362"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552362" href="https://news.ycombinator.com/vote?id=42552362&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div>
                  <p>Accept my condolence for your family in this time of sorrow. The QMODEM program was transformative in my life. Through USR robotics HST modem and QMODEM I was able to access a world far beyond the rural life in which I lived. The generousity of his shareware program probably touched more people than you will know.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552143"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552143" href="https://news.ycombinator.com/vote?id=42552143&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div>
                  <p>Hey Friel Family, I just wanted to reach out and say how sorry I am to hear about John’s passing. I didn’t know him or his work, but it’s clear he made a significant impact on many lives. Losing someone who has touched so many people is never easy. Even though I’m a stranger, I hope you find some comfort in the memories you shared and the love that surrounds you during this tough time. Sending you all my best wishes and support.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552535"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552535" href="https://news.ycombinator.com/vote?id=42552535&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div><p>Sorry for your loss. I was an avid Qmodem user back in the day.</p><p>Your father's software directly led to a lifetime passion for me. Dialing into a local BBS and being able to reach people around the world was, to me as a kid in the 80s, the single most magical thing imaginable.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552161"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552161" href="https://news.ycombinator.com/vote?id=42552161&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div>
                  <p>My condolences on your loss. I used QModem in the early 90s downloading shareware and the like. I just looked through my floppy disk holder and found a 3 1/2 inch floppy from 1992 with QModem on it :)
Your dad’s contribution to the BBS scene was huge and it was an important part of my own journey into computing.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42552238"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42552238" href="https://news.ycombinator.com/vote?id=42552238&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div><p>Add me to the list of early 90s QModem users! Just looked it up now and was reminded of the various download protocols like ZModem and Kermit. I haven't thought of those for years! When I used them, I don't think I even knew what a protocol was!</p><p>RIP JF.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42552491"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552491" href="https://news.ycombinator.com/vote?id=42552491&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div><p>I knew him personally. We worked on a few projects together in the late 90s/early 00s. He was a good man and I remember him trying to be a good father to you. I will miss him and his infectious laugh.</p><p>My condolences to you and your family.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552014"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552014" href="https://news.ycombinator.com/vote?id=42552014&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div>
                  <p>I only used QModem after it was purchased by Mustang Software but still have great memories! I was actually blown away by ZModem as a protocol when I first used it. How far we've come! This was a really interesting and innovative time period for computing and communications in general.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42552096"><td></td></tr>
                  <tr id="42552241"><td></td></tr>
            <tr id="42552365"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552365" href="https://news.ycombinator.com/vote?id=42552365&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div><p>I have a very early memory related to QModemPro (too young to remember the original).</p><p>Sorry for your loss, and grateful to your dad for his contributions.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552013"><td></td></tr>
            <tr id="42552268"><td></td></tr>
            <tr id="42552188"><td></td></tr>
            <tr id="42552216"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42552216" href="https://news.ycombinator.com/vote?id=42552216&amp;how=up&amp;goto=item%3Fid%3D42551900"></a></center>    </td><td><br><div>
                  <p>QModem was great software. I remember meeting many new friends from BBS days using that program. My condolences to you and your family.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42552102"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Performance of LLMs on Advent of Code 2024 (108 pts)]]></title>
            <link>https://www.jerpint.io/blog/advent-of-code-llms/</link>
            <guid>42551863</guid>
            <pubDate>Mon, 30 Dec 2024 18:09:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jerpint.io/blog/advent-of-code-llms/">https://www.jerpint.io/blog/advent-of-code-llms/</a>, See on <a href="https://news.ycombinator.com/item?id=42551863">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
        <header>
          
          

  <p>
    

    

    
      
      

      <span>
        
        
          6 minute read
        
      </span>
    
  </p>


        </header>
      

      <section itemprop="text">
        
        <p>In this post I’ll look at how LLMs performed on <a href="https://adventofcode.com/">Advent of Code 2024</a>.
Surprisingly, they didn’t perform nearly as well as I’d expect, which we’ll explore in this post.
Here are the main results:</p>



<p>All code to reproduce these experiments can be found <a href="https://huggingface.co/spaces/jerpint/advent24-llm/tree/main">here</a>, and the space itself with the results lives <a href="https://huggingface.co/spaces/jerpint/advent24-llm">here</a>.</p>

<h2 id="intro">Intro</h2>

<p>I recently <a href="https://www.jerpint.io/blog/advent-of-code-24/">wrote about my experience</a> with the 2024 advent of code challenge. 
Part of my challenge this year was not to use any LLMs, which I got really used to using in the last few years.
This was mostly a learning exercise for me to brush up on and learn more about leetcode-style problems.</p>

<p>As we all know by now, LLMs are particularly good at leedcode-style problems.
I think there’s a very small window of opportunity to evaluate the performance of LLMs on this competition before they inevitably scrape answers and 
have the chance to overfit to this new challenge.</p>

<p>I was mostly curious:</p>

<blockquote>
  <p>how well would someone do if they only used LLMs during the AoC challenge?</p>
</blockquote>

<p>Or asked differently:</p>

<blockquote>
  <p>How well will LLMs perform at actually never-before-seen problems?</p>
</blockquote>

<p>More importantly, I wanted to know how well would LLMs perform without any humans steering the responses. 
This mimics pretty well a scenario where specific requirements would be sent straight to an LLM without any human in the loop.</p>

<h2 id="setup">Setup</h2>

<p>I decided to test the models in a relatively simple framework.</p>

<ul>
  <li>
    <p>The model is given a prompt with the full problem description of both parts combined. 
This arguably makes it easier, since requirements change quite a bit from part one to part two (increasing the difficulty).</p>
  </li>
  <li>
    <p>The model must return a single script to be run and evaluated by me on the same inputs I had access to. To score a star, the answer had to be an exact match. 
This is very much in-line with AoC submissions, which don’t consider your code, just a properly submitted answer.</p>
  </li>
</ul>

<p>I used the same prompts accross all models. 
I did no attempt to do any prompt engineering, though I don’t think this will have that much impact in this case.
The problem descriptions themselves are already very clear and to the point-ish.</p>

<p>Here are the prompts:</p>

<div><pre><code><span>SYSTEM_PROMPT</span> <span>=</span> <span>"You are a programming assistant. You are solving the 2024 advent of code challenge."</span>
<span>PROMPT_TEMPLATE</span> <span>=</span> <span>"""You are solving the 2024 advent of code challenge.
You will be provided the description of each challenge. You are to provide the solution to each given challenge.
1) You can reason and explain your logic before writing the code.
2) You must write the code such that it can be parsed into an actual python file.
3) It will be parsed by the evaluator, so it must be valid python code.
4) All of the code must be in a single code block, delimited by ```python and ```.
5) To count as a proper submission, the code must print the result to each question asked.
6) Each question will have a single string as an answer. Make sure to print it that string, and nothing else.
7) The actual input to the question will be provided in a file relative to the python file, e.g. "./input.txt". You must read and parse from the file accordingly. You can safely assume the file will always be relative to the python file.

Here is an example of a proper submission:

You reasoning goes here ...

```python


file = "input.txt"

def your_function(...)
    ...

...
print(result1)


def your_other_function(...)
    ...

...
print(result2)

\```

Here is today's challenge description:
{problem_description}
"""</span>
</code></pre></div>

<p>I picked most of the SOTA models that mostly compare with each other on various coding benchmarks.</p>

<div><pre><code><span>all_models</span><span> </span><span>=</span><span> </span><span>{</span><span>
    </span><span>"openai"</span><span>:</span><span> </span><span>[</span><span>"gpt-4o"</span><span>],</span><span>
    </span><span>"gemini"</span><span>:</span><span> </span><span>[</span><span>"gemini-1.5-pro"</span><span>],</span><span>
    </span><span>"anthropic"</span><span>:</span><span> </span><span>[</span><span>"claude-3-5-sonnet-20241022"</span><span>],</span><span>
</span><span>}</span><span>
</span></code></pre></div>

<p>Of course, you could argue that the most advanced inference-time models aren’t on the list, 
but I also wanted to have a fair comparison for similar models.
I also think that while advent of code is difficult to me, it’s also much easier to people (and therefore LLMs) 
who have extensive experience in programming competitions (disclaimer: I definitely do not).</p>

<p>Therefore, I genuinely thought before running this experiment that this very selection of LLMs would perform really well out-of-the-box.</p>

<p>For each response, I then basically just parsed out the code blocks, ran the python file and collected <code>stdout</code> outputs from my machine.</p>

<p>I ran each model with a max timeout of 300 seconds (applied this to myself as well), which is very generous for most AoC problems (brute force is almost never the best or even possible solution, though lord knows I tried many a brute-force solve). 
If a model gives an error for whatever reason, this counts as a mistake.</p>

<p>Remember: The goal here is not to see how well a meat-bag can steer a model, but how well it can steer itself given only clear instructions on unseen problems.</p>

<h2 id="analysis">Analysis</h2>
<p>Here are the results:</p>



<p>Surprisingly, I did much better than the LLMs, this was something I was not expecting!</p>

<p>Some things to note with my current setup:</p>

<ul>
  <li>
    <p>A model has access to both problems at once, which in my opinion makes the solving easier (since problems tend to change and you can therefore think ahead).</p>
  </li>
  <li>
    <p>A model can only score as high as me: this is because I didn’t solve all problems, therefore I don’t have all solutions to evaluate the models on. 
Though you could argue a model could have scored one or two more stars, looking at the breakdown by day, there were very few instances where that might have actually happened (remember that part 1 needs to be solved in order to attempt part 2).</p>
  </li>
</ul>

<p>I really thought point 2) would be a problem, because I really thought the LLMs would perform really well, but there are only a few instances where it could have maybe made a difference in their score (e.g. problem 16 where I know im very close to the answer, I just needed to debug a little more).
This is because I have already solved all part 1s possible, so if a model would have successfully solved part 1 on a day I didn’t solve part 2, I didn’t have a way to evaluate it.
Of course, I could just start entering them manually to check, but I don’t want to inflate my scorecard.</p>

<p>So what’s going on? A few things:</p>

<ul>
  <li>
    <p>First, I suspect that we are seeing what many have pointed out over and over - these models are really good at using program templates for problems they’ve already seen, but not great at solving never-before-seen problems.
We might be seeing a change with test-time inference models, but that still requires A LOT of compute, as shown on the latest <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">arc benchmarks</a>.</p>
  </li>
  <li>
    <p>Second, a lot of the submissions had timeout errors, which means that their solutions might work if asked more explicitly for efficient solutions. 
However the models should know very well what AoC solutions entail, since it’s been going on for 10 years after all!</p>
  </li>
  <li>
    <p>Finally, some of the submissions raised some <code>Exceptions</code>, which would likely be fixed with a human reviewing this code and asking for changes.
You can also argue that an agentic setup with a critiquer might do better. This leads perhaps to the conclusion that these kinds of competitions could be good benchmarks for coding agents while they’re happening.
I’m sure that if the model had access to an interpreter and more inference-time compute they could probably fix some of those errors as well.</p>
  </li>
</ul>

<p>One thing to note is that I ran all of the prompts on December 26th, so there was basically no chance that models had been trained on submission code yet. 
I do suspect performance on AoC 24 will increase over time…</p>

        
      </section>

      

      


      
  


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YouTuber won DMCA fight with fake Nintendo lawyer by detecting spoofed email (249 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/12/youtuber-won-dmca-fight-with-fake-nintendo-lawyer-by-detecting-spoofed-email/</link>
            <guid>42550501</guid>
            <pubDate>Mon, 30 Dec 2024 16:04:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/12/youtuber-won-dmca-fight-with-fake-nintendo-lawyer-by-detecting-spoofed-email/">https://arstechnica.com/tech-policy/2024/12/youtuber-won-dmca-fight-with-fake-nintendo-lawyer-by-detecting-spoofed-email/</a>, See on <a href="https://news.ycombinator.com/item?id=42550501">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>Defending his livelihood, Neumayer started asking questions. At first, that led to his videos being reinstated. But that victory was short-lived, as the supposed Nintendo lawyer only escalated his demands, spooking the YouTuber into voluntarily removing some videos, The Verge reported, while continuing to investigate the potential troll.</p>
<h2>Reaching out directly to Nintendo helped, but questions remain</h2>
<p>The Verge has all the receipts, sharing emails from the fake lawyer and detailing Neumayer's fight blow-for-blow. Neumayer ultimately found that there was a patent lawyer with a similar name working for Nintendo in Japan, although he could not tell if that was the person sending the demands and Nintendo would not confirm to The Verge if Tatsumi Masaaki exists.</p>
<p>Only after contacting Nintendo directly did Neumayer finally get some information he could work with to challenge the takedowns. Reportedly, Nintendo replied, telling Neumayer that the fake lawyer's proton email address "is not a legitimate Nintendo email address and the details contained within the communication do not align with Nintendo of America Inc.’s enforcement practices."</p>
<p>Nintendo promised to investigate further, as Neumayer continued to receive demands from the fake lawyer. It took about a week after Nintendo's response for "Tatsumi" to start to stand down, writing in a stunted email to Neumayer, "I hereby retract all of my preceding claims." But even then, the troll went down fighting, The Verge reported.</p>
<p>The final messages from "Tatsumi" claimed that he'd only been suspended from filing claims and threatened that other Nintendo lawyers would be re-filing them. He then sent what The Verge described as "in some ways the most legit-looking email yet," using a publicly available web tool to spoof an official Nintendo email address while continuing to menace Neumayer.</p>
<p>It was that spoofed email that finally ended the façade, though, The Verge reported. Neumayer detected the spoof by checking the headers and IDing the tool used.</p>
<p>Although this case of copyright trolling is seemingly over, Neumayer—along with a couple other gamers trolled by "Tatsumi"—remain frustrated with YouTube, The Verge reported. After his fight with the fake Nintendo lawyer, Neumayer wants the streaming platform to update its policies and make it easier for YouTubers to defend against copyright abuse.</p>
<p>Back in May, when Ars reported on a YouTuber dismayed by a <a href="https://arstechnica.com/tech-policy/2024/05/washing-machine-chime-scandal-shows-how-absurd-youtube-copyright-abuse-can-get/">DMCA takedown over a washing machine chime</a> heard on his video, a YouTube researcher and director of policy and advocacy for the Electronic Frontier Foundation, Katharine Trendacosta told Ars that YouTube's current process discourages YouTubers from disputing copyright strikes.</p>
<p>“Every idiot can strike every YouTuber and there is nearly no problem to do so. It’s insane,” Neumayer said. “It has to change NOW.”</p>


          
                  </div></div>]]></description>
        </item>
    </channel>
</rss>