<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 05 Feb 2024 16:00:09 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[PostgreSQL UUID v7 support (101 pts)]]></title>
            <link>https://commitfest.postgresql.org/47/4388/</link>
            <guid>39260614</guid>
            <pubDate>Mon, 05 Feb 2024 12:39:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://commitfest.postgresql.org/47/4388/">https://commitfest.postgresql.org/47/4388/</a>, See on <a href="https://news.ycombinator.com/item?id=39260614">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<table>
	  <thead>
	    <tr>
	      <th>When</th>
	      <th>Who</th>
	      <th>What</th>
	    </tr>
	  </thead>
	  <tbody>
	    	  
	    <tr>
	      <td>2024-02-01 18:16:38</td>
	      <td>vigneshwaran C (vignesh.postgres)</td>
	      <td>Closed in commitfest 2024-01 with status: Moved to next CF</td>
	    </tr>
	    	  
	    <tr>
	      <td>2024-01-25 12:05:56</td>
	      <td>Aleksander Alekseev (a.alekseev)</td>
	      <td>New status: Ready for Committer</td>
	    </tr>
	    	  
	    <tr>
	      <td>2024-01-24 13:34:23</td>
	      <td>Aleksander Alekseev (a.alekseev)</td>
	      <td>New status: Needs review</td>
	    </tr>
	    	  
	    <tr>
	      <td>2024-01-24 12:31:44</td>
	      <td>Aleksander Alekseev (a.alekseev)</td>
	      <td>New status: Waiting on Author</td>
	    </tr>
	    	  
	    <tr>
	      <td>2024-01-22 17:00:28</td>
	      <td>Przemysław Sztoch (psztoch)</td>
	      <td>Added psztoch as reviewer</td>
	    </tr>
	    	  
	    <tr>
	      <td>2024-01-22 15:03:08</td>
	      <td>Aleksander Alekseev (a.alekseev)</td>
	      <td>New status: Ready for Committer</td>
	    </tr>
	    	  
	    <tr>
	      <td>2024-01-22 15:03:02</td>
	      <td>Aleksander Alekseev (a.alekseev)</td>
	      <td>Added a.alekseev as reviewer</td>
	    </tr>
	    	  
	    <tr>
	      <td>2024-01-22 04:24:31</td>
	      <td>Nikolay Samokhvalov (nikolay)</td>
	      <td>Posted review with messageid &lt;170589747167.1131.401023316165084490.pgcf@coridan.postgresql.org&gt;</td>
	    </tr>
	    	  
	    <tr>
	      <td>2024-01-22 04:22:37</td>
	      <td>Nikolay Samokhvalov (nikolay)</td>
	      <td>Added nikolay as reviewer</td>
	    </tr>
	    	  
	    <tr>
	      <td>2023-12-04 10:06:18</td>
	      <td>John Naylor (john.naylor)</td>
	      <td>Closed in commitfest 2023-11 with status: Moved to next CF</td>
	    </tr>
	    	  
	    <tr>
	      <td>2023-10-09 10:15:45</td>
	      <td>Chris Travers (einhverfr)</td>
	      <td>Posted comment with messageid &lt;169684654521.1130.7798729884309122343.pgcf@coridan.postgresql.org&gt;</td>
	    </tr>
	    	  
	    <tr>
	      <td>2023-10-09 10:06:02</td>
	      <td>Chris Travers (einhverfr)</td>
	      <td>Added einhverfr as reviewer</td>
	    </tr>
	    	  
	    <tr>
	      <td>2023-10-02 09:59:16</td>
	      <td>Peter Eisentraut (petere)</td>
	      <td>Closed in commitfest 2023-09 with status: Moved to next CF</td>
	    </tr>
	    	  
	    <tr>
	      <td>2023-07-06 12:24:27</td>
	      <td>Daniel Gustafsson (d_gustafsson)</td>
	      <td>Closed in commitfest 2023-07 with status: Moved to next CF</td>
	    </tr>
	    	  
	    <tr>
	      <td>2023-06-22 17:30:27</td>
	      <td>Nikolay Samokhvalov (nikolay)</td>
	      <td>Changed authors to Andrey Borodin (x4m), Kirk Wolak (kirkw)</td>
	    </tr>
	    	  
	    <tr>
	      <td>2023-06-22 17:30:27</td>
	      <td>Nikolay Samokhvalov (nikolay)</td>
	      <td>Changed targetversion to 17</td>
	    </tr>
	    	  
	    <tr>
	      <td>2023-06-22 17:29:56</td>
	      <td>Nikolay Samokhvalov (nikolay)</td>
	      <td>Attached mail thread CAAhFRxitJv=yoGnXUgeLB_O+M7J2BJAmb5jqAT9gZ3bij3uLDA@mail.gmail.com</td>
	    </tr>
	    	  
	    <tr>
	      <td>2023-06-22 17:29:56</td>
	      <td>Nikolay Samokhvalov (nikolay)</td>
	      <td>Created patch record</td>
	    </tr>
	    
	  </tbody>
	</table>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: How many of you are self employed? (175 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39259602</link>
            <guid>39259602</guid>
            <pubDate>Mon, 05 Feb 2024 10:38:41 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39259602">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="39259602">
      <td><span></span></td>      <td><center><a id="up_39259602" href="https://news.ycombinator.com/vote?id=39259602&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=39259602">Ask HN: How many of you are self employed?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_39259602">103 points</span> by <a href="https://news.ycombinator.com/user?id=asim">asim</a> <span title="2024-02-05T10:38:41"><a href="https://news.ycombinator.com/item?id=39259602">3 hours ago</a></span> <span id="unv_39259602"></span> | <a href="https://news.ycombinator.com/hide?id=39259602&amp;goto=item%3Fid%3D39259602">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20How%20many%20of%20you%20are%20self%20employed%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=39259602&amp;auth=f2676248bd51fc9a0a9af7d831aea20ccb503ae5">favorite</a> | <a href="https://news.ycombinator.com/item?id=39259602">97&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><p>How many of you are self employed? What do you do? I won't count startups that are funded, but more the individual who started something for themselves. Curious to know what sustains people.</p></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table><table>
            <tbody><tr id="39260025"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260025" href="https://news.ycombinator.com/vote?id=39260025&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I've been self-employed for the last ~18 months. In the past I started two VC-backed startups, then was a PM at some later-stage companies before returning to entrepreneurship.<p>This time around, I'm building a solo "digital product studio" [1] instead of a startup. So, I'm staying one person, haven't raised money, and have multiple revenue-generating products. Product revenue doesn't cover my costs yet, so I do occasional consulting to bridge the gap.</p><p>I like the flexibility of this lifestyle. I'm based in NYC, but writing this from Tokyo where I've been doing a creative residence for the past two weeks.</p><p>And, a fun technicality - I truly self-employed in the sense that I have a salary and a payroll system. This is because my company is registered as an S-Corp in the USA, which requires the owner to be on a salary.</p><p>[1] <a href="https://www.contraption.co/essays/digital-product-studios/" rel="nofollow">https://www.contraption.co/essays/digital-product-studios/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39261223"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39261223" href="https://news.ycombinator.com/vote?id=39261223&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>I have a similar story. I’ve been self-employed for the last 6 months after 14 years of startups. Started out as an engineer in TN, and moved to SF a couple years ago where I got into management just in time to help with rounds of layoffs. Got some severe burnout, quit, and now I’m working on my own products while doing consulting/contracting on the side through my agency. I live fulltime in an RV traveling the country and working remotely. At the moment, I never want to work for someone else again.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39260152"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260152" href="https://news.ycombinator.com/vote?id=39260152&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>This is super cool! I was/am a startup founder too, bootstrapped an open source project, raised VC money, but ultimately it didn't work out. Now thinking about a solo business again and the studio model was exactly what I was looking at. How did you decide what to build?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39260268"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260268" href="https://news.ycombinator.com/vote?id=39260268&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>My first product Postcard [1] is a personal website builder. I built that because I thought a personal website maker made logical sense as an indie business, and I wanted a way to stay in touch with friends after I deleted social media. But, it's hard to scale  - user acquisition is the name of the game, willingness to pay is lower, the product isn't super differentiated, and I don't have a lot of experience in B2C. So, it makes money and continues to grow - but is small in absolute terms.<p>I decided to work on a more ambitious project that I had been thinking about for years, inspired by a product I wanted at my last startup, and by the chaos of being in 100+ slack channels at a previous job. I hate how people use Discord + Slack - they're good for urgent communications, but we need a "low-attention" version for important communications in communities and at work. So, I started building this product last Summer.</p><p>Booklet is an async, newsletter-first community platform [2]. Something like Google Groups with the polish of Slack. Booklet's far more complex than Postcard, but plays into my skills more - such as complex permissions systems, email marketing infrastructure, B2B sales, and being able to incorporate all the latest OpenAI goodies quickly. I launched it about three months ago, it has revenue, and I've been scrambling to build things in response to user feedback. I'm thinking of doing a bit of a relaunch next week as some foundational flows come together, such as full PWA support, search, and Stripe member sync.</p><p>Coincidentally - the project I launched to dogfood Booklet, called FRCTNL [3], is doing quite well. I had no intention of monetizing it, but I included a referral link to the accounting service I use. People have been using that referral link, and last quarter FRCTNL was my highest-earning product.</p><p>I'm sure in a few years I will have some great stories after the fact about the lessons I was missing in plain sight. But, things feel a little chaotic, uncertain, and fun at the moment. The core theme is building things that I want. My main insight so far has been to build unique, differentiated products that I want to use myself.</p><p>[1] <a href="https://postcard.page/" rel="nofollow">https://postcard.page</a></p><p>[2] <a href="https://booklet.group/" rel="nofollow">https://booklet.group</a></p><p>[3] <a href="https://frctnl.xyz/" rel="nofollow">https://frctnl.xyz</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260465"><td></td></tr>
                        <tr id="39260564"><td></td></tr>
                <tr id="39260698"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260698" href="https://news.ycombinator.com/vote?id=39260698&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>Thanks! I'm trying to do more essay writing there [1]. Most indie makers seem to have some content strategy where they share their story. I avoid social media and don't desire to become a Youtuber, so I'm focusing on writing as a content channel. I'm focusing essays on my creative process, inspiration, and specific experiences as an operator - the stuff I want to read. I'm avoiding prospective predictions about the future, being negative (an actual self-imposed rule), and talking about things that feel more like theory than practice.<p>The next piece I plan to publish tomorrow will be a recap of a talk I gave over the weekend, covering how most of our knowledge work practices come from factories, and ways software engineers are at the forefront of changing those industrial-era practices.</p><p>P.S. - if you're ever in NYC, come join for a dinner of Dimes Square Ventures [2], which is a little community I run for local indie makers (using Booklet!)</p><p>[1] <a href="https://contraption.co/essays" rel="nofollow">https://contraption.co/essays</a></p><p>[2] <a href="https://dimessquareventures.com/" rel="nofollow">https://dimessquareventures.com</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39260446"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260446" href="https://news.ycombinator.com/vote?id=39260446&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>In Poland, where I'm from, it's pretty much the norm to be self-employed in Poland in the IT sector.<p>I believe that around 50% of workers are technically contracted one-man companies, and this percentage is inversely correlated with the seniority level - the greater the earnings (and the sense of job security that goes along with expertise and experience), the greater the incentive.</p><p>Going B2B makes a substantial difference in terms of fiscal burdens. Other than that, your day-to-day work looks pretty much the same though. You're just sending monthly invoices to the same employer, typically a single one, sometimes for years on end.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260607"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260607" href="https://news.ycombinator.com/vote?id=39260607&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>That's for tax reasons only though, technically it is just another form of long term employment. As soon as you have multiple customers that you send invoices to with some regularity and you have autonomy would you pass the 'self employed' test in other countries. If you refer to them as your client and you only have one then you're technically an employee, if you refer to your contact at your client as 'your boss' then you also are an employee.<p>We have a lot of this in NL as well, the long term effect is the slow erosion of the social safety net. Because good luck if your client decides they no longer need your services, suddenly you find out what the downside of being self employed is. Nothing to fall back on. So save like your life depends on it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39261023"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39261023" href="https://news.ycombinator.com/vote?id=39261023&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I can't see why a company couldn't have one long-term customer. It's not unusual eg. in the building sector (where large construction projects, not unlike software projects, can take years to complete).<p>Another common example is MDs - quite a lot of private doctor's offices are contracted by the National Health Fund, basically providing their services for the public healthcare that way.</p><p>Clearly having multiple customers isn't a reasonable requirement for a small company. Software engineering isn't like private residential plumbing - "sink drain unclogged, next please!" :)</p><p><i>&gt; good luck if your client decides they no longer need your services, suddenly you find out what the downside of being self employed is.</i></p><p>That goes without saying, and the same obviously applies if you're running a "regular" company, with employees, like a restaurant or whatever.</p><p>The risk is arguably even greater, as you will usually pile up some financial obligations (such as credits) and other commodities limiting your financial fluidity (remember Covid? Restaurant owners do).</p><p>By the way, you can insure yourself against loss of income. Many insurance companies offer this service.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39261113"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39261113" href="https://news.ycombinator.com/vote?id=39261113&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>&gt; By the way, you can insure yourself against loss of income. Many insurance companies offer this service.<p>In the US, I am not familiar with insurance for loss of income due to simply not being able to sell products/services.  Usually, the loss of income has to be a result of covered natural disasters, vandalism, legal issues, etc.  Most business insurance policies even specifically exclude pandemics, as many found out recently.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39261142"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39261142" href="https://news.ycombinator.com/vote?id=39261142&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>What benefit does that provide? Some commenters are saying it reduces taxes. Why would it reduce taxes? Why would the government reduce taxes for people who jump through these hoops?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39260497"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260497" href="https://news.ycombinator.com/vote?id=39260497&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>This would be <i>quite</i> risky where I am from, for both the freelancer and the employer. Being self employed, but only for a single customer, is false self-employment. If you get caught, your employer has to pay taxes and social security contributions retroactively for up to 4 years, and afaik both the employer <i>and</i> the freelancer are liable for the money owed to the tax office and social securities. If you are caught doing this premeditated, it might be a criminal offence.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39260594"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260594" href="https://news.ycombinator.com/vote?id=39260594&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>This risk does exist in Poland, but among the criteria for "false self-employment" is being expected to work within fixed hours, and in a location specified by the employer.<p>As IT workers - even those who've got contracts of employment - typically do flexible hours, and pretty often work from home (or otherwise remotely), it doesn't really apply.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260542"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260542" href="https://news.ycombinator.com/vote?id=39260542&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>If you're from a country like Germany, most of what you're skipping financially is the insurance cost that's covered by the employer (social security etc.)
Which means you don't have access to the social safety in case you need it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39261211"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39261211" href="https://news.ycombinator.com/vote?id=39261211&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>In Poland(since I'm also from here), biggest chunk of savings when working as self-employed is income tax and mandatory public retiremend fund.  
Most Software Engineers go B2B/self-employment route because not only taxes are lower, but savings from paying minimal public retirement are pretty big with higher salaries. We're at a point where most people in Poland do not believe in retirement system anymore, it's unsustainable and will crash or will be kept with social, minimal retirement, no matter what you paid in. That's why most of us want to save on the side, put that into ETFs or housing rather than count on the government, especially that over last decade or so almost destroyed this country.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39260657"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39260657" href="https://news.ycombinator.com/vote?id=39260657&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>If you earn enough, the savings can easily surpass the risk for a singular person.<p>It's worse for society overall, but let's not pretend the high earners don't mostly subsidize the safety net.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260732"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39260732" href="https://news.ycombinator.com/vote?id=39260732&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>The IT sector in Poland (again) is a booming one, and most of the services are effectively exported, as we're talking outsourcing.<p>I guess it translates into the lack of political will to curb the practice, too, as the government would risk that self-employed programmers (and whatnot) may not fall in line, but mostly log out of the system instead, registering their companies abroad, for example. I guess the government isn't eager to start this cat-and-mouse game, preferring a smaller slice of a bigger pie.</p><p>There are even some extra incentives on the top of the flat income tax rate (which is an option for all one-man companies, regardless of the sector)...</p><p>Like the "IP Box" tax relief, which drives the income tax rate from the regular 19% all the way down to 5%, as long as you get your services classified as "research and development". It takes some patience and loads of legal paperwork, which you obviously have to pay for (the latter, that is), but it's well worth it in the end.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39261145"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_39261145" href="https://news.ycombinator.com/vote?id=39261145&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>&gt;registering their companies abroad, for example.<p>"Registering" the business only is a trap and a way to pay much much more _when_ the tax office decides you are avoiding taxes. To truly do that, you need to move your center of life abroad, which generally means being outside Poland for 183 days a year.</p><p>&gt;I guess the government isn't eager to start this cat-and-mouse game, preferring a smaller slice of a bigger pie.</p><p>I agree that it mostly is the case, as I still pay few times more taxes (and VAT on consumption...) than average citizen, even when using 12% lump sum tax.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="39260721"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260721" href="https://news.ycombinator.com/vote?id=39260721&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>&gt; Being self employed, but only for a single customer, is false self-employment<p>This is exactly what happens in Poland and everyone involved feels very smart for cheating the system. That's also why software from Poland is such a tacky crap despite so many "talents". The software professionals have no leverage to push back, they only can walk away. The irresistibile benefit is that one can write off buying a car into operating costs, so the dream of PREMIUM GERMAN CAR prevails over doing anything creative.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260852"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39260852" href="https://news.ycombinator.com/vote?id=39260852&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>&gt; That's also why software from Poland is such a tacky crap despite so many "talents"<p>[citation needed]</p><p>How do you recognize "software from Poland" anyway?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39261000"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39261000" href="https://news.ycombinator.com/vote?id=39261000&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>&gt; How do you recognize "software from Poland" anyway?<p>Whatever is created in all these nearshoring and outsourcing centers in Warsaw, Kraków, and Wrocław. Currently mostly Azure, SAP, business Java and Angular.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39261029"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_39261029" href="https://news.ycombinator.com/vote?id=39261029&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>Yeah, but how do you know which application was and which wasn't? Apart from some big customers, it usually isn't public knowledge. Eg. my previous project was a banking app for a rich Middle Eastern country. There's no "acknowledgements" section in the app.<p>My original question still stands - by what metrics do you regard "software from Poland" as "tacky crap"? I'm not being belligerent about it, but somewhat curious, sure.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39261047"><td></td></tr>
                                          <tr id="39260670"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260670" href="https://news.ycombinator.com/vote?id=39260670&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>My home country also regulates this scenario - one-man companies with ~single client - for businesses strongly. This is hidden employment. How would it be beneficial to anyone?<p>- the subcontractor doesn't get any social security. Has to provide everything for himself from the private sector. And pensions (however meager). Theoretically he's free to have multiple clients or vary prices but I guess for most this is a pipe dream and they are dependent. For some tax dodging he gives up the whole legal safety net of being employed. Based on your contract you are freely exchangeable.</p><p>- the contracting company has more freedoms with getting/tossing employees, although loses a safety net of subcontractor suddenly leaving or changing prices.</p><p>- the government loses oversight of actual corporate structures.</p><p>Instead of fixing the flaws in the social system, hidden employment just throws it in the bin because haha less taxes, more money and freedom.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260841"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260841" href="https://news.ycombinator.com/vote?id=39260841&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>&gt; How would it be beneficial to anyone?<p>Most programmers provide outsourcing services for companies from abroad (including outside of the EU).</p><p>Low taxes help them - and the umbrella companies AKA software houses - to remain competitive on the global market.</p><p>So even if they only pay a smaller fraction of their income to the budget, it's still better than if they didn't get the gig to begin with, because the contracts would go elsewhere.</p><p>&gt; the subcontractor doesn't get any social security.</p><p>You do get healthcare insurance in Poland; no difference here. You're paying those fees just the same way.</p><p>You're only required to pay minimum pension charges though, so you have to take care of that yourself. (Objectively speaking, investing your savings in the pension system, of all places, probably isn't an optimal strategy anyway. At any rate, noone stops you from paying more than you're legally required, if you think that it is).</p><p>&gt; the contracting company has more freedoms with getting/tossing employees, although loses a safety net of subcontractor suddenly leaving or changing prices.</p><p>It's obviously a trade-off. Being able to let people go without fuss if a customer downscales their budget (I was on the receiving end of this last year) is a competitive advantage.</p><p>&gt; the government loses oversight of actual corporate structures</p><p>What do you mean by that?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39261049"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39261049" href="https://news.ycombinator.com/vote?id=39261049&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>My point is that this is a very wrong direction for any country but especially eastern europe.
It's like: welfare/health care system is bad, taxes are not used well. Top earning knowledge workers want an exit hatch, let's cater for them and they can hop off the welfare tax system. This way they can also provide cheap prices to foreign companies.<p>So many problems. These are just top off my head:</p><p>- Countries should want internally organised production, strong companies with own IP, not one-man "companies" producing IP to external entities.</p><p>- Those foreign companies will switch to other countries with better prices (Asia, Africa) any time if their programming scene improves. It's not like they have stakes like when building a factory.</p><p>- Lot of people think they can invest better, create a better future pension for themselves. This is often  true, and why would we want to allow exits, further eroding the whole system? There should be a base pension fund with everyone involved.</p><p>&gt; the government loses oversight of actual corporate structures</p><p>To the government the company could be a 5 person shell, while it actually employs/pays salaries of 100s of families. Theoretically you could roll up the contracts, but that would be very complicated.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="39260039"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260039" href="https://news.ycombinator.com/vote?id=39260039&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>Me! I started my solo, startup law practice almost by accident via a Hacker News comment years ago. It's now my primary source of income.<p>It's hard, but less hard than what startup founders do. It's nice having control of my schedule, but the flip side is that there's never a day off. Personally, I think being self-employed is great for people who naturally work really hard and want to capture the full output of their labor.</p><p>I don't think I could ever go back to full time employment for someone else. It's addicting having your own business that actually cash flows!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260610"><td></td></tr>
                  <tr id="39260927"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260927" href="https://news.ycombinator.com/vote?id=39260927&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I’ve been self employed full time for almost 20 years, with one to two employees during that time.  It’s a product company which I won’t link to because customers think we’re a big organization.<p>I worked nights and weekends for about 10 years on the products before they were good enough and selling enough that I could go full time.  I’ve focused heavily on not having technical debt, and making the app as user friendly as possible to cut down on customer support burden.</p><p>Customer support work on holidays and vacations is required, but only for an hour or two.  It’s been a pretty good gig.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260108"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260108" href="https://news.ycombinator.com/vote?id=39260108&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I'm self employed for around 10 years now. Before that I was employed as frontend developer for around 7 years.<p>I started self employment as software consultant, which worked pretty well despite not having any connections from my previous employment. I only needed one or two projects a year to sustain my lifestyle. Getting two companies a year to accept your application isn't hard. If you write at least 5 applications a month, you need a success rate of less than 5%.</p><p>I changed to technical writing later, because text is less of a struggle than code, and educational articles that explain how to use software (e.g., services, tools, SDKs, frameworks, etc.) are paid pretty well, especially compared to the non-technical writing tasks. Regular software consulting projects take months and can haunt you for years. An article takes a few days and after that you can do other things you find interesting.</p><p>Via technical writing, I got into other kinds of text related jobs the software industry offers, like social media management (e.g., Twitter/X, newsletters, blogs) for companies with developer audiences.</p><p>Usually I work less than 20h a week and I can do it from everywhere, which allows me to travel often and having enough time off to enjoy it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260237"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260237" href="https://news.ycombinator.com/vote?id=39260237&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>How do you get clients for tech writing, if I may ask? Are you maintaining a stable pool of companies and doing work as they need it, or do you always have to fish for new opportunities?
I've been authoring sw dev books that target a niche audience, and I've done some writing for the company that develops the sw itself. I'm wondering whether to expand to other areas, too—I've got the feeling that 2024 is going to be a rather challenging year, financially speaking.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39260635"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260635" href="https://news.ycombinator.com/vote?id=39260635&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>As all articles I wrote under my own name (some are ghost writing) are implicit advertisement, most clients approached me after they read some of my pieces.<p>Over the years I got a hand full of stable clients. Some want content every month, some every quarter, etc.</p><p>Then there is word of mouth. The people I work with at my client companies are usually in the content business, so they need constant influx of quality content  and know many other people who need it too.</p><p>I also work with a tech content agency, they always have a few articles a month I can work on, if business is slow.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39260169"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260169" href="https://news.ycombinator.com/vote?id=39260169&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I am. I founded PG Support (<a href="https://pgsupport.dk/" rel="nofollow">https://pgsupport.dk</a>).
We are a small team of experienced PostgreSQL consultants.<p>I really like having close contact to customers as well having a very high variance in the type of work that I do.</p><p>I also run DebianSupport (<a href="https://debiansupport.com/" rel="nofollow">https://debiansupport.com</a>) from which we provide professional services for Linux. Mostly Debian and Ubuntu but not exclusively so.</p><p>Having used Linux extensively since 1997 and PostgreSQL since 2000, I really like working with both pieces of technology. And I love being able to provide value from my skills and experience.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260236"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260236" href="https://news.ycombinator.com/vote?id=39260236&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>Me as well! I've started working at startups as an employee about ~6 years ago.<p>And did my first freelance project about 3 years ago. Mostly doing FullStack with a FE focus. From then I've been working on and off due to studies.</p><p>It certainly has its upsides and downsides. Here in Germany, freelancing allows one to somewhat escape generally low salary for employed software developers (as compared to the US). But what I don't enjoy is the distance to the actual issue/customer at hand. It's on average a much more corporate form of work.
And the last couple months have been very slow in terms of demand.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260113"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260113" href="https://news.ycombinator.com/vote?id=39260113&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>Would living on investment income be considered as self employed. I quit my management job of Tech department of my company.<p>Saved enough money to generate 80 percent of my last drawn salary from my investments.</p><p>Now focusing on creating my own fintech web app, with generative ai integration.</p><p>I like the feeling of working independently and years of corporate job had taken taken toll on mental health.</p><p>Currently building skills in fullstack web development and generative ai. 
Would take freelance job for some extra cash
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260472"><td></td></tr>
                <tr id="39260962"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260962" href="https://news.ycombinator.com/vote?id=39260962&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I am based out of India
70% of my portfolio is in equity mutual funds, and the rest in safe instruments.
The Indian stock markets have given good returns and my equity portfolio has been doing 17%  CAGR (10% inflation-adjusted)<p>But I have planned for conservative returns of 12 percent overall
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39260167"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260167" href="https://news.ycombinator.com/vote?id=39260167&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>I was self employed for about 10 years until one of the startups I did along the way turned into something and now I have to work there.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39260296"><td></td></tr>
                <tr id="39260634"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260634" href="https://news.ycombinator.com/vote?id=39260634&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>That's exactly how it goes though. You put something together on an idle Tuesday evening and you end up as the person running a 30 people company if you're not very careful about it. Success has a massive price tag.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="39260189"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260189" href="https://news.ycombinator.com/vote?id=39260189&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I am for a couple years now. I do consulting work [1] around Rust/Embedded Systems/Systems Engineering, usually helping teams that are either kicking off a project (so helping with planning, scoping, and backfilling knowledge gaps), or people who want help building a proof of concept (so: just build it for me asap) for existing companies exploring new ideas or for startups that are working on demos for investors, etc.<p>I've been active on both the open source and commercial side of embedded Rust since the beginning, so it's been fun to watch it grow both as an ecosystem as well as from a commercial adoption perspective. Doing consulting makes it easier to help do more open source docs + support in the open, and seems to be one of more sustainable ways to do OSS in my opinion, if you can swing it.</p><p>[1]: <a href="https://onevariable.com/" rel="nofollow">https://onevariable.com/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260051"><td></td></tr>
                <tr id="39260074"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260074" href="https://news.ycombinator.com/vote?id=39260074&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>Part time freelancing is underrated. More people should try it. That frees up a lot of time for indie hacking</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39260110"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260110" href="https://news.ycombinator.com/vote?id=39260110&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>Do you have any tips for starting out trying freelancing gigs? I think the main issue would be visibility for me, I still need to build a protifolio that speaks for my skillset, because right now I don't think I have much demonstrable skills outside of a traditional hiring interview pipeline.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39260158"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39260158" href="https://news.ycombinator.com/vote?id=39260158&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I run this free community for fractional tech workers - come check it out:<p><a href="https://news.ycombinator.com/item?id=37419830">https://news.ycombinator.com/item?id=37419830</a></p><p>The reality is that being self-employed requires building business skills and being able to sell your skills. I previously founded a VC-funded developer marketplace, and the people that won all the jobs were the great communicators - not the most experienced (or inexpensive) developers.</p><p>Fractional work is a nice in-between where you ideally have a retained part-time contract, paid weekly or monthly, so that you aren't constantly looking for new projects.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260262"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39260262" href="https://news.ycombinator.com/vote?id=39260262&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>Exactly this. I remember back when elance was a thing I signed up and I won my first gig in 2 days. I asked my client why he choose me (charging $1k) when others were bidding to do the job for $50 and he said he liked the message I sent him... Communications is everything.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39260329"><td></td></tr>
                  <tr id="39260342"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260342" href="https://news.ycombinator.com/vote?id=39260342&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I have been self-employed since 2008, when I quit my job in software engineering to go all-in on my software business (that dated from 2003). That failed spectacularly, because I only focused on technology and not on the value I was creating, and with few customers, I had to do on-site contracting for more than a year before going on full-time parental leave.<p>I then rebooted my software project, launched a landing site and started talking to prospects (hundreds of them), before I set out to pivot my existing product to something that might gain traction. (I wound up throwing away 95 percent of the code.) I spent 2014 through 2019 with the product in beta, barely making a living off of a few enterprise support contracts and doing freelance photography (and depleting my savings), but spending at least 80 percent of my time on building the product and getting it to a finished state.</p><p>(Some people seem to be able to build a product in a weekend that gets eager customers. I'm not one of those people, choosing to build something that was, in retrospect, much too big of a project for one person. I probably also spent too much time polishing the product before commercializing it, likely due to a fear of failure.)</p><p>In 2019, the product was finally commercialized as a SaaS service. I remember thinking that I either wanted it to be a spectacular success, or a spectacular failure (so that I could focus on other things, after close to 20 years).</p><p>It was neither, but has been growing steadily ever since. I would have made much more money working for someone else, but the freedom is unparalleled. I get to set my own hours and focus on things I consider important. I enjoy doing everything from support calls and UX work to building a compiler and a type system (that I have mentioned before on HN).</p><p>I also have no one I need to answer to, other than our customers. That has been important over the past couple of years, when a series of health emergencies in my family has diverted my attention elsewhere. I have been very fortunate to be able to do so, focusing on what's important, without having to ask permission to cut down on work temporarily.</p><p>Overall, I wouldn't trade this for anything. This year, my product will gain a sister product in a more lucrative field (I'm hoping), and I have plans to commercialize my compiler, both as a service and as a traditionally-licensed library. So I'm excited to stay solo and keep working on building the business.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260624"><td></td></tr>
                <tr id="39260774"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260774" href="https://news.ycombinator.com/vote?id=39260774&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>Thanks, it certainly has been. After only a couple of years in the software industry proper, though, I felt I had seen all I needed to see. Crunch time. Arbitrary, ill-informed management decisions. Management who didn't believe in the product the team was passionate about. Products canceled through no fault of anyone working on it. Office politics and bickering.<p>With my own business, I gain agency. If the product fails, it's because I failed to market it properly, or the product vision was bad and did not resonate with enough customers, or because I failed to execute on that vision. When all the decisions are out of your hands, and you can't even see what prompted them, they can feel capricious and arbitrary.</p><p>With my own business, I am in control. I don't have one boss, I have hundreds of them. And as long as I continue to provide them with value, I get to continue doing what I'm doing. I like those terms.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260802"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39260802" href="https://news.ycombinator.com/vote?id=39260802&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>Yes, to all of that. But: the amount of control is directly proportional to how fat your wallet is, as it gets leaner you lose some of that agency so make sure you never even go close to depleting your reserves. That's a lesson I learned the hard way at some point and it causes me to be pretty cautious from a financial perspective. So far so good ;)</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39260636"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260636" href="https://news.ycombinator.com/vote?id=39260636&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>Self-employed for 10 years now as the co-runner of Apsis Labs (<a href="https://apsis.io);/" rel="nofollow">https://apsis.io);</a> we're a software development agency with a focus on healthcare technology and HIPAA compliance.<p>After a decade of building products for clients, we've recently launched our own bootstrapped SaaS offering: <a href="https://reamdocs.com/" rel="nofollow">https://reamdocs.com</a> --- the contract management platform we've built in-house to manage our services business for the last 5 years.</p><p>That all pays the bills, but what sustains me is my time outside of work: my family, my hobbies, my cat. Joys which I have time for because self-employment has allowed me to set my own hours, walk away from toxic clients, and freedom to do what is interesting on any given day.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39261070"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39261070" href="https://news.ycombinator.com/vote?id=39261070&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I am currently working on helping small companies solve technical problems, build a strong technical culture, and scale their engineering orgs. Both hands on code and leadership, as needed.<p>For a summary, <a href="https://orib.dev/consulting.html" rel="nofollow">https://orib.dev/consulting.html</a></p><p>I'm just starting out with this, but it's looking promising.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260982"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260982" href="https://news.ycombinator.com/vote?id=39260982&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I've been self-employed for the past 5 years, coincidentally because job recruitment processes were so ridiculous and time sinks at the time (spent ~6 months getting bounced around and ghosted), can't even imagine how things are nowadays.<p>In the past 3 years through my company (<a href="https://webesque.agency/" rel="nofollow">https://webesque.agency</a>) with the aim of expanding in digital product delivery, though I haven't invested enough time and marketing on that front yet. Thus I continue to contract and consult on DevOps automation and Cloud related projects for the time being.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260061"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260061" href="https://news.ycombinator.com/vote?id=39260061&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I’be been self employed for ~1.5yrs after quitting my job to build a GUI for Kubernetes [1]<p>It’s not easy to get started, but im very happy with this change after being a FTE for 15yrs. It’s a refreshing experience having to talk to customers to understand their pain points and then build something for it</p><p>I’m also doing some part time freelancing, so with my products + freelancing I’m earning way more than as a FTE</p><p>[1] <a href="https://aptakube.com/" rel="nofollow">https://aptakube.com</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260310"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260310" href="https://news.ycombinator.com/vote?id=39260310&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>Hey I bought aptakube! Very neat product, doesn't suffer from any of the issues that the other GUI k8s apps have. I found it on an awesome-tauri github list.<p>However I still rely on k9s due to the key bindings and plugin system being irreplaceable. That could change, I'd really like to do things like toggle FluxCD resources or do other custom actions directly in aptakube.</p><p>Great product!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260365"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260365" href="https://news.ycombinator.com/vote?id=39260365&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>Thank you :)<p>I’ve been thinking of building custom UI for popular CRDs like FluxCD/Argo/others, but I need to get a few highly requested features out of the way first.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39260312"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260312" href="https://news.ycombinator.com/vote?id=39260312&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I make video games [1] and TikToks [2] about them. I’m a solo game dev doing the coding, design and business/marketing stuff. I’ve worked in mainstream games and as a CTO at a few companies but my passion is making small games I can prototype in a few days and release within a week or so. After trying a lot of different roles the satisfaction of building something hundreds of thousands of people play that I have total control over is deeply fulfilling.<p>[1] <a href="https://attackmove.io/playit" rel="nofollow">https://attackmove.io/playit</a></p><p>[2] <a href="https://tiktok.com/@attackmove" rel="nofollow">https://tiktok.com/@attackmove</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260405"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260405" href="https://news.ycombinator.com/vote?id=39260405&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>What's your experience with tiktok? How well does it perform in terms of attracting people to buy your stuff? Do videos besides plays on popular memes du jour do any good?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39260442"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260442" href="https://news.ycombinator.com/vote?id=39260442&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>It’s been amazing. I’ve had about 30% of people that follow me join my Discord which is 10x higher than I expected. Meme videos do the best but a loyal fan base see and enjoys my regular dev style videos too. I do the same on Reels and Shorts with less success. But I enjoy learning which videos do better on each platform.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39260575"><td></td></tr>
                              <tr id="39260532"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260532" href="https://news.ycombinator.com/vote?id=39260532&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I’m self employed for nearly five years now. Based in France, working mostly with software companies scaling up. I have been learning as I go and have managed to find some good ‘cornerstone’ clients that I have been working with regularly since I started. These are supplemented with shorter contracts and freelance work, anything from a one-off gig to a couple of months.<p>I set myself the goal of earning the same or better (after taxes) as what I was as a full time worker in the same sorts of companies I am now working in, and I’ve always managed that - plus I get many more hours in the week to spend with the family, I don’t have to travel constantly, and I can run every day without thinking too hard about how I am going to fit it in around sleep, family, work, and the rest of life.</p><p>I sometimes miss a few things about working in a ‘normal’ job in France (job security, sick leave, paid vacations) but the freedom to create my own schedule and fit work around life a lot of the time more than makes up for that.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260130"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260130" href="https://news.ycombinator.com/vote?id=39260130&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>I'm virtually self employed, started about one year ago. Taxes are HIGH where I live, it's a little cheaper for employer to subcontract, so he pays the same overall, I got a raise. I currently have probably one of highest pays for my region, so would have to move to other city to earn more. Plus I do some freelancing in free hours, to bring some more money. Bought a little too big home for my means (it was just before everything suddenly went 2x, "finished" it just before pandemic), so I'm spending everything I get. I earn twice what some of my friends get, but they have more for "life".</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39260373"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260373" href="https://news.ycombinator.com/vote?id=39260373&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I'm doing full time freelancing in the UK and loving it tbh, but sometimes it can get a bit difficult to handle.<p>I was doing typical employment until October 2023 while doing some part time freelancing. The company had layoffs and I decided to do more freelancing while I look for the next job. Then the freelancing pay became double or more what I'd make from normal employment, so I am unlikely to get back to "being an employee" unless the situation changes significantly.</p><p>Initially I was worried about being able to find enough gigs to be able to make enough income to survive, but now my main struggle is to not take on too many responsibilities at once to be able to keep the clients happy, and even had to turn a few people down because I'm getting too busy...</p><p>I do get some spare time for myself to work on my own things, a few of those projects do bring income, but my main income source and the thing that takes most of my day is the freelancing.</p><p>I hired an accounting company to take care of taxes and stuff.</p><p>Most of my clients are in the UK.</p><p>I work partially through Toptal and some through my network.</p><p>I can't complain too much about it!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260344"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260344" href="https://news.ycombinator.com/vote?id=39260344&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>I created a SaaS product (b2b) 7+ years ago. I work on it by myself with no team whatsoever. It generates around a million in profit in every year.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39260912"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260912" href="https://news.ycombinator.com/vote?id=39260912&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>I want one of these so bad! Without spilling secrets: was it luck or a systematic process that gave you the idea for your company?</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39260081"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260081" href="https://news.ycombinator.com/vote?id=39260081&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>I've been a self-employed Ops-Engineer/Sysadmin for hire/Sysadmin-as-a-service for 14 years now, after doing the same as an employee for a consulting company for the 10 years before that.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39260105"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260105" href="https://news.ycombinator.com/vote?id=39260105&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>How does this work? I see sysadmin as having a large day to day component. Do they call you in to setup processes or are you the emergency call?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39260170"><td></td></tr>
                        <tr id="39260380"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260380" href="https://news.ycombinator.com/vote?id=39260380&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I am working on my second startup. The first startup was bootstrapped hardware company.  We released a few electronic devices.  I worked on the first product part-time. Just before it was released, I decided to leave my job and work on the company full time.<p>I worked on the second product full time for about a year. It was profitable, but I never scaled the business up enough to make a living from it. I went back to work full time.</p><p>By this point, my appetite for hustling on the weekend to build a business in a market with limited growth potential had diminished.  I focused on paying off my mortgage and saving money. After a few years, I had saved enough money to be able to cut back my hours and focus more on my hobbies and passion projects.</p><p>I've been working on my current startup Emurse full time since the end of 2020.  I am covering costs with the revenue from my first startup and living off savings. We're getting ready to launch the first paid product soon.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260302"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260302" href="https://news.ycombinator.com/vote?id=39260302&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>Does selling programming ebooks count?<p>I left my job in 2014 (worked for a semiconductor company), muddled around for 4+ years with not much to show before writing programming ebooks allowed me to earn a living. Fast forward to 2024 and I'm still only earning about 1/3 of what I used to earn (without even considering inflation, what my salary could've been now, etc), but it works for me.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260507"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260507" href="https://news.ycombinator.com/vote?id=39260507&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I've been self employed at Hog Bay Software [1] since 2004. I mostly build Mac apps. A few years I did some consulting. A few other years I worked with some other people, but generally it's been just me the whole time.<p>&gt; Curious to know what sustains people.</p><p>It allows me to work on what I am interested in, for me that's the key. For whatever reason I've decided that text productivity apps are really interesting, and worth my time to build. So that's what I do. It hasn't lead to great riches, but I wake up pretty much every day excited to work on what's next. Pretty fun!</p><p>[1] <a href="https://www.hogbaysoftware.com/" rel="nofollow">https://www.hogbaysoftware.com</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260689"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260689" href="https://news.ycombinator.com/vote?id=39260689&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>I'm about to wrap up my 3rd year as a one-person company (<a href="https://www.exaresearch.com/" rel="nofollow">https://www.exaresearch.com</a>) specializing in spacecraft flight dynamics and space situtational awareness (SSA). I basically do custom orbit determination things. Super fun. I spent 30+ years in the business before going out on my own. It's going great, primarily because I have a large network in a niche field.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39260470"><td></td></tr>
            <tr id="39260271"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260271" href="https://news.ycombinator.com/vote?id=39260271&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I've been bootstrapping orbitalhq.com for a few years now.<p>Pretty standard bootstrap story - used consultancy to get us out the door. These days, we're (nearly) self sustained through license revenue, with a small amount of consulting (1-2 projects a year).</p><p>It's been a tough ride, and in hindsight, I wish I hadn't pivoted away from consulting quite as early as I had, however we've managed to stay alive, so - so far - independent, which has been nice.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39260316"><td></td></tr>
                  <tr id="39260590"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260590" href="https://news.ycombinator.com/vote?id=39260590&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>I started working for myself in '86, have been at it ever since with temporary excursion whenever a project got larger and I ended up hiring people or having partners. Wouldn't have it any other way in spite of the obvious risks and drawbacks.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39260618"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260618" href="https://news.ycombinator.com/vote?id=39260618&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>I technically am self employed as I have my own LLC, but I generally work as a full time engineer for companies. It just helps for tax reasons but also flexibility in my work, ie choose my own hours and pay rate.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39260314"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260314" href="https://news.ycombinator.com/vote?id=39260314&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I'm freelancing two days a week, which generates most of my income.<p>For the rest of the week, I'm bootstrapping a language learning app.(<a href="https://vokabeln.io/" rel="nofollow">https://vokabeln.io/</a>)</p><p>I'm lucky enough not having to pay rent and also living expenses in Czechia are not that high.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260556"><td></td></tr>
                <tr id="39260651"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260651" href="https://news.ycombinator.com/vote?id=39260651&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>It's infrequently used because poll votes don't count as upvotes for the poll, which they really should. And is probably my longest standing RFI for HN.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39260299"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260299" href="https://news.ycombinator.com/vote?id=39260299&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I've been self employed for 3 years now. I help companies get started with machine learning. ChatGPT is the best marketing for me: everyone and their dog wants ML because of ChatGPT.<p>I do advisory work and development too.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260093"><td></td></tr>
                <tr id="39260308"><td></td></tr>
                <tr id="39260377"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39260377" href="https://news.ycombinator.com/vote?id=39260377&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>I don't know. Usually clients talk to me to see if we click, ask questions and decide if I can help or not.<p>On one hand I have been doing software development for a long time, 40+ years of coding (currently Go), 25+ years as a manager (CTO, DoE, HoD,...) in large enterprises and startups. Founded three startups, one fizzled out, one went belly up in 2001 and one sold successfully.</p><p>But I learn something new every day.</p><p>On the other hand I have no training in coaching - but have been doing it for some years now. You decide.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39260479"><td></td></tr>
            <tr id="39260566"><td></td></tr>
            <tr id="39260599"><td></td></tr>
            <tr id="39260211"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260211" href="https://news.ycombinator.com/vote?id=39260211&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>I never understood what self employed means vs a startup. If I start a Delaware C and raise 20M am I self employed?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39260238"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260238" href="https://news.ycombinator.com/vote?id=39260238&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>Self employed means you're a "one man band" and you have no intention of changing it. A startup is when you may start on your own, but you want to grow it, have employees etc.<p>I decided long time ago the former makes me a lot happier than the later.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39260166"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260166" href="https://news.ycombinator.com/vote?id=39260166&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>Independently wealthy after working 10 years in the Bay Area. I live in Amsterdam now, but I still consult on for startups if the project seems fun, good to have some extra money for the nieces and nephews.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39260707"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260707" href="https://news.ycombinator.com/vote?id=39260707&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>I was self employed from 2009 to 2018. I was an embedded programmer / debugger and I had two major sources of income debugging engineering development boards for two boutique companies. I got the first gig from a friend who worked at one, the second by spamming the embedded world conference, and a handful more through linkedin. I couldn’t really travel because I needed a lab with test equipment. But I did double my previous salary north of 200k and I worked as much or as little as needed. It ended up being an average 40hr work week overall. The one thing I missed out on was equity. A decade without bonuses, RSUs, or options really set me back compared to my peers. Plus I hated having to stress about finding new contracts or if I was going to get cut loose. Finally I quit and went back to a big employer and I’m much happier to not have that financial stress and to have equity again.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39260545"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39260545" href="https://news.ycombinator.com/vote?id=39260545&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><p><span>Yeah 10 years contracting now taking time off to build my own products. Currently focussing on developing my innovation pipeline to systematically evaluate products/opportunities to decide when to continue or abandon them.<p>Next job is idea generation, prioritisation and mockups of ones that pass the initial filter. Then the real work starts.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39260286"><td></td></tr>
                <tr id="39260625"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39260625" href="https://news.ycombinator.com/vote?id=39260625&amp;how=up&amp;goto=item%3Fid%3D39259602"></a></center>    </td><td><br><div>
                  <p><span>Seconded. Was doing it for a year and it feels great, most of the times. I slept in a wrong position tonight and now my back hurts. I’ll either wait the crisis out or… idk tbh, thought of working for animal shelter. The world doesn’t motivate me to do anything atm.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39260633"><td></td></tr>
                  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parisians vote in favour of tripling parking costs for SUVs (124 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/feb/04/parisians-vote-in-favour-of-tripling-parking-costs-for-suvs</link>
            <guid>39259533</guid>
            <pubDate>Mon, 05 Feb 2024 10:30:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/feb/04/parisians-vote-in-favour-of-tripling-parking-costs-for-suvs">https://www.theguardian.com/world/2024/feb/04/parisians-vote-in-favour-of-tripling-parking-costs-for-suvs</a>, See on <a href="https://news.ycombinator.com/item?id=39259533">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Parisians have voted to <a href="https://www.theguardian.com/world/2023/dec/08/paris-mayor-plans-to-triple-suv-parking-tariffs-cut-air-pollution" data-link-name="in body link">triple parking costs</a> for sports utility vehicles (SUVs), as the city aims to tackle air pollution and climate breakdown by targeting rich drivers in heavy, large and polluting cars.</p><p>In a referendum on Sunday, which was closely watched by other capital cities, including <a href="https://www.theguardian.com/environment/2024/feb/02/london-could-introduce-suv-parking-charge-sadiq-khan-indicates" data-link-name="in body link">London</a>, 54.6% voted in favour of special parking fees for SUVs, according to provisional results. However, the turnout – at about 5.7% of Paris’s registered voters – was lower than green campaigners had hoped for.</p><p>“Parisians have made a clear choice … other cities will follow,” said Paris’s Socialist mayor, Anne Hidalgo, adding that road safety and air pollution were key reasons for the vote.</p><p>Hidalgo had previously described the move to curb the presence of SUVs through <a href="https://www.theguardian.com/world/2024/feb/02/paris-residents-set-to-vote-on-plan-to-triple-parking-charges-for-suvs" data-link-name="in body link">raising parking prices</a> as “a form of social justice”. She said the aim was to deliberately target the richest drivers of expensive, heavy and polluting cars who had not yet made changes to their behaviour to address the climate crisis.</p><figure id="e167ef5d-bdd4-4d1c-a179-7df58a0b180e" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:4,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Sadiq Khan says he will monitor effectiveness of Paris plan to raise charges on SUVs&quot;,&quot;elementId&quot;:&quot;e167ef5d-bdd4-4d1c-a179-7df58a0b180e&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/environment/2024/feb/02/london-could-introduce-suv-parking-charge-sadiq-khan-indicates&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>The new parking tariffs could come into force at the start of September. The cost of on-street parking for an SUV or 4x4 car would rise to €18 (£15) an hour in the centre of Paris and €12 an hour in the rest of the city.</p><p>The prices will apply to vehicles weighing more than 1.6 tonnes with a combustion engine or hybrid vehicles, and more than 2 tonnes for electric vehicles. The move will not apply to Paris residents’ parking.</p><p>Tony Renucci, director of the air quality campaign group Respire, said: “The result of the vote is a victory for Paris residents’ quality of life.” He added that Paris was sending a message that “the presence of these monsters on wheels was no longer desirable on our streets”.</p><p>Emmanuel Grégoire, Paris’s deputy mayor, posted on X as voting began: “Heavier, more dangerous, more polluting … SUVs are an environmental disaster.”</p><p>Last year, Paris held a similar vote on whether to ban <a href="https://www.theguardian.com/world/2023/aug/31/rented-e-scooters-cleared-from-paris-streets-on-eve-of-ban" data-link-name="in body link">rented electric scooters</a> and subsequently became the first European capital to do so. The turnout for that vote – 103,000 people, about 7% of registered voters – was higher than for the vote on SUVs.</p><p>Under Hidalgo, Paris has for years raised pressure on drivers by increasing parking costs and gradually banning diesel vehicles, while expanding the bicycle lane network in the congested capital. The city has reduced the number of on-street parking spaces in order to make drivers use underground parking. There was a 71% rise in the use of bikes between the end of the Covid lockdowns and 2023, city hall said.</p><p>Paris’s deputy mayor in charge of transport, David Belliard, of the Green party, said about 10% of vehicles in Paris would be hit by the higher parking fees, which could bring in up to €35m for the city each year.</p><p>The motorists’ lobby group 40 Millions d’Automobilistes had argued that drivers should be free to choose whatever vehicle they want, warning that the move to raise parking tariffs was unjustified and the work of “an ultra-urban and anti-car minority”.</p><p>Christophe Béchu, France’s environment minister, Christophe Béchu, told broadcaster RTL that the SUV surcharge amounted to “a kind of punitive environmentalism” – even if drivers should “opt for lighter vehicles”.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How I'm able to take notes in mathematics lectures using LaTeX and Vim (2019) (120 pts)]]></title>
            <link>https://castel.dev/post/lecture-notes-1/</link>
            <guid>39259316</guid>
            <pubDate>Mon, 05 Feb 2024 10:00:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://castel.dev/post/lecture-notes-1/">https://castel.dev/post/lecture-notes-1/</a>, See on <a href="https://news.ycombinator.com/item?id=39259316">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I started using LaTeX to write lecture notes in the second semester of my bachelor in mathematics, and I’ve been using it ever since,
which makes for a total of more than 1700 pages of notes.
To give you an idea of what those notes look like, here are some examples:</p>
<p><span>
      <a href="https://castel.dev/static/ba66c52b01c8fe6b31c97af52b5eb7f7/c2c88/ca1.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="ca1" title="ca1" src="https://castel.dev/static/ba66c52b01c8fe6b31c97af52b5eb7f7/a4de1/ca1.png" srcset="https://castel.dev/static/ba66c52b01c8fe6b31c97af52b5eb7f7/e3b59/ca1.png 170w,
https://castel.dev/static/ba66c52b01c8fe6b31c97af52b5eb7f7/d6fe0/ca1.png 340w,
https://castel.dev/static/ba66c52b01c8fe6b31c97af52b5eb7f7/a4de1/ca1.png 680w,
https://castel.dev/static/ba66c52b01c8fe6b31c97af52b5eb7f7/e9ece/ca1.png 1020w,
https://castel.dev/static/ba66c52b01c8fe6b31c97af52b5eb7f7/c2c88/ca1.png 1173w" sizes="(max-width: 680px) 100vw, 680px" loading="lazy" decoding="async">
  </a>
    </span>
<span>
      <a href="https://castel.dev/static/1a393ac73dbb96f6fbd98b258e8a325a/a78ad/ca2.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="ca2" title="ca2" src="https://castel.dev/static/1a393ac73dbb96f6fbd98b258e8a325a/a4de1/ca2.png" srcset="https://castel.dev/static/1a393ac73dbb96f6fbd98b258e8a325a/e3b59/ca2.png 170w,
https://castel.dev/static/1a393ac73dbb96f6fbd98b258e8a325a/d6fe0/ca2.png 340w,
https://castel.dev/static/1a393ac73dbb96f6fbd98b258e8a325a/a4de1/ca2.png 680w,
https://castel.dev/static/1a393ac73dbb96f6fbd98b258e8a325a/e9ece/ca2.png 1020w,
https://castel.dev/static/1a393ac73dbb96f6fbd98b258e8a325a/a78ad/ca2.png 1163w" sizes="(max-width: 680px) 100vw, 680px" loading="lazy" decoding="async">
  </a>
    </span>
<span>
      <a href="https://castel.dev/static/cb323a2ec665d98f93f78e13d3972e0e/73116/ca3.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="ca3" title="ca3" src="https://castel.dev/static/cb323a2ec665d98f93f78e13d3972e0e/a4de1/ca3.png" srcset="https://castel.dev/static/cb323a2ec665d98f93f78e13d3972e0e/e3b59/ca3.png 170w,
https://castel.dev/static/cb323a2ec665d98f93f78e13d3972e0e/d6fe0/ca3.png 340w,
https://castel.dev/static/cb323a2ec665d98f93f78e13d3972e0e/a4de1/ca3.png 680w,
https://castel.dev/static/cb323a2ec665d98f93f78e13d3972e0e/e9ece/ca3.png 1020w,
https://castel.dev/static/cb323a2ec665d98f93f78e13d3972e0e/73116/ca3.png 1155w" sizes="(max-width: 680px) 100vw, 680px" loading="lazy" decoding="async">
  </a>
    </span></p>
<p>These lecture notes — including figures — are made while attending the lecture and have not been edited afterwards.
To make note taking using LaTeX viable, I had four goals in mind:</p>
<ul>
<li>Writing text and mathematical formulas in LaTeX should be as fast as the lecturer writing on a blackboard: no delay is acceptable.</li>
<li>Drawing figures should be almost as fast as the lecturer.</li>
<li>Managing notes, i.e. adding a note, compiling all my notes, compiling the last two lectures, searching in notes, etc. should be easy and quick.</li>
<li>Annotating pdf documents using LaTeX should be possible for when I want to write notes alongside a pdf document.</li>
</ul>
<p>This blog post will focus on the first item: writing LaTeX.</p>
<div>
<ul>
<li><a href="#vim-and-latex">Vim and LaTeX</a></li>
<li>
<p><a href="#snippets">Snippets</a></p>
<ul>
<li><a href="#whats-a-snippet">What’s a snippet?</a></li>
<li><a href="#using-ultisnips-to-create-snippets">Using UltiSnips to create snippets</a></li>
</ul>
</li>
<li>
<p><a href="#latex-snippets">LaTeX snippets</a></p>
<ul>
<li><a href="#environments">Environments</a></li>
<li><a href="#inline-and-display-math">Inline and display math</a></li>
<li><a href="#sub--and-superscripts">Sub- and superscripts</a></li>
<li><a href="#fractions">Fractions</a></li>
<li><a href="#sympy-and-mathematica">Sympy and Mathematica</a></li>
<li><a href="#postfix-snippets">Postfix snippets</a></li>
<li><a href="#other-snippets">Other snippets</a></li>
<li><a href="#course-specific-snippets">Course specific snippets</a></li>
<li><a href="#context">Context</a></li>
</ul>
</li>
<li><a href="#correcting-spelling-mistakes-on-the-fly">Correcting spelling mistakes on the fly</a></li>
<li><a href="#in-conclusion">In conclusion</a></li>
</ul>
</div>
<h2 id="vim-and-latex">Vim and LaTeX</h2>
<p>For writing text and mathematical formulas in LaTeX, I use Vim.
Vim is a powerful general purpose text editor that’s very extensible.
I use it for writing code, LaTeX, markdown, … basically everything that’s text-based.
It has a fairly steep learning curve, but once you’ve got the basics down, it’s hard to get back to an editor without Vim keybindings.
Here’s what my screen looks like when I’m editing LaTeX:</p>
<p><span>
      <a href="https://castel.dev/static/4282d15d03150c89182ab56634d2b4b0/d7e34/vimzathura.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Vim and Zathura" title="Vim and Zathura" src="https://castel.dev/static/4282d15d03150c89182ab56634d2b4b0/a4de1/vimzathura.png" srcset="https://castel.dev/static/4282d15d03150c89182ab56634d2b4b0/e3b59/vimzathura.png 170w,
https://castel.dev/static/4282d15d03150c89182ab56634d2b4b0/d6fe0/vimzathura.png 340w,
https://castel.dev/static/4282d15d03150c89182ab56634d2b4b0/a4de1/vimzathura.png 680w,
https://castel.dev/static/4282d15d03150c89182ab56634d2b4b0/e9ece/vimzathura.png 1020w,
https://castel.dev/static/4282d15d03150c89182ab56634d2b4b0/e0c87/vimzathura.png 1360w,
https://castel.dev/static/4282d15d03150c89182ab56634d2b4b0/d7e34/vimzathura.png 1919w" sizes="(max-width: 680px) 100vw, 680px" loading="lazy" decoding="async">
  </a>
    </span></p>
<p>On the left you see Vim and on the right my pdf viewer, <a href="https://en.wikipedia.org/wiki/Zathura_(document_viewer)" target="_blank" rel="nofollow noopener noreferrer">Zathura</a>, which also has Vim-like keybindings.
I’m using Ubuntu with <a href="https://github.com/baskerville/bspwm" target="_blank" rel="nofollow noopener noreferrer">bspwm</a> as my window manager.
The LaTeX plugin I’m using in Vim is <a href="https://github.com/lervag/vimtex" target="_blank" rel="nofollow noopener noreferrer">vimtex</a>.
It provides syntax highlighting, table of contents view, synctex, etc.
Using <a href="https://github.com/junegunn/vim-plug" target="_blank" rel="nofollow noopener noreferrer">vim-plug</a>, I configured it as follows:</p>
<div data-language="vim"><pre><code>Plug <span>'lervag/vimtex'</span>
<span>let</span> g<span>:</span>tex_flavor<span>=</span><span>'latex'</span>
<span>let</span> g<span>:</span>vimtex_view_method<span>=</span><span>'zathura'</span>
<span>let</span> g<span>:</span>vimtex_quickfix_mode<span>=</span><span>0</span>
<span>set</span> conceallevel<span>=</span><span>1</span>
<span>let</span> g<span>:</span>tex_conceal<span>=</span><span>'abdmg'</span></code></pre></div>
<p>The last two lines configure the concealment.
This is a feature where LaTeX code is replaced or made invisible when your cursor is not on that line.
By making <code>\[</code>, <code>\]</code>, <code>$</code> invisible, they’re less obtrusive which gives you a better overview of the document. This feature also replaces <code>\bigcap</code> by by <code>∩</code>, <code>\in</code> by <code>∈</code> etc.
The following animation should make that clear.</p>
<p><img src="https://castel.dev/d362e0b837be308b0f8897482a331ec6/conceal.gif" alt="conceal"></p>
<p>With this set up, I come to the crux of this blog post: writing LaTeX as fast as the lecturer can write on the blackboard.
This is where snippets come into play.</p>
<h2 id="snippets">Snippets</h2>
<h3 id="whats-a-snippet">What’s a snippet?</h3>
<p>A snippet is a short reusable piece of text that can be triggered by some other text.
For example, when I type <code>sign</code> and press <kbd>Tab</kbd>, the word <code>sign</code> will be expanded to a signature:</p>
<p><img src="https://castel.dev/5ab28b1ad6114b68f4ce459d4f89eeae/sign.gif" alt="sign"></p>
<p>Snippets can also be dynamic: when I type <code>today</code> and press <kbd>Tab</kbd>, the word <code>today</code> will be replaced by the current date, and <code>box</code> <kbd>Tab</kbd> becomes a box that automatically grows in size.</p>
<p><img src="https://castel.dev/1b6301a6561d104b4bc79654cb75470d/today.gif" alt="today"></p>
<p><img src="https://castel.dev/8b7caefc472eae0941f41365d16e8d2d/box.gif" alt="box"></p>
<p>You can even use one snippet inside another:</p>
<p><img src="https://castel.dev/cdd1fded48947d163c2e5e6b974cfec0/todaybox.gif" alt="todaybox"></p>
<h3 id="using-ultisnips-to-create-snippets">Using UltiSnips to create snippets</h3>
<p>I use the plugin <a href="https://github.com/SirVer/ultisnips" target="_blank" rel="nofollow noopener noreferrer">UltiSnips</a> to manage my snippets. My configuration is</p>
<div data-language="vim"><pre><code>Plug <span>'sirver/ultisnips'</span>
<span>let</span> g<span>:</span>UltiSnipsExpandTrigger <span>=</span> <span>'&lt;tab&gt;'</span>
<span>let</span> g<span>:</span>UltiSnipsJumpForwardTrigger <span>=</span> <span>'&lt;tab&gt;'</span>
<span>let</span> g<span>:</span>UltiSnipsJumpBackwardTrigger <span>=</span> <span>'&lt;s-tab&gt;'</span></code></pre></div>
<p>The code for the <code>sign</code> snippet is the following:</p>
<div data-language="snip"><pre><code><span>snippet <span>sign </span><span>"Signature"</span></span>
Yours sincerely,

Gilles Castel
<span>endsnippet</span></code></pre></div>
<p>For dynamic snippets, you can put code between backticks <code>``</code> which will be run when the snippet is expanded.
Here, I’ve used bash to format the current date: <code>date + %F</code>.</p>
<div data-language="snip"><pre><code><span>snippet <span>today </span><span>"Date"</span></span>
`date +%F`
<span>endsnippet</span></code></pre></div>
<p>You can also use Python inside a <code>`!p ... `</code> block.
Have a look at the code for the <code>box</code> snippet:</p>
<div data-language="snip"><pre><code><span>snippet <span>box </span><span>"Box"</span></span>
`<span>!p</span><span> snip<span>.</span>rv <span>=</span> <span>'┌'</span> <span>+</span> <span>'─'</span> <span>*</span> <span>(</span><span>len</span><span>(</span>t<span>[</span><span>1</span><span>]</span><span>)</span> <span>+</span> <span>2</span><span>)</span> <span>+</span> <span>'┐'</span>`</span>
│ <span>$1</span> │
`<span>!p</span><span> snip<span>.</span>rv <span>=</span> <span>'└'</span> <span>+</span> <span>'─'</span> <span>*</span> <span>(</span><span>len</span><span>(</span>t<span>[</span><span>1</span><span>]</span><span>)</span> <span>+</span> <span>2</span><span>)</span> <span>+</span> <span>'┘'</span>`</span>
<span>$0</span>
<span>endsnippet</span></code></pre></div>
<p>These Python code blocks will be replaced by the value of the variable <code>snip.rv</code>.
Inside these blocks, you have access to the current state of the snippet, e.g. <code>t[1]</code> contains the first tab stop, <code>fn</code> the current filename, …</p>
<h2 id="latex-snippets">LaTeX snippets</h2>
<p>Using snippets, writing LaTeX is a lot faster than writing it by hand.
Especially some of the more complex snippets can save you a lot of time and frustration.  Let’s begin with some simple snippets.</p>
<h3 id="environments">Environments</h3>
<p>To insert an environment, all I have to do is type <code>beg</code> at the beginning of a line.
Then I type the name of the environment, which is mirrored in the <code>\end{}</code> command. Pressing <kbd>Tab</kbd> places the cursor inside the newly created environment.</p>
<p><img src="https://castel.dev/d5ed641a9cb568d13dd56d98fec6376b/beginend.gif" alt="beginend"></p>
<p>The code for this snippet is the following.</p>
<div data-language="snip"><pre><code><span>snippet <span>beg </span><span>"begin{} / end{}"</span> <span>bA</span></span>
\begin{<span>$1</span>}
	<span>$0</span>
\end{<span>$1</span>}
<span>endsnippet</span></code></pre></div>
<p>The <code>b</code> means that this snippet will only be expanded at the beginning of a line and <code>A</code> stands for auto expand, which means I do not have to press <kbd>Tab</kbd> to expand the snippet. Tab stops — i.e. places you can jump to by pressing <kbd>Tab</kbd> and <kbd>Shift</kbd>+<kbd>Tab</kbd> — are represented by <code>$1</code>, <code>$2</code>, … and the last one with <code>$0</code>.</p>
<h3 id="inline-and-display-math">Inline and display math</h3>
<p>Two of my most frequently used snippets are <code>mk</code> and <code>dm</code>.
They’re the snippets responsible for starting math mode.
The first one is a snippet for inline math, the second one for displayed math.</p>
<p><img src="https://castel.dev/12a16b6a90c4d85a887c7cde0f18bb8c/mkdm.gif" alt="mkdm"></p>
<p>The snippet for inline math is ‘smart’: it knows when to insert a space after the dollar sign.
When I start typing a word directly behind the closing <code>$</code>, it adds a space.
However, when I type a non-word character, it does not add a space, which would be preferred for example in the case of <code>$p$-value</code>.</p>
<p><img src="https://castel.dev/1172a1f556fd36b42f7c3642b3952e2c/mk_space.gif" alt="mk_space"></p>
<p>The code for this snippet is the following.</p>
<div data-language="snip"><pre><code><span>snippet <span>mk </span><span>"Math"</span> <span>wA</span></span>
$<span>${1}</span>$`<span>!p</span><span>
<span>if</span> t<span>[</span><span>2</span><span>]</span> <span>and</span> t<span>[</span><span>2</span><span>]</span><span>[</span><span>0</span><span>]</span> <span>not</span> <span>in</span> <span>[</span><span>','</span><span>,</span> <span>'.'</span><span>,</span> <span>'?'</span><span>,</span> <span>'-'</span><span>,</span> <span>' '</span><span>]</span><span>:</span>
    snip<span>.</span>rv <span>=</span> <span>' '</span>
<span>else</span><span>:</span>
    snip<span>.</span>rv <span>=</span> <span>''</span>
`</span><span>$2</span>
<span>endsnippet</span></code></pre></div>
<p>The <code>w</code> at the end of the first line means that this snippet will expand at word boundaries, so e.g. <code>hellomk</code> won’t expand, but <code>hello mk</code> will.</p>
<p>The snippet for displayed math is more simple, but it also is quite handy; it makes me never forget ending equations with a period.</p>
<p><img src="https://castel.dev/a682e97365ecfe19fe0c5bc08bacfec6/dm.gif" alt="dm"></p>
<div data-language="snip"><pre><code><span>snippet <span>dm </span><span>"Math"</span> <span>wA</span></span>
\[
<span>$1</span>
.\] <span>$0</span>
<span>endsnippet</span></code></pre></div>
<h3 id="sub--and-superscripts">Sub- and superscripts</h3>
<p>Another useful snippet is one for subscripts.
It changes changes <code>a1</code> to <code>a_1</code> and <code>a_12</code> to <code>a_{12}</code>.</p>
<p><img src="https://castel.dev/f26633cb822c6b52d346dfbba6e1a1bd/subscripts.gif" alt="subscripts"></p>
<p>The code for this snippet uses a regular expression for its trigger.
It expands the snippet when you type a character followed by a digit, which encoded by <code>[A-Za-z]\d</code>, or a character followed by <code>_</code> and two digits: <code>[A-Za-z]_\d\d</code>.</p>
<div data-language="snip"><pre><code><span>snippet <span>'([A-Za-z])(\d)' </span><span>"auto subscript"</span> <span>wrA</span></span>
`<span>!p</span><span> snip<span>.</span>rv <span>=</span> match<span>.</span>group<span>(</span><span>1</span><span>)</span>`_`!p snip<span>.</span>rv <span>=</span> match<span>.</span>group<span>(</span><span>2</span><span>)</span>`</span>
<span>endsnippet</span>

<span>snippet <span>'([A-Za-z])_(\d\d)' </span><span>"auto subscript2"</span> <span>wrA</span></span>
`<span>!p</span><span> snip<span>.</span>rv <span>=</span> match<span>.</span>group<span>(</span><span>1</span><span>)</span>`_<span>{</span>`!p snip<span>.</span>rv <span>=</span> match<span>.</span>group<span>(</span><span>2</span><span>)</span>`</span>}
<span>endsnippet</span></code></pre></div>
<p>When you wrap parts of a regular expression in a group using parenthesis, e.g. <code>(\d\d)</code>, you can use them in the expansion of the snippet via <code>match.group(i)</code> in Python.</p>
<p>As for superscripts, I use <code>td</code>, which becomes <code>^{}</code>.  However, for squared, cubed, complement and a handful of other common ones, I use dedicated snippets such as <code>sr</code>, <code>cb</code> and <code>comp</code>.</p>
<p><img src="https://castel.dev/da855cd9e67c48f508fab5ddc8c877fe/superscripts.gif" alt="superscripts"></p>
<div data-language="snip"><pre><code><span>snippet <span>sr </span><span>"^2"</span> <span>iA</span></span>
^2
<span>endsnippet</span>

<span>snippet <span>cb </span><span>"^3"</span> <span>iA</span></span>
^3
<span>endsnippet</span>

<span>snippet <span>compl </span><span>"complement"</span> <span>iA</span></span>
^{c}
<span>endsnippet</span>

<span>snippet <span>td </span><span>"superscript"</span> <span>iA</span></span>
^{<span>$1</span>}<span>$0</span>
<span>endsnippet</span></code></pre></div>
<h3 id="fractions">Fractions</h3>
<p>One of my most convenient snippets is one for fractions. This makes the following expansions:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>//</code></td>
<td>→</td>
<td><code>\frac{}{}</code></td>
</tr>
<tr>
<td><code>3/</code></td>
<td>→</td>
<td><code>\frac{3}{}</code></td>
</tr>
<tr>
<td><code>4\pi^2/</code></td>
<td>→</td>
<td><code>\frac{4\pi^2}{}</code></td>
</tr>
<tr>
<td><code>(1 + 2 + 3)/</code></td>
<td>→</td>
<td><code>\frac{1 + 2 + 3}{}</code></td>
</tr>
<tr>
<td><code>(1+(2+3)/)</code></td>
<td>→</td>
<td><code>(1 + \frac{2+3}{})</code></td>
</tr>
<tr>
<td><code>(1 + (2+3))/</code></td>
<td>→</td>
<td><code>\frac{1 + (2+3)}{}</code></td>
</tr>
</tbody>
</table>
<p><img src="https://castel.dev/3f83f1bdc3078aa16382e80a276f199f/frac.gif" alt="frac"></p>
<p>The code for the first one is easy:</p>
<div data-language="snip"><pre><code><span>snippet <span>// </span><span>"Fraction"</span> <span>iA</span></span>
\\frac{<span>$1</span>}{<span>$2</span>}<span>$0</span>
<span>endsnippet</span></code></pre></div>
<p>The second and third examples are made possible using regular expressions to match for expressions like <code>3/</code>, <code>4ac/</code>, <code>6\pi^2/</code>, <code>a_2/</code>, etc.</p>
<div data-language="snip"><pre><code><span>snippet <span>'((\d+)|(\d*)(\\)?([A-Za-z]+)((\^|_)(\{\d+\}|\d))*)/' </span><span>"Fraction"</span> <span>wrA</span></span>
\\frac{`<span>!p</span><span> snip<span>.</span>rv <span>=</span> match<span>.</span>group<span>(</span><span>1</span><span>)</span>`</span>}{<span>$1</span>}<span>$0</span>
<span>endsnippet</span></code></pre></div>
<p>As you can see, regular expressions can become quite overwhelming, but here’s a diagram that should explain it:</p>
<p><img src="https://castel.dev/908df5ec254f9cd5b08b99b8a54318d0/regex.svg" alt="Diagram of the regular expression"></p>
<p>In the fourth and fifth cases, it tries to find the matching parenthesis. As this isn’t possible using the regular expression engine of UltiSnips, I resorted to using Python:</p>
<div data-language="snip"><pre><code><span>priority</span> 1000
<span>snippet <span>'^.*\)/' </span><span>"() Fraction"</span> <span>wrA</span></span>
`<span>!p</span><span>
stripped <span>=</span> match<span>.</span>string<span>[</span><span>:</span><span>-</span><span>1</span><span>]</span>
depth <span>=</span> <span>0</span>
i <span>=</span> <span>len</span><span>(</span>stripped<span>)</span> <span>-</span> <span>1</span>
<span>while</span> <span>True</span><span>:</span>
	<span>if</span> stripped<span>[</span>i<span>]</span> <span>==</span> <span>')'</span><span>:</span> depth <span>+=</span> <span>1</span>
	<span>if</span> stripped<span>[</span>i<span>]</span> <span>==</span> <span>'('</span><span>:</span> depth <span>-=</span> <span>1</span>
	<span>if</span> depth <span>==</span> <span>0</span><span>:</span> <span>break</span><span>;</span>
	i <span>-=</span> <span>1</span>
snip<span>.</span>rv <span>=</span> stripped<span>[</span><span>0</span><span>:</span>i<span>]</span> <span>+</span> <span>"\\frac{"</span> <span>+</span> stripped<span>[</span>i<span>+</span><span>1</span><span>:</span><span>-</span><span>1</span><span>]</span> <span>+</span> <span>"}"</span>
`</span>{<span>$1</span>}<span>$0</span>
<span>endsnippet</span></code></pre></div>
<p>The last snippet concerning fractions I’d like to share is one that uses your selection to make a fraction.  You can use it by first selecting some text, then pressing <kbd>Tab</kbd>, typing <code>/</code> and pressing <kbd>Tab</kbd> again.</p>
<p><img src="https://castel.dev/0da01b9566dc437fabf0c563ed14941f/visualfrac.gif" alt="visualfrac"></p>
<p>The code makes use of the <code>${VISUAL}</code> variable that represents your selection.</p>
<div data-language="snip"><pre><code><span>snippet <span>/ </span><span>"Fraction"</span> <span>iA</span></span>
\\frac{<span>${VISUAL}</span>}{<span>$1</span>}<span>$0</span>
<span>endsnippet</span></code></pre></div>
<h3 id="sympy-and-mathematica">Sympy and Mathematica</h3>
<p>Another cool — but less used — snippet is one that uses <a href="https://www.sympy.org/" target="_blank" rel="nofollow noopener noreferrer">sympy</a> to evaluate mathematical expressions. For example: <code>sympy</code> <kbd>Tab</kbd> expands to <code>sympy | sympy</code>, and <code>sympy 1 + 1 sympy</code> <kbd>Tab</kbd> expands to <code>2</code>.</p>
<p><img src="https://castel.dev/8c8f143fdcdc2f3e6de9853cf7ea3def/sympy.gif" alt="sympy"></p>
<div data-language="snip"><pre><code><span>snippet <span>sympy </span><span>"sympy block "</span> <span>w</span></span>
sympy <span>$1</span> sympy<span>$0</span>
<span>endsnippet</span>

<span>priority</span> 10000
<span>snippet <span>'sympy(.*)sympy' </span><span>"evaluate sympy"</span> <span>wr</span></span>
`<span>!p</span><span>
<span>from</span> sympy <span>import</span> <span>*</span>
x<span>,</span> y<span>,</span> z<span>,</span> t <span>=</span> symbols<span>(</span><span>'x y z t'</span><span>)</span>
k<span>,</span> m<span>,</span> n <span>=</span> symbols<span>(</span><span>'k m n'</span><span>,</span> integer<span>=</span><span>True</span><span>)</span>
f<span>,</span> g<span>,</span> h <span>=</span> symbols<span>(</span><span>'f g h'</span><span>,</span> cls<span>=</span>Function<span>)</span>
init_printing<span>(</span><span>)</span>
snip<span>.</span>rv <span>=</span> <span>eval</span><span>(</span><span>'latex('</span> <span>+</span> match<span>.</span>group<span>(</span><span>1</span><span>)</span><span>.</span>replace<span>(</span><span>'\\'</span><span>,</span> <span>''</span><span>)</span> \
    <span>.</span>replace<span>(</span><span>'^'</span><span>,</span> <span>'**'</span><span>)</span> \
    <span>.</span>replace<span>(</span><span>'{'</span><span>,</span> <span>'('</span><span>)</span> \
    <span>.</span>replace<span>(</span><span>'}'</span><span>,</span> <span>')'</span><span>)</span> <span>+</span> <span>')'</span><span>)</span>
`</span>
<span>endsnippet</span></code></pre></div>
<p>For the Mathematica users out there, you can do something similar:</p>
<p><img src="https://castel.dev/82f7f187a685a52ed7665fded563f936/mathematica.gif" alt="mathematica"></p>
<div data-language="snip"><pre><code><span>priority</span> 1000
<span>snippet <span>math </span><span>"mathematica block"</span> <span>w</span></span>
math <span>$1</span> math<span>$0</span>
<span>endsnippet</span>

<span>priority</span> 10000
<span>snippet <span>'math(.*)math' </span><span>"evaluate mathematica"</span> <span>wr</span></span>
`<span>!p</span><span>
<span>import</span> subprocess
code <span>=</span> <span>'ToString['</span> <span>+</span> match<span>.</span>group<span>(</span><span>1</span><span>)</span> <span>+</span> <span>', TeXForm]'</span>
snip<span>.</span>rv <span>=</span> subprocess<span>.</span>check_output<span>(</span><span>[</span><span>'wolframscript'</span><span>,</span> <span>'-code'</span><span>,</span> code<span>]</span><span>)</span>
`</span>
<span>endsnippet</span></code></pre></div>
<h3 id="postfix-snippets">Postfix snippets</h3>
<p>Some other snippets I find worth sharing are postfix snippets.
Examples of such snippets are <code>phat</code> → <code>\hat{p}</code> and <code>zbar</code> → <code>\overline{z}</code>.
A similar snippet is a postfix vector, for example <code>v,.</code> → <code>\vec{v}</code> and <code>v.,</code> → <code>\vec{v}</code>.
The order of <code>,</code> and <code>.</code> doesn’t matter, so I can press them both at the same time.
These snippets are a real time-saver, because you can type in the same order the lecturer writes on the blackboard.</p>
<p><img src="https://castel.dev/7c2e62aae292bc4d65eae363c816e206/barhatvec.gif" alt="barhatvec"></p>
<p>Note that I can still use <code>bar</code> and <code>hat</code> prefix too, as I’ve added them with a lower priority.
The code for those snippets is:</p>
<div data-language="snip"><pre><code><span>priority</span> 10
<span>snippet <span>"bar" </span><span>"bar"</span> <span>riA</span></span>
\overline{<span>$1</span>}<span>$0</span>
<span>endsnippet</span>

<span>priority</span> 100
<span>snippet <span>"([a-zA-Z])bar" </span><span>"bar"</span> <span>riA</span></span>
\overline{`<span>!p</span><span> snip<span>.</span>rv<span>=</span>match<span>.</span>group<span>(</span><span>1</span><span>)</span>`</span>}
<span>endsnippet</span></code></pre></div>
<div data-language="snip"><pre><code><span>priority</span> 10
<span>snippet <span>"hat" </span><span>"hat"</span> <span>riA</span></span>
\hat{<span>$1</span>}<span>$0</span>
<span>endsnippet</span>

<span>priority</span> 100
<span>snippet <span>"([a-zA-Z])hat" </span><span>"hat"</span> <span>riA</span></span>
\hat{`<span>!p</span><span> snip<span>.</span>rv<span>=</span>match<span>.</span>group<span>(</span><span>1</span><span>)</span>`</span>}
<span>endsnippet</span></code></pre></div>
<div data-language="snip"><pre><code><span>snippet <span>"(\\?\w+)(,\.|\.,)" </span><span>"Vector postfix"</span> <span>riA</span></span>
\vec{`<span>!p</span><span> snip<span>.</span>rv<span>=</span>match<span>.</span>group<span>(</span><span>1</span><span>)</span>`</span>}
<span>endsnippet</span> </code></pre></div>
<h3 id="other-snippets">Other snippets</h3>
<p>I have about 100 other commonly used snippets.
They are available <a href="https://github.com/gillescastel/latex-snippets" target="_blank" rel="nofollow noopener noreferrer">here</a>.
Most of them are quite simple.
For example, <code>!&gt;</code> becomes <code>\mapsto</code>, <code>-&gt;</code> becomes <code>\to</code>, etc.</p>
<p><img src="https://castel.dev/4ee36ef41c17f1ded1fffe13ba32b60d/complex5.gif" alt="complex5"></p>
<p><code>fun</code> becomes <code>f: \R \to \R :</code>, <code>!&gt;</code> → <code>\mapsto</code>, <code>-&gt;</code> → <code>\to</code>, <code>cc</code> → <code>\subset</code>.</p>
<p><img src="https://castel.dev/a45092c95d91059b779f74cedda840e1/fun3.gif" alt="fun3"></p>
<p><code>lim</code> becomes <code>\lim_{n \to \infty}</code>, <code>sum</code> → <code>\sum_{n = 1}^{\infty}</code>, <code>ooo</code> → <code>\infty</code></p>
<p><img src="https://castel.dev/2be9c99e6ab5787fc5196bbf4ce525a7/sum4.gif" alt="sum4"></p>
<p><img src="https://castel.dev/3be522f5e6e5ca7f871138732ba1b14d/bazel2.gif" alt="bazel2"></p>
<h3 id="course-specific-snippets">Course specific snippets</h3>
<p>Beside my commonly used snippets, I also have course specific snippets.
These are loaded by adding the following to my <code>.vimrc</code>:</p>
<div data-language="vim"><pre><code><span>set</span> <span>rtp</span><span>+=</span>~<span>/</span>current_course</code></pre></div>
<p>where <code>current_course</code> is a <a href="https://en.wikipedia.org/wiki/Symbolic_link" target="_blank" rel="nofollow noopener noreferrer">symlink</a> to my currently activated course (more about that in another blog post).
In that folder, I have a file <code>~/current_course/UltiSnips/tex.snippets</code> in which I include course specific snippets. For example, for quantum mechanics, I have snippets for bra/ket notation.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>&lt;a|</code></td>
<td>→</td>
<td><code>\bra{a}</code></td>
</tr>
<tr>
<td><code>&lt;q|</code></td>
<td>→</td>
<td><code>\bra{\psi}</code></td>
</tr>
<tr>
<td><code>|a&gt;</code></td>
<td>→</td>
<td><code>\ket{a}</code></td>
</tr>
<tr>
<td><code>|q&gt;</code></td>
<td>→</td>
<td><code>\ket{\psi}</code></td>
</tr>
<tr>
<td><code>&lt;a|b&gt;</code></td>
<td>→</td>
<td><code>\braket{a}{b}</code></td>
</tr>
</tbody>
</table>
<p>As <code>\psi</code> is used a lot in quantum mechanics, I replace all instances of <code>q</code> in a braket with <code>\psi</code> when expanded.</p>
<p><img src="https://castel.dev/268d331fdb83b6fa3563b6f2b0baac66/braket.gif" alt="braket"></p>
<div data-language="snip"><pre><code><span>snippet <span>"\&lt;(.*?)\|" </span><span>"bra"</span> <span>riA</span></span>
\bra{`<span>!p</span><span> snip<span>.</span>rv <span>=</span> match<span>.</span>group<span>(</span><span>1</span><span>)</span><span>.</span>replace<span>(</span><span>'q'</span><span>,</span> f<span>'\psi'</span><span>)</span><span>.</span>replace<span>(</span><span>'f'</span><span>,</span> f<span>'\phi'</span><span>)</span>`</span>}
<span>endsnippet</span>

<span>snippet <span>"\|(.*?)\&gt;" </span><span>"ket"</span> <span>riA</span></span>
\ket{`<span>!p</span><span> snip<span>.</span>rv <span>=</span> match<span>.</span>group<span>(</span><span>1</span><span>)</span><span>.</span>replace<span>(</span><span>'q'</span><span>,</span> f<span>'\psi'</span><span>)</span><span>.</span>replace<span>(</span><span>'f'</span><span>,</span> f<span>'\phi'</span><span>)</span>`</span>}
<span>endsnippet</span>

<span>snippet <span>"(.*)\\bra{(.*?)}([^\|]*?)\&gt;" </span><span>"braket"</span> <span>riA</span></span>
`<span>!p</span><span> snip<span>.</span>rv <span>=</span> match<span>.</span>group<span>(</span><span>1</span><span>)</span>`\braket<span>{</span>`!p snip<span>.</span>rv <span>=</span> match<span>.</span>group<span>(</span><span>2</span><span>)</span>`<span>}</span><span>{</span>`!p snip<span>.</span>rv <span>=</span> match<span>.</span>group<span>(</span><span>3</span><span>)</span><span>.</span>replace<span>(</span><span>'q'</span><span>,</span> f<span>'\psi'</span><span>)</span><span>.</span>replace<span>(</span><span>'f'</span><span>,</span> f<span>'\phi'</span><span>)</span>`</span>}
<span>endsnippet</span></code></pre></div>
<h3 id="context">Context</h3>
<p>One thing to consider when writing these snippets is, ‘will these snippets collide with usual text?’
For example, according to my dictionary, there are about 72 words in English and 2000 words in Dutch that contain <code>sr</code>, which means that while I’m typing the word <code>disregard</code>, the <code>sr</code> would expand to <code>^2</code>, giving me <code>di^2egard</code>.</p>
<p>The solution to this problem is adding a <em>context</em> to snippets. Using the syntax highlighting of Vim, it can be determined whether or not UltiSnips should expand the snippet depending if you’re in math or text. Add the following to the top of your snippets file:</p>
<div data-language="python"><pre><code><span>global</span> !p
<span>def</span> <span>math</span><span>(</span><span>)</span><span>:</span>
    <span>return</span> vim<span>.</span><span>eval</span><span>(</span><span>'vimtex#syntax#in_mathzone()'</span><span>)</span> <span>==</span> <span>'1'</span>

<span>def</span> <span>comment</span><span>(</span><span>)</span><span>:</span> 
    <span>return</span> vim<span>.</span><span>eval</span><span>(</span><span>'vimtex#syntax#in_comment()'</span><span>)</span> <span>==</span> <span>'1'</span>

<span>def</span> <span>env</span><span>(</span>name<span>)</span><span>:</span>
    <span>[</span>x<span>,</span>y<span>]</span> <span>=</span> vim<span>.</span><span>eval</span><span>(</span><span>"vimtex#env#is_inside('"</span> <span>+</span> name <span>+</span> <span>"')"</span><span>)</span> 
    <span>return</span> x <span>!=</span> <span>'0'</span> <span>and</span> x <span>!=</span> <span>'0'</span>

endglobal</code></pre></div>
<p>Now you can add <code>context "math()"</code> to the snippets you’d only want to expand in a mathematical context.</p>
<div data-language="snip"><pre><code><span>context</span> "math()"
<span>snippet <span>sr </span><span>"^2"</span> <span>iA</span></span>
^2
<span>endsnippet</span></code></pre></div>
<p>Note that a ‘mathematical context’ is a subtle thing.
Sometimes you add some text inside a math environment by using <code>\text{...}</code>.
In that case, you do not want snippets to expand.
However, in the following case: <code>\[ \text{$...$} \]</code>, they <em>should</em> expand.
The following animation illustrates these subtleties.</p>
<p><img src="https://castel.dev/e1b3ca2dd4fc1f5fe39679fb6ba38aad/syntaxtree.gif" alt="syntaxtree"></p>
<p>Similarly, you can add <code>context "env('itemize')"</code> to snippets that only should expand in an <code>itemize</code> environment or <code>context "comment()"</code> for snippets in comments.</p>
<details>
<summary>
This section was edited on 2020-12-24.
The new version of vimtex makes the code for detecting the context a lot simpler.
Click to see old version
</summary>
<div data-language="python"><pre><code><span>global</span> !p
texMathZones <span>=</span> <span>[</span><span>'texMathZone'</span> <span>+</span> x <span>for</span> x <span>in</span> <span>[</span><span>'A'</span><span>,</span> <span>'AS'</span><span>,</span> <span>'B'</span><span>,</span> <span>'BS'</span><span>,</span> <span>'C'</span><span>,</span> <span>'CS'</span><span>,</span>
<span>'D'</span><span>,</span> <span>'DS'</span><span>,</span> <span>'E'</span><span>,</span> <span>'ES'</span><span>,</span> <span>'F'</span><span>,</span> <span>'FS'</span><span>,</span> <span>'G'</span><span>,</span> <span>'GS'</span><span>,</span> <span>'H'</span><span>,</span> <span>'HS'</span><span>,</span> <span>'I'</span><span>,</span> <span>'IS'</span><span>,</span> <span>'J'</span><span>,</span> <span>'JS'</span><span>,</span>
<span>'K'</span><span>,</span> <span>'KS'</span><span>,</span> <span>'L'</span><span>,</span> <span>'LS'</span><span>,</span> <span>'DS'</span><span>,</span> <span>'V'</span><span>,</span> <span>'W'</span><span>,</span> <span>'X'</span><span>,</span> <span>'Y'</span><span>,</span> <span>'Z'</span><span>,</span> <span>'AmsA'</span><span>,</span> <span>'AmsB'</span><span>,</span> <span>'AmsC'</span><span>,</span>
<span>'AmsD'</span><span>,</span> <span>'AmsE'</span><span>,</span> <span>'AmsF'</span><span>,</span> <span>'AmsG'</span><span>,</span> <span>'AmsAS'</span><span>,</span> <span>'AmsBS'</span><span>,</span> <span>'AmsCS'</span><span>,</span> <span>'AmsDS'</span><span>,</span> <span>'AmsES'</span><span>,</span>
<span>'AmsFS'</span><span>,</span> <span>'AmsGS'</span> <span>]</span><span>]</span>

texIgnoreMathZones <span>=</span> <span>[</span><span>'texMathText'</span><span>]</span>

texMathZoneIds <span>=</span> vim<span>.</span><span>eval</span><span>(</span><span>'map('</span><span>+</span><span>str</span><span>(</span>texMathZones<span>)</span><span>+</span><span>", 'hlID(v:val)')"</span><span>)</span>
texIgnoreMathZoneIds <span>=</span> vim<span>.</span><span>eval</span><span>(</span><span>'map('</span><span>+</span><span>str</span><span>(</span>texIgnoreMathZones<span>)</span><span>+</span><span>", 'hlID(v:val)')"</span><span>)</span>

ignore <span>=</span> texIgnoreMathZoneIds<span>[</span><span>0</span><span>]</span>

<span>def</span> <span>math</span><span>(</span><span>)</span><span>:</span>
	synstackids <span>=</span> vim<span>.</span><span>eval</span><span>(</span><span>"synstack(line('.'), col('.') - (col('.')&gt;=2 ? 1 : 0))"</span><span>)</span>
	<span>try</span><span>:</span>
		first <span>=</span> <span>next</span><span>(</span>
            i <span>for</span> i <span>in</span> <span>reversed</span><span>(</span>synstackids<span>)</span>
            <span>if</span> i <span>in</span> texIgnoreMathZoneIds <span>or</span> i <span>in</span> texMathZoneIds
        <span>)</span>
		<span>return</span> first <span>!=</span> ignore
	<span>except</span> StopIteration<span>:</span>
		<span>return</span> <span>False</span>
endglobal</code></pre></div>
</details>
<h2 id="correcting-spelling-mistakes-on-the-fly">Correcting spelling mistakes on the fly</h2>
<p>While inserting mathematics is an important part of my note-taking setup, most of the time I’m typing English. At about 80 words per minute, my typing skills are not bad, but I still make a lot of typos.
This is why I added a keybinding to Vim that corrects the spelling mistakes, without interrupting my flow. When I press <kbd>Ctrl+L</kbd> while I’m typing, the previous spelling mistake is corrected. It looks like this:</p>
<p><img src="https://castel.dev/0add705683b79cb94498104251a6d5b8/spellcheck.gif" alt="spellcheck"></p>
<p>My configuration for spell check is the following:</p>
<div data-language="vim"><pre><code><span>setlocal</span> <span>spell</span>
<span>set</span> <span>spelllang</span><span>=</span>nl<span>,</span>en_gb
inoremap <span>&lt;</span>C<span>-</span><span>l</span><span>&gt;</span> <span>&lt;</span><span>c</span><span>-</span>g<span>&gt;</span><span>u</span><span>&lt;</span>Esc<span>&gt;</span><span>[</span>s1z<span>=</span>`<span>]</span>a<span>&lt;</span><span>c</span><span>-</span>g<span>&gt;</span><span>u</span></code></pre></div>
<p>It basically jumps to the previous spelling mistake <code>[s</code>, then picks the first suggestion <code>1z=</code>, and then jumps back <code>`]a</code>. The <code>&lt;c-g&gt;u</code> in the middle make it possible to undo the spelling correction quickly.</p>
<h2 id="in-conclusion">In conclusion</h2>
<p>Using snippets in Vim, writing LaTeX is no longer an annoyance, but rather a pleasure.
In combination with spell check on the fly, it allows for a comfortable mathematical note-taking setup.
A few pieces are missing though, for example drawing figures digitally and embedding them in a LaTeX document. This is a topic I’d like to tackle in a future blog post.</p>
<p>Liked this blog post? Consider <a href="https://castel.dev/coffee">buying me a coffee</a>!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deno in 2023 (273 pts)]]></title>
            <link>https://deno.com/blog/deno-in-2023</link>
            <guid>39259068</guid>
            <pubDate>Mon, 05 Feb 2024 09:23:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deno.com/blog/deno-in-2023">https://deno.com/blog/deno-in-2023</a>, See on <a href="https://news.ycombinator.com/item?id=39259068">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In 2023, Deno shifted into high gear with respect to Node/npm compatibility and
performance work, while continuing to move towards our goal of radically
simplifying web development. Here’s a summary of what changed in 2023:</p>
<ul>
<li>Deno now understands <code>package.json</code> files and has the ability to import
<strong>built-in Node modules</strong> using <code>node:</code> specifiers like <code>node:fs</code> and
<code>node:crypto</code>. <a href="#enhanced-compatibility-with-node-and-npm">Read more.</a></li>
<li>A new web server API,
<a href="#simpler-faster-web-server-with-denoserve"><code>Deno.serve</code>, was stabilized</a> and
HTTP throughput improved ~73% over the year.</li>
<li><strong><code>deno compile</code></strong> got support for
<a href="https://deno.com/blog/v1.32#deno-compile-support-for-web-workers-and-dynamic-import" rel="noopener noreferrer">workers, dynamic imports</a>,
and <a href="https://deno.com/blog/v1.34#deno-compile-supports-npm-packages" rel="noopener noreferrer">npm modules</a>.</li>
<li>Deno made better use of <strong>web streams</strong> (<code>ReadableStream</code> and
<code>WriteableStream</code>) and <code>AbortController</code> throughout its APIs to narrow the gap
between browser and server-side programming.</li>
<li><a href="#more-flexible-denojson">Deno’s configuration file, <code>deno.json</code> , now doubles as an import map</a>.
We flattened the schema, added glob support, and a useful top-level
<code>"exclude"</code> attribute — all which allows for terse adjustments to how Deno is
executed.</li>
<li><a href="#jupyter-notebooks"><strong>Jupyter</strong>, the open source notebook tool, added support for JavaScript and TypeScript using Deno</a></li>
<li><a href="#webgpu"><strong>WebGPU</strong> was finally added to Deno</a> after nearly a year of
development.</li>
<li>Deno’s <strong>zero configuration TypeScript</strong> got better editor integration with
<a href="#smoother-development-experience-with-lsp">substantial improvements in the LSP and VS Code integration</a>.</li>
<li><a href="#fresh"><strong>Fresh</strong>, the Deno native web framework grew in functionality</a> with
proper Tailwind support, Partials, and layout files.</li>
<li><a href="#expanding-denos-cloud-business"><strong>Deno Deploy</strong> became more powerful</a> with
globally distributed primatives: <strong>Deno KV, Queues, Cron</strong>.</li>
<li><a href="#expanding-denos-cloud-business">We released <strong>self-service Subhosting</strong></a> for
platforms looking to deploy and execute untrusted multi-tenant JavaScript.</li>
</ul>
<p>Let’s dive deeper into the changes below.</p>
<h2 id="enhanced-compatibility-with-node-and-npm">Enhanced Compatibility with Node and npm</h2><p>In a significant move toward ecosystem harmony, Deno expanded its capabilities
in 2023 by incorporating built-in Node modules, such as <code>node:fs</code> and
<code>node:child_process</code>, accessible through the <code>node:</code> specifier. This addition
builds on the native npm support introduced in late 2022, using the <code>npm:</code>
specifier, further bridging the gap between Deno and the Node ecosystem. As of
now, Deno boasts
<a href="https://docs.deno.com/runtime/manual/node/compatibility" rel="noopener noreferrer">partial or full support for 38 of the 42 Node built-in APIs</a>,
marking a substantial stride in compatibility.</p>
<p>To facilitate a smoother transition from Node to Deno, we introduced several new
features:</p>
<ul>
<li>The
<a href="https://docs.deno.com/runtime/manual/tools/unstable_flags#--unstable-byonm" rel="noopener noreferrer"><code>--unstable-byonm</code> flag</a>
allows the use of your preferred npm package manager within Deno, enhancing
flexibility.</li>
<li>With
<a href="https://docs.deno.com/runtime/manual/tools/unstable_flags#--unstable-sloppy-imports" rel="noopener noreferrer">the <code>--unstable-sloppy-imports</code> flag</a>,
we’ve relaxed the strictness of module imports, accommodating a wider range of
coding styles and practices.</li>
<li>The
<a href="https://docs.deno.com/runtime/manual/tools/unstable_flags#--unstable-unsafe-proto" rel="noopener noreferrer"><code>--unstable-unsafe-proto</code> flag</a>
introduces support for <code>Object.prototype.__proto__</code>, a feature upon which
numerous npm packages depend.</li>
</ul>
<p>These enhancements are particularly useful for developers looking to run
existing Node projects in Deno without extensive modifications.</p>
<p>A notable advancement in the past year was the
<a href="https://deno.com/blog/v1.31#stabilization-of-node-api" rel="noopener noreferrer">stabilization of the Node-API</a>. This
development eliminates the need for the <code>--unstable</code> flag when utilizing npm
packages dependent on Node-API, thereby broadening the range of supported npm
packages and streamlining the integration process.</p>
<h2 id="simpler-faster-web-server-with-denoserve">Simpler, faster web server with <code>Deno.serve()</code></h2><p>We’ve significantly streamlined web server creation in Deno by introducing and
stabilizing the <code>Deno.serve()</code> function, allowing developers to launch a server
with minimal code:</p>
<div><pre><span>Deno</span><span>.</span><span>serve</span><span>(</span><span>(</span><span>req</span><span>)</span> <span>=&gt;</span> <span>new</span> <span>Response</span><span>(</span><span>"Hello, world"</span><span>)</span><span>)</span><span>;</span></pre></div><p>This enhancement is part of our ongoing commitment to simplify development
processes and reduce the need for extensive boilerplate code. The <code>Deno.serve()</code>
function embodies this philosophy, enabling efficient and concise server setup.</p>
<p>Performance has seen substantial improvements through targeted optimizations in
the core libraries and the event loop mechanism. In benchmark tests using a
basic “Hello, world” server setup, Deno’s HTTP throughput has seen remarkable
gains, nearly doubling since late 2023. When compared to a similar Node.js
server, Deno now demonstrates a ~61% increase in throughput, alongside notable
enhancements in tail latency and memory efficiency.</p>
<figure>

<p><img src="https://deno.com/blog/deno-in-2023/http-benchmark.png" alt="HTTP benchmark in requests per second" title=""></p>
<figcaption>Run on bare metal 8 core,  64GB ram, Intel Xeon E-2378G with `wrk -d 10s`</figcaption>

</figure>

<p>These advancements are not limited to Deno-specific projects; they extend to
modules and applications utilizing the <code>node:http</code> module, thanks to
<code>Deno.serve</code>’s underlying architecture.</p>
<p>A key factor in these performance gains is the improved integration of Deno’s
HTTP interfaces with the <a href="https://hyper.rs/" rel="noopener noreferrer">Hyper</a> and
<a href="https://docs.rs/reqwest/latest/reqwest/" rel="noopener noreferrer">Reqwest</a> libraries. This integration
has minimized unnecessary data allocation and duplication across different
layers of the Deno runtime.</p>
<p>Furthermore, we’ve optimized the Deno event loop, responsible for managing
asynchronous operations and resource monitoring, to reduce overhead and enhance
the runtime’s overall efficiency. These collective improvements underscore our
dedication to providing a robust, high-performance environment for web
development.</p>
<h2 id="more-flexible-denojson">More flexible <code>deno.json</code></h2><p>At Deno, we stand by the principle of zero-configuration programming,
particularly valuing the simplicity of single-file programs, even for those
written in TypeScript. Recognizing that larger projects often demand more
sophisticated setups, we’ve continuously improved our optional <code>deno.json</code>
configuration file to meet these complex needs without sacrificing ease of use:</p>
<ul>
<li><strong>Streamlined Configuration</strong>: We’ve transformed <code>deno.json</code> to double as an
import map, effectively reducing the need for separate configuration files and
simplifying project setups.
<a href="https://deno.com/blog/v1.30#denojson-becomes-an-import-map" rel="noopener noreferrer">Learn more about import maps</a>.</li>
<li><strong>Enhanced Formatting Options</strong>: <code>deno fmt</code> now supports semicolons, offering
more flexibility in code styling to accommodate diverse developer preferences.</li>
<li><strong>Node and npm Compatibility</strong>: Integration with <code>package.json</code> enhances
compatibility, making it easier for projects to transition between Node and
Deno environments.
<a href="https://deno.com/blog/v1.31#packagejson-support" rel="noopener noreferrer">See how we improved compatibility</a>.</li>
<li><strong>Simplified Configuration Structure</strong>: We’ve flattened the <code>deno.json</code>
structure, making it more intuitive and easier to navigate for developers.
<a href="https://deno.com/blog/v1.33#flatter-denojson-configuration" rel="noopener noreferrer">Discover the simpler structure</a>.</li>
<li><strong>Glob Support</strong>: The introduction of glob patterns in <code>deno.json</code> allows for
more precise control over file and directory inclusion or exclusion in various
operations such as formatting, linting, and testing.
<a href="https://deno.com/blog/v1.34#glob-support-in-denojson-and-cli-flags" rel="noopener noreferrer">Explore glob support details</a>.</li>
</ul>
<p>These enhancements are part of our ongoing commitment to making Deno not only
powerful and versatile for large-scale applications but also simple and
accessible for smaller projects.</p>
<h2 id="smoother-development-experience-with-lsp">Smoother development experience with LSP</h2><p>Deno’s Language Server Protocol (LSP) integration elevates the development
experience within editors and IDEs, offering robust features like precise
go-to-definition, comprehensive IntelliSense, and seamless code formatting for
TypeScript projects. This year, we’ve dedicated significant effort to enhance
the LSP, making coding in Deno smoother and more intuitive:</p>
<ul>
<li><strong>Extended Auto-Complete</strong>: Now includes support for <code>npm:</code> and <code>node:</code>
specifiers, streamlining the use of Node modules within Deno.</li>
<li><strong>VSCode Extension Activation</strong>: The Deno VSCode extension is now triggered
automatically when a <code>deno.json</code> file is detected in your project, ensuring
immediate access to Deno’s powerful tooling.</li>
<li><strong>Intelligent Import Management</strong>: Imports in TypeScript and JavaScript files
are now updated automatically when files are renamed, maintaining code
consistency and reducing manual refactoring.</li>
<li><strong>Efficient Document Pre-Loading</strong>: Ensures features like “find references”
work seamlessly across all files in a project, enhancing code navigation and
understanding.</li>
</ul>
<p>To fully leverage these improvements, try the
<a href="https://marketplace.visualstudio.com/items?itemName=denoland.vscode-deno" rel="noopener noreferrer">Deno extension for Visual Studio Code</a>,
designed to integrate these enhancements directly into your development
workflow.</p>
<h2 id="webgpu">WebGPU</h2><p>Deno has now integrated
<a href="https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API" rel="noopener noreferrer">WebGPU</a>, a
cutting-edge technology that empowers developers to harness the power of GPU
hardware directly with JavaScript. This high-performance, low-level interface is
designed for a wide range of applications, from graphics rendering to data
analysis, and machine learning, all within the familiar environment of
web-standard JavaScript.</p>
<p>After a year of dedicated development, WebGPU is accessible in Deno behind the
<code>--unstable-webgpu</code> flag, marking a significant milestone in expanding the
capabilities of Deno applications. This feature is especially promising for
developers looking to push the boundaries of what’s possible with JavaScript in
areas requiring intense computational power.</p>
<p>We are also actively developing features to enable WebGPU for rendering in
native GUI windows, further broadening the potential use cases for Deno
developers.
<a href="https://deno.com/blog/v1.40#webgpu-windowing--bring-your-own-window" rel="noopener noreferrer">Stay updated on this feature’s progress</a>.</p>
<p>To explore practical applications and see WebGPU in action within Deno, visit
our <a href="https://github.com/denoland/webgpu-examples" rel="noopener noreferrer">WebGPU examples repository</a>,
which provides a variety of sample projects and code snippets.</p>
<h2 id="jupyter-notebooks">Jupyter notebooks</h2><p><a href="https://jupyter.org/" rel="noopener noreferrer">Jupyter</a>, the open source notebook tool, added support
for JavaScript and TypeScript using Deno. This means data science,
visualization, and more can all be done using modern JavaScript and TypeScript
and web standards APIs.</p>
<p>Here’s an example of grabbing data with <code>fetch</code> and visualizing it with
<code>observablehq/plot</code>:</p>
<p><img src="https://deno.com/blog/deno-in-2023/jupyter-scatter-plot.png" alt="A scatter plot in Jupyter notebooks using TypeScript with Deno" title=""></p>
<p>Jupyter support has also enabled building generative AI/ML models using
JavaScript and TypeScript, as Andrew Ng and DeepLearning.AI have developed
<a href="https://www.deeplearning.ai/short-courses/build-llm-apps-with-langchain-js/" rel="noopener noreferrer">a generative AI course on building LLM Apps with LangChain.js</a>
that uses Deno.</p>
<h2 id="notable-open-source-rust-crates">Notable open source rust crates</h2><p>Deno’s surface area touches a wide range of open source projects, which we
eagerly contribute to in order to expand Deno’s feature set and optimize
performance. This year, we released a few Rust crates that developers might find
useful independently of Deno itself:</p>
<ul>
<li><a href="https://github.com/denoland/rustls-tokio-stream" rel="noopener noreferrer"><code>rustls-tokio-stream</code></a> a
Rust crate that replaces tokio-rustls adding more advanced features like
supporting duplex I/O via&nbsp;<code>tokio::io::split</code>. Critically it does not require
either read or write polling to perform handshakes.</li>
<li><a href="https://github.com/denoland/fastwebsockets" rel="noopener noreferrer"><code>fastwebsockets</code></a>&nbsp;is a minimal,
fast WebSocket server implementation that sits behind Deno’s WebSocket
implementation. It completely passes the&nbsp;Autobahn TestSuite&nbsp;and fuzzed with
LLVM’s&nbsp;libfuzzer. You can use it as a raw websocket frame parser and deal with
spec compliance yourself, or you can use it as a full-fledged websocket
server.</li>
<li><a href="https://github.com/denoland/monch" rel="noopener noreferrer"><code>monch</code></a> is a parser inspired
by&nbsp;<a href="https://crates.io/crates/nom" rel="noopener noreferrer"><code>nom</code></a>, but specifically for strings and
with some additional combinators we use in Deno. It backs <code>deno_task_shell</code>.</li>
<li><a href="https://github.com/denoland/deno_task_shell" rel="noopener noreferrer"><code>deno_task_shell</code></a> is a cross
platform shell implementation that helps <code>deno task</code> run across windows and
unix. Think of it as a more advanced version of the common Node.js utility
<code>cross-env</code>.</li>
</ul>
<h2 id="fresh">Fresh</h2><p>We continued to make&nbsp;<a href="https://fresh.deno.dev/" rel="noopener noreferrer">Fresh</a>, Deno’s modern full stack
web framework that sends zero client-side JavaScript by default, easier to use
and more performant.</p>
<ul>
<li>We’ve added support
for&nbsp;<a href="https://deno.com/blog/fresh-1.4#layouts" rel="noopener noreferrer"><code>_layout</code>&nbsp;files to allow for sharing components across routes</a>,
increased flexibility
in&nbsp;<a href="https://deno.com/blog/fresh-1.4#organise-your-code-with-route-groups" rel="noopener noreferrer">organizing code with route groups</a>,
and removed boilerplate in passing data from a route handler to a component by
introducing&nbsp;<a href="https://deno.com/blog/fresh-1.3#async-route-components" rel="noopener noreferrer">async route components</a>.
We
also&nbsp;<a href="https://deno.com/blog/fresh-1.3#fresh-linting-rules" rel="noopener noreferrer">improved linting rules and error messages in the editor</a>,
as well
as&nbsp;<a href="https://deno.com/blog/fresh-1.2#simplified-testing-of-fresh-projects" rel="noopener noreferrer">simplified testing</a>.</li>
<li>Islands, the core of Fresh’s design, also received a ton of new features
making it more robust so it can handle a wider range of use cases.
We&nbsp;<a href="https://deno.com/blog/fresh-1.2#passing-signals-uint8arrays-and-circular-data-in-island-props" rel="noopener noreferrer">broadened the kinds of data that you could pass to island props</a>&nbsp;to
include Preact Signals, JSX, JSON, and more. Also, islands can
now&nbsp;<a href="https://deno.com/blog/fresh-1.2#passing-jsx-to-islands-and-nesting-islands-within-each-other" rel="noopener noreferrer">be nested</a>,&nbsp;<a href="https://deno.com/blog/fresh-1.2#subdirectories-in-the-islands-folder" rel="noopener noreferrer">organized in subdirectories</a>,
and&nbsp;<a href="https://deno.com/blog/fresh-1.3#export-multiple-islands-in-the-same-file" rel="noopener noreferrer">many of them can be exported from the same file</a>.</li>
<li>Fresh’s performance also increased, with the
new&nbsp;<a href="https://deno.com/blog/fresh-1.4#faster-page-loads-with-ahead-of-time-compilation-1" rel="noopener noreferrer">ahead-of-time compilation step to cache client-side assets</a>&nbsp;(you
can enable this optimization step in Deno Deploy with GitHub Actions instead
of GitHub Automatic), snappier client-side navigation
with&nbsp;<a href="https://deno.com/blog/fresh-1.5#client-side-navigation-with-partials" rel="noopener noreferrer">the introduction of Partials</a>,
as well as
an&nbsp;<a href="https://deno.com/blog/fresh-1.6#improved-islands-bundling-strategy" rel="noopener noreferrer">improved islands bundling strategy</a>.</li>
<li>We’ve improved Fresh plugin system to be able
add&nbsp;<a href="https://deno.com/blog/fresh-1.6#plugin-api-enhancements" rel="noopener noreferrer">islands</a>,&nbsp;<a href="https://deno.com/blog/fresh-1.3#adding-routes-andor-middlewares-from-plugins" rel="noopener noreferrer">middlewares, and routes</a>.
One notable win
is&nbsp;<a href="https://deno.com/blog/fresh-1.6#first-class-tailwind-css-plugin" rel="noopener noreferrer">moving off TwindCSS to TailwindCSS</a>,
which is actively maintained and is a performance boost. The Fresh community
also grew,
with&nbsp;<a href="https://github.com/uki00a/awesome-fresh/" rel="noopener noreferrer">more projects either built in Fresh or libraries and plugins built for Fresh</a>.</li>
</ul>
<p>We’ve got a lot planned for Fresh in 2024, such
as&nbsp;<a href="https://github.com/denoland/fresh/issues/2108" rel="noopener noreferrer">view transitions, hot module reloading, and faster JSX transforms</a>.</p>
<h2 id="expanding-denos-cloud-business">Expanding Deno’s cloud business</h2><p>Though we’ve covered the big updates from our open source projects, the overall
Deno picture would be incomplete without mentioning developments on the
commercial side.</p>
<p>Though&nbsp;<a href="https://deno.com/subhosting" rel="noopener noreferrer"><strong>Deno Subhosting</strong></a>&nbsp;has been around for a
while&nbsp;<a href="https://deno.com/blog/netlify-edge-functions-on-deno-deploy" rel="noopener noreferrer">powering Netlify’s edge functions</a>&nbsp;and&nbsp;<a href="https://deno.com/blog/deco-cx-subhosting-serve-their-clients-storefronts-fast" rel="noopener noreferrer">Deco.cx’s customer’s e-commerce stores</a>,
this year&nbsp;<a href="https://deno.com/blog/subhosting" rel="noopener noreferrer">we made it self-service</a>&nbsp;so that anyone can deploy
and run their user’s code
programmatically&nbsp;<a href="https://apidocs.deno.com/#get-/projects/-projectId-/deployments" rel="noopener noreferrer">via our Subhosting API</a>&nbsp;for
free. It’s built to run third-party, untrusted code securely, as it’s
<a href="https://deno.com/blog/subhosting-security-run-untrusted-code" rel="noopener noreferrer">designed from the ground up for maximum tenant isolation</a>.
(Not sure what to use Deno Subhosting API for? Check out
<a href="https://deno.com/blog/deno-in-2023#fresh-easier-and-faster-is-gaining-momentum-%EF%B8%8F" rel="noopener noreferrer">this tutorial on how to build your own cloud IDE</a>.)</p>
<p>We’re made strides towards our vision of radically simplifying web development
with the launch of cloud primitives: <strong><a href="https://deno.com/kv" rel="noopener noreferrer">Deno KV</a>, <a href="https://deno.com/blog/queues" rel="noopener noreferrer">Queues</a>,
and <a href="https://deno.com/blog/cron" rel="noopener noreferrer">Cron</a></strong>. They’re built right into the runtime so you can get
setup without juggling API keys or futzing with config:</p>
<div><pre><span>const</span> kv <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span></pre></div><p>These cloud primitives seamlessly become globally distributed services when you
use Deno Deploy, optimizing your servers and applications for performance.</p>
<p>We’ve also broadened access to Deno KV by turning it
into&nbsp;<a href="https://github.com/denoland/denokv" rel="noopener noreferrer">its own open sourced binary</a>, added
support&nbsp;<a href="https://docs.deno.com/kv/manual/on_deploy#connect-to-managed-databases-from-outside-of-deno-deploy" rel="noopener noreferrer">to remotely connect to any Deno KV instance</a>,
made it accessible in Node/npm
via&nbsp;<a href="https://www.npmjs.com/package/@deno/kv" rel="noopener noreferrer">our official Deno KV npm module</a>,
as well as
adding&nbsp;<a href="https://docs.deno.com/kv/manual/backup" rel="noopener noreferrer">support for continuous replication with point-in-time recovery to S3 and GCS</a>.</p>
<p>We’ve got some big plans to simplify cloud development even further with more
features and new primitives, so stay tuned.</p>
<h2 id="deno-2-">Deno 2 👀</h2><p>We’re preparing for Deno 2, which will offer improved compatibility with Node
and npm, by
<a href="https://docs.deno.com/runtime/manual/advanced/migrate_deprecations" rel="noopener noreferrer">providing a migration guide</a>
to ensure a smooth transition. Alongside an improved runtime, we also have some
exciting announcements around managing and optimizing dependencies for your
projects. Stay tuned in the coming weeks for a more detailed roadmap of what’s
to come. If you want a sneak peak - <a href="https://jsr.io/waitlist" rel="noopener noreferrer">look here</a>.</p>
<p><em>Don’t miss any updates! <a href="https://twitter.com/deno_land" rel="noopener noreferrer">Follow us on Twitter</a>.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zellij – A terminal workspace with batteries included (tmux alternative) (160 pts)]]></title>
            <link>https://zellij.dev/screencasts/</link>
            <guid>39258823</guid>
            <pubDate>Mon, 05 Feb 2024 08:52:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zellij.dev/screencasts/">https://zellij.dev/screencasts/</a>, See on <a href="https://news.ycombinator.com/item?id=39258823">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
    <main>

        

        <article>
            

            
            
            

                  
                
                <h3 id="basic-functionality--floating-panes--command-panes--scrollback-editingtutorialsbasic-functionality"><a href="https://zellij.dev/tutorials/basic-functionality">Basic Functionality + Floating Panes + Command Panes + Scrollback Editing</a></h3>
<figure><a href="https://zellij.dev/tutorials/basic-functionality">
    <img src="https://zellij.dev/img/tutorial-1-preview.png"> 
</a></figure><a href="https://zellij.dev/tutorials/basic-functionality">
</a>
Here we demonstrate some of the more basic functionality of Zellij and terminal multiplexers in general.
<p>We’ll show how to open multiple terminal tiled and floating panes.</p>
<p>Instead of retyping a command or searching through our shell history, we’ll see how we can use Command Panes to keep frequently run commands around.
Finally, we’ll talk about editing a pane’s scrollback with our own <code>$EDITOR</code></p>
<h3 id="layoutstutorialslayouts"><a href="https://zellij.dev/tutorials/layouts">Layouts</a></h3>
<figure><a href="https://zellij.dev/tutorials/layouts">
    <img src="https://zellij.dev/img/tutorial-2-preview.png"> 
</a></figure><a href="https://zellij.dev/tutorials/layouts">
</a>
<p>This tutorial walks you through creating Zellij <a href="https://zellij.dev/documentation/creating-a-layout">layouts</a> to automate tasks and workflows.</p>
<p>Layouts describe a pre-defined set of panes and tabs with different terminals, commands and plugins. They can be great to automate and formalize workflows and tasks.</p>

            </article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yandex to divest its Russia-based businesses (146 pts)]]></title>
            <link>https://ir.yandex/press-releases?year=2024</link>
            <guid>39258560</guid>
            <pubDate>Mon, 05 Feb 2024 08:18:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ir.yandex/press-releases?year=2024">https://ir.yandex/press-releases?year=2024</a>, See on <a href="https://news.ycombinator.com/item?id=39258560">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mount"><p><img src="https://yastatic.net/q/logoaas/v2/Yandex.png?size=35" alt="Yandex"></p><div><header><div><p><a href="https://yandex.com/">Yandex</a></p></div><p><nav><a href="https://ir.yandex/">For Investors</a><a href="https://yandex.com/company">Company</a><a href="https://yandex.com/jobs">Jobs</a><a href="https://tech.yandex.com/">For Developers</a><a href="https://yandex.com/adv">For Advertisers</a><a href="https://events.yandex.com/events">Events</a><a href="https://research.yandex.com/">Research</a></nav></p><div><p><a href="https://ir.yandex/">en</a><a href="https://ir.yandex.ru/">ru</a></p></div></header><nav><div><ul><li><a href="https://ir.yandex/">IR Home</a></li><li><a href="https://ir.yandex/financial-releases">Financials</a></li><li><a href="https://ir.yandex/press-releases">Press Releases</a></li><li><a href="https://ir.yandex/sec-filings">SEC Filings</a></li><li><a href="https://ir.yandex/shareholder-structure">For Shareholders</a></li><li><a href="https://ir.yandex/corporate-governance/">Corporate Governance</a></li><li><a href="https://ir.yandex/events-and-presentations">Events</a></li><li><a href="https://ir.yandex/analyst-coverage">Analysts</a></li><li><a href="https://ir.yandex/faq">FAQ</a></li><li><a href="https://ir.yandex/convertible-bonds">Convertible Bonds</a></li></ul></div></nav></div><div><div><nav><div><ul><li><a href="https://ir.yandex/">IR Home</a></li><li><a href="https://ir.yandex/financial-releases">Financials</a></li><li><a href="https://ir.yandex/press-releases">Press Releases</a></li><li><a href="https://ir.yandex/sec-filings">SEC Filings</a></li><li><a href="https://ir.yandex/shareholder-structure">For Shareholders</a></li><li><a href="https://ir.yandex/corporate-governance/">Corporate Governance</a></li><li><a href="https://ir.yandex/events-and-presentations">Events</a></li><li><a href="https://ir.yandex/analyst-coverage">Analysts</a></li><li><a href="https://ir.yandex/faq">FAQ</a></li><li><a href="https://ir.yandex/convertible-bonds">Convertible Bonds</a></li></ul></div></nav></div><div><p><a href="https://ir.yandex/">en</a><a href="https://ir.yandex.ru/">ru</a></p></div><div><p><header><nav><a href="https://ir.yandex/">For Investors</a><a href="https://yandex.com/company">Company</a><a href="https://yandex.com/jobs">Jobs</a><a href="https://tech.yandex.com/">For Developers</a><a href="https://yandex.com/adv">For Advertisers</a><a href="https://events.yandex.com/events">Events</a><a href="https://research.yandex.com/">Research</a></nav></header></p></div></div><div><div><p><span>2024</span><span>2023</span><span>2022</span><span>2021</span><span>2020</span><span>2019</span><span>2018</span><span>2017</span><span>2016</span><span>2015</span><span>2014</span><span>2013</span><span>2012</span><span>2011</span><span>2010</span><span>2009</span><span>2008</span><span>2007</span><span>2006</span><span>2005</span></p></div><div><div><p><a href="https://ir.yandex/press-releases?year=2024&amp;id=05-02-2024"><span>February 05, 2024</span><span>Yandex N.V. Announces Binding Agreement to Divest its Russia-based Businesses</span></a></p></div><div><p>Sign up to our news</p></div></div></div><div id="prefooter"><div><div><p><a rel="noopener noreferrer" href="https://ir.yandex/email-alerts"><span>Subscribe to Email alerts</span></a></p></div><div><p><a rel="noopener noreferrer" href="https://ir.yandex/contact-us"><span>Contact IR</span></a></p></div></div><div><div><p><h3>Yandex LLC</h3><h4>Head office in Russia: Moscow</h4></p><div><div><h5>Head office</h5><p>16, Leo Tolstoy St., Moscow, Russia 119021</p><div><p>tel.:&nbsp;<a href="tel:+7495739-70-00">+7 495 739-70-00</a></p></div><div><p>fax:&nbsp;<a href="tel:+7495739-70-70">+7 495 739-70-70</a></p></div></div><div><h5>Advertising clients</h5><div><p>tel.:&nbsp;<a href="tel:+7495739-37-77">+7 495 739-37-77</a></p></div><div><p>fax:&nbsp;<a href="tel:+7495739-23-32">+7 495 739-23-32</a></p></div><div><p><a href="mailto:adv@yandex-team.ru">adv@yandex-team.ru</a></p></div></div><div><h5>Investor Relations</h5><div><p>tel.:&nbsp;<a href="tel:+7495974-35-38">+7 495 974-35-38</a></p></div><div><p><a href="mailto:askIR@yandex-team.ru">askIR@yandex-team.ru</a></p></div></div><div><h5>Public relations</h5><div><p><a href="mailto:pr@yandex-team.ru">pr@yandex-team.ru</a></p></div></div><div><h5>Corporate Secretary</h5><div><p><a href="mailto:secretary@yandex-team.ru">secretary@yandex-team.ru</a></p></div></div><div><h5>Sustainability</h5><div><p><a href="mailto:sustainability@yandex-team.com">sustainability@yandex-team.com</a></p></div></div></div></div><div><p><h3>Yandex N.V.</h3><h4>Registered office in Amsterdam</h4></p><div><p>Schiphol Boulevard 165, 1118 BG Schiphol, The Netherlands</p><div><p>tel.:&nbsp;<a href="tel:+310202066970">+31 0 20 206 6970</a></p></div></div></div></div><div><p>Official Telegram channel for individual investors <a href="https://t.me/yndx_forinvestors" target="_blank">https://t.me/yndx_forinvestors</a> (in&nbsp;Russian only)</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub is preparing for IPv6 support for Github.com (104 pts)]]></title>
            <link>https://www.githubstatus.com/incidents/5y8b8lsqbbyq</link>
            <guid>39257841</guid>
            <pubDate>Mon, 05 Feb 2024 05:59:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.githubstatus.com/incidents/5y8b8lsqbbyq">https://www.githubstatus.com/incidents/5y8b8lsqbbyq</a>, See on <a href="https://news.ycombinator.com/item?id=39257841">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <div>
      <p>We are investigating reports of degraded performance.</p>
      
    </div>

    <div>
      <!-- postmortem if it's published -->

      <!-- incident updates in reverse order -->
        <div>
          <p>
            Resolved
          </p>
          <div>
            <p><span>This incident was the result of an infrastructure change that was made to our load balancers to prepare us for IPv6 enablement of GitHub.com. This change was deployed to a subset of our global edge sites.<p>The change had the unintended consequence of causing IPv4 addresses to start being passed as an IPv4-mapped IPv6-compatible address to our IP Allow List functionality.</p><p>For example 10.1.2.3 became ::ffff:10.1.2.3. While our IP Allow List functionality was developed with IPv6 in mind, it wasn't developed to handle these mapped addresses, and hence started blocking requests as it deemed these to be not in the defined list of allowed addresses. Request error rates peaked at 0.23% of all requests.</p><p>We have so far identified three remediation items here:</p><p>- Update the IP Allow List functionality to handle IPv4-mapped addresses.<br>- Audit the rest of our stack to confirm there are no further places this IPv4-mapped IPv6 addresses flaw exists.<br>- Improve our testing and monitoring processes to better catch these issues in the future.</p></span>
            </p>
            <p>
              Posted <span data-datetime-unix="1706713036000"></span>Jan <var data-var="date">31</var>, <var data-var="year">2024</var> - <var data-var="time">14:57</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>We have resolved the issue and confirmed all regions are now operating as expected.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1706713004000"></span>Jan <var data-var="date">31</var>, <var data-var="year">2024</var> - <var data-var="time">14:56</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>The fix for ip allow lists is currently rolling out; and we are awaiting confirmation from specific geographic regions.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1706712574000"></span>Jan <var data-var="date">31</var>, <var data-var="year">2024</var> - <var data-var="time">14:49</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>We are rolling out a fix to resolve the issues with IP allow lists. This should be resolved shortly.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1706711635000"></span>Jan <var data-var="date">31</var>, <var data-var="year">2024</var> - <var data-var="time">14:33</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Some customers are experiencing issues with IP allow lists.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1706710463000"></span>Jan <var data-var="date">31</var>, <var data-var="year">2024</var> - <var data-var="time">14:14</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Investigating
          </p>
          <div>
            <p><span>We are currently investigating this issue.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1706710451000"></span>Jan <var data-var="date">31</var>, <var data-var="year">2024</var> - <var data-var="time">14:14</var> UTC
            </p>
          </div>
        </div>

      <!-- affected components -->
    </div>

    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WebAssembly Playground (117 pts)]]></title>
            <link>https://observablehq.com/@chaosalchemist/wasm-playground</link>
            <guid>39257529</guid>
            <pubDate>Mon, 05 Feb 2024 04:58:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://observablehq.com/@chaosalchemist/wasm-playground">https://observablehq.com/@chaosalchemist/wasm-playground</a>, See on <a href="https://news.ycombinator.com/item?id=39257529">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><nav><div><a href="https://observablehq.com/"><svg role="img" viewBox="0 0 25 28" width="25" height="28" aria-label="Observable" fill="currentColor" style="width:22px"><path d="M12.5 22.6667C11.3458 22.6667 10.3458 22.4153 9.5 21.9127C8.65721 21.412 7.98339 20.7027 7.55521 19.8654C7.09997 18.9942 6.76672 18.0729 6.56354 17.1239C6.34796 16.0947 6.24294 15.0483 6.25 14C6.25 13.1699 6.30417 12.3764 6.41354 11.6176C6.52188 10.8598 6.72292 10.0894 7.01563 9.30748C7.30833 8.52555 7.68542 7.84763 8.14479 7.27274C8.62304 6.68378 9.24141 6.20438 9.95208 5.87163C10.6979 5.51244 11.5458 5.33333 12.5 5.33333C13.6542 5.33333 14.6542 5.58467 15.5 6.08733C16.3428 6.588 17.0166 7.29733 17.4448 8.13459C17.8969 8.99644 18.2271 9.9103 18.4365 10.8761C18.6448 11.841 18.75 12.883 18.75 14C18.75 14.8301 18.6958 15.6236 18.5865 16.3824C18.4699 17.1702 18.2639 17.9446 17.9719 18.6925C17.6698 19.4744 17.2948 20.1524 16.8427 20.7273C16.3906 21.3021 15.7927 21.7692 15.0479 22.1284C14.3031 22.4876 13.4542 22.6667 12.5 22.6667ZM14.7063 16.2945C15.304 15.6944 15.6365 14.864 15.625 14C15.625 13.1073 15.326 12.3425 14.7292 11.7055C14.1313 11.0685 13.3885 10.75 12.5 10.75C11.6115 10.75 10.8688 11.0685 10.2708 11.7055C9.68532 12.3123 9.36198 13.1405 9.375 14C9.375 14.8927 9.67396 15.6575 10.2708 16.2945C10.8688 16.9315 11.6115 17.25 12.5 17.25C13.3885 17.25 14.124 16.9315 14.7063 16.2945ZM12.5 27C19.4031 27 25 21.1792 25 14C25 6.82075 19.4031 1 12.5 1C5.59687 1 0 6.82075 0 14C0 21.1792 5.59687 27 12.5 27Z" fill="currentColor"></path></svg></a></div></nav><div><div><a href="https://observablehq.com/@chaosalchemist"><picture><source srcset="https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=60&amp;format=avif 1x, https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=120&amp;format=avif 2x, https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=180&amp;format=avif 3x" type="image/avif"><source srcset="https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=60&amp;format=webp 1x, https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=120&amp;format=webp 2x, https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=180&amp;format=webp 3x" type="image/webp"><source srcset="https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=60&amp;format=jpeg 1x, https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=120&amp;format=jpeg 2x, https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=180&amp;format=jpeg 3x" type="image/jpeg"><source srcset="https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=60&amp;format=png 1x, https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=120&amp;format=png 2x, https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=180&amp;format=png 3x" type="image/png"><img alt="@chaosalchemist" src="https://avatars.observableusercontent.com/avatar/29739a1b68a530a976fc61af325093d3fc1fccf4067ed4cfee2264934c2de71b?s=60"></picture></a><div><p><a href="https://observablehq.com/@chaosalchemist">Chaos Alchemist</a></p></div></div></div><div data-cy="metadata-bar"><div data-cy="access-level" title="Anyone can see edits to this notebook."><svg viewBox="0 0 16 16" width="16" height="16" fill="none" stroke="currentColor" stroke-width="1.8"><circle cx="8" cy="8" r="7.1"></circle><path d="M1.14285 10.5286 L14.8571 10.5286 M1.14285 5.71429H14.8571 M8 1.14285C7.42857 2.09523 5.14285 4.21428 5.14285 8C5.14285 12.5714 8 14.8571 8 14.8571 M8 1.14285C8.57143 2.09523 10.8571 4.21428 10.8571 8C10.8571 12.5714 8 14.8571 8 14.8571"></path></svg><p>Public</p></div><div title=""><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M1 14C1 13.4477 1.44772 13 2 13L4 13C4.55228 13 5 13.4477 5 14C5 14.5523 4.55228 15 4 15L2 15C1.44772 15 1 14.5523 1 14Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.884 3.65785L7.78484 10.3041L8.12765 11.9351L9.59745 11.1493L12.6967 4.50309L10.884 3.65785ZM10.823 1.42262C10.3224 1.18921 9.72744 1.40577 9.49404 1.90631L5.83135 9.76098C5.7399 9.95707 5.71453 10.1775 5.75904 10.3893L6.44467 13.6513C6.5818 14.3038 7.30684 14.6418 7.89476 14.3275L10.8344 12.7559C11.0252 12.6539 11.1778 12.4928 11.2692 12.2967L14.9319 4.44202C15.1653 3.94148 14.9487 3.3465 14.4482 3.11309L10.823 1.42262Z" fill="currentColor"></path></svg><p>Edited </p></div><div><svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor" style="flex-shrink:0"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.5 1.75C3.80964 1.75 3.25 2.30964 3.25 3C3.25 3.69036 3.80964 4.25 4.5 4.25C5.19036 4.25 5.75 3.69036 5.75 3C5.75 2.30964 5.19036 1.75 4.5 1.75ZM1.75 3C1.75 1.48122 2.98122 0.25 4.5 0.25C6.01878 0.25 7.25 1.48122 7.25 3C7.25 4.16599 6.52434 5.1625 5.5 5.56253V7H8.5C9.4199 7 10.1947 6.37895 10.4281 5.53327C9.44188 5.11546 8.75 4.13853 8.75 3C8.75 1.48122 9.98122 0.25 11.5 0.25C13.0188 0.25 14.25 1.48122 14.25 3C14.25 4.18168 13.5047 5.18928 12.4585 5.57835C12.1782 7.51343 10.5127 9 8.5 9H5.5V10.4375C6.52434 10.8375 7.25 11.834 7.25 13C7.25 14.5188 6.01878 15.75 4.5 15.75C2.98122 15.75 1.75 14.5188 1.75 13C1.75 11.834 2.47566 10.8375 3.5 10.4375L3.5 9V7V5.56253C2.47566 5.1625 1.75 4.16599 1.75 3ZM4.5 11.75C3.80964 11.75 3.25 12.3096 3.25 13C3.25 13.6904 3.80964 14.25 4.5 14.25C5.19036 14.25 5.75 13.6904 5.75 13C5.75 12.3096 5.19036 11.75 4.5 11.75ZM10.25 3C10.25 2.30964 10.8096 1.75 11.5 1.75C12.1904 1.75 12.75 2.30964 12.75 3C12.75 3.69036 12.1904 4.25 11.5 4.25C10.8096 4.25 10.25 3.69036 10.25 3Z"></path></svg><p><span>1&nbsp;fork</span></p></div><div><svg viewBox="0 0 16 16" fill="var(--white)" stroke="var(--moon-gray)" stroke-width="1.8" stroke-linecap="round" stroke-linejoin="round" width="16" height="16" style="width:16px;height:16px;stroke-width:2"><path d="M13.075 3.925A3.157 3.157 0 0 0 10.842 3c-.838 0-1.641.478-2.233 1.07L8 4.68l-.609-.61c-1.233-1.233-3.233-1.378-4.466-.145a3.158 3.158 0 0 0 0 4.467L3.534 9 8 13.788 12.466 9l.609-.608a3.157 3.157 0 0 0 0-4.467z"></path></svg><p>8</p><!-- --><p>&nbsp;Like</p><!-- --><p>s</p></div></div><div><p>5</p></div><div data-cy="welcome-mat-banner"><p>Start building your own data visualizations from examples like this.</p></div><div><div data-cy="welcome-mat-footer"><p><span>Powering the world’s best data teams. Use Observable to explore your data, create visualizations, and share insights faster.</span></p></div><div><div><p>More from Observable creators</p></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Caesars abruptly cancels contract with DEF CON (488 pts)]]></title>
            <link>https://forum.defcon.org/node/248360</link>
            <guid>39256930</guid>
            <pubDate>Mon, 05 Feb 2024 03:22:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forum.defcon.org/node/248360">https://forum.defcon.org/node/248360</a>, See on <a href="https://news.ycombinator.com/item?id=39256930">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
							
								
								<div> <p><a href="https://filedata/fetch?id=248362&amp;d=1707096806" target="_blank" rel="nofollow"></a><a href="https://forum.defcon.org/filedata/fetch?id=248362&amp;d=1707096806"><img alt="Click image for larger version  Name:	IMG_0685.jpg Views:	0 Size:	91.2 KB ID:	248362" title="IMG_0685.jpg" data-attachmentid="248362" width="600" height="464" data-align="center" src="https://forum.defcon.org/filedata/fetch?id=248362&amp;d=1707096806&amp;type=medium" data-fullsize-url="filedata/fetch?id=248362&amp;d=1707096806" data-thumb-url="filedata/fetch?id=248362&amp;d=1707096806&amp;type=thumb" data-title="Click on the image to see the original version" data-caption="IMG_0685.jpg"></a></p><br>
 </div>    <div>
<p><span><b>DEF CON was canceled. We un-canceled it.</b></span></p></div> <p>


After a great 25 year relationship Caesars abruptly terminated their contract with DEF CON, leaving us with no venue for DC 32, and just about seven months to Con!</p><p>

We don’t know why Caesars canceled us, they won’t say beyond it being a strategy change and it is not related to anything that DEF CON or our community has done. This kind of no-notice cancellation of a contract is unheard of in the conference business. The parting is confusing, but amicable.</p><p>

So now we have a challenge. Without a venue, will we be able to UN-Cancel DEF CON 32 before time runs out?</p><p>

Hackers are flexible. We find solutions. We need a space that can handle an event our size, and configurable enough to accommodate our content. We need a location close to our announced dates, and with super short notice... No small feat.</p><p>

We immediately scrambled a venue strike team to Las Vegas. Floors were walked. Meetings were held. Hands were shook and options weighed. When the smoke cleared, the field narrowed to one obvious choice and we began forging the requisite agreements.</p><p>

W00T! DEF CON Is UN-CANCELED!</p><p>

DEF CON 32 will still be August 8-11 2024, but now held at the Las Vegas Convention Center (LVCC) with workshops and training at the Sahara.</p><p>

DEF CON 32 will be an adventure where we can try things not possible in our old Casino Hotel spaces. What specifically you ask? Well we are still learning all the specifics but we will have more space, a proper food court, and the largest indoor venue LCD wall in the country.<br>
There are still many questions to be answered, and we have started a live FAQ section on the Forums for DEF CON 32 where we will be updating questions and answers. The initial FAQ is located here: <a href="https://forum.defcon.org/node/248358" target="_blank">https://forum.defcon.org/node/248358</a></p><p>

I look forward to seeing everyone this summer, the start of a new DEF CON era!</p><p>

The Dark Tangent</p><p>

P.S. We made shirts and stickers:<br>
<a href="https://shop.defcon.org/products/def-con-un-canceled-mens-t-shirt" target="_blank">https://shop.defcon.org/products/def...d-mens-t-shirt</a><br>
<a href="https://shop.defcon.org/products/def-con-un-canceled-sticker-set" target="_blank">https://shop.defcon.org/products/def...ed-sticker-set</a></p></div><div><p>
		
		
			Last edited by <a href="https://forum.defcon.org/member/6-dark-tangent" data-vbnamecard="6">The Dark Tangent</a>; <span>1 hour ago</span>.
		
		

	</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A practical guide to quitting your smartphone (187 pts)]]></title>
            <link>https://www.nytimes.com/2024/02/01/technology/iphone-mental-health-flip-phone.html</link>
            <guid>39256176</guid>
            <pubDate>Mon, 05 Feb 2024 01:03:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/02/01/technology/iphone-mental-health-flip-phone.html">https://www.nytimes.com/2024/02/01/technology/iphone-mental-health-flip-phone.html</a>, See on <a href="https://news.ycombinator.com/item?id=39256176">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/02/01/technology/iphone-mental-health-flip-phone.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[UUID Benchmark War (101 pts)]]></title>
            <link>https://ardentperf.com/2024/02/03/uuid-benchmark-war/</link>
            <guid>39254871</guid>
            <pubDate>Sun, 04 Feb 2024 21:50:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ardentperf.com/2024/02/03/uuid-benchmark-war/">https://ardentperf.com/2024/02/03/uuid-benchmark-war/</a>, See on <a href="https://news.ycombinator.com/item?id=39254871">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
<p>This month’s <a href="https://www.pgsqlphriday.com/rules/">PGSQL Phriday</a> #015 topic is about UUIDs, <a href="https://mydbanotebook.org/post/uuid-fight/">hosted by Lætitia Avrot</a>. Lætitia has called for a debate. No, no, no. I say let’s have an all-out war. A benchmark war.</p>



<p>I have decided to orchestrate a benchmark war between four different methods of storing a primary key:</p>



<ol>
<li>use a text field to store UUIDs</li>



<li>use PostgreSQL’s native <code>uuid</code> data type</li>



<li>use the new uuidv7 code currently in CommitFest which we’re hoping will be in PostgreSQL 17 <em>(i think we might still be waiting on something related to the approval process for the official standard)</em></li>



<li>use the classic, efficient, fast, sql-standard <code>bigint generated as identity</code> data type.</li>
</ol>



<p>The challenge is simple: <strong>insert one million rows into a large table, while concurrently querying it, AS FAST AS YOU CAN!!!</strong></p>



<p>Sadly I ran out of time. There’s one issue with my script, and I know how to fix it, but I need to get this blog published today or I won’t make the PGSQL Phriday cutoff!!</p>



<p>My problem is a bit amusing… <strong>the results successfully demonstrate how each option is better than the previous</strong>… until you get to bigint. I’ve demonstrated the size benefits of bigint but my uuidv7 performance is so good it basically matched bigint 😂 – but I’m guessing this is because my concurrent queries aren’t increasing cache pressure by using a uuid column yet… I suspect fixing this will demonstrate the performance gap between uuidv7 and bigint.</p>



<p>Regardless: there’s some good and useful stuff here – so let’s go ahead and dive in.</p>



<ol><li><a href="https://ardentperf.com/2024/02/03/uuid-benchmark-war/#results-summary">Results Summary</a></li><li><a href="https://ardentperf.com/2024/02/03/uuid-benchmark-war/#setup-overview">Setup Overview</a></li><li><a href="https://ardentperf.com/2024/02/03/uuid-benchmark-war/#cost">Cost</a></li><li><a href="https://ardentperf.com/2024/02/03/uuid-benchmark-war/#run-overview">Run Overview</a></li><li><a href="https://ardentperf.com/2024/02/03/uuid-benchmark-war/#run-details">Run Details</a></li><li><a href="https://ardentperf.com/2024/02/03/uuid-benchmark-war/#full-benchmark-scripts">Full Benchmark Scripts</a></li></ol>



<h2 id="results-summary">Results Summary</h2>



<figure><table><tbody><tr><td><strong>Test</strong></td><td><strong>Time to Insert One Million Rows</strong></td><td><strong>Average Rate</strong></td><td><strong>Size at Finish</strong></td><td><strong>Performance and Size Improvement</strong></td></tr><tr><td><strong>uuid::text</strong></td><td>410 sec</td><td>2421 tps</td><td>4.31 gb</td><td>–</td></tr><tr><td><strong>uuidv4</strong></td><td>375 sec</td><td>2670 tps</td><td>2.65 gb</td><td>10% faster and <br>63% smaller than text</td></tr><tr><td><strong>uuidv7</strong></td><td>290 sec</td><td>3420 tps</td><td>2.47 gb</td><td>30% faster and <br>7% smaller than uuidv4</td></tr><tr><td><strong>bigint</strong></td><td>290 sec</td><td>3480 tps</td><td>1.97 gb</td><td>same speed (in this test) and <br>25% smaller than uuidv7</td></tr></tbody></table></figure>



<figure><a href="https://ardentperf.files.wordpress.com/2024/02/image-4.png"><img data-attachment-id="3251" data-permalink="https://ardentperf.com/2024/02/03/uuid-benchmark-war/image-4-2/" data-orig-file="https://ardentperf.files.wordpress.com/2024/02/image-4.png" data-orig-size="3110,1732" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-4" data-image-description="" data-image-caption="" data-medium-file="https://ardentperf.files.wordpress.com/2024/02/image-4.png?w=300" data-large-file="https://ardentperf.files.wordpress.com/2024/02/image-4.png?w=750" width="1024" height="570" src="https://ardentperf.files.wordpress.com/2024/02/image-4.png?w=1024" alt="" srcset="https://ardentperf.files.wordpress.com/2024/02/image-4.png?w=1024 1024w, https://ardentperf.files.wordpress.com/2024/02/image-4.png?w=2048 2048w, https://ardentperf.files.wordpress.com/2024/02/image-4.png?w=150 150w, https://ardentperf.files.wordpress.com/2024/02/image-4.png?w=300 300w, https://ardentperf.files.wordpress.com/2024/02/image-4.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<figure><a href="https://ardentperf.files.wordpress.com/2024/02/image-5.png"><img data-attachment-id="3252" data-permalink="https://ardentperf.com/2024/02/03/uuid-benchmark-war/image-5-2/" data-orig-file="https://ardentperf.files.wordpress.com/2024/02/image-5.png" data-orig-size="3108,1734" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-5" data-image-description="" data-image-caption="" data-medium-file="https://ardentperf.files.wordpress.com/2024/02/image-5.png?w=300" data-large-file="https://ardentperf.files.wordpress.com/2024/02/image-5.png?w=750" width="1024" height="571" src="https://ardentperf.files.wordpress.com/2024/02/image-5.png?w=1024" alt="" srcset="https://ardentperf.files.wordpress.com/2024/02/image-5.png?w=1024 1024w, https://ardentperf.files.wordpress.com/2024/02/image-5.png?w=2048 2048w, https://ardentperf.files.wordpress.com/2024/02/image-5.png?w=150 150w, https://ardentperf.files.wordpress.com/2024/02/image-5.png?w=300 300w, https://ardentperf.files.wordpress.com/2024/02/image-5.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<h2 id="setup-overview">Setup Overview</h2>



<p>This benchmark is fully scripted and reproducible, based on my <a href="https://ardentperf.com/2024/01/08/copy-and-paste-a-new-postgres-dev-env-in-5-min/">Copy-and-Paste Postgres Dev Env</a>. Anyone can reproduce with a few simple copy-and-paste steps; the full details are at the bottom of this blog post.</p>



<p><strong>Processor:</strong> 1 full core AMD EPYC Genoa, 3.7 GHz, 1 thread per core, 2GB memory (ec2 c7a.medium)<br><strong>Storage:</strong> 16k IOPS, 1000 MB/s Throughput, 100 GB (ebs gp3)<br><strong>Operating System:</strong> Ubuntu 22.04 LTS, Kernel 6.2.0-1017-aws #17~22.04.1-Ubuntu<br><strong>PostgreSQL:</strong> main development branch as of Jan 29 2024 with v17 of the <a href="https://commitfest.postgresql.org/47/4388/">UUIDv7 patch</a><br><strong>Settings:</strong> shared_buffers=1G / max_wal_size=204800</p>



<p><strong>Schema:</strong> single table with 3 columns (id, foreign_table_id and data::bigint), data type for <code>id</code> and <code>foreign_table_id</code> follows the test<br><strong>Initial Table Size:</strong> 20 million rows</p>



<p><em>Aside: The new c7a/m7a/r7a EC2 instance family is interesting because of the switch to core-per-vCPU, similar to graviton. While the top-end <strong>r7i</strong>.48xlarge (intel) has 96 physical cores with hyperthreading, the top-end <strong>r7a</strong>.48xlarge (amd) is a beast with 192 physical cores and 1.5TB of memory. I look forward to playing with PostgreSQL on one of these machines someday.</em> 🏇</p>



<h2 id="cost">Cost</h2>



<p>While my copy-and-paste-dev-env defaults to the free tier, I switched to a non-bursting instance family and added some beefy gp3 storage for this performance test.</p>



<p>The storage is significantly over-provisioned and could definitely be scaled down on all three dimensions to save quite a bit of money. I was in a hurry to make the publishing deadline and didn’t take the time to optimize this part.</p>



<p>It took a total of 4 hours and 13 minutes to run the benchmark 3 times in a row. The setup steps are copy-and-paste and take less than 10 minutes so lets round up to 4.5 hours. I also repeated the full test (three loops) on a second server to verify performance consistency. According to the official AWS pricing calculator at <a href="https://calculator.aws/#/">calculator.aws</a> this is the cost breakdown:</p>



<ul>
<li>ec2 r7a.medium on-demand pricing: $0.07608/hr</li>



<li>ebs gp3 100GB 16k iops 100 MBps throughput: $0.14795/hr</li>



<li>two servers complete benchmark in 4.5 hours = total 9 hours</li>
</ul>



<p><strong>GRAND TOTAL for both servers = US$ 2.02</strong></p>



<p>Less than a cup of coffee.</p>



<p>Holy smokes batman – do we live in a different world than 20 years ago when it comes to the price of benchmarking or what?!! Also why is coffee so expensive?</p>



<p>Of course I had it running a bit longer while I was iterating and getting things ironed out, but I think the point stands. 🙂</p>



<h2 id="run-overview">Run Overview</h2>



<p>Tests:</p>



<ol>
<li><code>bigint generated by default as identity (cache 20)</code></li>



<li><code>text default gen_random_uuid()</code></li>



<li><code>uuid default gen_random_uuid()</code></li>



<li><code>uuid default uuidv7()</code> <em>(current proposed syntax; may change before future release)</em></li>
</ol>



<p>Workload:</p>



<pre><code>INIT_TABLE_ROWS=20000000<br>PGBENCH_TRANSACTIONS_PER_CLIENT=100000<br>PGBENCH_CLIENTS=10<p>echo "<br>    \set search_data random(1,$INIT_TABLE_ROWS)<br>    insert into records(data) values(random()*$INIT_TABLE_ROWS);<br>    select * from records where data=:search_data;<br>" &gt;txn.sql</p><p>pgbench --no-vacuum --transactions=$PGBENCH_TRANSACTIONS_PER_CLIENT<br>   --client=$PGBENCH_CLIENTS --file=txn.sql --progress=5 --report-per-command<br>   &gt;test_log.txt 2&gt;&amp;1</p></code></pre>



<h2 id="run-details">Run Details</h2>



<p>I ran the test on two servers, and on each server the whole test was repeated three times. Performance was consistent across both servers and all loops.</p>



<pre><code>grep "tps," test_log.txt</code></pre>



<figure><a href="https://ardentperf.files.wordpress.com/2024/02/image-6.png"><img data-attachment-id="3258" data-permalink="https://ardentperf.com/2024/02/03/uuid-benchmark-war/image-6-2/" data-orig-file="https://ardentperf.files.wordpress.com/2024/02/image-6.png" data-orig-size="2890,1754" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-6" data-image-description="" data-image-caption="" data-medium-file="https://ardentperf.files.wordpress.com/2024/02/image-6.png?w=300" data-large-file="https://ardentperf.files.wordpress.com/2024/02/image-6.png?w=750" width="1024" height="621" src="https://ardentperf.files.wordpress.com/2024/02/image-6.png?w=1024" alt="" srcset="https://ardentperf.files.wordpress.com/2024/02/image-6.png?w=1024 1024w, https://ardentperf.files.wordpress.com/2024/02/image-6.png?w=2046 2046w, https://ardentperf.files.wordpress.com/2024/02/image-6.png?w=150 150w, https://ardentperf.files.wordpress.com/2024/02/image-6.png?w=300 300w, https://ardentperf.files.wordpress.com/2024/02/image-6.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>One quick note – there is a dip in the TPS around 300 seconds. I haven’t verified in the logs but I suspect this is just the system checkpoint kicking in; I force a checkpoint right before starting each test and PostgreSQL defaults to 5 minute intervals. (And we increased the <code>max_wal_size</code> so that we wouldn’t get an early checkpoint.)</p>



<p>The data load of 20 million records took significantly longer for uuid::text than the other tests, which is reflected in the table sizes tracked over time.</p>



<pre><code>while true; do<br>  psql --csv -Xtc "select extract(epoch from now()),  relname, <br>                pg_relation_size(oid)<br>              from pg_class where relname like 'records%'<br>  "<br>  sleep 15<br>done &gt;relation_sizes.csv</code></pre>



<figure><a href="https://ardentperf.files.wordpress.com/2024/02/image-9.png"><img data-attachment-id="3265" data-permalink="https://ardentperf.com/2024/02/03/uuid-benchmark-war/image-9/" data-orig-file="https://ardentperf.files.wordpress.com/2024/02/image-9.png" data-orig-size="2340,1700" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-9" data-image-description="" data-image-caption="" data-medium-file="https://ardentperf.files.wordpress.com/2024/02/image-9.png?w=300" data-large-file="https://ardentperf.files.wordpress.com/2024/02/image-9.png?w=750" loading="lazy" width="1024" height="743" src="https://ardentperf.files.wordpress.com/2024/02/image-9.png?w=1024" alt="" srcset="https://ardentperf.files.wordpress.com/2024/02/image-9.png?w=1024 1024w, https://ardentperf.files.wordpress.com/2024/02/image-9.png?w=2045 2045w, https://ardentperf.files.wordpress.com/2024/02/image-9.png?w=150 150w, https://ardentperf.files.wordpress.com/2024/02/image-9.png?w=300 300w, https://ardentperf.files.wordpress.com/2024/02/image-9.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>The best way to get a fast, high-level look at what’s happening internally with PosgreSQL is to use wait events. We will zoom in on the higlighted section above, with <code>uuid</code> runs from the first loop and <code>bigint</code> from the second loop.</p>



<p>It’s immediately obvious that the biggest difference between the runs is the amount of time spent in <code>IO:DataFileRead</code> by database connections executing SQL.</p>



<pre><code>while true; do<br>  psql --csv -Xtc "select extract(epoch from now()),query,wait_event_type,wait_event <br>                   from pg_stat_activity <br>                   where (application_name='pgbench' or query like 'insert%')<br>                     and state='active';<br>  "<br>  sleep 15<br>done &gt;wait_events.csv</code></pre>



<figure><a href="https://ardentperf.files.wordpress.com/2024/02/image-10-5.png"><img data-attachment-id="3304" data-permalink="https://ardentperf.com/2024/02/03/uuid-benchmark-war/image-10-5/" data-orig-file="https://ardentperf.files.wordpress.com/2024/02/image-10-5.png" data-orig-size="2690,1630" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-10-5" data-image-description="" data-image-caption="" data-medium-file="https://ardentperf.files.wordpress.com/2024/02/image-10-5.png?w=300" data-large-file="https://ardentperf.files.wordpress.com/2024/02/image-10-5.png?w=750" loading="lazy" width="1024" height="620" src="https://ardentperf.files.wordpress.com/2024/02/image-10-5.png?w=1024" alt="" srcset="https://ardentperf.files.wordpress.com/2024/02/image-10-5.png?w=1024 1024w, https://ardentperf.files.wordpress.com/2024/02/image-10-5.png?w=2046 2046w, https://ardentperf.files.wordpress.com/2024/02/image-10-5.png?w=150 150w, https://ardentperf.files.wordpress.com/2024/02/image-10-5.png?w=300 300w, https://ardentperf.files.wordpress.com/2024/02/image-10-5.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>We need to read blocks from the disk when they are not in the PostgreSQL buffer cache. Conveniently, PostgreSQL makes it very easy to inspect the contents of the buffer cache. This is where the big difference between uuidv4 and uuidv7 becomes clear. Because of the lack of data locality in uuidv4 data, the primary key index is consuming a huge amount of the buffer cache in order to support new data being inserted – and this cache space is no longer available for other indexes and tables, and this significantly slows down the entire workload. Putting it another way: for some workloads uuidv4 significantly increases the total working set size.</p>



<pre><code>while true; do<br>  psql --csv -Xtc "create extension if not exists pg_buffercache" -c "<br>          SELECT extract(epoch from now()), n.nspname, c.relname, count(*) AS buffers<br>             FROM pg_buffercache b JOIN pg_class c<br>             ON b.relfilenode = pg_relation_filenode(c.oid) AND<br>                b.reldatabase IN (0, (SELECT oid FROM pg_database<br>                                      WHERE datname = current_database()))<br>             JOIN pg_namespace n ON n.oid = c.relnamespace<br>             GROUP BY n.nspname, c.relname<br>             ORDER BY count(*) DESC<br>             LIMIT 10;<br>  "<br>  sleep 15<br>done &gt;buffer_cache.csv</code></pre>



<figure><a href="https://ardentperf.files.wordpress.com/2024/02/image-10-4.png"><img data-attachment-id="3303" data-permalink="https://ardentperf.com/2024/02/03/uuid-benchmark-war/image-10-4/" data-orig-file="https://ardentperf.files.wordpress.com/2024/02/image-10-4.png" data-orig-size="2892,1756" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-10-4" data-image-description="" data-image-caption="" data-medium-file="https://ardentperf.files.wordpress.com/2024/02/image-10-4.png?w=300" data-large-file="https://ardentperf.files.wordpress.com/2024/02/image-10-4.png?w=750" loading="lazy" width="1024" height="621" src="https://ardentperf.files.wordpress.com/2024/02/image-10-4.png?w=1024" alt="" srcset="https://ardentperf.files.wordpress.com/2024/02/image-10-4.png?w=1024 1024w, https://ardentperf.files.wordpress.com/2024/02/image-10-4.png?w=2045 2045w, https://ardentperf.files.wordpress.com/2024/02/image-10-4.png?w=150 150w, https://ardentperf.files.wordpress.com/2024/02/image-10-4.png?w=300 300w, https://ardentperf.files.wordpress.com/2024/02/image-10-4.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>PostgreSQL uses a clock sweep algorithm to manage its cache. This is different from the linux page cache active/inactive system; PostgreSQL tracks a “usage count” on each buffer that can range from 0 to 5. The usage count is incremented any time a block already in the cache is used, and decremented any time a connection needs to use a block that isn’t currently in the cache – it sweeps through the cache and looks for a block that’s eligible for eviction, decrementing usage counts as it goes. Hot blocks like index root nodes naturally maintain a high usage count and stay in the cache. Cache management algorithms are always an interesting topic (for example, see <a href="https://brooker.co.za/blog/2023/12/15/sieve.html">Marc Brooker’s recent blog article about SIEVE</a>). I am a fan of PostgreSQL’s current algorithm – I think it works well and it’s easy to understand and reason about.</p>



<p>Not too long ago, Nathan Bossart added a very light-weight function to PostgreSQL to allow more detailed inspection of usage count data. Lets see what it looked like for this zoomed in time window.</p>



<pre><code>while true; do<br>  psql --csv -Xtc "SELECT extract(epoch from now()),* FROM pg_buffercache_usage_counts();"<br>  sleep 15<br>done &gt;usage_counts.csv</code></pre>



<figure><a href="https://ardentperf.files.wordpress.com/2024/02/image-10-1.png"><img data-attachment-id="3273" data-permalink="https://ardentperf.com/2024/02/03/uuid-benchmark-war/image-10-1/" data-orig-file="https://ardentperf.files.wordpress.com/2024/02/image-10-1.png" data-orig-size="2892,1754" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-10-1" data-image-description="" data-image-caption="" data-medium-file="https://ardentperf.files.wordpress.com/2024/02/image-10-1.png?w=300" data-large-file="https://ardentperf.files.wordpress.com/2024/02/image-10-1.png?w=750" loading="lazy" width="1024" height="621" src="https://ardentperf.files.wordpress.com/2024/02/image-10-1.png?w=1024" alt="" srcset="https://ardentperf.files.wordpress.com/2024/02/image-10-1.png?w=1024 1024w, https://ardentperf.files.wordpress.com/2024/02/image-10-1.png?w=2048 2048w, https://ardentperf.files.wordpress.com/2024/02/image-10-1.png?w=150 150w, https://ardentperf.files.wordpress.com/2024/02/image-10-1.png?w=300 300w, https://ardentperf.files.wordpress.com/2024/02/image-10-1.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>I’ve looked at a number of charts like this from different workloads over the past year or two. My experience has been that when the data set fits in the cache we see everything at usage count 5 (as expected), and when the working set is larger than the cache there’s always a spectrum between hot and cold similar to this picture. And the balance across the spectrum reflects the balance of pressure between hits and misses on the cache. I’ve seen where a single large query can cause a spike in lower-usage-count buffers, and then the lines slope back downward as the workload pushes usage counts back up. In this particular workload, we see the large blocks of usage count 5 during initial data load before the data set reaches 1GB and we see the spectrum of usage counts once it exceeds 1GB. During the benchmark itself, the lines slope very slightly upward because our benchmark is adding a million rows, so we’re slowly increasing the total working set size, which causes more cache miss pressure relative to cache hits.</p>



<p>Interesting stuff! Hope this makes it clear why you should never store your UUIDs in a text field, and why some of us are excited for uuidv7 when it’s eventually ready. It should also be clear how even with uuidv7 there is still a total size penalty for choosing UUID (128-bit) over bigint (64-bit). And hopefully we can demonstrate at some point in the future how that can translate into performance impact for some workloads.</p>



<h2 id="full-benchmark-scripts">Full Benchmark Scripts</h2>



<p>To run this benchmark, <strong>follow the instructions at</strong> <a href="https://ardentperf.com/2024/01/08/copy-and-paste-a-new-postgres-dev-env-in-5-min/">Copy-and-Paste Postgres Dev Env</a> with the following modifications:</p>



<figure><table><tbody><tr><td></td><td>Open the web page <a href="https://ardentperf.com/2024/01/08/copy-and-paste-a-new-postgres-dev-env-in-5-min/">Copy-and-Paste Postgres Dev Env</a> and begin following those steps. Make the adjustments listed below.</td></tr><tr><td>step1</td><td>Copy the following command to switch to c7a.medium and a gp3 volume with the specs above:<p><code>aws ec2 run-instances --region us-east-1 --key-name $KEY --instance-type c7a.medium --tag-specifications "ResourceType=instance,Tags=[{Key=Name,Value=pgdev}]" --image-id ami-0c7217cdde317cfec --block-device-mappings "DeviceName=/dev/sda1,Ebs={VolumeSize=100,VolumeType=gp3,Iops=16000,Throughput=1000}"</code></p></td></tr><tr><td>step4</td><td>After checking out the PostgreSQL source code, copy the following commands to switch to a commit from January 29 2024 and apply v17 of the UUIDv7 patch from the PostgreSQL mailing lists.<p><code>cd git/postgres</code><br><code><br>git checkout 97287bdfae41</code><br><code><br>wget https://www.postgresql.org/message-id/attachment/155245/v17-0001-Implement-UUID-v7.patch</code><br><code><br>patch -p1 &lt;v17-0001-Implement-UUID-v7.patch</code></p><p>—–</p><p><em>The current commitfest entry for UUIDv7 is at <a href="https://commitfest.postgresql.org/47/4388/">https://commitfest.postgresql.org/47/4388/</a><p>v17 of the patch is available from this email <a href="https://www.postgresql.org/message-id/0906F8BA-CA52-4956-AA68-E9193E50DFDF%40yandex-team.ru">https://www.postgresql.org/message-id/0906F8BA-CA52-4956-AA68-E9193E50DFDF%40yandex-team.ru</a></p><p>You can see recent commits to the PostgreSQL main development branch at <a href="https://git.postgresql.org/gitweb/?p=postgresql.git;a=shortlog">https://git.postgresql.org/gitweb/?p=postgresql.git;a=shortlog</a></p></em></p></td></tr><tr><td>step5</td><td>After compiling PostgreSQL, copy the following commands to apply two custom parameters before we start running our benchmark.<p><code>echo "max_wal_size=204800" &gt;&gt;$HOME/etc/postgres.d/postgresql.dev.conf</code><br><code><br>echo "shared_buffers=1GB" &gt;&gt;$HOME/etc/postgres.d/postgresql.dev.conf</code></p></td></tr><tr><td>benchmark</td><td>The benchmark is very simple and straightforward. I’ve copied the code into a GIST that you can copy or download.<p><a href="https://gist.github.com/ardentperf/da1e10641983a94c51f3261f4890ffb1">https://gist.github.com/ardentperf/da1e10641983a94c51f3261f4890ffb1</a></p><p>The key bits for quick reference:</p><p><code>create table records (<br> id $1 primary key<br> ,foreign_table_id $2<br> ,data bigint<br>);<br>create index on records(data);<br>insert into records(data) select random()*$INIT_TABLE_ROWS from generate_series(1, $INIT_TABLE_ROWS) s;<br>vacuum analyze records;<br>checkpoint;</code></p></td></tr></tbody></table></figure>

				

				
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Netflix: Piracy is difficult to compete against and growing rapidly (387 pts)]]></title>
            <link>https://torrentfreak.com/netflix-piracy-is-difficult-to-compete-against-and-growing-rapidly-240204/</link>
            <guid>39254807</guid>
            <pubDate>Sun, 04 Feb 2024 21:44:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/netflix-piracy-is-difficult-to-compete-against-and-growing-rapidly-240204/">https://torrentfreak.com/netflix-piracy-is-difficult-to-compete-against-and-growing-rapidly-240204/</a>, See on <a href="https://news.ycombinator.com/item?id=39254807">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>

<span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to TorrentFreak." href="https://torrentfreak.com/"><span property="name">Home</span></a><meta property="position" content="1"></span> &gt; <span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to the Piracy category archives." href="https://torrentfreak.com/category/piracy/"><span property="name">Piracy</span></a><meta property="position" content="2"></span> &gt; <span></span>
</p>
<p>
<span> </span>
As a member of ACE and the MPA, Netflix is at the frontline of the global battle against online piracy. The company doesn't often address the subject directly but in a recent SEC filing, Netflix writes that it's difficult to compete against the free entertainment piracy offers. Not only that, it's growing rapidly too.
</p>
</div><div>
<p><img decoding="async" src="https://torrentfreak.com/images/netflix-logo-1.jpg" alt="netflix logo" width="300" height="180" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20180'%3E%3C/svg%3E" data-lazy-src="https://torrentfreak.com/images/netflix-logo-1.jpg">From the launch of its online streaming service fifteen years ago, Netflix positioned itself as a piracy competitor.</p>
<p>The idea was to take market share away from piracy sites, by offering a legal and more convenient streaming platform.</p>
<p>Initially, this seemed to work. Netflix amassed hundreds of millions of subscribers, some of whom left their piracy habits behind. However, as the ‘streaming wars’ turned legal and convenient streaming platforms into isolated and pricey content silos, momentum started to shift. </p>
<p>In recent years piracy <a href="https://torrentfreak.com/canada-is-a-video-piracy-hotspot-while-brazil-shows-positive-signs-240121/">started to grow again</a>, including in well-served markets such as the United States. In theory, this <a href="https://torrentfreak.com/could-piracy-help-netflix-win-the-streaming-wars-240108/">may help Netflix</a> in its battle with other legal platforms, but that’s a consolation prize if the war against piracy is lost. </p>
<p>There are no concrete signs that Netflix is crumbling, but piracy is a concern. This <a href="https://torrentfreak.com/netflix-sees-popcorn-time-as-a-serious-competitor-150121/">isn’t breaking news</a>; piracy has been repeatedly highlighted as tough competition in the company’s <a href="https://www.investopedia.com/terms/1/10-k.asp">10-K filings</a> at the SEC.</p>
<h2>Piracy is a Tough Competitor</h2>
<p>Earlier this week, Netflix submitted its latest 10-K filing. The mandatory document provides information that helps investors to gather key information about publicly traded companies. In the “competition” section of the annual overview, piracy is again mentioned several times.</p>
<center><img decoding="async" src="https://torrentfreak.com/images/sec-netflix.jpg" alt="sec netflix" width="600" height="366" srcset="https://torrentfreak.com/images/sec-netflix.jpg 1166w, https://torrentfreak.com/images/sec-netflix-300x183.jpg 300w" sizes="(max-width: 600px) 100vw, 600px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20600%20366'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/sec-netflix.jpg 1166w, https://torrentfreak.com/images/sec-netflix-300x183.jpg 300w" data-lazy-src="https://torrentfreak.com/images/sec-netflix.jpg"></center>
<p>Netflix explains that the online video landscape is a competitive business. New services and distribution models could impact the business of the leading video streaming platform. This includes legal competitors as well as piracy. </p>
<p>“The various economic models underlying these channels include subscription, transactional, ad-supported and piracy-based models. All of these have the potential to capture meaningful segments of the entertainment video market,” Netflix writes. </p>
<p>These are in part standard disclosures, as every company faces competition. However, Netflix believes that online piracy is particularly compelling because it’s free for consumers. That makes it very hard to compete against. </p>
<p>“Piracy also threatens to damage our business, as its fundamental proposition to consumers is so compelling and difficult to compete against: virtually all content for free,” Netflix writes.</p>
<h2>Growing and Hard to Stop</h2>
<p>When Netflix launched, its on-demand streaming experience was more convenient than most pirate sites. At the time, torrent sites were dominant but still required users to have some technical knowledge and the patience to wait for content to download.</p>
<p>Today, most pirate sites use on-demand streaming, taking away a major edge for Netflix. And because piracy is so compelling for consumers, it is growing rapidly worldwide, threatening legal services. </p>
<p>“In light of the compelling consumer proposition, piracy services are subject to rapid global growth, and our efforts to prevent that growth may be insufficient,” Netflix notes. </p>
<p>“If we are unable to successfully or profitably compete with current and new competitors, our business will be adversely affected, and we may not be able to increase or maintain market share, revenues or profitability.”</p>
<h2>(Un)authorized Copying?</h2>
<p>The concerns voiced by Netflix are real, but the company isn’t near its demise. These 10-K filings are supposed to detail risks and Netflix is not the only company mentioning piracy as a potential threat. </p>
<center><strong>A Netflix Competitor</strong></center><br><center><img decoding="async" src="https://torrentfreak.com/images/netflix-compet.jpg" alt="netflix competitor" width="600" height="464" srcset="https://torrentfreak.com/images/netflix-compet.jpg 1342w, https://torrentfreak.com/images/netflix-compet-300x232.jpg 300w" sizes="(max-width: 600px) 100vw, 600px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20600%20464'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/netflix-compet.jpg 1342w, https://torrentfreak.com/images/netflix-compet-300x232.jpg 300w" data-lazy-src="https://torrentfreak.com/images/netflix-compet.jpg"></center>
<p>When we started looking for similar mentions by other businesses, we stumbled upon similar concerns and, strangely enough, some identical ones. Apparently, there’s quite a bit of copying going on, as SEC filings from several companies include identical passages.</p>
<p><em><a href="https://d18rn0p25nwr6d.cloudfront.net/CIK-0001065280/c5e64982-659f-4726-97c9-c57767c3bec3.pdf">Netflix</a>: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth”</em></p><p><em>
<p><a href="https://www.sec.gov/Archives/edgar/data/1936037/000119312524007757/d356530ds1a.htm">Triller Corp</a>: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth”</p>
<p><a href="https://www.sec.gov/Archives/edgar/data/1484769/000162828023005135/fubo-20221231.htm">FuboTV</a>: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth”</p>
<p><a href="https://ir.cssentertainment.com/node/11811/html">Redbox Entertainment</a>: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth”</p>
<p><a href="https://finance.yahoo.com/sec-filing/IMAQ/0001654954-23-010817_1846235?guccounter=1">IMAQ</a>: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth”</p>
</em></p><p><em><a href="https://www.sec.gov/Archives/edgar/data/1776909/000121390020035136/f424b31120_curiositystream.htm">CuriosityStream</a>: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth”</em></p>
<p>We don’t know where these references originate. Netflix has mentioned it for a while, that’s for sure, and apparently, the use of this language is widespread and subject to rapid global growth.</p>
<p>It’s clear, however, that piracy is a concern for Netflix. While Reed Hastings <a href="https://torrentfreak.com/netflix-uses-pirate-sites-to-determine-what-shows-to-buy-130914/">wasn’t worried about piracy</a> a decade ago, the company now spends millions of dollars tackling the problem. </p>
<p>The streaming giant <a href="https://torrentfreak.com/netflix-becomes-a-member-of-the-mpaa/">joined the MPA</a> a few years ago and is also a <a href="https://torrentfreak.com/mpaa-dramatically-expanding-ace-global-anti-piracy-coalition-190507/">member of anti-piracy coalition ACE</a>. In addition, Netflix also has an <a href="https://torrentfreak.com/netflix-continues-to-expand-its-global-anti-piracy-team-220307">in-house anti-piracy department</a> that keeps an eye on piracy threats.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Solving the darknet riddle (2022) (113 pts)]]></title>
            <link>https://sizeof.life/posts/darknet-riddle/</link>
            <guid>39254274</guid>
            <pubDate>Sun, 04 Feb 2024 20:47:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sizeof.life/posts/darknet-riddle/">https://sizeof.life/posts/darknet-riddle/</a>, See on <a href="https://news.ycombinator.com/item?id=39254274">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <p>An ‘.onion’ link was posted on Reddit back in June 2020 with a suspected riddle. It got me curious.</p>
<p>Some random characters and a sentence: “How deep can you enter?”</p>
<p>Within the page source-code a comment was present with a series of numbers/letters that looked like a hexadecimal.</p>

  <figure>
    <img src="https://sizeof.life/img/hex.png">
    
  </figure>


<p>After converting the hex to ASCII it read: ‘chaos is the key’.</p>
<p>I noticed that when the page is refreshed, every time a different letter is highlighted in yellow.</p>
<p>When the ‘x’ char was highlighted a link was appended to the word ‘enter’…

  </p><figure>
    <img src="https://sizeof.life/img/x.png">
    
  </figure>


<p>The link was to an image (below).</p>

  <figure>
    <img src="https://sizeof.life/img/hidden_message01.png">
    
  </figure>


<p>After google translating the text in the image which did not reveal anything useful, it was time to examine the image file itself.</p>
<p>I suspected the next “message” would be hidden in image metadata or perhaps a pixel analysis would need to be done but after downloading the file and opening it, the message was already visible.</p>
<p>See the image area below, it may not be visible in the browser but if you download it and look closely enough you’ll see it.</p>

  <figure>
    <img src="https://sizeof.life/img/hidden_message02.png">
    
  </figure>


<blockquote>
<p>The hidden message contained another riddle:
Congrats! You passed level 1
#2 Send an email
TO: A000668(9)@8,41235641483227
SUBJECT: hello world</p>
</blockquote>
<p>The domain part of the email was easier to figure out, after a quick google it was apparent that it’s ‘protonmail.com’.</p>
<p>The first part of the email turned out to be a the 9th <a href="https://en.wikipedia.org/wiki/Mersenne_prime">Mersenne prime</a> number. Making the email address <a href="https://sizeof.life/cdn-cgi/l/email-protection#6e5c5d5e5b565a5d5e5e575c5f5d58575d575b5f2e1e1c011a0100030f0702400d0103"><span data-cfemail="1d2f2e2d2825292e2d2d242f2c2e2b242e24282c5d6d6f72697273707c7471337e7270">[email&nbsp;protected]</span></a>.</p>
<p>Few hours after sending the email I received a response with a binary code in the message.</p>

  <figure>
    <img src="https://sizeof.life/img/binary.png">
    
  </figure>


<p>Binary conversion revealed an URL that contained a text file called ‘haystack.txt’.</p>

   <figure>
     <img src="https://sizeof.life/img/haystack.png">
     
   </figure>
 

<p>There was also an instruction in another file that stated:</p>

  <figure>
    <img src="https://sizeof.life/img/hint.png">
    
  </figure>


<p>At this point it was apparent that there is a ‘needle in a haystack’ file.</p>
<p>The numbers in the text file appeared to be prime numbers and my next thought was to check if there are any non-primes in the file.</p>
<p>There was 1000000 numbers in the file. Manually checking each one was out of the question so I turned for help to Python. I wrote a quick script to check for primes, output non-prime numbers (if any) and left it running.</p>
<p>After a while one non-prime number was returned.
<img src="https://sizeof.life/img/script.png" alt="script"></p>
<p>Next part of the puzzle was to navigate to the provided URL and use the needle somehow. The URL contained a a short message:</p>
<blockquote>
<p>‘If you can not get me what I need,
maybe you must start at Level 0‘</p>
</blockquote>
<p><img src="https://sizeof.life/img/needle.png" alt="nedle"></p>
<p>There was no inputs or buttons on the page but there was some clues left. It took me a short while to figure it out. The clues were hidden in the following parts of the puzzle:</p>
<p>Clue 1:
<img src="https://sizeof.life/img/clue1.png" alt="clue1"></p>
<p>Clue 2:
<img src="https://sizeof.life/img/clue2.png" alt="clue2"></p>
<p>Clue 3:
<img src="https://sizeof.life/img/clue3.png" alt="clue3"></p>
<p>The ‘get‘ word was chosen purposely and it referred to a http GET request.</p>
<p>After making the GET request while passing the non-prime ‘needle’ as the parameter another message appeared.
<img src="https://sizeof.life/img/get.png" alt="get-request"></p>
<p>The message was in hexadecimal and after converting it, another puzzle was revealed:</p>
<blockquote>
<p>uonlyneed3dovetails2findwhere2go
mfcJE9adl3v7z/Tuxeo5XqM.eTzKB/Y#nsglRevPnr7atQ</p>
</blockquote>
<p><strong>‘You only need 3 dovetails 2 find where 2 go’</strong></p>

  <figure>
    <img src="https://sizeof.life/img/640px-Riffle_shuffle.jpg">
    
  </figure>


<p>This took me a while to figure out. At first I though it may have something to do with the ‘ / ‘ but after a quick research I realized that <a href="https://en.wikipedia.org/wiki/Shuffling#Riffle">dovetail</a> is also a thing in card games and it’s a riffle-shuffle technique.</p>
<p>This would mean that the provided string needs to be riffle-shuffled 3 times to reveal <em>where to go</em>.</p>
<p>I seem to have lost the python script I wrote back then to do this for me but after it has done shuffling, the string has formed another URL.</p>
<p>After entering the URL, it seemed like it was the last puzzle (at least until more parts are developed):</p>
<p><img src="https://sizeof.life/img/end.png" alt="get-request"></p>
<p>That was FUN!</p>
<p>To whoever designed this puzzle: well done! It was very interesting.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stract: Open-souce, non-profit search engine (366 pts)]]></title>
            <link>https://stract.com/</link>
            <guid>39254172</guid>
            <pubDate>Sun, 04 Feb 2024 20:36:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stract.com/">https://stract.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39254172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Customise your search with an  <a href="https://stract.com/settings/optics" data-svelte-h="svelte-1prlvom">optic</a>:</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dijkstra's interview on Dutch TV (2000) (132 pts)]]></title>
            <link>https://pncnmnp.github.io/blogs/translating-dijakstra.html</link>
            <guid>39253944</guid>
            <pubDate>Sun, 04 Feb 2024 20:16:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pncnmnp.github.io/blogs/translating-dijakstra.html">https://pncnmnp.github.io/blogs/translating-dijakstra.html</a>, See on <a href="https://news.ycombinator.com/item?id=39253944">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><a id="rss-feed" href="https://pncnmnp.github.io/rss.xml">If you like this blog post, do subscribe to my RSS feed</a></p>
<h2>First Published: 1<sup>st</sup> May 2021</h2>
<p><i>Writers make national literature, while translators make universal literature. - José Saramago</i></p><p>Among computer science graduates, <a href="https://en.wikipedia.org/wiki/Edsger_W._Dijkstra">Edsgar W. Dijkstra</a> is a man who needs little introduction. A few decades back, an interesting interview of his <a href="https://www.cs.utexas.edu/users/EWD/video-audio/NoorderlichtVideo.html">was broadcasted on Dutch TV</a>. The video description is as follows:</p>
<blockquote>In the autumn of 2000, the Dutch broadcasting organization VPRO Television visited Austin to make a video of a visit with the most famous Dutch computing scientist. The product of this project was broadcast in April 2001 as a 25-minute episode of the science series Noorderlicht, under the title "Denken Als Discipline".</blockquote>
<p>Roughly a year after this video was broadcasted, <a href="https://www.cnet.com/news/computer-science-pioneer-dijkstra-dies/">Prof. Dijkstra died</a> after a long struggle with cancer. This blog post would not have been possible without the help of some kind souls (Jos Wassink, Karin Spiegel, and Chris Kotrla) who dedicated their time to digitize and translate the above interview in English. I am merely curating the thoughts Prof. Dijkstra expressed during the interview, for improved readability.</p> 

<h2>The Interview</h2>
<p><b>Prof. Dijkstra:</b> You just cobble something together to sell. It need not be any good. As long as you can fool people into buying it, you can always try to make better versions later. So then you get these version numbers, even with decimals, version 2.6 or 2.7. That's nonsense. While version 1 should have been the finished product.</p>
<blockquote>Computer science is no more about computers than astronomy is about telescopes.</blockquote>
<p><b>Narrator:</b> Professor Edsger W. Dijkstra is Holland's first programmer. In 1972, he received the Turing Award, the Nobel Prize of Computer Science. He and his wife now live in Austin, Texas, where they moved in 1984.</p>
<p><b>Prof. Dijkstra:</b> At the time, most university departments in the Netherlands aimed to water down their curriculum. At the same time, the University of Texas in Austin tried to reduce the student enrollment and to increase their quality. It was an opposite development which was much more attractive than what was happening in Dutch higher education. </p>
<blockquote>The universities will continue to lack the courage to teach hard science, they will continue to misguide the students, and each next stage of infantilization of the curriculum will be hailed as educational progress.</blockquote>
<p><b>Narrator:</b> Quality, correctness, and elegance are what Dijkstra thinks should characterize a computer program. In 1954 he resolved to make programming a science, but it has been an uphill struggle.</p>
<p><b>Prof. Dijkstra:</b> I lose no sleep that businesses feel they cannot afford to deliver first-rate products. It doesn't keep me from continuing my work.</p>
<blockquote>You should not give the world what it asks for, but what it needs.</blockquote>
<p><b>Prof. Dijkstra:</b> There are very different programming styles. I tend to see them as Mozart versus Beethoven. When Mozart started to write, the composition was finished. He wrote the manuscript in one go. In beautiful handwriting, too. Beethoven was a doubter and a struggler, who started writing before he finished the composition and then glued corrections onto the page. In one place he did this nine times. When they peeled them, the last version proved identical to the first one. That iterative method of programming is somehow a very Anglo-Saxon custom. British education is pervaded by it. People learn, when they write, not to try to get it right the first time. Just write what's on your mind and then rewrite repeatedly, to get the product you want. That's partly why word processors are marketed so aggressively and partly why they have been so successful there. While it is one of the advantages of working with pen and paper that when you start a sentence, you should have it ready.</p>
<p><b>Narrator:</b> Dijkstra is a prolific writer: musings, talks, mathematical proofs, and books. His EWD's, these initials followed by a serial number are considered his best scientific contributions.</p>
<p><b>Prof. Dijkstra:</b> For myself, the most important thing has been the daily discipline of neatly writing down your thoughts and what you do. Due to modern technology, they have been much more influential than they would have been in the past. You might describe them as a modern form of scientific correspondence. Albeit that the intellectual traffic has been mostly one-directional. The serial numbers have crept up to over 1300. In length, they vary enormously, from 80 pages to one page. As time goes by, they tend to become ever shorter. Everyone to whom I sent them, was implicitly willing to function as an internal node of the dissemination tree and to send second-generation copies on to others. How many people they have reached, I have never been able to estimate. A few hundred, I'd say.</p>
<p><b>Prof. Dijkstra:</b> In order to compose, you have to write scores. But to be a composer is not to write scores. To be a composer is to conceive music. In the early days of programming, you had to write machine code. Meaningless sequences of capitals and numbers. That's the analog of writing scores. People thought that <i>that</i> was programming. Later, that was made easier by the invention of the higher programming languages: Fortran, Pascal, C++, and suchlike. People thought that those languages would solve the programming problem. But when you look closely, the trivial aspects of programming had been automated while the harder ones remained. The higher programming languages which had been intended to facilitate programming proved, coupled with the increasing ambitions of the applications to be more intellectually demanding to the programmer.</p>
<blockquote>The competent programmer is fully aware of the limited size of his own skull. He, therefore, approaches his tasks in full humility and avoids clever tricks like the plague. - EWD 340</blockquote>
<p><b>Prof. Dijkstra:</b> I remember, in 1970 or thereabouts, I first went to explain to companies how to develop programs and keep them under tight control. I first went to Paris and then to Brussels. In Paris, I delivered a lecture at the Sorbonne and people were very enthusiastic. On the way home, I told the same story at a large software house in Brussels. The lecture was a complete failure. In a sense, one of my least successful lectures. I later found out why. The management didn't want faultless programs because the company derived its stability from maintenance contracts. And the programmers weren't interested because they derived their intellectual excitement from the fact that they didn't quite know what they were doing. They felt that if you knew precisely what you were doing and didn't run risks, it was a boring job.</p>
<blockquote>We should not add bugs to a program out of nonchalance. We should do so systematically and with great care.</blockquote>
<p><b>Prof. Dijkstra:</b> If in physics there's something you don't understand you can always hide behind the uncharted depths of nature. You can always blame God. You didn't make it so complex yourself. But if your program doesn't work, there is no one to hide behind. You cannot hide behind an obstinate nature. A zero is a zero, a one is a one. If it doesn't work, you've messed up.</p>
<blockquote>I realized my previous projects had been finger exercises. I had to tackle complexity itself. But it took a long time to muster the courage to do so.</blockquote>
<p><b>Narrator:</b> At the end of the 60s, Dijkstra saw that the complexity got the better of the programmers. And that it threatened the most prestigious projects.</p>
<p><b>Prof. Dijkstra:</b> That happened in 1969 just after the first successful moon landing. I was at a NATO conference on software engineering in Rome where I met Joel Aron, who was head of IBM's Federal Systems Division, which had been responsible for the software of the moon shot. I knew that each Apollo flight required some 40,000 new lines of code. I don't know what unit a line of code is but 40,000 is a lot. I was duly impressed that they got so many lines of code correct. So when I met Joel, I said: "how do you do it?". "Do what?", he asked. "Getting that software right". "Right?", he said. He said that in one of the calculations of the orbit of the lunar module, the moon had been defined as repelling instead of attracting. They had discovered that error by accident. Imagine, <i>by accident</i> five days before the shot. I went white and said: "Those guys have been lucky!" Yes, Joel Aron agreed.</p>
<blockquote>Program testing can convincingly show the presence of bugs, but it hopelessly inadequate to show their absence.</blockquote>
<p><b>Narrator:</b> Dijkstra knew personally how frighteningly complex programs could become. He had finished the operating system of Holland's biggest computer, the X-8 under great pressure.</p>
<p><b>Prof. Dijkstra:</b> I feared that I wouldn't get it right. That I would lose control of it. For the X-8 in Eindhoven, we tried to design a multiprogramming system. We were working with a machine with a real-time interrupt which really means that you cannot test your program. I knew that if we designed the program with insufficient care, we would end up with something which, once entered into the computer wouldn't work, and would make irreproducible errors, the cause of which we would be unable to determine. That was the reason why we applied all possible control mechanisms in the development stage.</p>
<blockquote>Elegance is not a dispensable luxury but a factor that decides between success and failure.</blockquote>
<p><b>Prof. Dijkstra:</b> One of the things I discovered as early as the 1960s is that mathematical elegance is not a question of esthetics, of taste, or fashion, but something you can translate into a technical notion. The Concise Oxford Dictionary gives one of the meanings of elegant as <i>ingeniously simple and effective</i>. In practice, a program is manageable if you make a truly elegant program, firstly because it is shorter than most alternatives and consists of discrete paths each of which you can replace by an alternative implementation without influencing the rest of the program. But also, curiously the most elegant programs are often the most efficient.</p>
<blockquote>When there were no computers, programming was no problem. When we had a few weak computers, it became a mild problem. Now that we have gigantic computers, programming is a gigantic problem.</blockquote>
<p><b>Prof. Dijkstra:</b> My first programming years were a bit strange compared to now in that I programmed for non-existent machines. My friends Bram Loopstra and Carel Scholten built the machine and in the meantime, I wrote the relevant software. I was used to not testing a program because the machine to test it on wasn't finished. I knew right away that you had to create something that could keep under your intellectual control.</p>
<p><b>Prof. Dijkstra:</b> I started work at the Mathematical Centre in March 1952. My wife-to-be had worked there since the summer of 1949. She had a job as a calculator. I was taken on as a programmer. And I rather liked her. Our first date was the concert on the occasion of the Mathematical Congress in 1954 in Amsterdam. Then the news was out that I was sweet on her. People complimented me on my taste. Professionally, I was strongly influenced by my mother. She was a brilliant mathematician. I remember when I had bought my books for the next school year I saw the goniometry book, which scared me with all those Greek letters. I asked my mother if goniometry was hard. She said, "not at all". "Make sure you know all the formulae by heart and if you need more than five lines, you're on the wrong track."</p>
<p><b>Prof. Dijkstra:</b> Why has elegance found so little following? That is the reality of it. Elegance has the disadvantage if that's what it is that hard work is needed to achieve it and a good education to appreciate it.</p>

<p><a id="back-link" href="https://pncnmnp.github.io/blog.html">←</a>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sapling: Experimental vi-inspired editor where you edit code, not text (178 pts)]]></title>
            <link>https://github.com/kneasle/sapling</link>
            <guid>39253798</guid>
            <pubDate>Sun, 04 Feb 2024 20:01:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kneasle/sapling">https://github.com/kneasle/sapling</a>, See on <a href="https://news.ycombinator.com/item?id=39253798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:kneasle/sapling" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Fsrp2c6q423zz6YqE3COJ13hI1gxgZJF7u1vW0MznVOU3CfAiahsOiJGTwC0rg9BW4yt5wuwFh4xtYWYtITvjA" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="kneasle/sapling" data-current-org="" data-current-owner="kneasle" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=eIDiJxiRn9X4Ldpmp%2FYINDNuwHLz1I8s2NIaxvfFgi%2F%2FniPwb2VIgOMzeCXS8YpMtM%2BxzMsBWlAHPXcxZhZcOq0eC3q%2Bymk%2FPbNphvk84Nb1EFTy87QL2MXuqfzMkLJPVYWM9CutROcLSZwBR0TS5Ln%2FWqy%2FlLgOmlijy0J1KTv5IiRKsEyv0gTx15ANtPf1jc%2FxuHsAspCdzeiVadX2VN%2Fg6Y0JlYjmz6gvW47YkYpMUGdB5nbDlLxd82dOUw9g0ZxlA7%2B3Pb7wW%2FbM6NjbRt4RXlbIdmNJwsOgAi0g9YGOLnvK6AOBWn1Qh2bXvifCt05JuYAKAVAouse1KJ2%2BWO5BvYmlvHSUqgIslLmQ4itFpomkHftLyHUBlJlU0Q3Ar9wjt0nlI9dtSFlvkZpNaRgn9nhiQkj5%2FISgU0uuNrSZL3BEdYfTGQyMcnX7wj2wRVaE1XE6xJpzh5xH%2Fn9%2FLjr9glF0JeFy%2FRCO30YLvQCJ6pBOXQm3raXBKBl28UYg2PyxtFfPvfFgZA%3D%3D--6hPzDRoQuNTrC3ih--Q53Zv4sllbX0E3fLdnEPQw%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=kneasle%2Fsapling" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/kneasle/sapling&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="5b16bce859c90f213c6c1a71abc030b2573635ddcd37619222be5d27b402d63f" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How the codpiece flopped (127 pts)]]></title>
            <link>https://www.bbc.com/future/article/20240202-what-happened-to-the-codpiece</link>
            <guid>39253603</guid>
            <pubDate>Sun, 04 Feb 2024 19:40:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/future/article/20240202-what-happened-to-the-codpiece">https://www.bbc.com/future/article/20240202-what-happened-to-the-codpiece</a>, See on <a href="https://news.ycombinator.com/item?id=39253603">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="futurearticle20240202-what-happened-to-the-codpiece"><div id="headline-futurearticle20240202-what-happened-to-the-codpiece"><div><p>(Image credit: </p><!-- --><p>Getty Images</p><!-- --><p>)</p></div><div><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h8ywt4.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h8ywt4.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h8ywt4.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h8ywt4.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h8ywt4.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h8ywt4.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h8ywt4.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h8ywt4.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="A replica of Henry VIII's suit of armour being loaded into a car (Credit: Getty Images)" src="https://ychef.files.bbci.co.uk/976x549/p0h8ywt4.jpg" alt="A replica of Henry VIII's suit of armour being loaded into a car (Credit: Getty Images)" id=""></picture></div></div><div><article><div><p>Some codpieces were empty – while others were used to store potpourri.</p><div><p>S</p><div><p>Some time around 1536, Hans Holbein the Younger was finessing Henry VIII's crotch. With a fine brush in his hand and a palette of watercolour paints beside him, the master artist took pains to give his client's ornately decorated bulge its due prominence.</p>
<p>In the resulting sketch – a full-size preparatory drawing for <a href="https://www.npg.org.uk/collections/search/portrait/mw03080/King-Henry-VIII-King-Henry-VII">a mural</a> that once covered an entire wall at Whitehall Palace in London – the king is, it's often said, majestic and <a href="https://link.springer.com/chapter/10.1057/978-1-137-51144-7_8">virile</a>. Henry VIII's feet are planted firmly apart, with both hands resting suggestively below his waist, clutching objects that seem to <a href="https://books.google.com/books/about/Thrust.html?id=Fd_kDwAAQBAJ&amp;printsec=frontcover&amp;source=kp_read_button&amp;hl=en&amp;newbks=1&amp;newbks_redir=1">direct the viewer</a> towards his ludicrously proportioned genitals. According to a contemporary account, the final painting left viewers feeling "<a href="https://brill.com/downloadpdf/book/edcoll/9781848881372/BP000006.pdf">abashed and annihilated</a>".</p>
<p>For a brief moment in the Renaissance, in between the invention of the microscope, printing press, and pencils – along with other technologies that uphold modern society – upper class men were rather preoccupied with erecting another innovation: the codpiece.</p>
<p>These "<a href="https://www.vice.com/en/article/wdzd49/metallic-package-379-v17n3">pretty personal palaces for penises</a>", as one writer has called them, consisted of pockets of fabric worn over the crotch and padded out to form a bizarre array of evocative shapes: spirals, orbs, and upwards-curling sausages. Some even had faces on them. This was a rare opportunity for men to give their nether regions a dash of flair, and many opted for confections of sumptuous fabrics, such as silk, velvet, and satin, embellished with jewels, gold, and – the ultimate show of macho fecundity – cute little bows.</p>
<p>But what were codpieces for? And why did they disappear?</p></div></div><div id="future/article/20240202-what-happened-to-the-codpiece-p0h8qt6v"><picture><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qt6v.webp" type="image/webp"><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qt6v.jpg" type="image/jpeg"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qt6v.webp" type="image/webp"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qt6v.jpg" type="image/jpeg"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qt6v.webp" type="image/webp"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qt6v.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="In Henry VIII's most famous portrait, he's dripping with furs, gold, and rubies – but it's his codpiece that really commands attention (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/720x900/p0h8qt6v.jpg" alt="In Henry VIII's most famous portrait, he's dripping with furs, gold, and rubies – but it's his codpiece that really commands attention (Credit: Alamy)" id=""></picture><div><p>In Henry VIII's most famous portrait, he's dripping with furs, gold, and rubies – but it's his codpiece that really commands attention (Credit: Alamy)</p></div></div><div><p>Initially, codpieces were made of steel and added to armour, to help protect knights' fertility on the battlefield. But soon they presented a neat solution for an awkward everyday problem.</p>
<p>Until the late 15thCentury, it was common for men to wear a long tunic or doublet – essentially, a dress – with <a href="https://www.cam.ac.uk/research/features/what-goes-up-must-come-down-a-brief-history-of-the-codpiece">hose (tights) on their legs</a>. Then the fashion changed. Doublets gradually inched their way upwards over the years, becoming so short that they no longer covered the crotch. This was particularly dangerous, because the hose men wore at the time came individually, like socks, leaving open spaces that were somewhat… revealing.&nbsp;</p>
<p>"So basically, you would put one leg in one hose, and then tie it to your doublet. And then you do the same with the other leg," says Victoria Bartels, a professor of early modern Italy at Syracuse University in New York. With the trendy new cropped versions, "it kind of left an area that needed to be covered," she says. &nbsp;</p>
<p>The problem was highlighted in a particularly graphic description in the medieval poem <a href="https://chaucer.fas.harvard.edu/pages/parsons-prologue-and-tale">The Canterbury Tales</a>. "Alas, some of them show the bulge of their shape, and the horrible swollen members, that it seems like the malady of hernia," complains a character known as "the person" in the story's prologue.</p>
<p>The gaps led to a moral panic, with priests worrying that the new style would prove irresistible to "sodomites", and lead to <a href="https://www.cam.ac.uk/research/features/what-goes-up-must-come-down-a-brief-history-of-the-codpiece">the corruption of young men</a>.</p></div><div id="future/article/20240202-what-happened-to-the-codpiece-p0h8qvmg"><picture><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qvmg.webp" type="image/webp"><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qvmg.jpg" type="image/jpeg"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qvmg.webp" type="image/webp"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qvmg.jpg" type="image/jpeg"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qvmg.webp" type="image/webp"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qvmg.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Choosing a codpiece in a contrasting colour helped men to make their genitals the centre of attention (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/720x900/p0h8qvmg.jpg" alt="Choosing a codpiece in a contrasting colour helped men to make their genitals the centre of attention (Credit: Alamy)" id=""></picture><div><p>Choosing a codpiece in a contrasting colour helped men to make their genitals the centre of attention (Credit: Alamy)</p></div></div><div><p>The first codpieces were limp triangles of fabric that were tucked in to cover the openings between each hose. But it didn't take long for men to take full advantage of these new garments, and start padding them.</p>
<p>Within a couple of decades, these loose flaps had morphed into phallic objects of monstrous proportions. Early modern wannabe lotharios would pack their codpieces with horsehair, fabric and straw, sometimes stashing useful items away inside such as <a href="https://books.google.com/books/about/Thrust.html?id=Fd_kDwAAQBAJ&amp;printsec=frontcover&amp;source=kp_read_button&amp;hl=en&amp;newbks=1&amp;newbks_redir=1">handkerchiefs and money</a> – Bartels has even has encountered accounts of their use for storing potpourri.</p>
<p>From the streets of Florence, where they were known as <a href="https://onlinelibrary.wiley.com/doi/10.1111/j.1445-5994.2004.00635.x">sacco</a>, to Paris, where they were called <a href="https://onlinelibrary.wiley.com/doi/10.1111/j.1445-5994.2004.00635.x">braguettes</a>, young men swaggered around with their prosthetic genitalia, drawing the eye downwards wherever they went. What had begun as a modesty device was now being used to the opposite effect.&nbsp;&nbsp;&nbsp;</p>
<p>The symbolism of these protruding packages wasn't lost on the Renaissance men who wore them. The very word "codpiece" comes from the Old English <a href="https://www.oed.com/dictionary/codpiece_n?tl=true&amp;tab=etymology">"cod", meaning "scrotum"</a>, while one satirical text Bartels found compared the protection they offered – especially on armour – to that provided by the shells of nuts and seeds. Like these <a href="https://www.cam.ac.uk/research/features/what-goes-up-must-come-down-a-brief-history-of-the-codpiece">"braguettes naturelles"</a> – nature's codpieces – they helped to ensure the propagation of the next generation.</p></div><div id="future/article/20240202-what-happened-to-the-codpiece-p0h8qtyc"><picture><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qtyc.webp" type="image/webp"><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qtyc.jpg" type="image/jpeg"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qtyc.webp" type="image/webp"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qtyc.jpg" type="image/jpeg"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qtyc.webp" type="image/webp"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qtyc.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Some Renaissance men liked to adorn their crotches with cute little bows (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/720x900/p0h8qtyc.jpg" alt="Some Renaissance men liked to adorn their crotches with cute little bows (Credit: Alamy)" id=""></picture><div><p>Some Renaissance men liked to adorn their crotches with cute little bows (Credit: Alamy)</p></div></div><div><p>Henry VIII, whose <a href="https://www.sciencedaily.com/releases/2011/03/110303153114.htm">reproductive woes</a> led him to invent a new branch of Christianity, notoriously used the figurative associations of the codpiece to <a href="https://muse.jhu.edu/pub/4/article/41213/summary">maximum effect</a>. The mural at Whitehall Palace was destroyed in a fire, but the original drawing survived and others were encouraged to copy it. Consequently, the king's ornamental crotch lives on in tens of paintings to this day, reassuring those who view it that he is more than <a href="https://lucyworsley.com/a-little-article-on-the-history-of-the-codpiece/">capable of producing an heir</a>. &nbsp;</p>
<p>Even 200 years after his death, admirers could visit Henry VIII's statue and marvel at his fecundity, at the Tower of London. A painted wooden effigy there came with flowing fabric robes and a secret mechanism that revealed a <a href="https://www.google.co.uk/books/edition/Thrust/Fd_kDwAAQBAJ?hl=en&amp;gbpv=1&amp;printsec=frontcover">swinging codpiece</a>. "If you press a spot on the floor with your feet, you will see something surprising with regard to this figure, but I will not say more," wrote one visitor, according to the book Thrust: A Spasmodic Pictorial History of the Codpiece in Art. Women would <a href="https://www.google.co.uk/books/edition/Thrust/Fd_kDwAAQBAJ?hl=en&amp;gbpv=1&amp;printsec=frontcover">stick pins</a> into it in the hope that it would help them to have children.</p>
<p>At the time, such brazen displays of virility weren't unusual. On the <a href="https://www.atlasobscura.com/places/cappella-colleoni">wrought iron front gate</a> to the Cappella Colleoni chapel in Bergamot, Italy, built in the late 15th Century, is the Colleoni coat of arms – and on it, are three comma-like shapes. These are testicles, thought to have been included as a display of macho strength, and because of the similarity of the family name to the <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8365.2008.00635.x">word for them, coglioni</a>.</p></div><div id="future/article/20240202-what-happened-to-the-codpiece-p0h8qszc"><picture><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qszc.webp" type="image/webp"><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qszc.jpg" type="image/jpeg"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qszc.webp" type="image/webp"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qszc.jpg" type="image/jpeg"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qszc.webp" type="image/webp"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qszc.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Even Henry VIII's suits of armour were given suggestive bulges (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/720x900/p0h8qszc.jpg" alt="Even Henry VIII's suits of armour were given suggestive bulges (Credit: Alamy)" id=""></picture><div><p>Even Henry VIII's suits of armour were given suggestive bulges (Credit: Alamy)</p></div></div><div><p>Men also used their new crotch adornments to project military prowess. Firstly, there was the fact that codpieces were sometimes added to armour. But Bartels explains that the rise of the codpiece also coincided with the Italian Wars, in which <a href="https://www.cambridge.org/core/books/abs/cambridge-history-of-war/warfare-and-italian-states-13001500/DBEBA97FAE6018EA0243F6AC7DCA60AE">mercenaries</a> from Northern Europe went to battle on behalf of Spain, France and Italy. One work-perk the soldiers received was an exemption from Sumptuary Laws – regulations that determined how lavish different social groups were allowed to be – and they seized this sartorial opportunity. "They are super flashy dressers… they don these huge codpieces," says Bartels. This forged yet another tie between the codpiece and military culture.</p>
<p>Even to their contemporaries, however, these overt and determined displays of masculinity were often the focus of great ridicule. As the historian <a href="https://books.google.com/books/about/Materializing_Gender_in_Early_Modern_Eng.html?id=ItOtNF1u24sC&amp;printsec=frontcover&amp;source=kp_read_button&amp;hl=en&amp;newbks=1&amp;newbks_redir=1">Will Fisher writes</a> in the book Materializing Gender in Early Modern English Literature and Culture, satirists would thrill their audiences with suggestive scenes in which it seems that a character is about to reveal their genitals… before pulling out something unexpected, like an orange. Other unusual items found in fictional codpieces included "ballads, bottles, napkins, pistols, hair, and even a looking glass".</p></div><div id="future/article/20240202-what-happened-to-the-codpiece-p0h8qv8c"><picture><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qv8c.webp" type="image/webp"><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qv8c.jpg" type="image/jpeg"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qv8c.webp" type="image/webp"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qv8c.jpg" type="image/jpeg"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qv8c.webp" type="image/webp"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qv8c.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Some codpieces were more evocative than others (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/720x900/p0h8qv8c.jpg" alt="Some codpieces were more evocative than others (Credit: Alamy)" id=""></picture><div><p>Some codpieces were more evocative than others (Credit: Alamy)</p></div></div><div><p>By as early as the late 16th Century, the codpiece was already in decline – and sources start to refer to them as out of fashion, says Bartels. Oddly, just before these engorgements vanished they began to shrink down to <a href="https://www.theguardian.com/books/2015/apr/30/wolf-hall-codpieces-too-small-says-literature-researcher">minute proportions</a>. "You start to see this other fashion trend called the peascod," says Bartels. These puffed-out, distended doublets were worn over a shirt, protruding out like prosthetic potbellies. "It looks ridiculous to modern eyes," she says. These were often accompanied by pillowy or even skirt-like breeches, and the combination was in competition for the same bodily real estate as the codpiece, since they both incorporated the genitals, she notes.&nbsp;&nbsp;</p>
<p>Today, very few codpieces survive. Those that remain include the metallic bulges in armouries, a set of <a href="https://www.cam.ac.uk/research/features/what-goes-up-must-come-down-a-brief-history-of-the-codpiece">wool and velvet ones</a> that belonged to a Swedish count and his sons, and a drawerful at the Museum of London – initially <a href="https://lucyworsley.com/i-went-to-the-museum-of-london/">classified as shoulder pads</a> by a starchy Victorian curator, according to the historian Lucy Worsley. Other than that, we can only glimpse the priapic grandeur of this lost garment though paintings and sculptures from the era.</p>
<p>However, though authentic renaissance codpieces are now rare, public enthusiasm for them hasn't disappeared completely. In the 1970s and 80s, rock bands such as Jethro Tull and Kiss began aweing their audiences with leopard-print, leather, metallic studded, and demon-faced versions – the latter even had their own <a href="https://www.cbc.ca/radio/day6/kiss-band-costume-designer-rebecca-sevrin-1.7039209">codpiece-seamstress</a> until they disbanded last year. &nbsp;&nbsp;</p>
<p>Codpieces have also been making a comeback in <a href="https://www.theguardian.com/fashion/2019/dec/27/codpiece-envy-fashion-reinvents-16th-century-accessory">high fashion</a> – part of a trend for so-called "<a href="https://www.theguardian.com/fashion/2019/dec/27/codpiece-envy-fashion-reinvents-16th-century-accessory">Tudor power dressing</a>" – and on historical television series including Wolf Hall. There's just one problem: modern producers can't bring themselves to <a href="https://www.theguardian.com/books/2015/apr/30/wolf-hall-codpieces-too-small-says-literature-researcher">make them big enough</a>.</p>
<p>--</p>
<p><em>If you liked this story,&nbsp;</em><a href="https://cloud.email.bbc.com/SignUp10_08?&amp;at_bbc_team=studios&amp;at_medium=Onsite&amp;at_objective=acquisition&amp;at_ptr_name=bbc.com&amp;at_link_origin=featuresarticle&amp;at_campaign=essentiallist&amp;at_campaign_type=owned"><em><strong>sign up for The Essential List newsletter</strong></em></a><em>&nbsp;– a handpicked selection of features, videos and can't-miss news delivered to your inbox every Friday.</em></p>
<p><em>Join one million Future fans by liking us on&nbsp;</em><a href="https://www.facebook.com/BBCFuture/"><em><strong>Facebook</strong></em></a><em>, or follow us on&nbsp;</em><a href="https://twitter.com/BBC_Future"><em><strong>Twitter</strong></em></a><em>&nbsp;or&nbsp;</em><a href="https://www.instagram.com/bbcfuture_official/"><em><strong>Instagram</strong></em></a><em>.</em></p></div></div></article></div>;</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Gödel, Escher, Bach is the most influential book in my life (351 pts)]]></title>
            <link>https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428</link>
            <guid>39253099</guid>
            <pubDate>Sun, 04 Feb 2024 18:48:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428">https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428</a>, See on <a href="https://news.ycombinator.com/item?id=39253099">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://philosophygeek.medium.com/?source=post_page-----49d785a4e428--------------------------------"><div aria-hidden="false"><p><img alt="Mark Johnson" src="https://miro.medium.com/v2/resize:fill:88:88/1*ivAx6b9Z9gnoFsJceGMkFQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="2061"><a href="https://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567" rel="noopener ugc nofollow" target="_blank"><em>Gödel, Escher, Bach: An Eternal Golden Braid</em></a><em> </em>(henceforth: GEB), the Pulitzer Prize winning book written in 1978 by Douglas Hofstadter, is described in its cryptic tagline as “a metaphorical fugue on minds and machines in the spirit of Lewis Carroll.”</p><p id="9ab3">I recently reread GEB and got fired up by how brilliantly Hofstadter fuses computation, epistemology, and consciousness. After failed attempts at explaining the book to three of my smartest friends, I decided to write something up.</p><p id="7f72">The problem is that a simple reduction like “GEB is about how complex systems arise from simpler systems” is akin to describing <em>Ulysses</em> as “a day in the life of Leopold Bloom.” More detailed descriptions run the risk of diving into the depth that’s only understandable after having read the book.</p><p id="ca55">This post is a more modest attempt to explain to myself why GEB is important, and focuses on three mental models that have profoundly affected my life: <strong>epistemic limits</strong>,<strong> self-reference</strong>, and <strong>isomorphism</strong>.</p><p id="b371">If it causes you to read or reread it, then all the better.</p><p id="00df">Here we go!</p><figure><figcaption>Kurt and Albert, hanging out at Princeton.</figcaption></figure><p id="d440">The main character of the book is <a href="https://en.wikipedia.org/wiki/Kurt_G%C3%B6del" rel="noopener ugc nofollow" target="_blank">Kurt Gödel</a>, the most important person in the 20th century you’ve never heard of. Gödel is the kind of guy that shows up to <a href="https://en.wikipedia.org/wiki/Albert_Einstein" rel="noopener ugc nofollow" target="_blank">his buddy’s</a> 70th birthday with <a href="https://en.wikipedia.org/wiki/G%C3%B6del_metric" rel="noopener ugc nofollow" target="_blank">an exact solution to the Einstein field equations</a> as a present. Despite being the greatest mathematician of his generation, he wasn’t stuffy in the least: his favorite movie was Snow White and the Seven Dwarves.</p><p id="b348">Gödel is most famous for his Incompleteness Theorems, which established limits on mathematics. For the first chunk of the 20th century, mathematicians were obsessed with formalizing mathematics and then proving meta-theorems <em>about</em> those formal systems. In particular, there was a strongly-held belief that for any well-formed formula (a “grammatically correct” statement in math, e.g., A=B is well-formed whereas AA==+B is not), you could use mathematics to <em>decide</em> whether it was true or false.</p><p id="909f">If you think about it for a second, this makes perfect sense: it <em>seems</em> like you should be able to determine whether any statement is true or false.</p><p id="3010">Nope! Gödel proved in 1931 that mathematics is not decidable, an earth-shattering result. He proved that there are statements in mathematics, which are <em>true but not provable</em> within the system. Worse yet, it turns out that you can’t build a more powerful mathematical system. Once a system becomes sufficiently complex, there will always be statements which are undecidable. You’re left with a choice: either have weak system of mathematics or accept that there will always be theorems out of reach. A rough analogy to incompleteness Heisenberg’s Uncertainty Principle, which shows that physics makes it impossible to determine <em>both</em> the position and velocity of a particle with exact precision.</p><p id="fa72">Wouldn’t it be nice if every question had an answer? That’s a lovely fantasy, but Gödel shows that there are <strong>fundamental epistemic limits to the universe</strong>, things that no genius will help us to know, no alien race could teach us, no machine could be built to solve, and no new kinds of mathematics will uncover. How frustrating.</p><p id="167d">A key feature of powerful mathematical systems (or perhaps, any system that generates complexity…) is that they involve <strong>self-reference</strong>, that is, they contain ways of talking about themselves. “This sentence is true” is an example. Because self-referential systems can manipulate and talk about themselves, they systems are very powerful and immediately run into fun paradoxes. Is the statement “This sentence is false.” true or false? Either way, it doesn’t end well.</p><p id="c684">A third major theme of the book is <strong>isomorphism</strong>, which is unique to Hofstadter’s vernacular. In formal mathematics, “isomorphism” takes on a version of “equivalence.” For example, it turns out that many different formalizations of mathematics are provably isomorphic, like Turing Machines, arithmetic, set theory, and formal logic. Hofstadter deliberately uses the term more loosely to describe two systems that are structurally similar. I find this quite useful because it forces one to define the structures of the system, why they are similar, and why other parts of the system are less important. We might describe the way that planets fly around stars as <em>isomorphic</em> to the way that electrons fly around nuclei.</p><figure><figcaption>Escher’s famous Drawing Hands.</figcaption></figure><p id="8a61">The two minor characters, <a href="https://mcescher.com/" rel="noopener ugc nofollow" target="_blank">M.C.Escher</a> and <a href="https://en.wikipedia.org/wiki/Johann_Sebastian_Bach" rel="noopener ugc nofollow" target="_blank">Johann Sebastian Bach</a>, are reflections of Gödel in art and both liberally use self reference. Escher draws pictures of hands drawing hands (!) and water “falling” in an infinite loop. His images don’t just play tricks on the eye, they force paradoxical conclusions, regardless of your angle of interpretation. On the musical side, Papa Bach was most famous for his complex fugues, which are basically the same melody played on top of each other. Common versions of this you might have sung as a child are “Row, row, row your boat” and “Frère Jacques.” Both Escher and Bach are woven into the story (like a fugue?), providing tangible examples to the more abstruse mathematical concepts.</p><figure><figcaption>A playlist full of Bach, fugues, and other tracks referenced in GEB.</figcaption></figure><p id="b898">Perhaps the most astonishing part of the book is the quality of the writing itself. Each chapter begins with a clever dialog between Achilles and the Tortoise (inspired by Lewis Carroll), and a few of their anthropomorphic friends. They deal with a whole range of bizarre situations, like record player so powerful that it can play any record (including a record that can destroy the record player) and asking a Djinn for a meta-wish (“I wish for 5 more wishes”). Hofstadter’s greatest achievement is his palindromic Crab Canon in Chapter VII, which is a dialog that can be read backwards and forwards. Of course, these aren’t just cute dialogs: each is isomorphic to the themes in the following chapter. Oftentimes a dialog is a more understandable exposition of the chapter’s theme than the chapter itself.</p><p id="440f">And, naturally in a book about self-reference, GEB itself is highly self-referential. Themes are often resolved hundreds of pages later and require going back to appreciate fully the depth of Hofstadter’s argument. Mercifully, he’s a gifted and lucid writer so, even though there are chapters that are dense, it’s always tractable to read.</p><p id="5afb">After 742 pages and even after having written the paragraphs above, I still struggle with a simple answer to the question: “What is this book about?” The best I can come up with is that GEB equips you with mental models to contemplate philosophy.</p><p id="04c7">So to end, a few personal examples about how GEB has influenced my own thinking.</p><p id="1696"><a rel="noopener" href="https://philosophygeek.medium.com/moving-from-com-to-org-34150bea9ade">I recently joined Stand Together</a>, who shares my strong belief in <strong>bottom up solutions</strong>. Perhaps the idea that bottom up solutions are better isn’t just an empirical statement of sociology, but fundamental to the nature of complex systems. Indeed, Hofstadter goes through many examples of how complexity emerges from simpler systems, often which look nothing like the higher-level systems. Consciousness itself doesn’t exist in neurons, and yet neurons as a system create consciousness in humans (this is critical to Hoftstadter’s argument that machines can think). There’s also a fantastical example in a dialog with the Anteater, who has conversations with Aunt Hilary, an ant colony. She is perfectly capable of having a robust conversation with the Anteater, powered by the ants in the colony. Of course, the ants themselves are individuals with their own cares and concerns and has no knowledge of the emergent intelligence, much like Aunt Hilary has no knowledge of her inner workings.</p><p id="0ddf">How DNA expresses as proteins, how the brain functions at multiple levels, how we understand and use words, how programs don’t have access to the underlying transistors, how Aunt Hilary doesn’t know what the ants are doing… all of these are a set of isomorphisms that suggest that bottom up is better than top down. An additional tenet of Stand Together is “believe in people,” which means that the smallest units act intelligently. Like ants or neurons, we make local decisions every day that bubble up into the structure of society, without anyone telling us what to do.</p><p id="fbbe">The idea that epistemic limits exist in something as universal as mathematics has humbled me about the limits of knowledge for complex human systems. Utopian thought experiments often generate useful frames of exploration, but ought not be confused with reality. Utopians often try to pull out the “bugs” from human systems, often which are endemic to the system, or “features,” as we might say in the trade. Bugs might not be desirable, but sometimes, <em>the bugs can’t just be ripped out of the system without destroying the system itself</em>. Our time would be better spent figuring out—within the system—optimize for minimizing the downsides of the “bugs” while maximizing the value of the features. Think about this with respect to capitalism, socialism, and communism…</p><p id="c7f8">A final area where GEB has influenced me is in designing software products. Hugh Dubberly has been my collaborator for years, starting off with our deep dive into cybernetics, the study of feedback loops. We believe that iteration is key to quality; perfection is impossible out of the gate. Further, the system used to generate quality software is a series of feedback loops between customers and the company, product and engineering, and so on. Though specific product frameworks have changed over the years, that obsession with iteration and feedback permeates everything I’ve implemented.</p><p id="ea19">My modest goal in writing this post was to have something I could send to a friend, rather than to spend an hour fumbling a feeble explanation of <em>Gödel, Escher, Bach</em>. I had a secondary goal in the back of my head… if you have a copy of GEB on your shelf collecting dust and you’ve never read more than a chapter or two, dust it off and see how it goes this time.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ProofWiki: Online compendium of mathematical proofs (137 pts)]]></title>
            <link>https://proofwiki.org/wiki/Main_Page</link>
            <guid>39252531</guid>
            <pubDate>Sun, 04 Feb 2024 17:51:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://proofwiki.org/wiki/Main_Page">https://proofwiki.org/wiki/Main_Page</a>, See on <a href="https://news.ycombinator.com/item?id=39252531">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mw-content-text" lang="en" dir="ltr"><h2><span id="Welcome_to_.7F.27.22.60UNIQ-MathJax-1-QINU.60.22.27.7F"></span><span id="Welcome_to_'&quot;`UNIQ-MathJax-1-QINU`&quot;'">
Welcome to $\mathsf{Pr} \infty \mathsf{fWiki}$
</span></h2>
<div><p><a href="https://proofwiki.org/wiki/File:Logo.png"><img alt="Logo.png" src="https://proofwiki.org/w/images/c/c9/Logo.png" decoding="async" width="135" height="135"></a></p>
<p><b>$\mathsf{Pr} \infty \mathsf{fWiki}$</b> is an online compendium of mathematical proofs! Our goal is the collection, collaboration and classification of mathematical proofs. If you are interested in helping create an online resource for math proofs feel free to <b><a href="https://proofwiki.org/wiki/Special:RequestAccount" title="Special:RequestAccount">register for an account</a></b>. Thanks and enjoy!
</p><p>If you have any questions, comments, or suggestions please post on the <b><a href="https://proofwiki.org/wiki/Talk:Main_Page" title="Talk:Main Page">discussion</a></b> page, or contact one of the <span><a rel="nofollow" href="https://www.proofwiki.org/w/index.php?title=Special%3AListUsers&amp;username=&amp;group=sysop&amp;limit=50">administrators</a></span>. Also, feel free to take a look at the <a href="https://proofwiki.org/wiki/Help:FAQ" title="Help:FAQ">frequently asked questions</a> because you may not be the first with your idea.
</p><p>To see what's currently happening in the community, visit the <b><a href="https://proofwiki.org/wiki/ProofWiki:Community_Portal" title="ProofWiki:Community Portal"> community portal</a></b>.
</p>
</div>
<center><big><a href="https://proofwiki.org/wiki/Category:Proofs" title="Category:Proofs">26,727 Proofs</a> <b>—</b> <a href="https://proofwiki.org/wiki/Category:Definitions" title="Category:Definitions">25,226 Definitions</a> <b>—</b> <a href="https://proofwiki.org/wiki/Help:Contents" title="Help:Contents">Help</a></big></center>
<p><a href="https://twitter.com/ProofWiki" data-show-count="false">Follow @ProofWiki</a>
</p>
<h2><span id="Featured_Proof">
Featured Proof
</span></h2>


<h2><span id="Theorem">Theorem</span></h2>
<dl><dd>$\ds \sum_{n \mathop = 0}^\infty \frac {F_n} {2^n} = 2$</dd></dl>
<p>where $F_n$ is the $n$th <a href="https://proofwiki.org/wiki/Definition:Fibonacci_Number" title="Definition:Fibonacci Number">Fibonacci number</a>.
</p>
<h2><span id="Proof">Proof</span></h2>
<p>Let us define a <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a> which satisfies the <a href="https://proofwiki.org/wiki/Definition:Probability_Measure/Definition_3" title="Definition:Probability Measure/Definition 3">Kolmogorov Axioms</a> such that it is the set of all combinations of <a href="https://proofwiki.org/wiki/Definition:Coin/Coin-Tossing" title="Definition:Coin/Coin-Tossing">flipping</a> a fair <a href="https://proofwiki.org/wiki/Definition:Coin" title="Definition:Coin">coin</a> until you receive two <a href="https://proofwiki.org/wiki/Definition:Coin/Head" title="Definition:Coin/Head">heads</a> in a row.
</p><p>Let $X_n$ be the event of some outcome from <a href="https://proofwiki.org/wiki/Definition:Coin/Coin-Tossing" title="Definition:Coin/Coin-Tossing">flipping</a> $n$ fair <a href="https://proofwiki.org/wiki/Definition:Coin" title="Definition:Coin">coins</a> in a row, then $\Pr(X_n) = \dfrac 1 {2^n}$.
</p><p>In the <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a> defined above, we now demonstrate that for a given number of flips $n$, there are exactly $F_{n - 1}$ outcomes contained in the <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a>.
</p>
<h3><span id="Illustration">Illustration</span></h3>
<dl><dd>$\begin{array}{c|c|cc}
n &amp; \map f n &amp; \text {Sample Space}: \Omega \\
\hline
1 &amp; 0 &amp; \text {impossible} \\
2 &amp; 1 &amp; HH \\
3 &amp; 1 &amp; THH \\
4 &amp; 2 &amp; (HTHH), (TTHH) \\
5 &amp; 3 &amp; (THTHH), (HTTHH), (TTTHH) \\
6 &amp; 5 &amp; (HTHTHH), (TTHTHH), (THTTHH), (HTTTHH), (TTTTHH) \\
\hline
\cdots &amp; \cdots &amp; \cdots \\
\hline
n &amp; F_{n - 1} &amp; \cdots \\
\hline
\end{array}$</dd></dl>
<p><br>
Reviewing the illustration above, for any given value of $n$:
</p><p>For <b>ALL</b> combinations displayed in <a href="https://proofwiki.org/wiki/Definition:Matrix/Row" title="Definition:Matrix/Row">row</a> $n$ (that is $\map f n$) , we can place a $T$ in front and that new combination would exist in the <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a> for $\paren {n + 1}$.
</p><p>For example:
</p>
<dl><dd>$\paren {HTHH}, \paren {TTHH} \to \paren {THTHH}, \paren {TTTHH}$</dd></dl>
<p><br>
However, we also see that for only those combinations starting with a $T$ (that is $\map f {n - 1}$), can we place an $H$ in front and that new combination will also exist in the <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a> for $\paren {n + 1}$.
</p><p>For example:
</p>
<dl><dd>$\paren {TTHH} \to \paren {HTTHH}$</dd></dl>
<p><br>
Therefore, we have:
</p>
<table>
<tbody><tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \map f n\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds F_{n - 1}\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \map f {n + 1}\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \map f n + \map f {n - 1}\)
</td>
<td>
</td>
<td>
</td>
<td>$\map f n$ is adding a $T$ in front and $\map f {n - 1}$ is adding an $H$ in front
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds F_{n - 1} + F_{n - 2}\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds F_n\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr></tbody></table>
<p>The sum of the probabilities of outcomes in a sample space is one by the second <a href="https://proofwiki.org/wiki/Definition:Probability_Measure/Definition_3" title="Definition:Probability Measure/Definition 3">Kolmogorov Axiom</a>.
</p>
<table>
<tbody><tr>
<td>\((\text {II})\) &nbsp;
</td>
<td>$:$ &nbsp;
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>&nbsp;&nbsp; \(\ds \map \Pr \Omega \)
</td>
<td>&nbsp; \(\ds = \) &nbsp;
</td>
<td>\(\ds 1 \) &nbsp;&nbsp;
</td>
<td>&nbsp;&nbsp;
</td></tr></tbody></table>
<p>Hence:
</p>
<table>
<tbody><tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \sum_{n \mathop = 1}^\infty \frac {F_{n - 1} } {2^n}\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds 1\)
</td>
<td>
</td>
<td>
</td>
<td><a href="https://proofwiki.org/wiki/Definition:Probability_Measure/Definition_3" title="Definition:Probability Measure/Definition 3">$2$nd Kolmogorov Axiom</a>
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>\(\ds \leadsto \ \ \)
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \sum_{n \mathop = 0}^\infty \frac {F_n} {2^{n + 1} }\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds 1\)
</td>
<td>
</td>
<td>
</td>
<td>reindexing the sum
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>\(\ds \leadsto \ \ \)
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \sum_{n \mathop = 0}^\infty \frac {F_n} {2^n}\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds 2\)
</td>
<td>
</td>
<td>
</td>
<td>multiplying both sides by $2$
</td>
<td>
</td></tr></tbody></table>
<p>$\blacksquare$
</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple fixes zero-day bug in Apple Vision Pro that 'may have been exploited' (109 pts)]]></title>
            <link>https://techcrunch.com/2024/01/31/apple-vision-pro-zero-day-security-bug-exploited/</link>
            <guid>39252321</guid>
            <pubDate>Sun, 04 Feb 2024 17:32:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/01/31/apple-vision-pro-zero-day-security-bug-exploited/">https://techcrunch.com/2024/01/31/apple-vision-pro-zero-day-security-bug-exploited/</a>, See on <a href="https://news.ycombinator.com/item?id=39252321">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">A day after reporters published their first hands-on review of Apple’s Vision Pro, the technology giant released its first security patch for the mixed reality headset to fix a vulnerability that “may have been exploited” by hackers in the wild.</p>
<p>On Wednesday, Apple released visionOS 1.0.2, the software that runs on the Vision Pro, with a fix for a vulnerability in WebKit, the browser engine that runs Safari and other web apps. Apple said the bug, if exploited, allowed malicious code to run on an affected device.</p>
<p>It’s the same vulnerability that Apple patched last week when <a href="https://techcrunch.com/2024/01/23/iphone-users-should-turn-on-apples-stolen-device-protection-feature/" target="_blank" rel="noopener">it rolled out iOS 17.3</a>, which included fixes for iPhones, iPads, Macs and Apple TV — all of which rely on WebKit. No patches for this bug, <a href="https://support.apple.com/en-us/HT214070" target="_blank" rel="noopener">officially tracked as CVE-2024-23222</a>, were released for Apple Watch.</p>
<p>It’s not immediately clear if malicious hackers used the vulnerability to specifically exploit Apple’s Vision Pro, and Apple spokesperson Scott Radcliffe would not say when asked by TechCrunch.</p>
<p>It also isn’t yet known who was exploiting the vulnerability, or for what reason.</p>
<p>It is not uncommon for malicious actors, such as spyware makers, to target weaknesses in WebKit as a way to break into the device’s underlying operating system and the user’s personal data. WebKit bugs can sometimes be exploited when a victim visits a malicious domain in their browser, or the in-app browser.</p>
<p>Apple rolled out several patches for WebKit bugs last year.</p>
<p>Vision Pro is expected to be available starting Friday.</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How do transformers work? (122 pts)]]></title>
            <link>https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi</link>
            <guid>39252235</guid>
            <pubDate>Sun, 04 Feb 2024 17:24:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi">https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi</a>, See on <a href="https://news.ycombinator.com/item?id=39252235">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><em><span>👋 Hi, this is</span><a href="https://twitter.com/gergelyorosz" rel="nofollow ugc noopener"> </a><span>Venkat and here with a free, full issue of the The ZenMode Engineer Newsletter. In every issue, I cover one topic explained in a simpler terms in areas related to computer technologies and beyond.</span></em></p><p>Transformers have become synonymous with cutting-edge AI, particularly in the realm of natural language processing (NLP).</p><p>But what exactly makes them tick? How do these models navigate the intricacies of language with such remarkable efficiency and accuracy? </p><p>Buckle up, because we're about to  learn the heart of the transformer architecture.</p><p>But.. Before we deep dive into it lets understand where its been used.. if you have used google translate/ ChatGPT both rely on these.</p><blockquote><p><em><strong>Google Translate:</strong><span> This widely used platform relies heavily on transformers to achieve fast and accurate translations across over 100 languages. It considers the entire sentence context, not just individual words, leading to more natural-sounding translations.</span></em></p><p><em><strong>Netflix Recommendation System:</strong><span> Ever wondered how Netflix suggests shows and movies you might enjoy? Transformers analyze your viewing history and other users' data to identify patterns and connections, ultimately recommending content tailored to your preferences.</span></em></p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png" width="727" height="1024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:727,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><strong>The Big Picture: Encoder and Decoder Dance</strong></p><p>Imagine a factory, but instead of assembling physical objects, it processes language. This factory has two main departments:</p><ol><li><p><strong>The Encoder:</strong><span> This is the information extractor, meticulously dissecting the input text, understanding its individual elements, and uncovering the hidden connections between them.</span></p></li><li><p><strong>The Decoder:</strong><span> Armed with the encoder's insights, the decoder crafts the desired output, be it a translated sentence, a concise summary, or even a brand new poem.</span></p></li></ol><p><strong>Encoder: Decoding the Input Labyrinth</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png" width="255" height="338.1111111111111" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:537,&quot;width&quot;:405,&quot;resizeWidth&quot;:255,&quot;bytes&quot;:167254,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The encoder's journey begins with </span><strong>Input Embedding</strong><span>, where each word is transformed from its textual form into a numerical representation (vector). Think of it as assigning each word a unique identifier.  </span></p><p>Consider this example.</p><ol><li><p><strong>Input Text:</strong><span> The process begins with the raw text sentence, such as "The cat sat on the mat."</span></p></li><li><p><strong>Input Embedding Layer:</strong></p><ul><li><p>This layer acts as a translator, converting each word into a numerical vector.</p></li><li><p>Imagine a large dictionary where each word has a corresponding vector address.</p></li><li><p>These vectors capture various aspects of word meaning:</p><ul><li><p>Semantic relationships (e.g., "cat" is closer to "pet" than "chair").</p></li><li><p>Syntactic roles (e.g., "cat" is often a noun, while "sat" is a verb).</p></li><li><p>Context within the sentence (e.g., "mat" here likely refers to a floor mat).</p></li></ul></li></ul></li><li><p><strong>Vector Representation:</strong></p><ul><li><p>The output of this layer is a sequence of numerical vectors, each representing a word:</p><ul><li><p>"The" -&gt; [0.2, 0.5, -0.1, ...]</p></li><li><p>"cat" -&gt; [0.8, -0.3, 0.4, ...]</p></li><li><p>"sat" -&gt; [-0.1, 0.7, 0.2, ...]</p></li><li><p>...</p></li></ul></li></ul></li></ol><p>But the encoder doesn't stop there. It employs the following key mechanisms to delve deeper:</p><ul><li><p><strong>Self-Attention Layer:</strong><span> This is the game-changer. Imagine shining a spotlight on each word, but instead of illuminating it in isolation, you also highlight how it connects to all other words in the sentence. This allows the encoder to grasp the context, nuances, and relationships within the text, not just the individual words.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif" width="1456" height="876" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:876,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>ref from Raimi Karim blog (used only to refernce)</figcaption></figure></div><p><span>Consider  this example sentence again "</span><em><strong>The quick brown fox jumps over the lazy dog.</strong></em><span>" </span></p><ol><li><p><strong>Word Embeddings:</strong><span> First, each word is transformed into a numerical representation called a "word embedding." Think of it as assigning each word a unique identifier in a giant vocabulary map.</span></p></li><li><p><strong>Query, Key, Value:</strong><span> Next, the Self-Attention mechanism creates three special vectors for each word:</span></p><ul><li><p><strong>Query (Q):</strong><span> This vector asks "What information do I need from other words?"</span></p></li><li><p><strong>Key (K):</strong><span> This vector acts like a label, saying "This is the information I have to offer."</span></p></li><li><p><strong>Value (V):</strong><span> This vector holds the actual information, like the word's meaning and context.</span></p></li></ul></li><li><p><strong>Attention Scores:</strong><span> Now comes the interesting part. The Self-Attention layer compares the Query vector of each word with the Key vectors of all other words in the sentence. </span></p><p><span>This helps it understand how relevant each word is to the current word. Based on this comparison, it calculates an </span><strong>attention score</strong><span> for each pair of words.</span></p><p>Imagine shining a spotlight on each word. The brighter the spotlight on another word, the higher the attention score, meaning the more relevant that word is to the current word.</p></li><li><p><strong>Weighted Values:</strong><span> Finally, the Self-Attention layer uses the attention scores to weigh the Value vectors of all other words. Words with higher attention scores get more weight, contributing more to the final representation of the current word.</span></p><p>Think of it like taking a weighted average of the information from other words, where the weights are determined by how relevant they are.</p></li><li><p><strong>New Word Representation:</strong><span> By considering the context provided by other words, the Self-Attention layer creates a new, enriched representation of each word. This new representation captures not just the word's own meaning, but also how it relates to and is influenced by other words in the sentence.</span></p></li></ol></li><li><p><strong>Multi-Head Attention:</strong><span> This is like having multiple teams of analysts, each focusing on different aspects of the connections between words. It allows the encoder to capture various facets of the relationships, enriching its understanding.</span></p><p><strong>Sentence:</strong><span> "The quick brown fox jumps over the lazy dog."</span></p><ol><li><p><strong>Individual Heads:</strong><span> Instead of one Self-Attention mechanism, Multi-Head Attention uses several independent "heads" (often 4-8). Each head has its own set of Query, Key, and Value vectors for each word.</span></p></li><li><p><strong>Diverse Attention:</strong><span> Each head computes attention scores differently, focusing on various aspects of word relationships:</span></p><ul><li><p>One head might attend to grammatical roles (e.g., "fox" and "jumps").</p></li><li><p>Another might focus on word order (e.g., "the" and "quick").</p></li><li><p>Another might capture synonyms or related concepts (e.g., "quick" and "fast").</p></li></ul></li><li><p><strong>Combining Perspectives:</strong><span> After each head generates its own weighted values, their outputs are concatenated. This combines the diverse insights from different attention mechanisms.</span></p></li><li><p><strong>Final Representation:</strong><span> This combined representation holds a richer understanding of the sentence, incorporating various relationships between words, not just a single focus.</span></p></li></ol></li><li><p><strong>Positional Encoding:</strong><span> Since transformers don't process word order directly, this layer injects information about each word's position in the sentence. It's like giving the analysts a map so they know the order in which to consider the words.</span></p><p>Sure, let's delve into positional encoding using an example sentence:</p><p><strong>Sentence:</strong><span> "The quick brown fox jumps over the lazy dog."</span></p><p><strong>Here's how positional encoding works step-by-step:</strong></p><ol><li><p><strong>Word Embeddings:</strong></p><ul><li><p>Each word ("The", "quick", etc.) is converted into a numerical representation called a word embedding, like a unique identifier in a vast vocabulary map.</p></li><li><p>Imagine these embeddings as vectors:</p><ul><li><p>"The": [0.2, 0.5, -0.1, ...]</p></li><li><p>"quick": [0.8, -0.3, 0.4, ...]</p></li><li><p>"brown": [..., ...]</p></li><li><p>...</p></li></ul></li></ul></li><li><p><strong>Positional Information:</strong></p><ul><li><p>Each word's embedding is combined with additional values based on its position in the sentence.</p></li><li><p>These values are calculated using sine and cosine functions at different frequencies:</p><ul><li><p>Lower frequencies capture long-range dependencies (e.g., "quick" and "fox" are related).</p></li><li><p>Higher frequencies encode short-range relationships (e.g., "jumps" and "over" are close).</p></li></ul></li><li><p>Think of these additional values as "position vectors":</p><ul><li><p>"The": [position 1 vector]</p></li><li><p>"quick": [position 2 vector]</p></li><li><p>"brown": [position 3 vector]</p></li><li><p>...</p></li></ul></li></ul></li><li><p><strong>Combining Embeddings and Positions:</strong></p><ul><li><p>The original word embedding and the position vector are added together, creating a new, enriched representation for each word:</p><ul><li><p>"The": [0.2, 0.5, -0.1, ...] + [position 1 vector] = new enriched embedding</p></li><li><p>"quick": [0.8, -0.3, 0.4, ...] + [position 2 vector] = new enriched embedding</p></li><li><p>"brown": [..., ...] + [position 3 vector] = new enriched embedding</p></li><li><p>...</p></li></ul></li></ul></li><li><p><strong>Understanding Order:</strong></p><ul><li><p>Even if the sentence order changes (e.g., "Dog lazy jumps..."), the position vectors ensure relative positions are maintained.</p></li><li><p>The model can still learn that "jumps" is more related to "over" than, say, "The".</p></li></ul></li></ol></li><li><p><strong>Feed Forward Network(FFN):</strong><span> This adds a layer of non-linearity, enabling the model to learn more complex relationships that might not be easily captured by attention mechanisms alone.</span></p><p>You've already delved into the sentence through previous layers. You understand individual words, their relationships, and their positions. Now, the FFN arrives like a detective magnifying glass, ready to uncover intricate details not immediately visible.</p><p><strong>The FFN does this through three key steps:</strong></p><ol><li><p><strong>Non-linear Transformation:</strong><span> Instead of straightforward calculations, the FFN uses non-linear functions like ReLU to add complexity. Think of it as applying a special filter to the existing information, revealing hidden patterns and connections that simple arithmetic might miss. This allows the FFN to capture more nuanced relationships between words.</span></p></li><li><p><strong>Multi-layered Analysis:</strong><span> The FFN isn't just one step; it's typically a chain of two or more fully connected layers. Each layer builds upon the previous one, transforming the information step-by-step. Imagine you're examining the sentence under increasing magnification, uncovering finer details with each layer.</span></p></li><li><p><strong>Dimensionality Shift:</strong><span> The FFN expands the information's size (e.g., from 512 dimensions to 2048) in the first layer. This allows it to analyze a wider range of features and capture more complex patterns. Think of it as spreading out the information on a larger canvas for deeper examination. Then, it contracts it back to the original size (e.g., 512 again) in the final layer to ensure compatibility with subsequent layers.</span></p></li></ol><p><strong>Applying this to our sentence:</strong></p><ul><li><p>Imagine the FFN helps identify that "quick" and "brown" not only describe the "fox" but also subtly connect to its perceived speed through their combined meaning.</p></li><li><p>Or, it might delve deeper into the relationship between "jumps" and "over," understanding the action and spatial context beyond just their individual definitions.</p></li></ul></li><li><p><strong>Repeat, Refine, Repeat:</strong><span> These layers (self-attention, multi-head attention, etc.) are stacked and repeated multiple times. With each iteration, the encoder refines its understanding, building a comprehensive representation of the input text.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif" width="480" height="322" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:322,&quot;width&quot;:480,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>image source: pillow lab blog</figcaption></figure></div><p><strong>Decoder: Weaving the Output Tapestry</strong></p><p>Now, the decoder takes the baton. But unlike the encoder, it has an additional challenge: generating the output word by word without peeking at the future. To achieve this, it utilizes:</p><ul><li><p><strong>Masked Self-Attention:</strong><span> Similar to the encoder's self-attention, but with a twist. The decoder only attends to previously generated words, ensuring it doesn't cheat and use future information. It's like writing a story one sentence at a time, without knowing how it ends.</span></p></li><li><p><strong>Encoder-Decoder Attention:</strong><span> This mechanism allows the decoder to consult the encoded input, like referring back to a reference document while writing. It ensures the generated output stays coherent and aligned with the original text.</span></p></li><li><p><strong>Multi-Head Attention and Feed Forward Network:</strong><span> Just like the encoder, these layers help the decoder refine its understanding of the context and relationships within the text.</span></p></li><li><p><strong>Output Layer:</strong><span> Finally, the decoder translates its internal representation into the actual output word, one by one. It's like the final assembly line, putting the pieces together to form the desired outcome.</span></p></li></ul><p><strong>Beyond the Basics:</strong></p><p>Remember, this is just a glimpse into the fascinating world of transformers. The specific architecture can vary depending on the task and dataset, with different numbers of layers and configurations. </p><p>Additionally, each layer involves complex mathematical operations that go beyond the scope of this explanation. </p><p>But hopefully, this has equipped you with a fundamental understanding of how transformers work and why they have revolutionized the field of NLP. </p><p>So, the next time you encounter a seamless machine translation or marvel at the creativity of an AI-powered text generator, remember the intricate dance of the encoder and decoder within the transformer, weaving magic with the power of attention and parallel processing.</p><p><em>Paper: https://arxiv.org/abs/1706.03762 </em></p><div data-attrs="{&quot;url&quot;:&quot;https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="CaptionedButtonToDOM"><p>Thank you for reading The ZenMode. This post is public so feel free to share it.</p><p data-attrs="{&quot;url&quot;:&quot;https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="ButtonCreateButton"><a href="https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel="nofollow ugc noopener"><span>Share</span></a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beyond self-attention: How a small language model predicts the next token (395 pts)]]></title>
            <link>https://shyam.blog/posts/beyond-self-attention/</link>
            <guid>39251909</guid>
            <pubDate>Sun, 04 Feb 2024 16:54:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shyam.blog/posts/beyond-self-attention/">https://shyam.blog/posts/beyond-self-attention/</a>, See on <a href="https://news.ycombinator.com/item?id=39251909">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="single"><p><time datetime="2024-01-29 17:25:21 -0700 -0700">Jan 29, 2024</time>
<span>·</span>
<span>17754 words</span>
<span>·</span>
<span>84 minute read</span></p><div><p>I trained a small (~10 million parameter) <a href="https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29" target="_blank" rel="noopener">transformer</a> following <a href="https://karpathy.ai/" target="_blank" rel="noopener">Andrej Karpathy</a>’s excellent tutorial, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Let’s build GPT: from scratch, in code, spelled out</a>. After getting it working, I wanted to understand, as deeply as possible, what it was doing internally and how it produced its results.</p><p>The <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">original paper</a>, as well every transformer tutorial I found, focuses primarily on <a href="https://machinelearningmastery.com/the-transformer-attention-mechanism/" target="_blank" rel="noopener">multi-head self-attention</a>, the mechanism by which transformers learn multiple relationships between tokens without relying on recurrences or convolution. But none of the papers or tutorials I encountered give a satisfying explanation of what happens <em>after attention</em>: <strong>how exactly do the results of the attention computation turn into accurate predictions for the next token?</strong></p><p>I thought I could run a few example prompts through the small but working transformer I’d trained, examine the internal states, and figure this out. What I thought would be a quick investigation turned out to be a 6-month deep dive, but yielded some results I think are worth sharing. Specifically, I have a working theory that explains how the transformer produces its predictions and some empirical evidence that suggests this explanation is at least plausible.</p><p>For those readers familiar with transformers and eager for the punchline, here it is: Each transformer block (containing a multi-head self-attention layer and feed-forward network) learns weights that associate a given prompt with a class of strings found in the training corpus. <strong>The distribution of tokens that follow those strings in the training corpus is, approximately, what the block outputs as its predictions for the next token.</strong> Each block may associate the same prompt with a different class of training corpus strings, resulting in a different distribution of next tokens and thus different predictions. The final transformer output is a linear combination of each block’s predictions.</p><p>I implemented imperative code that does what I’m proposing the transformer is doing. It produces outputs very similar to the transformer, which I’ll review in detail in a <a href="#evaluating-the-approximation">later section</a>.</p><p>In this post, I’m going to briefly introduce the model and training data, demo some evidence for my proposed explanation, give a detailed walkthrough of the imperative code implementation of it, and present the supporting evidence I have for my theory. I’ve tried to keep the main narrative succinct, with links to relevant technical details and justifications in the <a href="#appendices">appendices</a> or other notebooks in the <a href="https://shyam.blog/posts/beyond-self-attention/%28https://github.com/spather/transformer-experiments%29">repo</a>.</p><blockquote><p>This project is my first foray into this type of open-ended ML research. I’m sure I have made errors or omissions that would be obvious to more experienced researchers. I welcome any feedback on this work at <code>shyam.pather at gmail dot com</code>.</p></blockquote><h2 id="the-model-and-setup">The Model and Setup <a href="#the-model-and-setup">🔗</a></h2><blockquote><h3 id="disclaimer">Disclaimer <a href="#disclaimer">🔗</a></h3><p>I want to start by saying upfront: the code for the model I trained isn’t mine. It came from <a href="https://karpathy.ai/" target="_blank" rel="noopener">Andrej Karpathy</a>’s video, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Let’s build GPT: from scratch, in code, spelled out</a> (highly recommend).</p><p>I typed in the code by copying what I saw on the screen as I watched the video. For things that weren’t clear onscreen, I referenced the <a href="https://github.com/karpathy/ng-video-lecture" target="_blank" rel="noopener">GitHub repo for the video</a> and the <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener">nanoGPT repo</a>. After getting it working, I made only minor changes to make it work with the rest of the code in/structure of <a href="https://github.com/spather/transformer-experiments" target="_blank" rel="noopener">my repository</a>, resulting in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb" target="_blank" rel="noopener">this implementation</a>. In summary: the core language model is Andrej Karpathy’s work, not mine. The analysis and all the supporting code behind it are my original contributions. I’ll acknowledge and cite influential papers, posts, tutorials, and other resources in the relevant places.</p></blockquote><h3 id="model-overview">Model Overview <a href="#model-overview">🔗</a></h3><p>The model is a 6-block, decoder-only <a href="https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29" target="_blank" rel="noopener">transformer</a>:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/a9f2adc6c1c25ebb263caf42df37f4429c4ed44eda0a0a228cba52b7a00aeb9d.png" alt=""></p><p>It’s trained on the <a href="https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt" target="_blank" rel="noopener">TinyShakespeare data set</a> which contains 40,000 lines of Shakespeare’s plays. After about an hour of training on an RTX 4000 GPU, it is able to produce reasonable-looking faux Shakespeare.</p><p>Given a prompt, the model predicts tokens that it thinks should follow. Let’s look at an example: starting with the prompt, <code>ROMEO:</code>, and sampling 500 tokens from the model’s predictions, we get:</p><pre tabindex="0"><code>ROMEO:
If thou wilt triumphant be virtue, and since from any
bold virtue that is made a bawd of earth, then the
duke desires of patience and perish:
take up the other husband, dislike his tent
back.

First Citizen:
Ourself goes, go back: you have no consul, but the disguised gods.

Second Citizen:
We choose him in the world, he did runk itself.

First Citizen:
Sir, I am I a man changed him and thriving, I have heard the
king.

CORIOLANUS:
Consider him!

AUFIDIUS:
Most gracious irice, and you must danc
</code></pre><p>It’s not Shakespeare but structurally, it’s plausible Shakespeare. It looks like the script for a play, the language sounds archaic, the character names/titles come from real Shakespeare plays. Most of the words are English words. Punctuation and capitalization are mostly sensible. Clearly, none of the text actually makes sense, but still, it’s not bad for an hour of training.</p><p>The <strong>tokens in the model are characters</strong>, not words. Given a prompt, the model predicts a probability distribution for the next character. For example, given the prompt <code>'my most gr</code>, the model predicts these probabilities for the next token:</p><pre tabindex="0"><code>'a' 0.819
'e' 0.081
'i' 0.059
'o' 0.036
'u' 0.004
'y' 0.001
'w' 0.000
'r' 0.000
'g' 0.000
's' 0.000
</code></pre><p><a href="#i-model-details">Appendix I</a> provides a few more details about the model. Beyond that, if you want to know more, <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb" target="_blank" rel="noopener">the code</a> and <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Andrej’s video</a> are the best resources.</p><h3 id="transformer-block-structure">Transformer Block Structure <a href="#transformer-block-structure">🔗</a></h3><p>Each of the 6 blocks in the architecture diagram above contains two significant sub-components: a multi-head self-attention layer and a feed-forward network, wired together via a mix of direct and residual connections as follows:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/72a30adc39ebf5f278c0a257fb46f26e6d666d113736e36ce394db587110260c.png" alt=""></p><p>The <code>Block</code> module implements this wiring in PyTorch:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Block</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>"""One transformer block"""</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> __init__(self, n_embed, n_head):
</span></span><span><span>        super()<span>.</span>__init__()
</span></span><span><span>        head_size <span>=</span> n_embed <span>//</span> n_head
</span></span><span><span>        self<span>.</span>sa <span>=</span> MultiHeadAttention(n_head, head_size)
</span></span><span><span>        self<span>.</span>ffwd <span>=</span> FeedForward(n_embed)
</span></span><span><span>        self<span>.</span>ln1  <span>=</span> nn<span>.</span>LayerNorm(n_embed)
</span></span><span><span>        self<span>.</span>ln2 <span>=</span> nn<span>.</span>LayerNorm(n_embed)
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>sa(self<span>.</span>ln1(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>ffwd(self<span>.</span>ln2(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> x
</span></span></code></pre></div><p>While many words have been written and spoken about multi-head attention, comparatively little has been said about the feed-forward network because, it seems, comparatively little is known:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/b8214cdd1f6c9466bb984529984c757d780148fb4fe44bfed7714216e12bff73.png" alt=""></p><p>Screenshot from <a href="https://stats.stackexchange.com/q/485910">https://stats.stackexchange.com/q/485910</a></p><p>I started this investigation wondering what comes after attention. Literally, the feed-forward network does. In the transformer I studied, across all 6 blocks, the feed-forward networks comprise over 65% of the total trainable parameters, so they must play some important role.</p><p>As I’ll show <a href="#transformation-via-vector-addition">later</a>, it turns out that the output of the feed-forward network is the primary factor that determines how a block transforms its input into its output.</p><h2 id="demo-my-proposal-in-action">Demo: My Proposal In Action <a href="#demo-my-proposal-in-action">🔗</a></h2><p>In this section, I’m going to show an example that illustrates what I’m proposing the transformer is doing. In the next section, I’ll go into detail about how this is implemented.</p><p>Imagine we did the following:</p><ul><li>Ran the prompt, <code>'And only l'</code>, through the model and extracted the output value of the feed-forward network in the first transformer block.</li><li>Went back to the training corpus, found all substrings of the same length as our prompt (10-characters), ran all of them through the model, and filtered out just the ones whose feed-forward network outputs in the first block have a cosine similarity of 0.95 or greater when compared to that of the prompt, <code>'And only l'</code>.</li></ul><p>We’d come up with this set of strings:</p><pre tabindex="0"><code>'hat only l'    's sickly l'    ' as\nthey l'   'r kingly l'    're; they l'
'eby they l'    'ar, they l'    'im, only l'    'ling any l'    'life may l'
'nobility l'    'e\nBy any l'   ' as they l'    ', if any l'    ' hastily l'
'tly they l'    ' ghastly l'    '\nMy only l'   'For many l'    'r in any l'
' till my l'    'all they l'    'hen they l'    'at Henry l'    'oolishly l'
'er:\nThey l'   'may they l'    'or stony l'    'ur Henry l'    'l gladly l'
'yet they l'    'y;\nDelay l'   'e, on my l'    'or Henry l'    'I dearly l'
' if they l'    ' she may l'    't\nfairly l'   'ould say l'    'd all my l'
'her they l'    ' Stanley l'    ' and may l'    'uld they l'    'u all my l'
'friendly l'    'h gently l'    'e deadly l'    'f all my l'    'n all my l'
'Ere they l'    'steel my l'    ' tell my l'    'e kingly l'    'learn my l'
'd he say l'    't basely l'    'Thursday l'    'iciously l'    " 'if any l"
' as many l'    'hy glory l'    'not very l'    'a goodly l'    'e surely l'
'quiously l'    ', fairly l'    'lord! my l'    'entle my l'    ', he may l'
'our holy l'    ' worldly l'    ' my only l'    ' all, my l'
'ul, they l'    'o lately l'    's in any l'    ' no lady l'
'ter many l'    'Our holy l'    't vainly l'    'e\nA lady l'
' you may l'    'y greedy l'    'untimely l'    'directly l'
'er on my l'    'e wistly l'    'ng Henry l'    'And only l'
's kindly l'    'KE:\nThey l'   ' of many l'    'o, on my l'
</code></pre><p>There’s a clear pattern across these: they all end in <code>y l</code> and several of them end in <code>ly l</code>. Similarity in the space of feed-forward network outputs seems to correspond to human-interpretable patterns.</p><p>Next, imagine we went back to the training corpus, found each of these strings and built a distribution of all the characters that came after them. We’d find, for example:</p><ul><li><code>'hat only l'</code> is followed by <code>i</code> (“T<code>hat only l</code><strong>i</strong>ke a gulf it did remain”)</li><li><code>'l gladly l'</code> is followed by <code>e</code> (“I’l<code>l gladly l</code><strong>e</strong>arn.”)</li><li><code>'n all my l'</code> is followed by both <code>a</code> and <code>i</code> (“I<code>n all my l</code><strong>a</strong>nds and leases whatsoever” and “never saw you before i<code>n all my l</code><strong>i</strong>fe”)</li></ul><p>Doing this for the complete set of 94 strings, we’d end up with this distribution:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/170aed320bd4ab2e2647d8d1ef50b499b215ce1905cff1b5db6fe78dd83c3df3.png" alt=""></p><p>The various tokens in our model’s vocabulary appear on the x-axis and the normalized frequency of occurrence on the y-axis. This plot shows that <code>i</code> was the most frequent, then <code>o</code>, then <code>a</code>, and finally, <code>e</code>.</p><p>Now let’s look at the final output of the transformer as a whole when given <code>And only l</code> as a prompt:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/08cb64e75759e2a329a0a296931433b06c02c2fb92b6dd0bb440af807cde1d86.png" alt=""></p><p>This is a probability distribution representing the model’s predictions for the next token. Notice that it’s strikingly similar to the normalized frequency distribution shown in the previous plot!</p><p>We can quantify how similar they are. <a href="https://en.wikipedia.org/wiki/Hellinger_distance" target="_blank" rel="noopener">Hellinger distance</a> is a measure of overlap between probability distributions. Given distributions \(P\) and \(Q\), the Hellinger distance between them is:</p><p>$$
H(P, Q) = \frac{1}{\sqrt{2}} \sqrt{\sum_{i=1}^n (\sqrt{p_i} - \sqrt{q_i})^2}
$$</p><p>Or, in code:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>hellinger_distance</span>(
</span></span><span><span>    p: torch<span>.</span>Tensor,
</span></span><span><span>    q: torch<span>.</span>Tensor,
</span></span><span><span>):
</span></span><span><span>    <span>return</span> ((p<span>.</span>sqrt() <span>-</span> q<span>.</span>sqrt())<span>**</span><span>2</span>)<span>.</span>sum(dim<span>=-</span><span>1</span>)<span>.</span>sqrt() <span>/</span> math<span>.</span>sqrt(<span>2</span>)
</span></span></code></pre></div><p>Hellinger distance of 0 means the two distributions are identical and 1 means they have no overlap.</p><p>The Hellinger distance between the two distributions above - the distribution formed from the tokens that follow the strings with similar feed-forward network outputs and the distribution the model predicts - is 0.07: very nearly identical.</p><p>For the sake of keeping the demo brief, I chose an example where the first block’s similar strings alone are enough to produce a distribution that closely matches the final output of the transformer. Typically, we’d need to need to do the same exercise - finding the strings in the training corpus that produce similar feed-forward network outputs to the prompt and building a distribution from the tokens that succeed them - for all 6 transformer blocks, and then calculate a weighted sum of the resulting distributions in order to get a good match. We’ll do that in the next section and see that <strong>across a sample of 20,000 prompts, the average Hellinger distance between distributions computed this way and the corresponding transformer output was just 0.17</strong>.</p><p>This small average Hellinger distances suggests the results produced by this approach are a good approximation for the transformer’s outputs. In addition, as I’ll explain in the <a href="#interpretation-why-does-the-approximation-work">interpretation</a> section, I think the approach itself is a reasonable approximation of what the transformer is actually doing.</p><h2 id="implementation-approximating-the-transformer-output-with-feed-forward-network-outputs">Implementation: Approximating the Transformer Output with Feed-forward Network Outputs <a href="#implementation-approximating-the-transformer-output-with-feed-forward-network-outputs">🔗</a></h2><p>In this section, I’m going to walk through in some detail and with code, the exact procedure I used to approximate the transformer’s output using strings that produced similar feed-forward network outputs. If you’re not interested in the implementation, skip this section and proceed to the <a href="#evaluating-the-approximation">evaluation</a> section.</p><p>To recap, this is the procedure to compute the approximation:</p><ol><li>Run a prompt through the model and save the feed-forward network outputs for each block.</li><li>For each block:<ul><li>Find the strings in the training corpus that produce the most similar feed-forward network outputs to the prompt for that block.</li><li>For each string found, build a frequency distribution of the tokens that come after it in the training corpus.</li><li>Sum the frequency distributions for all strings found for the current block.</li></ul></li><li>Compute a weighted sum of the frequency distributions for each block computed in the previous step.</li><li>Normalize the weighted sum to get a probability distribution.</li></ol><h3 id="procedure-setup">Procedure Setup <a href="#procedure-setup">🔗</a></h3><p>The first step of the procedure - running a prompt through the model and saving the feed-forward network outputs for each block - is straightforward to accomplish with some basic PyTorch hooks. But the first part of step two - finding the strings in the training corpus that produce similar feed-forward network outputs - requires some additional machinery to do efficiently.</p><p>I did all the analysis with length 10 strings for compute and storage efficiency (but I also observed that the results hold for both shorter and longer strings). The 1,115,394-character long training corpus contains 858,923 unique, length 10 substrings. Each feed-forward network output is a 384-dimensional vector of <code>float32</code> values and the model produces 6 of them (one for each block). Comparing the 6 384-dimensional feed-forward outputs for any prompt to 6 * 858,923 = 5,153,538 feed-forward outputs from all the other strings takes a long time. To able to work with this data, I had to pre-compute things. I built the following pipeline:</p><ol><li>I chose 20,000 length 10 strings from the training corpus at random to use as prompts in this experiment.</li><li>Overnight, I ran a process to compute the cosine similarity between the feed-forward network outputs the model produced for the 20,000 prompts and those it produced for the 858,923 unique length 10 substrings of the training corpus. I did this in batches and saved the results to disk.</li><li>Even after pre-computing the cosine similarity results, searching through all of them to find the closest matches took a long time. Experiments showed matches of interest never had a cosine similarity below 0.7, so I ran another step to pre-filter the results of step 2 to just those entries with cosine similarity &gt;= 0.7. This greatly reduced the number of entries to search through.</li></ol><p>The code for this pre-computation and pre-filtering is too much to include in this post, but the implementation is available in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/cosine-sims.ipynb" target="_blank" rel="noopener">the <code>cosine-sims</code> experiment notebook</a>.</p><h3 id="procedure-walkthrough">Procedure Walkthrough <a href="#procedure-walkthrough">🔗</a></h3><p>In this section, we’ll build up the code step by step and run it on one prompt at a time and for just one block. Over the following sections, we’ll extend it to additional blocks, run it across a large number of prompts, and examine the results.</p><p>First, we need to grab 20,000 length 10 strings from the training corpus to use as prompts:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Get all the unique substrings in the text</span>
</span></span><span><span>strings10 <span>=</span> all_unique_substrings(text<span>=</span>ts<span>.</span>text, substring_length<span>=</span><span>10</span>)
</span></span><span><span>
</span></span><span><span>n_prompts <span>=</span> <span>20000</span>
</span></span><span><span>
</span></span><span><span>torch<span>.</span>manual_seed(<span>1337</span>)
</span></span><span><span>indices <span>=</span> torch<span>.</span>randperm(len(strings10))[:n_prompts]
</span></span><span><span>prompts <span>=</span> [strings10[i<span>.</span>item()] <span>for</span> i <span>in</span> indices]
</span></span></code></pre></div><p>As described in the <a href="#procedure-setup">Procedure Setup</a> section, I previously ran all these strings through the model, grabbed the feed-forward network outputs for each block, and pre-computed the cosine similarities to all the unique length 10 substrings in the training corpus. And then I pre-filtered the results to just those with cosine similarity &gt;= 0.7.</p><p>The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/cosine-sims.ipynb" target="_blank" rel="noopener">the <code>cosine-sims</code> experiment notebook</a> that implements all this also exports a helper function, <code>filter_on_prefiltered_results()</code>, that we can use to find the most similar strings to a given prompt by searching over the pre-filtered results.</p><blockquote><p>If you’re curious about how this works, check out the notebook. It’s pretty straightforward and the unit test provides a simple example that illustrates the shape of the inputs and outputs.</p></blockquote><p>To use <code>filter_on_prefiltered_results()</code>, we just need to tell it how to find the prefiltered files:</p><div><pre tabindex="0"><code data-lang="python"><span><span>prefiltered_threshold<span>=</span><span>0.7</span>
</span></span><span><span>prefiltered_results_folder <span>=</span> environment<span>.</span>data_root <span>/</span> <span>'cosine_sim_results/large_files/slen10'</span> <span>/</span> <span>f</span><span>'prefiltered_</span><span>{</span>prefiltered_threshold<span>}</span><span>'</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>prefiltered_filename</span>(block_idx: int, q_idx: int) <span>-&gt;</span> Path:
</span></span><span><span>    <span>return</span> prefiltered_results_folder <span>/</span> <span>f</span><span>'cosine_sim_ffwd_out_</span><span>{</span>q_idx<span>:</span><span>05d</span><span>}</span><span>_</span><span>{</span>block_idx<span>:</span><span>02d</span><span>}</span><span>.pt'</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>load_prefiltered_data</span>(block_idx: int, q_idx: int):
</span></span><span><span>    <span>return</span> torch<span>.</span>load(prefiltered_filename(block_idx, q_idx))
</span></span></code></pre></div><blockquote><p>Note on the use of <code>q_idx</code> here and in the rest of the code: <code>q_idx</code> refers to “query index”. The job that pre-computes all the cosine similarities takes a set of “queries” or values to compare to. These queries are the feed-forward network outputs the model produces for the prompts. There is a 1:1 correspondence between queries and prompts and so I’ve used the terms interchangeably in the code.</p></blockquote><p>To start, we’ll use the same prompt - <code>'And only l'</code> - we used in the earlier demo. It happens to be the prompt at index 57:</p><pre tabindex="0"><code>'And only l'
</code></pre><p>We’ll find the strings whose feed-forward network outputs in block 0 had a cosine similarity of 0.95 or greater when compared to the block 0 feed forward network output of the prompt.</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.95</span>
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>94
</code></pre><p>This produced the 94 similar strings we saw in the demo. We can print them again to be sure:</p><div><pre tabindex="0"><code data-lang="python"><span><span>print(<span>f</span><span>"Original string: </span><span>{</span>repr(prompts[q_idx])<span>}</span><span>"</span>)
</span></span><span><span>print(<span>"Similar strings: </span><span>\n</span><span>"</span>)
</span></span><span><span>
</span></span><span><span>data_columns<span>=</span>[
</span></span><span><span>    [repr(s) <span>for</span> s <span>in</span> similar_strings[<span>0</span>][i : i <span>+</span> <span>20</span>]] <span>for</span> i <span>in</span> range(<span>0</span>, len(similar_strings[<span>0</span>]), <span>20</span>)
</span></span><span><span>]
</span></span><span><span>
</span></span><span><span>print(text_table(
</span></span><span><span>    headers<span>=</span>[],
</span></span><span><span>    data_columns<span>=</span>data_columns,
</span></span><span><span>    col_widths<span>=</span>[<span>18</span> <span>for</span> _ <span>in</span> data_columns]
</span></span><span><span>))
</span></span></code></pre></div><pre tabindex="0"><code>Original string: 'And only l'
Similar strings:

'hat only l'      's sickly l'      ' as\nthey l'     'r kingly l'      're; they l'
'eby they l'      'ar, they l'      'im, only l'      'ling any l'      'life may l'
'nobility l'      'e\nBy any l'     ' as they l'      ', if any l'      ' hastily l'
'tly they l'      ' ghastly l'      '\nMy only l'     'For many l'      'r in any l'
' till my l'      'all they l'      'hen they l'      'at Henry l'      'oolishly l'
'er:\nThey l'     'may they l'      'or stony l'      'ur Henry l'      'l gladly l'
'yet they l'      'y;\nDelay l'     'e, on my l'      'or Henry l'      'I dearly l'
' if they l'      ' she may l'      't\nfairly l'     'ould say l'      'd all my l'
'her they l'      ' Stanley l'      ' and may l'      'uld they l'      'u all my l'
'friendly l'      'h gently l'      'e deadly l'      'f all my l'      'n all my l'
'Ere they l'      'steel my l'      ' tell my l'      'e kingly l'      'learn my l'
'd he say l'      't basely l'      'Thursday l'      'iciously l'      " 'if any l"
' as many l'      'hy glory l'      'not very l'      'a goodly l'      'e surely l'
'quiously l'      ', fairly l'      'lord! my l'      'entle my l'      ', he may l'
'our holy l'      ' worldly l'      ' my only l'      ' all, my l'
'ul, they l'      'o lately l'      's in any l'      ' no lady l'
'ter many l'      'Our holy l'      't vainly l'      'e\nA lady l'
' you may l'      'y greedy l'      'untimely l'      'directly l'
'er on my l'      'e wistly l'      'ng Henry l'      'And only l'
's kindly l'      'KE:\nThey l'     ' of many l'      'o, on my l'
</code></pre><p>Next, we’ll need to build a frequency distribution for the tokens that came after these strings in the text. To make this easy and efficient (we’ll eventually be doing many times), we can pre-compute the next token frequency distributions for all the unique length 10 substrings in the training corpus. The helper function <code>build_next_token_map()</code>, implemented in the <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/common/text-analysis.ipynb" target="_blank" rel="noopener">text-analysis module</a>, does this.</p><div><pre tabindex="0"><code data-lang="python"><span><span>next_token_map10 <span>=</span> build_next_token_map(
</span></span><span><span>    text<span>=</span>ts<span>.</span>text,
</span></span><span><span>    prefix_len<span>=</span><span>10</span>,
</span></span><span><span>    vocab_size<span>=</span>tokenizer<span>.</span>vocab_size,
</span></span><span><span>    stoi<span>=</span>tokenizer<span>.</span>stoi
</span></span><span><span>)
</span></span></code></pre></div><p>The return value stored in <code>next_token_map10</code> is a dictionary that maps each unique length 10 substring in the training corpus to a frequency distribution of the tokens that come after it. Conceptually, it looks something like this:</p><div><pre tabindex="0"><code data-lang="python"><span><span>{
</span></span><span><span>    <span>'the common'</span>: {
</span></span><span><span>        <span>' '</span>: <span>12</span>, <span>"'"</span>: <span>1</span>, <span>','</span>: <span>1</span>, <span>'?'</span>: <span>1</span>, <span>'a'</span>: <span>1</span>, <span>'s'</span>: <span>5</span>, <span>'w'</span>: <span>3</span>
</span></span><span><span>    },
</span></span><span><span>    <span>' the gods '</span>: {
</span></span><span><span>        <span>'b'</span>: <span>1</span>, <span>'c'</span>: <span>1</span>, <span>'d'</span>: <span>2</span>, <span>'f'</span>: <span>1</span>, <span>'g'</span>: <span>1</span>, <span>'h'</span>: <span>2</span>, <span>'k'</span>: <span>2</span>, <span>'s'</span>: <span>2</span>, <span>'t'</span>: <span>1</span>, <span>'w'</span>: <span>2</span>
</span></span><span><span>    },
</span></span><span><span>    <span>' authority'</span>: {
</span></span><span><span>        <span>'</span><span>\n</span><span>'</span>: <span>1</span>, <span>' '</span>: <span>5</span>, <span>','</span>: <span>5</span>, <span>':'</span>: <span>2</span>, <span>';'</span>: <span>1</span>
</span></span><span><span>    },
</span></span><span><span>    <span>...</span>
</span></span><span><span>}
</span></span></code></pre></div><p>In reality, the values are actually tensors of shape <code>(vocab_size,)</code> where <code>vocab_size</code> is the number of unique tokens the vocabulary (65, in our case). The item at index <code>i</code> in the tensor is the count of occurrences of the <code>i</code>th token after the string in that entry’s key. So it looks more like:</p><div><pre tabindex="0"><code data-lang="python"><span><span>{
</span></span><span><span>      <span>'the common'</span>: torch<span>.</span>tensor([
</span></span><span><span>            <span>0</span>, <span>12</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>5</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>3</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>
</span></span><span><span>      ]),
</span></span><span><span>      <span>' the gods '</span>: torch<span>.</span>tensor([
</span></span><span><span>            <span>0</span>, <span>12</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>5</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>3</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>
</span></span><span><span>      ]),
</span></span><span><span>      <span>' authority'</span>: torch<span>.</span>tensor([
</span></span><span><span>          <span>0</span>, <span>12</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>          <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>          <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>          <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>5</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>3</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>
</span></span><span><span>      ]),
</span></span><span><span>    <span>...</span>
</span></span><span><span>}
</span></span></code></pre></div><p>Next, we need to sum the frequency distributions for all the strings we found to have similar feed-forward network outputs to our prompt. Because <code>next_token_map10</code> stores the individual frequency distributions as tensors, this is easy to accomplish:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span></code></pre></div><p>We stack up the distributions for each similar string into a single tensor and then sum across all of them. We can now turn this into a probability distribution by dividing each entry by the sum of all the entries:</p><div><pre tabindex="0"><code data-lang="python"><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span></code></pre></div><p>Finally, we can visualize this distribution:</p><div><pre tabindex="0"><code data-lang="python"><span><span>plot_prob_distribution_for_tokens(prob_distribution, title<span>=</span><span>'Probability distribution using only block 0 similar strings'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/c04c3fbe83a543ea834691f8ef6c5ecdee18522f3e3e456cd9ea81209eb60b00.png" alt=""></p><p>It’s the same distribution we saw in the demo.</p><p>Now let’s code the comparison to the model output:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>logits<span>.</span>plot_probs(title<span>=</span><span>'Probability distribution from model'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/08cb64e75759e2a329a0a296931433b06c02c2fb92b6dd0bb440af807cde1d86.png" alt=""></p><p>Again, the two distributions look very similar, and in this example, the approximation uses only values from the first block. To better compare them, we can look at the distributions in text form:</p><div><pre tabindex="0"><code data-lang="python"><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            i: 0.389
o: 0.204            o: 0.250
a: 0.195            a: 0.222
e: 0.160            e: 0.139
</code></pre><p>Finally, we can also compare the Hellinger distance between these distributions:</p><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.0711)
</code></pre><p>By combining the next token frequency distributions of the similar strings from just the first layer of the model, we are able to pretty closely approximate the output of the transformer. Of course, I chose an example that works particularly well.</p><p>Here’s an example where the frequency distribution from just the first layer doesn’t work well:</p><pre tabindex="0"><code>'hing tremb'
</code></pre><p>Using the same method, we can identify 57 strings from the training corpus that produce similar feed-forward network outputs to the prompt:</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.95</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>57
</code></pre><p>We can look up, sum, and normalize the frequency distributions of tokens that follow these strings in the training corpus, and compare the result to the model outputs, as we did before:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
l: 0.999            e: 0.543
e: 0.000            l: 0.343
r: 0.000            r: 0.114
</code></pre><p>Unlike the previous example, these distributions are quite different. The top 3 tokens are the same in each, but they’re in the wrong order and their probabilities are far apart. These differences contribute to a large Hellinger distance:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.6305)
</code></pre><p>For the prompt, <code>'hing tremb'</code>, just using the values from the first block results in a poor approximation of the transformer’s output. We’ll soon add the contributions from other blocks and when we do, we’ll get the Hellinger distance between the approximation and the real transformer output for this prompt down from 0.63 to just 0.02.</p><h3 id="similarity-thresholds">Similarity Thresholds <a href="#similarity-thresholds">🔗</a></h3><p>In the preceding examples, I used a similarity threshold of 0.95: I searched for strings whose feed-forward network outputs in block 0 produced values with a cosine similarity of 0.95 or greater when compared to the feed-forward network output of the prompt.</p><p>A different threshold would have yielded different results. For example, doing the same exercise for prompt id 57 (<code>'And only l'</code>) with a threshold of 0.90 finds 612 similar strings, vs the 94 we had before:</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.90</span>
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>612
</code></pre><p>If we do the rest of the approximation procedure, we see different (and worse) results:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            o: 0.584
o: 0.204            i: 0.251
a: 0.195            a: 0.095
e: 0.160            e: 0.066
u: 0.004            u: 0.002
l: 0.000            y: 0.001
</code></pre><p>The top 5 tokens are the same, but when ranked by probability, the approximation has a different ordering than the model. The Hellinger distance is also higher:</p><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.2856)
</code></pre><p>Loosening the similarity threshold introduced strings into the calculation that resulted in a worse approximation. Tightening beyond 0.95 also produces worse results than we got with 0.95, presumably because we’re excluding strings that were needed to produce a good approximation:</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.97</span>
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>33
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            o: 0.278
o: 0.204            i: 0.250
a: 0.195            a: 0.250
e: 0.160            e: 0.222
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.1498)
</code></pre><p>For the first block, 0.95 appears to be a sweet spot. I came up with this threshold through manual tuning: trying different values and binary searching towards one that produced the best results. The full history of this tuning exercise is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/40_widening_similar_space.ipynb" target="_blank" rel="noopener">the similar space analysis notebook</a>.</p><p>In the end, I found the following thresholds produce the best results for each block:</p><table><thead><tr><th>Block</th><th>Similarity Threshold</th></tr></thead><tbody><tr><td>0</td><td>0.95</td></tr><tr><td>1</td><td>0.94</td></tr><tr><td>2</td><td>0.85</td></tr><tr><td>3</td><td>0.76</td></tr><tr><td>4</td><td>0.81</td></tr><tr><td>5</td><td>0.89</td></tr></tbody></table><blockquote><p>When I first started exploring this space, I assumed the approximation would get better the more similarity I could find. I tried a number of techniques, including experimenting with Euclidean distance vs cosine similarity, searching across strings of different lengths, etc. Every time I succeeded in finding strings with more similar feed-forward network outputs to use in the approximation, the results got worse. I realized that, at least for some blocks, including <em>less</em> similar values in the mix produced better approximations, probably because those blocks had learned to map prompts to broader classes of strings in the training corpus.</p></blockquote><h3 id="going-beyond-the-first-block">Going Beyond the First Block <a href="#going-beyond-the-first-block">🔗</a></h3><p>Thus far, we’ve only considered feed-forward network outputs from the first block. Now we’ll incorporate the contributions from the other blocks.</p><p>First, let’s find the strings that produce similar feed-forward network outputs in each block, using the similarity thresholds listed above. For now, we’ll do this for just one query (index 57, <code>'And only l'</code>):</p><div><pre tabindex="0"><code data-lang="python"><span><span>similarity_thresholds<span>=</span>[<span>0.95</span>, <span>0.94</span>, <span>0.85</span>, <span>0.76</span>, <span>0.81</span>, <span>0.89</span>]
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>
</span></span><span><span>similar_strings_per_block <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>    similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>        load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>        q_idx_start<span>=</span>q_idx,
</span></span><span><span>        q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>        filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_thresholds[block_idx]
</span></span><span><span>    )
</span></span><span><span>    similar_strings <span>=</span> [
</span></span><span><span>        [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>        <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>    ]
</span></span><span><span>    similar_strings_per_block<span>.</span>append(similar_strings)
</span></span></code></pre></div><p>Let’s summarize how many strings we found for each block based on these thresholds:</p><div><pre tabindex="0"><code data-lang="python"><span><span>print(text_table(
</span></span><span><span>    headers<span>=</span>[<span>"Block Index"</span>, <span>"Similarity Threshold"</span>, <span>"# of Similar Strings"</span>],
</span></span><span><span>    data_columns<span>=</span>[
</span></span><span><span>        [<span>f</span><span>"</span><span>{</span>block_idx<span>:</span><span>&gt;10</span><span>}</span><span>"</span> <span>for</span> block_idx <span>in</span> range(n_layer)],
</span></span><span><span>        [<span>f</span><span>"</span><span>{</span>threshold<span>:</span><span>&gt;19</span><span>}</span><span>"</span> <span>for</span> threshold <span>in</span> similarity_thresholds],
</span></span><span><span>        [<span>f</span><span>"</span><span>{</span>len(similar_strings[<span>0</span>])<span>:</span><span>&gt;19</span><span>}</span><span>"</span> <span>for</span> similar_strings <span>in</span> similar_strings_per_block],
</span></span><span><span>    ],
</span></span><span><span>    col_widths<span>=</span>[<span>14</span>, <span>23</span>, <span>23</span>]
</span></span><span><span>))
</span></span></code></pre></div><pre tabindex="0"><code>Block Index   Similarity Threshold   # of Similar Strings
-----------   --------------------   --------------------
         0                   0.95                     94
         1                   0.94                     47
         2                   0.85                     70
         3                   0.76                    108
         4                   0.81                    175
         5                   0.89                   2237
</code></pre><p>Now that we’ve identified the right strings for each block, we can do the next step of the approximation procedure: build the frequency distributions for the tokens that follow those strings, and sum them up. We’re going to be doing this several times over, so let’s define a function for it:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>frequency_distribution_from_similar_strings</span>(
</span></span><span><span>    similar_strings_per_block: Sequence[Sequence[Sequence[str]]],
</span></span><span><span>    next_token_map: Dict[str, torch<span>.</span>Tensor],
</span></span><span><span>) <span>-&gt;</span> torch<span>.</span>Tensor:
</span></span><span><span>    <span># freqs_per_block_per_query is a list of lists of tensors. The outer list has</span>
</span></span><span><span>    <span># one item per block. The inner list has one item per query. Each</span>
</span></span><span><span>    <span># tensor is the next token frequency distribution for a particular</span>
</span></span><span><span>    <span># block and query.</span>
</span></span><span><span>    freqs_per_block_per_query: List[List[torch<span>.</span>Tensor]] <span>=</span> [[] <span>for</span> _ <span>in</span> range(n_layer)]
</span></span><span><span>
</span></span><span><span>    <span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>        <span>for</span> similar_strings <span>in</span> similar_strings_per_block[block_idx]:
</span></span><span><span>            freqs_per_block_per_query[block_idx]<span>.</span>append(
</span></span><span><span>                torch<span>.</span>stack([next_token_map[string] <span>for</span> string <span>in</span> similar_strings])<span>.</span>sum(
</span></span><span><span>                    dim<span>=</span><span>0</span>
</span></span><span><span>                )
</span></span><span><span>            )
</span></span><span><span>
</span></span><span><span>    <span># Stack all frequency tensors into a single tensor of shape</span>
</span></span><span><span>    <span># (n_layer, n_queries, vocab_size)</span>
</span></span><span><span>    freqs <span>=</span> torch<span>.</span>stack(
</span></span><span><span>        [
</span></span><span><span>            torch<span>.</span>stack(freqs_per_block_per_query[block_idx])
</span></span><span><span>            <span>for</span> block_idx <span>in</span> range(n_layer)
</span></span><span><span>        ]
</span></span><span><span>    )
</span></span><span><span>
</span></span><span><span>    <span>return</span> freqs
</span></span></code></pre></div><p>This function, <code>frequency_distribution_from_similar_strings()</code>, does the equivalent of this code we looked at earlier:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span></code></pre></div><p>But with two key differences:</p><ul><li>It does this calculation for all the blocks, using the similar strings we found for each block above.</li><li>It allows for more than one query. In the code we’ve looked at so far, we only evaluated the approximation for a single prompt. In the next section, we’ll be running it for lots of prompts so I’ve written the code in a more general form to a allow for this. Specifically, the code allows for <code>similar_strings_per_block</code> to contain not just a single list of strings per block but multiple: one for each query.</li></ul><p>Let’s run this on the <code>similar_strings_per_block</code> we constructed earlier:</p><div><pre tabindex="0"><code data-lang="python"><span><span>freq_distribution <span>=</span> frequency_distribution_from_similar_strings(
</span></span><span><span>    similar_strings_per_block,
</span></span><span><span>    next_token_map10,
</span></span><span><span>)
</span></span><span><span>freq_distribution<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([6, 1, 65])
</code></pre><p>It produces a tensor of shape <code>(6, 1, 65)</code>: 6 blocks, 1 query, 65 tokens in the vocabulary. If we’d been working with more queries, the middle dimension would be larger.</p><p>So now we have a frequency distribution for each block, based on the strings found for each block using the similarity thresholds. We now need to turn this into a probability distribution.</p><p>Earlier, when we just had a single frequency distribution for a single block, we just normalized it. But now we have multiple frequency distributions - one for each block - and need to combine them. In my experiments, I found that a weighted sum of these distributions produced the best results.</p><p>As with the similarity thresholds, I was able to find a set of good weights by trial and error. I also tried a deep-learning approach to find weights, but did not get better results than with the hand-tuned approach. The procedure for both hand-tuning and learning weights is implemented in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/40_widening_similar_space.ipynb" target="_blank" rel="noopener">the similar space notebook</a>, the same one used for tuning thresholds.</p><p>For now, let’s use the optimal weights I found:</p><div><pre tabindex="0"><code data-lang="python"><span><span>weights <span>=</span> torch<span>.</span>tensor([<span>0.01</span>, <span>0.01</span>, <span>0.1</span>, <span>1.5</span>, <span>6</span>, <span>0.01</span>])<span>.</span>unsqueeze(dim<span>=</span><span>1</span>)<span>.</span>unsqueeze(dim<span>=</span><span>2</span>) <span># (n_layer, 1, 1)</span>
</span></span><span><span>total_freq_distribution <span>=</span> (freq_distribution <span>*</span> weights)<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum(dim<span>=-</span><span>1</span>, keepdim<span>=</span><span>True</span>)
</span></span></code></pre></div><p>We multiply the frequency distributions by the weights, sum across all blocks, and then normalize into a probability distribution. We can now look at how the approximation’s distribution compares to the model’s.</p><blockquote><p>Note: in the code below, we have to index into the <code>prob_distribution</code> tensor with <code>[0]</code> because its first dimension is the number of queries. We’re only working with a single query, so we can just take the first element.</p></blockquote><div><pre tabindex="0"><code data-lang="python"><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution[<span>0</span>], tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            i: 0.363
o: 0.204            o: 0.265
a: 0.195            a: 0.213
e: 0.160            e: 0.147
u: 0.004            u: 0.011
l: 0.000            y: 0.000
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution[<span>0</span>], logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.0731)
</code></pre><p>In this particular case, adding the other layers didn’t change the approximation much (if anything, it’s very slightly worse based on Hellinger distance). But let’s look at the example that didn’t work well when we considered just the first layer: prompt id 40 (<code>'hing tremb'</code>).</p><div><pre tabindex="0"><code data-lang="python"><span><span>similarity_thresholds<span>=</span>[<span>0.95</span>, <span>0.94</span>, <span>0.85</span>, <span>0.76</span>, <span>0.81</span>, <span>0.89</span>]
</span></span><span><span>q_idx <span>=</span> <span>40</span>
</span></span><span><span>
</span></span><span><span>similar_strings_per_block <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>    similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>        load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>        q_idx_start<span>=</span>q_idx,
</span></span><span><span>        q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>        filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_thresholds[block_idx]
</span></span><span><span>    )
</span></span><span><span>    similar_strings <span>=</span> [
</span></span><span><span>        [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>        <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>    ]
</span></span><span><span>    similar_strings_per_block<span>.</span>append(similar_strings)
</span></span><span><span>
</span></span><span><span>freq_distribution <span>=</span> frequency_distribution_from_similar_strings(
</span></span><span><span>    similar_strings_per_block,
</span></span><span><span>    next_token_map10,
</span></span><span><span>)
</span></span><span><span>weights <span>=</span> torch<span>.</span>tensor([<span>0.01</span>, <span>0.01</span>, <span>0.1</span>, <span>1.5</span>, <span>6</span>, <span>0.01</span>])<span>.</span>unsqueeze(dim<span>=</span><span>1</span>)<span>.</span>unsqueeze(dim<span>=</span><span>2</span>) <span># (n_layer, 1, 1)</span>
</span></span><span><span>total_freq_distribution <span>=</span> (freq_distribution <span>*</span> weights)<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum(dim<span>=-</span><span>1</span>, keepdim<span>=</span><span>True</span>)
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution[<span>0</span>], tokenizer<span>.</span>itos)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
l: 0.999            l: 0.997
e: 0.000            e: 0.002
r: 0.000            r: 0.000
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor([0.0233])
</code></pre><p>Remember that for this example, when we used just the first layer’s similar strings, the approximation was quite different from the model’s prediction and had a Hellinger distance of &gt;0.63. Now it’s nearly identical and has a Hellinger distance of 0.02. So using the rest of the layers really helped this example.</p><p>In the next section, we’ll extend the code to evaluate the approximation over the whole set of 20,000 prompts. The section after that will look at how well the approximation does across all the prompts.</p><h3 id="extending-to-all-20000-prompts">Extending to All 20,000 Prompts <a href="#extending-to-all-20000-prompts">🔗</a></h3><p>We now have all the pieces we need to run the approximation procedure for all 20,000 prompts. First, let’s find the strings with similar feed-forward network outputs for all the prompts, for all blocks:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Takes about 7 minutes to run</span>
</span></span><span><span>
</span></span><span><span>similarity_thresholds<span>=</span>[<span>0.95</span>, <span>0.94</span>, <span>0.85</span>, <span>0.76</span>, <span>0.81</span>, <span>0.89</span>]
</span></span><span><span>
</span></span><span><span>similar_strings_per_block <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>    similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>        load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>        q_idx_start<span>=</span><span>0</span>,
</span></span><span><span>        q_idx_end<span>=</span>n_prompts,
</span></span><span><span>        filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_thresholds[block_idx]
</span></span><span><span>    )
</span></span><span><span>    similar_strings <span>=</span> [
</span></span><span><span>        [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>        <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>    ]
</span></span><span><span>    similar_strings_per_block<span>.</span>append(similar_strings)
</span></span></code></pre></div><p>Next, we compute the frequency distributions for each query based on the strings we found, perform the weighted sum, and normalize to produce a probability distribution.</p><div><pre tabindex="0"><code data-lang="python"><span><span>freq_distribution <span>=</span> frequency_distribution_from_similar_strings(
</span></span><span><span>    similar_strings_per_block,
</span></span><span><span>    next_token_map10,
</span></span><span><span>)
</span></span><span><span>weights <span>=</span> torch<span>.</span>tensor([<span>0.01</span>, <span>0.01</span>, <span>0.1</span>, <span>1.5</span>, <span>6</span>, <span>0.01</span>])<span>.</span>unsqueeze(dim<span>=</span><span>1</span>)<span>.</span>unsqueeze(dim<span>=</span><span>2</span>) <span># (n_layer, 1, 1)</span>
</span></span><span><span>total_freq_distribution <span>=</span> (freq_distribution <span>*</span> weights)<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum(dim<span>=-</span><span>1</span>, keepdim<span>=</span><span>True</span>)
</span></span><span><span>prob_distribution<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([20000, 65])
</code></pre><p>The output is a tensor of shape (20000, 65): one 65-entry distribution for each of 20,000 prompts.</p><p>In order to compare, we need to run all the prompts through the model and get the output probability distributions the model predicts:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts)
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_probs <span>=</span> logits<span>.</span>probs()
</span></span><span><span>model_probs <span>=</span> model_probs[:, <span>-</span><span>1</span>, :] <span># We're only interested in the last token</span>
</span></span></code></pre></div><p>Now we have outputs from the approximation and from the model for all prompts. In the next section, we’ll measure the Hellinger distance between them and evaluate the results.</p><h2 id="evaluating-the-approximation">Evaluating the Approximation <a href="#evaluating-the-approximation">🔗</a></h2><p>In earlier sections, we compared output from the approximation to output from the model for individual prompts. Now that we have both outputs for all prompts, we can compare them and look at aggregate results.</p><p>First, we can compute the Hellinger distance between the approximation and the model’s prediction for each prompt:</p><div><pre tabindex="0"><code data-lang="python"><span><span>h <span>=</span> hellinger_distance(prob_distribution, model_probs)
</span></span><span><span>h<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([20000])
</code></pre><p>This produced 20,000 Hellinger distance scores, one for each prompt. We can start by looking at some basic stats:</p><div><pre tabindex="0"><code data-lang="python"><span><span>h<span>.</span>mean(), h<span>.</span>std(), h<span>.</span>min(), h<span>.</span>max()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(0.1677), tensor(0.1215), tensor(0.0013), tensor(0.9994))
</code></pre><p>The average Hellinger distance is just below 0.17, with a standard deviation of around 0.12, suggesting a distribution that skews low (a good thing). We’ve also got at least one really excellent sample (a min of 0.0013) and at least one really terrible one (max of 0.9994).</p><p>Let’s look at the distribution:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/e9c543696d0748c74bccfac0780e9e6a5cd7610dafc6e650fb5dab2192fc8399.png" alt=""></p><p>Indeed, the distribution is skewed left, indicating most queries have Hellinger distance scores on the lower end.</p><p>The numbers and the distribution graph look promising, but is the approximation really a good one? It’s hard to say without something to compare against and it’s not obvious what a good comparison might be.</p><p>A thought experiment: let’s imagine that for some prompt, the model produced a distribution that looked like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/d01e3755f6278cc6f19ae5656ab3ba6fd7b4ecb59c69303db814b6cc43fb0435.png" alt=""></p><p>The tokens <code>b</code> and <code>d</code> have nearly the same predicted probability (0.49 vs 0.51). The model predicts an approximately equal chance of these tokens coming next. Now imagine our approximation, or another model, predicted this distribution:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/02bddd27aedd082ab84a2b5dd45dacae4e745dc49ed363df5725916e4b370844.png" alt=""></p><p>Nearly the same, but the probabilities are reversed: <code>b</code> has probability 0.51 and <code>d</code> has 0.49. Would we care about this difference? Clearly both distributions are saying that <code>b</code> and <code>d</code> are about equally likely. If used for inference, either distribution would probably produce acceptable results. For most use cases I could imagine, the difference would just be noise.</p><p>The Hellinger distance between the two imagined distributions above is 0.0141. Not zero, but we’re saying it doesn’t matter for practical purposes. If 0.0141 is a Hellinger distance that doesn’t matter much, what about 0.02? Or 0.025? We can imagine there is some threshold Hellinger distance below which we wouldn’t care and above which we would consider distributions to be meaningfully different. What is that threshold value?</p><p>If we knew it, then we could look at how close the average Hellinger distance between our approximation’s predictions and model’s come to this threshold. That would be a measure of the goodness of the approximation.</p><p>I did an experiment to estimate what the threshold is. I trained the same transformer architecture three more times, starting with a different random seed each time and stopping at approximately the same training and validation loss as I did for the original model. This gave me three alternative transformers with roughly the same performance, but with different weights due to the different random initial starting points:</p><table><thead><tr><th>Model</th><th>Seed</th><th>Est. Training Loss</th><th>Est. Validation Loss</th></tr></thead><tbody><tr><td>Original Model</td><td>1337</td><td>0.9334</td><td>1.5063</td></tr><tr><td>Alternate 1</td><td>1442</td><td>0.9293</td><td>1.5038</td></tr><tr><td>Alternate 2</td><td>88</td><td>0.9294</td><td>1.4991</td></tr><tr><td>Alternate 3</td><td>99999</td><td>0.9339</td><td>1.4941</td></tr></tbody></table><blockquote><p>I used the same training/validation sets, hyperparameters, optimizer, etc. for the three alternate models as for the original model. The training code and output for the alternate models is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/alternate-models.ipynb" target="_blank" rel="noopener">the <code>alternate-models</code> experiment notebook</a>. Training code for the original model is at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb" target="_blank" rel="noopener">the main transformer notebook</a>.</p></blockquote><p>I then ran the same 20,000 prompts through the alternative models and calculated the Hellinger distance between their outputs and that of the original model. <a href="#ii-evaluation-of-main-model-vs-3-alternate-models">Appendix II</a> shows the code used to do this. The table below shows the aggregate results.</p><table><thead><tr><th>Comparison</th><th>Mean Hellinger Distance</th></tr></thead><tbody><tr><td>Original vs Alternate 1</td><td>0.1064 ± 0.0823</td></tr><tr><td>Original vs Alternate 2</td><td>0.1057 ± 0.0817</td></tr><tr><td>Original vs Alternate 3</td><td>0.1053 ± 0.0828</td></tr></tbody></table><p>The original model and the three alternate models are “equivalent” in the sense that they perform about equally well in terms of training and validation loss. I could have used any of them as the basis for this post. In other words, the differences between them likely aren’t meaningful - just noise.</p><p>Across all three alternate models, the average Hellinger distance was ~0.11 ± 0.08. We only have 3 data points, so it’s not a perfect measure, but ~0.11 is probably a reasonable lower bound for the threshold Hellinger distance we are looking for.</p><p>For comparison, the average Hellinger distance between the model and the approximation was ~0.17. A little higher than 0.11, but within a standard deviation.</p><p>Plotting the distributions of the various Hellinger distances shows this nicely:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/d82fe84ec51461c861f5fbc1c2c273935d4d8ac8ceceaeeb292d79cd5fb9ee19.png" alt=""></p><p>There is clearly less deviation between the alternates and the original model than between the approximation and the original model, but it’s not wildly different. I think this result suggests the approximation is quite good. Most of the difference is within the “acceptable noise” threshold.</p><h2 id="interpretation-why-does-the-approximation-work">Interpretation: Why Does the Approximation Work? <a href="#interpretation-why-does-the-approximation-work">🔗</a></h2><p>The analysis in the previous section shows that the outputs of the approximation are quite similar to the transformer’s outputs. But that doesn’t necessarily mean that the approximation procedure is similar to what the transformer is actually doing. The approximation and the transformer might just represent two different ways of computing the same result.</p><p>My intuition is that this is not the case: <strong>I think the approximation is at least something like what the transformer is doing</strong>. In this section, I’ll break down <em>how</em> I think the transformer computes something similar to the approximation and then present some supporting evidence.</p><p>The key ideas are:</p><ul><li>The transformer, as its name suggests*, performs a series of transformations on its embedded input. The transformer blocks transform embeddings within embedding space and the final linear layer at the end transforms from embedding space to logit space.</li><li>Within each transformer block, the transformation from input to output embedding is done via vector addition: the block’s output embedding is its input embedding plus the output of the self-attention layer, plus the output of the feed-forward network. Of the two added components, the feed-forward network output value is dominant in determining the final output.</li><li>Within embedding space, subspaces exist that correspond to specific tokens. An embedding within the subspace for a particular token produces an output distribution in which all the probability is concentrated on that token (that token has probability near 1 and all other tokens have probability near 0). Embeddings that lie between the subspaces for multiple tokens result in outputs that distribute all the probability across those tokens.</li><li>The feed-forward network output at each block is an “adjustment vector” that orients the block output towards the subspaces for the tokens that the approximation procedure would predict: those that follow the strings in the training corpus that produce similar feed-forward network outputs at that block.</li></ul><p>In the subsections below, I’ll go into each of these ideas in more detail.</p><blockquote><p>*It’s unclear whether the name “transformer” alludes to transforming an input sequence to an output sequence (the use case in the original paper was machine translation) or the transformations within the layers of the model.</p></blockquote><h3 id="the-model-is-a-series-of-transformations">The Model is a Series of Transformations <a href="#the-model-is-a-series-of-transformations">🔗</a></h3><p>Once the input to the model has been embedded, we can view the model as a series of transformations:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/c0b6c591002c7e1f931a0bcc794b88454aab02158d1125bef2f8422dbc0e8264.png" alt=""></p><p>The sequence of 6 transformer blocks takes a tensor in embedding space (\(\mathbb{R}^{384}\), since <code>n_embed=384</code>) as input and outputs another tensor in embedding space. In this sense, represents a transformation <em>within</em> embedding space. In fact, each transformer block is itself a transformation within embedding space and the stack of all 6 blocks composes these individual transformations. It isn’t literally implemented this way in code, but its equivalent to:</p><div><pre tabindex="0"><code data-lang="python"><span><span>output_embedding <span>=</span> block6(block5(block4(block3(block2(block1(input_embedding))))))
</span></span></code></pre></div><p>At the end of the sequence of blocks, the model sends the output embedding through a LayerNorm operation and then a linear layer that transforms from embedding space into logit space (\(\mathbb{R}^{65}\), since <code>vocab_size=65</code>). Finally, the softmax layer at the end turns the logits into probabilities for the next token.</p><h3 id="transformation-via-vector-addition">Transformation via Vector Addition <a href="#transformation-via-vector-addition">🔗</a></h3><p>We looked at the internal logic within a transformer block in the earlier <a href="#transformer-block-structure">Transformer Block Structure</a> section. To recap, the <code>forward()</code> method of the <code>Block</code> module looks like this:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Block</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>"""One transformer block"""</span>
</span></span><span><span>
</span></span><span><span>    <span>...</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>sa(self<span>.</span>ln1(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>ffwd(self<span>.</span>ln2(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> x
</span></span></code></pre></div><p><a id="block-logic-with-intermediates"></a>
This is equivalent to the following code, which, by using some intermediate local variables, clarifies what’s really going on:</p><div><pre tabindex="0"><code data-lang="python"><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        sa_out <span>=</span> self<span>.</span>sa(self<span>.</span>ln1(x))
</span></span><span><span>        ffwd_out <span>=</span> self<span>.</span>ffwd(self<span>.</span>ln2(x <span>+</span> sa_out))
</span></span><span><span>
</span></span><span><span>        <span>return</span> x <span>+</span> sa_out <span>+</span> ffwd_out
</span></span></code></pre></div><p><strong>The output of the block is equal to the input (<code>x</code>), plus the self-attention output (<code>sa_out</code>), plus the feed forward network output (<code>ffwd_out</code>).</strong> We can think of the block as taking the input embedding, and then making two adjustments to it.</p><p>These values being added together are vectors in \(\mathbb{R}^{384}\). If we imagine the embedding space reduced to just two dimensions, it might look something like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/12e04e2fdd477aaa55660b4020a4ae6b9a72c55038f98009049330cfecfc4291.png" alt=""></p><p>The red vector represents the input embedding. The green vector represents the self-attention output (<code>sa_out</code> in code), and the blue vector represents the feed-forward network output (<code>ffwd_out</code> in code). The gray arrow represent the final sum, or the output of the first block: where you end up when you arrange the individual vectors tip to tail.</p><p>The plot above shows the additions that happen within just one block. Subsequent blocks add their self-attention outputs and feed-forward network outputs, starting from the output of this block. If we add the vectors from those other blocks to the diagram, it looks like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/24b90fb31e5e330043b12b6a3b0bf9e8bed15bbd4cd57e98f0cbdc4b393fc71f.png" alt=""></p><p>Again, the red arrow represents the input vector, each green arrow represents one block’s self-attention output, each blue arrow represents one block’s feed-forward network output. Arranged tip to tail, their endpoint represents the final output from the stack of 6 blocks, depicted by the gray arrow.</p><p>Though it’s only in two dimensions, the diagram above is based on real data and is drawn “to scale”, in a way: the length of each 2D vector is the same as the \(\mathbb{R}^{384}\) vector it represents for a real query (index 57). In addition, the cosine similarity between each 2D blue / green arrow and the sum of the arrows that precede it is the same as the cosine similarity between the corresponding self-attention/feed-forward network output and the block input in the real data.</p><blockquote><p>Code to generate the 2D representation from real data is in the <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a>.</p></blockquote><p>We can observe two interesting patterns:</p><ul><li>The feed-forward network outputs are generally longer than the self-attention outputs (the vectors have larger norms)</li><li>Within a given block, the feed-forward network output and the self-attention output point in roughly the same direction.</li></ul><p>Look at what happens when we eliminate the self-attention outputs from the vector sum, leaving just the feed-forward network outputs:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/7fff50753ede8a54541e69eaf00215ea285f523817e8361d1fe08ac5e0c6cd8a.png" alt=""></p><p>The inner blue curve in the above plot represents the sum of the input vector and only the feed-forward network outputs from each block. The tip to tail arrangement of these vectors ends at a point far from where the previous arrangement (including the self-attention outputs) ended. But notice that the feed-forward-only endpoint (shorter gray arrow) is quite closely aligned in <em>direction</em> with the original endpoint (longer gray arrow).</p><p>This plot shows values for only one query and we lose a lot of information dropping from 384 dimensions to 2. But the pattern does seem to hold in general and in the full, high-dimensional embedding space. The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a> provides a deep-dive into this phenomenon across all 20,000 queries.</p><p>The takeaway is that <strong>simplifying the transformation performed by the blocks to just the contributions of the feed-forward networks results in an output vector that is shorter (has a smaller norm) than the original output but points in roughly the same direction</strong>. And the difference in norms would have no impact on the transformer’s final output, because of the LayerNorm operation after the stack of blocks. That LayerNorm step will adjust the norm of any input vector to similar value regardless of its initial magnitude; the final linear layer that follows it will always see inputs of approximately the same norm (see <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">Appendix III</a> for a walk-through of this).</p><p>An important clarification: I’m not suggesting that we could remove the self attention computation from the transformer. The feed-forward networks take the self-attention output as part of their input (<code>ffwd_out = self.ffwd(self.ln2(x + </code><strong><code>sa_out</code></strong><code>))</code>); they would compute very different values were the self-attention outputs removed. What I am saying is that, after all block processing has been completed as normal including the self-attention computations, we get roughly the same result if we consider only the feed-forward network contributions, as our approximation does. This is probably because the feed-forward network outputs pass on some of the information they receive as input from the self-attention output.</p><p>For some additional evidence that an approximation based only on feed-forward network outputs can produce similar outputs to the transformer, see <a href="#iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">Appendix IV</a>.</p><h3 id="token-subspaces">Token Subspaces <a href="#token-subspaces">🔗</a></h3><p>In the examples we’ve seen so far, the model outputs have been distributions that include significant non-zero probabilities for several tokens. For example:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/a90c56ba1e72733d29a614671ad61789b5196562087c1464273da469843cb54d.png" alt=""></p><p>Though we haven’t seen one yet, we might wonder whether <strong>specific inputs</strong> exist that compel the model to predict a <strong>single token</strong> with <strong>near certainty</strong>. In other words, do some inputs cause the model to output a probability distribution in which just one token has probability very near 1 and all other tokens very near zero? Such a distribution might look like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/ecc30428196c016ef2970ee406b6ea46a79a2a24c1521d8843c2dc90f83ffc83.png" alt=""></p><p>In fact, we can ask this question about any stage of the model. “Input” doesn’t have to refer to the initial input to the model, but could be the input to any layer within the model. For example, consider only the layers that transform the final block’s output embedding to logit space (the final LayerNorm and linear layers):</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/4c4339d5f6612480cf52b0d34f2c1732c99485033c1196076543f86ec0925af5.png" alt=""></p><p>Is there some embedding block 6 might emit that would yield an output probability distribution in which some token, say the letter <code>a</code>, has probability very near 1?</p><h4 id="learning-token-subspaces">Learning Token Subspaces <a href="#learning-token-subspaces">🔗</a></h4><p>With the right math, it may be possible to find this embedding analytically. But it’s also possible to “learn” (in the sense of deep learning) such an embedding. Here’s the basic idea:</p><ul><li>Pick a point in the transformer where the input to subsequent layers is an embedding. This could be the input to any of the transformer blocks, or the point right after the final block (as shown in the diagram above).</li><li>Pick a token to learn an embedding for.</li><li>Create an embedding tensor and initialize it with random values. This tensor is the parameter the learning algorithm will optimize; the weights of the transformer are fixed.</li><li>Execute a forward pass by evaluating the transformer from the selected point, using the embedding as input. This will produce some set of logits.</li><li>Compute <a href="https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/#nll" target="_blank" rel="noopener">negative log likelihood loss</a> relative to the token we’re learning an embedding for.</li><li>Do a backward pass, updating the embedding tensor according to the gradients.</li></ul><p>My implementation of this is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/learn-embeddings.ipynb" target="_blank" rel="noopener">the learned embeddings notebook</a>. I used it to learn embeddings for all tokens at various stages of the model and saved them. We can load one - a learned embedding that produces a distribution giving token <code>a</code> probability almost 1 - and check that it does what we expect when given to the part of the model shown in the diagram above:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/0dce2cb6b27518fe0a1a26685995c9050cf791b48cc4e407a080182039905dca.png" alt=""></p><p>As expected, all the probability mass is concentrated on <code>a</code>. Inference using this distribution would generate <code>a</code> with near certainty.</p><p>The same procedure can learn embeddings for use at other parts of the model. If we wanted to find an embedding for <code>a</code> that could be input to block 6, we could run the same learning algorithm but use this part of the transformer in the forward pass:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/37991d03eeb45809255009149e32de771715221bb9d315efb6e2aa78b26a897c.png" alt=""></p><p>It’s more computationally expensive to learn embeddings at earlier stages of the model because the optimizer has to contend with a larger computation graph involving operations from all included blocks. Thankfully, as I’ll explain <a href="#use-only-final-subspaces">shortly</a>, we need only the embeddings learned for the part of the transformer after all the blocks (embeddings that go straight into the final LayerNorm layer) to show how the transformer operates like the approximation.</p><h4 id="from-embeddings-to-subspaces">From Embeddings to Subspaces <a href="#from-embeddings-to-subspaces">🔗</a></h4><p>For any token, the procedure described in the previous section can learn an embedding that makes the model predict that token with probability near 1. It turns out <strong>there isn’t just one such embedding for each token.</strong> We can learn many different embeddings that all produce probability distributions that assign a given token nearly all the probability mass. It was easy to learn thousands of unique embeddings for every token in the vocabulary.</p><p>I think <strong>the model has learned a complex, non-linear embedding subspace corresponding to each token</strong>. Any embedding within that subspace results in an output distribution that assigns the token near certain probability. Each embedding I was able to learn is probably a point in the embedding subspace for the corresponding token.</p><p>If we imagine the full embedding space (\(\mathbb{R}^{384}\)) reduced to \(\mathbb{R}^3\) (and the complex subspaces reduced to 2D planes), it might look something like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/04a243b63386cc0e853d350cb0177eeb33f2c15d2aacdaf59ddb4dc38f48b444.png" alt=""></p><p>I don’t know how to determine the exact subspaces for each token mathematically. But I do know how to get a workable approximation of them <strong>if we’re willing to pretend that they are linear</strong>. They are almost certainly not linear, even at the end of the model, because of the non-linear LayerNorm operation. But they are likely <em>closer</em> to linear near the end of the model because the LayerNorm is the only non-linearity. Earlier in the model, each feed-forward network introduces an additional non-linearity via its ReLU operation.</p><blockquote><p><a href="https://www.lesswrong.com/posts/jfG6vdJZCwTQmG7kb/re-examining-layernorm" target="_blank" rel="noopener">This post on LessWrong</a> illustrates of the non-linearity of LayerNorm clearly.</p></blockquote><p>Pretending the subspaces are linear actually works quite well for the part of the model after the transformer blocks. And that is the only part of the model we need to consider for this analysis (as I’ll explain <a href="#use-only-final-subspaces">soon</a>).</p><h4 id="linear-approximations-for-subspaces">Linear Approximations for Subspaces <a href="#linear-approximations-for-subspaces">🔗</a></h4><p>The idea is quite simple: for a given token, we can learn a whole lot of different embeddings, treating each one as a data point. Then we can determine the best fitting line, plane, or other low-dimensional linear subspace that fits the data.</p><p>Again, if we imagine our embedding space reduced to just 3 dimensions, it might look something like the following diagram. The blue dots each represent a learned embedding and the red arrow is the line that minimizes projected distance from each point.</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/a32cb4b311513c8c1bbb0af5cae1d68b1e96efeddfbde076f8e9fca02772d605.png" alt=""></p><p>We can use Singular Value Decomposition (SVD) to find the best fitting linear subspace for the learned embeddings.</p><blockquote><p>To learn more about singular value decomposition in this context, I recommend reading Jeremy Kun’s excellent two-part post. <a href="https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/" target="_blank" rel="noopener">Part 1</a> <a href="https://jeremykun.com/2016/05/16/singular-value-decomposition-part-2-theorem-proof-algorithm/" target="_blank" rel="noopener">Part 2</a>.</p></blockquote><p><a href="#v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">Appendix V</a> walks through the code that uses SVD to find a linear approximation for the subspace corresponding to one token. I did this for all tokens, using the embeddings I learned for the final stages of the transformer. In every case, I was able to find a single vector (1-D subspace) that approximates the token subspace quite well.</p><blockquote><p>For completeness, I also tried this at earlier stages of the transformer and found, as expected, that the linear approximations, even at higher dimensions, didn’t fit the data as well. The relevant experiments are in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">the approximation details notebook</a></p></blockquote><h4 id="mixing-subspace-approximations">Mixing Subspace Approximations <a href="#mixing-subspace-approximations">🔗</a></h4><p>By learning a large number of embeddings for each token and then using SVD on them, we can find one vector for each token that approximates its subspace. Given one of these vectors, any embedding that falls on its span will produce an output distribution that concentrates all the probability mass on the corresponding token. But many of the real transformer outputs we’ve seen distribute the probability mass across several tokens. How do we get from subspaces for individual tokens to embeddings that produce these more diverse distributions?</p><p>We can create embeddings that produce probability distributions where several tokens have substantial probability via linear combinations of the subspace approximation vectors for those tokens. This is the distribution we get when we create an embedding by simply adding the approximation vectors for the subspaces for <code>a</code> and <code>b</code>:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/211fa3aed3f5cbbe1a5adf41e014e27068c39ffaf21aff36c833a851b500e211.png" alt=""></p><p>The sum of subspace approximations vectors for two tokens is an embedding somewhere in between the two subspaces, which results in a final distribution that is the combination of the two tokens.</p><p>Sadly, adding the approximation vectors for <code>a</code> and <code>b</code>, without weighting either one, results in not quite a 50-50 distribution across the two tokens (as shown above). I think there are three reasons for this:</p><ol><li>The approximation vectors are just approximations and not perfect representations of their subspaces.</li><li>The subspace approximation vectors are not perfectly orthogonal. To the extent that <code>a</code>’s vector has a small component that points in the direction of <code>b</code>, the sum results in an overweighting of <code>b</code>.</li><li>The final linear layer of the model produces logits of different magnitudes for different tokens. For example, given the approximation for <code>a</code>, the logit for <code>a</code> is ~18.2. The logit for <code>b</code> from its approximation is ~19.5.</li></ol><p>Together, these errors accumulate and the softmax function at the very end exaggerates even small differences. For more analysis on the reasoning behind the differences and how they might be compensated for, see <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/60_combining_token_subspaces.ipynb" target="_blank" rel="noopener">the combining token subspaces notebook</a>.</p><p>These imperfections aside, I think we can conclude that <strong>it’s possible to derive an embedding that produces a distribution for multiple tokens via a linear combination of the approximation vectors for those tokens’ subspaces</strong>.</p><h3 id="putting-it-all-together">Putting it All Together <a href="#putting-it-all-together">🔗</a></h3><p>To summarize where we are, the preceding sections have shown:</p><ul><li>The transformer blocks perform a series of transformations in embedding space.</li><li>Those transformations can be thought of as moving from one point in embedding space to another by adding the feed-forward network output vector to the input embedding.</li><li>Embedding space contains subspaces corresponding to predicting particular tokens and embeddings between subspaces for multiple tokens result in predictions including all those tokens.</li></ul><p>This section adds the final piece, which is the correspondence between what the transformer is doing and what the approximation is doing:</p><ul><li>Within a block, adding the feed-forward network output vector to the input produces an output embedding that better aligns with the embedding subspaces of specific tokens. And <strong>those tokens are the same ones predicted in the approximation</strong>: they’re the tokens that follow the strings in the training corpus that yield similar feed-forward network outputs to the current prompt.</li></ul><p>Let’s look at an example that shows this. The following is the output distribution predicted by the approximation for the prompt, <code>med me Aut</code> (query index 33), using only the feed-forward network outputs from the final block:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/0a46316fd1ec97000bed4b44242e7aad0809b16acd65a98507ff1b987e313291.png" alt=""></p><p>Based on the strings in the training corpus with similar feed-forward network outputs at the final block, the approximation predicts <code>o</code> is the most likely next token and <code>h</code> is next.</p><p>Next, we need to look at the feed-forward network output for the prompt in this block and determine which token subspaces it’s most oriented towards. I’m going to show a little code here, because I think it’s the best way to explain what’s going on. Readers who aren’t interested in the implementation can focus only on the output.</p><p>First we need to actually grab the feed-forward outputs (we haven’t needed them so far because we’ve been working with precomputed/prefiltered similarity data). We’ll use some <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer-helpers.ipynb" target="_blank" rel="noopener">helper functions</a> that provide easy access to the transformer’s intermediate representations:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Tokenize the strings</span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts)
</span></span><span><span>
</span></span><span><span><span># Embed the tokens</span>
</span></span><span><span>embeddings <span>=</span> accessors<span>.</span>embed_tokens(tokens)
</span></span><span><span>
</span></span><span><span><span># Instantiate TransformerAccessors</span>
</span></span><span><span>accessors <span>=</span> TransformerAccessors(m, device)
</span></span><span><span>
</span></span><span><span><span># Run them through the model with hooks attached that let us look at</span>
</span></span><span><span><span># intermediate values</span>
</span></span><span><span>_, io_accessors <span>=</span> accessors<span>.</span>run_model(embeddings)
</span></span><span><span>
</span></span><span><span><span># Grab the outputs of the ffwd networks at each layer</span>
</span></span><span><span>ffwd_outs <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    io_accessors[block_idx]<span>.</span>output(<span>'ffwd'</span>)[:, <span>-</span><span>1</span>, :]<span>.</span>clone()
</span></span><span><span>    <span>for</span> block_idx <span>in</span> range(n_layer)
</span></span><span><span>])
</span></span><span><span>
</span></span><span><span><span># Free up some memory</span>
</span></span><span><span><span>del</span> io_accessors
</span></span><span><span>_ <span>=</span> gc<span>.</span>collect()
</span></span><span><span>
</span></span><span><span>ffwd_outs<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([6, 20000, 384])
</code></pre><p>To determine which token subspaces the feed-forward network output aligns with, we’ll project it onto the subspace approximation for each token, then determine which projections are most similar to the original vector. To do this, we’ll need to get the projection matrix for the rank 1 approximation to each token subspace:</p><blockquote><p>The code below uses the <code>projection_matrix_for_rank_k_approximation()</code> helper function, defined in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/common/svd-helpers.ipynb" target="_blank" rel="noopener">the SVD helpers notebook</a>.</p></blockquote><blockquote><p>In the case of a rank 1 approximation, the projection isn’t really necessary. We could just take the cosine similarity with the approximation vector, but I wanted to keep this code general because I tried out higher-dimensional approximations in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">other places</a>.</p></blockquote><div><pre tabindex="0"><code data-lang="python"><span><span>filename_for_token <span>=</span> FilenameForToken(tokenizer)
</span></span><span><span>subspace_dims <span>=</span> <span>1</span>
</span></span><span><span>projection_matrices <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    projection_matrix_for_rank_k_approximation(
</span></span><span><span>        original_matrix<span>=</span>torch<span>.</span>load(
</span></span><span><span>            learned_embeddings_dir <span>/</span>  <span>'no_blocks'</span> <span>/</span> <span>f</span><span>"</span><span>{</span>filename_for_token(token)<span>}</span><span>.pt"</span>,
</span></span><span><span>            map_location<span>=</span>device,
</span></span><span><span>        )[:, <span>0</span>, :],
</span></span><span><span>        k<span>=</span>subspace_dims,
</span></span><span><span>    )
</span></span><span><span>    <span>for</span> token <span>in</span> tokenizer<span>.</span>chars
</span></span><span><span>])
</span></span></code></pre></div><p>Now we’ll perform the projections and find the top 5 most similar ones to the original feed-forward output vector:</p><div><pre tabindex="0"><code data-lang="python"><span><span>projections <span>=</span> projection_matrices <span>@</span> ffwd_outs[block_idx, q_idx, :]
</span></span><span><span>values, indices <span>=</span> torch<span>.</span>topk(
</span></span><span><span>    F<span>.</span>cosine_similarity(projections, ffwd_outs[block_idx][q_idx], dim<span>=-</span><span>1</span>),
</span></span><span><span>    k<span>=</span><span>5</span>,
</span></span><span><span>    dim<span>=</span><span>0</span>,
</span></span><span><span>)
</span></span><span><span>tokens <span>=</span> [tokenizer<span>.</span>chars[i<span>.</span>item()] <span>for</span> i <span>in</span> indices]
</span></span><span><span>list(zip(tokens, values<span>.</span>tolist()))
</span></span></code></pre></div><pre tabindex="0"><code>[('o', 0.5074884295463562),
 ('h', 0.40787822008132935),
 ('i', 0.26926180720329285),
 ('u', 0.22823508083820343),
 ('y', 0.20325089991092682)]
</code></pre><p>It turns out that <code>o</code> and <code>h</code> are the most similar, indicating that the feed-forward network output is most oriented towards the subspaces for these tokens. And these are the same tokens that the approximation predicted from the strings with similar feed-forward network outputs (see the distribution above).</p><p>Another example, this time looking at query index 36 (<code>if and thy</code>), but staying in the final block:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/fb2e608eb48c085de00f28831641ff4d49f7f3fd63b198fc3fc08d02a3cc7c45.png" alt=""></p><div><pre tabindex="0"><code data-lang="python"><span><span>projections <span>=</span> projection_matrices <span>@</span> ffwd_outs[block_idx, q_idx, :]
</span></span><span><span>values, indices <span>=</span> torch<span>.</span>topk(
</span></span><span><span>    F<span>.</span>cosine_similarity(projections, ffwd_outs[block_idx][q_idx], dim<span>=-</span><span>1</span>),
</span></span><span><span>    k<span>=</span><span>5</span>,
</span></span><span><span>    dim<span>=</span><span>0</span>,
</span></span><span><span>)
</span></span><span><span>tokens <span>=</span> [tokenizer<span>.</span>chars[i<span>.</span>item()] <span>for</span> i <span>in</span> indices]
</span></span><span><span>list(zip(tokens, values<span>.</span>tolist()))
</span></span></code></pre></div><pre tabindex="0"><code>[(' ', 0.5869003534317017),
 ('s', 0.47689366340637207),
 ('\n', 0.38412901759147644),
 ('$', 0.23048195242881775),
 ('a', 0.21783535182476044)]
</code></pre><p>Here <code></code>(space), <code>s</code>, and <code>\n</code> (newline) were the tokens predicted from what follows the strings with similar feed-forward outputs, and indeed these are the token subspaces most aligned with the prompt’s feed-forward output.</p><h4 id="aggregate-performance">Aggregate Performance <a href="#aggregate-performance">🔗</a></h4><p>In the previous section, I purposely picked examples that exhibit strong correlation between the approximation’s predictions and the most aligned subspaces, to illustrate the point most clearly. Of course, there are other examples for which the correlation is less strong. Rather than looking at specific cases, let’s try to get a sense of how well the correlation holds up across all 20,000 prompts.</p><p>This immediately leads to a question: what is the right measure of aggregate performance? Unfortunately, even if the hypothesis - that the prompt’s feed-forward output aligns with the subspaces for tokens predicted from the strings with similar feed forward outputs - is true, a few practical issues make it difficult to demonstrate objectively:</p><ul><li>We don’t have exact definitions of the token subspaces, just imperfect, linear approximations.</li><li>Magnitudes don’t line up: the tokens with the most probability mass in the approximation’s predictions don’t always correspond to the subspaces with the greatest cosine similarity (because of the imperfect approximations, because the adjustment required may be bigger or smaller for some tokens vs others based on the input embedding’s current alignment, because, as explained in the <a href="#mixing-subspace-approximations">Mixing Subspace Approximations</a> section, the model is more “sensitive” to some tokens than others).</li></ul><p>Given these impediments, we can’t just do something simple like normalizing the cosine similarities and computing Hellinger distance with the predicted probability distribution.</p><p>Instead, we need to devise a criterion on which to judge whether the data from a particular prompt supports the hypothesis or not. Then we can evaluate aggregate performance by how many of the 20,000 prompts satisfy the criterion. I experimented with several different approaches and in the end came up with this candidate criterion:</p><p>High-level description: <em>Do the subspaces for the tokens containing 90% of the probability mass in the approximation’s predictions appear in the top half of all token subspaces when ranked by cosine similarity with the prompt’s feed-forward output vector?</em></p><p>Exact definition:</p><ul><li>Define <code>top_n</code> as the number of tokens required to cover at least 90% of the probability mass in the approximation’s predictions for this prompt.</li><li>Define <code>n_subspaces</code> as <code>tokenizer.vocab_size // 2</code> (32, based on our 65-token vocabulary).</li><li>Determine: Are the subspaces for the first <code>top_n</code> tokens predicted by the approximation in the first <code>n_subspaces</code> subspaces ranked by cosine similarity with the prompt’s feed-forward output vector?</li></ul><p>Admittedly, this is an arbitrary definition and reasonable people could debate any of the specifics. But I do think it gives as an indication of whether the data from a particular example prompt supports the hypothesis, while allowing for some of the measurement challenges noted above.</p><p>I evaluated this criteria at three places: the outputs of blocks 6, 5, and 4, using projection matrices derived from learned embeddings at each of these places.</p><blockquote><p>I didn’t evaluate at earlier blocks because the GPU time required to learn embeddings at those blocks became prohibitive. The further back in the model, the bigger the computation graph that the learning algorithm needs to optimize over.</p></blockquote><p>The table below shows the results:</p><blockquote><p>The code that produced these results appears at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">the approximation details notebook</a></p></blockquote><table><thead><tr><th>Block</th><th># of Prompts Satisfying Criterion</th></tr></thead><tbody><tr><td>6</td><td>16357 (81.78%)</td></tr><tr><td>5</td><td>10142 (50.71%)</td></tr><tr><td>4</td><td>7760 (38.80%)</td></tr></tbody></table><p>These numbers aren’t exactly a ringing endorsement. As expected, they get worse the further back we go, probably due to the increased non-linearity.</p><p><a id="use-only-final-subspaces"></a>
What if we always used the subspace approximations from the very end of the transformer (which are likely to be the most linear), even when comparing against feed-forward network outputs from earlier blocks? The results get better:</p><table><thead><tr><th>Block</th><th># of Prompts Satisfying Criterion</th></tr></thead><tbody><tr><td>6</td><td>16357 (81.78%)</td></tr><tr><td>5</td><td>13652 (68.26%)</td></tr><tr><td>4</td><td>11630 (58.15%)</td></tr><tr><td>3</td><td>11469 (57.34%)</td></tr><tr><td>2</td><td>10404 (52.02%)</td></tr><tr><td>1</td><td>9942 (49.71%)</td></tr></tbody></table><blockquote><p>Like many good findings, this one resulted from a bug. I accidentally ran the analysis using the projection matrices for the final part of the transformer with the feed-forward network outputs from earlier blocks and was surprised when the numbers turned out to be so good.</p></blockquote><p>It’s valid to use the subspace approximations (and corresponding projection matrices) from the end of the transformer at earlier stages. All blocks operate in the same embedding space and each one seems to make a small refinement on the output of its predecessors, rather than wild changes in direction. So if any block’s feed-forward network output adjusts an embedding towards the subspaces for a set of tokens as defined at the end of the transformer, it is likely also adjusting it towards whatever the subspaces would be for those same tokens at the block where it operates.</p><blockquote><p>The <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" target="_blank" rel="noopener">logit lens post</a>, in the section “why? / is this surprising?” provides an explanation that supports this idea. In summary, the residual connections encourage the transformer to learn weights that operate within the same basis across blocks and the use of weight decay in training results in a computation that’s spread out over as many layers as possible, with each layer making only a small, incremental change.</p></blockquote><p>To put these numbers in perspective, I investigated how likely it would be for the criterion I’ve defined here to be satisfied by chance. In other words, if we assume the hypothesis is false and that the cosine similarities between the feed-forward network output and the token subspace approximation vectors are random, rather than expressing meaningful relationships, how likely would it be for the criterion to still be satisfied?</p><p>I ran a simulation of this, taking care to ensure that the distribution of randomly generated cosine similarities matches the real data, among other details. The implementation is at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">the approximation details notebook</a>. The final results are:</p><table><thead><tr><th>Block</th><th>Likely % of Prompts Satisfying Criteria By Chance</th></tr></thead><tbody><tr><td>6</td><td>20.76% ± 0.25%</td></tr><tr><td>5</td><td>20.55% ± 0.26%</td></tr><tr><td>4</td><td>18.37% ± 0.24%</td></tr><tr><td>3</td><td>18.20% ± 0.24%</td></tr><tr><td>2</td><td>17.04% ± 0.23%</td></tr><tr><td>1</td><td>16.31% ± 0.23%</td></tr></tbody></table><p>So the best performance numbers we have are clearly much better than chance. But in fairness, they’re still not a slam dunk.</p><p>Even when we use approximations for the most linear subspaces we have, I think there is still a lot of noise in the measurement, for all the reasons outlined earlier in this section. Personally, I take the numbers to be overall supportive of the hypothesis, at least directionally, though I wish the evidence was more conclusive.</p><h3 id="final-summary-of-correspondence-between-transformer-and-approximation">Final Summary of Correspondence Between Transformer and Approximation <a href="#final-summary-of-correspondence-between-transformer-and-approximation">🔗</a></h3><p>The analyses in this post point towards two ideas. First, that the approximation and the transformer produce similar outputs. Second, that there is a correspondence between the approximation procedure and what the transformer is doing. I think the evidence for the first idea is quite strong. The evidence for the second is less clear cut, but still suggests it’s probably at least partially right.</p><p>To close, I want to provide a high-level summary of what I think that correspondence is, even if I can’t yet demonstrate it more definitively:</p><table><thead><tr><th>Concept</th><th>Transformer</th><th>Approximation</th></tr></thead><tbody><tr><td>Prompts map to classes of strings in the training corpus.</td><td>The transformer learns an embedding scheme, along with weights in its self-attention and feed-forward networks, that cause strings in the training corpus with similar characteristics to produce similar output values. Prompts that share those characteristics also produce similar output values.</td><td>The approximation performs the same mapping as the transformer by examining the feed-forward network outputs from all substrings in the training corpus and identifying the ones similar to the outputs from a given prompt.</td></tr><tr><td>Predictions for the tokens likely to follow a prompt derive from the frequency distribution of tokens that follow strings in the training corpus that produce feed-forward network output values similar to those of the prompt.</td><td>A feed-forward network output is a compressed, latent representation, in the embedding space, of the frequency distribution of the tokens that follow strings in the training corpus that produce similar outputs. The weights in the final linear layer map the latent representation into logit space such that it become the correct probability distribution after applying the softmax operation.</td><td>The approximation reconstructs the same frequency distribution manually, by looking up the strings identified as having similar outputs in the training corpus and counting the tokens that follow them. Normalizing the frequency distribution turns it into a probability distribution.</td></tr><tr><td>Final output is a weighted sum of predictions from each block.</td><td>As shown <a href="#transformation-via-vector-addition">earlier</a>, the transformer output is roughly the vector sum of all feed-forward network outputs and the input embedding. The learned weights in the layers within a block determine the magnitude and direction of the output and thus how much it influences the overall direction of the final sum.</td><td>The approximation performs a weighted sum of the distributions determined for each block. The weights control the degree of influence of any given block and are manually selected to produce results as close to the transformer’s as possible.</td></tr></tbody></table><h3 id="what-about-attention">What About Attention? <a href="#what-about-attention">🔗</a></h3><p>I began this post by observing that most explanations of how transformers work focus on attention but don’t say how attention results turn into the final predictions. I may be guilty of the opposite: I’ve written at length about how the transformers produce their output probabilities and said very little about attention.</p><p>To wrap up the analysis, I’d like to rectify this with a few words about attention. In the mechanism I’ve laid out, whether executed in the form of the approximation or the transformer, a key operation is mapping the prompt to a class of strings from the training corpus at each block. Predictions for the next token follow directly from the distribution of tokens that follow those strings in the training corpus. <strong>Making good predictions depends on mapping the prompt to the right class of training corpus strings. And that is the job of self-attention.</strong></p><p>The self-attention layers learn to identify patterns across the tokens that make up a prompt. Those patterns might be simple, such as a common sequence appearing at the beginning or end of the prompt (for example, as we saw <a href="#demo-my-proposal-in-action">earlier</a>, strings that end in ‘y l’). They can also be more general: instead of matching specific tokens, they might match <em>kinds</em> of tokens, such as vowels or capitals, in specific places. The learned weights in the attention heads determine which patterns they respond to, and thus which strings in the training corpus produce similar values. The output of the self-attention heads, when passed through the feed-forward network, yield representations in embedding space that encode information about the distribution of tokens in the training corpus that follow those strings.</p><p>Because the transformer has multiple blocks and each block has multiple attention heads (6 blocks and 6 heads per block in the one we looked at), it’s possible to evaluate each prompt against a large number of different potential patterns. The richness and diversity of the patterns that the attention heads can identify gives the transformer its predictive power.</p><h2 id="closing-thoughts">Closing Thoughts <a href="#closing-thoughts">🔗</a></h2><p>I started this project because I wanted to understand the transformer architecture. It’s given me a satisfying explanation of what at least one transformer is doing, but has been even more fruitful as an exercise in learning how to learn. This was my first foray into an open-ended ML research project on my own. It taught me how to interrogate the internals of models, how to set up experiments to answer questions, and, perhaps most importantly, how to keep moving the project forward when I felt stuck.</p><p>Language models have always seemed magical to me, from the first time I used ChatGPT. I wondered if finding a reductive explanation for what happens internally would rob them of their magic. In fact, I think the opposite has happened. I’ve come to appreciate the beauty in an elegantly simple mechanism that produces such rich complexity in its outputs.</p><p>I don’t know whether the results I found here have any generality beyond the small transformer I trained or if any of it will be of use to anyone else. Regardless, it’s been a joy to do this work and I’m grateful to have had the things I needed along the way: time, resources, and endless support from my family and mentors.</p><h2 id="appendices">Appendices <a href="#appendices">🔗</a></h2><h3 id="i-model-details">I: Model Details <a href="#i-model-details">🔗</a></h3><p>Some notable specs:</p><ul><li>Vocabulary size: 65 (the unique characters in the TinyShakespeare dataset)</li><li>Embedding size (<code>n_embed</code>): 384</li><li>Number of transformer blocks (<code>n_layer</code>): 6</li><li>Number of attention heads (<code>n_head</code>): 6</li><li>Context window size (<code>block_size</code>): 256</li></ul><p>The feed-forward networks comprise over 65% of the total trainable parameters:</p><div><pre tabindex="0"><code data-lang="python"><span><span>all_trainable_params <span>=</span> [p <span>for</span> p <span>in</span> m<span>.</span>parameters() <span>if</span> p<span>.</span>requires_grad]
</span></span><span><span>n_all_trainable_params <span>=</span> sum([np<span>.</span>prod(p<span>.</span>size()) <span>for</span> p <span>in</span> all_trainable_params])
</span></span><span><span>
</span></span><span><span>ffwd_trainable_params <span>=</span> [
</span></span><span><span>    p
</span></span><span><span>    <span>for</span> block_idx <span>in</span> range(n_layer)
</span></span><span><span>    <span>for</span> p <span>in</span> m<span>.</span>blocks[block_idx]<span>.</span>ffwd<span>.</span>parameters()
</span></span><span><span>    <span>if</span> p<span>.</span>requires_grad
</span></span><span><span>]
</span></span><span><span>n_ffwd_trainable_params <span>=</span> sum([np<span>.</span>prod(p<span>.</span>size()) <span>for</span> p <span>in</span> ffwd_trainable_params])
</span></span><span><span>
</span></span><span><span>print(
</span></span><span><span>    <span>f</span><span>"</span><span>{</span>n_ffwd_trainable_params<span>:</span><span>,</span><span>}</span><span> ffwd params out of </span><span>{</span>n_all_trainable_params<span>:</span><span>,</span><span>}</span><span> total params (</span><span>{</span>n_ffwd_trainable_params <span>/</span> n_all_trainable_params<span>:</span><span>.2%</span><span>}</span><span>)"</span>
</span></span><span><span>)
</span></span></code></pre></div><pre tabindex="0"><code>7,089,408 ffwd params out of 10,788,929 total params (65.71%)
</code></pre><h3 id="ii-evaluation-of-main-model-vs-3-alternate-models">II: Evaluation of Main Model vs 3 Alternate Models <a href="#ii-evaluation-of-main-model-vs-3-alternate-models">🔗</a></h3><p>As described in the <a href="#evaluating-the-approximation">Evaluation section</a>, I trained the same transformer architecture used for the main model in this post three additional times, starting from a different random seed each time. This appendix shows the code used to measure the average Hellinger distance between the output of the main models and each of the three alternates, across the 20,000 sample prompts. The results provide a plausible lower bound for the threshold Hellinger distance that indicates a meaningful change in an output probability distribution.</p><p>First, we instantiate the three alternate models from their saved weights:</p><div><pre tabindex="0"><code data-lang="python"><span><span>alt_models_dir <span>=</span> environment<span>.</span>data_root <span>/</span> <span>'alternate-models/model-training/20240112-training/outputs/'</span>
</span></span><span><span><span>assert</span> alt_models_dir<span>.</span>exists(), <span>"Alternate models directory does not exist. Run the training code in ../experiments/alternate-models.ipynb."</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span># Instantiate the three alternative trained models</span>
</span></span><span><span>m_alt1, _ <span>=</span> create_model_and_tokenizer(
</span></span><span><span>    saved_model_filename<span>=</span>alt_models_dir <span>/</span> <span>'shakespeare-20240112-1.pt'</span>,
</span></span><span><span>    dataset<span>=</span>ts,
</span></span><span><span>    device<span>=</span>device,
</span></span><span><span>)
</span></span><span><span>m_alt2, _ <span>=</span> create_model_and_tokenizer(
</span></span><span><span>    saved_model_filename<span>=</span>alt_models_dir <span>/</span> <span>'shakespeare-20240112-2.pt'</span>,
</span></span><span><span>    dataset<span>=</span>ts,
</span></span><span><span>    device<span>=</span>device,
</span></span><span><span>)
</span></span><span><span>m_alt3, _ <span>=</span> create_model_and_tokenizer(
</span></span><span><span>    saved_model_filename<span>=</span>alt_models_dir <span>/</span> <span>'shakespeare-20240112-3.pt'</span>,
</span></span><span><span>    dataset<span>=</span>ts,
</span></span><span><span>    device<span>=</span>device,
</span></span><span><span>)
</span></span></code></pre></div><p>Next, we feed the input tokens into the original model and the three alternate models and get their output probability distributions:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts)
</span></span><span><span>
</span></span><span><span>model_probs <span>=</span> get_model_probs(m, tokens)
</span></span><span><span>alt_model_probs1 <span>=</span> get_model_probs(m_alt1, tokens)
</span></span><span><span>alt_model_probs2 <span>=</span> get_model_probs(m_alt2, tokens)
</span></span><span><span>alt_model_probs3 <span>=</span> get_model_probs(m_alt3, tokens)
</span></span></code></pre></div><p>Now we can compute the Hellinger distance between the outputs from the three alternative models and the outputs from the original model. Remember that each of the model probabilities tensors (<code>model_probs</code> and <code>alt_model_probs*</code>) is a 20,000x65 tensor i.e. 20,000 probability distributions of 65 elements each.</p><p>We’re computing the Hellinger distance between those probability distributions. So for each alternative model, we end up with 20,000 Hellinger distance values. These values tell us, for each prompt, how the probability distribution for the next token predicted by one of the alternate models differed from the probability distribution predicted by the original model.</p><div><pre tabindex="0"><code data-lang="python"><span><span>h_alt1 <span>=</span> hellinger_distance(model_probs, alt_model_probs1)
</span></span><span><span>h_alt2 <span>=</span> hellinger_distance(model_probs, alt_model_probs2)
</span></span><span><span>h_alt3 <span>=</span> hellinger_distance(model_probs, alt_model_probs3)
</span></span></code></pre></div><p>With the Hellinger distances computed, we can look at aggregate stats:</p><div><pre tabindex="0"><code data-lang="python"><span><span>h_alts <span>=</span> torch<span>.</span>stack([h_alt1, h_alt2, h_alt3], dim<span>=</span><span>1</span>)
</span></span><span><span>h_alts<span>.</span>mean(dim<span>=</span><span>0</span>), h_alts<span>.</span>std(dim<span>=</span><span>0</span>), h_alts<span>.</span>min(dim<span>=</span><span>0</span>)<span>.</span>values, h_alts<span>.</span>max(dim<span>=</span><span>0</span>)<span>.</span>values
</span></span></code></pre></div><pre tabindex="0"><code>(tensor([0.1064, 0.1057, 0.1053]),
 tensor([0.0823, 0.0817, 0.0828]),
 tensor([0.0005, 0.0008, 0.0008]),
 tensor([0.8351, 0.7881, 0.8743]))
</code></pre><p>For all three alternate models, the average Hellinger distance was ~0.11 ± 0.08. All had very small minimums (&lt;= 0.0008) and maximums around ~0.80.</p><h3 id="iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">III: The Output Embedding’s Norm Doesn’t Matter Because of the Final LayerNorm <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">🔗</a></h3><p>This appendix demonstrates the assertion from the <a href="#transformation-via-vector-addition">Transformation via Vector Addition</a> that the norm of the output embeddings from the final transformer block does not matter, because of the LayerNorm before the final linear layer.</p><p>To begin, let’s grab the final block outputs for the first 1000 prompts:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Get the block outputs for the first 1000 prompts</span>
</span></span><span><span>
</span></span><span><span>tokens_sample <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts[:<span>1000</span>])
</span></span><span><span>_, io_accessors_sample <span>=</span> accessors<span>.</span>run_model(accessors<span>.</span>embed_tokens(tokens_sample))
</span></span><span><span>
</span></span><span><span>final_block_outputs <span>=</span> io_accessors_sample[<span>-</span><span>1</span>]<span>.</span>output(<span>'.'</span>)[:, <span>-</span><span>1</span>, :]<span>.</span>clone()
</span></span><span><span>
</span></span><span><span><span>del</span> io_accessors_sample
</span></span><span><span>_ <span>=</span> gc<span>.</span>collect()
</span></span><span><span>
</span></span><span><span>final_block_outputs<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([1000, 384])
</code></pre><p>Next, let’s create a copy of those outputs scaled by a factor of 10:</p><div><pre tabindex="0"><code data-lang="python"><span><span>scaled_final_block_outputs <span>=</span> final_block_outputs <span>*</span> <span>10</span>
</span></span><span><span>scaled_final_block_outputs<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([1000, 384])
</code></pre><p>Comparing average norms, we see that those of the scaled outputs indeed are 10 times bigger:</p><div><pre tabindex="0"><code data-lang="python"><span><span>final_block_outputs<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean(), scaled_final_block_outputs<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(22.8909), tensor(228.9091))
</code></pre><p>Now, let’s put both the original and scaled outputs through the final LayerNorm of the model and calculate the average norm of the results:</p><div><pre tabindex="0"><code data-lang="python"><span><span>layer_normed_original <span>=</span> m<span>.</span>ln_f(final_block_outputs)<span>.</span>detach()
</span></span><span><span>layer_normed_scaled <span>=</span> m<span>.</span>ln_f(scaled_final_block_outputs)<span>.</span>detach()
</span></span><span><span>
</span></span><span><span>layer_normed_original<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean(), layer_normed_scaled<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.1262), tensor(23.1263))
</code></pre><p>They’re virtually identical.</p><p>In the preceding example, the output norms were so close to identical because the two inputs differed only in scale: they had the same direction, or cosine similarity of 1. Vectors that have different norms and different directions will emerge from the LayerNorm with norms that are still quite similar but a little further apart.</p><p>To see an example, we can add a little noise to one of the vectors and then scale it:</p><div><pre tabindex="0"><code data-lang="python"><span><span>original_vector <span>=</span> final_block_outputs[<span>0</span>]
</span></span><span><span>
</span></span><span><span><span># Add some random noise to the original vector</span>
</span></span><span><span>torch<span>.</span>manual_seed(<span>42</span>) <span># keep the noise consistent</span>
</span></span><span><span>comparison_vector <span>=</span> original_vector <span>+</span> torch<span>.</span>randn_like(original_vector) <span>*</span> <span>0.1</span>
</span></span><span><span>
</span></span><span><span><span># And scale it</span>
</span></span><span><span>comparison_vector <span>=</span> comparison_vector <span>/</span> comparison_vector<span>.</span>norm()
</span></span><span><span>comparison_vector <span>*=</span> <span>10</span> <span>*</span> original_vector<span>.</span>norm()
</span></span><span><span>
</span></span><span><span>original_vector<span>.</span>norm(), comparison_vector<span>.</span>norm(), F<span>.</span>cosine_similarity(original_vector, comparison_vector, dim<span>=-</span><span>1</span>)
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.7909), tensor(237.9092), tensor(0.9967))
</code></pre><p>The <code>comparison_vector</code>’s norm is exactly 10x that of <code>original_vector</code>, but they’re not perfectly aligned in direction, though still quite close.</p><div><pre tabindex="0"><code data-lang="python"><span><span>m<span>.</span>ln_f(original_vector)<span>.</span>detach()<span>.</span>norm(), m<span>.</span>ln_f(comparison_vector)<span>.</span>detach()<span>.</span>norm()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.1496), tensor(23.1671))
</code></pre><p>Their norms after layer norm are close but further apart than in the previous example.</p><p>If we add a lot more noise, we’ll end up with two vectors with quite different directions:</p><div><pre tabindex="0"><code data-lang="python"><span><span>original_vector <span>=</span> final_block_outputs[<span>0</span>]
</span></span><span><span>
</span></span><span><span><span># Add some random noise to the original vector</span>
</span></span><span><span>torch<span>.</span>manual_seed(<span>4211</span>) <span># keep the noise consistent</span>
</span></span><span><span>comparison_vector <span>=</span> original_vector <span>+</span> torch<span>.</span>randn_like(original_vector) <span>*</span> <span>2</span>
</span></span><span><span>
</span></span><span><span><span># And scale it</span>
</span></span><span><span>comparison_vector <span>=</span> comparison_vector <span>/</span> comparison_vector<span>.</span>norm()
</span></span><span><span>comparison_vector <span>*=</span> <span>10</span> <span>*</span> original_vector<span>.</span>norm()
</span></span><span><span>
</span></span><span><span>original_vector<span>.</span>norm(), comparison_vector<span>.</span>norm(), F<span>.</span>cosine_similarity(original_vector, comparison_vector, dim<span>=-</span><span>1</span>)
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.7909), tensor(237.9093), tensor(0.5178))
</code></pre><p>But their norms after layer norm are only a little more divergent:</p><div><pre tabindex="0"><code data-lang="python"><span><span>m<span>.</span>ln_f(original_vector)<span>.</span>detach()<span>.</span>norm(), m<span>.</span>ln_f(comparison_vector)<span>.</span>detach()<span>.</span>norm()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.1496), tensor(23.0546))
</code></pre><p>So in summary:</p><ul><li>The LayerNorm will remove substantial differences in input norms.</li><li>Norms of the outputs from the LayerNorm will vary a little depending on how closely aligned the input vectors were.</li></ul><h3 id="iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">IV: Summary of Experiment on Relative Impact of Self-Attention and Feed Forward Network Outputs <a href="#iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">🔗</a></h3><p>The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a> contains the implementation of an experiment to understand the relative impact of the self-attention outputs and the feed-forward network outputs on the final output of the transformer. This appendix summarizes the experiment and the results.</p><p>Experiment procedure:</p><ul><li>I ran all 20,000 prompts through the model and captured the final output probability distributions as well as the intermediate self-attention outputs, feed-forward network outputs, and final block outputs for each block.</li><li>For each block, I then ran two tests. First, instead of sending the block output as normally implemented (<code>x + sa_out + ffwd_out</code>, as shown <a href="#block-logic-with-intermediates">earlier</a>) to the next stage of the model, I sent a version that omits the self-attention output i.e. just <code>x + ffwd_out</code>, and saved the final output probability distribution that resulted. Then, I did the same thing but removed the feed-forward network output instead, sending on just <code>x + sa_out</code>.</li><li>I then calculated the Hellinger distance between the probability distribution produced with the regular block output and that produced by each of the two modifications.</li></ul><p>The table below shows the results, averaged across all 20,000 prompts:</p><blockquote><p>For the implementation of this analysis, see <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a></p></blockquote><table><thead><tr><th>Block</th><th>H(output(<code>x+sa_out+ffwd_out</code>), output(<code>x+ffwd_out</code>))</th><th>H(output(<code>x+sa_out+ffwd_out</code>), output(<code>x+sa_out</code>))</th></tr></thead><tbody><tr><td>1</td><td>0.11 ± 0.07</td><td>0.70 ± 0.17</td></tr><tr><td>2</td><td>0.07 ± 0.04</td><td>0.19 ± 0.11</td></tr><tr><td>3</td><td>0.09 ± 0.07</td><td>0.15 ± 0.10</td></tr><tr><td>4</td><td>0.06 ± 0.05</td><td>0.13 ± 0.10</td></tr><tr><td>5</td><td>0.04 ± 0.03</td><td>0.14 ± 0.10</td></tr><tr><td>6</td><td>0.03 ± 0.03</td><td>0.17 ± 0.10</td></tr></tbody></table><blockquote><p>Remember that Hellinger distance ranges between 0 and 1, with 0 meaning identical and 1 meaning no overlap. A larger Hellinger distance in this table means a larger divergence between the experiment output and the normal transformer output.</p></blockquote><p>The effect is most pronounced in the first block: omitting the feed-forward network output results in an almost completely different probability distribution (H = 0.70) but omitting the self-attention output results in a very similar distribution (H = 0.11). Across the rest of the layers, the difference is less dramatic, but the feed-forward network output always has the larger impact.</p><blockquote><p>Though I think these results support the notion that an approximation based only on feed-forward network outputs can produce similar results to the transformer, it would be interesting to see if the approximation would improve if we include the self-attention outputs, particularly for some of the intermediate layers. But I’m leaving that as an area for future investigation.</p></blockquote><h3 id="v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">V: Performing SVD to Get a Linear Approximation of a Token Subspace <a href="#v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">🔗</a></h3><p>This appendix walks through an example of how we can find a linear approximation for a token subspace using SVD. First, let’s load all the embeddings learned for the token <code>a</code> at the output of the last block of the transformer (input to the final layer norm and linear layer):</p><div><pre tabindex="0"><code data-lang="python"><span><span>learned_embeddings_dir <span>=</span> environment<span>.</span>data_root <span>/</span> <span>'learned_embeddings'</span>
</span></span><span><span>multi_emb_a <span>=</span> torch<span>.</span>load(learned_embeddings_dir <span>/</span> <span>'no_blocks'</span> <span>/</span> <span>'lower_a.pt'</span>, map_location<span>=</span>device)
</span></span><span><span>multi_emb_a<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([100, 1, 384])
</code></pre><p>We’ve got 100 different 384-dimensional embedding vectors. Each one, when given as input to the final blocks in the transformer, produces an output distribution that assigns nearly all the probability mass to the token, <code>a</code>. Each one can be thought of as a point in the subspace for token <code>a</code>.</p><p>We can stack these embeddings form a 100x384 matrix:</p><p>$$
\begin{bmatrix}
e_{1,1} &amp; e_{1,2} &amp; \dots &amp; e_{1,384} \\
e_{2,1} &amp; e_{2,2} &amp; \dots &amp; e_{2,384} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
e_{100,1} &amp; e_{100,2} &amp; \dots &amp; e_{100,384}
\end{bmatrix}
$$</p><p>Next, we can run SVD on this matrix:</p><div><pre tabindex="0"><code data-lang="python"><span><span>_, S, V <span>=</span> torch<span>.</span>linalg<span>.</span>svd(multi_emb_a[:, <span>-</span><span>1</span>, :])
</span></span></code></pre></div><p>For this analysis, we’re only interested in the singular values (<code>S</code>) and the right singular vectors (<code>V</code>). We can plot the singular values:</p><div><pre tabindex="0"><code data-lang="python"><span><span>_ <span>=</span> plt<span>.</span>plot(S<span>.</span>numpy(), <span>'-o'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/ec5755d7859f77e55c3bf34a432c33226741616e9b493b5eec96c716ac1e7fe5.png" alt=""></p><p>The first singular value is much larger (over 6x) than the next, which suggests the first right singular vector alone might be a good approximation for the subspace that predicts token <code>a</code>. We can test what gets predicted when we use this first right singular vector as an embedding:</p><div><pre tabindex="0"><code data-lang="python"><span><span>v0a <span>=</span> adjust_singular_vector_sign(V[<span>0</span>], multi_emb_a[:, <span>-</span><span>1</span>, :])
</span></span><span><span>logits <span>=</span> LogitsWrapper(accessors<span>.</span>logits_from_embedding(unsqueeze_emb(v0a)), tokenizer)
</span></span><span><span>logits<span>.</span>plot_probs(title<span>=</span><span>'Next Token Probability Distribution from First Right Singular Vector of embeddings for Token "a"'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/968e8e424522937d5366586abd902715e1cdf771fddbb2d4a36144cbe07746e2.png" alt=""></p><p>In this distribution, <code>a</code> has probability near 1 and every other token has probability near 0. So the first right singular vector is effectively another embedding that produces an output predicting <code>a</code> with near certainty.</p><p>But it’s different from the other 100 learned embeddings in an important way: it’s the vector that is best aligned with <em>all</em> of them. More formally, it’s the vector that minimizes the squared distance to all 100 other embedding vectors. In this way, the first right singular vector is like a good summary of the embedding vectors we started from.</p><p>The first right singular vector is a unit vector (as are all the singular vectors):</p><pre tabindex="0"><code>tensor(1.0000)
</code></pre><p>Any vector along its span will produce an output distribution predicting <code>a</code>, similar to the one above (see <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">Appendix III</a> for an explanation of why the transformer output is invariant to the scale of the final embedding). So the span of this vector is a good, linear approximation to the subspace for token <code>a</code>.</p><p>The same results we saw here for token <code>a</code> hold for the other tokens too. For each, if we stack all the learned embeddings and perform SVD, we find that the first right singular vector forms a good linear approximation of the token subspace.</p></div></section></div>]]></description>
        </item>
    </channel>
</rss>