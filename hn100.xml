<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 21 Jan 2025 11:30:14 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Startup Winter: Hacker News Lost Its Faith (149 pts)]]></title>
            <link>https://www.vincentschmalbach.com/startup-winter-hacker-news-lost-its-faith/</link>
            <guid>42778266</guid>
            <pubDate>Tue, 21 Jan 2025 10:00:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vincentschmalbach.com/startup-winter-hacker-news-lost-its-faith/">https://www.vincentschmalbach.com/startup-winter-hacker-news-lost-its-faith/</a>, See on <a href="https://news.ycombinator.com/item?id=42778266">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>In 2013, a failing founder posted their story on Hacker News (<a href="https://news.ycombinator.com/item?id=5255209">link</a>). The responses were overwhelmingly supportive: "Failure is just an event, not who you are." "Get back up and try again!" "This is valuable experience for next time."</p>
<p>Fast forward to 2025. Another founder shares their journey of six failed attempts (<a href="https://news.ycombinator.com/item?id=42771676">link</a>). The sentiment in the comments is strikingly different: "Would have been better to work at BigTech." "The rat race isn't worth it." "Most interesting stories remain buried while we're presented with a somewhat skewed reality."</p>
<p>This shift isn't isolated to these two posts. The same forum that championed "fail fast, fail often" now regularly questions whether the startup path makes sense at all.</p>
<p>What's changed?</p>
<ol>
<li>
<p>The human cost has become more visible. Stories of burnout, failed relationships, and mental health struggles are no longer swept under the rug of "hustle culture."</p>
</li>
<li>
<p>Big Tech compensation has transformed the risk-reward equation. When senior engineers can make $300K+ at established companies, the financial argument for startups becomes harder to justify.</p>
</li>
<li>
<p>The VC model's limitations have become apparent. The focus on hypergrowth and exits has left many founders feeling trapped between authentic business building and investor expectations.</p>
</li>
<li>
<p>The industry has matured. The low-hanging fruit of the mobile/web era has largely been picked, making truly innovative opportunities harder to find.</p>
</li>
</ol>
<p>I believe we're entering what might be called a "Startup Winter" - not because startups have stopped being created, but because the mythology around them has frozen over.</p>
<p>What might emerge from this winter could be a startup ecosystem that's less glamorous but more authentic. One where alternative paths to innovation are celebrated alongside the traditional VC-backed route.</p>

<!-- #comments -->

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Censoring '#Democrat' on Instagram (130 pts)]]></title>
            <link>https://mstdn.chrisalemany.ca/@chris/113864600222476627</link>
            <guid>42777938</guid>
            <pubDate>Tue, 21 Jan 2025 09:08:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mstdn.chrisalemany.ca/@chris/113864600222476627">https://mstdn.chrisalemany.ca/@chris/113864600222476627</a>, See on <a href="https://news.ycombinator.com/item?id=42777938">Hacker News</a></p>
Couldn't get https://mstdn.chrisalemany.ca/@chris/113864600222476627: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[It sure looks like Meta stole a lot of books to build its AI (134 pts)]]></title>
            <link>https://lithub.com/it-sure-looks-like-meta-stole-a-lot-of-books-to-build-its-ai/</link>
            <guid>42775545</guid>
            <pubDate>Tue, 21 Jan 2025 01:59:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lithub.com/it-sure-looks-like-meta-stole-a-lot-of-books-to-build-its-ai/">https://lithub.com/it-sure-looks-like-meta-stole-a-lot-of-books-to-build-its-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=42775545">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
				
				
				
				<p>It‚Äôs a grim week for Meta. The company formerly known as Facebook, and before that <a href="https://en.wikipedia.org/wiki/History_of_Facebook" target="_blank">Facemash</a>, ‚Äúdesigned to evaluate the attractiveness of female Harvard students,‚Äù now encompasses Facebook, Instagram, Threads, WhatsApp, and Meta, the failed vision for a remote workplace, fun-zone, and Zucker-verse where <a href="https://x.com/MetaHorizon/status/1579947568372404226" target="_blank">legs are always just around the corner.</a></p>
<p>CEO and founder Mark Zuckerberg announced that <a href="https://theintercept.com/2025/01/09/facebook-instagram-meta-hate-speech-content-moderation/" target="_blank">slurs are okay</a> on their platforms, added <a href="https://apnews.com/article/meta-facebook-zuckerberg-board-members-dana-white-199436c62c934ebb751b564f874ad2f6" target="_blank">a pro-Trump UFC boss to their board</a>, and made <a href="https://www.axios.com/2025/01/10/mark-zuckerberg-joe-rogan-facebook-censorship-biden" target="_blank">appearances in the aggrieved weirdo media world</a> to make some convoluted case that we need more masculine energy in business, more resentment overall, and more fealty to Don Trump. Zuckerberg has also recently switched up his personal style so that he now looks like he‚Äôs perpetually in a sitcom flashback where an older actor is unconvincingly costumed to look like their younger self.</p>
<p>And in the Northern District of California,&nbsp;<em>Wired&nbsp;</em>reports, <a href="https://www.wired.com/story/new-documents-unredacted-meta-copyright-ai-lawsuit/" target="_blank">recently unredacted court documents reveal</a> that Meta used a database of pirated books to train its AI systems<i>. </i>These documents were unsealed as part of a copyright lawsuit, one of the earliest of <a href="https://www.wired.com/story/ai-copyright-case-tracker/" target="_blank">many similar cases</a>, called <i>Kadrey et al. v. Meta Platforms.</i> The plaintiffs in this case are a number of writers and performers, including Richard Kadrey, Christopher Golden, Junot Diaz, Laura Lippman, Sarah Silverman, Ta-Nehisi Coates, and‚Äîjump scare!‚ÄîMike Huckabee.</p>
<p>The <a href="https://www.wired.com/story/new-documents-unredacted-meta-copyright-ai-lawsuit/" target="_blank">new documents quote</a> Meta employees frankly admitting to using stolen stuff from a notorious piracy site:</p>
<p>‚Ä¶an internal quote from a Meta employee, included in the documents, in which they speculated, ‚ÄúIf there is media coverage suggesting we have used a dataset we know to be pirated, such as LibGen, this may undermine our negotiating position with regulators on these issues.‚Äù‚Ä¶</p>
<p>‚Ä¶These newly unredacted documents reveal exchanges between Meta employees unearthed in the discovery process, like a Meta engineer telling a colleague that they hesitated to access LibGen data because ‚Äútorrenting from a [meta-owned] corporate laptop doesn‚Äôt feel right üòÉ‚Äù. They also allege that internal discussions about using LibGen data were escalated to Meta CEO Mark Zuckerberg (referred to as ‚ÄúMZ‚Äù in the memo handed over during discovery) and that Meta‚Äôs AI team was ‚Äúapproved to use‚Äù the pirated material.</p>
<p>Meta has claimed that they used publicly available material that was legally accessible under fair use doctrine, but that doesn‚Äôt pass the smell test to me: just because something is public on the internet, doesn‚Äôt make it legal.</p>
<p>The plaintiffs are arguing that they should be allowed to expand their case <a href="https://www.wired.com/story/new-documents-unredacted-meta-copyright-ai-lawsuit/" target="_blank">to incorporate these new findings</a>:</p>
<p>‚ÄúMeta, through a corporate representative who testified on November 20, 2024, has now admitted under oath to uploading (aka ‚Äòseeding‚Äô) pirated files containing Plaintiffs‚Äô works on ‚Äòtorrent‚Äô sites,‚Äù the motion alleges. (Seeding is when torrented files are then shared with other peers after they have finished downloading.)</p>
<p>‚ÄúThis torrenting activity turned Meta itself into a distributor of the very same pirated copyrighted material that it was also downloading for use in its commercially available AI models.‚Äù</p>
<p>Legally, Meta and their lawyers may find a way to finagle the law and get around this. But in plain terms, it doesn‚Äôt seem defensible for a major company with tons of lawyers, money, and talent to knowingly use stolen work to build something that they then turn around and sell.</p>
<p>I‚Äôm not naive enough to think that this lawsuit, or any of the many others currently winding their way through the courts, will end in this kind of software leaving the market‚Äîin America, you can‚Äôt unring a bell that‚Äôs been valued in the billions. But I do hope that the writers and artists whose work was stolen are compensated.</p>
<p>In spite of all this, tech-optimists continue to push AI in more places, and people in power continue to trumpet it as the future of everything. In the case of publishing, for example, the excellent <a href="https://www.instagram.com/xoxopublishinggg/#" target="_blank">xoxopublishinggg</a> Instagram account has been posting anonymous responses about publishing workers‚Äô experiences with AI in the workplace‚Äîit seems like a lot of publishers are at least curious about these tools in ways that don‚Äôt bode well for an AI-less future.</p>
<p>If you‚Äôre considering using AI, or are feeling pressure at work to do so, you can add ‚Äúbuilt on piracy‚Äù to the list of concerns about this tech, alongside its environmental impact, its human toll on underpaid and marginalized workers, and the simple fact that it is incapable of making anything good.</p>
				
										
									
				

				

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ruff: Python linter and code formatter written in Rust (176 pts)]]></title>
            <link>https://github.com/astral-sh/ruff</link>
            <guid>42775029</guid>
            <pubDate>Tue, 21 Jan 2025 00:49:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/astral-sh/ruff">https://github.com/astral-sh/ruff</a>, See on <a href="https://news.ycombinator.com/item?id=42775029">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><h2 tabindex="-1" dir="auto">Ruff</h2><a id="user-content-ruff" aria-label="Permalink: Ruff" href="#ruff"></a></p>
<p dir="auto"><a href="https://github.com/astral-sh/ruff"><img src="https://camo.githubusercontent.com/051a04ae958f4a1a5d6444df4cdc520305eef93d5028e6d4c7cd16efa3136cd4/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f61737472616c2d73682f727566662f6d61696e2f6173736574732f62616467652f76322e6a736f6e" alt="Ruff" data-canonical-src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json"></a>
<a href="https://pypi.python.org/pypi/ruff" rel="nofollow"><img src="https://camo.githubusercontent.com/14e1bc70770d22cc586d31fd726ffce6dbf5d248e5fbc700216542adfd6a4e07/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f727566662e737667" alt="image" data-canonical-src="https://img.shields.io/pypi/v/ruff.svg"></a>
<a href="https://github.com/astral-sh/ruff/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/05d0bcb2b2007f0c40a1a3d3eb693e9eec4c3d85aaea9cde6d463c3c3d89629c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f727566662e737667" alt="image" data-canonical-src="https://img.shields.io/pypi/l/ruff.svg"></a>
<a href="https://pypi.python.org/pypi/ruff" rel="nofollow"><img src="https://camo.githubusercontent.com/4756572c4e7149f6aa63e5a2c872c021eedfaa361c6a322d306bdd6b0a5c62d9/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f727566662e737667" alt="image" data-canonical-src="https://img.shields.io/pypi/pyversions/ruff.svg"></a>
<a href="https://github.com/astral-sh/ruff/actions"><img src="https://github.com/astral-sh/ruff/workflows/CI/badge.svg" alt="Actions status"></a>
<a href="https://discord.com/invite/astral-sh" rel="nofollow"><img src="https://camo.githubusercontent.com/647359c14a78a007576a41e446f1956e89ed1a91f673dfe19b848eebd94f502d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d2532333538363546322e7376673f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="Discord" data-canonical-src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white"></a></p>
<p dir="auto"><a href="https://docs.astral.sh/ruff/" rel="nofollow"><strong>Docs</strong></a> | <a href="https://play.ruff.rs/" rel="nofollow"><strong>Playground</strong></a></p>
<p dir="auto">An extremely fast Python linter and code formatter, written in Rust.</p>
<p dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://user-images.githubusercontent.com/1309177/232603514-c95e9b0f-6b31-43de-9a80-9e844173fd6a.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://user-images.githubusercontent.com/1309177/232603516-4fb4892d-585c-4b20-b810-3db9161831e4.svg">
    <img alt="Shows a bar chart with benchmark results." src="https://user-images.githubusercontent.com/1309177/232603516-4fb4892d-585c-4b20-b810-3db9161831e4.svg">
  </picture></themed-picture>
</p>
<p dir="auto">
  <i>Linting the CPython codebase from scratch.</i>
</p>
<ul dir="auto">
<li>‚ö°Ô∏è 10-100x faster than existing linters (like Flake8) and formatters (like Black)</li>
<li>üêç Installable via <code>pip</code></li>
<li>üõ†Ô∏è <code>pyproject.toml</code> support</li>
<li>ü§ù Python 3.13 compatibility</li>
<li>‚öñÔ∏è Drop-in parity with <a href="https://docs.astral.sh/ruff/faq/#how-does-ruffs-linter-compare-to-flake8" rel="nofollow">Flake8</a>, isort, and <a href="https://docs.astral.sh/ruff/faq/#how-does-ruffs-formatter-compare-to-black" rel="nofollow">Black</a></li>
<li>üì¶ Built-in caching, to avoid re-analyzing unchanged files</li>
<li>üîß Fix support, for automatic error correction (e.g., automatically remove unused imports)</li>
<li>üìè Over <a href="https://docs.astral.sh/ruff/rules/" rel="nofollow">800 built-in rules</a>, with native re-implementations
of popular Flake8 plugins, like flake8-bugbear</li>
<li>‚å®Ô∏è First-party <a href="https://docs.astral.sh/ruff/integrations/" rel="nofollow">editor integrations</a> for
<a href="https://github.com/astral-sh/ruff-vscode">VS Code</a> and <a href="https://docs.astral.sh/ruff/editors/setup" rel="nofollow">more</a></li>
<li>üåé Monorepo-friendly, with <a href="https://docs.astral.sh/ruff/configuration/#config-file-discovery" rel="nofollow">hierarchical and cascading configuration</a></li>
</ul>
<p dir="auto">Ruff aims to be orders of magnitude faster than alternative tools while integrating more
functionality behind a single, common interface.</p>
<p dir="auto">Ruff can be used to replace <a href="https://pypi.org/project/flake8/" rel="nofollow">Flake8</a> (plus dozens of plugins),
<a href="https://github.com/psf/black">Black</a>, <a href="https://pypi.org/project/isort/" rel="nofollow">isort</a>,
<a href="https://pypi.org/project/pydocstyle/" rel="nofollow">pydocstyle</a>, <a href="https://pypi.org/project/pyupgrade/" rel="nofollow">pyupgrade</a>,
<a href="https://pypi.org/project/autoflake/" rel="nofollow">autoflake</a>, and more, all while executing tens or hundreds of
times faster than any individual tool.</p>
<p dir="auto">Ruff is extremely actively developed and used in major open-source projects like:</p>
<ul dir="auto">
<li><a href="https://github.com/apache/airflow">Apache Airflow</a></li>
<li><a href="https://github.com/apache/superset">Apache Superset</a></li>
<li><a href="https://github.com/tiangolo/fastapi">FastAPI</a></li>
<li><a href="https://github.com/huggingface/transformers">Hugging Face</a></li>
<li><a href="https://github.com/pandas-dev/pandas">Pandas</a></li>
<li><a href="https://github.com/scipy/scipy">SciPy</a></li>
</ul>
<p dir="auto">...and <a href="#whos-using-ruff">many more</a>.</p>
<p dir="auto">Ruff is backed by <a href="https://astral.sh/" rel="nofollow">Astral</a>. Read the <a href="https://astral.sh/blog/announcing-astral-the-company-behind-ruff" rel="nofollow">launch post</a>,
or the original <a href="https://notes.crmarsh.com/python-tooling-could-be-much-much-faster" rel="nofollow">project announcement</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Testimonials</h2><a id="user-content-testimonials" aria-label="Permalink: Testimonials" href="#testimonials"></a></p>
<p dir="auto"><a href="https://twitter.com/tiangolo/status/1591912354882764802" rel="nofollow"><strong>Sebasti√°n Ram√≠rez</strong></a>, creator
of <a href="https://github.com/tiangolo/fastapi">FastAPI</a>:</p>
<blockquote>
<p dir="auto">Ruff is so fast that sometimes I add an intentional bug in the code just to confirm it's actually
running and checking the code.</p>
</blockquote>
<p dir="auto"><a href="https://twitter.com/schrockn/status/1612615862904827904" rel="nofollow"><strong>Nick Schrock</strong></a>, founder of <a href="https://www.elementl.com/" rel="nofollow">Elementl</a>,
co-creator of <a href="https://graphql.org/" rel="nofollow">GraphQL</a>:</p>
<blockquote>
<p dir="auto">Why is Ruff a gamechanger? Primarily because it is nearly 1000x faster. Literally. Not a typo. On
our largest module (dagster itself, 250k LOC) pylint takes about 2.5 minutes, parallelized across 4
cores on my M1. Running ruff against our <em>entire</em> codebase takes .4 seconds.</p>
</blockquote>
<p dir="auto"><a href="https://github.com/bokeh/bokeh/pull/12605" data-hovercard-type="pull_request" data-hovercard-url="/bokeh/bokeh/pull/12605/hovercard"><strong>Bryan Van de Ven</strong></a>, co-creator
of <a href="https://github.com/bokeh/bokeh/">Bokeh</a>, original author
of <a href="https://docs.conda.io/en/latest/" rel="nofollow">Conda</a>:</p>
<blockquote>
<p dir="auto">Ruff is ~150-200x faster than flake8 on my machine, scanning the whole repo takes ~0.2s instead of
~20s. This is an enormous quality of life improvement for local dev. It's fast enough that I added
it as an actual commit hook, which is terrific.</p>
</blockquote>
<p dir="auto"><a href="https://twitter.com/timothycrosley/status/1606420868514877440" rel="nofollow"><strong>Timothy Crosley</strong></a>,
creator of <a href="https://github.com/PyCQA/isort">isort</a>:</p>
<blockquote>
<p dir="auto">Just switched my first project to Ruff. Only one downside so far: it's so fast I couldn't believe
it was working till I intentionally introduced some errors.</p>
</blockquote>
<p dir="auto"><a href="https://github.com/astral-sh/ruff/issues/465#issuecomment-1317400028" data-hovercard-type="issue" data-hovercard-url="/astral-sh/ruff/issues/465/hovercard"><strong>Tim Abbott</strong></a>, lead
developer of <a href="https://github.com/zulip/zulip">Zulip</a>:</p>
<blockquote>
<p dir="auto">This is just ridiculously fast... <code>ruff</code> is amazing.</p>
</blockquote>

<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<p dir="auto">For more, see the <a href="https://docs.astral.sh/ruff/" rel="nofollow">documentation</a>.</p>
<ol dir="auto">
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#rules">Rules</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#support">Support</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#whos-using-ruff">Who's Using Ruff?</a></li>
<li><a href="#license">License</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started<a id="user-content-getting-started"></a></h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">For more, see the <a href="https://docs.astral.sh/ruff/" rel="nofollow">documentation</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Ruff is available as <a href="https://pypi.org/project/ruff/" rel="nofollow"><code>ruff</code></a> on PyPI.</p>
<p dir="auto">Invoke Ruff directly with <a href="https://docs.astral.sh/uv/" rel="nofollow"><code>uvx</code></a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uvx ruff check   # Lint all files in the current directory.
uvx ruff format  # Format all files in the current directory."><pre>uvx ruff check   <span><span>#</span> Lint all files in the current directory.</span>
uvx ruff format  <span><span>#</span> Format all files in the current directory.</span></pre></div>
<p dir="auto">Or install Ruff with <code>uv</code> (recommended), <code>pip</code>, or <code>pipx</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# With uv.
uv tool install ruff@latest  # Install Ruff globally.
uv add --dev ruff            # Or add Ruff to your project.

# With pip.
pip install ruff

# With pipx.
pipx install ruff"><pre><span><span>#</span> With uv.</span>
uv tool install ruff@latest  <span><span>#</span> Install Ruff globally.</span>
uv add --dev ruff            <span><span>#</span> Or add Ruff to your project.</span>

<span><span>#</span> With pip.</span>
pip install ruff

<span><span>#</span> With pipx.</span>
pipx install ruff</pre></div>
<p dir="auto">Starting with version <code>0.5.0</code>, Ruff can be installed with our standalone installers:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# On macOS and Linux.
curl -LsSf https://astral.sh/ruff/install.sh | sh

# On Windows.
powershell -c &quot;irm https://astral.sh/ruff/install.ps1 | iex&quot;

# For a specific version.
curl -LsSf https://astral.sh/ruff/0.9.2/install.sh | sh
powershell -c &quot;irm https://astral.sh/ruff/0.9.2/install.ps1 | iex&quot;"><pre><span><span>#</span> On macOS and Linux.</span>
curl -LsSf https://astral.sh/ruff/install.sh <span>|</span> sh

<span><span>#</span> On Windows.</span>
powershell -c <span><span>"</span>irm https://astral.sh/ruff/install.ps1 | iex<span>"</span></span>

<span><span>#</span> For a specific version.</span>
curl -LsSf https://astral.sh/ruff/0.9.2/install.sh <span>|</span> sh
powershell -c <span><span>"</span>irm https://astral.sh/ruff/0.9.2/install.ps1 | iex<span>"</span></span></pre></div>
<p dir="auto">You can also install Ruff via <a href="https://formulae.brew.sh/formula/ruff" rel="nofollow">Homebrew</a>, <a href="https://anaconda.org/conda-forge/ruff" rel="nofollow">Conda</a>,
and with <a href="https://docs.astral.sh/ruff/installation/" rel="nofollow">a variety of other package managers</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">To run Ruff as a linter, try any of the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ruff check                          # Lint all files in the current directory (and any subdirectories).
ruff check path/to/code/            # Lint all files in `/path/to/code` (and any subdirectories).
ruff check path/to/code/*.py        # Lint all `.py` files in `/path/to/code`.
ruff check path/to/code/to/file.py  # Lint `file.py`.
ruff check @arguments.txt           # Lint using an input file, treating its contents as newline-delimited command-line arguments."><pre>ruff check                          <span><span>#</span> Lint all files in the current directory (and any subdirectories).</span>
ruff check path/to/code/            <span><span>#</span> Lint all files in `/path/to/code` (and any subdirectories).</span>
ruff check path/to/code/<span>*</span>.py        <span><span>#</span> Lint all `.py` files in `/path/to/code`.</span>
ruff check path/to/code/to/file.py  <span><span>#</span> Lint `file.py`.</span>
ruff check @arguments.txt           <span><span>#</span> Lint using an input file, treating its contents as newline-delimited command-line arguments.</span></pre></div>
<p dir="auto">Or, to run Ruff as a formatter:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ruff format                          # Format all files in the current directory (and any subdirectories).
ruff format path/to/code/            # Format all files in `/path/to/code` (and any subdirectories).
ruff format path/to/code/*.py        # Format all `.py` files in `/path/to/code`.
ruff format path/to/code/to/file.py  # Format `file.py`.
ruff format @arguments.txt           # Format using an input file, treating its contents as newline-delimited command-line arguments."><pre>ruff format                          <span><span>#</span> Format all files in the current directory (and any subdirectories).</span>
ruff format path/to/code/            <span><span>#</span> Format all files in `/path/to/code` (and any subdirectories).</span>
ruff format path/to/code/<span>*</span>.py        <span><span>#</span> Format all `.py` files in `/path/to/code`.</span>
ruff format path/to/code/to/file.py  <span><span>#</span> Format `file.py`.</span>
ruff format @arguments.txt           <span><span>#</span> Format using an input file, treating its contents as newline-delimited command-line arguments.</span></pre></div>
<p dir="auto">Ruff can also be used as a <a href="https://pre-commit.com/" rel="nofollow">pre-commit</a> hook via <a href="https://github.com/astral-sh/ruff-pre-commit"><code>ruff-pre-commit</code></a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="- repo: https://github.com/astral-sh/ruff-pre-commit
  # Ruff version.
  rev: v0.9.2
  hooks:
    # Run the linter.
    - id: ruff
      args: [ --fix ]
    # Run the formatter.
    - id: ruff-format"><pre>- <span>repo</span>: <span>https://github.com/astral-sh/ruff-pre-commit</span>
  <span><span>#</span> Ruff version.</span>
  <span>rev</span>: <span>v0.9.2</span>
  <span>hooks</span>:
    <span><span>#</span> Run the linter.</span>
    - <span>id</span>: <span>ruff</span>
      <span>args</span>: <span>[ --fix ]</span>
    <span><span>#</span> Run the formatter.</span>
    - <span>id</span>: <span>ruff-format</span></pre></div>
<p dir="auto">Ruff can also be used as a <a href="https://github.com/astral-sh/ruff-vscode">VS Code extension</a> or with <a href="https://docs.astral.sh/ruff/editors/setup" rel="nofollow">various other editors</a>.</p>
<p dir="auto">Ruff can also be used as a <a href="https://github.com/features/actions">GitHub Action</a> via
<a href="https://github.com/astral-sh/ruff-action"><code>ruff-action</code></a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="name: Ruff
on: [ push, pull_request ]
jobs:
  ruff:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/ruff-action@v3"><pre><span>name</span>: <span>Ruff</span>
<span>on</span>: <span>[ push, pull_request ]</span>
<span>jobs</span>:
  <span>ruff</span>:
    <span>runs-on</span>: <span>ubuntu-latest</span>
    <span>steps</span>:
      - <span>uses</span>: <span>actions/checkout@v4</span>
      - <span>uses</span>: <span>astral-sh/ruff-action@v3</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration<a id="user-content-configuration"></a></h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Ruff can be configured through a <code>pyproject.toml</code>, <code>ruff.toml</code>, or <code>.ruff.toml</code> file (see:
<a href="https://docs.astral.sh/ruff/configuration/" rel="nofollow"><em>Configuration</em></a>, or <a href="https://docs.astral.sh/ruff/settings/" rel="nofollow"><em>Settings</em></a>
for a complete list of all configuration options).</p>
<p dir="auto">If left unspecified, Ruff's default configuration is equivalent to the following <code>ruff.toml</code> file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Exclude a variety of commonly ignored directories.
exclude = [
    &quot;.bzr&quot;,
    &quot;.direnv&quot;,
    &quot;.eggs&quot;,
    &quot;.git&quot;,
    &quot;.git-rewrite&quot;,
    &quot;.hg&quot;,
    &quot;.ipynb_checkpoints&quot;,
    &quot;.mypy_cache&quot;,
    &quot;.nox&quot;,
    &quot;.pants.d&quot;,
    &quot;.pyenv&quot;,
    &quot;.pytest_cache&quot;,
    &quot;.pytype&quot;,
    &quot;.ruff_cache&quot;,
    &quot;.svn&quot;,
    &quot;.tox&quot;,
    &quot;.venv&quot;,
    &quot;.vscode&quot;,
    &quot;__pypackages__&quot;,
    &quot;_build&quot;,
    &quot;buck-out&quot;,
    &quot;build&quot;,
    &quot;dist&quot;,
    &quot;node_modules&quot;,
    &quot;site-packages&quot;,
    &quot;venv&quot;,
]

# Same as Black.
line-length = 88
indent-width = 4

# Assume Python 3.9
target-version = &quot;py39&quot;

[lint]
# Enable Pyflakes (`F`) and a subset of the pycodestyle (`E`)  codes by default.
select = [&quot;E4&quot;, &quot;E7&quot;, &quot;E9&quot;, &quot;F&quot;]
ignore = []

# Allow fix for all enabled rules (when `--fix`) is provided.
fixable = [&quot;ALL&quot;]
unfixable = []

# Allow unused variables when underscore-prefixed.
dummy-variable-rgx = &quot;^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$&quot;

[format]
# Like Black, use double quotes for strings.
quote-style = &quot;double&quot;

# Like Black, indent with spaces, rather than tabs.
indent-style = &quot;space&quot;

# Like Black, respect magic trailing commas.
skip-magic-trailing-comma = false

# Like Black, automatically detect the appropriate line ending.
line-ending = &quot;auto&quot;"><pre><span><span>#</span> Exclude a variety of commonly ignored directories.</span>
<span>exclude</span> = [
    <span><span>"</span>.bzr<span>"</span></span>,
    <span><span>"</span>.direnv<span>"</span></span>,
    <span><span>"</span>.eggs<span>"</span></span>,
    <span><span>"</span>.git<span>"</span></span>,
    <span><span>"</span>.git-rewrite<span>"</span></span>,
    <span><span>"</span>.hg<span>"</span></span>,
    <span><span>"</span>.ipynb_checkpoints<span>"</span></span>,
    <span><span>"</span>.mypy_cache<span>"</span></span>,
    <span><span>"</span>.nox<span>"</span></span>,
    <span><span>"</span>.pants.d<span>"</span></span>,
    <span><span>"</span>.pyenv<span>"</span></span>,
    <span><span>"</span>.pytest_cache<span>"</span></span>,
    <span><span>"</span>.pytype<span>"</span></span>,
    <span><span>"</span>.ruff_cache<span>"</span></span>,
    <span><span>"</span>.svn<span>"</span></span>,
    <span><span>"</span>.tox<span>"</span></span>,
    <span><span>"</span>.venv<span>"</span></span>,
    <span><span>"</span>.vscode<span>"</span></span>,
    <span><span>"</span>__pypackages__<span>"</span></span>,
    <span><span>"</span>_build<span>"</span></span>,
    <span><span>"</span>buck-out<span>"</span></span>,
    <span><span>"</span>build<span>"</span></span>,
    <span><span>"</span>dist<span>"</span></span>,
    <span><span>"</span>node_modules<span>"</span></span>,
    <span><span>"</span>site-packages<span>"</span></span>,
    <span><span>"</span>venv<span>"</span></span>,
]

<span><span>#</span> Same as Black.</span>
<span>line-length</span> = <span>88</span>
<span>indent-width</span> = <span>4</span>

<span><span>#</span> Assume Python 3.9</span>
<span>target-version</span> = <span><span>"</span>py39<span>"</span></span>

[<span>lint</span>]
<span><span>#</span> Enable Pyflakes (`F`) and a subset of the pycodestyle (`E`)  codes by default.</span>
<span>select</span> = [<span><span>"</span>E4<span>"</span></span>, <span><span>"</span>E7<span>"</span></span>, <span><span>"</span>E9<span>"</span></span>, <span><span>"</span>F<span>"</span></span>]
<span>ignore</span> = []

<span><span>#</span> Allow fix for all enabled rules (when `--fix`) is provided.</span>
<span>fixable</span> = [<span><span>"</span>ALL<span>"</span></span>]
<span>unfixable</span> = []

<span><span>#</span> Allow unused variables when underscore-prefixed.</span>
<span>dummy-variable-rgx</span> = <span><span>"</span>^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$<span>"</span></span>

[<span>format</span>]
<span><span>#</span> Like Black, use double quotes for strings.</span>
<span>quote-style</span> = <span><span>"</span>double<span>"</span></span>

<span><span>#</span> Like Black, indent with spaces, rather than tabs.</span>
<span>indent-style</span> = <span><span>"</span>space<span>"</span></span>

<span><span>#</span> Like Black, respect magic trailing commas.</span>
<span>skip-magic-trailing-comma</span> = <span>false</span>

<span><span>#</span> Like Black, automatically detect the appropriate line ending.</span>
<span>line-ending</span> = <span><span>"</span>auto<span>"</span></span></pre></div>
<p dir="auto">Note that, in a <code>pyproject.toml</code>, each section header should be prefixed with <code>tool.ruff</code>. For
example, <code>[lint]</code> should be replaced with <code>[tool.ruff.lint]</code>.</p>
<p dir="auto">Some configuration options can be provided via dedicated command-line arguments, such as those
related to rule enablement and disablement, file discovery, and logging level:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ruff check --select F401 --select F403 --quiet"><pre>ruff check --select F401 --select F403 --quiet</pre></div>
<p dir="auto">The remaining configuration options can be provided through a catch-all <code>--config</code> argument:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ruff check --config &quot;lint.per-file-ignores = {'some_file.py' = ['F841']}&quot;"><pre>ruff check --config <span><span>"</span>lint.per-file-ignores = {'some_file.py' = ['F841']}<span>"</span></span></pre></div>
<p dir="auto">To opt in to the latest lint rules, formatter style changes, interface updates, and more, enable
<a href="https://docs.astral.sh/ruff/rules/" rel="nofollow">preview mode</a> by setting <code>preview = true</code> in your configuration
file or passing <code>--preview</code> on the command line. Preview mode enables a collection of unstable
features that may change prior to stabilization.</p>
<p dir="auto">See <code>ruff help</code> for more on Ruff's top-level commands, or <code>ruff help check</code> and <code>ruff help format</code>
for more on the linting and formatting commands, respectively.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Rules<a id="user-content-rules"></a></h2><a id="user-content-rules" aria-label="Permalink: Rules" href="#rules"></a></p>

<p dir="auto"><strong>Ruff supports over 800 lint rules</strong>, many of which are inspired by popular tools like Flake8,
isort, pyupgrade, and others. Regardless of the rule's origin, Ruff re-implements every rule in
Rust as a first-party feature.</p>
<p dir="auto">By default, Ruff enables Flake8's <code>F</code> rules, along with a subset of the <code>E</code> rules, omitting any
stylistic rules that overlap with the use of a formatter, like <code>ruff format</code> or
<a href="https://github.com/psf/black">Black</a>.</p>
<p dir="auto">If you're just getting started with Ruff, <strong>the default rule set is a great place to start</strong>: it
catches a wide variety of common errors (like unused imports) with zero configuration.</p>

<p dir="auto">Beyond the defaults, Ruff re-implements some of the most popular Flake8 plugins and related code
quality tools, including:</p>
<ul dir="auto">
<li><a href="https://pypi.org/project/autoflake/" rel="nofollow">autoflake</a></li>
<li><a href="https://pypi.org/project/eradicate/" rel="nofollow">eradicate</a></li>
<li><a href="https://pypi.org/project/flake8-2020/" rel="nofollow">flake8-2020</a></li>
<li><a href="https://pypi.org/project/flake8-annotations/" rel="nofollow">flake8-annotations</a></li>
<li><a href="https://pypi.org/project/flake8-async" rel="nofollow">flake8-async</a></li>
<li><a href="https://pypi.org/project/flake8-bandit/" rel="nofollow">flake8-bandit</a> (<a href="https://github.com/astral-sh/ruff/issues/1646" data-hovercard-type="issue" data-hovercard-url="/astral-sh/ruff/issues/1646/hovercard">#1646</a>)</li>
<li><a href="https://pypi.org/project/flake8-blind-except/" rel="nofollow">flake8-blind-except</a></li>
<li><a href="https://pypi.org/project/flake8-boolean-trap/" rel="nofollow">flake8-boolean-trap</a></li>
<li><a href="https://pypi.org/project/flake8-bugbear/" rel="nofollow">flake8-bugbear</a></li>
<li><a href="https://pypi.org/project/flake8-builtins/" rel="nofollow">flake8-builtins</a></li>
<li><a href="https://pypi.org/project/flake8-commas/" rel="nofollow">flake8-commas</a></li>
<li><a href="https://pypi.org/project/flake8-comprehensions/" rel="nofollow">flake8-comprehensions</a></li>
<li><a href="https://pypi.org/project/flake8-copyright/" rel="nofollow">flake8-copyright</a></li>
<li><a href="https://pypi.org/project/flake8-datetimez/" rel="nofollow">flake8-datetimez</a></li>
<li><a href="https://pypi.org/project/flake8-debugger/" rel="nofollow">flake8-debugger</a></li>
<li><a href="https://pypi.org/project/flake8-django/" rel="nofollow">flake8-django</a></li>
<li><a href="https://pypi.org/project/flake8-docstrings/" rel="nofollow">flake8-docstrings</a></li>
<li><a href="https://pypi.org/project/flake8-eradicate/" rel="nofollow">flake8-eradicate</a></li>
<li><a href="https://pypi.org/project/flake8-errmsg/" rel="nofollow">flake8-errmsg</a></li>
<li><a href="https://pypi.org/project/flake8-executable/" rel="nofollow">flake8-executable</a></li>
<li><a href="https://pypi.org/project/flake8-future-annotations/" rel="nofollow">flake8-future-annotations</a></li>
<li><a href="https://pypi.org/project/flake8-gettext/" rel="nofollow">flake8-gettext</a></li>
<li><a href="https://pypi.org/project/flake8-implicit-str-concat/" rel="nofollow">flake8-implicit-str-concat</a></li>
<li><a href="https://github.com/joaopalmeiro/flake8-import-conventions">flake8-import-conventions</a></li>
<li><a href="https://pypi.org/project/flake8-logging/" rel="nofollow">flake8-logging</a></li>
<li><a href="https://pypi.org/project/flake8-logging-format/" rel="nofollow">flake8-logging-format</a></li>
<li><a href="https://pypi.org/project/flake8-no-pep420" rel="nofollow">flake8-no-pep420</a></li>
<li><a href="https://pypi.org/project/flake8-pie/" rel="nofollow">flake8-pie</a></li>
<li><a href="https://pypi.org/project/flake8-print/" rel="nofollow">flake8-print</a></li>
<li><a href="https://pypi.org/project/flake8-pyi/" rel="nofollow">flake8-pyi</a></li>
<li><a href="https://pypi.org/project/flake8-pytest-style/" rel="nofollow">flake8-pytest-style</a></li>
<li><a href="https://pypi.org/project/flake8-quotes/" rel="nofollow">flake8-quotes</a></li>
<li><a href="https://pypi.org/project/flake8-raise/" rel="nofollow">flake8-raise</a></li>
<li><a href="https://pypi.org/project/flake8-return/" rel="nofollow">flake8-return</a></li>
<li><a href="https://pypi.org/project/flake8-self/" rel="nofollow">flake8-self</a></li>
<li><a href="https://pypi.org/project/flake8-simplify/" rel="nofollow">flake8-simplify</a></li>
<li><a href="https://pypi.org/project/flake8-slots/" rel="nofollow">flake8-slots</a></li>
<li><a href="https://pypi.org/project/flake8-super/" rel="nofollow">flake8-super</a></li>
<li><a href="https://pypi.org/project/flake8-tidy-imports/" rel="nofollow">flake8-tidy-imports</a></li>
<li><a href="https://pypi.org/project/flake8-todos/" rel="nofollow">flake8-todos</a></li>
<li><a href="https://pypi.org/project/flake8-type-checking/" rel="nofollow">flake8-type-checking</a></li>
<li><a href="https://pypi.org/project/flake8-use-pathlib/" rel="nofollow">flake8-use-pathlib</a></li>
<li><a href="https://pypi.org/project/flynt/" rel="nofollow">flynt</a> (<a href="https://github.com/astral-sh/ruff/issues/2102" data-hovercard-type="issue" data-hovercard-url="/astral-sh/ruff/issues/2102/hovercard">#2102</a>)</li>
<li><a href="https://pypi.org/project/isort/" rel="nofollow">isort</a></li>
<li><a href="https://pypi.org/project/mccabe/" rel="nofollow">mccabe</a></li>
<li><a href="https://pypi.org/project/pandas-vet/" rel="nofollow">pandas-vet</a></li>
<li><a href="https://pypi.org/project/pep8-naming/" rel="nofollow">pep8-naming</a></li>
<li><a href="https://pypi.org/project/pydocstyle/" rel="nofollow">pydocstyle</a></li>
<li><a href="https://github.com/pre-commit/pygrep-hooks">pygrep-hooks</a></li>
<li><a href="https://pypi.org/project/pylint-airflow/" rel="nofollow">pylint-airflow</a></li>
<li><a href="https://pypi.org/project/pyupgrade/" rel="nofollow">pyupgrade</a></li>
<li><a href="https://pypi.org/project/tryceratops/" rel="nofollow">tryceratops</a></li>
<li><a href="https://pypi.org/project/yesqa/" rel="nofollow">yesqa</a></li>
</ul>
<p dir="auto">For a complete enumeration of the supported rules, see <a href="https://docs.astral.sh/ruff/rules/" rel="nofollow"><em>Rules</em></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing<a id="user-content-contributing"></a></h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome and highly appreciated. To get started, check out the
<a href="https://docs.astral.sh/ruff/contributing/" rel="nofollow"><strong>contributing guidelines</strong></a>.</p>
<p dir="auto">You can also join us on <a href="https://discord.com/invite/astral-sh" rel="nofollow"><strong>Discord</strong></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support<a id="user-content-support"></a></h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<p dir="auto">Having trouble? Check out the existing issues on <a href="https://github.com/astral-sh/ruff/issues"><strong>GitHub</strong></a>,
or feel free to <a href="https://github.com/astral-sh/ruff/issues/new"><strong>open a new one</strong></a>.</p>
<p dir="auto">You can also ask for help on <a href="https://discord.com/invite/astral-sh" rel="nofollow"><strong>Discord</strong></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements<a id="user-content-acknowledgements"></a></h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">Ruff's linter draws on both the APIs and implementation details of many other
tools in the Python ecosystem, especially <a href="https://github.com/PyCQA/flake8">Flake8</a>, <a href="https://github.com/PyCQA/pyflakes">Pyflakes</a>,
<a href="https://github.com/PyCQA/pycodestyle">pycodestyle</a>, <a href="https://github.com/PyCQA/pydocstyle">pydocstyle</a>,
<a href="https://github.com/asottile/pyupgrade">pyupgrade</a>, and <a href="https://github.com/PyCQA/isort">isort</a>.</p>
<p dir="auto">In some cases, Ruff includes a "direct" Rust port of the corresponding tool.
We're grateful to the maintainers of these tools for their work, and for all
the value they've provided to the Python community.</p>
<p dir="auto">Ruff's formatter is built on a fork of Rome's <a href="https://github.com/rome/tools/tree/main/crates/rome_formatter"><code>rome_formatter</code></a>,
and again draws on both API and implementation details from <a href="https://github.com/rome/tools">Rome</a>,
<a href="https://github.com/prettier/prettier">Prettier</a>, and <a href="https://github.com/psf/black">Black</a>.</p>
<p dir="auto">Ruff's import resolver is based on the import resolution algorithm from <a href="https://github.com/microsoft/pyright">Pyright</a>.</p>
<p dir="auto">Ruff is also influenced by a number of tools outside the Python ecosystem, like
<a href="https://github.com/rust-lang/rust-clippy">Clippy</a> and <a href="https://github.com/eslint/eslint">ESLint</a>.</p>
<p dir="auto">Ruff is the beneficiary of a large number of <a href="https://github.com/astral-sh/ruff/graphs/contributors">contributors</a>.</p>
<p dir="auto">Ruff is released under the MIT license.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Who's Using Ruff?<a id="user-content-whos-using-ruff"></a></h2><a id="user-content-whos-using-ruff" aria-label="Permalink: Who's Using Ruff?" href="#whos-using-ruff"></a></p>
<p dir="auto">Ruff is used by a number of major open-source projects and companies, including:</p>
<ul dir="auto">
<li><a href="https://github.com/albumentations-team/albumentations">Albumentations</a></li>
<li>Amazon (<a href="https://github.com/aws/serverless-application-model">AWS SAM</a>)</li>
<li>Anthropic (<a href="https://github.com/anthropics/anthropic-sdk-python">Python SDK</a>)</li>
<li><a href="https://github.com/apache/airflow">Apache Airflow</a></li>
<li>AstraZeneca (<a href="https://github.com/AstraZeneca/magnus-core">Magnus</a>)</li>
<li><a href="https://github.com/python-babel/babel">Babel</a></li>
<li>Benchling (<a href="https://github.com/benchling/refac">Refac</a>)</li>
<li><a href="https://github.com/bokeh/bokeh">Bokeh</a></li>
<li>CrowdCent (<a href="https://github.com/crowdcent/numerblox">NumerBlox</a>) </li>
<li><a href="https://github.com/pyca/cryptography">Cryptography (PyCA)</a></li>
<li>CERN (<a href="https://getindico.io/" rel="nofollow">Indico</a>)</li>
<li><a href="https://github.com/iterative/dvc">DVC</a></li>
<li><a href="https://github.com/dagger/dagger">Dagger</a></li>
<li><a href="https://github.com/dagster-io/dagster">Dagster</a></li>
<li>Databricks (<a href="https://github.com/mlflow/mlflow">MLflow</a>)</li>
<li><a href="https://github.com/langgenius/dify">Dify</a></li>
<li><a href="https://github.com/tiangolo/fastapi">FastAPI</a></li>
<li><a href="https://github.com/godotengine/godot">Godot</a></li>
<li><a href="https://github.com/gradio-app/gradio">Gradio</a></li>
<li><a href="https://github.com/great-expectations/great_expectations">Great Expectations</a></li>
<li><a href="https://github.com/encode/httpx">HTTPX</a></li>
<li><a href="https://github.com/pypa/hatch">Hatch</a></li>
<li><a href="https://github.com/home-assistant/core">Home Assistant</a></li>
<li>Hugging Face (<a href="https://github.com/huggingface/transformers">Transformers</a>,
<a href="https://github.com/huggingface/datasets">Datasets</a>,
<a href="https://github.com/huggingface/diffusers">Diffusers</a>)</li>
<li>IBM (<a href="https://github.com/Qiskit/qiskit">Qiskit</a>)</li>
<li>ING Bank (<a href="https://github.com/ing-bank/popmon">popmon</a>, <a href="https://github.com/ing-bank/probatus">probatus</a>)</li>
<li><a href="https://github.com/ibis-project/ibis">Ibis</a></li>
<li><a href="https://github.com/unifyai/ivy">ivy</a></li>
<li><a href="https://github.com/jupyter-server/jupyter_server">Jupyter</a></li>
<li><a href="https://kraken.tech/" rel="nofollow">Kraken Tech</a></li>
<li><a href="https://github.com/hwchase17/langchain">LangChain</a></li>
<li><a href="https://litestar.dev/" rel="nofollow">Litestar</a></li>
<li><a href="https://github.com/jerryjliu/llama_index">LlamaIndex</a></li>
<li>Matrix (<a href="https://github.com/matrix-org/synapse">Synapse</a>)</li>
<li><a href="https://github.com/oxsecurity/megalinter">MegaLinter</a></li>
<li>Meltano (<a href="https://github.com/meltano/meltano">Meltano CLI</a>, <a href="https://github.com/meltano/sdk">Singer SDK</a>)</li>
<li>Microsoft (<a href="https://github.com/microsoft/semantic-kernel">Semantic Kernel</a>,
<a href="https://github.com/microsoft/onnxruntime">ONNX Runtime</a>,
<a href="https://github.com/microsoft/LightGBM">LightGBM</a>)</li>
<li>Modern Treasury (<a href="https://github.com/Modern-Treasury/modern-treasury-python">Python SDK</a>)</li>
<li>Mozilla (<a href="https://github.com/mozilla/gecko-dev">Firefox</a>)</li>
<li><a href="https://github.com/python/mypy">Mypy</a></li>
<li><a href="https://github.com/nautobot/nautobot">Nautobot</a></li>
<li>Netflix (<a href="https://github.com/Netflix/dispatch">Dispatch</a>)</li>
<li><a href="https://github.com/neondatabase/neon">Neon</a></li>
<li><a href="https://nokia.com/" rel="nofollow">Nokia</a></li>
<li><a href="https://github.com/nonebot/nonebot2">NoneBot</a></li>
<li><a href="https://github.com/pyro-ppl/numpyro">NumPyro</a></li>
<li><a href="https://github.com/onnx/onnx">ONNX</a></li>
<li><a href="https://github.com/OpenBB-finance/OpenBBTerminal">OpenBB</a></li>
<li><a href="https://github.com/Open-Wine-Components/umu-launcher">Open Wine Components</a></li>
<li><a href="https://github.com/pdm-project/pdm">PDM</a></li>
<li><a href="https://github.com/PaddlePaddle/Paddle">PaddlePaddle</a></li>
<li><a href="https://github.com/pandas-dev/pandas">Pandas</a></li>
<li><a href="https://github.com/python-pillow/Pillow">Pillow</a></li>
<li><a href="https://github.com/python-poetry/poetry">Poetry</a></li>
<li><a href="https://github.com/pola-rs/polars">Polars</a></li>
<li><a href="https://github.com/PostHog/posthog">PostHog</a></li>
<li>Prefect (<a href="https://github.com/PrefectHQ/prefect">Python SDK</a>, <a href="https://github.com/PrefectHQ/marvin">Marvin</a>)</li>
<li><a href="https://github.com/pyinstaller/pyinstaller">PyInstaller</a></li>
<li><a href="https://github.com/pymc-devs/pymc/">PyMC</a></li>
<li><a href="https://github.com/pymc-labs/pymc-marketing">PyMC-Marketing</a></li>
<li><a href="https://github.com/pytest-dev/pytest">pytest</a></li>
<li><a href="https://github.com/pytorch/pytorch">PyTorch</a></li>
<li><a href="https://github.com/pydantic/pydantic">Pydantic</a></li>
<li><a href="https://github.com/PyCQA/pylint">Pylint</a></li>
<li><a href="https://github.com/pyvista/pyvista">PyVista</a></li>
<li><a href="https://github.com/reflex-dev/reflex">Reflex</a></li>
<li><a href="https://github.com/online-ml/river">River</a></li>
<li><a href="https://rippling.com/" rel="nofollow">Rippling</a></li>
<li><a href="https://github.com/sansyrox/robyn">Robyn</a></li>
<li><a href="https://github.com/saleor/saleor">Saleor</a></li>
<li>Scale AI (<a href="https://github.com/scaleapi/launch-python-client">Launch SDK</a>)</li>
<li><a href="https://github.com/scipy/scipy">SciPy</a></li>
<li>Snowflake (<a href="https://github.com/Snowflake-Labs/snowcli">SnowCLI</a>)</li>
<li><a href="https://github.com/sphinx-doc/sphinx">Sphinx</a></li>
<li><a href="https://github.com/DLR-RM/stable-baselines3">Stable Baselines3</a></li>
<li><a href="https://github.com/encode/starlette">Starlette</a></li>
<li><a href="https://github.com/streamlit/streamlit">Streamlit</a></li>
<li><a href="https://github.com/TheAlgorithms/Python">The Algorithms</a></li>
<li><a href="https://github.com/altair-viz/altair">Vega-Altair</a></li>
<li>WordPress (<a href="https://github.com/WordPress/openverse">Openverse</a>)</li>
<li><a href="https://github.com/zenml-io/zenml">ZenML</a></li>
<li><a href="https://github.com/zulip/zulip">Zulip</a></li>
<li><a href="https://github.com/pypa/build">build (PyPA)</a></li>
<li><a href="https://github.com/pypa/cibuildwheel">cibuildwheel (PyPA)</a></li>
<li><a href="https://github.com/delta-io/delta-rs">delta-rs</a></li>
<li><a href="https://github.com/alteryx/featuretools">featuretools</a></li>
<li><a href="https://github.com/mesonbuild/meson-python">meson-python</a></li>
<li><a href="https://github.com/wntrblm/nox">nox</a></li>
<li><a href="https://github.com/pypa/pip">pip</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Show Your Support</h3><a id="user-content-show-your-support" aria-label="Permalink: Show Your Support" href="#show-your-support"></a></p>
<p dir="auto">If you're using Ruff, consider adding the Ruff badge to your project's <code>README.md</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)"><pre><span>[</span><span>![</span>Ruff<span>]</span><span>(</span><span>https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json</span><span>)]</span><span>(</span><span>https://github.com/astral-sh/ruff</span><span>)</span></pre></div>
<p dir="auto">...or <code>README.rst</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content=".. image:: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json
    :target: https://github.com/astral-sh/ruff
    :alt: Ruff"><pre>.. <span>image</span>:: <span>https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json</span>
    <span>:target:</span> <span>https://github.com/astral-sh/ruff</span>
    <span>:alt:</span> <span>Ruff</span></pre></div>
<p dir="auto">...or, as HTML:</p>
<div dir="auto" data-snippet-clipboard-copy-content="<a href=&quot;https://github.com/astral-sh/ruff&quot;><img src=&quot;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&quot; alt=&quot;Ruff&quot; style=&quot;max-width:100%;&quot;></a>"><pre><span>&lt;</span><span>a</span> <span>href</span>="<span>https://github.com/astral-sh/ruff</span>"<span>&gt;</span><span>&lt;</span><span>img</span> <span>src</span>="<span>https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json</span>" <span>alt</span>="<span>Ruff</span>" <span>style</span>="<span>max-width:100%;</span>"<span>&gt;</span><span>&lt;/</span><span>a</span><span>&gt;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License<a id="user-content-license"></a></h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This repository is licensed under the <a href="https://github.com/astral-sh/ruff/blob/main/LICENSE">MIT License</a></p>
<p><a href="https://astral.sh/" rel="nofollow">
    <img src="https://raw.githubusercontent.com/astral-sh/ruff/main/assets/svg/Astral.svg" alt="Made by Astral">
  </a>
</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Elon Gives Nazi Salute During Inauguration Speech (118 pts)]]></title>
            <link>https://www.youtube.com/watch?v=e2bbb-6Clhs</link>
            <guid>42774621</guid>
            <pubDate>Mon, 20 Jan 2025 23:53:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=e2bbb-6Clhs">https://www.youtube.com/watch?v=e2bbb-6Clhs</a>, See on <a href="https://news.ycombinator.com/item?id=42774621">Hacker News</a></p>
Couldn't get https://www.youtube.com/watch?v=e2bbb-6Clhs: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse engineering Call of Duty anti-cheat (320 pts)]]></title>
            <link>https://ssno.cc/posts/reversing-tac-1-4-2025/</link>
            <guid>42774221</guid>
            <pubDate>Mon, 20 Jan 2025 23:07:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ssno.cc/posts/reversing-tac-1-4-2025/">https://ssno.cc/posts/reversing-tac-1-4-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=42774221">Hacker News</a></p>
Couldn't get https://ssno.cc/posts/reversing-tac-1-4-2025/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Elon Musk appears to make back-to-back fascist salutes at inauguration rally (198 pts)]]></title>
            <link>https://www.theguardian.com/technology/2025/jan/20/trump-elon-musk-salute</link>
            <guid>42773778</guid>
            <pubDate>Mon, 20 Jan 2025 22:21:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/2025/jan/20/trump-elon-musk-salute">https://www.theguardian.com/technology/2025/jan/20/trump-elon-musk-salute</a>, See on <a href="https://news.ycombinator.com/item?id=42773778">Hacker News</a></p>
Couldn't get https://www.theguardian.com/technology/2025/jan/20/trump-elon-musk-salute: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Matt Mullenweg, Automattic's CEO, Seems Bound and Determined to Wreck WordPress (110 pts)]]></title>
            <link>https://digitalcxo.com/article/matt-mullenweg-automattics-ceo-seems-bound-and-determined-to-wreck-wordpress/</link>
            <guid>42773311</guid>
            <pubDate>Mon, 20 Jan 2025 21:32:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://digitalcxo.com/article/matt-mullenweg-automattics-ceo-seems-bound-and-determined-to-wreck-wordpress/">https://digitalcxo.com/article/matt-mullenweg-automattics-ceo-seems-bound-and-determined-to-wreck-wordpress/</a>, See on <a href="https://news.ycombinator.com/item?id=42773311">Hacker News</a></p>
Couldn't get https://digitalcxo.com/article/matt-mullenweg-automattics-ceo-seems-bound-and-determined-to-wreck-wordpress/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Did Elon Musk Appear to Sieg Heil at Trump Inauguration? (239 pts)]]></title>
            <link>https://www.jpost.com/international/article-838444</link>
            <guid>42772995</guid>
            <pubDate>Mon, 20 Jan 2025 21:02:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jpost.com/international/article-838444">https://www.jpost.com/international/article-838444</a>, See on <a href="https://news.ycombinator.com/item?id=42772995">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            
<section>
	<section>
		


    <section>
        <h2>Musk was seen making the gesture a total of three times on live television.</h2>
    </section>

<section>
            <section>
                <section>
                    <time datetime="2025-01-20T22:29:54+02:00">
                        JANUARY 20, 2025 22:29
                    </time>
                </section>
                    <section><b>Updated:</b> JANUARY 20, 2025 23:30</section>
            </section>
            <section data-share-url="https://www.jpost.com/international/article-838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                    <nav>
                        <ul>
                            <li data-share-url="https://www.jpost.com/international/article-838444#838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                                
                            </li>
                            <li data-share-url="https://www.jpost.com/international/article-838444#838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                                
                            </li>
                            <li data-share-url="https://www.jpost.com/international/article-838444#838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                                
                            </li>
                            <li data-share-url="https://www.jpost.com/international/article-838444#838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                                
                            </li>
                        </ul>
                    </nav>
                </section>
        </section>


	</section>
	<section>
			<figure>
				<img src="https://images.jpost.com/image/upload/q_auto/c_fill,g_faces:center,h_537,w_822/644997" width="290" height="260" alt=" Elon Musk makes controversial gesture at Washington DC arena (photo credit: SCREENSHOT/X)" title=" Elon Musk makes controversial gesture at Washington DC arena">
				<figcaption>
					<section>
						<section> Elon Musk makes controversial gesture at Washington DC arena</section>
						<section>(photo credit: SCREENSHOT/X)</section>
					</section>
				</figcaption>
			</figure>
	</section>

</section>
<section>
			
			<section itemprop="articleBody" id="startBannerSticky">
				<p>US billionaire <a href="https://www.jpost.com/tags/elon-musk">Elon Musk</a> appeared to make a Heil Hitler salute at the Washington DC Trump parade on Monday, following Trump's inauguration.&nbsp;</p><p><a href="https://www.jpost.com/middle-east/article-837785">Musk</a> was seen making the gesture a total of three times on live television.</p><blockquote data-media-max-width="560"><p lang="en" dir="ltr">Elon Musk does what looks like a Hitler salute after talking of victory at Trump inauguration, thanking supporters for assuring "the future of civilisation" <a href="https://t.co/xp0kmJ5dFQ" rel="nofollow">pic.twitter.com/xp0kmJ5dFQ</a></p>‚Äî James Jackson (@derJamesJackson) <a href="https://twitter.com/derJamesJackson/status/1881436166144262376?ref_src=twsrc%5Etfw" rel="nofollow">January 20, 2025</a></blockquote><p>He then appeared on stage at the Capital One Area in front of 20,000 Trump supporters, where he thanked supporters before making the gesture.</p><blockquote><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/Breaking?src=hash&amp;ref_src=twsrc%5Etfw" rel="nofollow">#Breaking</a>: Senior Trump administration official Elon Musk thanks supporters with a Nazi salute. <a href="https://t.co/WzSZFUYvEG" rel="nofollow">pic.twitter.com/WzSZFUYvEG</a></p>‚Äî Noga Tarnopolsky ◊†◊í◊î ◊ò◊®◊†◊ï◊§◊ï◊ú◊°◊ß◊ô ŸÜŸàÿ∫ÿß ÿ™ÿ±ŸÜŸàÿ®ŸàŸÑÿ≥ŸÉŸä (@NTarnopolsky) <a href="https://twitter.com/NTarnopolsky/status/1881436487558090831?ref_src=twsrc%5Etfw" rel="nofollow">January 20, 2025</a></blockquote><p>Social media users reacted with horror, with one writing, "Remember when Democrats called MAGA rallies "Nazi rallies?" President un-elect Elon Musk just did the Nazi Sieg Heil salute."</p><h3>Mars space travel</h3><p>The Tesla and Space X owner appeared excited by Trump's mention of Mars in his inaugural speech, given he has reportedly urged NASA to drop its plans to return to the moon and go straight to Mars, according to Politico.</p><p>President Donald Trump said the US would launch astronauts to plant the ‚Äústars and stripes‚Äù on Mars.</p><p>"We're gonna take DOGE to Mars!" said Musk in his speech, "Can you imagine how awesome it will be to have American astronauts plant the flag on another planet for the first time! How inspiring would that be?!"</p><section><hr><div>            <p>Stay updated with the latest news!</p>            <p>Subscribe to The Jerusalem Post Newsletter</p>        </div><hr></section><p>This comes amid a Washington Post report that Donald Trump's government <a href="https://www.jpost.com/american-politics/article-837770">advisory panel</a>, led by Elon Musk, will be sued soon after the incoming US president is sworn in on Monday.
				</p>
			</section>
		</section>



            
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Official DeepSeek R1 Now on Ollama (178 pts)]]></title>
            <link>https://ollama.com/library/deepseek-r1</link>
            <guid>42772983</guid>
            <pubDate>Mon, 20 Jan 2025 21:00:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ollama.com/library/deepseek-r1">https://ollama.com/library/deepseek-r1</a>, See on <a href="https://news.ycombinator.com/item?id=42772983">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<div>
		
		<div>
			
<div id="summary">
	<h2 id="summary-display">
		<span id="summary-content">
		
			DeepSeek's first generation reasoning models with comparable performance to OpenAI-o1. 
		
		</span>
		
	</h2>
	
</div>

			<div>
				<p><span x-test-size="">1.5b</span>
					
						<span x-test-size="">7b</span>
					
						<span x-test-size="">8b</span>
					
						<span x-test-size="">14b</span>
					
						<span x-test-size="">32b</span>
					
						<span x-test-size="">70b</span>
					
						<span x-test-size="">671b</span>
					
				</p>
				<p>
					
					  <span>
						<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor">
						  <path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 005.25 21h13.5A2.25 2.25 0 0021 18.75V16.5M16.5 12L12 16.5m0 0L7.5 12m4.5 4.5V3"></path>
						</svg>
						<span x-test-pull-count="">40.4K</span>
						<span>&nbsp;Pulls</span>
					  </span>
					
					
						<span>
							<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor">
								<path stroke-linecap="round" stroke-linejoin="round" d="M12 6v6h4.5m4.5 0a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z"></path>
							</svg>
							<span>Updated&nbsp;</span>
							<span x-test-updated="">2 hours ago</span>
						</span>
					
				</p>
			</div>
		</div>
		
		
  



		
	</div>
	
  
    
  

  
  
  <div id="readme">
    <p>
      <h2>Readme</h2>

      
    </p>
    <div id="display">
        
          <p><img src="https://ollama.com/assets/library/deepseek-v3/069ccc94-63b0-41e6-b2b3-e8e56068ab1a" width="320"></p>

<p>DeepSeek‚Äôs first-generation reasoning models, achieving performance comparable to OpenAI-o1 across math, code, and reasoning tasks.</p>

<h2>Models</h2>

<p><strong>1.5B Qwen DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:1.5b
</code></pre>

<p><strong>7B Qwen DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:7b
</code></pre>

<p><strong>8B Llama DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:8b
</code></pre>

<p><strong>14B Qwen DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:14b
</code></pre>

<p><strong>32B Qwen DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:32b
</code></pre>

<p><strong>70B Llama DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:70b
</code></pre>

<p><strong>671B DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:671b
</code></pre>

<p><img src="https://ollama.com/assets/library/deepseek-r1/e44d096e-fa46-4cae-b2f2-53991e8c8da0" alt="deepseek"></p>

        
      </div>
    
  </div>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Authors seek Meta's torrent client logs and seeding data in AI piracy probe (137 pts)]]></title>
            <link>https://torrentfreak.com/authors-seek-metas-torrent-client-logs-and-seeding-data-in-ai-piracy-probe-250120/</link>
            <guid>42772771</guid>
            <pubDate>Mon, 20 Jan 2025 20:38:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/authors-seek-metas-torrent-client-logs-and-seeding-data-in-ai-piracy-probe-250120/">https://torrentfreak.com/authors-seek-metas-torrent-client-logs-and-seeding-data-in-ai-piracy-probe-250120/</a>, See on <a href="https://news.ycombinator.com/item?id=42772771">Hacker News</a></p>
Couldn't get https://torrentfreak.com/authors-seek-metas-torrent-client-logs-and-seeding-data-in-ai-piracy-probe-250120/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[ROCm Device Support Wishlist (163 pts)]]></title>
            <link>https://github.com/ROCm/ROCm/discussions/4276</link>
            <guid>42772170</guid>
            <pubDate>Mon, 20 Jan 2025 19:31:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ROCm/ROCm/discussions/4276">https://github.com/ROCm/ROCm/discussions/4276</a>, See on <a href="https://news.ycombinator.com/item?id=42772170">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

      <div data-body-version="299b9ab50a702b61e654a07387c220b1fb5c6d12f8237f4cbfff86784c2f2b0b" data-error="" id="discussioncomment-11894448" data-gid="DC_kwDOAzpr8c4AtX6w" data-url="/ROCm/ROCm/discussions/4276/comments/11894448" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
        



      <div>
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11894448/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Honestly, anything that has 16GB VRAM or more (or the ability to have reserved more, for eg. the iGPUs like 680/780/890M and Strix Halo iGPUs).</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-07e6d4f2-e873-4340-a7b4-cc09177208fa" for="discussion-upvote-button-DiscussionComment-11894448" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    1 reply
                  </span>
                </p>
            </div>

        </div>
          <div data-child-comments="" id="child-comments-discussioncomment-11894448">
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11896093,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="738f5e1a0c79912c63b83622201031e6c8db67ed112aea719ab7d413def28e9d" data-hovercard-type="user" data-hovercard-url="/users/niklassheth/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/niklassheth"><img src="https://avatars.githubusercontent.com/u/20130217?s=60&amp;v=4" width="30" height="30" alt="@niklassheth"></a></p>

        <div data-body-version="3366d9f7e19e5e556f43df8081525bcb532d9434f2a1a325fd67425a3e1fc6c8" id="discussioncomment-11896093" data-gid="DC_kwDOAzpr8c4AtYUd" data-url="/ROCm/ROCm/discussions/4276/comments/11896093" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896093/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">As evidence for this, you can look at the used price of GPUs. Even an 8 year old P40 is going for $300 on eBay because it has 24GB VRAM. If the MI60 was supported it could be a good budget option.</p>
    </div>
    
</task-lists>

          

        </div>

    </div>


        
    </div>
  <div data-body-version="9a4ede9d2f426b2cac5c97c9d588b433e455df2f2d11f82b00ab42d18dfbcc2d" data-error="" id="discussioncomment-11894747" data-gid="DC_kwDOAzpr8c4AtX_b" data-url="/ROCm/ROCm/discussions/4276/comments/11894747" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11894747/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I would like support for ROCm to be restored to all the relatively recent GPUs (last 5-6 years) AMD has released and then dropped ROCm support for.  New I could not care much about. Actually supporting the AMD cards I bought in the past would be great.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-46052c64-2b58-4120-a6cf-206ce13fbac7" for="discussion-upvote-button-DiscussionComment-11894747" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="9eab11215d5ae982b020b1b9ab57bf9509331d016278ec1083667515affd030d" data-error="" id="discussioncomment-11894841" data-gid="DC_kwDOAzpr8c4AtYA5" data-url="/ROCm/ROCm/discussions/4276/comments/11894841" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11894841/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I think it might be interesting to share here that Debian has built a CI at <a href="https://ci.rocm.debian.net/" rel="nofollow">ci.rocm.debian.net</a> where the ROCm stack, and any package that depends on it, is continuously tested. Our CI includes all of the architectures listed above.</p>
<p dir="auto">We would be happy to cooperate on increasing service support for Debian and derivatives.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-66903d35-f63f-4fec-8302-b6b1fe0fab74" for="discussion-upvote-button-DiscussionComment-11894841" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="baf615a896f6281ae4dce8c4e2704158995d26e3d65d562d075cbd45095fcb3f" data-error="" id="discussioncomment-11894876" data-gid="DC_kwDOAzpr8c4AtYBc" data-url="/ROCm/ROCm/discussions/4276/comments/11894876" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
        



      <div>
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11894876/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">This is not a "device" support wish, but a "platform" one. Stable Diffusion on native Windows with AMD GPUs is not possible until we get "Windows" support for "AI Libraries" (specifically MIOpen) here: <a href="https://rocm.docs.amd.com/projects/install-on-windows/en/develop/reference/component-support.html" rel="nofollow">https://rocm.docs.amd.com/projects/install-on-windows/en/develop/reference/component-support.html</a></p>
<p dir="auto">This is required to get PyTorch working. I've seen so many AMD users in recent times selling their AMD GPUs and buying "the competition" because WSL and ZLUDA are their only options, and those are half-baked solutions. Native Windows support should be a top priority.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-f247b436-6275-427f-8fa9-e125513f1e76" for="discussion-upvote-button-DiscussionComment-11894876" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    1 reply
                  </span>
                </p>
            </div>

        </div>
          <div data-child-comments="" id="child-comments-discussioncomment-11894876">
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895073,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="1e8bb9a91b6ac1e121d7c4a51111e736de21a302b5d916fa108e8f7b88b11dcd" data-hovercard-type="user" data-hovercard-url="/users/tocram1/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/tocram1"><img src="https://avatars.githubusercontent.com/u/22381620?s=60&amp;v=4" width="30" height="30" alt="@tocram1"></a></p>

        <div data-body-version="59dcaa13ac84895a8d972d45bb76f53ce6fdd795fa4f412208b6ec88f5bb79e3" id="discussioncomment-11895073" data-gid="DC_kwDOAzpr8c4AtYEh" data-url="/ROCm/ROCm/discussions/4276/comments/11895073" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895073/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Definitely agree on this one, this is a major hurdle in MY opinion.</p>
    </div>
    
</task-lists>

          

        </div>

    </div>


        
    </div>
  <div data-body-version="75fc6a10eb474b0ac38ca75b93996824dad488c2c5d63281ce5943635eb31b55" data-error="" id="discussioncomment-11894914" data-gid="DC_kwDOAzpr8c4AtYCC" data-url="/ROCm/ROCm/discussions/4276/comments/11894914" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
        



      <div>
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11894914/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">A bit older than the ones listed there, but I own a 5700XT, and a good few other people do too, from my extensive looking for how to get it to work online.</p>
<p dir="auto">Still holding on to the precompiled wheel for torch 1.13 ROCM 5.2 for Python 3.10 which is the last one that works (after setting HSA_OVERRIDE_GFX_VERSION). Later versions seem to either outright crash, or import correctly but then crash when a tensor is sent to the GPU.</p>
<p dir="auto">Using this older version as a workaround was doable back when torch 2.0 was new, but now as most new code has already been using 2.0+ for a while, it's effectively not functional at all anymore for any recently written code.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-3e032699-4bf6-4370-b1d3-6ddd7baaaec0" for="discussion-upvote-button-DiscussionComment-11894914" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    1 reply
                  </span>
                </p>
            </div>

        </div>
          <div data-child-comments="" id="child-comments-discussioncomment-11894914">
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895908,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="fb945d44ec154bf7c9a296dc4c0adfd92652dd919a443c392fae053fa4dab4bc" data-hovercard-type="user" data-hovercard-url="/users/SicLuceatLux/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/SicLuceatLux"><img src="https://avatars.githubusercontent.com/u/43041463?s=60&amp;v=4" width="30" height="30" alt="@SicLuceatLux"></a></p>

        

    </div>


        
    </div>
  <div data-body-version="0c261b8b353a6a30cb9a1d99f88e3f6d4a9c1043e5f1ccd22425a35a36cedb04" data-error="" id="discussioncomment-11894998" data-gid="DC_kwDOAzpr8c4AtYDW" data-url="/ROCm/ROCm/discussions/4276/comments/11894998" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11894998/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Considering my GPU( 6600 XT)  was released near the end of 2021 it would be nice to know that I don't need to buy a new GPU every year just to have support. It would also be nice to have actual proper Windows support instead of having to deal with the clusterfuck that is Zluda, or other translation layers. This kind of treatment from AMD is why I'll probably go nvidia the next time my budget allows it.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-563ac12c-b2cc-4200-86b1-8876cb9400ce" for="discussion-upvote-button-DiscussionComment-11894998" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="a4116625bd825f160374a9f736bc058290ceb23dd78775a84ba17d56344fe604" data-error="" id="discussioncomment-11895157" data-gid="DC_kwDOAzpr8c4AtYF1" data-url="/ROCm/ROCm/discussions/4276/comments/11895157" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
        



      <div>
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895157/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">APU support opens the door for introducing this software to a wide audience, please consider hitting the entire APU line (3 and 3.5.) Early ROCm worked for 780m and got me in the front door of working with this software at all (that said I had to use env var hacks to get it functioning). Later versions of ROCm stopped working at all.</p>
<p dir="auto">The hobbyist crowd would greatly benefit from APU support, which hopefully has the AMD financial incentive of market share and product familiarity (hobbyist engineers who do something neat at home and then bring the concepts to work, where you then pick up the larger purchases)</p>
<p dir="auto">If I was able to feel confident in better consumer ROCm support I would have gladly dropped money for 2 AMD graphics cards for the LLM stuff I do.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-cc9cfe79-f4fb-44e8-b72e-684504465400" for="discussion-upvote-button-DiscussionComment-11895157" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    2 replies
                  </span>
                </p>
            </div>

        </div>
          <div data-child-comments="" id="child-comments-discussioncomment-11895157">

    <div>
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895323,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="08551d753a51e5999ddb8a5a0c88a89d7354468fcdbedea428c9cbc91efbed94" data-hovercard-type="user" data-hovercard-url="/users/randomstuff/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/randomstuff"><img src="https://avatars.githubusercontent.com/u/946727?s=60&amp;v=4" width="30" height="30" alt="@randomstuff"></a></p>

        <div data-body-version="7d32692793bbb76ac2be4d90993111340635b8e9c7da65be969ea9fcca74dc4e" id="discussioncomment-11895323" data-gid="DC_kwDOAzpr8c4AtYIb" data-url="/ROCm/ROCm/discussions/4276/comments/11895323" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895323/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I have been somewhat successfully been able to run Stable Diffusion on a <a href="https://www.gabriel.urdhr.fr/2022/08/28/trying-to-run-stable-diffusion-on-amd-ryzen-5-5600g/" rel="nofollow">AMD Ryzen 5 5600G</a> APU (with 16 Go / 32 Go of RAM).</p>
<p dir="auto">The performance speedup was not super impressive compared to the same implementation executed on CPU or the OpenVINO CPU implementation. I am not sure if we could get a nice speedup for this kind of device but it would be really interesting.</p>
<p dir="auto">The thing was highly unstable and had a tendency of crashing the whole system real quick. Anyway, it was nearly working for some programs. Si maybe there is not so many things missing to a have something stable :)</p>
    </div>
    
</task-lists>

          

        </div>

    </div>
    <div>
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895573,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="32cfbb4ae281dd01619117b092b54359b030ceebe3fa78fae5d7a1d00c8e3af1" data-hovercard-type="user" data-hovercard-url="/users/Abdull/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/Abdull"><img src="https://avatars.githubusercontent.com/u/529862?s=60&amp;v=4" width="30" height="30" alt="@Abdull"></a></p>

        <div data-body-version="b0ed72ab4fe39ad8c54bf1ce3679bf025c35489ed37cf82d6366f4c770e98616" id="discussioncomment-11895573" data-gid="DC_kwDOAzpr8c4AtYMV" data-url="/ROCm/ROCm/discussions/4276/comments/11895573" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895573/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">important for Framework 13 AMD laptop users</p>
    </div>
    
</task-lists>

          

        </div>

    </div>

</div>


        
    </div>
  <div data-body-version="6dd366079f09ed3314a27f573c24f7694472b1e9620886d0a44809818ef06c97" data-error="" id="discussioncomment-11895185" data-gid="DC_kwDOAzpr8c4AtYGR" data-url="/ROCm/ROCm/discussions/4276/comments/11895185" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
        



      <div>
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895185/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">why not just all, like the other company? ;-)</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-0cb80f69-16e9-4b33-bd3e-9e7a6375862a" for="discussion-upvote-button-DiscussionComment-11895185" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    3 replies
                  </span>
                </p>
            </div>

        </div>
          <div data-child-comments="" id="child-comments-discussioncomment-11895185">

    <div>
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895241,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="58150230df92bc1e3b1bc985b82360100bf1bdba1f722ceda041f8f23cb00282" data-hovercard-type="user" data-hovercard-url="/users/powderluv/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/powderluv"><img src="https://avatars.githubusercontent.com/u/74956?s=60&amp;v=4" width="30" height="30" alt="@powderluv"></a></p>

        <div data-body-version="94f3e3e23b56bb956c129e2c7dc34d254852eeea837fccfffe0c3067d22f4cf4" id="discussioncomment-11895241" data-gid="DC_kwDOAzpr8c4AtYHJ" data-url="/ROCm/ROCm/discussions/4276/comments/11895241" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895241/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Eventually. But what would you prioritize? :)</p>
    </div>
    
</task-lists>

          

        </div>

    </div>
    <div>
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895322,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="034fd912b967c7ac7fac064c34a1ccfb3958f66bd1dacdc08fc1105d3d840e37" data-hovercard-type="user" data-hovercard-url="/users/shiltian/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/shiltian"><img src="https://avatars.githubusercontent.com/u/7587318?s=60&amp;v=4" width="30" height="30" alt="@shiltian"></a></p>

        <div data-body-version="444c854734197b911f0b04229e1509352e0fc94e5bf147f1ee9568e88a92539d" id="discussioncomment-11895322" data-gid="DC_kwDOAzpr8c4AtYIa" data-url="/ROCm/ROCm/discussions/4276/comments/11895322" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895322/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">my .02 is starting from all RDNA 2 and newer. that's gonna take time. by the time they are all supported, RDNA 1 might have fairly faded out.</p>
    </div>
    
</task-lists>

          

        </div>

    </div>
    <div>
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11896209,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="94548708abd6a2498081a9869a81137e0e2ef62a4ed3bc113fce92c1c962dfae" data-hovercard-type="user" data-hovercard-url="/users/TKCZ/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/TKCZ"><img src="https://avatars.githubusercontent.com/u/22118472?s=60&amp;v=4" width="30" height="30" alt="@TKCZ"></a></p>

        <div data-body-version="c416c01b691f7b738b438424cc3d8c9266f848bdc999a364f68ea3f3048f1f14" id="discussioncomment-11896209" data-gid="DC_kwDOAzpr8c4AtYWR" data-url="/ROCm/ROCm/discussions/4276/comments/11896209" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896209/language_detections">
      
    </inline-machine-translation>
  <div>
        <blockquote>
<p dir="auto">Eventually. But what would you prioritize? :)</p>
</blockquote>
<p dir="auto">Definitelly start with RDNA2 &amp; 3, since these series offered models with 16GB VRAM. That much memory deserves to stay around officially supported for as long as possible. Let alone the fact that cards like 6800XT still pack decent punch for local AI inference.</p>
    </div>
    
</task-lists>

          

        </div>

    </div>

</div>


        
    </div>
  <div data-body-version="9f4dab7adc8c5e819850961f54e8c18019d30f85b4c1a707fd0cd3a15bac7c40" data-error="" id="discussioncomment-11895317" data-gid="DC_kwDOAzpr8c4AtYIV" data-url="/ROCm/ROCm/discussions/4276/comments/11895317" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895317/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">ROCm windows, All RDNA3 and newer. Don't forget integrated GPUs. Maybe next year?</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-9f39403a-251c-4a3f-ab44-50269333e5bd" for="discussion-upvote-button-DiscussionComment-11895317" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="7b94ee7e4a683ea60daf9aa12987a695462f6dfe1dc018b8560ecc2e80d02174" data-error="" id="discussioncomment-11895348" data-gid="DC_kwDOAzpr8c4AtYI0" data-url="/ROCm/ROCm/discussions/4276/comments/11895348" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895348/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I wish for AMD to look back at the RX 500  and the RX5000 series. And the reason being, the physical architectures for both lend themselves to really interesting compute because the RX 580 architecturally is very good to use as a modular scale up and scale down at 75 w. And based based on some back of the napkin maths that I've done an RX 580 8 gig with a 8 billion parameter model with a quant size of eight. Can pull about 15 to 30 tokens per second.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-6713e9db-8776-42b2-ab7a-134f822bdcb7" for="discussion-upvote-button-DiscussionComment-11895348" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>


          <div data-body-version="5456848d6531fe9f1f551b6f7229a70b9d12cac492941d4f1a0e1ed720aec788" data-error="" id="discussioncomment-11895367" data-gid="DC_kwDOAzpr8c4AtYJH" data-url="/ROCm/ROCm/discussions/4276/comments/11895367" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895367/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Hi,<br>
IMO all products going forward should be able to run all typical ML software: stable diffusion, LLMs, pytorch.<br>
Doesn't have to be crazy fast, but support it and then improve it over time.<br>
And simultaneously, but fine if at a lower pace, walk backwards and support the older products.</p>
<p dir="auto">So you should start by supporting strix halo and RDNA 4. Then RDNA 3 and prior APUs.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-c27855b0-7ef2-48ff-ba05-4cba14af7fb3" for="discussion-upvote-button-DiscussionComment-11895367" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="368d8a87f1f3eb9b8da0616cf95aca6b54ce9e0833bc0ce5dc74a2cb880d16d2" data-error="" id="discussioncomment-11895435" data-gid="DC_kwDOAzpr8c4AtYKL" data-url="/ROCm/ROCm/discussions/4276/comments/11895435" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895435/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Thank you for reaching out and at least trying to extend the device support. The limited consumer hardware support has always been one of the weakest point of ROCm, and if AMD is serious about the future of ROCm, at least any upcoming hardware should be supported. Being able to get used to a platform without spending 1000s is actually huge.<br>
Currently, even if unsupported, many actually work fine. I did and I'm still doing some PyTorch stuff on a 8700GE, which has a gfx1103 GPU in it. Works, but there are some nasty minor issues like this one here, causing the GPU driver to crash now and then, seemingly a firmware issue: <a data-error-text="Failed to load title" data-id="2474223008" data-permission-text="Title is private" data-url="https://github.com/lamikr/rocm_sdk_builder/issues/141" data-hovercard-type="issue" data-hovercard-url="/lamikr/rocm_sdk_builder/issues/141/hovercard" href="https://github.com/lamikr/rocm_sdk_builder/issues/141">lamikr/rocm_sdk_builder#141</a><br>
I think, it's a rather small step for AMD to make those 99% working devices to a 100% officially supported level.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-57cb16f5-c3d1-498c-9a2e-e5f430664f0a" for="discussion-upvote-button-DiscussionComment-11895435" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="054686a2c43c6262a1f6b4938167d23622b1aa0a11c76249c06691a6e298954a" data-error="" id="discussioncomment-11895444" data-gid="DC_kwDOAzpr8c4AtYKU" data-url="/ROCm/ROCm/discussions/4276/comments/11895444" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895444/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">At one time Kaveri was promoted as a hybrid processor, and while HSA was being implemented its support disappeared. It would be fair, given the promises of marketers, to make HSA + ROCm for APU Kaveri.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-356e5a80-3267-4c1b-acbd-0ada8783843b" for="discussion-upvote-button-DiscussionComment-11895444" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="313f54341a67795a6b3549078c47509a3bbe9a9250b1711d454fc3dfb6b44323" data-error="" id="discussioncomment-11895539" data-gid="DC_kwDOAzpr8c4AtYLz" data-url="/ROCm/ROCm/discussions/4276/comments/11895539" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
        



      <div>
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895539/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Missing poll option: actually support the ‚úÖ marked devices consistently</p>
<p dir="auto">There's not much point having a green icon in the support matrix if it doesn't mean your device is supported.<br>
aotriton supports only MI2xx, MI3xx, 7800, 7900.<br>
hipblaslt supports only MI2xx, MI3xx, 7800, 7900.<br>
Dao-AILab/flash-attention (which AMD contributed ROCm support to) doesn't support MI100. I think it's picky about consumer cards too but don't remember the models.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-37226189-31a4-4fad-bedf-c579ecabd45d" for="discussion-upvote-button-DiscussionComment-11895539" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    1 reply
                  </span>
                </p>
            </div>

        </div>
          <div data-child-comments="" id="child-comments-discussioncomment-11895539">
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895810,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="76750ea7a41fa5e9e9b1ea2e04c74ab5908b60b290bbeac03e20dcfe27e8f1e8" data-hovercard-type="user" data-hovercard-url="/users/IMbackK/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/IMbackK"><img src="https://avatars.githubusercontent.com/u/13803414?s=60&amp;v=4" width="30" height="30" alt="@IMbackK"></a></p>

        <div data-body-version="1b50beb7542cde7184da8ea2e0afb47d7f62bdd41426fc550e5676f7b5cc55bf" id="discussioncomment-11895810" data-gid="DC_kwDOAzpr8c4AtYQC" data-url="/ROCm/ROCm/discussions/4276/comments/11895810" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895810/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Yeah this is the big thing. Besides missing rdna1 support, the actual support matrix (not the mostly useless check mark, what the code supports) is mostly fine, except random libraries that then support only a subset of those.</p>
    </div>
    
</task-lists>

          

        </div>

    </div>


        
    </div>
  <div data-body-version="96b624a0db8669f4d32a09be017348010050de83168e06266ca1d3088deec029" data-error="" id="discussioncomment-11895566" data-gid="DC_kwDOAzpr8c4AtYMO" data-url="/ROCm/ROCm/discussions/4276/comments/11895566" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895566/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto"><strong>Chicken and egg.</strong>  I don't own any of your hardware because I can't run the software I want to run. Support Flux, Stable Diffusion, Tencent Hunyuan Video, Nvidia Cosmos, and I'll be in the market to buy your cards. As it stands, I can't leverage your offering.  I'll gladly build AMD workstations if your hardware can do the things I want.</p>
<p dir="auto"><strong>Prioritize VRAM.</strong> The next battle is local image and video models. Nobody wants to use hosted SaaS and all the film and VFX people will be running local Comfy, Hunyuan, etc. in just a few years time. These models need a tremendous amount of VRAM, so you need to build consumer/prosumer SKUs that have it.</p>
<p dir="auto">If you time this right and build high VRAM consumer cards with broad software support, the next generation of media production could ride on your platform.</p>
<p dir="auto">Let me underscore: you <em>must</em> be able to run popular image and video models.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-8d82a3f8-78d9-4455-b7b6-76c39c74bb48" for="discussion-upvote-button-DiscussionComment-11895566" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>

    



      <div data-body-version="09ee47da6431ac30b79faf5af02f101bfd50d483cfdbaa7b784b56efa45bee11" data-error="" id="discussioncomment-11895881" data-gid="DC_kwDOAzpr8c4AtYRJ" data-url="/ROCm/ROCm/discussions/4276/comments/11895881" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895881/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Long term support for APU's with a lot of ram and usable performance gets a bunch of developers to try things quickly and builds confidence in shipping to enterprise hardware in prod. Low power, high ram, usable performance and out of the box support for all the major tools with NO BUGS. Learn from Ballmer: Developers developers developers developers....</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-83513683-c4a5-4452-abc1-18d0b13407ca" for="discussion-upvote-button-DiscussionComment-11895881" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="429a8fd23a83a0f1f55e3bf56e326d8bab30e9ebc9e65a0897192fbad18c0115" data-error="" id="discussioncomment-11895947" data-gid="DC_kwDOAzpr8c4AtYSL" data-url="/ROCm/ROCm/discussions/4276/comments/11895947" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895947/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Basically, anything with more than 16GB of RAM. APUs capable of using system RAM could be a cost-effective entry-level option for running large language models (LLMs). While they might be slower, many people don‚Äôt require real-time LLM output.</p>
<p dir="auto">Everyone wants to run large models, but not everyone can afford to spend several thousand dollars on professional GPUs, multiple high-end prosumer cards, or Macs.</p>
<p dir="auto">The ROCm user base could grow rapidly if people could leverage their existing iGPUs for AI tasks. I wish my Raven Ridge APU could handle slow AI tasks, like sorting and tagging photos on my Nextcloud NAS, and similar applications.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-fe79600e-9a2d-4241-a4ab-f13eb5a7ffb9" for="discussion-upvote-button-DiscussionComment-11895947" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="60b2d147161c5e686c514dae9cdf1e582e98a78493f59d64b1479ae579d9bced" data-error="" id="discussioncomment-11895995" data-gid="DC_kwDOAzpr8c4AtYS7" data-url="/ROCm/ROCm/discussions/4276/comments/11895995" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895995/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">xbox and playstation, period.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-cc72407d-7745-4740-9da5-a4657fe9a2e1" for="discussion-upvote-button-DiscussionComment-11895995" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="db5020ede0a87cb441e4c80ee05bdc1b63cb1628b9c9cc55d6023f527156673e" data-error="" id="discussioncomment-11896000" data-gid="DC_kwDOAzpr8c4AtYTA" data-url="/ROCm/ROCm/discussions/4276/comments/11896000" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896000/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Strix Halo and Phoenix APUs could possibly be the most popular so definitely them but full support for the 7600xt and better seems proper. including the 6000 series equivalents. I feel the 7600xt's appeal over a regular 7600 is ROCm more than gaming</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-18baf3d8-268d-4841-b5a6-4d5f4b20a7ef" for="discussion-upvote-button-DiscussionComment-11896000" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="4eb0d589f42b297099933e81c73b825739045177384b8a92ac25acf48779bcd9" data-error="" id="discussioncomment-11896009" data-gid="DC_kwDOAzpr8c4AtYTJ" data-url="/ROCm/ROCm/discussions/4276/comments/11896009" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896009/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">You have a much wider reach with iGPUs than with dGPUs, so I wouldn't stop supporting RDNA and Vega which AMD still sells widely.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-b440b199-229e-4756-9ee4-082580964130" for="discussion-upvote-button-DiscussionComment-11896009" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="6389b831651f10398b1800c0693ba58bd996c1ccf901d9d557f998797699a33e" data-error="" id="discussioncomment-11896170" data-gid="DC_kwDOAzpr8c4AtYVq" data-url="/ROCm/ROCm/discussions/4276/comments/11896170" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896170/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">For Windows and WSL users, shouldn't using DirectML be an option? I may be wrong here though</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-b8760a12-1b9b-4244-80af-60d64fe749e7" for="discussion-upvote-button-DiscussionComment-11896170" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="0d511713bb5eb3f4b1179446b5e95932e1f2401f25e9c3f9e8a403a21d5d94a4" data-error="" id="discussioncomment-11896197" data-gid="DC_kwDOAzpr8c4AtYWF" data-url="/ROCm/ROCm/discussions/4276/comments/11896197" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896197/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Would like to see the the W7600 and W7700 supported - small memory, but single-slot! Very useful for compact builds, or if more PCIe cards are needed. I've combined these with Mellanox NICs and Highpoint HBAs.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-94216dac-1578-46d2-93fa-aeeb95d1e4dd" for="discussion-upvote-button-DiscussionComment-11896197" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="ad8da41e9c92f907c38045f5d6e2ef421dcf89ded01a6affa6baecf8d968ea73" data-error="" id="discussioncomment-11896210" data-gid="DC_kwDOAzpr8c4AtYWS" data-url="/ROCm/ROCm/discussions/4276/comments/11896210" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896210/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">All of them. GCN 4 and up if I had to be hard-pressed into an answer.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-38d3cb38-b7e2-4786-9aef-131a59a9de5e" for="discussion-upvote-button-DiscussionComment-11896210" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="27e881f62dfca99403d134c3956b66d37cf90170fc599f4e5ea75120bf6d47f9" data-error="" id="discussioncomment-11896220" data-gid="DC_kwDOAzpr8c4AtYWc" data-url="/ROCm/ROCm/discussions/4276/comments/11896220" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896220/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">It's important to note that "Support" means wildly different things to different people.</p>
<ul dir="auto">
<li>The compiler knows how to emit code for the GPU</li>
<li>The driver knows how to load code onto / allocate memory etc for the GPU</li>
<li>The libraries have been compiled for that GPU (so can actually run)</li>
<li>The libraries have GPU-specific optimisations implemented (i.e. are faster)</li>
<li>The ROCm release process tests / validates on that GPU</li>
<li>The various CI systems run on that GPU</li>
<li>Variations on where tickets can be raised</li>
</ul>
<p dir="auto">I believe this is where things have gone south. The ROCm release testing / validation is thorough and somewhat linear in the number of GPUs tested on so adding more hardware to that set costs lots of time. The cards behave fairly similarly to one another so it's also not especially informative. However writing "Supported: Foo" without actually putting Foo through the same internal testing as all others seems problematic.</p>
<p dir="auto">What a decent fraction of people want is for ROCm to run on their gaming card(s). Whether it actually went through the internal validation is somewhat less important than it refusing to run entirely because the libraries haven't been compiled for their hardware.</p>
<p dir="auto">I think the solution to this is really obvious. Build the userspace software for all the targets. Let "Supported" continue to mean whatever it currently does. That'll mean people can install the ROCm distribution and get something which at least attempts to run on their hardware. Optionally introduce a new term, whatever marketing like, to refer to the hardware that isn't on the supported list.</p>
<p dir="auto">I personally don't care at all whether the latest ROCm release has been carefully tested on the graphics card I'm using locally but I'm really annoyed when it wasn't compiled for it.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-d959b2b9-1153-4707-957d-95847ec7801c" for="discussion-upvote-button-DiscussionComment-11896220" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="25a357989dad020ca60381a652abe8e51a10e3de2d0adeb53075b86de7328e0d" data-error="" id="discussioncomment-11896278" data-gid="DC_kwDOAzpr8c4AtYXW" data-url="/ROCm/ROCm/discussions/4276/comments/11896278" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896278/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I'll keep it short, please don't drop gfx906.</p>
<p dir="auto">and maybe what I'm asking for is too much given that they seem to strictly include officially supported, CDNA, GPUs, but it would be lovely if you could get support for consumer GPUs eg. gfx906 and up added to prebuilt docker images like vllm-dev/-ci.<br>
thank you guys, ROCm is getting better and better.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-c02a500b-e04e-4b67-b4a2-1e470920e860" for="discussion-upvote-button-DiscussionComment-11896278" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="67d6d7105dd537c0790422c11383d05f508346f2128ab90a3c4ac5f4d21e7c56" data-error="" id="discussioncomment-11896331" data-gid="DC_kwDOAzpr8c4AtYYL" data-url="/ROCm/ROCm/discussions/4276/comments/11896331" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896331/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I would like to see full-stack generational support on Linux with both the runtime and HIP SDK for consumer cards on RDNA[1|2|3] and then extending it to GCN where possible.</p>
<p dir="auto">At the moment AMD have structured [and in my opinion artificially limited] their compatibility matrices to the upper tiers of the hardware stacks, e.g. 6800 and up on RDNA2, preventing those who own 6750XT and lower stack cards from using ROCm/HIP.</p>
<p dir="auto">I understand that a lot of the unsupported hardware "just works" in some fashion but it is not officially supported nor documented as being supported or which components/libraries work and which do not - this is just bad all around as nobody will buy GPU hardware or buy into the ROCm software ecosystem without knowing before hand that the hardware is capable, the hardware feature set is fully supported in software and it is relatively easy to get up and running as an end user, this is especially bad from a PR perspective for AMD in wanting to get more people to use their hardware when it is restricted by hardware/pricing tiers and by the AMD software itself.</p>
<p dir="auto">Nobody is going to purchase a GPU in order to dip into trying out the ROCm ecosystem if they have to purchase the highest tiers of GPU to know for sure that ROCm will fully support the hardware.</p>
<p dir="auto">Artificial limits which segregate capable hardware through a lack of support in or a lack of enablement in software is not a good practice and it harms not only end-users/customers but AMD and ROCm adoption too.</p>
<p dir="auto">I would like to see AMD's stance on this change to one of starting from full enablement from the outset where the hardware features exist to do that and remove any artificial tiering/segregation entirely, tear down those walls.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-424addef-6d6f-44a3-9bda-3280ef11dedd" for="discussion-upvote-button-DiscussionComment-11896331" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="284b2343c73f6c317db7f14871d9dd54a3c4148d693f00cd831a70cc846e288d" data-error="" id="discussioncomment-11896340" data-gid="DC_kwDOAzpr8c4AtYYU" data-url="/ROCm/ROCm/discussions/4276/comments/11896340" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896340/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">You know, the reason AMD is way behind Nvidia in market share is because Nvidia literally supports every single discrete graphics processor model with CUDA from the low end to their high end.  They realized what AMD never has, and has continuously annoyed people, especially Linux users.  Not everyone writing software for CUDA has an ultra high end GPU product <em>because they can't afford them</em>.  But they have a very robust software development community where the underlying SDKs largely just works.  Intel has effectively realized the same thing with ARC.</p>
<p dir="auto">If AMD can't at least match the Linux support matrix with the Windows support matrix going forward you're rapidly going to grow completely irrelevant with new generations of software tinkerers who are primarily writing software on Linux. Windows from a compute performance point of view is demonstrably poor in comparison, and the user experience is degrading over time as Microsoft lets their OS foundation crumble while they're off chasing unicorns.</p>
<p dir="auto">What's my vote?  Every GPU currently under active driver support including Vega. Vega still has a fairly large-ish install base.  Arbitrary decisions to drop support may make business decisions, but it annoys customers who buy hardware only to have it dropped from support not long after it was still being sold.  But if you must narrow down your support matrix everything from the RX 5000 series and newer.  Either actually compete with the CUDA hardware support experience or get out of the game.  Those are your only options because that's the market you're trying to grow and Nvidia is the dominant player.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-6646e146-c08b-4df3-8bb2-927c82b815cb" for="discussion-upvote-button-DiscussionComment-11896340" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="65e02158d0d0f60fde3be97886b2f1f80177a108e4a09a0f193315454137f75c" data-error="" id="discussioncomment-11896361" data-gid="DC_kwDOAzpr8c4AtYYp" data-url="/ROCm/ROCm/discussions/4276/comments/11896361" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896361/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I'm not sure why, but for me it seems that for example RDNA2 cards (and APUs) are not unsupported, but rather blocked. With setting <code>HSA_OVERRIDE_GFX_VERSION=10.3.0</code> ROCm (using pytorch) runs fine. Since Linux 6.10 and the APU memory fix it even works on my Ryzen 6000 notebook. Why the need for that environment variable?</p>
<p dir="auto">If you don't have the resources to run all tests on all types of consumer GPU/APU and you want to ensure that you provide a build that is fully tested: Why not an "enterprise release" that is fully tested and has only support for tested card and a "community edition" that has all GPUs enabled but is not fully tested (both build from the same source code with the same versions, only difference is the activated cards).</p>
<p dir="auto">If you're thinking supporting a lot of consumer GPUs is a waste of money: A big reason why NVIDIA is so successful in the AI market, because every student with even the smallest NVIDIA GPU was able to experiment with CUDA on her/his own machine. Years later when those students are working in the industry, with what products/APIs do they have experience? And when they make decisions, what will they buy for their company?<br>
AMD missed the first big wave, so they need to catch up, and maybe they get a second chance to really gain market share at some point (when for example NVIDIA is having issues with a new generation).</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-3db34814-8b6d-499d-8c08-1a49909c1da3" for="discussion-upvote-button-DiscussionComment-11896361" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="5adf4a21994224ef202e238c472a29caef73e0d49b91c99c4ccf15b93d227319" data-error="" id="discussioncomment-11896375" data-gid="DC_kwDOAzpr8c4AtYY3" data-url="/ROCm/ROCm/discussions/4276/comments/11896375" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896375/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">This is fundamentally misguided. It's not a question of which specific GPUs are supported now. It's a question of:</p>
<ol dir="auto">
<li>Will support consistently be added for new GPUs at the time of launch? Your competition has day 1 support for new GPUs, and doesn't arbitrarily decide that half the GPUs in a new generation will be incompatible.</li>
<li>Will support consistently be kept for GPUs that are currently supported? You competition still runs on ancient laptops out-of-the-box.</li>
<li>Do consumers trust you to keep to your word for 1 &amp; 2? You've walked back support before.</li>
</ol>
<p dir="auto">Reiterating from my prior comment <a href="https://github.com/amd/RyzenAI-SW/issues/2#issuecomment-1999684155" data-hovercard-type="issue" data-hovercard-url="/amd/RyzenAI-SW/issues/2/hovercard">here</a>:</p>
<blockquote>
<p dir="auto">Your competition has CUDA: Compute <em>Unified</em> Device Architecture - and indeed it supports practically everything from ancient laptops on up. In terms of effort versus reward, things target CUDA largely because they can port to it - using easily available hardware - <em>once</em> - and get fairly decent performance across a very large chunk of the market. (And then tune it later if necessary.)</p>
<p dir="auto"><a href="https://github.com/ROCm/ROCm/issues/1180" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/1180/hovercard">Meanwhile</a>, <a href="https://github.com/ROCm/ROCm/issues/2429" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/2429/hovercard">ROCm</a> <a href="https://github.com/ROCm/ROCm/issues/1659" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/1659/hovercard">support</a> <a href="https://github.com/ROCm/ROCm/issues/1714" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/1714/hovercard">is</a> <a href="https://github.com/ROCm/ROCm/issues/1306" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/1306/hovercard">a</a> <a href="https://github.com/ROCm/ROCm/issues/666" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/666/hovercard">minefield</a> <a href="https://github.com/ROCm/ROCm/issues/887" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/887/hovercard">at</a> <a href="https://github.com/ROCm/ROCm/issues/1880" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/1880/hovercard">best</a>.</p>
</blockquote>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-f69bf508-67d7-40d3-87f3-2fa27351cb64" for="discussion-upvote-button-DiscussionComment-11896375" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="ebcd1fe4bc079c968e2da6d9e486b26e57d33c33e1091450cf4878b73a1c1bd3" data-error="" id="discussioncomment-11896406" data-gid="DC_kwDOAzpr8c4AtYZW" data-url="/ROCm/ROCm/discussions/4276/comments/11896406" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896406/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Supporting gfx1036 is a must. It's probably the most available device and would allow many people to see if something is working on ROCm at all. It's also a great option to test on RDNA2 when you have other generation dGPUs.</p>
<p dir="auto">But, as many people said already, supporting all devices is the only correct option if you want broad developer support. After you've dropped gfx906 support on Windows (after it barely started) I don't see any reason to invest development effort in current AMD hardware. And the UDNA news sound like you'll immediately drop all previous architectures upon release.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-2dbbbb07-e9a3-4bad-8b37-be85b36d4d61" for="discussion-upvote-button-DiscussionComment-11896406" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>


  
  <!-- '"` --><!-- </textarea></xmp> --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I am (not) a failure: Lessons learned from six failed startup attempts (407 pts)]]></title>
            <link>http://blog.rongarret.info/2025/01/i-am-not-failure-lessons-learned-from.html</link>
            <guid>42771676</guid>
            <pubDate>Mon, 20 Jan 2025 18:40:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://blog.rongarret.info/2025/01/i-am-not-failure-lessons-learned-from.html">http://blog.rongarret.info/2025/01/i-am-not-failure-lessons-learned-from.html</a>, See on <a href="https://news.ycombinator.com/item?id=42771676">Hacker News</a></p>
Couldn't get http://blog.rongarret.info/2025/01/i-am-not-failure-lessons-learned-from.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Personalized Duolingo (kind of) for vocabulary building (117 pts)]]></title>
            <link>https://github.com/baturyilmaz/wordpecker-app</link>
            <guid>42770200</guid>
            <pubDate>Mon, 20 Jan 2025 16:27:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/baturyilmaz/wordpecker-app">https://github.com/baturyilmaz/wordpecker-app</a>, See on <a href="https://news.ycombinator.com/item?id=42770200">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">WordPecker App</h2><a id="user-content-wordpecker-app" aria-label="Permalink: WordPecker App" href="#wordpecker-app"></a></p>
<p dir="auto">A personalized language-learning app that brings the magic of Duolingo-style lessons to your own curated vocabulary lists and contexts.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/baturyilmaz/wordpecker-app/blob/main/docs/assets/createlist-addword.gif"><img src="https://github.com/baturyilmaz/wordpecker-app/raw/main/docs/assets/createlist-addword.gif" alt="WordPecker App" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The Idea</h2><a id="user-content-the-idea" aria-label="Permalink: The Idea" href="#the-idea"></a></p>
<p dir="auto">Learning a new language can be straightforward, but mastering it is the real challenge. While it's relatively easy to grasp the basics, developing strong communication or reading skills depends on having a solid vocabulary. To build this, you must learn words and phrases at various levels, yet exposure is key. For instance, if you've studied English but don‚Äôt live in an English-speaking country, advancing your skills becomes significantly harder. You can read books, watch movies, or browse blogs, but fully immersing yourself in the language is still difficult. Real progress often requires extra effort‚Äîstudying and revisiting words and phrases encountered in your daily life.</p>
<p dir="auto">However, this process can be inconvenient. You have to pause whatever you‚Äôre doing to note new words, search their meanings, record them, and then review them later. This is time-consuming and tiring. As a result, although you might improve, the learning process can feel painfully slow and inefficient.</p>
<p dir="auto">To solve this, I have an idea for an app that merges personalized learning with the efficiency of flashcards‚Äîa blend of Duolingo-like lessons and custom study lists.</p>
<p dir="auto">Imagine you‚Äôre reading a book‚Äîsay, Harry Potter. As you come across unfamiliar words, you open the app and create a new list with details like:</p>
<ul dir="auto">
<li>Name: Harry Potter Book</li>
<li>Description: The first book in the series</li>
<li>Context: Harry Potter</li>
</ul>
<p dir="auto">Once the list is created, you add the words or phrases you‚Äôve found. The app automatically provides their meanings in context after you save them, so you can continue reading without interruption.</p>
<p dir="auto">Later, you revisit the list and pick one of two options: Learn or Quiz.</p>
<ul dir="auto">
<li>Learn: This mode delivers structured lessons in a Duolingo style. Text, visuals, and exercises are dynamically generated using LLMs.</li>
</ul>
<p dir="auto"><strong>Note</strong>: Text-based multiple-choice questions are <em>currently</em> the only question type available.</p>
<ul dir="auto">
<li>Quiz: When you‚Äôre ready, you can test what you‚Äôve learned through interactive quizzes. They‚Äôre engaging and gamified, awarding points and showing your progress. For example, it might indicate you‚Äôve mastered 75% of your list.</li>
</ul>
<p dir="auto"><strong>Note</strong>: The feature that displays something like ‚ÄúYou‚Äôve mastered 75% of your list‚Äù is planned but not yet built.</p>
<p dir="auto">The key advantage is that the app keeps your learning tied to the context in which you originally saw the words. By returning to them in their original setting, you strengthen those specific neural pathways, speeding up retention and making learning significantly more effective.</p>
<p dir="auto"><strong>In short, it‚Äôs like having a personalized Duolingo where you can create and learn from your own lists. It‚Äôs a powerful way to immerse yourself in the language and make steady progress.</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/baturyilmaz/wordpecker-app/blob/main/docs/assets/wordpecker.png"><img src="https://github.com/baturyilmaz/wordpecker-app/raw/main/docs/assets/wordpecker.png" alt="WordPecker App"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How It Works</h2><a id="user-content-how-it-works" aria-label="Permalink: How It Works" href="#how-it-works"></a></p>
<ol dir="auto">
<li><strong>Encounter New Words</strong>: While reading or watching something, open the app and add new words or phrases to a contextual list (e.g., Harry Potter Book, Science Blog, Netflix Show, etc.).</li>
<li><strong>Automatic Definitions</strong>: The app automatically fetches the word definitions (in the context) after you save them.</li>
<li><strong>Learn</strong>: Dive into Learn Mode, practice exercises‚Äîjust like Duolingo, but tailored to your words.</li>
<li><strong>Quiz</strong>: Switch to Quiz Mode anytime to check your retention.</li>
<li><strong>Review and Repeat</strong>: Visit your lists.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=QIwPGAXgNLU" rel="nofollow"><img src="https://camo.githubusercontent.com/ce8cb8004b6fb42934cc37701f48bd18959f395b41524265d1088717f4b0b72f/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f51497750474158674e4c552f302e6a7067" alt="Alt text" data-canonical-src="https://img.youtube.com/vi/QIwPGAXgNLU/0.jpg"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Create List &amp; Add Word</h3><a id="user-content-create-list--add-word" aria-label="Permalink: Create List &amp; Add Word" href="#create-list--add-word"></a></p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=t1U5vzm5Qw0" rel="nofollow"><img src="https://camo.githubusercontent.com/e92a75632b509e8a7e57fd5c09ddda1dd7ba32653ce162f889bc241b759d5740/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f74315535767a6d355177302f302e6a7067" alt="Alt text" data-canonical-src="https://img.youtube.com/vi/t1U5vzm5Qw0/0.jpg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<ul dir="auto">
<li><strong>Additional Question Types</strong>: Currently, only text-based multiple-choice questions are supported. In the future, I plan to introduce more Duolingo-style exercises such as fill-in-the-blanks, listening comprehension, and visual matching.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/baturyilmaz/wordpecker-app/blob/main/docs/assets/roadmap-questions.jpeg"><img src="https://github.com/baturyilmaz/wordpecker-app/raw/main/docs/assets/roadmap-questions.jpeg" alt="Roadmap"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Progress Tracking</strong>: Display detailed statistics‚Äîlike a mastery percentage‚Äîand possibly introduce daily goals or streaks to keep learners motivated.</p>
</li>
<li>
<p dir="auto"><strong>List Sharing</strong>: Allow users to share their custom vocabulary lists with others.</p>
</li>
<li>
<p dir="auto"><strong>Improved Onboarding</strong>: Provide a quick tutorial or sample list for new users, helping them understand the app‚Äôs features and workflow more easily.</p>
</li>
<li>
<p dir="auto"><strong>Integration with Other Platforms</strong>: Connecting with e-readers?, browsers, or note-taking apps so users can add new words without leaving those platforms.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>Node.js &gt;= 16</li>
<li>npm or yarn</li>
<li>A Supabase account</li>
<li>An OpenAI API key</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Clone the repository:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/baturyilmaz/wordpecker-app.git
cd wordpecker-app"><pre>git clone https://github.com/baturyilmaz/wordpecker-app.git
<span>cd</span> wordpecker-app</pre></div>
<p dir="auto">Install dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install backend dependencies
cd backend
npm install

# Install frontend dependencies
cd frontend
npm install"><pre><span><span>#</span> Install backend dependencies</span>
<span>cd</span> backend
npm install

<span><span>#</span> Install frontend dependencies</span>
<span>cd</span> frontend
npm install</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Create <code>.env</code> files:</p>
<p dir="auto">Backend <code>.env</code>:</p>
<div data-snippet-clipboard-copy-content="PORT=
OPENAI_API_KEY=
SUPABASE_URL=
SUPABASE_SERVICE_KEY="><pre><code>PORT=
OPENAI_API_KEY=
SUPABASE_URL=
SUPABASE_SERVICE_KEY=
</code></pre></div>
<p dir="auto">Frontend <code>.env</code>:</p>
<div data-snippet-clipboard-copy-content="VITE_SUPABASE_URL=
VITE_SUPABASE_ANON_KEY=
VITE_API_URL="><pre><code>VITE_SUPABASE_URL=
VITE_SUPABASE_ANON_KEY=
VITE_API_URL=
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development</h3><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto">Start the backend:</p>

<p dir="auto">Start the frontend:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<ul dir="auto">
<li>Frontend: React.js with TypeScript</li>
<li>Backend: Express.js</li>
<li>Database: Supabase (PostgreSQL)</li>
<li>Auth: Supabase Auth</li>
<li>AI: OpenAI API</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome!</p>
<ol dir="auto">
<li>Fork the repo</li>
<li>Create a feature branch</li>
<li>Commit changes</li>
<li>Push to your branch</li>
<li>Open a pull request</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="https://github.com/baturyilmaz/wordpecker-app/blob/main/LICENSE">MIT</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: I'm Peter Roberts, immigration attorney, who does work for YC and startups. AMA (300 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42770125</link>
            <guid>42770125</guid>
            <pubDate>Mon, 20 Jan 2025 16:20:29 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42770125">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=42770125: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Mixxx: GPL DJ Software (511 pts)]]></title>
            <link>https://mixxx.org/</link>
            <guid>42769871</guid>
            <pubDate>Mon, 20 Jan 2025 15:53:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mixxx.org/">https://mixxx.org/</a>, See on <a href="https://news.ycombinator.com/item?id=42769871">Hacker News</a></p>
Couldn't get https://mixxx.org/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse engineering my #1 Hacker News article (121 pts)]]></title>
            <link>https://danielwirtz.com/blog/successful-hacker-news-article</link>
            <guid>42769325</guid>
            <pubDate>Mon, 20 Jan 2025 14:53:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danielwirtz.com/blog/successful-hacker-news-article">https://danielwirtz.com/blog/successful-hacker-news-article</a>, See on <a href="https://news.ycombinator.com/item?id=42769325">Hacker News</a></p>
Couldn't get https://danielwirtz.com/blog/successful-hacker-news-article: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek-R1 (1379 pts)]]></title>
            <link>https://github.com/deepseek-ai/DeepSeek-R1</link>
            <guid>42768072</guid>
            <pubDate>Mon, 20 Jan 2025 12:37:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/DeepSeek-R1">https://github.com/deepseek-ai/DeepSeek-R1</a>, See on <a href="https://news.ycombinator.com/item?id=42768072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DeepSeek-R1</h2><a id="user-content-deepseek-r1" aria-label="Permalink: DeepSeek-R1" href="#deepseek-r1"></a></p>



<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true"><img src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3"></a>
</p>
<hr>
<p><a href="https://www.deepseek.com/" rel="nofollow">
    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/badge.svg?raw=true">
  </a>
  <a href="https://chat.deepseek.com/" rel="nofollow">
    <img alt="Chat" src="https://camo.githubusercontent.com/a8ed26619b0338e36bfd80f920c9fe96127ff7f12f25a0190e2a94d00a4fa5b9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa496253230436861742d446565705365656b25323052312d3533366166353f636f6c6f723d353336616635266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&amp;logoColor=white">
  </a>
  <a href="https://huggingface.co/deepseek-ai" rel="nofollow">
    <img alt="Hugging Face" src="https://camo.githubusercontent.com/5e3115539d4583e22d65cb89eb1759e767cb9e1d70772923292fcfc80a654be4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d446565705365656b25323041492d6666633130373f636f6c6f723d666663313037266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;logoColor=white">
  </a>
</p>
<p><a href="https://discord.gg/Tc7c45Zzu5" rel="nofollow">
    <img alt="Discord" src="https://camo.githubusercontent.com/e227481a149714ed5187e4fd0b60b9f736099c2dd2083e6c091e29f1446cbb1a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d446565705365656b25323041492d3732383964613f6c6f676f3d646973636f7264266c6f676f436f6c6f723d776869746526636f6c6f723d373238396461" data-canonical-src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da">
  </a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true">
    <img alt="Wechat" src="https://camo.githubusercontent.com/562efc618da65f0a69bc804395005b8124f5c2ed2eb73441c4e359185cc01467/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5765436861742d446565705365656b25323041492d627269676874677265656e3f6c6f676f3d776563686174266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white">
  </a>
  <a href="https://twitter.com/deepseek_ai" rel="nofollow">
    <img alt="Twitter Follow" src="https://camo.githubusercontent.com/8272710ecd020c821b4f62c1c455efb89e0db4eb179c5f5f971c3c1f69452c54/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547769747465722d646565707365656b5f61692d77686974653f6c6f676f3d78266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white">
  </a>
</p>
<p><a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-CODE">
    <img alt="Code License" src="https://camo.githubusercontent.com/d8caf1d64169802e8094bcf0013f0b54d3a9547263b4e59eb43531d7d77993e4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64655f4c6963656e73652d4d49542d6635646535333f26636f6c6f723d663564653533" data-canonical-src="https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;color=f5de53">
  </a>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-MODEL">
    <img alt="Model License" src="https://camo.githubusercontent.com/edb8b566827851c52a732ee62c71e1c0231f588d5181c5925a518d98f35b4ff4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6f64656c5f4c6963656e73652d4d6f64656c5f41677265656d656e742d6635646535333f26636f6c6f723d663564653533" data-canonical-src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;color=f5de53">
  </a>
</p>
<p dir="auto">
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>üëÅÔ∏è</a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">1. Introduction</h2><a id="user-content-1-introduction" aria-label="Permalink: 1. Introduction" href="#1-introduction"></a></p>
<p dir="auto">We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.
DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.
With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.
However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,
we introduce DeepSeek-R1, which incorporates cold-start data before RL.
DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.
To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark.jpg"><img width="80%" src="https://github.com/deepseek-ai/DeepSeek-R1/raw/main/figures/benchmark.jpg"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">2. Model Summary</h2><a id="user-content-2-model-summary" aria-label="Permalink: 2. Model Summary" href="#2-model-summary"></a></p>
<hr>
<p dir="auto"><strong>Post-Training: Large-Scale Reinforcement Learning on the Base Model</strong></p>
<ul dir="auto">
<li>
<p dir="auto">We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.</p>
</li>
<li>
<p dir="auto">We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.
We believe the pipeline will benefit the industry by creating better models.</p>
</li>
</ul>
<hr>
<p dir="auto"><strong>Distillation: Smaller Models Can Be Powerful Too</strong></p>
<ul dir="auto">
<li>We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.</li>
<li>Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">3. Model Downloads</h2><a id="user-content-3-model-downloads" aria-label="Permalink: 3. Model Downloads" href="#3-model-downloads"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1 Models</h3><a id="user-content-deepseek-r1-models" aria-label="Permalink: DeepSeek-R1 Models" href="#deepseek-r1-models"></a></p>
<div dir="auto">
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>#Total Params</strong></th>
<th><strong>#Activated Params</strong></th>
<th><strong>Context Length</strong></th>
<th><strong>Download</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>DeepSeek-R1-Zero</td>
<td>671B</td>
<td>37B</td>
<td>128K</td>
<td><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero" rel="nofollow">ü§ó HuggingFace</a></td>
</tr>
<tr>
<td>DeepSeek-R1</td>
<td>671B</td>
<td>37B</td>
<td>128K</td>
<td><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1" rel="nofollow">ü§ó HuggingFace</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</div>
<p dir="auto">DeepSeek-R1-Zero &amp; DeepSeek-R1 are trained based on DeepSeek-V3-Base.
For more details regrading the model architecture, please refer to <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> repository.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1-Distill Models</h3><a id="user-content-deepseek-r1-distill-models" aria-label="Permalink: DeepSeek-R1-Distill Models" href="#deepseek-r1-distill-models"></a></p>

<p dir="auto">DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.
We slightly change their configs and tokenizers. Please use our setting to run these models.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">4. Evaluation Results</h2><a id="user-content-4-evaluation-results" aria-label="Permalink: 4. Evaluation Results" href="#4-evaluation-results"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1-Evaluation</h3><a id="user-content-deepseek-r1-evaluation" aria-label="Permalink: DeepSeek-R1-Evaluation" href="#deepseek-r1-evaluation"></a></p>
<p dir="auto">For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="570d04263f45e653820916ae742049fc">$0.6$</math-renderer>, a top-p value of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="570d04263f45e653820916ae742049fc">$0.95$</math-renderer>, and generate 64 responses per query to estimate pass@1.</p>
<div dir="auto">
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Category</th>
<th>Benchmark (Metric)</th>
<th>Claude-3.5-Sonnet-1022</th>
<th>GPT-4o 0513</th>
<th>DeepSeek V3</th>
<th>OpenAI o1-mini</th>
<th>OpenAI o1-1217</th>
<th>DeepSeek R1</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Architecture</td>
<td>-</td>
<td>-</td>
<td>MoE</td>
<td>-</td>
<td>-</td>
<td>MoE</td>
</tr>
<tr>
<td></td>
<td># Activated Params</td>
<td>-</td>
<td>-</td>
<td>37B</td>
<td>-</td>
<td>-</td>
<td>37B</td>
</tr>
<tr>
<td></td>
<td># Total Params</td>
<td>-</td>
<td>-</td>
<td>671B</td>
<td>-</td>
<td>-</td>
<td>671B</td>
</tr>
<tr>
<td>English</td>
<td>MMLU (Pass@1)</td>
<td>88.3</td>
<td>87.2</td>
<td>88.5</td>
<td>85.2</td>
<td><strong>91.8</strong></td>
<td>90.8</td>
</tr>
<tr>
<td></td>
<td>MMLU-Redux (EM)</td>
<td>88.9</td>
<td>88.0</td>
<td>89.1</td>
<td>86.7</td>
<td>-</td>
<td><strong>92.9</strong></td>
</tr>
<tr>
<td></td>
<td>MMLU-Pro (EM)</td>
<td>78.0</td>
<td>72.6</td>
<td>75.9</td>
<td>80.3</td>
<td>-</td>
<td><strong>84.0</strong></td>
</tr>
<tr>
<td></td>
<td>DROP (3-shot F1)</td>
<td>88.3</td>
<td>83.7</td>
<td>91.6</td>
<td>83.9</td>
<td>90.2</td>
<td><strong>92.2</strong></td>
</tr>
<tr>
<td></td>
<td>IF-Eval (Prompt Strict)</td>
<td><strong>86.5</strong></td>
<td>84.3</td>
<td>86.1</td>
<td>84.8</td>
<td>-</td>
<td>83.3</td>
</tr>
<tr>
<td></td>
<td>GPQA-Diamond (Pass@1)</td>
<td>65.0</td>
<td>49.9</td>
<td>59.1</td>
<td>60.0</td>
<td><strong>75.7</strong></td>
<td>71.5</td>
</tr>
<tr>
<td></td>
<td>SimpleQA (Correct)</td>
<td>28.4</td>
<td>38.2</td>
<td>24.9</td>
<td>7.0</td>
<td><strong>47.0</strong></td>
<td>30.1</td>
</tr>
<tr>
<td></td>
<td>FRAMES (Acc.)</td>
<td>72.5</td>
<td>80.5</td>
<td>73.3</td>
<td>76.9</td>
<td>-</td>
<td><strong>82.5</strong></td>
</tr>
<tr>
<td></td>
<td>AlpacaEval2.0 (LC-winrate)</td>
<td>52.0</td>
<td>51.1</td>
<td>70.0</td>
<td>57.8</td>
<td>-</td>
<td><strong>87.6</strong></td>
</tr>
<tr>
<td></td>
<td>ArenaHard (GPT-4-1106)</td>
<td>85.2</td>
<td>80.4</td>
<td>85.5</td>
<td>92.0</td>
<td>-</td>
<td><strong>92.3</strong></td>
</tr>
<tr>
<td>Code</td>
<td>LiveCodeBench (Pass@1-COT)</td>
<td>33.8</td>
<td>34.2</td>
<td>-</td>
<td>53.8</td>
<td>63.4</td>
<td><strong>65.9</strong></td>
</tr>
<tr>
<td></td>
<td>Codeforces (Percentile)</td>
<td>20.3</td>
<td>23.6</td>
<td>58.7</td>
<td>93.4</td>
<td><strong>96.6</strong></td>
<td>96.3</td>
</tr>
<tr>
<td></td>
<td>Codeforces (Rating)</td>
<td>717</td>
<td>759</td>
<td>1134</td>
<td>1820</td>
<td><strong>2061</strong></td>
<td>2029</td>
</tr>
<tr>
<td></td>
<td>SWE Verified (Resolved)</td>
<td><strong>50.8</strong></td>
<td>38.8</td>
<td>42.0</td>
<td>41.6</td>
<td>48.9</td>
<td>49.2</td>
</tr>
<tr>
<td></td>
<td>Aider-Polyglot (Acc.)</td>
<td>45.3</td>
<td>16.0</td>
<td>49.6</td>
<td>32.9</td>
<td><strong>61.7</strong></td>
<td>53.3</td>
</tr>
<tr>
<td>Math</td>
<td>AIME 2024 (Pass@1)</td>
<td>16.0</td>
<td>9.3</td>
<td>39.2</td>
<td>63.6</td>
<td>79.2</td>
<td><strong>79.8</strong></td>
</tr>
<tr>
<td></td>
<td>MATH-500 (Pass@1)</td>
<td>78.3</td>
<td>74.6</td>
<td>90.2</td>
<td>90.0</td>
<td>96.4</td>
<td><strong>97.3</strong></td>
</tr>
<tr>
<td></td>
<td>CNMO 2024 (Pass@1)</td>
<td>13.1</td>
<td>10.8</td>
<td>43.2</td>
<td>67.6</td>
<td>-</td>
<td><strong>78.8</strong></td>
</tr>
<tr>
<td>Chinese</td>
<td>CLUEWSC (EM)</td>
<td>85.4</td>
<td>87.9</td>
<td>90.9</td>
<td>89.9</td>
<td>-</td>
<td><strong>92.8</strong></td>
</tr>
<tr>
<td></td>
<td>C-Eval (EM)</td>
<td>76.7</td>
<td>76.0</td>
<td>86.5</td>
<td>68.9</td>
<td>-</td>
<td><strong>91.8</strong></td>
</tr>
<tr>
<td></td>
<td>C-SimpleQA (Correct)</td>
<td>55.4</td>
<td>58.7</td>
<td><strong>68.0</strong></td>
<td>40.3</td>
<td>-</td>
<td>63.7</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Distilled Model Evaluation</h3><a id="user-content-distilled-model-evaluation" aria-label="Permalink: Distilled Model Evaluation" href="#distilled-model-evaluation"></a></p>
<div dir="auto">
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>AIME 2024 pass@1</th>
<th>AIME 2024 cons@64</th>
<th>MATH-500 pass@1</th>
<th>GPQA Diamond pass@1</th>
<th>LiveCodeBench pass@1</th>
<th>CodeForces rating</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4o-0513</td>
<td>9.3</td>
<td>13.4</td>
<td>74.6</td>
<td>49.9</td>
<td>32.9</td>
<td>759</td>
</tr>
<tr>
<td>Claude-3.5-Sonnet-1022</td>
<td>16.0</td>
<td>26.7</td>
<td>78.3</td>
<td>65.0</td>
<td>38.9</td>
<td>717</td>
</tr>
<tr>
<td>o1-mini</td>
<td>63.6</td>
<td>80.0</td>
<td>90.0</td>
<td>60.0</td>
<td>53.8</td>
<td><strong>1820</strong></td>
</tr>
<tr>
<td>QwQ-32B-Preview</td>
<td>44.0</td>
<td>60.0</td>
<td>90.6</td>
<td>54.5</td>
<td>41.9</td>
<td>1316</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Qwen-1.5B</td>
<td>28.9</td>
<td>52.7</td>
<td>83.9</td>
<td>33.8</td>
<td>16.9</td>
<td>954</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Qwen-7B</td>
<td>55.5</td>
<td>83.3</td>
<td>92.8</td>
<td>49.1</td>
<td>37.6</td>
<td>1189</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Qwen-14B</td>
<td>69.7</td>
<td>80.0</td>
<td>93.9</td>
<td>59.1</td>
<td>53.1</td>
<td>1481</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Qwen-32B</td>
<td><strong>72.6</strong></td>
<td>83.3</td>
<td>94.3</td>
<td>62.1</td>
<td>57.2</td>
<td>1691</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Llama-8B</td>
<td>50.4</td>
<td>80.0</td>
<td>89.1</td>
<td>49.0</td>
<td>39.6</td>
<td>1205</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Llama-70B</td>
<td>70.0</td>
<td><strong>86.7</strong></td>
<td><strong>94.5</strong></td>
<td><strong>65.2</strong></td>
<td><strong>57.5</strong></td>
<td>1633</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">5. Chat Website &amp; API Platform</h2><a id="user-content-5-chat-website--api-platform" aria-label="Permalink: 5. Chat Website &amp; API Platform" href="#5-chat-website--api-platform"></a></p>
<p dir="auto">You can chat with DeepSeek-R1 on DeepSeek's official website: <a href="https://chat.deepseek.com/" rel="nofollow">chat.deepseek.com</a>, and switch on the button "DeepThink"</p>
<p dir="auto">We also provide OpenAI-Compatible API at DeepSeek Platform: <a href="https://platform.deepseek.com/" rel="nofollow">platform.deepseek.com</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">6. How to Run Locally</h2><a id="user-content-6-how-to-run-locally" aria-label="Permalink: 6. How to Run Locally" href="#6-how-to-run-locally"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1 Models</h3><a id="user-content-deepseek-r1-models-1" aria-label="Permalink: DeepSeek-R1 Models" href="#deepseek-r1-models-1"></a></p>
<p dir="auto">Please visit <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> repo for more information about running DeepSeek-R1 locally.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1-Distill Models</h3><a id="user-content-deepseek-r1-distill-models-1" aria-label="Permalink: DeepSeek-R1-Distill Models" href="#deepseek-r1-distill-models-1"></a></p>
<p dir="auto">DeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.</p>
<p dir="auto">For instance, you can easily start a service using <a href="https://github.com/vllm-project/vllm">vLLM</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager"><pre>vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager</pre></div>
<p dir="auto">You can also easily start a service using <a href="https://github.com/sgl-project/sglang">SGLang</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2"><pre>python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2</pre></div>
<p dir="auto"><strong>NOTE: We recommend setting an appropriate temperature (between 0.5 and 0.7) when running these models, otherwise you may encounter issues with endless repetition or incoherent output.</strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">7. License</h2><a id="user-content-7-license" aria-label="Permalink: 7. License" href="#7-license"></a></p>
<p dir="auto">This code repository and the model weights are licensed under the <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE">MIT License</a>.
DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:</p>
<ul dir="auto">
<li>DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from <a href="https://github.com/QwenLM/Qwen2.5">Qwen-2.5 series</a>, which are originally licensed under <a href="https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE" rel="nofollow">Apache 2.0 License</a>, and now finetuned with 800k samples curated with DeepSeek-R1.</li>
<li>DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under <a href="https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE" rel="nofollow">llama3.1 license</a>.</li>
<li>DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under <a href="https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE" rel="nofollow">llama3.3 license</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">8. Citation</h2><a id="user-content-8-citation" aria-label="Permalink: 8. Citation" href="#8-citation"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">9. Contact</h2><a id="user-content-9-contact" aria-label="Permalink: 9. Contact" href="#9-contact"></a></p>
<p dir="auto">If you have any questions, please raise an issue or contact us at <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/service@deepseek.com">service@deepseek.com</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Celestial Navigation for Drones (180 pts)]]></title>
            <link>https://www.mdpi.com/2504-446X/8/11/652</link>
            <guid>42767797</guid>
            <pubDate>Mon, 20 Jan 2025 12:02:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mdpi.com/2504-446X/8/11/652">https://www.mdpi.com/2504-446X/8/11/652</a>, See on <a href="https://news.ycombinator.com/item?id=42767797">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <section id="sec1-drones-08-00652" type="intro"><h2 data-nested="1">  1. Introduction</h2><p>Celestial navigation is among the oldest forms of navigation in aviation [<a href="#B1-drones-08-00652">1</a>]. The abundance of salient stars, known to high levels of precision, make them a useful cue for navigators when operating in clear conditions. The elevation of a star above the horizon would be measured, yielding a ‚Äòline of position‚Äô. This process was repeated with different stars to fully determine the navigator‚Äôs position. The advancement of imaging and computing hardware saw the integration of autonomous star trackers into manned aircraft, as seen, for example, in Lockheed‚Äôs SR-71. These autonomous star trackers consisted of a mechanically stabilized telescope and inertial sensors, whose observations, when combined, could produce position estimates to within 0.3 nautical miles for up to 10 h of operation. While accurate, these sensors tended to be both heavy and voluminous, making them undesirable for more modern Size, Weight and Power Constrained (SWAP-C) applications.</p><p>By the advent of the 21st century, Global Positioning System (GPS) had become ubiquitous in avionic navigation. The introduction of GPS caused the interest in celestial navigation to wither due to its relative inaccuracy. Consequently, celestial navigation is primarily seen only in space-based systems, whose orientation must be known to high levels of precision. Nonetheless, celestial navigation was identified as a desirable alternative to GPS [<a href="#B2-drones-08-00652">2</a>], primarily due its robustness against potential jamming. Critically, few GPS-denied alternatives exist that are capable of using passive sensors to estimate global position at night or over the ocean. For this reason, celestial navigation remains an important topic of research. The methodology presented in this paper demonstrates how celestial navigation may be utilized on low cost Uncrewed Aerial Vehicles (UAVs) that lack the precision of an expensive Attitude and Heading Reference System (AHRS).</p><p>Existing works on strapdown celestial navigation tend to be focused on simulation. One such example derives the equations for a celestial camera mounted directly to a strapdown inertial system [<a href="#B3-drones-08-00652">3</a>]. Similarly, a more recent study identified that a strapdown celestial system is contingent on the initial conditions and will tend to diverge over time [<a href="#B4-drones-08-00652">4</a>]. While some promise has been shown in the utility of such systems [<a href="#B5-drones-08-00652">5</a>], the primary problem with the navigation method is that the strapdown system must estimate the camera biases and delineate these from true position error. This is difficult to achieve, as camera bias is perceptually identical to motion over the Earth. Utilizing the celestial system as a highly accurate attitude reference tends to produce useful results in high altitude aircraft [<a href="#B6-drones-08-00652">6</a>] and spacecraft [<a href="#B7-drones-08-00652">7</a>,<a href="#B8-drones-08-00652">8</a>,<a href="#B9-drones-08-00652">9</a>,<a href="#B10-drones-08-00652">10</a>,<a href="#B11-drones-08-00652">11</a>]; however, as a modular solution, the celestial system must have a direct input to the AHRS to improve the inertial position estimate. Alternatively, it has been theoretically shown that atmospheric refraction can be observed to resolve positions [<a href="#B12-drones-08-00652">12</a>]. This technique relies on the observation of the atmospheric refraction of starlight, which occurs most significantly closest to the horizon. The angle of refraction tends to be minimal and difficult to observe, requiring a highly stabilized viewing platform.</p><p>There are a number of reasons why celestial navigation has not become ubiquitous in aviation and particularly in UAV applications. Firstly, there is significant complexity in designing and configuring the hardware. A celestial system must typically be gimballed to within arcminutes of precision, such that a narrow field of a view sensor may lock onto a single star. This could be achieved through gyroscopic stabilization; however, the inclusion of multiple kilograms of stabilization hardware is rarely justified to achieve a navigation outcome with significantly worse performance than GPS. Secondly, celestial systems require a clear view of the night sky, which places practical limitations on their use. Thirdly, alongside the mechanical complexity, there exists a significant computational complexity required to integrate a celestial payload into an existing system. A star database must be maintained, image processing algorithms must be implemented to identify and track stars, and the system may or may not require an interface for the existing AHRS to time and orientate data. These reasons have overshadowed the benefit of having an additional GPS-denied modality for navigation available to an aircraft.</p><p>Previous research has addressed the latter of these points, demonstrating that modern computer vision libraries and embedded hardware are capable of handling the computational requirements of celestial navigation [<a href="#B13-drones-08-00652">13</a>,<a href="#B14-drones-08-00652">14</a>]. We address the first of these points in this study by demonstrating the use of a strapdown celestial system for navigation. Contrary to the stabilized alternative, a strapdown celestial system contains low mechanical complexity, is lightweight, and can be implemented at a low cost. The primary trade-off is in the accuracy of the navigation system. While high levels of accuracy are theoretically achievable (depending on the quality of the optical system), the limiting factor with strapdown celestial navigation is the accuracy of the AHRS. As a rough guide, an attitude error of 1¬∞ correlates with a position error of approximately 100&nbsp;km. It is not uncommon for low cost autopilots (such as the Cube Orange running ArduPlane v4.5 firmware) to produce attitude errors in the vicinity of multiple degrees. Such biases would lead to positional offsets that are far too significant for use in any real application.</p><p>We address this shortcoming by demonstrating how a simple orbital motion can significantly improve a celestial position estimate. This technique has been used on the ground to correct for boresight errors [<a href="#B15-drones-08-00652">15</a>], and we demonstrate through experimentation that a similar technique may be used on aircraft to correct for attitude errors. The principle behind this maneuver is that a misaligned boresight will trace a circle about the true boresight if a full azimuthal revolution is performed. The misalignment presents itself in the navigation frame as a circular error in latitude and longitude, enabling the averaging of the results to attain an improved position estimate. We demonstrate that this technique is effective even when the camera is misaligned with the AHRS, offering a reliable method for estimating the position to within 4&nbsp;km from an unknown state. While lacking precision, this method of localization is absolute, cheap to implement, and relatively lightweight.</p><p>To our knowledge, no such technique currently exists in the literature. The methods proposed here are intended to demonstrate the application of strapdown celestial navigation on a fixed-wing drone platform, while addressing the key practical difficulties in doing so. Existing works are iterative, relying on the integration of celestial measurements with inertial measurements using a filter such as an Extended Kalman Filter (EKF) or Unscented Kalman Filter (UKF). This research demonstrates that a strapdown system can be treated as a stand-alone, modular addition to an inertial system, such that the measurements do not need to be integrated into the filter. Independence from an inertial filter enables the celestial system to produce true global position measurements that are not affected by initial conditions. Provided the use of an accurate clock, the results presented in this paper will not degrade over time.</p><p>The remainder of this paper addresses the theory, implementation, and results of the proposed method. We outline the equations for observing stars and estimating global positions in <a href="#sec2-drones-08-00652">Section 2</a>, we describe the methdology used in star detection/tracking and the experimental configuration, we explicitly define the equations for computing the mean position in <a href="#sec3-drones-08-00652">Section 3</a>, we present the results from the flight trial and simulation in <a href="#sec4-drones-08-00652">Section 4</a>, we discuss the results in <a href="#sec5-drones-08-00652">Section 5</a>, and we conclude in <a href="#sec6-drones-08-00652">Section 6</a>.</p></section><section id="sec2-drones-08-00652" type=""><h2 data-nested="1">  2. Theory</h2><section id="sec2dot1-drones-08-00652" type=""><h4 data-nested="2">  2.1. Star Observation</h4><div><p>An observer may estimate their position with knowledge of the current sidereal time and direction vectors to three or more stars in the local North East Down (NED) frame. A camera system is mounted relative to the body frame of the aircraft, with Direction Cosine Matrix (DCM) </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>b</mi>
      <mo>/</mo>
      <mi>c</mi>
    </mrow>
  </msub>
</semantics></math><p> describing the orientation of the camera with respect to the aircraft. Given an aircraft with the orientation described by the DCM </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p>, an observation </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">X</mi>
    <mi>c</mi>
  </msup>
</semantics></math><p> in the camera frame may be transformed to the local NED frame by the following equation:</p><div id="FD1-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msup>
      <mi mathvariant="bold">X</mi>
      <mi>l</mi>
    </msup>
    <mo>=</mo>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>l</mi>
        <mo>/</mo>
        <mi>b</mi>
      </mrow>
    </msub>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>b</mi>
        <mo>/</mo>
        <mi>c</mi>
      </mrow>
    </msub>
    <msup>
      <mi mathvariant="bold">X</mi>
      <mi>c</mi>
    </msup>
  </mrow>
</semantics></math>
      </p>
      <p><label>(1)</label>
      </p>
    </div></div><div><p>Multiple stars are observed simultaneously through the imaging system. A calibrated camera may be described by the pinhole projection model, such that
        </p><p>
        where </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">x</mi>
    <mi>p</mi>
  </msup>
</semantics></math><p> contains the homogeneous pixel coordinates of the projected star, </p><math display="inline"><semantics>
  <mi mathvariant="bold">K</mi>
</semantics></math><p> is the camera intrinsic matrix, and </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">X</mi>
    <mi>c</mi>
  </msup>
</semantics></math><p> is the direction vector of the star in the camera coordinate frame. Consequently, with knowledge of the camera intrinsic matrix, given an observation </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">x</mi>
    <mi>p</mi>
  </msup>
</semantics></math><p>, the vector </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">X</mi>
    <mi>c</mi>
  </msup>
</semantics></math><p> may be computed as follows:</p><p>
        where the scale factor </p><math display="inline"><semantics>
  <mi>Œ±</mi>
</semantics></math><p> may be computed with the knowledge that </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">X</mi>
    <mi>c</mi>
  </msup>
</semantics></math><p> is unitary. Given that </p><math display="inline"><semantics>
  <mrow>
    <mrow>
      <mo>|</mo>
    </mrow>
    <msup>
      <mi mathvariant="bold">X</mi>
      <mi>c</mi>
    </msup>
    <mrow>
      <mo>|</mo>
      <mo>&nbsp;</mo>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
  </mrow>
</semantics></math><p>, it follows that </p><math display="inline"><semantics>
  <mrow>
    <mrow>
      <mi>Œ±</mi>
      <mo>|</mo>
    </mrow>
    <msup>
      <mi mathvariant="bold">K</mi>
      <mrow>
        <mo>‚àí</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <msup>
      <mi mathvariant="bold">x</mi>
      <mi>p</mi>
    </msup>
    <mrow>
      <mo>|</mo>
      <mo>&nbsp;</mo>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
  </mrow>
</semantics></math><p>. Therefore, </p><math display="inline"><semantics>
  <mi>Œ±</mi>
</semantics></math><p> can be calculated as follows:</p></div></section><section id="sec2dot2-drones-08-00652" type=""><h4 data-nested="2">  2.2. Position Estimation</h4><div><p>The global position may be calculated using the zenith-angle, </p><math display="inline"><semantics>
  <mi>Œ∂</mi>
</semantics></math><p>, of stars. Following the methodology presented by [<a href="#B16-drones-08-00652">16</a>], each measurement </p><math display="inline"><semantics>
  <mi>Œ∂</mi>
</semantics></math><p> generates a plane that intersects with the terrestrial sphere, yielding a circle on which the observer may be located. Two observations generate a line, of which only two points on this line intersect with the terrestrial sphere. This case is depicted in <a href="#drones-08-00652-f001">Figure 1</a>. Three observations precisely define the location of the observer, and more than three observations can create an over-constrained system. We may utilize redundant measurements by estimating the least-squares approximation from three or more star observations.</p></div><div><p>Given <span>n</span> stars, with right ascension </p><math display="inline"><semantics>
  <mi>Œ±</mi>
</semantics></math><p> and declination </p><math display="inline"><semantics>
  <mi>Œ¥</mi>
</semantics></math><p>, observed at a zenith angle </p><math display="inline"><semantics>
  <mi>Œ∂</mi>
</semantics></math><p>, we may define <span>n</span> planes within the geographic coordinate system. If we define </p><math display="inline"><semantics>
  <mi>Œª</mi>
</semantics></math><p> as the longitude of an observer whose zenith is directed towards the star, which can be expressed as the difference between the star‚Äôs right ascension and the current Greenwich hour angle of Aries, </p><math display="inline"><semantics>
  <mrow>
    <mi>Œª</mi>
    <mo>=</mo>
    <mi>Œ±</mi>
    <mo>‚àí</mo>
    <mi>G</mi>
    <mi>H</mi>
    <mi>A</mi>
    <mo>Œì</mo>
  </mrow>
</semantics></math><p>, then it can be seen that the equation of a plane whose normal vector is directed towards the star is given by
        </p><div id="FD5-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi>a</mi>
    <mi>x</mi>
    <mo>+</mo>
    <mi>b</mi>
    <mi>y</mi>
    <mo>+</mo>
    <mi>c</mi>
    <mi>z</mi>
    <mo>‚àí</mo>
    <mi>p</mi>
    <mo>=</mo>
    <mn>0</mn>
  </mrow>
</semantics></math>
      </p>
      <p><label>(5)</label>
      </p>
    </div><p>
        where
        </p><div id="FD6-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mi>a</mi>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">cos</mo>
          <mi>Œª</mi>
          <mo form="prefix">cos</mo>
          <mi>Œ¥</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mi>b</mi>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">sin</mo>
          <mi>Œª</mi>
          <mo form="prefix">cos</mo>
          <mi>Œ¥</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mi>c</mi>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">sin</mo>
          <mi>Œ¥</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mi>p</mi>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">cos</mo>
          <mi>Œ∂</mi>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</semantics></math>
      </p>
      <p><label>(6)</label>
      </p>
    </div></div><div><p>Therefore, given a minimum of three star observations, we can build matrices </p><math display="inline"><semantics>
  <mi mathvariant="bold">A</mi>
</semantics></math><p> and </p><math display="inline"><semantics>
  <mi mathvariant="bold">p</mi>
</semantics></math><p> to formulate a least-squares approximation of the intersection of these planes. Vertically stacking each equation yields
        </p><p>
        where
        </p><div id="">
      <p>
        <math display="block"><semantics>
  <mtable displaystyle="true">
    <mtr>
      <mtd>
        <mrow>
          <mi mathvariant="bold">A</mi>
          <mo>=</mo>
          <mfenced open="[" close="]">
            <mtable>
              <mtr>
                <mtd>
                  <msub>
                    <mi>a</mi>
                    <mn>1</mn>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>b</mi>
                    <mn>1</mn>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>c</mi>
                    <mn>1</mn>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>a</mi>
                    <mn>2</mn>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>b</mi>
                    <mn>2</mn>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>c</mi>
                    <mn>2</mn>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mo>‚ãÆ</mo>
                </mtd>
                <mtd>
                  <mo>‚ãÆ</mo>
                </mtd>
                <mtd>
                  <mo>‚ãÆ</mo>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>a</mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>b</mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>c</mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
          <mo>,</mo>
          <mspace width="4pt"></mspace>
          <mi mathvariant="bold">x</mi>
          <mo>=</mo>
          <mfenced open="[" close="]">
            <mtable>
              <mtr>
                <mtd>
                  <mi>x</mi>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>y</mi>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>z</mi>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
          <mo>,</mo>
          <mspace width="4pt"></mspace>
          <mi mathvariant="bold">p</mi>
          <mo>=</mo>
          <mfenced open="[" close="]">
            <mtable>
              <mtr>
                <mtd>
                  <msub>
                    <mi>p</mi>
                    <mn>1</mn>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>p</mi>
                    <mn>2</mn>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mo>‚ãÆ</mo>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>p</mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</semantics></math>
      </p>
      
    </div></div><div><p>We apply the standard least-squares solution to find the point of intersection of the planes, </p><math display="inline"><semantics>
  <mi mathvariant="bold">x</mi>
</semantics></math><p>, as follows:</p><div id="FD8-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi mathvariant="bold">x</mi>
    <mo>=</mo>
    <msup>
      <mrow>
        <mo>(</mo>
        <msup>
          <mi mathvariant="bold">A</mi>
          <mi>T</mi>
        </msup>
        <mi mathvariant="bold">A</mi>
        <mo>)</mo>
      </mrow>
      <mrow>
        <mo>‚àí</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <msup>
      <mi mathvariant="bold">A</mi>
      <mi>T</mi>
    </msup>
    <mi mathvariant="bold">p</mi>
  </mrow>
</semantics></math>
      </p>
      <p><label>(8)</label>
      </p>
    </div></div><div><p>Putting </p><math display="inline"><semantics>
  <mi mathvariant="bold">x</mi>
</semantics></math><p> back into geographical coordinates, we find the latitude and longitude which best fits the star observations:</p><div id="FD9-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mtable displaystyle="true">
    <mtr>
      <mtd>
        <mrow>
          <mi>œï</mi>
          <mo>=</mo>
          <msup>
            <mo form="prefix">tan</mo>
            <mrow>
              <mo>‚àí</mo>
              <mn>1</mn>
            </mrow>
          </msup>
          <mfenced open="(" close=")">
            <mstyle scriptlevel="0" displaystyle="true">
              <mfrac>
                <mi>z</mi>
                <msqrt>
                  <mrow>
                    <msup>
                      <mi>x</mi>
                      <mn>2</mn>
                    </msup>
                    <mo>+</mo>
                    <msup>
                      <mi>y</mi>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                </msqrt>
              </mfrac>
            </mstyle>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</semantics></math>
      </p>
      <p><label>(9)</label>
      </p>
    </div></div><p>This value for the latitude and longitude minimizes the squared error in the position vectors. This will be used in <a href="#sec3-drones-08-00652">Section 3</a> for converting many observations into a single position estimate.</p></section></section><section id="sec3-drones-08-00652" type="methods"><h2 data-nested="1">  3. Methods</h2><div><p>A modular celestial navigation system should be mounted rigidly with respect to the AHRS. The orientation of the aircraft, </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p>, is provided by the autopilot. The orientation of the camera, </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>b</mi>
      <mo>/</mo>
      <mi>c</mi>
    </mrow>
  </msub>
</semantics></math><p>, is unknown and must be calibrated to obtain accurate positional information. For the purposes of this study, we assume that the precise orientation of the camera remains unknown, and we consequently accept that the individual position estimates will be erroneous. It will be shown later that this misalignment may be overcome through the use of averaging. This is particularly useful given that the camera itself may be mounted separately from the autopilot, thus being subjected to factors such as vibration, aerodynamic loading, or changes in AHRS biases, which cause the relative orientation between the AHRS and the camera to change over time.</p></div><section id="sec3dot1-drones-08-00652" type=""><h4 data-nested="2">  3.1. Star Detection and Tracking</h4><div><p>Stars are detected within an image using a basic binary thresholding operation. Given a mean pixel value within the frame, </p><math display="inline"><semantics>
  <mi>Œº</mi>
</semantics></math><p>, and standard deviation, </p><math display="inline"><semantics>
  <mi>œÉ</mi>
</semantics></math><p>, a binary threshold is set at </p><math display="inline"><semantics>
  <mrow>
    <mi>Œº</mi>
    <mo>+</mo>
    <mn>5</mn>
    <mi>œÉ</mi>
  </mrow>
</semantics></math><p>, as advised in [<a href="#B17-drones-08-00652">17</a>]. The contours are extracted, and an agglomerative clustering algorithm is applied to cluster redundant detections.</p></div><div><p>Once detected, a standard Kalman filter is used to track individual stars between frames. Each star is defined by its state vector:</p><p>
        and a constant sized bounding box. The states </p><math display="inline"><semantics>
  <mrow>
    <mi>u</mi>
    <mo>,</mo>
    <mi>v</mi>
    <mo>,</mo>
    <mover accent="true">
      <mi>u</mi>
      <mo>Àô</mo>
    </mover>
    <mo>,</mo>
    <mi>and</mi>
    <mover accent="true">
      <mi>v</mi>
      <mo>Àô</mo>
    </mover>
  </mrow>
</semantics></math><p> describe the x position, y position, x velocity, and y velocity of the star on the image plane, respectively. The position of the star is taken to be the subpixel maxima, computed as a weighted average of the </p><math display="inline"><semantics>
  <mrow>
    <mn>3</mn>
    <mo>√ó</mo>
    <mn>3</mn>
  </mrow>
</semantics></math><p> Region of Interest (ROI) centered on the peak pixel value, as described by [<a href="#B18-drones-08-00652">18</a>]
        </p><div id="FD12-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mrow>
      <mo>(</mo>
      <mi>u</mi>
      <mo>,</mo>
      <mi>v</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfenced separators="" open="(" close=")">
      <mstyle scriptlevel="0" displaystyle="true">
        <mfrac>
          <mrow>
            <msub>
              <mo>‚àë</mo>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>I</mi>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>u</mi>
              <mi>i</mi>
            </msub>
          </mrow>
          <mrow>
            <msub>
              <mo>‚àë</mo>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>I</mi>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
          </mrow>
        </mfrac>
      </mstyle>
      <mo>,</mo>
      <mstyle scriptlevel="0" displaystyle="true">
        <mfrac>
          <mrow>
            <msub>
              <mo>‚àë</mo>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>I</mi>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>v</mi>
              <mi>j</mi>
            </msub>
          </mrow>
          <mrow>
            <msub>
              <mo>‚àë</mo>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>I</mi>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
          </mrow>
        </mfrac>
      </mstyle>
    </mfenced>
  </mrow>
</semantics></math>
      </p>
      <p><label>(12)</label>
      </p>
    </div></div><div><p>The state transition matrix </p><math display="inline"><semantics>
  <mi mathvariant="bold">F</mi>
</semantics></math><p> is defined assuming a constant velocity as follows:</p><div id="FD13-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi mathvariant="bold">F</mi>
    <mo>=</mo>
    <mfenced open="[" close="]">
      <mtable>
        <mtr>
          <mtd>
            <mn>1</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mrow>
              <mo>Œî</mo>
              <mi>t</mi>
            </mrow>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>1</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mrow>
              <mo>Œî</mo>
              <mi>t</mi>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>1</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>1</mn>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</semantics></math>
      </p>
      <p><label>(13)</label>
      </p>
    </div></div><div><p>The position and velocity of the stars are observed directly; therefore, the observation matrix </p><math display="inline"><semantics>
  <mi mathvariant="bold">H</mi>
</semantics></math><p> is defined as the </p><math display="inline"><semantics>
  <mrow>
    <mn>4</mn>
    <mo>√ó</mo>
    <mn>4</mn>
  </mrow>
</semantics></math><p> identity, such that the observation equation is simply given&nbsp;by
        </p></div><div><p>Following the standard Kalman filter equations, the a priori state and covariance estimates are computed:</p><div id="FD16-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mover accent="true">
        <mi mathvariant="bold">P</mi>
        <mo>^</mo>
      </mover>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <mo>=</mo>
    <mi mathvariant="bold">F</mi>
    <msub>
      <mi mathvariant="bold">P</mi>
      <mi>k</mi>
    </msub>
    <msup>
      <mi mathvariant="bold">F</mi>
      <mi>T</mi>
    </msup>
    <mo>+</mo>
    <mi mathvariant="bold">Q</mi>
  </mrow>
</semantics></math>
      </p>
      <p><label>(16)</label>
      </p>
    </div><p>
        for the covariance matrix </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">P</mi>
    <mi>k</mi>
  </msub>
</semantics></math><p> with process noise </p><math display="inline"><semantics>
  <mi mathvariant="bold">Q</mi>
</semantics></math><p>. The innovation is calculated as follows:</p><div id="FD17-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">y</mi>
      <mi>k</mi>
    </msub>
    <mo>=</mo>
    <msub>
      <mi mathvariant="bold">z</mi>
      <mi>k</mi>
    </msub>
    <mo>‚àí</mo>
    <msub>
      <mover accent="true">
        <mi mathvariant="bold">x</mi>
        <mo>^</mo>
      </mover>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
  </mrow>
</semantics></math>
      </p>
      <p><label>(17)</label>
      </p>
    </div><p>
        with covariance, </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">S</mi>
    <mi mathvariant="bold">k</mi>
  </msub>
</semantics></math><p>, given by
        </p><div id="FD18-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">S</mi>
      <mi>k</mi>
    </msub>
    <mo>=</mo>
    <msub>
      <mover accent="true">
        <mi mathvariant="bold">P</mi>
        <mo>^</mo>
      </mover>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <mo>+</mo>
    <msub>
      <mi mathvariant="bold">R</mi>
      <mi>k</mi>
    </msub>
  </mrow>
</semantics></math>
      </p>
      <p><label>(18)</label>
      </p>
    </div><p>
        where </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">R</mi>
    <mi>k</mi>
  </msub>
</semantics></math><p> is the measurement noise. The Kalman gain is computed as follows:</p><div id="FD19-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">K</mi>
      <mi>k</mi>
    </msub>
    <mo>=</mo>
    <msub>
      <mover accent="true">
        <mi mathvariant="bold">P</mi>
        <mo>^</mo>
      </mover>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <msubsup>
      <mi mathvariant="bold">S</mi>
      <mi>k</mi>
      <mrow>
        <mo>‚àí</mo>
        <mn>1</mn>
      </mrow>
    </msubsup>
  </mrow>
</semantics></math>
      </p>
      <p><label>(19)</label>
      </p>
    </div><p>
        and the posterior updates are applied to the a priori estimates:</p><div id="FD20-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">x</mi>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <mo>=</mo>
    <msub>
      <mover accent="true">
        <mi mathvariant="bold">x</mi>
        <mo>^</mo>
      </mover>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <mo>+</mo>
    <msub>
      <mi mathvariant="bold">K</mi>
      <mi>k</mi>
    </msub>
    <msub>
      <mi mathvariant="bold">y</mi>
      <mi>k</mi>
    </msub>
  </mrow>
</semantics></math>
      </p>
      <p><label>(20)</label>
      </p>
    </div><div id="FD21-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">P</mi>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <mo>=</mo>
    <mrow>
      <mo>(</mo>
      <mi mathvariant="bold">I</mi>
      <mo>‚àí</mo>
      <msub>
        <mi mathvariant="bold">K</mi>
        <mi>k</mi>
      </msub>
      <mo>)</mo>
    </mrow>
    <msub>
      <mover accent="true">
        <mi mathvariant="bold">P</mi>
        <mo>^</mo>
      </mover>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
  </mrow>
</semantics></math>
      </p>
      <p><label>(21)</label>
      </p>
    </div></div><div><p>The measurement noise, </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">R</mi>
    <mi>k</mi>
  </msub>
</semantics></math><p>, is defined as a diagonal matrix, with elements equal to </p><math display="inline"><semantics>
  <mrow>
    <mo>[</mo>
    <mn>0.5</mn>
    <mspace width="4pt"></mspace>
    <mspace width="4pt"></mspace>
    <mn>0.5</mn>
    <mspace width="4pt"></mspace>
    <mspace width="4pt"></mspace>
    <mn>1.0</mn>
    <mspace width="4pt"></mspace>
    <mspace width="4pt"></mspace>
    <mn>1.0</mn>
    <mo>]</mo>
  </mrow>
</semantics></math><p>, and the process noise is also defined as a diagonal matrix, with elements equal to </p><math display="inline"><semantics>
  <mrow>
    <mo>[</mo>
    <mn>4.0</mn>
    <mspace width="4pt"></mspace>
    <mspace width="4pt"></mspace>
    <mn>4.0</mn>
    <mspace width="4pt"></mspace>
    <mspace width="4pt"></mspace>
    <mn>2.0</mn>
    <mspace width="4pt"></mspace>
    <mspace width="4pt"></mspace>
    <mn>2.0</mn>
    <mo>]</mo>
  </mrow>
</semantics></math><p>. The measurement noise was selected based on the calibration accuracy of the camera, and the process noise was experimentally tuned to minimize the occurrence of lost tracks under high motion conditions. The a priori state estimate is used to propagate the bounding box containing the region of interest, within which the peak is detected using Equation (<a href="#FD12-drones-08-00652">12</a>). The bounding box is centered on </p><math display="inline"><semantics>
  <mrow>
    <mo>[</mo>
    <mi>u</mi>
    <mo>,</mo>
    <mi>v</mi>
    <mo>]</mo>
  </mrow>
</semantics></math><p>, rounded to the nearest pixel. The width and height of the bounding box were fixed at 21 pixels for this imaging system, given an image resolution of </p><math display="inline"><semantics>
  <mrow>
    <mn>1936</mn>
    <mo>√ó</mo>
    <mn>1216</mn>
  </mrow>
</semantics></math><p> and a field of view of 53.5&nbsp;deg. A visual snapshot of the tracking system can be seen in <a href="#drones-08-00652-f002">Figure 2</a>.</p></div></section><section id="sec3dot2-drones-08-00652" type=""><h4 data-nested="2">  3.2. Experimental Configuration</h4><p>Celestial imagery was captured in-flight from a 4 m flying wing UAV. The selected autopilot (Cube Orange) was mounted at the center of mass, and the celestial payload was mounted in the shoulder of the aircraft (see <a href="#drones-08-00652-f003">Figure 3</a>). The celestial payload collected imagery at a rate of 10 &nbsp;Hz, as well as the attitude and position data from the autopilot, enabling post-analysis. A Raspberry Pi 5 (Raspberry Pi Ltd., Cambridge, UK) was used as the companion computer, interfacing with the celestial camera and the autopilot. An Alvium 1800 U-240 (Allied Vision, Stradtroda, Germany) monochrome sensor fitted with a f/1.4 6 mm wide angle lens was chosen for the imaging system. A serial Universal Asynchronous Receiver Transmitter (UART) link between the Raspberry Pi and the Cube Orange facilitated the transport of MAVLink v2.0 messages. The AHRS data from ArduPlane‚Äôs EKF3 was recorded at a rate of 30&nbsp;Hz, and the ground truth GPS position data were recorded at a rate of 10&nbsp;Hz. The Raspberry Pi clock was synchronized with GPS time prior to takeoff. The Raspberry Pi and the camera were mounted on a PLA 3D-printed structure (see <a href="#drones-08-00652-f004">Figure 4</a>) for integration into the airframe.</p><p>The test flight was conducted on a moonless night. The wind was modest, typically remaining below 5&nbsp;m/s. The flight plan consisted of both straight legs and orbital trajectories with varying radii. An overview of the flight plan can be seen in <a href="#drones-08-00652-t001">Table 1</a>.</p><p>The total flight lasted 72 min. Astronomical twilight ceased at 19:32, and takeoff was conducted at 19:53. The GPS receiver in the aircraft was not allowed to be switched off to enable emergency failsafes and prevent fence breaches. We address the consequences of this in <a href="#sec4dot4-drones-08-00652">Section 4.4</a>.</p></section><section id="sec3dot3-drones-08-00652" type=""><h4 data-nested="2">  3.3. Position Estimation</h4><div><p>The star tracker in <a href="#sec3dot1-drones-08-00652">Section 3.1</a> operates on distorted images. The pixel location from each star tracker is extracted. rectified, and subsequently converted to a unit vector in NED coordinates according to <a href="#sec2dot1-drones-08-00652">Section 2.1</a>. Given an observation </p><math display="inline"><semantics>
  <mrow>
    <msup>
      <mi mathvariant="bold">X</mi>
      <mi>l</mi>
    </msup>
    <mo>=</mo>
    <msup>
      <mrow>
        <mo>[</mo>
        <mi>x</mi>
        <mspace width="4pt"></mspace>
        <mi>y</mi>
        <mspace width="4pt"></mspace>
        <mi>z</mi>
        <mo>]</mo>
      </mrow>
      <mi>T</mi>
    </msup>
  </mrow>
</semantics></math><p>, the elevation of a star above the horizon is calculated as
        </p><div id="FD22-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi>e</mi>
    <mi>l</mi>
    <mo>=</mo>
    <mo>‚àí</mo>
    <msup>
      <mo form="prefix">tan</mo>
      <mrow>
        <mo>‚àí</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac>
        <mi>y</mi>
        <msqrt>
          <mrow>
            <msup>
              <mi>x</mi>
              <mn>2</mn>
            </msup>
            <mo>+</mo>
            <msup>
              <mi>y</mi>
              <mn>2</mn>
            </msup>
          </mrow>
        </msqrt>
      </mfrac>
    </mstyle>
  </mrow>
</semantics></math>
      </p>
      <p><label>(22)</label>
      </p>
    </div><p>
        and, subsequently, the zenith-angle </p><math display="inline"><semantics>
  <mi>Œ∂</mi>
</semantics></math><p> is calculated as </p><math display="inline"><semantics>
  <mrow>
    <mn>90</mn>
    <mo>‚àí</mo>
    <mi>e</mi>
    <mi>l</mi>
  </mrow>
</semantics></math><p>.</p></div><div><p>In the initial case, the camera orientation </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p> is not known to a high level of precision. Assuming that an attempt is made to mount the camera in alignment with the autopilot, an initial DCM may be formulated from some combination of 90¬∞ Euler rotations. It will be seen that as long as this orientation is accurate to within a hemisphere of tolerance, the exact value does not matter, as it will be recalculated in flight.</p></div><div><p>If a minimum of six stars are visible within the frame, a Random Sample Consensus (RANSAC) approach to position estimation may be used to remove bias from false detections. RANSAC is a technique used to identify outliers in sample data and omit them from the estimation process. This is useful for detecting misidentified stars or for removing non-star detections (such as satellites or overhead planes). In the context of position estimation, the RANSAC algorithm randomly selects three stars to generate a position estimate. Subsequently, the algorithm compares the location of the remaining stars against the position estimate. If the remaining stars are within some heuristic tolerance, they are considered inliers. If they are outside of tolerance, they are considered outliers. The position estimate which minimizes the mean squared error of the inliers is chosen as the estimate. A pseudocode implementation of this RANSAC position estimation is outlined in Algorithm 1.
        </p><table><tbody><tr><td><b>Algorithm 1</b> Position-RANSAC</td></tr><tr><td><p><math display="inline"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>l</mi>
        <mo>/</mo>
        <mi>b</mi>
      </mrow>
    </msub>
    <mo>,</mo>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>b</mi>
        <mo>/</mo>
        <mi>c</mi>
      </mrow>
    </msub>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi>G</mi>
    <mi>H</mi>
    <mi>A</mi>
    <mo>Œì</mo>
    <mo>‚Üê</mo>
    <mi>g</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>G</mi>
    <mi>H</mi>
    <mi>A</mi>
    <mi>A</mi>
    <mi>r</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mo>(</mo>
    <mo>)</mo>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi>A</mi>
    <mi>s</mi>
    <mo>‚Üê</mo>
    <mo>[</mo>
    <mspace width="4pt"></mspace>
    <mo>]</mo>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi>P</mi>
    <mi>s</mi>
    <mo>‚Üê</mo>
    <mo>[</mo>
    <mspace width="4pt"></mspace>
    <mo>]</mo>
  </mrow>
</semantics></math></p><div><p><b>for</b></p><math display="inline"><semantics>
  <mrow>
    <mo>&nbsp;</mo>
    <mi>o</mi>
    <mi>b</mi>
    <mi>s</mi>
  </mrow>
</semantics></math><p> in </p><math display="inline"><semantics>
  <mrow>
    <mi>o</mi>
    <mi>b</mi>
    <mi>s</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>v</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>s</mi>
  </mrow>
</semantics></math>&nbsp;<p><b>do</b></p></div><p>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>a</mi>
    <mi>z</mi>
    <mo>,</mo>
    <mi>e</mi>
    <mi>l</mi>
    <mo>=</mo>
    <mi>c</mi>
    <mi>o</mi>
    <mi>m</mi>
    <mi>p</mi>
    <mi>u</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>A</mi>
    <mi>z</mi>
    <mi>E</mi>
    <mi>l</mi>
    <mo>(</mo>
    <mi>o</mi>
    <mi>b</mi>
    <mi>s</mi>
    <mo>,</mo>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>l</mi>
        <mo>/</mo>
        <mi>b</mi>
      </mrow>
    </msub>
    <mo>,</mo>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>b</mi>
        <mo>/</mo>
        <mi>c</mi>
      </mrow>
    </msub>
    <mo>)</mo>
  </mrow>
</semantics></math></p><p>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>a</mi>
    <mo>,</mo>
    <mi>b</mi>
    <mo>,</mo>
    <mi>c</mi>
    <mo>,</mo>
    <mi>p</mi>
    <mo>=</mo>
    <mi>c</mi>
    <mi>o</mi>
    <mi>m</mi>
    <mi>p</mi>
    <mi>u</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>P</mi>
    <mi>o</mi>
    <mi>s</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>C</mi>
    <mi>o</mi>
    <mi>e</mi>
    <mi>f</mi>
    <mi>f</mi>
    <mi>i</mi>
    <mi>c</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>s</mi>
    <mo>(</mo>
    <mi>G</mi>
    <mi>H</mi>
    <mi>A</mi>
    <mo>Œì</mo>
    <mo>,</mo>
    <mi>a</mi>
    <mi>z</mi>
    <mo>,</mo>
    <mi>e</mi>
    <mi>l</mi>
    <mo>,</mo>
    <mi>o</mi>
    <mi>b</mi>
    <mi>s</mi>
    <mo>.</mo>
    <mi>r</mi>
    <mi>a</mi>
    <mo>,</mo>
    <mi>o</mi>
    <mi>b</mi>
    <mi>s</mi>
    <mo>.</mo>
    <mi>d</mi>
    <mi>e</mi>
    <mi>c</mi>
    <mo>)</mo>
  </mrow>
</semantics></math></p><p>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>A</mi>
    <mi>s</mi>
    <mo>.</mo>
    <mi>a</mi>
    <mi>p</mi>
    <mi>p</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mo>(</mo>
    <mo>[</mo>
    <mi>a</mi>
    <mo>,</mo>
    <mi>b</mi>
    <mo>,</mo>
    <mi>c</mi>
    <mo>]</mo>
    <mo>)</mo>
  </mrow>
</semantics></math></p><p>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>P</mi>
    <mi>s</mi>
    <mo>.</mo>
    <mi>a</mi>
    <mi>p</mi>
    <mi>p</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mo>(</mo>
    <mo>[</mo>
    <mi>p</mi>
    <mo>]</mo>
    <mo>)</mo>
  </mrow>
</semantics></math></p><p><b>end for</b></p><p><math display="inline"><semantics>
  <mrow>
    <mi>i</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>s</mi>
    <mo>‚Üê</mo>
    <mn>0</mn>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi>o</mi>
    <mi>u</mi>
    <mi>t</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>s</mi>
    <mo>‚Üê</mo>
    <mn>0</mn>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi>b</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>‚Üê</mo>
    <mo movablelimits="true" form="prefix">inf</mo>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi mathvariant="bold">x</mi>
    <mo>‚Üê</mo>
    <mo>[</mo>
    <mn>0</mn>
    <mo>,</mo>
    <mn>0</mn>
    <mo>,</mo>
    <mn>0</mn>
    <mo>]</mo>
  </mrow>
</semantics></math></p><div><p><b>for</b></p><math display="inline"><semantics>
  <mrow>
    <mn>0</mn>
    <mo>&lt;</mo>
    <mi>i</mi>
    <mo>&lt;</mo>
    <mi>i</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>s</mi>
  </mrow>
</semantics></math>&nbsp;
                      <p><b>do</b></p></div><div>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>j</mi>
    <mo>‚Üê</mo>
  </mrow>
</semantics></math><p> 3 randomly selected indexes</p></div><div><p>&nbsp;&nbsp;&nbsp;&nbsp;build </p><math display="inline"><semantics>
  <mi mathvariant="bold">A</mi>
</semantics></math><p> and </p><math display="inline"><semantics>
  <mi mathvariant="bold">p</mi>
</semantics></math><p> using </p><math display="inline"><semantics>
  <mrow>
    <mi>A</mi>
    <mi>s</mi>
    <mo>[</mo>
    <mi>j</mi>
    <mo>]</mo>
  </mrow>
</semantics></math><p> and </p><math display="inline"><semantics>
  <mrow>
    <mi>P</mi>
    <mi>s</mi>
    <mo>[</mo>
    <mi>j</mi>
    <mo>]</mo>
  </mrow>
</semantics></math></div><p>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mover accent="true">
      <mi mathvariant="bold">x</mi>
      <mo>^</mo>
    </mover>
    <mo>‚Üê</mo>
    <msup>
      <mrow>
        <mo>(</mo>
        <msup>
          <mi mathvariant="bold">A</mi>
          <mi>T</mi>
        </msup>
        <mi mathvariant="bold">A</mi>
        <mo>)</mo>
      </mrow>
      <mrow>
        <mo>‚àí</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <msup>
      <mi mathvariant="bold">A</mi>
      <mi>T</mi>
    </msup>
    <mi mathvariant="bold">p</mi>
  </mrow>
</semantics></math></p><p>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>i</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>‚Üê</mo>
    <mn>0</mn>
  </mrow>
</semantics></math></p><p><b>for</b> all remaining indexes <span>k</span>&nbsp;<b>do</b></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi mathvariant="script">E</mi>
    <mo>‚Üê</mo>
    <mi>P</mi>
    <mi>s</mi>
    <mrow>
      <mo>[</mo>
      <mi>k</mi>
      <mo>]</mo>
    </mrow>
    <mo>‚àí</mo>
    <mi>A</mi>
    <mi>s</mi>
    <mrow>
      <mo>[</mo>
      <mi>k</mi>
      <mo>]</mo>
    </mrow>
    <mover accent="true">
      <mi mathvariant="bold">x</mi>
      <mo>^</mo>
    </mover>
  </mrow>
</semantics></math></p><div>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p><b>if</b></p><math display="inline"><semantics>
  <mrow>
    <mi mathvariant="script">E</mi>
    <mo>&lt;</mo>
    <mi>t</mi>
    <mi>o</mi>
    <mi>l</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>c</mi>
    <mi>e</mi>
  </mrow>
</semantics></math>&nbsp;<p><b>then</b></p></div><div>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>i</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
  </mrow>
</semantics></math><p> += </p><math display="inline"><semantics>
  <mi mathvariant="script">E</mi>
</semantics></math></div><p><b>else</b></p><p><b>end if</b></p><p><b>end for</b></p><div>&nbsp;&nbsp;&nbsp;&nbsp;<p><b>if</b></p><math display="inline"><semantics>
  <mrow>
    <mi>i</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>s</mi>
    <mo>:</mo>
    <mi>o</mi>
    <mi>u</mi>
    <mi>t</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>s</mi>
    <mo>&gt;</mo>
    <mi>a</mi>
    <mi>c</mi>
    <mi>c</mi>
    <mi>e</mi>
    <mi>p</mi>
    <mi>t</mi>
    <mi>a</mi>
    <mi>b</mi>
    <mi>l</mi>
    <mi>e</mi>
    <mspace width="4pt"></mspace>
    <mi>r</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
  </mrow>
</semantics></math>&nbsp;<p><b>then</b></p></div><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>m</mi>
    <mi>e</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>=</mo>
    <mi>i</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>/</mo>
    <mi>i</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>s</mi>
  </mrow>
</semantics></math></p><div>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p><b>if</b></p><math display="inline"><semantics>
  <mrow>
    <mi>m</mi>
    <mi>e</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>&lt;</mo>
    <mi>b</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
  </mrow>
</semantics></math>&nbsp;<p><b>then</b></p></div><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>b</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>=</mo>
    <mi>m</mi>
    <mi>e</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
  </mrow>
</semantics></math></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi mathvariant="bold">x</mi>
    <mo>‚Üê</mo>
    <mover accent="true">
      <mi mathvariant="bold">x</mi>
      <mo>^</mo>
    </mover>
  </mrow>
</semantics></math></p><p><b>end if</b></p><p><b>end if</b></p><p><b>end for</b></p><p><math display="inline"><semantics>
  <mrow>
    <mi>œï</mi>
    <mo>‚Üê</mo>
    <msup>
      <mo form="prefix">tan</mo>
      <mrow>
        <mo>‚àí</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <mfenced open="(" close=")">
      <mstyle scriptlevel="0" displaystyle="true">
        <mfrac>
          <mrow>
            <mi mathvariant="bold">x</mi>
            <mo>[</mo>
            <mn>2</mn>
            <mo>]</mo>
          </mrow>
          <msqrt>
            <mrow>
              <mi mathvariant="bold">x</mi>
              <msup>
                <mrow>
                  <mo>[</mo>
                  <mn>0</mn>
                  <mo>]</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <mi mathvariant="bold">x</mi>
              <msup>
                <mrow>
                  <mo>[</mo>
                  <mn>1</mn>
                  <mo>]</mo>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </msqrt>
        </mfrac>
      </mstyle>
    </mfenced>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi>Œª</mi>
    <mo>‚Üê</mo>
    <msup>
      <mo form="prefix">tan</mo>
      <mrow>
        <mo>‚àí</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <mfenced open="(" close=")">
      <mstyle scriptlevel="0" displaystyle="true">
        <mfrac>
          <mrow>
            <mi mathvariant="bold">x</mi>
            <mo>[</mo>
            <mn>1</mn>
            <mo>]</mo>
          </mrow>
          <mrow>
            <mi mathvariant="bold">x</mi>
            <mo>[</mo>
            <mn>0</mn>
            <mo>]</mo>
          </mrow>
        </mfrac>
      </mstyle>
    </mfenced>
  </mrow>
</semantics></math></p></td></tr></tbody></table></div><div><p>We first demonstrate that, in the general case, the DCM </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p> is not fixed during flight. In an integrated solution, the celestial system would provide attitude measurements to the EKF, and the offset from this attitude would be estimated. As a modular solution, however, this is infeasible. Taking the output from the EKF to be the true orientation of the aircraft, we can use the true GPS position to compute </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p>. We use the Kabsch-RANSAC methodology presented in [<a href="#B19-drones-08-00652">19</a>] to calculate the ideal rotation between a star‚Äôs theoretical location and its actual location. In conjunction with the autopilot output, this rotation yields </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p>. Testing over an 89 s section of flat and level flight, it can be seen in <a href="#drones-08-00652-f005">Figure 5</a> that the camera roll angle shifts by approximately 0.2¬∞, the pitch angle shifts by approximately 0.05¬∞, and the yaw angle shifts by approximately 0.3¬∞. In the context of celestial navigation, these are significant perturbances which, had the offsets been attributed to position as opposed to camera orientation, would have been interpreted as around 30&nbsp;km of positional offset. Indeed, by assuming that </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p> is fixed, we can see in <a href="#drones-08-00652-f006">Figure 6</a> that the latitude drifts by approximately 0.2¬∞ over the same short window of flat and level flight. Over an extended period (hours), the orientation may shift such that positional errors are in the range of hundreds of kilometers.</p></div><p>It is clear that a stand-alone celestial module cannot function in the conventional manner with consumer-grade hardware, unless the autopilot is capable of integrating the attitude output from the celestial system into its own filter and estimating the camera orientation. We will demonstrate now a method which, given a very rough initial estimate of the camera orientation, is capable of returning position estimates to within 4&nbsp;km. This is significant in the context of long flights, where alternative dead-reckoning solutions would experience positional drift that is either linear (assuming velocity measurements are available) or quadratic (assuming only acceleration measurements) as a function of time.</p><div><p>By performing a rotation through 360¬∞ of compass heading, at an approximately constant yaw rate, the position estimates can be averaged to find an approximation of the orbital center. Provided <span>n</span> images throughout an orbit are available, there exists <span>n</span> latitude and longitude estimates, which may be expressed as unit vectors in the Earth Centred Earth Fixed (ECEF) frame </p><math display="inline"><semantics>
  <msubsup>
    <mi mathvariant="bold">p</mi>
    <mi>i</mi>
    <mrow>
      <mi>e</mi>
      <mi>c</mi>
      <mi>e</mi>
      <mi>f</mi>
    </mrow>
  </msubsup>
</semantics></math><p>. The mean position is taken to be the mean of these vectors:</p><div id="FD23-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msup>
      <mover accent="true">
        <mi mathvariant="bold">p</mi>
        <mo>¬Ø</mo>
      </mover>
      <mrow>
        <mi>e</mi>
        <mi>c</mi>
        <mi>e</mi>
        <mi>f</mi>
      </mrow>
    </msup>
    <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac>
        <mn>1</mn>
        <mi>n</mi>
      </mfrac>
    </mstyle>
    <munderover>
      <mo>‚àë</mo>
      <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mrow>
      <mi>n</mi>
    </munderover>
    <msubsup>
      <mi mathvariant="bold">p</mi>
      <mi>i</mi>
      <mrow>
        <mi>e</mi>
        <mi>c</mi>
        <mi>e</mi>
        <mi>f</mi>
      </mrow>
    </msubsup>
  </mrow>
</semantics></math>
      </p>
      <p><label>(23)</label>
      </p>
    </div></div><div><p>The latitude and longitude are simply calculated as
        </p><p>
        where <span>x</span>, <span>y</span>, and <span>z</span> are the elements of the mean position vector.</p></div><div><p>Once an initial position estimate has been formulated, it may be used to calculate a more precise estimate of the camera orientation </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>b</mi>
      <mo>/</mo>
      <mi>l</mi>
    </mrow>
  </msub>
</semantics></math><p>. The aim of orientation estimation is to find the rotation matrix </p><math display="inline"><semantics>
  <mi mathvariant="bold">R</mi>
</semantics></math><p> that minimizes the residual error between a set of observations </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">u</mi>
    <mi>c</mi>
  </msup>
</semantics></math><p> represented in the camera frame of reference and a set of theoretical unit vectors </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">v</mi>
    <mi>l</mi>
  </msup>
</semantics></math><p> in the NED frame of reference:</p><div id="FD26-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi mathvariant="script">E</mi>
    <mo>=</mo>
    <munderover>
      <mo>‚àë</mo>
      <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mrow>
      <mi>n</mi>
    </munderover>
    <mrow>
      <mo stretchy="false">‚Äñ</mo>
    </mrow>
    <msubsup>
      <mi mathvariant="bold">v</mi>
      <mi>i</mi>
      <mi>l</mi>
    </msubsup>
    <mo>‚àí</mo>
    <mi mathvariant="bold">R</mi>
    <msubsup>
      <mi mathvariant="bold">u</mi>
      <mi>i</mi>
      <mi>c</mi>
    </msubsup>
    <msup>
      <mrow>
        <mo stretchy="false">‚Äñ</mo>
      </mrow>
      <mn>2</mn>
    </msup>
    <mo>,</mo>
  </mrow>
</semantics></math>
      </p>
      <p><label>(26)</label>
      </p>
    </div></div><div><p>In this case, the rotation matrix </p><math display="inline"><semantics>
  <mi mathvariant="bold">R</mi>
</semantics></math><p> is composed of the aircraft DCM, </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p>, and the camera DCM, </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>b</mi>
      <mo>/</mo>
      <mi>c</mi>
    </mrow>
  </msub>
</semantics></math><p>:</p><p>
        where we accept </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p> as a deterministic output from the autopilot, and so the camera orientation may be found given </p><math display="inline"><semantics>
  <mi mathvariant="bold">R</mi>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p>:</p><div id="FD28-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>b</mi>
        <mo>/</mo>
        <mi>c</mi>
      </mrow>
    </msub>
    <mo>=</mo>
    <msubsup>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>l</mi>
        <mo>/</mo>
        <mi>b</mi>
      </mrow>
      <mi>T</mi>
    </msubsup>
    <mi mathvariant="bold">R</mi>
  </mrow>
</semantics></math>
      </p>
      <p><label>(28)</label>
      </p>
    </div></div><div><p>Each observation </p><math display="inline"><semantics>
  <msubsup>
    <mi mathvariant="bold">u</mi>
    <mi>i</mi>
    <mi>c</mi>
  </msubsup>
</semantics></math><p> is correlated with a database star. This correlation is obtained through star identification. During the instantiation of the star tracker, a lost-in-space log-polar star identification algorithm is used to determine the IDs of each star in the frame [<a href="#B20-drones-08-00652">20</a>]. The theoretical position vectors </p><math display="inline"><semantics>
  <msubsup>
    <mi mathvariant="bold">v</mi>
    <mi>i</mi>
    <mi>l</mi>
  </msubsup>
</semantics></math><p> in the NED coordinates are generated from the star‚Äôs right ascension and declination, the time of observation, and the estimated latitude and longitude [<a href="#B13-drones-08-00652">13</a>]. This allows for the use of the Kabsch algorithm [<a href="#B21-drones-08-00652">21</a>] to find the rotation </p><math display="inline"><semantics>
  <mi mathvariant="bold">R</mi>
</semantics></math><p> between the observed stars and the database stars. Following the implementation in [<a href="#B19-drones-08-00652">19</a>], the algorithm is provided in Algorithm 2, where </p><math display="inline"><semantics>
  <mi mathvariant="bold">A</mi>
</semantics></math><p> is the matrix of <span>n</span> observation vectors with dimension </p><math display="inline"><semantics>
  <mrow>
    <mo>[</mo>
    <mi>n</mi>
    <mo>√ó</mo>
    <mn>3</mn>
    <mo>]</mo>
  </mrow>
</semantics></math><p> and </p><math display="inline"><semantics>
  <mi mathvariant="bold">B</mi>
</semantics></math><p> is the matrix of <span>n</span> theoretical vectors, also with dimension </p><math display="inline"><semantics>
  <mrow>
    <mo>[</mo>
    <mi>n</mi>
    <mo>√ó</mo>
    <mn>3</mn>
    <mo>]</mo>
  </mrow>
</semantics></math><p>. The resulting rotation </p><math display="inline"><semantics>
  <mi mathvariant="bold">R</mi>
</semantics></math><p> is used in conjunction with the most recent autopilot attitude data to find the camera orientation </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>b</mi>
      <mo>/</mo>
      <mi>c</mi>
    </mrow>
  </msub>
</semantics></math><p>.
        </p><table><tbody><tr><td><b>Algorithm 2</b> Kabsch</td></tr><tr><td><div><p>Translate vectors in </p><math display="inline"><semantics>
  <mi mathvariant="bold">A</mi>
</semantics></math><p> and </p><math display="inline"><semantics>
  <mi mathvariant="bold">B</mi>
</semantics></math><p> such that centroid is at origin</p></div><div><p>Compute the matrix </p><math display="inline"><semantics>
  <mrow>
    <mi mathvariant="bold">C</mi>
    <mo>=</mo>
    <msup>
      <mi mathvariant="bold">A</mi>
      <mi>T</mi>
    </msup>
    <mi mathvariant="bold">B</mi>
  </mrow>
</semantics></math></div><div><p>Compute the singular value decomposition </p><math display="inline"><semantics>
  <mrow>
    <mi mathvariant="bold">U</mi>
    <mo>,</mo>
    <mo>Œ£</mo>
    <mo>,</mo>
    <mi mathvariant="bold">V</mi>
    <mo>=</mo>
    <mi>SVD</mi>
    <mo>(</mo>
    <mi mathvariant="bold">C</mi>
    <mo>)</mo>
  </mrow>
</semantics></math><p> such that </p><math display="inline"><semantics>
  <mrow>
    <mi mathvariant="bold">C</mi>
    <mo>=</mo>
    <mi mathvariant="bold">U</mi>
    <mo>Œ£</mo>
    <msup>
      <mi mathvariant="bold">V</mi>
      <mi>T</mi>
    </msup>
  </mrow>
</semantics></math></div><div><p>Set diagonal elements </p><math display="inline"><semantics>
  <msub>
    <mo>Œ£</mo>
    <mn>1</mn>
  </msub>
</semantics></math><p> through to </p><math display="inline"><semantics>
  <msub>
    <mo>Œ£</mo>
    <mrow>
      <mi>n</mi>
      <mo>‚àí</mo>
      <mn>1</mn>
    </mrow>
  </msub>
</semantics></math><p> = 1.</p></div><p><b>else</b></p><p><b>end if</b></p><div><p>Compute the rotation matrix </p><math display="inline"><semantics>
  <mrow>
    <mi mathvariant="bold">R</mi>
    <mo>=</mo>
    <mi mathvariant="bold">V</mi>
    <mo>Œ£</mo>
    <msup>
      <mi mathvariant="bold">U</mi>
      <mi>T</mi>
    </msup>
  </mrow>
</semantics></math></div></td></tr></tbody></table></div><div><p>Once the camera orientation has been found, two options are presented, depending on the computational power of the flight computer. If the computer is capable of parallel processing, then the set of theoretical star locations </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">v</mi>
    <mi>l</mi>
  </msup>
</semantics></math><p> may be re-calculated using the updated latitude and longitude, and the estimated positions may be re-calculated using the updated camera DCM </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>b</mi>
      <mo>/</mo>
      <mi>c</mi>
    </mrow>
  </msub>
</semantics></math><p>. This enables the system to recursively converge on a position estimate from a single set of orbital data. Alternatively, it is possible to repeat an orbit and calculate a new set of latitudes and longitudes. This method does not require post hoc processing and may be better suited to real-time applications. For the purposes of this study, we use the former method, as this allows us to treat each orbit as an independent sample, yielding an independent position estimate, thus giving us deeper insight into how the characteristics of a given orbit affect the position estimate. An example of this process can be seen in <a href="#drones-08-00652-f007">Figure 7</a>.</p></div></section></section><section id="sec4-drones-08-00652" type="results"><h2 data-nested="1">  4. Results</h2><section id="sec4dot1-drones-08-00652" type=""><h4 data-nested="2">  4.1. Position Estimation</h4><div><p>Following the experimental configuration in <a href="#sec3-drones-08-00652">Section 3</a>, the position of the aircraft is estimated for each orbit. The camera was initially aligned using a rotation matrix built from Euler angles of </p><math display="inline"><semantics>
  <mrow>
    <mo>‚àí</mo>
    <msup>
      <mn>90</mn>
      <mo>‚àò</mo>
    </msup>
    <mo>,</mo>
    <msup>
      <mn>0</mn>
      <mo>‚àò</mo>
    </msup>
  </mrow>
</semantics></math><p>, and </p><math display="inline"><semantics>
  <msup>
    <mn>180</mn>
    <mo>‚àò</mo>
  </msup>
</semantics></math><p> in yaw, pitch, and roll, respectively, for a yaw‚Äìpitch‚Äìroll rotation sequence. This is a coarse approximation based on the orientation with which the camera was mounted to the airframe. Of course, the true orientation differs from this; as can be seen in <a href="#drones-08-00652-f005">Figure 5</a>, there is approximately a 5¬∞ error in this initial orientation.</p></div><p>Each orbit is treated as an independent observation. This is achieved by resetting the camera orientation at the initialization of every orbit, thus requiring the algorithm to recalculate the camera DCM. The output from each of the orbital motions listed in <a href="#drones-08-00652-t001">Table 1</a> is shown in <a href="#drones-08-00652-t002">Table 2</a>. The position error is measured as the distance between the final celestial position estimate and the center of the orbit, calculated using the Haversine function.</p></section><section id="sec4dot2-drones-08-00652" type=""><h4 data-nested="2">  4.2. Estimation Accuracy</h4><div><p>It can be seen that the orbits which are neither climbing nor descending produced more accurate positional data. These orbits had larger radii, which consequently produced greater numbers of position estimates per orbit. Additionally, the variance in the pitch and roll axis during ascent/descent tends to be greater, as the total energy control system utilizes the pitch axis to throttle the climb/descent rate. By taking these two factors into account, we calculate the standard error as a function of the covariance in pitch and roll, as well as the number of samples. The generalized variance is calculated by taking the determinant of the covariance matrix in pitch and roll, where this matrix is given by
        </p><div id="FD29-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mo>Œ£</mo>
      <mrow>
        <mi>Œ∏</mi>
        <mo>,</mo>
        <mi>œï</mi>
      </mrow>
    </msub>
    <mo>=</mo>
    <mfenced open="[" close="]">
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mi>COV</mi>
              <mo>(</mo>
              <mi>Œ∏</mi>
              <mo>,</mo>
              <mi>Œ∏</mi>
              <mo>)</mo>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mi>COV</mi>
              <mo>(</mo>
              <mi>Œ∏</mi>
              <mo>,</mo>
              <mi>œï</mi>
              <mo>)</mo>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mi>COV</mi>
              <mo>(</mo>
              <mi>œï</mi>
              <mo>,</mo>
              <mi>Œ∏</mi>
              <mo>)</mo>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mi>COV</mi>
              <mo>(</mo>
              <mi>œï</mi>
              <mo>,</mo>
              <mi>œï</mi>
              <mo>)</mo>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</semantics></math>
      </p>
      <p><label>(29)</label>
      </p>
    </div><p>
        where the function </p><math display="inline"><semantics>
  <mrow>
    <mi>COV</mi>
    <mo>(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo>)</mo>
  </mrow>
</semantics></math><p> describes the covariance between sets <span>x</span> and <span>y</span>:</p><div id="FD30-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi>COV</mi>
    <mrow>
      <mo>(</mo>
      <mi>x</mi>
      <mo>,</mo>
      <mi>y</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac>
        <mrow>
          <msubsup>
            <mo>‚àë</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>n</mi>
          </msubsup>
          <mrow>
            <mo>(</mo>
            <msub>
              <mi>x</mi>
              <mi>i</mi>
            </msub>
            <mo>‚àí</mo>
            <mover accent="true">
              <mi>x</mi>
              <mo>¬Ø</mo>
            </mover>
            <mo>)</mo>
          </mrow>
          <mrow>
            <mo>(</mo>
            <msub>
              <mi>y</mi>
              <mi>i</mi>
            </msub>
            <mo>‚àí</mo>
            <mover accent="true">
              <mi>y</mi>
              <mo>¬Ø</mo>
            </mover>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mi>n</mi>
      </mfrac>
    </mstyle>
  </mrow>
</semantics></math>
      </p>
      <p><label>(30)</label>
      </p>
    </div><p>
        given <span>n</span> samples. The standard deviation is found given the generalized variance:</p><p>
        and thus the standard error in roll and pitch is calculated given the standard deviation and the number of samples as follows:</p></div><div><p>It can be seen in <a href="#drones-08-00652-f008">Figure 8</a> that the relationship between the estimated standard error and the true error is approximately linear. The accuracy of the first orbit is likely due to chance; this is a good indicator that the factors dictating an accurate positional estimate are indeed the variance in pitch and roll throughout the orbit and the total number of samples. The inverse of the trend-line may be used as an estimate for the Circular Error Probable (CEP), where
        </p><div id="FD33-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi>C</mi>
    <mi>E</mi>
    <mi>P</mi>
    <mo>=</mo>
    <mn>1205</mn>
    <mo>√ó</mo>
    <mi>S</mi>
    <mi>E</mi>
    <mo>‚àí</mo>
    <mn>0.567</mn>
  </mrow>
</semantics></math>
      </p>
      <p><label>(33)</label>
      </p>
    </div><p>
        and </p><math display="inline"><semantics>
  <mrow>
    <mi>C</mi>
    <mi>E</mi>
    <mi>P</mi>
  </mrow>
</semantics></math><p> is estimated in kilometers.</p></div></section><section id="sec4dot3-drones-08-00652" type=""><h4 data-nested="2">  4.3. Initial Conditions</h4><div><p>We demonstrate here that the initial value of the camera rotation matrix has little effect on the final position estimate. This is significant because, as shown in <a href="#sec3-drones-08-00652">Section 3</a>, many factors may cause changes in the relative orientation between the camera and the AHRS. Choosing orbit 5 as an example, we show that as long as the estimated boresight of the camera is accurate to within a hemisphere of tolerance (that is, the estimated boresight is within </p><math display="inline"><semantics>
  <msup>
    <mn>90</mn>
    <mo>‚àò</mo>
  </msup>
</semantics></math><p> of the true boresight), the algorithm will converge near the true location. A graphical representation of the first iteration given various camera calibrations is shown in <a href="#drones-08-00652-f009">Figure 9</a>. In each case, with a position error less than 90¬∞, the position estimate converged to within the estimated CEP. It can be seen in <a href="#drones-08-00652-t003">Table 3</a> that an initial calibration error beyond 90¬∞ results in a position estimate on the opposite side of the Earth.</p></div></section><section id="sec4dot4-drones-08-00652" type=""><h4 data-nested="2">  4.4. Simulation of Wind Effects</h4><p>The flight trial was conducted with modest amounts of wind, utilizing GPS to conduct an orbit. We postulate that in the presence of wind, the position is better attained by fixing the pitch and roll of the aircraft, such that a constant yaw rate is achieved in the local NED frame. This approach does not require GPS to perform an orbit and only relies on compass heading and inertial attitude sensors to follow a trajectory. Simulation results strongly indicate that GPS is not required but is in fact detrimental to the orbital method of position estimation. As a consequence of not using GPS, the aircraft is subjected to lateral drift in the local NED frame. The amount of drift experienced by the aircraft during an orbit is, however, minimal in comparison to the scale to which position is being estimated.</p><p>We used JSBSim to simulate the aircraft‚Äôs flight dynamics, in conjunction with Ardupilot‚Äôs software-in-the-loop simulator to generate synthesized motion-blurred imagery. The images were generated following the methodology in [<a href="#B13-drones-08-00652">13</a>], factoring in the motion blur experienced by the camera. A strong southerly wind of 54 km/h was added to the simulation. An orbit was performed using GPS, and another was performed using a fixed pitch and roll angle with GPS disabled. During the fixed-attitude orbit, the aircraft drifted 1.05 km. The simulated trajectory of the GPS-denied orbit can be seen in <a href="#drones-08-00652-f010">Figure 10</a>.</p><p>Applying the same methodology, the position was calculated for both the GPS-guided orbit and the GPS-denied orbit. The control protocol of the GPS-guided orbit was to maintain a ground track at a fixed radius about a point in an identical manner to what was performed in the real flight test. This method adjusts the pitch and roll of the vehicle to compensate for wind and altitude. Up-wind sections of the orbit are prolonged in time, and down-wind sections are contracted in time. The resulting position estimates are heavily biased towards the prolonged portion of the orbit, which consequently skews the mean calculation. Under high wind conditions, it can be seen that following a fixed-radius ground track is not optimal for position calculation. The resulting position error was 18.27 km, as seen in <a href="#drones-08-00652-f011">Figure 11</a>. By contrast, the fixed-attitude orbit achieved a position error of 2.29 km, despite being subjected to over 1 km of drift. This highlights the importance of fixing the aircraft attitude rather than following a constant-radius orbit.</p></section></section><section id="sec5-drones-08-00652" type="discussion"><h2 data-nested="1">  5. Discussion</h2><p>The results presented in <a href="#sec4-drones-08-00652">Section 4</a> demonstrate that strapdown celestial navigation as a modular solution has potential use in Global Navigation Satellite System (GNSS)-denied UAV navigation. The celestial camera was mounted to the airframe, separate from the autopilot, but still utilizing the AHRS orientation information coming from the autopilot. This has significant implications for integration into SWAP-C airframes, in which the inclusion of stabilization hardware adds unwanted mass. With a modern GPS receiver weighing only a few grams and producing estimates to within 1m accuracy, it is understandable that, given the choice, an alternative celestial navigation solution would not be included. Yet, GNSS denial is increasing in prevalence, and alternative navigation solutions must be explored if a UAV is to operate under such circumstances.</p><p>The most profound outcome from this study is the celestial system‚Äôs independence from the initial conditions. Provided a functional AHRS and navigation system which is capable of performing an orbit, the celestial system can produce a position estimate from any unknown position on the globe, with a camera that is aligned to within 90¬∞. The remaining source of error lies in the clock drift, which is typically equal to 3 ppm (0.3 s per 24 h) for a modern real-time clock. This has significant implications for long endurance aircraft, which may need to operate for many hours in RF contested environments. It has been shown that the rate of divergence in dead reckoning navigation is substantial without a velocity estimate [<a href="#B22-drones-08-00652">22</a>], particularly in consumer grade systems. Even with velocity measurements, in the presence of wind, a tactical grade aircraft may drift by 10 km per hour. In such cases, over the course of multiple hours, the precision offered by the proposed method is a vast improvement.</p><p>For loitering aircraft, this method of localization is convenient, as the flight plan need not change. For aircraft travelling significant distances, it may be sufficient to intermittently perform a single orbit, and utilize the more erroneous position estimate for course correction. The act of performing an orbit through a full compass rotation effectively nullifies the biases and offsets between the AHRS and the celestial camera. The main source of error for each independent position estimate during an orbit, is the misalignment of the estimated zenith with the true zenith. This may be caused by a number of factors, such as aerodynamic loading causing minor perturbations in the camera‚Äôs orientation relative to the autopilot, improper alignment or calibration of the inertial system, or simply due to estimation errors caused by improperly estimated centrifugal acceleration. In each of these cases, it is expected that the error remains approximately constant throughout an orbit. Consequently, zenith errors at one particular azimuth are cancelled out by the zenith errors at the opposing (180¬∞ offset) azimuth. This is the reason why such a method is capable of working with almost arbitrary levels of initial error.</p><p>This flight was conducted using GPS to maintain the orbital position. We recognize that, in a true GNSS denied environment, wind errors will cause the aircraft to drift during an orbit. While this may introduce some error into the position estimate, we show in <a href="#sec4-drones-08-00652">Section 4</a> that the primary factor governing the accuracy of a position estimate is the variance in the zenith-angle throughout the orbit. The circular position errors seen for example in <a href="#drones-08-00652-f007">Figure 7</a>, are not caused by the change in aircraft position. They are caused by a misalignment of the optical system, and consequently may be reproduced by simply maintaining constant roll and pitch throughout a full compass rotation. In <a href="#sec4dot4-drones-08-00652">Section 4.4</a> we show that even in high wind conditions, the appropriate strategy is to maintain roll rate and accept that there will be positional drift throughout the orbit. This strongly suggests that the presence of GPS during the flight test offered no advantages to the algorithm.</p><p>An obvious limitation of this method is its dependence on sky visibility. Some research has shown that short-wave infrared cameras offer a daylight visible alternative to visible spectrum cameras [<a href="#B23-drones-08-00652">23</a>,<a href="#B24-drones-08-00652">24</a>]. These tend to have far lower signal-to-noise ratios, resulting in more erroneous measurements. It may be the case that the proposed positioning method is capable of nullifying the increased observational error from short-wave infrared observations. This may be a potential topic for future research.</p></section><section id="sec6-drones-08-00652" type="conclusions"><h2 data-nested="1">  6. Conclusions</h2><p>This study proposed a method for obtaining more accurate positional estimates from a modular strapdown celestial navigation system. We hypothesized that by flying in an orbital motion, the errors in estimated zenith angles would cancel one another at opposing azimuths. The methodology was tested in a real flight, demonstrating that the position can be routinely estimated to within 4 km by performing orbits at a fixed altitude and airspeed. Throughout each orbit, positional estimates are generated from the individual celestial images, and these positions are averaged at the conclusion of the orbit. We show that by recursively estimating the mean position and using this position to recalibrate the orientation of the camera, the algorithm is capable of converging near the true location. Testing found that the algorithm is robust against initial conditions, requiring no knowledge of the prior position and only requiring the camera to be aligned to within a hemisphere of tolerance.</p></section>
  </div><div>
    <section><h2>Author Contributions</h2><p>Conceptualization, S.T. and J.C.; methodology, S.T.; software, S.T.; validation, S.T.; formal analysis, S.T.; investigation, S.T.; resources, J.C.; data curation, S.T. and J.C.; writing‚Äîoriginal draft preparation, S.T.; writing‚Äîreview and editing, J.C.; visualization, S.T.; supervision, J.C.; project administration, J.C. All authors have read and agreed to the published version of the manuscript.</p></section><section><h2>Funding</h2><p>This research received no external funding.</p></section><section><h2>Data Availability Statement</h2><p>The data presented in this study are available on request from the corresponding author.</p></section><section id="html-ack"><h2>Acknowledgments</h2><p>This work was supported by Scope Global Pty Ltd. under the Commonwealth Scholarships Program, and the Commonwealth of South Australia under the Australian Government Research Training Program.</p></section><section><h2>Conflicts of Interest</h2><p>The authors declare no conflict of interest.</p></section><section id="html-references_list"><h2>References</h2><ol><li id="B1-drones-08-00652" data-content="1.">Gatty, H. Aerial Navigation‚ÄîMethods and Equipment. <span>SAE Trans.</span> <b>1932</b>, <span>27</span>, 153‚Äì170. [<a href="https://scholar.google.com/scholar_lookup?title=Aerial+Navigation%E2%80%94Methods+and+Equipment&amp;author=Gatty,+H.&amp;publication_year=1932&amp;journal=SAE+Trans.&amp;volume=27&amp;pages=153%E2%80%93170&amp;doi=10.4271/320042" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.4271/320042" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B2-drones-08-00652" data-content="2.">Pappalardi, F.; Dunham, S.; LeBlang, M.; Jones, T.; Bangert, J.; Kaplan, G. Alternatives to GPS. In Proceedings of the MTS/IEEE Oceans 2001. An Ocean Odyssey. Conference Proceedings (IEEE Cat. No. 01CH37295), Honolulu, HI, USA, 5‚Äì8 November 2001; IEEE: New York, NY, USA, 2001; Volume 3, pp. 1452‚Äì1459. [<a href="https://scholar.google.com/scholar_lookup?title=Alternatives+to+GPS&amp;conference=Proceedings+of+the+MTS/IEEE+Oceans+2001.+An+Ocean+Odyssey.+Conference+Proceedings+(IEEE+Cat.+No.+01CH37295)&amp;author=Pappalardi,+F.&amp;author=Dunham,+S.&amp;author=LeBlang,+M.&amp;author=Jones,+T.&amp;author=Bangert,+J.&amp;author=Kaplan,+G.&amp;publication_year=2001&amp;pages=1452%E2%80%931459&amp;doi=10.1109/OCEANS.2001.968047" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1109/OCEANS.2001.968047" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B3-drones-08-00652" data-content="3.">Ali, J.; Zhang, C.; Fang, J. An algorithm for astro-inertial navigation using CCD star sensors. <span>Aerosp. Sci. Technol.</span> <b>2006</b>, <span>10</span>, 449‚Äì454. [<a href="https://scholar.google.com/scholar_lookup?title=An+algorithm+for+astro-inertial+navigation+using+CCD+star+sensors&amp;author=Ali,+J.&amp;author=Zhang,+C.&amp;author=Fang,+J.&amp;publication_year=2006&amp;journal=Aerosp.+Sci.+Technol.&amp;volume=10&amp;pages=449%E2%80%93454&amp;doi=10.1016/j.ast.2006.01.004" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1016/j.ast.2006.01.004" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B4-drones-08-00652" data-content="4.">Ting, F.; Xiaoming, H. Inertial/celestial integrated navigation algorithm for long endurance unmanned aerial vehicle. <span>Acta Tech. CSAV (Ceskoslov. Akad. Ved.)</span> <b>2017</b>, <span>62</span>, 205‚Äì217. [<a href="https://scholar.google.com/scholar_lookup?title=Inertial/celestial+integrated+navigation+algorithm+for+long+endurance+unmanned+aerial+vehicle&amp;author=Ting,+F.&amp;author=Xiaoming,+H.&amp;publication_year=2017&amp;journal=Acta+Tech.+CSAV+(Ceskoslov.+Akad.+Ved.)&amp;volume=62&amp;pages=205%E2%80%93217" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li><li id="B5-drones-08-00652" data-content="5.">Levine, S.; Dennis, R.; Bachman, K.L. Strapdown Astro-Inertial Navigation Utilizing the Optical Wide-angle Lens Startracker. <span>Navigation</span> <b>1990</b>, <span>37</span>, 347‚Äì362. [<a href="https://scholar.google.com/scholar_lookup?title=Strapdown+Astro-Inertial+Navigation+Utilizing+the+Optical+Wide-angle+Lens+Startracker&amp;author=Levine,+S.&amp;author=Dennis,+R.&amp;author=Bachman,+K.L.&amp;publication_year=1990&amp;journal=Navigation&amp;volume=37&amp;pages=347%E2%80%93362&amp;doi=10.1002/j.2161-4296.1990.tb01561.x" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1002/j.2161-4296.1990.tb01561.x" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B6-drones-08-00652" data-content="6.">Chen, H.; Gao, H.; Zhang, H. Integrated navigation approaches of vehicle aided by the strapdown celestial angles. <span>Int. J. Adv. Robot. Syst.</span> <b>2020</b>, <span>17</span>, 1729881420932008. [<a href="https://scholar.google.com/scholar_lookup?title=Integrated+navigation+approaches+of+vehicle+aided+by+the+strapdown+celestial+angles&amp;author=Chen,+H.&amp;author=Gao,+H.&amp;author=Zhang,+H.&amp;publication_year=2020&amp;journal=Int.+J.+Adv.+Robot.+Syst.&amp;volume=17&amp;pages=1729881420932008&amp;doi=10.1177/1729881420932008" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1177/1729881420932008" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B7-drones-08-00652" data-content="7.">Gai, E.; Daly, K.; Harrison, J.; Lemos, L. Star-sensor-based satellite attitude/attitude rate estimator. <span>J. Guid. Control Dyn.</span> <b>1985</b>, <span>8</span>, 560‚Äì565. [<a href="https://scholar.google.com/scholar_lookup?title=Star-sensor-based+satellite+attitude/attitude+rate+estimator&amp;author=Gai,+E.&amp;author=Daly,+K.&amp;author=Harrison,+J.&amp;author=Lemos,+L.&amp;publication_year=1985&amp;journal=J.+Guid.+Control+Dyn.&amp;volume=8&amp;pages=560%E2%80%93565&amp;doi=10.2514/3.56393" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.2514/3.56393" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B8-drones-08-00652" data-content="8.">Wang, J.; Chun, J. Attitude determination using a single star sensor and a star density table. <span>J. Guid. Control Dyn.</span> <b>2006</b>, <span>29</span>, 1329‚Äì1338. [<a href="https://scholar.google.com/scholar_lookup?title=Attitude+determination+using+a+single+star+sensor+and+a+star+density+table&amp;author=Wang,+J.&amp;author=Chun,+J.&amp;publication_year=2006&amp;journal=J.+Guid.+Control+Dyn.&amp;volume=29&amp;pages=1329%E2%80%931338&amp;doi=10.2514/1.17249" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.2514/1.17249" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B9-drones-08-00652" data-content="9.">Mao, X.; Du, X.; Fang, H. Precise attitude determination strategy for spacecraft based on information fusion of attitude sensors: Gyros/GPS/Star-sensor. <span>Int. J. Aeronaut. Space Sci.</span> <b>2013</b>, <span>14</span>, 91‚Äì98. [<a href="https://scholar.google.com/scholar_lookup?title=Precise+attitude+determination+strategy+for+spacecraft+based+on+information+fusion+of+attitude+sensors:+Gyros/GPS/Star-sensor&amp;author=Mao,+X.&amp;author=Du,+X.&amp;author=Fang,+H.&amp;publication_year=2013&amp;journal=Int.+J.+Aeronaut.+Space+Sci.&amp;volume=14&amp;pages=91%E2%80%9398&amp;doi=10.5139/IJASS.2013.14.1.91" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.5139/IJASS.2013.14.1.91" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B10-drones-08-00652" data-content="10.">Guo, C.; Tong, X.; Liu, S.; Lu, X.; Chen, P.; Jin, Y.; Xie, H. High-precision attitude estimation method of star sensors and gyro based on complementary filter and unscented Kalman filter. <span>Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.</span> <b>2017</b>, <span>42</span>, 49‚Äì53. [<a href="https://scholar.google.com/scholar_lookup?title=High-precision+attitude+estimation+method+of+star+sensors+and+gyro+based+on+complementary+filter+and+unscented+Kalman+filter&amp;author=Guo,+C.&amp;author=Tong,+X.&amp;author=Liu,+S.&amp;author=Lu,+X.&amp;author=Chen,+P.&amp;author=Jin,+Y.&amp;author=Xie,+H.&amp;publication_year=2017&amp;journal=Int.+Arch.+Photogramm.+Remote+Sens.+Spat.+Inf.+Sci.&amp;volume=42&amp;pages=49%E2%80%9353&amp;doi=10.5194/isprs-archives-XLII-3-W1-49-2017" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.5194/isprs-archives-XLII-3-W1-49-2017" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B11-drones-08-00652" data-content="11.">De Almeida Martins, F.; Carrara, V.; d‚ÄôAmore, R. Positionless Attitude Estimation with Integrated Star and Horizon Sensors. <span>IEEE Access</span> <b>2023</b>, <span>12</span>, 2340‚Äì2348. [<a href="https://scholar.google.com/scholar_lookup?title=Positionless+Attitude+Estimation+with+Integrated+Star+and+Horizon+Sensors&amp;author=De+Almeida+Martins,+F.&amp;author=Carrara,+V.&amp;author=d%E2%80%99Amore,+R.&amp;publication_year=2023&amp;journal=IEEE+Access&amp;volume=12&amp;pages=2340%E2%80%932348&amp;doi=10.1109/ACCESS.2023.3348077" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1109/ACCESS.2023.3348077" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B12-drones-08-00652" data-content="12.">Gao, Z.; Wang, H.; Wang, W.; Xu, Y. SIMU/Triple star sensors integrated navigation method of HALE UAV based on atmospheric refraction correction. <span>J. Navig.</span> <b>2022</b>, <span>75</span>, 704‚Äì726. [<a href="https://scholar.google.com/scholar_lookup?title=SIMU/Triple+star+sensors+integrated+navigation+method+of+HALE+UAV+based+on+atmospheric+refraction+correction&amp;author=Gao,+Z.&amp;author=Wang,+H.&amp;author=Wang,+W.&amp;author=Xu,+Y.&amp;publication_year=2022&amp;journal=J.+Navig.&amp;volume=75&amp;pages=704%E2%80%93726&amp;doi=10.1017/S037346332100093X" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1017/S037346332100093X" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B13-drones-08-00652" data-content="13.">Teague, S.; Chahl, J. Imagery synthesis for drone celestial navigation simulation. <span>Drones</span> <b>2022</b>, <span>6</span>, 207. [<a href="https://scholar.google.com/scholar_lookup?title=Imagery+synthesis+for+drone+celestial+navigation+simulation&amp;author=Teague,+S.&amp;author=Chahl,+J.&amp;publication_year=2022&amp;journal=Drones&amp;volume=6&amp;pages=207&amp;doi=10.3390/drones6080207" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.3390/drones6080207" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B14-drones-08-00652" data-content="14.">Teague, S.; Chahl, J. Strapdown Celestial Attitude Estimation from Long Exposure Images for UAV Navigation. <span>Drones</span> <b>2023</b>, <span>7</span>, 52. [<a href="https://scholar.google.com/scholar_lookup?title=Strapdown+Celestial+Attitude+Estimation+from+Long+Exposure+Images+for+UAV+Navigation&amp;author=Teague,+S.&amp;author=Chahl,+J.&amp;publication_year=2023&amp;journal=Drones&amp;volume=7&amp;pages=52&amp;doi=10.3390/drones7010052" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.3390/drones7010052" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B15-drones-08-00652" data-content="15.">Zhang, H.; Zhang, C.; Tong, S.; Wang, R.; Li, C.; Tian, Y.; He, D.; Jiang, D.; Pu, J. Celestial Navigation and Positioning Method Based on Super-Large Field of View Star Sensors. In Proceedings of the China Satellite Navigation Conference, Jinan, China, 22‚Äì24 May 2024; Springer: Berlin/Heidelberg, Germany, 2023; pp. 475‚Äì487. [<a href="https://scholar.google.com/scholar_lookup?title=Celestial+Navigation+and+Positioning+Method+Based+on+Super-Large+Field+of+View+Star+Sensors&amp;conference=Proceedings+of+the+China+Satellite+Navigation+Conference&amp;author=Zhang,+H.&amp;author=Zhang,+C.&amp;author=Tong,+S.&amp;author=Wang,+R.&amp;author=Li,+C.&amp;author=Tian,+Y.&amp;author=He,+D.&amp;author=Jiang,+D.&amp;author=Pu,+J.&amp;publication_year=2023&amp;pages=475%E2%80%93487&amp;doi=10.1007/978-981-99-6928-9_41" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1007/978-981-99-6928-9_41" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B16-drones-08-00652" data-content="16.">Van Allen, J.A. Basic principles of celestial navigation. <span>Am. J. Phys.</span> <b>2004</b>, <span>72</span>, 1418‚Äì1424. [<a href="https://scholar.google.com/scholar_lookup?title=Basic+principles+of+celestial+navigation&amp;author=Van+Allen,+J.A.&amp;publication_year=2004&amp;journal=Am.+J.+Phys.&amp;volume=72&amp;pages=1418%E2%80%931424&amp;doi=10.1119/1.1778391" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1119/1.1778391" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B17-drones-08-00652" data-content="17.">Liebe, C.C. Accuracy performance of star trackers-a tutorial. <span>IEEE Trans. Aerosp. Electron. Syst.</span> <b>2002</b>, <span>38</span>, 587‚Äì599. [<a href="https://scholar.google.com/scholar_lookup?title=Accuracy+performance+of+star+trackers-a+tutorial&amp;author=Liebe,+C.C.&amp;publication_year=2002&amp;journal=IEEE+Trans.+Aerosp.+Electron.+Syst.&amp;volume=38&amp;pages=587%E2%80%93599&amp;doi=10.1109/TAES.2002.1008988" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1109/TAES.2002.1008988" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B18-drones-08-00652" data-content="18.">Delabie, T.; Schutter, J.D.; Vandenbussche, B. An accurate and efficient gaussian fit centroiding algorithm for star trackers. <span>J. Astronaut. Sci.</span> <b>2014</b>, <span>61</span>, 60‚Äì84. [<a href="https://scholar.google.com/scholar_lookup?title=An+accurate+and+efficient+gaussian+fit+centroiding+algorithm+for+star+trackers&amp;author=Delabie,+T.&amp;author=Schutter,+J.D.&amp;author=Vandenbussche,+B.&amp;publication_year=2014&amp;journal=J.+Astronaut.+Sci.&amp;volume=61&amp;pages=60%E2%80%9384&amp;doi=10.1007/s40295-015-0034-4" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1007/s40295-015-0034-4" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B19-drones-08-00652" data-content="19.">Teague, S.; Chahl, J. Bootstrap geometric ground calibration method for wide angle star sensors. <span>JOSA A</span> <b>2024</b>, <span>41</span>, 654‚Äì663. [<a href="https://scholar.google.com/scholar_lookup?title=Bootstrap+geometric+ground+calibration+method+for+wide+angle+star+sensors&amp;author=Teague,+S.&amp;author=Chahl,+J.&amp;publication_year=2024&amp;journal=JOSA+A&amp;volume=41&amp;pages=654%E2%80%93663&amp;doi=10.1364/JOSAA.517943" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1364/JOSAA.517943" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B20-drones-08-00652" data-content="20.">Wei, X.; Zhang, G.; Jiang, J. Star identification algorithm based on log-polar transform. <span>J. Aerosp. Comput. Inf. Commun.</span> <b>2009</b>, <span>6</span>, 483‚Äì490. [<a href="https://scholar.google.com/scholar_lookup?title=Star+identification+algorithm+based+on+log-polar+transform&amp;author=Wei,+X.&amp;author=Zhang,+G.&amp;author=Jiang,+J.&amp;publication_year=2009&amp;journal=J.+Aerosp.+Comput.+Inf.+Commun.&amp;volume=6&amp;pages=483%E2%80%93490&amp;doi=10.2514/1.30393" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.2514/1.30393" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B21-drones-08-00652" data-content="21.">Lawrence, J.; Bernal, J.; Witzgall, C. A purely algebraic justification of the Kabsch-Umeyama algorithm. <span>J. Res. Natl. Inst. Stand. Technol.</span> <b>2019</b>, <span>124</span>, 1. [<a href="https://scholar.google.com/scholar_lookup?title=A+purely+algebraic+justification+of+the+Kabsch-Umeyama+algorithm&amp;author=Lawrence,+J.&amp;author=Bernal,+J.&amp;author=Witzgall,+C.&amp;publication_year=2019&amp;journal=J.+Res.+Natl.+Inst.+Stand.+Technol.&amp;volume=124&amp;pages=1&amp;doi=10.6028/jres.124.028" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.6028/jres.124.028" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B22-drones-08-00652" data-content="22.">Gebre-Egziabher, D.; Powell, J.; Enge, P. Design and performance analysis of a low-cost aided dead reckoning navigation system. <span>Gyroscopy Navig.</span> <b>2001</b>, <span>4</span>, 83‚Äì92. [<a href="https://scholar.google.com/scholar_lookup?title=Design+and+performance+analysis+of+a+low-cost+aided+dead+reckoning+navigation+system&amp;author=Gebre-Egziabher,+D.&amp;author=Powell,+J.&amp;author=Enge,+P.&amp;publication_year=2001&amp;journal=Gyroscopy+Navig.&amp;volume=4&amp;pages=83%E2%80%9392" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li><li id="B23-drones-08-00652" data-content="23.">Wang, W.; Wei, X.; Li, J.; Zhang, G. Guide star catalog generation for short-wave infrared (SWIR) all-time star sensor. <span>Rev. Sci. Instrum.</span> <b>2018</b>, <span>89</span>. [<a href="https://scholar.google.com/scholar_lookup?title=Guide+star+catalog+generation+for+short-wave+infrared+(SWIR)+all-time+star+sensor&amp;author=Wang,+W.&amp;author=Wei,+X.&amp;author=Li,+J.&amp;author=Zhang,+G.&amp;publication_year=2018&amp;journal=Rev.+Sci.+Instrum.&amp;volume=89&amp;doi=10.1063/1.5023157" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1063/1.5023157" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B24-drones-08-00652" data-content="24.">Ni, Y.; Wang, X.; Dai, D.; Tan, W.; Qin, S. Adaptive section non-uniformity correction method of short-wave infrared star images for a star tracker. <span>Appl. Opt.</span> <b>2022</b>, <span>61</span>, 6992‚Äì6999. [<a href="https://scholar.google.com/scholar_lookup?title=Adaptive+section+non-uniformity+correction+method+of+short-wave+infrared+star+images+for+a+star+tracker&amp;author=Ni,+Y.&amp;author=Wang,+X.&amp;author=Dai,+D.&amp;author=Tan,+W.&amp;author=Qin,+S.&amp;publication_year=2022&amp;journal=Appl.+Opt.&amp;volume=61&amp;pages=6992%E2%80%936999&amp;doi=10.1364/AO.457458" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1364/AO.457458" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li></ol></section><section id="FiguresandTables" type="display-objects"><div id="drones-08-00652-f001">
    <p><b>Figure 1.</b>
      The circles around points </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p> show the intersection of their respective planes with the terrestrial sphere, forming circles about which the observation could have been made. Points </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p> are taken from the observed stars, located such that their respective star is observed at the zenith. The zenith-angles </p><math display="inline"><semantics>
  <msub>
    <mi>Œ∂</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>Œ∂</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p> represent the angle at which the stars were actually observed. In this case, two stars were observed, resulting in two potential points (</p><math display="inline"><semantics>
  <msub>
    <mi>P</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>P</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p>) at which this observation could have been made. Additional observations serve to reduce this ambiguity.
</p><!--     <p><a class="html-figpopup" href="#fig_body_display_drones-08-00652-f001">
      Click here to enlarge figure
    </a></p> -->

  </div>
<div id="fig_body_display_drones-08-00652-f001">
  <div> <p><b>Figure 1.</b>
      The circles around points </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p> show the intersection of their respective planes with the terrestrial sphere, forming circles about which the observation could have been made. Points </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p> are taken from the observed stars, located such that their respective star is observed at the zenith. The zenith-angles </p><math display="inline"><semantics>
  <msub>
    <mi>Œ∂</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>Œ∂</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p> represent the angle at which the stars were actually observed. In this case, two stars were observed, resulting in two potential points (</p><math display="inline"><semantics>
  <msub>
    <mi>P</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>P</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p>) at which this observation could have been made. Additional observations serve to reduce this ambiguity.</p></div>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g001.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g001.png" alt="Drones 08 00652 g001" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g001.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g001.png"></p>
</div>
<div id="drones-08-00652-f002">
  
  <p><b>Figure 2.</b>
      Star tracker operating on video footage captured in-flight. Image intensity is amplified 10√ó.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f002">
  <p><b>Figure 2.</b>
      Star tracker operating on video footage captured in-flight. Image intensity is amplified 10√ó.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g002.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g002.png" alt="Drones 08 00652 g002" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g002.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g002.png"></p>
</div>
<div id="drones-08-00652-f003">
  
  <p><b>Figure 3.</b>
      Platform layout of the autopilot and celestial payload within the airframe.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f003">
  <p><b>Figure 3.</b>
      Platform layout of the autopilot and celestial payload within the airframe.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g003.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g003.png" alt="Drones 08 00652 g003" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g003.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g003.png"></p>
</div>
<div id="drones-08-00652-f004">
  
  <p><b>Figure 4.</b>
      The celestial payload, consisting of a Raspberry Pi 5 and an Alvium 1800 U-240 monochrome sensor fitted with a 6 mm f/1.4 wide angle lens.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f004">
  <p><b>Figure 4.</b>
      The celestial payload, consisting of a Raspberry Pi 5 and an Alvium 1800 U-240 monochrome sensor fitted with a 6 mm f/1.4 wide angle lens.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g004.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g004.png" alt="Drones 08 00652 g004" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g004.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g004.png"></p>
</div>
<div id="drones-08-00652-f005">
  
  <p><b>Figure 5.</b>
      True camera orientation in the aircraft body frame, as calculated using a combination of stellar observations, aircraft attitude, and GPS data. Data are captured from a straight segment of flight over 89.5 s (895 video frames). <b>Top</b>: Calculated camera yaw angle in aircraft body frame. <b>Middle</b>: Calculated camera pitch angle in aircraft body frame. <b>Bottom</b>: Calculated camera roll angle in aircraft body frame.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f005">
  <p><b>Figure 5.</b>
      True camera orientation in the aircraft body frame, as calculated using a combination of stellar observations, aircraft attitude, and GPS data. Data are captured from a straight segment of flight over 89.5 s (895 video frames). <b>Top</b>: Calculated camera yaw angle in aircraft body frame. <b>Middle</b>: Calculated camera pitch angle in aircraft body frame. <b>Bottom</b>: Calculated camera roll angle in aircraft body frame.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g005.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g005.png" alt="Drones 08 00652 g005" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g005.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g005.png"></p>
</div>
<div id="drones-08-00652-f006">
  
  <p><b>Figure 6.</b>
      Position estimation over a an 89.5 s straight segment of flight. It can be seen that, by assuming the camera orientation is fixed, significant positional error makes its way into the estimate. <b>Top</b>: Latitude estimated using celestial imagery with Ardupilot AHRS. <b>Bottom</b>: Longitude estimated using celestial imagery with Ardupilot AHRS.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f006">
  <p><b>Figure 6.</b>
      Position estimation over a an 89.5 s straight segment of flight. It can be seen that, by assuming the camera orientation is fixed, significant positional error makes its way into the estimate. <b>Top</b>: Latitude estimated using celestial imagery with Ardupilot AHRS. <b>Bottom</b>: Longitude estimated using celestial imagery with Ardupilot AHRS.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g006.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g006.png" alt="Drones 08 00652 g006" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g006.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g006.png"></p>
</div>
<div id="drones-08-00652-f007">
  
  <p><b>Figure 7.</b>
      Visualization of the position estimates as the algorithm converges on the location of the aircraft. Each blue point represents an independent position that was calculated from an image frame, the red dot represent the estimated position taken from the mean of the individual estimates, and the yellow dot represents the true center of the aircraft‚Äôs orbit.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f007">
  <p><b>Figure 7.</b>
      Visualization of the position estimates as the algorithm converges on the location of the aircraft. Each blue point represents an independent position that was calculated from an image frame, the red dot represent the estimated position taken from the mean of the individual estimates, and the yellow dot represents the true center of the aircraft‚Äôs orbit.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g007.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g007.png" alt="Drones 08 00652 g007" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g007.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g007.png"></p>
</div>
<div id="drones-08-00652-f008">
  
  <p><b>Figure 8.</b>
      Plot of the standard error in pitch and variance versus the true error. It can be seen that the relationship is approximately linear.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f008">
  <p><b>Figure 8.</b>
      Plot of the standard error in pitch and variance versus the true error. It can be seen that the relationship is approximately linear.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g008.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g008.png" alt="Drones 08 00652 g008" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g008.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g008.png"></p>
</div>
<div id="drones-08-00652-f009">
  
  <p><b>Figure 9.</b>
      Visualization of position estimates for a given camera misalignment. It can be seen that as long as the camera is aligned to within 90¬∞, the mean estimated position provides a good approximation of the true position.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f009">
  <p><b>Figure 9.</b>
      Visualization of position estimates for a given camera misalignment. It can be seen that as long as the camera is aligned to within 90¬∞, the mean estimated position provides a good approximation of the true position.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g009.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g009.png" alt="Drones 08 00652 g009" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g009.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g009.png"></p>
</div>
<div id="drones-08-00652-f010">
  
  <p><b>Figure 10.</b>
      Simulated trajectory of the aircraft during a GPS-denied orbit in high wind conditions.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f010">
  <p><b>Figure 10.</b>
      Simulated trajectory of the aircraft during a GPS-denied orbit in high wind conditions.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g010.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g010.png" alt="Drones 08 00652 g010" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g010.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g010.png"></p>
</div>
<div id="drones-08-00652-f011">
  
  <p><b>Figure 11.</b>
      Comparison of position estimation in high wind conditions. It can be seen that the course followed by a GPS-guided orbit introduces errors in the estimation process. The GPS-denied orbit follows a fixed-attitude orbit, resulting in drift, but ultimately yielding a more accurate position.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f011">
  <p><b>Figure 11.</b>
      Comparison of position estimation in high wind conditions. It can be seen that the course followed by a GPS-guided orbit introduces errors in the estimation process. The GPS-denied orbit follows a fixed-attitude orbit, resulting in drift, but ultimately yielding a more accurate position.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g011.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g011.png" alt="Drones 08 00652 g011" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g011.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g011.png"></p>
</div>
<div id="drones-08-00652-t001">
  
  <p><b>Table 1.</b>
    Flight plan for the test flight.
  </p>
</div>
<div id="table_body_display_drones-08-00652-t001">
  

      <p><b>Table 1.</b>
    Flight plan for the test flight.</p>
      <table>
        <thead><tr><th>Description</th><th>Direction</th><th>Repeats</th><th>Radius (m)</th><th>Altitude (m)</th></tr></thead><tbody><tr><td>Takeoff</td><td> </td><td> </td><td> </td><td>30</td></tr><tr><td>Orbit to Altitude</td><td>CCW</td><td>4</td><td>300</td><td>800</td></tr><tr><td>Straight Legs</td><td>N/S</td><td>6</td><td>2500</td><td>800</td></tr><tr><td>Orbit</td><td>CW</td><td>1</td><td>1200</td><td>800</td></tr><tr><td>Orbit</td><td>CW</td><td>2</td><td>600</td><td>800</td></tr><tr><td>Orbit</td><td>CCW</td><td>1</td><td>600</td><td>800</td></tr><tr><td>Orbit</td><td>CCW</td><td>1</td><td>1200</td><td>800</td></tr><tr><td>Orbit to Altitude</td><td>CCW</td><td>1</td><td>300</td><td>100</td></tr><tr><td>Land</td><td> </td><td> </td><td> </td><td>0</td></tr></tbody>
      </table>



</div>
<div id="drones-08-00652-t002">
  
  <p><b>Table 2.</b>
    Celestial positioning results.
  </p>
</div>
<div id="table_body_display_drones-08-00652-t002">
  

      <p><b>Table 2.</b>
    Celestial positioning results.</p>
      <table>
        <thead><tr><th>Description</th><th>Direction</th><th>Radius (m)</th><th>Pos. Error (km)</th><th>Iterations</th></tr></thead><tbody><tr><td>Orbit to Altitude (1)</td><td>CCW</td><td>300</td><td>3.56</td><td>5</td></tr><tr><td>Orbit to Altitude (2)</td><td>CCW</td><td>300</td><td>7.18</td><td>4</td></tr><tr><td>Orbit to Altitude (3)</td><td>CCW</td><td>300</td><td>9.67</td><td>4</td></tr><tr><td>Orbit to Altitude (4)</td><td>CCW</td><td>300</td><td>9.89</td><td>4</td></tr><tr><td>Orbit (5)</td><td>CW</td><td>1200</td><td>2.21</td><td>4</td></tr><tr><td>Orbit (6)</td><td>CW</td><td>600</td><td>3.48</td><td>5</td></tr><tr><td>Orbit (7)</td><td>CW</td><td>600</td><td>2.54</td><td>5</td></tr><tr><td>Orbit (8)</td><td>CCW</td><td>600</td><td>1.73</td><td>5</td></tr><tr><td>Orbit (9)</td><td>CCW</td><td>1200</td><td>2.90</td><td>4</td></tr><tr><td>Orbit to Altitude (10)</td><td>CCW</td><td>300</td><td>7.16</td><td>6</td></tr></tbody>
      </table>



</div>
<div id="drones-08-00652-t003">
  
  <p><b>Table 3.</b>
    Sensitivity to initial camera calibration.
  </p>
</div>
<div id="table_body_display_drones-08-00652-t003">
  

      <p><b>Table 3.</b>
    Sensitivity to initial camera calibration.</p>
      <table>
        <thead><tr><th>Initial Orientation Error</th><th>Final Pos. Error (km)</th><th>Iterations</th></tr></thead><tbody><tr><td>Nil (calibrated)</td><td>2.47</td><td>2</td></tr><tr><td>Initial Guess (<math display="inline"><semantics>
  <mrow>
    <mo>‚âà</mo>
    <msup>
      <mn>5</mn>
      <mo>‚àò</mo>
    </msup>
  </mrow>
</semantics></math>)</td><td>2.49</td><td>4</td></tr><tr><td>45¬∞</td><td>2.46</td><td>5</td></tr><tr><td>60¬∞</td><td>2.51</td><td>5</td></tr><tr><td>85¬∞</td><td>2.49</td><td>6</td></tr><tr><td>120¬∞</td><td>19,987.66</td><td>5</td></tr></tbody>
      </table>



</div>
</section><section><table><tbody><tr id=""><td></td><td><p><b>Disclaimer/Publisher‚Äôs Note:</b> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></td></tr></tbody></table></section>
    <section id="html-copyright"><br>¬© 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer">https://creativecommons.org/licenses/by/4.0/</a>).</section>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using eSIMs with devices that only have a physical SIM slot via a 9eSIM SIM car (334 pts)]]></title>
            <link>https://neilzone.co.uk/2025/01/using-esims-with-devices-that-only-have-a-physical-sim-slot-via-a-9esim-sim-card-with-android-and-linux/</link>
            <guid>42767584</guid>
            <pubDate>Mon, 20 Jan 2025 11:33:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neilzone.co.uk/2025/01/using-esims-with-devices-that-only-have-a-physical-sim-slot-via-a-9esim-sim-card-with-android-and-linux/">https://neilzone.co.uk/2025/01/using-esims-with-devices-that-only-have-a-physical-sim-slot-via-a-9esim-sim-card-with-android-and-linux/</a>, See on <a href="https://news.ycombinator.com/item?id=42767584">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<article>
    <p>Do you have a phone, tablet, or laptop (or, well, any device‚Ä¶) which will only take a physical SIM, but with which you‚Äôd like to use eSIMs?</p>
<p>Then this is a blogpost for you, as that‚Äôs exactly what this is: a physical SIM, onto which one can provision eSIMs, using software to swap between them.</p>
<h2 id="what-i-bought-from-9esim">What I bought from 9eSIM</h2>
<p>I bought the bundle - SIM and smartcard reader - from <a href="https://www.9esim.com/">9eSIM</a>.</p>
<p>The first shipment from China got lost but, after a bit of waiting, they posted another one without complaint. That was shipped by a different delivery company, and it arrived in just over a week.</p>
<p>In the box, I got:</p>
<ul>
<li>
<p>A SIM card, with the usual push-out sections to change it from normal to micro to nano.</p>
<ul>
<li>The SIM was in a blue envelope, not attached to the cardboard saying ‚Äú9eSIM v2‚Äù - at first, I thought ‚Äúthey‚Äôve forgotten to put in the SIM card‚Äù.</li>
<li>If you want to use the smartcard reader, <em>do not discard the SIM‚Äôs packaging</em>. You will need it to make the smartcard reader work reliably.</li>
</ul>
</li>
<li>
<p>A SIM card adapter.</p>
</li>
<li>
<p>A USB smartcard reader, and a USB-A to USB-C adapter.</p>
</li>
</ul>
<p>Including delivery, it came to about ¬£30.</p>
<h2 id="use-the-sims-packaging-to-make-it-fit-in-the-smartcard-reader">Use the SIM‚Äôs packaging to make it fit in the smartcard reader</h2>
<p>I spent a <em>lot</em> of time trying to get the SIM and smartcard reader working.</p>
<p>The solution was a frustratingly simple one: the best way to use the supplied smartcard reader is to use the original packaging for the SIM.</p>
<p>This packaging is the right size to slide into the reader while positioning the SIM‚Äôs contacts over the contact points on the reader. I will keep that together with the smartcard reader.</p>
<p>When I realised that - rather than trying to slide the popped-out SIM into the right place and keep it there - it Just Worked.</p>
<h2 id="adding-and-switching-esims">Adding and switching eSIMs</h2>
<p>The SIM card is advertised as have ‚Äúa memory capacity of 1.6M, [which] can store up to 50 groups of eSIM profile data‚Äù.</p>
<p>To make use of it, one needs to download one or more eSIMs to the SIM.</p>
<p>To do this, and to switch between profiles, one needs to use a ‚ÄúLocal Profile Agent‚Äù (or ‚ÄúLPA‚Äù).</p>
<p>I tested the process for this using both Android and Linux, and both worked just fine.</p>
<h2 id="test-esim-profiles-you-can-try-to-for-free">Test eSIM profiles you can try to for free</h2>
<p>While I was getting used to how it worked, I didn‚Äôt want to buy an actual eSIM.</p>
<p>Fortunately, there are a couple of options.</p>
<p>First, there are four official <a href="https://source.android.com/docs/core/connect/esim-test-profiles">Android test eSIM profiles</a></p>
<p>While I could start to provision these, I could not download them, with what appears to be a TLS error.</p>
<p>Second, osmocom has a <a href="https://euicc-manual.osmocom.org/docs/rsp/known-test-profile/">very useful page of other test profiles</a>.</p>
<p>I was able to install test eSIM profiles for <a href="https://qr.esim.tf/1$rsp.truphone.com$QR-G-5C-1LS-1W1Z9P7">TruPhone</a> and <a href="https://qr.esim.tf/1$rsp.truphone.com$QRF-SPEEDTEST">TruPhone / Speedtest</a>.</p>
<p>These provisioned correctly onto the SIM. I could not use them to make or receive calls or start a data session - which was fine - but they did show up in Android‚Äôs SIM card manager as available eSIMs.</p>
<h2 id="adding-and-switching-esims-via-android">Adding and switching eSIMs via Android</h2>
<p>There is an .apk from the 9eSIM site.</p>
<p>I wasn‚Äôt too keen on using this but, well, it worked, and is probably the simplest option.</p>
<p>I tested it first by putting the SIM into my phone, replacing my main physical SIM. This was recognised by the app immediately.</p>
<p>Once I had worked out how to correctly seat the SIM in the reader, I tried that too, using a USB-C hub to connect the smartcard reader to my phone. That worked fine too.</p>
<p>Provisioning eSIMs using the Android application was easy as long as the QR code was on a different screen, since I could just scan the QR code for the eSIM using the camera. I‚Äôve not looked to see if there is a way to use Android to import a QR code on its own screen.</p>
<p>Using the 9eSIM application, one can enable and disable eSIMs, and swap between them. Once enabled, they appeared in Android‚Äôs own SIM management settings. I did not need to reboot.</p>
<p>Deleting an eSIM is also easy. One needs to type a security phrase - the name of the eSIM - to trigger deletion, which is a simple means of avoiding accidental deletion.</p>
<h2 id="adding-and-switching-esims-via-linux">Adding and switching eSIMs via Linux</h2>
<p>Since I want to use the SIM with the integrated WWAN modem of a laptop running Linux, I was keen to see if I could get this all to work using Linux and Free software.</p>
<p>So far, I have not found a way of writing profiles to the SIM while it is in the laptop - I need to take it out and put it in the smartcard reader.</p>
<p>And, if I‚Äôm going to do that then, from a practical point of view, it is little more effort to hook it up to my phone and swap and provisions eSIMs from there.</p>
<p>Still, I wanted to get it working within Linux and FOSS just because.</p>
<h3 id="the-smartcard-reader-and-linux">The smartcard reader and Linux</h3>
<p>Connecting the smartcard reader and running <code>lsusb</code>:</p>
<pre tabindex="0"><code>Bus 001 Device 011: ID 058f:9540 Alcor Micro Corp. AU9540 Smartcard Reader
</code></pre><p>It looks like this is a quite common smartcard reader, sometimes built into laptops.</p>
<p><code>lsusb -s 1:11 -v</code>:</p>
<pre tabindex="0"><code>Bus 001 Device 011: ID 058f:9540 Alcor Micro Corp. AU9540 Smartcard Reader
Negotiated speed: Full Speed (12Mbps)
Device Descriptor:
  bLength                18
  bDescriptorType         1
  bcdUSB               2.01
  bDeviceClass            0 [unknown]
  bDeviceSubClass         0 [unknown]
  bDeviceProtocol         0 
  bMaxPacketSize0         8
  idVendor           0x058f Alcor Micro Corp.
  idProduct          0x9540 AU9540 Smartcard Reader
  bcdDevice            1.20
  iManufacturer           1 Generic
  iProduct                2 EMV Smartcard Reader
  iSerial                 0 
  bNumConfigurations      1
  Configuration Descriptor:
    bLength                 9
    bDescriptorType         2
    wTotalLength       0x005d
    bNumInterfaces          1
    bConfigurationValue     1
    iConfiguration          0 
    bmAttributes         0xa0
      (Bus Powered)
      Remote Wakeup
    MaxPower               50mA
    Interface Descriptor:
      bLength                 9
      bDescriptorType         4
      bInterfaceNumber        0
      bAlternateSetting       0
      bNumEndpoints           3
      bInterfaceClass        11 Chip/SmartCard
      bInterfaceSubClass      0 [unknown]
      bInterfaceProtocol      0 
      iInterface              0 
      ChipCard Interface Descriptor:
        bLength                54
        bDescriptorType        33
        bcdCCID              1.10
        nMaxSlotIndex           0
        bVoltageSupport         7  5.0V 3.0V 1.8V 
        dwProtocols             3  T=0 T=1
        dwDefaultClock       3700
        dwMaxiumumClock     12000
        bNumClockSupported      3
        dwDataRate           9946 bps
        dwMaxDataRate      688172 bps
        bNumDataRatesSupp.    138
        dwMaxIFSD             254
        dwSyncProtocols  00000007  2-wire 3-wire I2C
        dwMechanical     00000000 
        dwFeatures       000404BE
          Auto configuration based on ATR
          Auto activation on insert
          Auto voltage selection
          Auto clock change
          Auto baud rate change
          Auto PPS made by CCID
          Auto IFSD exchange (T=1)
          Short and extended APDU level exchange
        dwMaxCCIDMsgLen       272
        bClassGetResponse    echo
        bClassEnvelope       echo
        wlcdLayout           none
        bPINSupport             0 
        bMaxCCIDBusySlots       1
      Endpoint Descriptor:
        bLength                 7
        bDescriptorType         5
        bEndpointAddress     0x81  EP 1 IN
        bmAttributes            3
          Transfer Type            Interrupt
          Synch Type               None
          Usage Type               Data
        wMaxPacketSize     0x0004  1x 4 bytes
        bInterval               1
      Endpoint Descriptor:
        bLength                 7
        bDescriptorType         5
        bEndpointAddress     0x02  EP 2 OUT
        bmAttributes            2
          Transfer Type            Bulk
          Synch Type               None
          Usage Type               Data
        wMaxPacketSize     0x0010  1x 16 bytes
        bInterval               0
      Endpoint Descriptor:
        bLength                 7
        bDescriptorType         5
        bEndpointAddress     0x83  EP 3 IN
        bmAttributes            2
          Transfer Type            Bulk
          Synch Type               None
          Usage Type               Data
        wMaxPacketSize     0x0010  1x 16 bytes
        bInterval               0
Binary Object Store Descriptor:
  bLength                 5
  bDescriptorType        15
  wTotalLength       0x000c
  bNumDeviceCaps          1
  USB 2.0 Extension Device Capability:
    bLength                 7
    bDescriptorType        16
    bDevCapabilityType      2
    bmAttributes   0x00000002
      HIRD Link Power Management (LPM) Supported
Device Status:     0x0000
  (Bus Powered)
</code></pre><p>I spent quite a long time trying to work out why it would not detect the SIM in the smartcard reader (which, as above, was solved by using the supplied packaging material).</p>
<p>I have not reproduced all my troubleshooting here, but it was reasonably obvious from the error messages that, while my laptop recognised and could talk to the smartcard reader, the SIM was not recognised.</p>
<h3 id="lpac"><code>lpac</code></h3>
<p>I started with a command line tool called <a href="https://github.com/estkme-group/lpac"><code>lpac</code></a>.</p>
<p>It has a <a href="https://github.com/estkme-group/lpac/releases/download/v2.2.1/lpac_2.1.0_amd64.deb">.deb release version</a>, which I installed with:</p>
<pre tabindex="0"><code>sudo apt install lpac_2.1.0_amd64.deb -y
</code></pre><p>It installed without error to <code>/usr/bin/lpac</code>.</p>
<p><code>lpac</code>‚Äôs output is in json, so run it through <code>jq</code> to prettify it.</p>
<p><code>lpac chip info | jq</code> shows information about the SIM card / eUICC:</p>
<p>(I‚Äôve removed some potentially sensitive/personal bits)</p>
<pre tabindex="0"><code>{
  "type": "lpa",
  "payload": {
    "code": 0,
    "message": "success",
    "data": {
      "eidValue": "8904xxxxx",
      "EuiccConfiguredAddresses": {
        "defaultDpAddress": "smdp-plus-0.eu.cd.rsp.kigen.com",
        "rootDsAddress": "lpa.ds.gsma.com"
      },
      "EUICCInfo2": {
        "profileVersion": "2.3.1",
        "svn": "2.3.0",
        "euiccFirmwareVer": "36.17.4",
        "extCardResource": {
          "installedApplication": 11,
          "freeNonVolatileMemory": 1528320,
          "freeVolatileMemory": 32739
        },
        "uiccCapability": [
          "usimSupport",
          "isimSupport",
          "csimSupport",
          "akaMilenage",
          "akaCave",
          "akaTuak128",
          "akaTuak256",
          "gbaAuthenUsim",
          "gbaAuthenISim",
          "eapClient",
          "javacard",
          "multipleUsimSupport",
          "multipleIsimSupport",
          "multipleCsimSupport"
        ],
        "ts102241Version": "15.1.0",
        "globalplatformVersion": "2.3.0",
        "rspCapability": [
          "additionalProfile",
          "testProfileSupport"
        ],
        "euiccCiPKIdListForVerification": [
          "8137xxxxx"
        ],
        "euiccCiPKIdListForSigning": [
          "8137xxxxx"
        ],
        "euiccCategory": null,
        "ppVersion": "1.0.0",
        "sasAcreditationNumber": "KN-DN-UP-0924",
        "certificationDataObject": {
          "platformLabel": null,
          "discoveryBaseURL": null
        }
      },
      "rulesAuthorisationTable": [
        {
          "pprIds": [
            "ppr1",
            "ppr2"
          ],
          "allowedOperators": [
            {
              "plmn": "eeeeee",
              "gid1": null,
              "gid2": null
            }
          ],
          "pprFlags": []
        }
      ]
    }
  }
}
</code></pre><p>It appears to use a platform run by Kigen for remote SIM provisioning, so it looks like there is a dependency on a third party‚Äôs infrastructure to make all this work.</p>
<p>Use <code>lpac profile list | jq</code> to list the downloaded SIMs, and see their ICCID and AID.</p>
<p>To enable / disable downloaded eSIMs, use <code>lpac profile {enable,disable} ICCID/AID | jq</code>.</p>
<p>Using <a href="https://qr.esim.tf/1$rsp.truphone.com$QRF-SPEEDTEST">this test profile</a>, I tried adding/downloading SIMs via <code>lpac</code>.</p>
<p>Specifying the SM-DP+ address (<code>-s</code>) and the matching ID (<code>-m</code>) worked:</p>
<pre tabindex="0"><code>lpac profile download -s rsp.truphone.com -m QRF-SPEEDTEST`
</code></pre><p>Specifying the activation code (<code>-a</code>) in the manner set out in the documentation failed <em>(Edit: quoting issue; see below)</em>:</p>
<pre tabindex="0"><code>lpac profile download -a LPA:1$rsp.truphone.com$QRF-SPEEDTEST | jq
</code></pre><p>It returned:</p>
<pre tabindex="0"><code>{
  "type": "progress",
  "payload": {
    "code": 0,
    "message": "es10b_cancel_session",
    "data": null
  }
}
{
  "type": "progress",
  "payload": {
    "code": 0,
    "message": "es9p_cancel_session",
    "data": null
  }
}
{
  "type": "lpa",
  "payload": {
    "code": -1,
    "message": "activation_code",
    "data": "invalid"
  }
}
</code></pre><p>To check this was not just an issue with the test profiles, I bought a <a href="https://www.lycamobile.co.uk/en/bundles/sim-only-deals/#30-day-plans">30 day, ¬£2.50 LycaMobile eSIM</a> for testing, so that I could try it with an ‚Äúactual‚Äù eSIM, rather than a test eSIM.</p>
<p>Lyca‚Äôs email gave me a QR code, but did not specify (explicitly) the SM-DP+ address or matching ID.</p>
<p>The image‚Äôs alt-text was just my order number with Lyca.</p>
<p>I used GNOME‚Äôs <a href="https://apps.gnome.org/en-GB/Decoder/">‚ÄúDecoder‚Äù</a>, which lets one scan a QR card either from the device‚Äôs camera or from a screenshot.</p>
<p>That gave me the full activation code, which looked like this:</p>
<pre tabindex="0"><code>LPA:1$dp-plus-par07-01.oasis-smartsim.com$XHY48-xxxxx-xxxxx-xxxxx$x.x.x.x.x.x.xxxxx.x.x.x.x
</code></pre><p>(I don‚Äôt know how much of it is sensitive!)</p>
<p>This too failed using <code>lpac</code>.</p>
<p>I have not dug into <em>why</em> it failed.</p>
<p>I missed the opportunity to test extracting the parameters and pushing them manually into <code>lpac</code> via <code>-s</code> and <code>-m</code> as I had already used EasyLPAC at that point. I suspect that that would have worked.</p>
<p><em>Update</em>: doh, it is just a matter of quoting the string, with single quotes. So:</p>
<pre tabindex="0"><code>lpac profile download -a 'LPA:1$rsp.truphone.com$QRF-BETTERROAMING-PMRDGIR2EARDEIT5'
</code></pre><p>How embarrassing.</p>
<p>I tried to update the <code>lpac</code> documentation to make this more obvious, but <a href="https://github.com/estkme-group/lpac/pull/183#issuecomment-2602304507">the maintainer decided that it wasn‚Äôt needed</a>. So I guess this blogpost will have to do :)</p>
<p>Thanks to <a href="https://toot.wales/@tswsl">Tom</a> for that :)</p>
<h3 id="linux-gui-easylpac">Linux GUI: EasyLPAC</h3>
<p>There is a GUI for <code>lpac</code>, called <a href="https://github.com/creamlike1024/EasyLPAC">EasyLPAC</a>.</p>
<p>I used <a href="https://github.com/creamlike1024/EasyLPAC/releases/tag/0.7.7.2">the latest Github release</a>, with the version of <code>lpac</code> that I had already installed.</p>
<p>I did not install it; I just ran it, and it worked.</p>
<p>I tested, on a different machine, the version of EasyLPAC which comes with <code>lpac</code> included, but I could not get this to work. I did not pursue this.</p>
<p>I was able to download an eSIM using EasyLPAC in a few different ways:</p>
<ul>
<li>having an LPA:1 activation code on the system clipboard</li>
<li>having a QR code image on the system clipboard</li>
<li>‚Äúscanning an image file‚Äù, which consisted of opening a QR code saved as an image file on my computer</li>
</ul>
<p>It was really quite straightforward.</p>
<p>I noted that the instructions say:</p>
<blockquote>
<p>Note: Reading LPA activation code and QRCode from clipboard not working in Wayland</p>
</blockquote>
<p>But they did work for me, using Wayland.</p>
<p>Importing the LycaMobile eSIM, using the details extracted from the QR code, worked.</p>
<h2 id="connectivity-worked-fine-via-linux">Connectivity worked fine via Linux</h2>
<p>I put the 9eSIM into my laptop‚Äôs SIM slot, and booted it.</p>
<p>In GNOME‚Äôs ‚ÄúMobile Network‚Äù settings, it was recognised as a LycaMobile SIM.</p>
<p>I had to set the APN manually.</p>
<p>It took a couple of minutes for the eSIM to activate and, once activated, I was able to browse the Internet.</p>
<p>So there we go: an eSIM on a physical SIM on a laptop running Debian, with very little effort.</p>
<h2 id="other-options">Other options</h2>
<p>I quite like the look of <a href="https://shop.sysmocom.de/sysmoEUICC1-eUICC-for-consumer-eSIM-RSP/sysmoEUICC1">this Sysmocom SIM</a>.</p>
<p>I‚Äôd be interested in giving that a try some point.</p>



  <div>
	<hr>   
	<h2>You may also like:</h2>
	
    
</div>


</article>

        </div></div>]]></description>
        </item>
    </channel>
</rss>